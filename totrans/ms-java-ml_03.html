<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Unsupervised Machine Learning Techniques"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Unsupervised Machine Learning Techniques</h1></div></div></div><p>In the last chapter, we focused on supervised learning, that is, learning from a training dataset that was labeled. In the real world, obtaining data with labels is often difficult. In many domains, it is virtually impossible to label data either due to the cost of labeling or difficulty in labeling due to the sheer volume or velocity at which data is generated. In those situations, unsupervised learning, in its various forms, offers the right approaches to explore, visualize, and perform descriptive and predictive modeling. In many applications, unsupervised learning is often coupled with supervised learning as a first step to isolate interesting data elements for labeling. </p><p>In this chapter, we will focus on various methodologies, techniques, and algorithms that are practical and well-suited for unsupervised learning. We begin by noting the issues that are common between supervised and unsupervised learning when it comes to handling data and transformations. We will then briefly introduce the particular challenges faced in unsupervised learning owing to the lack of "ground truth" and the nature of learning under those conditions. </p><p>We will then discuss the techniques of feature analysis and dimensionality reduction applied to unlabeled datasets. This is followed by an introduction to the broad spectrum of clustering methods and discussions on the various algorithms in practical use, just as we did with supervised learning in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>, showing how each algorithm works, when to use it, and its advantages and limitations. We will conclude the section on clustering by presenting the different cluster evaluation techniques.</p><p>Following the treatment of clustering, we will approach the subject of outlier detection. We will contrast various <a id="id467" class="indexterm"/>techniques and algorithms that illustrate what makes some objects outliers—also called anomalies—within a given dataset.</p><p>The chapter will conclude with clustering and outlier detection experiments, conducted with a real-world dataset and an analysis of the results obtained. In this case study, we will be using ELKI and SMILE Java libraries for the machine learning tasks and will present code and results from the experiments. We hope that this will provide the reader with a sense of the power and ease of use of these tools. </p><div class="section" title="Issues in common with supervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Issues in common with supervised learning</h1></div></div></div><p>Many <a id="id468" class="indexterm"/>of the issues that we discussed related to supervised learning are also common with unsupervised learning. Some of them are listed here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Types of features handled by the algorithm</strong></span>: Most clustering and outlier algorithms need numeric representation to work effectively. Transforming categorical or ordinal data has to be done carefully</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Curse of dimensionality</strong></span>: Having a large number of features results in sparse spaces and affects the performance of clustering algorithms. Some option must be chosen to suitably reduce dimensionality—either feature selection where only a subset of the most relevant features are retained, or feature extraction, which transforms the feature space into a new set of principal variables of a lower dimensional space</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Scalability in memory and training time</strong></span>: Many unsupervised learning algorithms cannot scale up to more than a few thousands of instances either due to memory or training time constraints</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Outliers and noise in data</strong></span>: Many algorithms are affected by noise in the features, the presence of anomalous data, or missing values. They need to be transformed and handled appropriately</li></ul></div></div></div>
<div class="section" title="Issues specific to unsupervised learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Issues specific to unsupervised learning</h1></div></div></div><p>The following <a id="id469" class="indexterm"/>are some issues that pertain to unsupervised learning techniques:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Parameter setting</strong></span>: Deciding on number of features, usefulness of features, number of clusters, shapes of clusters, and so on, pose enormous challenges to certain unsupervised methods</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Evaluation methods</strong></span>: Since unsupervised learning methods are ill-posed due to lack of ground-truth, evaluation of algorithms becomes very subjective.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Hard or soft labeling</strong></span>: Many unsupervised learning problems require giving labels <a id="id470" class="indexterm"/>to the data in an exclusive or probabilistic manner. This poses a problem for many algorithms</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Interpretability of results and models</strong></span>: Unlike supervised learning, the lack of ground truth and the nature of some algorithms make interpreting the results from both model and labeling even more difficult</li></ul></div></div>
<div class="section" title="Feature analysis and dimensionality reduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec28"/>Feature analysis and dimensionality reduction</h1></div></div></div><p>Among <a id="id471" class="indexterm"/>the first tools to master are the different feature analysis <a id="id472" class="indexterm"/>and dimensionality reduction techniques. As in supervised learning, the need for reducing dimensionality arises from numerous reasons similar to those discussed earlier for feature selection and reduction.</p><p>A smaller number of discriminating dimensions makes visualization of data and clusters much easier. In many applications, unsupervised dimensionality reduction techniques are used for compression, which can then be used for transmission or storage of data. This is particularly useful when the larger data has an overhead. Moreover, applying dimensionality reduction techniques can improve the scalability in terms of memory and computation speeds of many algorithms.</p><div class="section" title="Notation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec30"/>Notation</h2></div></div></div><p>We will <a id="id473" class="indexterm"/>use similar notation to what was used in the <a id="id474" class="indexterm"/>chapter on supervised learning. The examples are in <span class="emphasis"><em>d</em></span> dimensions and are represented as vector:</p><p><span class="strong"><strong>x </strong></span>= <span class="emphasis"><em>(x</em></span><span class="emphasis"><em><sub>1</sub></em></span><span class="emphasis"><em>,x</em></span><span class="emphasis"><em><sub>2</sub></em></span><span class="emphasis"><em>,…x</em></span><span class="emphasis"><em><sub>d</sub></em></span><span class="emphasis"><em> )</em></span><span class="emphasis"><em><sup>T</sup></em></span></p><p>The entire dataset containing <span class="emphasis"><em>n</em></span> examples can be represented as an observation matrix:</p><div class="mediaobject"><img src="graphics/B05137_03_005.jpg" alt="Notation"/></div><p>The idea of dimensionality reduction is to find k ≤ <span class="emphasis"><em>d</em></span> features either by transformation of the input features, projecting or combining them such that the lower dimension <span class="emphasis"><em>k</em></span> captures or preserves <a id="id475" class="indexterm"/>interesting properties of the original <a id="id476" class="indexterm"/>dataset.</p></div><div class="section" title="Linear methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec31"/>Linear methods</h2></div></div></div><p>Linear <a id="id477" class="indexterm"/>dimensionality methods are some of the <a id="id478" class="indexterm"/>oldest statistical techniques to reduce features or transform the data into lower dimensions, preserving interesting discriminating properties. </p><p>Mathematically, with linear methods we are performing a transformation, such that a new data element is created using a linear transformation of the original data element:</p><div class="mediaobject"><img src="graphics/B05137_03_008.jpg" alt="Linear methods"/></div><p>
<span class="strong"><strong>s = Wx</strong></span>
</p><p>Here, <span class="strong"><strong>W</strong></span><sub>k × d</sub> is the linear transformation matrix. The variables <span class="strong"><strong>s</strong></span> are also referred to as latent or hidden variables.</p><p>In this topic, we will discuss the two most practical and often-used methodologies. We will list some variants of these techniques so that the reader can use the tools to experiment with them. The main assumption here—which often forms the limitation—is the linear relationships between the transformations.</p><div class="section" title="Principal component analysis (PCA)"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec28"/>Principal component analysis (PCA)</h3></div></div></div><p>PCA is a <a id="id479" class="indexterm"/>widely-used technique <a id="id480" class="indexterm"/>for dimensionality reduction(<span class="emphasis"><em>References</em></span> [1]). The original coordinate system is rotated to a new coordinate system that exploits the directions of maximum variance in the data, resulting in uncorrelated variables in a lower-dimensional subspace that were correlated in the original feature space. PCA is sensitive to the scaling of the features. </p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec41"/>Inputs and outputs</h4></div></div></div><p>PCA is generally <a id="id481" class="indexterm"/>effective on numeric <a id="id482" class="indexterm"/>datasets. Many tools provide the categorical-to-continuous transformations for the nominal features, but this affects the performance. The number of principal components, or <span class="emphasis"><em>k</em></span>, is also an input provided by the user.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec42"/>How does it work?</h4></div></div></div><p>PCA, in its <a id="id483" class="indexterm"/>most basic form, tries to find projections of data onto new axes, which are known as <span class="strong"><strong>principal components</strong></span>. Principal <a id="id484" class="indexterm"/>components are projections that capture maximum variance directions from the original space. In simple words, PCA finds the first principal component through rotation of the original axes of the data in the direction of maximum variance. The technique finds the next principal component by again determining the next best axis, orthogonal to the first axis, by seeking the second highest variance and so on until most variances are captured. Generally, most tools give either a choice of number of principal components or the option to keep finding components until some percentage, for example, 99%, of variance in the original dataset is captured.</p><p>Mathematically, the objective of finding maximum variance can be written as</p><div class="mediaobject"><img src="graphics/B05137_03_013.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_03_014.jpg" alt="How does it work?"/></div><p>
<span class="emphasis"><em>λ</em></span>
<span class="strong"><strong>v</strong></span> = <span class="strong"><strong>Cv</strong></span> is the eigendecomposition</p><p>This is equivalent to:</p><div class="mediaobject"><img src="graphics/B05137_03_017.jpg" alt="How does it work?"/></div><p>Here, <span class="strong"><strong>W</strong></span> is the principal components and <span class="strong"><strong>S</strong></span> is the new transformation of the input data. Generally, eigenvalue decomposition or singular value decomposition is used in the computation part.</p><div class="mediaobject"><img src="graphics/B05137_03_021.jpg" alt="How does it work?"/><div class="caption"><p>Figure 1: Principal Component Analysis</p></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec43"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">One <a id="id485" class="indexterm"/>of the advantages of PCA is that<a id="id486" class="indexterm"/> it is optimal in that it minimizes the reconstruction error of the data.</li><li class="listitem" style="list-style-type: disc">PCA assumes normal distribution. </li><li class="listitem" style="list-style-type: disc">The computation of variance-covariance matrix can become intensive for large datasets with high-dimensions. Alternatively, <span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>) can be used as it <a id="id487" class="indexterm"/>works iteratively and there is no need for an explicit covariance matrix.</li><li class="listitem" style="list-style-type: disc">PCA has issues when there is noise in the data. </li><li class="listitem" style="list-style-type: disc">PCA fails when the data lies in the complex manifold, a topic that we will discuss in the non-linear dimensionality reduction section.</li><li class="listitem" style="list-style-type: disc">PCA assumes a correlation between the features and in the absence of those correlations, it is unable to do any transformations; instead, it simply ranks them.</li><li class="listitem" style="list-style-type: disc">By transforming the original feature space into a new set of variables, PCA causes a loss in interpretability of the data.</li><li class="listitem" style="list-style-type: disc">There are many other variants of PCA that are popular and overcome some of the biases and assumptions of PCA.</li></ul></div><p>
<span class="strong"><strong>Independent Component Analysis</strong></span> (<span class="strong"><strong>ICA</strong></span>) assumes that there are mixtures of non-Gaussians <a id="id488" class="indexterm"/>from the source and, using the <a id="id489" class="indexterm"/>generative technique, tries to find <a id="id490" class="indexterm"/>the decompositions of original data in the smaller mixtures or components (<span class="emphasis"><em>References</em></span> [2]). The key difference between PCA and ICA is that PCA creates components that are uncorrelated, while ICA creates components that are independent. </p><p>Mathematically, it assumes <span class="inlinemediaobject"><img src="graphics/B05137_03_022.jpg" alt="Advantages and limitations"/></span> as a mixture of independent sources ∈ <span class="inlinemediaobject"><img src="graphics/B05137_03_024.jpg" alt="Advantages and limitations"/></span>, such that each data element <span class="emphasis"><em>y</em></span> = [<span class="emphasis"><em>y</em></span>
<span class="emphasis"><em><sup>1</sup></em></span>
<span class="emphasis"><em>,y</em></span>
<span class="emphasis"><em><sup>2</sup></em></span>
<span class="emphasis"><em>,….y</em></span>
<span class="emphasis"><em><sup>k</sup></em></span>
<span class="emphasis"><em> </em></span>]<span class="emphasis"><em><sup>T</sup></em></span> and independence is implied by <span class="inlinemediaobject"><img src="graphics/B05137_03_026.jpg" alt="Advantages and limitations"/></span>:</p><p>
<span class="strong"><strong>Probabilistic Principal Component Analysis</strong></span> (<span class="strong"><strong>PPCA</strong></span>) is based on finding the components <a id="id491" class="indexterm"/>using mixture models <a id="id492" class="indexterm"/>and maximum likelihood formulations using <span class="strong"><strong>Expectation Maximization</strong></span> (<span class="strong"><strong>EM</strong></span>) (<span class="emphasis"><em>References</em></span> [3]). It overcomes the issues of missing data and outlier impacts that PCA faces.</p></div></div><div class="section" title="Random projections (RP)"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec29"/>Random projections (RP)</h3></div></div></div><p>When <a id="id493" class="indexterm"/>data is separable by a large margin—even if it is high-dimensional data—one can randomly project the data down to a low-dimensional <a id="id494" class="indexterm"/>space without impacting separability and achieve good generalization with a relatively small amount of data. Random Projections use this technique and the details are described here (<span class="emphasis"><em>References</em></span> [4]).</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec44"/>Inputs and outputs</h4></div></div></div><p>Random <a id="id495" class="indexterm"/>projections work with both numeric and categorical <a id="id496" class="indexterm"/>features, but categorical features are transformed into binary. Outputs are lower dimensional representations of the input data elements. The number of dimensions to project, <span class="emphasis"><em>k</em></span>, is part of user-defined input.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec45"/>How does it work?</h4></div></div></div><p>This technique uses random projection matrices to project the input data into a lower dimensional space. The original data <span class="inlinemediaobject"><img src="graphics/B05137_03_027.jpg" alt="How does it work?"/></span> is transformed to the lower dimension space <span class="inlinemediaobject"><img src="graphics/B05137_03_028.jpg" alt="How does it work?"/></span> where <span class="emphasis"><em>k &lt;&lt; p</em></span> using:</p><div class="mediaobject"><img src="graphics/B05137_03_030.jpg" alt="How does it work?"/></div><p>Here columns <a id="id497" class="indexterm"/>in the <span class="emphasis"><em>k</em></span> x <span class="emphasis"><em>d</em></span> matrix <span class="strong"><strong>R</strong></span> are i.i.d zero mean normal variables and are scaled to unit length. There are variants of how the random matrix <span class="strong"><strong>R</strong></span> is constructed using probabilistic sampling. Computational complexity of RP is <span class="emphasis"><em>O(knd)</em></span>, which scales much better than PCA. In many practical datasets, it has been shown that RP gives results comparable to PCA and can scale to large dimensions and datasets.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec46"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It scales<a id="id498" class="indexterm"/> to very large values of dataset size and <a id="id499" class="indexterm"/>dimensionalities. In text and image learning problems, with large dimensions, this technique has been successfully used as the preprocessing technique.</li><li class="listitem" style="list-style-type: disc">Sometimes a large information loss can occur while using RP. </li></ul></div></div></div><div class="section" title="Multidimensional Scaling (MDS)"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec30"/>Multidimensional Scaling (MDS)</h3></div></div></div><p>There <a id="id500" class="indexterm"/>are many forms of MDS—classical, metric, and non-metric. The main idea of MDS is to preserve the pairwise <a id="id501" class="indexterm"/>similarity/distance values. It generally involves transforming the high dimensional data into two or three dimensions (<span class="emphasis"><em>References</em></span> [5]). </p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec47"/>Inputs and outputs</h4></div></div></div><p>MDS can <a id="id502" class="indexterm"/>work with both numeric and categorical <a id="id503" class="indexterm"/>data based on the user-selected distance function. The number of dimensions to transform to, <span class="emphasis"><em>k</em></span>, is a user-defined input.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec48"/>How does it work?</h4></div></div></div><p>Given <span class="emphasis"><em>n</em></span> data <a id="id504" class="indexterm"/>elements, an <span class="emphasis"><em>n</em></span> x <span class="emphasis"><em>n</em></span> affinity or distance matrix is computed. There are choices of using distances such as Euclidean, Mahalanobis, or similarity concepts such as cosine similarity, Jaccard coefficients, and so on. MDS in its very basic form tries to find a mapping of the distance matrix in a <a id="id505" class="indexterm"/>lower dimensional space where the Euclidean distance between the transformed points is similar to the affinity matrix. </p><p>Mathematically:</p><div class="mediaobject"><img src="graphics/B05137_03_036.jpg" alt="How does it work?"/></div><p>Here <span class="inlinemediaobject"><img src="graphics/B05137_03_037.jpg" alt="How does it work?"/></span> input space and <span class="inlinemediaobject"><img src="graphics/B05137_03_038.jpg" alt="How does it work?"/></span> mapped space.</p><p>If the input affinity space is transformed using kernels then the MDS becomes a non-linear method for dimensionality reduction. Classical MDS is equivalent to PCA when the distances between the points in input space is Euclidean distance.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec49"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The key <a id="id506" class="indexterm"/>disadvantage is the subjective <a id="id507" class="indexterm"/>choice of the lower dimension needed to interpret the high dimensional data, normally restricted to two or three for humans. Some data may not map effectively in this lower dimensional space.</li><li class="listitem" style="list-style-type: disc">The advantage is you can perform linear and non-linear mapping to the lowest dimensions using the framework. </li></ul></div></div></div></div><div class="section" title="Nonlinear methods"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec32"/>Nonlinear methods</h2></div></div></div><p>In general, nonlinear <a id="id508" class="indexterm"/>dimensionality reduction <a id="id509" class="indexterm"/>involves either performing nonlinear transformations to the computations in linear methods such as KPCA or finding nonlinear relationships in the lower dimension as in manifold learning. In some domains and datasets, the structure of the data in lower dimensions is nonlinear—and that is where techniques such as KPCA are effective—while in some domains the data does not unfold in lower dimensions and you need manifold learning. </p><div class="section" title="Kernel Principal Component Analysis (KPCA)"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec31"/>Kernel Principal Component Analysis (KPCA)</h3></div></div></div><p>Kernel <a id="id510" class="indexterm"/>PCA uses <a id="id511" class="indexterm"/>the Kernel trick described in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>, with the PCA algorithm for transforming the data in a high-dimensional space to find effective mapping (<span class="emphasis"><em>References</em></span> [6]).</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec50"/>Inputs and outputs</h4></div></div></div><p>Similar to <a id="id512" class="indexterm"/>PCA with addition of <a id="id513" class="indexterm"/>choice of kernel and kernel parameters. For example, if <span class="strong"><strong>Radial Basis Function</strong></span> (<span class="strong"><strong>RBF</strong></span>) or Gaussian Kernel is chosen, then <a id="id514" class="indexterm"/>the kernel, along with the gamma parameter, becomes user-selected values.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec51"/>How does it work?</h4></div></div></div><p>In the same way as <span class="strong"><strong>Support Vector Machines</strong></span> (<span class="strong"><strong>SVM</strong></span>) was discussed in the previous chapter, KPCA <a id="id515" class="indexterm"/>transforms the <a id="id516" class="indexterm"/>input space to high dimensional feature space using the "kernel trick". The entire PCA machinery of finding maximum variance is then carried out in the transformed space. </p><p>As in PCA:</p><div class="mediaobject"><img src="graphics/B05137_03_014.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_03_040.jpg" alt="How does it work?"/></div><p>Instead of linear covariance matrix, a nonlinear transformation is applied to the input space using kernel methods by constructing the <span class="emphasis"><em>N</em></span> x <span class="emphasis"><em>N</em></span> matrix, in place of doing the actual transformations using <span class="emphasis"><em>ϕ</em></span>
<span class="emphasis"><em>(x)</em></span>.</p><p>
<span class="emphasis"><em>k(x,y) = ((</em></span>
<span class="emphasis"><em>ϕ</em></span>
<span class="emphasis"><em>(x),</em></span>
<span class="emphasis"><em>ϕ</em></span>
<span class="emphasis"><em>(y)) = </em></span>
<span class="emphasis"><em>ϕ</em></span>
<span class="emphasis"><em>(x)</em></span>
<span class="emphasis"><em><sup>T</sup></em></span>
<span class="emphasis"><em> </em></span>
<span class="emphasis"><em>ϕ</em></span>
<span class="emphasis"><em>(y)</em></span>
</p><p>Since the kernel transformation doesn't actually transform the features into explicit feature space, the principal components found can be interpreted as projections of data onto the components. In the following figure, a binary nonlinear dataset, generated using the scikit-learn example on circles (<span class="emphasis"><em>References</em></span> [27]), demonstrates the linear separation after KPCA <a id="id517" class="indexterm"/>using the RBF kernel and returning to almost similar input space by the inverse transform:</p><div class="mediaobject"><img src="graphics/B05137_03_046.jpg" alt="How does it work?"/><div class="caption"><p>Figure 2: KPCA on Circle Dataset and Inverse Transform.</p></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec52"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">KPCA <a id="id518" class="indexterm"/>overcomes the nonlinear <a id="id519" class="indexterm"/>mapping presented by PCA. </li><li class="listitem" style="list-style-type: disc">KPCA has similar issues with outlier, noisy, and missing values to standard PCA. There are robust methods and variations to overcome this.</li><li class="listitem" style="list-style-type: disc">KPCA has scalability issues in space due to an increase in the kernel matrix, which can become a bottleneck in large datasets with high dimensions. SVD can be used in these situations, as an alternative.</li></ul></div></div></div><div class="section" title="Manifold learning"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec32"/>Manifold learning</h3></div></div></div><p>When <a id="id520" class="indexterm"/>high dimensional data is embedded in lower <a id="id521" class="indexterm"/>dimensions that are nonlinear, but have complex structure, manifold learning is very effective.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec53"/>Inputs and outputs</h4></div></div></div><p>Manifold <a id="id522" class="indexterm"/>learning algorithms require two user-provided parameters: <span class="emphasis"><em>k</em></span>, representing <a id="id523" class="indexterm"/>the number of neighbors for the initial search, and <span class="emphasis"><em>n</em></span>, the number of manifold coordinates. </p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec54"/>How does it work?</h4></div></div></div><p>As seen<a id="id524" class="indexterm"/> in the following figure, the three-dimensional S-Curve, plotted using the scikit-learn utility (<span class="emphasis"><em>References</em></span> [27]), is represented in 2D PCA and in 2D manifold using LLE. It is interesting to observe how the blue, green, and red dots are mixed up in the PCA representation while the manifold learning representation using LLE cleanly separates the colors. It can also be observed that the rank ordering of Euclidean distances is not maintained in the manifold representation:</p><div class="mediaobject"><img src="graphics/B05137_03_049.jpg" alt="How does it work?"/><div class="caption"><p>Figure 3: Data representation after PCA and manifold learning</p></div></div><p>To preserve the structure, the geodesic distance is preserved instead of the Euclidean distance. The general approach is to build a graph structure such as an adjacency matrix, and then compute geodesic distance using different assumptions. In the Isomap Algorithm, the global pairwise distances are preserved (<span class="emphasis"><em>References</em></span> [7]). In the <span class="strong"><strong>Local Linear Embedding</strong></span> (<span class="strong"><strong>LLE</strong></span>) Algorithm, the mapping <a id="id525" class="indexterm"/>is done to take care of local neighborhood, that is, nearby <a id="id526" class="indexterm"/>points map to nearby points in the transformation (<span class="emphasis"><em>References</em></span> [9]). Laplacian Eigenmaps is similar to LLE, except it tries to maintain the "locality" instead of "local linearity" in LLE by using graph Laplacian (<span class="emphasis"><em>References</em></span> [8]).</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec55"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Isomap is <a id="id527" class="indexterm"/>non-parametric; it preserves the global structure, and <a id="id528" class="indexterm"/>has no local optimum, but is hampered by speed.</li><li class="listitem" style="list-style-type: disc">LLE and Laplacian Eigenmaps are non-parametric, have <a id="id529" class="indexterm"/>no local optima, are fast, but don't preserve global structure.</li></ul></div></div></div></div></div>
<div class="section" title="Clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Clustering</h1></div></div></div><p>Clustering <a id="id530" class="indexterm"/>algorithms can be categorized in different ways based on the techniques, the outputs, the process, and other considerations. In this topic, we will present some of the most widely used clustering algorithms.</p><div class="section" title="Clustering algorithms"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec33"/>Clustering algorithms</h2></div></div></div><p>There <a id="id531" class="indexterm"/>is a rich set of clustering techniques in use today for a wide variety of applications. This section presents some of them, explaining how they work, what kind of data they can be used with, and what their advantages and drawbacks are. These include algorithms that are prototype-based, density-based, probabilistic partition-based, hierarchy-based, graph-theory-based, and those based on neural networks.  </p><div class="section" title="k-Means"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec33"/>k-Means</h3></div></div></div><p>k-means is a <a id="id532" class="indexterm"/>centroid- or prototype-based iterative algorithm that<a id="id533" class="indexterm"/> employs partitioning and relocation methods (<span class="emphasis"><em>References</em></span> [10]). k-means finds clusters of spherical shape depending on the distance metric used, as in the case of Euclidean distance. </p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec56"/>Inputs and outputs</h4></div></div></div><p>k-means <a id="id534" class="indexterm"/>can handle mostly numeric features. Many tools provide categorical <a id="id535" class="indexterm"/>to numeric transformations, but having a large number of categoricals in the computation can lead to non-optimal clusters. User-defined <span class="emphasis"><em>k</em></span>, the number of clusters to be found, and the distance metric to use for computing closeness are two basic inputs. k-means <a id="id536" class="indexterm"/>generates clusters, association of data to each cluster, and <a id="id537" class="indexterm"/>centroids of clusters as the output.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec57"/>How does it work?</h4></div></div></div><p>The most <a id="id538" class="indexterm"/>common variant known as Lloyd's algorithm initializes <span class="emphasis"><em>k</em></span> centroids <a id="id539" class="indexterm"/>for the given dataset by picking data elements randomly from the set. It assigns each data element to the centroid it is closest to, using some distance metric such as Euclidean distance. It then computes the mean of the data points for each cluster to form the new centroid and the process is repeated until either the maximum number of iterations is reached or there is no change in the centroids. </p><p>Mathematically, each step of the clustering can be seen as an optimization step where the equation to optimize is given by:</p><div class="mediaobject"><img src="graphics/B05137_03_053.jpg" alt="How does it work?"/></div><p>Here, ci is all points belong to cluster <span class="emphasis"><em>i</em></span>. The problem of minimizing is classified as NP-hard and hence k-Means has a tendency to get stuck in local optimum.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec58"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The choice of<a id="id540" class="indexterm"/> the number of clusters, <span class="emphasis"><em>k</em></span>, is difficult to pick, but normally <a id="id541" class="indexterm"/>search techniques such as varying <span class="emphasis"><em>k</em></span> for different values and measuring metrics such as sum of square errors can be used to find a good threshold. For smaller datasets, hierarchical k-Means can be tried.</li><li class="listitem" style="list-style-type: disc">k-means can converge faster than most algorithms for smaller values of <span class="emphasis"><em>k</em></span> and can find effective global clusters.</li><li class="listitem" style="list-style-type: disc">k-means convergence can be affected by initialization of the centroids and hence there are many variants to perform random restarts with different seeds and so on.</li><li class="listitem" style="list-style-type: disc">k-means can perform badly when there are outliers and noisy data points. Using robust techniques such as medians instead of means, k-Medoids, overcomes this to a certain extent.</li><li class="listitem" style="list-style-type: disc">k-means does not <a id="id542" class="indexterm"/>find effective clusters when they are of arbitrary shapes <a id="id543" class="indexterm"/>or have different densities.</li></ul></div></div></div><div class="section" title="DBSCAN"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec34"/>DBSCAN</h3></div></div></div><p>Density-based <a id="id544" class="indexterm"/>spatial clustering of applications with <a id="id545" class="indexterm"/>noise (DBSCAN) is a density-based partitioning algorithm. It separates dense region in the space from sparse regions (<span class="emphasis"><em>References</em></span> [14]). </p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec59"/>Inputs and outputs</h4></div></div></div><p>Only <a id="id546" class="indexterm"/>numeric features are used in DBSCAN. The user-defined <a id="id547" class="indexterm"/>parameters are <span class="emphasis"><em>MinPts</em></span> and the neighborhood factor given by <span class="emphasis"><em>ϵ</em></span>.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec60"/>How does it work?</h4></div></div></div><p>The algorithm <a id="id548" class="indexterm"/>first finds the ϵ-neighborhood of every point <span class="emphasis"><em>p</em></span>, given by <span class="inlinemediaobject"><img src="graphics/B05137_03_058.jpg" alt="How does it work?"/></span>. A <span class="emphasis"><em>high density</em></span> region is identified as a region where the number of points in a ϵ-neighborhood is greater than or equal to the given <span class="emphasis"><em>MinPts</em></span>; the point such a ϵ-neighborhood is defined around is called a <span class="emphasis"><em>core points</em></span>. Points within the ϵ-neighborhood of a <span class="emphasis"><em>core point</em></span> are said to be <span class="emphasis"><em>directly reachable</em></span>. All <span class="emphasis"><em>core points</em></span> that can in effect be reached by hopping from one directly reachable core point to another point <span class="emphasis"><em>directly reachable</em></span> from the second point, and so on, are considered to be in the same cluster. Further, any point that has fewer than <span class="emphasis"><em>MinPts</em></span> in its <span class="emphasis"><em>ϵ</em></span>-neighborhood, but is directly reachable from a core point, belongs to the same cluster as the core point. These points at the edge of a cluster are called <span class="emphasis"><em>border points</em></span>. A <span class="emphasis"><em>noise point</em></span> is any point that is neither a core point nor a border point. </p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec61"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The DBSCAN algorithm <a id="id549" class="indexterm"/>does not require the number of clusters to be specified <a id="id550" class="indexterm"/>and can find it automatically from the data.</li><li class="listitem" style="list-style-type: disc">DBSCAN can find clusters of various shapes and sizes.</li><li class="listitem" style="list-style-type: disc">DBSCAN has in-built robustness to noise and can find outliers from the datasets.</li><li class="listitem" style="list-style-type: disc">DBSCAN is not completely deterministic in its identification of the points and its categorization into border or core depends on the order of data processed. </li><li class="listitem" style="list-style-type: disc">Distance metrics selected such as Euclidean distance can often affect performance due to the curse of <a id="id551" class="indexterm"/>dimensionality.</li><li class="listitem" style="list-style-type: disc">When there are <a id="id552" class="indexterm"/>clusters with large variations in the densities, the static choice of <span class="emphasis"><em>{MinPts, </em></span><span class="emphasis"><em>ϵ</em></span><span class="emphasis"><em>}</em></span> can pose a big limitation.</li></ul></div></div></div><div class="section" title="Mean shift"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec35"/>Mean shift</h3></div></div></div><p>Mean <a id="id553" class="indexterm"/>shift is a very effective clustering algorithm <a id="id554" class="indexterm"/>in many image, video, and motion detection based datasets (<span class="emphasis"><em>References</em></span> [11]). </p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec62"/>Inputs and outputs</h4></div></div></div><p>Only <a id="id555" class="indexterm"/>numeric features are accepted as data input in the mean shift <a id="id556" class="indexterm"/>algorithm. The choice of kernel and the bandwidth of the kernel are user-driven choices that affect the performance. Mean shift generates modes of data points and clusters data around the modes.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec63"/>How does it work?</h4></div></div></div><p>Mean <a id="id557" class="indexterm"/>shift is based on the statistical concept of <span class="strong"><strong>kernel density estimation</strong></span> (<span class="strong"><strong>KDE</strong></span>), which <a id="id558" class="indexterm"/>is a probabilistic method to estimate the underlying data distribution from the sample.</p><p>A kernel density estimate for kernel <span class="emphasis"><em>K</em></span> (<span class="strong"><strong>x</strong></span>) of given bandwidth <span class="emphasis"><em>h</em></span> is given by:</p><div class="mediaobject"><img src="graphics/B05137_03_063.jpg" alt="How does it work?"/></div><p>For <span class="emphasis"><em>n</em></span> points with dimensionality <span class="emphasis"><em>d</em></span>. The mean shift algorithm works by moving each data point in the direction of local increasing density. To estimate this direction, gradient is applied to the KDE and the gradient takes the form of:</p><div class="mediaobject"><img src="graphics/B05137_03_065.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_03_066.jpg" alt="How does it work?"/></div><p>Here g(<span class="strong"><strong>x</strong></span>)= –K'(<span class="strong"><strong>x</strong></span>) is the derivative of the kernel. The vector, m(<span class="strong"><strong>x</strong></span>), is called the mean shift vector and it is used to move the points in the direction </p><p>
<span class="strong"><strong>x</strong></span><sup>(t+1)</sup> = <span class="strong"><strong>x</strong></span><sup>t</sup> + m(<span class="strong"><strong>x</strong></span>)</p><p>Also, it is <a id="id559" class="indexterm"/>guaranteed to converge when the gradient of the density function is zero. Points that end up in a similar location are marked as clusters belonging to the same region.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec64"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Mean shift <a id="id560" class="indexterm"/>is non-parametric and makes no underlying assumption <a id="id561" class="indexterm"/>on the data distribution. </li><li class="listitem" style="list-style-type: disc">It can find non-complex clusters of varying shapes and sizes.</li><li class="listitem" style="list-style-type: disc">There is no need to explicitly give the number of clusters; the choice of the bandwidth parameter, which is used in estimation, implicitly controls the clusters.</li><li class="listitem" style="list-style-type: disc">Mean shift has no local optima for a given bandwidth parameter and hence it is deterministic.</li><li class="listitem" style="list-style-type: disc">Mean shift is robust to outliers and noisy points because of KDE.</li><li class="listitem" style="list-style-type: disc">The mean shift algorithm is computationally slow and does not scale well with large datasets. </li><li class="listitem" style="list-style-type: disc">Bandwidth selection should be done judiciously; otherwise it can result in merged modes, or the appearance of extra, shallow modes. </li></ul></div></div></div><div class="section" title="Expectation maximization (EM) or Gaussian mixture modeling (GMM)"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec36"/>Expectation maximization (EM) or Gaussian mixture modeling (GMM)</h3></div></div></div><p>GMM or <a id="id562" class="indexterm"/>EM is a probabilistic <a id="id563" class="indexterm"/>partition-based <a id="id564" class="indexterm"/>method that partitions data into <span class="emphasis"><em>k</em></span> clusters <a id="id565" class="indexterm"/>using probability distribution-based techniques (<span class="emphasis"><em>References</em></span> [13]).</p><div class="section" title="Input and output"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec65"/>Input and output</h4></div></div></div><p>Only <a id="id566" class="indexterm"/>numeric features are allowed in EM/GMM. The <a id="id567" class="indexterm"/>model parameter is the number of mixture components, given by <span class="emphasis"><em>k</em></span>.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec66"/>How does it work?</h4></div></div></div><p>GMM is a <a id="id568" class="indexterm"/>generative method that assumes that there are <span class="emphasis"><em>k</em></span> Gaussian components, each Gaussian component has a mean <span class="emphasis"><em>µ</em></span><sub>i</sub> and covariance Ʃ<sub>i</sub>. The following expression represents the probability of the dataset given the <span class="emphasis"><em>k</em></span> Gaussian components:</p><div class="mediaobject"><img src="graphics/B05137_03_072.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_03_073.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_03_074.jpg" alt="How does it work?"/></div><p>The two-step task of finding the means {<span class="strong"><strong>µ</strong></span><sub>1</sub>, <span class="strong"><strong>µ</strong></span><sub>2</sub>, …<span class="strong"><strong>µ</strong></span><sub>k</sub>} for each of the <span class="emphasis"><em>k</em></span> Gaussian components such that the data points assigned to each maximizes the probability of that component is <a id="id569" class="indexterm"/>done using the <span class="strong"><strong>Expectation Maximization</strong></span> (<span class="strong"><strong>EM</strong></span>) process. </p><p>The iterative process can be defined into an E-step, that computes the <span class="emphasis"><em>expected</em></span> cluster for all data points for the cluster, in an iteration <span class="emphasis"><em>i</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_03_077.jpg" alt="How does it work?"/></div><p>The M-step maximizes to compute <span class="emphasis"><em>µ</em></span>t+1 given the data points belonging to the cluster:</p><div class="mediaobject"><img src="graphics/B05137_03_079.jpg" alt="How does it work?"/></div><p>The EM process can result in GMM convergence into local optimum.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec67"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Works very <a id="id570" class="indexterm"/>well with any features; for <a id="id571" class="indexterm"/>categorical data, discrete probability is calculated, while for numeric a continuous probability function is estimated. </li><li class="listitem" style="list-style-type: disc">It has computational scalability problems. It can result in local optimum.</li><li class="listitem" style="list-style-type: disc">The value of <span class="emphasis"><em>k</em></span> Gaussians has to be given <span class="emphasis"><em>apriori</em></span>, similar to k-Means.</li></ul></div></div></div><div class="section" title="Hierarchical clustering"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec37"/>Hierarchical clustering</h3></div></div></div><p>Hierarchical <a id="id572" class="indexterm"/>clustering is a connectivity-based <a id="id573" class="indexterm"/>method of clustering that is widely used to analyze and explore the data more than it is used as a clustering technique (<span class="emphasis"><em>References</em></span> [12]). The idea is to iteratively build binary trees either from top or bottom, such that similar points are grouped together. Each level of the tree provides interesting summarization of the data.</p><div class="section" title="Input and output"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec68"/>Input and output</h4></div></div></div><p>Hierarchical <a id="id574" class="indexterm"/>clustering generally works on similarity-<a id="id575" class="indexterm"/>based transformations and so both categorical and continuous data are accepted. Hierarchical clustering only needs the similarity or distance metric to compute similarity and does not need the number of clusters like in k-means or GMM. </p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec69"/>How does it work?</h4></div></div></div><p>There are <a id="id576" class="indexterm"/>many variants of hierarchical clustering, but we will discuss agglomerative clustering. Agglomerative clustering works by first putting all the data elements in their own groups. It then iteratively merges the groups based on the similarity metric used until there is a single group. Each level of the tree or groupings provides unique segmentation of the data and it is up to the analyst to choose the right level that fits the problem domain. Agglomerative clustering is normally visualized using a dendrogram plot, which shows merging of data points at similarity. The popular choices of similarity methods used are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Single linkage</strong></span>: Similarity is the <a id="id577" class="indexterm"/>minimum distance between the groups of points:<div class="mediaobject"><img src="graphics/B05137_03_080.jpg" alt="How does it work?"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Complete linkage</strong></span>: Similarity is <a id="id578" class="indexterm"/>the maximum distance between the groups of points:<div class="mediaobject"><img src="graphics/B05137_03_081.jpg" alt="How does it work?"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Average linkage</strong></span>: Average <a id="id579" class="indexterm"/>similarity between the groups of points:<div class="mediaobject"><img src="graphics/B05137_03_082.jpg" alt="How does it work?"/></div></li></ul></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec70"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Hierarchical <a id="id580" class="indexterm"/>clustering imposes a hierarchical structure <a id="id581" class="indexterm"/>on the data even when there may not be such a structure present.</li><li class="listitem" style="list-style-type: disc">The choice of similarity metrics can result in a vastly different set of merges and dendrogram plots, so it has a large dependency on user input.</li><li class="listitem" style="list-style-type: disc">Hierarchical clustering suffers from scalability with increased data points. Based on the distance metrics used, it can be sensitive to noise and outliers.</li></ul></div></div></div><div class="section" title="Self-organizing maps (SOM)"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec38"/>Self-organizing maps (SOM)</h3></div></div></div><p>SOM is a <a id="id582" class="indexterm"/>neural network based <a id="id583" class="indexterm"/>method that can be viewed as dimensionality reduction, manifold learning, or clustering technique (<span class="emphasis"><em>References</em></span> [17]). Neurobiological studies show that our brains map different functions to different areas, known as topographic maps, which form the basis of this technique.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec71"/>Inputs and outputs</h4></div></div></div><p>Only <a id="id584" class="indexterm"/>numeric features are used in SOM. Model <a id="id585" class="indexterm"/>parameters consists of distance function, (generally Euclidean distance is used) and the lattice parameters in terms of width and height or number of cells in the lattice.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec72"/>How does it work?</h4></div></div></div><p>SOM, also known as Kohonen <a id="id586" class="indexterm"/>networks, can be thought of as a two-layer neural network where each <a id="id587" class="indexterm"/>output layer is a two-dimensional lattice, arranged in rows and columns and each neuron is fully connected to the input layer. </p><p>Like neural networks, the weights are initially generated using random values. The process has three distinct training phases:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Competitive phase</strong></span>: Neurons <a id="id588" class="indexterm"/>in this phase compete based on the discriminant values, generally based on distance between neuron weight and input vector; such that the minimal distance between the two decides which neuron the input gets assigned to. Using Euclidean distance, the distance between an input <span class="emphasis"><em>x</em></span>i and neuron in the lattice position <span class="emphasis"><em>(j, i)</em></span> is given by <span class="emphasis"><em>w</em></span><sub>ji</sub>:<div class="mediaobject"><img src="graphics/B05137_03_086.jpg" alt="How does it work?"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Cooperation phase</strong></span>: In this phase, the winning neurons find the best spatial location in the <a id="id589" class="indexterm"/>topological neighborhood. The topological neighborhood for the <a id="id590" class="indexterm"/>winning neuron <span class="emphasis"><em>I</em></span>(<span class="strong"><strong>x</strong></span>) for a given neuron <span class="emphasis"><em>(j, i)</em></span>, at a distance <span class="emphasis"><em>S</em></span><sub>ij</sub>, neighborhood of size σ, is defined by:<div class="mediaobject"><img src="graphics/B05137_03_091.jpg" alt="How does it work?"/></div><p>The neighborhood size is defined in the way that it decreases with time using some well-known decay functions such as an exponential, function defined as follows:</p><div class="mediaobject"><img src="graphics/B05137_03_092.jpg" alt="How does it work?"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Adaptive phase</strong></span>: In this phase, the weights of the winning neuron and its neighborhood <a id="id591" class="indexterm"/>neurons are updated. The update to weights is generally done using: <div class="mediaobject"><img src="graphics/B05137_03_093.jpg" alt="How does it work?"/></div><p>Here, the learning rate <span class="emphasis"><em>n(t)</em></span> is again defined as exponential decay like the neighborhood size.</p></li></ul></div><p>SOM Visualization using Unified Distance Matrix (U-Matrix) creates a single metric of average distance <a id="id592" class="indexterm"/>between the weights of the neuron and its neighbors, which then can be visualized in different color intensities. This helps to identify <span class="emphasis"><em>similar</em></span> neurons in the neighborhood.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec73"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The biggest <a id="id593" class="indexterm"/>advantage of SOM is that it is easy to <a id="id594" class="indexterm"/>understand and clustering of the data in two dimensions with U-matrix visualization enables understanding the patterns very effectively.</li><li class="listitem" style="list-style-type: disc">Choice of similarity/distance function makes vast difference in clusters and must be carefully chosen by the user.</li><li class="listitem" style="list-style-type: disc">SOM's computational complexity makes it impossible to use on datasets greater than few thousands in size.</li></ul></div></div></div></div><div class="section" title="Spectral clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec34"/>Spectral clustering</h2></div></div></div><p>Spectral <a id="id595" class="indexterm"/>clustering is a partition-based clustering technique <a id="id596" class="indexterm"/>using graph theory as its basis (<span class="emphasis"><em>References</em></span> [15]). It converts the dataset into a connected graph and does graph partitioning to find the clusters. This is a popular method in image processing, motion detection, and some unstructured data-based domains.</p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec39"/></h3></div></div></div><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec74"/>Inputs and outputs</h4></div></div></div><p>Only <a id="id597" class="indexterm"/>numeric features are used in spectral clustering. Model <a id="id598" class="indexterm"/>parameters such as the choice of kernel, the kernel parameters, the number of eigenvalues to select, and partitioning algorithms such as k-Means must be correctly defined for optimum performance.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec75"/>How does it work?</h4></div></div></div><p>The following steps <a id="id599" class="indexterm"/>describe how the technique is used in practice:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Given the data points, an affinity (or adjacency) matrix is computed using a smooth kernel function such as the Gaussian kernel:<div class="mediaobject"><img src="graphics/B05137_03_095.jpg" alt="How does it work?"/></div>
For the points that are closer, <span class="inlinemediaobject"><img src="graphics/B05137_03_096.jpg" alt="How does it work?"/></span> and for points further away, <span class="inlinemediaobject"><img src="graphics/B05137_03_097.jpg" alt="How does it work?"/></span></li><li class="listitem">The next step is to compute the graph Laplacian matrix using various methods of <a id="id600" class="indexterm"/>normalizations. All Laplacian matrix methods use the diagonal degree matrix <span class="emphasis"><em>D</em></span>, which measures degree at each node in the graph:<div class="mediaobject"><img src="graphics/B05137_03_099.jpg" alt="How does it work?"/></div><p>A simple Laplacian matrix is <span class="emphasis"><em>L = D (degree matrix) – A(affinity matrix)</em></span>.</p></li><li class="listitem">Compute the first <span class="emphasis"><em>k</em></span> eigenvalues from the eigenvalue problem or the generalized eigenvalue problem.</li><li class="listitem">Use a partition algorithm such as k-Means to further separate clusters in the k-dimensional subspace.</li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec76"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Spectral <a id="id601" class="indexterm"/>clustering works very well when the cluster shape <a id="id602" class="indexterm"/>or size is irregular and non-convex. Spectral clustering has too many parameter choices and tuning to get good results is quite an involved task.</li><li class="listitem" style="list-style-type: disc">Spectral clustering has been shown, theoretically, to be more stable in the presence of noisy data. Spectral clustering has good performance when the clusters are not well separated.</li></ul></div></div></div></div><div class="section" title="Affinity propagation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec35"/>Affinity propagation</h2></div></div></div><p>Affinity <a id="id603" class="indexterm"/>propagation can be viewed as an extension <a id="id604" class="indexterm"/>of K-medoids method for its similarity with picking exemplars from the data (<span class="emphasis"><em>References</em></span> [16]). Affinity propagation uses graphs with distance or the similarity matrix and picks all examples in the training data as exemplars. Iterative message passing as <span class="emphasis"><em>affinities</em></span> between data points automatically detects clusters, the exemplars, and <a id="id605" class="indexterm"/>even the <a id="id606" class="indexterm"/>number of clusters. </p><div class="section"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec40"/></h3></div></div></div><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec77"/>Inputs and outputs</h4></div></div></div><p>Typically, other <a id="id607" class="indexterm"/>than maximum number of iterations, which is common <a id="id608" class="indexterm"/>to most algorithms, no input parameters are required.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec78"/>How does it work?</h4></div></div></div><p>Two kinds <a id="id609" class="indexterm"/>of messages are exchanged between the data points that we will explain first:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">
Responsibility <span class="emphasis"><em>r(i,k)</em></span>: This is a message from the data point to the candidate exemplar. This gives a metric of how well the exemplar is suited for that data point compared to other exemplars. The rules for updating the responsibility are as follows:
<span class="inlinemediaobject"><img src="graphics/B05137_03_102.jpg" alt="How does it work?"/></span>
where
<span class="emphasis"><em>s(i, k)</em></span> = similarity between two data points <span class="emphasis"><em>i</em></span> and <span class="emphasis"><em>k</em></span>.<p><span class="emphasis"><em>a(i, k) </em></span>= availability of exemplar <span class="emphasis"><em>k</em></span> for <span class="emphasis"><em>i</em></span>.</p></li><li class="listitem" style="list-style-type: disc">Availability <span class="emphasis"><em>a(i,k)</em></span>: This is a message from the candidate exemplar to a data point. This gives a metric indicating how good of a support the exemplar can be to the data point, considering other data points in the calculations. This can be viewed as soft cluster assignment. The rule for updating the availability is as follows:<div class="mediaobject"><img src="graphics/B05137_03_109.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_03_110.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_03_111.jpg" alt="How does it work?"/><div class="caption"><p>Figure 4: Message types used in Affinity Propagation</p></div></div></li></ul></div><p>The algorithm can be summed up as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Initialize <span class="inlinemediaobject"><img src="graphics/B05137_03_216.jpg" alt="How does it work?"/></span></li><li class="listitem">For all increments <span class="emphasis"><em>i</em></span> to <span class="emphasis"><em>n</em></span>:<div class="mediaobject"><img src="graphics/B05137_03_217.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_03_218.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_03_219.jpg" alt="How does it work?"/></div></li><li class="listitem">End.</li><li class="listitem">For all <span class="strong"><strong>x</strong></span><sub>i</sub> such that <span class="emphasis"><em>(r(i,i) + a(i,i) &gt; 0)</em></span><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>x</strong></span><sub>i</sub> is exemplar.</li><li class="listitem">All non-exemplars <span class="strong"><strong>x</strong></span><sub>j</sub> are assigned <a id="id610" class="indexterm"/>to the closest exemplar using the similarity measure <span class="emphasis"><em>s(i, j)</em></span>.</li></ol></div></li><li class="listitem">End.</li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec79"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Affinity <a id="id611" class="indexterm"/>propagation is a deterministic algorithm. Both k-means or K-medoids are sensitive to the selection of initial points, which <a id="id612" class="indexterm"/>is overcome by considering every point as an exemplar.</li><li class="listitem" style="list-style-type: disc">The number of clusters doesn't have to be specified and is automatically determined through the process.</li><li class="listitem" style="list-style-type: disc">It works in non-metric spaces and doesn't require distances/similarity to even have constraining <a id="id613" class="indexterm"/>properties such as triangle inequality or <a id="id614" class="indexterm"/>symmetry. This makes the algorithm usable on a wide variety of datasets with categorical and text data and so on:</li><li class="listitem" style="list-style-type: disc">The algorithm can be parallelized easily due to its update methods and it has fast training time.</li></ul></div></div></div></div><div class="section" title="Clustering validation and evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec36"/>Clustering validation and evaluation</h2></div></div></div><p>Clustering <a id="id615" class="indexterm"/>validation and evaluation is one of the most <a id="id616" class="indexterm"/>important mechanisms to determine the usefulness of the algorithms (<span class="emphasis"><em>References</em></span> [18]). These topics can be broadly classified into two categories:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Internal evaluation measures</strong></span>: In this the measures uses some form of clustering <a id="id617" class="indexterm"/>quality from the data themselves, without any access to the ground truth.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>External evaluation measures</strong></span>: In this the measures use some external information <a id="id618" class="indexterm"/>such as known ground truth or class labels.</li></ul></div><div class="section" title="Internal evaluation measures"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec41"/>Internal evaluation measures</h3></div></div></div><p>Internal <a id="id619" class="indexterm"/>evaluation uses only the clusters and data information<a id="id620" class="indexterm"/> to gather metrics about how good the clustering results are. The applications may have some influence over the choice of the measures. Some algorithms are biased towards particular evaluation metrics. So care must be taken in choosing the right metrics, algorithms, and parameters based on these considerations. Internal evaluation measures are based on different qualities, as mentioned here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Compactness</strong></span>: Variance in <a id="id621" class="indexterm"/>the clusters measured using different strategies is used to give compactness values; the lower the variance, the more compact the cluster.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Separation</strong></span>: How well <a id="id622" class="indexterm"/>are the clusters separated from each other?</li></ul></div><div class="section" title="Notation"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec80"/>Notation</h4></div></div></div><p>Here's a compact <a id="id623" class="indexterm"/>explanation of the notation used in what follows: dataset with all data elements =<span class="emphasis"><em>D</em></span>, number of data elements =<span class="emphasis"><em>n</em></span>, dimensions or features <a id="id624" class="indexterm"/>of each data element=<span class="emphasis"><em>d</em></span>, center of entire data <span class="emphasis"><em>D = c</em></span>, number of clusters = <span class="emphasis"><em>NC</em></span>, <span class="emphasis"><em>i</em></span><sup>th</sup> cluster = <span class="emphasis"><em>C</em></span><sub>i</sub>, number of data in the <span class="emphasis"><em>i</em></span><sup>th</sup> cluster =<span class="emphasis"><em>n</em></span><sub>i</sub>, center of <span class="emphasis"><em>i</em></span><sup>th</sup> cluster = <span class="emphasis"><em>c</em></span><sub>i</sub>, variance in the <span class="emphasis"><em>i</em></span><sup>th</sup> cluster = σ(<span class="emphasis"><em>C</em></span><sub>i</sub>), distance between two points <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> = <span class="emphasis"><em>d (x,y)</em></span>.</p></div><div class="section" title="R-Squared"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec81"/>R-Squared</h4></div></div></div><p>The goal is to <a id="id625" class="indexterm"/>measure the degree of difference <a id="id626" class="indexterm"/>between clusters using the ratio of the sum of squares between clusters to the total sum of squares on the whole data. The formula is given as follows:</p><div class="mediaobject"><img src="graphics/B05137_03_129.jpg" alt="R-Squared"/></div></div><div class="section" title="Dunn's Indices"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec82"/>Dunn's Indices</h4></div></div></div><p>The goal <a id="id627" class="indexterm"/>is to identify dense and well-separated <a id="id628" class="indexterm"/>clusters. The measure is given by maximal values obtained from the following formula:</p><div class="mediaobject"><img src="graphics/B05137_03_130.jpg" alt="Dunn's Indices"/></div></div><div class="section" title="Davies-Bouldin index"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec83"/>Davies-Bouldin index</h4></div></div></div><p>The goal is <a id="id629" class="indexterm"/>to identify clusters <a id="id630" class="indexterm"/>with low intra-cluster distances and high inter-cluster distances:</p><div class="mediaobject"><img src="graphics/B05137_03_131.jpg" alt="Davies-Bouldin index"/></div><div class="section" title="Silhouette's index"><div class="titlepage"><div><div><h5 class="title"><a id="ch03lvl5sec09"/>Silhouette's index</h5></div></div></div><p>The goal <a id="id631" class="indexterm"/>is to measure the pairwise difference <a id="id632" class="indexterm"/>of between-cluster and within-cluster distances. It is also used to find <a id="id633" class="indexterm"/>optimal cluster number by maximizing the index. The <a id="id634" class="indexterm"/>formula is given by:</p><div class="mediaobject"><img src="graphics/B05137_03_132.jpg" alt="Silhouette's index"/></div><p>Here <span class="inlinemediaobject"><img src="graphics/B05137_03_133.jpg" alt="Silhouette's index"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_03_134.jpg" alt="Silhouette's index"/></span>.</p></div></div></div><div class="section" title="External evaluation measures"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec42"/>External evaluation measures</h3></div></div></div><p>The <a id="id635" class="indexterm"/>external evaluation measures <a id="id636" class="indexterm"/>of clustering have similarity to classification metrics using elements from the confusion matrix or using information theoretic metrics from the data and labels. Some of the most commonly used measures are as follows.</p><div class="section" title="Rand index"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec84"/>Rand index</h4></div></div></div><p>Rand <a id="id637" class="indexterm"/>index measures the correct decisions made <a id="id638" class="indexterm"/>by the clustering algorithm using the following formula:</p><div class="mediaobject"><img src="graphics/B05137_03_135.jpg" alt="Rand index"/></div></div><div class="section" title="F-Measure"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec85"/>F-Measure</h4></div></div></div><p>F-Measure <a id="id639" class="indexterm"/>combines the precision and recall measures <a id="id640" class="indexterm"/>applied to clustering as given in the following formula:</p><div class="mediaobject"><img src="graphics/B05137_03_136-New.jpg" alt="F-Measure"/></div><div class="mediaobject"><img src="graphics/B05137_03_138.jpg" alt="F-Measure"/></div><p>Here, <span class="emphasis"><em>n</em></span><sub>ij</sub> is the number of data <a id="id641" class="indexterm"/>elements of class <span class="emphasis"><em>i</em></span> in the cluster <span class="emphasis"><em>j</em></span>, <span class="emphasis"><em>n</em></span><sub>j</sub> is the <a id="id642" class="indexterm"/>number of data in the cluster <span class="emphasis"><em>j</em></span> and <span class="emphasis"><em>n</em></span><sub>i</sub> is the number of data in the class <span class="emphasis"><em>i</em></span>. The higher the F-Measure, the better the clustering quality.</p></div><div class="section" title="Normalized mutual information index"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec86"/>Normalized mutual information index</h4></div></div></div><p>NMI is <a id="id643" class="indexterm"/>one of the many entropy-based measures applied to clustering. The entropy associated with a clustering <span class="emphasis"><em>C</em></span> is a measure of the uncertainty about a cluster picking a data element randomly.</p><p><span class="inlinemediaobject"><img src="graphics/B05137_03_143.jpg" alt="Normalized mutual information index"/></span>
 where <span class="inlinemediaobject"><img src="graphics/B05137_03_144.jpg" alt="Normalized mutual information index"/></span> is the probability of the element getting picked in cluster <span class="emphasis"><em>C</em></span>i.</p><p>Mutual information between two clusters is given by:</p><div class="mediaobject"><img src="graphics/B05137_03_146.jpg" alt="Normalized mutual information index"/></div><p>Here <span class="inlinemediaobject"><img src="graphics/B05137_03_147.jpg" alt="Normalized mutual information index"/></span>, which is the <a id="id644" class="indexterm"/>probability of the element being picked by both clusters <span class="emphasis"><em>C</em></span> and <span class="emphasis"><em>C<sup>'</sup></em></span>.</p><p>
<span class="strong"><strong>Normalized mutual information</strong></span> (<span class="strong"><strong>NMI</strong></span>) has many forms; one is given by:</p><div class="mediaobject"><img src="graphics/B05137_03_150.jpg" alt="Normalized mutual information index"/></div></div></div></div></div>
<div class="section" title="Outlier or anomaly detection"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec30"/>Outlier or anomaly detection</h1></div></div></div><p>Grubbs, in 1969, offers <a id="id645" class="indexterm"/>the definition, "An outlying observation, or <a id="id646" class="indexterm"/>outlier, is one that appears to deviate markedly from other members of the sample in which it occurs".</p><p>Hawkins, in 1980, defined outliers or anomaly as "an observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism".</p><p>Barnett and Lewis, 1994, defined it as "an observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data".</p><div class="section" title="Outlier algorithms"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec37"/>Outlier algorithms</h2></div></div></div><p>Outlier <a id="id647" class="indexterm"/>detection techniques are classified based on <a id="id648" class="indexterm"/>different approaches to what it means to be an outlier. Each approach defines outliers in terms of some property that sets apart some objects from others in the dataset:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Statistical-based</strong></span>: This is <a id="id649" class="indexterm"/>improbable according to a chosen distribution</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Distance-based</strong></span>: This is isolated <a id="id650" class="indexterm"/>from neighbors according to chosen distance measure and fraction of neighbors within threshold distance</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Density-based</strong></span>: This is <a id="id651" class="indexterm"/>more isolated from its neighbors than they are in turn from their neighbors</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Clustering-based</strong></span>: This <a id="id652" class="indexterm"/>is in isolated clusters relative to other clusters or is not a member of any cluster</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>High-dimension-based</strong></span>: This is an <a id="id653" class="indexterm"/>outlier by usual techniques after data is projected to lower dimensions, or by choosing an appropriate metric for high dimensions</li></ul></div><div class="section" title="Statistical-based"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec43"/>Statistical-based</h3></div></div></div><p>Statistical-based techniques that use parametric methods for outlier detection assume some <a id="id654" class="indexterm"/>knowledge of the distribution of the data (<span class="emphasis"><em>References</em></span> [19]). From <a id="id655" class="indexterm"/>the observations, the model parameters are estimated. Data points that have probabilities lower than a threshold value in the model are considered outliers. When the distribution is not known or none is suitable to assume, non-parametric methods are used.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec87"/>Inputs and outputs</h4></div></div></div><p>Statistical <a id="id656" class="indexterm"/>methods for outlier detection work with real-valued <a id="id657" class="indexterm"/>datasets. The choice of distance metric may be a user-selected input in the case of parametric methods assuming multivariate distributions. In the case of non-parametric methods using frequency-based histograms, a user-defined threshold frequency is used. Selection of kernel method and bandwidth are also user-determined in <a id="id658" class="indexterm"/>Kernel Density Estimation techniques. The output from <a id="id659" class="indexterm"/>statistical-based methods is a score indicating outlierness.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec88"/>How does it work?</h4></div></div></div><p>Most of the <a id="id660" class="indexterm"/>statistical-based outlier detections either assume a distribution or fit a distribution to the data to detect probabilistically the least likely data generated from the distribution. These methods have two distinct steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Training step</strong></span>: Here, an estimate of the model to fit the data is performed</li><li class="listitem"><span class="strong"><strong>Testing step</strong></span>: On each instance a goodness of fit is performed based on the model and the particular instance, yielding a score and the outlierness</li></ol></div><p>Parametric-based methods assume a distribution model such as multivariate Gaussians and the training normally involves estimating the means and variance using techniques such as <span class="strong"><strong>Maximum Likelihood Estimates</strong></span> (<span class="strong"><strong>MLE</strong></span>). The testing typically includes techniques <a id="id661" class="indexterm"/>such as mean-variance or box-plot tests, accompanied by assumptions such as "if outside three standard deviations, then outlier".</p><p>A normal multivariate distribution can be estimated as: </p><div class="mediaobject"><img src="graphics/B05137_03_151.jpg" alt="How does it work?"/></div><p>with the mean <span class="strong"><strong>µ</strong></span> and covariance Ʃ.</p><p>The Mahalanobis distance can be the estimate of the data point from the distribution given by the equation <span class="inlinemediaobject"><img src="graphics/B05137_03_154.jpg" alt="How does it work?"/></span>. Some variants such as <span class="strong"><strong>Minimum Covariant Determinant</strong></span> (<span class="strong"><strong>MCD</strong></span>) are also used when Mahalanobis distance is affected by outliers.</p><p>A <a id="id662" class="indexterm"/>non-parametric method involves techniques such as constructing histograms for every feature using frequency or width-based methods. When the ratio of the data in a bin to that of the average over the histogram is below a user defined threshold, such a bin is termed sparse. A lower probability of feature results in a higher outlier score. The total outlier score can be computed as: </p><div class="mediaobject"><img src="graphics/B05137_03_155.jpg" alt="How does it work?"/></div><p>Here, <span class="emphasis"><em>w</em></span><sub>f</sub> is the weight given to feature <span class="emphasis"><em>f</em></span>, <span class="emphasis"><em>p</em></span><sub>f</sub> is the probability of the value of the feature in the test data point, and <span class="emphasis"><em>F</em></span> is the sum of weights of the feature set. Kernel Density Estimations are <a id="id663" class="indexterm"/>also used in non-parametric methods using user-defined kernels and bandwidth.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec89"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">When <a id="id664" class="indexterm"/>the model fits or distribution of the data is known, these <a id="id665" class="indexterm"/>methods are very efficient as you don't have to store entire data, just the key statistics for doing tests.</li><li class="listitem" style="list-style-type: disc">Assumptions of distribution, however, can pose a big issue in parametric methods. Most non-parametric methods using kernel density estimates don't scale well with large datasets.</li></ul></div></div></div><div class="section" title="Distance-based methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec44"/>Distance-based methods</h3></div></div></div><p>Distance-based <a id="id666" class="indexterm"/>algorithms work under the general <a id="id667" class="indexterm"/>assumption that normal data has other data points closer to it while anomalous data is well isolated from its neighbors (<span class="emphasis"><em>References</em></span> [20]). </p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec90"/>Inputs and outputs</h4></div></div></div><p>Distance-based <a id="id668" class="indexterm"/>techniques require natively numeric or <a id="id669" class="indexterm"/>categorical features to be transformed to numeric values. Inputs to distance-based methods are the distance metric used, the distance threshold ϵ, and π, the threshold fraction, which together determine if a point is an outlier. For KNN methods, the choice <span class="emphasis"><em>k</em></span> is an input.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec91"/>How does it work?</h4></div></div></div><p>There are <a id="id670" class="indexterm"/>many variants of distance-based outliers and we will discuss how each of them works at a high level:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">DB (<span class="emphasis"><em>ϵ</em></span><span class="emphasis"><em>, π</em></span>) Algorithms: Given a radius of <span class="emphasis"><em>ϵ</em></span> and threshold of π, a data point is considered as an outlier if π percentage of points have distance to the point less than <span class="emphasis"><em>ϵ</em></span>. There are further variants using nested loop structures, grid-based structures, and index-based structures on how the computation is done.</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>KNN</em></span>-based methods are also very common where the outlier score is computed <a id="id671" class="indexterm"/>either by taking the <span class="emphasis"><em>KNN</em></span> distance to the point or the average distance to point from <span class="emphasis"><em>{1NN,2NN,3NN…KNN}</em></span>.</li></ul></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec92"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <a id="id672" class="indexterm"/>main advantage of distance-based algorithms <a id="id673" class="indexterm"/>is that they are non-parametric and make no assumptions on distributions and how to fit models.</li><li class="listitem" style="list-style-type: disc">The distance calculations are straightforward and computed in parallel, helping the algorithms to scale on large datasets.</li><li class="listitem" style="list-style-type: disc">The major issues with distance-based methods is the curse of dimensionality discussed in the first chapter; for large dimensional data, sparsity can lead to noisy outlierness.</li></ul></div></div></div><div class="section" title="Density-based methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec45"/>Density-based methods</h3></div></div></div><p>Density-based methods extend the distance-based methods by not only measuring the local <a id="id674" class="indexterm"/>density of the given point, but also the <a id="id675" class="indexterm"/>local densities of its neighborhood points. Thus, the relative factor added gives it the edge in finding more complex outliers that are local or global in nature, but at the added cost of computation.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec93"/>Inputs and outputs</h4></div></div></div><p>Density-based <a id="id676" class="indexterm"/>algorithm must be supplied the minimum <a id="id677" class="indexterm"/>number of points <span class="emphasis"><em>MinPts</em></span> in a neighborhood of input radius <span class="emphasis"><em>ϵ</em></span> centered on an object that determines it is a core object in a cluster. </p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec94"/>How does it work?</h4></div></div></div><p>We <a id="id678" class="indexterm"/>will first discuss the <span class="strong"><strong>Loca</strong></span>
<span class="strong"><strong>Outlier Factor</strong></span> (<span class="strong"><strong>LOF</strong></span>) method <a id="id679" class="indexterm"/>and then discuss some variants of LOF [21]. </p><p>Given the <span class="emphasis"><em>MinPts</em></span> as the parameter, LOF of a data point is:</p><div class="mediaobject"><img src="graphics/B05137_03_168.jpg" alt="How does it work?"/></div><p>Here |<span class="emphasis"><em>N</em></span>
<span class="emphasis"><em><sub>MinPts</sub></em></span>
<span class="emphasis"><em> (p)</em></span>| is the number of data points in the neighborhood of point <span class="emphasis"><em>p</em></span>, and <span class="emphasis"><em>lrd</em></span>
<span class="emphasis"><em><sub>MinPts</sub></em></span> is the local <a id="id680" class="indexterm"/>reachability density of the point and is defined as: </p><div class="mediaobject"><img src="graphics/B05137_03_171.jpg" alt="How does it work?"/></div><p>Here <span class="inlinemediaobject"><img src="graphics/B05137_03_172.jpg" alt="How does it work?"/></span> is the reachability of the point and is defined as:</p><div class="mediaobject"><img src="graphics/B05137_03_173.jpg" alt="How does it work?"/></div><p>One of the disadvantages of LOF is that it may miss outliers whose neighborhood density is close to that of its neighborhood. <span class="strong"><strong>Connectivity-based outliers</strong></span> (<span class="strong"><strong>COF</strong></span>) using set-based nearest <a id="id681" class="indexterm"/>path and set-based nearest trail originating from the data point are used to improve on LOF. COF treats the low-density region differently to the isolated region and overcomes the disadvantage of LOF:</p><div class="mediaobject"><img src="graphics/B05137_03_175.jpg" alt="How does it work?"/></div><p>Another disadvantage of LOF is that when clusters are in varying densities and not separated, LOF will generate counter-intuitive scores. One way to overcome this is to use the <span class="strong"><strong>influence space</strong></span> (<span class="strong"><strong>IS</strong></span>) of the <a id="id682" class="indexterm"/>points using KNNs and its reverse KNNs or RNNs. RNNs have the given point as one of their K nearest neighbors. Outlierness of the point is known as Influenced Outliers or INFLO and is given by:</p><div class="mediaobject"><img src="graphics/B05137_03_176.jpg" alt="How does it work?"/></div><p>Here, <span class="emphasis"><em>den</em></span>(<span class="emphasis"><em>p</em></span>) is the local density of <span class="emphasis"><em>p</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_03_179.jpg" alt="How does it work?"/><div class="caption"><p>Figure 5: Density-based outlier detection methods are particularly suited for finding local as well as global outliers</p></div></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec95"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It <a id="id683" class="indexterm"/>has been shown that density-based methods are <a id="id684" class="indexterm"/>more effective than distance-based methods.</li><li class="listitem" style="list-style-type: disc">Density-based outlier detection has high computational cost and, often, poor interpretability.</li></ul></div></div></div><div class="section" title="Clustering-based methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec46"/>Clustering-based methods</h3></div></div></div><p>Some <a id="id685" class="indexterm"/>believe that clustering techniques, where <a id="id686" class="indexterm"/>the goal is to find groups of data points located together, are in some sense antithetical to the problem of anomaly or outlier detection. However, as an advanced unsupervised learning technique, clustering analysis offers several methods to find interesting groups of clusters that are either located far off from other clusters or do not lie in any clusters at all.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec96"/>Inputs and outputs</h4></div></div></div><p>As <a id="id687" class="indexterm"/>seen before, clustering techniques work well with <a id="id688" class="indexterm"/>real-valued data, although some categorical values translated to numeric values are tolerated. In the case of k-Means and k-Medoids, input <a id="id689" class="indexterm"/>values include the number of clusters <span class="emphasis"><em>k</em></span> and the <a id="id690" class="indexterm"/>distance metric. Variants may require a threshold score to identify outlier groups. For Gaussian Mixture Models using EM, the number of mixture components must be supplied by the user. When using CBLOF, two user-defined parameters are expected: the size of small clusters and the size of large clusters. Depending on the algorithm used, individual or groups of objects are output as outliers.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec97"/>How does it work?</h4></div></div></div><p>As we <a id="id691" class="indexterm"/>discussed in the section on clustering, there are various types of clustering methods and we will give a few examples of how clustering algorithms have been extended for outlier detection. </p><p>k-Means or k-Medoids and their variants generally cluster data elements together and are affected by outliers or noise. Instead of preprocessing these data points by removal or transformation, such points that weaken the "tightness" of the clusters are considered outliers. Typically, outliers are revealed by a two-step process of first running clustering algorithms and then evaluating some form of outlier score that measures distance from point to centroid. Also, many variants treat clusters of size smaller than a threshold as an outlier group. </p><p>
<span class="strong"><strong>Gaussian mixture modeling</strong></span> (<span class="strong"><strong>GMM</strong></span>) using <span class="strong"><strong>Expectation maximization</strong></span> (<span class="strong"><strong>EM</strong></span>) is another well-known <a id="id692" class="indexterm"/>clustering-based outlier detection <a id="id693" class="indexterm"/>technique, where a data point that has low probability of belonging to a cluster becomes an outlier and the outlier score becomes the inverse of the EM probabilistic output score.</p><p>
<span class="strong"><strong>Cluster-based Local Outlier Factor</strong></span> (<span class="strong"><strong>CBLOF</strong></span>) uses a two-stage process to find outliers. First, a clustering <a id="id694" class="indexterm"/>algorithm performs partitioning of data into clusters of various sizes. Using two user-defined parameters, size of large clusters, and size of small clusters, two sets of clusters are formed:</p><div class="mediaobject"><img src="graphics/B05137_03_215.jpg" alt="How does it work?"/></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec98"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Given <a id="id695" class="indexterm"/>that clustering-based techniques are well-understood, results <a id="id696" class="indexterm"/>are more interpretable and there are more tools available for these techniques. </li><li class="listitem" style="list-style-type: disc">Many clustering algorithms only detect clusters, and are less effective in unsupervised techniques compared to outlier algorithms that give scores or ranks or otherwise identify outliers.</li></ul></div></div></div><div class="section" title="High-dimensional-based methods"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec47"/>High-dimensional-based methods</h3></div></div></div><p>One <a id="id697" class="indexterm"/>of the key issues with distance-, density-, or even clustering-based methods, is the curse of dimensionality. As dimensions <a id="id698" class="indexterm"/>increase, the contrast between distances diminishes and the concept of neighborhood becomes less meaningful. The normal points in this case look like outliers and false positives increase by large volume. We will discuss some of the latest approaches taken in addressing this problem.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec99"/>Inputs and outputs</h4></div></div></div><p>Algorithms <a id="id699" class="indexterm"/>that project data to lower-dimensional <a id="id700" class="indexterm"/>subspaces can handle missing data well. In these techniques, such as SOD, <span class="emphasis"><em>ϕ</em></span>, the number of ranges in each dimension becomes an input (<span class="emphasis"><em>References</em></span> [25]). When using an evolutionary algorithm, the number of cells with the lowest sparsity coefficients is another input parameter to the algorithm. </p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec100"/>How does it work?</h4></div></div></div><p>The <a id="id701" class="indexterm"/>broad idea to solve the high dimensional outlier issue is to: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Either have a robust distance metric coupled with all of the previous techniques, so that outliers can be identified in full dimensions</li><li class="listitem" style="list-style-type: disc">Or project data on to smaller subspaces and find outliers in the smaller subspaces</li></ul></div><p>The <span class="strong"><strong>Angle-based Outlier Degree</strong></span> (<span class="strong"><strong>ABOD</strong></span>) method uses the basic assumption that if a data <a id="id702" class="indexterm"/>point in high dimension is an outlier, all the vectors originating from it towards data points nearest to it will <a id="id703" class="indexterm"/>be in more or less the same direction.</p><div class="mediaobject"><img src="graphics/B05137_03_184.jpg" alt="How does it work?"/><div class="caption"><p>Figure 6: The ABOD method of distinguishing outliers from inliers</p></div></div><p>Given a point <span class="emphasis"><em>p</em></span>, and any two points <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span>, the angle between the two points and <span class="emphasis"><em>p</em></span> is given by:</p><div class="mediaobject"><img src="graphics/B05137_03_187.jpg" alt="How does it work?"/></div><p>Measure of variance used as the ABOD score is given by:</p><div class="mediaobject"><img src="graphics/B05137_03_188.jpg" alt="How does it work?"/></div><p>The smaller the ABOD value, the smaller the measure of variance in the angle spectrum, and the larger the chance of the point being the outlier.</p><p>Another method that has been very useful in high dimensional data is using the <span class="strong"><strong>Subspace Outlier Detection</strong></span> (<span class="strong"><strong>SOD</strong></span>) approach (<span class="emphasis"><em>References</em></span> [23]). The idea is to partition the high dimensional <a id="id704" class="indexterm"/>space such that there are an equal number of ranges, say <span class="emphasis"><em>ϕ</em></span>, in each of the <span class="emphasis"><em>d</em></span> dimensions. Then the Sparsity Coefficient for a cell <span class="emphasis"><em>C</em></span> formed by picking a range in each of the <span class="emphasis"><em>d</em></span> dimensions is measured as follows:</p><div class="mediaobject"><img src="graphics/B05137_03_192.jpg" alt="How does it work?"/></div><p>Here, <span class="emphasis"><em>n</em></span> is the total number of data points and <span class="emphasis"><em>N(C)</em></span> is the number of data points in cell <span class="emphasis"><em>C</em></span>. Generally, the data <a id="id705" class="indexterm"/>points lying in cells with negative sparsity coefficient are considered outliers.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec101"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The ABOD <a id="id706" class="indexterm"/>method is <span class="emphasis"><em>O(n</em></span><sup>3</sup><span class="emphasis"><em>)</em></span> with the number <a id="id707" class="indexterm"/>of data points and becomes impractical with larger datasets.</li><li class="listitem" style="list-style-type: disc">The sparsity coefficient method in subspaces requires efficient search in lower dimension and the problem becomes NP-Hard and some form of evolutionary or heuristic based search is employed.</li><li class="listitem" style="list-style-type: disc">The sparsity coefficient methods being NP-Hard can result in local optima.</li></ul></div></div></div><div class="section" title="One-class SVM"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec48"/>One-class SVM</h3></div></div></div><p>In many <a id="id708" class="indexterm"/>domains there is a particular class or category <a id="id709" class="indexterm"/>of interest and the "rest" do not matter. Finding a boundary around this class of interest is the basic idea behind one-class SVM (<span class="emphasis"><em>References</em></span> [26]). The basic assumption is that all the points of the positive class (class of interest) cluster together while the other class elements are spread around and we can find a tight hyper-sphere around the clustered instances. SVM, which has great theoretical foundations and applications in binary classifications is reformulated to solve one-class SVM. The following figure illustrates how a nonlinear boundary is simplified by using one-class SVM with slack so as to not overfit complex functions:</p><div class="mediaobject"><img src="graphics/B05137_03_197.jpg" alt="One-class SVM"/><div class="caption"><p>Figure 7: One-Class SVM for nonlinear boundaries</p></div></div><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec102"/>Inputs and outputs</h4></div></div></div><p>Data <a id="id710" class="indexterm"/>inputs are generally numeric features. Many SVMs can take <a id="id711" class="indexterm"/>nominal features and apply binary transformations to them. Also needed are: marking the class of interest, SVM hyper-parameters such as kernel choice, kernel parameters and cost parameter, among others. Output is a SVM model that can predict whether instances belong to the class of interest or not. This is different from scoring models, which we have seen previously.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec103"/>How does it work?</h4></div></div></div><p>The <a id="id712" class="indexterm"/>input is training instances {<span class="strong"><strong>x</strong></span><sub>1</sub>,<span class="strong"><strong>x</strong></span><sub>2</sub>…<span class="strong"><strong>x</strong></span><sub>n</sub>} with certain instances marked to be in class +1 and rest in -1.</p><p>The input to SVM also needs a kernel that does the transformation <span class="emphasis"><em>ϕ</em></span> from input space to features space as <span class="inlinemediaobject"><img src="graphics/B05137_03_199.jpg" alt="How does it work?"/></span> using: </p><div class="mediaobject"><img src="graphics/B05137_03_200.jpg" alt="How does it work?"/></div><p>Create a hyper-sphere that bounds the classes using SVM reformulated equation as: </p><div class="mediaobject"><img src="graphics/B05137_03_201.jpg" alt="How does it work?"/></div><p>Such that <span class="inlinemediaobject"><img src="graphics/B05137_03_202.jpg" alt="How does it work?"/></span>+<span class="inlinemediaobject"><img src="graphics/B05137_03_203.jpg" alt="How does it work?"/></span>, <span class="inlinemediaobject"><img src="graphics/B05137_03_204.jpg" alt="How does it work?"/></span> </p><p>
<span class="emphasis"><em>R</em></span> is the radius of the hyper-sphere with center <span class="strong"><strong>c</strong></span> and <span class="emphasis"><em>ν</em></span> ∈ (0,1] represents an upper bound on the fraction of the data that are outliers.</p><p>As in <a id="id713" class="indexterm"/>normal SVM, we perform optimization using quadratic programming is done to obtain the solution as the decision boundary.</p></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch03lvl4sec104"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The key <a id="id714" class="indexterm"/>advantage to using one-class SVM—as is true of <a id="id715" class="indexterm"/>binary SVM—is the many theoretical guarantees in error and generalization bounds.</li><li class="listitem" style="list-style-type: disc">High-dimensional data can be easily mapped in one-class SVM.</li><li class="listitem" style="list-style-type: disc">Non-linear SVM with kernels can even find non-spherical shapes to bound the clusters of data.</li><li class="listitem" style="list-style-type: disc">The training cost in space and memory increases as the size of the data increases.</li><li class="listitem" style="list-style-type: disc">Parameter tuning, especially the kernel parameters and the cost parameter tuning with unlabeled data is a big challenge.</li></ul></div></div></div></div><div class="section" title="Outlier evaluation techniques"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec38"/>Outlier evaluation techniques</h2></div></div></div><p>Measuring <a id="id716" class="indexterm"/>outliers in terms of labels, ranks, and <a id="id717" class="indexterm"/>scores is an area of active research. When the labels or the ground truth is known, the idea of evaluation becomes much easier as the outlier class is known and standard metrics can be employed. But when the ground truth is not known, the evaluation and validation methods are very subjective and there is no well-defined, rigorous statistical process.</p><div class="section" title="Supervised evaluation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec49"/>Supervised evaluation</h3></div></div></div><p>In cases <a id="id718" class="indexterm"/>where the ground truth is known, the evaluation of outlier algorithms is basically the task of finding the best thresholds for outlier scores (scoring-based outliers). </p><p>The balance between reducing the false positives and improving true positives is the key concept and Precision-Recall curves (described in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>) are used to find the best optimum threshold. Confidence score, predictions, and actual labels are used in supervised learning to plot PRCurves, and instead of confidence scores, outlier scores are ranked and used here. ROC curves and area under curves are also used in many applications to evaluate thresholds. Comparing two or more algorithms and selection of the best can also be done using area under curve metrics when the ground truth is known.</p></div><div class="section" title="Unsupervised evaluation"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec50"/>Unsupervised evaluation</h3></div></div></div><p>In <a id="id719" class="indexterm"/>most real-world cases, knowing the ground truth is very difficult, at least during the modeling task. Hawkins describes the evaluation method in this case at a very high level as "a sample containing outliers would show up such characteristics as large gaps between 'outlying' and 'inlying' observations and the deviation between outliers and the group of inliers, as measured on some suitably standardized scale".</p><p>The general <a id="id720" class="indexterm"/>technique used in evaluating outliers when the ground truth is not known is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Histogram of outlier scores</strong></span>: A visualization-based method, where outlier scores are grouped into predefined bins and users can select thresholds based on outlier counts, scores, and thresholds.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Score normalization and distance functions</strong></span>: In this technique, some form of normalization is done to make sure all outlier algorithms that produce scores have the same ranges. Some form of distance or similarity or correlation based method is used to find commonality of outliers across different algorithms. The general intuition here is: the more the algorithms that weigh the data point as outlier, the higher the probability of that point actually being an outlier.</li></ul></div></div></div></div>
<div class="section" title="Real-world case study"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec31"/>Real-world case study</h1></div></div></div><p>Here <a id="id721" class="indexterm"/>we present a case study that illustrates how to apply clustering and outlier techniques described in this chapter in the real world, using open-source Java frameworks and a well-known image dataset.</p><div class="section" title="Tools and software"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec39"/>Tools and software</h2></div></div></div><p>We will <a id="id722" class="indexterm"/>now introduce two new tools that were used in the <a id="id723" class="indexterm"/>experiments for this chapter: SMILE and Elki. SMILE <a id="id724" class="indexterm"/>features a Java <a id="id725" class="indexterm"/>API that was used to illustrate feature reduction using PCA, Random Projection, and IsoMap. Subsequently, the graphical interface of Elki was used to perform <a id="id726" class="indexterm"/>unsupervised learning—specifically, clustering and outlier detection. Elki comes <a id="id727" class="indexterm"/>with a rich set of algorithms for cluster analysis and outlier detection <a id="id728" class="indexterm"/>including a large number of model <a id="id729" class="indexterm"/>evaluators to <a id="id730" class="indexterm"/>choose from.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note07"/>Note</h3><p>Find out more about SMILE at: <a class="ulink" href="http://haifengl.github.io/smile/">http://haifengl.github.io/smile/</a> and to learn more about Elki, visit <a class="ulink" href="http://elki.dbs.ifi.lmu.de/">http://elki.dbs.ifi.lmu.de/</a>.</p></div></div></div><div class="section" title="Business problem"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec40"/>Business problem</h2></div></div></div><p>Character-recognition <a id="id731" class="indexterm"/>is a problem that occurs in many business areas, for example, the translation of medical reports and hospital charts, postal code <a id="id732" class="indexterm"/>recognition in the postal service, check deposit service in retail banking, and others. Human handwriting can vary widely among individuals. Here, we are looking exclusively at handwritten digits, 0 to 9. The problem is made interesting due to the verisimilitude within certain sets of digits, such as 1/2/7 and 6/9/0. In our experiments in this chapter we use clustering and outlier analysis using several different algorithms to illustrate the relative strengths and weaknesses of the methods. Given the widespread use of these techniques in data mining applications, our main focus is to gain insights into the data and the algorithms and evaluation measures; we do not apply the models for prediction on test data.</p></div><div class="section" title="Machine learning mapping"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec41"/>Machine learning mapping</h2></div></div></div><p>As <a id="id733" class="indexterm"/>suggested by the title of the chapter, our <a id="id734" class="indexterm"/>experiments aim to demonstrate Unsupervised Learning by ignoring the labels identifying the digits in the dataset. Having learned from the dataset, clustering and outlier analyses can yield invaluable information for describing patterns in the data, and are often used to explore these patterns and inter-relationships in the data, and not just to predict the class of unseen data. In the experiments described here, we are concerned with description and exploration rather than prediction. Labels are used when available by external evaluation measures, as they are in these experiments as well.</p></div><div class="section" title="Data collection"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec42"/>Data collection</h2></div></div></div><p>This is <a id="id735" class="indexterm"/>already done <a id="id736" class="indexterm"/>for us. For details on <a id="id737" class="indexterm"/>how the data was collected, see: The MNIST database: <a class="ulink" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>.</p></div><div class="section" title="Data quality analysis"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec43"/>Data quality analysis</h2></div></div></div><p>Each <a id="id738" class="indexterm"/>feature in a data point is the greyscale value of one of 784 pixels. Consequently, the type of all features is numeric; there are no <a id="id739" class="indexterm"/>categorical types except for the class attribute, which is a numeral in the range 0-9. Moreover, there are no missing data elements in the dataset. Here is a table with some basic statistics for a few pixels. The images are pre-centred in the 28 x 28 box so in most examples, the data along the borders of the box are zeros:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Feature</p>
</th><th style="text-align: left" valign="bottom">
<p>Average</p>
</th><th style="text-align: left" valign="bottom">
<p>Std Dev</p>
</th><th style="text-align: left" valign="bottom">
<p>Min</p>
</th><th style="text-align: left" valign="bottom">
<p>Max</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>pixel300</p>
</td><td style="text-align: left" valign="top">
<p>94.25883</p>
</td><td style="text-align: left" valign="top">
<p>109.117</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>255</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel301</p>
</td><td style="text-align: left" valign="top">
<p>72.778</p>
</td><td style="text-align: left" valign="top">
<p>103.0266</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>255</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel302</p>
</td><td style="text-align: left" valign="top">
<p>49.06167</p>
</td><td style="text-align: left" valign="top">
<p>90.68359</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>255</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel303</p>
</td><td style="text-align: left" valign="top">
<p>28.0685</p>
</td><td style="text-align: left" valign="top">
<p>70.38963</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>255</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel304</p>
</td><td style="text-align: left" valign="top">
<p>12.84683</p>
</td><td style="text-align: left" valign="top">
<p>49.01016</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>255</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel305</p>
</td><td style="text-align: left" valign="top">
<p>4.0885</p>
</td><td style="text-align: left" valign="top">
<p>27.21033</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>255</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel306</p>
</td><td style="text-align: left" valign="top">
<p>1.147</p>
</td><td style="text-align: left" valign="top">
<p>14.44462</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>254</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel307</p>
</td><td style="text-align: left" valign="top">
<p>0.201667</p>
</td><td style="text-align: left" valign="top">
<p>6.225763</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>254</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel308</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel309</p>
</td><td style="text-align: left" valign="top">
<p>0.009167</p>
</td><td style="text-align: left" valign="top">
<p>0.710047</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>55</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>pixel310</p>
</td><td style="text-align: left" valign="top">
<p>0.102667</p>
</td><td style="text-align: left" valign="top">
<p>4.060198</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>237</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 1: Summary of features from the original dataset before pre-processing </em></span></p></blockquote></div><p>The <span class="strong"><strong>Mixed National Institute of Standards and Technology</strong></span> (<span class="strong"><strong>MNIST</strong></span>) dataset is a widely <a id="id740" class="indexterm"/>used dataset for evaluating unsupervised learning methods. The MNIST dataset is mainly chosen because the clusters in high dimensional data are not well separated. </p><p>The original MNIST dataset had black and white images from NIST. They were normalized to fit in a 20 x 20 pixel box while maintaining the aspect ratio. The images were centered <a id="id741" class="indexterm"/>in a 28 x 28 image by computing the <a id="id742" class="indexterm"/>center of mass and translating it to position it at the center of the 28 x 28 dimension grid.</p><p>Each pixel is in a range from 0 to 255 based on the intensity. The 784 pixel values are flattened out and become a high dimensional feature set for each image. The following figure depicts a sample digit 3 from the data, with mapping to the grid where each pixel has an integer value from 0 to 255.</p><p>The experiments described in this section are intended to show the application of unsupervised learning techniques to a well-known dataset. As was done in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span> with supervised learning techniques, multiple experiments were carried out using several clustering and outlier methods. Results from experiments with and without feature reduction are presented for each of the selected methods followed by an analysis of the results.</p></div><div class="section" title="Data sampling and transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec44"/>Data sampling and transformation</h2></div></div></div><p>Since our <a id="id743" class="indexterm"/>focus is on exploring the dataset using various <a id="id744" class="indexterm"/>unsupervised techniques and not on the predictive aspect, we are not concerned with train, validation, and test samples here. Instead, we use the entire dataset to train the models to perform clustering analysis.</p><p>In the case of outlier detection, we create a reduced sample of only two classes of data, namely, 1 and 7. The choice of a dataset with two similarly shaped digits was made in order to set up a problem space in which the discriminating power of the various anomaly detection techniques would stand out in greater relief.</p></div><div class="section" title="Feature analysis and dimensionality reduction"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec45"/>Feature analysis and dimensionality reduction</h2></div></div></div><p>We <a id="id745" class="indexterm"/>demonstrate different feature analysis and <a id="id746" class="indexterm"/>dimensionality reduction methods—PCA, Random Projection, and IsoMap—using the Java API of the SMILE machine learning toolkit. </p><div class="mediaobject"><img src="graphics/B05137_03_209.jpg" alt="Feature analysis and dimensionality reduction"/><div class="caption"><p>Figure 8: Showing digit 3 with pixel values distributed in a 28 by 28 matrix ranging from 0 to 254.</p></div></div><p>The <a id="id747" class="indexterm"/>code for loading the dataset and <a id="id748" class="indexterm"/>reading the values is given here along with inline comments:</p><div class="informalexample"><pre class="programlisting">//parser to parse the tab delimited file
DelimitedTextParser parser = new DelimitedTextParser();parser.setDelimiter("[\t]+");
//parse the file from the location
parser.parse("mnistData", new File(fileLocation);
//the header data file has column names to map
parser.setColumnNames(true);
//the class attribute or the response variable index
AttributeDataSet dataset = parser.setResponseIndex(new NominalAttribute("class"), 784);

//convert the data into two-dimensional array for using various techniques 
double[][] data = dataset.toArray(new double[dataset.size()][]);</pre></div><div class="section" title="PCA"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl4sec105"/>PCA</h3></div></div></div><p>The <a id="id749" class="indexterm"/>following snippet illustrates dimensionality reduction achieved using the API for PCA support:</p><div class="informalexample"><pre class="programlisting">//perform PCA with double data and using covariance //matrix
PCA pca = new PCA(data, true);
//set the projection dimension as two (for plotting here)
pca.setProjection(2);
//get the new projected data in the dimension
double[][] y = pca.project(data);</pre></div><div class="mediaobject"><img src="graphics/B05137_03_210.jpg" alt="PCA"/><div class="caption"><p>Figure 9: PCA on MNIST – On the left, we see that over 90 percent of variance in data is accounted for by fewer than half the original number of features; on the right, a representation of the data using the first two principal components.</p></div></div><p>Table 2: Summary of set of 11 random features after PCA</p><p>The PCA computation reduces the number of features to 274. In the following table you can see basic statistics for a randomly selected set of features. Feature data has been normalized as part of the PCA:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Features</p>
</th><th style="text-align: left" valign="bottom">
<p>Average</p>
</th><th style="text-align: left" valign="bottom">
<p>Std Dev</p>
</th><th style="text-align: left" valign="bottom">
<p>Min</p>
</th><th style="text-align: left" valign="bottom">
<p>Max</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>2.982922</p>
</td><td style="text-align: left" valign="top">
<p>-35.0821</p>
</td><td style="text-align: left" valign="top">
<p>19.73339</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>2.415088</p>
</td><td style="text-align: left" valign="top">
<p>-32.6218</p>
</td><td style="text-align: left" valign="top">
<p>31.63361</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>2.165878</p>
</td><td style="text-align: left" valign="top">
<p>-21.4073</p>
</td><td style="text-align: left" valign="top">
<p>16.50271</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1.78834</p>
</td><td style="text-align: left" valign="top">
<p>-27.537</p>
</td><td style="text-align: left" valign="top">
<p>31.52653</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1.652688</p>
</td><td style="text-align: left" valign="top">
<p>-21.4661</p>
</td><td style="text-align: left" valign="top">
<p>22.62837</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1.231167</p>
</td><td style="text-align: left" valign="top">
<p>-15.157</p>
</td><td style="text-align: left" valign="top">
<p>10.19708</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.861705</p>
</td><td style="text-align: left" valign="top">
<p>-6.04737</p>
</td><td style="text-align: left" valign="top">
<p>7.220233</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.631403</p>
</td><td style="text-align: left" valign="top">
<p>-6.80167</p>
</td><td style="text-align: left" valign="top">
<p>3.633182</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.606252</p>
</td><td style="text-align: left" valign="top">
<p>-5.46206</p>
</td><td style="text-align: left" valign="top">
<p>4.118598</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>10</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.578355</p>
</td><td style="text-align: left" valign="top">
<p>-4.21456</p>
</td><td style="text-align: left" valign="top">
<p>3.621186</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0.528816</p>
</td><td style="text-align: left" valign="top">
<p>-3.48564</p>
</td><td style="text-align: left" valign="top">
<p>3.896156</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 2: Summary of set of 11 random features after PCA </em></span></p></blockquote></div></div><div class="section" title="Random projections"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl4sec106"/>Random projections</h3></div></div></div><p>Here, we <a id="id750" class="indexterm"/>illustrate the straightforward usage of the API for performing data transformation using random projection:</p><div class="informalexample"><pre class="programlisting">//random projection done on the data with projection in //2 dimension
RandomProjection rp = new RandomProjection(data.length, 2, false);
//get the transformed data for plotting
double[][] projectedData = rp.project(data);</pre></div><div class="mediaobject"><img src="graphics/B05137_03_211.jpg" alt="Random projections"/><div class="caption"><p>Figure 10: PCA and Random projection - representations in two dimensions using Smile API</p></div></div></div><div class="section" title="ISOMAP"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl4sec107"/>ISOMAP</h3></div></div></div><p>This code snippet <a id="id751" class="indexterm"/>illustrates use of the API for Isomap transformation:</p><div class="informalexample"><pre class="programlisting">//perform isomap transformation of data, here in 2 //dimensions with k=10
IsoMap isomap = new IsoMap(data, 2, 10);
//get the transformed data back
double[][] y = isomap.getCoordinates();</pre></div><div class="mediaobject"><img src="graphics/B05137_03_212.jpg" alt="ISOMAP"/><div class="caption"><p>Figure 11: IsoMap – representation in two dimensions with k = 10 using Smile API</p></div></div></div><div class="section" title="Observations on feature analysis and dimensionality reduction"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec52"/>Observations on feature analysis and dimensionality reduction</h3></div></div></div><p>We <a id="id752" class="indexterm"/>can make the following observations from the <a id="id753" class="indexterm"/>results shown in the plots: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The PCA variance and number of dimensions plot clearly shows that around 100 linearly combined features have a similar representation or variance in the data (&gt; 95%) as that of the 784 original features. This is the key first step in any unsupervised feature reduction analysis.</li><li class="listitem" style="list-style-type: disc">Even PCA with two dimensions and not 100 as described previously shows some really good insights in the scatterplot visualization. Clearly, digits 2, 8, and 4 are very well separated from each other and that makes sense as they are written quite distinctly from each other. Digits such as {1,7}, {3,0,5}, and {1,9} in the low dimensional space are either overlapping or tightly clustered. This shows that with just two features it is not possible to discriminate effectively. It also shows that there is overlap in the characteristics or features amongst these classes.</li><li class="listitem" style="list-style-type: disc">The next plot comparing PCA with Random Projections, both done in lower dimension of 2, shows that there is much in common between the outputs. Both have similar separation for distinct classes as described in PCA previously. It is interesting to note that PCA does much better in separating digits {8,9,4}, for example, than Random Projections.</li><li class="listitem" style="list-style-type: disc">Isomap, the next plot, shows good discrimination, similar to PCA. Subjectively, it seems to be separating the data better than Random Projections. Visually, for instance, {3,0,5} is better separated out in Isomap than PCA.</li></ul></div></div></div><div class="section" title="Clustering models, results, and evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec46"/>Clustering models, results, and evaluation</h2></div></div></div><p>Two <a id="id754" class="indexterm"/>sets of experiments were conducted <a id="id755" class="indexterm"/>using the MNIST-6000 dataset. The dataset <a id="id756" class="indexterm"/>consists of 6,000 examples, each of which represents a hand-written digit as greyscale values of a 28 x 28 square of pixels.</p><p>First, we run some clustering techniques to identify the 10 clusters of digits. For the experiments in this part of the case study, we use the software Elki.</p><p>In the first set of experiments, there is no feature-reduction involved. All 28x28 pixels are used. Clustering <a id="id757" class="indexterm"/>techniques including k-Means, EM (Diagonal Gaussian Model Factory), DBSCAN, Hierarchical (HDBSCAN Hierarchy Extraction), as well as Affinity Propagation were used. In each case, we use metrics from two internal evaluators: Davies Bouldin and Silhouette, and several external evaluators: Precision, Recall, F1 measure, and Rand Index.</p><div class="mediaobject"><img src="graphics/B05137_03_213.jpg" alt="Clustering models, results, and evaluation"/><div class="caption"><p>Figure 12: K-Means – using Sum of Squared Errors (SSE) to find optimal <span class="emphasis"><em>k</em></span>, the number of clusters. An elbow in the curve, which is typically used to pick the optimal k value, is not particularly detectable in the plot.</p></div></div><p>In the <a id="id758" class="indexterm"/>case of k-Means, we <a id="id759" class="indexterm"/>did several runs using a range <a id="id760" class="indexterm"/>of k values. The plot <a id="id761" class="indexterm"/>shows that the Sum of Squared Errors (SSE) metric decreases with k.</p><p>The table shows results for <span class="emphasis"><em>k=10</em></span> and ranks for each are in parentheses:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Silhouette</p>
</th><th style="text-align: left" valign="bottom">
<p>Davies-Bouldin Index</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th><th style="text-align: left" valign="bottom">
<p>F1</p>
</th><th style="text-align: left" valign="bottom">
<p>Rand</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>K-Means Lloyd</p>
</td><td style="text-align: left" valign="top">
<p>+-0.09 0.0737 (1)</p>
</td><td style="text-align: left" valign="top">
<p>2.8489 (3)</p>
</td><td style="text-align: left" valign="top">
<p>0.4463 (3)</p>
</td><td style="text-align: left" valign="top">
<p>0.47843 (3)</p>
</td><td style="text-align: left" valign="top">
<p>0.4618 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.8881 (3)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>EM (Diagonal Gaussian Model Factory)</p>
</td><td style="text-align: left" valign="top">
<p>NaN</p>
</td><td style="text-align: left" valign="top">
<p>0 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1002 (6)</p>
</td><td style="text-align: left" valign="top">
<p>1 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1822 (4)</p>
</td><td style="text-align: left" valign="top">
<p>0.1003 (5)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>DBSCAN </p>
</td><td style="text-align: left" valign="top">
<p>0 (4)</p>
</td><td style="text-align: left" valign="top">
<p>0 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1003 (5)</p>
</td><td style="text-align: left" valign="top">
<p>1 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1823 (3)</p>
</td><td style="text-align: left" valign="top">
<p>0.1003 (5)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Hierarchical (HDBSCAN Hierarchy Extraction)</p>
</td><td style="text-align: left" valign="top">
<p>+-0.05 0.0435 (3)</p>
</td><td style="text-align: left" valign="top">
<p>2.7294</p>
</td><td style="text-align: left" valign="top">
<p>0.1632 (4)</p>
</td><td style="text-align: left" valign="top">
<p>0.9151 (2)</p>
</td><td style="text-align: left" valign="top">
<p>0.2770 (2)</p>
</td><td style="text-align: left" valign="top">
<p>0.5211 (4)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Hierarchical (Simplified Hierarchy Extraction)</p>
</td><td style="text-align: left" valign="top">
<p>NaN</p>
</td><td style="text-align: left" valign="top">
<p>0 (1)</p>
</td><td style="text-align: left" valign="top">
<p>1 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0017 (5)</p>
</td><td style="text-align: left" valign="top">
<p>0.0033 (6)</p>
</td><td style="text-align: left" valign="top">
<p>0.8999 (2)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Affinity Propagation</p>
</td><td style="text-align: left" valign="top">
<p>+-0.07 0.04690 (2)</p>
</td><td style="text-align: left" valign="top">
<p>1.7872 (2)</p>
</td><td style="text-align: left" valign="top">
<p>0.8279 (2)</p>
</td><td style="text-align: left" valign="top">
<p>0.0281 (4)</p>
</td><td style="text-align: left" valign="top">
<p>0.0543 (5)</p>
</td><td style="text-align: left" valign="top">
<p>0.9019 (1)</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 3. Evaluation of clustering algorithms for MNIST data </em></span></p></blockquote></div><p>In the <a id="id762" class="indexterm"/>second clustering experiment, the dataset was first <a id="id763" class="indexterm"/>pre-processed using PCA, and the resulting <a id="id764" class="indexterm"/>data with 273 features per example was used with the same algorithms as in the first experiment. The results are shown in the table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>Silhouette</p>
</th><th style="text-align: left" valign="bottom">
<p>Davies-Bouldin Index</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th><th style="text-align: left" valign="bottom">
<p>F1</p>
</th><th style="text-align: left" valign="bottom">
<p>Rand</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>K-Means Lloyd</p>
</td><td style="text-align: left" valign="top">
<p> +-0.14 0.0119</p>
</td><td style="text-align: left" valign="top">
<p>3.1830</p>
</td><td style="text-align: left" valign="top">
<p>0.3456</p>
</td><td style="text-align: left" valign="top">
<p>0.4418</p>
</td><td style="text-align: left" valign="top">
<p>0.3878 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.8601</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>EM (Diagonal Gaussian Model Factory)</p>
</td><td style="text-align: left" valign="top">
<p>+-0.16 -0.0402</p>
</td><td style="text-align: left" valign="top">
<p>3.5429</p>
</td><td style="text-align: left" valign="top">
<p>0.1808</p>
</td><td style="text-align: left" valign="top">
<p>0.3670</p>
</td><td style="text-align: left" valign="top">
<p>0.2422</p>
</td><td style="text-align: left" valign="top">
<p>0.7697</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>DBSCAN </p>
</td><td style="text-align: left" valign="top">
<p>+-0.13 -0.0351</p>
</td><td style="text-align: left" valign="top">
<p>1.3236</p>
</td><td style="text-align: left" valign="top">
<p>0.1078</p>
</td><td style="text-align: left" valign="top">
<p>0.9395 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1934</p>
</td><td style="text-align: left" valign="top">
<p>0.2143</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Hierarchical (HDBSCAN Hierarchy Extraction)</p>
</td><td style="text-align: left" valign="top">
<p>+-0.05 0.7920 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0968</p>
</td><td style="text-align: left" valign="top">
<p>0.1003</p>
</td><td style="text-align: left" valign="top">
<p>0.9996</p>
</td><td style="text-align: left" valign="top">
<p>0.1823</p>
</td><td style="text-align: left" valign="top">
<p>0.1005</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Affinity Propagation</p>
</td><td style="text-align: left" valign="top">
<p>+-0.09 0.0575</p>
</td><td style="text-align: left" valign="top">
<p>1.6296</p>
</td><td style="text-align: left" valign="top">
<p>0.6130 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.0311</p>
</td><td style="text-align: left" valign="top">
<p>0.0592</p>
</td><td style="text-align: left" valign="top">
<p>0.9009 (1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p> Subspace (DOC)</p>
</td><td style="text-align: left" valign="top">
<p>+-0.00 0.0</p>
</td><td style="text-align: left" valign="top">
<p>0 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.1003</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0.1823</p>
</td><td style="text-align: left" valign="top">
<p>0.1003</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 4. Evaluation of clustering algorithms for MNIST data after PCA </em></span></p></blockquote></div><div class="section" title="Observations and clustering analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec53"/>Observations and clustering analysis</h3></div></div></div><p>As <a id="id765" class="indexterm"/>shown in tables 2.1 and 2.2, different algorithms discussed <a id="id766" class="indexterm"/>in the sections on clustering are compared using different evaluation measures. </p><p>Generally, comparing different internal and external measures based on technical, domain and business requirements is very important. When labels or outcomes are available in the dataset, using external measures becomes an easier choice. When labeled data is not available, the norm is to use internal measures with some ranking for each and looking at comparative ranking across all measures. The important and often interesting observations are made at this stage:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Evaluating the performance of k-Means with varying <span class="emphasis"><em>k</em></span>, (shown in the figure) using a measure <a id="id767" class="indexterm"/>such as Sum of Squared Errors, is the <a id="id768" class="indexterm"/>basic step to see "optimality" of number of clusters. The figure clearly shows that as <span class="emphasis"><em>k</em></span> increases the score improves as cluster separation improves. </li><li class="listitem" style="list-style-type: disc">When we analyze Table 2.1 where all 784 features were used and all evaluation measures for the different algorithms are shown, some key things stand out:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">k-Means and Affinity Propagation both show a large overlap in the Silhouette index in terms of standard deviation and average, respectively (k-Means +-0.09 0.0737; Affinity Propagation +-0.07 0.04690). Hence it is difficult to analyze them on this metric.</li><li class="listitem" style="list-style-type: disc">In the measures such as DB Index (minimal is good), Rand Index (closer to 1 is good), we see that Affinity Propagation and Hierarchical Clustering show very good results.</li><li class="listitem" style="list-style-type: disc">In the measures where the labels are taken into account, Hierarchical Clustering, DBSCAN, and EM has either high Precision or high Recall and consequently, the F1 measure is low. k-Means gives the highest F1 measure when precision and recall are taken into consideration.</li></ul></div></li><li class="listitem" style="list-style-type: disc">In Table 2.2 where the dataset with 273 features—reduced using PCA with 95% variance retained—is run through the same algorithms and evaluated by the same measures, we make the following interesting observations:</li></ul></div><p>By reducing the features there is a negative impact on every measure for certain algorithms; for example, all the measures of k-Means degrade. An algorithm such as Affinity Propagation has a very low impact and in some cases even a positive impact when using reduced features. When compared to the results where all the features were used, AP shows similar Rand Index and F1, better Recall, DB Index and Silhouette measures, and small changes in Precision, demonstrating clear robustness. </p><p>Hierarchical <a id="id769" class="indexterm"/>Clustering shows similar results as before in terms of <a id="id770" class="indexterm"/>better DB index and Rand Index, and scores close to AP in Rand Index. </p></div></div><div class="section" title="Outlier models, results, and evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec47"/>Outlier models, results, and evaluation</h2></div></div></div><p>For <a id="id771" class="indexterm"/>the outlier detection techniques, we used a <a id="id772" class="indexterm"/>subset of the original dataset containing all examples <a id="id773" class="indexterm"/>of digit 1 and an under-sampled subset of digit 7 examples. The idea is that the similarity in shape of the two digits would cause the digit 7 examples to be found to be outliers. </p><p>The models used were selected from Angular, Distance-based, clustering, LOF, and One-Class SVM.</p><p>The outlier metrics used in the evaluation were ROC AUC, Average Precision, R-Precision, and Maximum F1 measure.</p><p>The following table shows the results obtained, with ranks in parentheses:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Algorithm</p>
</th><th style="text-align: left" valign="bottom">
<p>ROC AUC</p>
</th><th style="text-align: left" valign="bottom">
<p>Avg. Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>R-Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Maximum F1</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Angular (ABOD)</p>
</td><td style="text-align: left" valign="top">
<p>0.9515 (3)</p>
</td><td style="text-align: left" valign="top">
<p>0.1908 (4)</p>
</td><td style="text-align: left" valign="top">
<p>0.24 (4)</p>
</td><td style="text-align: left" valign="top">
<p>0.3298 (4)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Distance-based (KNN Outlier)</p>
</td><td style="text-align: left" valign="top">
<p>0.9863 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.4312 (3)</p>
</td><td style="text-align: left" valign="top">
<p>0.4533 (3)</p>
</td><td style="text-align: left" valign="top">
<p>0.4545 (3)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Distance Based (Local Isolation Coefficient)</p>
</td><td style="text-align: left" valign="top">
<p>0.9863 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.4312 (3)</p>
</td><td style="text-align: left" valign="top">
<p>0.4533 (3)</p>
</td><td style="text-align: left" valign="top">
<p>0.4545 (3)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Clustering (EM Outlier)</p>
</td><td style="text-align: left" valign="top">
<p>0.5 (5)</p>
</td><td style="text-align: left" valign="top">
<p>0.97823827 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.989 (1)</p>
</td><td style="text-align: left" valign="top">
<p>0.9945 (1)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LOF</p>
</td><td style="text-align: left" valign="top">
<p>0.4577 (6)</p>
</td><td style="text-align: left" valign="top">
<p>0.0499 (6)</p>
</td><td style="text-align: left" valign="top">
<p>0.08 (6)</p>
</td><td style="text-align: left" valign="top">
<p>0.0934 (6)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LOF (ALOKI)</p>
</td><td style="text-align: left" valign="top">
<p>0.5 (5)</p>
</td><td style="text-align: left" valign="top">
<p>0.0110 (7)</p>
</td><td style="text-align: left" valign="top">
<p>0.0110 (7)</p>
</td><td style="text-align: left" valign="top">
<p>0.0218 (7)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LOF (COF)</p>
</td><td style="text-align: left" valign="top">
<p>0.4577 (6)</p>
</td><td style="text-align: left" valign="top">
<p>0.0499 (6)</p>
</td><td style="text-align: left" valign="top">
<p>0.08 (6)</p>
</td><td style="text-align: left" valign="top">
<p>0.0934 (6)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>One-Class SVM (RBF)</p>
</td><td style="text-align: left" valign="top">
<p>0.9820 (2)</p>
</td><td style="text-align: left" valign="top">
<p>0.5637 (2)</p>
</td><td style="text-align: left" valign="top">
<p>0.5333 (2)</p>
</td><td style="text-align: left" valign="top">
<p>0.5697 (2)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>One-Class SVM (Linear)</p>
</td><td style="text-align: left" valign="top">
<p>0.8298 (4)</p>
</td><td style="text-align: left" valign="top">
<p>0.1137 (5)</p>
</td><td style="text-align: left" valign="top">
<p>0.16 (5)</p>
</td><td style="text-align: left" valign="top">
<p>0.1770 (5)</p>
</td></tr></tbody></table></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em> Table 5 Evaluation measures of Outlier analysis algorithms </em></span></p></blockquote></div><div class="section" title="Observations and analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl4sec108"/>Observations and analysis</h3></div></div></div><p>In the <a id="id774" class="indexterm"/>same way as we evaluated different clustering methods, we <a id="id775" class="indexterm"/>used several observations to compare a number of outlier algorithms. Once again, the right methodology is to judge an algorithm based on ranking across all the metrics and then getting a sense of how it does across the board as compared to other algorithms. The outlier metrics used here are all standard external measures used to compare outlier algorithms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It is interesting to see with the right parameters, that is, <span class="emphasis"><em>k=2</em></span>, EM can find the right distribution and find outliers more efficiently than most. It ranks very high and is first among the important metrics that include Maximum F1, R-Precision, and Avg. Precision.</li><li class="listitem" style="list-style-type: disc">1-Class SVM with non-linear RBF Kernel does consistently well across most measures, that is, ranks second best in ROC area, R-Precision and Avg. Precision, and Maximum F1. The difference between Linear SVM, which ranks about fifth in most rankings and 1-Class SVM, which ranks second shows that the problem is indeed nonlinear in nature. Generally, when the dimensions are high (784), and outliers are nonlinear and rare, 1-Class SVM with kernels do really well.</li><li class="listitem" style="list-style-type: disc">Local outlier-based techniques (LOF and its variants) are consistently ranked lower in almost all the measures. This gives the insight that the outlier problem may not be local, but rather global. Distance-based algorithms (KNN and Local Isolation) perform the best in ROC area under the curve and better than local outlier-based, even though using distance-based metrics gives the insight that the problem is indeed global and suited for distance-based measures.</li></ul></div><p>Table 1: Summary of features from the original dataset before pre-processing</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec32"/>Summary</h1></div></div></div><p>Both supervised and unsupervised learning methods share common concerns with respect to noisy data, high dimensionality, and demands on memory and time as the size of data grows. Other issues peculiar to unsupervised learning, due to the lack of ground truth, are questions relating to subjectivity in the evaluation of models and their interpretability, effect of cluster boundaries, and so on.</p><p>Feature reduction is an important preprocessing step that mitigates the scalability problem, in addition to presenting other advantages. Linear methods such as PCA, Random Projection, and MDS, each have specific benefits and limitations, and we must be aware of the assumptions inherent in each. Nonlinear feature reduction methods include KPCA and Manifold learning.</p><p>Among clustering algorithms, k-Means is a centroid-based technique initialized by selecting the number of clusters and it is sensitive to the initial choice of centroids. DBSCAN is one of the density-based algorithms that does not need initializing with number of clusters and is robust against noise and outliers. Among the probabilistic-based techniques are Mean Shift, which is deterministic and robust to noise, and EM/GMM, which performs well with all types of features. Both Mean Shift and EM/GMM tend to have scalability problems.</p><p>Hierarchical clustering is a powerful method involving building binary trees that iteratively groups data points until a similarity threshold is reached. Tolerance to noise depends on the similarity metric used. SOM is a two-layer neural network, allowing visualization of clusters in a 2-D grid. Spectral clustering treats the dataset as a connected graph and identifies clusters by graph partitioning. Affinity propagation, another graph-based technique, uses message passing between data points as affinities to detect clusters.</p><p>The validity and usefulness of clustering algorithms is demonstrated using various validation and evaluation measures. Internal measures have no access to ground truth; when labels are available, external measures can be used. Examples of internal measures are Silhouette index and Davies-Bouldin index. Rand index and F-measure are external evaluation measures.</p><p>Outlier and anomaly detection is an important area of unsupervised learning. Techniques are categorized as Statistical-based, Distance-based, Density-based, Clustering-based, High-dimensional-based, and One Class SVM. Outlier evaluation techniques include supervised evaluation, where ground truth is known, and unsupervised evaluation, when ground truth is not known.</p><p>Experiments using the SMILE Java API and Elki toolkit illustrate the use of the various clustering and outlier detection techniques on the MNIST6000 handwritten digits dataset. Results from different evaluation techniques are presented and compared.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec33"/>References</h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">K. Pearson (1901). <span class="emphasis"><em>On lines and planes of closest fit to systems of points in space</em></span>. Philosophical Magazine, 2:559–572.</li><li class="listitem">A. D. Back (1997). "<span class="emphasis"><em>A first application of independent component analysis to extracting structure from stock returns</em></span>," Neural Systems, vol. 8, no. 4, pp. 473–484.</li><li class="listitem">Tipping ME, Bishop CM (1999). <span class="emphasis"><em>Probabilistic principal component analysis</em></span>. Journal of the Royal Statistical Society, Series B, 61(3):611–622. 10.1111/1467-9868.00196</li><li class="listitem">Sanjoy Dasgupta (2000). <span class="emphasis"><em>Experiments with random projection</em></span>. In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence (UAI'00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.</li><li class="listitem">T. Cox and M. Cox (2001). <span class="emphasis"><em>Multidimensional Scaling</em></span>. Chapman Hall, Boca Raton, 2nd edition.</li><li class="listitem">Bernhard Schoelkopf, Alexander J. Smola, and Klaus-Robert Mueller (1999). <span class="emphasis"><em>Kernel principal component analysis</em></span>. In Advances in kernel methods, MIT Press, Cambridge, MA, USA 327-352.</li><li class="listitem">Tenenbaum, J.B.; De Silva, V.; &amp; Langford, J.C (2000).<span class="emphasis"><em>A global geometric framework for nonlinear dimensionality reduction</em></span>. Science. Vol. 290, Issue 5500, pp. 2319-2323</li><li class="listitem">M. Belkin and P. Niyogi (2003). <span class="emphasis"><em>Laplacian eigenmaps for dimensionality reduction and data representation</em></span>. Neural Computation, 15(6):1373–1396.</li><li class="listitem">S. Roweis and L. Saul (2000). <span class="emphasis"><em>Nonlinear dimensionality reduction by locally linear embedding</em></span>. Science, 290:2323–2326.</li><li class="listitem">Hartigan, J. and Wong, M (1979). <span class="emphasis"><em>Algorithm AS136: A k-means clustering algorithm</em></span>. Applied Statistics, 28, 100-108.</li><li class="listitem">Dorin Comaniciu and Peter Meer (2002). <span class="emphasis"><em>Mean Shift: A robust approach toward feature space analysis</em></span>. IEEE Transactions on Pattern Analysis and Machine Intelligence pp. 603-619.</li><li class="listitem">Hierarchical Clustering Jain, A. and Dubes, R (1988). <span class="emphasis"><em>Algorithms for Clustering Data</em></span>. Prentice-Hall, Englewood Cliffs, NJ.</li><li class="listitem">Mclachlan, G. and Basford, K (1988). <span class="emphasis"><em>Mixture Models: Inference and Applications to Clustering</em></span>. Marcel Dekker, New York, NY</li><li class="listitem">Ester, M., Kriegel, H-P., Sander, J. and Xu, X (1996). <span class="emphasis"><em>A density-based algorithm for discovering clusters in large spatial databases with noise</em></span>. In Proceedings of the 2<sup>nd</sup> ACM SIGKDD, 226-231, Portland, Oregon.</li><li class="listitem">Y. Ng, M. I. Jordan, and Y. Weiss (2001). <span class="emphasis"><em>On spectral clustering: Analysis and an algorithm</em></span>, in Advances in Neural Information Processing Systems. MIT Press, pp. 849–856.</li><li class="listitem">Delbert Dueck and Brendan J. Frey (2007). <span class="emphasis"><em>Non-metric affinity propagation for unsupervised image categorization</em></span>. In IEEE Int. Conf. Computer Vision (ICCV), pages 1–8.</li><li class="listitem">Teuvo Kohonen (2001). <span class="emphasis"><em>Self-Organizing Map</em></span>. Springer, Berlin, Heidelberg. 1995.Third, Extended Edition.</li><li class="listitem">M. Halkidi, Y. Batistakis, and M. Vazirgiannis (2001). <span class="emphasis"><em>On clustering validation techniques</em></span>, J. Intell. Inf. Syst., vol. 17, pp. 107–145.</li><li class="listitem">M. Markou, S. Singh (2003). <span class="emphasis"><em>Novelty detection: a review – part 1: statistical approaches</em></span>, Signal Process. 83 (12) 2481–2497</li><li class="listitem">Byers, S. D. AND Raftery, A. E (1998). <span class="emphasis"><em>Nearest neighbor clutter removal for estimating features in spatial point processes</em></span>. J. Amer. Statis. Assoc. 93, 577–584.</li><li class="listitem">Breunig, M. M., Kriegel, H.-P., Ng, R. T., AND Sander, J (1999). <span class="emphasis"><em>Optics-of: Identifying local outliers</em></span>. In Proceedings of the 3rd European Conference on Principles of Data Mining and Knowledge Discovery. Springer-Verlag, 262–270.</li><li class="listitem">Brito, M. R., Chavez, E. L., Quiroz, A. J., AND yukich, J. E (1997). <span class="emphasis"><em>Connectivity of the mutual k-nearest neighbor graph in clustering and outlier detection</em></span>. Statis. Prob. Lett. 35, 1, 33–42.</li><li class="listitem">Aggarwal C and Yu P S (2000). <span class="emphasis"><em>Outlier detection for high dimensional data</em></span>. In Proc ACM SIGMOD International Conference on Management of Data (SIGMOD), Dallas, TX.</li><li class="listitem">Ghoting, A., Parthasarathy, S., and Otey, M (2006). <span class="emphasis"><em>Fast mining of distance-based outliers in high dimensional spaces</em></span> In Proceedings SIAM Int Conf on Data Mining (SDM) Bethesda ML dimensional spaces. In Proc. SIAM Int. Conf. on Data Mining (SDM), Bethesda, ML.</li><li class="listitem">Kriegel, H.-P., Schubert, M., and Zimek, A (2008). <span class="emphasis"><em>Angle-based outlier detection</em></span>, In Proceedings ACM SIGKDD Int. Conf on Knowledge Discovery and Data Mining (SIGKDD) Las Vegas NV Conf. on Knowledge Discovery and Data Mining (SIGKDD), Las Vegas, NV.</li><li class="listitem">Schoelkopf, B., Platt, J. C., Shawe-Taylor, J. C., Smola, A. J., AND Williamson, R. C (2001). <span class="emphasis"><em>Estimating the support of a high-dimensional distribution</em></span>. Neural Comput. 13, 7, 1443–1471.</li><li class="listitem">F Pedregosa, et al. <span class="emphasis"><em>Scikit-learn: Machine learning in Python</em></span>. Journal of Machine Learning Research, 2825-2830.</li></ol></div></div></body></html>