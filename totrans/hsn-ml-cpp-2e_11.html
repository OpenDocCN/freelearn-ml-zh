<html><head></head><body>
		<div id="_idContainer938">
			<h1 class="chapter-number" id="_idParaDest-230"><a id="_idTextAnchor642"/>11</h1>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor643"/>Sentiment Analysis with BERT and Transfer Learning</h1>
			<p>The <strong class="bold">Transformer architecture</strong> is a <strong class="bold">neural network model</strong> that has gained significant<a id="_idIndexMarker1353"/> popularity<a id="_idIndexMarker1354"/> in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>). It was<a id="_idIndexMarker1355"/> first introduced in a paper by Vaswani et al. in 2017. The main advantage of the Transformer is its ability to handle parallel processing, which makes it faster than RNNs. Another important advantage of the Transformer is its ability to handle long-range dependencies in sequences. This is achieved through the use of attention mechanisms, which allow the model to focus on specific parts of the input when generating <span class="No-Break">the output.</span></p>
			<p>In recent years, the Transformer has been applied to a wide range of NLP tasks, including machine translation, question-answering, and summarization. Its success can be attributed to its simplicity, scalability, and effectiveness in capturing long-term dependencies. However, like any model, the Transformer also has some limitations, such as its high computational cost and reliance on large amounts of data for training. Despite these limitations, the Transformer remains a powerful tool for NLP researchers and practitioners. One of the factors that makes it possible is its ability to use and adapt already pre-trained networks for particular tasks with a much lower amount of computational resources and <span class="No-Break">training data.</span></p>
			<p>There are two main<a id="_idIndexMarker1356"/> approaches to transfer learning, known as <strong class="bold">fine-tuning</strong> and <strong class="bold">transfer learning</strong>. Fine-tuning <a id="_idIndexMarker1357"/>is a process that further adjusts a pre-trained model to better suit a specific task or dataset. It involves unfreezing some or all of the layers in the pre-trained model and training them on the new data. The process of transfer learning typically involves taking a pre-trained model, removing the final layers that are specific to the original task, and adding new layers or modifying the existing ones to fit the new task. The parameters of the model’s hidden layers are usually frozen during the training phase. This is one of the main differences <span class="No-Break">from fine-tuning.</span></p>
			<p>The following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>A general overview of the <span class="No-Break">Transformer architecture</span></li>
				<li>A brief discussion of Transformer’s main components and how they <span class="No-Break">work together</span></li>
				<li>An example of how to apply the transfer learning technique to build a new model for <span class="No-Break">sentiment analysis</span></li>
			</ul>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor644"/>Technical requirements</h1>
			<p>The following are the technical requirements for <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>The <span class="No-Break">PyTorch library</span></li>
				<li>A modern C++ compiler with <span class="No-Break">C++20 support</span></li>
				<li>The CMake build system version &gt;= <span class="No-Break">3.22</span></li>
			</ul>
			<p>The code files for this chapter can be found in the following GitHub <span class="No-Break">repo: </span><a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter11/pytorch"><span class="No-Break">https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-C-second-edition/tree/master/Chapter11/pytorch</span></a><span class="No-Break">.</span></p>
			<p>To configure the development environment, please follow the instructions described in the following <span class="No-Break">document: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/env_scripts/README.md"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/env_scripts/README.md</span></a><span class="No-Break">.</span></p>
			<p>Also, you can explore the scripts in that folder to see <span class="No-Break">configuration details.</span></p>
			<p>To build the example project for this chapter, you can use the following <span class="No-Break">script: </span><a href="https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/build_scripts/build_ch11.sh"><span class="No-Break">https://github.com/PacktPublishing/Hands-on-Machine-learning-with-C-Second-Edition/blob/main/build_scripts/build_ch11.sh</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-233"><a id="_idTextAnchor645"/>An overview of the Transformer architecture</h1>
			<p>The Transformer is a type of neural <a id="_idIndexMarker1358"/>network architecture that was first introduced in the paper <em class="italic">Attention Is All You Need</em> by Google researchers. It has become widely used in NLP and other domains due to its ability to process long-range dependencies and attention mechanisms. The following scheme shows the general <span class="No-Break">Transformer architecture:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer937">
					<img alt="Figure 11.1 – The Transformer architecture" src="image/B19849_11_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – The Transformer architecture</p>
			<p>There are two main components of the Transformer architecture: the encoder and the decoder. The encoder processes the input sequence, while the decoder generates the output sequence. Common elements of these Transformer components are <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker1359"/></span><span class="No-Break"> following:</span></p>
			<ul>
				<li><strong class="bold">Self-attention</strong>: The model uses self-attention mechanisms to learn the relationships between different parts of an input sequence. This allows it to capture long-distance dependencies, which are important for understanding context in <span class="No-Break">natural languages.</span></li>
				<li><strong class="bold">Cross-attention</strong>: This is a type of attention mechanism that is used when working with two or more sequences. In this case, the elements from one sequence attend to elements from another sequence, allowing the model to learn the relationships between the two inputs. It’s used for communication between the encoder and <span class="No-Break">decoder parts.</span></li>
				<li><strong class="bold">Multi-head attention</strong>: Instead of using a single attention mechanism, the Transformer uses multiple attention heads, each with its own weights. This helps to model different aspects of the input sequence and improve the model’s <span class="No-Break">representational power.</span></li>
				<li><strong class="bold">Positional encoding</strong>: Since the Transformer does not use recurrent or convolutional layers, it needs a way to preserve positional information about the input sequences. Positional encoding is used to add this information to the inputs. This is done by adding sine and cosine functions of different frequencies to the <span class="No-Break">embedding vectors.</span></li>
				<li><strong class="bold">Feedforward neural networks</strong>: Between the attention layers, there are fully connected feedforward neural networks. These networks help to transform the output of the attention layer into a more <span class="No-Break">meaningful representation.</span></li>
				<li><strong class="bold">Residual connections and normalization</strong>: Like many deep learning models, the Transformer includes residual connections and batch normalization to improve training stability <span class="No-Break">and convergence.</span></li>
			</ul>
			<p>Let’s see in detail the <a id="_idIndexMarker1360"/>main differences and tasks that are solved by the encoder and <span class="No-Break">decoder parts.</span></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor646"/>Encoder</h2>
			<p>The <strong class="bold">encoder</strong> part of <a id="_idIndexMarker1361"/>a Transformer is responsible for encoding<a id="_idIndexMarker1362"/> the input data into a fixed-length vector representation, known as an embedding. This embedding captures the important features and information from the input and represents it in a more abstract form. The encoder typically consists of multiple layers of self-attention mechanisms and feedforward networks. Each layer of the encoder helps to refine and improve the representation of the input, capturing more complex patterns <span class="No-Break">and dependencies.</span></p>
			<p>In terms of internal representations, the encoder produces embeddings that can be either contextual or positional. Contextual embeddings focus on capturing semantic and syntactic information from the text, while positional embeddings encode information about the order and position of words in the sequence. Both types of embeddings play a role in capturing the context and relationships between words in <span class="No-Break">a sentence:</span></p>
			<ul>
				<li>Contextual embeddings allow the model to understand the meaning of words based on their context, taking into account surrounding words and <span class="No-Break">their relationships</span></li>
				<li>Positional embeddings, on the other hand, provide information about the position of each word in the sequence, helping the model understand the order and structure of <span class="No-Break">the text</span></li>
			</ul>
			<p>Together, contextual and positional embeddings in the encoder help the Transformer to better understand and represent the input data, enabling it to generate more accurate and meaningful outputs in <span class="No-Break">downstream tasks.</span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor647"/>Decoder</h2>
			<p>The <strong class="bold">decoder</strong> part of <a id="_idIndexMarker1363"/>a Transformer takes the<a id="_idIndexMarker1364"/> encoded representation as input and generates the final output. It consists of layers of attention and feedforward networks, similar to the encoder. However, it also includes additional layers that allow it to predict and generate outputs. The decoder predicts the next token in a sequence based on the encoded representation of the input sequence and its own <span class="No-Break">previous predictions.</span></p>
			<p>The output probabilities of the Transformer decoder represent the likelihood of each possible token being the next word in the sequence. These probabilities are calculated using a softmax function, which assigns a probability value to each token based on its relevance to <span class="No-Break">the context.</span></p>
			<p>During training, the decoder uses the encoded embedding to generate predictions and compare them with the true output. The difference between the predicted and true outputs is used to update the parameters of the decoder and improve <span class="No-Break">its performance.</span></p>
			<p>Output sampling is an essential part of the decoder process. It involves selecting the next token based <a id="_idIndexMarker1365"/>on output probabilities. There are <a id="_idIndexMarker1366"/>many methods for sampling the output. The following list shows some of the <span class="No-Break">popular ones:</span></p>
			<ul>
				<li><strong class="bold">Greedy search</strong>: This is the simplest method of sampling, where the most probable token at each step is selected based on the softmax probability distribution of the token values. While it is fast and easy to implement, it may not always find the <span class="No-Break">optimal solution.</span></li>
				<li><strong class="bold">Top-k sampling</strong>: Top-k sampling selects the top <em class="italic">k</em> tokens with the highest probabilities from the softmax distribution at each step, instead of selecting the most probable token. This method can help to diversify the samples and prevent the model from getting stuck in a <span class="No-Break">local optimum.</span></li>
				<li><strong class="bold">Nucleus sampling</strong>: Nucleus sampling, also known as top-p sampling, is a variant of top-k sampling that selects a subset of tokens from the top of the softmax distribution based on their probabilities. By selecting multiple tokens within a range of probabilities, nucleus sampling can improve the diversity and coverage of <span class="No-Break">the output.</span></li>
			</ul>
			<p>You now have an understanding of how the main Transformer components work and what elements are used within it. But this leaves the topic of how inputs are preprocessed before the <a id="_idIndexMarker1367"/>decoder takes them to process <a id="_idIndexMarker1368"/>uncovered. Let’s see how input text can be converted into <span class="No-Break">Transformer input.</span></p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor648"/>Tokenization</h2>
			<p>Tokenization is the <a id="_idIndexMarker1369"/>process of breaking down <a id="_idIndexMarker1370"/>a sequence of text into smaller units, called tokens. These tokens can be individual words, subwords, or even characters, depending on the specific task and model architecture. For example, in the sentence <em class="italic">I love eating pizza</em>, the tokens would be <em class="italic">I</em>, <em class="italic">love</em>, <em class="italic">eating</em>, <span class="No-Break">and </span><span class="No-Break"><em class="italic">pizza</em></span><span class="No-Break">.</span></p>
			<p>There are many tokenization methods used in Transformer models. The most used are <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Word tokenization</strong>: This <a id="_idIndexMarker1371"/>method splits the text into individual words. It is the most common approach and is suitable for tasks such as translation and <span class="No-Break">text classification.</span></li>
				<li><strong class="bold">Subword tokenization</strong>: In this method, the text is split into smaller units, called <strong class="bold">subwords</strong>. This can improve performance on tasks where words are often misspelled or truncated, such as <span class="No-Break">machine translation.</span></li>
				<li><strong class="bold">Character tokenization</strong>: This method breaks down the text into individual characters. It can be useful for tasks that require fine-grained analysis, such as <span class="No-Break">sentiment analysis.</span></li>
			</ul>
			<p>The choice of tokenization method depends on the characteristics of the dataset and the requirements of <span class="No-Break">the task.</span></p>
			<p>In the following subsection, we will use the BERT model, which uses <strong class="bold">WordPiece tokenization</strong>. WordPiece<a id="_idIndexMarker1372"/> tokenization is designed to handle rare and out-of-vocabulary words effectively. Instead of treating each word as a separate token, it breaks down words into subword units called word pieces. This approach helps the model better handle unseen words while maintaining semantic information. In addition to word pieces, BERT also includes special tokens such as <strong class="source-inline">[CLS]</strong> (classify) and <strong class="source-inline">[SEP]</strong> (end). These tokens serve specific purposes in the model’s architecture. Here’s a step-by-step explanation of the WordPiece <span class="No-Break">tokenization algorithm:</span></p>
			<ol>
				<li><strong class="bold">Initial vocabulary</strong>: Start with a small vocabulary that includes special tokens used by the model and the initial alphabet. The initial alphabet contains all the characters present at the beginning of a word and the characters inside a word preceded by the <span class="No-Break">WordPiece prefix.</span></li>
				<li><strong class="bold">Prefixing</strong>: Add a prefix (such as <strong class="source-inline">##</strong> for BERT) to each character within a word, resulting in a split such as <strong class="source-inline">w ##o ##r ##d</strong>. This creates subwords from each character in the <span class="No-Break">original word.</span></li>
				<li><strong class="bold">Learning merge rules</strong>: WordPiece then learns merge rules based on frequency. Instead of simply merging the most frequent pair, it computes a score for each pair using the formula <strong class="source-inline">score = (freq_of_pair)/(freq_of_first_element×freq_of_second_element)</strong>. This prioritizes the merging of pairs where the individual parts are less frequent in <span class="No-Break">the vocabulary.</span></li>
				<li><strong class="bold">Merging pairs</strong>: The algorithm merges pairs with high scores, which means that the algorithm merges pairs that occur less frequently into <span class="No-Break">individual elements.</span></li>
				<li><strong class="bold">Iterative merging</strong>: The process repeats until a desired number of merge operations has been performed or a predefined threshold is reached. At this point, the final vocabulary <span class="No-Break">is created.</span></li>
			</ol>
			<p>With the vocabulary, we can tokenize any word from the input in a similar way as we did previously, during the vocabulary construction. So, at first, we search for the whole word in the vocabulary. If we don’t find it, we remove a character from the beginning preceding a subword with the <strong class="source-inline">##</strong> prefix and search again. This process is continued until we find a subword. If we don’t find any tokens for a word in a vocabulary, we usually skip <span class="No-Break">this word.</span></p>
			<p>The vocabulary is<a id="_idIndexMarker1373"/> typically represented as a dictionary or lookup table that maps each word to a unique integer index. The vocabulary size is an important factor in the performance of a Transformer, as it determines the complexity of the model and its ability to handle different types <span class="No-Break">of text.</span></p>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor649"/>Word embeddings</h2>
			<p>Even though we use tokenization to convert words into numbers, it doesn't provide any semantic meaning to the neural network. To be able to represent semantic proximity, we can use embedding. Embedding is where we map an arbitrary entity to a specific vector, for example, a node in a graph, an object in a picture, or the definition of a word. A set of embedding vectors can be treated as vector <span class="No-Break">meaning space.</span></p>
			<p>There are many approaches to creating embeddings for words. For example, the ones most known in classical NLP are Word2Vec and GloVe, which are based on statistical analysis and are actually <span class="No-Break">standalone models.</span></p>
			<p>For the Transformer, a different approach was proposed. With tokens as input, we pass them into the MLP layer, which gives embedding vectors for each of the tokens as output. This layer is trained along with the rest of the model and is an internal model component. In this way, we can have embeddings that are more fine-tuned for specific training data and <span class="No-Break">particular tasks.</span></p>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor650"/>Using the encoder and decoder parts separately</h2>
			<p>The original Transformer <a id="_idIndexMarker1374"/>architecture <a id="_idIndexMarker1375"/>has <a id="_idIndexMarker1376"/>both<a id="_idIndexMarker1377"/> parts, the encoder and the decoder. But recently, it has been found that these parts can be used separately to solve different tasks. The two most well-known base architectures based on them are BERT <span class="No-Break">and GPT.</span></p>
			<p><strong class="bold">BERT</strong> stands for <strong class="bold">bidirectional encoder representations from transformers</strong>. It’s a <a id="_idIndexMarker1378"/>state-of-the-art NLP model that uses a bidirectional approach to understand the context of words in a sentence. Unlike traditional models that only consider words in one direction, BERT can look at words both before and after a given word to better understand their meaning. It is based only on the encoder Transformer part. This makes it particularly useful for tasks that require understanding the context, such as semantic similarity and text classification. Also, it is designed to understand context in both directions, which means it can consider both the previous and following words in <span class="No-Break">a sentence.</span></p>
			<p>On the other <a id="_idIndexMarker1379"/>hand, <strong class="bold">GPT</strong>, which stands for <strong class="bold">generative pre-trained transformer</strong>, is a generative model based only on the decoder Transformer part. It is designed to generate human-like <a id="_idIndexMarker1380"/>text by<a id="_idIndexMarker1381"/> predicting the <a id="_idIndexMarker1382"/>next <a id="_idIndexMarker1383"/>word in <span class="No-Break">a sequence.</span></p>
			<p>In the next section, we will develop a sentiment analysis model with the PyTorch library using BERT as the <span class="No-Break">base model.</span></p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor651"/>Sentiment analysis example with BERT</h1>
			<p>In this section, we<a id="_idIndexMarker1384"/> are going to build a machine <a id="_idIndexMarker1385"/>learning model that can detect review sentiment (detect whether a review is positive or negative) using PyTorch. As a training set, we are going to use the Large Movie Review Dataset, which contains a set of 25,000 movie reviews for training and 25,000 for testing, both of which are <span class="No-Break">highly polarized.</span></p>
			<p>As we said before, we will use an already pre-trained BERT model. BERT was chosen due to its ability to understand context and relationships between words, making it particularly effective for tasks such as question-answering, sentiment analysis, and text classification. Let’s remember that transfer learning is a machine learning approach that involves transferring knowledge from a pre-trained model to a new or different problem domain. It is used when there is a lack of labeled data for the specific task at hand, or when training a model from scratch would be too <span class="No-Break">computationally expensive.</span></p>
			<p>Applying the transfer learning algorithm involves the <span class="No-Break">following steps:</span></p>
			<ol>
				<li><strong class="bold">Selection of a pre-trained model</strong>: The model should be chosen based on relevance to <span class="No-Break">the task.</span></li>
				<li><strong class="bold">Adding a new task-specific head</strong>: This could be, for example, a combination of fully connected linear layers with a final softmax <span class="No-Break">for classification.</span></li>
				<li><strong class="bold">Freezing the pre-trained parameters</strong>: Freezing the parameters allows the model to retain its <span class="No-Break">pre-learned knowledge.</span></li>
				<li><strong class="bold">Training on the new dataset</strong>: The model is trained using a combination of the pre-trained weights and the new data, allowing it to learn specific features and patterns relevant to the new domain in <span class="No-Break">new layers.</span></li>
			</ol>
			<p>We know that BERT-like models are used to extract some semantic knowledge from input data; in our case, it will be text. Also, BERT-like models usually represent extracted knowledge in the form of embedding vectors. These vectors can be used to train a new <a id="_idIndexMarker1386"/>model<a id="_idIndexMarker1387"/> head, for example, for a classification task. We will follow the previously <span class="No-Break">described steps.</span></p>
			<h2 id="_idParaDest-240"><a id="_idTextAnchor652"/>Exporting the model and vocabulary</h2>
			<p>The traditional<a id="_idIndexMarker1388"/> method of using some pre-trained model in C++ using the PyTorch library is to load this model as a TorchScript. A common approach to getting this script is to trace a model available in Python and save it. There are a lot of pre-trained models on the <a href="https://huggingface.co/">https://huggingface.co/</a> site. Also, these models are available with the Python API. So, let’s write a simple Python program to export the base <span class="No-Break">BERT model:</span></p>
			<ol>
				<li>The following code snippet shows how to import the required Python modules and load a pre-trained model <span class="No-Break">for tracing:</span><pre class="source-code">
import torch
from transformers import BertModel, BertTokenizer
model_name = "bert-base-cased"
tokenizer =BertTokenizer.from_pretrained(model_name,
                                         torchscript = True)
bert = BertModel.from_pretrained(model_name, torchscript=True)</pre><p class="list-inset">We imported the <strong class="source-inline">BertModel</strong> and <strong class="source-inline">BertTokenizer</strong> classes from the <strong class="source-inline">transformers</strong> module, which is the library from Hugging Face that allows us to work with different Transformer-based models. We used the <strong class="source-inline">bert-base-cased</strong> model, which is the raw BERT model that was trained to understand general language semantics on large corpus of texts, and we also loaded the tokenizer module specialized for BERT models—BertTokenizer. Notice also that we used the <strong class="source-inline">torchscript=True</strong> parameter to be able to trace and save the model. This parameter tells the library to use operators and modules suitable for Torch <span class="No-Break">JIT tracing.</span></p></li>				<li>Now that we have the loaded tokenizer object, we can tokenize some sample text for tracing <span class="No-Break">as follows:</span><pre class="source-code">
max_length = 128
tokenizer_out = tokenizer(text,
                          padding = "max_length",
                          max_length = max_length,
                          truncation = True,
                          return_tensors = "pt", )
attention_mask = tokenizer_out.attention_mask
input_ids = tokenizer_out.input_ids</pre><p class="list-inset">Here, we defined the <a id="_idIndexMarker1389"/>maximum number of tokens that can be generated, which is <strong class="source-inline">128</strong>. The BERT model we loaded can process a maximum of 512 tokens at once, so you should configure this number for your task, for example, you can use a smaller number of tokens to satisfy performance restrictions on embedded devices. Also, we told the tokenizer to truncate longer sequences and pad shorter sequences to <strong class="source-inline">max_length</strong>. We also made the tokenizer return PyTorch tensors by <span class="No-Break">specifying </span><span class="No-Break"><strong class="source-inline">return_tensors="pt"</strong></span><span class="No-Break">.</span></p><p class="list-inset">We used two values returned from the tokenizer: <strong class="source-inline">input_ids</strong>, which is the token values, and <strong class="source-inline">attention_mask</strong>, which is a binary mask filled with <strong class="source-inline">1</strong> for real tokens and <strong class="source-inline">0</strong> for padded tokens that shouldn’t <span class="No-Break">be processed.</span></p></li>				<li>Now that we have tokens and masks, we can export the model <span class="No-Break">as follows:</span><pre class="source-code">
model.eval()
traced_script_module = torch.jit.trace(model,
                                       [ input_ids,
                                       attention_mask ])
traced_script_module.save("bert_model.pt")</pre><p class="list-inset">We switched the model into evaluation mode because it will be used for tracing, not for training. Then, we used the <strong class="source-inline">torch.jit.trace</strong> function to trace the model on sample input that is the tuple of our generated tokens and attention mask. We used the <strong class="source-inline">save</strong> method of the traced module to save the model script into <span class="No-Break">a file.</span></p></li>				<li>As well as the model, we also have to export the tokenizer vocabulary, <span class="No-Break">as follows:</span><pre class="source-code">
vocab_file = open("vocab.txt", "w")
for i, j in tokenizer.get_vocab().items():
    vocab_file.write(f"{i} {j}\n")
vocab_file.close()</pre></li>			</ol>
			<p>Here, we just went<a id="_idIndexMarker1390"/> through all the available tokens in the tokenizer object and saved them as <strong class="source-inline">[value - id]</strong> pairs listed in the <span class="No-Break">text file.</span></p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor653"/>Implementing the tokenizer</h2>
			<p>We can load the <a id="_idIndexMarker1391"/>saved scripted model directly with the PyTorch C++ API, but we can’t do the same with the tokenizer. Also, there is no tokenizer implementation in the PyTorch C++ API. So, we have to implement the tokenizer <span class="No-Break">by ourselves:</span></p>
			<ol>
				<li>The simplest tokenizer can actually be implemented easily. It can have the following definition for the <span class="No-Break">header file:</span><pre class="source-code">
#include &lt;torch/torch.h&gt;
#include &lt;string&gt;
#include &lt;unordered_map&gt;
class Tokenizer {
 public:
  Tokenizer(const std::string&amp; vocab_file_path,
            int max_len = 128);
  std::pair&lt;torch::Tensor, torch::Tensor&gt; tokenize(
      const std::string text);
 private:
  std::unordered_map&lt;std::string, int&gt; vocab_;
  int max_len_{0};
}</pre><p class="list-inset">We defined the <strong class="source-inline">Tokenizer</strong> class with a constructor that takes the name of the vocabulary<a id="_idIndexMarker1392"/> file and the maximum length of the token sequence to produce. We also defined a single method, <strong class="source-inline">tokenize</strong>, that takes input text as <span class="No-Break">an argument.</span></p></li>				<li>The constructor can be implemented <span class="No-Break">as follows:</span><pre class="source-code">
Tokenizer::Tokenizer(const std::string&amp; vocab_file_path,
                     int max_len)
    : max_len_{max_len} {
  auto file = std::ifstream(vocab_file_path);
  std::string line;
  while (std::getline(file, line)) {
    auto sep_pos = line.find_first_of(' ');
    auto token = line.substr(0, sep_pos);
    auto id = std::stoi(line.substr(sep_pos + 1));
    vocab_.insert({token, id});
  }
}</pre><p class="list-inset">We simply opened the given text file and read it line by line. We split each line into two components, which are the token string value and the corresponding ID. These components are delimited with the space character. Also, we converted <strong class="source-inline">id</strong> from a <strong class="source-inline">string</strong> to an <strong class="source-inline">integer</strong> value. All parsed token ID pairs were saved into the <strong class="source-inline">std::unordered_map</strong> container to be able <a id="_idIndexMarker1393"/>to search for the token <span class="No-Break">ID effectively.</span></p></li>				<li>The <strong class="source-inline">tokenize</strong> method implementation is slightly complex. We define it <span class="No-Break">as follows:</span><pre class="source-code">
std::pair&lt;torch::Tensor, torch::Tensor&gt; Tokenizer::tokenize(const std::string text) {
  std::string pad_token = "[PAD]";
  std::string start_token = "[CLS]";
  std::string end_token = "[SEP]";
  auto pad_token_id = vocab_[pad_token];
  auto start_token_id = vocab_[start_token];
  auto end_token_id = vocab_[end_token];</pre><p class="list-inset">Here, we got the special token ID values from the loaded vocabulary. These token IDs are needed by the BERT model to correctly <span class="No-Break">process inputs.</span></p></li>				<li>The PAD token is used to mark empty tokens in the case when our input text is too short. It can be done <span class="No-Break">as follows:</span><pre class="source-code">
  std::vector&lt;int&gt; input_ids(max_len_, pad_token_id);
  std::vector&lt;int&gt; attention_mask(max_len_, 0);
  input_ids[0] = start_token_id;
  attention_mask[0] = 1;</pre><p class="list-inset">As in a Python program, we created two vectors, one for token IDs and one for the attention mask. We used <strong class="source-inline">pad_token_id</strong> as the default value for token IDs and filled the attention mask with zeros. Then, we put <strong class="source-inline">start_token_id</strong> as the first element and put the corresponding value in the <span class="No-Break">attention mask.</span></p></li>				<li>Having defined output containers, we now define the intermediate objects for input text processing, <span class="No-Break">as follows:</span><pre class="source-code">
  std::string word;
  std::istringstream ss(text);</pre><p class="list-inset">We moved our input text string into a <strong class="source-inline">stringstream</strong> object to be able to read it word by<a id="_idIndexMarker1394"/> word. We also defined the corresponding word <span class="No-Break">string object.</span></p></li>				<li>The top-level processing cycle can be defined <span class="No-Break">as follows:</span><pre class="source-code">
  int input_id = 1;
  while (getline(ss, word, ' ')) {
  // search token in the vocabulary and increment input_id
  if (input_id == max_len_ - 1) {
    break;
  }
}</pre><p class="list-inset">Here, we used the <strong class="source-inline">getline</strong> function to split an input string stream into words using the space character as a delimiter. This is a simple approach to splitting input text. Usually, tokenizers use more complex strategies for splitting. But it’s enough for our task and our dataset. Also, we added the check that if we reach the maximum sequence length, we stop text processing, so we <span class="No-Break">truncate it.</span></p></li>				<li>Then, we have to identify the longest possible prefix in the first word that exists in the vocabulary; it<a id="_idIndexMarker1395"/> can be a whole word. The implementation starts <span class="No-Break">as follows:</span><pre class="source-code">
  size_t start = 0;
while (start &lt; word.size()) {
  size_t end = word.size();
  std::string token;
  bool has_token = false;
  while (start &lt; end) {
    // search the prefix in the vocabulary
    end--;
  }
  if (input_id == max_len_ - 1) {
    break;
  }
  if (!has_token) {
    break;
  }
  start = end;
}</pre><p class="list-inset">Here, we defined the <strong class="source-inline">start</strong> variable to track the beginning of the prefix of a word, it's initialized with 0 which is the current word's beginning position. We defined the <strong class="source-inline">end</strong> variable to track the end of the prefix, it's initialized with the current word length as the word's last position. Initially, they pointed to the beginning and the end of the word. Then, in the internal loop, we continuously reduced the size of the prefix by decrementing the <strong class="source-inline">end</strong> variable. After each decrement, if we didn’t find the prefix in the vocabulary, we repeated the process until the end of the word. Also, we made it so that after a successful token search, we swapped the <strong class="source-inline">start</strong> and <strong class="source-inline">end</strong> variables to split the word and continue prefix searches for the remaining part of the word. This was done because a word can consist of several tokens. Also, in this code, we made a check for the maximum token sequence length to stop the whole token <span class="No-Break">search process.</span></p></li>				<li>The following step is<a id="_idIndexMarker1396"/> where we carry out a prefix search in <span class="No-Break">the vocabulary:</span><pre class="source-code">
  auto token = word.substr(start, end - start);
if (start &gt; 0) 
  token = "##" + token;
auto token_iter = vocab_.find(token);
if (token_iter != vocab_.end()) {
  attention_mask[input_id] = 1;
  input_ids[input_id] = token_iter-&gt;second;
  ++input_id;
  has_token = true;
  break;
}</pre><p class="list-inset">Here, we extracted the prefix from the original word by using the <strong class="source-inline">substr</strong> function. If the <strong class="source-inline">start</strong> variable is not <strong class="source-inline">0</strong>, we are working with the internal part of the word, so the addition of the <strong class="source-inline">##</strong> special prefix is carried out. We used the <strong class="source-inline">find</strong> method of the unordered map container to find a token (prefix). If the search was successful, we placed the token ID in the next position of the <strong class="source-inline">input_ids</strong> container, made a corresponding mark in <strong class="source-inline">attention_mask</strong>, increased the <strong class="source-inline">input_id</strong> index to move the current sequence position, and broke the loop to start working with the following <span class="No-Break">word part.</span></p></li>				<li>After implementing the code for filling in the input IDs and attention mask, we put them into PyTorch tensor objects and return the output <span class="No-Break">as follows:</span><pre class="source-code">
  attention_mask[input_id] = 1;
  input_ids[input_id] = end_token_id;
  auto input_ids_tensor = torch::tensor(
                             input_ids).unsqueeze(0);
  auto attention_masks_tensor = torch::tensor(
                                  attention_mask).unsqueeze(0);
return std::make_pair(input_ids_tensor,
                      attention_masks_ tensor);</pre><p class="list-inset">Before converting input IDs and masks into tensors, we finalized the token ID sequence with the <strong class="source-inline">end_token_id</strong> value. Then, we used the <strong class="source-inline">torch.tensor</strong> function to create tensor objects. This function can take different inputs, and one of them is just <strong class="source-inline">std::vector</strong> with numeric values. Also, we used the <strong class="source-inline">unsqueeze</strong> function to add the batch dimension to tensors. We also returned the<a id="_idIndexMarker1397"/> final tensors as a <span class="No-Break">standard pair.</span></p></li>			</ol>
			<p>In the following subsection, we will implement a dataset loader class using the <span class="No-Break">implemented tokenizer.</span></p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor654"/>Implementing the dataset loader</h2>
			<p>We have to develop <a id="_idIndexMarker1398"/>parser and data loader classes to move the dataset to memory in a format suitable for use <span class="No-Break">with PyTorch:</span></p>
			<ol>
				<li>Let’s start with the parser. The dataset we have is organized as follows: there are two folders for the training and testing sets, and each of these folders contains two child folders, named <strong class="source-inline">pos</strong> and <strong class="source-inline">neg</strong>, which is where the positive and negative review files are placed, respectively. Each file in the dataset contains exactly one review, and its sentiment is determined by the folder it’s placed in. In the following code sample, we will define the interface for the <span class="No-Break">reader class:</span><pre class="source-code">
#include &lt;string&gt;
#include &lt;vector&gt;
class ImdbReader {
 public:
    ImdbReader(const std::string&amp; root_path);
    size_t get_pos_size() const;
    size_t get_neg_size() const;
    const std::string&amp; get_pos(size_t index) const;
    const std::string&amp; get_neg(size_t index) const;
private:
using Reviews = std::vector&lt;std::string&gt;;
void read_ directory(const std::string&amp; path,
                     Reviews&amp; reviews);
 private:
    Reviews pos_samples_;
    Reviews neg_samples_;
    size_t max_size_{0};
};</pre><p class="list-inset">We defined two<a id="_idIndexMarker1399"/> vectors, <strong class="source-inline">pos_samples_</strong> and <strong class="source-inline">neg_samples_</strong>, which contain the reviews that were read from the <span class="No-Break">corresponding folders.</span></p></li>				<li>We will assume that the object of this class should be initialized with the path to the root folder where one of the datasets is placed (the training set or testing set). We can initialize this in the <span class="No-Break">following way:</span><pre class="source-code">
int main(int argc, char** argv) {
  if (argc &gt; 0) {
    auto root_path = fs::path(argv[1]);
    … ImdbReader train_reader(root_path / "train");
    ImdbReader test_reader(root_path / "test");
  }
}</pre><p class="list-inset">The most important parts of this class are the <strong class="source-inline">constructor</strong> and the <span class="No-Break"><strong class="source-inline">read_directory</strong></span><span class="No-Break"> methods.</span></p></li>				<li>The constructor is the main point wherein we fill the containers, <strong class="source-inline">pos_samples_</strong> and <strong class="source-inline">neg_samples_</strong>, with <a id="_idIndexMarker1400"/>actual reviews from the <strong class="source-inline">pos</strong> and <span class="No-Break"><strong class="source-inline">neg</strong></span><span class="No-Break"> folders:</span><pre class="source-code">
namespace fs = std::filesystem;
ImdbReader::ImdbReader(const std::string&amp; root_path) {
  auto root = fs::path(root_path);
  auto neg_path = root / "neg";
  auto pos_path = root / "pos";
  if (fs::exists(neg_path) &amp;&amp; fs::exists(pos_path)) {
    auto neg = std::async(std::launch::async, [&amp;]() {
      read_directory(neg_path, neg_samples_);
    });
    auto pos = std::async(std::launch::async, [&amp;]() {
      read_directory(pos_path, pos_samples_);
    });
    neg.get();
    pos.get();
  } else {
throw std::invalid_argument("ImdbReader incorrect 
                             path");
  }
}</pre></li>				<li>The <strong class="source-inline">read_directory</strong> method implements the logic for iterating files in the given directory and<a id="_idIndexMarker1401"/> reads them <span class="No-Break">as follows:</span><pre class="source-code">
void ImdbReader::read_directory(const std::string&amp; path,
                                Reviews&amp; reviews) {
  for (auto&amp; entry : fs::directory_iterator(path)) {
    if (fs::is_regular_file(entry)) {
      std::ifstream file(entry.path());
      if (file) {
        std::stringstream buffer;
        buffer &lt;&lt; file.rdbuf();
        reviews.push_back(buffer.str());
      }
    }
  }
}</pre><p class="list-inset">We used the standard library directory iterator class, <strong class="source-inline">fs::directory_iterator</strong>, to get every file in the folder. The object of this class returns the object of the <strong class="source-inline">fs::directory_entry</strong> class, and this object can be used to determine whether this is a regular file with the <strong class="source-inline">is_regular_file</strong> method. We got the file path of this entry with the <strong class="source-inline">path</strong> method. We read the whole file to one string object using the <strong class="source-inline">rdbuf</strong> method of the <strong class="source-inline">std::ifstream</strong> <span class="No-Break">type object.</span></p><p class="list-inset">Now that the <strong class="source-inline">ImdbReader</strong> class has been implemented, we can go further and start the dataset implementation. Our dataset class should return a pair of items: one representing the tokenized text and another the sentiment value. Also, we need to develop a custom function to convert the vector of tensors in a batch into one single tensor. This function is required if we want to make PyTorch compatible with our custom <span class="No-Break">training data.</span></p></li>				<li>Let’s define the <strong class="source-inline">ImdbSample</strong> type for a custom training data sample. We will use this with the <span class="No-Break"><strong class="source-inline">torch::data::Dataset</strong></span><span class="No-Break"> type:</span><pre class="source-code">
using ImdbData = std::pair&lt;torch::Tensor, torch::Tensor&gt;;
using ImdbExample = torch::data::Example&lt;ImdbData,
                                         torch::Tensor&gt;;</pre><p class="list-inset"><strong class="source-inline">ImdbData</strong> represents the training data and has two tensors for a token sequence and an <a id="_idIndexMarker1402"/>attention mask. <strong class="source-inline">ImdbSample</strong> represents the whole sample with a target value. A tensor contains <strong class="source-inline">1</strong> or <strong class="source-inline">0</strong> for positive or negative <span class="No-Break">sentiment, respectively.</span></p></li>				<li>The following code snippet shows the <strong class="source-inline">ImdbDataset</strong> <span class="No-Break">class’ declaration:</span><pre class="source-code">
class ImdbDataset : public torch::data::Dataset&lt;ImdbDataset, ImdbExample&gt; {
 public:
    ImdbDataset(const std::string&amp; dataset_path,
                std::shared_ptr&lt;Tokenizer&gt; tokenizer);
    // torch::data::Dataset implementation
    ImdbExample get(size_t index) override;
    torch::optional&lt;size_t&gt; size() const override;
 private:
    ImdbReader reader_;
    std::shared_ptr&lt;Tokenizer&gt; tokenizer_;
};</pre><p class="list-inset">We inherited our dataset class from the <strong class="source-inline">torch::data::Dataset</strong> class so that we can use it for data loader initialization. The PyTorch data loader object is responsible for sampling random training objects and making batches from them. The objects of our <strong class="source-inline">ImdbDataset</strong> class should be initialized with the root dataset path for the <strong class="source-inline">ImdbReader</strong> and <strong class="source-inline">Tokenizer</strong> objects. The constructor implementation is trivial; we just initialize the reader and store a pointer to the tokenizer. Notice that we used the pointer to the tokenizer to share it among the train and test datasets later. We overrode two methods from the <strong class="source-inline">torch::data::Dataset</strong> class: the <strong class="source-inline">get</strong> and <span class="No-Break"><strong class="source-inline">size</strong></span><span class="No-Break"> methods.</span></p></li>				<li>The following code shows how we implement the <span class="No-Break"><strong class="source-inline">size</strong></span><span class="No-Break"> method:</span><pre class="source-code">
torch::optional&lt;size_t&gt; ImdbDataset::size() const {
    return reader_.get_pos_size() + reader_.get_neg_size();
}</pre><p class="list-inset">The <strong class="source-inline">size</strong> method<a id="_idIndexMarker1403"/> returns the number of reviews in the <span class="No-Break"><strong class="source-inline">ImdbReader</strong></span><span class="No-Break"> object.</span></p></li>				<li>The <strong class="source-inline">get</strong> method has a more complicated implementation than the previous method, as shown in the <span class="No-Break">following code:</span><pre class="source-code">
ImdbExample ImdbDataset::get(size_t index) {
  torch::Tensor target;
  const std::string* review{nullptr};
  if (index &lt; reader_.get_pos_size()) {
    review = &amp;reader_.get_pos(index);
    target = torch::tensor(1, torch::dtype(torch::kLong));
  } else {
    review =
        &amp;reader_.get_neg(index - reader_.get_pos_size());
    target = torch::tensor(0, torch::dtype(torch::kLong));
  }
  // encode text
  auto tokenizer_out = tokenizer_-&gt;tokenize(*review);
  return {tokenizer_out, target.squeeze()};
}</pre><p class="list-inset">First, we got the review text and sentiment value from the given index (the function argument value). In the <strong class="source-inline">size</strong> method, we returned the total number of positive and negative reviews, so if the input index is greater than the number of positive reviews, then this index points to a negative one. Then, we subtracted the number of positive reviews <span class="No-Break">from it.</span></p><p class="list-inset">After we got the <a id="_idIndexMarker1404"/>correct index, we also got the corresponding text review, assigned its address to the <strong class="source-inline">review</strong> pointer, and initialized the <strong class="source-inline">target</strong> tensor. The <strong class="source-inline">torch::tensor</strong> function was used to initialize the <strong class="source-inline">target</strong> tensor. This function takes an arbitrary numeric value and tensor options such as the <span class="No-Break">required type.</span></p><p class="list-inset">With the review text, we just used the tokenizer object to create two tensors with token IDs and a sentiment mask. They are packed in the <strong class="source-inline">tokenizer_out</strong> pair object. We returned the pair of training tensors and one <span class="No-Break">target tensor.</span></p></li>				<li>To be able to effectively use batched training, we create a special class so that PyTorch is able to convert our non-standard sample type into a batch tensor. In the simplest case, we will get the <strong class="source-inline">std::vector</strong> object of training samples instead of a single batch tensor. It was done <span class="No-Break">as follows:</span><pre class="source-code">
torch::data::transforms::Collation&lt;ImdbExample&gt; {
  ImdbExample apply_batch(std::vector&lt;ImdbExample&gt; examples)
      override {
    std::vector&lt;torch::Tensor&gt; input_ids;
    std::vector&lt;torch::Tensor&gt; attention_masks;
    std::vector&lt;torch::Tensor&gt; labels;
    input_ids.reserve(examples.size());
    attention_masks.reserve(examples.size());
    labels.reserve(examples.size());
    for (auto&amp; example : examples) {
      input_ids.push_back(std::move(example.data.first));
      attention_masks.push_back(
          std::move(example.data.second));
      labels.push_back(std::move(example.target));
    }
    return {{torch::stack(input_ids),
             torch::stack(attention_masks)},
            torch::stack(labels)};
  }
}</pre></li>			</ol>
			<p>We inherited our<a id="_idIndexMarker1405"/> class from the special PyTorch <strong class="source-inline">torch::data::transforms::Collation</strong> type and specialized it with the template parameter of our <strong class="source-inline">ImdbExample</strong> class. Having such a class, we overrode the virtual <strong class="source-inline">apply_batch</strong> function with an implementation that takes as input the <strong class="source-inline">std::vector</strong> object containing <strong class="source-inline">ImdbExample</strong> objects and returns the single <strong class="source-inline">ImdbExample</strong> object. It means that we merged all input IDs, attention masks, and target tensors in three separate tensors. This was done by creating three separate containers for input IDs, attention masks, and target tensors. They were filled in a simple loop over the input samples. Then, we just used the <strong class="source-inline">torch::stack</strong> function to merge (stack) these containers in single tensors. This class will be used in the DataLoader<a id="_idIndexMarker1406"/> type object <span class="No-Break">construction later.</span></p>
			<h2 id="_idParaDest-243"><a id="_idTextAnchor655"/>Implementing the model</h2>
			<p>The next step is to<a id="_idIndexMarker1407"/> create a <strong class="source-inline">model</strong> class. We already have the exported model that we are going to use as a pre-trained part. We create a simple classification head with two linear fully connected layers and one dropout layer <span class="No-Break">for regularization:</span></p>
			<ol>
				<li>The header file for this class will look <span class="No-Break">as follows:</span><pre class="source-code">
#include &lt;torch/script.h&gt;
#include &lt;torch/torch.h&gt;
class ModelImpl : public torch::nn::Module {
 public:
    ModelImpl() = delete;
    ModelImpl(const std::string&amp; bert_model_path);
    torch::Tensor forward(at::Tensor input_ids,
                          at::Tensor attention_masks);
 private:
    torch::jit::script::Module bert_;
    torch::nn::Dropout dropout_;
    torch::nn::Linear fc1_;
    torch::nn::Linear fc2_;
};
TORCH_MODULE(Model);</pre><p class="list-inset">We included the <strong class="source-inline">torch\script.h</strong> header file to use the <strong class="source-inline">torch::jit::script::Module</strong> class. The instance of this class will be used as a representation of the previously exported BERT model. See the <strong class="source-inline">bert_</strong> member variable. Also, we defined member variables for the linear and dropout layers as instances of the <strong class="source-inline">torch::jit::script::Module</strong> class. We inherited our <strong class="source-inline">ModelImpl</strong> class from the <strong class="source-inline">torch::nn::Module</strong> class to integrate it into the PyTorch <span class="No-Break">auto-gradient system.</span></p></li>				<li>The constructor implementation looks <span class="No-Break">as follows:</span><pre class="source-code">
ModelImpl::ModelImpl(const std::string&amp; bert_model_path)
    : dropout_(register_module(
          "dropout",
          torch::nn::Dropout(
              torch::nn::DropoutOptions().p(0.2)))),
      fc1_(register_module(
          "fc1",
          torch::nn::Linear(
              torch::nn::LinearOptions(768, 512)))),
      fc2_(register_module(
          "fc2",
          torch::nn::Linear(
              torch::nn::LinearOptions(512, 2)))) {
  bert_ = torch::jit::load(bert_model_path);
}</pre><p class="list-inset">We used <strong class="source-inline">torch::jit::load</strong> to load <a id="_idIndexMarker1408"/>the model that we exported from Python. This function takes a single argument—the model filename. Also, we initialized the dropout and linear layers and registered them in the parent <strong class="source-inline">torch::nn::Module</strong> object. The <strong class="source-inline">fc1_</strong> linear layer is the input one; it takes the 768-dimensional output of the BERT model. The <strong class="source-inline">fc2_</strong> linear layer is the output layer. It processes the internal 512-dimensional state into <span class="No-Break">two classes.</span></p></li>				<li>The main model functionality is implemented in the <strong class="source-inline">forward</strong> function <span class="No-Break">as follows:</span><pre class="source-code">
torch::Tensor ModelImpl::forward(
    at::Tensor input_ids,
    at::Tensor attention_masks) {
  std::vector&lt;torch::jit::IValue&gt; inputs = {
      input_ids, attention_masks};
  auto bert_output = bert_.forward(inputs);
  auto pooler_output =
      bert_output.toTuple()-&gt;elements()[1].toTensor();
  auto x = fc1_(pooler_output);
  x = torch::nn::functional::relu(x);
  x = dropout_(x);
  x = fc2_(x);
  x = torch::softmax(x, /*dim=*/1);
  return x;
}</pre><p class="list-inset">This function takes the input token IDs plus the corresponding attention mask and returns a two-dimensional tensor with classification results. We interpret the sentiment analysis as a classification task. The <strong class="source-inline">forward</strong> implementation has two parts. One part is where we preprocess inputs with the loaded BERT model; this preprocessing is just an inference  with pretrained BERT model backbone. The second part is where we pass the BERT output through our<a id="_idIndexMarker1409"/> classification head. This is the trainable part. To use the BERT model, that is, the <strong class="source-inline">torch::jit:script::Module</strong> object, we packed inputs into the <strong class="source-inline">std::vector</strong> container of the <strong class="source-inline">torch::jit::Ivalue</strong> objects. The conversion from <strong class="source-inline">torch::Tensor</strong> was <span class="No-Break">done automatically.</span></p><p class="list-inset">Then, we used standard for PyTorch <strong class="source-inline">forward</strong> function for inference. This function returns the <strong class="source-inline">torch::jit</strong> tuple object; the return type actually depends on the initial model that was traced. So, to get the PyTorch tensor from the <strong class="source-inline">torch::jit</strong> value object, we explicitly used the <strong class="source-inline">toTuple</strong> method to say how to interpret the output result. Then, we accessed the second tuple element by using the <strong class="source-inline">elements</strong> method, which provides the indexing operator for tuple elements. Finally, to get the tensor, we used the <strong class="source-inline">toTensor</strong> method of the <strong class="source-inline">jit::Ivalue</strong> object, which is a <span class="No-Break">tuple element.</span></p><p class="list-inset">The BERT model we used returns two tensors. The first one represents the embedding values for input tokens, and the second one is the pooled output. Pooled output is the embeddings of the <strong class="source-inline">[CLS]</strong> token, for the input text. The linear layer weights that produced this output were trained from the next sentence prediction (classification) objective during BERT pre-training. So, these are the ideal values to use in the following text classification tasks. That is why we took the second element of the tuple returned by the <span class="No-Break">BERT model.</span></p><p class="list-inset">The second part of the <strong class="source-inline">forward</strong> function is also simple. We passed the BERT output to the <strong class="source-inline">fc1_</strong> linear layer followed by the <strong class="source-inline">relu</strong> activation function. After this operation, we got the <strong class="source-inline">512</strong> internal hidden state. Then, this state was processed by the <strong class="source-inline">dropout_</strong> module to introduce some regularization into the model. The final stage was the use of the <strong class="source-inline">fc2_</strong> output linear module, which returns a two-dimensional vector followed by the <strong class="source-inline">softmax</strong> function. The <strong class="source-inline">softmax</strong> function converts logits into probability values with the range <strong class="source-inline">[0,1]</strong> because raw logits from a linear layer can have arbitrary values and need to convert them into <span class="No-Break">target values.</span></p></li>			</ol>
			<p>Now, we have described<a id="_idIndexMarker1410"/> all the components required for the training process. Let’s see how the model training can <span class="No-Break">be implemented.</span></p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor656"/>Training the model</h2>
			<p>The first step in training <a id="_idIndexMarker1411"/>is creating the dataset object, which can be done <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
auto tokenizer = std::make_shared&lt;Tokenizer&gt;(vocab_path);
ImdbDataset train_dataset(dataset_path / "train", tokenizer);</pre>			<p>We created the <strong class="source-inline">tokenizer</strong> and <strong class="source-inline">train_dataset</strong> objects just by passing the paths to the corresponding files. The test dataset can be created in the same way with the same <strong class="source-inline">tokenizer</strong> object. Now that we have the dataset, we create a data loader <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
int batch_size = 8;
auto train_loader = torch::data::make_data_loader(
    train_dataset.map(Stack()),
    torch::data::DataLoaderOptions()
        .batch_size(batch_size)
        .workers(8));</pre>			<p>We specified the size of the batch and used the <strong class="source-inline">make_data_loader</strong> function to create the data loader object. This object uses the dataset to effectively load and organize training samples in batches. We used the transformation <strong class="source-inline">map</strong> function of the <strong class="source-inline">train_dataset</strong> object with an instance of our collation <strong class="source-inline">Stack</strong> class to allow PyTorch to merge our training samples into tensor batches. Also, we specified some data loader options, that is, the <a id="_idIndexMarker1412"/>batch size and the number of worker threads used to load and <span class="No-Break">preprocess data.</span></p>
			<p>We create the model object <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
torch::DeviceType device = torch::cuda::is_available()
    ? torch::DeviceType::CUDA
    : torch::DeviceType::CPU;
Model model(model_path);
model-&gt;to(device);</pre>			<p>We used the <strong class="source-inline">torch::cuda::is_available</strong> function to determine whether the CUDA device is available in a system and initialized the <strong class="source-inline">device</strong> variable correspondingly. Using a CUDA device can significantly improve the training and inference of a model. The model was created with the constructor that takes the path to the exported BERT model. After model object initialization, we moved this object to the <span class="No-Break">particular device.</span></p>
			<p>The last component required for training is an optimizer, which we create <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
torch::optim::AdamW optimizer(model-&gt;parameters(),
    torch::optim::AdamWOptions(1e-5));</pre>			<p>We used the <strong class="source-inline">AdamW</strong> optimizer, which is an improved version of the popular Adam optimizer. To construct the optimizer object, we passed the model parameters and the learning rate option as <span class="No-Break">constructor arguments.</span></p>
			<p>The training cycle <a id="_idIndexMarker1413"/>can be defined <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
for (int epoch = 0; epoch &lt; epochs; ++epoch) {
  model-&gt;train();
  for (auto&amp; batch : (*train_loader)) {
    optimizer.zero_grad();
    auto batch_label = batch.target.to(device);
    auto batch_input_ids =
        batch.data.first.squeeze(1).to(device);
    auto batch_attention_mask =
        batch.data.second.squeeze(1).to(device);
    auto output =
        model(batch_input_ids, batch_attention_mask);
    torch::Tensor loss =
        torch::cross_entropy_loss(output, batch_label);
    loss.backward();
    torch::nn::utils::clip_grad_norm_(model-&gt;parameters(),
                                      1.0);
    optimizer.step();
  }
}</pre>			<p>There are two nested loops. One is over the epochs and there is another, an internal one, that is over the batches. At the beginning of each internal loop is when a new epoch starts. We switched the model into training mode. This switch can be done once, but usually, you have some testing code that switches the model into evaluation mode, so this switch returns the model to the required state. Here, we omitted the test code for simplicity. It looks pretty similar to the training one, the only difference being disabling gradient calculations. In the internal loop, we used simple range-based <strong class="source-inline">for</strong> loop C++ syntax to iterate over batches. For every batch, at first, we cleared the gradient values by calling the <strong class="source-inline">zero_grad</strong> function for the optimizer object. Then, we decoupled the batch into separate tensor objects. Also, we moved these tensors to a GPU device if one is available. This was done with the <strong class="source-inline">.to(device)</strong> calls. We removed an additional dimension from the model input tensors with the <strong class="source-inline">squeeze</strong> method. This dimension appeared during an automatic <span class="No-Break">batch creation.</span></p>
			<p>Once all the tensors were prepared, we made a prediction with our model that gave us the <strong class="source-inline">output</strong> tensor. This output was used in the <strong class="source-inline">torch::cross_entropy_loss</strong> loss function, which is usually used for multi-class classification. It takes a tensor with probabilities for every class and the one-hot-encoded labels tensor. Then, we used the <strong class="source-inline">backward</strong> method of the <strong class="source-inline">loss</strong> tensor to calculate gradients. Also, we clipped gradients with the <strong class="source-inline">clip_grad_norm_</strong> function by setting a top value limit, to prevent them from exploding. Once the gradients were ready, we used the optimizer <strong class="source-inline">step</strong> function to update model weights according to the <span class="No-Break">optimizer algorithm.</span></p>
			<p>This architecture, with <a id="_idIndexMarker1414"/>the settings we used, can result in more than 80% accuracy in the sentiment analysis of movie reviews in 500 <span class="No-Break">training epochs.</span></p>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor657"/>Summary</h1>
			<p>This chapter introduced the Transformer architecture, a powerful model used in NLP and other fields of machine learning. We discussed the key components of the Transformer architecture, which include tokenization, embeddings, positional encoding, encoder, decoder, attention mechanisms, multi-head attention, cross-attention, residual connections, normalization layers, feedforward layers, and <span class="No-Break">sampling techniques.</span></p>
			<p>Finally, in the last part of this chapter, we developed an application so that we could perform a sentiment analysis of movie reviews. We applied the transfer learning technique to use the features learned by the pre-trained model in a new model designed for our specific task. We used the BERT model to produce a embedding representation of input texts and attached a linear layer classification head to classify review sentiments. We implemented a simple version of the tokenizer and dataset loader. We also developed the full training cycle of our <span class="No-Break">classification head.</span></p>
			<p>We used transfer learning instead of fine-tuning to utilize less computational resources because the fine-tuning technique usually involves re-training a full pre-trained model on a <span class="No-Break">new dataset.</span></p>
			<p>In the next chapter, we will discuss how to save and load model parameters. We will also look at the different APIs that exist in machine learning libraries for this purpose. Saving and loading model parameters can be quite an important part of the training process because it allows us to stop and restore training at an arbitrary moment. Also, saved model parameters can be used for evaluation purposes after the model has <span class="No-Break">been trained.</span></p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor658"/>Further reading</h1>
			<ul>
				<li>PyTorch <span class="No-Break">documentation: </span><a href="https://pytorch.org/cppdocs/"><span class="No-Break">https://pytorch.org/cppdocs/</span></a></li>
				<li>Hugging Face BERT model <span class="No-Break">documentation: </span><a href="https://huggingface.co/docs/transformers/model_doc/bert"><span class="No-Break">https://huggingface.co/docs/transformers/model_doc/bert</span></a></li>
				<li>An illustrated Transformer <span class="No-Break">explanation: </span><a href="https://jalammar.github.io/illustrated-transformer"><span class="No-Break">https://jalammar.github.io/illustrated-transformer</span></a></li>
				<li><em class="italic">Attention Is All You Need</em>, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia <span class="No-Break">Polosukhin: </span><a href="https://arxiv.org/abs/1706.03762"><span class="No-Break">https://arxiv.org/abs/1706.03762</span></a></li>
				<li>A list of already pre-trained BERT-like models for sentiment <span class="No-Break">analysis: </span><a href="https://huggingface.co/models?other=sentiment-analysis"><span class="No-Break">https://huggingface.co/models?other=sentiment-analysis</span></a></li>
			</ul>
		</div>
	

		<div class="Content" id="_idContainer939">
			<h1 id="_idParaDest-247" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor659"/>Part 4: Production and Deployment Challenges</h1>
			<p>The crucial feature of C++ is the ability of the program to be compiled and run on a variety of hardware platforms. You can train your complex <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) model on the fastest GPU in the data center and deploy it to tiny mobile devices with limited resources. This part will show you how to use C++ APIs of various ML frameworks to save and load trained models, and how to track and visualize a training process, which is crucial for ML practitioners to be able to control and check a model’s training performance. Also, we will learn how to build programs that use ML models on Android devices; in particular, we will create an object detection system that uses a <span class="No-Break">device’s camera.</span></p>
			<p>This part comprises the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B19849_12.xhtml#_idTextAnchor660"><em class="italic">Chapter 12</em></a>, <em class="italic">Exporting and Importing Models</em></li>
				<li><a href="B19849_13.xhtml#_idTextAnchor689"><em class="italic">Chapter 13</em></a>, <em class="italic">Tracking and Visualizing ML Experiments</em></li>
				<li><a href="B19849_14.xhtml#_idTextAnchor702"><em class="italic">Chapter 14</em></a>, <em class="italic">Deploying Models on a Mobile Platform</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer940">
			</div>
		</div>
		<div>
			<div class="Basic-Graphics-Frame" id="_idContainer941">
			</div>
		</div>
	</body></html>