- en: '*Chapter 5*: Feature Selection'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on how you began your data analytic work and your own intellectual
    interests, you might have a different perspective on the topic of **feature selection**.
    You might think, *yeah, yeah, it is an important topic, but I really want to get
    to the model building*. Or, at the other extreme, you might view feature selection
    as at the core of model building and believe that you are 90% of the way toward
    having your model once you have chosen your features. For now, let's just agree
    that we should spend a good chunk of time understanding the relationships between
    features – and their relationship to a target if we are building a supervised
    model – before we do any serious model specification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: It is helpful to approach our feature selection work with the attitude that
    less is more. If we can reach nearly the same degree of accuracy or explain as
    much of the variance with fewer features, we should select the simpler model.
    Sometimes, we can actually get better accuracy with fewer features. This can be
    hard to wrap our brains around, and even be a tad disappointing for those of us
    who cut our teeth on building models that told rich and complicated stories.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: But we are less concerned with parameter estimates than with the accuracy of
    our predictions when fitting machine learning models. Unnecessary features can
    contribute to overfitting and tax hardware resources.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We can sometimes spend months specifying the features of our model, even when
    there is a limited number of columns in the data. Bivariate correlations, such
    as those created in [*Chapter 2*](B17978_02_ePub.xhtml#_idTextAnchor025), *Examining
    Bivariate and Multivariate Relationships between Features and Targets*, give us
    some sense of what to expect, but the importance of a feature can vary significantly
    once other potentially explanatory features are introduced. The feature may no
    longer be significant, or, conversely, may only be significant when other features
    are included. Two features might be so highly correlated that including both of
    them offers very little additional information than including just one.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter takes a close look at feature selection techniques applicable
    to a variety of predictive modeling tasks. Specifically, we will explore the following
    topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Selecting features for classification models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting features for regression models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using forward and backward feature selection
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using exhaustive feature selection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating features recursively in a regression model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating features recursively in a classification model
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Boruta for feature selection
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using regularization and other embedded methods
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using principal component analysis
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will work with the `feature_engine`, `mlxtend`, and `boruta` packages in
    this chapter, in addition to the `scikit-learn` library. You can use `pip` to
    install these packages. I have chosen a dataset with a small number of observations
    for our work in this chapter, so the code should work fine even on suboptimal
    workstations.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将使用`feature_engine`、`mlxtend`和`boruta`包，以及`scikit-learn`库。您可以使用`pip`安装这些包。我选择了一个观测值数量较少的数据集用于本章的工作，因此代码即使在次优工作站上也能正常运行。
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We will work exclusively in this chapter with data from The National Longitudinal
    Survey of Youth, conducted by the United States Bureau of Labor Statistics. This
    survey started with a cohort of individuals in 1997 who were born between 1980
    and 1985, with annual follow-ups each year through 2017\. We will work with educational
    attainment, household demographic, weeks worked, and wage income data. The wage
    income column represents wages earned in 2016\. The NLS dataset can be downloaded
    for public use at [https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专门使用美国劳工统计局进行的《青年纵向调查》数据。这项调查始于1997年，调查对象为1980年至1985年间出生的一代人，每年进行一次年度跟踪调查，直至2017年。我们将使用教育成就、家庭人口统计、工作周数和工资收入数据。工资收入列代表2016年赚取的工资。NLS数据集可以下载供公众使用，网址为[https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search)。
- en: Selecting features for classification models
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为分类模型选择特征
- en: The most straightforward feature selection methods are based on each feature's
    relationship with a target variable. The next two sections examine techniques
    for determining the *k* best features based on their linear or non-linear relationship
    with the target. These are known as filter methods. They are also sometimes called
    univariate methods since they evaluate the relationship between the feature and
    the target independent of the impact of other features.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的特征选择方法是基于每个特征与目标变量的关系。接下来的两个部分将探讨基于特征与目标变量之间的线性或非线性关系来确定最佳*k*个特征的技术。这些被称为过滤方法。它们有时也被称为单变量方法，因为它们评估特征与目标变量之间的关系，而不考虑其他特征的影响。
- en: We use somewhat different strategies when the target is categorical than when
    it is continuous. We'll go over the former in this section and the latter in the
    next.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标变量为分类变量时，我们使用的策略与目标变量为连续变量时有所不同。在本节中，我们将介绍前者，在下一节中介绍后者。
- en: Mutual information classification for feature selection with a categorical target
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于分类目标的互信息特征选择
- en: We can use **mutual information** classification or **analysis of variance**
    (**ANOVA**) tests to select features when we have a categorical target. We will
    try mutual information classification first, and then ANOVA for comparison.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标变量为分类变量时，我们可以使用**互信息**分类或**方差分析**（**ANOVA**）测试来选择特征。我们将首先尝试互信息分类，然后进行ANOVA比较。
- en: Mutual information is a measure of how much information about a variable is
    provided by knowing the value of another variable. At the extreme, when features
    are completely independent, the mutual information score is 0.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 互信息是衡量通过知道另一个变量的值可以获得多少关于变量的信息的度量。在极端情况下，当特征完全独立时，互信息分数为0。
- en: We can use `scikit-learn`'s `SelectKBest` class to select the *k* features that
    have the highest predictive strength based on mutual information classification
    or some other appropriate measure. We can use hyperparameter tuning to select
    the value of *k*. We can also examine the scores of all features, whether they
    were identified as one of the *k* best or not, as we will see in this section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`scikit-learn`的`SelectKBest`类根据互信息分类或其他适当的度量选择具有最高预测强度的*k*个特征。我们可以使用超参数调整来选择*k*的值。我们还可以检查所有特征的分数，无论它们是否被识别为*k*个最佳特征之一，正如我们将在本节中看到的。
- en: 'Let''s first try mutual information classification to identify features that
    are related to completing a bachelor''s degree. Later, we will compare that with
    using ANOVA F-values as the basis for selection:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先尝试互信息分类来识别与完成学士学位相关的特征。稍后，我们将将其与使用ANOVA F值作为选择依据进行比较：
- en: 'We start by importing `OneHotEncoder` from `feature_engine` to encode some
    of the data, and `train_test_split` from `scikit-learn` to create training and
    testing data. We will also need `scikit-learn`''s `SelectKBest`, `mutual_info_classif`,
    and `f_classif` modules for our feature selection:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从`feature_engine`导入`OneHotEncoder`来编码一些数据，并从`scikit-learn`导入`train_test_split`来创建训练和测试数据。我们还需要`scikit-learn`的`SelectKBest`、`mutual_info_classif`和`f_classif`模块来进行特征选择：
- en: '[PRE0]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We load NLS data that has a binary variable for having completed a bachelor''s
    degree and features possibly related to degree attainment: `gender` feature, and
    scale the other data:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载了具有完成学士学位的二进制变量和可能与学位获得相关的特征的数据集：`gender`特征，并对其他数据进行缩放：
- en: '[PRE1]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We will do a complete case analysis of the NLS data throughout this chapter;
    that is, we will remove all observations that have missing values for any of the
    features. This is not usually a good approach and is particularly problematic
    when data is not missing at random or when there is a large number of missing
    values for one or more features. In such cases, it would be better to use some
    of the approaches that we used in [*Chapter 3*](B17978_03_ePub.xhtml#_idTextAnchor034),
    *Identifying and Fixing Missing Values*. We will do a complete case analysis in
    this chapter to keep the examples as straightforward as possible.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本章中，我们将对NLS数据进行完整案例分析；也就是说，我们将删除任何特征缺失的观测值。这通常不是一个好的方法，尤其是在数据不是随机缺失或一个或多个特征有大量缺失值时尤其有问题。在这种情况下，最好使用我们在[*第3章*](B17978_03_ePub.xhtml#_idTextAnchor034)中使用的某些方法，*识别和修复缺失值*。我们将在本章中进行完整案例分析，以使示例尽可能简单。
- en: 'Now we are ready to select features for our model of bachelor''s degree completion.
    One approach is to use mutual information classification. To do that, we set the
    `score_func` value of `SelectKBest` to `mutual_info_classif` and indicate that
    we want the five best features. Then, we call `fit` and use the `get_support`
    method to get the five best features:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好为我们的学士学位完成模型选择特征。一种方法是用互信息分类。为此，我们将`SelectKBest`的`score_func`值设置为`mutual_info_classif`，并指出我们想要五个最佳特征。然后，我们调用`fit`并使用`get_support`方法来获取五个最佳特征：
- en: '[PRE2]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If we also want to see the score for each feature, we can use the `scores_`
    attribute, though we need to do a little work to associate the scores with a particular
    feature name and sort the scores in descending order:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们还想看到每个特征的得分，我们可以使用`scores_`属性，尽管我们需要做一些工作来将得分与特定的特征名称关联起来，并按降序排序：
- en: '[PRE3]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: This is a stochastic process, so we will get different results each time we
    run it.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一个随机过程，所以每次运行它时我们都会得到不同的结果。
- en: 'To get the same results each time, you can pass a partial function to `score_func`:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了每次都能得到相同的结果，你可以将一个部分函数传递给`score_func`：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We can create a DataFrame with just the important features using the `selcols`
    array we created using `get_support`. (We could have used the `transform` method
    of `SelectKBest` instead. This would have returned the values of the selected
    features as a NumPy array.)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用使用`get_support`创建的`selcols`数组来创建仅包含重要特征的DataFrame。（我们也可以使用`SelectKBest`的`transform`方法。这将返回所选特征的值作为NumPy数组。）
- en: '[PRE5]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That is all we need to do to select the *k* best features for our model using
    mutual information.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们使用互信息来选择模型中*最佳k个特征*所需做的所有事情。
- en: ANOVA F-value for feature selection with a categorical target
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分类目标的特征选择的ANOVA F值
- en: Alternatively, we can use ANOVA instead of mutual information. ANOVA evaluates
    how different the mean for a feature is for each target class. This is a good
    metric for univariate feature selection when we can assume a linear relationship
    between features and the target and our features are normally distributed. If
    those assumptions do not hold, mutual information classification is a better choice.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用方差分析（ANOVA）而不是互信息。方差分析评估每个目标类中特征的平均值差异。当我们假设特征和目标之间存在线性关系，并且我们的特征是正态分布时，这是一个很好的单变量特征选择指标。如果这些假设不成立，互信息分类是一个更好的选择。
- en: 'Let''s try using ANOVA for our feature selection. We can set the `score_func`
    parameter of `SelectKBest` to `f_classif` to select based on ANOVA:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用ANOVA进行特征选择。我们可以将`SelectKBest`的`score_func`参数设置为`f_classif`，以便基于ANOVA进行选择：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This selected the same features as were selected when we used mutual information.
    Showing the scores gives us some indication of whether the selected value for
    *k* makes sense. For example, there is a greater drop in score from the fifth-
    to the sixth-best feature (77-61) than from the fourth to the fifth (85-77). There
    is an even bigger decline from the sixth to the seventh, however (61-37), suggesting
    that we should at least consider a value for *k* of 6\.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这选择了与我们使用互信息时选择的相同特征。显示得分给我们一些关于所选的*k*值是否合理的指示。例如，第五到第六个最佳特征的得分下降（77-61）比第四到第五个（85-77）的下降更大。然而，从第六到第七个的下降更大（61-37），这表明我们至少应该考虑*k*的值为6。
- en: ANOVA tests, and the mutual information classification we did earlier, do not
    take into account features that are only important in multivariate analysis. For
    example, `fatherhighgrade` might matter among individuals with similar GPA or
    SAT scores. We use multivariate feature selection methods later in this chapter.
    We do more univariate feature selection in the next section where we explore selection
    techniques appropriate for continuous targets.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ANOVA测试和之前我们做的互信息分类没有考虑在多元分析中仅重要的特征。例如，`fatherhighgrade`可能在具有相似GPA或SAT分数的个人中很重要。我们将在本章后面使用多元特征选择方法。在下一节中，我们将进行更多单变量特征选择，以探索适合连续目标的特征选择技术。
- en: Selecting features for regression models
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择回归模型的特征
- en: '`scikit-learn`''s selection module provides several options for selecting features
    when building regression models. (By regression models here, I do not mean linear
    regression models. I am only referring to *models with continuous targets*.) Two
    good options are selection based on F-tests and selection based on mutual information
    for regression. Let''s start with F-tests.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`scikit-learn`的选择模块在构建回归模型时提供了几个选择特征的选择。在这里，我不指线性回归模型。我只是在指*具有连续目标的模型*）。两个好的选择是基于F检验的选择和基于回归的互信息选择。让我们从F检验开始。'
- en: F-tests for feature selection with a continuous target
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于连续目标的特征选择的F检验
- en: The F-statistic is a measure of the strength of the linear correlation between
    a target and a single regressor. `Scikit-learn` has an `f_regression` scoring
    function, which returns F-statistics. We can use it with `SelectKBest` to select
    features based on that statistic.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: F统计量是目标与单个回归器之间线性相关强度的度量。`Scikit-learn`有一个`f_regression`评分函数，它返回F统计量。我们可以使用它与`SelectKBest`一起选择基于该统计量的特征。
- en: 'Let''s use F-statistics to select features for a model of wages. We use mutual
    information for regression in the next section to select features for the same
    target:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用F统计量来选择工资模型的特征。我们将在下一节中使用互信息来选择相同目标的特征：
- en: 'We start by importing the one-hot encoder from `feature_engine` and `train_test_split`
    and `SelectKBest` from `scikit-learn`. We also import `f_regression` to get F-statistics
    later:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从`feature_engine`导入one-hot编码器，从`scikit-learn`导入`train_test_split`和`SelectKBest`。我们还导入`f_regression`以获取后续的F统计量：
- en: '[PRE26]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we load the NLS data, including educational attainment, parental income,
    and wage income data:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载NLS数据，包括教育成就、家庭收入和工资收入数据：
- en: '[PRE27]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we create training and testing DataFrames, encode the `gender` feature,
    and scale the training data. We need to scale the target in this case since it
    is continuous:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建训练和测试数据框，对`gender`特征进行编码，并对训练数据进行缩放。在这种情况下，我们需要对目标进行缩放，因为它是有连续性的：
- en: '[PRE28]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You may have noticed that we are not encoding or scaling the testing data. We
    will need to do that eventually to validate our models. We will introduce validation
    later in this chapter and go over it in much more detail in the next chapter.
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们没有对测试数据进行编码或缩放。我们最终需要这样做以验证我们的模型。我们将在本章后面介绍验证，并在下一章中详细介绍。
- en: 'Now, we are ready to select features. We set `score_func` of `SelectKBest`
    to `f_regression` and indicate that we want the five best features. The `get_support`
    method of `SelectKBest` returns `True` for each feature that was selected:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已准备好选择特征。我们将`SelectKBest`的`score_func`设置为`f_regression`，并指出我们想要五个最佳特征。`SelectKBest`的`get_support`方法对每个被选中的特征返回`True`：
- en: '[PRE29]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can use the `scores_` attribute to see the score for each feature:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`scores_`属性来查看每个特征的得分：
- en: '[PRE30]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The disadvantage of the F-statistic is that it assumes a linear relationship
    between each feature and the target. When that assumption does not make sense,
    we can use mutual information for regression instead.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Mutual information for feature selection with a continuous target
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also use `SelectKBest` to select features using mutual information for
    regression:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: We need to set the `score_func` parameter of `SelectKBest` to `mutual_info_regression`,
    but there is a small complication. To get the same results each time we run the
    feature selection, we need to set a `random_state` value. As we discussed in the
    previous section, we can use a partial function for that. We pass `partial(mutual_info_regression,
    random_state=0)` to the score function.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can then run the `fit` method and use `get_support` to get the selected
    features. We can use the `scores_` attribute to give us the score for each feature:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We get fairly similar results with mutual information for regression as we did
    with F-tests. `parentincome` was selected with F-tests and `fatherhighgrade` with
    mutual information. Otherwise, the same features are selected.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of mutual information for regression compared with F-tests is
    that it does not assume a linear relationship between the feature and the target.
    If that assumption turns out to be unwarranted, mutual information is a better
    approach. (Again, there is also some randomness in the scoring and the score for
    each feature can bounce around within a limited range.)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Our choice of `k=5` to get the five best features is quite arbitrary. We can
    make it much more scientific with some hyperparameter tuning. We will go over
    tuning in the next chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The feature selection methods we have used so far are known as *filter methods*.
    They examine the univariate relationship between each feature and the target.
    They are a good starting point. Similar to our discussion in previous chapters
    of the usefulness of having correlations handy before we start examining multivariate
    relationships, it is helpful to at least explore filter methods. Often, though,
    our model fitting will require taking into account features that are important,
    or not, when other features are also included. To do that, we need to use wrapper
    or embedded methods for feature selection. We explore wrapper methods in the next
    few sections, starting with forward and backward feature selection.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Using forward and backward feature selection
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Forward and backward feature selection, as their names suggest, select features
    by adding them one by one – or subtracting them for backward selection – and assessing
    the impact on model performance after each iteration. Since both methods assess
    that performance based on a given algorithm, they are considered **wrapper** selection
    methods.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper feature selection methods have two advantages over the filter methods
    we have explored so far. First, they evaluate the importance of features as other
    features are included. Second, since features are evaluated based on their contribution
    to the performance of a specific algorithm, we get a better sense of which features
    will ultimately matter. For example, `satmath` seemed to be an important feature
    based on our results from the previous section. But it is possible that `satmath`
    is only important when we use a particular model, say linear regression, and not
    an alternative such as decision tree regression. Wrapper selection methods can
    help us discover that.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 包装特征选择方法相对于我们之前探索的过滤方法有两个优点。首先，它们在包含其他特征时评估特征的重要性。其次，由于特征是根据其对特定算法性能的贡献来评估的，因此我们能够更好地了解哪些特征最终会起作用。例如，根据我们上一节的结果，`satmath`似乎是一个重要的特征。但有可能`satmath`只有在使用特定模型时才重要，比如线性回归，而不是决策树回归等其他模型。包装选择方法可以帮助我们发现这一点。
- en: The main disadvantage of wrapper methods is that they can be quite expensive
    computationally since they retrain the model after each iteration. We will look
    at both forward and backward feature selection in this section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 包装方法的缺点主要在于它们在每次迭代后都会重新训练模型，因此在计算上可能相当昂贵。在本节中，我们将探讨前向和后向特征选择。
- en: Using forward feature selection
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用前向特征选择
- en: '**Forward feature selection** starts by identifying a subset of features that
    individually have a significant relationship with a target, not unlike the filter
    methods. But it then evaluates all possible combinations of the selected features
    for the combination that performs best with the chosen algorithm.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**前向特征选择**首先识别出与目标有显著关系的特征子集，这与过滤方法类似。但它随后评估所有可能的选择特征的组合，以确定与所选算法表现最佳的组合。'
- en: 'We can use forward feature selection to develop a model of bachelor''s degree
    completion. Since wrapper methods require us to choose an algorithm, and this
    is a binary target, let''s use `scikit-learn`''s `feature_selection` module of
    `mlxtend` to do the iteration required to select features:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用前向特征选择来开发一个完成学士学位的模型。由于包装方法要求我们选择一个算法，而这是一个二元目标，因此让我们使用`scikit-learn`的`mlxtend`模块中的`feature_selection`来进行选择特征的迭代：
- en: 'We start by importing the necessary libraries:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库：
- en: '[PRE32]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we load the NLS data again. We also create a training DataFrame, encode
    the `gender` feature, and standardize the remaining features:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们再次加载NLS数据。我们还创建了一个训练DataFrame，对`gender`特征进行编码，并对剩余特征进行标准化：
- en: '[PRE33]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We create a random forest classifier object and then pass that object to the
    feature selector of `mlxtend`. We indicate that we want it to select five features
    and that it should forward select. (We can also use the sequential feature selector
    to select backward.) After running `fit`, we can use the `k_feature_idx_` attribute
    to get the list of selected features:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个随机森林分类器对象，然后将该对象传递给`mlxtend`的特征选择器。我们指出我们想要选择五个特征，并且应该进行前向选择。（我们也可以使用顺序特征选择器进行后向选择。）运行`fit`后，我们可以使用`k_feature_idx_`属性来获取所选特征的列表：
- en: '[PRE34]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You might recall from the first section of this chapter that our univariate
    feature selection for the completed bachelor''s degree target gave us somewhat
    different results:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得本章的第一节，我们针对完成学士学位目标的多变量特征选择给出了不同的结果：
- en: '[PRE35]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Three of the features – `satmath`, `satverbal`, and `gpaoverall` – are the same.
    But our forward feature selection has identified `parentincome` and `gender_Female`
    as more important than `gpascience` and `gpaenglish`, which were selected in the
    univariate analysis. Indeed, `gender_Female` had among the lowest scores in the
    earlier analysis. These differences likely reflect the advantages of wrapper feature
    selection methods. We can identify features that are not important unless other
    features are included, and we are evaluating the effect on the performance of
    a particular algorithm, in this case, random forest classification.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个特征——`satmath`、`satverbal`和`gpaoverall`——是相同的。但我们的前向特征选择已经将`parentincome`和`gender_Female`识别为比在单变量分析中选择的`gpascience`和`gpaenglish`更重要的特征。实际上，`gender_Female`在早期分析中的得分最低。这些差异可能反映了包装特征选择方法的优点。我们可以识别出除非包含其他特征，否则不重要的特征，并且我们正在评估对特定算法（在这种情况下是随机森林分类）性能的影响。
- en: One disadvantage of forward selection is that *once a feature is selected, it
    is not removed, even though it may decline in importance as additional features
    are added*. (Recall that forward feature selection adds features iteratively based
    on the contribution of that feature to the model.)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Let's see whether our results vary with backward feature selection.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Using backward feature selection
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Backward feature selection starts with all features and eliminates the least
    important. It then repeats this process with the remaining features. We can use
    `mlxtend`'s `SequentialFeatureSelector` for backward selection in pretty much
    the same way we used it for forward selection.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'We instantiate a `RandomForestClassifier` object from the `scikit-learn` library
    and then pass it to `mlxtend`''s sequential feature selector:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Perhaps unsurprisingly, we get different results for our feature selection.
    `satmath` and `parentincome` are no longer selected, and `gpascience` and `gpaenglish`
    are.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Backward feature selection has the opposite drawback to forward feature selection.
    *Once a feature has been removed, it is not re-evaluated, even though its importance
    may change with different feature mixtures*. Let's try exhaustive feature selection
    instead.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Using exhaustive feature selection
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your results from forward and backward selection are unpersuasive, and you
    do not mind running a model while you go out for coffee or lunch, you can try
    exhaustive feature selection. **Exhaustive feature selection** trains a given
    model on all possible combinations of features and selects the best subset of
    features. But it does this at a price. As the name suggests, this procedure might
    exhaust both system resources and your patience.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use exhaustive feature selection for our model of bachelor''s degree
    completion:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the required libraries, including the `RandomForestClassifier`
    and `LogisticRegression` modules from `scikit-learn` and `ExhaustiveFeatureSelector`
    from `mlxtend`. We also import the `accuracy_score` module so that we can evaluate
    a model with the selected features:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we load the NLS educational attainment data and create training and testing
    DataFrames:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Then, we encode and scale the training and testing data:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We create a random forest classifier object and pass it to `mlxtend`''s `ExhaustiveFeatureSelector`.
    We tell the feature selector to evaluate all combinations of one to five features
    and return the combination with the highest accuracy in predicting degree attainment.
    After running `fit`, we can use the `best_feature_names_` attribute to get the
    selected features:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s evaluate the accuracy of this model. We first need to transform the
    training and testing data to include only the four selected features. Then, we
    can fit the random forest classifier again with just those features and generate
    the predicted values for bachelor''s degree completion. We can then calculate
    the percentage of the time we predicted the target correctly, which is 67%:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We get the same answer if we just use scikit-learn's `accuracy score` instead.
    (We calculate it in the previous step because it is pretty straightforward and
    it gives us a better sense of what is meant by accuracy in this case.)
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们只使用scikit-learn的`accuracy score`，我们也会得到相同的答案。（我们在上一步计算它，因为它相当直接，并且让我们更好地理解在这种情况下准确率的含义。）
- en: '[PRE50]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The accuracy score is often used to assess the performance of a classification
    model. We will lean on it in this chapter, but other measures might be equally
    or more important depending on the purposes of your model. For example, we are
    sometimes more concerned with sensitivity, the ratio of our correct positive predictions
    to the number of actual positives. We examine the evaluation of classification
    models in detail in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing
    for Model Evaluation.*
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率分数通常用于评估分类模型的性能。在本章中，我们将依赖它，但根据您模型的目的，其他指标可能同样重要或更重要。例如，我们有时更关心灵敏度，即我们的正确阳性预测与实际阳性数量的比率。我们在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)中详细探讨了分类模型的评估，*准备模型评估*。
- en: 'Let''s now try exhaustive feature selection with a logistic model:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们尝试使用逻辑模型进行全面特征选择：
- en: '[PRE51]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let''s look at the accuracy of the logistic model. We get a fairly similar
    accuracy score:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看逻辑模型的准确率。我们得到了相当相似的准确率分数：
- en: '[PRE52]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: One key advantage of the logistic model is that it is much faster to train,
    which really makes a difference with exhaustive feature selection. If we time
    the training for each model (probably not a good idea to do that on your computer
    unless it's a pretty high-end machine or you don't mind walking away from your
    computer for a while), we see a substantial difference in average training time
    – from an amazing 5 minutes for the random forest to 4 seconds for the logistic
    regression. (Of course, the absolute numbers are machine-dependent.)
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 逻辑模型的一个关键优势是它训练得更快，这对于全面特征选择来说确实有很大影响。如果我们为每个模型计时（除非你的电脑相当高端或者你不在乎离开电脑一会儿，否则这通常不是一个好主意），我们会看到平均训练时间有显著差异——从随机森林的惊人的5分钟到逻辑回归的4秒。（当然，这些绝对数字取决于机器。）
- en: '[PRE53]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Exhaustive feature selection can provide very clear guidance about the features
    to select, as I have mentioned, but that may come at too high a price for many
    projects. It may actually be better suited for *diagnostic work* than for use
    in a machine learning pipeline. If a linear model is appropriate, it can lower
    the computational costs considerably.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所述，全面特征选择可以提供关于要选择哪些特征的非常清晰的指导，但这可能对许多项目来说代价太高。实际上，它可能更适合于*诊断工作*而不是用于机器学习管道。如果一个线性模型是合适的，它可以显著降低计算成本。
- en: Wrapper methods, such as forward, backward, and exhaustive feature selection,
    tax system resources because they need to be trained with each iteration, and
    the more difficult the chosen algorithm is to implement, the more this is an issue.
    **Recursive feature elimination** (**RFE**) is something of a compromise between
    the simplicity of filter methods and the better information provided by wrapper
    methods. It is similar to backward feature selection, except it simplifies the
    removal of a feature at each iteration by basing it on the model's overall performance
    rather than re-evaluating each feature. We explore recursive feature selection
    in the next two sections.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 前向、后向和全面特征选择等包装方法会消耗系统资源，因为它们需要每次迭代时都进行训练，而选择的算法越难实现，这个问题就越严重。**递归特征消除（RFE**）在过滤方法的简单性和包装方法提供的信息之间是一种折衷。它与后向特征选择类似，但它在每次迭代中通过基于模型的整体性能而不是重新评估每个特征来简化特征的移除。我们将在下一节中探讨递归特征选择。
- en: Eliminating features recursively in a regression model
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在回归模型中递归消除特征
- en: A popular wrapper method is RFE. This method starts with all features, removes
    the lowest weighted one (based on a coefficient or feature importance measure),
    and repeats the process until the best-fitting model has been identified. When
    a feature is removed, it is given a ranking reflecting the point at which it was
    removed.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的包装方法是RFE。这种方法从所有特征开始，移除权重最低的一个（基于系数或特征重要性度量），然后重复此过程，直到确定最佳拟合模型。当移除一个特征时，它会得到一个反映其移除点的排名。
- en: 'RFE can be used for both regression models and classification models. We will
    start by using it in a regression model:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: RFE可以用于回归模型和分类模型。我们将从在回归模型中使用它开始：
- en: 'We import the necessary libraries, three of which we have not used yet: the
    `RFE`, `RandomForestRegressor`, and `LinearRegression` modules from `scikit-learn`:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Next, we load the NLS data on wages and create training and testing DataFrames:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We need to encode the `gender` feature and standardize the other features and
    the target (`wageincome`). We do not do any encoding or scaling of `completedba`,
    which is a binary feature:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now, we are ready to do some recursive feature selection. Since RFE is a wrapper
    method, we need to choose an algorithm around which the selection will be *wrapped*.
    Random forests for regression make sense in this case. We are modeling a continuous
    target and do not want to assume a linear relationship between the features and
    the target.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'RFE is fairly easy to implement with `scikit-learn`. We instantiate an RFE
    object, telling it what estimator we want in the process. We indicate `RandomForestRegressor`.
    We then fit the model and use `get_support` to get the selected features. We limit
    `max_depth` to `2` to avoid overfitting:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note that this gives us a somewhat different list of features than using a filter
    method (with F-tests) for the wage income target. `gpaoverall` and `motherhighgrade`
    are selected here and not the `gender` flag or `gpascience`.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `ranking_` attribute to see when each of the eliminated features
    was removed:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '`fatherhighgrade` was removed after the first interaction and `gpamath` after
    the second.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Let's run some test statistics. We fit only the selected features on a random
    forest regressor model. The `transform` method of the RFE selector gives us just
    the selected features with `treesel.transform(X_train_enc)`. We can use the `score`
    method to get the r-squared value, also known as the coefficient of determination.
    R-squared is a measure of the percentage of total variation explained by our model.
    We get a very low score, indicating that our model explains only a little of the
    variation. (Note that this is a stochastic process, so we will likely get different
    results each time we fit the model.)
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let''s see whether we get any better results using RFE with a linear regression
    model. This model returns the same features as the random forest regressor:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let''s evaluate the linear model:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The linear model is not really much better than the random forest model. This
    is likely a sign that, collectively, the features available to us only capture
    a small part of the variation in wages per week. This is an important reminder
    that we can identify several significant features and still have a model with
    limited explanatory power. (Perhaps it is also good news that our scores on standardized
    tests, and even our degree attainment, are important but not determinative of
    our wages many years later.)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Let's try RFE with a classification model.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Eliminating features recursively in a classification model
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RFE can also be a good choice for classification problems. We can use RFE to
    select features for a model of bachelor''s degree completion. You may recall that
    we used exhaustive feature selection to select features for that model earlier
    in this chapter. Let''s see whether we get better accuracy or an easier-to-train
    model with RFE:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the same libraries we have been working with so far in this chapter:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, we create training and testing data from the NLS educational attainment
    data:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then, we encode and scale the training and testing data:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: We instantiate a random forest classifier and pass it to the RFE selection method.
    We can then fit the model and get the selected features.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We can also show how the features are ranked by using the RFE `ranking_` attribute:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Let''s look at the accuracy of a model with the selected features using the
    same random forest classifier we used for our baseline model:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Recall that we had 67% accuracy with the exhaustive feature selection. We get
    about the same accuracy here. The benefit of RFE though is that it can be significantly
    easier to train than exhaustive feature selection.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Another option among wrapper and wrapper-like feature selection methods is the
    `scikit-learn` ensemble method. We use it with `scikit-learn`'s random forest
    classifier in the next section.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Using Boruta for feature selection
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Boruta package takes a unique approach to feature selection, though it has
    some similarities with wrapper methods. For each feature, Boruta creates a shadow
    feature, one with the same range of values as the original feature but with shuffled
    values. It then evaluates whether the original feature offers more information
    than the shadow feature, gradually removing features providing the least information.
    Boruta outputs confirmed, tentative, and rejected features with each iteration.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the Boruta package to select features for a classification model
    of bachelor''s degree completion (you can install the Boruta package with `pip`
    if you have not yet installed it):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the necessary libraries:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We load the NLS educational attainment data again and create the training and
    test DataFrames:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, we encode and scale the training and test data:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We run Boruta feature selection in much the same way that we ran RFE feature
    selection. We use random forest as our baseline method again. We instantiate a
    random forest classifier and pass it to Boruta''s feature selector. We then fit
    the model, which stops at `100` iterations, identifying `9` features that provide
    information:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'We can use the `ranking_` property to view the rankings of the features:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'To evaluate the model''s accuracy, we fit the random forest classifier model
    with just the selected features. We can then make predictions for the testing
    data and compute accuracy:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Part of Boruta's appeal is the persuasiveness of its selection of each feature.
    If a feature has been selected, then it likely does provide information that is
    not captured by combinations of features that exclude it. However, it is quite
    computationally expensive, not unlike exhaustive feature selection. It can help
    us sort out which features matter, but it may not always be suitable for pipelines
    where training speed matters.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: The last few sections have shown some of the advantages and some disadvantages
    of wrapper feature selection methods. We explore embedded selection methods in
    the next section. These methods provide more information than filter methods but
    without the computational costs of wrapper methods. They do this by embedding
    feature selection into the training process. We will explore embedded methods
    with the same data we have worked with so far.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Using regularization and other embedded methods
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Regularization** methods are embedded methods. Like wrapper methods, embedded
    methods evaluate features relative to a given algorithm. But they are not as expensive
    computationally. That is because feature selection is built into the algorithm
    already and so happens as the model is being trained.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'Embedded models use the following process:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Train a model.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate each feature's importance to the model's predictions.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove features with low importance.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regularization accomplishes this by adding a penalty to any model to constrain
    the parameters. **L1 regularization**, also referred to as **lasso regularization**,
    shrinks some of the coefficients in a regression model to 0, effectively eliminating
    those features.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Using L1 regularization
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use L1 regularization with logistic regression to select features for
    a bachelor''s degree attainment model:We need to first import the required libraries,
    including a module we will be using for the first time, `SelectFromModel` from
    `scikit-learn`:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Next, we load NLS data on educational attainment:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Then, we encode and scale the training and testing data:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Now we are ready to do feature selection based on logistic regression with
    an L1 penalty:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let''s evaluate the accuracy of the model. We get an accuracy score of `0.68`:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: This gives us fairly similar results to that of the forward feature selection
    for bachelor's degree completion. We used a random forest classifier as a wrapper
    method in that example.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regularization is a good choice for feature selection in a case like this,
    particularly when performance is a key concern. It does, however, assume a linear
    relationship between the features and the target, which might not be appropriate.
    Fortunately, there are embedded feature selection methods that do not make that
    assumption. A good alternative to logistic regression for the embedded model is
    a random forest classifier. We try that next with the same data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Using a random forest classifier
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, let''s use a random forest classifier:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `SelectFromModel` to use a random forest classifier rather than
    logistic regression:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: This actually selects very different features from the lasso regression. `satmath`,
    `fatherhighgrade`, and `gender_Female` are no longer selected, while `satverbal`
    and `gpaenglish` are. This is likely partly due to the relaxation of the assumption
    of linearity.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s evaluate the accuracy of the random forest classifier model. We get
    an accuracy score of **0.67**. This is pretty much the same score that we got
    with the lasso regression:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Embedded methods are generally less CPU-/GPU-intensive than wrapper methods
    but can nonetheless produce good results. With our models of bachelor's degree
    completion in this section, we get the same accuracy as we did with our models
    based on exhaustive feature selection.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Each of the methods we have discussed so far has important use cases, as we
    have discussed. However, we have not yet really discussed one very challenging
    feature selection problem. What do you do if you simply have too many features,
    many of which independently account for something important in your model? By
    too many, here I mean that there are so many features that the model cannot run
    efficiently, either for training or for predicting target values. How can we reduce
    the feature set without sacrificing some of the predictive power of our model?
    In that situation, **principal component analysis** (**PCA**) might be a good
    approach. We'll discuss PCA in the next section.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Using principal component analysis
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very different approach to feature selection than any of the methods we have
    discussed so far is PCA. PCA allows us to replace the existing feature set with
    a limited number of components, each of which explains an important amount of
    the variance. It does this by finding a component that captures the largest amount
    of variance, followed by a second component that captures the largest amount of
    remaining variance, and then a third component, and so on. One key advantage of
    this approach is that these components, known as **principal components**, are
    uncorrelated. We discuss PCA in detail in [*Chapter 15*](B17978_15_ePub.xhtml#_idTextAnchor170),
    *Principal Component Analysis*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Although I include PCA here as a feature selection approach, it is probably
    better to think of it as a tool for dimension reduction. We use it for feature
    selection when we need to limit the number of dimensions without sacrificing too
    much explanatory power.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s work with the NLS data again and use PCA to select features for a model
    of bachelor''s degree completion:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the necessary libraries. The only module we have not already
    used in this chapter is `scikit-learn`''s `PCA`:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Next, we create training and testing DataFrames once again:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'We need to scale and encode the data. Scaling is particularly important with
    PCA:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now, we instantiate a `PCA` object and fit a model:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: The `components_` attribute of the `PCA` object returns the scores of all 10
    features on each of the 5 components. The features that drive the first component
    most are those with scores that have the highest absolute value. In this case,
    that is `gpaoverall`, `gpaenglish`, and `gpascience`. For the second component,
    the most important features are `motherhighgrade`, `fatherhighgrade`, and `parentincome`.
    `satverbal` and `satmath` drive the third component.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following output, columns **0** through **4** are the five principal
    components:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Another way to understand these scores is that they indicate how much each feature
    contributes to the component. (Indeed, if for each component, you square each
    of the 10 scores and then sum the squares, you get a total of 1.)
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also examine how much of the variance in the features is explained by
    each component. The first component accounts for 46% of the variance alone, followed
    by an additional 19% for the second component. We can use NumPy''s `cumsum` method
    to see how much of feature variance is explained by the five components cumulatively.
    We can explain 87% of the variance in the 10 features with 5 components:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Let''s transform our features in the testing data based on these five principal
    components. This returns a NumPy array with only the five principal components.
    We look at the first few rows. We also need to transform the testing DataFrame:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: We can now fit a model of bachelor's degree completion using these principal
    components. Let's run a random forest classification.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create a random forest classifier object. We then pass the training
    data with the principal components and the target values to its `fit` method.
    We pass the testing data with the components to the classifier''s `predict` method
    and then get an accuracy score:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: A dimension reduction technique such as PCA can be a good option when the feature
    selection challenge is that we have highly correlated features and we want to
    reduce the number of dimensions without substantially reducing the explained variance.
    In this example, the high school GPA features moved together, as did the parental
    education and income levels and the SAT features. They became the key features
    for our first three components. (An argument can be made that our model could
    have had just those three components since together they accounted for 74% of
    the variance of the features.)
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: There are several modifications to PCA that might be useful depending on your
    data and modeling objectives. This includes strategies to handle outliers and
    regularization. PCA can also be extended to situations where the components are
    not linearly separable by using kernels. We discuss PCA in detail in [*Chapter
    15*](B17978_15_ePub.xhtml#_idTextAnchor170)*,* *Principal Component Analysis*.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Let's summarize what we've learned in this chapter.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went over a range of feature selection methods, from filter
    to wrapper to embedded methods. We also saw how they work with categorical and
    continuous targets. For wrapper and embedded methods, we considered how they work
    with different algorithms.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Filter methods are very easy to run and interpret and are easy on system resources.
    However, they do not take other features into account when evaluating each feature.
    Nor do they tell us how that assessment might vary by the algorithm used. Wrapper
    methods do not have any of these limitations but they are computationally expensive.
    Embedded methods are often a good compromise, selecting features based on multivariate
    relationships and a given algorithm without taxing system resources as much as
    wrapper methods. We also explored how a dimension reduction method, PCA, could
    improve our feature selection.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: You also probably noticed that I slipped in a little bit of model validation
    during this chapter. We will go over model validation in much more detail in the
    next chapter.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
