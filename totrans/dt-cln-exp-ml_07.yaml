- en: '*Chapter 5*: Feature Selection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on how you began your data analytic work and your own intellectual
    interests, you might have a different perspective on the topic of **feature selection**.
    You might think, *yeah, yeah, it is an important topic, but I really want to get
    to the model building*. Or, at the other extreme, you might view feature selection
    as at the core of model building and believe that you are 90% of the way toward
    having your model once you have chosen your features. For now, let's just agree
    that we should spend a good chunk of time understanding the relationships between
    features – and their relationship to a target if we are building a supervised
    model – before we do any serious model specification.
  prefs: []
  type: TYPE_NORMAL
- en: It is helpful to approach our feature selection work with the attitude that
    less is more. If we can reach nearly the same degree of accuracy or explain as
    much of the variance with fewer features, we should select the simpler model.
    Sometimes, we can actually get better accuracy with fewer features. This can be
    hard to wrap our brains around, and even be a tad disappointing for those of us
    who cut our teeth on building models that told rich and complicated stories.
  prefs: []
  type: TYPE_NORMAL
- en: But we are less concerned with parameter estimates than with the accuracy of
    our predictions when fitting machine learning models. Unnecessary features can
    contribute to overfitting and tax hardware resources.
  prefs: []
  type: TYPE_NORMAL
- en: We can sometimes spend months specifying the features of our model, even when
    there is a limited number of columns in the data. Bivariate correlations, such
    as those created in [*Chapter 2*](B17978_02_ePub.xhtml#_idTextAnchor025), *Examining
    Bivariate and Multivariate Relationships between Features and Targets*, give us
    some sense of what to expect, but the importance of a feature can vary significantly
    once other potentially explanatory features are introduced. The feature may no
    longer be significant, or, conversely, may only be significant when other features
    are included. Two features might be so highly correlated that including both of
    them offers very little additional information than including just one.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter takes a close look at feature selection techniques applicable
    to a variety of predictive modeling tasks. Specifically, we will explore the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting features for classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting features for regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using forward and backward feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using exhaustive feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating features recursively in a regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating features recursively in a classification model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Boruta for feature selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using regularization and other embedded methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will work with the `feature_engine`, `mlxtend`, and `boruta` packages in
    this chapter, in addition to the `scikit-learn` library. You can use `pip` to
    install these packages. I have chosen a dataset with a small number of observations
    for our work in this chapter, so the code should work fine even on suboptimal
    workstations.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We will work exclusively in this chapter with data from The National Longitudinal
    Survey of Youth, conducted by the United States Bureau of Labor Statistics. This
    survey started with a cohort of individuals in 1997 who were born between 1980
    and 1985, with annual follow-ups each year through 2017\. We will work with educational
    attainment, household demographic, weeks worked, and wage income data. The wage
    income column represents wages earned in 2016\. The NLS dataset can be downloaded
    for public use at [https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search).
  prefs: []
  type: TYPE_NORMAL
- en: Selecting features for classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most straightforward feature selection methods are based on each feature's
    relationship with a target variable. The next two sections examine techniques
    for determining the *k* best features based on their linear or non-linear relationship
    with the target. These are known as filter methods. They are also sometimes called
    univariate methods since they evaluate the relationship between the feature and
    the target independent of the impact of other features.
  prefs: []
  type: TYPE_NORMAL
- en: We use somewhat different strategies when the target is categorical than when
    it is continuous. We'll go over the former in this section and the latter in the
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Mutual information classification for feature selection with a categorical target
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use **mutual information** classification or **analysis of variance**
    (**ANOVA**) tests to select features when we have a categorical target. We will
    try mutual information classification first, and then ANOVA for comparison.
  prefs: []
  type: TYPE_NORMAL
- en: Mutual information is a measure of how much information about a variable is
    provided by knowing the value of another variable. At the extreme, when features
    are completely independent, the mutual information score is 0.
  prefs: []
  type: TYPE_NORMAL
- en: We can use `scikit-learn`'s `SelectKBest` class to select the *k* features that
    have the highest predictive strength based on mutual information classification
    or some other appropriate measure. We can use hyperparameter tuning to select
    the value of *k*. We can also examine the scores of all features, whether they
    were identified as one of the *k* best or not, as we will see in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first try mutual information classification to identify features that
    are related to completing a bachelor''s degree. Later, we will compare that with
    using ANOVA F-values as the basis for selection:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing `OneHotEncoder` from `feature_engine` to encode some
    of the data, and `train_test_split` from `scikit-learn` to create training and
    testing data. We will also need `scikit-learn`''s `SelectKBest`, `mutual_info_classif`,
    and `f_classif` modules for our feature selection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load NLS data that has a binary variable for having completed a bachelor''s
    degree and features possibly related to degree attainment: `gender` feature, and
    scale the other data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will do a complete case analysis of the NLS data throughout this chapter;
    that is, we will remove all observations that have missing values for any of the
    features. This is not usually a good approach and is particularly problematic
    when data is not missing at random or when there is a large number of missing
    values for one or more features. In such cases, it would be better to use some
    of the approaches that we used in [*Chapter 3*](B17978_03_ePub.xhtml#_idTextAnchor034),
    *Identifying and Fixing Missing Values*. We will do a complete case analysis in
    this chapter to keep the examples as straightforward as possible.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we are ready to select features for our model of bachelor''s degree completion.
    One approach is to use mutual information classification. To do that, we set the
    `score_func` value of `SelectKBest` to `mutual_info_classif` and indicate that
    we want the five best features. Then, we call `fit` and use the `get_support`
    method to get the five best features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we also want to see the score for each feature, we can use the `scores_`
    attribute, though we need to do a little work to associate the scores with a particular
    feature name and sort the scores in descending order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This is a stochastic process, so we will get different results each time we
    run it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To get the same results each time, you can pass a partial function to `score_func`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can create a DataFrame with just the important features using the `selcols`
    array we created using `get_support`. (We could have used the `transform` method
    of `SelectKBest` instead. This would have returned the values of the selected
    features as a NumPy array.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That is all we need to do to select the *k* best features for our model using
    mutual information.
  prefs: []
  type: TYPE_NORMAL
- en: ANOVA F-value for feature selection with a categorical target
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alternatively, we can use ANOVA instead of mutual information. ANOVA evaluates
    how different the mean for a feature is for each target class. This is a good
    metric for univariate feature selection when we can assume a linear relationship
    between features and the target and our features are normally distributed. If
    those assumptions do not hold, mutual information classification is a better choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try using ANOVA for our feature selection. We can set the `score_func`
    parameter of `SelectKBest` to `f_classif` to select based on ANOVA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This selected the same features as were selected when we used mutual information.
    Showing the scores gives us some indication of whether the selected value for
    *k* makes sense. For example, there is a greater drop in score from the fifth-
    to the sixth-best feature (77-61) than from the fourth to the fifth (85-77). There
    is an even bigger decline from the sixth to the seventh, however (61-37), suggesting
    that we should at least consider a value for *k* of 6\.
  prefs: []
  type: TYPE_NORMAL
- en: ANOVA tests, and the mutual information classification we did earlier, do not
    take into account features that are only important in multivariate analysis. For
    example, `fatherhighgrade` might matter among individuals with similar GPA or
    SAT scores. We use multivariate feature selection methods later in this chapter.
    We do more univariate feature selection in the next section where we explore selection
    techniques appropriate for continuous targets.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting features for regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`scikit-learn`''s selection module provides several options for selecting features
    when building regression models. (By regression models here, I do not mean linear
    regression models. I am only referring to *models with continuous targets*.) Two
    good options are selection based on F-tests and selection based on mutual information
    for regression. Let''s start with F-tests.'
  prefs: []
  type: TYPE_NORMAL
- en: F-tests for feature selection with a continuous target
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The F-statistic is a measure of the strength of the linear correlation between
    a target and a single regressor. `Scikit-learn` has an `f_regression` scoring
    function, which returns F-statistics. We can use it with `SelectKBest` to select
    features based on that statistic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use F-statistics to select features for a model of wages. We use mutual
    information for regression in the next section to select features for the same
    target:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the one-hot encoder from `feature_engine` and `train_test_split`
    and `SelectKBest` from `scikit-learn`. We also import `f_regression` to get F-statistics
    later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load the NLS data, including educational attainment, parental income,
    and wage income data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create training and testing DataFrames, encode the `gender` feature,
    and scale the training data. We need to scale the target in this case since it
    is continuous:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You may have noticed that we are not encoding or scaling the testing data. We
    will need to do that eventually to validate our models. We will introduce validation
    later in this chapter and go over it in much more detail in the next chapter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we are ready to select features. We set `score_func` of `SelectKBest`
    to `f_regression` and indicate that we want the five best features. The `get_support`
    method of `SelectKBest` returns `True` for each feature that was selected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the `scores_` attribute to see the score for each feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The disadvantage of the F-statistic is that it assumes a linear relationship
    between each feature and the target. When that assumption does not make sense,
    we can use mutual information for regression instead.
  prefs: []
  type: TYPE_NORMAL
- en: Mutual information for feature selection with a continuous target
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also use `SelectKBest` to select features using mutual information for
    regression:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to set the `score_func` parameter of `SelectKBest` to `mutual_info_regression`,
    but there is a small complication. To get the same results each time we run the
    feature selection, we need to set a `random_state` value. As we discussed in the
    previous section, we can use a partial function for that. We pass `partial(mutual_info_regression,
    random_state=0)` to the score function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can then run the `fit` method and use `get_support` to get the selected
    features. We can use the `scores_` attribute to give us the score for each feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We get fairly similar results with mutual information for regression as we did
    with F-tests. `parentincome` was selected with F-tests and `fatherhighgrade` with
    mutual information. Otherwise, the same features are selected.
  prefs: []
  type: TYPE_NORMAL
- en: A key advantage of mutual information for regression compared with F-tests is
    that it does not assume a linear relationship between the feature and the target.
    If that assumption turns out to be unwarranted, mutual information is a better
    approach. (Again, there is also some randomness in the scoring and the score for
    each feature can bounce around within a limited range.)
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Our choice of `k=5` to get the five best features is quite arbitrary. We can
    make it much more scientific with some hyperparameter tuning. We will go over
    tuning in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The feature selection methods we have used so far are known as *filter methods*.
    They examine the univariate relationship between each feature and the target.
    They are a good starting point. Similar to our discussion in previous chapters
    of the usefulness of having correlations handy before we start examining multivariate
    relationships, it is helpful to at least explore filter methods. Often, though,
    our model fitting will require taking into account features that are important,
    or not, when other features are also included. To do that, we need to use wrapper
    or embedded methods for feature selection. We explore wrapper methods in the next
    few sections, starting with forward and backward feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Using forward and backward feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Forward and backward feature selection, as their names suggest, select features
    by adding them one by one – or subtracting them for backward selection – and assessing
    the impact on model performance after each iteration. Since both methods assess
    that performance based on a given algorithm, they are considered **wrapper** selection
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper feature selection methods have two advantages over the filter methods
    we have explored so far. First, they evaluate the importance of features as other
    features are included. Second, since features are evaluated based on their contribution
    to the performance of a specific algorithm, we get a better sense of which features
    will ultimately matter. For example, `satmath` seemed to be an important feature
    based on our results from the previous section. But it is possible that `satmath`
    is only important when we use a particular model, say linear regression, and not
    an alternative such as decision tree regression. Wrapper selection methods can
    help us discover that.
  prefs: []
  type: TYPE_NORMAL
- en: The main disadvantage of wrapper methods is that they can be quite expensive
    computationally since they retrain the model after each iteration. We will look
    at both forward and backward feature selection in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Using forward feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Forward feature selection** starts by identifying a subset of features that
    individually have a significant relationship with a target, not unlike the filter
    methods. But it then evaluates all possible combinations of the selected features
    for the combination that performs best with the chosen algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use forward feature selection to develop a model of bachelor''s degree
    completion. Since wrapper methods require us to choose an algorithm, and this
    is a binary target, let''s use `scikit-learn`''s `feature_selection` module of
    `mlxtend` to do the iteration required to select features:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we load the NLS data again. We also create a training DataFrame, encode
    the `gender` feature, and standardize the remaining features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a random forest classifier object and then pass that object to the
    feature selector of `mlxtend`. We indicate that we want it to select five features
    and that it should forward select. (We can also use the sequential feature selector
    to select backward.) After running `fit`, we can use the `k_feature_idx_` attribute
    to get the list of selected features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You might recall from the first section of this chapter that our univariate
    feature selection for the completed bachelor''s degree target gave us somewhat
    different results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Three of the features – `satmath`, `satverbal`, and `gpaoverall` – are the same.
    But our forward feature selection has identified `parentincome` and `gender_Female`
    as more important than `gpascience` and `gpaenglish`, which were selected in the
    univariate analysis. Indeed, `gender_Female` had among the lowest scores in the
    earlier analysis. These differences likely reflect the advantages of wrapper feature
    selection methods. We can identify features that are not important unless other
    features are included, and we are evaluating the effect on the performance of
    a particular algorithm, in this case, random forest classification.
  prefs: []
  type: TYPE_NORMAL
- en: One disadvantage of forward selection is that *once a feature is selected, it
    is not removed, even though it may decline in importance as additional features
    are added*. (Recall that forward feature selection adds features iteratively based
    on the contribution of that feature to the model.)
  prefs: []
  type: TYPE_NORMAL
- en: Let's see whether our results vary with backward feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Using backward feature selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Backward feature selection starts with all features and eliminates the least
    important. It then repeats this process with the remaining features. We can use
    `mlxtend`'s `SequentialFeatureSelector` for backward selection in pretty much
    the same way we used it for forward selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'We instantiate a `RandomForestClassifier` object from the `scikit-learn` library
    and then pass it to `mlxtend`''s sequential feature selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Perhaps unsurprisingly, we get different results for our feature selection.
    `satmath` and `parentincome` are no longer selected, and `gpascience` and `gpaenglish`
    are.
  prefs: []
  type: TYPE_NORMAL
- en: Backward feature selection has the opposite drawback to forward feature selection.
    *Once a feature has been removed, it is not re-evaluated, even though its importance
    may change with different feature mixtures*. Let's try exhaustive feature selection
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: Using exhaustive feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If your results from forward and backward selection are unpersuasive, and you
    do not mind running a model while you go out for coffee or lunch, you can try
    exhaustive feature selection. **Exhaustive feature selection** trains a given
    model on all possible combinations of features and selects the best subset of
    features. But it does this at a price. As the name suggests, this procedure might
    exhaust both system resources and your patience.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use exhaustive feature selection for our model of bachelor''s degree
    completion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the required libraries, including the `RandomForestClassifier`
    and `LogisticRegression` modules from `scikit-learn` and `ExhaustiveFeatureSelector`
    from `mlxtend`. We also import the `accuracy_score` module so that we can evaluate
    a model with the selected features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load the NLS educational attainment data and create training and testing
    DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we encode and scale the training and testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We create a random forest classifier object and pass it to `mlxtend`''s `ExhaustiveFeatureSelector`.
    We tell the feature selector to evaluate all combinations of one to five features
    and return the combination with the highest accuracy in predicting degree attainment.
    After running `fit`, we can use the `best_feature_names_` attribute to get the
    selected features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s evaluate the accuracy of this model. We first need to transform the
    training and testing data to include only the four selected features. Then, we
    can fit the random forest classifier again with just those features and generate
    the predicted values for bachelor''s degree completion. We can then calculate
    the percentage of the time we predicted the target correctly, which is 67%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We get the same answer if we just use scikit-learn's `accuracy score` instead.
    (We calculate it in the previous step because it is pretty straightforward and
    it gives us a better sense of what is meant by accuracy in this case.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The accuracy score is often used to assess the performance of a classification
    model. We will lean on it in this chapter, but other measures might be equally
    or more important depending on the purposes of your model. For example, we are
    sometimes more concerned with sensitivity, the ratio of our correct positive predictions
    to the number of actual positives. We examine the evaluation of classification
    models in detail in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing
    for Model Evaluation.*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now try exhaustive feature selection with a logistic model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s look at the accuracy of the logistic model. We get a fairly similar
    accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: One key advantage of the logistic model is that it is much faster to train,
    which really makes a difference with exhaustive feature selection. If we time
    the training for each model (probably not a good idea to do that on your computer
    unless it's a pretty high-end machine or you don't mind walking away from your
    computer for a while), we see a substantial difference in average training time
    – from an amazing 5 minutes for the random forest to 4 seconds for the logistic
    regression. (Of course, the absolute numbers are machine-dependent.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Exhaustive feature selection can provide very clear guidance about the features
    to select, as I have mentioned, but that may come at too high a price for many
    projects. It may actually be better suited for *diagnostic work* than for use
    in a machine learning pipeline. If a linear model is appropriate, it can lower
    the computational costs considerably.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapper methods, such as forward, backward, and exhaustive feature selection,
    tax system resources because they need to be trained with each iteration, and
    the more difficult the chosen algorithm is to implement, the more this is an issue.
    **Recursive feature elimination** (**RFE**) is something of a compromise between
    the simplicity of filter methods and the better information provided by wrapper
    methods. It is similar to backward feature selection, except it simplifies the
    removal of a feature at each iteration by basing it on the model's overall performance
    rather than re-evaluating each feature. We explore recursive feature selection
    in the next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: Eliminating features recursively in a regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A popular wrapper method is RFE. This method starts with all features, removes
    the lowest weighted one (based on a coefficient or feature importance measure),
    and repeats the process until the best-fitting model has been identified. When
    a feature is removed, it is given a ranking reflecting the point at which it was
    removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'RFE can be used for both regression models and classification models. We will
    start by using it in a regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the necessary libraries, three of which we have not used yet: the
    `RFE`, `RandomForestRegressor`, and `LinearRegression` modules from `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load the NLS data on wages and create training and testing DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to encode the `gender` feature and standardize the other features and
    the target (`wageincome`). We do not do any encoding or scaling of `completedba`,
    which is a binary feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we are ready to do some recursive feature selection. Since RFE is a wrapper
    method, we need to choose an algorithm around which the selection will be *wrapped*.
    Random forests for regression make sense in this case. We are modeling a continuous
    target and do not want to assume a linear relationship between the features and
    the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'RFE is fairly easy to implement with `scikit-learn`. We instantiate an RFE
    object, telling it what estimator we want in the process. We indicate `RandomForestRegressor`.
    We then fit the model and use `get_support` to get the selected features. We limit
    `max_depth` to `2` to avoid overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that this gives us a somewhat different list of features than using a filter
    method (with F-tests) for the wage income target. `gpaoverall` and `motherhighgrade`
    are selected here and not the `gender` flag or `gpascience`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `ranking_` attribute to see when each of the eliminated features
    was removed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`fatherhighgrade` was removed after the first interaction and `gpamath` after
    the second.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's run some test statistics. We fit only the selected features on a random
    forest regressor model. The `transform` method of the RFE selector gives us just
    the selected features with `treesel.transform(X_train_enc)`. We can use the `score`
    method to get the r-squared value, also known as the coefficient of determination.
    R-squared is a measure of the percentage of total variation explained by our model.
    We get a very low score, indicating that our model explains only a little of the
    variation. (Note that this is a stochastic process, so we will likely get different
    results each time we fit the model.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s see whether we get any better results using RFE with a linear regression
    model. This model returns the same features as the random forest regressor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s evaluate the linear model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The linear model is not really much better than the random forest model. This
    is likely a sign that, collectively, the features available to us only capture
    a small part of the variation in wages per week. This is an important reminder
    that we can identify several significant features and still have a model with
    limited explanatory power. (Perhaps it is also good news that our scores on standardized
    tests, and even our degree attainment, are important but not determinative of
    our wages many years later.)
  prefs: []
  type: TYPE_NORMAL
- en: Let's try RFE with a classification model.
  prefs: []
  type: TYPE_NORMAL
- en: Eliminating features recursively in a classification model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RFE can also be a good choice for classification problems. We can use RFE to
    select features for a model of bachelor''s degree completion. You may recall that
    we used exhaustive feature selection to select features for that model earlier
    in this chapter. Let''s see whether we get better accuracy or an easier-to-train
    model with RFE:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the same libraries we have been working with so far in this chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create training and testing data from the NLS educational attainment
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we encode and scale the training and testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We instantiate a random forest classifier and pass it to the RFE selection method.
    We can then fit the model and get the selected features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also show how the features are ranked by using the RFE `ranking_` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s look at the accuracy of a model with the selected features using the
    same random forest classifier we used for our baseline model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Recall that we had 67% accuracy with the exhaustive feature selection. We get
    about the same accuracy here. The benefit of RFE though is that it can be significantly
    easier to train than exhaustive feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Another option among wrapper and wrapper-like feature selection methods is the
    `scikit-learn` ensemble method. We use it with `scikit-learn`'s random forest
    classifier in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using Boruta for feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Boruta package takes a unique approach to feature selection, though it has
    some similarities with wrapper methods. For each feature, Boruta creates a shadow
    feature, one with the same range of values as the original feature but with shuffled
    values. It then evaluates whether the original feature offers more information
    than the shadow feature, gradually removing features providing the least information.
    Boruta outputs confirmed, tentative, and rejected features with each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the Boruta package to select features for a classification model
    of bachelor''s degree completion (you can install the Boruta package with `pip`
    if you have not yet installed it):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the NLS educational attainment data again and create the training and
    test DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we encode and scale the training and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We run Boruta feature selection in much the same way that we ran RFE feature
    selection. We use random forest as our baseline method again. We instantiate a
    random forest classifier and pass it to Boruta''s feature selector. We then fit
    the model, which stops at `100` iterations, identifying `9` features that provide
    information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the `ranking_` property to view the rankings of the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To evaluate the model''s accuracy, we fit the random forest classifier model
    with just the selected features. We can then make predictions for the testing
    data and compute accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Part of Boruta's appeal is the persuasiveness of its selection of each feature.
    If a feature has been selected, then it likely does provide information that is
    not captured by combinations of features that exclude it. However, it is quite
    computationally expensive, not unlike exhaustive feature selection. It can help
    us sort out which features matter, but it may not always be suitable for pipelines
    where training speed matters.
  prefs: []
  type: TYPE_NORMAL
- en: The last few sections have shown some of the advantages and some disadvantages
    of wrapper feature selection methods. We explore embedded selection methods in
    the next section. These methods provide more information than filter methods but
    without the computational costs of wrapper methods. They do this by embedding
    feature selection into the training process. We will explore embedded methods
    with the same data we have worked with so far.
  prefs: []
  type: TYPE_NORMAL
- en: Using regularization and other embedded methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Regularization** methods are embedded methods. Like wrapper methods, embedded
    methods evaluate features relative to a given algorithm. But they are not as expensive
    computationally. That is because feature selection is built into the algorithm
    already and so happens as the model is being trained.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Embedded models use the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: Train a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate each feature's importance to the model's predictions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove features with low importance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regularization accomplishes this by adding a penalty to any model to constrain
    the parameters. **L1 regularization**, also referred to as **lasso regularization**,
    shrinks some of the coefficients in a regression model to 0, effectively eliminating
    those features.
  prefs: []
  type: TYPE_NORMAL
- en: Using L1 regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use L1 regularization with logistic regression to select features for
    a bachelor''s degree attainment model:We need to first import the required libraries,
    including a module we will be using for the first time, `SelectFromModel` from
    `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load NLS data on educational attainment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we encode and scale the training and testing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we are ready to do feature selection based on logistic regression with
    an L1 penalty:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s evaluate the accuracy of the model. We get an accuracy score of `0.68`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This gives us fairly similar results to that of the forward feature selection
    for bachelor's degree completion. We used a random forest classifier as a wrapper
    method in that example.
  prefs: []
  type: TYPE_NORMAL
- en: Lasso regularization is a good choice for feature selection in a case like this,
    particularly when performance is a key concern. It does, however, assume a linear
    relationship between the features and the target, which might not be appropriate.
    Fortunately, there are embedded feature selection methods that do not make that
    assumption. A good alternative to logistic regression for the embedded model is
    a random forest classifier. We try that next with the same data.
  prefs: []
  type: TYPE_NORMAL
- en: Using a random forest classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, let''s use a random forest classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `SelectFromModel` to use a random forest classifier rather than
    logistic regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This actually selects very different features from the lasso regression. `satmath`,
    `fatherhighgrade`, and `gender_Female` are no longer selected, while `satverbal`
    and `gpaenglish` are. This is likely partly due to the relaxation of the assumption
    of linearity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s evaluate the accuracy of the random forest classifier model. We get
    an accuracy score of **0.67**. This is pretty much the same score that we got
    with the lasso regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Embedded methods are generally less CPU-/GPU-intensive than wrapper methods
    but can nonetheless produce good results. With our models of bachelor's degree
    completion in this section, we get the same accuracy as we did with our models
    based on exhaustive feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the methods we have discussed so far has important use cases, as we
    have discussed. However, we have not yet really discussed one very challenging
    feature selection problem. What do you do if you simply have too many features,
    many of which independently account for something important in your model? By
    too many, here I mean that there are so many features that the model cannot run
    efficiently, either for training or for predicting target values. How can we reduce
    the feature set without sacrificing some of the predictive power of our model?
    In that situation, **principal component analysis** (**PCA**) might be a good
    approach. We'll discuss PCA in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very different approach to feature selection than any of the methods we have
    discussed so far is PCA. PCA allows us to replace the existing feature set with
    a limited number of components, each of which explains an important amount of
    the variance. It does this by finding a component that captures the largest amount
    of variance, followed by a second component that captures the largest amount of
    remaining variance, and then a third component, and so on. One key advantage of
    this approach is that these components, known as **principal components**, are
    uncorrelated. We discuss PCA in detail in [*Chapter 15*](B17978_15_ePub.xhtml#_idTextAnchor170),
    *Principal Component Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: Although I include PCA here as a feature selection approach, it is probably
    better to think of it as a tool for dimension reduction. We use it for feature
    selection when we need to limit the number of dimensions without sacrificing too
    much explanatory power.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s work with the NLS data again and use PCA to select features for a model
    of bachelor''s degree completion:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the necessary libraries. The only module we have not already
    used in this chapter is `scikit-learn`''s `PCA`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create training and testing DataFrames once again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to scale and encode the data. Scaling is particularly important with
    PCA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we instantiate a `PCA` object and fit a model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `components_` attribute of the `PCA` object returns the scores of all 10
    features on each of the 5 components. The features that drive the first component
    most are those with scores that have the highest absolute value. In this case,
    that is `gpaoverall`, `gpaenglish`, and `gpascience`. For the second component,
    the most important features are `motherhighgrade`, `fatherhighgrade`, and `parentincome`.
    `satverbal` and `satmath` drive the third component.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following output, columns **0** through **4** are the five principal
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Another way to understand these scores is that they indicate how much each feature
    contributes to the component. (Indeed, if for each component, you square each
    of the 10 scores and then sum the squares, you get a total of 1.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s also examine how much of the variance in the features is explained by
    each component. The first component accounts for 46% of the variance alone, followed
    by an additional 19% for the second component. We can use NumPy''s `cumsum` method
    to see how much of feature variance is explained by the five components cumulatively.
    We can explain 87% of the variance in the 10 features with 5 components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s transform our features in the testing data based on these five principal
    components. This returns a NumPy array with only the five principal components.
    We look at the first few rows. We also need to transform the testing DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can now fit a model of bachelor's degree completion using these principal
    components. Let's run a random forest classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create a random forest classifier object. We then pass the training
    data with the principal components and the target values to its `fit` method.
    We pass the testing data with the components to the classifier''s `predict` method
    and then get an accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A dimension reduction technique such as PCA can be a good option when the feature
    selection challenge is that we have highly correlated features and we want to
    reduce the number of dimensions without substantially reducing the explained variance.
    In this example, the high school GPA features moved together, as did the parental
    education and income levels and the SAT features. They became the key features
    for our first three components. (An argument can be made that our model could
    have had just those three components since together they accounted for 74% of
    the variance of the features.)
  prefs: []
  type: TYPE_NORMAL
- en: There are several modifications to PCA that might be useful depending on your
    data and modeling objectives. This includes strategies to handle outliers and
    regularization. PCA can also be extended to situations where the components are
    not linearly separable by using kernels. We discuss PCA in detail in [*Chapter
    15*](B17978_15_ePub.xhtml#_idTextAnchor170)*,* *Principal Component Analysis*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's summarize what we've learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went over a range of feature selection methods, from filter
    to wrapper to embedded methods. We also saw how they work with categorical and
    continuous targets. For wrapper and embedded methods, we considered how they work
    with different algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Filter methods are very easy to run and interpret and are easy on system resources.
    However, they do not take other features into account when evaluating each feature.
    Nor do they tell us how that assessment might vary by the algorithm used. Wrapper
    methods do not have any of these limitations but they are computationally expensive.
    Embedded methods are often a good compromise, selecting features based on multivariate
    relationships and a given algorithm without taxing system resources as much as
    wrapper methods. We also explored how a dimension reduction method, PCA, could
    improve our feature selection.
  prefs: []
  type: TYPE_NORMAL
- en: You also probably noticed that I slipped in a little bit of model validation
    during this chapter. We will go over model validation in much more detail in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
