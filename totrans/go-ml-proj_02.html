<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear Regression - House Price Prediction</h1>
                </header>
            
            <article>
                
<p>Linear regression is one of the world's oldest machine learning concepts. Invented in the early nineteenth century, it is still one of the more vulnerable methods of understanding the relationship between input and output.</p>
<p>The ideas behind linear regression is familiar to us all. We feel that some things are correlated with one another. Sometimes they are causal in nature. There exists a very fine line between correlation and causation. For example, summer sees more sales in ice creams and cold beverages, while winter sees more sales in hot cocoa and coffee. We could say that the seasons themselves cause the amount of sales—they're causal in nature. But are they really?</p>
<p>Without further analysis, the best thing we can say is that they are correlated with one another. The phenomenon of summer is connected to the phenomenon of greater-than the-rest-of-the-year sales of cold drinks and ice cream. The phenomenon of winter is connected, somehow, to the phenomenon of greater-than-the-rest-of-the-year sales of hot beverages.</p>
<p>Understanding the relationship between things is what linear regression, at its core, is all about. There can be many lenses through which linear regression may be viewed, but we will be viewing it through a machine learning lens. That is to say, we wish to build a machine learning model that will accurately predict the results, given some input.</p>
<p>The desire to use correlation for predictive purposes was indeed the very reason why linear regression was invented in the first place. Francis Galton, who was coincidentally Charles Darwin's cousin, hailed from an upper-class family whose lineage included doctors. He had given up his medical studies after a nervous breakdown and began travelling the world as a geologist—this was back when being a geologist was the coolest job (much like being a data scientist today)—however, it was said that Galton hadn't the mettle of Darwin, and soon he gave up the idea of travelling around the world, soured by experiences in Africa. Having inherited his wealth after his father died, Galton dabbled in all things that tickled his fancy, including biology.</p>
<p>The publication of his cousin's magnum opus, <em>On the Origin of Species</em>, made Galton double down on his pursuits in biology and ultimately, eugenics. Galton experimented, rather coincidentally in the same manner as Mendel, on peas. He had wanted to predict the characteristics of the offspring plants, when only information about the parent plants' characteristics were available. He realized that the offspring was often somewhere in between the characteristics of the parent plants. When Galton realized that he could derive a mathematical equation that represented inheritance using elliptical curve fitting, he invented regression.</p>
<p>The reasoning behind regression was simple: there was a driving force—a signal of sorts—that led the characteristics of the offspring plants to go towards the curve he had fitted. If that was the case, it meant that the driving force obeyed some mathematical law. And if it did obey the mathematical laws, then it could be used for prediction, Galton reasoned. To further refine his ideas, he sought the help of the mathematician Karl Pearson.</p>
<p>It took Galton and Pearson a few more attempts to refine the concept and quantify the trends. But ultimately they adopted a least-squares methodology for fitting the curves. </p>
<p>Even to this day, when linear regression is mentioned, it can be safely assumed that a least- squares model will be used, which is precisely what we will be doing.</p>
<p>We will be performing exploratory data analysis—this will allow us to understand the data better. Along the way, we will build and use the data structures necessary for a machine learning project. We will rely heavily on Gonum's plotting libraries for that. After that, we will run a linear regression, interpret the results, and identify the strengths and weaknesses of this technique of machine learning.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The project</h1>
                </header>
            
            <article>
                
<p>What we want to do is to create a model of house prices. We will be using <span class="Link">this open source dataset of house prices</span> <span class="Link">(<a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data">https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data</a>)</span> for our linear regression model. Specifically, the dataset is the data of price of houses that have been sold in the Ames area in Massachusetts, and their associated features.</p>
<p>As with any machine learning project, we start by asking the most basic of questions: what do we want to predict? In this case, I've already indicated that we're going to be predicting house prices, therefore all the other data will be used as signals to predict house prices. In statistical parlance, we call house prices the dependent variable and the other fields the independent variables.</p>
<p>In the following sections, we will build a graph of dependent logical conditions, then with that as a plan, write a program that finds a linear regression model.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Exploratory data analysis</h1>
                </header>
            
            <article>
                
<p>Exploratory data analysis is part and parcel of any model-building process. Understanding the algorithm at play, too, is important. Given that this chapter revolves around linear regression, it might be worth it to explore the data through the lens of understanding linear regression.</p>
<p>But first, let's look at the data. One of the first things I recommend any budding data scientist keen on machine learning to do is to explore the data, or a subset of it, to get a feel for it. I usually do it in a spreadsheet application such as Excel or Google Sheets. I then try to understand, in human ways, the meaning of the data.</p>
<p>This dataset comes with a description of fields, which I can't enumerate in full here. A snapshot, however, would be illuminating for the rest of the discussion in this chapter:</p>
<ul>
<li><kbd>SalePrice</kbd>: The property's sale price in dollars. This is the dependent variable that we're trying to predict.</li>
<li><kbd>MSSubClass</kbd>: The building class.</li>
<li><kbd>MSZoning</kbd>: The general zoning classification.</li>
<li><kbd>LotFrontage</kbd>: The linear feet of the street connected to the property.</li>
<li><kbd>LotArea</kbd>: The lot size in square feet.</li>
</ul>
<p>There can be multiple ways of understanding linear regression. However, one of my favorite ways of understanding linear regression directly ties into exploratory data analysis. Specifically, we're interested in looking at linear regression through the lens of the <strong>conditional</strong> <strong>expectation</strong> <strong>functions</strong> (<strong>CEFs</strong>) of the independent variable.</p>
<p>The conditional expectation function of a variable is simply the expected value of the variable, dependent upon the value of another variable. This seems like a rather dense subject to get through, so I shall offer three different views of the same topic in an attempt to clarify:</p>
<ul>
<li><strong>Statistical point of view</strong>: The conditional expectation function of a dependent variable <img class="fm-editor-equation" src="Images/2cf1f34c-e741-4f73-8546-3259edb2f161.png" style="width:1.08em;height:1.17em;" width="140" height="150"/>given a vector of covariates <img class="fm-editor-equation" src="Images/6fae8631-f622-418d-afbd-1e84858f95dd.png" style="width:1.25em;height:1.42em;" width="160" height="180"/>is simply the expected value of <img class="fm-editor-equation" src="Images/3e4b7cfd-0811-4773-80a8-2a44003be7cd.png" style="width:1.00em;height:1.08em;" width="140" height="150"/>(the average) when <img class="fm-editor-equation" src="Images/f10daf27-0d56-46bf-89d0-354502d9dc2f.png" style="width:1.17em;height:1.17em;" width="160" height="160"/>is fixed to <img class="fm-editor-equation" src="Images/e26b71db-9cd5-4c99-b437-528a9b9ef5e6.png" style="width:1.67em;height:1.33em;" width="220" height="180"/>.</li>
<li><strong>Programming point of view in pseudo-SQL</strong>: <kbd><span class="VerbatimChar">select avg(Y) from dataset where X = 'Xi'</span></kbd>. When conditioning upon multiple conditions, it's simply this: <kbd>select avg(Y) from dataset where X1 = 'Xik' and X2 = 'Xjl'</kbd>.</li>
<li><strong>Concrete example</strong>: What are the expected house prices if one of the independent variables—say, MSZoning—is RL? The expected house price is the population average, which translates to: of all the houses in Boston, what is the average price of house sold whose zoning type is RL?</li>
</ul>
<p>As it stands, this is a pretty bastardized version of what the CEF is—there are some subtleties involved in the definition of the CEF, but that is not within the scope of this book, so we shall leave that for later. For now, this rough understanding of CEF is enough to get us started with our exploratory data analysis.</p>
<p>The programming point of view in pseudo-SQL is useful because it informs us about what we would need so that we can quickly calculate the aggregate of data. We would need to create indices. Because our dataset is small, we can be relatively blasé about the data structures used to index the data.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Ingestion and indexing</h1>
                </header>
            
            <article>
                
<p>Perhaps the best way to index the data is to do it at the time of ingestion. We will use the <kbd><span class="VerbatimChar">encoding/csv</span></kbd> package found in the <kbd>Go standard</kbd> library to ingest the data and build the index.</p>
<p>Before we dive into the code, let's look at the notion of an index, and how one might be built. While indexes are extremely commonly used in databases, they are applicable in any production system as well. The purpose of the index is to allow us to access data quickly.</p>
<p>We want to build an index that will allow us to know at any time which row(s) has the value. In systems with much larger datasets, a more complicated index structure (such as a B-Tree) might be used. In the case of this dataset, however, a map-based index would be more than sufficient.</p>
<p>This is what our index looks like: <kbd><span class="VerbatimChar">[]map[string][]int</span></kbd>—it's a slice of maps. The first slice is indexed by the columns—meaning if we want column <kbd>0</kbd>, we simply get <kbd><span class="VerbatimChar">index[0]</span></kbd>, and get <kbd><span class="VerbatimChar">map[string][]int</span></kbd> in return. The map tells us what values are in the columns (the key of the map), and what rows contain those values (the value of the map).</p>
<p>Now, the question turns to: how do you know which variables associate with which column? A more traditional answer would be to have something like <kbd><span class="VerbatimChar">map[string]int</span></kbd>, where the key represents the variable name and the value represents the column number. While that is a valid strategy, I prefer to have <kbd><span class="VerbatimChar">[]string</span></kbd> as the associative map between the index and column name. Searching is <span class="VerbatimChar">O(N)</span>, but for the most part, if you have named variables, <span class="VerbatimChar">N</span> is small. In future chapters, we shall see much much larger <span class="VerbatimChar">N</span>s.</p>
<p>So, we return the index of column names as <kbd>[]string</kbd> or, in the case of reading <kbd>CSVs</kbd>, it's simply the first row, as shown in the following code snippet:</p>
<pre>// ingest is a function that ingests the file and outputs the header, data, and index.<br/>func ingest(f io.Reader) (header []string, data [][]string, indices []map[string][]int, err error) {<br/>  r := csv.NewReader(f)<br/><br/>  // handle header<br/>  if header, err = r.Read(); err != nil {<br/>    return<br/>  }<br/><br/>  indices = make([]map[string][]int, len(header))<br/>  var rowCount, colCount int = 0, len(header)<br/>  for rec, err := r.Read(); err == nil; rec, err = r.Read() {<br/>    if len(rec) != colCount {<br/>      return nil, nil, nil, errors.Errorf("Expected Columns: %d. Got %d columns in row %d", colCount, len(rec), rowCount)<br/>    }<br/>    data = append(data, rec)<br/>    for j, val := range rec {<br/>      if indices[j] == nil {<br/>        indices[j] = make(map[string][]int)<br/>      }<br/>      indices[j][val] = append(indices[j][val], rowCount)<br/>    }<br/>    rowCount++<br/>  }<br/>  return<br/>}</pre>
<p>Reading this code snippet, a good programmer would have alarm bells going off in their head. Why is everything a string? The answer to that is quite simple: we'll convert the types later. All we need right now is some basic count-based statistics for exploratory data analysis.</p>
<p>The key is in the <span class="VerbatimChar">indexes</span> that are returned by the function. What we have is a column count of unique values. This is how to count them:</p>
<pre>// cardinality counts the number of unique values in a column. <br/>// This assumes that the index i of indices represents a column.<br/>func cardinality(indices []map[string][]int) []int {<br/>  retVal := make([]int, len(indices))<br/>  for i, m := range indices {<br/>    retVal[i] = len(m)<br/>  }<br/>  return retVal<br/>}</pre>
<p>With this, we can then analyze the cardinality of each individual column—that is how many distinct values there are. If there are as many distinct values as there are rows in each column, then we can be quite sure that the column is not categorical. Or, if we know that the column is categorical, and there are as many distinct values as there are rows, then we know for sure that the column cannot be used in a linear regression.</p>
<p>Our main function now looks like this:</p>
<pre>func main() {<br/>  f, err := os.Open("train.csv")<br/>  mHandleErr(err)<br/>  hdr, data, indices, err := ingest(f)<br/>  mHandleErr(err)<br/>  c := cardinality(indices)<br/><br/>  fmt.Printf("Original Data: \nRows: %d, Cols: %d\n========\n", len(data), len(hdr))<br/>  c := cardinality(indices)<br/>  for i, h := range hdr {<br/>    fmt.Printf("%v: %v\n", h, c[i])<br/>  }<br/>  fmt.Println("")<br/><br/>}</pre>
<p>For completeness, this is the definition of <kbd><span class="VerbatimChar">mHandleError</span></kbd>:</p>
<pre>// mHandleErr is the error handler for the main function. <br/>// If an error happens within the main function, it is not <br/>// unexpected for a fatal error to be logged and for the program to immediately quit.<br/>func mHandleErr(err error){<br/>  if err != nil {<br/>    log.Fatal(err)<br/>  }<br/>}</pre>
<p>A quick <kbd><span class="VerbatimChar">go run *.go</span></kbd> indicates this result (which has been truncated):</p>
<pre><strong>$ go run *.go</strong><br/><strong>Rows: 1460</strong><br/><strong>========</strong><br/><strong>Id: 1460</strong><br/><strong>MSSubClass: 15</strong><br/><strong>MSZoning: 5</strong><br/><strong>LotFrontage: 111</strong><br/><strong>LotArea: 1073</strong><br/><strong>SaleCondition: 6</strong><br/><strong>SalePrice: 663</strong></pre>
<p>Alone, this tells us a lot of interesting facts, chief amongst which is that there is a lot more categorical data than there is continuous data. Additionally, for some columns that are indeed continuous in nature, there are only a few discrete values available. One particular example is the <kbd><span class="VerbatimChar">LowQualSF</span></kbd> column—it's a continuous variable, but there are only 24 unique values.</p>
<p>We'd like to calculate the CEF of the discrete covariates for further analysis. But before that can happen, we would need to clean up the data. While we're at it, we might also want to create a logical grouping of data structures.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Janitorial work</h1>
                </header>
            
            <article>
                
<p>A large part of doing data science work is focused on cleanup. In productionized systems, this data would typically be fetched directly from the database, already relatively clean (high -quality production data science work requires a database of clean data). However, we're not in production mode yet. We're still in the model-building phase. It would be helpful to imagine writing a program solely for cleaning data.</p>
<p>Let's look at our requirements: starting with our data, each column is a variable—most of them are independent variables, except for the last column, which is the dependent variable. Some variables are categorical, and some are continuous. Our task is to write a function that will convert the data, currently <kbd><span class="VerbatimChar">[][]string</span></kbd> to <kbd><span class="VerbatimChar">[][]float64</span></kbd>.</p>
<p>To do that, we would require all the data to be converted into <kbd><span class="VerbatimChar">float64</span></kbd>. For the continuous variables, it's an easy task: simply parse the string into a float. There are oddities that need to be handled, which I hope you had spotted by the time you opened the file in a spreadsheet. But the main pain is in converting categorical data to <kbd><span class="VerbatimChar">float64</span></kbd>.</p>
<p>Fortunately for us, people much smarter than have figured this out decades ago. There exists an encoding scheme that allows categorical data to play nicely with linear regression algorithms.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Encoding categorical data</h1>
                </header>
            
            <article>
                
<p>The trick to encode categorical data is to expand categorical data into multiple columns, each having a 1 or 0 representing whether it's true or false. This of course comes with some caveats and subtle issues that must be navigated with care. For the rest of this subsection, I shall use a real categorical variable to explain further.</p>
<p>Consider the <kbd><span class="VerbatimChar">LandSlope</span></kbd> variable. There are three possible values for <kbd><span class="VerbatimChar">LandSlope</span></kbd>:</p>
<ul>
<li>Gtl</li>
<li>Mod</li>
<li>Sev</li>
</ul>
<p>This is one possible encoding scheme (this is commonly known as one-hot encoding):</p>
<table style="border-collapse: collapse;width: 100%" class="table" border="1">
<tbody>
<tr>
<td>
<p><strong>Slope</strong></p>
</td>
<td>
<p><strong>Slope_Gtl</strong></p>
</td>
<td>
<p><strong>Slope_Mod</strong></p>
</td>
<td>
<p><strong>Slope_Sev</strong></p>
</td>
</tr>
<tr>
<td>
<p>Gtl</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>Mod</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>Sev</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This would be a terrible encoding scheme. To understand why, we must first understand linear regression by means of ordinary least squares. Without going into too much detail, the meat of OLS-based linear regression is the following formula (which I am so in love with that I have had multiple T-shirts with the formula printed on):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/14494194-447a-4049-8a0d-ac848552b99e.png" style="width:11.58em;height:2.00em;" width="1520" height="260"/></p>
<p>Here,<img class="fm-editor-equation" src="Images/2c92b8b8-8893-4bf6-b17e-7e386e28d60e.png" style="width:1.08em;height:1.17em;" width="160" height="160"/>is an(m x n) matrix and<img class="fm-editor-equation" src="Images/385ff6f9-3b14-4131-bd02-26b875c70791.png" style="width:1.08em;height:1.17em;" width="140" height="150"/>is an (m x 1) vector. The multiplications, therefore, are not straightforward multiplications—they are matrix multiplications. When one-hot encoding is used for linear regression, the resulting input matrix<img class="fm-editor-equation" src="Images/e1cffed5-f7e4-41fe-be0e-53e4b12f04d4.png" style="width:3.50em;height:1.58em;" width="520" height="230"/>will typically be singular—in other words, the determinant of the matrix is 0. The problem with singular matrices is that they cannot be inverted.</p>
<p>So, instead, we have this encoding scheme:</p>
<table style="border-collapse: collapse;width: 100%" class="table" border="1">
<tbody>
<tr>
<td>
<p><strong>Slope</strong></p>
</td>
<td>
<p><strong>Slope_Mod</strong></p>
</td>
<td>
<p><strong>Slope_Sev</strong></p>
</td>
</tr>
<tr>
<td>
<p>Gtl</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>Mod</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>Sev</p>
</td>
<td>
<p>0</p>
</td>
<td>
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Here, we see an application of the Go proverb make the zero value useful for being applied in a data science context. Indeed, clever encoding of categorical variables will yield slightly better results when dealing with previously unseen data.</p>
<p>The topic is far too wide to broach here, but if you have categorical data that can be partially ordered, then when exposed to unseen data, simply encode the unseen data to the closest ordered variable value, and the results will be slightly better than encoding to the zero value or using random encoding. We will cover more of this in the later parts of this chapter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Handling bad numbers</h1>
                </header>
            
            <article>
                
<p class="mce-root">Another part of the janitorial work is handling bad numbers. A good example is in the <kbd><span class="VerbatimChar">LotFrontage</span></kbd> variable. From the data description, we know that this is supposed to be a continuous variable. Therefore, all the numbers should be directly convertible to <kbd><span class="VerbatimChar">float64</span></kbd>. Looking at the data, however, we see that it's not true—there is data that is <span class="VerbatimChar">NA</span>.</p>
<p><kbd>LotFrontage</kbd>, according to the description, is the linear feet of the street connected to property. <span class="VerbatimChar">NA</span> could mean one of two things:</p>
<ul>
<li>We have no information on whether there is a street connected to the property</li>
<li>There is no street connected to the property</li>
</ul>
<p>In either case, it would be reasonable to replace <span class="VerbatimChar">NA</span> with 0. This is reasonable, because the second lowest value in <kbd><span class="VerbatimChar">LotFrontage</span></kbd> is 21. There are other ways of imputing the data, of course, and often the imputations will lead to better models. But for now, we'll impute it with 0.</p>
<p>We can also do the same with any other continuous variables in this dataset simply because they make sense when you replace the NA with 0. One tip is to use it in a sentence: this house has an Unknown <kbd>GarageArea</kbd>. If that is the case, then what should be the best guess? Well, it'd be helpful to assume that the house has no garage, so it's OK to replace NA with 0.</p>
<p>Note that this may not be the case in other machine learning projects. Remember—human insight may be fallible, but its often the best solution for a lot of irregularities in the data. If you happen to be a realtor, and you have a lot more domain knowledge, you can infuse said domain knowledge into the imputation phase—you can use variables to calculate and estimate other variables for example.</p>
<p>As for the categorical variables, we can for the most part treat NA as the zero value of the variable, so no change there if there is an NA. There is some categorical data for which NA or None wouldn't make sense. This is where the aforementioned clever encoding of category could come in handy. In the cases of these variables, we'll use the most commonly found value as the zero value:</p>
<ul>
<li><kbd>MSZoning</kbd></li>
<li><kbd>BsmtFullBath</kbd></li>
<li><kbd>BsmtHalfBath</kbd></li>
<li><kbd>Utilities</kbd></li>
<li><kbd>Functional</kbd></li>
<li><kbd>Electrical</kbd></li>
<li><kbd>KitchenQual</kbd></li>
<li><kbd>SaleType</kbd></li>
<li><kbd>Exterior1st</kbd></li>
<li><kbd>Exterior2nd</kbd></li>
</ul>
<p>Furthermore, there are some variables that are categorical, but the data is numerical. An example found in the dataset is the <span class="VerbatimChar"><kbd>MSSubclass</kbd></span> variable. It's essentially a categorical variable, but its data is numerical. When encoding these kinds of categorical data, it makes sense to have them sorted numerically, such that the 0 value is indeed the lowest value.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Final requirement</h1>
                </header>
            
            <article>
                
<p>Despite the fact that we're model building right now, we want to build with the future in mind. The future is a production-ready machine learning system that performs linear regression. So whatever functions and methods we write have to take into account other things that may occur in a production environment that may not occur in the model -building phase.</p>
<p>The following are things to consider:</p>
<ul>
<li><strong>Unseen values</strong>: We have to write a function that is able to encode previously unseen values.</li>
<li><strong>Unseen variables</strong>: At some point in the future we might pass a different version of the data in that may contain variables that are unknown at model-building time. We would have to handle that.</li>
<li><strong>Different imputation strategies</strong>: Different variables will require different strategies for guessing missing data.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Writing the code</h1>
                </header>
            
            <article>
                
<p>Up to this point, we have only done the cleanup in our heads. I personally find this to be a much more rewarding exercise: to mentally clean up the data before actually cleaning up. This is not because I'm highly confident that I will have handled all the irregularities in the data. Instead, I like this process because it clarifies what needs to be done. And that in turn guides the data structures required for the job.</p>
<p>But, once the thinking is done, it's time to validate our thinking with code.</p>
<p>We start with the <span class="VerbatimChar">clean</span> function:</p>
<pre>// hints is a slice of bools indicating whether it's a categorical variable<br/>func clean(hdr []string, data [][]string, indices []map[string][]int, hints []bool, ignored []string) (int, int, []float64, []float64, []string, []bool) {<br/>  modes := mode(indices)<br/>  var Xs, Ys []float64<br/>  var newHints []bool<br/>  var newHdr []string<br/>  var cols int<br/><br/>  for i, row := range data {<br/><br/>    for j, col := range row {<br/>      if hdr[j] == "Id" { // skip id<br/>        continue<br/>      }<br/>      if hdr[j] == "SalePrice" { // we'll put SalePrice into Ys<br/>        cxx, _ := convert(col, false, nil, hdr[j])<br/>        Ys = append(Ys, cxx...)<br/>        continue<br/>      }<br/><br/>      if inList(hdr[j], ignored) {<br/>        continue<br/>      }<br/><br/>      if hints[j] {<br/>        col = imputeCategorical(col, j, hdr, modes)<br/>      }<br/>      cxx, newHdrs := convert(col, hints[j], indices[j], hdr[j])<br/>      Xs = append(Xs, cxx...)<br/><br/>      if i == 0 {<br/>        h := make([]bool, len(cxx))<br/>        for k := range h {<br/>          h[k] = hints[j]<br/>        }<br/>        newHints = append(newHints, h...)<br/>        newHdr = append(newHdr, newHdrs...)<br/>      }<br/>    }<br/>    // add bias<br/><br/>    if i == 0 {<br/>      cols = len(Xs)<br/>    }<br/>  }<br/>  rows := len(data)<br/>  if len(Ys) == 0 { // it's possible that there are no Ys (i.e. the test.csv file)<br/>    Ys = make([]float64, len(data))<br/>  }<br/>  return rows, cols, Xs, Ys, newHdr, newHints<br/>}</pre>
<p><kbd><span class="VerbatimChar">clean</span></kbd> takes data (in the form of <kbd><span class="VerbatimChar">[][]string</span></kbd>), and with the help of the indices built earlier, we want to build a matrix of <kbd><span class="VerbatimChar">Xs</span></kbd> (which will be <span class="VerbatimChar"><kbd>float64</kbd></span>) and <kbd><span class="VerbatimChar">Ys</span></kbd>. In Go, it's a simple loop. We'll read over the input data and try to convert that. A <kbd>hints</kbd> slice is also passed in to help us figure out if a variable should be considered a categorical or continuous variable.</p>
<p>In particular, the treatment of any year variables is <span>of contention</span>. Some statisticians think it's fine to treat a year variable as a discrete, non-categorical variable, while some statisticians think otherwise. I'm personally of the opinion that it doesn't really matter. If treating a year variable as a categorical variable improves the model score, then by all means use it. It's unlikely, though.</p>
<p>The meat of the preceding code is the conversion of a string into <kbd><span class="VerbatimChar">[]float64</span></kbd>, which is what the <span class="VerbatimChar">convert</span> function does. We will look in that function in a bit, but it's important to note that the data has to be imputed before conversion. This is because Go's slices are well-typed. A <kbd><span class="VerbatimChar">[]float64</span></kbd> can only contain <kbd><span class="VerbatimChar">float64</span></kbd>.</p>
<p>While it's true that we can also replace any unknown data with <span class="VerbatimChar">NaN</span>, that would not be helpful, especially in the case of categorical data, where <span class="VerbatimChar">NA</span> might actually have semantic meaning. So, we impute categorical data before converting them. This is what <kbd><span class="VerbatimChar">imputeCategorical</span></kbd> looks like:</p>
<pre>// imputeCategorical replaces "NA" with the mode of categorical values<br/>func imputeCategorical(a string, col int, hdr []string, modes []string) string {<br/>  if a != "NA" || a != "" {<br/>    return a<br/>  }<br/>  switch hdr[col] {<br/>  case "MSZoning", "BsmtFullBath", "BsmtHalfBath", "Utilities", "Functional", "Electrical", "KitchenQual", "SaleType", "Exterior1st", "Exterior2nd":<br/>    return modes[col]<br/>  }<br/>  return a<br/>}</pre>
<p>What this function says is, if the value is not <kbd>NA</kbd> and the value is not an empty string, then it's a valid value, hence we return early. Otherwise, we will have to consider whether to return <kbd>NA</kbd> as a valid category.</p>
<p>For some specific categories, NAs are not valid categories, and they are replaced by the most-commonly occurring value. This is a logical thing to do—a shed in the middle of nowhere with no electricity, no gas, and no bath is a very rare occurrence. There are techniques to deal with that (such as LASSO regression), but we're not going to do that right now. Instead, we'll just replace them with the mode.</p>
<p>The mode was calculated in the <span class="VerbatimChar">clean</span> function. This is a very simple definition for finding the modes; we simply find the value that has the greatest length and return the value:</p>
<pre>// mode finds the most common value for each variable<br/>func mode(index []map[string][]int) []string {<br/>  retVal := make([]string, len(index))<br/>  for i, m := range index {<br/>    var max int<br/>    for k, v := range m {<br/>      if len(v) &gt; max {<br/>        max = len(v)<br/>        retVal[i] = k<br/>      }<br/>    }<br/>  }<br/>  return retVal<br/>}</pre>
<p>After we've imputed the categorical data, we'll convert all the data to <kbd><span class="VerbatimChar">[]float</span></kbd>. For numerical data, that will result in a slice with a single value. But for categorical data, it will result in a slice of 0s and 1s.</p>
<p>For the purposes of this chapter, any NAs found in the numerical data will be converted to <span class="VerbatimChar">0.0</span>. There are other valid strategies that will improve the results of the model very slightly, but these strategies are not brief.</p>
<p>And so, the conversion code looks simple:</p>
<pre>// convert converts a string into a slice of floats<br/>func convert(a string, isCat bool, index map[string][]int, varName string) ([]float64, []string) {<br/>  if isCat {<br/>    return convertCategorical(a, index, varName)<br/>  }<br/>  // here we deliberately ignore errors, because the zero value of float64 is well, zero.<br/>  f, _ := strconv.ParseFloat(a, 64)<br/>  return []float64{f}, []string{varName}<br/>}<br/><br/>// convertCategorical is a basic function that encodes a categorical variable as a slice of floats.<br/>// There are no smarts involved at the moment.<br/>// The encoder takes the first value of the map as the default value, encoding it as a []float{0,0,0,...}<br/>func convertCategorical(a string, index map[string][]int, varName string) ([]float64, []string) {<br/>  retVal := make([]float64, len(index)-1)<br/><br/>  // important: Go actually randomizes access to maps, so we actually need to sort the keys<br/>  // optimization point: this function can be made stateful.<br/>  tmp := make([]string, 0, len(index))<br/>  for k := range index {<br/>    tmp = append(tmp, k)<br/>  }<br/><br/>  // numerical "categories" should be sorted numerically<br/>  tmp = tryNumCat(a, index, tmp)<br/><br/>  // find NAs and swap with 0<br/>  var naIndex int<br/>  for i, v := range tmp {<br/>    if v == "NA" {<br/>      naIndex = i<br/>      break<br/>    }<br/>  }<br/>  tmp[0], tmp[naIndex] = tmp[naIndex], tmp[0]<br/><br/>  // build the encoding<br/>  for i, v := range tmp[1:] {<br/>    if v == a {<br/>      retVal[i] = 1<br/>      break<br/>    }<br/>  }<br/>  for i, v := range tmp {<br/>    tmp[i] = fmt.Sprintf("%v_%v", varName, v)<br/>  }<br/><br/>  return retVal, tmp[1:]<br/>}</pre>
<p>I would like to draw your attention to the <kbd><span class="VerbatimChar">convertCategorical</span></kbd> <span>function.</span> There is some verbosity involved in the code, but the verbosity wills away the magic. Because Go randomizes access to a map, it's important to get a list of keys, and then sort them. This way, all subsequent access will be deterministic.</p>
<p>The function also allows room for optimization—making this function a <kbd>stateful</kbd> function would optimize it further, but for this project we shan't bother.</p>
<p>This is our main function so far:</p>
<pre><span class="VerbatimChar">func main() {</span><br/><span class="VerbatimChar"><span> </span>f, err := os.Open("train.csv")</span><br/><span class="VerbatimChar"><span> </span>mHandleErr(err)</span><br/><span class="VerbatimChar"><span> </span>hdr, data, indices, err := ingest(f)<br/><span> </span></span><span class="VerbatimChar">mHandleErr(err)<br/><span> </span></span><span class="VerbatimChar">fmt.Printf("Original Data: nRows: %d, Cols: %dn========n", len(data), len(hdr))</span><br/><span class="VerbatimChar"><span> </span>c := cardinality(indices)</span><br/> <span class="VerbatimChar">for i, h := range hdr {</span><br/><span class="VerbatimChar"><span> </span><span> </span>fmt.Printf("%v: %vn", h, c[i])</span><br/> <span class="VerbatimChar">}</span><br/> <span class="VerbatimChar">fmt.Println("")<br/><span> </span></span><span class="VerbatimChar">fmt.Printf("Building into matricesn=============n")</span><br/> <span class="VerbatimChar">rows, cols, XsBack, YsBack, newHdr, _ := clean(hdr, data, indices, datahints, nil)</span><br/> <span class="VerbatimChar">Xs := tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(XsBack))</span><br/> <span class="VerbatimChar">Ys := tensor.New(tensor.WithShape(rows, 1), tensor.WithBacking(YsBack</span><br/> <span class="VerbatimChar">fmt.Printf("Xs:\n%+1.1snYs:\n%1.1sn", Xs, Ys)</span><br/> <span class="VerbatimChar">fmt.Println("")</span><br/><span class="VerbatimChar">}</span></pre>
<p>And the output of the code is as follows:</p>
<pre>Original Data:<br/>Rows: 1460, Cols: 81<br/>========<br/>Id: 1460<br/>MSSubClass: 15<br/>MSZoning: 5<br/>LotFrontage: 111<br/>LotArea: 1073<br/>Street: 2<br/> ⋮<br/>Building into matrices<br/>=============<br/>Xs:<br/>⎡ 0 0 ⋯ 1 0⎤<br/>⎢ 0 0 ⋯ 1 0⎥<br/> ⋮<br/>⎢ 0 0 ⋯ 1 0⎥<br/>⎣ 0 0 ⋯ 1 0⎦<br/>Ys:<br/>C[2e+05 2e+05 ⋯ 1e+05 1e+05]</pre>
<p>Note that while the original data had 81 variables, by the time we are done with the encoding there are 615 variables. This is what we want to pass into the regression. At this point, the seasoned data scientist may notice a few things that may not sit well with her. For example, the number of variables (615) is too close to the number of observations (1,460) for comfort, so we might run into some issues. We will address those issues later.</p>
<p>Another point to note is that we're converting the data to <kbd><span class="VerbatimChar">*tensor.Dense</span></kbd>. You can think of the <kbd><span class="VerbatimChar">*tensor.Dense</span></kbd> data structure as a matrix. It is an efficient data structure with a lot of niceness that we will use later.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further exploratory work</h1>
                </header>
            
            <article>
                
<p><span>At this point, it would be very tempting to just take these matrices and run the regression on them. While that could work, it wouldn't necessarily produce the best results. </span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The conditional expectation functions</h1>
                </header>
            
            <article>
                
<p>Instead, let's do what we originally set out to do: explore the <kbd>CEF</kbd>s of the variables. Fortunately, we already have the necessary data structures (in other words, the index), so writing the function to find the <kbd>CEF</kbd> is relatively easy.</p>
<p>The following is the code block:</p>
<pre>func CEF(Ys []float64, col int, index []map[string][]int) map[string]float64 {<br/>  retVal := make(map[string]float64)<br/>  for k, v := range index[col] {<br/>    var mean float64<br/>    for _, i := range v {<br/>      mean += Ys[i]<br/>    }<br/>    mean /= float64(len(v))<br/>    retVal[k]=mean<br/>  }<br/>  return retVal<br/>}</pre>
<p>This function finds the conditionally expected house price when a variable is held fixed. We can do an exploration of all the variables, but for the purpose of this chapter, I shall only share the exploration of one –the yearBuilt variable—as an example.</p>
<p>Now, YearBuilt is an interesting variable to dive deep into. It's a categorical variable (1950.5 makes no sense), but it's totally orderable as well (1,945 is smaller than 1,950). And there are many values of YearBuilt. So, instead of printing it out, we shall plot it out with the following function:</p>
<pre>// plotCEF plots the CEF. This is a simple plot with only the CEF. <br/>// More advanced plots can be also drawn to expose more nuance in understanding the data.<br/>func plotCEF(m map[string]float64) (*plot.Plot, error) {<br/>  ordered := make([]string, 0, len(m))<br/>  for k := range m {<br/>    ordered = append(ordered, k)<br/>  }<br/>  sort.Strings(ordered)<br/><br/>  p, err := plot.New()<br/>  if err != nil {<br/>    return nil, err<br/>  }<br/><br/>  points := make(plotter.XYs, len(ordered))<br/>  for i, val := range ordered {<br/>    // if val can be converted into a float, we'll use it<br/>    // otherwise, we'll stick with using the index<br/>    points[i].X = float64(i)<br/>    if x, err := strconv.ParseFloat(val, 64); err == nil {<br/>      points[i].X = x<br/>    }<br/><br/>    points[i].Y = m[val]<br/>  }<br/>  if err := plotutil.AddLinePoints(p, "CEF", points); err != nil {<br/>    return nil, err<br/>  }<br/>  return p, nil<br/>}</pre>
<p>Our ever-growing main function now has this appended to it:</p>
<pre><span class="VerbatimChar">ofInterest := 19 // variable of interest is in column 19</span><br/><span class="VerbatimChar">cef := CEF(YsBack, ofInterest, indices)</span><br/><span class="VerbatimChar">plt, err := plotCEF(cef)</span><br/><span class="VerbatimChar">mHandleErr(err)</span><br/><span class="VerbatimChar">plt.Title.Text = fmt.Sprintf("CEF for %v", hdr[ofInterest])</span><br/><span class="VerbatimChar">plt.X.Label.Text = hdr[ofInterest]</span><br/><span class="VerbatimChar">plt.Y.Label.Text = "Conditionally Expected House Price"<br/></span><span class="VerbatimChar">mHandleErr(plt.Save(25*vg.Centimeter, 25*vg.Centimeter, "CEF.png"))</span></pre>
<p class="mce-root">Running the program yields the following chart:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img src="Images/feaaffe2-2a95-4164-9e69-34870dce328f.png" style="width:49.25em;height:49.25em;" width="945" height="945"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">conditional<span> </span>expectation<span> </span>functions for Yearbuilt</div>
<p>Upon inspecting the chart, I must confess that I was a little surprised. I'm not particularly familiar with real estate, but my initial instincts were that older houses would cost more—houses, in my mind, age like fine wine; the older the house, the more expensive it would be. Clearly this is not the case. Oh well, live and learn.</p>
<p>The CEF exploration should be done for as many variables as possible. I am merely eliding for the sake of brevity in this book.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Skews</h1>
                </header>
            
            <article>
                
<p>Now let's look at how the data for the house prices are distributed:</p>
<pre>func hist(a []float64) (*plot.Plot, error){<br/>  h, err := plotter.NewHist(plotter.Values(a), 10)<br/>  if err != nil {<br/>    return nil, err<br/>  }<br/>  p, err := plot.New()<br/>  if err != nil {<br/>    return nil, err<br/>  }<br/><br/>  h.Normalize(1)<br/>  p.Add(h)<br/>  return p, nil<br/>}</pre>
<p>This section is added to the main function:</p>
<pre><span class="VerbatimChar">hist, err := plotHist(YsBack)</span><br/><span class="VerbatimChar">mHandleErr(err)</span><br/><span class="VerbatimChar">hist.Title.Text = "Histogram of House Prices"</span><br/><span class="VerbatimChar">mHandleErr(hist.Save(25*vg.Centimeter, 25*vg.Centimeter, "hist.png"))</span></pre>
<p>The following diagram is:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-498 image-border" src="Images/de54cca7-582e-4cd4-8363-25d323c2ce77.png" style="width:40.42em;height:39.83em;" width="978" height="964"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Histogram of House prices</div>
<p>As can be noted, the histogram of the prices is a little skewed. Fortunately, we can fix that by applying a function that performs the logging of the value and then adds 1. The standard library provides a function for this: <kbd><span class="VerbatimChar">math.Log1p</span></kbd>. So, we add the following to our main function:</p>
<pre><span class="VerbatimChar">for i := range YsBack {</span><br/> <span class="VerbatimChar">YsBack[i] = math.Log1p(YsBack[i])</span><br/> <span class="VerbatimChar">}</span><br/> <span class="VerbatimChar">hist2, err := plotHist(YsBack)</span><br/> <span class="VerbatimChar">mHandleErr(err)</span><br/> <span class="VerbatimChar">hist2.Title.Text = "Histogram of House Prices (Processed)"</span><br/> <span class="VerbatimChar">mHandleErr(hist2.Save(25*vg.Centimeter, 25*vg.Centimeter, "hist2.png"))</span></pre>
<p>The following diagram is :</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-499 image-border" src="Images/51667581-630e-48e9-b1bc-66981011c36e.png" style="width:44.92em;height:43.92em;" width="983" height="960"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="VerbatimChar">Histogram of House Prices (Processed)</span></div>
<p>Ahh! This looks better. We did this for all the <kbd><span class="VerbatimChar">Ys</span></kbd>. What about any of the <kbd><span class="VerbatimChar">Xs</span></kbd>? To do that, we will have to iterate through each column of <kbd><span class="VerbatimChar">Xs</span></kbd>, find out if they are skewed, and if they are, we need to apply the transformation function.</p>
<p>This is what we add to the main function:</p>
<pre>  it, err := native.MatrixF64(Xs)<br/>  mHandleErr(err)<br/>  for i, isCat := range datahints {<br/>    if isCat {<br/>      continue<br/>    }<br/>    skewness := skew(it, i)<br/>    if skewness &gt; 0.75 {<br/>      log1pCol(it, i)<br/>    }<br/>  }</pre>
<p><kbd><span class="VerbatimChar">native.MatrixF64s</span></kbd> takes a <kbd><span class="VerbatimChar">*tensor.Dense</span></kbd> and converts it into a native Go iterator. The underlying backing data doesn't change, therefore if one were to write <kbd><span class="VerbatimChar">it[0][0] = 1000</span></kbd>, the actual matrix itself would change too. This allows us to perform transformations without additional allocations. For this topic, it may not be as important; however, for larger projects, this will come to be very handy.</p>
<p>This also allows us to write the functions to check and mutate the matrix:</p>
<pre>// skew returns the skewness of a column/variable<br/>func skew(it [][]float64, col int) float64 {<br/>  a := make([]float64, 0, len(it[0]))<br/>  for _, row := range it {<br/>    for _, col := range row {<br/>      a = append(a, col)<br/>    }<br/>  }<br/>  return stat.Skew(a, nil)<br/>}<br/><br/>// log1pCol applies the log1p transformation on a column<br/>func log1pCol(it [][]float64, col int) {<br/>  for i := range it {<br/>    it[i][col] = math.Log1p(it[i][col])<br/>  }<br/>}</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multicollinearity</h1>
                </header>
            
            <article>
                
<p>As mentioned in the opening paragraphs of this section, the number of variables is a little high for comfort. When there is a high number of variables the chances of multicollinearity increases. Multicollinearity is when two or more variables are correlated with each other somehow.</p>
<p>From a cursory glance at the data, we can tell that is in fact true. A simple thing to note is <span class="VerbatimChar">GarageArea</span> is correlated with <span class="VerbatimChar">GarageCars</span>. In real life, this makes sense—a garage that can take two cars would be logically larger in area compared to a garage that can only store one car. Likewise, zoning is highly correlated with the neighborhood.</p>
<p>A good way to think about the variables is in terms of information included in the variables. Sometimes, the variables have information that overlaps. For example, when <span class="VerbatimChar">GarageArea</span> is 0, that overlaps with the <span class="VerbatimChar">GarageType</span> of <span class="VerbatimChar">NA—</span>after all, if you have no garage, the area of your garage is zero.</p>
<p>The difficult part is going through the list of variables, and deciding which to keep. It's something of an art that has help from algorithms. In fact, the first thing we're going to do is to find out how correlated a variable is with another variable. We do this by calculating the correlation matrix, then plotting out a heatmap.</p>
<p>To calculate the correlation matrix, we simply use the function in Gonum with this snippet:</p>
<pre>  m64, err := tensor.ToMat64(Xs, tensor.UseUnsafe())<br/>  mHandleErr(err)<br/>  corr := stat.CorrelationMatrix(nil, m64, nil)<br/>  hm, err := plotHeatMap(corr, newHdr)<br/>  mHandleErr(err)<br/>  hm.Save(60*vg.Centimeter, 60*vg.Centimeter, "heatmap.png")</pre>
<p>Let's go through this line by line:</p>
<p><kbd><span class="VerbatimChar">m64, err := tensor.ToMat64(Xs, tensor.UseUnsafe())</span></kbd> performs the conversion from <kbd><span class="VerbatimChar">*tensor.Dense</span></kbd> to <kbd><span class="VerbatimChar">mat.Mat64</span></kbd>. Because we don't want to allocate an additional chunk of memory, and we've determined that it's safe to actually reuse the data in the matrix, we pass in a <kbd><span class="VerbatimChar">tensor.UseUnsafe()</span></kbd> function option that tells Gorgonia to reuse the underlying memory in the Gonum matrix.</p>
<p><kbd><span class="VerbatimChar">stat.CorrelationMatrix(nil, m64, nil)</span></kbd> calculates the correlation matrix. The correlation matrix is a triangular matrix—a particularly useful data structure that the Gonum package provides. It is a clever little data structure for this use case because the matrix is mirrored along the diagonal.</p>
<p>Next, we plot <kbd>heatmap</kbd> using the following snippet of code:</p>
<pre>type heatmap struct {<br/>  x mat.Matrix<br/>}<br/><br/>func (m heatmap) Dims() (c, r int) { r, c = m.x.Dims(); return c, r }<br/>func (m heatmap) Z(c, r int) float64 { return m.x.At(r, c) }<br/>func (m heatmap) X(c int) float64 { return float64(c) }<br/>func (m heatmap) Y(r int) float64 { return float64(r) }<br/><br/>type ticks []string<br/><br/>func (t ticks) Ticks(min, max float64) []plot.Tick {<br/>  var retVal []plot.Tick<br/>  for i := math.Trunc(min); i &lt;= max; i++ {<br/>    retVal = append(retVal, plot.Tick{Value: i, Label: t[int(i)]})<br/>  }<br/>  return retVal<br/>}<br/><br/><br/>func plotHeatMap(corr mat.Matrix, labels []string) (p *plot.Plot, err error) {<br/>  pal := palette.Heat(48, 1)<br/>  m := heatmap{corr}<br/>  hm := plotter.NewHeatMap(m, pal)<br/>  if p, err = plot.New(); err != nil {<br/>    return<br/>  }<br/>  hm.NaN = color.RGBA{0, 0, 0, 0} // black<br/><br/>  // add and adjust the prettiness of the chart<br/>  p.Add(hm)<br/>  p.X.Tick.Label.Rotation = 1.5<br/>  p.Y.Tick.Label.Font.Size = 6<br/>  p.X.Tick.Label.Font.Size = 6<br/>  p.X.Tick.Label.XAlign = draw.XRight<br/>  p.X.Tick.Marker = ticks(labels)<br/>  p.Y.Tick.Marker = ticks(labels)<br/><br/>  // add legend<br/>  l, err := plot.NewLegend()<br/>  if err != nil {<br/>    return p, err<br/>  }<br/><br/>  thumbs := plotter.PaletteThumbnailers(pal)<br/>  for i := len(thumbs) - 1; i &gt;= 0; i-- {<br/>    t := thumbs[i]<br/>    if i != 0 &amp;&amp; i != len(thumbs)-1 {<br/>      l.Add("", t)<br/>      continue<br/>    }<br/>    var val float64<br/>    switch i {<br/>    case 0:<br/>      val = hm.Min<br/>    case len(thumbs) - 1:<br/>      val = hm.Max<br/>    }<br/>    l.Add(fmt.Sprintf("%.2g", val), t)<br/>  }<br/><br/>  // this is a hack. I place the legends between the axis and the actual heatmap<br/>  // because if the legend is on the right, we'd need to create a custom canvas to take<br/>  // into account the additional width of the legend.<br/>  //<br/>  // So instead, we shrink the legend width to fit snugly within the margins of the plot and the axes.<br/>  l.Left = true<br/>  l.XOffs = -5<br/>  l.ThumbnailWidth = 5<br/>  l.Font.Size = 5<br/><br/>  p.Legend = l<br/>  return<br/>}</pre>
<p>The <kbd><span class="VerbatimChar">plotter.NewHeatMap</span></kbd> function expects an interface, which is why I wrapped <kbd><span class="VerbatimChar">mat.Mat</span></kbd> in the <span class="VerbatimChar">heatmap</span> data structure, which provides the interface for the plotter to draw a heatmap. This pattern will become more and more common in the coming chapters—wrapping a data structure just to provide an additional interface to other functions. They are cheap and readily available and should be used to the fullest extent.</p>
<p>A large portion of this code involves a hack for the labels. The way Gonum plots work, is that when the canvas size is calculated, the label is considered to be inside the plot. To be able to draw the labels outside the plot, a lot of extra code would have to be written. So, instead, I shrunk the labels to fit into the gutter between the axis and the plot itself as to not overlay into important areas of the plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-500 image-border" src="Images/8d89cadf-dfb0-44b2-afd3-3521ff14abb2.png" style="width:39.75em;height:39.75em;" width="1950" height="1950"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Heatmap</div>
<p>Of particular note in this heatmap are the white streaks. We expect a variable to correlate with itself completely. But if you notice, there are areas of white lines that are somewhat parallel to the diagonal white line. These are total correlations. We will need to remove them.</p>
<p>Heatmaps are nice to look at but are quite silly. The human eye isn't great at telling hues apart. So what we're going to do is also report back the numbers. The correlation between variables is between -1 and 1. We're particularly interested in correlations that are close to either end.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This snippet prints the results:</p>
<pre>  // heatmaps are nice to look at, but are quite ridiculous.<br/>  var tba []struct {<br/>    h1, h2 string<br/>    corr float64<br/>  }<br/>  for i, h1 := range newHdr {<br/>    for j, h2 := range newHdr {<br/>      if c := corr.At(i, j); math.Abs(c) &gt;= 0.5 &amp;&amp; h1 != h2 {<br/>        tba = append(tba, struct {<br/>          h1, h2 string<br/>          corr float64<br/>        }{h1: h1, h2: h2, corr: c})<br/>      }<br/>    }<br/>  }<br/>  fmt.Println("High Correlations:")<br/>  for _, a := range tba {<br/>    fmt.Printf("\t%v-%v: %v\n", a.h1, a.h2, a.corr)<br/>  }</pre>
<p>Here I use an anonymous struct, instead of a named struct, because we're not going to reuse the data—it's solely for printing. An anonymous tuple would suffice. This is not the best practice in most cases.</p>
<p>This correlation plot shows only the correlation of the independent variables. To truly understand multicollinearity, we would have to find the correlation of each variable to each other, and to the dependent variable. This will be left as an exercise for the reader.</p>
<div class="packt_tip">If you were to plot the correlation matrix, it'd look the same as the one we have right here, but with an additional row and column for the dependent variable.</div>
<p>Ultimately, multicollinearity can only be detected after running a regression. The correlation plot is simply a shorthand way of guiding the inclusion and exclusion of variables. The actual process of removing multicollinearity is an iterative one, often with other statistics such as the variance inflation factor to lend a hand in deciding what to include and what not to include.</p>
<p>For the purpose of this chapter, I've identified multiple variables to be included—and the majority of variables are excluded. This can be found in the <kbd><span class="VerbatimChar">const.go</span></kbd> file. The commented out lines in the <span class="VerbatimChar">ignored</span> list are what was included in the final model.</p>
<p>As mentioned in the opening paragraph of this section, it's really a bit of an art, aided by algorithms.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Standardization</h1>
                </header>
            
            <article>
                
<p>As a last bit of transformation, we would need to standardize our input data. This allow us to compare models to see if one model is better than another. To do so, I wrote two different scaling algorithms:</p>
<pre>func scale(a [][]float64, j int) {<br/>  l, m, h := iqr(a, 0.25, 0.75, j)<br/>  s := h - l<br/>  if s == 0 {<br/>    s = 1<br/>  }<br/><br/>  for _, row := range a {<br/>    row[j] = (row[j] - m) / s<br/>  }<br/>}<br/><br/>func scaleStd(a [][]float64, j int) {<br/>  var mean, variance, n float64<br/>  for _, row := range a {<br/>    mean += row[j]<br/>    n++<br/>  }<br/>  mean /= n<br/>  for _, row := range a {<br/>    variance += (row[j] - mean) * (row[j] - mean)<br/>  }<br/>  variance /= (n-1)<br/><br/>  for _, row := range a {<br/>    row[j] = (row[j] - mean) / variance<br/>  }<br/>}</pre>
<p>If you come from the Python world of data science, the first <span class="VerbatimChar">scale</span> function is essentially what scikits-learn's <kbd><span class="VerbatimChar">RobustScaler</span></kbd> does. The second function is essentially <kbd><span class="VerbatimChar">StdScaler</span></kbd>, but with the variance adapted to work for sample data.</p>
<p>This function takes the values in a given column (<kbd><span class="VerbatimChar">j</span></kbd>) and scales them in such a way that all the values are constrained to within a certain value. Also, note that the input to both scaling functions is <kbd><span class="VerbatimChar">[][]float64</span></kbd>. This is where the benefits of the <kbd><span class="VerbatimChar">tensor</span></kbd> package comes in handy. A <kbd><span class="VerbatimChar">*tensor.Dense</span></kbd> can be converted to <kbd><span class="VerbatimChar">[][]float64</span></kbd> without any extra allocations. An additional beneficial side effect is that you can mutate <kbd><span class="VerbatimChar">a</span></kbd> and the tensor values will change as well. Essentially, <kbd><span class="VerbatimChar">[][]float64</span></kbd> will act as an iterator to the underlying tensor data.</p>
<p>Our transform function now looks like this:</p>
<pre>func transform(it [][]float64, hdr []string, hints []bool) []int {<br/>  var transformed []int<br/>  for i, isCat := range hints {<br/>    if isCat {<br/>      continue<br/>    }<br/>    skewness := skew(it, i)<br/>    if skewness &gt; 0.75 {<br/>      transformed = append(transformed, i)<br/>      log1pCol(it, i)<br/>    }<br/>  }<br/>  for i, h := range hints {<br/>    if !h {<br/>      scale(it, i)<br/>    }<br/>  }<br/>  return transformed<br/>}</pre>
<p>Note that we only want to scale the numerical variables. The categorical variables can be scaled, but there isn't really much difference.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Linear regression</h1>
                </header>
            
            <article>
                
<p>Now that that's all done, let's do some linear regression! But first, let's clean up our code. We'll move our exploratory work so far into a function called <kbd><span class="VerbatimChar">exploration()</span></kbd>. Then we will reread the file, split the dataset into training and testing dataset, and perform all the transformations before finally running the regression. For that, we will use <kbd>github.com/sajari/regression</kbd> and apply the regression.</p>
<p>The first part looks like this:</p>
<pre>func main() {<br/>  // exploratory() // commented out because we're done with exploratory work.<br/><br/>  f, err := os.Open("train.csv")<br/>  mHandleErr(err)<br/>  defer f.Close()<br/>  hdr, data, indices, err := ingest(f)<br/>  rows, cols, XsBack, YsBack, newHdr, newHints := clean(hdr, data, indices, datahints, ignored)<br/>  Xs := tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(XsBack))<br/>  it, err := native.MatrixF64(Xs)<br/>  mHandleErr(err)<br/><br/>  // transform the Ys<br/>  for i := range YsBack {<br/>    YsBack[i] = math.Log1p(YsBack[i])<br/>  }<br/>  // transform the Xs<br/>  transform(it, newHdr, newHints)<br/><br/>  // partition the data<br/>  shuffle(it, YsBack)<br/>  testingRows := int(float64(rows) * 0.2)<br/>  trainingRows := rows - testingRows<br/>  testingSet := it[trainingRows:]<br/>  testingYs := YsBack[trainingRows:]<br/>  it = it[:trainingRows]<br/>  YsBack = YsBack[:trainingRows]<br/>  log.Printf("len(it): %d || %d", len(it), len(YsBack))<br/>...</pre>
<p>We first ingest and clean the data, then we create an iterator for the matrix of <kbd>Xs</kbd> for easier access. We then transform both the <kbd>Xs</kbd> and the <kbd>Ys</kbd>. Finally, we shuffle the <kbd>Xs</kbd>, and partition them into a training dataset and a testing dataset.</p>
<p>Recall from the first chapter on knowing whether a model is good. A good model must be able to generalize to previously unseen combinations of values. To prevent overfitting, we must cross-validate our model.</p>
<p>In order to achieve that, we must only train on a limited subset of data, then use the model to predict on the test set of data. We can then get a score of how well it did when being run on the testing set.</p>
<p>Ideally, this should be done before the parsing of the data into the <kbd>Xs</kbd> and <kbd>Ys</kbd>. But we'd like to reuse the functions we wrote earlier, so we shan't do that. The separate functions of <span class="VerbatimChar">ingest</span> and clean, however, allows you to do that. And if you visit the repository on GitHub, you will find that all the functions for such an act can easily be done.</p>
<p>For now, we simply take out 20% of the dataset, and set it aside. A <span class="VerbatimChar">shuffle</span> is used to resample the rows so that we don't train on the same 80% every time.</p>
<p>Also, note that now the <kbd><span class="VerbatimChar">clean</span></kbd> function takes <span class="VerbatimChar"><kbd>ignored</kbd>,</span> while in the exploratory mode, it took <kbd><span class="VerbatimChar">nil</span></kbd>. This, along with the <span class="VerbatimChar"><kbd>shuffle</kbd>,</span> are important for cross-validation later on.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The regression</h1>
                </header>
            
            <article>
                
<p>And so, now we're ready to build the regression model. Bear in mind that this section is highly iterative in real life. I will describe the iterations, but will only share the model that I chose to settle on.</p>
<p>The <kbd><span class="VerbatimChar">github.com/sajari/regression</span></kbd> package does an admirable job. But we want to extend the package a little to be able to compare models and the coefficients of the parameters. So I wrote this function:</p>
<pre>func runRegression(Xs [][]float64, Ys []float64, hdr []string) (r *regression.Regression, stdErr []float64) {<br/>  r = new(regression.Regression)<br/>  dp := make(regression.DataPoints, 0, len(Xs))<br/>  for i, h := range hdr {<br/>    r.SetVar(i, h)<br/>  }<br/>  for i, row := range Xs {<br/>    if i &lt; 3 {<br/>      log.Printf("Y %v Row %v", Ys[i], row)<br/>    }<br/>    dp = append(dp, regression.DataPoint(Ys[i], row))<br/>  }<br/>  r.Train(dp...)<br/>  r.Run()<br/><br/>  // calculate StdErr<br/>  var sseY float64<br/>  sseX := make([]float64, len(hdr)+1)<br/>  meanX := make([]float64, len(hdr)+1)<br/>  for i, row := range Xs {<br/>    pred, _ := r.Predict(row)<br/>    sseY += (Ys[i] - pred) * (Ys[i] - pred)<br/>    for j, c := range row {<br/>      meanX[j+1] += c<br/>    }<br/>  }<br/>  sseY /= float64(len(Xs) - len(hdr) - 1) // n - df ; df = len(hdr) + 1<br/>  vecf64.ScaleInv(meanX, float64(len(Xs)))<br/>  sseX[0] = 1<br/>  for _, row := range Xs {<br/>    for j, c := range row {<br/>      sseX[j+1] += (c - meanX[j+1]) * (c - meanX[j+1])<br/>    }<br/>  }<br/>  sseY = math.Sqrt(sseY)<br/>  vecf64.Sqrt(sseX)<br/>  vecf64.ScaleInvR(sseX, sseY)<br/><br/>  return r, sseX<br/>}</pre>
<p><kbd><span class="VerbatimChar">runRegression</span></kbd> will perform the regression analysis, and print the outputs of the standard errors of the coefficients. It is an estimate of the standard deviation of the coefficients—imagine this model being run many many times: each time the coefficients might be slightly different. The standard error simply reports amount of variation in the coefficients.</p>
<p>The standard errors are calculated with the help of the <kbd><span class="VerbatimChar">gorgonia.org/vecf64</span></kbd> package, which performs in-place operations for vectors. Optionally, you may choose to write them as loops.</p>
<p>This function also introduces us to the API for the <kbd><span class="VerbatimChar">github.com/sajari/regression</span></kbd> package—to predict, simply use <kbd><span class="VerbatimChar">r.Predict(vars)</span></kbd>. This will be useful in cases where one would like to use this model for production.</p>
<p>For now, let us focus on the other half of the <span class="VerbatimChar">main</span> function:</p>
<pre>  // do the regessions<br/>  r, stdErr := runRegression(it, YsBack, newHdr)<br/>  tdist := distuv.StudentsT{Mu: 0, Sigma: 1, Nu: float64(len(it) - len(newHdr) - 1), Src: rand.New(rand.NewSource(uint64(time.Now().UnixNano())))}<br/>  fmt.Printf("R^2: %1.3f\n", r.R2)<br/>  fmt.Printf("\tVariable \tCoefficient \tStdErr \tt-stat\tp-value\n")<br/>  fmt.Printf("\tIntercept: \t%1.5f \t%1.5f \t%1.5f \t%1.5f\n", r.Coeff(0), stdErr[0], r.Coeff(0)/stdErr[0], tdist.Prob(r.Coeff(0)/stdErr[0]))<br/>  for i, h := range newHdr {<br/>    b := r.Coeff(i + 1)<br/>    e := stdErr[i+1]<br/>    t := b / e<br/>    p := tdist.Prob(t)<br/>    fmt.Printf("\t%v: \t%1.5f \t%1.5f \t%1.5f \t%1.5f\n", h, b, e, t, p)<br/>  }<br/>...</pre>
<p>Here, we run the regression, and then we print the results. We don't just want to output the regression coefficients. We also want to output the standard errors, the t-statistic, and the P-value. This would give us some confidence over the estimated coefficients.</p>
<p><kbd><span class="VerbatimChar">tdist := distuv.StudentsT{Mu: 0, Sigma: 1, Nu: float64(len(it) - len(newHdr) - 1), Src: rand.New(rand.NewSource(uint64(time.Now().UnixNano())))}</span></kbd> creates a Student's t-distribution, which we will compare against our data. The t-statistic is very simply calculated by dividing the coefficient by the standard error.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cross-validation</h1>
                </header>
            
            <article>
                
<p>And now we come to the final part—in order to compare models, we would like to cross-validate the model. We've already set aside a portion of the data. Now, we will have to test the model on the data that was set aside, and compute a score.</p>
<p>The score we'll be using is a Root Mean Square Error. It's used because it's simple and straightforward to understand:</p>
<pre>  // VERY simple cross validation<br/>  var MSE float64<br/>  for i, row := range testingSet {<br/>    pred, err := r.Predict(row)<br/>    mHandleErr(err)<br/>    correct := testingYs[i]<br/>    eStar := correct - pred<br/>    e2 := eStar * eStar<br/>    MSE += e2<br/>  }<br/>  MSE /= float64(len(testingSet))<br/>  fmt.Printf("RMSE: %v\n", math.Sqrt(MSE))</pre>
<p>With this, now we're really ready to run the regression analysis.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running the regression</h1>
                </header>
            
            <article>
                
<p>Simply run the program. If the program is run with an empty <span class="VerbatimChar">ignored</span> list, the result will show up as a bunch of <span class="VerbatimChar">NaN</span>s. Do you recall that earlier we have done some correlation analysis on how some variables are correlated with one another?</p>
<p>We'll start by adding those into our ignored list, and then run the regression. Once we have a score that is no longer <span class="VerbatimChar">NaN</span>, we can start comparing models.</p>
<p>The final model I have prints the following output:</p>
<pre><strong>R^2: 0.871</strong><br/><strong>  Variable Coefficient StdErr t-stat p-value</strong><br/><strong>  Intercept: 12.38352 0.14768 83.85454 0.00000</strong><br/><strong>  MSSubClass_30: -0.06466 0.02135 -3.02913 0.00412</strong><br/><strong>  MSSubClass_40: -0.03771 0.08537 -0.44172 0.36175</strong><br/><strong>  MSSubClass_45: -0.12998 0.04942 -2.63027 0.01264</strong><br/><strong>  MSSubClass_50: -0.01901 0.01486 -1.27946 0.17590</strong><br/><strong>  MSSubClass_60: -0.06634 0.01061 -6.25069 0.00000</strong><br/><strong>  MSSubClass_70: 0.04089 0.02269 1.80156 0.07878</strong><br/><strong>  MSSubClass_75: 0.04604 0.03838 1.19960 0.19420</strong><br/><strong>  MSSubClass_80: -0.01971 0.02177 -0.90562 0.26462</strong><br/><strong>  MSSubClass_85: -0.02167 0.03838 -0.56458 0.34005</strong><br/><strong>  MSSubClass_90: -0.05748 0.02222 -2.58741 0.01413</strong><br/><strong>  MSSubClass_120: -0.06537 0.01763 -3.70858 0.00043</strong><br/><strong>  MSSubClass_160: -0.15650 0.02135 -7.33109 0.00000</strong><br/><strong>  MSSubClass_180: -0.01552 0.05599 -0.27726 0.38380</strong><br/><strong>  MSSubClass_190: -0.04344 0.02986 -1.45500 0.13840</strong><br/><strong>  LotFrontage: -0.00015 0.00265 -0.05811 0.39818</strong><br/><strong>  LotArea: 0.00799 0.00090 8.83264 0.00000</strong><br/><strong>  Neighborhood_Blueste: 0.02080 0.10451 0.19903 0.39102</strong><br/><strong>  Neighborhood_BrDale: -0.06919 0.04285 -1.61467 0.10835</strong><br/><strong>  Neighborhood_BrkSide: -0.06680 0.02177 -3.06894 0.00365</strong><br/><strong>  Neighborhood_ClearCr: -0.04217 0.03110 -1.35601 0.15904</strong><br/><strong>  Neighborhood_CollgCr: -0.06036 0.01403 -4.30270 0.00004</strong><br/><strong>  Neighborhood_Crawfor: 0.08813 0.02500 3.52515 0.00082</strong><br/><strong>  Neighborhood_Edwards: -0.18718 0.01820 -10.28179 0.00000</strong><br/><strong>  Neighborhood_Gilbert: -0.09673 0.01858 -5.20545 0.00000</strong><br/><strong>  Neighborhood_IDOTRR: -0.18867 0.02825 -6.67878 0.00000</strong><br/><strong>  Neighborhood_MeadowV: -0.24387 0.03971 -6.14163 0.00000</strong><br/><strong>  Neighborhood_Mitchel: -0.15112 0.02348 -6.43650 0.00000</strong><br/><strong>  Neighborhood_NAmes: -0.11880 0.01211 -9.81203 0.00000</strong><br/><strong>  Neighborhood_NPkVill: -0.05093 0.05599 -0.90968 0.26364</strong><br/><strong>  Neighborhood_NWAmes: -0.12200 0.01913 -6.37776 0.00000</strong><br/><strong>  Neighborhood_NoRidge: 0.13126 0.02688 4.88253 0.00000</strong><br/><strong>  Neighborhood_NridgHt: 0.16263 0.01899 8.56507 0.00000</strong><br/><strong>  Neighborhood_OldTown: -0.15781 0.01588 -9.93456 0.00000</strong><br/><strong>  Neighborhood_SWISU: -0.12722 0.03252 -3.91199 0.00020</strong><br/><strong>  Neighborhood_Sawyer: -0.17758 0.02040 -8.70518 0.00000</strong><br/><strong>  Neighborhood_SawyerW: -0.11027 0.02115 -5.21481 0.00000</strong><br/><strong>  Neighborhood_Somerst: 0.05793 0.01845 3.13903 0.00294</strong><br/><strong>  Neighborhood_StoneBr: 0.21206 0.03252 6.52102 0.00000</strong><br/><strong>  Neighborhood_Timber: -0.00449 0.02825 -0.15891 0.39384</strong><br/><strong>  Neighborhood_Veenker: 0.04530 0.04474 1.01249 0.23884</strong><br/><strong>  HouseStyle_1.5Unf: 0.16961 0.04474 3.79130 0.00031</strong><br/><strong>  HouseStyle_1Story: -0.03547 0.00864 -4.10428 0.00009</strong><br/><strong>  HouseStyle_2.5Fin: 0.16478 0.05599 2.94334 0.00531</strong><br/><strong>  HouseStyle_2.5Unf: 0.04816 0.04690 1.02676 0.23539</strong><br/><strong>  HouseStyle_2Story: 0.03271 0.00937 3.49038 0.00093</strong><br/><strong>  HouseStyle_SFoyer: 0.02498 0.02777 0.89968 0.26604</strong><br/><strong>  HouseStyle_SLvl: -0.02233 0.02076 -1.07547 0.22364</strong><br/><strong>  YearBuilt: 0.01403 0.00151 9.28853 0.00000</strong><br/><strong>  YearRemodAdd: 5.06512 0.41586 12.17991 0.00000</strong><br/><strong>  MasVnrArea: 0.00215 0.00164 1.30935 0.16923</strong><br/><strong>  Foundation_CBlock: -0.01183 0.00873 -1.35570 0.15910</strong><br/><strong>  Foundation_PConc: 0.01978 0.00869 2.27607 0.03003</strong><br/><strong>  Foundation_Slab: 0.01795 0.03416 0.52548 0.34738</strong><br/><strong>  Foundation_Stone: 0.03423 0.08537 0.40094 0.36802</strong><br/><strong>  Foundation_Wood: -0.08163 0.08537 -0.95620 0.25245</strong><br/><strong>  BsmtFinSF1: 0.01223 0.00145 8.44620 0.00000</strong><br/><strong>  BsmtFinSF2: -0.00148 0.00236 -0.62695 0.32764</strong><br/><strong>  BsmtUnfSF: -0.00737 0.00229 -3.21186 0.00234</strong><br/><strong>  TotalBsmtSF: 0.02759 0.00375 7.36536 0.00000</strong><br/><strong>  Heating_GasA: 0.02397 0.02825 0.84858 0.27820</strong><br/><strong>  Heating_GasW: 0.06687 0.03838 1.74239 0.08747</strong><br/><strong>  Heating_Grav: -0.15081 0.06044 -2.49506 0.01785</strong><br/><strong>  Heating_OthW: -0.00467 0.10451 -0.04465 0.39845</strong><br/><strong>  Heating_Wall: 0.06265 0.07397 0.84695 0.27858</strong><br/><strong>  CentralAir_Y: 0.10319 0.01752 5.89008 0.00000</strong><br/><strong>  1stFlrSF: 0.01854 0.00071 26.15440 0.00000</strong><br/><strong>  2ndFlrSF: 0.01769 0.00131 13.46733 0.00000</strong><br/><strong>  FullBath: 0.10586 0.01360 7.78368 0.00000</strong><br/><strong>  HalfBath: 0.09048 0.01271 7.11693 0.00000</strong><br/><strong>  Fireplaces: 0.07432 0.01096 6.77947 0.00000</strong><br/><strong>  GarageType_Attchd: -0.37539 0.00884 -42.44613 0.00000</strong><br/><strong>  GarageType_Basment: -0.47446 0.03718 -12.76278 0.00000</strong><br/><strong>  GarageType_BuiltIn: -0.33740 0.01899 -17.76959 0.00000</strong><br/><strong>  GarageType_CarPort: -0.60816 0.06044 -10.06143 0.00000</strong><br/><strong>  GarageType_Detchd: -0.39468 0.00983 -40.16266 0.00000</strong><br/><strong>  GarageType_2Types: -0.54960 0.06619 -8.30394 0.00000</strong><br/><strong>  GarageArea: 0.07987 0.00301 26.56053 0.00000</strong><br/><strong>  PavedDrive_P: 0.01773 0.03046 0.58214 0.33664</strong><br/><strong>  PavedDrive_Y: 0.02663 0.01637 1.62690 0.10623</strong><br/><strong>  WoodDeckSF: 0.00448 0.00166 2.69397 0.01068</strong><br/><strong>  OpenPorchSF: 0.00640 0.00201 3.18224 0.00257</strong><br/><strong>  PoolArea: -0.00075 0.00882 -0.08469 0.39742</strong><br/><strong>  MoSold: 0.00839 0.01020 0.82262 0.28430</strong><br/><strong>  YrSold: -4.27193 6.55001 -0.65220 0.32239</strong><br/><strong>RMSE: 0.1428929042451045</strong></pre>
<p>The cross-validation results (a RMSE of 0.143) are decent—not the best, but not the worst either. This was done through careful elimination of variables. A seasoned econometrician may come into this, read the results, and decide that further feature engineering may be done.</p>
<p>Indeed, looking at these results, off the top of my head I could think of several other feature engineering that could be done—subtracting the year remodeled from the year sold (recency of remodeling/renovations). Another form of feature engineering is to run a PCA-whitening process on the dataset.</p>
<p>For linear regression models, I tend to stay away from complicated feature engineering. This is because the key benefit of a linear regression is that it's explainable in natural language.</p>
<p>For example, we can say this: for every unit increase in lot area size, if everything else is held constant, we can expect a 0.07103 times increment in house price.</p>
<p>A particularly counter intuitive result from this regression is the <span class="VerbatimChar"><kbd>PoolArea</kbd></span> variable. Interpreting the results, we would say: for every unit increase in pool area, we can expect a -0.00075 times increment in price, <em>ceteris paribus</em>. Granted, the p-value of the coefficient is 0.397, meaning that this coefficient could have been gotten by sheer random chance. Hence, we must be quite careful in saying this—having a pool decreases the value of your property in Ames, Massachusetts.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Discussion and further work</h1>
                </header>
            
            <article>
                
<p>This model is now ready to be used to predict things. Is this the best model? No, it's not. Finding the best model is a never ending quest. To be sure, there are indefinite ways of improving this model. One can use LASSO methods to determine the importance of variables before using them.</p>
<p>The model is not only the linear regression, but also the data cleaning functions and ingestion functions that come with it. This leads to a very high number of tweakable parameters. Maybe if you didn't like the way I imputed data, you can always write your own method!</p>
<p>Furthermore the code in this chapter can be cleaned up further. Instead of returning so many values in the <span class="VerbatimChar">clean</span> function, a new tuple type can be created to hold the Xs and Ys—a data frame of sorts. In fact, that's what we're going to build in the upcoming chapters. Several functions can be made more efficient using a state-holder struct.</p>
<p>If you will note, there are not very many statistical packages like P<span class="VerbatimChar">andas</span> for Go. This is not for the lack of trying. Go as a language is all about solving problems, not about building generic packages. There are definitely dataframe-like packages in Go, but in my experience, using them tends to blind one to the most obvious and efficient solutions. Often, it's better to build your own data structures that are specific to the problem at hand.</p>
<p>For the most part in Go, the model building is an iterative process, while productionizing the model is a process that happens after the model has been built. This chapter shows that with a little awkwardness, it is possible to build a model using an iterative process that immediately translates to a production-ready system.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have learned how to explore data (with some awkwardness) using Go. We plotted some charts and used them as a guiding rod to select variables for the regression. Following that, we implemented a regression model that came with reporting of errors which enabled us to compare models. Lastly, to ensure we were not over fitting, we used a RMSE score to cross-validate our model and came out with a fairly decent score. </p>
<p>This is just a taste of what is to come. The ideas in abstract are repeated over the next chapters—we will be cleaning data, then writing the machine learning model, which will be cross-validated. The only difference will generally be the data, and the models. </p>
<p>In the next chapter, we'll learn a simple way to determine if an email is spam or not.</p>


            </article>

            
        </section>
    </div>



  </body></html>