- en: Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the majority of machine learning problems involve labeled data, as we
    saw in the previous chapter, there is another important branch called **unsupervised
    learning**. This applies in situations where you may not have labels for the input
    data, and so the algorithm cannot work by trying to predict output labels from
    each input. Instead, unsupervised algorithms work by trying to spot patterns or
    structure in the input. It can be a useful technique when carrying out exploratory
    analysis on a large dataset with many different input variables. In this situation,
    it would be incredibly time-consuming to plot charts of all the different variables
    to try to spot patterns, so instead, unsupervised learning can be used to do this
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'As humans, we are very familiar with this concept: much of what we do is never
    explicitly taught to us by someone else. Instead, we explore the world around
    us, looking for, and discovering, patterns. For this reason, unsupervised learning
    is of particular interest to researchers who are trying to develop systems for **general
    intelligence**: computers that can learn what they need independently^([1]).'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to introduce two popular unsupervised algorithms
    and implement them in Go. First, we will use a **clustering algorithm** to separate
    a dataset into distinct groups without any guidance about what to look for. Then,
    we will use a technique called **principal component analysis** to compress a
    dataset by first finding hidden structures within it.
  prefs: []
  type: TYPE_NORMAL
- en: This will just scratch the surface of what unsupervised learning is able to
    achieve. Some cutting-edge algorithms are able to allow computers to carry out
    tasks that normally require human creativity. One example worth looking at is
    NVIDIA's system for creating realistic pictures from sketches^([2]). You can also
    find code examples online for networks that can make realistic changes to how
    an image appears, for instance, turning horses into zebras, or oranges into apples^([3]).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering algorithms are designed to split a dataset up into groups. Once trained,
    any new data can be assigned to a group when it arrives. Suppose you are working
    with a dataset of customer information for an e-commerce store. You might use
    clustering to identify groups of customers, for example, business/private customers.
    This information can then be used to make decisions about how to best serve those
    customer types.
  prefs: []
  type: TYPE_NORMAL
- en: You might also use clustering as a preparatory step before applying supervised
    learning. For example, a dataset of images may require manual labeling, which
    is often time-consuming and costly. If you can segment the dataset into groups
    with a clustering algorithm, then you may be able to save time by only labeling
    a fraction of the images, and then assuming that each cluster contains images
    with the same label.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering has also been applied to computer vision applications in autonomous
    vehicles, where it can be used to help a vehicle navigate on an unknown stretch
    of road. By clustering the data from the vehicles cameras, it is possible to identify
    which area of each incoming image contains the road on which the vehicle must
    drive^([4]).
  prefs: []
  type: TYPE_NORMAL
- en: 'For our example, we are going to use a dataset containing measurements of different
    types of iris flower, which you can download using the `./download-iris.sh` script
    in the code repository. This data is often used to demonstrate supervised learning:
    you can use machine learning to classify the data according to the species of
    iris flower. In this case, however, we will not provide labels to the clustering
    algorithm, meaning that it has to identify clusters purely from the measurement
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, load the data into Go, as we have done in previous examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to prepare the data by splitting the species column from the
    rest of the data: this will only be used for the final assessment of the groups
    after clustering. To do this, use the `DataFrameToXYs` func from previous examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can train an algorithm called **k-means** to try to split the dataset
    into three clusters. k-means works by initially choosing the middle (known as
    the **centroid**) of each cluster at random, and assigning each data point in
    the training set to its nearest centroid. It then iteratively updates the positions
    of each cluster, reassigning the data points at each step until it reaches convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**k-means** is a simple algorithm and is fast to train, so it is a good starting
    point when clustering data. However, it does require you to specify how many clusters
    to find, which is not always obvious. Other clustering algorithms, such as DBSCAN,
    do not have this limitation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the k-means implementation in goml, we can try to find three clusters
    within the data. Often, you may need to use trial and error to find out how many
    clusters to use—K. If you have lots of very small clusters after running k-means,
    then you probably need to reduce K:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have fitted the model to the data, we can generate a prediction from
    it; that is, find out which cluster each data point belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are able to plot the clusters using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'What this does is display the data using two of the input features, `Sepal
    width` and `Sepal length`, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f936c844-bdf7-4f9f-87a7-0531cad7c1e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The shape of each point is set according to the iris species, while the color
    is set by the output of k-means, that is, which cluster the algorithm has assigned
    each data point to. What we can now see is that the clusters match the species
    of each iris almost exactly: k-means has been able to subdivide the data into
    three distinct groups that correspond to the different species.'
  prefs: []
  type: TYPE_NORMAL
- en: While k-means works very well in this case, you might find that you need to
    use a different algorithm on your own datasets. The scikit-learn library for Python
    comes with a useful demonstration of which algorithms work best on different types
    of dataset^([5]). You might also find that it is helpful to prepare your data
    in some way; for example, normalize it or apply a non-linear transformation to
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) is a way to reduce the number of
    dimensions in a dataset. We can think of it as a way of compressing a dataset.
    Suppose you have 100 different variables in your dataset. It may be the case that
    many of these variables are correlated with each other. If this is the case, then
    it is possible to explain most of the variation in the data by combining variables
    to build a smaller set of data. PCA performs this task: it tries to find linear
    combinations of your input variables, and reports how much variation is explained
    by each combination.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA is a method for reducing the dimensions in a dataset: in effect, summarizing
    it so that you can focus on the most important features, which explain most of
    the variation in the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA can be useful for machine learning in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: It can be a useful preprocessing step before applying a supervised learning
    method. After running PCA on your data, you may discover, for instance, that 95%
    of the variation is explained by only a handful of variables. You can use this
    knowledge to reduce the number of variables in your input data, which means that
    your subsequent model will train much faster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can also be helpful when visualizing a dataset prior to building a model.
    If your data has more than three variables, it can be very hard to visualize it
    on a graph and understand what patterns it contains. PCA lets you transform the
    data so that you can plot just the most important aspects of it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our example, we are going to use PCA to visualize the iris dataset. Currently,
    this has four input features: petal width, petal length, sepal width, and sepal
    length. Using PCA, we can reduce this down to two variables, which we can then
    visualize easily on a scatter plot.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by loading the sepal data as before, and normalizing it as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to convert the data into a matrix format. The `gonum` library
    has a `mat64` type that we can use for this purpose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: PCA works by finding the **eigenvectors** and **eigenvalues** of the dataset.
    For this reason, most software libraries need the data to be in a matrix structure,
    so that standard linear algebra routines such as **blas** and **lapack** can be
    used to do the calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can make use of gonum''s `stat` package for the PCA implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us two variables: `components`, which is a matrix telling us how
    to map the original variables to the new components; and `variances`, which tells
    us how much variance is explained by each component. If we print out the variance
    in each component, we can see that the first two explain 96% of the entire dataset
    (component 1 to 73%, and component 2 to 23%):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can transform the data into the new components, and keep the first
    two so that we can use them for visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows each data point according to the first two principle
    components, while the colors indicate which iris species each one belongs to.
    We can now see that the three groups form distinct bands along the first component,
    which we could not have easily seen when plotting the four original input features
    against one another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e4e724c-c63c-4871-95ce-1d20bbf7ac30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You could now try training a supervised learning model to use the first two
    PCA features to predict the iris species: compare its performance against a model
    trained on all four input features.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered two common techniques in unsupervised machine
    learning. Both are often used by data scientists for exploratory analysis, but
    can also form part of a data processing pipeline in a production system. You have
    learned how to train a clustering algorithm to divide data automatically into
    groups. This technique might be used to categorize newly registered customers
    on an e-commerce website, so that they can be served with personalized information.
    We also introduced principal component analysis as a means of compressing data,
    in other words, reducing its dimensionality. This may be used as a preprocessing
    step before running a supervised learning technique in order to reduce the size
    of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, it is possible to make use of the `gonum` and `goml` libraries
    to build efficient implementations in Go with minimal code.
  prefs: []
  type: TYPE_NORMAL
- en: Further readings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://deepmind.com/blog/unsupervised-learning/](https://deepmind.com/blog/unsupervised-learning/).
    Retrieved April 12, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://blogs.nvidia.com/blog/2019/03/18/gaugan-photorealistic-landscapes-nvidia-research/](https://blogs.nvidia.com/blog/2019/03/18/gaugan-photorealistic-landscapes-nvidia-research/).
    Retrieved April 12, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://github.com/junyanz/CycleGAN](https://github.com/junyanz/CycleGAN).
    Retrieved April 12, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[http://robots.stanford.edu/papers/dahlkamp.adaptvision06.pdf](http://robots.stanford.edu/papers/dahlkamp.adaptvision06.pdf).
    Retrieved April 13, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods](https://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods).
    Retrieved April 12, 2019.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
