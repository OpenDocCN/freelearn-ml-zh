<html><head></head><body>
<div id="_idContainer009">
<h1 class="chapter-number" id="_idParaDest-16"><a id="_idTextAnchor014"/><span class="koboSpan" id="kobo.1.1">1</span></h1>
<h1 id="_idParaDest-17"><a id="_idTextAnchor015"/><span class="koboSpan" id="kobo.2.1">Introducing Conformal Prediction</span></h1>
<p><span class="koboSpan" id="kobo.3.1">This book is about conformal prediction, a modern framework for uncertainty quantification that is becoming increasingly popular in industry </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">and academia.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Machine learning and AI applications are everywhere. </span><span class="koboSpan" id="kobo.5.2">In the realm of machine learning, prediction is a fundamental task. </span><span class="koboSpan" id="kobo.5.3">Given a training dataset, we train a machine learning model to make predictions on </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">new data.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer008">
<span class="koboSpan" id="kobo.7.1"><img alt="Figure 1.1 – Machine learning prediction model" src="image/B19925_01_001.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.8.1">Figure 1.1 – Machine learning prediction model</span></p>
<p><span class="koboSpan" id="kobo.9.1">However, in many real-world applications, the predictions made by statistical, machine learning, and deep learning models are often incorrect or unreliable because of various factors, such as insufficient or incomplete data, issues arising during the modeling process, or simply because of the randomness and complexities of the </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">underlying problem.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">Predictions made by machine learning models often come without the uncertainty quantification required for confident and reliable decision-making. </span><span class="koboSpan" id="kobo.11.2">This is where conformal prediction comes in. </span><span class="koboSpan" id="kobo.11.3">By providing a clear measure of the reliability of its predictions, conformal prediction enhances the trustworthiness and explainability of machine learning models, making them more transparent and user-friendly </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">for decision-makers.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">This chapter will introduce conformal prediction and explore how it can be applied in </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">practical settings.</span></span></p>
<p><span class="koboSpan" id="kobo.15.1">In this chapter, we’re going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.17.1">Introduction to </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">conformal prediction</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">The origins of </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">conformal prediction</span></span></li>
<li><span class="koboSpan" id="kobo.21.1">How conformal prediction differs from traditional </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">machine learning</span></span></li>
<li><span class="koboSpan" id="kobo.23.1">The p-value and its role in </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">conformal prediction</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.25.1">The chapter will provide a practical understanding of conformal prediction and its applications. </span><span class="koboSpan" id="kobo.25.2">By the end of this chapter, you will be able to understand how conformal prediction can be applied to your own machine learning models to improve their reliability </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">and interpretability.</span></span></p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor016"/><span class="koboSpan" id="kobo.27.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.28.1">This book uses Python. </span><span class="koboSpan" id="kobo.28.2">The code for this book is hosted on GitHub and can be found here: </span><a href="https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction"><span class="koboSpan" id="kobo.29.1">https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction</span></a><span class="koboSpan" id="kobo.30.1"> You can run notebooks locally or upload them to Google </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">Colab (</span></span><a href="https://colab.research.google.com/"><span class="No-Break"><span class="koboSpan" id="kobo.32.1">https://colab.research.google.com/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.33.1">).</span></span></p>
<h1 id="_idParaDest-19"><a id="_idTextAnchor017"/><span class="koboSpan" id="kobo.34.1">Introduction to conformal prediction</span></h1>
<p><span class="koboSpan" id="kobo.35.1">In this section, we will introduce conformal prediction and explain how it can be used to improve the reliability </span><a id="_idIndexMarker000"/><span class="koboSpan" id="kobo.36.1">of predictions produced by statistical, machine learning, and deep learning models. </span><span class="koboSpan" id="kobo.36.2">We will provide an overview of the key ideas and concepts behind conformal prediction, including its underlying principles and benefits. </span><span class="koboSpan" id="kobo.36.3">By the end of this section, you will have a solid understanding of conformal prediction and why it is an important framework </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">to know.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">Conformal prediction is a powerful machine learning framework that provides valid confidence measures for individual predictions. </span><span class="koboSpan" id="kobo.38.2">This means that when you make a prediction using any model from the conformal prediction framework, you can also measure your confidence in </span><span class="No-Break"><span class="koboSpan" id="kobo.39.1">that prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.40.1">This is incredibly useful in many practical applications where it is crucial to have reliable and interpretable predictions. </span><span class="koboSpan" id="kobo.40.2">For example, in medical diagnosis, conformal prediction can provide a confidence level that a tumor is malignant versus benign. </span><span class="koboSpan" id="kobo.40.3">This enables physicians to make more informed treatment decisions based on the prediction confidence. </span><span class="koboSpan" id="kobo.40.4">In finance, conformal </span><a id="_idIndexMarker001"/><span class="koboSpan" id="kobo.41.1">prediction can provide prediction intervals estimating financial risk. </span><span class="koboSpan" id="kobo.41.2">This allows investors to quantify upside and </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">downside risks.</span></span></p>
<p><span class="koboSpan" id="kobo.43.1">Specifically, conformal prediction can determine a 95% chance a tumor is malignant, giving physicians high confidence in a cancer diagnosis. </span><span class="koboSpan" id="kobo.43.2">Or, it can predict an 80% probability that a stock price will fall between $50 and $60 next month, providing an estimated trading range. </span><span class="koboSpan" id="kobo.43.3">Conformal prediction increases trust and is valuable in real-world applications such as medical diagnosis and financial forecasting by delivering quantifiable confidence </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">in predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.45.1">The key benefit of conformal prediction is that it provides valid confidence measures for individual predictions. </span><span class="koboSpan" id="kobo.45.2">A conformal prediction model usually provides a prediction in the form of a prediction interval or prediction set with a specified confidence level, for example, 95%. </span><span class="koboSpan" id="kobo.45.3">In classification problems, conformal prediction can also calibrate class probabilities, enhancing confidence and </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">informed decision-making.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">In conformal prediction, “coverage” denotes the likelihood that the predicted region – whether a set of potential outcomes in classification tasks or a prediction interval in regression tasks – accurately encompasses the true values. </span><span class="koboSpan" id="kobo.47.2">Essentially, if you choose a coverage of 95%, it means there’s a 95% chance that the true values fall within the provided prediction set </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">or interval.</span></span></p>
<p><span class="koboSpan" id="kobo.49.1">We call such prediction regions “valid.” </span><span class="koboSpan" id="kobo.49.2">The requirement for the validity of predictions is crucial to ensure that the model does not contain prediction bias and is especially important in consequential applications such as health, finance, and self-driving cars. </span><span class="koboSpan" id="kobo.49.3">Valid predictions are a prerequisite of trust in the machine learning model that has produced </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">this prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">While there are alternative approaches to uncertainty quantification, such as Bayesian methods, Monte Carlo methods, and bootstrapping, to provide validity guarantees, such approaches require distribution-specific assumptions about the data – for example, an assumption that the data follows a normal distribution. </span><span class="koboSpan" id="kobo.51.2">However, the true underlying distribution of real-world data is generally unknown. </span><span class="koboSpan" id="kobo.51.3">Conversely, conformal prediction does not make distributional assumptions and can provide validity guarantees without making assumptions about the specifics of data distribution. </span><span class="koboSpan" id="kobo.51.4">This makes conformal prediction more broadly applicable to real data that may not satisfy common statistical assumptions such as normality, smoothness, and </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.53.1">In practice, the need for </span><a id="_idIndexMarker002"/><span class="koboSpan" id="kobo.54.1">distribution-specific assumptions limits the ability of methods such as Bayesian inference or bootstrapping to make formally rigorous statements about arbitrary real data sources. </span><span class="koboSpan" id="kobo.54.2">There is no guarantee that predictions from such methods will have the claimed confidence level or coverage across all data types, since the assumptions may not hold. </span><span class="koboSpan" id="kobo.54.3">This can create a mismatch between the confidence level communicated to users and the actual coverage achieved, leading to inaccurate decisions and misleading users about the reliability of </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">model predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.56.1">Conformal prediction sidesteps these issues by providing distribution-free finite sample validity guarantees without relying on hard-to-verify distributional assumptions about the data. </span><span class="koboSpan" id="kobo.56.2">This makes conformal prediction confidence estimates more trustworthy and robust for </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">real-world applications.</span></span></p>
<p><span class="koboSpan" id="kobo.58.1">Conformal prediction has </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">multiple benefits:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.60.1">Guaranteed coverage</span></strong><span class="koboSpan" id="kobo.61.1">: Conformal prediction guarantees the validity of prediction regions</span><a id="_idIndexMarker003"/><span class="koboSpan" id="kobo.62.1"> automatically. </span><span class="koboSpan" id="kobo.62.2">Any method from the conformal prediction framework guarantees the validity of prediction regions by mathematical design. </span><span class="koboSpan" id="kobo.62.3">In comparison, alternative methods output predictions that do not provide any validity guarantees. </span><span class="koboSpan" id="kobo.62.4">By way of an example, the popular NGBoost package does not produce valid prediction intervals (you can read more about it at the following </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">link: </span></span><a href="https://medium.com/@valeman/does-ngboost-work-evaluating-ngboost-against-critical-criteria-for-good-probabilistic-prediction-28c4871c1bab"><span class="No-Break"><span class="koboSpan" id="kobo.64.1">https://medium.com/@valeman/does-ngboost-work-evaluating-ngboost-against-critical-criteria-for-good-probabilistic-prediction-28c4871c1bab</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.65.1">).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.66.1">Distribution-free</span></strong><span class="koboSpan" id="kobo.67.1">: Conformal prediction is distribution-free and can be applied to any data distribution regardless of the properties of the distribution as long as the data is exchangeable. </span><span class="koboSpan" id="kobo.67.2">Exchangeability means that the order or index of the data points does not matter – shuffling or permuting the data points will not change the overall data distribution. </span><span class="koboSpan" id="kobo.67.3">For example, exchangeability assumes that observation 1, 2, 3 has the same distribution as observation 2, 3, 1 or 3, 1, 2. </span><span class="koboSpan" id="kobo.67.4">This is a weaker assumption than IID and is required to provide validity guarantees. </span><span class="koboSpan" id="kobo.67.5">Unlike many classical statistical models, conformal prediction does not make assumptions such as the data following a normal distribution. </span><span class="koboSpan" id="kobo.67.6">The data can have any distribution, even with irregularities such as fat tails. </span><span class="koboSpan" id="kobo.67.7">The only requirement is exchangeability. </span><span class="koboSpan" id="kobo.67.8">By relying only on exchangeability rather than strict distributional assumptions, conformal prediction provides finite sample guarantees on </span><a id="_idIndexMarker004"/><span class="koboSpan" id="kobo.68.1">prediction coverage that are distribution-free and applicable to any </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">data source.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.70.1">Model-agnostic</span></strong><span class="koboSpan" id="kobo.71.1">: Conformal prediction can be applied to any prediction model that produces point predictions in classification, regression, time series, computer vision, NLP, reinforcement learning, or other statistical, machine learning, and deep learning tasks. </span><span class="koboSpan" id="kobo.71.2">Conformal prediction has been successfully applied to many innovative model types, including recent innovations such as diffusion models and </span><strong class="bold"><span class="koboSpan" id="kobo.72.1">large language models</span></strong><span class="koboSpan" id="kobo.73.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.74.1">LLMs</span></strong><span class="koboSpan" id="kobo.75.1">). </span><span class="koboSpan" id="kobo.75.2">Conformal prediction does not require the model to be</span><a id="_idIndexMarker005"/><span class="koboSpan" id="kobo.76.1"> statistical, machine learning, or deep learning. </span><span class="koboSpan" id="kobo.76.2">It could be any model of any type, for example, a business heuristic developed by domain experts. </span><span class="koboSpan" id="kobo.76.3">If you have a model to make point predictions, you can use conformal prediction as an uncertainty quantification layer on top of your point prediction model to obtain a well-calibrated, reliable, and safe probabilistic </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">prediction model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.78.1">Non-intrusive</span></strong><span class="koboSpan" id="kobo.79.1">: Conformal prediction stands out in its simplicity and efficiency. </span><span class="koboSpan" id="kobo.79.2">Rather than overhauling your existing point prediction model, it seamlessly integrates with it. </span><span class="koboSpan" id="kobo.79.3">For businesses with established models already in production, this is a game changer. </span><span class="koboSpan" id="kobo.79.4">And for data scientists, the process is even more exciting. </span><span class="koboSpan" id="kobo.79.5">Simply overlay your point prediction model with the uncertainty quantification layer provided by conformal prediction, and you’re equipped with a state-of-the-art probabilistic </span><span class="No-Break"><span class="koboSpan" id="kobo.80.1">prediction model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.81.1">Dataset size</span></strong><span class="koboSpan" id="kobo.82.1">: Conformal prediction stands apart from typical statistical methods that depend on stringent data distribution assumptions, such as normality, or need vast datasets for solid guarantees. </span><span class="koboSpan" id="kobo.82.2">It offers inherent mathematical assurances of valid predictions without bias, irrespective of the dataset’s size. </span><span class="koboSpan" id="kobo.82.3">While smaller datasets may yield broader prediction intervals in regression tasks (or larger sets in classification), conformal prediction remains consistently valid. </span><span class="koboSpan" id="kobo.82.4">The validity is assured no matter the dataset size, underlying prediction model, or data distribution, making it a unique and unmatched method for </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">uncertainty quantification.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.84.1">Easy to use</span></strong><span class="koboSpan" id="kobo.85.1">: A few years back, the adoption of conformal prediction was limited due to the scarcity of open source libraries, even though esteemed universities and major corporations such as Microsoft had been utilizing it for years. </span><span class="koboSpan" id="kobo.85.2">Fast forward to today, and the landscape has dramatically shifted. </span><span class="koboSpan" id="kobo.85.3">There’s a rich selection of top-tier Python packages such as MAPIE and Amazon Fortuna, among others. </span><span class="koboSpan" id="kobo.85.4">This</span><a id="_idIndexMarker006"/><span class="koboSpan" id="kobo.86.1"> means that generating well-calibrated probabilistic predictions via conformal prediction is just a few lines of code away, making it straightforward to integrate into business applications. </span><span class="koboSpan" id="kobo.86.2">Furthermore, platforms such as KNIME have democratized its use, offering conformal prediction through low-code or </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">no-code solutions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.88.1">Fast</span></strong><span class="koboSpan" id="kobo.89.1">: The most widely embraced conformal prediction variant, inductive conformal prediction, stands out because it operates efficiently without the need to retrain the foundational model. </span><span class="koboSpan" id="kobo.89.2">In contrast, other methods, such as Bayesian networks, often necessitate retraining. </span><span class="koboSpan" id="kobo.89.3">This distinction means that inductive conformal prediction offers a streamlined approach, eliminating the time and computational costs associated with repeated </span><span class="No-Break"><span class="koboSpan" id="kobo.90.1">model retraining.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.91.1">Non-intrusive</span></strong><span class="koboSpan" id="kobo.92.1">: Unlike many uncertainty quantification techniques, conformal prediction seamlessly integrates without altering the underlying point prediction models. </span><span class="koboSpan" id="kobo.92.2">Its non-invasive nature is cost-effective and convenient, especially compared to other methods that demand potentially costly and complex adjustments to the machine or deep learning models. </span><span class="koboSpan" id="kobo.92.3">The benefits of using conformal prediction are truly incredible. </span><span class="koboSpan" id="kobo.92.4">You might be interested to know how conformal prediction achieves the unique and powerful benefits that it offers to </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">its users.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.94.1">The key objective of conformal prediction is to provide valid confidence measures that adapt based on the difficulty of making individual predictions. </span><span class="koboSpan" id="kobo.94.2">Conformal prediction uses “nonconformity measures” to </span><a id="_idIndexMarker007"/><span class="koboSpan" id="kobo.95.1">assess how well new observations fit with </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">previous observations.</span></span></p>
<p><span class="koboSpan" id="kobo.97.1">The overall workflow </span><a id="_idIndexMarker008"/><span class="koboSpan" id="kobo.98.1">consists of the </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.100.1">A conformal predictor learns from past training examples to quantify uncertainty around predictions for the </span><span class="No-Break"><span class="koboSpan" id="kobo.101.1">new observations.</span></span></li>
<li><span class="koboSpan" id="kobo.102.1">When quantifying uncertainty around predictions for the new observations, it calculates nonconformity scores, measuring how different or “nonconforming” the new observation is, compared to the training set (in the classical transductive version of conformal prediction) or calibration (in the most popular variant of conformal prediction – inductive </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">conformal prediction).</span></span></li>
<li><span class="koboSpan" id="kobo.104.1">These nonconformity scores are used to determine whether the new observation falls within the range of values expected based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">training data.</span></span></li>
<li><span class="koboSpan" id="kobo.106.1">The model calculates personalized confidence measures and prediction sets (in classification problems) or prediction intervals (in regression problems and time series forecasting) for </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">each prediction.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.108.1">The magic of conformal prediction lies in these nonconformity measures, which allow the model to evaluate each new prediction in the context of the previously seen data. </span><span class="koboSpan" id="kobo.108.2">This simple but powerful approach results in finite sample coverage guarantees adapted to the intrinsic difficulty of making a given prediction. </span><span class="koboSpan" id="kobo.108.3">The validity holds for any data distribution, prediction algorithm, or </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">dataset size.</span></span></p>
<p><span class="koboSpan" id="kobo.110.1">In this book, we will talk interchangeably about nonconformity and conformity measures; one is the inverse of the other, and depending on the application, it will be more convenient to use either conformity or </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">nonconformity measures.</span></span></p>
<h2 id="_idParaDest-20"><a id="_idTextAnchor018"/><span class="koboSpan" id="kobo.112.1">Understanding conformity measures</span></h2>
<p><span class="koboSpan" id="kobo.113.1">A conformity measure is a</span><a id="_idIndexMarker009"/><span class="koboSpan" id="kobo.114.1"> critical component of conformal prediction and is </span><a id="_idIndexMarker010"/><span class="koboSpan" id="kobo.115.1">essentially a function that assigns a numerical score (conformity score) to each object in a dataset. </span><span class="koboSpan" id="kobo.115.2">The conformity score indicates how well a new observation fits the observed data. </span><span class="koboSpan" id="kobo.115.3">When making a new prediction, we can use the conformity measure to calculate a conformity score for the new observation and compare it to the conformity scores of the previous observations. </span><span class="koboSpan" id="kobo.115.4">Based on this comparison, we can calculate a measure of confidence for our prediction. </span><span class="koboSpan" id="kobo.115.5">The conformity score indicates a degree of confidence in </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">the prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.117.1">The choice of</span><a id="_idIndexMarker011"/><span class="koboSpan" id="kobo.118.1"> conformity measure is a key step in conformal prediction. </span><span class="koboSpan" id="kobo.118.2">The conformity measure determines how we assess how similar new observations are to past examples. </span><span class="koboSpan" id="kobo.118.3">There are many options for defining conformity measures depending on </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">the problem.</span></span></p>
<p><span class="koboSpan" id="kobo.120.1">In a classification setting, a simple conformity measure could calculate the probability scores assigned to each class by the prediction model for a new observation. </span><span class="koboSpan" id="kobo.120.2">The class with the highest probability would have the best conformity or match to the </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">training data.</span></span></p>
<p><span class="koboSpan" id="kobo.122.1">The key advantage of conformal prediction is that we obtain valid prediction regions regardless of the conformity measure used. </span><span class="koboSpan" id="kobo.122.2">This is because conformal prediction relies only on the order induced by the conformity measure rather than its </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">exact form.</span></span></p>
<p><span class="koboSpan" id="kobo.124.1">So, we have the flexibility to incorporate domain knowledge in designing an appropriate conformity measure for the problem at hand. </span><span class="koboSpan" id="kobo.124.2">If the measure ranks how well new observations match past data, conformal prediction can be used to deliver finite sample </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">coverage guarantees.</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">While all conformal predictors provide valid prediction regions, the choice of conformity measure impacts their efficiency. </span><span class="koboSpan" id="kobo.126.2">Efficiency relates to the width of the prediction intervals or sets – narrower intervals contain more valuable information </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">for decision-making.</span></span></p>
<p><span class="koboSpan" id="kobo.128.1">Though validity holds for any conformity measure, thoughtfully choosing one tailored to the application can improve efficiency and produce narrower, more useful prediction intervals. </span><span class="koboSpan" id="kobo.128.2">The intervals should also be adaptable based on the model’s uncertainty – expanding for difficult predictions and contracting for </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">clear ones.</span></span></p>
<p><span class="koboSpan" id="kobo.130.1">Let’s illustrate this with an example. </span><span class="koboSpan" id="kobo.130.2">Say we have a dataset of patients diagnosed with a disease, with features such as age, gender, and test results. </span><span class="koboSpan" id="kobo.130.3">We want to predict whether new patients are </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">at risk.</span></span></p>
<p><span class="koboSpan" id="kobo.132.1">A simple conformity measure could calculate how similar the feature values are between new patients and those in the training data. </span><span class="koboSpan" id="kobo.132.2">New patients very different from the data would get low conformity scores and wide prediction intervals, indicating high uncertainty. </span><span class="koboSpan" id="kobo.132.3">While this conformity measure would produce valid intervals, we can improve efficiency with a more </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">tailored approach.</span></span></p>
<p><span class="koboSpan" id="kobo.134.1">By carefully selecting </span><a id="_idIndexMarker012"/><span class="koboSpan" id="kobo.135.1">conformity measures aligned to our prediction problem and domain knowledge, we can obtain high-quality conformal predictors that provide both validity </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">and efficiency.</span></span></p>
<p><span class="koboSpan" id="kobo.137.1">We will now talk briefly about the origins of </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">conformal prediction.</span></span></p>
<h1 id="_idParaDest-21"><a id="_idTextAnchor019"/><span class="koboSpan" id="kobo.139.1">The origins of conformal prediction</span></h1>
<p><span class="koboSpan" id="kobo.140.1">The origins of conformal </span><a id="_idIndexMarker013"/><span class="koboSpan" id="kobo.141.1">prediction are documented in </span><em class="italic"><span class="koboSpan" id="kobo.142.1">Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification</span></em><span class="koboSpan" id="kobo.143.1"> by Anastasios N. </span><span class="koboSpan" id="kobo.143.2">Angelopoulos and Stephen </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">Bates (</span></span><a href="https://arxiv.org/abs/2107.07511"><span class="No-Break"><span class="koboSpan" id="kobo.145.1">https://arxiv.org/abs/2107.07511</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.146.1">).</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.147.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.148.1">Conformal prediction was invented by my PhD supervisor Prof. </span><span class="koboSpan" id="kobo.148.2">Vladimir Vovk, a professor at Royal Holloway University of London. </span><span class="koboSpan" id="kobo.148.3">Vladimir Vovk graduated from Moscow State University, where he studied mathematics and became a student of one of the most notable mathematicians of the 20th century, Andrey Kolmogorov. </span><span class="koboSpan" id="kobo.148.4">During this time, initial ideas that later gave rise to the invention of conformal </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">prediction appeared.</span></span></p>
<p class="callout"><span class="koboSpan" id="kobo.150.1">The first edition of </span><em class="italic"><span class="koboSpan" id="kobo.151.1">Algorithmic Learning in a Random World</span></em><span class="koboSpan" id="kobo.152.1"> (</span><a href="https://link.springer.com/book/10.1007/b106715"><span class="koboSpan" id="kobo.153.1">https://link.springer.com/book/10.1007/b106715</span></a><span class="koboSpan" id="kobo.154.1">) by Vladimir Vovk, Alexander Gammerman, and Glenn Shafer was published in 2005. </span><span class="koboSpan" id="kobo.154.2">The second edition of the book was published in </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">2022 (</span></span><a href="https://link.springer.com/book/10.1007/978-3-031-06649-8"><span class="No-Break"><span class="koboSpan" id="kobo.156.1">https://link.springer.com/book/10.1007/978-3-031-06649-8</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.157.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.158.1">Conformal prediction was popularized in United States academia by Professor Larry Wasserman (Carnegie Mellon) and his collaborators, who have published some key papers and introduced conformal prediction to many other researchers in the </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">United States.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.160.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.161.1">In 2022, I finished my PhD in machine learning. </span><span class="koboSpan" id="kobo.161.2">In the same year, I created </span><em class="italic"><span class="koboSpan" id="kobo.162.1">Awesome Conformal Prediction</span></em><span class="koboSpan" id="kobo.163.1"> (</span><a href="https://github.com/valeman/awesome-conformal-prediction"><span class="koboSpan" id="kobo.164.1">https://github.com/valeman/awesome-conformal-prediction</span></a><span class="koboSpan" id="kobo.165.1">) – the most comprehensive professionally curated resource on conformal prediction, which has since received thousands of </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">GitHub stars.</span></span></p>
<p><span class="koboSpan" id="kobo.167.1">Conformal prediction </span><a id="_idIndexMarker014"/><span class="koboSpan" id="kobo.168.1">has grown rapidly from a niche research area into a mainstream framework for uncertainty quantification. </span><span class="koboSpan" id="kobo.168.2">The field has exploded in recent years, with over 1,000 research papers on conformal prediction estimated to be published in </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">2023 alone.</span></span></p>
<p><span class="koboSpan" id="kobo.170.1">This surge of research reflects the increasing popularity and applicability of conformal prediction across academia and industry. </span><span class="koboSpan" id="kobo.170.2">Major technology companies such as Microsoft, Amazon, DeepMind, and NVIDIA now conduct research into and apply conformal prediction. </span><span class="koboSpan" id="kobo.170.3">The framework has also been adopted in high-stakes domains such as healthcare and finance, where validity and reliability </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">are critical.</span></span></p>
<p><span class="koboSpan" id="kobo.172.1">In just over two decades since its introduction, conformal prediction has cemented itself as one of the premier and most trusted approaches for uncertainty quantification in machine learning. </span><span class="koboSpan" id="kobo.172.2">The field will continue to expand as more practitioners recognize the value of conformal prediction’s finite sample guarantees compared to traditional statistical methods reliant on asymptotic theory and unverifiable distributional assumptions. </span><span class="koboSpan" id="kobo.172.3">With growing research and adoption, conformal prediction is poised to become a standard tool for any application requiring rigorous uncertainty estimates alongside </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">point predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.174.1">At </span><em class="italic"><span class="koboSpan" id="kobo.175.1">NeurIPS 2022</span></em><span class="koboSpan" id="kobo.176.1">, one of the prominent mathematicians of our time, Emmanuel Candes (Stanford), delivered a key invited talk titled </span><em class="italic"><span class="koboSpan" id="kobo.177.1">Conformal Prediction in 2022</span></em><span class="koboSpan" id="kobo.178.1"> (</span><a href="https://slideslive.com/38996063/conformal-prediction-in-2022?ref=speaker-43789"><span class="koboSpan" id="kobo.179.1">https://slideslive.com/38996063/conformal-prediction-in-2022?ref=speaker-43789</span></a><span class="koboSpan" id="kobo.180.1">) to tens of thousands of attendees. </span><span class="koboSpan" id="kobo.180.2">In his talk, Emmanuel </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">Candes said:</span></span></p>
<p class="author-quote"><span class="koboSpan" id="kobo.182.1">Conformal inference methods are becoming all the rage in academia and industry alike. </span><span class="koboSpan" id="kobo.182.2">In a nutshell, these methods deliver exact prediction intervals for future observations without making any distributional assumption whatsoever other than having IID, and more generally, exchangeable data.</span></p>
<h2 id="_idParaDest-22"><a id="_idTextAnchor020"/><span class="koboSpan" id="kobo.183.1">The future of conformal prediction</span></h2>
<p><span class="koboSpan" id="kobo.184.1">For years, I have promoted conformal prediction as the premier framework for reliable probabilistic predictions. </span><span class="koboSpan" id="kobo.184.2">Excitingly, over the last 2-3 years, there has been an explosion of interest in and adoption of conformal prediction, including by major tech leaders such as Amazon, Microsoft, Google, and DeepMind. </span><span class="koboSpan" id="kobo.184.3">Many universities and companies are researching conformal </span><a id="_idIndexMarker015"/><span class="koboSpan" id="kobo.185.1">prediction, actively developing real-world applications, and releasing open source libraries such as MAPIE and </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">Amazon Fortuna.</span></span></p>
<p><span class="koboSpan" id="kobo.187.1">These trends will only accelerate as more practitioners recognize the power of conformal prediction for trustworthy uncertainty quantification. </span><span class="koboSpan" id="kobo.187.2">As renowned machine learning researcher Michael I. </span><span class="koboSpan" id="kobo.187.3">Jordan noted at the ICML 2021 workshop (</span><a href="https://icml.cc/virtual/2021/workshop/8373"><span class="koboSpan" id="kobo.188.1">https://icml.cc/virtual/2021/workshop/8373</span></a><span class="koboSpan" id="kobo.189.1">): "</span><em class="italic"><span class="koboSpan" id="kobo.190.1">Conformal prediction ideas are THE answer to UQ (uncertainty quantification); I think it’s the best I have seen – it’s simple, generalizable, and </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.191.1">so on</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.192.1">."</span></span></p>
<p><span class="koboSpan" id="kobo.193.1">Conformal prediction has an incredibly bright future as an indispensable tool for quantifying uncertainty in machine learning. </span><span class="koboSpan" id="kobo.193.2">Several key reasons are driving the momentum and adoption of </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">conformal prediction:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.195.1">Simplicity</span></strong><span class="koboSpan" id="kobo.196.1">: Conformal prediction is straightforward to understand and implement, making it accessible to practitioners without deep statistical expertise. </span><span class="koboSpan" id="kobo.196.2">At its core is the intuitive idea of measuring how new observations conform to </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">past data.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.198.1">Flexibility</span></strong><span class="koboSpan" id="kobo.199.1">: It can be applied to</span><a id="_idIndexMarker016"/><span class="koboSpan" id="kobo.200.1"> any machine learning model and data distribution. </span><span class="koboSpan" id="kobo.200.2">No modifications to existing predictors are needed. </span><span class="koboSpan" id="kobo.200.3">This model-agnostic property greatly expands the applicability of </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">conformal prediction.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.202.1">Theoretical guarantees</span></strong><span class="koboSpan" id="kobo.203.1">: The finite sample coverage guarantees provide an unmatched level of reliability compared to traditional statistical methods reliant on </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">asymptotic theory.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.205.1">These advantages perfectly position conformal prediction as the gold standard for uncertainty quantification in machine learning applications where trustworthy confidence estimates </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">are critical.</span></span></p>
<p><span class="koboSpan" id="kobo.207.1">I am confident that the adoption of conformal prediction in academic research and industry will accelerate rapidly in the coming years. </span><span class="koboSpan" id="kobo.207.2">Its simple yet powerful approach is cementing its place as an </span><a id="_idIndexMarker017"/><span class="koboSpan" id="kobo.208.1">essential tool for any practitioner or organization managing predictive uncertainty. </span><span class="koboSpan" id="kobo.208.2">The next few years will be incredibly exciting as we realize the full potential of this </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">game-changing framework.</span></span></p>
<h1 id="_idParaDest-23"><a id="_idTextAnchor021"/><span class="koboSpan" id="kobo.210.1">How conformal prediction differs from traditional machine learning</span></h1>
<p><span class="koboSpan" id="kobo.211.1">Conformal prediction allows</span><a id="_idIndexMarker018"/><span class="koboSpan" id="kobo.212.1"> the production of well-calibrated probabilistic predictions for any statistical, machine learning, or deep learning model. </span><span class="koboSpan" id="kobo.212.2">This is achieved without relying on restrictive assumptions required by other </span><a id="_idIndexMarker019"/><span class="koboSpan" id="kobo.213.1">methods such as Bayesian techniques, Monte Carlo simulation, and bootstrapping. </span><span class="koboSpan" id="kobo.213.2">Importantly, conformal prediction does not require subjective priors. </span><span class="koboSpan" id="kobo.213.3">It provides mathematically guaranteed, well-calibrated predictions every time – regardless of the underlying prediction model, data distribution, or </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">dataset size.</span></span></p>
<p><span class="koboSpan" id="kobo.215.1">A key limitation of traditional machine learning is the need for more reasonable confidence measures for individual predictions. </span><span class="koboSpan" id="kobo.215.2">Models may have excellent overall performance but not be able to quantify uncertainty for a given </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">input reliably.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">Conformal prediction solves this by outputting prediction regions and confidence measures with statistical validity guarantees. </span><span class="koboSpan" id="kobo.217.2">It achieves this distribution-free reliability without needing to modify the </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">underlying predictor.</span></span></p>
<p><span class="koboSpan" id="kobo.219.1">While machine learning and deep learning models struggle to quantify uncertainty, limiting user trust, conformal prediction has been successfully applied to consequential real-world problems. </span><span class="koboSpan" id="kobo.219.2">Examples include diagnosing depression, drug discovery, and predicting risks of cancer </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">and stroke.</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">By delivering trustworthy individual prediction uncertainties, conformal prediction unlocks the potential of machine learning for high-stakes applications requiring confidence measures </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">alongside predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">Conformal prediction has invaluable applications across many high-stakes domains where reliable confidence estimates </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">are critical:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.225.1">Medicine</span></strong><span class="koboSpan" id="kobo.226.1">: Conformal prediction can improve trust in AI severity ratings for medical imaging, assisting radiologists in disease grading using </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">MRI scans</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.228.1">Health</span></strong><span class="koboSpan" id="kobo.229.1">: It can detect anomalies and </span><a id="_idIndexMarker020"/><span class="koboSpan" id="kobo.230.1">provide reliable anomaly scores to inform </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">treatment decisions</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.232.1">Self-driving cars</span></strong><span class="koboSpan" id="kobo.233.1">: Conformal </span><a id="_idIndexMarker021"/><span class="koboSpan" id="kobo.234.1">prediction can improve autonomous vehicle safety by providing reliable prediction intervals for pedestrian positions </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">and trajectories</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.236.1">Finance</span></strong><span class="koboSpan" id="kobo.237.1">: It can ensure algorithmic fairness in lending by providing </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">well-calibrated predictions</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.239.1">Recommender systems</span></strong><span class="koboSpan" id="kobo.240.1">: Augmenting recommenders with conformal prediction can improve recommendations by guaranteeing high-quality suggestions and minimizing </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">erroneous items</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.242.1">By delivering rigorous confidence estimates, conformal prediction unlocks reliable and ethical AI applications in medicine, transportation, finance, and beyond. </span><span class="koboSpan" id="kobo.242.2">Its validity guarantees make it invaluable for high-stakes decisions relying on </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">machine learning.</span></span></p>
<h1 id="_idParaDest-24"><a id="_idTextAnchor022"/><span class="koboSpan" id="kobo.244.1">The p-value and its role in conformal prediction</span></h1>
<p><span class="koboSpan" id="kobo.245.1">In conformal prediction, p-values</span><a id="_idIndexMarker022"/><span class="koboSpan" id="kobo.246.1"> are key in constructing prediction regions and intervals with a guaranteed confidence level. </span><span class="koboSpan" id="kobo.246.2">However, their purpose is different than </span><a id="_idIndexMarker023"/><span class="koboSpan" id="kobo.247.1">in traditional statistical </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">hypothesis testing.</span></span></p>
<p><span class="koboSpan" id="kobo.249.1">Let’s walk through an example binary classification task to understand how this works. </span><span class="koboSpan" id="kobo.249.2">Suppose we want to predict whether a patient has a medical condition based on their symptoms </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">and characteristics:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.251.1">First, we calculate a nonconformity score that measures how different or “nonconforming” the new patient is compared to previously seen patients. </span><span class="koboSpan" id="kobo.251.2">We can define this score in various ways, such as the distance between </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">feature values.</span></span></li>
<li><span class="koboSpan" id="kobo.253.1">Next, we temporarily assign the patient each possible label – 0 (no condition) and 1 (has condition) – and recalculate the nonconformity score with that </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">assigned label.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.255.1">If the score is similar to scores for past patients with label 0, then label 0 conforms well to the data. </span><span class="koboSpan" id="kobo.255.2">To measure this</span><a id="_idIndexMarker024"/><span class="koboSpan" id="kobo.256.1"> fit statistically, we compute the p-value by comparing the test object’s “strangeness” using the test object’s nonconformity score to the nonconformity scores of the </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">previous patients.</span></span></p>
<p><span class="koboSpan" id="kobo.258.1">If this p-value exceeds our</span><a id="_idIndexMarker025"/><span class="koboSpan" id="kobo.259.1"> chosen significance level, we add label 0 to the prediction set since it fits the data. </span><span class="koboSpan" id="kobo.259.2">We repeat this for label 1 to see whether we should </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">include it.</span></span></p>
<p><span class="koboSpan" id="kobo.261.1">Ultimately, we construct a prediction set containing all labels whose nonconformity scores result in p-values exceeding the significance level. </span><span class="koboSpan" id="kobo.261.2">This provides finite sample guarantees on the confidence level of the </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">prediction regions.</span></span></p>
<p><span class="koboSpan" id="kobo.263.1">So, in summary, nonconformity scores measure how well each potential label conforms to the training data. </span><span class="koboSpan" id="kobo.263.2">Then p-values let us convert these scores into statistically rigorous prediction sets and intervals. </span><span class="koboSpan" id="kobo.263.3">The two concepts work hand in hand within the conformal </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">prediction framework.</span></span></p>
<h1 id="_idParaDest-25"><a id="_idTextAnchor023"/><span class="koboSpan" id="kobo.265.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.266.1">In this chapter, we have introduced conformal prediction and explained the multiple benefits of this powerful framework for reliably quantifying the uncertainty of predictions to improve trust in machine </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">learning models.</span></span></p>
<p><span class="koboSpan" id="kobo.268.1">We explained that the key benefit of conformal prediction is that, unlike any other probabilistic prediction framework, it provides valid probabilistic predictions accompanied by confidence measures, regardless of the underlying model, the dataset size, and the </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">data distribution.</span></span></p>
<p><span class="koboSpan" id="kobo.270.1">We then explored the origins of conformal prediction and saw how it has recently become a very popular framework adopted by leading universities </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">and companies.</span></span></p>
<p><span class="koboSpan" id="kobo.272.1">Finally, we looked at how conformal prediction differs from traditional machine learning and learned about the role of p-values in </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">conformal prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.274.1">In </span><a href="B19925_02.xhtml#_idTextAnchor024"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.275.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.276.1">, we will explain why conformal prediction is a valuable tool for quantifying the uncertainty of predictions, especially in critical settings such as healthcare, self-driving cars, and finance. </span><span class="koboSpan" id="kobo.276.2">We will also discuss the concept of uncertainty quantification and how the conformal prediction framework has successfully addressed the challenge of </span><span class="No-Break"><span class="koboSpan" id="kobo.277.1">quantifying uncertainty.</span></span></p>
</div>
</body></html>