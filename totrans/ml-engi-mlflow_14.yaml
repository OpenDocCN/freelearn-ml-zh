- en: '*Chapter 10*: Scaling Up Your Machine Learning Workflow'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 10 章*：扩展您的机器学习工作流程'
- en: In this chapter, you will learn about diverse techniques and patterns to scale
    your **machine learning** (**ML**) workflow in different scalability dimensions.
    We will look at using a Databricks managed environment to scale your MLflow development
    capabilities, adding Apache Spark for cases where you have larger datasets. We
    will explore NVIDIA RAPIDS and **graphics processing unit** (**GPU**) support,
    and the Ray distributed frameworks to accelerate your ML workloads. The format
    of this chapter is a small **proof-of-concept** with a defined canonical dataset
    to demonstrate a technique and toolchain.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将了解各种技术和模式，以在不同的可扩展性维度上扩展您的 **机器学习**（**ML**）工作流程。我们将探讨使用 Databricks 管理环境来扩展您的
    MLflow 开发能力，在您有更大的数据集的情况下添加 Apache Spark。我们将探索 NVIDIA RAPIDS 和 **图形处理单元**（**GPU**）支持，以及
    Ray 分布式框架来加速您的 ML 工作负载。本章的格式是一个小的 **概念验证**，包含一个定义良好的规范数据集，以展示一种技术和工具链。
- en: 'Specifically, we will look at the following sections in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，在本章中，我们将查看以下部分：
- en: Developing models with a Databricks Community Edition environment
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Databricks 社区版环境中开发模型
- en: Integrating MLflow with Apache Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 MLflow 与 Apache Spark 集成
- en: Integrating MLflow with NVIDIA RAPIDS (GPU)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 MLflow 与 NVIDIA RAPIDS (GPU) 集成
- en: Integrating MLflow with the Ray platform
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 MLflow 与 Ray 平台集成
- en: This chapter will require researching the appropriate setup for each framework
    introduced, based on the standard official documentation for each of the cases.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将需要研究每个框架的适当设置，基于每个案例的标准官方文档。
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need the following prerequisites:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要以下先决条件：
- en: The latest version of Docker installed on your machine. If you don't already
    have it installed, please follow the instructions at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的机器上安装的最新版本的 Docker。如果您尚未安装，请按照[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)中的说明进行操作。
- en: The latest version of Docker Compose installed—please follow the instructions
    at [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了最新版本的 Docker Compose—请按照[https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/)中的说明进行操作。
- en: Access to Git in the command line, and installed as described in [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命令行中访问 Git，并按照[https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)中的说明进行安装。
- en: Access to a Bash terminal (Linux or Windows).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问 Bash 终端（Linux 或 Windows）。
- en: Access to a browser.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问浏览器。
- en: Python 3.5+ installed.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了 Python 3.5+。
- en: The latest version of your ML library installed locally as described in [*Chapter
    3*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066), *Your Data Science Workbench*.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如[*第 3 章*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066)中所述，本地安装的您机器学习库的最新版本，*您的数据科学工作台*。
- en: An **Amazon Web Services** (**AWS**) account configured to run the MLflow model.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置为运行 MLflow 模型的 **Amazon Web Services**（**AWS**）账户。
- en: Developing models with a Databricks Community Edition environment
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Databricks 社区版环境中开发模型
- en: In many scenarios of small teams and companies, starting up a centralized ML
    environment might be a costly, resource-intensive, upfront investment. A team
    being able to quickly scale and getting a team up to speed is critical to unlocking
    the value of ML in an organization. The use of managed services is very relevant
    in these cases to start prototyping systems and to begin to understand the viability
    of using ML at a lower cost.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多小型团队和公司的场景中，启动一个集中的 ML 环境可能是一个成本高昂、资源密集的前期投资。一个能够快速扩展并使团队快速掌握技能的团队对于解锁组织内
    ML 的价值至关重要。在这些情况下，使用托管服务非常相关，以开始原型设计和开始了解以较低成本使用 ML 的可行性。
- en: A very popular managed ML and data platform is the Databricks platform, developed
    by the same company that developed MLflow. We will use in this section the Databricks
    Community Edition version and license targeted for students and personal use.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常流行的托管 ML 和数据平台是 Databricks 平台，由开发 MLflow 的同一家公司开发。在本节中，我们将使用针对学生和个人使用的 Databricks
    社区版版本和许可证。
- en: 'In order to explore the Databricks platform to develop and share models, you
    need to execute the following steps:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索 Databricks 平台以开发和共享模型，您需要执行以下步骤：
- en: Sign up to Databricks Community Edition at [https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)
    and create an account.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)上注册Databricks社区版，并创建一个账户。
- en: Log in to your account with your just-created credentials.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用您刚刚创建的凭据登录您的账户。
- en: Upload training data into Databricks. You can start by uploading the training
    data available in the `Chapter10/databricks_notebooks/training_data.csv` folder.
    In the following screenshot, you can see represented the **Data** tab on the left,
    and you should see your file uploaded to the platform:![Figure 10.1 – Uploading
    training data to Databricks](img/image0016.jpg)
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据上传到Databricks。您可以从上传位于`Chapter10/databricks_notebooks/training_data.csv`文件夹中的训练数据开始。在以下屏幕截图中，您可以看到左侧的**数据**标签页，并且应该看到您的文件已上传到平台：![Figure
    10.1 – Uploading training data to Databricks](img/image0016.jpg)
- en: Figure 10.1 – Uploading training data to Databricks
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Figure 10.1 – Uploading training data to Databricks
- en: Upload training data to Databricks. You can start by uploading the training
    data available in the `Chapter10/databricks_notebooks/input_prediction.csv` folder.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据上传到Databricks。您可以从上传位于`Chapter10/databricks_notebooks/input_prediction.csv`文件夹中的训练数据开始。
- en: Create a cluster to use for your workloads. You are allowed to have clusters
    for your workloads with a limit of 15 **gigabytes** (**GB**) of **random-access
    memory** (**RAM**) and with usage for a defined period of time.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个用于您工作负载的集群。您可以为您的负载拥有集群，限制为15 **GB**（**GB**）的**随机存取内存**（**RAM**），并且使用期限为定义的时间段。
- en: 'You can see an overview of the cluster-creation process in the following screenshot:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在以下屏幕截图中查看集群创建过程的概述：
- en: '![](img/image0027.jpg)'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![img/image0027.jpg](img/image0027.jpg)'
- en: Figure 10.2 – Creating a cluster in Databricks Community Edition
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Figure 10.2 – Creating a cluster in Databricks Community Edition
- en: Create a new notebook in your Databricks platform on your landing workspace
    page by clicking on the **Create a Blank Notebook** button at the top right of
    the page, as illustrated in the following screenshot:![Figure 10.3 – Creating
    a new notebook in Databricks Community Edition](img/Image_003.jpg)
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击页面右上角的**创建空白笔记本**按钮，在您的着陆工作空间页面上创建一个新的笔记本，如图所示：![Figure 10.3 – Creating
    a new notebook in Databricks Community Edition](img/Image_003.jpg)
- en: Figure 10.3 – Creating a new notebook in Databricks Community Edition
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Figure 10.3 – Creating a new notebook in Databricks Community Edition
- en: We are now ready to start a notebook to execute a basic training job in this
    managed environment. You can start by clicking on **Create Notebook**, as illustrated
    in the following screenshot:![Figure 10.4 – Creating your new notebook](img/image0047.jpg)
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在已准备好在这个托管环境中启动一个笔记本以执行基本的训练作业。您可以从点击**创建笔记本**开始，如图所示：![Figure 10.4 – Creating
    your new notebook](img/image0047.jpg)
- en: Figure 10.4 – Creating your new notebook
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Figure 10.4 – Creating your new notebook
- en: Upload training data to Databricks. You can start by uploading the training
    data available in the `Chapter10/databricks_notebooks/input_prediction.csv` folder.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据上传到Databricks。您可以从上传位于`Chapter10/databricks_notebooks/input_prediction.csv`文件夹中的训练数据开始。
- en: 'Import the needed libraries. We will adapt a `LogicRegression` model used to
    classify our running business case of the price of a `btc-usd` ticker, as follows:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库。我们将适配一个用于分类我们正在运行的`btc-usd`标的价格的业务案例的`LogicRegression`模型，如下所示：
- en: '[PRE0]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To read the data, due to the usage of the Databricks filesystem in the platform,
    it is more convenient to read the data in Spark and convert thereafter the DataFrame
    into `pandas`. We also split the data into training and test sets, as usual. Here
    is the code you''ll need for this:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于平台中使用了Databricks文件系统，为了读取数据，更方便的是在Spark中读取数据，然后将DataFrame转换为`pandas`。我们通常将数据分为训练集和测试集。以下是您需要的代码：
- en: '[PRE1]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Our next step will be to quickly train our classifier, as follows:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的下一步将是快速训练我们的分类器，如下所示：
- en: '[PRE2]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the top corner of the page, you can click on the **Experiment** button to
    view more details about your run, and you can click further to look at your model
    experiment, in the familiar interface of experiments, as illustrated in the following
    screenshot:![ Figure 10.5 – Experiment button](img/image0055.jpg)
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在页面右上角，您可以点击**实验**按钮查看关于您运行的更多详细信息，并且可以进一步点击查看您的模型实验，如图所示，以下屏幕截图所示：![Figure
    10.5 – Experiment button](img/image0055.jpg)
- en: Figure 10.5 – Experiment button
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Figure 10.5 – Experiment button
- en: 'One interesting feature that can scale and accelerate your ability to collaborate
    with others is the ability to publish model notebooks that are publicly accessible
    to everyone with whom you share a link, as illustrated in the following screenshot:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个有趣的功能是可以扩展并加速你与他人协作的能力，即能够发布公开可访问的模型笔记本，供与你分享链接的每个人使用，如下面的截图所示：
- en: '![Figure 10.6 – Publishing notebooks'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.6 – 发布笔记本'
- en: '](img/image0064.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image0064.jpg)'
- en: Figure 10.6 – Publishing notebooks
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 发布笔记本
- en: You can also export your notebook as a `dbc` file so that you can quickly start
    it up in a Databricks environment, and you can also share it in a repository,
    as you can see in the chapter folder, under `/databricks-notebooks/bitpred_poc.dbc`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将笔记本导出为 `dbc` 文件，以便您可以在 Databricks 环境中快速启动它，您还可以在存储库中共享它，如章节文件夹中所示，在 `/databricks-notebooks/bitpred_poc.dbc`。
- en: Having dealt with ways to scale your ability to run, develop, and distribute
    models using a Databricks environment, we will next look at integrating an Apache
    Spark flow into our inference workflows to handle scenarios where we have access
    to large datasets.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理了使用 Databricks 环境扩展运行、开发和分发模型的方法之后，我们将接下来探讨将 Apache Spark 流集成到我们的推理工作流程中，以处理我们有访问权的大型数据集的场景。
- en: Integrating MLflow with Apache Spark
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成 MLflow 与 Apache Spark
- en: Apache Spark is a very scalable and popular big data framework that allows data
    processing at a large scale. For more details and documentation, please go to
    [https://spark.apache.org/](https://spark.apache.org/). As a big data tool, it
    can be used to speed up parts of your ML inference, as it can be set at a training
    or an inference level.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个非常可扩展且流行的大数据框架，它允许在大规模上进行数据处理。有关更多详细信息和支持文档，请访问 [https://spark.apache.org/](https://spark.apache.org/)。作为一个大数据工具，它可以用来加速您的机器学习推理的部分，因为它可以在训练或推理级别进行设置。
- en: In this particular case, we will illustrate how to implement it to use the model
    developed in the previous section on the Databricks environment to scale the batch-inference
    job to larger amounts of data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定案例中，我们将说明如何实现它，以便使用上一节在 Databricks 环境中开发的模型来扩展批量推理作业到更大的数据量。
- en: 'In other to explore Spark integration with MLflow, we will execute the following
    steps:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索 Spark 与 MLflow 的集成，我们将执行以下步骤：
- en: Create a new notebook named `inference_job_spark` in Python, linking to a running
    cluster where the `bitpred_poc.ipynb` notebook was just created.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Python 中创建一个名为 `inference_job_spark` 的新笔记本，并将其链接到刚刚创建 `bitpred_poc.ipynb`
    笔记本的运行集群。
- en: Upload your data to `dbfs` on the File/Upload data link in the environment.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将您的数据上传到环境中的文件/上传数据链接的 `dbfs` 上。
- en: 'Execute the following script in a cell of the notebook, changing the `logged_model`
    and `df` filenames for the ones in your environment:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在笔记本的一个单元中执行以下脚本，将 `logged_model` 和 `df` 文件名更改为您环境中的文件名：
- en: '[PRE3]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This illustrative excerpt running on Databricks or on your own Spark cluster
    can scale to large datasets, using the power of distributed computing in Spark.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例摘录可以在 Databricks 或您自己的 Spark 集群上运行，并可以扩展到大型数据集，利用 Spark 的分布式计算能力。
- en: From scaling inference with Apache Spark, we will look now at using GPUs with
    the support of MLflow to scale hyperparameter optimization jobs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从使用 Apache Spark 扩展推理开始，我们现在将探讨在 MLflow 的支持下使用 GPU 来扩展超参数优化作业。
- en: Integrating MLflow with NVIDIA RAPIDS (GPU)
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成 MLflow 与 NVIDIA RAPIDS（GPU）
- en: Training and tuning ML models is a long and computationally expensive operation
    and is one of the operations that can benefit the most from parallel processing.
    We will explore in this section the integration of your MLflow training jobs,
    including hyperparameter optimization, with the NVIDIA RAPIDS framework.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '训练和调整机器学习模型是一个漫长且计算成本高昂的操作，并且是能够从并行处理中受益最大的操作之一。在本节中，我们将探讨将您的 MLflow 训练作业（包括超参数优化）与
    NVIDIA RAPIDS 框架集成。 '
- en: 'To integrate the NVIDIA RAPIDS library, follow the next steps:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要集成 NVIDIA RAPIDS 库，请按照以下步骤操作：
- en: 'Install RAPIDS in the most convenient way for your environment, outlined as
    follows:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据您的环境，以以下方式安装 RAPIDS：
- en: a. [https://rapids.ai/start.html](https://rapids.ai/start.html) contains detailed
    information on deployment options.
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a. [https://rapids.ai/start.html](https://rapids.ai/start.html) 包含有关部署选项的详细信息。
- en: b. [https://developer.nvidia.com/blog/run-rapids-on-google-colab/](https://developer.nvidia.com/blog/run-rapids-on-google-colab/)
    details how to run RAPIDS on **Google Colaboratory** (**Google Colab**).
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b. [https://developer.nvidia.com/blog/run-rapids-on-google-colab/](https://developer.nvidia.com/blog/run-rapids-on-google-colab/)
    详细说明了如何在 **Google Colaboratory**（**Google Colab**）上运行 RAPIDS。
- en: Install MLflow in your environment.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的环境中安装 MLflow。
- en: 'Import the needed libraries, as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式导入所需的库：
- en: '[PRE4]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Implement the `load_data` function, which is a helper function for loading
    data to be used by `cudf` DataFrame is a DataFrame library for loading, joining,
    aggregating, and filtering without knowing the details of **Compute Unified Device
    Architecture** (**CUDA**) programming. Here is the code you''ll need:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现 `load_data` 函数，这是一个辅助函数，用于将数据加载到将被 `cudf` DataFrame（一个用于加载、连接、聚合和过滤的 DataFrame
    库，无需了解 **计算统一设备架构**（**CUDA**）编程的细节）使用。以下是所需的代码：
- en: '[PRE5]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define a training loop, as follows:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式定义训练循环：
- en: '[PRE6]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Call the inner training loop, like this:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用内部训练循环，如下所示：
- en: '[PRE7]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Set up your main flow by reading an argument, if you are using the version
    deployed in Docker. The code to do this is illustrated in the following snippet:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你在 Docker 中部署了版本，通过读取参数设置你的主要流程。以下代码片段展示了如何执行此操作：
- en: '[PRE8]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define your trials and parameters to optimize, as follows:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式定义你的试验和要优化的参数：
- en: '[PRE9]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Run your main loop, as follows:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式运行你的主要循环：
- en: '[PRE10]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: After having dealt with using a highly scalable compute environment to serve
    models on top of the Ray platform, we will now consider a different problem, where
    we will look at options to track multiple runs from a local machine in a centralized
    cloud location.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理了使用高度可扩展的计算环境在 Ray 平台上提供模型之后，我们现在将考虑一个不同的问题，我们将探讨从本地机器在集中式云位置跟踪多个运行的选择。
- en: Integrating MLflow with the Ray platform
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 MLflow 与 Ray 平台集成
- en: The Ray framework ([https://docs.ray.io/en/master/](https://docs.ray.io/en/master/))
    is a distributed platform that allows you to quickly scale the deployment infrastructure.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 框架（[https://docs.ray.io/en/master/](https://docs.ray.io/en/master/））是一个分布式平台，它允许你快速扩展部署基础设施。
- en: With Ray, you can add arbitrary logic when running an ML platform that needs
    to scale in the same way as model serving. It's basically a web framework.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Ray，你可以在运行需要以与模型服务相同方式扩展的 ML 平台时添加任意逻辑。它基本上是一个网络框架。
- en: 'We preloaded the model and contents that will be used into the following folder
    of the repository: https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter10/mlflow-ray-serve-integration.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预先加载了模型和内容，并将它们放入以下存储库文件夹中：https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter10/mlflow-ray-serve-integration。
- en: 'In order to execute your model serving into Ray, execute the following steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将你的模型服务执行到 Ray 中，执行以下步骤：
- en: 'Install the Ray package by running the following command:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令安装 Ray 包：
- en: '[PRE11]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Install MLflow in your environment.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的环境中安装 MLflow。
- en: 'Import the needed libraries, as follows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式导入所需的库：
- en: '[PRE12]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Implement the model backend, which basically means wrapping up the model-serving
    function into your Ray serving environment. Here''s the code you''ll need:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实现模型后端，这基本上意味着将模型服务函数包装到你的 Ray 服务环境中。以下是所需的代码：
- en: '[PRE13]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Start the Ray server, as follows:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式启动 Ray 服务器：
- en: '[PRE14]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load the model and create a backend, like this:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式加载模型并创建后端：
- en: '[PRE15]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Test the serving platform by running the following command:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令测试服务平台：
- en: '[PRE16]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After having dealt with using a highly scalable compute environment to serve
    models on top of the Ray platform, we will look at the performance and monitoring
    component in the following chapter.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理了使用高度可扩展的计算环境在 Ray 平台上提供模型之后，我们将在下一章中查看性能和监控组件。
- en: Summary
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on scaling your ability to run, develop, and distribute
    models using a Databricks environment. We also looked at integrating an Apache
    Spark flow into our batch-inference workflows to handle scenarios where we have
    access to large datasets.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于使用 Databricks 环境扩展你运行、开发和分发模型的能力。我们还探讨了将 Apache Spark 流集成到我们的批量推理工作流程中，以处理我们有访问大型数据集的场景。
- en: We concluded the chapter with two approaches to scale hyperparameter optimization
    and **application programming interface** (**API**) serving with scalability,
    using the NVIDIA RAPIDS framework and the Ray distributed framework.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 NVIDIA RAPIDS 框架和 Ray 分布式框架，以可扩展性的方式总结了两种扩展超参数优化和 **应用程序编程接口**（**API**）服务的方法。
- en: In the next chapter and in further sections of the book, we will focus on the
    observability and performance monitoring of ML models.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章和本书的后续部分，我们将专注于 ML 模型的可观察性和性能监控。
- en: Further reading
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'In order to further your knowledge, you can consult the documentation at the
    following links:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步扩展您的知识，您可以查阅以下链接中的文档：
- en: '[https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html](https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html](https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html)'
- en: '[https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)'
- en: '[https://docs.databricks.com/applications/mlflow/index.html](https://docs.databricks.com/applications/mlflow/index.html)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.databricks.com/applications/mlflow/index.html](https://docs.databricks.com/applications/mlflow/index.html)'
