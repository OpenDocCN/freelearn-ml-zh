- en: '*Chapter 10*: Scaling Up Your Machine Learning Workflow'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about diverse techniques and patterns to scale
    your **machine learning** (**ML**) workflow in different scalability dimensions.
    We will look at using a Databricks managed environment to scale your MLflow development
    capabilities, adding Apache Spark for cases where you have larger datasets. We
    will explore NVIDIA RAPIDS and **graphics processing unit** (**GPU**) support,
    and the Ray distributed frameworks to accelerate your ML workloads. The format
    of this chapter is a small **proof-of-concept** with a defined canonical dataset
    to demonstrate a technique and toolchain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will look at the following sections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Developing models with a Databricks Community Edition environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating MLflow with Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating MLflow with NVIDIA RAPIDS (GPU)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrating MLflow with the Ray platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter will require researching the appropriate setup for each framework
    introduced, based on the standard official documentation for each of the cases.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of Docker installed on your machine. If you don't already
    have it installed, please follow the instructions at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of Docker Compose installed—please follow the instructions
    at [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to Git in the command line, and installed as described in [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a Bash terminal (Linux or Windows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.5+ installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of your ML library installed locally as described in [*Chapter
    3*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066), *Your Data Science Workbench*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An **Amazon Web Services** (**AWS**) account configured to run the MLflow model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing models with a Databricks Community Edition environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many scenarios of small teams and companies, starting up a centralized ML
    environment might be a costly, resource-intensive, upfront investment. A team
    being able to quickly scale and getting a team up to speed is critical to unlocking
    the value of ML in an organization. The use of managed services is very relevant
    in these cases to start prototyping systems and to begin to understand the viability
    of using ML at a lower cost.
  prefs: []
  type: TYPE_NORMAL
- en: A very popular managed ML and data platform is the Databricks platform, developed
    by the same company that developed MLflow. We will use in this section the Databricks
    Community Edition version and license targeted for students and personal use.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to explore the Databricks platform to develop and share models, you
    need to execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign up to Databricks Community Edition at [https://community.cloud.databricks.com/](https://community.cloud.databricks.com/)
    and create an account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Log in to your account with your just-created credentials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload training data into Databricks. You can start by uploading the training
    data available in the `Chapter10/databricks_notebooks/training_data.csv` folder.
    In the following screenshot, you can see represented the **Data** tab on the left,
    and you should see your file uploaded to the platform:![Figure 10.1 – Uploading
    training data to Databricks](img/image0016.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.1 – Uploading training data to Databricks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upload training data to Databricks. You can start by uploading the training
    data available in the `Chapter10/databricks_notebooks/input_prediction.csv` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a cluster to use for your workloads. You are allowed to have clusters
    for your workloads with a limit of 15 **gigabytes** (**GB**) of **random-access
    memory** (**RAM**) and with usage for a defined period of time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can see an overview of the cluster-creation process in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/image0027.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 10.2 – Creating a cluster in Databricks Community Edition
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create a new notebook in your Databricks platform on your landing workspace
    page by clicking on the **Create a Blank Notebook** button at the top right of
    the page, as illustrated in the following screenshot:![Figure 10.3 – Creating
    a new notebook in Databricks Community Edition](img/Image_003.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.3 – Creating a new notebook in Databricks Community Edition
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are now ready to start a notebook to execute a basic training job in this
    managed environment. You can start by clicking on **Create Notebook**, as illustrated
    in the following screenshot:![Figure 10.4 – Creating your new notebook](img/image0047.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.4 – Creating your new notebook
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upload training data to Databricks. You can start by uploading the training
    data available in the `Chapter10/databricks_notebooks/input_prediction.csv` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the needed libraries. We will adapt a `LogicRegression` model used to
    classify our running business case of the price of a `btc-usd` ticker, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To read the data, due to the usage of the Databricks filesystem in the platform,
    it is more convenient to read the data in Spark and convert thereafter the DataFrame
    into `pandas`. We also split the data into training and test sets, as usual. Here
    is the code you''ll need for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our next step will be to quickly train our classifier, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the top corner of the page, you can click on the **Experiment** button to
    view more details about your run, and you can click further to look at your model
    experiment, in the familiar interface of experiments, as illustrated in the following
    screenshot:![ Figure 10.5 – Experiment button](img/image0055.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 10.5 – Experiment button
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'One interesting feature that can scale and accelerate your ability to collaborate
    with others is the ability to publish model notebooks that are publicly accessible
    to everyone with whom you share a link, as illustrated in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Publishing notebooks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/image0064.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – Publishing notebooks
  prefs: []
  type: TYPE_NORMAL
- en: You can also export your notebook as a `dbc` file so that you can quickly start
    it up in a Databricks environment, and you can also share it in a repository,
    as you can see in the chapter folder, under `/databricks-notebooks/bitpred_poc.dbc`.
  prefs: []
  type: TYPE_NORMAL
- en: Having dealt with ways to scale your ability to run, develop, and distribute
    models using a Databricks environment, we will next look at integrating an Apache
    Spark flow into our inference workflows to handle scenarios where we have access
    to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating MLflow with Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a very scalable and popular big data framework that allows data
    processing at a large scale. For more details and documentation, please go to
    [https://spark.apache.org/](https://spark.apache.org/). As a big data tool, it
    can be used to speed up parts of your ML inference, as it can be set at a training
    or an inference level.
  prefs: []
  type: TYPE_NORMAL
- en: In this particular case, we will illustrate how to implement it to use the model
    developed in the previous section on the Databricks environment to scale the batch-inference
    job to larger amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other to explore Spark integration with MLflow, we will execute the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new notebook named `inference_job_spark` in Python, linking to a running
    cluster where the `bitpred_poc.ipynb` notebook was just created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload your data to `dbfs` on the File/Upload data link in the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the following script in a cell of the notebook, changing the `logged_model`
    and `df` filenames for the ones in your environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This illustrative excerpt running on Databricks or on your own Spark cluster
    can scale to large datasets, using the power of distributed computing in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: From scaling inference with Apache Spark, we will look now at using GPUs with
    the support of MLflow to scale hyperparameter optimization jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating MLflow with NVIDIA RAPIDS (GPU)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training and tuning ML models is a long and computationally expensive operation
    and is one of the operations that can benefit the most from parallel processing.
    We will explore in this section the integration of your MLflow training jobs,
    including hyperparameter optimization, with the NVIDIA RAPIDS framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'To integrate the NVIDIA RAPIDS library, follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install RAPIDS in the most convenient way for your environment, outlined as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a. [https://rapids.ai/start.html](https://rapids.ai/start.html) contains detailed
    information on deployment options.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b. [https://developer.nvidia.com/blog/run-rapids-on-google-colab/](https://developer.nvidia.com/blog/run-rapids-on-google-colab/)
    details how to run RAPIDS on **Google Colaboratory** (**Google Colab**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Install MLflow in your environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the needed libraries, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the `load_data` function, which is a helper function for loading
    data to be used by `cudf` DataFrame is a DataFrame library for loading, joining,
    aggregating, and filtering without knowing the details of **Compute Unified Device
    Architecture** (**CUDA**) programming. Here is the code you''ll need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a training loop, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Call the inner training loop, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up your main flow by reading an argument, if you are using the version
    deployed in Docker. The code to do this is illustrated in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define your trials and parameters to optimize, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run your main loop, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After having dealt with using a highly scalable compute environment to serve
    models on top of the Ray platform, we will now consider a different problem, where
    we will look at options to track multiple runs from a local machine in a centralized
    cloud location.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating MLflow with the Ray platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Ray framework ([https://docs.ray.io/en/master/](https://docs.ray.io/en/master/))
    is a distributed platform that allows you to quickly scale the deployment infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: With Ray, you can add arbitrary logic when running an ML platform that needs
    to scale in the same way as model serving. It's basically a web framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'We preloaded the model and contents that will be used into the following folder
    of the repository: https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter10/mlflow-ray-serve-integration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to execute your model serving into Ray, execute the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the Ray package by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Install MLflow in your environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the needed libraries, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the model backend, which basically means wrapping up the model-serving
    function into your Ray serving environment. Here''s the code you''ll need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Ray server, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the model and create a backend, like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test the serving platform by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After having dealt with using a highly scalable compute environment to serve
    models on top of the Ray platform, we will look at the performance and monitoring
    component in the following chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on scaling your ability to run, develop, and distribute
    models using a Databricks environment. We also looked at integrating an Apache
    Spark flow into our batch-inference workflows to handle scenarios where we have
    access to large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We concluded the chapter with two approaches to scale hyperparameter optimization
    and **application programming interface** (**API**) serving with scalability,
    using the NVIDIA RAPIDS framework and the Ray distributed framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter and in further sections of the book, we will focus on the
    observability and performance monitoring of ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to further your knowledge, you can consult the documentation at the
    following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html](https://www.mlflow.org/docs/latest/python_api/mlflow.sagemaker.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/managing-your-machine-learning-lifecycle-with-mlflow-and-amazon-sagemaker/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.databricks.com/applications/mlflow/index.html](https://docs.databricks.com/applications/mlflow/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
