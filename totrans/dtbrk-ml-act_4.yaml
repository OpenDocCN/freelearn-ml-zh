- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Getting to Know Your Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “Truth, like gold, is to be obtained not by its growth, but by washing away
    from it all that is not gold.”
  prefs: []
  type: TYPE_NORMAL
- en: ―Leo Tolstoy
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we explore features within the Databricks DI Platform that
    help improve and monitor data quality and facilitate data exploration. There are
    numerous approaches to getting to know your data better with Databricks. First,
    we cover how to oversee data quality with **Delta Live Tables** (**DLT**) to catch
    quality issues early and prevent the contamination of entire pipelines. We’ll
    take our first close look at Lakehouse Monitoring, which helps us analyze data
    changes over time and can alert us to changes that concern us. Lakehouse Monitoring
    is a big time-saver, allowing you to focus on mitigating or responding to data
    changes rather than creating notebooks that calculate standard metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on to data exploration, we will look at a couple of low-code approaches:
    Databricks Assistant and **AutoML**. Finally, we will touch on embeddings. We
    created embeddings from chunks of text in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123),
    and you’ll learn how to use Databricks **Vector Search** (**VS**) to explore your
    embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what you will learn as part of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Improving data integrity with DLT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring data quality with Databricks Lakehouse Monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring data with Databricks Assistant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating data profiles with AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for vector search and database indexing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing data retrieval with Databricks Vector Search
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving data integrity with DLT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last chapter, we introduced DLT as a helpful tool for streaming data
    and pipeline development. Here, we focus on how to use DLT as your go-to tool
    for actively tracking data quality. Generally, datasets are dynamic, not neat,
    and tidy like they often are in school and training. You can use code to clean
    data, of course, but there is a feature that makes the cleaning process even easier:
    DLT’s expectations. DLT’s expectations catch incoming data quality issues and
    automatically validate that incoming data passes specified rules and quality checks.
    For example, you might expect your customer data to have positive values for age
    or that dates follow a specific format. When data does not meet these expectations,
    it can negatively impact downstream data pipelines. With expectations implemented,
    you ensure that your pipelines won’t suffer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing expectations gives us more control over data quality, alerting
    us to unusual data requiring attention and action. There are several options when
    dealing with erroneous data in DLT:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we can set a warning, which will report the number of records that failed
    an expectation as a metric but still write those invalid records to the destination
    dataset; see `@dlt.expect()` in *Figure 4**.1* for an example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, we can drop invalid records so that the final dataset only contains
    records that meet our expectations; see `@dlt.expect_or_drop()` in *Figure 4**.1*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, we can fail the operation entirely, so nothing new is written (note that
    this option requires manual re-triggering of the pipeline).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we can quarantine the invalid data to another table to investigate
    it further. The following code should look familiar to the DLT code in [*Chapter
    3*](B16865_03.xhtml#_idTextAnchor123), but now with the addition of expectations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Using DLT expectations to enforce data quality](img/B16865_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Using DLT expectations to enforce data quality
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at our streaming transactions project as an example. In the *Applying
    our learning* section in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123), we used
    DLT to write the transaction data to a table. Utilizing the same DLT code, we
    will save ourselves the manual effort of cleaning the `CustomerID` column by adding
    an expectation to the original code to drop any records with a `null` `CustomerID`.
    We will set another expectation to warn us if the `Product` field is `null`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, when we call `generate_table()`, the DLT pipeline will automatically clean
    up our table by dropping any null `CustomerID` values and flagging records without
    a `Product` value. Moreover, DLT will automatically build helpful visualizations
    to immediately investigate the data’s quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'To try this yourself, update the DLT code from [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)
    (here’s the path to the notebook as a reminder: `Chapter 3``: Building` `Out Our`
    `Bronze Layer/Project: Streaming Transactions/delta_live_tables/`) to match *Figure
    4**.1*, and then rerun the DLT pipeline as you did before. Once the pipeline is
    complete, it generates the DAG. Click on the `synthetic_transactions_silver` table,
    then click the **Data Quality** tab from the table details. This will display
    information about the records processed, such as how many were written versus
    dropped for failing a given expectation, as shown in *Figure 4**.2*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – The DLT data quality visualizations](img/B16865_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – The DLT data quality visualizations
  prefs: []
  type: TYPE_NORMAL
- en: These insights illustrate how expectations help automatically clean up our tables
    and flag information that might be useful for data scientists using this table
    downstream. In this example, we see that all records passed the `valid_CustomerID`
    expectation, so now we know we don’t have to worry about null customer IDs in
    the table. Additionally, almost 80% of records are missing a `Product` value,
    which may be relevant for data science and **machine learning** (**ML**) projects
    that use this data.
  prefs: []
  type: TYPE_NORMAL
- en: Just as we’ve considered the correctness and consistency of incoming data, we
    also want to consider how we can expand our data quality oversight to include
    data drift, for example, when your data’s distribution changes over time. Observing
    data drift is where Databricks Lakehouse Monitoring emerges as a vital complement
    to DLT, offering a configurable framework to consistently observe and verify the
    statistical properties and quality of input data.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring data quality with Databricks Lakehouse Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Use Databricks Lakehouse Monitoring to proactively detect and respond to any
    deviations in your data distribution. Over time, your data may undergo changes
    in its underlying patterns. This could be feature drift, where the distribution
    of feature data changes over time, or concept drift, where the relationship between
    inputs and outputs of your model changes. Both types of drift can cause model
    quality to suffer. These changes can occur slowly or rapidly in your production
    environment, which is why monitoring your data even before it becomes an input
    into your ML models and data products is essential.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanics of Lakehouse Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To monitor a table in Databricks, you create a monitor attached to that table.
    To monitor the performance of a ML model, you attach the monitor to an inference
    table that holds the model’s inputs and corresponding predictions. Databricks
    Lakehouse Monitoring provides the following profile types of analysis: snapshot,
    time series, and inference.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to selecting the table to be monitored, called the **primary table**,
    you can optionally specify a baseline table to reference for measuring drift or
    the change in values over time. A baseline table is useful when you have a sample
    of what you expect your data to look like, such as the data with which your model
    was trained. Lakehouse Monitoring automatically computes drift relative to expected
    data values and distributions of the baseline table.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a table monitor automatically creates two metric tables, `profile_metrics`
    and `drift_metrics`. Lakehouse Monitoring computes metric values on the table
    for the time windows and data subsets or “slices” you specify when you create
    the monitor. You can also add your own custom metrics; see *Further reading* for
    details.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization and alerting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lakehouse Monitoring generates an SQL dashboard automatically for every monitor.
    These dashboards provide a crucial platform for examining metrics and acting on
    results. Databricks’ alert system serves as a vigilant guardian, promptly notifying
    you of the significant shifts in data quality or distributions you subscribe to.
    *Figure 4**.3* shows how all the Lakehouse Monitoring components work together,
    using the data from the primary table and optional baseline table to generate
    a profile metrics table and drift table, which then populate the dashboard and
    power alerts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Relationship between the input tables, the metric tables, the
    monitor, and the dashboard](img/B16865_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Relationship between the input tables, the metric tables, the monitor,
    and the dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Creating a monitor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can create a Databricks Lakehouse Monitor using the **user interface**
    (**UI**) or, for a more flexible and programmable approach, you can use the API.
    The API method is particularly advantageous when you want to script the creation
    of monitors to integrate them into your automated data pipelines. The following
    is a high-level summary of the steps to create a Lakehouse Monitor using the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Snapshot`, `TimeSeries`, and `Inference`. Each type is suitable for different
    monitoring scenarios, with the `TimeSeries` and `Inference` types requiring a
    timestamp column. The inference profile also requires `prediction_col` and `model_id_col`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`lakehouse_monitoring` module to call the `create_monitor` function, providing
    your table’s catalog schema, table name, the chosen profile type, and the output
    schema for the monitor’s results.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`MonitorCronSchedule` object, which takes a cron expression and a time zone
    ID.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Control access**: After you create the monitor, you can manage access to
    the resulting metrics tables and dashboard using Unity Catalog privileges.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`run_refresh` function to refresh and update the metric tables. You can also
    check the status of specific runs with the `get_refresh` function and list all
    refreshes associated with a monitor using `list_refreshes`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`get_monitor` function allows you to retrieve your monitor’s current settings
    for review.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 4**.4* shows an example of how to use the Lakehouse Monitoring API
    to create a `TimeSeries` profile monitor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Creating a simple TimeSeries table monitor](img/B16865_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Creating a simple TimeSeries table monitor
  prefs: []
  type: TYPE_NORMAL
- en: After establishing a robust framework for data quality monitoring with Databricks
    Lakehouse Monitoring, we can focus on enhancing our data exploration. This leads
    us to Databricks Assistant, a feature dedicated to helping developers be more
    productive in Databricks.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data with Databricks Assistant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Databricks Assistant is a feature designed to boost your productivity in Databricks.
    It has many capabilities, including generating SQL from English, explaining code,
    helping troubleshoot errors, and optimizing code. Databricks Assistant is an exciting
    feature to watch as more capabilities emerge, and we want to give you a taste
    of the possibilities. Since this chapter is about exploring and monitoring data,
    let’s see how you can use Databricks Assistant as a low-code solution to explore
    your data. Suppose you are analyzing the Favorita Sales Forecasting data. You
    are looking to uncover insights into retail store distributions across various
    regions. You have a specific query in mind: you want to understand the store landscape
    in the Guayas region. However, SQL queries aren’t your strong suit, and maybe
    crafting the perfect query seems daunting. In order to explore your data regardless,
    you can use Databricks Assistant. There is no notebook in the project repo for
    this section, but we encourage you to try Databricks Assistant on the Favorita
    Forecasting project tables. Any of the [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180)
    Favorita notebooks would be a great place to access Databricks Assistant. To access
    it, click on the icon shown in *Figure 4**.5*, found in the left-hand sidebar
    of a notebook. Clicking on the icon will open the chat interface to the left of
    the notebook, where we will type in our questions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – The Databricks Assistant icon](img/B16865_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – The Databricks Assistant icon
  prefs: []
  type: TYPE_NORMAL
- en: First, we ask Databricks Assistant how many stores and store types are in the
    state of Guayas in the `favorita_stores` table (*Figure 4**.6*). Note that Databricks
    Assistant does not require Unity Catalog, but using Unity Catalog provides table
    information. The additional information makes Databricks Assistant’s responses
    more helpful and specific to the table you’re working with. This should sound
    similar to the **Retrieval Augmented Generation** (**RAG**) project. We are augmenting
    the generation of answers by providing relevant information. Now, let’s see whether
    it can help us write the SQL query we need. Keep in mind that Databricks Assistant
    is powered by **generative AI**, so you may see different outputs when using it
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Question and response interaction with Databricks Assistant](img/B16865_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Question and response interaction with Databricks Assistant
  prefs: []
  type: TYPE_NORMAL
- en: Nice! Databricks Assistant gave us some SQL code to paste into a notebook and
    run directly. However, a quick scan of the query shows us that we will only get
    a distinct count of stores and types rather than looking at the store type distribution
    we want. Let’s refine our question and try again (*Figure 4**.7*).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – The updated question submitted to Databricks Assistant and the
    results](img/B16865_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – The updated question submitted to Databricks Assistant and the
    results
  prefs: []
  type: TYPE_NORMAL
- en: After submitting the second question in *Figure 4**.7*, Databricks Assistant
    provides a new query that accurately captures what we want to know. To make sure,
    let’s copy the SQL code provided, paste it into a notebook, and run it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Results from Databricks Assistant-generated SQL query](img/B16865_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – Results from Databricks Assistant-generated SQL query
  prefs: []
  type: TYPE_NORMAL
- en: Now we see the distribution of stores by type in the Guayas region, just as
    we wanted. Databricks Assistant is a handy tool, and we can confirm that playing
    with it is also fun! We encourage you to try it out on your own to see how you
    can use English to explore your data.
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Assistant also generates comment suggestions for your tables and
    fields. From the `favorita_stores` table we explored previously. We see in *Figure
    4**.9* that Databricks Assistant has a suggestion for a table comment to help
    others understand the table’s contents.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Databricks Assistant suggests a descriptive table comment](img/B16865_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Databricks Assistant suggests a descriptive table comment
  prefs: []
  type: TYPE_NORMAL
- en: Databricks Assistant can also generate descriptive comments for each field in
    the table by selecting the **AI Generate** button on the right-hand side of the
    table’s **Columns** tab page. You can accept or edit the suggested comments. You
    can toggle off the suggestions by selecting the **Hide AI suggestions** button,
    as shown in *Figure 4**.10*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Databricks Assistant suggests comments for the columns in a
    table](img/B16865_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Databricks Assistant suggests comments for the columns in a table
  prefs: []
  type: TYPE_NORMAL
- en: The generated comments might seem like they fall outside the scope of getting
    to know your data, but documentation is vital to making your data more easily
    discoverable for everyone else (and let’s hope others use generated comments so
    you can explore their datasets more easily too). The faster you understand a dataset,
    the easier it is to further explore that data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Databricks Assistant is a great way to analyze your data when you prefer to
    work in English rather than code directly, and when you have specific questions
    in mind. Now let’s discuss another method for broader **exploratory data analysis**
    (**EDA**): autogenerated notebooks using AutoML.'
  prefs: []
  type: TYPE_NORMAL
- en: Generating data profiles with AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We introduced Databricks AutoML in [*Chapter 1*](B16865_01.xhtml#_idTextAnchor016).
    This tool automates ML development and augments data science workflows. AutoML
    is best known for generating models, but we’ll get to modeling in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
    Since we’re talking about getting to know your data, we first want to focus on
    one extremely useful feature built into AutoML that often flies under the radar:
    autogenerated Python notebooks. AutoML provides a notebook for data exploration
    in addition to the notebook code for every experiment it runs. We will jump right
    into creating an AutoML experiment, view the data exploration code, and then return
    to explore the modeling portion later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover how to create an AutoML experiment via an API in the Favorita project
    notebooks. We encourage you to follow the instructions here to set up a simple
    regression experiment with the AutoML UI, so that we can take a look at the data
    profile created. Before you begin, make sure you have a DBR ML 9.1+ cluster running
    (you can use the **DBR ML 14.2** cluster set up in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start the experiment**: Navigate to the **Experiments** tab on the platform’s
    left-hand navigation bar. Click the **AutoML Experiment** button to initiate a
    new experiment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`ml_in_action.favorita_forecasting.favorite_train_set` table, or your own data
    (just make sure to update the problem type if `Regression` does not apply).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction target**: Choose the specific column in your dataset that you
    want to predict. The AutoML process will use the other columns in your dataset
    to try and predict the values in this target column. If you’re following along
    with the Favorita scenario, select **sales**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 4**.11* shows the configuration of an AutoML experiment using the `favorite_train_set`
    training table. It illustrates how you can customize the AutoML process to fit
    the specific requirements of your ML task. By selecting the appropriate problem
    type, dataset, and prediction target, you’re instructing the AutoML system on
    how to approach the model-building process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Configuration of an AutoML experiment using the Favorita train_set
    table](img/B16865_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Configuration of an AutoML experiment using the Favorita train_set
    table
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve filled out the UI and specified the Favorita table (or another
    table of your choice) as your input training dataset, click **Start AutoML** at
    the bottom of the screen. AutoML experiments may run for up to several hours as
    improvements in the evaluation metric continue, although you can set a shorter
    time limit for the experiment by changing the **Timeout** value (found under **Advanced**
    **Configuration**). As the experiment begins, a new page will open with progress
    updates. Once the experiment is complete, you will see links to two data artifacts:
    the notebook containing the code for the best model, labeled **View notebook for**
    **best model**, and the data exploration notebook, labeled **View data exploration
    notebook**, as shown in *Figure 4**.12*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – The AutoML experiment page with the links to view the notebook
    for the best model and the data exploration notebook](img/B16865_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – The AutoML experiment page with the links to view the notebook
    for the best model and the data exploration notebook
  prefs: []
  type: TYPE_NORMAL
- en: The exploration notebook uses `ydata-profiling`, formerly referred to as pandas’
    profiler library, to generate statistics and summarize data for all the fields
    in the table. It also provides alerts on fields with high correlation issues,
    which could negatively impact models. These warnings are also available in the
    MLflow experiment UI, as shown in *Figure 4**.13*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – AutoML warnings called out during the experiment run](img/B16865_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – AutoML warnings called out during the experiment run
  prefs: []
  type: TYPE_NORMAL
- en: Look through the data exploration notebook for an overview of your data, from
    summary statistics to thorough profiles for each variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now explored two tools for data exploration: Databricks Assistant and
    the `ydata-profiling` library. These are great places to start for many classical
    ML projects. Next, we’ll discuss a more advanced data format and how you can use
    the DI Platform to explore it: data embeddings and vector search. Embeddings are
    advanced transformations that translate complex, unstructured data into a numerical
    format conducive to ML algorithms, capturing intricate relationships within the
    data that are pivotal for sophisticated models.'
  prefs: []
  type: TYPE_NORMAL
- en: Using embeddings to understand unstructured data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve focused on how to explore your structured data. What about unstructured
    data, such as images or text? Recall that we converted PDF text chunks into a
    specific format called embeddings in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)’s
    RAG chatbot project work. We require embeddings, meaning numerical vector representations
    of the data, to perform a similarity (or hybrid) search between chunks of text.
    That way, when someone asks our chatbot a question, such as “What are the economic
    impacts of automation technologies using LLMs?” the chatbot will be able to search
    through the stored chunks of text from the arXiv articles, retrieve the most relevant
    chunks, and use those to better answer the question. For more visual readers,
    see the data preparation workflow in *Figure 4**.14*. We completed the **Data
    Preparation** step in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123). We’ll run
    through the remaining setup steps in the workflow now.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Vector database setup is the prerequisite process supporting
    RAG’s retrieval step](img/B16865_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Vector database setup is the prerequisite process supporting RAG’s
    retrieval step
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings are an essential part of building any chatbot. Pay attention to your
    embedding model to ensure it is relevant to the task. You wouldn’t want to build
    a chatbot designed to answer questions in French but use an embedding model that
    only knows English – your chatbot’s response quality will definitely suffer!
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings that capture the nuances of language are essential for the chatbot
    to understand and generate contextually relevant responses. Equally important
    are the searching and filtering techniques you apply to the **vector database**
    itself. A vector database is similar to a SQL database, but instead of storing
    tabular data, it stores vector embeddings. A search algorithm can then search
    the embeddings. In the final flow of a RAG project, a user’s question is also
    converted into embeddings, and the search algorithm uses those embeddings to find
    similar embeddings stored in the vector database. The chatbot receives the most
    similar embeddings from the vector database to help it craft a response to the
    user’s question.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider the requirements of a good vector database solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quality of the retrievals**: The correctness and completeness of embeddings
    returned as relevant by the search algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability of the solution**: The ability to scale per the number of requests
    coming to the application with dynamic traffic'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accessibility**: The ability to easily access, read, write to, and use the
    application in real time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Governance**: The ability to govern vector storage with the same access controls
    as the original sources used to create the vector embeddings and models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integration**: The ability to easily integrate with current market technologies
    and eliminate time spent stitching technologies and solutions together'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using embeddings and vector search is a powerful way to improve a variety of
    ML projects. There are many uses for vector databases, the most common of which
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RAG systems**: Vector search facilitates efficient data retrieval, which
    is then used to augment a **Large Language Model** (**LLM**)’s response. Augmenting
    an LLM with results from vector search leads to more accurate chatbot responses
    and minimizes errors such as hallucinations in LLM outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recommendation systems**: E-commerce and streaming platforms use vector search
    for efficient nearest-neighbor searches, matching user behavior with relevant
    suggestions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image and video recognition**: Vector search facilitates quick searches for
    similar features in images and videos.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bioinformatics**: Vector search can be applied to tasks such as DNA sequence
    alignment and protein structure similarity search to improve clinical research.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhancing data retrieval with Databricks Vector Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Databricks VS is transforming how we refine and retrieve data for LLMs. Functioning
    as a serverless similarity search engine, VS enables the storage of vector embeddings
    and metadata in a dedicated vector database. Through VS, you can generate dynamic
    vector search indices from **Delta** tables overseen by Unity Catalog. Using a
    straightforward API, you can retrieve the most similar vectors through queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of Databricks VS’s key benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Seamless integration**: VS works harmoniously within Databricks’ ecosystem,
    particularly Delta tables. This integration ensures that your data is always up
    to date, making it model-ready for ML applications. With VS, you can create a
    vector search index from a source Delta table and set the index to sync when the
    source table is updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streamlined operations**: VS significantly simplifies operational complexity
    by eliminating the need to manage third-party vector databases. VS runs on serverless
    compute, meaning Databricks handles the infrastructure management for you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced scalability**: Unlike standalone vector libraries, VS offers unparalleled
    scalability. VS handles large-scale data effortlessly, scaling automatically to
    meet the demands of your data and query load. This scalability is crucial for
    organizations with vast amounts of data and complex search requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unified data asset governance**: VS integrates with Unity Catalog; Unity
    Catalog handles data governance and access control lists. To prevent productional
    data leakage, you can manage access to the Databricks VS API and the underlying
    databases with Unity Catalog.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Serving integration**: Model Serving automates querying of the model
    serving endpoint for embedding generation without any overhead from users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexibility in embedding model support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of VS’s major strengths is its support for any of the embedding models of
    your choice. VS supports hosted or fully managed embeddings. Hosted embeddings
    are self-managed. You create the embeddings and save them on a Delta table. For
    fully managed embeddings, the prepared text is saved in a Delta table, and embeddings
    are created by Databricks Model Serving. Model Serving will convert your incoming
    data into embeddings with the model of your choice. Databricks VS can support
    any model through the Databricks Model Serving endpoints, the Foundation Model
    API (as mentioned in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)), or external
    models. External models include SaaS models, for example, OpenAI’s ChatGPT and
    Anthropic’s PaLM. You can connect external models through the Databricks unified
    model serving gateway. See *External models in Databricks Model Serving* in *Further
    reading* for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a vector search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned, VS is a serverless product. Hence, we require a real-time connection
    with our chatbot application to the relevant content stored in the vector database.
    We’ll cover this again in the *Applying our learning*, *Project – RAG chatbot*
    section, but if you’re ready to set up your own endpoint, you just need a few
    lines of code, as shown in *Figure 4**.15*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Creating a Databricks VS endpoint](img/B16865_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Creating a Databricks VS endpoint
  prefs: []
  type: TYPE_NORMAL
- en: Once you create an endpoint, you can host multiple vector search indices under
    one endpoint. The endpoint will scale according to the demand. The code to host
    your first index is presented in the *Apply our* *learning* section).
  prefs: []
  type: TYPE_NORMAL
- en: There are limitations on the number of endpoints you can create indices per
    endpoint, and the embedding dimensions. Please review the documentation linked
    in *Further reading*, as Databricks may remove or update limits as the product
    evolves.
  prefs: []
  type: TYPE_NORMAL
- en: Technology moves fast, especially in the world of generative AI. Databricks
    VS was built even as we wrote this book, and we expect it to continue evolving
    to search not only through text but also images and audio with a more robust hybrid
    search engine in the future.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve walked through various Databricks products and features geared toward
    helping you understand your data. Get ready to follow along in your own Databricks
    workspace as we work through the [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180)
    code by project and put these concepts into practice.
  prefs: []
  type: TYPE_NORMAL
- en: Applying our learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It’s time to apply these concepts to our example projects. We will use what
    we have learned to explore each project dataset, from using Databricks Assistant
    to AutoML, to creating a vector search index and exploring image data.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you begin, review, and prepare the technical requirements necessary
    for the hands-on work in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `missingno` library to address missing numbers in our synthetic
    transactions project data: [https://pypi.org/project/missingno/](https://pypi.org/project/missingno/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the RAG project, you will need to install the following either on your
    cluster or in the `CH4-01-Creating_VectorDB` notebook. If you choose to install
    them in the notebook, the code is included for you:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`typing_extensions==4.7.1`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers==4.30.2`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`llama-index==0.9.3`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`langchain==0.0.319`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unstructured[pdf,docx]==0.10.30`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Project – Favorita Store Sales – time-series forecasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the Favorita Store Sales project, we use many simple DBSQL queries for
    data exploration and to understand the relationships between datasets. Additionally,
    we use the `ydata_profiling` library to produce data profiles in HTML format,
    as shown in*Figure 4**.19*. To follow along in your own workspace, please refer
    to the following notebooks:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH4-01-Exploring_Favorita_Sales_Data`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH4-02-Exploring_Autogenerated_Notebook`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CH4-03-Imputing_Oil_Data`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the last chapter, we created tables of the Favorita Sales Forecasting dataset
    from Kaggle. Now it’s time to explore! Open up the first notebook, `CH4-01-Exploring_Favorita_Sales_Data`,
    to do some initial data exploration in SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: The first cell is a simple `select *` SQL command. After running the cell, focus
    on the in-cell **Data Profile** and **Visualization** options.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.16 – You can create visualizations and a data profile of a SQL query
    result in a notebook](img/B16865_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – You can create visualizations and a data profile of a SQL query
    result in a notebook
  prefs: []
  type: TYPE_NORMAL
- en: Choose **Data Profile** and investigate the information autogenerated about
    the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Visualization**. You can create a line chart with the date (month)
    versus the sum of sales and grouped by family. Notice that not all data is used
    to produce the visualization; we only see January at first. Once you save the
    visualization, the chart will display in the cell. The **Truncated data** message
    should be present at the bottom of the visualization. To increase the number of
    records in the chart, select **Aggregate over** **more data**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue to play with the options. In the next cell, we filter to the top-performing
    product families. Investigate how different or similar the charts appear.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we’ve done some initial exploration of the Favorita data, we can run
    a Databricks AutoML experiment to generate a baseline model. AutoML can be launched
    in the UI, which we demonstrated earlier in this chapter in the *Generating data
    profiles with AutoML* section (*Figure 4**.11*), or you can create an experiment
    via an API as shown in *Figure 4**.17*. For this project, we will launch both
    a regression experiment and a forecasting experiment. Let’s start with the regression
    run.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that at the top of the notebook, the default language is SQL rather than
    Python. Therefore, when we want to execute Python code, we need to include `%python`
    at the top of the cell. We use Python for AutoML in the last cell of the first
    notebook. We’ve set the `timeout_minutes` variable to `30`, so the experiment
    will run for up to 30 minutes. However, AutoML stops training models if the validation
    metric is no longer improving. In that case, the experiment will finish in less
    time. Once the run is complete, the notebook UI will display links to the MLflow
    experiment where the model versions are accessible, the best trial notebook with
    the best model’s code, and the data exploration notebook. Since this chapter focuses
    on exploring data, we will only open the data exploration notebook for now.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.17 – Creatin\uFEFFg an AutoML experiment from a notebook](img/B16865_04_17.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Creating an AutoML experiment from a notebook
  prefs: []
  type: TYPE_NORMAL
- en: AutoML for data exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you executed the final cell in `CH4-01-Exploring_Favorita_Sales_Data`,
    you received several links in the results, as shown in *Figure 4**.17*. Click
    on the link to the data exploration notebook (you can also open `CH4-02-Exploring_Autogenerated_Notebook`
    for a version that was autogenerated when we ran the AutoML experiment ourselves).
    Let’s look at this notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – EDA notebook created with AutoML](img/B16865_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – EDA notebook created with AutoML
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4**.18* shows a portion of the automatically generated data exploration
    notebook. This notebook imports libraries and points to the training data. It
    also automatically converts the datetime columns to pandas datetime.'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook uses a pandas-based library, so the notebook limits the data to
    10,000 rows.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the notebook imports the `ydata_profiling` library, along with three additional
    correlation calculations added in this case. The `ydata_profiling` library provides
    similar data to what we would get using `summary()` functions, such as details
    about missing and correlated data. The library is easily imported into a notebook
    for exploring data. Once finished, you can export the details to HTML or PDF for
    easy sharing. It’s a great timesaver when exploring new datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – ydata_profiling in the AutoML-created notebook](img/B16865_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – ydata_profiling in the AutoML-created notebook
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 4**.17*, we launched a regression experiment via the API
    by calling `automl.regress()`. Now, we create another experiment with a forecasting
    problem type using the UI.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.20 – Creating an A\uFEFFutoML forecasting experiment in the UI](img/B16865_04_20.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.20 – Creating an AutoML forecasting experiment in the UI
  prefs: []
  type: TYPE_NORMAL
- en: When we create an AutoML forecasting experiment, we can incorporate a country’s
    holidays into the model under the **Country Holidays** option in the **Advanced
    Configuration** section. We will choose **NONE** for this project, because although
    many countries are present in the drop-down menu, Ecuador is not among them. **NONE**
    will ensure holidays are not included as a feature in the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – AutoML forecasting experiment advanced configurations](img/B16865_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – AutoML forecasting experiment advanced configurations
  prefs: []
  type: TYPE_NORMAL
- en: Once all the fields are completed, click **Start AutoML** at the bottom of the
    screen to run the experiment (adjust the **Timeout** variable down if you want
    to ensure the run finishes within a given amount of time). When the experiment
    has finished running, you will see a new screen with a list of generated models.
  prefs: []
  type: TYPE_NORMAL
- en: We have now created an AutoML experiment in a notebook, used AutoML to generate
    a notebook for EDA, and used AutoML for time-series analysis. Databricks AutoML
    does a lot of the heavy lifting with minimal code for ML and EDA.
  prefs: []
  type: TYPE_NORMAL
- en: The Favorita sales dataset contains oil prices, which we think could impact
    sales, so let’s augment the autogenerated data exploration with some of our own
    to explore the oil price data and see how we can use it.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and cleaning oil price data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get started with the oil data, open the `CH4-03-Imputing_Oil_Data` notebook.
    We use the pandas API on Spark to view the data (*Figure 4**.22*). We reindex
    the data because it’s out of order and data for some dates is missing. Notice
    that initially, the data starts in May 2015 rather than January 2013\. Also, the
    dates May 9th and May 10th appear to be missing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – View the oil price data provided by Kaggle in the Favorita
    Sales dataset](img/B16865_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – View the oil price data provided by Kaggle in the Favorita Sales
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: If we add the `date` column as the index column, as shown in the notebook, you’ll
    see that the rows are then in order. However, January 5th and January 6th are
    missing. This occurs because stock prices are not given for holidays or weekends.
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.23 – Reindex to include all dates an\uFEFFd fill forward missing\
    \ prices](img/B16865_04_23.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Reindex to include all dates and fill forward missing prices
  prefs: []
  type: TYPE_NORMAL
- en: We use the `reindex()` command to create an updated index based on the minimum
    and maximum dates in the oil price data. The reindex creates rows for all missing
    dates. When new rows are created for the missing dates, the prices are NaNs. We
    use the `ffill()` function, or forward fill, to update the DataFrame by filling
    in dates without prices with the price from the day before. January 1st, 2013,
    doesn’t have a previous day to fill from, so it remains `NaN`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a clean and consistent silver table of oil prices that we can pull
    into our Favorita time-series models. In the next chapter, we will do feature
    engineering with the Favorita sales data.
  prefs: []
  type: TYPE_NORMAL
- en: Project – streaming transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The synthetic dataset does not need cleaning (since we created it), but we
    can still explore the data to understand it better. To follow along in your own
    workspace, please refer to the following notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH4-01-Exploring_Synthetic_Transactions_Data`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open the notebook to generate visualizations with the `seaborn` library, as
    shown in *Figure 4**.24*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Data visualizations of the synthetic transactions data](img/B16865_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Data visualizations of the synthetic transactions data
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to explore the transaction data further. Next, we’ll move on to the
    RAG chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Project – RAG chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the last chapter, we extracted chunks of text from our PDF documents. We
    converted those chunks into embeddings using the BGE embedding model. As a result,
    we are now ready to take the next step in preparing our data for retrieval. To
    follow along in your own workspace, please refer to the following notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH4-01-Creating_VectorDB`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explained in the *Enhancing data retrieval with Databricks Vector Search*
    section that Databricks VS is a serverless managed solution. In other words, we
    need to create the VS endpoint to host our indices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Creating the Databricks VS endpoint for the RAG chatbot](img/B16865_04_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – Creating the Databricks VS endpoint for the RAG chatbot
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, we will ingest the source table we prepared in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)
    into the index for retrieval search within the chatbot application.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Reading our source table containing text from the documentation](img/B16865_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – Reading our source table containing text from the documentation
  prefs: []
  type: TYPE_NORMAL
- en: We import functions from the project’s `utils` folder, `mlia_utils.rag_funcs`.
    The index only needs to be created once. Going forward, we will only write into
    it or read from it. We use an `if`/`else` clause to check whether an index with
    the name `catalog.database.docs_vsc_idx_cont` exists or not. If it exists, we
    just synchronize our `catalog.database.pdf_documentation_text` source table to
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Check whether the index exists and create it if needed](img/B16865_04_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 – Check whether the index exists and create it if needed
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few key pieces we must make sure are in place before proceeding
    with the rest of our chatbot project:'
  prefs: []
  type: TYPE_NORMAL
- en: '`catalog.database.pdf_documentation_text` is the Delta table you have prepared
    with your self-managed embedding functions as the primary data repository for
    their vector search index.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`catalog.database.docs_vsc_idx_cont` using Databricks VS. This index is linked
    to the source Delta table. The index is set on a trigger base update (`pipeline_type="TRIGGERED"`),
    which means that any modifications or new additions to the historical texts should
    be synced manually to your vector search index. If you wish to have changes reflected
    in the vector search index automatically, choose the `CONTINUOUS` mode instead.
    This continuous update mechanism ensures the data is always current and ready
    for analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_chunks` is automatically converted into embeddings with a model that
    is served through Databricks Model Serving. This can be specified in the setup
    under the `embedding_model_endpoint_name` parameter. This integration guarantees
    that the textual data is efficiently converted into vectors, making it suitable
    for advanced similarity searches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We import the `wait_for_index_to_be_ready()` function from the `utils` folder.
    We run the code in *Figure 4**.28* to repeatedly check the index’s status until
    it is in an “online” state. As the VS index only needs to be created once, this
    function can take some time before the embeddings reach your VS index. Proceed
    once your index is in a “ready” state.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.28 – The wait for index function checks for the index status until
    it is online and ready](img/B16865_04_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – The wait for index function checks for the index status until
    it is online and ready
  prefs: []
  type: TYPE_NORMAL
- en: Once the index is online, we call `get_index()` to perform a similarity search.
    We also call a `describe()` method to show you the broader API options you have.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.29 – Using get_index with the describe method shows the index configuration](img/B16865_04_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – Using get_index with the describe method shows the index configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we use self-managed embeddings, our VS index is not connected to any
    model that can convert our input text query into embeddings to be mapped to the
    one under a database, so we need to convert them first! Here we are again leveraging
    the BGE embedding model from the Foundational Model Serving API and then passing
    embedded text to our index for similarity search:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.30 – Converting our query text into embeddings and passing to the
    index for similarity search](img/B16865_04_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – Converting our query text into embeddings and passing to the index
    for similarity search
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result of the retrieved queries (only the first found one with
    two columns `pdf_name` and content):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.31– Sample of result of the retrieved query](img/B16865_04_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.31– Sample of result of the retrieved query
  prefs: []
  type: TYPE_NORMAL
- en: Now, our VS index is ready to be used in our chatbot. We will explore how to
    connect all the tools together to generate a proper human-readable answer in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297)!
  prefs: []
  type: TYPE_NORMAL
- en: Project – multilabel image classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the last chapter, we saved our image data into volumes. Next, we will explore
    our data. To follow along in your own workspace, please refer to the following
    notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CH4-01-Exploring_Dataset`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We create our training dataset of images. Print the labels and a sample of the
    training data for a look at what we want our model to classify.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.32 – Load and view the training dataset](img/B16865_04_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.32 – Load and view the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: We can use `display_image` to view a few pictures from our volumes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.33 – Display images within the notebook](img/B16865_04_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.33 – Display images within the notebook
  prefs: []
  type: TYPE_NORMAL
- en: It’s good to have an idea of the proportion of the different labels of our data.
    This is how you can view the proportion of data in our training dataset. Wherever
    you perform a multilabel classification, make sure you have a good distribution
    of labels. Otherwise, consider training individual models that might be combined
    or augment missing labels!
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.34 – View the proportion of labels in the training dataset](img/B16865_04_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.34 – View the proportion of labels in the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now we have done some high-level exploration of our image classification dataset.
    For this project, no transformations are needed, so our next step with this data
    will be in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is critical to understand your data before using it. This chapter highlighted
    a variety of methods to explore and analyze our data within the Databricks ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: We began by revisiting DLT, this time focusing on how we use a feature called
    **expectations** to monitor and improve our data quality. We also introduced Databricks
    Lakehouse Monitoring as another tool for observing data quality. Among its many
    capabilities, Lakehouse Monitoring detects shifts in data distribution and alerts
    users to anomalies, thus preserving data integrity throughout its life cycle.
    We used Databricks Assistant to explore data with ad hoc queries written in English
    and showed why AutoML is an extremely useful tool for data exploration by automatically
    creating comprehensive data exploration notebooks. Together, all of these tools
    create a strong foundation to understand and explore your data. Finally, the chapter
    delved into Databricks VS and how using it to find similar documents can improve
    chatbot responses.
  prefs: []
  type: TYPE_NORMAL
- en: We have now set the foundation for the next phase of our data journey. [*Chapter
    5*](B16865_05.xhtml#_idTextAnchor244) will focus on how to build upon our bronze-layer
    data to create rich sets of features for data science and ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following questions are meant to solidify key points to remember as well
    as tie the content back to your own experience:'
  prefs: []
  type: TYPE_NORMAL
- en: What are some low-code options for data exploration that we discussed in this
    chapter?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When might you use Databricks Assistant for data exploration, and when might
    you use AutoML’s data profile notebook?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How and why would you set expectations on your data?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When would you use a regular database versus a vector database? What are some
    common use cases for vector databases?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting thought into the questions, compare your answers to ours:'
  prefs: []
  type: TYPE_NORMAL
- en: Some low-code data exploration options include using the `ydata` library, in-cell
    data profile, Databricks Assistant, and AutoML.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Databricks Assistant is useful for data exploration when you have a good idea
    of the analyses you want to build and you want code assistance. Databricks Assistant
    is a great way to speed up the coding process or augment your SQL knowledge. On
    the other hand, AutoML is very useful for automatically creating a profile notebook
    that broadly covers your dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We would use Delta Live Tables to set expectations. Expectations are a way to
    flexibly handle data abnormalities and give the options to report bad data, drop
    that data, or fail the pipeline entirely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regular databases, or relational databases, are designed for data in tabular
    form, typically organized in rows or columns. A vector database is designed to
    store vector data, such as embeddings and high-dimensional data. Vector databases
    are optimized for operations based on vector space models, similarity searches,
    image and video analysis, and other ML problems. Some common use cases include
    **Retrieval Augmented Generation** (**RAG**) systems, recommendation systems,
    and image and video recognition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we pointed out specific technologies, technical features,
    and options. Please take a look at these resources to get deeper into the areas
    that interest you most:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Enabling visualizations with Aggregations in* *DBSQL*: [https://docs.databricks.com/sql/user/visualizations/index.html#enable-aggregation-in-a-visualization](https://docs.databricks.com/sql/user/visualizations/index.html#enable-aggregation-in-a-visualization)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the ydata profiler to explore data: [https://ydata-profiling.ydata.ai/docs/master/index.html](https://ydata-profiling.ydata.ai/docs/master/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advancing Spark - Meet the new Databricks* *Assistant*: [https://youtu.be/Tv8D72oI0xM](https://youtu.be/Tv8D72oI0xM)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introducing Databricks Assistant, a context-aware AI* *assistant*: [https://www.databricks.com/blog/introducing-databricks-assistant](https://www.databricks.com/blog/introducing-databricks-assistant)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model monitoring custom metrics* *creation*: [https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html](https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks Vector Search: [https://docs.databricks.com/en/generative-ai/vector-search.html](https://docs.databricks.com/en/generative-ai/vector-search.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'External models in Databricks Model Serving: [https://learn.microsoft.com/en-us/azure/databricks/generative-ai/external-models/](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/external-models/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
