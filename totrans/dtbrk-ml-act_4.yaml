- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Getting to Know Your Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解您的数据
- en: “Truth, like gold, is to be obtained not by its growth, but by washing away
    from it all that is not gold.”
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “真理，就像金子一样，不是通过其增长，而是通过洗去其中所有非金子的东西来获得的。”
- en: ―Leo Tolstoy
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ——列夫·托尔斯泰
- en: In this chapter, we explore features within the Databricks DI Platform that
    help improve and monitor data quality and facilitate data exploration. There are
    numerous approaches to getting to know your data better with Databricks. First,
    we cover how to oversee data quality with **Delta Live Tables** (**DLT**) to catch
    quality issues early and prevent the contamination of entire pipelines. We’ll
    take our first close look at Lakehouse Monitoring, which helps us analyze data
    changes over time and can alert us to changes that concern us. Lakehouse Monitoring
    is a big time-saver, allowing you to focus on mitigating or responding to data
    changes rather than creating notebooks that calculate standard metrics.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了 Databricks DI 平台内的功能，这些功能有助于提高和监控数据质量，并促进数据探索。使用 Databricks 更好地了解您的数据有无数种方法。首先，我们将介绍如何使用
    **Delta Live Tables** (**DLT**) 监督数据质量，以尽早发现质量问题并防止整个管道受到污染。我们将首先深入了解 Lakehouse
    Monitoring，它帮助我们分析数据随时间的变化，并可以提醒我们关注的变化。Lakehouse Monitoring 是一个节省大量时间的功能，让您能够专注于减轻或响应数据变化，而不是创建计算标准指标的笔记本。
- en: 'Moving on to data exploration, we will look at a couple of low-code approaches:
    Databricks Assistant and **AutoML**. Finally, we will touch on embeddings. We
    created embeddings from chunks of text in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123),
    and you’ll learn how to use Databricks **Vector Search** (**VS**) to explore your
    embeddings.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 探索数据，我们将探讨几种低代码方法：Databricks 助手和 **AutoML**。最后，我们将简要介绍嵌入。我们在 [*第 3 章*](B16865_03.xhtml#_idTextAnchor123)
    中从文本块中创建了嵌入，您将学习如何使用 Databricks **向量搜索** (**VS**) 来探索您的嵌入。
- en: 'Here is what you will learn as part of this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的一部分，您将学习以下内容：
- en: Improving data integrity with DLT
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DLT 提高数据完整性
- en: Monitoring data quality with Databricks Lakehouse Monitoring
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Databricks Lakehouse 监控监控数据质量
- en: Exploring data with Databricks Assistant
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Databricks 助手探索数据
- en: Generating data profiles with AutoML
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AutoML 生成数据配置文件
- en: Preparing data for vector search and database indexing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据以进行向量搜索和数据库索引
- en: Enhancing data retrieval with Databricks Vector Search
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Databricks 向量搜索增强数据检索
- en: Applying our learning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: Improving data integrity with DLT
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DLT 提高数据完整性
- en: 'In the last chapter, we introduced DLT as a helpful tool for streaming data
    and pipeline development. Here, we focus on how to use DLT as your go-to tool
    for actively tracking data quality. Generally, datasets are dynamic, not neat,
    and tidy like they often are in school and training. You can use code to clean
    data, of course, but there is a feature that makes the cleaning process even easier:
    DLT’s expectations. DLT’s expectations catch incoming data quality issues and
    automatically validate that incoming data passes specified rules and quality checks.
    For example, you might expect your customer data to have positive values for age
    or that dates follow a specific format. When data does not meet these expectations,
    it can negatively impact downstream data pipelines. With expectations implemented,
    you ensure that your pipelines won’t suffer.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了 DLT 作为流数据管道开发的实用工具。在这里，我们专注于如何将 DLT 作为您的首选工具来积极跟踪数据质量。通常，数据集是动态的，不像在学校和培训中那样整洁有序。当然，您可以使用代码来清理数据，但有一个功能使清理过程更加容易：DLT
    的预期。DLT 的预期捕获传入的数据质量问题，并自动验证传入数据是否通过指定的规则和质量检查。例如，您可能希望客户数据在年龄上有正值，或者日期遵循特定的格式。当数据不符合这些预期时，可能会对下游数据管道产生负面影响。实施预期后，您确保您的管道不会受到影响。
- en: 'Implementing expectations gives us more control over data quality, alerting
    us to unusual data requiring attention and action. There are several options when
    dealing with erroneous data in DLT:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 实施预期给我们更多的数据质量控制，提醒我们注意和采取行动的不寻常数据。在处理 DLT 中的错误数据时，有几个选项：
- en: First, we can set a warning, which will report the number of records that failed
    an expectation as a metric but still write those invalid records to the destination
    dataset; see `@dlt.expect()` in *Figure 4**.1* for an example.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们可以设置一个警告，该警告将报告未通过预期的记录数作为指标，但仍然将那些无效的记录写入目标数据集；请参阅 *图 4* 中的 `@dlt.expect()`
    示例。1
- en: Second, we can drop invalid records so that the final dataset only contains
    records that meet our expectations; see `@dlt.expect_or_drop()` in *Figure 4**.1*.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二，我们可以丢弃无效记录，这样最终数据集只包含满足我们预期的记录；参见*图4**.1*中的`@dlt.expect_or_drop()`。
- en: Third, we can fail the operation entirely, so nothing new is written (note that
    this option requires manual re-triggering of the pipeline).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三，我们可以完全失败操作，这样就不会写入任何新内容（请注意，此选项需要手动重新触发管道）。
- en: Finally, we can quarantine the invalid data to another table to investigate
    it further. The following code should look familiar to the DLT code in [*Chapter
    3*](B16865_03.xhtml#_idTextAnchor123), but now with the addition of expectations.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以将无效数据隔离到另一个表中以进一步调查。以下代码应该与[*第3章*](B16865_03.xhtml#_idTextAnchor123)中的DLT代码相似，但现在增加了预期。
- en: '![Figure 4.1 – Using DLT expectations to enforce data quality](img/B16865_04_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 使用DLT预期来强制数据质量](img/B16865_04_01.jpg)'
- en: Figure 4.1 – Using DLT expectations to enforce data quality
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 使用DLT预期来强制数据质量
- en: Let’s look at our streaming transactions project as an example. In the *Applying
    our learning* section in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123), we used
    DLT to write the transaction data to a table. Utilizing the same DLT code, we
    will save ourselves the manual effort of cleaning the `CustomerID` column by adding
    an expectation to the original code to drop any records with a `null` `CustomerID`.
    We will set another expectation to warn us if the `Product` field is `null`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以我们的流式事务项目为例。在[*第3章*](B16865_03.xhtml#_idTextAnchor123)中的*应用我们的学习*部分，我们使用DLT将交易数据写入表。利用相同的DLT代码，我们将通过在原始代码中添加一个预期来删除任何`CustomerID`为`null`的记录，从而节省手动清理`CustomerID`列的努力。我们还将设置另一个预期，如果`Product`字段为`null`，则发出警告。
- en: Now, when we call `generate_table()`, the DLT pipeline will automatically clean
    up our table by dropping any null `CustomerID` values and flagging records without
    a `Product` value. Moreover, DLT will automatically build helpful visualizations
    to immediately investigate the data’s quality.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们调用`generate_table()`时，DLT管道将自动清理我们的表，删除任何空`CustomerID`值，并标记没有`Product`值的记录。此外，DLT将自动构建有用的可视化，以便立即调查数据的质量。
- en: 'To try this yourself, update the DLT code from [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)
    (here’s the path to the notebook as a reminder: `Chapter 3``: Building` `Out Our`
    `Bronze Layer/Project: Streaming Transactions/delta_live_tables/`) to match *Figure
    4**.1*, and then rerun the DLT pipeline as you did before. Once the pipeline is
    complete, it generates the DAG. Click on the `synthetic_transactions_silver` table,
    then click the **Data Quality** tab from the table details. This will display
    information about the records processed, such as how many were written versus
    dropped for failing a given expectation, as shown in *Figure 4**.2*.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '要自己尝试，请更新[*第3章*](B16865_03.xhtml#_idTextAnchor123)中的DLT代码（这里提供笔记本路径作为提醒：`Chapter
    3``: Building` `Out Our` `Bronze Layer/Project: Streaming Transactions/delta_live_tables/`），以匹配*图4**.1，然后像之前一样重新运行DLT管道。一旦管道完成，它将生成DAG。点击`synthetic_transactions_silver`表，然后从表详情中点击**数据质量**选项卡。这将显示有关已处理的记录的信息，例如有多少条记录因未通过特定预期而被写入或丢弃，如图*图4**.2*所示。'
- en: '![Figure 4.2 – The DLT data quality visualizations](img/B16865_04_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – DLT数据质量可视化](img/B16865_04_02.jpg)'
- en: Figure 4.2 – The DLT data quality visualizations
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – DLT数据质量可视化
- en: These insights illustrate how expectations help automatically clean up our tables
    and flag information that might be useful for data scientists using this table
    downstream. In this example, we see that all records passed the `valid_CustomerID`
    expectation, so now we know we don’t have to worry about null customer IDs in
    the table. Additionally, almost 80% of records are missing a `Product` value,
    which may be relevant for data science and **machine learning** (**ML**) projects
    that use this data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这些见解说明了预期如何帮助我们自动清理表并标记可能对下游使用此表的数据科学家有用的信息。在这个例子中，我们看到所有记录都通过了`valid_CustomerID`预期，因此现在我们知道我们不必担心表中存在空客户ID。此外，近80%的记录缺少`Product`值，这可能对使用此数据的数据科学和**机器学习**（**ML**）项目相关。
- en: Just as we’ve considered the correctness and consistency of incoming data, we
    also want to consider how we can expand our data quality oversight to include
    data drift, for example, when your data’s distribution changes over time. Observing
    data drift is where Databricks Lakehouse Monitoring emerges as a vital complement
    to DLT, offering a configurable framework to consistently observe and verify the
    statistical properties and quality of input data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经考虑了传入数据的正确性和一致性，我们还想考虑如何扩大我们的数据质量监督范围，包括数据漂移，例如，当您的数据分布随时间变化时。观察数据漂移是
    Databricks Lakehouse 监控作为 DLT 的重要补充出现的地方，它提供了一个可配置的框架，以一致地观察和验证输入数据的统计特性和质量。
- en: Monitoring data quality with Databricks Lakehouse Monitoring
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Databricks Lakehouse 监控监控数据质量
- en: Use Databricks Lakehouse Monitoring to proactively detect and respond to any
    deviations in your data distribution. Over time, your data may undergo changes
    in its underlying patterns. This could be feature drift, where the distribution
    of feature data changes over time, or concept drift, where the relationship between
    inputs and outputs of your model changes. Both types of drift can cause model
    quality to suffer. These changes can occur slowly or rapidly in your production
    environment, which is why monitoring your data even before it becomes an input
    into your ML models and data products is essential.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Databricks Lakehouse 监控来主动检测和应对数据分布中的任何偏差。随着时间的推移，数据可能在其基本模式上发生变化。这可能是特征漂移，即特征数据的分布随时间变化，或者是概念漂移，即模型输入和输出之间的关系发生变化。这两种类型的漂移都可能导致模型质量下降。这些变化可能在生产环境中缓慢或迅速发生，这就是为什么在数据成为您的机器学习模型和数据产品的输入之前监控数据至关重要。
- en: Mechanics of Lakehouse Monitoring
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lakehouse 监控的机制
- en: 'To monitor a table in Databricks, you create a monitor attached to that table.
    To monitor the performance of a ML model, you attach the monitor to an inference
    table that holds the model’s inputs and corresponding predictions. Databricks
    Lakehouse Monitoring provides the following profile types of analysis: snapshot,
    time series, and inference.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Databricks 中监控一个表，您需要创建一个附加到该表的监控。要监控机器学习模型的性能，您需要将监控附加到包含模型输入和相应预测的推理表。Databricks
    Lakehouse 监控提供以下配置文件类型的分析：快照、时间序列和推理。
- en: In addition to selecting the table to be monitored, called the **primary table**,
    you can optionally specify a baseline table to reference for measuring drift or
    the change in values over time. A baseline table is useful when you have a sample
    of what you expect your data to look like, such as the data with which your model
    was trained. Lakehouse Monitoring automatically computes drift relative to expected
    data values and distributions of the baseline table.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择要监控的表，称为**主表**，您还可以选择性地指定一个基线表以参考测量漂移或随时间变化的价值。当您有一个您期望数据看起来如何的样本时，基线表非常有用，例如，使用您训练模型的数据。Lakehouse
    监控自动计算与基线表预期数据值和分布的漂移。
- en: Creating a table monitor automatically creates two metric tables, `profile_metrics`
    and `drift_metrics`. Lakehouse Monitoring computes metric values on the table
    for the time windows and data subsets or “slices” you specify when you create
    the monitor. You can also add your own custom metrics; see *Further reading* for
    details.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 自动创建表监控会自动创建两个度量表，`profile_metrics` 和 `drift_metrics`。Lakehouse 监控在您创建监控时指定的表上计算度量值，包括时间窗口和数据子集或“切片”。您还可以添加自己的自定义度量；有关详细信息，请参阅*进一步阅读*。
- en: Visualization and alerting
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化和警报
- en: Lakehouse Monitoring generates an SQL dashboard automatically for every monitor.
    These dashboards provide a crucial platform for examining metrics and acting on
    results. Databricks’ alert system serves as a vigilant guardian, promptly notifying
    you of the significant shifts in data quality or distributions you subscribe to.
    *Figure 4**.3* shows how all the Lakehouse Monitoring components work together,
    using the data from the primary table and optional baseline table to generate
    a profile metrics table and drift table, which then populate the dashboard and
    power alerts.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Lakehouse 监控为每个监控自动生成一个 SQL 仪表板。这些仪表板为检查指标和采取行动提供了一个关键平台。Databricks 的警报系统充当一个警惕的守护者，及时通知您订阅的数据质量或分布的重大变化。*图
    4**.3* 展示了 Lakehouse 监控的所有组件如何协同工作，使用主表和可选的基线表中的数据生成配置文件度量表和漂移表，然后填充仪表板并触发警报。
- en: '![Figure 4.3 – Relationship between the input tables, the metric tables, the
    monitor, and the dashboard](img/B16865_04_03.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – 输入表、指标表、监控器和仪表板之间的关系](img/B16865_04_03.jpg)'
- en: Figure 4.3 – Relationship between the input tables, the metric tables, the monitor,
    and the dashboard
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – 输入表、指标表、监控器和仪表板之间的关系
- en: Creating a monitor
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建监控器
- en: 'You can create a Databricks Lakehouse Monitor using the **user interface**
    (**UI**) or, for a more flexible and programmable approach, you can use the API.
    The API method is particularly advantageous when you want to script the creation
    of monitors to integrate them into your automated data pipelines. The following
    is a high-level summary of the steps to create a Lakehouse Monitor using the API:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 **用户界面**（**UI**）或，为了更灵活和可编程的方法，您可以使用 API 来创建 Databricks Lakehouse 监控器。当您想要编写脚本来创建监控器并将其集成到自动化数据管道中时，API
    方法特别有利。以下是用 API 创建 Lakehouse 监控器步骤的高级概述：
- en: '`Snapshot`, `TimeSeries`, and `Inference`. Each type is suitable for different
    monitoring scenarios, with the `TimeSeries` and `Inference` types requiring a
    timestamp column. The inference profile also requires `prediction_col` and `model_id_col`.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Snapshot`、`TimeSeries` 和 `Inference`。每种类型适用于不同的监控场景，其中 `TimeSeries` 和 `Inference`
    类型需要时间戳列。推理配置文件还需要 `prediction_col` 和 `model_id_col`。'
- en: '`lakehouse_monitoring` module to call the `create_monitor` function, providing
    your table’s catalog schema, table name, the chosen profile type, and the output
    schema for the monitor’s results.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `lakehouse_monitoring` 模块调用 `create_monitor` 函数，提供您的表的目录模式、表名、选择的配置文件类型以及监控器结果的输出模式。
- en: '`MonitorCronSchedule` object, which takes a cron expression and a time zone
    ID.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`MonitorCronSchedule` 对象，它接受 cron 表达式和时间区域 ID。'
- en: '**Control access**: After you create the monitor, you can manage access to
    the resulting metrics tables and dashboard using Unity Catalog privileges.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**控制访问**：创建监控器后，您可以使用 Unity Catalog 权限管理结果指标表和仪表板的访问权限。'
- en: '`run_refresh` function to refresh and update the metric tables. You can also
    check the status of specific runs with the `get_refresh` function and list all
    refreshes associated with a monitor using `list_refreshes`.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`run_refresh` 函数用于刷新和更新指标表。您还可以使用 `get_refresh` 函数检查特定运行的状况，并使用 `list_refreshes`
    列出与监控器关联的所有刷新操作。'
- en: '`get_monitor` function allows you to retrieve your monitor’s current settings
    for review.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`get_monitor` 函数允许您检索监控器的当前设置以供审查。'
- en: '*Figure 4**.4* shows an example of how to use the Lakehouse Monitoring API
    to create a `TimeSeries` profile monitor:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4**.4* 展示了如何使用 Lakehouse 监控 API 创建 `TimeSeries` 配置文件监控器的示例：'
- en: '![Figure 4.4 – Creating a simple TimeSeries table monitor](img/B16865_04_04.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – 创建简单的 TimeSeries 表监控](img/B16865_04_04.jpg)'
- en: Figure 4.4 – Creating a simple TimeSeries table monitor
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 创建简单的 TimeSeries 表监控器
- en: After establishing a robust framework for data quality monitoring with Databricks
    Lakehouse Monitoring, we can focus on enhancing our data exploration. This leads
    us to Databricks Assistant, a feature dedicated to helping developers be more
    productive in Databricks.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Databricks Lakehouse 监控建立稳健的数据质量监控框架后，我们可以专注于增强我们的数据探索。这使我们转向 Databricks
    助手，这是一个专门帮助开发者提高 Databricks 生产力功能。
- en: Exploring data with Databricks Assistant
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Databricks 助手探索数据
- en: 'Databricks Assistant is a feature designed to boost your productivity in Databricks.
    It has many capabilities, including generating SQL from English, explaining code,
    helping troubleshoot errors, and optimizing code. Databricks Assistant is an exciting
    feature to watch as more capabilities emerge, and we want to give you a taste
    of the possibilities. Since this chapter is about exploring and monitoring data,
    let’s see how you can use Databricks Assistant as a low-code solution to explore
    your data. Suppose you are analyzing the Favorita Sales Forecasting data. You
    are looking to uncover insights into retail store distributions across various
    regions. You have a specific query in mind: you want to understand the store landscape
    in the Guayas region. However, SQL queries aren’t your strong suit, and maybe
    crafting the perfect query seems daunting. In order to explore your data regardless,
    you can use Databricks Assistant. There is no notebook in the project repo for
    this section, but we encourage you to try Databricks Assistant on the Favorita
    Forecasting project tables. Any of the [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180)
    Favorita notebooks would be a great place to access Databricks Assistant. To access
    it, click on the icon shown in *Figure 4**.5*, found in the left-hand sidebar
    of a notebook. Clicking on the icon will open the chat interface to the left of
    the notebook, where we will type in our questions.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 助手是一个旨在提高你在 Databricks 中生产力的功能。它具有许多功能，包括从英语生成 SQL、解释代码、帮助调试错误和优化代码。随着更多功能的出现，Databricks
    助手是一个令人兴奋的功能，我们希望让你体验一下这些可能性。由于本章是关于探索和监控数据，让我们看看你如何使用 Databricks 助手作为低代码解决方案来探索你的数据。假设你正在分析
    Favorita 销售预测数据。你希望了解不同地区零售商店的分布情况。你心中有一个特定的查询：你想要了解瓜亚斯地区的商店格局。然而，SQL 查询并不是你的强项，也许编写完美的查询看起来很令人畏惧。为了探索你的数据，你可以使用
    Databricks 助手。本节的项目仓库中没有笔记本，但我们鼓励你尝试在 Favorita 预测项目表上使用 Databricks 助手。任何 [*第 4
    章*](B16865_04.xhtml#_idTextAnchor180) 的 Favorita 笔记本都是一个很好的地方来访问 Databricks 助手。要访问它，点击笔记本左侧侧边栏中显示的图标（*图
    4**.5*），点击图标将打开笔记本左侧的聊天界面，我们将在此处输入我们的问题。
- en: '![Figure 4.5 – The Databricks Assistant icon](img/B16865_04_05.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5 – Databricks 助手图标](img/B16865_04_05.jpg)'
- en: Figure 4.5 – The Databricks Assistant icon
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – Databricks 助手图标
- en: First, we ask Databricks Assistant how many stores and store types are in the
    state of Guayas in the `favorita_stores` table (*Figure 4**.6*). Note that Databricks
    Assistant does not require Unity Catalog, but using Unity Catalog provides table
    information. The additional information makes Databricks Assistant’s responses
    more helpful and specific to the table you’re working with. This should sound
    similar to the **Retrieval Augmented Generation** (**RAG**) project. We are augmenting
    the generation of answers by providing relevant information. Now, let’s see whether
    it can help us write the SQL query we need. Keep in mind that Databricks Assistant
    is powered by **generative AI**, so you may see different outputs when using it
    yourself.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们向 Databricks 助手询问在 `favorita_stores` 表中（*图 4**.6*）有多少家商店和商店类型。请注意，Databricks
    助手不需要 Unity Catalog，但使用 Unity Catalog 可以提供表信息。额外的信息使得 Databricks 助手的回答更加有用，并且更具体于你正在处理的表。这应该听起来类似于
    **检索增强生成**（**RAG**）项目。我们通过提供相关信息来增强答案的生成。现在，让我们看看它是否可以帮助我们编写所需的 SQL 查询。记住，Databricks
    助手由 **生成式 AI** 驱动，因此当你自己使用它时可能会看到不同的输出。
- en: '![Figure 4.6 – Question and response interaction with Databricks Assistant](img/B16865_04_06.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 与 Databricks 助手的提问和回答交互](img/B16865_04_06.jpg)'
- en: Figure 4.6 – Question and response interaction with Databricks Assistant
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 – 与 Databricks 助手的提问和回答交互
- en: Nice! Databricks Assistant gave us some SQL code to paste into a notebook and
    run directly. However, a quick scan of the query shows us that we will only get
    a distinct count of stores and types rather than looking at the store type distribution
    we want. Let’s refine our question and try again (*Figure 4**.7*).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！Databricks 助手给了我们一些 SQL 代码，可以直接粘贴到笔记本中并运行。然而，快速浏览查询后，我们发现我们只能得到商店和类型的唯一计数，而不是查看我们想要的商店类型分布。让我们完善我们的问题，然后再次尝试（*图
    4**.7*）。
- en: '![Figure 4.7 – The updated question submitted to Databricks Assistant and the
    results](img/B16865_04_07.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 提交给 Databricks 助手的更新问题及其结果](img/B16865_04_07.jpg)'
- en: Figure 4.7 – The updated question submitted to Databricks Assistant and the
    results
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 提交给 Databricks 助手的更新问题及其结果
- en: After submitting the second question in *Figure 4**.7*, Databricks Assistant
    provides a new query that accurately captures what we want to know. To make sure,
    let’s copy the SQL code provided, paste it into a notebook, and run it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 4.7* 中提交第二个问题后，Databricks 助手提供了一个新的查询，准确地捕捉了我们想要了解的内容。为了确保这一点，让我们复制提供的
    SQL 代码，将其粘贴到笔记本中，并运行它。
- en: '![Figure 4.8 – Results from Databricks Assistant-generated SQL query](img/B16865_04_08.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – Databricks 助手生成的 SQL 查询结果](img/B16865_04_08.jpg)'
- en: Figure 4.8 – Results from Databricks Assistant-generated SQL query
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – Databricks 助手生成的 SQL 查询结果
- en: Now we see the distribution of stores by type in the Guayas region, just as
    we wanted. Databricks Assistant is a handy tool, and we can confirm that playing
    with it is also fun! We encourage you to try it out on your own to see how you
    can use English to explore your data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们看到瓜亚基尔地区的商店类型分布，正如我们希望的那样。Databricks 助手是一个实用的工具，我们可以确认玩弄它也是一件有趣的事情！我们鼓励您亲自尝试，看看您如何可以使用英语来探索您的数据。
- en: Databricks Assistant also generates comment suggestions for your tables and
    fields. From the `favorita_stores` table we explored previously. We see in *Figure
    4**.9* that Databricks Assistant has a suggestion for a table comment to help
    others understand the table’s contents.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 助手还为您的表格和字段生成注释建议。从之前探索的 `favorita_stores` 表格中，我们在 *图 4.9* 中看到 Databricks
    助手为表格注释提出了建议，以帮助其他人理解表格的内容。
- en: '![Figure 4.9 – Databricks Assistant suggests a descriptive table comment](img/B16865_04_09.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – Databricks 助手建议一个描述性的表格注释](img/B16865_04_09.jpg)'
- en: Figure 4.9 – Databricks Assistant suggests a descriptive table comment
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – Databricks 助手建议一个描述性的表格注释
- en: Databricks Assistant can also generate descriptive comments for each field in
    the table by selecting the **AI Generate** button on the right-hand side of the
    table’s **Columns** tab page. You can accept or edit the suggested comments. You
    can toggle off the suggestions by selecting the **Hide AI suggestions** button,
    as shown in *Figure 4**.10*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks 助手还可以通过在表格的“列”标签页右侧选择“**AI 生成**”按钮为表格中的每个字段生成描述性注释。您可以接受或编辑建议的注释。您可以通过选择“**隐藏
    AI 建议**”按钮来关闭建议，如图 *图 4.10* 所示。10*。
- en: '![Figure 4.10 – Databricks Assistant suggests comments for the columns in a
    table](img/B16865_04_10.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – Databricks 助手为表格中的列建议注释](img/B16865_04_10.jpg)'
- en: Figure 4.10 – Databricks Assistant suggests comments for the columns in a table
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – Databricks 助手为表格中的列建议注释
- en: The generated comments might seem like they fall outside the scope of getting
    to know your data, but documentation is vital to making your data more easily
    discoverable for everyone else (and let’s hope others use generated comments so
    you can explore their datasets more easily too). The faster you understand a dataset,
    the easier it is to further explore that data.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的注释可能看起来像是超出了了解您数据的范围，但文档对于使您的数据更容易被其他人发现至关重要（并且我们希望其他人也能使用生成的注释，这样您就可以更轻松地探索他们的数据集）。您越快了解一个数据集，就越容易进一步探索这些数据。
- en: 'Databricks Assistant is a great way to analyze your data when you prefer to
    work in English rather than code directly, and when you have specific questions
    in mind. Now let’s discuss another method for broader **exploratory data analysis**
    (**EDA**): autogenerated notebooks using AutoML.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当您更喜欢用英语而不是直接编写代码来分析数据时，Databricks 助手是分析您数据的一个很好的方式，并且当您心中已有具体问题时。现在让我们讨论另一种更广泛的
    **探索性数据分析**（**EDA**）方法：使用 AutoML 自动生成的笔记本。
- en: Generating data profiles with AutoML
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AutoML 生成数据概要
- en: 'We introduced Databricks AutoML in [*Chapter 1*](B16865_01.xhtml#_idTextAnchor016).
    This tool automates ML development and augments data science workflows. AutoML
    is best known for generating models, but we’ll get to modeling in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
    Since we’re talking about getting to know your data, we first want to focus on
    one extremely useful feature built into AutoML that often flies under the radar:
    autogenerated Python notebooks. AutoML provides a notebook for data exploration
    in addition to the notebook code for every experiment it runs. We will jump right
    into creating an AutoML experiment, view the data exploration code, and then return
    to explore the modeling portion later.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第一章*](B16865_01.xhtml#_idTextAnchor016)中介绍了Databricks AutoML。这个工具自动化ML开发并增强数据科学工作流程。AutoML最出名的是生成模型，但我们将会在[*第六章*](B16865_06.xhtml#_idTextAnchor297)中讨论建模。由于我们正在讨论了解您的数据，我们首先想关注AutoML中内置的一个极其有用的功能，这个功能通常被忽视：自动生成的Python笔记本。除了为每个运行的实验提供笔记本代码外，AutoML还提供了一个用于数据探索的笔记本。我们将直接进入创建AutoML实验，查看数据探索代码，然后稍后再返回探索建模部分。
- en: 'We’ll cover how to create an AutoML experiment via an API in the Favorita project
    notebooks. We encourage you to follow the instructions here to set up a simple
    regression experiment with the AutoML UI, so that we can take a look at the data
    profile created. Before you begin, make sure you have a DBR ML 9.1+ cluster running
    (you can use the **DBR ML 14.2** cluster set up in [*Chapter 2*](B16865_02.xhtml#_idTextAnchor073)):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Favorita项目笔记本中介绍如何通过API创建AutoML实验。我们鼓励您遵循这里的说明，使用AutoML UI设置一个简单的回归实验，这样我们可以查看创建的数据概要。在您开始之前，请确保您有一个DBR
    ML 9.1+集群正在运行（您可以使用[*第二章*](B16865_02.xhtml#_idTextAnchor073)中设置的**DBR ML 14.2**集群）：
- en: '**Start the experiment**: Navigate to the **Experiments** tab on the platform’s
    left-hand navigation bar. Click the **AutoML Experiment** button to initiate a
    new experiment.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**启动实验**：导航到平台左侧导航栏上的**实验**标签页。点击**AutoML实验**按钮以启动新实验。'
- en: '`ml_in_action.favorita_forecasting.favorite_train_set` table, or your own data
    (just make sure to update the problem type if `Regression` does not apply).'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ml_in_action.favorita_forecasting.favorite_train_set` 表，或您自己的数据（只需确保如果`Regression`不适用，更新问题类型）。'
- en: '**Prediction target**: Choose the specific column in your dataset that you
    want to predict. The AutoML process will use the other columns in your dataset
    to try and predict the values in this target column. If you’re following along
    with the Favorita scenario, select **sales**.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预测目标**：选择您数据集中想要预测的特定列。AutoML过程将使用您数据集中的其他列来尝试预测目标列中的值。如果您正在跟随Favorita场景，请选择**销售**。'
- en: '*Figure 4**.11* shows the configuration of an AutoML experiment using the `favorite_train_set`
    training table. It illustrates how you can customize the AutoML process to fit
    the specific requirements of your ML task. By selecting the appropriate problem
    type, dataset, and prediction target, you’re instructing the AutoML system on
    how to approach the model-building process.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.11* 展示了使用`favorite_train_set`训练表进行AutoML实验的配置。它说明了您如何自定义AutoML过程以适应您ML任务的特定要求。通过选择适当的问题类型、数据集和预测目标，您指导AutoML系统如何进行模型构建过程。'
- en: '![Figure 4.11 – Configuration of an AutoML experiment using the Favorita train_set
    table](img/B16865_04_11.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图4.11 – 使用Favorita train_set表的AutoML实验配置](img/B16865_04_11.jpg)'
- en: Figure 4.11 – Configuration of an AutoML experiment using the Favorita train_set
    table
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 – 使用Favorita train_set表的AutoML实验配置
- en: 'Once you’ve filled out the UI and specified the Favorita table (or another
    table of your choice) as your input training dataset, click **Start AutoML** at
    the bottom of the screen. AutoML experiments may run for up to several hours as
    improvements in the evaluation metric continue, although you can set a shorter
    time limit for the experiment by changing the **Timeout** value (found under **Advanced**
    **Configuration**). As the experiment begins, a new page will open with progress
    updates. Once the experiment is complete, you will see links to two data artifacts:
    the notebook containing the code for the best model, labeled **View notebook for**
    **best model**, and the data exploration notebook, labeled **View data exploration
    notebook**, as shown in *Figure 4**.12*.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您填写了UI并指定了Favorita表（或您选择的另一个表）作为您的输入训练数据集，请点击屏幕底部的**开始AutoML**。AutoML实验可能需要数小时才能完成，因为评估指标的改进仍在继续，尽管您可以通过更改**超时**值（在**高级**
    **配置**下找到）为实验设置较短的时间限制。实验开始时，将打开一个新页面，显示进度更新。一旦实验完成，您将看到两个数据实体的链接：包含最佳模型代码的笔记本，标记为**查看最佳模型笔记本**，以及数据探索笔记本，标记为**查看数据探索笔记本**，如图*图4.12*所示。
- en: '![Figure 4.12 – The AutoML experiment page with the links to view the notebook
    for the best model and the data exploration notebook](img/B16865_04_12.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图4.12 – 包含查看最佳模型笔记本和数据探索笔记本链接的AutoML实验页面](img/B16865_04_12.jpg)'
- en: Figure 4.12 – The AutoML experiment page with the links to view the notebook
    for the best model and the data exploration notebook
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 – 包含查看最佳模型笔记本和数据探索笔记本链接的AutoML实验页面
- en: The exploration notebook uses `ydata-profiling`, formerly referred to as pandas’
    profiler library, to generate statistics and summarize data for all the fields
    in the table. It also provides alerts on fields with high correlation issues,
    which could negatively impact models. These warnings are also available in the
    MLflow experiment UI, as shown in *Figure 4**.13*.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 探索笔记本使用`ydata-profiling`（以前称为pandas的profiler库）生成统计数据并总结表中的所有字段数据。它还提供有关具有高度相关问题的字段的警报，这可能会对模型产生负面影响。这些警告也显示在MLflow实验UI中，如图*图4.13*所示。
- en: '![Figure 4.13 – AutoML warnings called out during the experiment run](img/B16865_04_13.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图4.13 – 实验运行期间调用的AutoML警告](img/B16865_04_13.jpg)'
- en: Figure 4.13 – AutoML warnings called out during the experiment run
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 – 实验运行期间调用的AutoML警告
- en: Look through the data exploration notebook for an overview of your data, from
    summary statistics to thorough profiles for each variable.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 查看数据探索笔记本，以了解您数据的概览，从汇总统计到每个变量的详细配置文件。
- en: 'We have now explored two tools for data exploration: Databricks Assistant and
    the `ydata-profiling` library. These are great places to start for many classical
    ML projects. Next, we’ll discuss a more advanced data format and how you can use
    the DI Platform to explore it: data embeddings and vector search. Embeddings are
    advanced transformations that translate complex, unstructured data into a numerical
    format conducive to ML algorithms, capturing intricate relationships within the
    data that are pivotal for sophisticated models.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经探索了两种数据探索工具：Databricks Assistant和`ydata-profiling`库。这些是许多经典机器学习项目开始的绝佳选择。接下来，我们将讨论更高级的数据格式以及如何使用DI平台来探索它：数据嵌入和向量搜索。嵌入是高级转换，将复杂、非结构化数据转换为有利于机器学习算法的数值格式，捕捉数据中的复杂关系，这对于复杂模型至关重要。
- en: Using embeddings to understand unstructured data
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用嵌入来理解非结构化数据
- en: So far, we’ve focused on how to explore your structured data. What about unstructured
    data, such as images or text? Recall that we converted PDF text chunks into a
    specific format called embeddings in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)’s
    RAG chatbot project work. We require embeddings, meaning numerical vector representations
    of the data, to perform a similarity (or hybrid) search between chunks of text.
    That way, when someone asks our chatbot a question, such as “What are the economic
    impacts of automation technologies using LLMs?” the chatbot will be able to search
    through the stored chunks of text from the arXiv articles, retrieve the most relevant
    chunks, and use those to better answer the question. For more visual readers,
    see the data preparation workflow in *Figure 4**.14*. We completed the **Data
    Preparation** step in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123). We’ll run
    through the remaining setup steps in the workflow now.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们关注的是如何探索您的结构化数据。那么非结构化数据，如图像或文本呢？回想一下，我们在 [*第 3 章*](B16865_03.xhtml#_idTextAnchor123)
    的 RAG 聊天机器人项目工作中将 PDF 文本块转换为称为嵌入的特定格式。我们需要嵌入，即数据的数值向量表示，以在文本块之间执行相似性（或混合）搜索。这样，当有人向我们的聊天机器人提问，例如“使用
    LLM 的自动化技术对经济有什么影响？”时，聊天机器人将能够搜索存储在 arXiv 文章中的文本块，检索最相关的块，并使用这些块更好地回答问题。对于更倾向于视觉阅读的读者，请参阅
    *图 4**.14* 中的数据准备工作流程。我们在 [*第 3 章*](B16865_03.xhtml#_idTextAnchor123) 中完成了 **数据准备**
    步骤。现在我们将运行工作流程中的剩余设置步骤。
- en: '![Figure 4.14 – Vector database setup is the prerequisite process supporting
    RAG’s retrieval step](img/B16865_04_14.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14 – 向量数据库设置是支持 RAG 检索步骤的先决过程](img/B16865_04_14.jpg)'
- en: Figure 4.14 – Vector database setup is the prerequisite process supporting RAG’s
    retrieval step
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – 向量数据库设置是支持 RAG 检索步骤的先决过程
- en: Embeddings are an essential part of building any chatbot. Pay attention to your
    embedding model to ensure it is relevant to the task. You wouldn’t want to build
    a chatbot designed to answer questions in French but use an embedding model that
    only knows English – your chatbot’s response quality will definitely suffer!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入是构建任何聊天机器人的基本部分。请注意您的嵌入模型，确保它与任务相关。您不希望构建一个旨在用法语回答问题的聊天机器人，但使用一个只知道英语的嵌入模型——您的聊天机器人的响应质量肯定会受到影响！
- en: Embeddings that capture the nuances of language are essential for the chatbot
    to understand and generate contextually relevant responses. Equally important
    are the searching and filtering techniques you apply to the **vector database**
    itself. A vector database is similar to a SQL database, but instead of storing
    tabular data, it stores vector embeddings. A search algorithm can then search
    the embeddings. In the final flow of a RAG project, a user’s question is also
    converted into embeddings, and the search algorithm uses those embeddings to find
    similar embeddings stored in the vector database. The chatbot receives the most
    similar embeddings from the vector database to help it craft a response to the
    user’s question.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 捕捉语言细微差别的嵌入对于聊天机器人理解和生成上下文相关的响应至关重要。同样重要的是您应用于**向量数据库**本身的搜索和过滤技术。向量数据库类似于 SQL
    数据库，但它存储的是向量嵌入，而不是表格数据。然后搜索算法可以搜索这些嵌入。在 RAG 项目的最终流程中，用户的提问也被转换为嵌入，搜索算法使用这些嵌入在向量数据库中找到相似的嵌入。聊天机器人从向量数据库中获取最相似的嵌入，以帮助它为用户的提问构建响应。
- en: 'Let’s consider the requirements of a good vector database solution:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个好的向量数据库解决方案的要求：
- en: '**Quality of the retrievals**: The correctness and completeness of embeddings
    returned as relevant by the search algorithm'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检索质量**：搜索算法返回的作为相关内容的嵌入的正确性和完整性'
- en: '**Scalability of the solution**: The ability to scale per the number of requests
    coming to the application with dynamic traffic'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解决方案的可扩展性**：根据应用程序接收到的请求数量和动态流量进行扩展的能力'
- en: '**Accessibility**: The ability to easily access, read, write to, and use the
    application in real time'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可访问性**：能够轻松访问、实时读取、写入和使用应用程序的能力'
- en: '**Governance**: The ability to govern vector storage with the same access controls
    as the original sources used to create the vector embeddings and models'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**治理**：使用与创建向量嵌入和模型所使用的原始源相同的访问控制来管理向量存储的能力'
- en: '**Integration**: The ability to easily integrate with current market technologies
    and eliminate time spent stitching technologies and solutions together'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成**：能够轻松集成当前市场技术并消除拼接技术和解决方案所需的时间'
- en: 'Using embeddings and vector search is a powerful way to improve a variety of
    ML projects. There are many uses for vector databases, the most common of which
    are as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌入和向量搜索是改进各种机器学习项目的强大方式。向量数据库有许多用途，其中最常见的是以下几种：
- en: '**RAG systems**: Vector search facilitates efficient data retrieval, which
    is then used to augment a **Large Language Model** (**LLM**)’s response. Augmenting
    an LLM with results from vector search leads to more accurate chatbot responses
    and minimizes errors such as hallucinations in LLM outputs.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RAG系统**：向量搜索促进了高效的数据检索，随后这些数据被用于增强**大型语言模型**（**LLM**）的响应。通过向量搜索的结果增强LLM，可以导致聊天机器人响应更加准确，并最小化LLM输出中的幻觉等错误。'
- en: '**Recommendation systems**: E-commerce and streaming platforms use vector search
    for efficient nearest-neighbor searches, matching user behavior with relevant
    suggestions.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐系统**：电子商务和流媒体平台使用向量搜索进行高效的最近邻搜索，将用户行为与相关建议相匹配。'
- en: '**Image and video recognition**: Vector search facilitates quick searches for
    similar features in images and videos.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像和视频识别**：向量搜索促进了在图像和视频中快速搜索相似特征。'
- en: '**Bioinformatics**: Vector search can be applied to tasks such as DNA sequence
    alignment and protein structure similarity search to improve clinical research.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生物信息学**：向量搜索可用于DNA序列比对和蛋白质结构相似性搜索等任务，以提升临床研究。'
- en: Enhancing data retrieval with Databricks Vector Search
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Databricks向量搜索增强数据检索
- en: Databricks VS is transforming how we refine and retrieve data for LLMs. Functioning
    as a serverless similarity search engine, VS enables the storage of vector embeddings
    and metadata in a dedicated vector database. Through VS, you can generate dynamic
    vector search indices from **Delta** tables overseen by Unity Catalog. Using a
    straightforward API, you can retrieve the most similar vectors through queries.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks VS正在改变我们为LLM精炼和检索数据的方式。作为无服务器相似性搜索引擎，VS允许在专用向量数据库中存储向量嵌入和元数据。通过VS，您可以从Unity
    Catalog管理的**Delta**表中生成动态向量搜索索引。使用简单的API，您可以通过查询检索最相似的向量。
- en: 'Here are some of Databricks VS’s key benefits:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Databricks VS的一些关键优势：
- en: '**Seamless integration**: VS works harmoniously within Databricks’ ecosystem,
    particularly Delta tables. This integration ensures that your data is always up
    to date, making it model-ready for ML applications. With VS, you can create a
    vector search index from a source Delta table and set the index to sync when the
    source table is updated.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无缝集成**：VS在Databricks生态系统内和谐工作，尤其是在Delta表方面。这种集成确保了您的数据始终保持最新，使其为机器学习应用做好准备。使用VS，您可以从源Delta表创建向量搜索索引，并在源表更新时设置索引同步。'
- en: '**Streamlined operations**: VS significantly simplifies operational complexity
    by eliminating the need to manage third-party vector databases. VS runs on serverless
    compute, meaning Databricks handles the infrastructure management for you.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化操作**：VS通过消除管理第三方向量数据库的需要，显著简化了操作复杂性。VS在无服务器计算上运行，这意味着Databricks为您处理基础设施管理。'
- en: '**Enhanced scalability**: Unlike standalone vector libraries, VS offers unparalleled
    scalability. VS handles large-scale data effortlessly, scaling automatically to
    meet the demands of your data and query load. This scalability is crucial for
    organizations with vast amounts of data and complex search requirements.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强的可扩展性**：与独立的向量库不同，VS提供了无与伦比的可扩展性。VS轻松处理大规模数据，自动扩展以满足您数据和查询负载的需求。这种可扩展性对于拥有大量数据且具有复杂搜索要求的企业至关重要。'
- en: '**Unified data asset governance**: VS integrates with Unity Catalog; Unity
    Catalog handles data governance and access control lists. To prevent productional
    data leakage, you can manage access to the Databricks VS API and the underlying
    databases with Unity Catalog.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统一的数据资产管理**：VS与Unity Catalog集成；Unity Catalog处理数据治理和访问控制列表。为了防止生产数据泄露，您可以使用Unity
    Catalog管理对Databricks VS API和底层数据库的访问。'
- en: '**Model Serving integration**: Model Serving automates querying of the model
    serving endpoint for embedding generation without any overhead from users.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型服务集成**：模型服务自动化了模型服务端点的查询，以生成嵌入，无需用户承担任何开销。'
- en: Flexibility in embedding model support
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持嵌入模型的灵活性
- en: One of VS’s major strengths is its support for any of the embedding models of
    your choice. VS supports hosted or fully managed embeddings. Hosted embeddings
    are self-managed. You create the embeddings and save them on a Delta table. For
    fully managed embeddings, the prepared text is saved in a Delta table, and embeddings
    are created by Databricks Model Serving. Model Serving will convert your incoming
    data into embeddings with the model of your choice. Databricks VS can support
    any model through the Databricks Model Serving endpoints, the Foundation Model
    API (as mentioned in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)), or external
    models. External models include SaaS models, for example, OpenAI’s ChatGPT and
    Anthropic’s PaLM. You can connect external models through the Databricks unified
    model serving gateway. See *External models in Databricks Model Serving* in *Further
    reading* for more information.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: VS的一个主要优势是它支持你选择的任何嵌入模型。VS支持托管或完全管理的嵌入。托管嵌入是自行管理的。你创建嵌入并将它们保存在Delta表中。对于完全管理的嵌入，准备好的文本保存在Delta表中，嵌入由Databricks
    Model Serving创建。Model Serving将使用你选择的模型将你的传入数据转换为嵌入。Databricks VS可以通过Databricks
    Model Serving端点、基础模型API（如[*第3章*](B16865_03.xhtml#_idTextAnchor123)中所述）或外部模型支持任何模型。外部模型包括SaaS模型，例如OpenAI的ChatGPT和Anthropic的PaLM。你可以通过Databricks统一模型服务网关连接外部模型。有关更多信息，请参阅“进一步阅读”中的*“Databricks
    Model Serving中的外部模型”*。
- en: Setting up a vector search
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置向量搜索
- en: As mentioned, VS is a serverless product. Hence, we require a real-time connection
    with our chatbot application to the relevant content stored in the vector database.
    We’ll cover this again in the *Applying our learning*, *Project – RAG chatbot*
    section, but if you’re ready to set up your own endpoint, you just need a few
    lines of code, as shown in *Figure 4**.15*.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，VS是一个无服务器产品。因此，我们需要与我们的聊天机器人应用程序到向量数据库中存储的相关内容保持实时连接。我们将在“应用我们的学习”，“项目
    – RAG聊天机器人”部分再次介绍这一点，但如果你准备好设置自己的端点，你只需要几行代码，如图*4.15*所示。
- en: '![Figure 4.15 – Creating a Databricks VS endpoint](img/B16865_04_15.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图4.15 – 创建Databricks VS端点](img/B16865_04_15.jpg)'
- en: Figure 4.15 – Creating a Databricks VS endpoint
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 – 创建Databricks VS端点
- en: Once you create an endpoint, you can host multiple vector search indices under
    one endpoint. The endpoint will scale according to the demand. The code to host
    your first index is presented in the *Apply our* *learning* section).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建端点，你就可以在单个端点下托管多个向量搜索索引。端点将根据需求进行扩展。托管第一个索引的代码在*“应用我们的学习”*部分中展示）。
- en: There are limitations on the number of endpoints you can create indices per
    endpoint, and the embedding dimensions. Please review the documentation linked
    in *Further reading*, as Databricks may remove or update limits as the product
    evolves.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 每个端点可以创建索引的数量和嵌入维度有限制。请查阅*“进一步阅读”*中链接的文档，因为随着产品的演变，Databricks可能会删除或更新限制。
- en: Technology moves fast, especially in the world of generative AI. Databricks
    VS was built even as we wrote this book, and we expect it to continue evolving
    to search not only through text but also images and audio with a more robust hybrid
    search engine in the future.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 技术发展迅速，尤其是在生成式AI的世界里。Databricks VS就是在撰写本书的同时被构建的，我们预计它将继续发展，未来不仅能够通过文本搜索，还能通过更强大的混合搜索引擎搜索图像和音频。
- en: We’ve walked through various Databricks products and features geared toward
    helping you understand your data. Get ready to follow along in your own Databricks
    workspace as we work through the [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180)
    code by project and put these concepts into practice.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了各种Databricks产品功能，旨在帮助你理解你的数据。准备好跟随我们在自己的Databricks工作区中工作，通过项目来执行[*第4章*](B16865_04.xhtml#_idTextAnchor180)的代码，并将这些概念付诸实践。
- en: Applying our learning
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: It’s time to apply these concepts to our example projects. We will use what
    we have learned to explore each project dataset, from using Databricks Assistant
    to AutoML, to creating a vector search index and exploring image data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候将这些概念应用到我们的示例项目中了。我们将利用所学知识来探索每个项目数据集，从使用Databricks助手到AutoML，再到创建向量搜索索引和探索图像数据。
- en: Technical requirements
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Before you begin, review, and prepare the technical requirements necessary
    for the hands-on work in this chapter:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请回顾并准备本章动手工作所需的技术要求：
- en: 'We use the `missingno` library to address missing numbers in our synthetic
    transactions project data: [https://pypi.org/project/missingno/](https://pypi.org/project/missingno/)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`missingno`库来解决合成交易项目数据中的缺失值：[https://pypi.org/project/missingno/](https://pypi.org/project/missingno/)
- en: 'For the RAG project, you will need to install the following either on your
    cluster or in the `CH4-01-Creating_VectorDB` notebook. If you choose to install
    them in the notebook, the code is included for you:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于RAG项目，你需要在你的集群或`CH4-01-Creating_VectorDB`笔记本中安装以下内容。如果你选择在笔记本中安装它们，代码已经为你准备好了：
- en: '`typing_extensions==4.7.1`'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`typing_extensions==4.7.1`'
- en: '`transformers==4.30.2`'
  id: totrans-133
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transformers==4.30.2`'
- en: '`llama-index==0.9.3`'
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`llama-index==0.9.3`'
- en: '`langchain==0.0.319`'
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`langchain==0.0.319`'
- en: '`unstructured[pdf,docx]==0.10.30`'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unstructured[pdf,docx]==0.10.30`'
- en: Project – Favorita Store Sales – time-series forecasting
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 – Favorita Store Sales – 时间序列预测
- en: 'For the Favorita Store Sales project, we use many simple DBSQL queries for
    data exploration and to understand the relationships between datasets. Additionally,
    we use the `ydata_profiling` library to produce data profiles in HTML format,
    as shown in*Figure 4**.19*. To follow along in your own workspace, please refer
    to the following notebooks:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Favorita Store Sales项目，我们使用许多简单的DBSQL查询进行数据探索，并了解数据集之间的关系。此外，我们使用`ydata_profiling`库以HTML格式生成数据配置文件，如图4.19所示。要在你的工作空间中跟随，请参考以下笔记本：
- en: '`CH4-01-Exploring_Favorita_Sales_Data`'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH4-01-Exploring_Favorita_Sales_Data`'
- en: '`CH4-02-Exploring_Autogenerated_Notebook`'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH4-02-Exploring_Autogenerated_Notebook`'
- en: '`CH4-03-Imputing_Oil_Data`'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CH4-03-Imputing_Oil_Data`'
- en: 'In the last chapter, we created tables of the Favorita Sales Forecasting dataset
    from Kaggle. Now it’s time to explore! Open up the first notebook, `CH4-01-Exploring_Favorita_Sales_Data`,
    to do some initial data exploration in SQL:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们创建了Kaggle的Favorita Sales Forecasting数据集的表格。现在，是时候探索了！打开第一个笔记本，`CH4-01-Exploring_Favorita_Sales_Data`，在SQL中进行一些初步的数据探索：
- en: The first cell is a simple `select *` SQL command. After running the cell, focus
    on the in-cell **Data Profile** and **Visualization** options.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个单元是一个简单的`select *` SQL命令。运行单元后，关注单元格内的**数据配置文件**和**可视化**选项。
- en: '![Figure 4.16 – You can create visualizations and a data profile of a SQL query
    result in a notebook](img/B16865_04_16.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图4.16 – 你可以在笔记本中创建SQL查询结果的视觉化和数据配置文件](img/B16865_04_16.jpg)'
- en: Figure 4.16 – You can create visualizations and a data profile of a SQL query
    result in a notebook
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 – 你可以在笔记本中创建SQL查询结果的视觉化和数据配置文件
- en: Choose **Data Profile** and investigate the information autogenerated about
    the data.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**数据配置文件**并调查自动生成关于数据的信息。
- en: Choose **Visualization**. You can create a line chart with the date (month)
    versus the sum of sales and grouped by family. Notice that not all data is used
    to produce the visualization; we only see January at first. Once you save the
    visualization, the chart will display in the cell. The **Truncated data** message
    should be present at the bottom of the visualization. To increase the number of
    records in the chart, select **Aggregate over** **more data**.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**可视化**。你可以创建一个以日期（月份）与销售额总和的折线图，并按系列分组。请注意，并非所有数据都用于生成可视化；我们最初只看到1月。一旦保存可视化，图表将显示在单元格中。可视化底部的**数据截断**消息应该存在。要增加图表中的记录数，请选择**在更多数据上聚合**。
- en: Continue to play with the options. In the next cell, we filter to the top-performing
    product families. Investigate how different or similar the charts appear.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续尝试不同的选项。在下一个单元中，我们将筛选出表现最好的产品系列。研究不同的图表看起来如何或有多相似。
- en: Now that we’ve done some initial exploration of the Favorita data, we can run
    a Databricks AutoML experiment to generate a baseline model. AutoML can be launched
    in the UI, which we demonstrated earlier in this chapter in the *Generating data
    profiles with AutoML* section (*Figure 4**.11*), or you can create an experiment
    via an API as shown in *Figure 4**.17*. For this project, we will launch both
    a regression experiment and a forecasting experiment. Let’s start with the regression
    run.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对Favorita数据进行了初步探索，我们可以运行一个Databricks AutoML实验来生成一个基线模型。AutoML可以在UI中启动，我们之前在本章的*使用AutoML生成数据配置文件*部分（*图4.11*）中演示了这一点，或者你可以通过API创建一个实验，如图4.17所示。对于这个项目，我们将启动一个回归实验和一个预测实验。让我们从回归运行开始。
- en: Notice that at the top of the notebook, the default language is SQL rather than
    Python. Therefore, when we want to execute Python code, we need to include `%python`
    at the top of the cell. We use Python for AutoML in the last cell of the first
    notebook. We’ve set the `timeout_minutes` variable to `30`, so the experiment
    will run for up to 30 minutes. However, AutoML stops training models if the validation
    metric is no longer improving. In that case, the experiment will finish in less
    time. Once the run is complete, the notebook UI will display links to the MLflow
    experiment where the model versions are accessible, the best trial notebook with
    the best model’s code, and the data exploration notebook. Since this chapter focuses
    on exploring data, we will only open the data exploration notebook for now.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在笔记本的顶部，默认语言是SQL而不是Python。因此，当我们想要执行Python代码时，需要在单元格顶部包含`%python`。我们在第一个笔记本的最后一个单元格中使用Python进行AutoML。我们将`timeout_minutes`变量设置为`30`，因此实验将运行最多30分钟。然而，如果验证指标不再提高，AutoML将停止训练模型。在这种情况下，实验将提前完成。一旦运行完成，笔记本UI将显示链接到MLflow实验，其中可以访问模型版本，最佳试验笔记本包含最佳模型的代码，以及数据探索笔记本。由于本章重点在于探索数据，我们现在只打开数据探索笔记本。
- en: "![Figure 4.17 – Creatin\uFEFFg an AutoML experiment from a notebook](img/B16865_04_17.jpg)"
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图4.17 – 从笔记本创建AutoML实验](img/B16865_04_17.jpg)'
- en: Figure 4.17 – Creating an AutoML experiment from a notebook
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 – 从笔记本创建AutoML实验
- en: AutoML for data exploration
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据探索的AutoML
- en: When you executed the final cell in `CH4-01-Exploring_Favorita_Sales_Data`,
    you received several links in the results, as shown in *Figure 4**.17*. Click
    on the link to the data exploration notebook (you can also open `CH4-02-Exploring_Autogenerated_Notebook`
    for a version that was autogenerated when we ran the AutoML experiment ourselves).
    Let’s look at this notebook.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在`CH4-01-Exploring_Favorita_Sales_Data`中的最后一个单元格执行操作时，你会在结果中收到几个链接，如*图4.17*所示。点击数据探索笔记本的链接（您也可以打开`CH4-02-Exploring_Autogenerated_Notebook`以查看我们在运行AutoML实验时自动生成的版本）。让我们看看这个笔记本。
- en: '![Figure 4.18 – EDA notebook created with AutoML](img/B16865_04_18.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图4.18 – 使用AutoML创建的EDA笔记本](img/B16865_04_18.jpg)'
- en: Figure 4.18 – EDA notebook created with AutoML
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18 – 使用AutoML创建的EDA笔记本
- en: '*Figure 4**.18* shows a portion of the automatically generated data exploration
    notebook. This notebook imports libraries and points to the training data. It
    also automatically converts the datetime columns to pandas datetime.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.18*显示了自动生成数据探索笔记本的一部分。该笔记本导入库并指向训练数据。它还自动将日期时间列转换为pandas日期时间。'
- en: The notebook uses a pandas-based library, so the notebook limits the data to
    10,000 rows.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 该笔记本使用基于pandas的库，因此笔记本限制了数据为10,000行。
- en: Next, the notebook imports the `ydata_profiling` library, along with three additional
    correlation calculations added in this case. The `ydata_profiling` library provides
    similar data to what we would get using `summary()` functions, such as details
    about missing and correlated data. The library is easily imported into a notebook
    for exploring data. Once finished, you can export the details to HTML or PDF for
    easy sharing. It’s a great timesaver when exploring new datasets.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，笔记本导入了`ydata_profiling`库，以及在此情况下添加的三个额外的相关性计算。`ydata_profiling`库提供了类似于使用`summary()`函数所获得的数据，例如关于缺失和相关性数据的详细信息。该库可以轻松导入笔记本以探索数据。一旦完成，您可以将其导出为HTML或PDF格式，以便轻松分享。在探索新数据集时，这是一个节省时间的好方法。
- en: '![Figure 4.19 – ydata_profiling in the AutoML-created notebook](img/B16865_04_19.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图4.19 – AutoML创建的笔记本中的ydata_profiling](img/B16865_04_19.jpg)'
- en: Figure 4.19 – ydata_profiling in the AutoML-created notebook
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19 – AutoML创建的笔记本中的ydata_profiling
- en: As shown in *Figure 4**.17*, we launched a regression experiment via the API
    by calling `automl.regress()`. Now, we create another experiment with a forecasting
    problem type using the UI.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图4.17*所示，我们通过调用`automl.regress()`通过API启动了一个回归实验。现在，我们使用UI创建另一个具有预测问题类型的实验。
- en: "![Figure 4.20 – Creating an A\uFEFFutoML forecasting experiment in the UI](img/B16865_04_20.jpg)"
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图4.20 – 在UI中创建AutoML预测实验](img/B16865_04_20.jpg)'
- en: Figure 4.20 – Creating an AutoML forecasting experiment in the UI
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20 – 在UI中创建AutoML预测实验
- en: When we create an AutoML forecasting experiment, we can incorporate a country’s
    holidays into the model under the **Country Holidays** option in the **Advanced
    Configuration** section. We will choose **NONE** for this project, because although
    many countries are present in the drop-down menu, Ecuador is not among them. **NONE**
    will ensure holidays are not included as a feature in the model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.21 – AutoML forecasting experiment advanced configurations](img/B16865_04_21.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 4.21 – AutoML forecasting experiment advanced configurations
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Once all the fields are completed, click **Start AutoML** at the bottom of the
    screen to run the experiment (adjust the **Timeout** variable down if you want
    to ensure the run finishes within a given amount of time). When the experiment
    has finished running, you will see a new screen with a list of generated models.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: We have now created an AutoML experiment in a notebook, used AutoML to generate
    a notebook for EDA, and used AutoML for time-series analysis. Databricks AutoML
    does a lot of the heavy lifting with minimal code for ML and EDA.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: The Favorita sales dataset contains oil prices, which we think could impact
    sales, so let’s augment the autogenerated data exploration with some of our own
    to explore the oil price data and see how we can use it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and cleaning oil price data
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To get started with the oil data, open the `CH4-03-Imputing_Oil_Data` notebook.
    We use the pandas API on Spark to view the data (*Figure 4**.22*). We reindex
    the data because it’s out of order and data for some dates is missing. Notice
    that initially, the data starts in May 2015 rather than January 2013\. Also, the
    dates May 9th and May 10th appear to be missing.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.22 – View the oil price data provided by Kaggle in the Favorita
    Sales dataset](img/B16865_04_22.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Figure 4.22 – View the oil price data provided by Kaggle in the Favorita Sales
    dataset
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: If we add the `date` column as the index column, as shown in the notebook, you’ll
    see that the rows are then in order. However, January 5th and January 6th are
    missing. This occurs because stock prices are not given for holidays or weekends.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.23 – Reindex to include all dates an\uFEFFd fill forward missing\
    \ prices](img/B16865_04_23.jpg)"
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: Figure 4.23 – Reindex to include all dates and fill forward missing prices
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: We use the `reindex()` command to create an updated index based on the minimum
    and maximum dates in the oil price data. The reindex creates rows for all missing
    dates. When new rows are created for the missing dates, the prices are NaNs. We
    use the `ffill()` function, or forward fill, to update the DataFrame by filling
    in dates without prices with the price from the day before. January 1st, 2013,
    doesn’t have a previous day to fill from, so it remains `NaN`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have a clean and consistent silver table of oil prices that we can pull
    into our Favorita time-series models. In the next chapter, we will do feature
    engineering with the Favorita sales data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Project – streaming transactions
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The synthetic dataset does not need cleaning (since we created it), but we
    can still explore the data to understand it better. To follow along in your own
    workspace, please refer to the following notebook:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '`CH4-01-Exploring_Synthetic_Transactions_Data`'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open the notebook to generate visualizations with the `seaborn` library, as
    shown in *Figure 4**.24*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.24 – Data visualizations of the synthetic transactions data](img/B16865_04_24.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 4.24 – Data visualizations of the synthetic transactions data
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to explore the transaction data further. Next, we’ll move on to the
    RAG chatbot.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Project – RAG chatbot
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the last chapter, we extracted chunks of text from our PDF documents. We
    converted those chunks into embeddings using the BGE embedding model. As a result,
    we are now ready to take the next step in preparing our data for retrieval. To
    follow along in your own workspace, please refer to the following notebook:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '`CH4-01-Creating_VectorDB`'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We explained in the *Enhancing data retrieval with Databricks Vector Search*
    section that Databricks VS is a serverless managed solution. In other words, we
    need to create the VS endpoint to host our indices.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.25 – Creating the Databricks VS endpoint for the RAG chatbot](img/B16865_04_25.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: Figure 4.25 – Creating the Databricks VS endpoint for the RAG chatbot
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, we will ingest the source table we prepared in [*Chapter 3*](B16865_03.xhtml#_idTextAnchor123)
    into the index for retrieval search within the chatbot application.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.26 – Reading our source table containing text from the documentation](img/B16865_04_26.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: Figure 4.26 – Reading our source table containing text from the documentation
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: We import functions from the project’s `utils` folder, `mlia_utils.rag_funcs`.
    The index only needs to be created once. Going forward, we will only write into
    it or read from it. We use an `if`/`else` clause to check whether an index with
    the name `catalog.database.docs_vsc_idx_cont` exists or not. If it exists, we
    just synchronize our `catalog.database.pdf_documentation_text` source table to
    it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.27 – Check whether the index exists and create it if needed](img/B16865_04_27.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: Figure 4.27 – Check whether the index exists and create it if needed
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few key pieces we must make sure are in place before proceeding
    with the rest of our chatbot project:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '`catalog.database.pdf_documentation_text` is the Delta table you have prepared
    with your self-managed embedding functions as the primary data repository for
    their vector search index.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`catalog.database.docs_vsc_idx_cont` using Databricks VS. This index is linked
    to the source Delta table. The index is set on a trigger base update (`pipeline_type="TRIGGERED"`),
    which means that any modifications or new additions to the historical texts should
    be synced manually to your vector search index. If you wish to have changes reflected
    in the vector search index automatically, choose the `CONTINUOUS` mode instead.
    This continuous update mechanism ensures the data is always current and ready
    for analysis.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`text_chunks` is automatically converted into embeddings with a model that
    is served through Databricks Model Serving. This can be specified in the setup
    under the `embedding_model_endpoint_name` parameter. This integration guarantees
    that the textual data is efficiently converted into vectors, making it suitable
    for advanced similarity searches.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We import the `wait_for_index_to_be_ready()` function from the `utils` folder.
    We run the code in *Figure 4**.28* to repeatedly check the index’s status until
    it is in an “online” state. As the VS index only needs to be created once, this
    function can take some time before the embeddings reach your VS index. Proceed
    once your index is in a “ready” state.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.28 – The wait for index function checks for the index status until
    it is online and ready](img/B16865_04_28.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 4.28 – The wait for index function checks for the index status until
    it is online and ready
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Once the index is online, we call `get_index()` to perform a similarity search.
    We also call a `describe()` method to show you the broader API options you have.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.29 – Using get_index with the describe method shows the index configuration](img/B16865_04_29.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: Figure 4.29 – Using get_index with the describe method shows the index configuration
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we use self-managed embeddings, our VS index is not connected to any
    model that can convert our input text query into embeddings to be mapped to the
    one under a database, so we need to convert them first! Here we are again leveraging
    the BGE embedding model from the Foundational Model Serving API and then passing
    embedded text to our index for similarity search:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.30 – Converting our query text into embeddings and passing to the
    index for similarity search](img/B16865_04_30.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 4.30 – Converting our query text into embeddings and passing to the index
    for similarity search
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the result of the retrieved queries (only the first found one with
    two columns `pdf_name` and content):'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.31– Sample of result of the retrieved query](img/B16865_04_31.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 4.31– Sample of result of the retrieved query
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Now, our VS index is ready to be used in our chatbot. We will explore how to
    connect all the tools together to generate a proper human-readable answer in [*Chapter
    6*](B16865_06.xhtml#_idTextAnchor297)!
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Project – multilabel image classification
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the last chapter, we saved our image data into volumes. Next, we will explore
    our data. To follow along in your own workspace, please refer to the following
    notebook:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '`CH4-01-Exploring_Dataset`'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We create our training dataset of images. Print the labels and a sample of the
    training data for a look at what we want our model to classify.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.32 – Load and view the training dataset](img/B16865_04_32.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Figure 4.32 – Load and view the training dataset
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: We can use `display_image` to view a few pictures from our volumes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.33 – Display images within the notebook](img/B16865_04_33.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: Figure 4.33 – Display images within the notebook
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: It’s good to have an idea of the proportion of the different labels of our data.
    This is how you can view the proportion of data in our training dataset. Wherever
    you perform a multilabel classification, make sure you have a good distribution
    of labels. Otherwise, consider training individual models that might be combined
    or augment missing labels!
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.34 – View the proportion of labels in the training dataset](img/B16865_04_34.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: Figure 4.34 – View the proportion of labels in the training dataset
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Now we have done some high-level exploration of our image classification dataset.
    For this project, no transformations are needed, so our next step with this data
    will be in [*Chapter 6*](B16865_06.xhtml#_idTextAnchor297).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is critical to understand your data before using it. This chapter highlighted
    a variety of methods to explore and analyze our data within the Databricks ecosystem.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: We began by revisiting DLT, this time focusing on how we use a feature called
    **expectations** to monitor and improve our data quality. We also introduced Databricks
    Lakehouse Monitoring as another tool for observing data quality. Among its many
    capabilities, Lakehouse Monitoring detects shifts in data distribution and alerts
    users to anomalies, thus preserving data integrity throughout its life cycle.
    We used Databricks Assistant to explore data with ad hoc queries written in English
    and showed why AutoML is an extremely useful tool for data exploration by automatically
    creating comprehensive data exploration notebooks. Together, all of these tools
    create a strong foundation to understand and explore your data. Finally, the chapter
    delved into Databricks VS and how using it to find similar documents can improve
    chatbot responses.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: We have now set the foundation for the next phase of our data journey. [*Chapter
    5*](B16865_05.xhtml#_idTextAnchor244) will focus on how to build upon our bronze-layer
    data to create rich sets of features for data science and ML projects.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following questions are meant to solidify key points to remember as well
    as tie the content back to your own experience:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: What are some low-code options for data exploration that we discussed in this
    chapter?
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When might you use Databricks Assistant for data exploration, and when might
    you use AutoML’s data profile notebook?
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How and why would you set expectations on your data?
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When would you use a regular database versus a vector database? What are some
    common use cases for vector databases?
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After putting thought into the questions, compare your answers to ours:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Some low-code data exploration options include using the `ydata` library, in-cell
    data profile, Databricks Assistant, and AutoML.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Databricks Assistant is useful for data exploration when you have a good idea
    of the analyses you want to build and you want code assistance. Databricks Assistant
    is a great way to speed up the coding process or augment your SQL knowledge. On
    the other hand, AutoML is very useful for automatically creating a profile notebook
    that broadly covers your dataset.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We would use Delta Live Tables to set expectations. Expectations are a way to
    flexibly handle data abnormalities and give the options to report bad data, drop
    that data, or fail the pipeline entirely.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Regular databases, or relational databases, are designed for data in tabular
    form, typically organized in rows or columns. A vector database is designed to
    store vector data, such as embeddings and high-dimensional data. Vector databases
    are optimized for operations based on vector space models, similarity searches,
    image and video analysis, and other ML problems. Some common use cases include
    **Retrieval Augmented Generation** (**RAG**) systems, recommendation systems,
    and image and video recognition.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we pointed out specific technologies, technical features,
    and options. Please take a look at these resources to get deeper into the areas
    that interest you most:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '*Enabling visualizations with Aggregations in* *DBSQL*: [https://docs.databricks.com/sql/user/visualizations/index.html#enable-aggregation-in-a-visualization](https://docs.databricks.com/sql/user/visualizations/index.html#enable-aggregation-in-a-visualization)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using the ydata profiler to explore data: [https://ydata-profiling.ydata.ai/docs/master/index.html](https://ydata-profiling.ydata.ai/docs/master/index.html)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advancing Spark - Meet the new Databricks* *Assistant*: [https://youtu.be/Tv8D72oI0xM](https://youtu.be/Tv8D72oI0xM)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Introducing Databricks Assistant, a context-aware AI* *assistant*: [https://www.databricks.com/blog/introducing-databricks-assistant](https://www.databricks.com/blog/introducing-databricks-assistant)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model monitoring custom metrics* *creation*: [https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html](https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Databricks Vector Search: [https://docs.databricks.com/en/generative-ai/vector-search.html](https://docs.databricks.com/en/generative-ai/vector-search.html)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'External models in Databricks Model Serving: [https://learn.microsoft.com/en-us/azure/databricks/generative-ai/external-models/](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/external-models/)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
