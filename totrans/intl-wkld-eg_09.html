<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer112">
			<h1 id="_idParaDest-134"><em class="italic"><a id="_idTextAnchor138"/>Chapter 7</em>: Machine Learning Workloads at the Edge</h1>
			<p>The growth of edge computing is not only driven by advancements in computationally efficient hardware devices but also by the advent of different software technologies that were only available on the cloud (or on-premises infrastructure) a decade back. For example, think of smartphones, smartwatches, or personal assistants such as <strong class="bold">Amazon Alexa</strong> that bring a mix of powerful hardware and software capabilities to consumers. Capabilities such as unlocking your phone or garage doors using facial recognition, having a conversation with Alexa using natural language, or riding an autonomous car have become the new normal. Thus, a need for cyber-physical systems to build intelligence throughout their lifetime based on continuous learning from their surroundings has become key for various workloads in today's world.</p>
			<p>It's important to realize that most of the top technology companies (such as <em class="italic">Apple</em>, <em class="italic">Amazon</em>, <em class="italic">Google</em>, and <em class="italic">Meta</em>, formerly <em class="italic">Facebook</em>) use <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) and have made it more accessible to consumers through their products. It's not a new technology, either, and has been used by industry sectors such as financial, healthcare, and industrial settings for a long time. In this chapter, we will focus on how ML capabilities can be leveraged for any <strong class="bold">internet of things</strong> (<strong class="bold">IoT</strong>) workload.</p>
			<p>We will continue to work on the connected hub solution and learn how to develop ML capabilities on the <strong class="bold">edge</strong> (aka a <strong class="bold">Greengrass</strong> device). In the previous chapters, you have already learned about processing different types of data on the edge, and now, it's time to learn how different ML models can perform inference on this data to derive intelligent insights on the edge. </p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Defining ML for IoT workloads</li>
				<li>Designing an ML workflow in the cloud </li>
				<li>Hands-on with ML workflow architecture </li>
			</ul>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor139"/>Technical requirements </h1>
			<p>The technical requirements for this chapter are the same as those outlined in <a href="B17595_02_Final_SS_ePub.xhtml#_idTextAnchor032"><em class="italic">Chapter 2</em></a>, <em class="italic">Foundations of Edge Workloads</em>. See the full requirements in that chapter.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor140"/>Defining ML for IoT workloads</h1>
			<p>ML technologies <a id="_idIndexMarker591"/>are no longer technologies <a id="_idIndexMarker592"/>of the future— they have transformed the lives of millions of people in the last few decades. So, what is ML? </p>
			<p class="author-quote">"Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed."</p>
			<p class="author-quote">– Arthur Samuel, 1959</p>
			<p>Let's look at some real-world examples of ML applications for IoT workloads from the <strong class="bold">consumer</strong> and <strong class="bold">industrial</strong> segments.</p>
			<p>First, here are <a id="_idIndexMarker593"/>some examples from the consumer segment: </p>
			<ul>
				<li>Smartphones or smartwatches that can identify your daily habits and provide recommendations related to fitness or productivity</li>
				<li>Personal assistants (such as Alexa, Google, and Siri) that you can interact with in a natural way for different needs such as controlling your lights and <strong class="bold">heating, ventilation, and air conditioning</strong> (<strong class="bold">HVAC</strong>)</li>
				<li>Smart cameras that can monitor your surroundings and detect unexpected behaviors or threats</li>
				<li>Smart garages <a id="_idIndexMarker594"/>that can recognize your car through its visual attributes, license plates, or even drivers' faces </li>
				<li>Self-driving cars that can continuously become smarter in identifying driving patterns, objects, and pedestrians in traffic </li>
			</ul>
			<p>Here are some <a id="_idIndexMarker595"/>examples from the industrial segment:</p>
			<ul>
				<li>Smart factories that enable better optimization of <strong class="bold">overall equipment effectiveness</strong> (<strong class="bold">OEE</strong>)</li>
				<li>Better worker safety and productivity in different industrial plants, warehouses, or working sites</li>
				<li>Real-time <strong class="bold">quality control</strong> (<strong class="bold">QC</strong>) using <strong class="bold">computer vision</strong> (<strong class="bold">CV</strong>) or audio to identify defects</li>
				<li>Improving supply chain to reduce waste and enhance customer experience such as 1 hour or 15 mins delivery (with drones) from Amazon.com</li>
			</ul>
			<p>To build the <a id="_idIndexMarker596"/>aforementioned capabilities, customers <a id="_idIndexMarker597"/>have used different ML frameworks and algorithms. For the sake of brevity, we are not going to cover every ML framework that exists today. We believe it's an area of data science that doesn't fit into the daily responsibilities of an IoT practitioner. But if you are interested in diving deeper, there are many <a id="_idIndexMarker598"/>books on ML/<strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) available. Thus, our focus in this chapter will be to learn a bit about the history and core concepts of ML systems, and the approach to integrating ML with IoT and edge workloads. </p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor141"/>What is the history of ML?</h2>
			<p>Today, as humans, we can communicate with machines of different kinds (from mobiles to self-driving cars) using voice, vision, or touch. This would not have been possible without the <a id="_idIndexMarker599"/>adoption of ML technologies. This is just the beginning, and ML will be more accessible in the years to come to transform our lives in different ways. You can take a quick tour through the history of this amazing technology in this article published by <em class="italic">Forbes</em>: <a href="https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/?sh=1ca6cea115e7">https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/?sh=1ca6cea115e7</a>.</p>
			<p>Beyond the research community, technology companies such as Amazon.com, Google, and others also started adopting ML technologies in the late 1990s. For example, Amazon.com used ML algorithms to learn about the reading preferences of their customers and built a model to notify them of new book releases matching their interests or genres. Google used ML for their search engine, Microsoft used it for identifying spam in emails, and so on. Since then, this technology has been adopted by many other industries for a plethora of use cases. </p>
			<p>Now that we have learned a bit about the background of ML, let's now try to understand the foundation of ML. </p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor142"/>What are the different types of ML systems?</h2>
			<p>Similar to distributed data systems, where there are different kinds of technologies to process <a id="_idIndexMarker600"/>different types of data, ML systems also come in different flavors. If we classify them into broader categories, the distinction can be described in this way: </p>
			<ul>
				<li><strong class="bold">Supervised ML (SML)</strong>—In this method of ML, the model is trained with a labeled <a id="_idIndexMarker601"/>dataset and requires human <a id="_idIndexMarker602"/>supervision (or a teacher). For example, let's consider a scenario where we need a connected hub solution to identify different objects such as cats, dogs, humans (or seasonal birds?) who might have intruded onto your premises. Thus, the images are required to be labeled by a human (or humans), and the models will be trained on that data using a classification algorithm before they are ready (that is, the models) to predict the outcomes. In the following screenshot, you can see some objects are already labeled while <a id="_idIndexMarker603"/>the rest are not. So, humans are required to <a id="_idIndexMarker604"/>do their due diligence with labeling for the model to be effective:</li>
			</ul>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="Images/B17595_07_01.jpg" alt="Figure 7.1 – A labeled training set of image classification&#13;&#10;" width="1397" height="455"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – A labeled training set of image classification</p>
			<p>The length of the training and the volume and quality of the data will determine the accuracy of the model.</p>
			<ul>
				<li><strong class="bold">Unsupervised ML (UML)</strong>—In this method of ML, the model is trained with an unlabeled <a id="_idIndexMarker605"/>dataset and requires <a id="_idIndexMarker606"/>no human supervision (or self-teaching). For example, let's consider that you have a new intruder on your premises, such as a deer, a wolf (or a tiger?), and you expect the model to detect that as an anomaly and notify you. In the following screenshot, you can see that none of the images is labeled and the model is required to figure out the anomaly on its own:</li>
			</ul>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="Images/B17595_07_02.jpg" alt="Figure 7.2 – An unlabeled training set of image classification&#13;&#10;" width="1051" height="1050"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – An unlabeled training set of image classification</p>
			<p>Considering the training dataset did not have pictures of a deer, a wolf, or a tiger, the model needs to be smart enough to identify that as an <strong class="bold">anomaly</strong> (or a novelty detection) using <a id="_idIndexMarker607"/>algorithms such as <strong class="bold">random forest</strong>. </p>
			<ul>
				<li><strong class="bold">Semi-supervised ML (SSML)</strong>—In this method of ML, the model is trained with a dataset that is a mix of unlabeled and labeled data (think of on-demand teaching <a id="_idIndexMarker608"/>where you need to learn <a id="_idIndexMarker609"/>most of the content on your own). So, let's consider a scenario where you collected pictures of your guests from a party thrown at your home. Different guests show up in different photos and most of them <a id="_idIndexMarker610"/>are not labeled, which is the unsupervised part of the algorithm (such as <strong class="bold">clustering</strong>). Now, as the host of the party, if you label the unique individuals once in the dataset, the ML model can immediately recognize those individuals in other pictures on its own, as shown in the following diagram:</li>
			</ul>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="Images/B17595_07_03.jpg" alt="Figure 7.3 – SSML &#13;&#10;" width="1154" height="670"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – SSML </p>
			<p>This might be very useful if you want to search photos of individuals or families and share these with them (who cares about photos of other families, huh?).</p>
			<ul>
				<li><strong class="bold">Reinforcement learning (RL)</strong>—In this method of ML, the model is trained to make a <a id="_idIndexMarker611"/>sequence of decisions in <a id="_idIndexMarker612"/>an environment and maximize a long-term objective. The model learns through an iterative process of trial and error. An agent such as a physical or virtual device uses this model to take actions guided by a policy at a given environment state and reaches a new state. This makes the agent eligible for a reward (positive or negative), and the agent continues to iterate on this process until it leads to the most optimal long-term rewards. The entire life cycle of an agent progressing from an initial state to a final state is <a id="_idIndexMarker613"/>called an <strong class="bold">episode</strong>. <p>For example, using RL, you can train a robot to take pictures of all guests from a party thrown at your home. The robot will stay on a specific track and capture images from that environment. It will receive a positive reward if it stays on track and captures acceptable images and a negative reward for going off track or capturing distorted snaps. As it continues to iterate through this process, eventually it will learn how to maximize its long-term objectives of capturing glorious images. The following diagram reflects this process of RL:</p></li>
			</ul>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="Images/B17595_07_04.jpg" alt="Figure 7.4 – RL &#13;&#10;" width="1187" height="486"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – RL </p>
			<p>We have explained a broader category of ML in the preceding diagram; however, there is a plethora <a id="_idIndexMarker614"/>of frameworks and algorithms <a id="_idIndexMarker615"/>that are beyond the scope of this book. So, in the next section, we will focus on the most common ones that can apply to data generated from IoT and edge workloads.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor143"/>Taxonomy of ML with IoT workloads</h2>
			<p>The three <a id="_idIndexMarker616"/>most common uses of ML in <a id="_idIndexMarker617"/>IoT are in the fields of classification, regression, and clustering, as depicted in the following diagram: </p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="Images/B17595_07_05.jpg" alt="Figure 7.5 – Summarized taxonomy of ML algorithms&#13;&#10;" width="1512" height="862"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Summarized taxonomy of ML algorithms</p>
			<p>Let's discuss the nomenclature in the preceding diagram in more detail, as follows:</p>
			<ul>
				<li><strong class="bold">Classification</strong>—Classification is an SML technique where you start with a set of observed <a id="_idIndexMarker618"/>values and derive some conclusion about unknown data. In the real world, classification can be used in image <a id="_idIndexMarker619"/>classification, speech recognition, drugs classification, sentiment analysis, biometric identification, and more.</li>
				<li><strong class="bold">Regression</strong>—Regression is an SML technique where you can predict a continuous value. The prediction happens by estimating the relationship between <a id="_idIndexMarker620"/>the dependent variable (<em class="italic">Y</em>) and one or more independent variables (<em class="italic">X</em>) using a best-fit straight line. In the real world, regression can be applied to forecasting the temperature tomorrow, the price of energy utilization, the price of gold, and so on.</li>
				<li><strong class="bold">Clustering</strong>—Clustering is a UML algorithm where you can group unlabeled data points. This is used very often for statistical analysis. The grouping of unlabeled <a id="_idIndexMarker621"/>data points in a dataset is performed by identifying data points in a dataset that share common properties and features. In the real world, this algorithm can be applied to market segmentation, medical imaging, anomaly detection, and social network analysis.</li>
			</ul>
			<p>In the hands-on lab section of this chapter, you will learn how to use a classification algorithm to classify objects (such as cars and pets). </p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor144"/>Why is ML accessible at the edge today?</h2>
			<p>We have already introduced you to three laws of edge computing in <a href="B17595_06_Final_SS_ePub.xhtml#_idTextAnchor119"><em class="italic">Chapter 6</em></a>,<em class="italic"> Processing and Consuming Data on the Cloud:</em> <em class="italic">Law of Physics</em> (latency-sensitive use cases), <em class="italic">Law of Economics</em> (cost-sensitive use cases), and <em class="italic">Law of the Land</em> (data-sensitive use cases). Based on <a id="_idIndexMarker622"/>these laws, we can identify various use cases in today's world, especially related to IoT and the edge, where it makes a lot of sense to process and generate insights from the data locally on the device or the gateway itself rather than publishing them continuously to the cloud.</p>
			<p>However, the constraint <a id="_idIndexMarker623"/>is the limited resources (such as <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>), <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>), memory, network, and energy) available on these edge devices or gateways. Thus, it is recommended to take advantage of the computing <a id="_idIndexMarker624"/>power of the cloud to build and train ML models using the preferred <a id="_idIndexMarker625"/>framework (such as <strong class="bold">MXNET</strong>, <strong class="bold">TensorFlow</strong>, <strong class="bold">PyTorch</strong>, <strong class="bold">Caffe</strong>, or <strong class="bold">Gluon</strong>) and then <a id="_idIndexMarker626"/>deploy the model to the edge for inferencing.</p>
			<p>For example, if there <a id="_idIndexMarker627"/>is a lot of noisy <a id="_idIndexMarker628"/>data generated in a smart home from a baby crying, a dog barking, or construction noises from the surroundings, the ML model can identify that as noisy data, trigger any specified action locally—such as an alert to check on the baby or the pet—but avoid publishing those data points to the cloud. In that way, a large amount of intermittent data that's of less long-term value can be filtered out at the site itself. </p>
			<p>ML at the <a id="_idIndexMarker629"/>edge is an evolving space and there are many emerging frameworks and hardware offerings available today from different vendors, a few of which are listed in the following tables.</p>
			<p>Here are common ML frameworks for the edge:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="Images/B17595_07_Table1.jpg" alt="Figure 7.6 – Common ML frameworks for the edge&#13;&#10;" width="1650" height="481"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Common ML frameworks for the edge</p>
			<p>Here are common hardware stacks for performing ML at the edge:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="Images/B17595_07_Table2.jpg" alt="Figure 7.7 – Common hardware stacks for performing ML at the edge&#13;&#10;" width="1643" height="943"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Common hardware stacks for performing ML at the edge</p>
			<p>You are <a id="_idIndexMarker630"/>already using Raspberry Pi as the underlying hardware for the different labs in this book. In the <em class="italic">Hands-on with ML architecture</em> section, you will learn how to train Apache MXNET-based ML models in the cloud and deploy them at the edge for inferencing. With this background, let's discuss how to get started with building ML applications for the edge.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor145"/>Designing an ML workflow in the cloud</h1>
			<p>ML is an <strong class="bold">end-to-end</strong> (<strong class="bold">E2E</strong>) iterative process consisting of multiple phases. As we explain the <a id="_idIndexMarker631"/>different phases throughout the rest of the book, we will <a id="_idIndexMarker632"/>align to the general <a id="_idIndexMarker633"/>guidelines provided by <strong class="bold">Cross Industry Standard Process for Data Mining</strong> (<strong class="bold">CRISP-DM</strong>) consortium. The CRISP-DM reference model was conceived in late 1996 by three pioneers of the emerging data mining market and continued to evolve through participation from multiple organizations and service suppliers across various industry segments. The following diagram shows the different phases of the CRISP-DM reference model:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="Images/B17595_07_08.jpg" alt="Figure 7.8 – Phases of the CRISP-DM reference model (redrawn from https://www.the-modeling-agency.com/crisp-dm.pdf)&#13;&#10;" width="1108" height="1126"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – Phases of the CRISP-DM reference model (redrawn f<a href="https://www.the-modeling-agency.com/crisp-dm.pdf">rom https://www.the-modeling-agency.com/crisp-dm</a>.pdf)</p>
			<p>This model is <a id="_idIndexMarker634"/>still considered a baseline and a proven tool for conducting successful data mining projects as its application is neutral and applies well to a wide variety of ML pipelines and workloads. Using the preceding reference model (<em class="italic">Figure 7.5</em>) as the foundation, the life cycle of an ML project can be expanded to the following activities:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="Images/B17595_07_Table3.jpg" alt="Figure 7.9 – Life cycle of an ML project&#13;&#10;" width="1495" height="597"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Life cycle of an ML project</p>
			<p>The workflow <a id="_idIndexMarker635"/>of the preceding ML activities can be visually depicted as follows:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="Images/B17595_07_10.jpg" alt="Figure 7.10 – E2E ML process&#13;&#10;" width="1650" height="778"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – E2E ML process</p>
			<p>In the following section, we will elaborate on these concepts in detail by using an image classification scenario for your connected home. </p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor146"/>Business understanding and problem framing</h2>
			<p>The first phase is working backward from the use case and understanding the requirements <a id="_idIndexMarker636"/>from a business perspective. Once that's clear, the business <a id="_idIndexMarker637"/>context gets translated to the technical requirements (such as the need for ML technologies) to achieve <a id="_idIndexMarker638"/>the required business outcomes. Does this concept sound familiar? If yes, congrats—you were able to relate to the concepts of <strong class="bold">domain-driven design</strong> (<strong class="bold">DDD</strong>), introduced in <a href="B17595_06_Final_SS_ePub.xhtml#_idTextAnchor119"><em class="italic">Chapter 6</em></a>, <em class="italic">Processing and Consuming Data on the Cloud</em>. The ML capabilities can be treated as just another bounded context with its own set of ubiquitous languages. </p>
			<p>But problem-solving using ML can be different, and here is a great quote from Peter Norvig (Director of Research at Google) on that:</p>
			<p class="author-quote">"Machine Learning changes the way you think about a problem. The focus shifts from a mathematical science to a natural science, running experiments and using statistics, not logic, to analyze its results." </p>
			<p>An organization needs to clearly identify whether the business problem they are trying to solve is an ML problem. If the problem can be solved using traditional programming methods, building ML models might be overkill. For example, if you plan to forecast the future revenue of your business for a specific quarter based on historical data, traditional analytical methods might be sufficient. But if you start factoring prediction of other variables such as weather, competitor campaigns, promotions, economy, this becomes a better fit for an ML problem. So, as a rule of thumb, always try to start your ML journey with the following questions: </p>
			<ul>
				<li><em class="italic">What problem is my organization or product facing? </em><p>Let's consider a scenario where you are trying to solve the problem of securing your family, pets, and neighbors from incoming traffic around your parking space.</p></li>
				<li><em class="italic">Would it be a good problem to solve using ML or classic analytics methods? </em><p>Here, a connected <strong class="bold">Home Base Solutions</strong> (<strong class="bold">HBS</strong>) hub is required to identify any living <a id="_idIndexMarker639"/>things in the parking space when a <a id="_idIndexMarker640"/>vehicle approaches or departs the <a id="_idIndexMarker641"/>area. This problem cannot be solved using a classic analytics method because you won't have the schedule available for every visitor, neighbor, or delivery van for your area. Thus, the hub can detect the movement of a vehicle(s) using motion sensors, capture different images from the surroundings (using the installed camera), and run real-time inferences to detect any objects around it. If that's found, it will alert the driver, kids, or pets in real time and avoid accidents.</p><p>Earlier, computer vision (CV) models relied on raw pixel data as an input, but this was found to be inefficient due to several other factors such as different backgrounds behind the object, lighting, camera angle, or focus. Thus, image classification is clearly an ML problem.</p></li>
				<li><em class="italic">If it's an ML problem, do I have enough data of optimal quality? </em><p>Considering the objects in scope here are generic—such as humans, cats, or cars—we can rely on public <a id="_idIndexMarker642"/>datasets such as <strong class="bold">Caltech-256</strong>. This dataset contains more than 30,000 images of 256 different types of objects.  </p><p>This is often the most common question we come across: <em class="italic">How much data is enough for training?</em> It really depends. </p><p>You should have at least a few thousand data points for basic linear models, and hundreds of thousands for neural networks (such as with image classification, as previously mentioned). More data of optimal quality enables the model to predict smarter. If you have less data or data of poor quality, the recommendation is to consider a purpose-built AI service non-ML solution first. I often like to quote that with data, it's always <em class="italic">garbage in = garbage out</em>. Thus, if you have questionable data quality, it is of less value to a classic analytics method or an ML process. Additionally, an ML process is more expensive as you will waste a lot of time and resources and incur costs to train models with questionable performance. </p></li>
			</ul>
			<p>Now, let's assume that you have met the preceding requirements and identified that the problem <a id="_idIndexMarker643"/>you are trying to solve is truly an ML one. In that <a id="_idIndexMarker644"/>case, you can choose the following best practices to summarize the problem framing: </p>
			<ol>
				<li>Formulate the ML problem into a set of questions with its respective inputs and desired outputs.</li>
				<li>Define tangible performance metrics for the project, such as accuracy, prediction, or latency.</li>
				<li>Establish the definition of success for the project.</li>
				<li>Frame a strategy for data sourcing and data annotation.</li>
				<li>Start simple—build a model that is easy to interpret, test, debug, and manage.</li>
			</ol>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor147"/>Data collection or integration</h2>
			<p>In this of the E2E ML process, you will identify a dataset that will feed as an input to the ML pipeline and evaluate the <a id="_idIndexMarker645"/>appropriate means to collect that. In the <a id="_idIndexMarker646"/>previous chapters, you have learned that for different IoT use cases, AWS provides a number of ways to ingest the raw data in bulk or in real time. In other <a id="_idIndexMarker647"/>real-world scenarios, if you have <strong class="bold">petabytes</strong> (<strong class="bold">PB</strong>) of historical data in your cloud platform or data centers from IoT devices and <strong class="bold">information technology</strong> (<strong class="bold">IT</strong>) systems, there <a id="_idIndexMarker648"/>are multiple ways to transfer that to a data lake in the cloud, as follows:</p>
			<ul>
				<li>Transfer over the public internet </li>
				<li>Transfer over a private network using a dedicated fiber channel setup from your data centers to AWS using AWS Direct Connect </li>
				<li>Transfer using hardware devices such as AWS Snowball, AWS Snowmobile, or AWS Snowcone, as it will take less time than transfering over the public internet <p class="callout-heading">Fun Fact</p><p class="callout">Transporting data through snow devices is very similar to how you return a package to Amazon.com! You get hardware with an E-link screen acting as the return label, where the data can be <a id="_idIndexMarker649"/>loaded and shipped back to AWS data centers. If you have <strong class="bold">exabytes</strong> (<strong class="bold">EB</strong>) of data, AWS can even send you a truck for data transportation referred to as an AWS snowmobile. Please refer to the AWS documentation, <em class="italic">How to get started with AWS Snow Family</em> (<a href="https://docs.aws.amazon.com/snowball/index.html">https://docs.aws.amazon.com/snowball/index.html</a>), to understand the required steps.</p></li>
			</ul>
			<p>Again, consider the <a id="_idIndexMarker650"/>scenario of developing an ML model to <a id="_idIndexMarker651"/>identify vehicles approaching a parking-space area. Here, you can use a training dataset from a public data repository such as <strong class="bold">Caltech</strong>, as you mostly require generic images of kids, pets, and moving objects such as cars and <a id="_idIndexMarker652"/>trucks to be classified. There will be two datasets in scope, as follows:</p>
			<ul>
				<li><strong class="bold">Training dataset</strong>—Public dataset from Caltech to be hosted on a data lake (<strong class="bold">Amazon Simple Storage Service</strong> (<strong class="bold">Amazon S3</strong>))</li>
				<li><strong class="bold">Inferencing dataset</strong>—Generated in real time on the hub </li>
			</ul>
			<p>The following code enables the downloading of two datasets from a public data repository:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="Images/B17595_07_11.jpg" alt="Figure 7.11 – Data understanding&#13;&#10;" width="815" height="437"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11 – Data understanding</p>
			<p>For a different <a id="_idIndexMarker653"/>use case where a public dataset is not an <a id="_idIndexMarker654"/>option, your organization needs to have enough data points with optimal quality.</p>
			<p>A summary of best practices for this phase is provided here: </p>
			<ul>
				<li>Define the various sources of data that you will use as an input to the ML pipeline</li>
				<li>Determine the form of data to be used as an input (that is, raw versus transformed) to the pipeline</li>
				<li>Use data lineage mechanisms to ensure that the data location and source are cataloged if required for further processing </li>
				<li>Use different AWS-managed services to collect, store, and process the data without additional heavy lifting</li>
			</ul>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor148"/>Data preparation</h2>
			<p><strong class="bold">Data preparation</strong> is a key <a id="_idIndexMarker655"/>step in the pipeline as the ML models cannot perform optimally if the underlying data is not cleaned, curated, and validated. With IoT workloads, since the <a id="_idIndexMarker656"/>edge devices are co-existing in a physical environment with humans (over being hosted in a physical data center), the amount of noisy data generated can be substantial. In addition, as the dataset continues to grow from the connected ecosystem, data validation through schema comparisons can help detect if the data structure in newly obtained datasets has changed (for example, when a feature is deprecated). You can also detect if your data has started to drift—that is, the underlying statistics of the incoming data are different from the initial dataset used to train the model. Drift can happen due to an underlying trend or seasonality of the data or other factors. </p>
			<p>Thus, the general recommendation is to start the data preparation with a small, statistically valid sample that can be iteratively improved with different strategies, such as the following:</p>
			<ul>
				<li>Checking for data anomalies </li>
				<li>Checking for changes in the data schema</li>
				<li>Checking for statistics of the different dataset versions</li>
				<li>Checking for data integrity, and so on</li>
			</ul>
			<p>AWS provides a number of ways to help you prepare data at scale. You have already played with <strong class="bold">AWS Glue</strong> in <a href="B17595_05_Final_SS_ePub.xhtml#_idTextAnchor090"><em class="italic">Chapter 5</em></a>, <em class="italic">Ingesting and Streaming Data from the Edge</em>. If you remember, AWS Glue <a id="_idIndexMarker657"/>allows you to manage the life cycle of data—such as to discover, clean, transform, and catalog. Once the data treatment is complete and the data quality meets the required standard, it can be fed as an input to an ML process.</p>
			<p>In this chapter, we have introduced you to a different problem statement though, which is dealing with an unstructured dataset (aka images). Considering you are using a public dataset that's already labeled, you will only split the dataset into a training and a validation subset. The most common approach used by data scientists is to split the available data into a training and a test dataset, which is generally 70-30 (%) or 80-20 (%).</p>
			<p>The following code enables the splitting of two datasets from a public data repository:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="Images/B17595_07_12.jpg" alt="Figure 7.12 – Data preparation&#13;&#10;" width="829" height="238"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12 – Data preparation</p>
			<p>In the real <a id="_idIndexMarker658"/>world, though, you may not have clean or labeled data. Thus, you can leverage services such as <strong class="bold">Amazon SageMaker Ground Truth</strong>, which has an inbuilt capability to label data (such as images, text, audio, video) automatically along with easy access to public and private human labelers. This is useful if <a id="_idIndexMarker659"/>you lack in-house ML skills or are cost-sensitive to hiring data science professionals. Ground Truth uses an ML model to automatically label the raw data and produce high-quality training datasets at a fraction of a cost. But if the model fails to label the data confidently, it will route the problem to humans for resolution. Another aspect of data preparation is to understand the patterns in your dataset. </p>
			<p>A summary of best practices for this phase is provided here:</p>
			<ul>
				<li>Profile your data through discovery and transformation.</li>
				<li>Choose the right tool for the right job (such as data labeling versus tuning).</li>
				<li>Understand the patterns from data compositions.</li>
			</ul>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor149"/>Data visualization and analytics</h1>
			<p>In this phase, you can continue the data exploration through various analytics and visualization tools to assess the data fitment for ML training post profiling. You can continue to leverage services such as Amazon Athena, Amazon Quicksight, and others introduced to you in <a href="B17595_06_Final_SS_ePub.xhtml#_idTextAnchor119"><em class="italic">Chapter 6</em></a>, <em class="italic">Processing and Consuming Data on the Cloud</em>.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor150"/>Feature engineering (FE)</h2>
			<p>In this <a id="_idIndexMarker660"/>phase, your responsibilities as IoT professionals are very limited. This is <a id="_idIndexMarker661"/>where the data scientists will determine the unique attributes in the dataset that can be useful in training the ML model. You can think of rows as observations and columns as properties (or attributes). As data scientists, your goal is to identify the columns that matter in solving a specific business problem (aka features). For example, with image classification, the color or brand of a car is not a key feature to determine it as a vehicle. This <a id="_idIndexMarker662"/>process of selecting and transforming variables <a id="_idIndexMarker663"/>to ensure the creation of an optimized ML model is referred to as <strong class="bold">FE</strong>. Thus, the key objective of FE is to curate data in a form that an ML algorithm can use to extract patterns and infer better results. </p>
			<p>Let's break down the different phases of FE, as follows:</p>
			<ul>
				<li><strong class="bold">Feature creation</strong> to identify <a id="_idIndexMarker664"/>the attributes from a dataset relevant to the problem in scope, such as height and width of the pixels for images</li>
				<li><strong class="bold">Feature transformation</strong> for data compatibility or quality transformation, such <a id="_idIndexMarker665"/>as resizing inputs to a fixed size or converting non-numeric to numeric data</li>
				<li><strong class="bold">Feature extraction</strong> to determine <a id="_idIndexMarker666"/>a reduced set of features that offers the most value</li>
				<li><strong class="bold">Feature selection</strong> to filter <a id="_idIndexMarker667"/>redundant features from a dataset by observing variance of correlation thresholds</li>
			</ul>
			<p>If the number of features in a dataset becomes substantially large compared to the observations it <a id="_idIndexMarker668"/>can generate, the ML model may suffer from a problem called <strong class="bold">overfitting</strong>. On the other hand, if the number of features is limited, the model may infer a lot of incorrect predictions. This problem is referred to as <strong class="bold">underfitting</strong>. In other <a id="_idIndexMarker669"/>words, the model has trained well on the test data but is unable to apply the generalization to new or unseen datasets. Thus, feature extraction can help optimize a set of features for ML processing that are sufficient to <a id="_idIndexMarker670"/>generate a comprehensive version of the original set. Other than reducing the overfitting risk, feature extraction also speeds up the training through data compression and accuracy improvements. Different feature extraction techniques include <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>), <strong class="bold">Independent Component Analysis</strong> (<strong class="bold">ICA</strong>), <strong class="bold">Linear Discriminant Analysis</strong> (<strong class="bold">LDA</strong>), and <strong class="bold">Canonical Correlation Analysis</strong> (<strong class="bold">CCA</strong>).</p>
			<p>AWS provides <a id="_idIndexMarker671"/>a number of ways to help you <a id="_idIndexMarker672"/>perform FE on your dataset at scale in an iterative <a id="_idIndexMarker673"/>way. For example, Amazon SageMaker as a <a id="_idIndexMarker674"/>managed service provides a hosted <strong class="bold">Jupyter</strong> notebook <a id="_idIndexMarker675"/>environment where you can use scikit-learn libraries to perform FE. If your organization is already invested in an <strong class="bold">extract, transform, load</strong> (<strong class="bold">ETL</strong>) framework such as AWS Glue, <strong class="bold">AWS Glue DataBrew</strong>, or a managed <a id="_idIndexMarker676"/>Hadoop framework such as <strong class="bold">Amazon Elastic MapReduce</strong> (<strong class="bold">Amazon EMR</strong>), the data scientists can perform FE and transformation <a id="_idIndexMarker677"/>there, prior to leveraging SageMaker to train and deploy the models. </p>
			<p>Another option is using <strong class="bold">Amazon SageMaker Processing</strong>. This feature provides a fully managed <a id="_idIndexMarker678"/>environment for running analytics jobs for FE and model evaluation at scale, along with incorporating various security and compliance requirements.</p>
			<p>Here is a <a id="_idIndexMarker679"/>summary of best practices for this phase:</p>
			<ul>
				<li>Evaluate the <a id="_idIndexMarker680"/>attributes from the dataset that fit the <em class="italic">feature</em> paradigm</li>
				<li>Consider features that are useful to solve the problem at hand and remove redundant ones</li>
				<li>Build an iteration mechanism to explore new features or feature combinations </li>
			</ul>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor151"/>Model training </h2>
			<p>The key <a id="_idIndexMarker681"/>activities in this phase include choosing an ML algorithm that's appropriate for your problem and then training the model with the preprocessed data (aka features) from the earlier phases. We have already introduced you to the ML algorithms that are most common for IoT workloads in the <em class="italic">Taxonomy of ML with IoT workloads</em> section. Let's dive a bit deeper into those algorithms, as follows:</p>
			<ul>
				<li><strong class="bold">Classification</strong>—Classification can be applied in two ways; that is, binomial or multiclass. Binomial is useful when you have a set of observed values around two groups or categories, such as dog versus cat, or email being spam or not spam. Multiclass includes more than two groups or categories such as a set of flowers —roses, lilies, orchids, or tulips. Different classification techniques include decision trees, random forests, logistic regression, and naive Bayes. </li>
				<li><strong class="bold">Regression</strong>—Classification is used to predict a discrete value, whereas regression is used to predict a continuous variable. Regression can be applied in three ways: <em class="italic">least square method</em>, <em class="italic">linear</em>, or <em class="italic">logistic</em>. </li>
				<li><strong class="bold">Clustering</strong>—<em class="italic">K-means</em> is a very popular clustering algorithm generally used to assign a group to unlabeled data. This algorithm is fast and scalable as it uses a methodology to assign each group by computing the distance between the data point and each group center.</li>
			</ul>
			<p>In the safety scenario around the parking space cited earlier, we are using a multiclass classification <a id="_idIndexMarker682"/>algorithm, since we expect the model to classify multiple categories of objects, such as humans (specially kids), cars, and animals (such as cats, dogs, and rabbits). AWS services such as SageMaker do the undifferentiated heavy lifting of creating and managing the underlying infrastructure required for the training. You can choose different types of instances, such as CPU- or GPU-enabled. In the following example, you only specify an instance type of <strong class="source-inline">ml.p2.xlarge</strong> along with other required parameters such as <strong class="source-inline">volume</strong> and <strong class="source-inline">instance_count</strong>, and SageMaker does the rest, using the estimator interface for instantiating and managing the infrastructure:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="Images/B17595_07_13.jpg" alt="Figure 7.13 – ML training infrastructure&#13;&#10;" width="829" height="384"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – ML training infrastructure</p>
			<p>You will be using the <strong class="bold">MXNet framework</strong> in this chapter, but SageMaker allows most other ML <a id="_idIndexMarker683"/>frameworks, such as TensorFlow, PyTorch, and Gluon, to train your model. </p>
			<p>Please note that <a id="_idIndexMarker684"/>model optimization is a critical aspect of ML where you need to train a model with different sets of parameters to identify the most performant one. SageMaker hyperparameter tuning jobs help to optimize the models using Bayesian optimization or random search techniques. As you can see in the following example, the model is getting trained using hyperparameters such as batch size and shape to solve your business problem:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="Images/B17595_07_14.jpg" alt="Figure 7.14 – ML training parameters&#13;&#10;" width="352" height="283"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – ML training parameters</p>
			<p>To make this process of model training easier and cost-effective for organizations new to ML, SageMaker supports automatic model tuning (through <strong class="bold">Autopilot</strong>) to automatically perform <a id="_idIndexMarker685"/>these actions on your behalf. It's also possible to use your custom ML algorithm as a container image and train it using SageMaker. For example, if you already have a homegrown image classification model that doesn't use any of the SageMaker-supported ML frameworks, you can use your model as a container image and retrain it in SageMaker without starting from scratch. SageMaker also offers a monitoring and debugging capability that allows clear visibility to the training metrics. </p>
			<p>Here is a summary of best practices of this phase:</p>
			<ul>
				<li>Choose the right algorithm and training parameters for your data or let the managed services choose these for you </li>
				<li>Ensure the dataset is segregated into training and test sets</li>
				<li>Apply incremental learning to build the most optimized model</li>
				<li>Monitor the training metrics to ensure the model performance doesn't degrade over time</li>
			</ul>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor152"/>Model evaluation and deployment </h2>
			<p>In this phase, the model is evaluated to assess if it solves the business problem in context. If it <a id="_idIndexMarker686"/>doesn't, you can build multiple models with different business rules or methodologies (such as a different algorithm, other training <a id="_idIndexMarker687"/>parameters, and so on) until you find the optimized model that meets the business KPIs. Data scientists may often uncover inferences for other business problems in this phase as they test the model(s) against a real application. In order to evaluate the model, it can be tested against <em class="italic">historical data</em> (aka offline evaluation) or <em class="italic">live data</em> (aka online evaluation). Once the ML algorithm passes the evaluation, the next step is to deploy the model(s) to production. </p>
			<p>The scenario for IoT/edge workloads gets a bit tricky, though, if your use cases primarily deal with offline processing and inferencing on the edge. In that case, you don't have access to the scale of the cloud, and thus the best practice is often to further optimize the models. <strong class="bold">SageMaker Neo</strong> can be <a id="_idIndexMarker688"/>useful in this scenario, as it allows you to train your model once and run anywhere in the cloud or at the edge. With this service, you can compile the model in most common frameworks (such as MXNET, TensorFlow, PyTorch, Keras) and deploy the optimized version on a target platform of your choice (such as hardware from Intel, NXP, NVIDIA, Apple, and so on). The following code helps to optimize the model based on different parameters such as OS and Architecture:</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="Images/B17595_07_15.jpg" alt="Figure 7.15 – Optimizing the model with SageMaker Neo&#13;&#10;" width="787" height="594"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.15 – Optimizing the model with SageMaker Neo</p>
			<p>The way SageMaker Neo works is, its compiler uses an ML model under the hood to apply the best <a id="_idIndexMarker689"/>available performance for your model on <a id="_idIndexMarker690"/>the respective edge platform or device. Neo can optimize models to perform up to 25 times faster with no loss in accuracy and requires as little as one-tenth of the footprint compared to a non-optimized model. But how? Let's explore, as follows: </p>
			<ul>
				<li>Neo has <a id="_idIndexMarker691"/>a compiler and a runtime component. </li>
				<li>The Neo compiler has an <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) that can read models developed using various frameworks. Once the read operation is complete, it converts the framework-specific functions and operations into a framework-agnostic intermediate representation. </li>
				<li>Once the conversion is complete, it then starts performing a set of optimizations. As a result of this optimization, binary code is generated and persisted to a shared object library, along with the model definition and parameters that are stored in separate files. </li>
				<li>Finally, Neo runtime APIs for the supported target platform can load and execute this compiled model to offer the required performance boost.</li>
			</ul>
			<p>As you can <a id="_idIndexMarker692"/>imagine, this optimization is powerful for edge <a id="_idIndexMarker693"/>devices as they are resource-constrained. The following screenshot diagrammatically represents how you can deploy an optimized model in your production environment. In the <em class="italic">Hands-on with ML architecture</em> section of this chapter, you will learn how to deploy a model trained with SageMaker Neo:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="Images/B17595_07_16.jpg" alt=" Figure 7.16 – Optimizing the ML model for the hardware&#13;&#10;" width="1650" height="412"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption"> Figure 7.16 – Optimizing the ML model for the hardware</p>
			<p>Once the model is deployed, you need to monitor the performance metrics over time. This is because it's pretty common for models to function less effectively as the real-world data may start to differ from the data that was used to train the model. The SageMaker Model Monitor service can help detect deviations and alert you, the data scientists, or ML operators, to take remedial action. </p>
			<p>Here is a summary of best practices for this phase:</p>
			<ul>
				<li>Evaluate if the model performance meets the business goals</li>
				<li>Identify the inferencing method required for your models (offline versus online)</li>
				<li>Deploy the <a id="_idIndexMarker694"/>model on the cloud with automatic <a id="_idIndexMarker695"/>scaling options or on the edge with hardware-specific optimization</li>
				<li>Monitor the model performance in production to identify drift and perform remediations</li>
			</ul>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor153"/>ML design principles</h2>
			<p>Now that you have learned about the common activities in an ML workflow, let's summarize the <a id="_idIndexMarker696"/>design principles from the steps explained in the preceding section, as follows: </p>
			<ul>
				<li>Work backward from the use case to identify if it's a problem that needs ML or that can be solved using a classic analytical approach.</li>
				<li>Collect enough data of optimal quality to have accurate ML models.  </li>
				<li>Perform profiling to understand the data relationships and compositions. Remember: <em class="italic">garbage in = garbage out</em>, thus data preparation is key in an ML process.</li>
				<li>Start with a small set of features (aka attributes) to solve a specific business problem and evolve through experimentation. </li>
				<li>Consider using different sets of data for training, evaluating, and inferencing purposes. </li>
				<li>Evaluate the accuracy of the models and continue to iterate until it's optimal for the business problem in context.</li>
				<li>Determine if the model needs to inference on real-time (online) or historical (offline) data. </li>
				<li>Host different variants of the model on the cloud or on the edge and identify the most optimal one against real-world data.</li>
				<li>Continuously monitor the metrics from the deployed models for accuracy, remediation, and improvement.</li>
				<li>Leverage <a id="_idIndexMarker697"/>managed services for offloading the heavy lifting of managing the underlying infrastructure.</li>
				<li>Automate the different activities of the pipeline as much as possible, such as data preparation, model training, evaluation, hosting, monitoring, and alerting. </li>
			</ul>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor154"/>ML anti-patterns for IoT workloads</h2>
			<p>Similar to any <a id="_idIndexMarker698"/>other distributed solutions, IoT workloads based on ML have anti-patterns as well. Here are some of them:</p>
			<ul>
				<li><strong class="bold">Don't put all your eggs in one basket</strong>—The E2E ML process includes many activities and thus requires different personas such as the following:<ul><li><em class="italic">Data engineer</em>—For data preparation, designing ETL (or ELT) processes</li><li><em class="italic">Data scientists</em>—For building, training, and optimizing the models</li><li><em class="italic">Development-operations (DevOps</em> or <em class="italic">MLOps) engineers</em>—For building a scalable and repeatable machine learning infrastructure built with operating and monitoring mechanisms</li><li><em class="italic">IoT engineers</em>—For building a cloud-to-edge deployment pipeline along with integration from the IoT gateway to different backend services</li></ul><p>In summary, expecting a single resource to perform all these activities at scale will lead to the failure of the project. </p></li>
				<li><strong class="bold">Don't assume the requirements; get aligned</strong>—A plethora of tools is available for analytical and ML purposes and each has its pros and cons. For example, consider the following: <ul><li>Both <strong class="bold">R</strong> and <strong class="bold">Python</strong> are popular programming languages for developing ML systems. In general, business analysts or statisticians will prefer R or other commercial <a id="_idIndexMarker699"/>solutions (such as <strong class="bold">MATLAB</strong>), while data scientists will choose Python.</li><li>Similarly, data scientists may have their preferred ML framework, such as TensorFlow over PyTorch. Choosing different languages or frameworks has downstream impacts—for example, the edge hardware may need to support the respective version or libraries required for inferencing the ML model. </li></ul><p>Thus, it's important <a id="_idIndexMarker700"/>for all the different personas engaged in ML activities to stay aligned on the business and technical requirements. </p></li>
				<li><strong class="bold">Plan for technical debt</strong>—ML systems for IoT workloads are prone to accruing technical debt due to their data dependencies from IoT sources that are often unreliable due to their noisiness. This may also happen due to dependencies on other upstreams having inconsistent data. For example, consider the following:<ul><li>If the composition of any feature (or a few) changes substantially, it will lead the model to behave differently in the real world. This problem is also referred <a id="_idIndexMarker701"/>to as the <strong class="bold">training-serving skew</strong>, where there is a discrepancy in how you handle data in the training and serving pipelines.</li><li>The reason ML workloads are different from traditional IT systems is their behavior can be determined only through real data over unit testing with a small sample.</li></ul></li>
			</ul>
			<p>Thus, it's key to monitor the accuracy of the model, optimize it iteratively based on the knowledge of the gathered data, and redeploy.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor155"/>Hands-on with ML architecture</h1>
			<p>In this section, you will deploy a solution on a connected HBS hub that will require you to build and <a id="_idIndexMarker702"/>train ML models on the cloud and then deploy them to the edge for inferencing. The following screenshot shows the architecture of the lab with the highlighted steps <em class="italic">(1-5)</em> that you will complete: </p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="Images/B17595_07_17.jpg" alt="Figure 7.17 – Hands-on ML architecture&#13;&#10;" width="1231" height="573"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.17 – Hands-on ML architecture</p>
			<p>Your objectives include the following, which are highlighted as distinct steps in the preceding architecture:</p>
			<ul>
				<li>Build the ML workflow using Amazon SageMaker </li>
				<li>Deploy the ML model from the cloud to the edge using AWS IoT Greengrass</li>
				<li>Perform ML inferencing on the edge and visualize the results</li>
			</ul>
			<p>The following <a id="_idIndexMarker703"/>table shows the list of components you will use during the lab:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="Images/B17595_07_Table4.jpg" alt="Figure 7.18 – Hands-on lab components&#13;&#10;" width="1642" height="360"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.18 – Hands-on lab components</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor156"/>Building the ML workflow </h2>
			<p>In this <a id="_idIndexMarker704"/>section, you will build, train, and test the ML model using Amazon SageMaker Studio.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Training models using Amazon SageMaker will incur additional cost. If you want to save on that, please use a trained ML model available in GitHub for your platform and skip to the next section, that is, Deploying the model from cloud to the edge.</p>
			<p>Amazon Sagemaker Studio is a web-based <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) that enables data scientists (or ML engineers) with a single-stop <a id="_idIndexMarker705"/>shop for all things ML. To train the model, you will use a public dataset from Caltech that has a collection of over 30,000 images across 256 object categories. Let's begin. Proceed as follows: </p>
			<ol>
				<li value="1">Please navigate to the <strong class="bold">Amazon SageMaker</strong> console and select <strong class="bold">SageMaker Domain Studio</strong> (from the left pane). If this is the first time you are interacting with the studio, you will be prompted to complete a one-time setup. Please choose <strong class="bold">Quick setup,</strong> click <strong class="bold">Submit,</strong> choose <strong class="bold">Default VPC with a subnet (s) of your choice</strong>, then click <strong class="bold">Save and continue</strong>.</li>
				<li> It will take a few minutes for the studio to be set up. Please wait until the status shows <strong class="bold">Ready</strong> and then click <strong class="bold">Launch app</strong> -&gt; <strong class="bold">Studio</strong>.</li>
				<li>This should open up the SageMaker Studio (aka Jupyter console) for you. In case you are new to Jupyter, consider this as an IDE for developing ML models similar to Eclipse, Visual Studio, and so on, used for developing distributed applications. </li>
				<li>Please upload the Jupyter notebook (<strong class="source-inline">Image-Classification*.ipynb</strong>) and the <strong class="source-inline">synset.txt</strong> file from the <strong class="source-inline">chapter7/notebook</strong> folder using the <strong class="bold">Upload file</strong> button in the top-left pane.</li>
				<li>Double-click <a id="_idIndexMarker706"/>to open the Jupyter notebook and choose the Python runtime and kernel, as shown in the following screenshot:<div id="_idContainer105" class="IMG---Figure"><img src="Images/B17595_07_19.jpg" alt="Figure 7.19 – Jupyter notebook kernel&#13;&#10;" width="1025" height="563"/></div><p class="figure-caption">Figure 7.19 – Jupyter notebook kernel</p></li>
				<li>Choosing a kernel is a critical step as it provides you with the appropriate runtime for training the ML model. Update the kernel (top right) to choose the GPU runtime for training the ML model. Since you will be processing images, a GPU runtime is preferable. After choosing the kernel, the Jupyter notebook will have the following kernel and configuration:<div id="_idContainer106" class="IMG---Figure"><img src="Images/B17595_07_20.jpg" alt="Figure 7.20 – Choosing a kernel&#13;&#10;" width="1263" height="458"/></div><p class="figure-caption">Figure 7.20 – Choosing a kernel</p></li>
				<li>Now, navigate <a id="_idIndexMarker707"/>through the code slowly. Please ensure you read the text preceding the code to better understand the functioning of each of these blocks. </li>
				<li>Click the <strong class="bold">Run</strong> button, as highlighted in the following screenshot, to execute one block at a time. Please don't click on the <strong class="bold">Run</strong> button if you see an asterisk (<strong class="source-inline">*</strong>) adjacent to the block. The asterisk implies that the code is still running. Please wait for that to disappear before you proceed: <div id="_idContainer107" class="IMG---Figure"><img src="Images/B17595_07_21.jpg" alt="Figure 7.21 – Running the steps&#13;&#10;" width="1257" height="453"/></div><p class="figure-caption">Figure 7.21 – Running the steps</p></li>
				<li>If you go <a id="_idIndexMarker708"/>through all the steps till the end, you will be able to complete the following steps:<ol><li>Downloading a training dataset </li><li>Preparing and preprocessing the data </li><li>Splitting the dataset into training and test samples </li><li>Training the model using the appropriate framework and parameters</li><li>Optimizing the model for the edge hardware </li><li>Hosting the model artifacts on the Amazon S3 repository</li></ol><p>The model training can take up to 10 minutes to complete. After the training, you have an ML model that's trained using the MXNet framework and is capable of performing classification on 256 different objects.</p></li>
				<li>Please <a id="_idIndexMarker709"/>navigate through the S3 bucket (<strong class="source-inline">sagemaker-&lt;region&gt;-&lt;accountid&gt;/ic-fulltraining/</strong>). Copy the S3 <strong class="bold">Uniform Resource Identifier</strong> (<strong class="bold">URI</strong>) for the model object (<strong class="source-inline">DLR-resnet50-*-cpu-ImageClassification.zip</strong>), as you will need it in the next section.</li>
			</ol>
			<p>Now that <a id="_idIndexMarker710"/>the model is trained on the cloud, you will deploy it on the edge using Greengrass for near-real-time inferencing.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor157"/>Deploying the model from cloud to the edge</h2>
			<p>As an IoT practitioner, deploying the model from the cloud to the edge is the step you will primarily <a id="_idIndexMarker711"/>be involved in. This is the transition point, where the ML or data science team provides a model that needs <a id="_idIndexMarker712"/>to be pushed to a fleet of devices on the edge. Although it's possible to automate these steps in the real world using a <strong class="bold">continuous integration/continuous deployment (CI/CD)</strong> pipeline, you will do it manually to learn the process in detail.</p>
			<p>Similar to the components you have created in the earlier chapters for deploying different processes on the edge (such as publisher, subscriber, aggregator), deploying an ML resource needs the same approach. We will create a component that includes the ML model trained in the previous section. We will continue to use the AWS console for continuity.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Greengrass provides a sample image classification model component that you toyed with in <a href="B17595_04_Final_SS_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 4</em></a>, <em class="italic">Extending the Cloud to the Edge</em>. Here, you are learning how to modify existing model components with your custom resources. In the real world, you may even have to create a new model component from scratch, where you can follow a similar process.</p>
			<p>Let's begin. Proceed as follows:</p>
			<ol>
				<li value="1">Please navigate to the <strong class="bold">Amazon IoT Greengrass</strong> console, click on <strong class="bold">Components</strong>, click on the <strong class="bold">My components</strong> tab, and then on <strong class="bold">Create Component</strong>. Under <strong class="bold">Component information</strong>, select <strong class="bold">Enter recipe as JSON</strong> as your component source.<p>In the <strong class="bold">Recipe</strong> box, paste the component recipe from the <strong class="source-inline">chapter7/recipe</strong> folder. Now, let's update the recipe to point to the trained model. Please replace <a id="_idIndexMarker713"/>the URI (marked with an arrow) with the S3 URI copied in <em class="italic">Step 10</em> of the <em class="italic">Building the ML workflow</em> section, as illustrated in the following screenshot. If you have skipped the earlier section and used a trained model from GitHub, please manually upload the model to the S3 bucket of your choice and update the S3 URI in the recipe accordingly:</p><div id="_idContainer108" class="IMG---Figure"><img src="Images/B17595_07_22.jpg" alt="Figure 7.22 – Recipe configuration&#13;&#10;" width="678" height="697"/></div><p class="figure-caption">Figure 7.22 – Recipe configuration</p></li>
				<li>Click <strong class="bold">Create component</strong> to finish creating the model resource. This model should appear on the <strong class="bold">My components</strong> tab of the <strong class="bold">Components</strong> page.</li>
				<li>Now, you need to create an inferencing component. This is the resource that triggers <a id="_idIndexMarker714"/>the model on the edge and publishes the results to the cloud. <p class="callout-heading">Note</p><p class="callout">Greengrass provides a sample image classification inferencing component, <strong class="source-inline">aws.greengrass.DLRImageClassification</strong>, that you already played with in <a href="B17595_04_Final_SS_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 4</em></a>, <em class="italic">Extending the Cloud to the Edge</em>. Here, you are learning how to use the same inferencing component to work with your custom ML model. In the real world, you may have to create a new inferencing component from scratch with a modified manifest file, as you just did with the model.</p></li>
				<li>On the <strong class="bold">Amazon Greengrass</strong> console, choose <strong class="bold">Deployments</strong>. On the <strong class="bold">Deployments</strong> page, revise your existing deployment and choose the following components: <ul><li><strong class="source-inline">variant.DLR.ImageClassification.ModelStore</strong>: ML model trained through SageMaker. You can choose this from the <strong class="bold">My components</strong> tab.</li><li><strong class="source-inline">aws.greengrass.DLRImageClassification</strong>: Inferencing script for the ML model. You can choose this from the <strong class="bold">Public Component</strong> tab.</li><li><strong class="source-inline">variant.DLR</strong>: Runtime required for the ML inferencing. You can choose this from the <strong class="bold">Public Component</strong> tab.</li></ul></li>
				<li>On the <strong class="bold">Select Components</strong> page, ensure the components shown in the following screenshot are chosen, and then click <strong class="bold">Next</strong>:<div id="_idContainer109" class="IMG---Figure"><img src="Images/B17595_07_23.jpg" alt="Figure 7.23 – Greengrass component dependencies&#13;&#10;" width="836" height="421"/></div><p class="figure-caption">Figure 7.23 – Greengrass component dependencies</p></li>
				<li>On the <strong class="bold">Configure components</strong> page, select the <strong class="source-inline">aws.greengrass.DLRImageClassification</strong> component. This will enable the <strong class="bold">Configure component</strong> option; click on that. </li>
				<li>Update <a id="_idIndexMarker715"/>the <strong class="bold">Configuration to merge</strong> section (right pane) with the following configuration and click on <strong class="bold">Confirm</strong>:<p class="source-code">{</p><p class="source-code">  "InferenceInterval": "5"</p><p class="source-code">}</p></li>
				<li>Keep all the other options as default in the following screens and choose <strong class="bold">Deploy</strong>. </li>
			</ol>
			<p>Let's now move on to the next section.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor158"/>Performing ML inferencing on the edge and validating results</h2>
			<p>So, by now, the models are built, trained, and deployed on Greengrass. The images to be inferred <a id="_idIndexMarker716"/>are stored in the following directory: </p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="Images/B17595_07_24.jpg" alt="" width="1650" height="177"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.24 – Images directory on Greengrass hub</p>
			<p>Similar to <a id="_idIndexMarker717"/>the cat image, you can deploy other images as well (such as dogs, humans, and so on) and configure the inferencing component to infer on them. </p>
			<p>Let's visualize the inferencing results that are being published from the edge to the cloud. These results are key in assessing if the model is performing at the expected accuracy level. We'll proceed as follows:</p>
			<ol>
				<li value="1">First, let's check if the component status shows as running and also check the Greengrass log to verify that there are no exceptions. Here's the code we'll need to do this:<p class="source-code">sudo /greengrass/v2/bin/greengrass-cli component list</p><p class="source-code">sudo tail -100f /greengrass/v2/logs/greengrass.log</p><p class="source-code">sudo tail -100f aws.greengrass.DLRImageClassification.log</p></li>
				<li>If there are no errors, please navigate to the AWS IoT console, choose <strong class="bold">Test</strong>, and then choose <strong class="bold">MQTT test client</strong>.</li>
				<li>Under <strong class="bold">Subscriptions</strong>, choose <strong class="source-inline">ml/dlr/image-classification</strong>. Click <strong class="bold">Subscribe</strong> to view the results, which should look like this:</li>
			</ol>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="Images/B17595_07_25.jpg" alt="Figure 7.25 – Inferenced results from ML&#13;&#10;" width="1300" height="665"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.25 – Inferenced results from ML</p>
			<p>If you <a id="_idIndexMarker718"/>get stuck or require help, please <a id="_idIndexMarker719"/>refer to the <strong class="bold">Troubleshooting</strong> section in GitHub.</p>
			<p>Congratulations on finishing the hands-on section of this chapter! Now, your connected <strong class="bold">HBS hub</strong> is equipped with ML capabilities that can operate in both online and offline conditions and can classify humans, pets, and vehicles in your parking-space area.</p>
			<p class="callout-heading">Challenge Zone (Optional)</p>
			<p class="callout">Can you figure out how to act on the inference results published to AWS IoT Core? This will be useful to automatically trigger an announcement through notifications/alarms for cautioning kids/pets strolling in the parking-space area.</p>
			<p class="callout"><strong class="bold">Hint</strong>: You need to define a routing logic in the IoT Rules Engine to push the results through a notification service. </p>
			<p>Isn't it <a id="_idIndexMarker720"/>incredible that you have now <a id="_idIndexMarker721"/>learned how to build ML capabilities for IoT workloads on the edge? It's time for a well-deserved break! Let's wrap up this chapter with a quick summary and a set of questions. </p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor159"/>Summary </h1>
			<p>In this chapter, you were introduced to ML concepts relevant to IoT workloads. You learned how to design ML pipelines, along with optimizing models for IoT workloads. You implemented an edge-to-cloud architecture to perform inferences on unstructured data (images). Finally, you validated the workflow by visualizing the inferencing results from the edge for additional insights. </p>
			<p>In the next chapter, you will learn how to implement DevOps and MLOps practices to achieve operational efficiency for IoT edge workloads deployed at scale.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor160"/>Knowledge check</h1>
			<p>Before moving on to the next chapter, test your knowledge by answering these questions. The answers can be found at the end of the book: </p>
			<ol>
				<li value="1">True or false: Two types of ML algorithms exist: supervised and unsupervised.</li>
				<li>Can you recall the four types of ML systems and their significance?</li>
				<li>True or false: K-means is a classification algorithm.</li>
				<li>Can you put the three phases of the ML project life cycle in the right order?</li>
				<li>Can you think of at least two common frameworks used for training ML models?</li>
				<li>What is the AWS service used for deploying trained models from the cloud to the edge?</li>
				<li>True or False: AWS IoT Greengrass only supports custom components for image classification problems.</li>
				<li>Can you tell me about one anti-pattern for ML with IoT workloads?</li>
			</ol>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor161"/>References</h1>
			<p>Take a look at the following resources for additional information on the concepts discussed in this chapter:</p>
			<ul>
				<li>CRISP: <a href="https://www.datascience-pm.com/crisp-dm-2/%0D">https://www.datascience-pm.com/crisp-dm-2/</a></li>
				<li><em class="italic">A Short History of Machine Learning</em>: <a href="https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/?sh=1ca6cea115e7%0D">https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/?sh=1ca6cea115e7</a></li>
				<li><em class="italic">Machine Learning on AWS</em>: <a href="https://aws.amazon.com/machine-learning/%0D">https://aws.amazon.com/machine-learning/</a></li>
				<li>ML with AWS IoT services: <a href="https://aws.amazon.com/blogs/iot/category/artificial-intelligence/sagemaker/%0D">https://aws.amazon.com/blogs/iot/category/artificial-intelligence/sagemaker/</a></li>
				<li><em class="italic">Using AWS IoT Greengrass Version 2 with Amazon SageMaker Neo and NVIDIA DeepStream Applications</em>: <a href="https://aws.amazon.com/blogs/iot/using-aws-iot-greengrass-version-2-with-amazon-sagemaker-neo-and-nvidia-deepstream-applications/%0D">https://aws.amazon.com/blogs/iot/using-aws-iot-greengrass-version-2-with-amazon-sagemaker-neo-and-nvidia-deepstream-applications/</a></li>
				<li>Well-Architected Framework for ML: <a href="https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/machine-learning-lens.html%0D">https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/machine-learning-lens.html</a></li>
				<li>Learn from AWS through <strong class="bold">Machine Learning University</strong> (<strong class="bold">MLU</strong>):  <a href="https://aws.amazon.com/machine-learning/mlu/">https://aws.amazon.com/machine-learning/mlu/</a></li>
			</ul>
		</div>
	</div></body></html>