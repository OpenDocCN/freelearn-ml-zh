["```py\nimport numpy as np\nimport math\n\n# sigmoidal function\ndef activation(x):\n    return 1 / (1 + math.exp(-x))\n\ninputs = np.array([1, 2, 3, 4])\nweights = np.array([0.2, 0.324, 0.1, .001])\nbias = 1.5\n\na = activation(np.dot(inputs.T, weights) + bias)\n\nprint a\n0.9341341524806636\n```", "```py\n# import numpy and matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn import linear_model, datasets, metrics\n# scikit-learn implementation of RBM\nfrom sklearn.neural_network import BernoulliRBM\nfrom sklearn.pipeline import Pipeline\n```", "```py\n# create numpy array from csv\nimages = np.genfromtxt('../data/mnist_train.csv', delimiter=',')\n```", "```py\n# 6000 images and 785 columns, 28X28 pixels + 1 response\nimages.shape\n\n(6000, 785)\n```", "```py\n# extract the X and y variable\nimages_X, images_y = images[:,1:], images[:,0]\n\n# values are much larger than 0-1 but scikit-learn RBM version assumes 0-1 scaling\nnp.min(images_X), np.max(images_X)\n(0.0, 255.0)\n```", "```py\nplt.imshow(images_X[0].reshape(28, 28), cmap=plt.cm.gray_r)\n\nimages_y[0]\n```", "```py\n# scale images_X to be between 0 and 1\n images_X = images_X / 255.\n\n # make pixels binary (either white or black)\n images_X = (images_X > 0.5).astype(float)\n\n np.min(images_X), np.max(images_X)\n (0.0, 1.0)\n```", "```py\nplt.imshow(images_X[0].reshape(28, 28), cmap=plt.cm.gray_r)\n\n images_y[0]\n```", "```py\n# import Principal Components Analysis module\n from sklearn.decomposition import PCA\n\n # extract 100 \"eigen-digits\"\n pca = PCA(n_components=100)\n pca.fit(images_X)\n\n # graph the 100 components\n plt.figure(figsize=(10, 10))\n for i, comp in enumerate(pca.components_):\n plt.subplot(10, 10, i + 1)\n plt.imshow(comp.reshape((28, 28)), cmap=plt.cm.gray_r)\n plt.xticks(())\n plt.yticks(())\n plt.suptitle('100 components extracted by PCA')\n\n plt.show()\n```", "```py\n# first 30 components explain 64% of the variance\n\n pca.explained_variance_ratio_[:30].sum()\n .637414\n```", "```py\n# Scree Plot\n\n # extract all \"eigen-digits\"\n full_pca = PCA(n_components=784)\n full_pca.fit(images_X)\n\n plt.plot(np.cumsum(full_pca.explained_variance_ratio_))\n\n # 100 components captures about 90% of the variance\n```", "```py\n# Use the pca object, that we have already fitted, to transform the first image in order to pull out the 100 new features\npca.transform(images_X[:1])\n\narray([[ 0.61090568, 1.36377972, 0.42170385, -2.19662828, -0.45181077, -1.320495 , 0.79434677, 0.30551126, 1.22978985, -0.72096767, ...\n\n# reminder that transformation is a matrix multiplication away\nnp.dot(images_X[:1]-images_X.mean(axis=0), pca.components_.T)\n\narray([[ 0.61090568, 1.36377972, 0.42170385, -2.19662828, -0.45181077, -1.320495 , 0.79434677, 0.30551126, 1.22978985, -0.72096767,\n```", "```py\n# instantiate our BernoulliRBM\n # we set a random_state to initialize our weights and biases to the same starting point\n # verbose is set to True to see the fitting period\n # n_iter is the number of back and forth passes\n # n_components (like PCA and LDA) represent the number of features to create\n # n_components can be any integer, less than , equal to, or greater than the original number of features\n\n rbm = BernoulliRBM(random_state=0, verbose=True, n_iter=20, n_components=100)\n\n rbm.fit(images_X)\n\n [BernoulliRBM] Iteration 1, pseudo-likelihood = -138.59, time = 0.80s\n [BernoulliRBM] Iteration 2, pseudo-likelihood = -120.25, time = 0.85s [BernoulliRBM] Iteration 3, pseudo-likelihood = -116.46, time = 0.85s ... [BernoulliRBM] Iteration 18, pseudo-likelihood = -101.90, time = 0.96s [BernoulliRBM] Iteration 19, pseudo-likelihood = -109.99, time = 0.89s [BernoulliRBM] Iteration 20, pseudo-likelihood = -103.00, time = 0.89s\n```", "```py\n# RBM also has components_ attribute\n len(rbm.components_)\n\n 100\n```", "```py\n# plot the RBM components (representations of the new feature sets)\n plt.figure(figsize=(10, 10))\n for i, comp in enumerate(rbm.components_):\n plt.subplot(10, 10, i + 1)\n plt.imshow(comp.reshape((28, 28)), cmap=plt.cm.gray_r)\n plt.xticks(())\n plt.yticks(())\n plt.suptitle('100 components extracted by RBM', fontsize=16)\n\n plt.show()\n```", "```py\n# It looks like many of these components are exactly the same but\n\n # this shows that all components are actually different (albiet some very slightly) from one another\n np.unique(rbm.components_.mean(axis=1)).shape\n\n (100,)\n```", "```py\n# Use our Boltzman Machine to transform a single image of a 5\n image_new_features = rbm.transform(images_X[:1]).reshape(100,)\n\n image_new_features\n\n array([ 2.50169424e-16, 7.19295737e-16, 2.45862898e-09, 4.48783657e-01, 1.64530318e-16, 5.96184335e-15, 4.60051698e-20, 1.78646959e-08, 2.78104276e-23, ...\n```", "```py\n# not the same as a simple matrix multiplication anymore\n # uses neural architecture (several matrix operations) to transform features\n np.dot(images_X[:1]-images_X.mean(axis=0), rbm.components_.T)\n\n array([[ -3.60557365, -10.30403384, -6.94375031, 14.10772267, -6.68343281, -5.72754674, -7.26618457, -26.32300164, ...\n```", "```py\n# get the most represented features\n top_features = image_new_features.argsort()[-20:][::-1]\n\n print top_features\n\n [56 63 62 14 69 83 82 49 92 29 21 45 15 34 28 94 18 3 79 58]\n\n print image_new_features[top_features]\n\n array([ 1\\. , 1\\. , 1\\. , 1\\. , 1\\. , 1\\. , 1\\. , 0.99999999, 0.99999996, 0.99999981, 0.99996997, 0.99994894, 0.99990515, 0.9996504 , 0.91615702, 0.86480507, 0.80646422, 0.44878366, 0.02906352, 0.01457827])\n```", "```py\n# plot the RBM components (representations of the new feature sets) for the most represented features\n plt.figure(figsize=(25, 25))\n for i, comp in enumerate(top_features):\n plt.subplot(5, 4, i + 1)\n plt.imshow(rbm.components_[comp].reshape((28, 28)), cmap=plt.cm.gray_r)\n plt.title(\"Component {}, feature value: {}\".format(comp, round(image_new_features[comp], 2)), fontsize=20)\n plt.suptitle('Top 20 components extracted by RBM for first digit', fontsize=30)\n\n plt.show()\n```", "```py\n# grab the least represented features\n bottom_features = image_new_features.argsort()[:20]\n\n plt.figure(figsize=(25, 25))\n for i, comp in enumerate(bottom_features):\n plt.subplot(5, 4, i + 1)\n plt.imshow(rbm.components_[comp].reshape((28, 28)), cmap=plt.cm.gray_r)\n plt.title(\"Component {}, feature value: {}\".format(comp, round(image_new_features[comp], 2)), fontsize=20)\n plt.suptitle('Bottom 20 components extracted by RBM for first digit', fontsize=30)\n\n plt.show()\n```", "```py\n# import logistic regression and gridsearch module for some machine learning\n\n from sklearn.linear_model import LogisticRegression\n from sklearn.model_selection import GridSearchCV\n\n # create our logistic regression\n lr = LogisticRegression()\n params = {'C':[1e-2, 1e-1, 1e0, 1e1, 1e2]}\n\n # instantiate a gridsearch class\n grid = GridSearchCV(lr, params)\n```", "```py\n # fit to our data\n grid.fit(images_X, images_y)\n\n # check the best params\n grid.best_params_, grid.best_score_\n\n ({'C': 0.1}, 0.88749999999999996)\n```", "```py\n# Use PCA to extract new features\n\nlr = LogisticRegression()\npca = PCA()\n\n# set the params for the pipeline\nparams = {'clf__C':[1e-1, 1e0, 1e1],\n'pca__n_components': [10, 100, 200]}\n\n# create our pipeline\npipeline = Pipeline([('pca', pca), ('clf', lr)])\n\n# instantiate a gridsearh class\ngrid = GridSearchCV(pipeline, params)\n```", "```py\n # fit to our data\ngrid.fit(images_X, images_y)\n\n# check the best params\ngrid.best_params_, grid.best_score_\n\n({'clf__C': 1.0, 'pca__n_components': 100}, 0.88949999999999996)\n```", "```py\n# Use the RBM to learn new features\n\nrbm = BernoulliRBM(random_state=0)\n\n# set up the params for our pipeline.\nparams = {'clf__C':[1e-1, 1e0, 1e1],\n'rbm__n_components': [10, 100, 200]\n}\n\n# create our pipeline\npipeline = Pipeline([('rbm', rbm), ('clf', lr)])\n\n# instantiate a gridsearch class\ngrid = GridSearchCV(pipeline, params)\n```", "```py\n# fit to our data\ngrid.fit(images_X, images_y)\n\n# check the best params\ngrid.best_params_, grid.best_score_\n\n({'clf__C': 1.0, 'rbm__n_components': 200}, 0.91766666666666663)\n```", "```py\n# set some fake word embeddings\nking = np.array([.2, -.5, .7, .2, -.9])\nman = np.array([-.5, .2, -.2, .3, 0.])\nwoman = np.array([.7, -.3, .3, .6, .1])\n\nqueen = np.array([ 1.4, -1\\. , 1.2, 0.5, -0.8])\n```", "```py\nnp.array_equal((king - man + woman), queen)\n\nTrue\n```", "```py\n# import the gensim package\n import gensim\n```", "```py\nimport logging\n\n logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n```", "```py\nfrom gensim.models import word2vec, Word2Vec\n\nsentences = word2vec.Text8Corpus('../data/text8')\n```", "```py\n# instantiate a gensim module on the sentences from above\n # min_count allows us to ignore words that occur strictly less than this value\n # size is the dimension of words we wish to learn\n model = gensim.models.Word2Vec(sentences, min_count=1, size=20)\n\n 2017-12-29 16:43:25,133 : INFO : collecting all words and their counts\n 2017-12-29 16:43:25,136 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n 2017-12-29 16:43:31,074 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n 2017-12-29 16:43:31,075 : INFO : Loading a fresh vocabulary\n 2017-12-29 16:43:31,990 : INFO : min_count=1 retains 253854 unique words (100% of original 253854, drops 0)\n 2017-12-29 16:43:31,991 : INFO : min_count=1 leaves 17005207 word corpus (100% of original 17005207, drops 0)\n 2017-12-29 16:43:32,668 : INFO : deleting the raw counts dictionary of 253854 items\n 2017-12-29 16:43:32,676 : INFO : sample=0.001 downsamples 36 most-common words\n 2017-12-29 16:43:32,678 : INFO : downsampling leaves estimated 12819131 word corpus (75.4% of prior 17005207)\n 2017-12-29 16:43:32,679 : INFO : estimated required memory for 253854 words and 20 dimensions: 167543640 bytes\n 2017-12-29 16:43:33,431 : INFO : resetting layer weights\n 2017-12-29 16:43:36,097 : INFO : training model with 3 workers on 253854 vocabulary and 20 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n 2017-12-29 16:43:37,102 : INFO : PROGRESS: at 1.32% examples, 837067 words/s, in_qsize 5, out_qsize 0\n 2017-12-29 16:43:38,107 : INFO : PROGRESS: at 2.61% examples, 828701 words/s,\n ... 2017-12-29 16:44:53,508 : INFO : PROGRESS: at 98.21% examples, 813353 words/s, in_qsize 6, out_qsize 0 2017-12-29 16:44:54,513 : INFO : PROGRESS: at 99.58% examples, 813962 words/s, in_qsize 4, out_qsize 0\n ... 2017-12-29 16:44:54,829 : INFO : training on 85026035 raw words (64096185 effective words) took 78.7s, 814121 effective words/s\n```", "```py\n# get the vectorization of a word\n model.wv['king']\n\n array([-0.48768288, 0.66667134, 2.33743191, 2.71835423, 4.17330408, 2.30985498, 1.92848825, 1.43448424, 3.91518641, -0.01281452, 3.82612252, 0.60087812, 6.15167284, 4.70150518, -1.65476751, 4.85853577, 3.45778084, 5.02583361, -2.98040175, 2.37563372], dtype=float32)\n```", "```py\n# woman + king - man = queen\n model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)\n\n [(u'emperor', 0.8988120555877686), (u'prince', 0.87584388256073), (u'consul', 0.8575721979141235), (u'tsar', 0.8558996319770813), (u'constantine', 0.8515684604644775), (u'pope', 0.8496872782707214), (u'throne', 0.8495982885360718), (u'elector', 0.8379884362220764), (u'judah', 0.8376096487045288), (u'emperors', 0.8356839418411255)]\n```", "```py\n# London is to England as Paris is to ____\n model.wv.most_similar(positive=['Paris', 'England'], negative=['London'], topn=1)\n\n KeyError: \"word 'Paris' not in vocabulary\"\n```", "```py\n# use a pretrained vocabulary with 3,000,000 words\n import gensim\n\n model = gensim.models.KeyedVectors.load_word2vec_format('../data/GoogleNews-vectors-negative300.bin', binary=True)\n\n # 3,000,000 words in our vocab\n len(model.wv.vocab)\n\n 3000000\n```", "```py\n# woman + king - man = queen\n model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n\n [(u'queen', 0.7118192911148071)]\n\n # London is to England as Paris is to ____\n model.wv.most_similar(positive=['Paris', 'England'], negative=['London'], topn=1)\n\n [(u'France', 0.6676377654075623)]\n```", "```py\n# Pick out the oddball word in a sentence\nmodel.wv.doesnt_match(\"duck bear cat tree\".split())\n\n'tree'\n```", "```py\n# grab a similarity score between 0 and 1\n\n# the similarity between the words woman and man, pretty similar\nmodel.wv.similarity('woman', 'man')\n0.766401223\n\n# similarity between the words tree and man, not very similar\nmodel.wv.similarity('tree', 'man')\n0.229374587\n```", "```py\n# helper function to try to grab embeddings for a word and returns None if that word is not found\ndef get_embedding(string):\n    try:\n        return model.wv[string]\n    except:\n        return None\n```", "```py\n# very original article titles\n sentences = [\n \"this is about a dog\",\n \"this is about a cat\",\n \"this is about nothing\"\n]\n```", "```py\n# Zero matrix of shape (3,300)\nvectorized_sentences = np.zeros((len(sentences),300))\n# for every sentence\nfor i, sentence in enumerate(sentences):\n    # tokenize sentence into words\n    words = sentence.split(' ')\n    # embed whichever words that we can\n    embedded_words = [get_embedding(w) for w in words]\n    embedded_words = filter(lambda x:x is not None, embedded_words)\n    # Take a mean of the vectors to get an estimate vectorization of the sentence\n    vectorized_sentence = reduce(lambda x,y:x+y, embedded_words)/len(embedded_words)\n    # change the ith row (in place) to be the ith vectorization\n    vectorized_sentences[i:] = vectorized_sentence\n\nvectorized_sentences.shape\n(3, 300)\n```", "```py\n# we want articles most similar to the reference word \"dog\"\nreference_word = 'dog'\n\n# take a dot product between the embedding of dof and our vectorized matrix\nbest_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-1]\n\n# output the most relevant sentence\nsentences[best_sentence_idx]\n\n'this is about a dog'\n```", "```py\nreference_word = 'cat'\nbest_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-1]\n\nsentences[best_sentence_idx]\n\n'this is about a cat'\n```", "```py\nreference_word = 'canine'\nbest_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-1]\n\nprint sentences[best_sentence_idx]\n\n'this is about a dog'\n\nreference_word = 'tiger'\nbest_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-1]\n\nprint sentences[best_sentence_idx]\n\n'this is about a cat'\n```", "```py\n# Chapter titles from Sinan's first book, \"The Principles of Data Science\n\n sentences = \"\"\"How to Sound Like a Data Scientist\n Types of Data\n The Five Steps of Data Science\n Basic Mathematics\n A Gentle Introduction to Probability\n Advanced Probability\n Basic Statistics\n Advanced Statistics\n Communicating Data\n Machine Learning Essentials\n Beyond the Essentials\n Case Studies \"\"\".split('\\n')\n```", "```py\n# Zero matrix of shape (3,300)\nvectorized_sentences = np.zeros((len(sentences),300))\n# for every sentence\nfor i, sentence in enumerate(sentences):\n    # tokenize sentence into words\n    words = sentence.split(' ')\n    # embed whichever words that we can\n    embedded_words = [get_embedding(w) for w in words]\n    embedded_words = filter(lambda x:x is not None, embedded_words)\n    # Take a mean of the vectors to get an estimate vectorization of the sentence\n    vectorized_sentence = reduce(lambda x,y:x+y, embedded_words)/len(embedded_words)\n    # change the ith row (in place) to be the ith vectorization\n    vectorized_sentences[i:] = vectorized_sentence\n\nvectorized_sentences.shape\n(12, 300)\n```", "```py\n# find chapters about math\nreference_word = 'math'\nbest_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-3:][::-1]\n\n[sentences[b] for b in best_sentence_idx]\n\n['Basic Mathematics', 'Basic Statistics', 'Advanced Probability ']\n```", "```py\n# which chapters are about giving talks about data\nreference_word = 'talk'\nbest_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-3:][::-1]\n\n[sentences[b] for b in best_sentence_idx]\n\n['Communicating Data ', 'How to Sound Like a Data Scientist', 'Case Studies ']\n```", "```py\n# which chapters are about AI\nreference_word = 'AI'\nbest_sentence_idx = np.dot(vectorized_sentences, get_embedding(reference_word)).argsort()[-3:][::-1]\n\n[sentences[b] for b in best_sentence_idx]\n\n['Advanced Probability ', 'Advanced Statistics', 'Machine Learning Essentials']\n```"]