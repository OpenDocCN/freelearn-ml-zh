- en: Music Genre Recommendation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音乐流派推荐
- en: In this chapter, we are going to go back to supervised learning. We have built
    numerous supervised learning algorithms for both classification and regression
    problems using learning algorithms such as logistic regression, Naive Bayes, random
    forest, and **Support Vector Machine** (**SVM**). However, the number of outputs
    from these models we have built has always been one. In our Twitter sentiment
    analysis project, the output could only be one of positive, negative, or neutral.
    On the other hand, in our housing price prediction project, the output was a log
    of house prices predicted. Unlike our previous projects, there are cases where
    we want our **machine learning** (**ML**) models to output multiple values. A
    recommendation system is one example of where we need ML models that can produce
    rank-ordered predictions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回到监督学习。我们已经使用逻辑回归、朴素贝叶斯、随机森林和**支持向量机**（**SVM**）等学习算法为分类和回归问题构建了大量的监督学习算法。然而，我们构建的这些模型输出的数量始终是单一的。在我们的Twitter情感分析项目中，输出只能是积极、消极或中性之一。另一方面，在我们的房价预测项目中，输出是预测的房价的对数。与我们的先前项目不同，有些情况下我们希望我们的**机器学习**（**ML**）模型能够输出多个值。推荐系统就是需要能够产生排序预测的ML模型的一个例子。
- en: In this chapter, we are going to use a dataset that contains various audio features,
    compiled from numerous music recordings. With this data, we are going to explore
    how the values of audio features, such as kurtosis and skewness of the sound spectrum,
    are distributed across different genres of songs. Then, we are going to start
    building multiple ML models that output the predicted probabilities of the given
    song belonging to each music genre, instead of producing just one prediction output
    of the most likely genre for a given song. Once we have these models built, we
    are going to take it a step further and ensemble the prediction results of these
    base models to build a meta model for the final recommendations of song music
    genres. We are going to use a different model validation metric, **Mean Reciprocal
    Rank** (**MRR**), to evaluate our ranking models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一个包含各种音频特征的数据库，这些特征是从众多音乐录音中编译而来的。利用这些数据，我们将探讨音频特征值，如声音谱的峰度和偏度，在不同歌曲流派中的分布情况。然后，我们将开始构建多个ML模型，这些模型将输出给定歌曲属于每个音乐流派预测的概率，而不是仅输出给定歌曲最可能流派的一个预测输出。一旦我们构建了这些模型，我们将进一步将这些基础模型的预测结果进行集成，以构建一个用于最终歌曲音乐流派推荐的元模型。我们将使用不同的模型验证指标，**平均倒数排名**（**MRR**），来评估我们的排序模型。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Problem definition for the Music Genre Recommendation project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音乐流派推荐项目的问题定义
- en: Data analysis for the audio features dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音频特征数据集的数据分析
- en: ML models for music genre classification
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 音乐流派分类的机器学习模型
- en: Ensembling base learning models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成基础学习模型
- en: Evaluating recommendation/rank-ordering models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估推荐/排序模型
- en: Problem definition
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题定义
- en: Let's get into greater detail and properly define what problems we are going
    to solve and what machine learning models we are going to build for this project.
    Music streaming services, such as Pandora and Spotify, require music recommendation
    systems, with which they can recommend and play songs that their listeners might
    like. There is more than one way to build a music recommendation system. One way
    is to look at what other similar users listened to, and the way to define similar
    users is to look at the history of songs that they listened to. However, this
    approach will not work well if the user is new to the platform and/or if we do
    not have enough of a history of songs he or she listened to. In this case, we
    cannot rely on the historical data. Instead, it will be better to use the attributes
    of the songs that the user is currently listening to recommend other music. One
    song attribute that can play an important role in music recommendation is the
    genre. It is highly likely that a user who is currently listening to music on
    the platform will like to continue listening to the same or similar music. Imagine
    you were listening to instrumental music and the music streaming application then
    suddenly played rock music. It would not be a smooth transition and it would not
    be a good user experience, as you most likely would have wanted to continue listening
    to instrumental music. By correctly identifying the genre of the songs and recommending
    the right song type to play, you can avoid disturbing the user experience of your
    music streaming service.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地探讨，并正确定义我们将要解决的问题以及为这个项目构建的机器学习模型。音乐流媒体服务，如Pandora和Spotify，需要音乐推荐系统，这样他们就可以推荐和播放听众可能喜欢的歌曲。构建音乐推荐系统的方式不止一种。一种方式是查看其他类似用户听过的歌曲，而定义类似用户的方法是查看他们听过的歌曲历史。然而，如果用户是平台的新用户，或者我们没有足够的历史歌曲数据，这种方法可能不会很好地工作。在这种情况下，我们不能依赖于历史数据。相反，使用用户当前正在听的歌曲的属性来推荐其他音乐会更好。一首歌曲的属性在音乐推荐中可以发挥重要作用的是音乐类型。一个用户当前在平台上听音乐时，很可能喜欢继续听相同或相似的音乐。想象一下，你正在听器乐音乐，而音乐流媒体应用突然播放了摇滚音乐。这不会是一个顺畅的过渡，也不会是一个好的用户体验，因为你很可能想继续听器乐音乐。通过正确识别歌曲的类型并推荐正确的歌曲类型来播放，你可以避免打扰你的音乐流媒体服务的用户体验。
- en: 'In order to build a music genre recommendation model, we are going to use **FMA:
    A Dataset For Music Analysis**, which contains a large amount of data for over
    100,000 tracks. The dataset contains information about the album, title, audio
    attributes, and so forth, and the full dataset can be found and downloaded from
    this link: [https://github.com/mdeff/fma](https://github.com/mdeff/fma). With
    this data, we are going to sub-select the features that are of interest and build
    numerous ML models that output the probability of each song belonging to different
    music genres. Then, we are going to rank-order the genres by probability. We will
    be experimenting with various learning algorithms, such as logistic regression,
    Naive Bayes, and SVM. We are going to take it a step further by using the ensembling
    technique to take the output of these models as an input to another ML model that
    produces the final prediction and recommendation output. We are going to use MRR as
    the metric to evaluate our music genre recommendation models.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建音乐类型推荐模型，我们将使用**FMA：音乐分析数据集**，它包含超过10万首歌曲的大量数据。该数据集包含关于专辑、标题、音频属性等信息，完整的数据集可以通过此链接找到并下载：[https://github.com/mdeff/fma](https://github.com/mdeff/fma)。有了这些数据，我们将选择感兴趣的特征，并构建多个输出每首歌曲属于不同音乐类型概率的机器学习模型。然后，我们将根据概率对音乐类型进行排序。我们将尝试各种学习算法，如逻辑回归、朴素贝叶斯和SVM。我们将进一步使用集成技术，将这些模型的输出作为另一个机器学习模型的输入，该模型产生最终的预测和推荐输出。我们将使用MRR作为评估我们的音乐类型推荐模型的指标。
- en: 'To summarize our problem definition for the music genre recommendation project:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们对音乐类型推荐项目的定义问题：
- en: What is the problem? We need a recommendation model that rank-orders music genres
    by how likely it is that a song belongs to each genre, so that we can properly
    identify the genre of a song and recommend what song to play next.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 问题是怎样的？我们需要一个推荐模型，该模型能够根据歌曲属于每个音乐类型的可能性进行排序，以便我们能够正确识别歌曲的类型并推荐下一首歌曲。
- en: Why is it a problem?Use of historical data for music recommendation is not reliable
    for those users who are new to the platform, as they will not have enough historical
    data for good music recommendations. In this case, we will have to use audio and
    other features to identify what music to play next. Correctly identifying and
    recommending the genre of music is the first step to figuring out what song to
    play next.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这为什么是个问题？使用历史数据为音乐推荐并不适用于那些刚接触该平台的新用户，因为他们不会有足够的历史数据来进行好的音乐推荐。在这种情况下，我们将不得不使用音频和其他特征来识别接下来要播放的音乐。正确识别和推荐音乐流派是确定接下来要播放哪首歌的第一步。
- en: What are some approaches to solving this problem? We are going to use publicly
    available music data, which not only contains information about the album, title,
    and artist of the song, but also contains information about numerous audio features.
    Then, we are going to build ML models that output the probabilities and use this
    probability output to rank-order genres for given song.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决这个问题的方法有哪些？我们将使用公开可用的音乐数据，这些数据不仅包含关于专辑、标题和艺术家信息，还包含关于众多音频特征的信息。然后，我们将构建输出概率的ML模型，并使用这个概率输出对给定歌曲的流派进行排序。
- en: What are the success criteria? We want the correct music genre to come up as
    one of the top predicted genres. We will use MRR as the metric to evaluate ranking
    models.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成功的标准是什么？我们希望正确的音乐流派能够作为预测流派中的前几个出现。我们将使用MRR作为评估排名模型的指标。
- en: Data analysis for the audio features dataset
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音频特征数据集的数据分析
- en: 'Let''s start looking into the audio features dataset. In order to focus on
    building recommendation models for music genres, we trimmed down the original
    dataset from **FMA: A Dataset For Music Analysis**. You can download this data
    from this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/sample.csv](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/sample.csv).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始研究音频特征数据集。为了专注于构建音乐流派推荐模型，我们将原始数据集**FMA：音乐分析数据集**进行了裁剪。您可以从以下链接下载此数据：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/sample.csv](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/sample.csv)。
- en: Target variable distribution
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标变量分布
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Similar to previous chapters, we used the `AggregateRowsBy` method in the Deedle
    data frame to count the number of records per genre. Then, we used the `DataBarBox`
    class to create a bar chart that shows the distribution of the target variable
    visually. As you can see from this code snippet (in line 10), we are using the
    first three letters of each genre as a label for each genre in the bar chart.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，我们在Deedle数据框中使用了`AggregateRowsBy`方法来计算每个流派记录的数量。然后，我们使用了`DataBarBox`类来创建一个条形图，直观地显示了目标变量的分布。如您从这段代码片段（第10行）中看到的那样，我们正在使用每个流派名称的前三个字母作为条形图中每个流派标签。
- en: 'When you run this code, you will see the following output for the distribution
    of the target variable:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行此代码时，您将看到以下输出，显示了目标变量的分布：
- en: '![](img/00097.gif)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00097.gif)'
- en: 'The following plot shows the bar chart for the distribution of the target variable:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了目标变量的条形图分布：
- en: '![](img/00098.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00098.jpeg)'
- en: As you can see from this chart, we have the largest number for Instrumental (**Ins**)
    music in our sample set and Electronic (**Ele**) and Rock (**Roc**) follow as
    the second and third. Although this sample set contains some songs in certain
    genres more so than others, this is a relatively well balanced set, where one
    or two genres do not take up the majority of the sample records. Now, let's look
    at the distributions of some of our features.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这张图表中可以看到，在我们的样本集中，**Instrumental**（**Ins**）音乐的数字最大，其次是**Electronic**（**Ele**）和**Rock**（**Roc**），分别位列第二和第三。尽管这个样本集中某些流派的歌曲比其他流派多，但这仍然是一个相对均衡的集合，其中没有任何一个流派占据样本记录的大多数。现在，让我们来看看我们一些特征的分部情况。
- en: Audio features – MFCC
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音频特征 – MFCC
- en: 'For this project, we are going to focus on a subset of features that the full
    dataset has. We are going to use **Mel Frequency Cepstral Coefficients** (**MFCCs**)
    and their statistical distributions as the features to our ML models. Simply put,
    **MFCC** is a representation of the sound spectrum and we will use its statistical
    distributions, kurtosis, skewness, min, max, mean, median, and standard deviation.
    If you look at the sample set you have downloaded from the previous step, you
    will see the columns are named according to the corresponding statistical distribution.
    We are going to first look at the distributions of each of these features. The
    following code snippet shows how we computed the quartiles for each feature:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们将关注完整数据集具有的子集特征。我们将使用 **梅尔频率倒谱系数**（**MFCCs**）及其统计分布作为我们机器学习模型的特征。简单来说，**MFCC**
    是声音频谱的表示，我们将使用其统计分布、峰度、偏度、最小值、最大值、平均值、中位数和标准差。如果您查看从上一步下载的样本集，您将看到列名是根据相应的统计分布命名的。我们将首先查看这些特征的分布。以下代码片段显示了我们是如何计算每个特征的四分位数的：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Similar to previous chapters, we are using the `Quantiles` method in the `Accord.Statistics.Measures`
    class to compute quartiles, which are the three numbers that separate the values
    into four subsets—the middle number between the min and median (25^(th) percentile),
    median (50^(th) percentile), and the middle number between the median and max
    (75^(th) percentile). As you can see in line 6 of this code snippet, we are only
    showing the first four coefficients' statistical distributions. For your further
    experiments, you can look at the distributions of all the MFCC features, not limited
    to only these four. Let's quickly take a look at just a couple of the distributions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，我们在 `Accord.Statistics.Measures` 类中使用 `Quantiles` 方法来计算四分位数，这三个数将值分为四个子集——最小值和中位数之间的中间数（25^(th)
    百分位数），中位数（50^(th) 百分位数），以及中位数和最大值之间的中间数（75^(th) 百分位数）。如您在代码片段的第 6 行中看到的那样，我们只显示了前四个系数的统计分布。对于您进一步的实验，您可以查看所有
    MFCC 特征的分布，而不仅限于这四个。让我们快速看一下其中的一些分布。
- en: 'The distribution for the kurtosis of the first four coefficients looks like
    the following:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个系数峰度的分布看起来如下：
- en: '![](img/00099.gif)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00099.gif)'
- en: 'As you can see from this output, the majority of the kurtosis values fall between
    -2 and 5, but there are cases where the kurtosis can take large values. Let''s
    now look at the skewness distributions for the first four coefficients:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从输出结果中可以看到，大多数峰度值介于 -2 和 5 之间，但也有一些情况峰度可以取较大的值。现在让我们看看前四个系数的偏度分布：
- en: '![](img/00100.gif)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00100.gif)'
- en: 'Skewness varies between narrower ranges. Typically, the skewness values seem
    to fall between -15 and 5\. Lastly, let''s look at the distributions of the mean
    of the first four coefficients:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 偏度变化范围较窄。通常，偏度值似乎介于 -15 和 5 之间。最后，让我们看看前四个系数平均值的分布：
- en: '![](img/00101.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00101.jpeg)'
- en: As you can see from this output, the mean values seem to vary and have wide
    ranges. It can take any values between -1,000 and 300.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从输出结果中可以看到，平均值似乎有所变化，并且范围较广。它可以取介于 -1,000 和 300 之间的任何值。
- en: Now that we have a rough idea of how the audio features' distributions look,
    let's see if we can find any discrepancies in the feature distributions among
    different genres. We are going to plot a scatter plot where the *x* axis is the
    index of each feature and the *y* axis is the values for the given feature. Let's
    look at these plots first, as it will be easier to understand with some visuals.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对音频特征分布有了大致的了解，让我们看看是否能在不同流派的特征分布中找到任何差异。我们将绘制一个散点图，其中 *x* 轴是每个特征的索引，*y*
    轴是给定特征的值。让我们先看看这些图表，因为有了视觉辅助将更容易理解。
- en: 'The following plots show the distributions of kurtosis for four different genres:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了四个不同流派峰度的分布：
- en: '![](img/00102.jpeg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00102.jpeg)'
- en: As briefly mentioned previously, the *x* axis refers to the index of each feature.
    Since we have 20 individual features for kurtosis of MFCCs, the x-values span
    from 1 to 20\. On the other hand, the *y* axis shows the distributions of the
    given feature. As you can see from this chart, there are some differences in the
    feature distributions among different genres, which will help our ML models to
    learn how to correctly predict the genre of a given song.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，*x* 轴指的是每个特征的索引。由于我们有 20 个 MFCC 峰度的独立特征，x 值的范围从 1 到 20。另一方面，*y* 轴显示了给定特征的分布。如您从这张图表中看到的那样，不同流派之间特征分布存在一些差异，这将有助于我们的机器学习模型学习如何正确预测给定歌曲的流派。
- en: 'The following plots show the distributions of skewness for four different genres:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了四种不同流派偏度的分布：
- en: '![](img/00103.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00103.jpeg)'
- en: 'Lastly, the following plots show the mean distributions for four different
    genres:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以下图表显示了四种不同流派的平均分布：
- en: '![](img/00104.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00104.jpeg)'
- en: The distributions of the mean values for each feature seem more similar among
    different genres, when compared to the kurtosis and skewness.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 与峰度和偏度相比，不同流派中每个特征的均值分布似乎更相似。
- en: 'In order to create these charts, we have used the `ScatterplotBox` class. The
    following code shows how we created the previous charts:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建这些图表，我们使用了`ScatterplotBox`类。以下代码展示了我们如何创建之前的图表：
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see from this code, we start iterating through different statistical
    distributions (`kurtosis`, `min`, `max`, and so on) from line 2 and, for each
    of those statistical distributions, we sub-select the columns that we are interested
    in from `featuresDF` in line 7\. Then, we wrote and used a helper function that
    builds an array of x-y pairs for the scatter plot and display it using the `Show`
    method of the `ScatterplotBox` class.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这段代码中看到的那样，我们从第 2 行开始迭代不同的统计分布（`峰度`、`最小值`、`最大值`等），在第 7 行中从`featuresDF`中子选择我们感兴趣的列。然后，我们编写并使用了一个辅助函数来构建散点图的
    x-y 对数组，并使用`ScatterplotBox`类的`Show`方法显示它。
- en: 'The code for the helper function that builds x-y pairs for scatter plots is
    as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 构建散点图 x-y 对的辅助函数的代码如下：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As you can see from this code, this method takes the index of the feature as
    an x value and takes the value of the feature as a y value.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这段代码中看到的，该方法将特征的索引作为 x 值，将特征的值作为 y 值。
- en: The full code for this data analysis step can be found at this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/DataAnalyzer.cs).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据分析步骤的完整代码可以在以下链接找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/DataAnalyzer.cs)。
- en: ML models for music genre classification
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 音乐流派分类的机器学习模型
- en: We will now start building ML models for music genre classification. In this
    project, the output of our ML models will take a slightly different form. Unlike
    other supervised learning models that we have built, we want our models to output
    the likelihoods or probabilities for each genre for a given song. So, instead
    of the model output being one value, we would like our models to output eight
    values, where each value will represent the probability of the given song belonging
    to each of the eight genres—electronic, experimental, folk, hip-hop, instrumental,
    international, pop, and rock. In order to achieve this, we will be using the `Probabilities`
    method from each of the model classes, on top of the `Decide` method that we have
    been using so far.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将开始构建音乐流派分类的机器学习模型。在这个项目中，我们机器学习模型的输出将采取稍微不同的形式。与其他我们已经构建的监督学习模型不同，我们希望我们的模型为给定歌曲的每个流派输出可能性或概率。因此，我们的模型输出将不是单个值，而是八个值，其中每个值将代表给定歌曲属于八个流派（电子、实验、民谣、嘻哈、乐器、国际、流行和摇滚）之一的概率。为了实现这一点，我们将在我们迄今为止一直在使用的`Decide`方法之上使用每个模型类中的`Probabilities`方法。
- en: Logistic regression
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'The first model we are going to experiment with is logistic regression. The
    following code shows how we built a logistic regression classifier with an 80/20
    split for training and testing sets:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要实验的第一个模型是逻辑回归。以下代码展示了我们如何构建一个用于训练和测试集 80/20 分割的逻辑回归分类器：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you should be familiar with it already, we used `SplitSetValidation` to split
    our sample set into train and test sets. We are using 80% of our sample set for
    training and the other 20% for testing and evaluating our models. We are using `MultinomialLogisticRegression`
    as our model for the multi-class classifier and `MultinomialLogisticLearning`
    with `GradientDescent` as our learning algorithm. Similar to the previous chapters,
    we are using `ZeroOneLoss` for our `Loss` function for the classifier.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如您应该已经熟悉的那样，我们使用`SplitSetValidation`将我们的样本集分为训练集和测试集。我们使用样本集的80%进行训练，其余的20%用于测试和评估我们的模型。我们使用`MultinomialLogisticRegression`作为多类分类器的模型，使用带有`GradientDescent`的`MultinomialLogisticLearning`作为学习算法。与前面的章节类似，我们使用`ZeroOneLoss`作为分类器的`Loss`函数。
- en: As you can see at the base of this code, we are storing the trained logistic
    regression classifier model into a separate variable, `logitTrainedModel`, and
    also the indexes of the train and test sets for use in training and testing other
    learning algorithms. We do this so that we can do head-to-head comparisons of
    model performance among different ML models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在代码底部所看到的，我们将训练好的逻辑回归分类器模型存储到单独的变量`logitTrainedModel`中，同时也存储了训练和测试集的索引，以便在训练和测试其他学习算法时使用。我们这样做是为了能够在不同的ML模型之间进行模型性能的面对面比较。
- en: 'The code to do in-sample and out-of-sample predictions using this trained logistic
    regression model is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此训练好的逻辑回归模型进行样本内和样本外预测的代码如下：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As briefly mentioned before, we are using the `Probabilities` method from the `MultinomialLogisticRegression`
    model, which outputs an array of probabilities, and each index represents the
    probability of the given song being the corresponding music genre. The following
    code shows how we encoded each of the genres:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们使用`MultinomialLogisticRegression`模型的`Probabilities`方法，它输出一个概率数组，每个索引代表给定歌曲属于相应音乐类型的概率。以下代码展示了我们如何编码每个类型：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let's try training another ML model using the same indexes for train and test
    sets that we used for the logistic regression model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用与逻辑回归模型相同的训练和测试集索引来训练另一个ML模型。
- en: SVM with the Gaussian kernel
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带高斯核的SVM
- en: 'Using the following code, you can train a multi-class SVM model:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，您可以训练一个多类SVM模型：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see from this code, there is one minor difference between the SVM
    model that we built previously. We are using `MulticlassSupportVectorLearning` instead
    of `LinearRegressionNewtonMethod` or `FanChenLinSupportVectorRegression`, which
    we used in [Chapter 5](part0056.html#1LCVG0-5ebdf09927b7492888e31e8436526470),
    *Fair Value of House and Property*. This is because we now have a multi-class
    classification problem and need to use a different learning algorithm for such
    SVM models. As we discussed in another chapter previously, the hyper-parameters,
    such as `Epsilon`, `Tolerance`, and `Complexity`, can be tuned and you should
    experiment with other values for better-performing models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这段代码中可以看到，与我们之前构建的SVM模型相比，只有一个细微的差别。我们使用`MulticlassSupportVectorLearning`而不是之前在[第五章](part0056.html#1LCVG0-5ebdf09927b7492888e31e8436526470)“房屋和财产的公允价值”中使用的`LinearRegressionNewtonMethod`或`FanChenLinSupportVectorRegression`。这是因为我们现在有一个多类分类问题，需要为这样的SVM模型使用不同的学习算法。正如我们在另一章中讨论过的，超参数，如`Epsilon`、`Tolerance`和`Complexity`，是可以调整的，您应该尝试其他值以获得性能更好的模型。
- en: One thing to note here is that when we are training our SVM model, we use the
    same train set that we used for building the logistic regression model. As you
    can see at the base of the code, we sub-select records with the same indexes in
    the train set that we used previously for the logistic regression model. This
    is to make sure that we can correctly do a head-to-head comparison of the performance
    of this SVM model against that of the logistic regression model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一点是，当我们训练SVM模型时，我们使用与构建逻辑回归模型相同的训练集。如您在代码底部所看到的，我们选择了与之前用于逻辑回归模型的训练集相同索引的记录。这是为了确保我们可以正确地进行SVM模型与逻辑回归模型性能的面对面比较。
- en: 'Similar to the case of the previous logistic regression model, we are using
    the following code for in-sample and out-of-sample predictions, using the trained
    SVM model:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的逻辑回归模型类似，我们使用以下代码进行样本内和样本外预测，使用训练好的SVM模型：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `MulticlassSupportVectorMachine` class also provides the `Probabilities`
    method, with which we can get the likelihoods of a song belonging to each of the
    eight genres. We store these probability outputs into separate variables, `svmTrainProbabilities`
    and `svmTestProbabilities`, for our future model evaluation and for ensembling
    the models.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`MulticlassSupportVectorMachine` 类还提供了 `Probabilities` 方法，通过这个方法我们可以得到一首歌曲属于八个流派中的每一个流派的可能性。我们将这些概率输出存储到单独的变量中，`svmTrainProbabilities`
    和 `svmTestProbabilities`，用于我们未来的模型评估和模型集成。'
- en: Naive Bayes
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: 'We are going to build one more machine learning model for music genre classification.
    We are going to train a Naive Bayes classifier. The following code shows how you
    can build a Naive Bayes classifier for input with continuous values:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个用于音乐流派分类的更多机器学习模型。我们将训练一个朴素贝叶斯分类器。以下代码展示了如何为具有连续值的输入构建朴素贝叶斯分类器：
- en: '[PRE10]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see from this code, we are using `NormalDistribution` as a distribution
    for `NaiveBayesLearning`. Unlike in the previous chapters, where we had word counts
    as features of our Naive Bayes classifiers, we have continuous values for our
    audio features. In this case, we need to build a Gaussian Naive Bayes classifier. Similar
    to when we were building an SVM model, we are training our Naive Bayes classifier
    with the same train set that we used for the logistic regression model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从这段代码中可以看出，我们正在使用 `NormalDistribution` 作为 `NaiveBayesLearning` 的分布。与之前章节不同，那时我们的朴素贝叶斯分类器的特征是词频，我们现在有音频特征的连续值。在这种情况下，我们需要构建一个高斯朴素贝叶斯分类器。与构建
    SVM 模型时类似，我们使用与逻辑回归模型相同的训练集来训练朴素贝叶斯分类器。
- en: 'The following code shows how we can get the probability output for in-sample
    and out-of-sample predictions using the trained Naive Bayes classifier:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用训练好的朴素贝叶斯分类器获取样本内和样本外的预测概率输出：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Similar to the `MulticlassSupportVectorMachine` and `MultinomialLogisticRegression`
    classes, the `NaiveBayes` model also provides the `Probabilities` method.  As
    you can see from the code, we store the predicted probabilities for both in-sample
    and out-of-sample records into two separate variables, `nbTrainProbabilities`
    and `nbTestProbabilities`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `MulticlassSupportVectorMachine` 和 `MultinomialLogisticRegression` 类类似，`NaiveBayes`
    模型也提供了 `Probabilities` 方法。从代码中可以看出，我们将样本内和样本外的预测概率分别存储到两个单独的变量中，`nbTrainProbabilities`
    和 `nbTestProbabilities`。
- en: In the following section, we will take a look at how we can combine and ensemble
    these models we have built so far. The full code for building ML models can be
    found at this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/Modeling.cs).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何将我们迄今为止构建的这些模型进行组合和集成。构建机器学习模型的完整代码可以在以下链接找到：[https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.7/Modeling.cs)。
- en: Ensembling base learning models
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将基础学习模型进行集成
- en: Ensemble learning is where you combine trained models together in order to improve
    their predictive power. The random forest classifier that we built in previous
    chapters is an example of ensemble learning. It builds a forest of decision trees,
    where the individual trees are trained with a portion of the samples and features
    in the sample set. This method of ensemble learning is called **bagging**. The
    ensemble method that we are going to use in this chapter is **stacking**. Stacking
    is when you build a new ML model using the outputs of the other models, which
    are called **base learning models**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是将训练好的模型结合起来以提高其预测能力的方法。我们在之前章节中构建的随机森林分类器就是集成学习的一个例子。它构建了一个决策树森林，其中每个树都是使用样本集的一部分样本和特征进行训练的。这种集成学习方法被称为
    **bagging**。本章我们将使用的集成方法是 **stacking**。Stacking 是指使用其他模型的输出构建一个新的机器学习模型，这些模型被称为
    **基础学习模型**。
- en: 'In this project, we are going to built a new Naive Bayes classifier model on
    top of the predicted probability output from those logistic regression, SVM, and
    Naive Bayes models that we built in the previous section. The first thing we need
    to do to build a new model with the probability output of the base models is to
    build the training input. The following code shows how we combined all the outputs
    from the base models:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将在之前章节中构建的逻辑回归、SVM 和 Naive Bayes 模型的预测概率输出之上构建一个新的朴素贝叶斯分类器模型。构建新模型的第一步是构建训练输入。以下代码展示了我们如何将基础模型的全部输出组合起来：
- en: '[PRE12]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As you can see from this code, we are concatenating the predicted probabilities
    from all three models that we built so far. Using this probability output data
    as input, we are going to build a new meta-model, using the Naive Bayes learning
    algorithm. The following code is how we trained this meta-model:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如此代码所示，我们正在将迄今为止构建的三个模型的预测概率进行拼接。使用这些概率输出数据作为输入，我们将构建一个新的元模型，使用朴素贝叶斯学习算法。以下是我们训练此元模型的代码：
- en: '[PRE13]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'From this code, you can see that we are still using `NormalDistribution`, as
    the input is a set of continuous values. Then, we train this new Naive Bayes classifier
    with the combined probability output of the base learning models that we trained
    before. Similar to the previous steps, we get the prediction output from this
    meta-model by using the `Probabilities` method and store these results into separate
    variables. The code to get the prediction output for the train and test sets using
    this new meta-model is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从此代码中，你可以看到我们仍在使用`NormalDistribution`，因为输入是一组连续值。然后，我们使用之前训练的基础学习模型的组合概率输出来训练这个新的朴素贝叶斯分类器。类似于之前的步骤，我们通过使用`Probabilities`方法从元模型获取预测输出，并将这些结果存储到单独的变量中。使用此新元模型获取训练集和测试集预测输出的代码如下：
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Now that we have all the models built, let's start looking at the performances
    of these models. In the following section, we will evaluate the performance of
    base models as well as the meta-model we just built.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了所有模型，让我们开始查看这些模型的性能。在接下来的部分，我们将评估基础模型以及我们刚刚构建的元模型的性能。
- en: Evaluating recommendation/rank-ordering models
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估推荐/排序模型
- en: Evaluating recommendation models that rank-order the outcomes is quite different
    from evaluating classification models. Aside from whether the model prediction
    is right or wrong, we also care about in which rank the correct outcome comes
    in the recommendation models. In other words, a model that predicted the correct
    outcome to be the second from the top is a better model than a model that predicted
    the correct outcome to be fourth or fifth from the top. For example, when you
    search for something on a search engine, getting the most appropriate document
    on the top of the first page is great, but it is still OK to have that document
    as the second or third link on the first page, as long as it does not appear at
    the bottom of the first page or the next page. We are going to discuss some ways
    to evaluate such recommendation and ranking models in the following sections.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 评估对结果进行排序的推荐模型与评估分类模型有很大不同。除了模型预测是否正确之外，我们还关心正确结果在推荐模型中的排名。换句话说，一个预测正确结果为第二名的模型比预测为第四或第五名的模型要好。例如，当你在一个搜索引擎上搜索某物时，在第一页顶部获得最合适的文档是很好的，但即使该文档作为第一页或第二页上的第二个或第三个链接出现，也是可以接受的，只要它不会出现在第一页或第二页的底部。在接下来的几节中，我们将讨论一些评估此类推荐和排序模型的方法。
- en: Prediction accuracy
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测准确率
- en: 'The first and the simplest metric to look at is accuracy. For the first logistic
    regression model we built, we can use the following code to get the accuracy:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要查看的最简单指标是准确率。对于我们所构建的第一个逻辑回归模型，我们可以使用以下代码来获取准确率：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'For the following models, SVM and Naive Bayes classifiers, we can use the following
    code to compute the accuracy for the train and test set predictions:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下模型，SVM和朴素贝叶斯分类器，我们可以使用以下代码来计算训练集和测试集预测的准确率：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We used the `SplitSetValidation` class for the first logistic regression model,
    so it computes the accuracy while the model is being fit. However, for the subsequent
    models, we trained SVM and Naive Bayes models individually, so we need to use
    the `ZeroOneLoss` class to compute accuracies.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一个逻辑回归模型中使用了`SplitSetValidation`类，因此在模型拟合的同时计算准确率。然而，对于后续的模型，我们分别训练了SVM和朴素贝叶斯模型，因此我们需要使用`ZeroOneLoss`类来计算准确率。
- en: 'When you run this code, you will see the accuracy output for the logistic regression
    model as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行此代码时，你会看到逻辑回归模型的准确率输出如下所示：
- en: '![](img/00105.gif)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00105.gif)'
- en: 'For the Naive Bayes model, the accuracy results look as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于朴素贝叶斯模型，准确率结果如下所示：
- en: '![](img/00106.gif)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00106.gif)'
- en: 'And for the SVM model, the output looks as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于SVM模型，输出如下所示：
- en: '![](img/00107.gif)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00107.gif)'
- en: 'Lastly, the accuracy results for the meta-model look as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，元模型的准确率结果如下所示：
- en: '![](img/00108.gif)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00108.gif)'
- en: From these results, we can see that the Naive Bayes classifier performed the
    best by predicting the correct genre for about 42% of the time. The logistic regression
    model comes in as the second best model with the second highest accuracy and the
    SVM model comes in as the worst model in terms of prediction accuracy. Interestingly,
    the meta-model that we built with the output from the other three models did not
    perform so well. It did better than the SVM model, but performed worse than the
    Naive Bayes and logistic regression classifiers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrices
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next thing we are going to look at is confusion matrices. In the case of
    binary classification in [Chapter 2](part0028.html#QMFO0-5ebdf09927b7492888e31e8436526470), *Spam
    Email Filtering*, we explored a case where the confusion matrix was a 2 x 2 matrix.
    However, in this project, our models have `8` outcomes and the shape of the confusion
    matrix will be 8 x 8\. Let''s first look at how we can build such a confusion
    matrix:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The code for the helper function, `BuildConfusionMatrix`, looks as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once you run this code, you are going to get an 8 x 8 matrix, where the rows
    are the actual and observed genres and the columns are the predicted genres from
    the models. The following is the confusion matrix for our logistic regression
    model:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00109.gif)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: 'The numbers in bold represent the number of records that the model predicted
    correctly. For example, this logistic regression model predicted **79** songs
    correctly as **Electronic** and **33** songs were predicted as **Electronic**,
    where they were actually **Experimental**. One thing noticeable here is that this
    logistic regression model did not do so well for predicting Pop songs. It only
    had one prediction for Pop, but that prediction was wrong and the song was actually
    a **Hip-Hop** song. Let''s now look at the confusion matrix of the Naive Bayes
    classifier''s predictions:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00110.gif)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: As expected from the accuracy results, the confusion matrix looks better than
    that for logistic regression. A higher proportion of predictions in each category
    were right, when compared to the logistic regression classifier. The Naive Bayes
    classifier seemed to do much better for **Pop** songs as well.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the confusion matrix for the SVM classifier:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.gif)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: 'As expected, the prediction results are not good. The SVM model predicted 100%
    of the records as **Electronic**. Lastly, let''s look at how the meta-model did:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00112.gif)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: This confusion matrix looks slightly better than that of the SVM model. However,
    the majority of the predictions were either **Instrumental** or **International**
    and only a handful of records were predicted as other genres.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the confusion matrix is a good way to check misclassifications by
    models and find out the weaknesses and strengths of the models. These results
    are well aligned with the accuracy results, where the Naive Bayes classifier outperformed
    all the other models and the meta-model did not do so well, although it is not
    the worst among the four models that we have built.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Mean Reciprocal Rank
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next evaluation metric we are going to look at is MRR. MRR can be used
    where a model produces a list of outcomes and it measures the overall quality
    of the rankings. Let''s first look at the equation:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00113.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'As you can see, it is an average of the sum of the inverse of the ranks. Consider
    the following example:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00114.gif)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: In the first example, the correct genre was the second in rank, so the reciprocal
    rank is **1/2**.  The second example's correct genre was the first in rank, so
    the reciprocal rank is **1/1**, which is **1**. Following this process, we can
    get the reciprocal ranks for all the records and the final MRR value is simply
    the average of those reciprocal ranks. This tells us the general quality of the
    rankings. In this example, the **MRR** is **0.57**, which is above 1/2\. So, this
    MRR number suggests that, on average, the correct genres come up within the top
    two predicted genres by the model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to compute the MRR for our models, we first need to transform the
    probability output into rankings and then compute the MRR from this transformed
    model output. The following code snippet shows how we computed the MRR for our
    models:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This code uses two helper functions, `GetPredictionRanks` and `ComputeMeanReciprocalRank`.
    The `GetPredictionRanks` method transforms the probability output of a model into
    rankings and the `ComputeMeanReciprocalRank` method calculates the MRR from the
    rankings. The helper function, `GetPredictionRanks`, looks as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We are using the `Matrix.ArgSort` method from the `Accord.Math` package to rank-order
    the genres for each record. `Matrix.ArgSort` returns the indexes of the genres
    after sorting them by probability in ascending order. However, we want them to
    be sorted in descending order so that the most likely genre comes up as the first
    in rank. This is why we reverse the order of the sorted indexes using the `Reversed`
    method.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'The helper function, `ComputeMeanReciprocalRank`, looks as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This is our implementation of the equation for the MRR calculation that we discussed
    previously. This method iterates through each record and gets the rank of the
    correct genre.  Then, it reciprocates the rank, sums all of the reciprocals, and
    finally divides this sum by the number of records to get the MRR number.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start looking at the MRR scores for the models that we have built so
    far. The following output shows the MRR scores for the `Logistic Regression Classifier`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.gif)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: 'The in-sample and out-of-sample MRR scores for the Naive Bayes classifier look
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00116.gif)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: 'And the results for the SVM classifier are as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00117.gif)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, the MRR scores for the meta-model look as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00118.gif)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: From these outputs, we can see that the Naive Bayes classifier has the best
    MRR scores at around `0.61`, while the SVM classifier has the worst MRR scores
    at around `0.33`. The meta-model has MRR scores at around `0.4`. This is aligned
    with the results we have found from looking at the prediction accuracy and confusion
    matrix in the previous steps.  From these MRR scores, we can see that the correct
    genres generally fall within the top two ranks for the Naive Bayes classifier.
    On the other hand, the correct genres typically come up as the third from the
    top for the SVM classifier and within the top three for the meta-model. As you
    can see from these cases, we can understand the overall quality of the rankings
    by looking at the MRR measures.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built our first recommendation model to rank-order the likelihood
    of each of the outcomes. We started this chapter by defining the problems that
    we were going to solve and the modeling and the evaluation approaches that we
    were going to use. Then, we looked at the distributions of the variables in our
    sample set. First, we looked at how well the target variables were distributed
    among different classes or genres and noticed that it was a well-balanced sample
    set with no one genre taking up the majority of the samples in our dataset. Then,
    we looked at the distributions of the audio features. In this project, we focused
    mainly on MFCCs and their statistical distributions, such as kurtosis, skewness,
    min, and max. By looking at the quartiles and the scatter plots of these features,
    we confirmed that the feature distributions differed among the music genres.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'During our model-building step, we experimented with three learning algorithms:
    logistic regression, SVM, and Naive Bayes. Since we were building multi-class
    classification models, we had to use different learning algorithms from previous
    chapters. We learned how to use the `MultinomialLogisticRegression` and `MulticlassSupportVectorMachine`
    classes in the Accord.NET framework, as well as when to use `NormalDistribution`
    for `NaiveBayesLearning`. We then discussed how we could build a meta-model that
    ensembled the prediction results from the base learning models to improve the
    predictive power of the ML models. Lastly, we discussed how evaluating ranking
    models differed from other classification models and looked at the accuracy, confusion
    matrix, and MRR metrics to evaluate our ML models.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to use a hand-written digit image dataset
    to build a classifier that classifies each image into the corresponding digit.
    We are going to discuss some techniques to reduce the dimensions of the feature
    set and how to apply them to the image dataset. We will also discuss how to build
    a neural network in C# using the Accord.NET framework, which is the backbone of
    deep learning.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用手写数字图像数据集来构建一个分类器，该分类器将每个图像分类到相应的数字。我们将讨论一些减少特征集维度的技术以及如何将这些技术应用到图像数据集中。我们还将讨论如何使用
    Accord.NET 框架在 C# 中构建神经网络，该框架是深度学习的核心。
