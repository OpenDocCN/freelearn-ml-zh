<html><head></head><body>
		<div id="_idContainer100">
			<h1 class="chapter-number"><a id="_idTextAnchor121"/>10</h1>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor122"/>Training and Evaluating Classical Machine Learning Systems and Neural Networks</h1>
			<p>Modern machine learning frameworks are designed to be user-friendly for programmers. The popularity of the Python programming environment (and R) has shown that designing, developing, and testing machine learning models can be focused on the machine learning task and not on the programming tasks. The developers of the machine learning models can focus on developing the entire system and not on programming the internals of the algorithms. However, this bears a darker side – a lack of understanding of the internals of the models and how they are trained, evaluated, <span class="No-Break">and validated.</span></p>
			<p>In this chapter, I’ll dive a bit deeper into the process of training and evaluation. We’ll start with the basic theory behind different algorithms before learning how they are trained. We’ll start with the classical machine learning models, exemplified by decision trees. Then, we’ll gradually move toward deep learning, where we’ll explore both dense neural networks and more advanced types <span class="No-Break">of networks.</span></p>
			<p>The most important part of this chapter is understanding the difference between training/evaluating algorithms and testing/validating the entire machine learning software system. I’ll explain this by describing how machine learning algorithms are used as part of a production machine learning system (or what the entire machine learning system <span class="No-Break">looks like).</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Training and <span class="No-Break">test processes</span></li>
				<li>Training classical machine <span class="No-Break">learning models</span></li>
				<li>Training deep <span class="No-Break">learning models</span></li>
				<li>Misleading results – problems with <span class="No-Break">data leaks</span></li>
			</ul>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor123"/>Training and testing processes</h1>
			<p>Machine learning <a id="_idIndexMarker366"/>has revolutionized the way we solve complex problems by enabling computers to learn from data and make predictions or decisions without being explicitly programmed. One crucial aspect of machine learning is training models, which involves teaching algorithms to recognize patterns and relationships in data. Two fundamental methods for training machine learning models are <strong class="source-inline">model.fit()</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">model.predict()</strong></span><span class="No-Break">.</span></p>
			<p>The <strong class="source-inline">model.fit()</strong> function lies at the heart of training a machine learning model. It is the process by which a model learns from a labeled dataset to make accurate predictions. During training, the model adjusts its internal parameters to minimize the discrepancy between its predictions and the true labels in the training data. This iterative optimization process, often referred to as “learning,” allows the model to generalize its knowledge and perform well on <span class="No-Break">unseen data.</span></p>
			<p>In addition to the training data and labels, the <strong class="source-inline">model.fit()</strong> function also takes various hyperparameters as arguments. These hyperparameters include the number of epochs (that is, the number of times the model will iterate over the entire dataset), the batch size (the number of samples processed before updating the parameters), and the learning rate (determining the step size for parameter updates). Properly tuning these hyperparameters is crucial to ensure effective training and prevent issues such as overfitting <span class="No-Break">or underfitting.</span></p>
			<p>Once the training process is <a id="_idIndexMarker367"/>complete, the trained model can be used to make predictions on new, unseen data. This is where the <strong class="source-inline">model.predict()</strong> method comes into play. Given a trained model and a set of input data, the <strong class="source-inline">model.predict()</strong> function applies the learned weights and biases to generate predictions or class probabilities. The predicted outputs can then be used for various purposes, such as classification, regression, or anomaly detection, depending on the nature of the problem <span class="No-Break">at hand.</span></p>
			<p>We saw examples of this interface in previous chapters. Now, it is time to understand what’s under the hood of this interface and how the process of training works. In the previous chapter, we looked at this process as a black box, where the process was done once the program moved past the line with <strong class="source-inline">model.fit()</strong>. This is the basics of the process, but it is not only that. The process is iterative and depends on the algorithm/model that is being trained. As every model has different parameters, the fit function <a id="_idIndexMarker368"/>can take more parameters. The additional parameters can also be added when we instantiate the model, even before the training process. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> presents this process as a <span class="No-Break">gray box:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer097">
					<img alt="Figure 10.1 – Gray box for training a machine learning model" src="image/B19548_10_1.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Gray box for training a machine learning model</p>
			<p>Before we start <a id="_idIndexMarker369"/>the training process, we split the data into training and test sets (which we discussed previously). At the same time, we select the parameters for the machine learning model that we use. These can be anything from the number of trees (for random forest) to the number of iterations and batch size for <span class="No-Break">neural networks.</span></p>
			<p>The training process is iterative, where a model is trained on the data, evaluated internally, and then re-trained to find a better fit for the data. In this chapter, we’ll explore how this internal <span class="No-Break">training works.</span></p>
			<p>Finally, once the model has been trained, it is set for the testing process. In the testing process, we use pre-defined performance measures to check how well the model’s learned pattern can be reproduced for <span class="No-Break">new data.</span></p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor124"/>Training classical machine learning models</h1>
			<p>We’ll start by <a id="_idIndexMarker370"/>training a model that lets us look inside it. We’ll use the CART decision tree classifier, where we can visualize the actual decision tree that is trained. We’ll use the same numerical data we used in the previous chapter. First, let’s read the data and create the <span class="No-Break">train/test split:</span></p>
			<pre class="source-code">
# read the file with data using openpyxl
import pandas as pd
# we read the data from the excel file,
# which is the defect data from the ant 1.3 system
dfDataAnt13 = pd.read_excel('./chapter_6_dataset_numerical.xlsx',
                            sheet_name='ant_1_3',
                            index_col=0)
# prepare the dataset
import sklearn.model_selection
X = dfDataAnt13.drop(['Defect'], axis=1)
y = dfDataAnt13.Defect
X_train, X_test, y_train, y_test = \
        sklearn.model_selection.train_test_split(X, y, random_state=42, train_size=0.9)</pre>			<p>The <a id="_idIndexMarker371"/>preceding code reads an Excel file named <strong class="source-inline">'chapter_6_dataset_numerical.xlsx'</strong> using the <strong class="source-inline">pd.read_excel()</strong> function from pandas. The file is read into a DataFrame called <strong class="source-inline">dfDataAnt13</strong>. The <strong class="source-inline">sheet_name</strong> parameter specifies the sheet within the Excel file to read, while the <strong class="source-inline">index_col</strong> parameter sets the first column as the index of <span class="No-Break">the DataFrame.</span></p>
			<p>The code prepares the dataset for training a machine learning model. It assigns the independent variables (features) to the <strong class="source-inline">X</strong> variable by dropping the <strong class="source-inline">'Defect'</strong> column from the <strong class="source-inline">dfDataAnt13</strong> DataFrame using the <strong class="source-inline">drop()</strong> method. The dependent variable (target) is assigned to the <strong class="source-inline">y</strong> variable by selecting the <strong class="source-inline">'Defect'</strong> column from the <span class="No-Break"><strong class="source-inline">dfDataAnt13</strong></span><span class="No-Break"> DataFrame.</span></p>
			<p>The <strong class="source-inline">sklearn.model_selection.train_test_split()</strong> function is used to split the dataset into training and testing sets. The <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> variables are split into <strong class="source-inline">X_train</strong>, <strong class="source-inline">X_test</strong>, <strong class="source-inline">y_train</strong>, and <strong class="source-inline">y_test</strong> variables. The <strong class="source-inline">train_size</strong> parameter is set to <strong class="source-inline">0.9</strong>, indicating that 90% of the data will be used for training and the remaining 10% will be used for testing. The <strong class="source-inline">random_state</strong> parameter is set to <strong class="source-inline">42</strong> to ensure reproducibility of <span class="No-Break">the split.</span></p>
			<p>Once the<a id="_idIndexMarker372"/> data has been prepared, we can import the decision tree library and train <span class="No-Break">the model:</span></p>
			<pre class="source-code">
# now that we have the data prepared
# we import the decision tree classifier and train it
from sklearn.tree import DecisionTreeClassifier
# first we create an empty classifier
decisionTreeModel = DecisionTreeClassifier()
# then we train the classifier
decisionTreeModel.fit(X_train, y_train)
# and we test it for the test set
y_pred_cart = decisionTreeModel.predict(X_test)</pre>			<p>The preceding code fragment imports the <strong class="source-inline">DecisionTreeClassifier</strong> class from the <strong class="source-inline">sklearn.tree</strong> module. An empty decision tree classifier object is created and assigned to the <strong class="source-inline">decisionTreeModel</strong> variable. This object will be trained on the dataset that was prepared in the previous fragment. The <strong class="source-inline">fit()</strong> method is called on the <strong class="source-inline">decisionTreeModel</strong> object to train the classifier. The <strong class="source-inline">fit()</strong> method takes the training data (<strong class="source-inline">X_train</strong>) and the corresponding target values (<strong class="source-inline">y_train</strong>) as input. The classifier will learn patterns and relationships in the training data to <span class="No-Break">make predictions.</span></p>
			<p>The trained decision tree classifier is used to predict the target values for the test dataset (<strong class="source-inline">X_test</strong>). The <strong class="source-inline">predict()</strong> method is called on the <strong class="source-inline">decisionTreeModel</strong> object, passing <strong class="source-inline">X_test</strong> as the input. The predicted target values are stored in the <strong class="source-inline">y_pred_cart</strong> variable. The predicted model needs to be evaluated, so let’s evaluate the accuracy, precision, and recall of <span class="No-Break">the model:</span></p>
			<pre class="source-code">
# now, let's evaluate the code
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
print(f'Accuracy: {accuracy_score(y_test, y_pred_cart):.2f}')
print(f'Precision: {precision_score(y_test, y_pred_cart, average="weighted"):.2f}, Recall: {recall_score(y_test, y_pred_cart, average="weighted"):.2f}')</pre>			<p>This code fragment results in the <span class="No-Break">following output:</span></p>
			<pre class="source-code">
Accuracy: 0.83
Precision: 0.94, Recall: 0.83</pre>			<p>The <a id="_idIndexMarker373"/>metrics show that the model is not that bad. It classified 83% of the data in the test set correctly. It is a bit more sensitive to the true positives (higher precision) than to true negatives (lower recall). This means that it tends to miss some of the defect-prone modules in its predictions. However, the decision tree model lets us take a look inside the model and explore the pattern that it learned from the data. The following code fragment <span class="No-Break">does this:</span></p>
			<pre class="source-code">
from sklearn.tree import export_text
tree_rules = export_text(decisionTreeModel, feature_names=list(X_train.columns))
print(tree_rules)</pre>			<p>The preceding code fragment exports the decision tree in the form of text that we print. The <strong class="source-inline">export_text()</strong> function takes two arguments – the first one is the decision tree to visualize and the next one is the list of features. In our case, the list of features is the list of columns in <span class="No-Break">the dataset.</span></p>
			<p>The entire decision tree is quite complex in this case, but the first decision path looks <span class="No-Break">like this:</span></p>
			<pre class="source-code">
|--- WMC &lt;= 36.00
|   |--- ExportCoupling &lt;= 1.50
|   |   |--- NOM &lt;= 2.50
|   |   |   |--- NOM &lt;= 1.50
|   |   |   |   |--- class: 0
|   |   |   |--- NOM &gt;  1.50
|   |   |   |   |--- WMC &lt;= 5.50
|   |   |   |   |   |--- class: 0
|   |   |   |   |--- WMC &gt;  5.50
|   |   |   |   |   |--- CBO &lt;= 4.50
|   |   |   |   |   |   |--- class: 1
|   |   |   |   |   |--- CBO &gt;  4.50
|   |   |   |   |   |   |--- class: 0
|   |   |--- NOM &gt;  2.50
|   |   |   |--- class: 0</pre>			<p>This<a id="_idIndexMarker374"/> decision path looks very similar to a large <strong class="source-inline">if-then</strong> statement, which we could write ourselves if we knew the patterns in the data. This pattern is not simple, which means that the data is quite complex. It can be non-linear and requires complex models to capture the dependencies. It can also require a lot of effort to find the right balance between the performance of the model and its ability to generalize <span class="No-Break">the data.</span></p>
			<p>So, here is my best practice for working with this kind <span class="No-Break">of model.</span></p>
			<p class="callout-heading">Best practice #54</p>
			<p class="callout">If you want to understand your numerical data, use models that <span class="No-Break">provide explainability.</span></p>
			<p>In the previous chapters, I advocated for using AutoML models as they are robust and save us a lot of trouble finding the right module. However, if we want to understand our data a bit better and understand the patterns, we can start with models such as decision trees. Their insight into the data provides us with a good overview of what we can get out of <span class="No-Break">the data.</span></p>
			<p>As a counter-example, let’s look at the data from another module from the same dataset. Let’s read<a id="_idIndexMarker375"/> it and perform <span class="No-Break">the split:</span></p>
			<pre class="source-code">
# read the file with data using openpyxl
import pandas as pd
# we read the data from the excel file,
# which is the defect data from the ant 1.3 system
dfDataCamel12 = pd.read_excel('./chapter_6_dataset_numerical.xlsx',
                            sheet_name='camel_1_2',
                            index_col=0)
# prepare the dataset
import sklearn.model_selection
X = dfDataCamel12.drop(['Defect'], axis=1)
y = dfDataCamel12.Defect
X_train, X_test, y_train, y_test = \
        sklearn.model_selection.train_test_split(X, y, random_state=42, train_size=0.9)</pre>			<p>Now, let’s train a new model for <span class="No-Break">that data:</span></p>
			<pre class="source-code">
# now that we have the data prepared
# we import the decision tree classifier and train it
from sklearn.tree import DecisionTreeClassifier
# first we create an empty classifier
decisionTreeModelCamel = DecisionTreeClassifier()
# then we train the classifier
decisionTreeModelCamel.fit(X_train, y_train)
# and we test it for the test set
y_pred_cart_camel = decisionTreeModel.predict(X_test)</pre>			<p>So far, so good – no errors, no problems. Let’s check the performance of <span class="No-Break">the model:</span></p>
			<pre class="source-code">
# now, let's evaluate the code
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
print(f'Accuracy: {accuracy_score(y_test, y_pred_cart_camel):.2f}')
print(f'Precision: {precision_score(y_test, y_pred_cart_camel, average="weighted"):.2f}, Recall: {recall_score(y_test, y_pred_cart_camel, average="weighted"):.2f}')</pre>			<p>The <a id="_idIndexMarker376"/>performance, however, is not as high as it <span class="No-Break">was previously:</span></p>
			<pre class="source-code">
Accuracy: 0.65
Precision: 0.71, Recall: 0.65</pre>			<p>Now, let’s print <span class="No-Break">the tree:</span></p>
			<pre class="source-code">
from sklearn.tree import export_text
tree_rules = export_text(decisionTreeModel, feature_names=list(X_train.columns))
print(tree_rules)</pre>			<p>As we can see, the results are also <span class="No-Break">quite complex:</span></p>
			<pre class="source-code">
|--- WMC &gt;  36.00
|   |--- DCC &lt;= 3.50
|   |   |--- WMC &lt;= 64.50
|   |   |   |--- NOM &lt;= 17.50
|   |   |   |   |--- ImportCoupling &lt;= 7.00
|   |   |   |   |   |--- NOM &lt;= 6.50
|   |   |   |   |   |   |--- class: 0
|   |   |   |   |   |--- NOM &gt;  6.50
|   |   |   |   |   |   |--- CBO &lt;= 4.50
|   |   |   |   |   |   |   |--- class: 0
|   |   |   |   |   |   |--- CBO &gt;  4.50
|   |   |   |   |   |   |   |--- ExportCoupling &lt;= 13.00
|   |   |   |   |   |   |   |   |--- NOM &lt;= 16.50
|   |   |   |   |   |   |   |   |   |--- class: 1
|   |   |   |   |   |   |   |   |--- NOM &gt;  16.50
|   |   |   |   |   |   |   |   |   |--- class: 0
|   |   |   |   |   |   |   |--- ExportCoupling &gt;  13.00
|   |   |   |   |   |   |   |   |--- class: 0
|   |   |   |   |--- ImportCoupling &gt;  7.00
|   |   |   |   |   |--- class: 0
|   |   |   |--- NOM &gt;  17.50
|   |   |   |   |--- class: 1
|   |   |--- WMC &gt;  64.50
|   |   |   |--- class: 0</pre>			<p>If <a id="_idIndexMarker377"/>we look at the very first decision in this tree and the previous one, it is based on the WMC feature. <strong class="bold">WMC</strong> means <strong class="bold">weighted method per class</strong> and is one of the classical software metrics that was introduced in the 1990s by Chidamber and Kamerer. The metric captures both the complexity and the size of the class (in a way) and it is quite logical that large classes are more defect-prone – simply because there is more chance to make a mistake if there is more source code. In the case of this model, this is a bit more complicated as the model recognizes that the classes <a id="_idIndexMarker378"/>with WMC over 36 are more prone to errors than others, apart from classes that are over 64.5, which are less prone to errors. The latter is also a known phenomenon that large classes are also more difficult to test and therefore can contain <span class="No-Break">undiscovered defects.</span></p>
			<p>Here is my next best practice, which is about the explainability <span class="No-Break">of models.</span></p>
			<p class="callout-heading">Best practice #55</p>
			<p class="callout">The best models are those that capture the empirical phenomena in <span class="No-Break">the data.</span></p>
			<p>Although machine learning models can capture any kind of dependencies, the best models are the ones that can capture logical, empirical observations. In the previous examples, the model could capture the software engineering empirical observations related to the size of the classes and their defect-proneness. Having a model that captures empirical relations leads to better products and <span class="No-Break">explainable AI.</span></p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor125"/>Understanding the training process</h1>
			<p>From the<a id="_idIndexMarker379"/> software engineer’s perspective, the training process is rather simple – we fit the model, validate it, and use it. We check how good the model is in terms of the performance metrics. If the model is good enough, and we can explain it, then we develop the entire product around it, or we use it in a larger <span class="No-Break">software product.</span></p>
			<p>When the model does not learn anything useful, we need to understand why this is the case and whether there could be another model that can. We can use the visualization techniques we learned about in <a href="B19548_06.xhtml#_idTextAnchor074"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> to explore the data and clear it from noise using the techniques from <a href="B19548_04.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 4</em></span></a><span class="No-Break">.</span></p>
			<p>Now, let’s explore the process of how the decision tree model learns from the data. The <strong class="source-inline">DecisionTree</strong> classifier learns from the provided data by recursively partitioning the feature space based on the values of the features in the training dataset. It constructs a binary tree where each internal node represents a feature and a decision rule based on a threshold value, and each leaf node represents a predicted class <span class="No-Break">or outcome.</span></p>
			<p>The training is done <span class="No-Break">in steps:</span></p>
			<ol>
				<li><strong class="bold">Selecting the best feature</strong>: The classifier evaluates different features and determines the one that best separates the data into different classes. This is typically done using a measure of impurity or information gain, such as Gini impurity <span class="No-Break">or entropy.</span></li>
				<li><strong class="bold">Splitting the dataset</strong>: Once the best feature has been selected, the classifier splits the dataset into two or more subsets based on the values of that feature. Each subset represents a different branch or path in the <span class="No-Break">decision tree.</span></li>
				<li><strong class="bold">Repeating the process recursively</strong>: The preceding steps are repeated for each subset or branch of the decision tree, treating them as separate datasets. The process continues until a stopping condition is met, such as reaching a maximum depth, a minimum number of samples at a node, or other <span class="No-Break">predefined criteria.</span></li>
				<li><strong class="bold">Assigning class labels</strong>: At the leaf nodes of the decision tree, the classifier assigns class labels based on the majority class of the samples in that region. This means that when making predictions, the classifier assigns the most frequent class in the leaf node to the unseen samples that fall into <span class="No-Break">that region.</span></li>
			</ol>
			<p>During the<a id="_idIndexMarker380"/> learning process, the <strong class="source-inline">DecisionTree</strong> classifier aims to find the best splits that maximize the separation of classes and minimize the impurity within each resulting subset. By recursively partitioning the feature space based on the provided training data, the classifier learns decision rules that allow it to make predictions for <span class="No-Break">unseen data.</span></p>
			<p>It’s important to note that decision trees are prone to overfitting, meaning they can memorize the training data too well and not generalize well to new data. Techniques such as pruning, limiting the maximum depth, or using ensemble methods such as random forest can help mitigate overfitting and improve the performance of decision <span class="No-Break">tree models.</span></p>
			<p>We used the random forest classifier previously in this book, so we won’t dive into the details here. Although random forests are better at generalizing data, they are opaque compared to decision trees. We cannot explore what the model learned – we can only explore which features contribute the most to <span class="No-Break">the verdict.</span></p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor126"/>Random forest and opaque models</h1>
			<p>Let’s train the <a id="_idIndexMarker381"/>random forest classifier based on the same data as in the counter-example and check whether the model performs better and whether the model uses similar features as the <strong class="source-inline">DecisionTree</strong> classifier in the <span class="No-Break">original counter-example.</span></p>
			<p>Let’s instantiate, train, and validate the model on the same data using the following fragment <span class="No-Break">of code:</span></p>
			<pre class="source-code">
from sklearn.ensemble import RandomForestClassifier
randomForestModel = RandomForestClassifier()
randomForestModel.fit(X_train, y_train)
y_pred_rf = randomForestModel.predict(X_test)</pre>			<p>After evaluating the model, we obtain the following <span class="No-Break">performance metrics:</span></p>
			<pre class="source-code">
Accuracy: 0.62
Precision: 0.63, Recall: 0.62</pre>			<p>Admittedly, these metrics are different than the metrics in the decision trees, but the overall performance is not that much different. The difference in accuracy of 0.03 is negligible. First, we can extract the important features, reusing the same techniques that were presented in <a href="B19548_05.xhtml#_idTextAnchor060"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">:</span></p>
			<pre class="source-code">
# now, let's check which of the features are the most important ones
# first we create a dataframe from this list
# then we sort it descending
# and then filter the ones that are not imporatnt
dfImportantFeatures = pd.DataFrame(randomForestModel.feature_importances_, index=X.columns, columns=['importance'])
# sorting values according to their importance
dfImportantFeatures.sort_values(by=['importance'],
                                ascending=False,
                                inplace=True)
# choosing only the ones that are important, skipping
# the features which have importance of 0
dfOnlyImportant = dfImportantFeatures[dfImportantFeatures['importance'] != 0]
# print the results
print(f'All features: {dfImportantFeatures.shape[0]}, but only {dfOnlyImportant.shape[0]} are used in predictions. ')</pre>			<p>We <a id="_idIndexMarker382"/>can visualize the set of features used in the decision by executing the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
# we use matplotlib and seaborn to make the plot
import matplotlib.pyplot as plt
import seaborn as sns
# Define size of bar plot
# We make the x axis quite much larger than the y-axis since
# there is a lot of features to visualize
plt.figure(figsize=(40,10))
# plot Searborn bar chart
# we just use the blue color
sns.barplot(y=dfOnlyImportant['importance'],
            x=dfOnlyImportant.index,
            color='steelblue')
# we make the x-labels rotated so that we can fit
# all the features
plt.xticks(rotation=90)
sns.set(font_scale=6)
# add chart labels
plt.title('Importance of features, in descending order')
plt.xlabel('Feature importance')
plt.ylabel('Feature names')</pre>			<p>This <a id="_idIndexMarker383"/>code helps us understand the importance chart shown in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2</em>. Here, again, the WMC is the most important feature. This means that a lot of trees in the forest are using this metric to make decisions. However, we do not know the algorithm since the forest is an ensemble classifier – it uses voting for the decisions – meaning that always more than one tree is used when making the <span class="No-Break">final call/prediction:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer098">
					<img alt="Figure 10.2 – Feature importance chart for the random forest classifier." src="image/B19548_10_2.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Feature importance chart for the random forest classifier.</p>
			<p>Please note that the model is more complex than just a linear combination of these features. This chart illustrates something that is not a best practice, but a best experience. So, I will use it as a best practice to illustrate the importance <span class="No-Break">of it.</span></p>
			<p class="callout-heading">Best practice #56</p>
			<p class="callout">Simple, but explainable, models can often capture data in a <span class="No-Break">good way.</span></p>
			<p>What I’ve learned throughout my experiments with different types of data is that if there is a pattern, a simple model will capture it. If there is no pattern, or if the data has a lot of exceptions from rules, then even the most complex models will have problems in finding the patterns. Therefore, if you cannot explain your results, do not use them in your product, as there is a chance that these results will make the products <span class="No-Break">quite useless.</span></p>
			<p>However, there is a light at the end of the tunnel here. Some models can capture very complex patterns, but they are opaque – <span class="No-Break">neural networks.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor127"/>Training deep learning models</h1>
			<p>Training a<a id="_idIndexMarker384"/> dense neural network involves various steps. First, we prepare the data. This typically involves tasks such as feature scaling, handling missing values, encoding categorical variables, and splitting the data into training and <span class="No-Break">validation sets.</span></p>
			<p>Then, we define the architecture of the dense neural network. This includes specifying the number of layers, the number of neurons in each layer, the activation functions to be used, and any regularization techniques, such as dropout or <span class="No-Break">batch normalization.</span></p>
			<p>Once the model has been defined, we need to initialize it. We create an instance of the neural network model based on the defined architecture. This involves creating an instance of the neural network class or using a predefined model architecture available in a deep learning library. We also need to define a loss function that quantifies the error between the predicted output of the model and the actual target values. The choice of loss function depends on the nature of the problem, such as classification (cross-entropy) or regression (mean <span class="No-Break">squared error).</span></p>
			<p>In addition to the loss function, we need an optimizer. The optimizer algorithm will update the weights of the neural network during training. Common optimizers include <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>), Adam, <span class="No-Break">and RMSprop.</span></p>
			<p>Then, we can train the model. Here, we iterate over the training data for multiple epochs (passes through the entire dataset). In each epoch, perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li><strong class="bold">Forward pass</strong>: We feed a batch of input data into the model and compute the <span class="No-Break">predicted output.</span></li>
				<li><strong class="bold">Compute loss</strong>: We compare the predicted output with the actual target values using the defined loss function to calculate <span class="No-Break">the loss.</span></li>
				<li><strong class="bold">Backward pass</strong>: We propagate the loss backward through the network to compute the gradients of the weights concerning the loss <span class="No-Break">using backpropagation.</span></li>
				<li><strong class="bold">Update the weights</strong>: We use the optimizer to update the weights of the neural network based on the computed gradients, adjusting the network parameters to minimize <span class="No-Break">the loss.</span></li>
			</ol>
			<p>We repeat these steps for each batch in the training data until all batches have <span class="No-Break">been processed.</span></p>
			<p>Finally, we<a id="_idIndexMarker385"/> need to perform the validation process, just like in the previous models. Here, we compute a validation metric (such as accuracy or mean squared error) to assess how well the model is generalizing to unseen data. This helps us monitor the model’s progress and <span class="No-Break">detect overfitting.</span></p>
			<p>Once the model has been trained and validated, we can evaluate its performance on a separate test dataset that was not used during training or validation. Here, we calculate relevant evaluation metrics to assess the model’s accuracy, precision, recall, or other <span class="No-Break">desired metrics.</span></p>
			<p>So, let’s do this for our dataset. First, we must define the architecture of the model using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import torch
import torch.nn as nn
import torch.optim as optim
# Define the neural network architecture
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_size, num_classes)
    def forward(self, x):
        out = self.fc1(x)
        out = self.relu(out)
        out = self.fc2(out)
        return out</pre>			<p>Here, we define a class called <strong class="source-inline">NeuralNetwork</strong>, which is a subclass of <strong class="source-inline">nn.Module</strong>. This class represents our neural network model. It has two fully connected layers (<strong class="source-inline">fc1</strong> and <strong class="source-inline">fc2</strong>) with a <strong class="source-inline">ReLU</strong> activation function in between. The network looks something like the one shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer099">
					<img alt="Figure 10.3 – Neural network used for predicting defects." src="image/B19548_10_3.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Neural network used for predicting defects.</p>
			<p>This <a id="_idIndexMarker386"/>visualization was created using <a href="http://alexlenail.me/NN-SVG/index.html">http://alexlenail.me/NN-SVG/index.html</a>. The number of neurons in the hidden layer is 64, but in this figure, only 16 are shown to make it more readable. The network starts with 6 input neurons, then 64 neurons in the hidden layer (middle), and two neurons for the decision classes at <span class="No-Break">the end.</span></p>
			<p>Now, we can define the hyperparameters for training the network and <span class="No-Break">instantiate it:</span></p>
			<pre class="source-code">
# Define the hyperparameters
input_size = X_train.shape[1]  # Number of input features
hidden_size = 64              # Number of neurons in the hidden layer
num_classes = 2               # Number of output classes
# Create an instance of the neural network
model = NeuralNetwork(input_size, hidden_size, num_classes)
# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Convert the data to PyTorch tensors
X_train_tensor = torch.Tensor(X_train.values)
y_train_tensor = torch.LongTensor(y_train.values)
X_test_tensor = torch.Tensor(X_test.values)
# Training the neural network
num_epochs = 10000
batch_size = 32</pre>			<p>Here, we<a id="_idIndexMarker387"/> create an instance of the <strong class="source-inline">NeuralNetwork</strong> class called <strong class="source-inline">model</strong> with the specified input size, hidden size, and number of output classes, as we defined in the first code fragment. We define the loss function (cross-entropy loss) and the optimizer (Adam optimizer) to train the model. The data is then converted into PyTorch tensors using <strong class="source-inline">torch.Tensor()</strong> and <strong class="source-inline">torch.LongTensor()</strong>. Finally, we say that we want to train the model in 10,000 epochs (iterations) with 32 elements (data points) in <span class="No-Break">each iteration:</span></p>
			<pre class="source-code">
for epoch in range(num_epochs):
    for I in range(0, len(X_train_tensor), batch_size):
        batch_X = X_train_tensor[i:i+batch_size]
        batch_y = y_train_tensor[i:i+batch_size]
        # Forward pass
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        # Backward and optimize
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    # Print the loss at the end of each epoch
    if (epoch % 100 == 0):
      print(""Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.3f"")</pre>			<p>Now, we <a id="_idIndexMarker388"/>can get the predictions for the test data and obtain the <span class="No-Break">performance metrics:</span></p>
			<pre class="source-code">
with torch.no_grad():
    model.eval()  # Set the model to evaluation mode
    X_test_tensor = torch.Tensor(X_test.values)
    outputs = model(X_test_tensor)
    _, predicted = torch.max(outputs.data, 1)
    y_pred_nn = predicted.numpy()
# now, let's evaluate the code
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
print(f'Accuracy: {accuracy_score(y_test, y_pred_nn):.2f}')
print(f'Precision: {precision_score(y_test, y_pred_nn, average="weighted"):.2f}, Recall: {recall_score(y_test, y_pred_nn, average="weighted"):.2f}')</pre>			<p>The performance metrics are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Accuracy: 0.73
Precision: 0.79, Recall: 0.73</pre>			<p>So, this is a bit better than the previous models, but it’s not great. The patterns are just not there. We could make the network larger by increasing the number of hidden layers, but this does not make the <span class="No-Break">predictions better.</span></p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor128"/>Misleading results – data leaking</h1>
			<p>In the <a id="_idIndexMarker389"/>training process, we use one set of data and in the test set, we use another set. The best training process is when these two datasets are separate. If they are not, we get into something that is called a data leak problem. This <a id="_idIndexMarker390"/>problem is when we have the same data points in both the train and test sets. Let’s illustrate this with <span class="No-Break">an example.</span></p>
			<p>First, we need to create a new split, where we have some data points in both sets. We can do that by using the split function and setting 20% of the data points to the test set. This means that at least 10% of the data points are in <span class="No-Break">both sets:</span></p>
			<pre class="source-code">
X_trainL, X_testL, y_trainL, y_testL = \
        sklearn.model_selection.t<a id="_idTextAnchor129"/>rain_test_split(X, y, random_state=42, train_size=0.8)</pre>			<p>Now, we can use the same code to make predictions on this data and then calculate the <span class="No-Break">performance metrics:</span></p>
			<pre class="source-code">
# now, let's evaluate the model on this new data
with torch.no_grad():
    model.eval()  # Set the model to evaluation mode
    X_test_tensor = torch.Tensor(X_testL.values)
    outputs = model(X_test_tensor)
    _, predicted = torch.max(outputs.data, 1)
    y_pred_nn = predicted.numpy()
print(f'Accuracy: {accuracy_score(y_testL, y_pred_nn):.2f}')
print(f'Precision: {precision_score(y_testL, y_pred_nn, average="weighted"):.2f}, Recall: {recall_score(y_testL, y_pred_nn, average="weighted"):.2f}')</pre>			<p>The results are <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
Accuracy: 0.85
Precision: 0.86, Recall: 0.85</pre>			<p>The results are better than before. However, they are only better because 10% of the data points were used in both the training and the test sets. This means that the model performs much worse than the metrics suggest. Hence, we’ve come to my next <span class="No-Break">best practice.</span></p>
			<p class="callout-heading">Best practice #56</p>
			<p class="callout">Always make sure that the data points in both the train and test sets <span class="No-Break">are separate.</span></p>
			<p>Although<a id="_idIndexMarker391"/> we made this mistake on purpose here, it is quite easy to make it in practice. Please note the <strong class="source-inline">random_state=42</strong> parameter<a id="_idIndexMarker392"/> in the split function. Setting it explicitly ensures that the split is repeatable. However, if we do not do this, we can end up with different splits every time we make them and thus we can end up with the data <span class="No-Break">leak problem.</span></p>
			<p>The data leak problem is even more difficult to discover when we’re working with images or text. Just the fact that an image comes from two different files does not guarantee that it is different. For example, images taken one after another during highway driving will be different but will not be too different, and if they end up in test and train sets, we get a whole new dimension of the data <span class="No-Break">leak problem.</span></p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor130"/>Summary</h1>
			<p>In this chapter, we discussed various topics related to machine learning and neural networks. We explained how to read data from an Excel file using the pandas library and prepare the dataset for training a machine learning model. We explored the use of decision tree classifiers and demonstrated how to train a decision tree model using scikit-learn. We also showed how to make predictions using the <span class="No-Break">trained model.</span></p>
			<p>Then, we discussed how to switch from a decision tree classifier to a random forest classifier, which is an ensemble of decision trees. We explained the necessary code modifications and provided an example. Next, we shifted our focus to using a dense neural network in PyTorch. We described the process of creating the neural network architecture, training the model, and making predictions using the <span class="No-Break">trained model.</span></p>
			<p>Lastly, we explained the steps involved in training a dense neural network, including data preparation, model architecture, initializing the model, defining a loss function and optimizer, the training loop, validation, hyperparameter tuning, <span class="No-Break">and evaluation.</span></p>
			<p>Overall, we covered a range of topics related to machine learning algorithms, including decision trees, random forests, and dense neural networks, along with their respective <span class="No-Break">training processes.</span></p>
			<p>In the next chapter we explore how to train more advanced machine learning models - for <span class="No-Break">example AutoEncoders.</span></p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor131"/>References</h1>
			<ul>
				<li><em class="italic">Chidamber, S.R. and C.F. Kemerer, A metrics suite for object oriented design. IEEE Transactions on Software Engineering, 1994. 20(6): </em><span class="No-Break"><em class="italic">p. 476–493.</em></span></li>
			</ul>
		</div>
	</body></html>