<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Clustering Data</h1></div></div></div><p>We will now shift our focus to <strong>unsupervised learning</strong>. In this chapter, we will study several <strong>clustering</strong> algorithms, or <strong>clusterers</strong><a id="id611" class="indexterm"/>, and how we can implement them in Clojure. We will also demonstrate several Clojure libraries that provide implementations of clustering algorithms. Towards the end of the chapter, we explore will <strong>dimensionality</strong> <strong>reduction</strong> and how it can be used to provide an understandable visualization of the supplied sample data.</p><p>Clustering or <strong>cluster analysis</strong> is basically a method of grouping<a id="id612" class="indexterm"/> data or samples together. As a form of unsupervised learning, a clustering model is trained using unlabeled data, by which we mean the samples in the training data will not contain the class or category of the input values. Rather, the training data does not describe values for the output variable of a given set of inputs. A clustering model must determine similarities between several input values and infer the classes of these input values on its own. The sample values can thus be partitioned into a number of clusters using such a model.</p><p>There are several practical applications of clustering in real-world problems. Clustering is often used in image analysis, image segmentation, software evolution systems, and social network analysis. Outside the domain of computer science, clustering algorithms are used in biological classification, gene analysis, and crime analysis.</p><p>There are several clustering algorithms that have been published till date. Each of these algorithms has a unique notion of how a cluster is defined and how input values are combined into new clusters. Unfortunately, there is no given solution for any clustering problem and each algorithm must be evaluated on a trial-and-error basis to determine which model is best suited for the supplied training data. Of course, this is one of the aspects of unsupervised learning, in the sense that there is no definite way to say that a given solution is the best fit for any given data. </p><p>This is due to the fact that the input data is unlabeled, and a simple yes/no based reward system to train cannot easily be inferred from data in which the output variable or class of the input values is unknown.</p><p>In this chapter, we will describe a handful of clustering techniques that can be applied on unlabeled data.</p><div><div><div><div><h1 class="title"><a id="ch07lvl1sec46"/>Using K-means clustering</h1></div></div></div><p>The <strong>K-means clustering</strong> algorithm<a id="id613" class="indexterm"/> is a clustering technique that is based on vector quantization (for more information, refer to "Algorithm AS 136: A K-Means Clustering Algorithm"). This algorithm partitions a number of sample vectors into <em>K</em> clusters and hence derives its name. In this section, we will study the nature and implementation of the K-means algorithm.</p><p>
<strong>Quantization</strong>, in signal processing, is the process of mapping a large set of values into a smaller set of values. For example, an analog signal can be quantized to 8 bits and the signal can be represented by 256 levels of quantization<a id="id614" class="indexterm"/>. Assuming that the bits represent values within the range of 0 to 5 volts, the 8-bit quantization allows a resolution of 5/256 volts per bit. In the context of clustering, quantization of input or output <a id="id615" class="indexterm"/>can be done for the following reasons:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To restrict the clustering to a finite set of clusters.</li><li class="listitem" style="list-style-type: disc">To accommodate a range of values in the sample data that need to have some level of tolerance while clustering is performed. This kind of flexibility is crucial in grouping together unknown or unexpected sample values.</li></ul></div><p>The gist of the algorithm can be concisely described as follows. The <em>K</em> mean values, or <em>centroids</em>, are first randomly initialized. The distance of each sample value from each centroid is then calculated. A sample value is grouped into a given centroid's cluster depending on which centroid has <a id="id616" class="indexterm"/>the minimum distance from the given sample. In a multidimensional space for multiple features or input values, the distance of a sample input vector is measured by <strong>Euclidean distance</strong> between<a id="id617" class="indexterm"/> the input vector and a given centroid. This phase of the algorithm is termed as the <strong>assignment step</strong>.</p><p>The next phase in the <em>K</em>-means algorithm is the <strong>update step</strong><a id="id618" class="indexterm"/>. The values of the centroids are adjusted based on the partitioned input values generated from the previous step. These two steps are then repeated until the difference between the centroid values in two consecutive iterations becomes negligible. Thus, the final result of the algorithm is the clusters or classes of each set of input values in the given training data.</p><p>The iterations<a id="id619" class="indexterm"/> performed by the <em>K</em>-means algorithm can be illustrated using the following plots:</p><div><img src="img/4351OS_07_01.jpg" alt="Using K-means clustering"/></div><p>Each of the plots depict the centroid and partitioned sample values produced by each iteration of the algorithm for a given set of input values. The clusters in a given iteration are shown in different colors in each plot. The final plot represents the final partitioned set of input values produced by the <em>K</em>-means algorithm.</p><p>The optimization objective<a id="id620" class="indexterm"/> of the <em>K</em>-means clustering algorithm can be formally defined as follows:</p><div><img src="img/4351OS_07_02.jpg" alt="Using K-means clustering"/></div><p>In the optimization problem defined in the preceding equation, the terms <img src="img/4351OS_07_03.jpg" alt="Using K-means clustering"/> represent the <em>K</em>-mean values around which the input values are clustered. The <em>K</em>-means algorithm minimizes the size of the clusters and also determines the mean values for which these clusters can be minimized in size.</p><p>This algorithm requires <img src="img/4351OS_07_04.jpg" alt="Using K-means clustering"/> sample values and <img src="img/4351OS_07_05.jpg" alt="Using K-means clustering"/> initial mean values as inputs. In the assignment step, the input values are assigned to clusters around the initial mean values supplied to the algorithm. In the <a id="id621" class="indexterm"/>later update step, the new mean values are calculated from the input values. In most implementations, the new mean values are calculated as the mean of all input values that belongs to a given cluster.</p><p>Most implementations initialize the <img src="img/4351OS_07_05.jpg" alt="Using K-means clustering"/> initial mean values to some randomly chosen input values. This technique is called the <strong>Forgy method</strong><a id="id622" class="indexterm"/> of random initialization.</p><p>The <em>K</em>-means algorithm is NP-hard when either the number of clusters <em>K</em> or the number of dimensions in the input data <em>d</em> is unbound. When both these values are fixed, the <em>K</em>-means algorithm has a time complexity of <img src="img/4351OS_07_06.jpg" alt="Using K-means clustering"/>. There are several variations of this algorithm that vary on how the new mean values are calculated.</p><p>We will now demonstrate how we can implement the <em>K</em>-means algorithm in pure Clojure, while using no external libraries. <a id="id623" class="indexterm"/>We begin by defining bits and pieces of the algorithm, which are then later combined to provide a basic visualization of the <em>K</em>-means algorithm.</p><p>We can say that the distance between two numbers is the absolute difference between their values and this can be implemented as a <code class="literal">distance</code> function, as shown in the following code:</p><div><pre class="programlisting">(defn distance [a b]
  (if (&lt; a b) (- b a) (- a b)))</pre></div><p>If we are given a number of mean values, we can calculate the closest mean from a given number by using a composition of the <code class="literal">distance</code> and <code class="literal">sort-by</code> functions, as shown in the following code:</p><div><pre class="programlisting">(defn closest [point means distance]
  (first (sort-by #(distance % point) means)))</pre></div><p>To demonstrate the <code class="literal">closest</code> function defined in the preceding code, we will first need to define some data, that is, a sequence of numbers and a couple of mean values, as shown in the following code:</p><div><pre class="programlisting">(def data '(2 3 5 6 10 11 100 101 102))
(def guessed-means '(0 10))</pre></div><p>We can now use the <code class="literal">data</code> and <code class="literal">guessed-means</code> variables with the <code class="literal">closest</code> function and an arbitrary number, as shown in the following REPL output:</p><div><pre class="programlisting">user&gt; (closest 2 guessed-means distance)
0
user&gt; (closest 9 guessed-means distance)
10
user&gt; (closest 100 guessed-means distance)
10</pre></div><p>Given the means <code class="literal">0</code> and <code class="literal">10</code>, the <code class="literal">closest</code> function returns <code class="literal">0</code> as the closest mean to <code class="literal">2</code>, and <code class="literal">10</code> as that for <code class="literal">9</code> and <code class="literal">100</code>. Thus, a set of data points can be grouped by the means, which are closest to them. We can implement a function that implements this grouping operation using the <code class="literal">closest</code> and <code class="literal">group-by</code> functions as follows:</p><div><pre class="programlisting">(defn point-groups [means data distance]
  (group-by #(closest % means distance) data))</pre></div><p>The <code class="literal">point-groups</code> function<a id="id624" class="indexterm"/> defined in the preceding code requires three arguments, namely the initial mean values, the collection of points to be grouped, and lastly a function that returns the distance of a point from a given mean. Note that the <code class="literal">group-by</code> function applies a function, which is passed as the first parameter, to a collection, which is then passed as the second parameter. </p><p>We can apply the <code class="literal">point-groups</code> function on the list of numbers represented by the <code class="literal">data</code> variable to group the given values by their distance from the guessed means, represented by <code class="literal">guessed-means</code> as shown in the following code:</p><div><pre class="programlisting">user&gt; (point-groups guessed-means data distance)
{0 [2 3 5], 10 [6 10 11 100 101 102]}</pre></div><p>As shown in the preceding code, the <code class="literal">point-groups</code> function partitions the sequence <code class="literal">data</code> into two groups. To calculate the new set of mean values from these groups of input values, we must calculate their average value, which can be implemented using the <code class="literal">reduce</code> and <code class="literal">count</code> functions, as shown in the following code:</p><div><pre class="programlisting">(defn average [&amp; list]
  (/ (reduce + list)
     (count list)))</pre></div><p>We implement a function to apply the <code class="literal">average</code> function defined in the preceding code to the previous mean values and<a id="id625" class="indexterm"/> the map of groups returned by the <code class="literal">point-groups</code> function. We will do this with the help of the following code:</p><div><pre class="programlisting">(defn new-means [average point-groups old-means]
  (for [m old-means]
    (if (contains? point-groups m)
      (apply average (get point-groups m)) 
      m)))</pre></div><p>In the <code class="literal">new-means</code> function defined in the preceding code, for each value in the previous mean values, we apply the <code class="literal">average</code> function to the points that are grouped by the mean value. Of course, the <code class="literal">average</code> function must be applied to the points of a given mean only if the mean has any points grouped by it. This is checked using the <code class="literal">contains?</code> function in the <code class="literal">new-means</code> function. We can inspect the value returned by the <code class="literal">new-means</code> function on our sample data in the REPL, as shown in the following output:</p><div><pre class="programlisting">user&gt; (new-means average
        (point-groups guessed-means data distance)
                 guessed-means)
(10/3 55)</pre></div><p>As shown in the preceding output, the new mean values are calculated as <code class="literal">(10/3 55)</code> from the initial mean values <code class="literal">(0 10)</code>. To implement the <em>K</em>-means algorithm, we must apply the <code class="literal">new-means</code> function iteratively over the new mean values returned by it. This iteration can be performed using the <code class="literal">iterate</code> function, which requires a function that takes a single argument to be passed to it. </p><p>We can define a function to use with the <code class="literal">iterate</code> function by currying the <code class="literal">new-means</code> function over the old mean values passed to it, as shown in the following code:</p><div><pre class="programlisting">(defn iterate-means [data distance average]
  (fn [means]
    (new-means average
               (point-groups means data distance)
               means)))</pre></div><p>The <code class="literal">iterate-means</code> function<a id="id626" class="indexterm"/> defined in the preceding code returns a function that calculates the new mean values from a given set of initial mean values, as shown in the following output:</p><div><pre class="programlisting">user&gt; ((iterate-means data distance average) '(0 10))
(10/3 55)
user&gt; ((iterate-means data distance average) '(10/3 55))
(37/6 101)</pre></div><p>As shown in the preceding output, the mean value is observed to change on applying the function returned by the <code class="literal">iterate-means</code> function a couple of times. This returned function can be passed to the <code class="literal">iterate</code> function and we can inspect the iterated mean values using the <code class="literal">take</code> function, as shown in the following code:</p><div><pre class="programlisting">user&gt; (take 4 (iterate (iterate-means data distance average)
                       '(0 10)))
((0 10) (10/3 55) (37/6 101) (37/6 101))</pre></div><p>It's observed that the mean<a id="id627" class="indexterm"/> value changes only in the first three iterations and converges to the value <code class="literal">(37/6 10)</code> for the sample data that we have defined. The termination condition of the <em>K</em>-means algorithm is the convergence of the mean values and thus we must iterate over the values returned by the <code class="literal">iterate-means</code> function until the returned mean value does not differ from the previously returned mean value. Since the <code class="literal">iterate</code> function lazily returns an infinite sequence, we must implement a function that limits this sequence by the convergence of the elements in the sequence. This behavior can be implemented by lazy realization using the <code class="literal">lazy-seq</code> and <code class="literal">seq</code> functions as shown in the following code:</p><div><pre class="programlisting">(defn take-while-unstable
  ([sq] (lazy-seq (if-let [sq (seq sq)]
                    (cons (first sq)
                          (take-while-unstable 
                           (rest sq) (first sq))))))
  ([sq last] (lazy-seq (if-let [sq (seq sq)]
                         (if (= (first sq) last)
                           nil
                           (take-while-unstable sq))))))</pre></div><p>The <code class="literal">take-while-unstable</code> function<a id="id628" class="indexterm"/> defined in the preceding code splits a lazy sequence into its head and tail terms and then compares the first element of the sequence with the first element of the tail of the sequence to return an empty list, or <code class="literal">nil</code>, if the two elements are equal. However, if they are not equal, the <code class="literal">take-while-unstable</code> function is invoked again on the tail of the sequence. Note the use of the <code class="literal">if-let</code> macro, which is simply a <code class="literal">let</code> form with an <code class="literal">if</code> expression as its body to check if the sequence <code class="literal">sq</code> is empty. We can inspect the value returned by the <code class="literal">take-while-unstable</code> function in the REPL as shown in the following output:</p><div><pre class="programlisting">user&gt; (take-while-unstable
       '(1 2 3 4 5 6 7 7 7 7))
(1 2 3 4 5 6 7)
user&gt; (take-while-unstable 
       (iterate (iterate-means data distance average)
                '(0 10)))
((0 10) (10/3 55) (37/6 101))</pre></div><p>Using the final mean value <a id="id629" class="indexterm"/>we have calculated, we can determine the clusters of input values using the <code class="literal">vals</code> function on the map returned by the <code class="literal">point-groups</code> function, as shown in the following code:</p><div><pre class="programlisting">(defn k-cluster [data distance means]
  (vals (point-groups means data distance)))</pre></div><p>Note that the <code class="literal">vals</code> function returns all the values in a given map as a sequence.</p><p>The <code class="literal">k-cluster</code> function defined in the preceding code produces the final clusters of input values returned by the <em>K</em>-means algorithm. We can apply the <code class="literal">k-cluster</code> function on the final mean value <code class="literal">(37/6 101)</code> to return the final clusters of input values, as shown in the following output:</p><div><pre class="programlisting">user&gt; (k-cluster data distance '(37/6 101))
([2 3 5 6 10 11] [100 101 102])</pre></div><p>To visualize the change in the clusters of input values, we can apply the <code class="literal">k-cluster</code> function on the sequence of values returned by composing the <code class="literal">iterate</code> and <code class="literal">iterate-means</code> functions. We must limit this sequence by convergence of the values in all clusters and this can be done using the <code class="literal">take-while-unstable</code> function<a id="id630" class="indexterm"/>, as shown in the following code:</p><div><pre class="programlisting">user&gt; (take-while-unstable
       (map #(k-cluster data distance %)
            (iterate (iterate-means data distance average)
             '(0 10))))
(([2 3 5] [6 10 11 100 101 102])
 ([2 3 5 6 10 11] [100 101 102]))</pre></div><p>We can refactor the preceding expression into a function that requires only the initial set of guessed mean values by binding the <code class="literal">iterate-means</code> function to the sample data. The functions used to calculate the distance of a given input value from a mean value and the average mean value from a set of input values are as shown in the following code:</p><div><pre class="programlisting">(defn k-groups [data distance average]
  (fn [guesses]
    (take-while-unstable
     (map #(k-cluster data distance %)
          (iterate (iterate-means data distance average)
                   guesses)))))</pre></div><p>We can bind the <code class="literal">k-groups</code> function defined in the preceding code with our sample data and the <code class="literal">distance</code> and <code class="literal">average</code> functions, which operate on numeric values as shown in the following code:</p><div><pre class="programlisting">(def grouper
  (k-groups data distance average))</pre></div><p>Now, we can apply the <code class="literal">grouper</code> function<a id="id631" class="indexterm"/> on any arbitrary set of mean values to visualize<a id="id632" class="indexterm"/> the changes in the clusters over the various iterations of the <em>K</em>-means algorithm, as shown in the following code:</p><div><pre class="programlisting">user&gt; (grouper '(0 10))
(([2 3 5] [6 10 11 100 101 102])
 ([2 3 5 6 10 11] [100 101 102]))
user&gt; (grouper '(1 2 3))
(([2] [3 5 6 10 11 100 101 102])
 ([2 3 5 6 10 11] [100 101 102])
 ([2 3] [5 6 10 11] [100 101 102])
 ([2 3 5] [6 10 11] [100 101 102])
 ([2 3 5 6] [10 11] [100 101 102]))
user&gt; (grouper '(0 1 2 3 4))
(([2] [3] [5 6 10 11 100 101 102])
 ([2] [3 5 6 10 11] [100 101 102])
 ([2 3] [5 6 10 11] [100 101 102])
 ([2 3 5] [6 10 11] [100 101 102])
 ([2] [3 5 6] [10 11] [100 101 102])
 ([2 3] [5 6] [10 11] [100 101 102]))</pre></div><p>As we mentioned earlier, if the number of mean values is greater than the number of inputs, we end up with a number of clusters equal to the number of input values, in which each cluster contains a single input value. This can be verified in the REPL using the <code class="literal">grouper</code> function, as shown in the following code:</p><div><pre class="programlisting">user&gt; (grouper (range 200))
(([2] [3] [100] [5] [101] [6] [102] [10] [11]))</pre></div><p>We can extend the preceding implementation to apply to vector values and not just numeric values, by changing the <code class="literal">distance</code> and <code class="literal">average</code> distance, which are parameters to the <code class="literal">k-groups</code> function. We can implement these two functions for vector values as follows:</p><div><pre class="programlisting">(defn vec-distance [a b]
  (reduce + (map #(* % %) (map - a b))))

(defn vec-average [&amp; list]
  (map #(/ % (count list)) (apply map + list)))</pre></div><p>The <code class="literal">vec-distance</code> function<a id="id633" class="indexterm"/> defined in the preceding code implements the squared Euclidean distance between two vector values as the sum of the squared differences between the corresponding elements in the two vectors. We can also calculate the average of some vector values by adding them together and dividing each resulting element by the number <a id="id634" class="indexterm"/>of vectors that were added together, as shown in the <code class="literal">vec-average</code> function defined in the preceding code. We can inspect the returned values of these functions in the REPL as shown in the following output:</p><div><pre class="programlisting">user&gt; (vec-distance [1 2 3] [5 6 7])
48
user&gt; (vec-average  [1 2 3] [5 6 7])
(3 4 5)</pre></div><p>We can now define some of the following vector values to use as sample data for our clustering algorithm:</p><div><pre class="programlisting">(def vector-data
  '([1 2 3] [3 2 1] [100 200 300] [300 200 100] [50 50 50]))</pre></div><p>We can now use the <code class="literal">k-groups</code> function with the <code class="literal">vector-data</code>, <code class="literal">vec-distance</code>, and <code class="literal">vec-average</code> variables to print the various clusters iterated through to produce the final set of clusters, as shown in the following code:</p><div><pre class="programlisting">user&gt; ((k-groups vector-data vec-distance vec-average)
       '([1 1 1] [2 2 2] [3 3 3]))
(([[1 2 3] [3 2 1]] [[100 200 300] [300 200 100] [50 50 50]])

 ([[1 2 3] [3 2 1] [50 50 50]]
  [[100 200 300] [300 200 100]])

 ([[1 2 3] [3 2 1]]
  [[100 200 300] [300 200 100]]
  [[50 50 50]]))</pre></div><p>Another improvement we can add to this implementation is updating identical mean values by the <code class="literal">new-means</code> function. If we pass a list of identical mean values to the <code class="literal">new-means</code> function, both the mean values will get updated. However, in the classic <em>K</em>-means algorithm, only one mean from two identical mean values is updated. This behavior can be verified in the REPL, by passing a list of identical means such as <code class="literal">'(0 0)</code> to the <code class="literal">new-means</code> function, as shown in the following code:</p><div><pre class="programlisting">user&gt; (new-means average 
                 (point-groups '(0 0) '(0 1 2 3 4) distance) 
                 '(0 0))
(2 2)</pre></div><p>We can avoid this problem by checking the number of occurrences of a given mean in the set of mean values and only updating a single mean value if multiple occurrences of it are found. We can implement this using the <code class="literal">frequencies</code> function<a id="id635" class="indexterm"/>, which returns a map with keys as elements from the original collection passed to the <code class="literal">frequencies</code> function and values as the frequencies of occurrences of these elements. We can thus redefine the <code class="literal">new-means</code> function, as shown in the following code:</p><div><pre class="programlisting">(defn update-seq [sq f]
  (let [freqs (frequencies sq)]
    (apply concat
     (for [[k v] freqs]
       (if (= v 1) 
         (list (f k))
         (cons (f k) (repeat (dec v) k)))))))
(defn new-means [average point-groups old-means]
  (update-seq
   old-means
   (fn [o]
     (if (contains? point-groups o)
       (apply average (get point-groups o)) o))))</pre></div><p>The <code class="literal">update-seq</code> function defined<a id="id636" class="indexterm"/> in the preceding code applies a function <code class="literal">f</code> to the elements in a sequence <code class="literal">sq</code>. The function <code class="literal">f</code> is only applied to a single element if the element is repeated in the sequence. We can now observe that only a single mean value changes when we apply the redefined <code class="literal">new-means</code> function<a id="id637" class="indexterm"/> on the sequence of identical means <code class="literal">'(0 0)</code>, as shown in the following output:</p><div><pre class="programlisting">user&gt; (new-means average
                 (point-groups '(0 0) '(0 1 2 3 4) distance)
                 '(0 0))
(2 0)</pre></div><p>A consequence of the preceding redefinition of the <code class="literal">new-means</code> function is that the <code class="literal">k-groups</code> function now produces identical clusters when applied to both distinct and identical initial mean values, such as <code class="literal">'(0 1)</code> and <code class="literal">'(0 0)</code>, as shown in the following code:</p><div><pre class="programlisting">user&gt; ((k-groups '(0 1 2 3 4) distance average)
       '(0 1))
(([0] [1 2 3 4]) ([0 1] [2 3 4]))
user&gt; ((k-groups '(0 1 2 3 4) distance average)
       '(0 0))
(([0 1 2 3 4]) ([0] [1 2 3 4]) ([0 1] [2 3 4]))</pre></div><p>This new behavior of the <code class="literal">new-means</code> function with respect to identical initial mean values also extends to vector values as shown in the following output:</p><div><pre class="programlisting">user&gt; ((k-groups vector-data vec-distance vec-average)
       '([1 1 1] [1 1 1] [1 1 1]))
(([[1 2 3] [3 2 1] [100 200 300] [300 200 100] [50 50 50]])
 ([[1 2 3] [3 2 1]] [[100 200 300] [300 200 100] [50 50 50]])
 ([[1 2 3] [3 2 1] [50 50 50]] [[100 200 300] [300 200 100]])
 ([[1 2 3] [3 2 1]] [[100 200 300] [300 200 100]] [[50 50 50]]))</pre></div><p>In conclusion, the <code class="literal">k-cluster</code> and <code class="literal">k-groups</code> functions defined <a id="id638" class="indexterm"/>in the preceding example depict how <em>K</em>-means clustering can be implemented in idiomatic Clojure.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec14"/>Clustering data using clj-ml</h2></div></div></div><p>The <code class="literal">clj-ml</code> library<a id="id639" class="indexterm"/> provides several implementations of clustering algorithms <a id="id640" class="indexterm"/>derived from the Java Weka library. We will now demonstrate how we can use the <code class="literal">clj-ml</code> library to build a <em>K</em>-means clusterer.</p><div><div><h3 class="title"><a id="note40"/>Note</h3><p>The <code class="literal">clj-ml</code> and Incanter libraries can be added to a Leiningen project by adding the following dependency to the <code class="literal">project.clj</code> file:</p><div><pre class="programlisting">
<strong>[cc.artifice/clj-ml "0.4.0"]</strong>
<strong>[incanter "1.5.4"]</strong>
</pre></div><p>For the example that will follow, the namespace declaration should look similar to the following declaration:</p><div><pre class="programlisting">
<strong>(ns my-namespace</strong>
<strong>  (:use [incanter core datasets]</strong>
<strong>        [clj-ml data clusterers]))</strong>
</pre></div></div></div><p>For the examples that use the <code class="literal">clj-ml library</code> in this chapter, we will use the <strong>Iris</strong> dataset from the Incanter library as our training data. This dataset is essentially a sample of 150 flowers and four feature variables that are measured for these samples. The features of the flowers that are measured in the Iris dataset are the width and length of petal and sepal of the flowers. The sample values are distributed over three species or categories, namely Virginica, Setosa, and Versicolor. The data is available as a <img src="img/4351OS_07_07.jpg" alt="Clustering data using clj-ml"/> sized matrix in which the species of a given flower is represented as the last column in this matrix.</p><p>We can select the features from the Iris dataset as a vector using the <code class="literal">get-dataset</code>, <code class="literal">sel</code>, and <code class="literal">to-vector</code> functions from the Incanter library, as shown in the following code. We can then convert this vector into a <code class="literal">clj-ml</code> dataset using the <code class="literal">make-dataset</code> function from the <code class="literal">clj-ml</code> library. This is done by passing the keyword names of the feature values as a template to the <code class="literal">make-dataset</code> function as shown in the following code:</p><div><pre class="programlisting">(def features [:Sepal.Length
               :Sepal.Width
               :Petal.Length
               :Petal.Width])

(def iris-data (to-vect (sel (get-dataset :iris)
                             :cols features)))

(def iris-dataset
  (make-dataset "iris" features iris-data))</pre></div><p>We can print the <code class="literal">iris-dataset</code> variable defined<a id="id641" class="indexterm"/> in the preceding code in the REPL to give us some information on <a id="id642" class="indexterm"/>what it contains as shown in the following code and output:</p><div><pre class="programlisting">user&gt; iris-dataset
#&lt;ClojureInstances @relation iris

@attribute Sepal.Length numeric
@attribute Sepal.Width numeric
@attribute Petal.Length numeric
@attribute Petal.Width numeric

@data
5.1,3.5,1.4,0.2
4.9,3,1.4,0.2
4.7,3.2,1.3,0.2
...
4.7,3.2,1.3,0.2
6.2,3.4,5.4,2.3
5.9,3,5.1,1.8&gt;</pre></div><p>We can create a clusterer using the <code class="literal">make-clusterer</code> function from the <code class="literal">clj-ml.clusterers</code> namespace. We can specify the type of cluster to create as the first argument to the <code class="literal">make-cluster</code> function. The second optional argument is a map of options to be used to create the specified clusterer. We can train a given clusterer using the <code class="literal">cluster-build</code> function from the <code class="literal">clj-ml</code> library. In the following code, we create a new <em>K</em>-means clusterer using the <code class="literal">make-clusterer</code> function with the <code class="literal">:k-means</code> keyword and define a simple helper function to help train this clusterer with any given dataset:</p><div><pre class="programlisting">(def k-means-clusterer
  (make-clusterer :k-means
                  {:number-clusters 3}))

(defn train-clusterer [clusterer dataset]
  (clusterer-build clusterer dataset)
  clusterer)</pre></div><p>The <code class="literal">train-clusterer</code> function can be applied to the clusterer instance defined by the <code class="literal">k-means-clusterer</code> variable<a id="id643" class="indexterm"/> and<a id="id644" class="indexterm"/> the sample data represented by the <code class="literal">iris-dataset</code> variable, as shown in the following code and output:</p><div><pre class="programlisting">user&gt; (train-clusterer k-means-clusterer iris-dataset)
#&lt;SimpleKMeans
kMeans
======

Number of iterations: 6
Within cluster sum of squared errors: 6.982216473785234
Missing values globally replaced with mean/mode

Cluster centroids:
                            Cluster#
Attribute       Full Data          0          1          2
                    (150)       (61)       (50)       (39)
==========================================================
Sepal.Length       5.8433     5.8885      5.006     6.8462
Sepal.Width        3.0573     2.7377      3.428     3.0821
Petal.Length        3.758     4.3967      1.462     5.7026
Petal.Width        1.1993      1.418      0.246     2.0795</pre></div><p>As shown in the preceding<a id="id645" class="indexterm"/> output, the trained clusterer contains <code class="literal">61</code> values in the first cluster (cluster <code class="literal">0</code>), <code class="literal">50</code> values in the second cluster (cluster <code class="literal">1</code>), and <code class="literal">39</code> values in the third cluster (cluster <code class="literal">2</code>). The preceding output also gives us some information about the mean values of the individual features in the training data. We can now predict the classes of the input data using the trained clusterer and the <code class="literal">clusterer-cluster</code> function<a id="id646" class="indexterm"/> as shown in the following code:</p><div><pre class="programlisting">user&gt; (clusterer-cluster k-means-clusterer iris-dataset)
#&lt;ClojureInstances @relation 'clustered iris'

@attribute Sepal.Length numeric
@attribute Sepal.Width numeric
@attribute Petal.Length numeric
@attribute Petal.Width numeric
@attribute class {0,1,2}

@data
5.1,3.5,1.4,0.2,1
4.9,3,1.4,0.2,1
4.7,3.2,1.3,0.2,1
...
6.5,3,5.2,2,2
6.2,3.4,5.4,2.3,2
5.9,3,5.1,1.8,0&gt;</pre></div><p>The <code class="literal">clusterer-cluster</code> function<a id="id647" class="indexterm"/> uses the trained clusterer to return a new dataset that contains an additional fifth attribute that represents the category of a given sample value. As shown in the preceding code, this new attribute has the values <code class="literal">0</code>, <code class="literal">1</code>, and <code class="literal">2</code>, and the sample values also contain <a id="id648" class="indexterm"/>valid values for this new feature. In<a id="id649" class="indexterm"/> conclusion, the <code class="literal">clj-ml</code> library provides a good framework for working with clustering algorithms. In the preceding example, we created a <em>K</em>-means clusterer using the <code class="literal">clj-ml</code> library.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec47"/>Using hierarchical clustering</h1></div></div></div><p>
<strong>Hierarchical clustering</strong><a id="id650" class="indexterm"/> is another method of cluster analysis in which input values from the training data are grouped together into a hierarchy. The process of creating the hierarchy can be done in a top-down approach, where in all observations are first part of a single cluster and are then divided into smaller clusters. Alternatively, we can group the input values using a bottom-up methodology, where each cluster is initially a sample value from the training data and these clusters are then combined together. The former top-down approach is termed as <strong>divisive clustering</strong><a id="id651" class="indexterm"/> and the later bottom-up method is called <strong>agglomerative clustering</strong><a id="id652" class="indexterm"/>. </p><p>Thus, in agglomerative clustering, we combine clusters into larger clusters, whereas we divide clusters into smaller ones in divisive clustering. In terms of performance, modern implementations of agglomerative clustering algorithms have a time complexity of <img src="img/4351OS_07_08.jpg" alt="Using hierarchical clustering"/>, while those of divisive clustering have much higher complexity of <img src="img/4351OS_07_09.jpg" alt="Using hierarchical clustering"/>.</p><p>Suppose we have six input values in our training data. In the following illustration, assume that these input values are positioned according to some two-dimensional metric to measure the overall value of a given input value:</p><div><img src="img/4351OS_07_10.jpg" alt="Using hierarchical clustering"/></div><p>We can apply agglomerative clustering on these input values to produce the following hierarchy of clusters:</p><div><img src="img/4351OS_07_11.jpg" alt="Using hierarchical clustering"/></div><p>The values <em>b</em> and <em>c</em> are observed to be the closest to each other in the spatial distribution and are hence grouped into a cluster. Similarly, the nodes <em>d</em> and <em>e</em> are also grouped into another cluster. The final result of hierarchically clustering the input value is a single binary tree or a <strong>dendogram</strong><a id="id653" class="indexterm"/> of the sample values. In effect, clusters such as <em>bc</em> and <em>def</em> are added to the hierarchy as binary subtrees of values or of other clusters. Although this process tends to appear very simple in a two-dimensional space, the solution to the problem of determining the distance and hierarchy between input values is much less trivial when applied over several dimensions of features.</p><p>In both agglomerative and<a id="id654" class="indexterm"/> divisive clustering techniques, the similarity between input values from the sample data has to be calculated. This can be done by measuring the distance between two sets of input values, grouping them into clusters using the calculated distance, and then determining the linkage or similarity between two clusters of input values.</p><p>The choice of the distance metric in a hierarchical clustering algorithm will determine the shape of the clusters that are produced by the algorithm. A couple of commonly used measures of the distance between two input vectors <img src="img/4351OS_07_12.jpg" alt="Using hierarchical clustering"/> and <img src="img/4351OS_07_13.jpg" alt="Using hierarchical clustering"/> are the Euclidean distance <img src="img/4351OS_07_14.jpg" alt="Using hierarchical clustering"/> and the squared Euclidean distance <img src="img/4351OS_07_15.jpg" alt="Using hierarchical clustering"/>, which can be formally expressed as follows:</p><div><img src="img/4351OS_07_16.jpg" alt="Using hierarchical clustering"/></div><p>Another commonly used metric of the distance between input values is the maximum distance <img src="img/4351OS_07_17.jpg" alt="Using hierarchical clustering"/>, which<a id="id655" class="indexterm"/> calculates the maximum absolute difference of corresponding elements in two given vectors. This function can be expressed as follows:</p><div><img src="img/4351OS_07_18.jpg" alt="Using hierarchical clustering"/></div><p>The second aspect of a hierarchical clustering algorithm is the linkage criteria<a id="id656" class="indexterm"/>, which is an effective measure of similarity or dissimilarity between two clusters of input values. Two commonly used methods of determining the linkage between two input values are <strong>complete linkage clustering</strong><a id="id657" class="indexterm"/> and <strong>single linkage clustering</strong><a id="id658" class="indexterm"/>. Both of these methods are forms of agglomerative clustering.</p><p>In agglomerative clustering,<a id="id659" class="indexterm"/> two input values or clusters with the shortest distance metric are combined into a new cluster. Of course, the definition of "shortest distance" is what is unique in any agglomerative clustering technique. In complete linkage clustering, input values farthest from each other are used to determine the grouping. Hence, this method is also termed as <strong>farthest neighbor clustering</strong><a id="id660" class="indexterm"/>. This metric of the distance <img src="img/4351OS_07_19.jpg" alt="Using hierarchical clustering"/> between two values can be formally expressed as follows:</p><div><img src="img/4351OS_07_20.jpg" alt="Using hierarchical clustering"/></div><p>In the preceding equation, the function <img src="img/4351OS_07_21.jpg" alt="Using hierarchical clustering"/> is the selected metric of distance between two input vectors. Complete linkage clustering will essentially group together values or clusters that have the maximum value of the distance metric <img src="img/4351OS_07_22.jpg" alt="Using hierarchical clustering"/>. This operation of grouping together clusters is repeated until a single cluster is produced.</p><p>In single linkage clustering, values that are nearest to each other are grouped together. Hence, single linkage clustering is also called <strong>nearest neighbor clustering</strong>. This can be be formally stated using the following expression:</p><div><img src="img/4351OS_07_23.jpg" alt="Using hierarchical clustering"/></div><p>Another popular hierarchical clustering technique is the<a id="id661" class="indexterm"/> <strong>Cobweb algorithm</strong><a id="id662" class="indexterm"/>. This algorithm is a form of <strong>conceptual clustering</strong><a id="id663" class="indexterm"/>, in which a concept is created for each cluster produced by the clustering method used. By the term "concept", we mean a concise formal description of the data clustered together. Interestingly, conceptual clustering is closely related to decision tree learning, which we already discussed in <a class="link" href="ch03.html" title="Chapter 3. Categorizing Data">Chapter 3</a>, <em>Categorizing Data</em>. The Cobweb algorithm groups all clusters into a <strong>classification tree</strong>, in which each node contains a formal summary of the values or clusters that are its child nodes. This information can then be used to determine and predict the category of an input value with some missing features. In this sense, this technique can be used when some of the samples in the test data have missing or unknown features.</p><p>We now demonstrate a simple implementation <a id="id664" class="indexterm"/>of hierarchical clustering. In this implementation, we take a slightly different approach where we embed part of the required functionality into the standard vector data structure provided by the Clojure language.</p><div><div><h3 class="title"><a id="note42"/>Note</h3><p>For the upcoming example, we require the <code class="literal">clojure.math.numeric-tower</code> library that can be added to a Leiningen project by adding the following dependency to the <code class="literal">project.clj</code> file:</p><div><pre class="programlisting">[org.clojure/math.numeric-tower "0.0.4"]</pre></div><p>The namespace declaration for the example should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:use [clojure.math.numeric-tower :only [sqrt]]))</pre></div></div></div><p>For this implementation, we will use the Euclidean distance between two points as a distance metric. We can calculate this distance from the sum of squares of the elements in an input vector, which can be computed using a composition of the <code class="literal">reduce</code> and <code class="literal">map</code> functions as follows:</p><div><pre class="programlisting">(defn sum-of-squares [coll]
  (reduce + (map * coll coll)))</pre></div><p>The <code class="literal">sum-of-squares</code> function<a id="id665" class="indexterm"/> defined in the preceding code will be used to determine the distance metric. We will define<a id="id666" class="indexterm"/> two protocols that abstract the operations we perform on a particular data type. From an engineering perspective, these two protocols could be combined into a single protocol, since both the protocols will be used in combination. </p><p>However, we use the following two protocols for this example for the sake of clarity:</p><div><pre class="programlisting">(defprotocol Each
  (each [v op w]))

(defprotocol Distance
  (distance [v w]))</pre></div><p>The <code class="literal">each</code> function defined in the <code class="literal">Each</code> protocol applies a given operation <code class="literal">op</code> on corresponding elements in two collections <code class="literal">v</code> and <code class="literal">w</code>. The <code class="literal">each</code> function is quite similar to the standard <code class="literal">map</code> function, but <code class="literal">each</code> allows the data type of <code class="literal">v</code> to decide how to apply the function <code class="literal">op</code>. The <code class="literal">distance</code> function defined in the <code class="literal">Distance</code> protocol calculates the distance between any two collections <code class="literal">v</code> and <code class="literal">w</code>. Note that we use the generic term "collection" since we are dealing with abstract protocols and not concrete implementations of the functions of these protocols. For this example, we will implement the preceding protocols as part of the vector data type. Of course, these protocols could also be extended to other data types such as sets and maps.</p><p>In this example, we will implement single linkage clustering as the linkage criteria. First, we will have to define a function to determine the two closest vectors from a set of vector values. To do this, we can apply the <code class="literal">min-key</code> function, which returns the key with the least associated value in a collection, on a vector. Interestingly, this is possible in Clojure since we can treat a vector as a map with the index values of the various elements in the vector as its keys. We will implement this with the help of the following code:</p><div><pre class="programlisting">(defn closest-vectors [vs]
  (let [index-range (range (count vs))]
    (apply min-key
           (fn [[x y]] (distance (vs x) (vs y)))
           (for [i index-range
                 j (filter #(not= i %) index-range)]
             [i j]))))</pre></div><p>The <code class="literal">closest-vectors</code> function<a id="id667" class="indexterm"/> defined in the preceding code determines all possible combinations of the indexes of the vector <code class="literal">vs</code> using the <code class="literal">for</code> form. Note that the vector <code class="literal">vs</code> is a vector of vectors. The <code class="literal">distance</code> function is then applied over the values of the possible index combinations<a id="id668" class="indexterm"/> and these distances are then compared using the <code class="literal">min-key</code> function. The function finally returns the index values of the two inner vector values that have the least distance from each other, thus implementing single linkage clustering.</p><p>We will also need to calculate the mean value of two vectors that have to be clustered together. We can implement this using the <code class="literal">each</code> function we had previously defined in the <code class="literal">Each</code> protocol and the <code class="literal">reduce</code> function, as follows:</p><div><pre class="programlisting">(defn centroid [&amp; xs]
  (each
   (reduce #(each %1 + %2) xs)
   *
   (double (/ 1 (count xs)))))</pre></div><p>The <code class="literal">centroid</code> function<a id="id669" class="indexterm"/> defined in the preceding code will calculate the mean value of a sequence of vector values. Note the use of the <code class="literal">double</code> function to ensure that the value returned by the <code class="literal">centroid</code> function is a double-precision number.</p><p>We now implement the <code class="literal">Each</code> and <code class="literal">Distance</code> protocols as part of the vector data type, which is fully qualified as <code class="literal">clojure.lang.PersistentVector</code>. This is done using the <code class="literal">extend-type</code> function<a id="id670" class="indexterm"/> as follows:</p><div><pre class="programlisting">(extend-type clojure.lang.PersistentVector
  Each
  (each [v op w]
    (vec
     (cond
      (number? w) (map op v (repeat w))
      (vector? w) (if (&gt;= (count v) (count w))
                    (map op v (lazy-cat w (repeat 0)))
                    (map op (lazy-cat v (repeat 0)) w)))))
  Distance 
  ;; implemented as Euclidean distance
  (distance [v w] (-&gt; (each v - w)
                      sum-of-squares
                      sqrt)))</pre></div><p>The <code class="literal">each</code> function is implemented such that it applies the <code class="literal">op</code> operation to each element in the <code class="literal">v</code> vector and a second argument <code class="literal">w</code>. The <code class="literal">w</code> parameter could be either a vector or a number. In case <code class="literal">w</code> is a number, we simply map the function <code class="literal">op</code> over <code class="literal">v</code> and the repeated value of the number <code class="literal">w</code>. If <code class="literal">w</code> is a vector, we pad the smaller vector with <code class="literal">0</code> values using the <code class="literal">lazy-cat</code> function and map <code class="literal">op</code> over the two vectors. Also, we wrap the entire expression in a <code class="literal">vec</code> function to ensure that the value returned is always a vector.</p><p>The <code class="literal">distance</code> function is<a id="id671" class="indexterm"/> implemented as the Euclidean distance between two vector values <code class="literal">v</code> and <code class="literal">w</code> using the <code class="literal">sum-of-squares</code> function that we previously defined and the <code class="literal">sqrt</code> function from the <code class="literal">clojure.math.numeric-tower</code> namespace.</p><p>We have all the pieces needed to implement a function that performs hierarchical clustering on vector values. We can implement hierarchical clustering primarily using the centroid and <code class="literal">closest-vectors</code> functions that we had previously defined, as follows:</p><div><pre class="programlisting">(defn h-cluster
  "Performs hierarchical clustering on a
  sequence of maps of the form { :vec [1 2 3] } ."
  [nodes]
  (loop [nodes nodes]
    (if (&lt; (count nodes) 2)
      nodes
      (let [vectors    (vec (map :vec nodes))
            [l r]      (closest-vectors vectors)
            node-range (range (count nodes))
            new-nodes  (vec
                        (for [i node-range
                              :when (and (not= i l)
                                         (not= i r))]
                          (nodes i)))]
        (recur (conj new-nodes
                     {:left (nodes l) :right (nodes r)
                      :vec (centroid
                            (:vec (nodes l))
                            (:vec (nodes r)))}))))))</pre></div><p>We can pass a vector of maps to the <code class="literal">h-cluster</code> function defined in the preceding code. Each map in this vector contains a vector as the value of the keyword <code class="literal">:vec</code>. The <code class="literal">h-cluster</code> function combines all the vector values from the <code class="literal">:vec</code> keywords in these maps and determines the two closest vectors using the <code class="literal">closest-vectors</code> function<a id="id672" class="indexterm"/>. Since the value returned by the <code class="literal">closest-vectors</code> function is a vector of two index values, we determine all the vectors with indexes other than the two index values returned by the <code class="literal">closest-vectors</code> function. This is done using a special form of the <code class="literal">for</code> macro that allows a conditional clause to be specified with the <code class="literal">:when</code> key parameter. The mean value of the two closest vectors is then calculated using the <code class="literal">centroid</code> function. A new map is<a id="id673" class="indexterm"/> created using the mean value and then added to the original vector to replace the two closest vector values. The process is repeated until the vector contains a single cluster, using the <code class="literal">loop</code> form. We can inspect the behavior of the <code class="literal">h-cluster</code> function in the REPL as shown in the following code:</p><div><pre class="programlisting">user&gt; (h-cluster [{:vec [1 2 3]} {:vec [3 4 5]} {:vec [7 9 9]}])
[{:left {:vec [7 9 9]},
  :right {:left {:vec [1 2 3]},
          :right {:vec [3 4 5]},
          :vec [2.0 3.0 4.0]},
  :vec [4.5 6.0 6.5] }]</pre></div><p>When applied to three vector values <code class="literal">[1 2 3]</code>, <code class="literal">[3 4 5]</code>, and <code class="literal">[7 9 9]</code>, as shown in the preceding code, the <code class="literal">h-cluster</code> function groups the vectors <code class="literal">[1 2 3]</code> and <code class="literal">[3 4 5]</code> into a single cluster. This cluster has the mean value of <code class="literal">[2.0 3.0 4.0]</code>, which is calculated from the vectors <code class="literal">[1 2 3]</code> and <code class="literal">[3 4 5]</code>. This new cluster is then grouped with the vector <code class="literal">[7 9 9]</code> in the next iteration,<a id="id674" class="indexterm"/> thus producing a single cluster with a mean value of <code class="literal">[4.5 6.0 6.5]</code>. In conclusion, the <code class="literal">h-cluster</code> function can be used to hierarchically cluster vector values into a single hierarchy.</p><p>The <code class="literal">clj-ml</code> library provides an implementation of the Cobweb hierarchical clustering algorithm. We can instantiate such a clusterer using the <code class="literal">make-clusterer</code> function with the <code class="literal">:cobweb</code> argument.</p><div><pre class="programlisting">(def h-clusterer (make-clusterer :cobweb))</pre></div><p>The clusterer defined by the <code class="literal">h-clusterer</code> variable<a id="id675" class="indexterm"/> shown in the preceding code can be trained using the <code class="literal">train-clusterer</code> function and <code class="literal">iris-dataset</code> dataset, which we had previously defined, as follows: The <code class="literal">train-clusterer</code> function and <code class="literal">iris-dataset</code> can be implemented as shown in the following code:</p><div><pre class="programlisting">user&gt; (train-clusterer h-clusterer iris-dataset)
#&lt;Cobweb Number of merges: 0
Number of splits: 0
Number of clusters: 3

node 0 [150]
|   leaf 1 [96]
node 0 [150]
|   leaf 2 [54]</pre></div><p>As shown in the preceding REPL output, the Cobweb clustering algorithm partitions the input data into two clusters. One cluster has 96 samples and the other cluster has 54 samples, which is quite a different result compared to the <em>K</em>-means clusterer, we had previously used. In summary, the <code class="literal">clj-ml</code> library provides an easy-to-use implementation of the Cobweb clustering algorithm.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec48"/>Using Expectation-Maximization</h1></div></div></div><p>The <strong>Expectation-Maximization</strong> (<strong>EM</strong>) algorithm<a id="id676" class="indexterm"/> is a probabilistic approach for determining a clustering model that fits the supplied training data. This algorithm determines the <strong>Maximum Likelihood Estimate</strong> (<strong>MLE</strong>)<a id="id677" class="indexterm"/> of the parameters of a formulated clustering model (for more information, refer to <em>Maximum likelihood theory and applications for distributions generated when observing a function of an exponential family variable</em>).</p><p>Suppose we want to determine <a id="id678" class="indexterm"/>the probability of a coin toss being a head or a tail. If we flip the coin <img src="img/4351OS_07_24.jpg" alt="Using Expectation-Maximization"/> times, we end up with <img src="img/4351OS_07_25.jpg" alt="Using Expectation-Maximization"/> occurrences of heads and <img src="img/4351OS_07_26.jpg" alt="Using Expectation-Maximization"/> occurrences of tails. We can estimate the actual probability of occurrence of a head <img src="img/4351OS_07_27.jpg" alt="Using Expectation-Maximization"/> as the ratio of the number of occurrences of a head to the total number of coin tosses performed, using the following equation:</p><div><img src="img/4351OS_07_28.jpg" alt="Using Expectation-Maximization"/></div><p>The probability <img src="img/4351OS_07_29.jpg" alt="Using Expectation-Maximization"/> defined in the preceding equation is the MLE of the probability <img src="img/4351OS_07_30.jpg" alt="Using Expectation-Maximization"/>. In the context of machine learning, the MLE can be maximized to determine the probability of occurrence of a given class or category. However, this estimated probability may not be statistically distributed in a well-defined way over the available training data, which makes it hard to determine the MLE efficiently. The problem is simplified by introducing a set of hidden values to account for the unobserved values in the training data. The hidden values are not directly measured from the data, but are determined from factors that influence the data. The likelihood function of the parameters <img src="img/4351OS_07_31.jpg" alt="Using Expectation-Maximization"/> for a given set of observed values <img src="img/4351OS_07_12.jpg" alt="Using Expectation-Maximization"/> and a set of hidden values <img src="img/4351OS_07_32.jpg" alt="Using Expectation-Maximization"/> is defined as the probability of occurrence of <img src="img/4351OS_07_12.jpg" alt="Using Expectation-Maximization"/> and <img src="img/4351OS_07_32.jpg" alt="Using Expectation-Maximization"/> for a given set of parameters <img src="img/4351OS_07_31.jpg" alt="Using Expectation-Maximization"/>. The likelihood is mathematically written as <img src="img/4351OS_07_33.jpg" alt="Using Expectation-Maximization"/>, and can be expressed as follows:</p><div><img src="img/4351OS_07_34.jpg" alt="Using Expectation-Maximization"/></div><p>The EM algorithm comprises two steps—the expectation step<a id="id679" class="indexterm"/> and the maximization step. In the expectation step, we calculate the expected value<a id="id680" class="indexterm"/> of the <strong>log likelihood</strong> function. This step determines a metric <img src="img/4351OS_07_35.jpg" alt="Using Expectation-Maximization"/>, which must be maximized in the next step, that is, the maximization step<a id="id681" class="indexterm"/> of the algorithm. These two steps can be formally summarized as follows:</p><div><img src="img/4351OS_07_36.jpg" alt="Using Expectation-Maximization"/></div><p>In the preceding equation, the value of <img src="img/4351OS_07_31.jpg" alt="Using Expectation-Maximization"/> that maximizes the value of the function <em>Q</em> is iteratively calculated until it converges to a particular value. The term <img src="img/4351OS_07_37.jpg" alt="Using Expectation-Maximization"/> represents the estimated parameters in the <img src="img/4351OS_07_38.jpg" alt="Using Expectation-Maximization"/> iteration of the algorithm. Also, the term <img src="img/4351OS_07_39.jpg" alt="Using Expectation-Maximization"/> is the expected value of the log likelihood function.</p><p>The <code class="literal">clj-ml</code> library also provides an EM clusterer. We can create an EM clusterer using the <code class="literal">make-clusterer</code> function<a id="id682" class="indexterm"/> with the <code class="literal">:expectation-maximization</code> keyword as its argument, as shown in the following code:</p><div><pre class="programlisting">(def em-clusterer (make-clusterer :expectation-maximization
                                  {:number-clusters 3}))</pre></div><p>Note that we must also specify the number of clusters to produce as an option to the <code class="literal">make-clusterer</code> function.</p><p>We can train the clusterer defined<a id="id683" class="indexterm"/> by the <code class="literal">em-clusterer </code>variable in the preceding code using the <code class="literal">train-clusterer</code> function<a id="id684" class="indexterm"/> and <code class="literal">iris-dataset</code> dataset, which we had previously defined, as follows:</p><div><pre class="programlisting">user&gt; (train-clusterer em-clusterer iris-dataset)
#&lt;EM
EM
==

Number of clusters: 3


               Cluster
Attribute            0       1       2
                (0.41)  (0.25)  (0.33)
=======================================
Sepal.Length
  mean           5.9275  6.8085   5.006
  std. dev.      0.4817  0.5339  0.3489

Sepal.Width
  mean           2.7503  3.0709   3.428
  std. dev.      0.2956  0.2867  0.3753

Petal.Length
  mean           4.4057  5.7233   1.462
  std. dev.      0.5254  0.4991  0.1719

Petal.Width
  mean           1.4131  2.1055   0.246
  std. dev.      0.2627  0.2456  0.1043</pre></div><p>As shown in the preceding output, the EM clusterer partitions the given dataset into three clusters in which the clusters<a id="id685" class="indexterm"/> are distributed as approximately 41 percent, 25 percent, and 35 percent of the samples in the training data.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec49"/>Using SOMs</h1></div></div></div><p>As we mentioned<a id="id686" class="indexterm"/> earlier in <a class="link" href="ch04.html" title="Chapter 4. Building Neural Networks">Chapter 4</a>, <em>Building Neural Networks</em>, SOMs can be used to model unsupervised machine learning problems such as clustering (for more information, refer to <em>Self-organizing Maps as Substitutes for K-Means Clustering</em>). To quickly recap, an SOM is a type of ANN that maps input values with a high number of dimensions to <a id="id687" class="indexterm"/>a low-dimensional output space. This mapping preserves patterns and topological relations between the input values. The neurons in the output space of an SOM will have higher activation values for input values that are spatially close to each other. Thus, SOMs are a good solution for clustering input data with a large number of dimensions.</p><p>The Incanter library<a id="id688" class="indexterm"/> provides a concise SOM implementation that we can use to cluster the input variables from the Iris dataset. We will demonstrate how we can use this SOM implementation for clustering in the example that will follow.</p><div><div><h3 class="title"><a id="note44"/>Note</h3><p>The Incanter library can be added to a Leiningen project by adding the following dependency to the <code class="literal">project.clj</code> file:</p><div><pre class="programlisting">[incanter "1.5.4"]</pre></div><p>For the upcoming example, the namespace declaration should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter core som stats charts datasets]))</pre></div></div></div><p>We first define the sample data to cluster using the <code class="literal">get-dataset</code>, <code class="literal">sel</code>, and <code class="literal">to-matrix</code> functions from the Incanter library as follows:</p><div><pre class="programlisting">(def iris-features (to-matrix (sel (get-dataset :iris)
                                   :cols [:Sepal.Length
                                          :Sepal.Width
                                          :Petal.Length
                                          :Petal.Width])))</pre></div><p>The <code class="literal">iris-features</code> variable<a id="id689" class="indexterm"/> defined in the preceding code is in fact a <img src="img/4351OS_07_40.jpg" alt="Using SOMs"/> sized matrix that represents the values of the four input variables that we have selected from the Iris dataset. Now, we can use the <code class="literal">som-batch-train</code> function<a id="id690" class="indexterm"/> from the <code class="literal">incanter.som</code> namespace to create and train an SOM using these selected features, as follows:</p><div><pre class="programlisting">(def som (som-batch-train
          iris-features :cycles 10))</pre></div><p>The <code class="literal">som</code> variable defined is actually a map with several key-value pairs. The <code class="literal">:dims</code> key in this map contains a vector that represents the dimensions in the lattice of neurons in the trained SOM, as shown in the following code and output:</p><div><pre class="programlisting">user&gt; (:dims som)
[10.0 2.0]</pre></div><p>Thus, we can say that the neural lattice of the trained SOM is a <img src="img/4351OS_07_41.jpg" alt="Using SOMs"/> matrix. The <code class="literal">:sets</code> key of the map represented by the <code class="literal">som</code> variable gives us the positional grouping of the various indexes of the input values in the lattice of neurons of the SOM, as shown in the following output:</p><div><pre class="programlisting">user&gt; (:sets som)
{[4 1] (144 143 141 ... 102 100),
 [8 1] (149 148 147 ... 50),
 [9 0] (49 48 47 46 ... 0)}</pre></div><p>As shown in the preceding REPL output, the input data is partitioned into three clusters. We can calculate the mean <a id="id691" class="indexterm"/>values of each feature using the <code class="literal">mean</code> function from the <code class="literal">incanter.stats</code> namespace as follows:</p><div><pre class="programlisting">(def feature-mean
  (map #(map mean (trans
                   (sel iris-features :rows ((:sets som) %))))
       (keys (:sets som))))</pre></div><p>We can implement a function to plot these mean values using the <code class="literal">xy-plot</code>, <code class="literal">add-lines</code>, and <code class="literal">view</code> functions from the Incanter library as follows:</p><div><pre class="programlisting">(defn plot-means []
  (let [x (range (ncol iris-features))
        cluster-name #(str "Cluster " %)]
    (-&gt; (xy-plot x (nth feature-mean 0)
                 :x-label "Feature"
                 :y-label "Mean value of feature"
                 :legend true
                 :series-label (cluster-name 0))
        (add-lines x (nth feature-mean 1)
                   :series-label (cluster-name 1))
        (add-lines x (nth feature-mean 2)
                   :series-label (cluster-name 2))
        view)))</pre></div><p>The following linear plot is produced on calling the <code class="literal">plot-means</code> function<a id="id692" class="indexterm"/> defined in the preceding code:</p><div><img src="img/4351OS_07_42.jpg" alt="Using SOMs"/></div><p>The preceding plot gives us an idea of the mean values of the various features in the three clusters determined by the SOM. The plot shows that two of the clusters (<em>Cluster 0</em> and <em>Cluster 1</em>) have similar features. The third cluster, however, has significantly different mean values for these set of features and is thus shown as a different shape in the plot. Of course, this plot doesn't <a id="id693" class="indexterm"/>give us much information about the distribution or variance of input values around these mean values. To visualize these features, we need to somehow transform the number of dimensions of the input data to two or three dimensions, which can be easily visualized. We will discuss more on this concept of reducing the number of features in the training data in the next section of this chapter.</p><p>We can also print the clusters and the actual categories of the input values using the <code class="literal">frequencies</code> and <code class="literal">sel</code> functions as follows:</p><div><pre class="programlisting">(defn print-clusters []
  (doseq [[pos rws] (:sets som)]
    (println pos \:
             (frequencies
              (sel (get-dataset :iris) 
                   :cols :Species :rows rws)))))</pre></div><p>We can call the function <code class="literal">print-clusters</code> defined in the preceding code to produce the following REPL output:</p><div><pre class="programlisting">user&gt; (print-clusters)
[4 1] : {virginica 23}
[8 1] : {virginica 27, versicolor 50}
[9 0] : {setosa 50}
nil</pre></div><p>As shown in the preceding output, the <code class="literal">virginica</code> and <code class="literal">setosa</code> species seem to be appropriately classified into two clusters. However, the cluster containing the input values of the <code class="literal">versicolor</code> species also contains 27 samples of the <code class="literal">virginica</code> species. This problem could be<a id="id694" class="indexterm"/> remedied by using more sample data to train the SOM or by modeling a higher number of features.</p><p>In conclusion, the Incanter library provides us with a concise implementation of an SOM, which we can train using the Iris dataset as shown in the preceding example.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec50"/>Reducing dimensions in the data</h1></div></div></div><p>In order to easily visualize the distribution of some unlabeled data in which the input values have multiple dimensions, we must reduce the number of feature dimensions to two or three. Once we have reduced the number of dimensions of the input data to two or three dimensions, we can trivially plot the data to provide a more understandable visualization of it. This process of reducing the number of dimensions in the input data is known as <strong>dimensionality reduction</strong><a id="id695" class="indexterm"/>. As this process reduces the total number of dimensions used to represent the sample data, it is also useful for data compression.</p><p>
<strong>Principal Component Analysis</strong> (<strong>PCA</strong>) <a id="id696" class="indexterm"/>is a form of dimensionality reduction in which the input variables in the<a id="id697" class="indexterm"/> sample data are transformed into linear uncorrelated variables (for more information, refer to <em>Principal Component Analysis</em>). These transformed features are called the <strong>principal components</strong> of the sample data.</p><p>PCA uses a covariance matrix<a id="id698" class="indexterm"/> and a matrix operation called <strong>Singular Value Decomposition</strong> (<strong>SVD</strong>) to <a id="id699" class="indexterm"/>calculate the principal components of a given set of input values. The covariance matrix denoted as <img src="img/4351OS_07_43.jpg" alt="Reducing dimensions in the data"/>, can be determined from a set of input vectors <img src="img/4351OS_07_12.jpg" alt="Reducing dimensions in the data"/> with <img src="img/4351OS_07_44.jpg" alt="Reducing dimensions in the data"/> samples as follows:</p><div><img src="img/4351OS_07_45.jpg" alt="Reducing dimensions in the data"/></div><p>The covariance matrix<a id="id700" class="indexterm"/> is generally calculated from the input values after mean normalization, which is simply ensuring that each feature has a zero mean value. Also, the features could be scaled before determining the covariance matrix. Next, the SVD of the covariance matrix<a id="id701" class="indexterm"/> is determined as follows:</p><div><img src="img/4351OS_07_46.jpg" alt="Reducing dimensions in the data"/></div><p>SVD <a id="id702" class="indexterm"/>can be thought<a id="id703" class="indexterm"/> of as factorization of a matrix <img src="img/4351OS_07_47.jpg" alt="Reducing dimensions in the data"/> with size <img src="img/4351OS_07_48.jpg" alt="Reducing dimensions in the data"/> into three matrices <img src="img/4351OS_07_49.jpg" alt="Reducing dimensions in the data"/>, <img src="img/4351OS_07_50.jpg" alt="Reducing dimensions in the data"/>, and <img src="img/4351OS_07_51.jpg" alt="Reducing dimensions in the data"/>. The matrix <img src="img/4351OS_07_49.jpg" alt="Reducing dimensions in the data"/> has a size of <img src="img/4351OS_07_52.jpg" alt="Reducing dimensions in the data"/>, the matrix <img src="img/4351OS_07_50.jpg" alt="Reducing dimensions in the data"/> has a size of <img src="img/4351OS_07_48.jpg" alt="Reducing dimensions in the data"/>, and the matrix <img src="img/4351OS_07_51.jpg" alt="Reducing dimensions in the data"/> has a size of <img src="img/4351OS_07_53.jpg" alt="Reducing dimensions in the data"/>. The matrix <img src="img/4351OS_07_47.jpg" alt="Reducing dimensions in the data"/> actually represents the <img src="img/4351OS_07_54.jpg" alt="Reducing dimensions in the data"/> input vectors with <img src="img/4351OS_07_55.jpg" alt="Reducing dimensions in the data"/> dimensions in the sample data. The matrix <img src="img/4351OS_07_50.jpg" alt="Reducing dimensions in the data"/> is a diagonal matrix <a id="id704" class="indexterm"/>and is called the <strong>singular value</strong><a id="id705" class="indexterm"/> of the matrix <img src="img/4351OS_07_47.jpg" alt="Reducing dimensions in the data"/>, and the matrices <img src="img/4351OS_07_49.jpg" alt="Reducing dimensions in the data"/> and <img src="img/4351OS_07_51.jpg" alt="Reducing dimensions in the data"/> are called the <strong>left and right singular vectors</strong> of <img src="img/4351OS_07_47.jpg" alt="Reducing dimensions in the data"/>, respectively. In the context of PCA, the matrix <img src="img/4351OS_07_50.jpg" alt="Reducing dimensions in the data"/> is termed as the <strong>reduction component</strong><a id="id706" class="indexterm"/> and the matrix <img src="img/4351OS_07_49.jpg" alt="Reducing dimensions in the data"/> is termed as the <strong>rotation component</strong><a id="id707" class="indexterm"/> of the sample data.</p><p>The PCA algorithm<a id="id708" class="indexterm"/> to reduce the <img src="img/4351OS_07_55.jpg" alt="Reducing dimensions in the data"/> dimensions in the <img src="img/4351OS_07_54.jpg" alt="Reducing dimensions in the data"/> input vectors to <img src="img/4351OS_07_56.jpg" alt="Reducing dimensions in the data"/> dimensions can<a id="id709" class="indexterm"/> be summarized using the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">
Calculate the covariance matrix <img src="img/4351OS_07_43.jpg" alt="Reducing dimensions in the data"/> from the input vectors <img src="img/4351OS_07_12.jpg" alt="Reducing dimensions in the data"/>.
</li><li class="listitem">
Calculate the matrices <img src="img/4351OS_07_49.jpg" alt="Reducing dimensions in the data"/>, <img src="img/4351OS_07_50.jpg" alt="Reducing dimensions in the data"/>, and <img src="img/4351OS_07_51.jpg" alt="Reducing dimensions in the data"/> by applying SVD on the covariance matrix <img src="img/4351OS_07_43.jpg" alt="Reducing dimensions in the data"/>.
</li><li class="listitem">
From the <img src="img/4351OS_07_52.jpg" alt="Reducing dimensions in the data"/> matrix <img src="img/4351OS_07_49.jpg" alt="Reducing dimensions in the data"/>, select the first <img src="img/4351OS_07_56.jpg" alt="Reducing dimensions in the data"/> columns to produce the matrix <img src="img/4351OS_07_57.jpg" alt="Reducing dimensions in the data"/>, which is termed as the <strong>reduced left singular vector</strong> or <strong>reduced rotation matrix</strong><a id="id710" class="indexterm"/> of the matrix <img src="img/4351OS_07_43.jpg" alt="Reducing dimensions in the data"/>. This matrix <a id="id711" class="indexterm"/>represents the <img src="img/4351OS_07_56.jpg" alt="Reducing dimensions in the data"/> principal components of the<a id="id712" class="indexterm"/> sample data and will have a size of <img src="img/4351OS_07_58.jpg" alt="Reducing dimensions in the data"/>.
</li><li class="listitem">
Calculate the vectors with <img src="img/4351OS_07_56.jpg" alt="Reducing dimensions in the data"/> dimensions, denoted by <img src="img/4351OS_07_32.jpg" alt="Reducing dimensions in the data"/>, as follows:
<div><img src="img/4351OS_07_59.jpg" alt="Reducing dimensions in the data"/></div></li></ol></div><p>Note that the input to the PCA algorithm is the set of input vectors <img src="img/4351OS_07_12.jpg" alt="Reducing dimensions in the data"/> from the sample data after mean normalization and feature scaling.</p><p>Since the matrix <img src="img/4351OS_07_57.jpg" alt="Reducing dimensions in the data"/> calculated in the preceding steps has <img src="img/4351OS_07_56.jpg" alt="Reducing dimensions in the data"/> columns, the matrix <img src="img/4351OS_07_32.jpg" alt="Reducing dimensions in the data"/> will have a size of <img src="img/4351OS_07_60.jpg" alt="Reducing dimensions in the data"/>, which represents the <img src="img/4351OS_07_54.jpg" alt="Reducing dimensions in the data"/> input vectors in <img src="img/4351OS_07_56.jpg" alt="Reducing dimensions in the data"/> dimensions. We should note that a lower value of the number of dimensions <img src="img/4351OS_07_56.jpg" alt="Reducing dimensions in the data"/> could result in a higher loss of variance in the data. Hence, we should choose <img src="img/4351OS_07_56.jpg" alt="Reducing dimensions in the data"/> such that only a small fraction of the variance is lost.</p><p>The original input vectors <img src="img/4351OS_07_12.jpg" alt="Reducing dimensions in the data"/> can<a id="id713" class="indexterm"/> be recreated from the matrix <img src="img/4351OS_07_32.jpg" alt="Reducing dimensions in the data"/> and the reduced<a id="id714" class="indexterm"/> left singular vector <img src="img/4351OS_07_57.jpg" alt="Reducing dimensions in the data"/> as follows:</p><div><img src="img/4351OS_07_61.jpg" alt="Reducing dimensions in the data"/></div><p>The Incanter library includes some functions to perform PCA. In the example that will follow, we will use PCA to provide a better visualization of the Iris dataset.</p><div><div><h3 class="title"><a id="note46"/>Note</h3><p>The namespace declaration of the upcoming example should look similar to the following declaration:</p><div><pre class="programlisting">(ns my-namespace
  (:use [incanter core stats charts datasets]))</pre></div></div></div><p>We first define the training data using the <code class="literal">get-dataset</code>, <code class="literal">to-matrix</code>, and <code class="literal">sel</code> functions, as shown in the following code:</p><div><pre class="programlisting">(def iris-matrix (to-matrix (get-dataset :iris)))
(def iris-features (sel iris-matrix :cols (range 4)))
(def iris-species (sel iris-matrix :cols 4))</pre></div><p>Similar to the previous example, we will use the first four columns of the Iris dataset as sample data for the input variables of the training data.</p><p>PCA is performed by the <code class="literal">principal-components</code> function from the <code class="literal">incanter.stats</code> namespace. This function returns a map that contains the rotation matrix <img src="img/4351OS_07_49.jpg" alt="Reducing dimensions in the data"/> and the reduction matrix <img src="img/4351OS_07_50.jpg" alt="Reducing dimensions in the data"/> from PCA, <a id="id715" class="indexterm"/>which we described earlier. We can select columns from the reduction matrix of the input data using the <code class="literal">sel</code> function as shown in the following code:</p><div><pre class="programlisting">(def pca (principal-components iris-features))

(def U (:rotation pca))
(def U-reduced (sel U :cols (range 2)))</pre></div><p>As shown in the preceding code,<a id="id716" class="indexterm"/> the rotation matrix of the PCA of the input data can be fetched using the <code class="literal">:rotation</code> keyword on the value returned by the <code class="literal">principal-components</code> function<a id="id717" class="indexterm"/>. We can now calculate the reduced features <em>Z</em> using the reduced rotation matrix and the original matrix of features represented by the <code class="literal">iris-features</code> variable, as shown in the following code:</p><div><pre class="programlisting">(def reduced-features (mmult iris-features U-reduced))</pre></div><p>The reduced features can then be visualized by selecting the first two columns of the <code class="literal">reduced-features</code> matrix and plotting them using the <code class="literal">scatter-plot</code> function, as shown in the following code:</p><div><pre class="programlisting">(defn plot-reduced-features []
  (view (scatter-plot (sel reduced-features :cols 0)
                      (sel reduced-features :cols 1)
                      :group-by iris-species
                      :x-label "PC1"
                      :y-label "PC2")))</pre></div><p>The following plot is generated on calling the <code class="literal">plot-reduced-features</code> function<a id="id718" class="indexterm"/> defined in the preceding code:</p><div><img src="img/4351OS_07_62.jpg" alt="Reducing dimensions in the data"/></div><p>The scatter plot illustrated in the preceding diagram gives us a good visualization of the distribution of the input data.<a id="id719" class="indexterm"/> The blue and green clusters in the preceding plot are shown to have<a id="id720" class="indexterm"/> similar values for the given set of features. In summary, the Incanter library supports PCA, which allows for the easy visualization of some sample data.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec51"/>Summary</h1></div></div></div><p>In this chapter, we explored several clustering algorithms that can be used to model some unlabeled data. The following are some of the other points that we have covered:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We explored the <em>K</em>-means algorithm and hierarchical clustering techniques while providing sample implementations of these methods in pure Clojure. We also described how we can leverage these techniques through the <code class="literal">clj-ml</code> library.</li><li class="listitem" style="list-style-type: disc">We discussed the EM algorithm, which is a probabilistic clustering technique, and also described how we can use the <code class="literal">clj-ml</code> library to build an EM clusterer.</li><li class="listitem" style="list-style-type: disc">We also explored how we can use SOMs to fit clustering problems with a high number of dimensions. We also demonstrated how we can use the Incanter library to build an SOM that can be used for clustering.</li><li class="listitem" style="list-style-type: disc">Lastly, we studied dimensionality reduction and PCA, and how we can use PCA to provide a better visualization of the Iris dataset using the Incanter library.</li></ul></div><p>In the following chapter, we will explore the concepts of anomaly detection and recommendation systems using machine learning techniques.</p></div></body></html>