- en: '*Chapter 10*: Feature Transformation and Scaling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you have seen how to manage drift and drift detection
    in streaming and online machine learning models. Drift detection, although not
    the main concept in machine learning, is a very important accessory aspect of
    machine learning in production.
  prefs: []
  type: TYPE_NORMAL
- en: Although many secondary topics are important in machine learning, some of the
    accessory topics are especially important with online models. Drift detection
    is particularly important, as the model's autonomy in relearning makes it slightly
    more black-box to the developer or data scientist. This has great advantages only
    as long as the retraining process is correctly managed by drift detection and
    comparable methods.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will see another secondary machine learning topic that
    has important implications for online machine learning and streaming. Feature
    transformation and scaling are practices that are relatively well defined in traditional,
    batch machine learning. They do not generally pose any theoretical difficulty.
  prefs: []
  type: TYPE_NORMAL
- en: In online machine learning, scaling and feature transformation is not as straightforward.
    It is necessary to adapt the practice to the possibility that new data is not
    exactly comparable to the original data. This causes questions as to whether or
    not to refit feature transformations and scalers on every new piece of data arriving,
    but also on whether such practices will introduce bias into your already trained
    and continuously re-training models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that are covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of data preparation with streaming data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling data for streaming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming features in a streaming context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code for this book on GitHub at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python).
    If you are not yet familiar with Git and GitHub, the easiest way to download the
    notebooks and code samples is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the link of the repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to the green **Code** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Download ZIP**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you download the ZIP file, unzip it in your local environment, and you
    will be able to access the code through your preferred Python editor.
  prefs: []
  type: TYPE_NORMAL
- en: Python environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To follow along with this book, you can download the code in the repository
    and execute it using your preferred Python editor.
  prefs: []
  type: TYPE_NORMAL
- en: If you are not yet familiar with Python environments, I would advise you to
    check out Anaconda ([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)),
    which comes with Jupyter Notebook and JupyterLabs, which are both great for executing
    notebooks. It also comes with Spyder and VSCode for editing scripts and programs.
  prefs: []
  type: TYPE_NORMAL
- en: If you have difficulty installing Python or the associated programs on your
    machine, you can check out Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    or Kaggle Notebooks ([https://www.kaggle.com/code](https://www.kaggle.com/code)),
    which both allow you to run Python code in online notebooks for free, without
    any setup required.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The code in the book will generally use Colab and Kaggle Notebooks with Python
    version *3.7.13*, and you can set up your own environment to mimic this.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of data preparation with streaming data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before deep-diving into specific algorithms and solutions, let''s first have
    a general discussion of why data preparation may be different when working with
    data that arrives in a streaming fashion. Multiple reasons can be identified,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The first, obvious issue is data drift. As discussed in much detail in the previous
    chapter, trends and descriptive statistics of your data can slowly change over
    time due to data drift. If your feature engineering or data preparation processes
    are too dependent on your data following certain distributions, you may run into
    problems when data drift occurs. As many solutions for this have been proposed
    in the previous chapter, this topic will be left out of consideration in the current
    chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second issue is that population parameters are unknown. When observing
    data in a streaming fashion, it is possible, and even likely, that your estimates
    of the population parameters are slowly going to improve over time. As seen in
    [*Chapter 3*](B18335_03_ePub.xhtml#_idTextAnchor051), precision in your estimates
    of descriptive statistics will improve with the amount of data you have. When
    the descriptive statistic estimates are improving, the fact that they are changing
    over time does not make it easy to fix your formulas for data preparation, feature
    engineering, scaling, and the like:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As the first example of this, consider the range. The range represents the minimum
    and maximum values of the data that you observe. This is used extensively in data
    scaling and also in other algorithms. Now, imagine that the range (minimum and
    maximum values) in a batch can be different from the global range (global minimum
    and global maximum) of the data. After all, when new data arrives, you may observe
    a value that is higher or lower than anything observed in your historical data,
    just by the process of random sampling. Observing an observation that is higher
    than your maximum may cause an issue in scaling if you do not treat it right.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Another example of this is when scaling with a normal distribution. The standard
    deviation and average in your batch may be different from the population standard
    deviation and population average. This may cause your scaler to behave differently
    after some time, which is a sort of data drift that is induced by your own scaling
    algorithm. Clearly, this must be avoided.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Many other cases of such problems exist, including observing new categories
    in a categorical value, which will lead to problems with your one-hot encoder
    or your models that use categorical variables. You can also imagine that occurring
    new types of values in your data such as NAs and InFs need to be managed well,
    rather than having them cause bugs. This is true in general, but when working
    with streaming, this tends to cause even more trouble than with regular data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will learn what scaling is and how to work with it.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling data for streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first part of this section, let's start by looking at some solutions
    for streaming scaling data. Before going into the solutions, let's do a quick
    recap of what scaling is and how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing scaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Numerical variables can be of any scale, meaning they can have very high average
    values or low average values, for example. Some machine learning algorithms are
    not at all impacted by the scale of a variable, whereas other machine learning
    algorithms can be strongly impacted.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling is the practice of taking a numerical variable and reducing its range,
    and potentially its standard deviation, to a pre-specified range. This will allow
    all machine learning algorithms to learn from the data without problems.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with MinMaxScaler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To achieve this goal, a commonly used method is the Min-Max scaler. The Min-Max
    scaler will take an input variable in any range and reduce all of the values to
    fall in between the range (`0` to `1`), meaning that the minimum value of the
    scaled variable will be `0` and the maximum of the scaled variable will be `1`.
    Sometimes, an alternative is used in which the minimum is not `0`, but `-1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical formula for Min-Max scaling is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Scaling with StandardScaler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another very common approach to scaling is standardizing. Standardizing is a
    method strongly based on statistics, which allows you to take any variable and
    take it back to a standard normal distribution. The standard normal distribution
    has an average of `0` and a standard deviation of `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical formula for the StandardScaler is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The values of the scaled variable will not be in any specific range; the new
    value of the scaled variable represents the number of standard deviations that
    the original value is away from the original mean. A very extreme value (imagine
    four or five standard deviations away from the mean) would have a value of four
    or five, which by the way can be both positive and negative.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing your scaling method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The choice of scaling algorithm depends on the use case, and it is generally
    a good idea to do tuning of your machine learning pipeline in which different
    scaling methods are used with different algorithms. After all, the choice of scaling
    method has an impact on the performance of the training of the method.
  prefs: []
  type: TYPE_NORMAL
- en: The Min-Max scaler is known to have difficulty with outliers. After all, a very
    extreme outlier would be set to the maximum value, that is, to `1`. Then, this
    may cause the other values to be reduced to a much smaller range.
  prefs: []
  type: TYPE_NORMAL
- en: The StandardScaler deals with this in a better way, as the outliers would still
    be outliers and simply take on high values in the scaled variable. This can be
    a disadvantage at the same time, mainly when you are using machine learning algorithms
    that need the values to be between `0` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Adapting scaling to a streaming context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now have a look at how we can adapt each of those approaches to the case
    of streaming data. We'll start with the Min-Max scaler.
  prefs: []
  type: TYPE_NORMAL
- en: Adapting the MinMaxScaler to streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The MinMaxScaler works perfectly on a fixed dataset. It guarantees that the
    values of the scaled data will be between `0` and `1`, just as required by some
    machine learning algorithms. However, in the case of streaming data, this is much
    less easy to manage.
  prefs: []
  type: TYPE_NORMAL
- en: 'When new data arrives one by one (in a stream), it is impossible to decide
    on the minimum or maximum value. After all, you cannot expect one value to be
    both minimum and maximum. The same problem occurs when batching: there is no guarantee
    that the batch maximum is higher than the global maximum, and the same for the
    minimum.'
  prefs: []
  type: TYPE_NORMAL
- en: You could use the training data to decide on the minimum and the maximum, but
    then the problem is that your new data could be above the training maximum or
    below the training minimum. This would result in the scaled values being outside
    of the range (`0` to `1`).
  prefs: []
  type: TYPE_NORMAL
- en: A solution for this is to use a running minimum and a running maximum. This
    means that you continue updating the MinMaxScaler so that every time a lower minimum
    is observed, you update the minimum in the MinMaxScaler formula, and every time
    a higher maximum is observed, you update the maximum.
  prefs: []
  type: TYPE_NORMAL
- en: The advantage of this method is that it guarantees that your scaled data will
    always be between `0` and `1`. A disadvantage is that the first values for training
    the MinMaxScaler will be scaled quite badly. This is easily solved by using some
    training data to initialize the MinMaxScaler. Outliers can also be a problem,
    as having one very extreme value will strongly affect the MinMaxScaler's formula,
    and scores will be very different after that. This could be solved by using an
    outlier detection method as described extensively in [*Chapter 5*](B18335_05_ePub.xhtml#_idTextAnchor097).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now move on to a Python implementation of an adaptive MinMaxScaler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we will use the implementation of the MinMaxScaler in the Python
    library, `River`. We will use the following data for this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-1
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram of this data can be created using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting histogram looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Resulting histogram of Code Block 10-2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Resulting histogram of Code Block 10-2
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to scale this data, let''s use the `MinMaxScaler` function from River.
    Looping through the data will simulate the data arriving in a streaming fashion,
    and the use of the `learn_one` method shows that the data is updated step by step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-3
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it will be interesting to see the histogram of the scaled data. It can
    be created as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-4
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram is shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Resulting histogram of Code Block 10-4'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Resulting histogram of Code Block 10-4
  prefs: []
  type: TYPE_NORMAL
- en: This histogram clearly shows that we have been successful in scaling the data
    into the `0` to `1` range.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have seen the theory and implementation of the MinMaxScaler, let's
    now see the StandardScaler, a common alternative to this method.
  prefs: []
  type: TYPE_NORMAL
- en: Adapting the Standard Scaler to streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem that may occur in standard scaling when observing more extreme data
    in the future, is not exactly the same problem as the one that is seen in Min-Max
    scaling. Where the Min-Max scaler uses the minimum and the maximum to compute
    the scaling method, the standard scaler uses the mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: The reason why this is very different is that the minimum and maximum are relatively
    likely to be surpassed at one point in time. This would result in the scaled values
    being higher than `1` or lower than `0`, which may pose real problems for your
    machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In the standard scaler, any extreme values occurring in the future will impact
    your estimate of the global mean and standard deviation, but they are much less
    likely to impact them very severely. After all, the mean and the standard deviation
    are much less sensitive to the observation of a small number of extreme values.
  prefs: []
  type: TYPE_NORMAL
- en: Given this theoretical consideration, you may conclude that it isn't really
    necessary to update the standard scaler. However, it may be best to update it
    anyway, as this is a good way to keep your machine learning methods up to date.
    The added value of this will be less impacting than when using the Min-Max scaler,
    but it is a best practice to do it anyway.
  prefs: []
  type: TYPE_NORMAL
- en: 'One solution that you can use is to use the AdaptiveStandardScaler in the `Riverml`
    package. It uses an exponentially-weighted running mean and variance to make sure
    that slight drifts of the normal distribution of your data are taken into account
    without having it weigh too strongly. Let''s see a Python example of how to use
    the AdaptiveStandardScaler:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following data for this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-5
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This data follows a normal distribution, as you can see from the histogram.
    You can create a histogram as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-6
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting histogram is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Resulting histogram of Code Block 10-6'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Resulting histogram of Code Block 10-6
  prefs: []
  type: TYPE_NORMAL
- en: The data clearly follows a normal distribution, but it is not centered around
    `0` and it is not standardized to a standard deviation of `1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to scale this data, let''s use `StandardScaler` from River. Again, we
    will loop through the data to simulate streaming. Also, we again use the `learn_one`
    method to update the data step by step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-7
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'To verify that it has worked correctly, let''s redo the histogram using the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The histogram is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Resulting histogram of Code Block 10-8'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Resulting histogram of Code Block 10-8
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the data is clearly centered around `0`, and the new, scaled
    value indicates the number of standard deviations that each data point is away
    from the mean.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will see how to adapt feature transformation in a streaming
    context.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming features in a streaming context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling data is a way of pre-processing data for machine learning, but many
    other statistical methods can be used for data preparation. In this second part
    of this chapter let's deep dive into the **principal component analysis** (**PCA**)
    method, a much-used method for preparing data at the beginning of any machine
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA is a machine learning method that can be used for multiple applications.
    When working with highly multivariate data, PCA can be used in an interpretative
    way, where you use it to make sense of and analyze multivariate datasets. This
    is a use of PCA in data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to use PCA is to prepare data for machine learning. From a high-level
    point of view, PCA could be seen as an alternative to scaling that reduces the
    number of variables of your data to make it easier for the model to fit. This
    is the use of PCA that is most relevant for the current chapter, and this is how
    it will be used in the example.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical definition of PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA works on multivariate data (or data with multiple columns). These columns
    generally have a business definition. The goal of PCA is to keep all information
    in the data but change the current variable definitions into variables with different
    interpretations.
  prefs: []
  type: TYPE_NORMAL
- en: The new variables are called the **principal components**, and they are found
    in such a way that the first component contains the most possible variation, and
    the second component is the component that is orthogonal to the first one and
    explains the most variation possible while being orthogonal.
  prefs: []
  type: TYPE_NORMAL
- en: 'A schematical overview is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Schematic overview of PCA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Schematic overview of PCA
  prefs: []
  type: TYPE_NORMAL
- en: This example clearly shows how the original data on the left is transformed
    into principal components on the right. The first principal component has much
    more value in terms of information than any of the original variables. When working
    with hundreds of variables, you can imagine that you will need to retain only
    a limited number of components (based on different criteria and your use case),
    which may make it easier for your machine learning algorithm to learn from the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Regular PCA in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To have a good comparison between regular and incremental PCA, it is good to
    get everybody up to speed and do a quick example of a regular PCA first:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, let''s create some simulated sample data to work on the example.
    We can make a small example dataset as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-9
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – The resulting data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – The resulting data
  prefs: []
  type: TYPE_NORMAL
- en: 'You can make a plot of this data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-10
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The scatter plot shows a plot that is quite similar to the sketch in the earlier
    schematic drawing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – The resulting image of Code Block 10-10'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – The resulting image of Code Block 10-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now use a regular PCA to identify the components and transform the data.
    The following block of code shows how to fit a PCA using `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformed data looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – The transformed data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – The transformed data
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot it just like we did with the previous data. This can be done using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Code Block 10-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Plot of the transformed data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – Plot of the transformed data
  prefs: []
  type: TYPE_NORMAL
- en: You can clearly see that this looks a lot like the resulting plot in the earlier
    theoretical introduction. This PCA has successfully identified the first principal
    component to be the component that explains the largest part of the data. The
    second component explains the largest part of the remaining data (after the first
    component).
  prefs: []
  type: TYPE_NORMAL
- en: Incremental PCA for streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA, in a streaming context, cannot be easily calculated on individual data
    points. After all, you can imagine that it is impossible to determine the standard
    deviation of a single data point, and therefore, there is no possible way to determine
    the best components.
  prefs: []
  type: TYPE_NORMAL
- en: 'The proposed solution is to do this through batches and to compute your PCA
    in batches rather than all at once. The `scikit-learn` package has a functionality
    called `IncrementalPCA`, which allows you to fit PCA in batches. Let''s use the
    following code for fitting `IncrementalPCA` on the same data as before and compare
    the results. The code to fit and transform using `IncrementalPCA` is shown in
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 10-13
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The transformed data using this second method looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – The transformed data using incremental PCA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.10 – The transformed data using incremental PCA
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s also make a plot of this data to see whether this batch-wise PCA
    was successful in fitting the real components, or whether it is far away from
    the original PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: Code Block 10-14
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting scatter plot is shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – The scatter plot of the transformed data using incremental
    PCA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18335_10_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.11 – The scatter plot of the transformed data using incremental PCA
  prefs: []
  type: TYPE_NORMAL
- en: This scatter plot shows that the PCA has been correctly fitted. Do not be confused
    by the fact that the incremental PCA has inversed the first component (the image
    is mirrored left to right compared to the preceding one). This is not wrong but
    just mirrored. This incremental PCA has captured the two components very well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have seen some common methods for data preparation being
    adapted to streaming and online data. For streaming data, it is important to have
    easily refitting or re-estimating models.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of the chapter, you have seen two methods for scaling. The
    MinMaxScaler scales the data to the `0` to `1` range and, therefore, needs to
    make sure that none of the new data points get outside of this range. The StandardScaler
    uses a statistical normalization process using the mean and standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: The second part of the chapter demonstrated a regular PCA and a new, incremental
    version called `IncrementalPCA`. This incremental method allows you to fit PCA
    in batches, which can help you when fitting PCA on streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: 'With scaling and feature transformation in this chapter, and drift detection
    in the previous chapter, you have already seen a good part of the auxiliary tasks
    of machine learning on streaming. In the coming chapter, you will see the third
    and last secondary topic to machine learning and streaming, which is catastrophic
    forgetting: an impactful problem that can occur in online machine learning, causing
    the model to forget important learned trends. The chapter will explain how to
    detect and avoid it.'
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*MinMaxScaler in River*: [https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/](https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*StandardScaler in River*: [https://riverml.xyz/latest/api/preprocessing/StandardScaler/](https://riverml.xyz/latest/api/preprocessing/StandardScaler/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*PCA in scikit-learn*: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Incremental PCA in scikit-learn*: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
