<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Java Libraries and Platforms for Machine Learning</h1>
                </header>
            
            <article>
                
<p>Implementing machine learning algorithms by yourself is probably the best way to learn machine learning, but you can progress much faster if you step on the shoulders of the giants and leverage one of the existing open source libraries.</p>
<p>This chapter reviews various libraries and platforms for machine learning in Java. The goal is to understand what each library brings to the table and what kind of problems it is able to solve.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>The requirement of Java for implementing a machine learning application</li>
<li>Weka, a general purpose machine learning platform</li>
<li>The Java machine learning library, a collection of machine learning algorithms</li>
<li>Apache Mahout, a scalable machine learning platform</li>
<li>Apache Spark, a distributed machine learning library</li>
<li>Deeplearning4j, a deep learning library</li>
<li>MALLET, a text mining library</li>
</ul>
<p>We'll also discuss how to design the complete machine learning application stack for both single-machine and big data apps by using these libraries with other components.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The need for Java</h1>
                </header>
            
            <article>
                
<p>New machine learning algorithms are often first scripted at university labs, gluing together several languages such as shell scripting, Python, R, MATLAB, Scala, or C++ to provide a new concept and theoretically analyze its properties. An algorithm might take a long path of refactoring before it lands in a library with standardized input or output and interfaces. While Python, R, and MATLAB are quite popular, they are mainly used for scripting, research, and experimenting. Java, on the other hand, is the de facto enterprise language, which could be attributed to static typing, robust IDE support, good maintainability, as well as decent threading model and high performance concurrent data structure libraries. Moreover, there are already many Java libraries available for machine learning, which makes it really convenient to apply them in existing Java applications and leverage powerful machine learning capabilities.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Machine learning libraries</h1>
                </header>
            
            <article>
                
<p>There are over 70 Java-based open source machine learning projects listed on the <a href="https://mloss.org/software/"><span class="URLPACKT">MLOSS.org</span></a> website, and probably many more unlisted projects live at university servers, GitHub, or Bitbucket. In this section, we will review the major libraries and platforms, the kind of problems they can solve, the algorithms they support, and the kind of data they can work with.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Weka</h1>
                </header>
            
            <article>
                
<p><strong>Waikato Environment for Knowledge Analysis</strong> (<strong>WEKA</strong>) is a machine learning library that was developed at the University of Waikato, New Zealand, and is probably the most well-known Java library. It is a general purpose library that is able to solve a wide variety of machine learning tasks, such as classification, regression, and clustering. It features a rich graphical user interface, command-line interface, and Java API. You can check out Weka at <span class="URLPACKT"><a href="http://www.cs.waikato.ac.nz/ml/weka/">http://www.cs.waikato.ac.nz/ml/weka/</a></span>.</p>
<p>At the time of writing this book, Weka contains 267 algorithms in total: data preprocessing (82), attribute selection (33), classification and regression (133), clustering (12), and association rules mining (7). Graphical interfaces are well suited for exploring your data, while the Java API allows you to develop new machine learning schemes and use the algorithms in your applications.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Weka is distributed under the <strong>GNU General Public License</strong> (<strong>GNU GPL</strong>), which means that you can copy, distribute, and modify it as long as you track changes in source files and keep it under GNU GPL. You can even distribute it commercially, but you must disclose the source code or obtain a commercial license.</p>
<p>In addition to several supported file formats, Weka features its own default data format, ARFF, to describe data by attribute-data pairs. It consists of two parts. The first part contains a header, which specifies all of the attributes and their types, for instance, nominal, numeric, date, and string. The second part contains the data, where each line corresponds to an instance. The last attribute in the header is implicitly considered the target variable and missing data is marked with a question mark. For example, returning to the example from <span class="ChapterrefPACKT"><a href="11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml">Chapter 1</a></span>, <em>Applied Machine Learning Quick Start</em>, the <kbd>Bob</kbd> instance written in an ARFF file format would be as follows:</p>
<pre>@RELATION person_dataset @ATTRIBUTE `Name` STRING @ATTRIBUTE `Height` NUMERIC @ATTRIBUTE `Eye color`{blue, brown, green} @ATTRIBUTE `Hobbies` STRING @DATA 'Bob', 185.0, blue, 'climbing, sky diving' 'Anna', 163.0, brown, 'reading' 'Jane', 168.0, ?, ? </pre>
<p>The file consists of three sections. The first section starts with the <kbd>@RELATION &lt;String&gt;</kbd> keyword, specifying the dataset name. The next section starts with the <kbd>@ATTRIBUTE</kbd> keyword, followed by the attribute name and type. The available types are <kbd>STRING</kbd>, <kbd>NUMERIC</kbd>, <kbd>DATE</kbd>, and a set of categorical values. The last attribute is implicitly assumed to be the target variable that we want to predict. The last section starts with the <kbd>@DATA</kbd> keyword, followed by one instance per line. Instance values are separated by commas and must follow the same order as attributes in the second section.</p>
<p>More Weka examples will be demonstrated in <a href="e0c71e12-6bd7-4f63-b71d-78bb5a87b801.xhtml"><span class="ChapterrefPACKT">Chapter 3</span></a>, <em>Basic Algorithms <span>–</span> Classification, Regression, and Clustering</em>, and <a href="6ac8d4de-1e7f-4f60-9cf0-93ab2fe55e4d.xhtml"><span class="ChapterrefPACKT">Chapter 4</span></a>, <em>Customer Relationship Prediction with Ensembles</em>.</p>
<div class="mce-root packt_tip">To learn more about Weka, pick up a quick-start book—<em>Weka How-to,</em> by <em>Kaluza, Packt Publishing</em> to start coding, or look into <em>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations</em> by <em>Witten and Frank</em>, <em>Morgan Kaufmann Publishers</em> for theoretical background and in-depth explanations.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Weka's Java API is organized into the following top-level packages:</p>
<ul>
<li><kbd>weka.associations</kbd>: These are data structures and algorithms for association rules learning, including <strong>Apriori</strong>, <strong>predictive Apriori</strong>, <strong>FilteredAssociator</strong>, <strong>FP-Growth</strong>, <strong>Generalized Sequential Patterns</strong> (<strong>GSP</strong>), <strong>hotSpot</strong>, and <strong>Tertius</strong>.</li>
<li><kbd>weka.classifiers</kbd>: These are supervised learning algorithms, evaluators, and data structures. The package is further split into the following components:
<ul>
<li><kbd>weka.classifiers.bayes</kbd>: This implements Bayesian methods, including Naive Bayes, Bayes net, Bayesian logistic regression, and so on.</li>
<li><kbd>weka.classifiers.evaluation</kbd>: These are supervised evaluation algorithms for nominal and numerical prediction, such as evaluation statistics, confusion matrix, ROC curve, and so on.</li>
<li><kbd>weka.classifiers.functions</kbd>: These are regression algorithms, including linear regression, isotonic regression, Gaussian processes, <strong>Support Vector Machines </strong><span>(</span><strong>SVMs</strong><span>)</span>, multilayer perceptron, voted perceptron, and others.</li>
<li><kbd>weka.classifiers.lazy</kbd>: These are instance-based algorithms such as k-nearest neighbors, K*, and lazy Bayesian rules.</li>
<li><kbd>weka.classifiers.meta</kbd>: These are supervised learning meta-algorithms, including AdaBoost, bagging, additive regression, random committee, and so on.</li>
<li><kbd>weka.classifiers.mi</kbd>: These are multiple-instance learning algorithms, such as citation k-nearest neighbors, diverse density, AdaBoost, and others.</li>
<li><kbd>weka.classifiers.rules</kbd>: These are decision tables and decision rules based on the separate-and-conquer approach, RIPPER, PART, PRISM, and so on.</li>
<li><kbd>weka.classifiers.trees</kbd>: These are various decision trees algorithms, including ID3, C4.5, M5, functional tree, logistic tree, random forest, and so on.</li>
<li><kbd>weka.clusterers</kbd>: These are clustering algorithms, including k-means, CLOPE, Cobweb, DBSCAN hierarchical clustering, and FarthestFirst.</li>
<li><kbd>weka.core</kbd>: These are various utility classes such as the attribute class, statistics class, and instance class.</li>
<li><kbd>weka.datagenerators</kbd>: These are data generators for classification, regression, and clustering algorithms.</li>
<li><kbd>weka.estimators</kbd>: These are various data distribution estimators for discrete/nominal domains, conditional probability estimations, and so on.</li>
<li><kbd>weka.experiment</kbd>: These are a set of classes supporting necessary configuration, datasets, model setups, and statistics to run experiments.</li>
<li><kbd>weka.filters</kbd>: These are attribute-based and instance-based selection algorithms for both supervised and unsupervised data preprocessing.</li>
<li><kbd>weka.gui</kbd>: These are graphical interface implementing explorer, experimenter, and knowledge flow applications. The Weka Explorer allows you to investigate datasets, algorithms, as well as their parameters, and visualize datasets with scatter plots and other visualizations. The Weka Experimenter is used to design batches of experiments, but it can only be used for classification and regression problems.The Weka KnowledgeFlow implements a visual drag-and-drop user interface to build data flows and, for example, load data, apply filter, build classifier, and evaluate it.</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Java machine learning</h1>
                </header>
            
            <article>
                
<p>The <strong>Java Machine Learning Library</strong> (<strong>Java-ML</strong>) is a collection of machine learning algorithms with a common interface for algorithms of the same type. It only features the Java API, and so it is primarily aimed at software engineers and programmers. Java-ML contains algorithms for data preprocessing, feature selection, classification, and clustering. In addition, it features several Weka bridges to access Weka's algorithms directly through the Java-ML API. It can be downloaded from <span class="URLPACKT"><a href="http://java-ml.sourceforge.net/">http://java-ml.sourceforge.net</a>.</span></p>
<p>Java-ML is also a general-purpose machine learning library. Compared to Weka, it offers more consistent interfaces and implementations of recent algorithms that are not present in other packages, such as an extensive set of state-of-the-art similarity measures and feature-selection techniques, for example, <strong>dynamic time warping</strong><span> (</span><strong>DTW</strong><span>)</span>, random forest attribute evaluation, and so on. Java-ML is also available under the GNU GPL license.</p>
<p>Java-ML supports all types of files as long as they contain one data sample per line and the features are separated by a symbol such as a comma, semicolon, or tab.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The library is organized around the following top-level packages:</p>
<ul>
<li><kbd>net.sf.javaml.classification</kbd>: These are classification algorithms, including Naive Bayes, random forests, bagging, self-organizing maps, k-nearest neighbors, and so on</li>
<li><kbd>net.sf.javaml.clustering</kbd>: These are clustering algorithms such as k-means, self-organizing maps, spatial clustering, Cobweb, ABC, and others</li>
<li><kbd>net.sf.javaml.core</kbd>: These are classes representing instances and datasets</li>
<li><kbd>net.sf.javaml.distance</kbd>: These are algorithms that measure instance distance and similarity, for example, Chebyshev distance, cosine distance/similarity, Euclidean distance, Jaccard distance/similarity, Mahalanobis distance, Manhattan distance, Minkowski distance, Pearson correlation coefficient, Spearman's footrule distance, DTW, and so on</li>
<li><kbd>net.sf.javaml.featureselection</kbd>: These are algorithms for feature evaluation, scoring, selection, and ranking, for instance, gain ratio, ReliefF, Kullback-Leibler divergence, symmetrical uncertainty, and so on</li>
<li><kbd>net.sf.javaml.filter</kbd>: These are methods for manipulating instances by filtering, removing attributes, setting classes or attribute values, and so on</li>
<li><kbd>net.sf.javaml.matrix</kbd>: This implements in-memory or file-based arrays</li>
<li><kbd>net.sf.javaml.sampling</kbd>: This implements sampling algorithms to select a subset of datasets</li>
<li><kbd>net.sf.javaml.tools</kbd>: These are utility methods on dataset, instance manipulation, serialization, Weka API interface, and so on</li>
<li><kbd>net.sf.javaml.utils</kbd>: These are utility methods for algorithms, for example, statistics, math methods, contingency tables, and others</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Apache Mahout</h1>
                </header>
            
            <article>
                
<p>The Apache Mahout project aims to build a scalable machine learning library. It is built atop scalable, distributed architectures, such as Hadoop, using the MapReduce paradigm, which is an approach for processing and generating large datasets with a parallel, distributed algorithm using a cluster of servers.</p>
<p>Mahout features a console interface and the Java API as scalable algorithms for clustering, classification, and collaborative filtering. It is able to solve three business problems:</p>
<ul>
<li><strong>Item recommendation</strong>: Recommending items such as <em><em>People who liked this movie also liked</em></em></li>
<li><strong>Clustering</strong>: Sorting of text documents into groups of topically-related documents</li>
<li><strong>Classification</strong>: Learning which topic to assign to an unlabelled document</li>
</ul>
<p>Mahout is distributed under a commercially friendly Apache license, which means that you can use it as long as you keep the Apache license included and display it in your program's copyright notice.</p>
<p>Mahout features the following libraries:</p>
<ul>
<li><kbd>org.apache.mahout.cf.taste</kbd>: These are collaborative filtering algorithms based on user-based and item-based collaborative filtering and matrix factorization with ALS</li>
<li><kbd>org.apache.mahout.classifier</kbd>: These are in-memory and distributed implementations, including logistic regression, Naive Bayes, random forest, <strong>hidden Markov models</strong> (<strong>HMM</strong>), and multilayer perceptron</li>
<li><kbd>org.apache.mahout.clustering</kbd>: These are clustering algorithms such as canopy clustering, k-means, fuzzy k-means, streaming k-means, and spectral clustering</li>
<li><kbd>org.apache.mahout.common</kbd>: These are utility methods for algorithms, including distances, MapReduce operations, iterators, and so on</li>
<li><kbd>org.apache.mahout.driver</kbd>: This implements a general-purpose driver to run main methods of other classes</li>
<li><kbd>org.apache.mahout.ep</kbd>: This is the evolutionary optimization using the recorded-step mutation</li>
<li><kbd>org.apache.mahout.math</kbd>: These are various math utility methods and implementations in Hadoop</li>
<li><kbd>org.apache.mahout.vectorizer</kbd>: These are classes for data presentation, manipulation, and MapReduce jobs</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Apache Spark</h1>
                </header>
            
            <article>
                
<p>Apache Spark, or simply Spark, is a platform for large-scale data processing builds atop Hadoop, but, in contrast to Mahout, it is not tied to the MapReduce paradigm. Instead, it uses in-memory caches to extract a working set of data, process it, and repeat the query. This is reported to be up to ten times as fast as a Mahout implementation that works directly with data stored in the disk. It can be grabbed from <span class="URLPACKT"><a href="https://spark.apache.org/">https://spark.apache.org</a></span>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>There are many modules built atop Spark, for instance, GraphX for graph processing, Spark Streaming for processing real-time data streams, and MLlib for machine learning library featuring classification, regression, collaborative filtering, clustering, dimensionality reduction, and optimization.</p>
<p>Spark's MLlib can use a Hadoop-based data source, for example, <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) or HBase, as well as local files. The supported data types include the following:</p>
<ul>
<li><strong>Local vectors</strong> are stored on a single machine. Dense vectors are presented as an array of double-typed values, for example, (2.0, 0.0, 1.0, 0.0), while sparse vector is presented by the size of the vector, an array of indices, and an array of values, for example, [4, (0, 2), (2.0, 1.0)].</li>
<li><strong>Labelled point</strong> is used for supervised learning algorithms and consists of a local vector labelled with double-typed class values. The label can be a class index, binary outcome, or a list of multiple class indices (multiclass classification). For example, a labelled dense vector is presented as [1.0, (2.0, 0.0, 1.0, 0.0)].</li>
<li><strong>Local matrices</strong> store a dense matrix on a single machine. It is defined by matrix dimensions and a single double-array arranged in a column-major order.</li>
<li><strong>Distributed matrices</strong> operate on data stored in Spark's <strong>Resilient Distributed Dataset</strong> (<strong>RDD</strong>), which represents a collection of elements that can be operated on in parallel. There are three presentations: row matrix, where each row is a local vector that can be stored on a single machine, row indices are meaningless; indexed row matrix, which is similar to row matrix, but the row indices are meaningful, that is, rows can be identified and joins can be executed; and coordinate matrix, which is used when a row cannot be stored on a single machine and the matrix is very sparse.</li>
</ul>
<p>Spark's MLlib API library provides interfaces for various learning algorithms and utilities, as outlined in the following list:</p>
<ul>
<li><kbd>org.apache.spark.mllib.classification</kbd>: These are binary and multiclass classification algorithms, including linear SVMs, logistic regression, decision trees, and Naive Bayes</li>
<li><kbd>org.apache.spark.mllib.clustering</kbd>: These are k-means clustering algorithms</li>
<li><kbd>org.apache.spark.mllib.linalg</kbd>: These are data presentations, including dense vectors, sparse vectors, and matrices</li>
<li><kbd>org.apache.spark.mllib.optimization</kbd>: These are the various optimization algorithms that are used as low-level primitives in MLlib, including gradient descent, <strong>stochastic gradient descent</strong> (<strong>SGD</strong>), update schemes for distributed SGD, and the limited-memory <span><strong>Broyden–Fletcher–Goldfarb–Shanno</strong> (</span><strong>BFGS</strong>) algorithm</li>
<li><kbd>org.apache.spark.mllib.recommendation</kbd>: These are model-based collaborative filtering techniques implemented with alternating least squares matrix factorization</li>
<li><kbd>org.apache.spark.mllib.regression</kbd>: These are regression learning algorithms, such as linear least squares, decision trees, Lasso, and Ridge regression</li>
<li><kbd>org.apache.spark.mllib.stat</kbd>: These are statistical functions for samples in sparse or dense vector format to compute the mean, variance, minimum, maximum, counts, and nonzero counts</li>
<li><kbd>org.apache.spark.mllib.tree</kbd>: This implements classification and regression decision tree-learning algorithms</li>
<li><kbd>org.apache.spark.mllib.util</kbd>: These are a collection of methods used for loading, saving, preprocessing, generating, and validating the data</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deeplearning4j</h1>
                </header>
            
            <article>
                
<p>Deeplearning4j, or DL4J, is a deep learning library written in Java. It features a distributed as well as a single-machine deep learning framework that includes and supports various neural network structures such as feedforward neural networks, RBM, convolutional neural nets, deep belief networks, autoencoders, and others. DL4J can solve distinct problems, such as identifying faces, voices, spam, or e-commerce fraud.</p>
<p>Deeplearning4j is also distributed under the Apache 2.0 license and can be downloaded from <span class="URLPACKT"><a href="http://deeplearning4j.org/">http://deeplearning4j.org</a></span>. The library is organized as follows:</p>
<ul>
<li><kbd>org.deeplearning4j.base</kbd>: These are loading classes</li>
<li><kbd>org.deeplearning4j.berkeley</kbd>: These are math utility methods</li>
<li><kbd>org.deeplearning4j.clustering</kbd>: This is the implementation of k-means clustering</li>
<li><kbd>org.deeplearning4j.datasets</kbd>: This is dataset manipulation, including import, creation, iterating, and so on</li>
<li><kbd>org.deeplearning4j.distributions</kbd>: These are utility methods for distributions</li>
<li><kbd>org.deeplearning4j.eval</kbd>: These are evaluation classes, including the confusion matrix</li>
<li><kbd>org.deeplearning4j.exceptions</kbd>: This implements the exception handlers</li>
<li><kbd>org.deeplearning4j.models</kbd>: These are supervised learning algorithms, including deep belief networks, stacked autoencoders, stacked denoising autoencoders, and RBM</li>
<li><kbd>org.deeplearning4j.nn</kbd>: These are the implementations of components and algorithms based on neural networks, such as neural networks, multi-layer networks, convolutional multi-layer networks, and so on</li>
<li><kbd>org.deeplearning4j.optimize</kbd>: These are neural net optimization algorithms, including back propagation, multi-layer optimization, output layer optimization, and so on</li>
<li><kbd>org.deeplearning4j.plot</kbd>: These are various methods for rendering data</li>
<li><kbd>org.deeplearning4j.rng</kbd>: This is a random data generator</li>
<li><kbd>org.deeplearning4j.util</kbd>: These are helper and utility methods</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">MALLET</h1>
                </header>
            
            <article>
                
<p>The <strong>Machine Learning for Language Toolkit</strong> (<strong>MALLET</strong>) is a large library of natural language processing algorithms and utilities. It can be used in a variety of tasks such as document classification, document clustering, information extraction, and topic modelling. It features a command-line interface as well as a Java API for several algorithms such as Naive Bayes, HMM, Latent Dirichlet topic models, logistic regression, and conditional random fields.</p>
<p>MALLET is available under the Common Public License 1.0, which means that you can even use it in commercial applications. It can be downloaded from <span class="URLPACKT"><a href="http://mallet.cs.umass.edu/">http://mallet.cs.umass.edu</a></span>. A MALLET instance is represented by name, label, data, and source. However, there are two methods to import data into the MALLET format, as shown in the following list:</p>
<ul>
<li><strong>Instance per file</strong>: Each file or document corresponds to an instance and MALLET accepts the directory name for the input.</li>
<li><strong>Instance per line</strong>: Each line corresponds to an instance, where the following format is assumed—the <kbd>instance_name</kbd> label token. Data will be a feature vector, consisting of distinct words that appear as tokens and their occurrence count.</li>
</ul>
<p>The library is comprised of the following packages:</p>
<ul>
<li><kbd>cc.mallet.classify</kbd>: These are algorithms for training and classifying instances, including AdaBoost, bagging, C4.5, as well as other decision tree models, multivariate logistic regression, Naive Bayes, and Winnow2.</li>
<li><kbd>cc.mallet.cluster</kbd>: These are unsupervised clustering algorithms, including greedy agglomerative, hill climbing, k-best, and k-means clustering.</li>
<li><kbd>cc.mallet.extract</kbd>: This implements tokenizers, document extractors, document viewers, cleaners, and so on.</li>
<li><kbd>cc.mallet.fst</kbd>: This implements sequence models, including conditional random fields, HMM, maximum entropy Markov models, and corresponding algorithms and evaluators.</li>
<li><kbd>cc.mallet.grmm</kbd>: This implements graphical models and factor graphs such as inference algorithms, learning, and testing, for example, loopy belief propagation, Gibbs sampling, and so on.</li>
<li><kbd>cc.mallet.optimize</kbd>: These are optimization algorithms for finding the maximum of a function, such as gradient ascent, limited-memory BFGS, stochastic meta ascent, and so on.</li>
<li><kbd>cc.mallet.pipe</kbd>: These are methods as pipelines to process data into MALLET instances.</li>
<li><kbd>cc.mallet.topics</kbd>: These are topics modelling algorithms, such as Latent Dirichlet allocation, four-level pachinko allocation, hierarchical PAM, DMRT, and so on.</li>
<li><kbd>cc.mallet.types</kbd>: This implements fundamental data types such as dataset, feature vector, instance, and label.</li>
<li><kbd>cc.mallet.util</kbd>: These are miscellaneous utility functions such as command-line processing, search, math, test, and so on.</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Encog Machine Learning Framework</h1>
                </header>
            
            <article>
                
<p>Encog is a machine learning framework in Java/C# that was developed by Jeff Heaton, a data scientist. It supports normalizing and processing data and a variety of advanced algorithm such as SVM, Neural Networks, Bayesian Networks, Hidden Markov Models, Genetic Programming, and Genetic Algorithms. It has been actively developed since 2008. It supports multi-threading, which boosts performance on multi-core systems.</p>
<p>It can be found at <a href="https://www.heatonresearch.com/encog/">https://www.heatonresearch.com/encog/</a>. MLMethod is the base interface, which includes all of the methods for the models. The following are some of the interfaces and classes that it includes:</p>
<ul>
<li><kbd>MLRegression</kbd>: This interface defines regression algorithms</li>
<li><kbd>MLClassification</kbd>: This interface defines classification algorithms</li>
<li><kbd>MLClustering</kbd>: This interface defines clustering algorithms</li>
<li><kbd>MLData</kbd>: This class represents a vector used in a model, either for input or output</li>
<li><kbd>MLDataPair</kbd>: The functionality of this class is similar to that of <kbd>MLData</kbd>, but can be used for both input and output</li>
<li><kbd>MLDataSet</kbd>: This represents the list of <kbd>MLDataPair</kbd> instances for trainers</li>
<li><kbd>FreeformNeuron</kbd>: This class is used as a neuron</li>
<li><kbd>FreeformConnection</kbd>: This shows the weighted connection between neurons</li>
<li><kbd>FreeformContextNeuron</kbd>: This represents a context neuron</li>
<li><kbd>InputSummation</kbd>: This value specifies how the inputs are summed to form a single neuron</li>
<li><kbd>BasicActiveSummation</kbd>: This is the simple sum of all input neurons</li>
<li><kbd>BasicFreeConnection</kbd>: This is the basic weighted connection between neurons</li>
<li><kbd>BasicFreeformLayer</kbd>: This interface provides a layer</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">ELKI</h1>
                </header>
            
            <article>
                
<p>ELKI creates an environment for developing KDD applications supported by index structures, with an emphasis on unsupervised learning. It provides various implementations for cluster analysis and outlier detection. It provides index structures such as R*-tree for performance boosting and scalability. It is widely used in research areas by students and faculties up until now and has been gaining attention from other parties recently.</p>
<p>ELKI uses the AGPLv3 license, and can be found at <a href="https://elki-project.github.io/">https://elki-project.github.io/</a>. It is comprised of the following packages:</p>
<ul>
<li><kbd>de.lmu.ifi.dbs.elki.algorithm</kbd>: Contains various algorithms such as clustering, classification, itemset mining, and so on</li>
<li><kbd>de.lmu.ifi.dbs.elki.outlier</kbd>: Defines an outlier-based algorithm</li>
<li><kbd>de.lmu.ifi.dbs.elki.statistics</kbd>: Defines a statistical analysis algorithm</li>
<li><kbd>de.lmu.ifi.dbs.elki.database</kbd>: This is the ELKI database layer</li>
<li><kbd>de.lmu.ifi.dbs.elki.index</kbd>: This is for index structure implementation</li>
<li><kbd>de.lmu.ifi.dbs.elki.data</kbd>: Defines various data types and database object types</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">MOA</h1>
                </header>
            
            <article>
                
<p><strong>Massive Online Analysis</strong> (<strong>MOA</strong>) contains a vast collection of various machine learning algorithms that includes algorithms for classification, regression, clustering, outlier detection, concept drift detection and recommender system, and tools for evaluation. All algorithms are designed for large-scale machine learning, with the concept of drift and deals with big streams of real-time data. It also works and integrates well with Weka.</p>
<p>It is available as a GNU license and can be downloaded from <a href="https://moa.cms.waikato.ac.nz/">https://moa.cms.waikato.ac.nz/</a>. The following are its main packages:</p>
<ul>
<li><kbd>moa.classifiers</kbd>: Contains the algorithms for classification</li>
<li><kbd>moa.clusters</kbd>: Contains the algorithms for clustering</li>
<li><kbd>moa.streams</kbd>: Contains the classes related to working with streams</li>
<li><kbd>moa.evaluation</kbd>: Used for evaluating</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Comparing libraries</h1>
                </header>
            
            <article>
                
<p>The following table summarizes all of the presented libraries. The table is, by no means, exhaustive<span>—</span>there are many more libraries that cover specific problem domains. This review should serve as an overview of the big names in the Java machine learning world:</p>
<table style="border-collapse: collapse;width: 100%" class="table" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Libraries</strong></td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Problem domains</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>License</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Architecture</strong></p>
</td>
<td>
<p class="CDPAlignCenter CDPAlign"><strong>Algorithms</strong></p>
</td>
</tr>
<tr>
<td>
<p>Weka</p>
</td>
<td>
<p>General purpose</p>
</td>
<td>
<p>GNU GPL</p>
</td>
<td>
<p>Single machine</p>
</td>
<td>
<p>Decision trees, Naive Bayes, neural network, random forest, AdaBoost, hierarchical clustering, and so on</p>
</td>
</tr>
<tr>
<td>
<p>Java-ML</p>
</td>
<td>
<p>General purpose</p>
</td>
<td>
<p>GNU GPL</p>
</td>
<td>
<p>Single machine</p>
</td>
<td>
<p>K-means clustering, self-organizing maps, Markov chain clustering, Cobweb, random forest, decision trees, bagging, distance measures, and so on</p>
</td>
</tr>
<tr>
<td>
<p>Mahout</p>
</td>
<td>
<p>Classification, recommendation and clustering</p>
</td>
<td>
<p>Apache 2.0 License</p>
</td>
<td>
<p>Distributed single machine</p>
</td>
<td>
<p>Logistic regression, Naive Bayes, random forest, HMM, multilayer perceptron, k-means clustering, and so on</p>
</td>
</tr>
<tr>
<td>
<p>Spark</p>
</td>
<td>
<p>General purpose</p>
</td>
<td>
<p>Apache 2.0 License</p>
</td>
<td>
<p>Distributed</p>
</td>
<td>
<p>SVM, logistic regression, decision trees, Naive Bayes, k-means clustering, linear least squares, Lasso, ridge regression, and so on</p>
</td>
</tr>
<tr>
<td>
<p>DL4J</p>
</td>
<td>
<p>Deep learning</p>
</td>
<td>
<p>Apache 2.0 License</p>
</td>
<td>
<p>Distributed single machine</p>
</td>
<td>
<p>RBM, deep belief networks, deep autoencoders, recursive neural tensor networks, convolutional neural network, and stacked denoising autoencoders</p>
</td>
</tr>
<tr>
<td>
<p>MALLET</p>
</td>
<td>
<p>Text mining</p>
</td>
<td>
<p>Common Public License 1.0</p>
</td>
<td>
<p>Single machine</p>
</td>
<td>
<p>Naive Bayes, decision trees, maximum entropy, HMM, and conditional random fields</p>
</td>
</tr>
<tr>
<td>
<p>Encog</p>
</td>
<td>
<p>Machine Learning Framework</p>
</td>
<td>
<p>Apache 2.0 License</p>
</td>
<td>
<p>Cross Platform</p>
</td>
<td>
<p>SVM, Neural Network, Bayesian Networks, HMMs, Genetic Programming, and Genetic Algorithms</p>
</td>
</tr>
<tr>
<td>
<p>ELKI</p>
</td>
<td>
<p>Data Mining</p>
</td>
<td>
<p>AGPL</p>
</td>
<td>
<p>Distributed single machine</p>
</td>
<td>
<p>Cluster Detection, Anomaly Detection, Evaluation, Index</p>
</td>
</tr>
<tr>
<td>
<p>MOA</p>
</td>
<td>
<p>Machine Learning</p>
</td>
<td>
<p>GNU GPL</p>
</td>
<td>Distributed single machine</td>
<td>
<p>Classification, Regression, Clustering, Outlier Detection, Recommender System, Frequent Pattern Mining</p>
</td>
</tr>
</tbody>
</table>
<div class="packt_figure"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a machine learning application</h1>
                </header>
            
            <article>
                
<p>Machine learning applications, especially those focused on classification, usually follow the same high-level workflow that's shown in the following diagram. The workflow is comprised of two phases—training the classifier and the classification of new instances. Both phases share common steps, as shown here:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-533 image-border" src="Images/f2371275-8390-457f-bc94-2b1fb45251b9.png" style="width:54.58em;height:16.83em;" width="655" height="202"/></p>
<p>First, we use a set of training data, select a representative subset as the training set, preprocess the missing data, and extract its features. A selected supervised learning algorithm is used to train a model, which is deployed in the second phase. The second phase puts a new data instance through the same preprocessing and feature extraction procedure and applies the learned model to obtain the instance label. If you are able to collect new labelled data, periodically rerun the learning phase to retrain the model and replace the old one with the retrained one in the classification phase.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Traditional machine learning architecture</h1>
                </header>
            
            <article>
                
<p>Structured data, such as transactional, customers, analytical, and market data, usually resides within a local relational database. Given a query language, such as SQL, we can query the data used for processing, as shown in the workflow in the preceding diagram. Usually, all the data can be stored in memory and further processed with a machine learning library such as Weka, Java-ML, or MALLET.</p>
<p>A common practice in the architecture design is to create data pipelines, where different steps in the workflow are split. For instance, in order to create a client data record, we might have to scrap the data from different data sources. The record can be then saved in an intermediate database for further processing.</p>
<p>To understand how the high-level aspects of big data architecture differ, let's first clarify when data is considered big.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Dealing with big data</h1>
                </header>
            
            <article>
                
<p>Big data existed long before the phrase was invented. For instance, banks and stock exchanges have been processing billions of transactions daily for years and airline companies have worldwide real-time infrastructures for operational management of passenger booking, and so on. So, what is big data really? Doug Laney (2001) suggested that big data is defined by three Vs: volume, velocity, and variety. Therefore, to answer the question of whether your data is big, we can translate this into the following three sub-questions:</p>
<ul>
<li><strong>Volume</strong>: Can you store your data in memory?</li>
<li><strong>Velocity</strong>: Can you process new incoming data with a single machine?</li>
<li><strong>Variety</strong>: Is your data from a single source?</li>
</ul>
<p>If you answered all of these questions with yes, then your data is probably not big, and you have just simplified your application architecture.</p>
<p>If your answer to all of these questions was no, then your data is big! However, if you have mixed answers, then it's complicated. Some may argue that one V is important; others may say that the other Vs are more important. From a machine learning point of view, there is a fundamental difference in algorithm implementation in order process the data in memory or from distributed storage. Therefore, a rule of thumb is: if you cannot store your data in memory, then you should look into a big data machine learning library.</p>
<p>The exact answer depends on the problem that you are trying to solve. If you're starting a new project, I suggest that you start off with a single-machine library and prototype your algorithm, possibly with a subset of your data if the entire data does not fit into the memory. Once you've established good initial results, consider moving to something more heavy duty such as Mahout or Spark.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Big data application architecture</h1>
                </header>
            
            <article>
                
<p>Big data, such as documents, web blogs, social networks, sensor data, and others, are stored in a NoSQL database, such as MongoDB, or a distributed filesystem, such as HDFS. In case we deal with structured data, we can deploy database capabilities using systems such as Cassandra or HBase, which are built atop Hadoop. Data processing follows the MapReduce paradigm, which breaks data processing problems into smaller sub problems and distributes tasks across processing nodes. Machine learning models are finally trained with machine learning libraries such as Mahout and Spark.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_infobox"><span>MongoDB is a NoSQL database, which stores documents in a JSON-like format. You can read more about it at </span><span class="URLPACKT"><a href="https://www.mongodb.org/">https://www.mongodb.org</a></span><span>. Hadoop is a framework for the distributed processing of large datasets across a cluster of computers. It includes its own filesystem format, HDFS, job scheduling framework, YARD, and implements the MapReduce approach for parallel data processing. We can learn more about Hadoop at </span><span class="URLPACKT"><a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></span><span>. Cassandra is a distributed database management system that was built to provide fault-tolerant, scalable, and decentralized storage. More information is available at </span><span class="URLPACKT"><a href="http://cassandra.apache.org/">http://cassandra.apache.org/</a></span><span>. HBase is another database that focuses on random read/write access for distributed storage. More information is available at </span><span class="URLPACKT"><a href="https://hbase.apache.org/">https://hbase.apache.org/</a></span><span>.</span></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Selecting a machine learning library has an important impact on your application architecture. The key is to consider your project requirements. What kind of data do you have? What kind of problem are you trying to solve? Is your data big? Do you need distributed storage? What kind of algorithm are you planning to use? Once you figure out what you need to solve your problem, pick a library that best fits your needs.</p>
<p>In the next chapter, we will cover how to complete basic machine learning tasks such as classification, regression, and clustering by using some of the presented libraries.</p>


            </article>

            
        </section>
    </div>



  </body></html>