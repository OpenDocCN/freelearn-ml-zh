- en: '15'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ethics in Machine Learning Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ethics involves data acquisition and management and focuses on collecting data,
    with a particular focus on protecting individuals and organizations from any harm
    that could be inflicted upon them. However, data is not the only source of bias
    in **machine learning** (**ML**) systems.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms and ways of data processing are also prone to introducing bias to
    the data. Despite our best efforts, some of the steps in data processing may even
    emphasize the bias and let it spread beyond algorithms and toward other parts
    of ML-based systems, such as user interfaces or decision-making components.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this chapter, we’ll focus on the bias in ML systems. We’ll start
    by exploring sources of bias and briefly discussing these sources. Then, we’ll
    explore ways to spot biases, how to minimize them, and finally how to communicate
    potential bias to the users of our system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Bias and ML – is it possible to have an objective AI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring and monitoring for bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing mechanisms to prevent ML bias from spreading in the entire system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias and ML – is it possible to have an objective AI?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the intertwined domains of ML and software engineering, the allure of data-driven
    decision-making and predictive modeling is undeniable. These fields, which once
    operated largely in silos, now converge in numerous applications, from software
    development tools to automated testing frameworks. However, as we increasingly
    rely on data and algorithms, a pressing concern emerges: the issue of bias. Bias,
    in this context, refers to systematic and unfair discrepancies that can manifest
    in the decisions and predictions of ML models, often stemming from the very data
    used in software engineering processes.'
  prefs: []
  type: TYPE_NORMAL
- en: The sources of bias in software engineering data are multifaceted. They can
    arise from historical project data, user feedback loops, or even the design and
    objectives of the software itself. For instance, if a software tool is predominantly
    tested and refined using feedback from a specific demographic, it might inadvertently
    underperform or misbehave for users outside that group. Similarly, a defect prediction
    model might be skewed if trained on data from projects that lack diversity in
    team composition or coding practices.
  prefs: []
  type: TYPE_NORMAL
- en: The implications of such biases extend beyond mere technical inaccuracies. They
    can lead to software products that alienate or disadvantage certain user groups,
    perpetuating and amplifying existing societal inequalities. For example, a development
    environment might offer suggestions that resonate more with one cultural context
    than another, or a software recommendation system might favor applications from
    well-known developers, sidelining newcomers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, bias is defined to be an inclination or prejudice for, or against,
    one person or group. In ML, bias is when a model systematically produces prejudiced
    results. There are several types of bias in ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prejudicial bias**: This is a type of bias that is present in the empirical
    world and made its way into ML models and algorithms – both knowingly and unknowingly.
    An example is racial bias or gender bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurement bias**: This is a type of bias that is introduced through a systematic
    error in our measurement instruments. For example, we measure the McCabe complexity
    of software modules by counting the if/for statements, excluding the while loops.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling bias**: This is a type of bias that occurs when our sample does
    not reflect the real distribution of data. It can be the case that we sample too
    often or too seldom from a specific class – such a bias in our data will affect
    inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm bias**: This is a type of bias that occurs when we use the wrong
    algorithm for the task at hand. A wrong algorithm may not generalize well and
    therefore it may introduce bias into the inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confirmation bias**: This is a type of bias that is introduced when we remove/select
    data points that are aligned with the theoretical notions that we want to capture.
    By doing this, we introduce the bias that confirms our theory, rather than reflecting
    the empirical world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list is, by no means, exclusive. Bias can be introduced in many ways and
    through many ways, but it is always our responsibility to identify it, monitor
    it, and reduce it.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there are a few frameworks that can allow us to identify bias – Fair
    ML, IBM AI Fairness 360, and Microsoft Fairlearn, just to name a few. These frameworks
    allow us to scrutinize our algorithms and datasets in search of the most common
    biases.
  prefs: []
  type: TYPE_NORMAL
- en: Donald et al. present a recent overview of methods and tools for reducing bias
    in software engineering, which includes ML. The important part of that article
    is that it focuses on use cases, which is important for understanding bias; bias
    is not something universal but depends on the dataset and the use case of that
    data. In addition to the sources of bias presented previously, they also recognize
    that bias is something that can change over time, just as our society changes
    and just as our data changes. Although Donald et al.’s work is generic, it tends
    to focus on one of the data types – natural language – and how bias can be present.
    They provide an overview of tools and techniques that can help identify such phenomena
    as hateful language.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, however, we’ll focus on one of the frameworks that is a bit
    more generic to illustrate how to work with bias in general.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring and monitoring for bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s look at one of these frameworks – IBM AI Fairness 360 ([https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)).
    The basis for this framework is the ability to set variables that can be linked
    to bias and then calculate how different the other variables are. So, let’s dive
    into an example of how to calculate bias for a dataset. Since bias is often associated
    with gender or similar attributes, we need to use a dataset that contains it.
    So far in this book, we have not used any dataset that contained this kind of
    attribute, so we need to find another one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take the Titanic survival dataset to check if there was any bias in terms
    of survivability between male and female passengers. First, we need to install
    the IBM AI Fairness 360 framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can start creating a program that will check for bias. We need to
    import the appropriate libraries and create the data. In this example, we’ll create
    the data of salaries, which is biased toward men:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This data contains four different attributes – age, income, gender, and whether
    the person is recommended to be hired or not. It is difficult to spot whether
    there is a bias between the genders, but let’s apply the IBM fairness algorithm
    to check for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code creates a data split and calculates the fairness metric
    – dataset disparity. The important part of the algorithm is where we set the protected
    attribute – gender (`protected_attribute_names=[''Gender''])`). We manually set
    the attribute that we think could be prone to bias, which is an important observation.
    The fairness framework does not set any attributes automatically. Then, we set
    which values of this attribute indicate the privileged and unprivileged groups
    – `unprivileged_groups=[{''Gender'': 1}]`. Once the code executes, we get an understanding
    of whether there is bias in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This means that the algorithm could reduce the disparity but did not remove
    it completely. The disparity value of 0.86 means that there is a bias toward the
    privileged group (in this case males). The value of 0.5 means that the bias is
    reduced, but it is still far from 0.0, which would indicate the lack of bias.
    The fact that the bias was reduced and not removed can indicate that there is
    just too little data to be able to reduce the bias completely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, let’s take a look at the real dataset, which can contain a bias
    – the Titanic dataset. The dataset contains protected attributes such as gender
    and it is significantly larger so that we have a better chance to reduce the bias
    even more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the dataset in place, we can write the script that will calculate
    the disparity metrics, which quantifies how much difference there is in the data
    based on the controlled variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we need to convert the `''Sex''` column of the DataFrame, `df`, into
    a binary format: `1` for male and `0` for female. Then, we need to drop the `''Name''`
    column from the DataFrame as it could be confused with the index. Then, the data
    is split into training and testing sets using the `train_test_split` function.
    20% of the data (`test_size=0.2`) is reserved for testing, and the rest is used
    for training. `random_state=42` ensures the reproducibility of the split.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we convert the training and testing DataFrames into a `BinaryLabelDataset`
    format, which is a specific format used by the fairness framework. The target
    variable (or label) is `''Survived''`, and the protected attribute (that is, the
    attribute we’re concerned about in terms of fairness) is `''Sex''`. The framework
    considers females (`''Sex'': 0`) as the unprivileged group and males (`''Sex'':
    1`) as the privileged group.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mean_difference` method computes the difference in mean outcomes between
    the privileged and unprivileged groups. A value of 0 indicates perfect fairness,
    while a non-zero value indicates some disparity. Then, the code uses the `Reweighing`
    method to mitigate bias in the training dataset. This method assigns weights to
    the instances in the dataset to ensure fairness. The transformed dataset (`train_bld_transformed`)
    has these new weights. Then, we calculate the same metric on the transformed dataset.
    This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the algorithm has balanced the datasets so that the survival
    rate is the same for male and female passengers. We can now use this dataset to
    train a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: First, we initialize `StandardScaler`. This scaler standardizes features by
    removing the mean and scaling to unit variance. Then, we transform and standardize
    the features of the training dataset (`train_bld_transformed.features`) using
    the `fit_transform` method of the scaler. The standardized features are stored
    in `X_train`. Then, we extract the labels of the transformed training dataset
    using the `ravel()` method, resulting in `y_train`. After, we train the logistic
    regression classifier (`clf`) using the standardized features (`X_train`) and
    labels (`y_train`).
  prefs: []
  type: TYPE_NORMAL
- en: Then, we standardize the features of the test dataset (`test_bld.features`)
    using the transform method of the scaler to obtain `X_test`. We do the same with
    the `y_test` data. We use the trained classifier (`clf`) to make predictions on
    the standardized test features and store them in `y_pred`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we calculate the evaluation scores for the dataset and print the report
    with accuracy, precision, and recall.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve come to my best practice related to bias.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #73'
  prefs: []
  type: TYPE_NORMAL
- en: If a dataset contains variables that can be prone to bias, use the disparity
    metric to get a quick orientation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to check for bias, although we do not always have access to
    the variables that we need for its calculations, such as gender or age. If we
    do not, we should look for attributes that can be correlated with them and check
    for bias against these attributes.
  prefs: []
  type: TYPE_NORMAL
- en: Other metrics of bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The dataset disparity metrics that we’ve used so far are only some of the metrics
    related to bias. Some of the other metrics that are available in the IBM AI Fairness
    360 framework are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**True positive rate**: The ratio of true positives conditioned on the protected
    attribute. This is usually used for classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False discovery rate**: The difference between the false discovery ratio
    between the privileged and unprivileged groups in classification tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generalize binary confusion matrix**: The confusion matrix conditioner on
    the protected attributes in the classification tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ratio between privileged and unprivileged instances, which can be used for
    all kinds of tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several metrics in addition to these, but the ones we’ve covered here
    illustrate the most important point – or two points. First of all, we can see
    that there needs to be an attribute, called the protected attribute, which can
    help us understand the bias. Without such an attribute, the framework cannot do
    any calculations and therefore it cannot provide any useful feedback for the developers.
    The second point is the fact that the metrics are based on the unbalance between
    different groups – privileged and unprivileged – which we define ourselves. We
    cannot use this framework to discover bias that is hidden.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden biases are biases that are not directly represented by attributes. For
    example, there are differences in the occupations that men and women have, and
    therefore the occupation can be such an attribute that is correlated to the gender,
    but not equal to it. This means that we cannot treat this as a protected attribute,
    but we need to consider it – basically, there are no occupations that are purely
    male or purely female occupations, but different occupations have different proportions
    of men and women.
  prefs: []
  type: TYPE_NORMAL
- en: Developing mechanisms to prevent ML bias from spreading throughout the system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unfortunately, it is generally not possible to completely remove bias from ML
    as we often do not have access to the attributes needed to reduce the bias. However,
    we can reduce the bias and reduce the risk that the bias spreads to the entire
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Awareness and education are some of the most important measures that we can
    use to manage bias in software systems. We need to understand the potential sources
    of bias and their implications. We also need to identify biases related to protected
    attributes (for example, gender) and identify whether other attributes can be
    correlated with them (for example, occupation and address). Then, we need to educate
    our team about the ethical implications of biased models.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we need to diversify our data collection. We must ensure that the data
    we collect is representative of the population we’re to model. To avoid over-representing
    or under-representing certain groups, we need to ensure that our data collection
    procedures are scrutinized before they are applied. We also need to monitor for
    biases in the collected data and reduce them. For example, if we identify a bias
    in credit scores, we can introduce the data that will prevent this bias from being
    strengthened by our model.
  prefs: []
  type: TYPE_NORMAL
- en: During data preprocessing, we need to ensure that we handle missing data correctly.
    Instead of just removing the data points or imputing them with mean values, we
    should use the right imputation, which takes care of the differences between the
    privileged and unprivileged groups.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to actively work with the detection of bias. We should use statistical
    tests to check whether the data distribution is biased toward certain groups,
    at which point we need to visualize the distributions and identify potential bias.
    We’ve already discussed visualization techniques; at this point, we can add that
    we need to use different symbols for privileged and unprivileged groups to visualize
    two distributions on the same diagram, for example.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to working with the data, we also need to work with algorithmic
    fairness, which is when we design the models. We need to set fairness constraints
    and we need to introduce the attributes that can help us to identify privileged
    and unprivileged groups. For example, if we know that different occupations have
    a certain bias toward genders, we need to introduce superficial gender-bias attributes
    that can help us to create a model that will take that into account and prevent
    the bias from spreading to other parts of our system. We can also make post-hoc
    adjustments to the model after training. For example, when predicting a salary,
    we can adjust that salary based on pre-defined rules after the prediction. That
    can help to reduce the biases inherent in the model.
  prefs: []
  type: TYPE_NORMAL
- en: We can also use fairness-enhancing interventions, such as the IBM Fairness tools
    and techniques, which include debiasing, reweighing, and disparate impact removal.
    This can help us to achieve interpretable models or allow us to use model interpretation
    tools to understand how decisions are being made. This can help in identifying
    and rectifying bias.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we can regularly audit our models for bias and fairness. This includes
    both automated checks and human reviews. This helps us understand whether there
    are biases that cannot be captured automatically and that we need to react to.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have come to my next best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Best practice #74'
  prefs: []
  type: TYPE_NORMAL
- en: Complement automated bias management with regular audits.
  prefs: []
  type: TYPE_NORMAL
- en: We need to accept the fact that bias is inherent in data, so we need to act
    accordingly. Instead of relying on algorithms to detect bias, we need to manually
    monitor for bias and understand it. Therefore, I recommend making regular checks
    for bias manually. Make classifications and predictions and check whether they
    strengthen or reduce bias by comparing them to the expected data without bias.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of our responsibilities as software engineers is to ensure that we develop
    software systems that contribute to the greater good of society. We love working
    with technology development, but the technology needs to be developed responsibly.
    In this chapter, we looked at the concept of bias in ML and how to work with it.
    We looked at the IBM Fairness framework, which can assist us in identifying bias.
    We also learned that automated bias detection is too limited to be able to remove
    bias from the data completely.
  prefs: []
  type: TYPE_NORMAL
- en: There are more frameworks to explore and more studies and tools are available
    every day. These frameworks are more specific and provide a means to capture more
    domain-specific bias – in medicine and advertising. Therefore, my final recommendation
    in this chapter is to explore the bias frameworks that are specific to the task
    at hand and for the domain at hand.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Donald, A., et al., Bias Detection for Customer Interaction Data: A Survey
    on Datasets, Methods, and Tools. IEEE* *Access, 2023.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bellamy, R.K., et al., AI Fairness 360: An extensible toolkit for detecting,
    understanding, and mitigating unwanted algorithmic bias. arXiv preprint* *arXiv:1810.01943,
    2018.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zhang, Y., et al. Introduction to AI fairness. In Extended Abstracts of the
    2020 CHI Conference on Human Factors in Computing* *Systems. 2020.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Alves, G., et al. Reducing unintended bias of ml models on tabular and textual
    data. In 2021 IEEE 8th International Conference on Data Science and Advanced Analytics
    (DSAA).* *2021\. IEEE.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Raza, S., D.J. Reji, and C. Ding, Dbias: detecting biases and ensuring fairness
    in news articles. International Journal of Data Science and Analytics, 2022:*
    *p. 1-21.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
