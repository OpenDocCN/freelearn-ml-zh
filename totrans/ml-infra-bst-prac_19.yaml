- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Ethics in Machine Learning Systems
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习系统的伦理
- en: Ethics involves data acquisition and management and focuses on collecting data,
    with a particular focus on protecting individuals and organizations from any harm
    that could be inflicted upon them. However, data is not the only source of bias
    in **machine learning** (**ML**) systems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理涉及数据获取和管理，重点是收集数据，特别关注保护个人和组织免受可能对他们造成的任何伤害。然而，数据并不是机器学习（ML）系统中偏见的唯一来源。
- en: Algorithms and ways of data processing are also prone to introducing bias to
    the data. Despite our best efforts, some of the steps in data processing may even
    emphasize the bias and let it spread beyond algorithms and toward other parts
    of ML-based systems, such as user interfaces or decision-making components.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 算法和数据处理方式也容易引入数据偏见。尽管我们尽了最大努力，数据处理的一些步骤甚至可能强调偏见，使其超出算法范围，向基于机器学习的系统其他部分扩散，例如用户界面或决策组件。
- en: Therefore, in this chapter, we’ll focus on the bias in ML systems. We’ll start
    by exploring sources of bias and briefly discussing these sources. Then, we’ll
    explore ways to spot biases, how to minimize them, and finally how to communicate
    potential bias to the users of our system.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本章中，我们将关注机器学习系统中的偏见。我们将首先探讨偏见的来源，并简要讨论这些来源。然后，我们将探讨发现偏见的方法、如何最小化偏见，以及最后如何向我们的系统用户传达潜在的偏见。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Bias and ML – is it possible to have an objective AI?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见与机器学习 – 是否可能拥有一个客观的人工智能？
- en: Measuring and monitoring for bias
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量和监控偏见
- en: Reducing bias
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少偏见
- en: Developing mechanisms to prevent ML bias from spreading in the entire system
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制定机制以防止机器学习偏见在整个系统中扩散
- en: Bias and ML – is it possible to have an objective AI?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏见与机器学习 – 是否可能拥有一个客观的人工智能？
- en: 'In the intertwined domains of ML and software engineering, the allure of data-driven
    decision-making and predictive modeling is undeniable. These fields, which once
    operated largely in silos, now converge in numerous applications, from software
    development tools to automated testing frameworks. However, as we increasingly
    rely on data and algorithms, a pressing concern emerges: the issue of bias. Bias,
    in this context, refers to systematic and unfair discrepancies that can manifest
    in the decisions and predictions of ML models, often stemming from the very data
    used in software engineering processes.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和软件工程的交织领域中，数据驱动决策和预测建模的吸引力无可否认。这些曾经主要在孤岛中运作的领域，现在在众多应用中汇聚，从软件开发工具到自动化测试框架。然而，随着我们越来越依赖数据和算法，一个紧迫的问题出现了：偏见问题。在这个背景下，偏见指的是在机器学习模型的决策和预测中表现出的系统性和不公平的差异，通常源于软件工程过程中的数据。
- en: The sources of bias in software engineering data are multifaceted. They can
    arise from historical project data, user feedback loops, or even the design and
    objectives of the software itself. For instance, if a software tool is predominantly
    tested and refined using feedback from a specific demographic, it might inadvertently
    underperform or misbehave for users outside that group. Similarly, a defect prediction
    model might be skewed if trained on data from projects that lack diversity in
    team composition or coding practices.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程数据中偏见的来源是多方面的。它们可能源于历史项目数据、用户反馈循环，甚至软件本身的设计和目标。例如，如果一个软件工具主要使用特定人群的反馈进行测试和改进，它可能会无意中在那些群体之外的用户中表现不佳或行为不当。同样，如果训练数据来自缺乏团队构成或编码实践多样性的项目，缺陷预测模型可能会出现偏差。
- en: The implications of such biases extend beyond mere technical inaccuracies. They
    can lead to software products that alienate or disadvantage certain user groups,
    perpetuating and amplifying existing societal inequalities. For example, a development
    environment might offer suggestions that resonate more with one cultural context
    than another, or a software recommendation system might favor applications from
    well-known developers, sidelining newcomers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种偏见的后果不仅限于技术上的不准确。它们可能导致软件产品使某些用户群体感到疏远或处于不利地位，从而持续和放大现有的社会不平等。例如，一个开发环境可能对某一文化背景的提议比对另一文化背景的提议更响亮，或者一个软件推荐系统可能会偏向于知名开发者的应用程序，而忽视新来者。
- en: 'Generally, bias is defined to be an inclination or prejudice for, or against,
    one person or group. In ML, bias is when a model systematically produces prejudiced
    results. There are several types of bias in ML:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，偏差被定义为对某个人或群体的倾向或偏见。在机器学习中，偏差是指模型系统地产生有偏见的结果。机器学习中存在几种类型的偏差：
- en: '**Prejudicial bias**: This is a type of bias that is present in the empirical
    world and made its way into ML models and algorithms – both knowingly and unknowingly.
    An example is racial bias or gender bias.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏见偏差**：这是一种存在于经验世界中并进入机器学习模型和算法中的偏差——无论是故意还是无意。一个例子是种族偏见或性别偏见。'
- en: '**Measurement bias**: This is a type of bias that is introduced through a systematic
    error in our measurement instruments. For example, we measure the McCabe complexity
    of software modules by counting the if/for statements, excluding the while loops.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测量偏差**：这是一种通过我们测量工具中的系统性错误引入的偏差。例如，我们通过计算if/for语句来衡量软件模块的McCabe复杂性，而排除while循环。'
- en: '**Sampling bias**: This is a type of bias that occurs when our sample does
    not reflect the real distribution of data. It can be the case that we sample too
    often or too seldom from a specific class – such a bias in our data will affect
    inference.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样偏差**：这是一种当我们的样本不能反映数据的真实分布时出现的偏差。可能的情况是我们从特定类别中采样过于频繁或过于稀少——这种偏差会影响推理。'
- en: '**Algorithm bias**: This is a type of bias that occurs when we use the wrong
    algorithm for the task at hand. A wrong algorithm may not generalize well and
    therefore it may introduce bias into the inference.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法偏差**：这是一种在我们使用错误的算法来完成手头任务时出现的偏差。一个错误的算法可能无法很好地泛化，因此它可能会在推理中引入偏差。'
- en: '**Confirmation bias**: This is a type of bias that is introduced when we remove/select
    data points that are aligned with the theoretical notions that we want to capture.
    By doing this, we introduce the bias that confirms our theory, rather than reflecting
    the empirical world.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确认偏差**：这是一种在我们移除/选择与我们要捕捉的理论概念一致的数据点时引入的偏差。通过这样做，我们引入了证实我们理论的偏差，而不是反映经验世界。'
- en: This list is, by no means, exclusive. Bias can be introduced in many ways and
    through many ways, but it is always our responsibility to identify it, monitor
    it, and reduce it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表绝不是排他的。偏差可以通过许多方式以多种方式引入，但始终是我们的责任去识别它、监控它并减少它。
- en: Luckily, there are a few frameworks that can allow us to identify bias – Fair
    ML, IBM AI Fairness 360, and Microsoft Fairlearn, just to name a few. These frameworks
    allow us to scrutinize our algorithms and datasets in search of the most common
    biases.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些框架可以让我们识别偏差——公平机器学习、IBM AI公平360和微软Fairlearn，仅举几个例子。这些框架允许我们仔细审查我们的算法和数据集，以寻找最常见的偏差。
- en: Donald et al. present a recent overview of methods and tools for reducing bias
    in software engineering, which includes ML. The important part of that article
    is that it focuses on use cases, which is important for understanding bias; bias
    is not something universal but depends on the dataset and the use case of that
    data. In addition to the sources of bias presented previously, they also recognize
    that bias is something that can change over time, just as our society changes
    and just as our data changes. Although Donald et al.’s work is generic, it tends
    to focus on one of the data types – natural language – and how bias can be present.
    They provide an overview of tools and techniques that can help identify such phenomena
    as hateful language.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Donald等人最近概述了减少软件工程中偏差的方法和工具，包括机器学习。那篇文章的重要部分是它侧重于用例，这对于理解偏差很重要；偏差不是普遍存在的，而是取决于数据集和该数据的使用案例。除了之前提出的偏差来源外，他们还认识到偏差是随着时间的推移而变化的，就像我们的社会变化和我们的数据变化一样。尽管Donald等人的工作具有普遍性，但它倾向于关注一种数据类型——自然语言——以及偏差可能存在的方式。他们概述了可以帮助识别诸如仇恨言论等现象的工具和技术。
- en: In this chapter, however, we’ll focus on one of the frameworks that is a bit
    more generic to illustrate how to work with bias in general.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，然而，我们将关注一个稍微更通用的框架，以说明如何一般性地处理偏差问题。
- en: Measuring and monitoring for bias
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量和监控偏差
- en: Let’s look at one of these frameworks – IBM AI Fairness 360 ([https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)).
    The basis for this framework is the ability to set variables that can be linked
    to bias and then calculate how different the other variables are. So, let’s dive
    into an example of how to calculate bias for a dataset. Since bias is often associated
    with gender or similar attributes, we need to use a dataset that contains it.
    So far in this book, we have not used any dataset that contained this kind of
    attribute, so we need to find another one.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些框架中的一个——IBM AI公平性360（[https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)）。这个框架的基础是能够设置可以与偏见相关联的变量，然后计算其他变量之间的差异。所以，让我们深入一个如何计算数据集偏见的例子。由于偏见通常与性别或类似属性相关联，我们需要使用包含这种属性的数据集。到目前为止，在这本书中，我们还没有使用过包含这种属性的数据集，因此我们需要找到另一个。
- en: 'Let’s take the Titanic survival dataset to check if there was any bias in terms
    of survivability between male and female passengers. First, we need to install
    the IBM AI Fairness 360 framework:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以泰坦尼克号生存数据集来检查男性和女性乘客在生存方面的偏见。首先，我们需要安装IBM AI公平性360框架：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we can start creating a program that will check for bias. We need to
    import the appropriate libraries and create the data. In this example, we’ll create
    the data of salaries, which is biased toward men:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以开始创建一个检查偏见的程序。我们需要导入适当的库并创建数据。在这个例子中，我们将创建薪资数据，该数据倾向于男性：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This data contains four different attributes – age, income, gender, and whether
    the person is recommended to be hired or not. It is difficult to spot whether
    there is a bias between the genders, but let’s apply the IBM fairness algorithm
    to check for that:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这份数据包含四个不同的属性——年龄、收入、性别以及是否建议雇佣这个人。很难发现性别之间是否存在偏见，但让我们应用IBM公平性算法来检查这一点：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code creates a data split and calculates the fairness metric
    – dataset disparity. The important part of the algorithm is where we set the protected
    attribute – gender (`protected_attribute_names=[''Gender''])`). We manually set
    the attribute that we think could be prone to bias, which is an important observation.
    The fairness framework does not set any attributes automatically. Then, we set
    which values of this attribute indicate the privileged and unprivileged groups
    – `unprivileged_groups=[{''Gender'': 1}]`. Once the code executes, we get an understanding
    of whether there is bias in the dataset:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '上述代码创建了一个数据分割并计算了公平性指标——数据集差异。算法的重要部分在于我们设置了受保护属性——性别（`protected_attribute_names=[''Gender'']`）。我们手动设置了我们认为可能存在偏见的属性，这是一个重要的观察。公平性框架不会自动设置任何属性。然后，我们设置了该属性的哪些值表示特权组和非特权组——`unprivileged_groups=[{''Gender'':
    1}]`。一旦代码执行，我们就能了解数据集中是否存在偏见：'
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This means that the algorithm could reduce the disparity but did not remove
    it completely. The disparity value of 0.86 means that there is a bias toward the
    privileged group (in this case males). The value of 0.5 means that the bias is
    reduced, but it is still far from 0.0, which would indicate the lack of bias.
    The fact that the bias was reduced and not removed can indicate that there is
    just too little data to be able to reduce the bias completely.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着算法可以减少差异，但并没有完全消除。差异值0.86表示对特权组（在这种情况下是男性）存在偏见。值0.5表示偏见已经减少，但仍然远未达到0.0，这会表明没有偏见。偏见减少而没有被消除的事实可能表明数据量太少，无法完全减少偏见。
- en: 'Therefore, let’s take a look at the real dataset, which can contain a bias
    – the Titanic dataset. The dataset contains protected attributes such as gender
    and it is significantly larger so that we have a better chance to reduce the bias
    even more:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们看看实际的包含偏见的真实数据集——泰坦尼克号数据集。该数据集包含受保护属性，如性别，并且它非常大，这样我们就有更好的机会进一步减少偏见：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have the dataset in place, we can write the script that will calculate
    the disparity metrics, which quantifies how much difference there is in the data
    based on the controlled variable:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据集，我们可以编写脚本计算差异度量，该度量量化了基于控制变量的数据差异程度：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'First, we need to convert the `''Sex''` column of the DataFrame, `df`, into
    a binary format: `1` for male and `0` for female. Then, we need to drop the `''Name''`
    column from the DataFrame as it could be confused with the index. Then, the data
    is split into training and testing sets using the `train_test_split` function.
    20% of the data (`test_size=0.2`) is reserved for testing, and the rest is used
    for training. `random_state=42` ensures the reproducibility of the split.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将DataFrame `df` 中的`'Sex'`列转换为二进制格式：男性为`1`，女性为`0`。然后，我们需要从DataFrame中删除`'Name'`列，因为它可能会与索引混淆。然后，使用`train_test_split`函数将数据分为训练集和测试集。20%的数据（`test_size=0.2`）保留用于测试，其余用于训练。`random_state=42`确保分割的可重复性。
- en: 'Next, we convert the training and testing DataFrames into a `BinaryLabelDataset`
    format, which is a specific format used by the fairness framework. The target
    variable (or label) is `''Survived''`, and the protected attribute (that is, the
    attribute we’re concerned about in terms of fairness) is `''Sex''`. The framework
    considers females (`''Sex'': 0`) as the unprivileged group and males (`''Sex'':
    1`) as the privileged group.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们将训练和测试的DataFrame转换为`BinaryLabelDataset`格式，这是公平框架使用的特定格式。目标变量（或标签）是`''Survived''`，受保护的属性（即我们在公平性方面关注的属性）是`''Sex''`。该框架将女性（`''Sex'':
    0`）视为无特权群体，将男性（`''Sex'': 1`）视为特权群体。'
- en: 'The `mean_difference` method computes the difference in mean outcomes between
    the privileged and unprivileged groups. A value of 0 indicates perfect fairness,
    while a non-zero value indicates some disparity. Then, the code uses the `Reweighing`
    method to mitigate bias in the training dataset. This method assigns weights to
    the instances in the dataset to ensure fairness. The transformed dataset (`train_bld_transformed`)
    has these new weights. Then, we calculate the same metric on the transformed dataset.
    This results in the following output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`mean_difference`方法计算特权群体和无特权群体之间平均结果的差异。0值表示完全公平，而非零值表示存在一些差异。然后，代码使用`Reweighing`方法来减轻训练数据集中的偏差。这种方法通过给数据集中的实例分配权重来确保公平性。转换后的数据集（`train_bld_transformed`）具有这些新的权重。然后，我们在转换后的数据集上计算相同的指标。这导致以下输出：'
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This means that the algorithm has balanced the datasets so that the survival
    rate is the same for male and female passengers. We can now use this dataset to
    train a model:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着算法已经平衡了数据集，使得男性和女性的生存率相同。现在我们可以使用这个数据集来训练一个模型：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we initialize `StandardScaler`. This scaler standardizes features by
    removing the mean and scaling to unit variance. Then, we transform and standardize
    the features of the training dataset (`train_bld_transformed.features`) using
    the `fit_transform` method of the scaler. The standardized features are stored
    in `X_train`. Then, we extract the labels of the transformed training dataset
    using the `ravel()` method, resulting in `y_train`. After, we train the logistic
    regression classifier (`clf`) using the standardized features (`X_train`) and
    labels (`y_train`).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化`StandardScaler`。这个缩放器通过去除均值并缩放到单位方差来标准化特征。然后，我们使用缩放器的`fit_transform`方法对训练数据集的特征（`train_bld_transformed.features`）进行转换和标准化。标准化的特征存储在`X_train`中。然后，我们使用`ravel()`方法从转换后的训练数据集中提取标签，得到`y_train`。之后，我们使用标准化的特征（`X_train`）和标签（`y_train`）来训练逻辑回归分类器（`clf`）。
- en: Then, we standardize the features of the test dataset (`test_bld.features`)
    using the transform method of the scaler to obtain `X_test`. We do the same with
    the `y_test` data. We use the trained classifier (`clf`) to make predictions on
    the standardized test features and store them in `y_pred`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用缩放器的转换方法对测试数据集的特征（`test_bld.features`）进行标准化，以获得`X_test`。我们对`y_test`数据也进行同样的操作。我们使用训练好的分类器（`clf`）对标准化的测试特征进行预测，并将结果存储在`y_pred`中。
- en: Finally, we calculate the evaluation scores for the dataset and print the report
    with accuracy, precision, and recall.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算数据集的评估分数，并打印包含准确率、精确率和召回率的报告。
- en: With that, we’ve come to my best practice related to bias.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就来到了关于偏差的最佳实践。
- en: 'Best practice #73'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #73'
- en: If a dataset contains variables that can be prone to bias, use the disparity
    metric to get a quick orientation of the data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集包含可能存在偏差的变量，请使用差异指标来快速了解数据。
- en: It is important to check for bias, although we do not always have access to
    the variables that we need for its calculations, such as gender or age. If we
    do not, we should look for attributes that can be correlated with them and check
    for bias against these attributes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们并不总是能够访问用于其计算的变量，例如性别或年龄，检查偏差是很重要的。如果我们没有，我们应该寻找可以与之相关的属性，并检查对这些属性的偏差。
- en: Other metrics of bias
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他偏差度量标准
- en: 'The dataset disparity metrics that we’ve used so far are only some of the metrics
    related to bias. Some of the other metrics that are available in the IBM AI Fairness
    360 framework are as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止使用的数据集差异度量标准只是与偏差相关的一些度量标准。IBM AI Fairness 360框架中可用的其他一些度量标准如下：
- en: '**True positive rate**: The ratio of true positives conditioned on the protected
    attribute. This is usually used for classification.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正率**：在受保护属性条件下的真正率的比率。这通常用于分类。'
- en: '**False discovery rate**: The difference between the false discovery ratio
    between the privileged and unprivileged groups in classification tasks.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假发现率**：在分类任务中，特权组和未特权组之间假发现率的差异。'
- en: '**Generalize binary confusion matrix**: The confusion matrix conditioner on
    the protected attributes in the classification tasks.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用二元混淆矩阵**：在分类任务中对受保护属性进行混淆矩阵的条件。'
- en: The ratio between privileged and unprivileged instances, which can be used for
    all kinds of tasks.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特权实例与未特权实例之间的比率，可用于各种任务。
- en: There are several metrics in addition to these, but the ones we’ve covered here
    illustrate the most important point – or two points. First of all, we can see
    that there needs to be an attribute, called the protected attribute, which can
    help us understand the bias. Without such an attribute, the framework cannot do
    any calculations and therefore it cannot provide any useful feedback for the developers.
    The second point is the fact that the metrics are based on the unbalance between
    different groups – privileged and unprivileged – which we define ourselves. We
    cannot use this framework to discover bias that is hidden.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些之外，还有一些度量标准，但我们在这里提到的这些度量标准说明了最重要的观点——或者两个观点。首先，我们可以看到需要有一个属性，称为受保护属性，这可以帮助我们理解偏差。没有这样的属性，框架无法进行任何计算，因此它无法为开发者提供任何有用的反馈。第二个观点是，这些度量标准是基于不同群体之间——特权组和未特权组——的不平衡，这是我们自行定义的。我们不能使用这个框架来发现隐藏的偏差。
- en: Hidden biases are biases that are not directly represented by attributes. For
    example, there are differences in the occupations that men and women have, and
    therefore the occupation can be such an attribute that is correlated to the gender,
    but not equal to it. This means that we cannot treat this as a protected attribute,
    but we need to consider it – basically, there are no occupations that are purely
    male or purely female occupations, but different occupations have different proportions
    of men and women.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏的偏差是指没有直接由属性表示的偏差。例如，男性和女性在职业上有差异，因此职业可以是一个与性别相关但不等于性别的属性。这意味着我们不能将其视为受保护属性，但我们需要考虑它——基本上，没有纯粹男性或纯粹女性的职业，但不同的职业有不同的男性和女性的比例。
- en: Developing mechanisms to prevent ML bias from spreading throughout the system
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发机制以防止机器学习偏差在整个系统中传播
- en: Unfortunately, it is generally not possible to completely remove bias from ML
    as we often do not have access to the attributes needed to reduce the bias. However,
    we can reduce the bias and reduce the risk that the bias spreads to the entire
    system.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，通常无法完全从机器学习中去除偏差，因为我们往往无法访问减少偏差所需的属性。然而，我们可以减少偏差并降低偏差传播到整个系统的风险。
- en: Awareness and education are some of the most important measures that we can
    use to manage bias in software systems. We need to understand the potential sources
    of bias and their implications. We also need to identify biases related to protected
    attributes (for example, gender) and identify whether other attributes can be
    correlated with them (for example, occupation and address). Then, we need to educate
    our team about the ethical implications of biased models.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 意识和教育是我们可以用来管理软件系统偏差的最重要措施之一。我们需要了解偏差的潜在来源及其影响。我们还需要识别与受保护属性（例如，性别）相关的偏差，并确定其他属性是否可以与之相关联（例如，职业和地址）。然后，我们需要教育我们的团队了解偏差模型伦理影响。
- en: Then, we need to diversify our data collection. We must ensure that the data
    we collect is representative of the population we’re to model. To avoid over-representing
    or under-representing certain groups, we need to ensure that our data collection
    procedures are scrutinized before they are applied. We also need to monitor for
    biases in the collected data and reduce them. For example, if we identify a bias
    in credit scores, we can introduce the data that will prevent this bias from being
    strengthened by our model.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要多样化我们的数据收集。我们必须确保我们收集的数据能够代表我们要建模的群体。为了避免过度或不足代表某些群体，我们需要确保在应用之前对数据收集程序进行审查。我们还需要监控收集到的数据中的偏差并减少它们。例如，如果我们发现信用评分中存在偏差，我们可以引入数据，以防止我们的模型加强这种偏差。
- en: During data preprocessing, we need to ensure that we handle missing data correctly.
    Instead of just removing the data points or imputing them with mean values, we
    should use the right imputation, which takes care of the differences between the
    privileged and unprivileged groups.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据预处理期间，我们需要确保我们正确处理缺失数据。而不仅仅是删除数据点或用平均值填充它们，我们应该使用正确的填充方法，这种方法会考虑到特权和不特权群体之间的差异。
- en: We also need to actively work with the detection of bias. We should use statistical
    tests to check whether the data distribution is biased toward certain groups,
    at which point we need to visualize the distributions and identify potential bias.
    We’ve already discussed visualization techniques; at this point, we can add that
    we need to use different symbols for privileged and unprivileged groups to visualize
    two distributions on the same diagram, for example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要积极工作于偏差检测。我们应该使用统计测试来检查数据分布是否偏向于某些群体，此时我们需要可视化分布并识别潜在的偏差。我们已经讨论了可视化技术；在这个阶段，我们可以补充说，我们需要为特权和不特权群体使用不同的符号，以便在同一个图表上可视化两个分布，例如。
- en: In addition to working with the data, we also need to work with algorithmic
    fairness, which is when we design the models. We need to set fairness constraints
    and we need to introduce the attributes that can help us to identify privileged
    and unprivileged groups. For example, if we know that different occupations have
    a certain bias toward genders, we need to introduce superficial gender-bias attributes
    that can help us to create a model that will take that into account and prevent
    the bias from spreading to other parts of our system. We can also make post-hoc
    adjustments to the model after training. For example, when predicting a salary,
    we can adjust that salary based on pre-defined rules after the prediction. That
    can help to reduce the biases inherent in the model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了与数据合作外，我们还需要在模型设计时考虑算法公平性。我们需要设置公平性约束，并引入可以帮助我们识别特权和不特权群体的属性。例如，如果我们知道不同的职业对性别存在一定的偏见，我们需要引入表面上的性别偏见属性，以帮助我们创建一个考虑到这一点并防止偏差传播到系统其他部分的模型。我们还可以在训练后对模型进行事后调整。例如，在预测薪水时，我们可以在预测后根据预定义的规则调整那个薪水。这有助于减少模型中固有的偏差。
- en: We can also use fairness-enhancing interventions, such as the IBM Fairness tools
    and techniques, which include debiasing, reweighing, and disparate impact removal.
    This can help us to achieve interpretable models or allow us to use model interpretation
    tools to understand how decisions are being made. This can help in identifying
    and rectifying bias.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用公平性增强干预措施，例如IBM的公平性工具和技术，包括去偏差、重新加权以及消除不同影响。这可以帮助我们实现可解释的模型，或者允许我们使用模型解释工具来理解决策是如何做出的。这有助于识别和纠正偏差。
- en: Finally, we can regularly audit our models for bias and fairness. This includes
    both automated checks and human reviews. This helps us understand whether there
    are biases that cannot be captured automatically and that we need to react to.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以定期审计我们的模型以检查偏差和公平性。这包括自动检查和人工审查。这有助于我们了解是否存在无法自动捕捉的偏差，以及我们需要做出反应的偏差。
- en: With that, we have come to my next best practice.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们来到了我的下一个最佳实践。
- en: 'Best practice #74'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '最佳实践 #74'
- en: Complement automated bias management with regular audits.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通过定期的审计来补充自动化偏差管理。
- en: We need to accept the fact that bias is inherent in data, so we need to act
    accordingly. Instead of relying on algorithms to detect bias, we need to manually
    monitor for bias and understand it. Therefore, I recommend making regular checks
    for bias manually. Make classifications and predictions and check whether they
    strengthen or reduce bias by comparing them to the expected data without bias.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要接受数据中固有的偏差这一事实，因此我们需要相应地采取行动。而不是依赖算法来检测偏差，我们需要手动监控偏差并理解它。因此，我建议定期手动检查偏差。进行分类和预测，并通过将它们与无偏差的预期数据进行比较来检查它们是否增强了或减少了偏差。
- en: Summary
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: One of our responsibilities as software engineers is to ensure that we develop
    software systems that contribute to the greater good of society. We love working
    with technology development, but the technology needs to be developed responsibly.
    In this chapter, we looked at the concept of bias in ML and how to work with it.
    We looked at the IBM Fairness framework, which can assist us in identifying bias.
    We also learned that automated bias detection is too limited to be able to remove
    bias from the data completely.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作为软件工程师，我们的一项责任是确保我们开发的软件系统对社会的大局有益。我们热爱与技术开发打交道，但技术的发展需要负责任地进行。在本章中，我们探讨了机器学习中的偏差概念以及如何与之合作。我们研究了IBM公平性框架，该框架可以帮助我们识别偏差。我们还了解到，自动偏差检测过于有限，无法完全从数据中消除偏差。
- en: There are more frameworks to explore and more studies and tools are available
    every day. These frameworks are more specific and provide a means to capture more
    domain-specific bias – in medicine and advertising. Therefore, my final recommendation
    in this chapter is to explore the bias frameworks that are specific to the task
    at hand and for the domain at hand.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有更多的框架可以探索，每天都有更多的研究和工具可用。这些框架更加具体，提供了一种捕捉更多特定领域偏差的方法——在医学和广告领域。因此，在本章的最后，我的建议是探索针对当前任务和领域的特定偏差框架。
- en: References
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Donald, A., et al., Bias Detection for Customer Interaction Data: A Survey
    on Datasets, Methods, and Tools. IEEE* *Access, 2023.*'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Donald, A. 等，客户交互数据偏差检测：关于数据集、方法和工具的调查。IEEE* *Access，2023年。*'
- en: '*Bellamy, R.K., et al., AI Fairness 360: An extensible toolkit for detecting,
    understanding, and mitigating unwanted algorithmic bias. arXiv preprint* *arXiv:1810.01943,
    2018.*'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Bellamy, R.K. 等，AI公平性360：一个用于检测、理解和缓解不希望算法偏差的可扩展工具包。arXiv预印本* *arXiv:1810.01943，2018年。*'
- en: '*Zhang, Y., et al. Introduction to AI fairness. In Extended Abstracts of the
    2020 CHI Conference on Human Factors in Computing* *Systems. 2020.*'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Zhang, Y. 等。人工智能公平性简介。在 2020 年 CHI 计算机系统人类因素会议扩展摘要中。2020年。*'
- en: '*Alves, G., et al. Reducing unintended bias of ml models on tabular and textual
    data. In 2021 IEEE 8th International Conference on Data Science and Advanced Analytics
    (DSAA).* *2021\. IEEE.*'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Alves, G. 等。减少机器学习模型在表格和文本数据上的无意偏差。在 2021 年 IEEE 第 8 届数据科学和高级分析会议（DSAA）中。2021年。IEEE。*'
- en: '*Raza, S., D.J. Reji, and C. Ding, Dbias: detecting biases and ensuring fairness
    in news articles. International Journal of Data Science and Analytics, 2022:*
    *p. 1-21.*'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Raza, S.，D.J. Reji 和 C. Ding，Dbias：检测新闻文章中的偏差并确保公平性。国际数据科学和分析杂志，2022年：* *第
    1-21 页。*'
