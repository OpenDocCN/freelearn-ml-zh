<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05" class="calibre1"/>Chapter 5. Generic Object Detection for Industrial Applications</h1></div></div></div><p class="calibre8">This chapter will introduce you to the world of generic object detection, with a closer look at the advantages that industrial applications yield compared to the standard academic research cases. As many of you will know, OpenCV 3 contains the well-known <a id="id449" class="calibre1"/><strong class="calibre9">Viola and Jones algorithm</strong> (embedded as the CascadeClassifier class), which was specifically designed for robust face detection. However, the same interface can efficiently be used to detect any desired object class that suits your needs.</p><div><h3 class="title2"><a id="note65" class="calibre1"/>Note</h3><p class="calibre8">More information on the Viola and Jones algorithm can be found in the following publication:</p><p class="calibre8">Rapid object detection using a boosted cascade of simple features, Viola P. and Jones M., (2001). In Computer Vision and Pattern Recognition, 2001 (CVPR 2001). Proceedings of the 2001 IEEE Computer Society Conference on (Vol. 1, pp. I-511). IEEE.</p></div><p class="calibre8">This chapter assumes that you have a basic knowledge of the cascade classification interface of OpenCV 3. If not, here are some great starting points for understanding this interface and the basic usage of the supplied parameters and software:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre1" href="http://docs.opencv.org/master/modules/objdetect/doc/cascade_classification.html">http://docs.opencv.org/master/modules/objdetect/doc/cascade_classification.html</a></li><li class="listitem"><a class="calibre1" href="http://docs.opencv.org/master/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html">http://docs.opencv.org/master/doc/tutorials/objdetect/cascade_classifier/cascade_classifier.html</a></li><li class="listitem"><a class="calibre1" href="http://docs.opencv.org/master/doc/user_guide/ug_traincascade.html">http://docs.opencv.org/master/doc/user_guide/ug_traincascade.html</a></li></ul></div><div><h3 class="title2"><a id="note66" class="calibre1"/>Note</h3><p class="calibre8">Or you can simply read one of the PacktPub books that discuss this topic in more detail such as <a class="calibre1" title="Chapter 3. Recognizing Facial Expressions with Machine Learning" href="part0029_split_000.html#RL0A1-940925703e144daa867f510896bffb69">Chapter 3</a>, <em class="calibre10">Training a Smart Alarm to Recognize the Villain and His Cat</em>, of the <em class="calibre10">OpenCV for Secret Agents</em> book by Joseph Howse.</p></div><p class="calibre8">In this chapter, I will take you on a tour through specific elements that are important when using the Viola and Jones face detection framework for generic object detection. You will learn how to adapt your training data to the specific situation of your setup, how to make your object detection model rotation invariant, and you will find guidelines on how to improve the accuracy of your detector by smartly using environment parameters and situational knowledge. We will dive deeper into the actual object class model and explain what happens, combined with some smart tools for visualizing the actual process of object detection. Finally, we will look at GPU possibilities, which will lead to faster processing times. All of this will be combined with code samples and example use cases of general object detection.</p></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch05lvl1sec37" class="calibre1"/>Difference between recognition, detection, and categorization</h1></div></div></div><p class="calibre8">For completely <a id="id450" class="calibre1"/>understanding this chapter, it is important that you understand that the Viola and Jones detection framework based on cascade classification is actually an object categorization technique and that it differs a lot from the concept of object recognition. This leads to a common mistake in computer vision projects, where people do not analyze the problem well enough beforehand and thus wrongfully decide to use this technique for their problems. Take into consideration the setup described in the following figure, which consists of a computer with a camera attached to it. The computer has an internal description of four objects (plane, cup, car, and snake). Now, we consider the case where three new images are supplied to the camera of the system.</p><div><img src="img/00071.jpeg" alt="Difference between recognition, detection, and categorization" class="calibre11"/><div><p class="calibre28">A simple computer vision setup</p></div></div><p class="calibre12"> </p><p class="calibre8">In the case that image A is presented to the system, the system creates a description of the given input image and tries to match it with the descriptions of the images in the computer memory database. Since that specific cup is in a slightly rotated position, the descriptor of the cup's memory image will have a closer match than the other object images in memory and thus this system is able to successfully recognize the known cup. This process is called <a id="id451" class="calibre1"/><strong class="calibre9">object recognition</strong>, and is applied in cases where we know exactly which object we want to find in our input image.</p><div><table border="0" cellspacing="0" cellpadding="0" class="blockquote1" summary="Block quote"><tr class="calibre13"><td valign="top" class="calibre14"> </td><td valign="top" class="calibre14"><p class="calibre15"><em class="calibre16">"The goal of object recognition is to match (recognize) a specific object or scene. Examples include recognizing a specific building, such as the Pisa Tower, or a specific painting, such as the Mona Lisa. The object is recognized despite changes in scale, camera viewpoint, illumination conditions and partial occlusion."</em></p></td><td valign="top" class="calibre14"> </td></tr><tr class="calibre13"><td valign="top" class="calibre14"> </td><td colspan="2" valign="top" class="calibre17">--<em class="calibre16">Andrea Vedaldi and Andrew Zisserman</em></td></tr></table></div><p class="calibre8">However, this technique has some downsides. If an object is presented to the system that doesn't have a description in the image database, the system will still return the closest match and thus the result could be very misleading. To avoid this we tend to put a threshold on the matching quality. If the threshold is not reached, we simply do not provide a match.</p><p class="calibre8">When image B is <a id="id452" class="calibre1"/>presented to the same system, we experience a new problem. The difference between the the given input image and the cup image in memory is so large (different size, different shape, different print, and so on) that the descriptor of image B will not be matched to the description of the cup in memory, again a large downside of object recognition. The problems even rise further, when image C is presented to the system. There, the known car from computer memory is presented to the camera system, but it is presented in a completely different setup and background than the one in memory. This could lead to the background influencing the object descriptor so much that the object is not recognized anymore.</p><p class="calibre8"><strong class="calibre9">Object detection</strong> goes<a id="id453" class="calibre1"/> a bit further; it tries to find a given object in varying setups by learning a more object specific description instead of just a description of the image itself. In a situation where the detectable object class becomes more complex, and the variation of an object is large over several input images—we are no longer talking about single object detection, but rather about detecting a class of objects—this is where <strong class="calibre9">object categorization</strong><a id="id454" class="calibre1"/> comes into play.</p><p class="calibre8">With object categorization, we try to learn a generic model for the object class that can handle a lot of variation inside the object class, as shown in the following figure:</p><div><img src="img/00072.jpeg" alt="Difference between recognition, detection, and categorization" class="calibre11"/><div><p class="calibre28">An example of object classes with lots of variation: cars and chairs/sofas</p></div></div><p class="calibre12"> </p><p class="calibre8">Inside such a single object class, we try to cope with different forms of variation, as seen in the following figure:</p><div><img src="img/00073.jpeg" alt="Difference between recognition, detection, and categorization" class="calibre11"/><div><p class="calibre28">Variation within a single object class: illumination changes, object pose, clutter, occlusions, intra-class appearance, and viewpoint</p></div></div><p class="calibre12"> </p><p class="calibre8">It is very important to make <a id="id455" class="calibre1"/>sure that your application actually is of the third and latter case if you plan to use the Viola and Jones object detection framework. In that case, the object instances you want to detect are not known beforehand and they have a large intra-class variance. Each object instance can have differences in shape, color, size, orientation, and so on. The Viola and Jones algorithm will model all that variance into a single object model that will be able to detect any given instance of the class, even if the object instance has never been seen before. And this is the large power of object categorization techniques, where they generalize well over a set of given object samples to learn specifics for the complete object class.</p><p class="calibre8">These techniques allow us to train <a id="id456" class="calibre1"/>object detectors for more complex classes and thus make object categorization techniques ideal to use in industrial applications such as object inspection, object picking, and so on, where typically used threshold-based segmentation techniques seem to fail due this large variation in the setup.</p><p class="calibre8">If your application does not handle objects in these difficult situations, then consider using other techniques such as object recognition if it suits your needs!</p><p class="calibre8">Before we start with the real work, let me take the time to introduce to you the basic steps that are common in object detection applications. It is important to pay equal attention to all the steps and definitely not to try and skip some of them for gaining time. These would all influence the end result of the object detector interface:</p><div><ol class="orderedlist"><li class="listitem" value="1"><strong class="calibre9">Data collection</strong>: This<a id="id457" class="calibre1"/> step includes collecting the necessary data for building and testing your object detector. The data can be acquired from a range of sources going from video sequences to images captured by a webcam. This step will also make sure that the data is formatted correctly to be ready to be passed to the training stage.</li><li class="listitem" value="2"><strong class="calibre9">The actual model training</strong>: In this <a id="id458" class="calibre1"/>step, you will use the data gathered in the first step to train an object model that will be able to detect that model class. Here, we will investigate the different training parameters and focus on defining the correct settings for your application.</li><li class="listitem" value="3"><strong class="calibre9">Object detection</strong>: Once <a id="id459" class="calibre1"/>you have a trained object model, you can use it to try and detect object instances in the given test images.</li><li class="listitem" value="4"><strong class="calibre9">Validation</strong>: Finally, it is<a id="id460" class="calibre1"/> important to validate the detection result of the third step, by comparing each detection with a manually defined ground truth of the test data. Various options for efficiency and accuracy validation will be discussed.</li></ol><div></div><p class="calibre8">Let's continue by explaining the first step, the data collection in more detail, which is also the first subtopic of this chapter.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec38" class="calibre1"/>Smartly selecting and preparing application specific training data</h1></div></div></div><p class="calibre8">In this section, we <a id="id461" class="calibre1"/>will discuss how much training samples are needed according to the situational context and highlight some important aspects when preparing your annotations on the positive training samples.</p><p class="calibre8">Let's start by defining the principle of object categorization and its relation to training data, which can be seen in the following figure:</p><div><img src="img/00074.jpeg" alt="Smartly selecting and preparing application specific training data" class="calibre11"/><div><p class="calibre28">An example of positive and negative training data for an object model</p></div></div><p class="calibre12"> </p><p class="calibre8">The idea is that the algorithm takes a set of positive object instances, which contain the different presentations<a id="id462" class="calibre1"/> of the object you want to detect (this means object instances under different lighting conditions, different scales, different orientations, small shape changes, and so on) and a set of negative object instances, which contains everything that you do not want to detect with your model. Those are then smartly combined into an object model and used to detect new object instances in any given input image as seen in the figure above.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec49" class="calibre1"/>The amount of training data</h2></div></div></div><p class="calibre8">Many object detection<a id="id463" class="calibre1"/> algorithms depend heavily on large quantities of training data, or at least that is what is expected. This paradigm came to existence due to the academic research cases, mainly focusing on very challenging cases such as pedestrian and car detection. These are both object classes where a huge amount of intra-class variance exists, resulting in:</p><div><ul class="itemizedlist"><li class="listitem">A very large positive and negative training sample set, leading up to thousands and even millions of samples for each set.</li><li class="listitem">The removal of all information that pollutes the training set, rather than helping it, such as color information, and simply using feature information that is more robust to all this intra-class variation such as edge information and pixel intensity differences.</li></ul></div><p class="calibre8">As a result, models were trained that successfully detect pedestrians and cars in about every possible situation, with the downside that training them required several weeks of processing.. However, when you look at more industrial specific cases, such as the picking of fruit from bins or the grabbing of objects from a conveyor belt, you can see that the amount of variance in objects and background is rather limited compared to these very challenging academic research cases. And this is a fact that we can use to our own advantage.</p><p class="calibre8">We know that the accuracy of the resulting object model is highly dependent on the training data used. In cases where your detector needs to work in all possible situations, supplying huge amounts of data seems reasonable. The complex learning algorithms will then decide which information is useful and which is not. However, in more confined cases, we could build object models by considering what our object model actually needs to do.</p><p class="calibre8">For example, the Facebook DeepFace application, used for detecting faces in every possible situation using the neural networks approach uses 4.4 million labeled faces.</p><div><h3 class="title2"><a id="note67" class="calibre1"/>Note</h3><p class="calibre8">More information on the DeepFace algorithm can be found in:</p><p class="calibre8">Deepface: Closing the gap to human-level performance in face verification, Taigman Y., Yang M., Ranzato M. A., and Wolf L. (2014, June). In Computer Vision and Pattern Recognition (CVPR), 2014, IEEE Conference on (pp. 1701-1708).</p></div><p class="calibre8">We therefore <a id="id464" class="calibre1"/>suggest using only meaningful positive and negative training samples for your object model by following a set of simple rules:</p><div><ul class="itemizedlist"><li class="listitem">For the positive samples, only use <a id="id465" class="calibre1"/><strong class="calibre9">natural occurring samples</strong>. There are many tools out there that create artificially rotated, translated, and skewed images to turn a small training set into a large training set. However, research has proven that the resulting detector is less performant than simply collecting positive object samples that cover the actual situation of your application. Better use a small set of decent <a id="id466" class="calibre1"/><strong class="calibre9">high quality object samples</strong>, rather than using a large set of low quality non-representative samples for your case.</li><li class="listitem">For the negative samples, there are two possible approaches, but both start from the principle that you collect negative samples in the situation where your detector will be used, which is very different from the normal way of training object detects, where just a large set of random samples not containing the object are being used as negatives.<div><ul class="itemizedlist1"><li class="listitem">Either point a camera at your scene and start grabbing random frames to sample negative windows from.</li><li class="listitem">Or use your positive images to your advantage. Cut out the actual object regions and make the pixels black. Use those masked images as negative training data. Keep in mind that in this case the ratio between background information and actual object occurring in the window needs to be large enough. If your images are filled with object instances, cutting them will result in a complete loss of relevant background information and thus reduce the discriminative power of your negative training set.</li></ul></div></li><li class="listitem">Try to use a very small set of negative windows. If in your case only 4 or 5 background situations can occur, then there is no need to use 100 negative images. Just take those five specific cases and sample negative windows from them.</li></ul></div><p class="calibre8">Efficiently collecting data in this way ensures that you will end up with a very robust model for your specific application! However, keep in mind that this also has some consequences. The resulting model will not be robust towards different situations than the ones trained for. However, the benefit in training time and the reduced need of training samples completely outweighs this downside.</p><div><h3 class="title2"><a id="note68" class="calibre1"/>Note</h3><p class="calibre8">Software for negative sample generation<a id="id467" class="calibre1"/> based on OpenCV 3 can be found at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/generate_negatives/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/generate_negatives/</a>.</p></div><p class="calibre8">You can use the<a id="id468" class="calibre1"/> negative sample generation software to generate samples like you can see in the following figure, where object annotations of strawberries are removed and replaced by black pixels.</p><div><img src="img/00075.jpeg" alt="The amount of training data" class="calibre11"/><div><p class="calibre28">An example of the output of the negative image generation tool, where annotations are cut out and replaced by black pixels</p></div></div><p class="calibre12"> </p><p class="calibre8">As you can see, the ratio between the object pixels and the background pixels is still large enough in order to ensure that the model will not train his background purely based on those black pixel regions. Keep in mind that avoiding the approach of using these black pixelated images, by simply collecting negative images, is always better. However, many companies forget this important part of data collection and just end up without a negative data set meaningful for the application. Several tests I performed proved that using a negative dataset from random frames from your application have a more discriminative negative power than black pixels cutout based images.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec50" class="calibre1"/>Creating object annotation files for the positive samples</h2></div></div></div><p class="calibre8">When preparing your <a id="id469" class="calibre1"/>positive data samples, it is important to put some time in your annotations, which are the actual locations of your object instances inside the larger images. Without decent annotations, you will never be able to create decent object detectors. There are many tools out there for annotation, but I have made one for you based on OpenCV 3, which allows you to quickly loop over images and put annotations on top of them.</p><div><h3 class="title2"><a id="note69" class="calibre1"/>Note</h3><p class="calibre8">Software for <a id="id470" class="calibre1"/>object annotation based on OpenCV 3 can be found at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/object_annotation/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/object_annotation/</a>.</p></div><p class="calibre8">The OpenCV team <a id="id471" class="calibre1"/>was kind enough to also integrate this tool into the main repository under the apps section. This means that if you build and install the OpenCV apps during installation, that the tool is also accessible by using the following command:</p><div><pre class="programlisting">
<strong class="calibre9">/opencv_annotation -images &lt;folder location&gt; -annotations &lt;output file&gt;</strong>
</pre></div><p class="calibre8">Using the software is quite straightforward:</p><div><ol class="orderedlist"><li class="listitem" value="1">Start by running the CMAKE script inside the GitHub folder of the specific project. After running CMAKE, the software will be accessible through an executable. The same approach applies for every piece of software in this chapter. Running the CMAKE interface is quite straightforward:<div><pre class="programlisting">
<strong class="calibre9">cmakemake</strong>
<strong class="calibre9">./object_annotation -images &lt;folder location&gt; -annotations &lt;output file&gt;</strong>
</pre></div></li><li class="listitem" value="2">This will result in an executable that needs some input parameters, being the location of the positive image files and the output detection file.<div><h3 class="title2"><a id="note70" class="calibre1"/>Note</h3><p class="calibre8">Keep in mind to always assign the absolute path of all files!</p></div></li><li class="listitem" value="3">First, parse the content of your positive image folder to a file (by using the supplied <code class="email">folder_listing</code> software inside the object annotation folder), and then follow this by executing the annotation command:<div><pre class="programlisting">
<strong class="calibre9">./folder_listing –folder &lt;folder&gt; -images &lt;images.txt&gt;</strong>
</pre></div></li><li class="listitem" value="4">The folder listing tool should generate a file, which looks exactly like this:<div><img src="img/00076.jpeg" alt="Creating object annotation files for the positive samples" class="calibre11"/><div><p class="calibre28">A sample positive samples file generated by the folder listing tool</p></div></div><p class="calibre30"> </p></li><li class="listitem" value="5">Now, fire up the annotation tool with the following command:<div><pre class="programlisting">
<strong class="calibre9">./object_annotation –images &lt;images.txt&gt; -annotations &lt;annotations.txt&gt;</strong>
</pre></div></li><li class="listitem" value="6">This will <a id="id472" class="calibre1"/>fire up the software and give you the first image in a window, ready to apply annotations, as shown in the following figure:<div><img src="img/00077.jpeg" alt="Creating object annotation files for the positive samples" class="calibre11"/><div><p class="calibre28">A sample of the object annotation tool</p></div></div><p class="calibre30"> </p></li><li class="listitem" value="7">You can start by selecting the top-left corner of the object, then moving the mouse until you reach the bottom right corner of the object, which can be seen in the left part of the preceding figure. However, the software allows you to start your annotation from each possible corner. If you are unhappy with the selection, then reapply this step, until the annotation suits your needs.</li><li class="listitem" value="8">Once you agree on the selected bounding box, press the button that confirms a selection, which is key <em class="calibre10">C</em> by default. This will confirm the annotation, change its color from red to green, and add it to the annotations file. Be sure only to accept an annotation if you are 100% sure of the selection.</li><li class="listitem" value="9">Repeat the preceding two steps for the same image until you have annotated every single object instance in the image, as seen in the right part of the preceding example image. Then press the button that saves the result and loads in the following image, which is the <em class="calibre10">N</em> key by default.</li><li class="listitem" value="10">Finally, you<a id="id473" class="calibre1"/> will end up with a file called <code class="email">annotations.txt</code>, which combines the location of the image files together with the ground truth locations of all object instances that occur inside the training images.</li></ol><div></div><div><h3 class="title2"><a id="note71" class="calibre1"/>Note</h3><p class="calibre8">If you want to adapt the buttons that need to be pressed for all the separate actions, then open up the <code class="email">object_annotation.cpp</code> file and browse to line 100 and line 103. There you can adapt the ASCII values assigned to the button you want to use for the operation.</p><p class="calibre8">An overview of all ASCII codes assigned to <a id="id474" class="calibre1"/>your keyboard keys can be found at <a class="calibre1" href="http://www.asciitable.com/">http://www.asciitable.com/</a>.</p></div><p class="calibre8">The output from the software is a list of object detections in a <code class="email">*.txt</code> file for each folder of positive image samples, which has a specific structure as seen in the following figure:</p><div><img src="img/00078.jpeg" alt="Creating object annotation files for the positive samples" class="calibre11"/><div><p class="calibre28">An example of an object annotation tool</p></div></div><p class="calibre12"> </p><p class="calibre8">It starts with the absolute file location of each image in the folder. There was a choice of not using relative paths since the file will then be fully dependent on the location where it is stored. However, if you know what you are doing, then using relative file locations in relation to the executable should work just fine. Using the absolute path makes it more universal and more failsafe. The file location is followed by the number of detections for that specific image, which allows us to know beforehand how many ground truth objects we can expect. For each of the objects, the (x, y) coordinates are stored to the top-left corner combined with the width and the height of the bounding box. This is continued for each image, which is each time a new line appears in the detection output file.</p><div><h3 class="title2"><a id="tip07" class="calibre1"/>Tip</h3><p class="calibre8">It is important for further model training that each set of ground truth values captured from other annotation systems is first converted to this format in order to ensure the decent working of the cascade classifier software embedded in OpenCV 3.</p></div><p class="calibre8">A second point <a id="id475" class="calibre1"/>of attention when processing positive training images containing object instances, is that you need to pay attention to the way you perform the actual placement of the bounding box of an object instance. A good and accurately annotated ground truth set will always give you a more reliable object model and will yield better test and accuracy results. Therefore, I suggest using the following points of attention when performing object annotation for your application:</p><div><ul class="itemizedlist"><li class="listitem">Make sure that the bounding box contains the complete object, but at the same time avoid as much background information as possible. The ratio of object information compared to background information should always be larger than 80%. Otherwise, the background could yield enough features to train your model on and the end result will be your detector model focusing on the wrong image information.</li><li class="listitem">Viola and Jones suggests using squared annotations, based on a 24x24 pixel model, because it fits the shape of a face. However, this is not mandatory! If your object class is more rectangular like, then do annotate rectangular bounding boxes instead of squares. It is observed that people tend to push rectangular shaped objects in a square model size, and then wonder why it is not working correctly. Take, for example, the case of a pencil detector, where the model dimensions will be more like 10x70 pixels, which is in relation to the actual pencil dimensions.</li><li class="listitem">Try doing concise batches of images. It is better to restart the application 10 times, than to have a system crash when you are about to finish a set of 1,000 images with corresponding annotations. If somehow, the software or your computer fails it ensures that you only need to redo a small set.</li></ul></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec51" class="calibre1"/>Parsing your positive dataset into the OpenCV data vector</h2></div></div></div><p class="calibre8">Before the OpenCV 3 software <a id="id476" class="calibre1"/>allows you to train a cascade classifier object model, you will need to push your data into an OpenCV specific data vector format. This can be done by using the provided sample creation tool of OpenCV.</p><div><h3 class="title2"><a id="note72" class="calibre1"/>Note</h3><p class="calibre8">The sample creation tool<a id="id477" class="calibre1"/> can be found at <a class="calibre1" href="https://github.com/Itseez/opencv/tree/master/apps/createsamples/">https://github.com/Itseez/opencv/tree/master/apps/createsamples/</a> and should be built automatically if OpenCV was installed correctly, which makes it usable through the <code class="email">opencv_createsamples</code> command.</p></div><p class="calibre8">Creating the <a id="id478" class="calibre1"/>sample vector is quite easy and straightforward by applying the following instruction from the command line interface:</p><div><pre class="programlisting">
<strong class="calibre9">./opencv_createsamples –info annotations.txt –vec images.vec –bg negatives.txt –num amountSamples –w model_width –h model_height</strong>
</pre></div><pre><a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/average_dimensions/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/average_dimensions/</a>.</pre></div><pre>average</sub> h<sub class="calibre29">average</sub>] are [60 60].</li><li class="listitem">If we would use those [60 60] dimensions, then we would have a model that can only detect apples equal and larger to that size. However, moving away from the tree will result in not a single apple being detected anymore, since the apples will become smaller in size.</li><li class="listitem">Therefore, I suggest reducing the dimensions of the model to, for example, [30 30]. This will result in a model that still has enough pixel information to be robust enough and it will be able to detect up to half the apples of the training apples size.</li><li class="listitem">Generally speaking, the rule of thumb can be to take half the size of the average dimensions of the annotated data and ensure that your largest dimension is not bigger than 100 pixels. This last guideline is to ensure that training your model will not increase exponentially in time due to the large model size. If your largest dimension is still over 100 pixels, then just keep halving the dimensions until you go below this threshold.</li></ul></div><p class="calibre8">You have now <a id="id486" class="calibre1"/>prepared your positive training set. The last thing you should do is create a folder with the negative images, from which you will sample the negative windows randomly, and apply the folder listing functionality to it. This will result in a negative data referral file that will be used by the training interface.</pre></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec39" class="calibre1"/>Parameter selection when training an object model</h1></div></div></div><p class="calibre8">Once you have built <a id="id487" class="calibre1"/>a decent training samples dataset, which is ready to process, the time has arrived to fire up the cascade classifier training software of OpenCV 3, which uses the Viola and Jones cascade classifier framework to train your object detection model. The training itself is based on applying the boosting algorithm on either Haar wavelet features or Local Binary Pattern features. Several types of boosting are supported by the OpenCV interface, but for convenience, we use the frequently used AdaBoost interface.</p><div><h3 class="title2"><a id="note75" class="calibre1"/>Note</h3><p class="calibre8">If you are interested in knowing all the technical details of the feature calculation, then have a look at the following papers which describe them in detail:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">HAAR</strong>: Papageorgiou, Oren and Poggio, "A general framework for object detection", International Conference on Computer Vision, 1998.</li><li class="listitem"><strong class="calibre9">LBP</strong>: T. Ojala, M. Pietikäinen, and D. Harwood (1994), "Performance evaluation of texture measures with classification based on Kullback discrimination of distributions", Proceedings of the 12th IAPR International Conference on Pattern Recognition (ICPR 1994), vol. 1, pp. 582 - 585.</li></ul></div></div><p class="calibre8">This section will discuss several parts of the training process in more detail. It will first elaborate on how OpenCV runs its cascade classification process. Then, we will take a deeper look at all<a id="id488" class="calibre1"/> the training parameters provided and how they can influence the training process and accuracy of the resulting model. Finally, we will open up the model file and look in more detail at what we can find there.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec52" class="calibre1"/>Training parameters involved in training an object model</h2></div></div></div><pre><code class="email">-numNeg</code>: This is the<a id="id495" class="calibre1"/> amount of negative samples used at each stage. However, this is not the same as the amount of negative images that were supplied by the negative data. The training samples negative windows from these images in a sequential order at the model size dimensions. Choosing the right amount of<a id="id496" class="calibre1"/> negatives is highly dependent on your application.<div><ul class="itemizedlist1"><li class="listitem">If your application has close to no variation, then supplying a small number of windows could simply do the trick because they will contain most of the background variance.</li><li class="listitem">On the other hand, if the background variation is large, a huge number of samples would be needed to ensure that you train as much random background noise as possible into your model.</li><li class="listitem">A good start is taking a ratio between the number of positive and the number of negative samples equaling 0.5, so double the amount of negative versus positive windows.</li><li class="listitem">Keep in mind that each negative window that is classified correctly at an early stage will be discarded for training in the next stage since it cannot add any extra value to the training process. Therefore, you must be sure that enough unique windows can be grabbed from the negative images. For example, if a model uses 500 negatives at each stage and 100% of those negatives get correctly classified at each stage, then training a model of 20 stages will need 10,000 unique negative samples! Considering that the sequential grabbing of samples does not ensure uniqueness, due to the limited pixel wise movement, this amount can grow drastically.</li></ul></div></li><li class="listitem"><code class="email">-numStages</code>: This is<a id="id497" class="calibre1"/> the amount of weak classifier stages, which is highly dependent on the complexity of the application.<div><ul class="itemizedlist1"><li class="listitem">The more stages, the longer the training process will take since it becomes harder at each stage to find enough training windows and to find features that correctly separate the data. Moreover, the training time increases in an exponential manner when adding stages.</li><li class="listitem">Therefore, I suggest looking at the reported acceptance ratio that is outputted at each training stage. Once this reaches values of 10^(-5), you can conclude that your model will have reached the best descriptive and generalizing power it could get, according to the training data provided.</li><li class="listitem">Avoid training it to levels of 10^(-5) or lower to avoid overtraining your cascade on your training data. Of course, depending on the amount of training data supplied, the amount of stages to reach this level can differ a lot.</li></ul></div></li><li class="listitem"><code class="email">-bg</code>: This<a id="id498" class="calibre1"/> refers to the location of the text file that contains the locations of the negative training images, also called the negative samples description file.</li><li class="listitem"><code class="email">-vec</code>: This <a id="id499" class="calibre1"/>refers to the location of the training data vector that was generated in the previous step using the create_samples application, which is built-in to the OpenCV 3 software.</li><li class="listitem"><code class="email">-precalcValBufSize</code> and <code class="email">-precalcIdxBufSize</code>: These parameters assign the amount <a id="id500" class="calibre1"/>of memory used to calculate all features and the corresponding weak classifiers from the <a id="id501" class="calibre1"/>training data. If you have enough RAM memory available, increase these values to 2048 MB or 4096 MB, which will speed up the precalculation time for the features drastically.</li><li class="listitem"><code class="email">-featureType</code>: Here, you<a id="id502" class="calibre1"/> can choose which kind of features are used for creating the weak classifiers.<div><ul class="itemizedlist1"><li class="listitem">HAAR wavelets are reported to give higher accuracy models.</li><li class="listitem">However, consider training test classifiers with the LBP parameter. It decreases training time of an equal sized model drastically due to the integer calculations instead of the floating point calculations.</li></ul></div></li><li class="listitem"><code class="email">-minHitRate</code>: This is the<a id="id503" class="calibre1"/> threshold that defines how much of your positive samples can be misclassified as negatives at each stage. The default value is 0.995, which is already quite high. The training algorithm will select its stage threshold so that this value can be reached.<div><ul class="itemizedlist1"><li class="listitem">Making it 0.999, as many people do, is simply impossible and will make your training stop probably after the first stage. It means that only 1 out of 1,000 samples can be wrongly classified over a complete stage.</li><li class="listitem">If you have very challenging data, then lowering this, for example, to 0.990 could be a good start to ensure that the training actually ends up with a useful model.</li></ul></div></li><li class="listitem"><code class="email">-maxFalseAlarmRate</code>: This is <a id="id504" class="calibre1"/>the threshold that defines how much of your negative samples need to be <a id="id505" class="calibre1"/>classified as negatives before the boosting process should stop adding weak classifiers to the current stage. The default value is 0.5 and ensures that a stage of weak classifier will only do slightly better than random guessing on the negative samples. Increasing this value too much could lead to a single stage that already filters out most of your given windows, resulting in a very slow model at detection time due to the vast amount of features that need to be validated for each window. This will simply remove the large advantage of the concept of early window rejection.</li></ul></div><p class="calibre8">The parameters discussed earlier are the most important ones to dig into when trying to train a successful classifier. Once this works, you can increase the performance of your classifier even more, by <a id="id506" class="calibre1"/>looking at the way boosting forms its weak classifiers. This can be adapted by the <code class="email">-maxDepth</code> and <code class="email">-maxWeakCount</code> parameters. However, for most cases, using <a id="id507" class="calibre1"/><strong class="calibre9">stump weak classifiers</strong> (single layer decision trees) on single features is the best way to start, ensuring that single stage evaluation is not too complex and thus fast at detection time.</pre></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec53" class="calibre1"/>The cascade classification process in detail</h2></div></div></div><p class="calibre8">Once you select<a id="id508" class="calibre1"/> the <a id="id509" class="calibre1"/>correct training parameters, you can start the cascade classifier training process, which will build your cascade classifier object detection model. In order to fully understand the cascade classification process that builds up your object model, it is important to know how OpenCV does its training of the object model, based on the boosting process.</p><p class="calibre8">Before we do this, we will have a quick look at the outline of the boosting principle in general.</p><div><h3 class="title2"><a id="note77" class="calibre1"/>Note</h3><p class="calibre8">More information on the boosting principle can be found in Freund Y., Schapire R., and Abe N (1999). A short introduction to boosting. Journal-Japanese Society For Artificial Intelligence, 14(771-780), 1612</p></div><p class="calibre8">The idea behind boosting is that you have a very large pool of features that can be shaped into classifiers. Using all those features for a single classifier would mean that every single window in your test image will need to be processed for all these features, which will take a very long time and make your detection slow, especially if you consider how many negative windows are available in a test image. To avoid this, and to reject as many negative windows as fast as possible, boosting selects the features that are best at separating the positive and negative data and combines them into classifiers, until the classifier does a <a id="id510" class="calibre1"/>bit better than random guessing on the negative samples. This first step is called a weak classifier. Boosting repeats this process until the combination of all these weak classifiers reach the desired accuracy of the algorithm. The combination is called the strong classifier. The main advantage of this process is that tons of negative samples will already be discarded by the few early stages, with only evaluating a small set of features, thus decreasing detection time a lot.</p><p class="calibre8">We will now try to explain the complete process using the output generated by the cascade training software embedded in OpenCV 3. The following figure illustrates how a strong cascade classifier is built from a set of stages of weak classifiers.</p><div><img src="img/00080.jpeg" alt="The cascade classification process in detail" class="calibre11"/><div><p class="calibre28">A combination of weak classifier stages and early rejection of misclassified windows resulting in the famous cascade structure</p></div></div><p class="calibre12"> </p><p class="calibre8">The cascade classifier training process follows an iterative process to train subsequent stages of weak classifiers (1…N). Each stage consists of a set of weak classifiers, until the criteria for that specific stage have been reached. The following steps are an overview of what is happening at training each stage in OpenCV 3, according to the input parameters given and the training data provided. If you are interested in more specific details of each subsequent step, then do read the research paper of Viola and Jones (you can have a look at the citation on the first page of this chapter) on cascade classifiers. All steps described here are subsequently repeated for each stage until the desired accuracy for the strong classifier is reached. The following figure shows how such a stage output looks like:</p><div><img src="img/00081.jpeg" alt="The cascade classification process in detail" class="calibre11"/><div><p class="calibre28">An example output of a classifier stage training</p></div></div><p class="calibre12"> </p><div><div><div><div><h3 class="title2"><a id="ch05lvl3sec26" class="calibre1"/>Step 1 – grabbing positive and negative samples</h3></div></div></div><p class="calibre8">You will notice that the first<a id="id511" class="calibre1"/> thing the training does is grabbing <a id="id512" class="calibre1"/>training samples for the current stage—first the positive samples from the data vector you supplied, and then the random negative window samples from the negative images that you supplied. This will be outputted for both steps as:</p><div><pre class="programlisting">
<strong class="calibre9">POS:number_pos_samples_grabbed:total_number_pos_samples_needed NEG:number_neg_samples_grabbed:acceptanceRatioAchieved</strong>
</pre></div><p class="calibre8">If no positive <a id="id513" class="calibre1"/>samples can be found anymore, an error will be generated and training will be stopped. The total number of samples needed will increase once you start discarding positives that are no longer useful. The <a id="id514" class="calibre1"/>grabbing of the negatives for the current stage can take much longer than the positive sample grabbing since all windows that are correctly classified by the previous stages are discarded and new ones are searched. The deeper you go into the amount of stages, the harder this gets. As long as the number of samples grabbed keeps increasing (and yes, this can be very slow, so be patient), your application is still running. If no more negatives are found, the application will end training and you will need to lower the amount of negatives for each stage or add extra negative images.</p><p class="calibre8">The acceptance ratio that is achieved by the previous stage is reported after the grabbing of the negative windows. This value indicates whether the model trained until now is strong enough for your detection purposes or not!</p></div><div><div><div><div><h3 class="title2"><a id="ch05lvl3sec27" class="calibre1"/>Step 2 – precalculation of integral image and all possible features from the training data</h3></div></div></div><p class="calibre8">Once we have both positive <a id="id515" class="calibre1"/>and negative window-sized samples, the precalculation will calculate every single feature that is possible within the window size and apply it for each training sample. This can take some time according to the size of your model and according to the amount of training samples, especially when knowing that a model of 24x24 pixels can yield more than 16,000 features. As suggested earlier, assigning more memory can help out here or you could decide on selecting LBP features, of which the calculation is rather fast compared to HAAR features.</p><p class="calibre8">All features are <a id="id516" class="calibre1"/>calculated on the integral image representation of the original input window. This is done in order to speed up the calculation of the features. The paper by Viola and Jones explains in detail why this integral image representation is used.</p><p class="calibre8">The features calculated are dumped into a large feature pool from which the boosting process can select the features needed to train the weak classifiers that will be used within each stage.</p></div><div><div><div><div><h3 class="title2"><a id="ch05lvl3sec28" class="calibre1"/>Step 3 – firing up the boosting process</h3></div></div></div><p class="calibre8">Now, the cascade classifier <a id="id517" class="calibre1"/>training is ready for the actual boosting process. This happens in several small steps:</p><div><ul class="itemizedlist"><li class="listitem">Every possible weak classifier inside the feature pool is being calculated. Since we use stumps, which are basically weak classifiers based on single feature to create a decision tree, there are as many weak classifiers as features. If you prefer, you can decide to train actual decision trees with a predefined maximum depth, but this goes outside of the scope of this chapter.</li><li class="listitem">Each weak classifier is trained in order to minimize the misclassification rate on the training samples. For example, when using Real AdaBoost as a boosting technique, the Gini index is minimized.<div><h3 class="title2"><a id="note78" class="calibre1"/>Note</h3><p class="calibre8">More information on the Gini index, used for the misclassification rate on the training samples can be found in:</p><p class="calibre8">Gastwirth, J. L. (1972). The estimation of the Lorenz curve and Gini index. The Review of Economics and Statistics, 306-316.</p></div></li><li class="listitem">The weak classifier with the lowest misclassification rate is added as the next weak classifier to the current stage.</li><li class="listitem">Based on the weak classifiers that are already added to the stage, the algorithm calculates the overall stage threshold, which is set so that the desired hit rate is guaranteed.</li><li class="listitem">Now, the weights of the samples are adapted based on their classification in the last iteration, which will yield a new set of weak classifiers in the next iteration, and thus the whole process can start again.</li><li class="listitem">During the combination of the weak classifiers inside a single stage, which is visualized in the training output, the boosting process makes sure that:<div><ul class="itemizedlist1"><li class="listitem">The overall stage threshold does not drop below the minimum hit rate that was selected by the training parameters.</li><li class="listitem">The false alarm rate on the negative samples decreases compared to the previous stage.</li></ul></div></li><li class="listitem">This process continues until:<div><ul class="itemizedlist1"><li class="listitem">The false acceptance ratio on the negative samples is lower than the maximum false alarm rate set. The process then simply starts training a new stage of weak classifiers for the detection model.</li><li class="listitem">The required stage false alarm rate is reached, which is <code class="email">maxFalseAlarmRate^#stages</code>. This will yield an end to the model training since the model satisfies our requirements and better results cannot be achieved anymore. This will not happen often, since this value drops rather quickly, and after several stages, this would mean that you correctly classify more than 99% of your positive and negative samples all together.</li><li class="listitem">The hit <a id="id518" class="calibre1"/>rate drops below the stage specific minimal hit rate, which is the <code class="email">minHitRate^#stages</code>. At this stage, too many positives get wrongly classified and the maximum performance for your model is reached.</li></ul></div></li></ul></div></div><div><div><div><div><h3 class="title2"><a id="ch05lvl3sec29" class="calibre1"/>Step 4 – saving the temporary result to a stage file</h3></div></div></div><p class="calibre8">After training<a id="id519" class="calibre1"/> each stage, the stage specific details about the weak classifiers and the thresholds are stored in the data folder, in a separate XML file. If the desired number of stages has been reached, then these subfiles are combined into a single cascade XML file.</p><p class="calibre8">However, the fact that every stage is stored separately means that you can stop the training at any time and create an in-between object detection model, by simply restarting the training command, but changing the <code class="email">-numStages</code> parameter to the stage value on which you want to check the model's performance. This is ideal when you want to perform an evaluation on a validation set to ensure that your model does not start overfitting on the training data!</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch05lvl2sec54" class="calibre1"/>The resulting object model explained in detail</h2></div></div></div><p class="calibre8">It has been observed<a id="id520" class="calibre1"/> that many users of the cascade classifier algorithm embedded in OpenCV 3 do not know the meaning of the inner construction of the object model which is stored in the XML files, which sometimes leads to wrong perceptions of the algorithm. This subsection will explain each internal part of the trained object models. We will discuss a model based on stump-typed weak classifiers, but the idea is practically the same for any other type of weak classifiers inside a stage, such as decision trees. The biggest difference is that the weight calculation inside the model gets more complex as compared to when using stump features. As to the weak classifiers structure inside each stage, this will be discussed for both HAAR- and LBP-based features since these are the two most used features inside OpenCV for training cascade classifiers.</p><div><h3 class="title2"><a id="note79" class="calibre1"/>Note</h3><p class="calibre8">The two models that will be used for explaining everything can be found at</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre1" href="http://OpenCVsource/data/haarcascades/haarcascade_frontalface_default.xml">OpenCVsource/data/haarcascades/haarcascade_frontalface_default.xml</a></li><li class="listitem"><a class="calibre1" href="http://OpenCVsource/data/lbpcascades/lbpcascade_frontalface.xml">OpenCVsource/data/lbpcascades/lbpcascade_frontalface.xml</a></li></ul></div></div><p class="calibre8">The first part of each XML stored model describes the parameters that specify the characteristics of the model itself and some of the important training parameters. Subsequently, we can find the type of training that is used, which is limited to boosting for now, and the type of features used for building the weak classifiers. We also have the width and height of the object model that will be trained, the parameters of the boosting process, which include the type of boosting used, the selected minimum hit ratio, and the selected maximum false acceptance rate. It also contains information about how the weak classifier stages are built, in our case as a combination of one feature deep trees, called stumps, with a maximum of 100 weak classifiers on a single stage. For the HAAR wavelet based model, we can then see which features are used, being only the basic upright features or the combined rotated 45-degree set.</p><p class="calibre8">After the training-specific parameters, it starts to get interesting. Here, we find more information about the actual structure of the cascade classifier object model. The amount of stages is described, and then iteratively the model sums up the training results and thresholds for each separate stage which were generated by the boosting process. The basic structure of an object model can be seen here:</p><div><pre class="programlisting">&lt;stages&gt;
    &lt;_&gt;
        &lt;maxWeakCount&gt;&lt;/maxWeakCount&gt;
        &lt;stageThreshold&lt;/stageThreshold&gt;
        &lt;weakClassifiers&gt;
            &lt;!-- tree 0 --&gt;
            &lt;_&gt;
                &lt;internalNodes&gt;&lt;/internalNodes&gt;
                &lt;leafValues&gt;&lt;/leafValues&gt;&lt;/_&gt;
            &lt;!-- tree 1 --&gt;
            &lt;_&gt;
                &lt;internalNodes&gt;&lt;/internalNodes&gt;
                &lt;leafValues&gt;&lt;/leafValues&gt;&lt;/_&gt;
            &lt;!-- tree 2 --&gt;
            … … …
    &lt;!-- stage 1 --&gt;
    … … …
&lt;/stages&gt;
&lt;features&gt;
    … … …
&lt;/features&gt;</pre></div><p class="calibre8">We start with an empty iteration tag for each stage. At each stage the number of weak classifiers that were used are <a id="id521" class="calibre1"/>defined, which in our case shows how many single layer decision trees (stumps) were used inside the stage. The stage threshold defines the threshold on the final stage score for a window. This is generated by scoring the window with each weak classifier and then summing and weighing the results for the complete stage. For each single weak classifier, we collect the internal structure, based on the decision nodes and layers used. The values present are the boosting values used for creating the decision tree and the leaf values, which are used to score a window that is evaluated by the weak classifier.</p><p class="calibre8">The specifics for the internal node structure are different for HAAR wavelets and -based features. The storage of the leaf scores is equal. The values of the internal nodes, however, specify the relation to the bottom part of the code, which contains the actual features area, and which are also different for both the HAAR and the LBP approach. The difference between both techniques can be seen in the following sections, grabbing for both models the first tree of the first stage and a part of the feature set.</p><div><div><div><div><h3 class="title2"><a id="ch05lvl3sec30" class="calibre1"/>HAAR-like wavelet feature models</h3></div></div></div><p class="calibre8">The following are<a id="id522" class="calibre1"/> two code<a id="id523" class="calibre1"/> snippets from the HAAR wavelet feature-based model, containing the internal node structure and the features structure:</p><div><pre class="programlisting">&lt;internalNodes&gt;
0 -1 445 -1.4772760681807995e-02
&lt;/internalNodes&gt;
… … …
&lt;_&gt;
    &lt;rects&gt;
        &lt;_&gt;23 10 1 3 -1.&lt;/_&gt;
        &lt;_&gt;23 11 1 1 3.&lt;/_&gt;
    &lt;/rects&gt;
    &lt;tilted&gt;0&lt;/tilted&gt;
&lt;/_&gt;</pre></div><p class="calibre8">For the internal nodes, there are four values present at each node:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Node left and node right</strong>: These<a id="id524" class="calibre1"/> values indicate that we have a stump with two leafs.</li><li class="listitem"><strong class="calibre9">The node feature index</strong>: This <a id="id525" class="calibre1"/>points the index of the feature used at this node according to its position inside the features list of that model.</li><li class="listitem"><strong class="calibre9">The node threshold</strong>: This is the <a id="id526" class="calibre1"/>threshold that is set on the feature value for this weak classifier, which is learned from all the positive and negative samples in this stage of training. Since we are looking at models with stump based weak classifiers, this is also the stage threshold, which is set in the boosting process.</li></ul></div><p class="calibre8">The features<a id="id527" class="calibre1"/> inside the HAAR-based model are described by a set of rectangles, which can be up to three rectangles, so as to calculate every possible feature from a window. Then, there is a value indicating if the feature itself is tilted over 45 degrees or not. For each rectangle, which is a partial feature value, we have:</p><div><ul class="itemizedlist"><li class="listitem">The location of the rectangle, which is defined by upper-left corner x and y coordinates and the width and height of the rectangle.</li><li class="listitem">The weight for that specific partial feature. These weights are used to combine both partial feature rectangles into a predefined feature. These weights allow us to represent each feature with less rectangles than is actually necessary. An example of this can be seen in the following figure:</li></ul></div><div><img src="img/00082.jpeg" alt="HAAR-like wavelet feature models" class="calibre11"/><div><p class="calibre28">A three rectangle feature can be represented by a two rectangle weighted combination reducing the need of an extra area calculation</p></div></div><p class="calibre12"> </p><p class="calibre8">The feature sum is finally calculated by first summing all values of the pixels inside the rectangle and then multiplying it with the weight factor. Finally, those weighted sums are combined together to yield as a final feature value. Keep in mind that all the coordinates retrieved for a single feature are in relation to the window/model size and not the complete image which is processed.</p></div><div><div><div><div><h3 class="title2"><a id="ch05lvl3sec31" class="calibre1"/>Local binary pattern models</h3></div></div></div><p class="calibre8">The following <a id="id528" class="calibre1"/>are two<a id="id529" class="calibre1"/> code snippets from the LBP feature-based model, containing the internal node structure and the features structure:</p><div><pre class="programlisting">&lt;internalNodes&gt;
0 -1 46 -67130709 -21569 -1426120013 -1275125205 -21585
-16385 587145899 -24005
&lt;/internalNodes&gt;
… … …
&lt;_&gt;
    &lt;rect&gt;0 0 3 5&lt;/rect&gt;
&lt;/_&gt;</pre></div><pre><a id="note80" class="calibre1"/>Note</h3><p class="calibre8">The software for visualizing Haar wavelet or LBP models can be found at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/visualize_models/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/visualize_models/</a>.</pre></div><p class="calibre8">The software takes<a id="id535" class="calibre1"/> in several input arguments, such as the model location, the image where the visualization needs to happen, and the output folder where the results need to be stored. However, in order to use the software correctly, there are some points of attention:</p><div><ul class="itemizedlist"><li class="listitem">The model needs to be HAAR wavelet or LBP feature based. Deleted because this functionality is no longer supported in OpenCV 3.</li><li class="listitem">You need to supply an image that is an actual model detection for visualization purposes and resize it to the model scale or a positive training sample from the training data. This is to ensure that a feature of your model is placed at the correct location.</li><li class="listitem">Inside the code, you can adapt the visualization scales, one being for the video output of your model and one for the images that represent the stages.</li></ul></div><p class="calibre8">The following two figures illustrate the visualization result of the Haar wavelet and LBP feature based frontal face model respectively, both incorporated into the OpenCV 3 repository under the data folder. The reason for the low image resolution of the visualization is quite obvious. The training process happens on a model scale; therefore, I wanted to start from an image of that size to illustrate that specific details of an object get removed, while general specifics of the object class still occur to be able to differentiate classes.</p><div><img src="img/00084.jpeg" alt="Visualization tool for object models" class="calibre11"/><div><p class="calibre28">A set of frames from the video visualization of the frontal face model for both Haar wavelet and Local Binary Pattern features</p></div></div><p class="calibre12"> </p><p class="calibre8">The visualizations<a id="id536" class="calibre1"/> for example also clearly show that an LBP model needs less features and thus less weak classifiers to separate the training data successfully, which yields a faster model at detection time.</p><div><img src="img/00085.jpeg" alt="Visualization tool for object models" class="calibre11"/><div><p class="calibre28">A visualization of the first stage of the frontal face model for both Haar wavelet and Local Binary Pattern features</p></div></div><p class="calibre12"> </p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch05lvl2sec55" class="calibre1"/>Using cross-validation to achieve the best model possible</h2></div></div></div><p class="calibre8">Making sure that<a id="id537" class="calibre1"/> you get the absolute best model given your training, testing the data can be done by applying a cross validation approach, such as the leave-one-out approach. The idea behind this is that you combine both training and test set and vary the test set that you use from the larger set. With each random test set and training set, you build a separate model and you perform the evaluation using precision-recall, which is discussed further in this chapter. Finally, the model that provides the best result could be adopted as a final solution. Thus, it could mitigate the impact of an error due to a new instance that is not represented in the training set.</p><div><h3 class="title2"><a id="note81" class="calibre1"/>Note</h3><p class="calibre8">More information on the topic of cross validation can be found in Kohavi R. (1995, August), a study of cross-validation and bootstrap for accuracy estimation and model selection in Ijcai (Vol. 14, No. 2, pp. 1137-1145).</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec40" class="calibre1"/>Using scene specific knowledge and constraints to optimize the detection result</h1></div></div></div><p class="calibre8">Once your cascade <a id="id538" class="calibre1"/>classifier object model is trained, you can use it to detect instances of the same object class in new input images, which are supplied to the system. However, once you apply your object model, you will notice that there are still false positive detections and objects that are not found. This section will cover techniques to improve your detection results, by removing, for example, most of the false positive detections with scene-specific knowledge.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch05lvl2sec56" class="calibre1"/>Using the parameters of the detection command to influence your detection result</h2></div></div></div><p class="calibre8">If you apply <a id="id539" class="calibre1"/>an object model to a given input image, you must consider several things. Let's first take a look at the detection function and some of the parameters that can be used to filter out your detection output. OpenCV 3 supplies three possible interfaces. We will discuss the benefits of using each one of them.</p><p class="calibre8"><strong class="calibre9">Interface 1:</strong></p><div><pre class="programlisting">void CascadeClassifier::detectMultiScale(InputArray image, vector&lt;Rect&gt;&amp; objects, double scaleFactor=1.1, int minNeighbors=3, int flags=0, Size minSize=Size(), Size maxSize=Size())</pre></div><p class="calibre8">The first interface is the most basic one. It allows you to fast evaluate your trained model on a given test image. There are several elements on this basic interface that will allow you to manipulate the detection output. We will discuss these parameters in some more detail and highlight some points of attention when selecting the correct value.</p><p class="calibre8">scaleFactor is the scale step used to downscale the original image in order to create the image pyramid, which allows us to perform multiscale detections using only a single scale model. One downside is that this doesn't allow you to detect objects that are smaller than the object size. Using a value of 1.1 means that in each step the dimensions are reduced by 10% compared to the previous step.</p><div><ul class="itemizedlist"><li class="listitem">Increasing this value will make your detector run faster since it has less scale levels to evaluate, but it will yield the risk of losing detections that are in between scale steps.</li><li class="listitem">Decreasing the value will make your detector run slower, since more scale levels need to be evaluated, but it will increase the chance of detecting objects that were missed before. Also, it will yield more detections on an actual object, resulting in a higher certainty.</li><li class="listitem">Keep in mind that adding scale levels also gives rise to more false positive detections, since those are bound to each layer of the image pyramid.</li></ul></div><p class="calibre8">A second<a id="id540" class="calibre1"/> interesting parameter to adapt for your needs is the <code class="email">minNeighbors</code> parameter. It describes how many overlapping detections occur due to the sliding window approach. Each detection overlapping by more than 50% with another will be merged together as a sort of nonmaxima suppression.</p><div><ul class="itemizedlist"><li class="listitem">Putting this value on 0 means that you will get all detections generated by the windows that get through the complete cascade. However, due to the sliding window approach (with steps of 8 pixels) many detections will happen for a single window, due to the nature of cascade classifiers, which train in some variance on object parameters in order to better generalize over an object class.</li><li class="listitem">Adding a value means that you want to count how many windows there should be, at least those combined by the nonmaxima suppression in order to keep the detection. This is interesting since an actual object should yield far more detections than a false positive. So, increasing this value will reduce the number of false positive detections (which have a low amount of overlapping detections) and keep the true detections (which have a large amount of overlapping detections).</li><li class="listitem">A downside is that on a certain point, actual objects with a lower certainty of detections and thus less overlapping windows will disappear while some false positive detections might still stand.</li></ul></div><p class="calibre8">Use the <code class="email">minSize</code> and <code class="email">maxSize</code> parameters to effectively reduce the scale space pyramid. In an industrial setup with, for example, a fixed camera position, such as a conveyor belt setup, you can in most cases guarantee that objects will have certain dimensions. Adding scale values in this case and thus defining a scale range will decrease processing time for a single image a lot, by removing undesired scale levels. As an extra advantage, all false positive detections on those undesired scales will also disappear. If you leave these values blank, the algorithm will start building the image pyramid at input image dimensions, in a bottom-up manner, downscale in steps equaling the scale percentage, until one of the dimensions is smaller than the largest object dimension. This will be the top of the image pyramid, which is also the place where later, at the detection time, the detection algorithm will start running its object detector.</p><p class="calibre8"><strong class="calibre9">Interface 2:</strong></p><div><pre class="programlisting">void CascadeClassifier::detectMultiScale(InputArray image, vector&lt;Rect&gt;&amp; objects, vector&lt;int&gt;&amp; numDetections, double scaleFactor=1.1, int minNeighbors=3, int flags=0, Size minSize=Size(), Size maxSize=Size())</pre></div><p class="calibre8">The second interface brings a small addition, by adding the <code class="email">numDetections</code> parameter. This allows you to put the <code class="email">minNeighbors</code> value on 1, applying the merging of overlapping windows <a id="id541" class="calibre1"/>as nonmaxima suppression, but at the same time returning you a value of the overlapping windows, which were merged. This value can be seen as a certainty score of your detection. The higher the value, the better or the more certain the detection.</p><p class="calibre8"><strong class="calibre9">Interface 3:</strong></p><div><pre class="programlisting">void CascadeClassifier::detectMultiScale(InputArray image, std::vector&lt;Rect&gt;&amp; objects, std::vector&lt;int&gt;&amp; rejectLevels, std::vector&lt;double&gt;&amp; levelWeights, double scaleFactor=1.1, int minNeighbors=3, int flags=0, Size minSize=Size(), Size maxSize=Size(), bool outputRejectLevels=false )</pre></div><p class="calibre8">A downside of this interface is that 100 windows with a very small certainty of detection on an individual basis can simply out rule a single detection with a very high individual certainty of detection. This is where the third interface can bring us the solution. It allows us to look at the individual scores of each detection window (described by the threshold value of the last stage of the classifier). You can then grab all those values and threshold the certainty score of those individual windows. When applying nonmaxima suppression in this case, the threshold values of all overlapping windows are combined.</p><div><h3 class="title2"><a id="tip09" class="calibre1"/>Tip</h3><p class="calibre8">Keep in mind that if you want to try out the third interface in OpenCV 3.0, you have to put the parameter <code class="email">outputRejectLevels</code> on <code class="email">true</code>. If you do not do this, then the level weights matrix, which has the threshold scores, will not be filled.</p></div><div><h3 class="title2"><a id="note82" class="calibre1"/>Note</h3><p class="calibre8">Software illustrating the two most used interfaces for object detection can be found at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_simple">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_simple</a> and <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_score">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/detect_score</a>. OpenCV detection interfaces change frequently and that it is possible that new interfaces are already available which are not discussed here.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch05lvl2sec57" class="calibre1"/>Increasing object instance detection and reducing false positive detections</h2></div></div></div><p class="calibre8">Once you have<a id="id542" class="calibre1"/> chosen the most appropriate way of retrieving<a id="id543" class="calibre1"/> the object detections for your application, you can evaluate the proper output of your algorithm. Two of the most common problems found after training an object detector are:</p><div><ul class="itemizedlist"><li class="listitem">Object instances that are not being detected.</li><li class="listitem">Too much false positive detections.</li></ul></div><p class="calibre8">The reason for <a id="id544" class="calibre1"/>the first problem can be explained by looking at the generic object model that we trained for the object class based on positive training samples of that object class. This lets us conclude that the training either:</p><div><ul class="itemizedlist"><li class="listitem">Did not contain enough positive training samples, making it impossible to generalize well over new object samples. In this case, it is important to add those false negative detections as positive samples to the training set and retrain your model with the extra data. This principle is called "reinforced learning".</li><li class="listitem">We overtrained our model to the training set, again reducing the generalization of the model. To avoid this, reduce the model in stages and thus in complexity.</li></ul></div><p class="calibre8">The second <a id="id545" class="calibre1"/>problem is quite normal and happens more than often. It is impossible to supply enough negative samples and at the same time ensure that there will not be a single negative window that could still yield a positive detection at a first run. This is mainly due to the fact that it is very hard for us humans to understand how the computer sees an object based on features. On the other hand, it is impossible to grasp every possible scenario (lighting conditions, interactions during the production process, filth on the camera, and so on) at the very start when training an object detector. You should see the creation of a good and stable model as an iterative process.</p><div><h3 class="title2"><a id="note83" class="calibre1"/>Note</h3><p class="calibre8">An approach to avoid the influence of lighting conditions can be to triplicate the training set by generating artificial dark and artificial bright images for each sample. However, keep in mind the disadvantages of artificial data as discussed in the beginning of this chapter.</p></div><p class="calibre8">In order to reduce the amount of false positive detections, we generally need to add more negative samples. However, it is important not to add randomly generated negative windows, since the extra knowledge that they would bring to the model would, in most cases, simply be minimal. It is better to add meaningful negative windows that can increase the quality of the detector. This is known as <a id="id546" class="calibre1"/><strong class="calibre9">hard negative mining</strong> using a <a id="id547" class="calibre1"/><strong class="calibre9">bootstrapping</strong> process. The principle is rather simple:</p><div><ol class="orderedlist"><li class="listitem" value="1">Start by training a first object model based on your initial training set of positive and negative window samples.</li><li class="listitem" value="2">Now, collect a set of negative images, which are either specific to your application (if you want to train an object detector specific to your setup) or which are more general (if you want your object detector to work in versatile conditions).</li><li class="listitem" value="3">Run your<a id="id548" class="calibre1"/> detector on that set of negative images, with a low certainty threshold and save all found detections. Cut them out of the supplied negative images and rescale them towards the object model size dimensions.</li><li class="listitem" value="4">Now, retrain your object model, but add all the found windows to your negative training set in order to ensure that your model will now be trained with this extra knowledge.</li></ol><div></div><p class="calibre8">This will ensure that <a id="id549" class="calibre1"/>the accuracy of your model goes up by a fair and decent amount depending on the quality of your negative images.</p><div><h3 class="title2"><a id="tip10" class="calibre1"/>Tip</h3><p class="calibre8">When adding the found extra and useful negative samples, add them to the top of your <code class="email">background.txt</code> file! This forces the OpenCV training interface to first grab these more important negative samples before sampling all the standard negative training images provided! Be sure that they have exactly the required model size so that they can only be used once as a negative training sample.</p></div></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch05lvl1sec41" class="calibre1"/>Obtaining rotation invariance object detection</h1></div></div></div><pre><a id="tip85" class="calibre1"/>Tip</h3><p class="calibre8">Software for performing rotation invariant object detection based on the described third approach can be found at <a class="calibre1" href="https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/rotation_invariant_detection/">https://github.com/OpenCVBlueprints/OpenCVBlueprints/tree/master/chapter_5/source_code/rotation_invariant_detection/</a>.</pre></div><p class="calibre8">The biggest advantage of this approach is that you only need to train a single orientation model and can put your time in updating and tweaking that single model in order to make it as efficient as possible. Another advantage is that you can combine all the detections in different rotations, by providing some overlap, and then increase the certainty of a detection, by smartly removing false positives that do not get detections over multiple orientations. So basically, it is kind of a trade-off between benefits and downsides of the approach.</p><p class="calibre8">However, there are still some downsides to this approach:</p><div><ul class="itemizedlist"><li class="listitem">You will need to apply a multiscale detector to each layer of your 3D representation matrix. This will definitely increase the search time for object instances compared to single orientation object detection.</li><li class="listitem">You will create false positive detections on each orientation, which will also be warped back, thus increasing the total number of false positive detections.</li></ul></div><p class="calibre8">Let's take a deeper look at parts of the source code used for performing this rotation invariance and explain what is actually happening. The first interesting part can be found in the creation of the 3D matrix of rotated images:</p><div><pre class="programlisting">// Create the 3D model matrix of the input image
Mat image = imread(input_image);
int steps = max_angle / step_angle;
vector&lt;Mat&gt; rotated_images;
cvtColor(rotated, rotated, COLOR_BGR2GRAY);
equalizeHist( rotated, rotated );
for (int i = 0; i &lt; steps; i ++){
   // Rotate the image
   Mat rotated = image.clone();
   rotate(image, (i+1)*step_angle, rotated);
   // Preprocess the images
   
   // Add to the collection of rotated and processed images
   rotated_images.push_back(rotated);
}</pre></div><p class="calibre8">Basically, what <a id="id553" class="calibre1"/>we do is read the original image, create a vector of Mat objects that can contain each rotated input image, and apply the rotation function on top of it. As you will notice, we immediately apply all preprocessing which is needed for efficient object detection using the cascade classifier interface such as rendering the image to grayscale values and applying a histogram equalization in order to cope a bit with illumination changes.</p><p class="calibre8">The rotate function can be seen here:</p><div><pre class="programlisting">void rotate(Mat&amp; src, double angle, Mat&amp; dst)
{
    Point2f pt(src.cols/2., src.rows/2.);
    Mat r = getRotationMatrix2D(pt, angle, 1.0);
    warpAffine(src, dst, r, cv::Size(src.cols, src.rows));
}</pre></div><p class="calibre8">This code first calculates a rotation matrix based on the angle, which is expressed in degrees, that we want to be rotated and then applies an affine transformation based on this rotation matrix. Keep in mind that rotating an image like this can lead to an information loss of objects at the borders. This code example assumes your objects will occur at the center of the image and thus this does not influence the result. You can avoid this by enlarging the original image by adding black borders to that. The width and height of the image are equal so that the image information loss is minimal. This can be done by adding the following code right behind the reading of the original input image:</p><div><pre class="programlisting">Size dimensions = image.size();
if(dimensions.rows &gt; dimensions.cols){
   Mat temp = Mat::ones(dimensions.rows, dimensions.rows, image.type()) * 255;
   int extra_rows = dimensions.rows - dimensions.cols;
   image.copyTo(temp(0, extra_rows/2, image.rows, image.cols));
   image = temp.clone();
}
if(dimensions.cols &gt; dimensions.rows){
   Mat temp = Mat::ones(dimensions.cols, dimensions.cols, image.type()) * 255;
   int extra_cols = dimensions.cols - dimensions.rows;
   image.copyTo(temp(extra_cols/2, 0, image.rows, image.cols));
   image = temp.clone();
}</pre></div><p class="calibre8">This code will simply <a id="id554" class="calibre1"/>expand the original image to match a square region depending on the largest dimension.</p><p class="calibre8">Finally, on each level of the 3D image representation, a detection is performed and the found detections are warped back to the original image using a similar approach as warping the original image:</p><div><ol class="orderedlist"><li class="listitem" value="1">Take the four corner points of the found detection in the rotated image and add them into a matrix for rotation warping (code line 95-103).</li><li class="listitem" value="2">Apply the inverse transformation matrix based on the angle of the current rotated image (code line 106-108).</li><li class="listitem" value="3">Finally, draw a rotated rectangle on the information of the rotated four matrix points (code line 111-128).</li></ol><div></div><p class="calibre8">The following figure shows the exact result of applying a rotation invariant face detection to an image with faces in multiple orientations.</p><div><img src="img/00086.jpeg" alt="Obtaining rotation invariance object detection" class="calibre11"/><div><p class="calibre28">Rotation invariant face detection starting with the following angle steps [1 degree, 10 degrees, 25 degrees, 45 degrees]</p></div></div><p class="calibre12"> </p><p class="calibre8">We see that four times <a id="id555" class="calibre1"/>the suggested technique is applied to the same input image. We played around with the parameters in order to see the influence on detection time and the detections returned. In all cases, we applied a search from 0 to 360 degrees, but changed the angle step in between each stage of the 3D rotation matrix from 0 to 45 degrees.</p><div><table border="1" class="blockquote1"><colgroup class="calibre20"><col class="calibre21"/><col class="calibre21"/></colgroup><thead class="calibre22"><tr class="calibre23"><th valign="bottom" class="calibre24">
<p class="calibre15">Applied angle step</p>
</th><th valign="bottom" class="calibre24">
<p class="calibre15">Total time for executing all detections</p>
</th></tr></thead><tbody class="calibre25"><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">1 degree</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">220 seconds</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">10 degrees</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">22.5 seconds</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">25 degrees</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">8.6 seconds</p>
</td></tr><tr class="calibre23"><td valign="top" class="calibre14">
<p class="calibre15">45 degrees</p>
</td><td valign="top" class="calibre14">
<p class="calibre15">5.1 seconds</p>
</td></tr></tbody></table></div><p class="calibre8">As we can see, the detection time is reduced drastically when increasing the step of the angle. Knowing that an object model on itself could cover at least 20 degrees in total, we can easily reduce the step in order to significantly decrease the processing time.</p></div></body></html>