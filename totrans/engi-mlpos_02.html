<html><head></head><body>
		<div id="_idContainer021">
			<h1 id="_idParaDest-15"><a id="_idTextAnchor015"/>Chapter 1: Fundamentals of an MLOps Workflow</h1>
			<p><strong class="bold">Machine learning</strong> (<strong class="bold">ML</strong>) is maturing from research to applied business solutions. However, the grim reality is that only 2% of companies using ML have successfully deployed a model in production to enhance their business processes, reported by DeepLearning.AI (<a href="https://info.deeplearning.ai/the-batch-companies-slipping-on-ai-goals-self-training-for-better-vision-muppets-and-models-china-vs-us-only-the-best-examples-proliferating-patents">https://info.deeplearning.ai/the-batch-companies-slipping-on-ai-goals-self-training-for-better-vision-muppets-and-models-china-vs-us-only-the-best-examples-proliferating-patents</a>). What makes it so hard? And what do we need to do to improve the situation?  </p>
			<p>To get a solid understanding of this problem and its solution, in this chapter, we will delve into the evolution and intersection of software development and ML. We'll begin by reflecting on some of the trends in traditional software development, starting from the waterfall model to agile to DevOps practices, and how these are evolving to industrialize ML-centric applications. You will be introduced to a systematic approach to operationalizing AI using <strong class="bold">Machine Learning Operations </strong>(<strong class="bold">MLOps</strong>). By the end of this chapter, you will have a solid understanding of MLOps and you will be equipped to implement a generic MLOps workflow that can be used to build, deploy, and monitor a wide range of ML applications.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>The evolution of infrastructure and software development</li>
				<li>Traditional software development challenges</li>
				<li>Trends of ML adoption in software development</li>
				<li>Understanding MLOps </li>
				<li>Concepts and workflow of MLOps</li>
			</ul>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor016"/>The evolution of infrastructure and software development</h1>
			<p>With the genesis of the modern internet age (around 1995), we witnessed a rise in software <a id="_idIndexMarker000"/>applications, ranging from operating systems such as Windows 95 to the Linux operating system and websites <a id="_idIndexMarker001"/>such as Google and Amazon, which have been serving the world (online) for over two decades. This has resulted in a culture of continuously improving services by collecting, storing, and processing a massive amount of data from user interactions. Such developments have been shaping the evolution of IT infrastructure and software development.  </p>
			<p>Transformation in IT infrastructure has picked up pace since the start of this millennium. Since then, businesses have increasingly adopted cloud computing as it opens up new possibilities for businesses to outsource IT infrastructure maintenance while provisioning necessary IT resources such as storage and computation resources and services required to run and scale their operations.  </p>
			<p>Cloud computing offers on-demand provisioning and the availability of IT resources such as data storage and computing resources without the need for active management by the user of the IT resources. For example, businesses provisioning computation and storage resources do not have to manage these resources directly and are not responsible for keeping them running – the maintenance is outsourced to the cloud service provider. </p>
			<p>Businesses using cloud computing can reap benefits as there's no need to buy and maintain IT resources; it enables them to have less in-house expertise for IT resource maintenance and this allows businesses to optimize costs and resources. Cloud computing enables scaling on demand and users pay as per the usage of resources. As a result, we have seen companies adopting cloud computing as part of their businesses and IT infrastructures.</p>
			<p>Cloud computing became popular in the industry from 2006 onward when Sun Microsystems launched Sun Grid in March 2006. It is a hardware and data resource sharing service. This service was acquired by Oracle and was later named Sun Cloud. Parallelly, in the same year (2006), another cloud computing service was launched by Amazon called Elastic Compute Cloud. This <a id="_idIndexMarker002"/>enabled new possibilities for businesses to provision computation, storage, and scaling capabilities on demand. Since then, the transformation across industries has been organic toward adopting cloud computing.  </p>
			<p>In the last decade, many companies on a global and regional scale have catalyzed the cloud transformation, with companies such as Google, IBM, Microsoft, UpCloud, Alibaba, and others heavily investing in the research and development of cloud services. As a result, a shift from localized computing (companies having their own servers and data centers) to on-demand computing has taken place due to the availability of robust and scalable <a id="_idIndexMarker003"/>cloud services. Now businesses and organizations are able to provision <a id="_idIndexMarker004"/>resources on-demand on the cloud to satisfy their data processing needs. </p>
			<p>With these <a id="_idIndexMarker005"/>developments, we have witnessed <strong class="bold">Moore's law</strong> in operation, which states that the number of transistors on a microchip doubles every 2 years – though the cost of computers has halved, this has been true so far. Subsequently, some trends are developing as follows.</p>
			<h2 id="_idParaDest-17"><a id="_idTextAnchor017"/>The rise of machine learning and deep learning</h2>
			<p>Over the last <a id="_idIndexMarker006"/>decade, we have witnessed the adoption of <a id="_idIndexMarker007"/>ML in everyday life applications. Not only for esoteric applications such as <strong class="bold">Dota</strong> or <strong class="bold">AlphaGo</strong>, but ML has also made its way to pretty standard applications such as machine translation, image processing, and voice recognition. </p>
			<p>This adoption is powered by developments in infrastructure, especially in terms of the utilization of computation power. It has unlocked the potential of deep learning and ML.. We can observe deep learning breakthroughs correlated with computation developments in <em class="italic">Figure 1.1</em> (sourced from OpenAI: <a href="https://openai.com/blog/ai-and-compute">https://openai.com/blog/ai-and-compute</a>):</p>
			<div>
				<div id="_idContainer007" class="IMG---Figure">
					<img src="image/B16572_01_01.jpg" alt="Figure 1.1 – Demand for deep learning over time supported by computation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – Demand for deep learning over time supported by computation</p>
			<p>These breakthroughs in deep learning are enabled by the exponential growth in computing, which <a id="_idIndexMarker008"/>increases around 35 times every 18 months. Looking ahead in time, with <a id="_idIndexMarker009"/>such demands we may hit roadblocks in terms of scaling up central computing for CPUs, GPUs, or TPUs. This has forced us to look at <a id="_idIndexMarker010"/>alternatives such as <strong class="bold">distributed learning</strong> where computation for data processing is distributed across multiple computation nodes. We have seen some breakthroughs in distributed learning, such as federated learning and edge computing approaches. Distributed learning has shown promise to serve the growing demands of deep learning.  </p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor018"/>The end of Moore's law</h2>
			<p>Prior to 2012, AI results closely <a id="_idIndexMarker011"/>tracked Moore's law, with compute doubling every 2 years. Post-2012, compute has been doubling every 3.4 months (sourced from AI Index 2019 – <a href="https://hai.stanford.edu/research/ai-index-2019">https://hai.stanford.edu/research/ai-index-2019</a>). We can observe from <em class="italic">Figure 1.1</em> that demand for deep learning and <strong class="bold">high-performance computing </strong>(<strong class="bold">HPC</strong>) has been increasing exponentially with around 35x growth in computing every 18 months whereas Moore's law is seen to be outpaced (2x every 18 months). Moore's law is still applicable to the case of CPUs (single-core performance) but not to new hardware architectures <a id="_idIndexMarker012"/>such as GPUs and TPUs. This makes Moore's law obsolete and outpaced in contrast to current demands and trends.</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor019"/>AI-centric applications</h2>
			<p>Applications are <a id="_idIndexMarker013"/>becoming AI-centric – we see that across multiple industries. Virtually every application is starting <a id="_idIndexMarker014"/>to use AI, and these applications are running separately on distributed workloads such as <strong class="bold">HPC</strong>, <strong class="bold">microservices</strong>, and <strong class="bold">big data</strong>, as <a id="_idIndexMarker015"/>shown in <em class="italic">Figure 1.2</em>:</p>
			<div>
				<div id="_idContainer008" class="IMG---Figure">
					<img src="image/B16572_01_002.jpg" alt="Figure 1.2 – Applications running on distributed workloads&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – Applications running on distributed workloads</p>
			<p>By combining HPC and AI, we can enable the benefits of computation needed to train deep learning and ML models. With the overlapping of big data and AI, we can leverage extracting required data at scale for AI model training, and with the overlap of microservices and AI we can serve the AI models for inference to enhance business operations and <a id="_idIndexMarker016"/>impact. This way, distributed applications have become the new norm. Developing AI-centric applications at scale requires a synergy of distributed applications (HPC, microservices, and big data) and for this, a new way of developing software is required. </p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor020"/>Software development evolution</h2>
			<p>Software development has evolved hand in hand with infrastructural developments to facilitate the efficient <a id="_idIndexMarker017"/>development of applications using the infrastructure. Traditionally, software development started with the waterfall method of development where development is done linearly by gathering requirements to design and develop. The waterfall model has many limitations, which led to the evolution of software development over the years in the form of Agile methodologies and the DevOps method, as shown in <em class="italic">Figure 1.3</em>:</p>
			<div>
				<div id="_idContainer009" class="IMG---Figure">
					<img src="image/B16572_01_003.jpg" alt="Figure 1.3 – Software development evolution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 – Software development evolution</p>
			<h3>The waterfall method</h3>
			<p>The <strong class="bold">waterfall method</strong> was <a id="_idIndexMarker018"/>used to develop software from the onset of the internet age (~1995). It is a non-iterative way of developing software. It is delivered in a unidirectional way. Every stage is pre-organized and executed one after another, starting from requirements gathering to software design, development, and testing. The waterfall method is feasible and suitable when requirements are well-defined, specific, and do not change over time. Hence this is not suitable for dynamic projects where requirements change and evolve as per user demands. In such cases, where there is continuous modification, the waterfall method cannot be used to develop software. These are the major disadvantages of waterfall development methods:</p>
			<ul>
				<li>The entire set of requirements has to be given before starting the development; modifying them during or after the project development is not possible.</li>
				<li>There are fewer chances to create or implement reusable components.</li>
				<li>Testing can only be done after the development is finished. Testing is not intended to be iterable; it is not possible to go back and fix anything once it is done. Moreover, customer acceptance tests often introduced changes, resulting in a delay in delivery and high costs. This way of development and testing can have a negative impact on the project delivery timeline and costs.</li>
				<li>Most of the time, users of the system are provisioned with a system based on the developer's understanding, which is not user-centric and can come short of meeting their needs. </li>
			</ul>
			<h3>The Agile method</h3>
			<p>The <strong class="bold">Agile method</strong> facilitates an <a id="_idIndexMarker019"/>iterative and progressive approach to software development. Unlike the waterfall method, Agile approaches are precise and user-centric. The method is bidirectional and often involves end users or customers in the development and testing process so they have the opportunity to test, give feedback, and suggest improvements throughout the project development process and phases. Agile has several advantages over the waterfall method:</p>
			<ul>
				<li>Requirements are defined before starting the development, but they can be modified at any time.</li>
				<li>It is possible to create or implement reusable components.</li>
				<li>The solution or project can be modular by segregating the project into different modules that are delivered periodically.</li>
				<li>The users or customers can co-create by testing and evaluating developed solution modules periodically to ensure the business needs are satisfied. Such a user-centric process ensures quality outcomes focused on meeting customer and business needs. </li>
			</ul>
			<p>The following diagram <a id="_idIndexMarker020"/>shows the difference between <strong class="bold">Waterfall</strong> and <strong class="bold">Agile</strong> methodologies:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B16572_01_004.jpg" alt="Figure 1.4 – Difference between waterfall and agile methods&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – Difference between waterfall and agile methods</p>
			<h3>The DevOps method</h3>
			<p>The <strong class="bold">DevOps method</strong> extends agile development practices by further streamlining the movement of software change <a id="_idIndexMarker021"/>through the <em class="italic">build</em>, <em class="italic">test</em>, <em class="italic">deploy</em>, and <em class="italic">delivery</em> stages. DevOps empowers cross-functional teams with the autonomy to execute their software applications driven by continuous integration, continuous deployment, and continuous delivery. It encourages collaboration, integration, and automation among software developers and IT operators to improve the efficiency, speed, and quality of delivering customer-centric software. DevOps provides a streamlined software development framework for designing, testing, deploying, and monitoring systems in production. DevOps has made it possible to ship software to production in minutes and to keep it running reliably.</p>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor021"/>Traditional software development challenges</h1>
			<p>In the <a id="_idIndexMarker022"/>previous section, we observed the shift in traditional software development from the waterfall model to agile and DevOps practices. Agile and DevOps practices have enabled companies to ship software reliably. DevOps has made it possible to ship software to production in minutes and to keep it running reliably. This approach has been so successful that many companies are already adopting it, so why can't we keep doing the same thing for ML applications?</p>
			<p>The leading cause is that there's a fundamental difference between ML development and traditional software development: <em class="italic">Machine learning is not just code; it is code plus data</em>. A ML model is created by applying an algorithm (via code) to fit the data to result in a ML model, as shown in <em class="italic">Figure 1.5</em>:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B16572_01_005.jpg" alt="Figure 1.5 – Machine learning = data + code&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – Machine learning = data + code</p>
			<p>While code is <a id="_idIndexMarker023"/>meticulously crafted in the development environment, data comes from multiple sources for training, testing, and inference. It is robust and changing over time in terms of volume, velocity, veracity, and variety. To keep up with evolving data, code evolves over time. For perspective, their relationship can be observed as if code and data live in separate planes that share the time dimension but are independent in all other aspects. The challenge of an ML development process is to create a bridge between these two planes in a controlled way:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B16572_01_006.jpg" alt="Figure 1.6 – Data and code progression over time&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6 – Data and code progression over time</p>
			<p>Data and code, with the progression of time, end up going in two directions with one objective <a id="_idIndexMarker024"/>of building and maintaining a robust and scalable ML system. This disconnect causes several challenges that need to be solved by anyone trying to put a ML model in production. It comes with challenges such as slow, brittle, fragmented, and inconsistent deployment, and a lack of reproducibility and traceability. </p>
			<p>To overcome these challenges, MLOps offers a systematic approach by bridging data and code together over the progression of time. This is the solution to challenges posed by traditional software development methods with regard to ML applications. Using the MLOps method, data and code progress over time in one direction with one objective of building and maintaining a robust and scalable ML system:</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B16572_01_007.jpg" alt="Figure 1.7 – MLOps – data and code progressing together&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7 – MLOps – data and code progressing together</p>
			<p>MLOps facilitates ML model development, deployment, and monitoring in a streamlined and systematic approach. It empowers data science and IT teams to collaborate, validate, and <a id="_idIndexMarker025"/>govern their operations. All the operations executed by the teams are recorded or audited, end-to-end traceable, and repeatable. In the coming sections, we will learn how MLOps enables data science and IT teams to build and maintain robust and scalable ML systems.</p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor022"/>Trends of ML adoption in software development  </h1>
			<p>Before we delve into <a id="_idIndexMarker026"/>the workings of the MLOps method and workflow, it is beneficial to understand the big picture and trends as to where and <a id="_idIndexMarker027"/>how MLOps is disrupting the world. As many applications are becoming AI-centric, software development is evolving to facilitate ML. ML will increasingly become part of software development, mainly due to the following reasons:</p>
			<ul>
				<li><strong class="bold">Investments</strong>: In 2019, investments in global private AI clocked over $70 billion, with start-up investments related to AI over $37 billion, M&amp;A $34 billion, IPOs $5 billion, and minority stake valued at around $2 billion. The forecast for AI globally shows fast growth in market value as AI reached $9.5 billion in 2018 and is anticipated to reach a market value of $118 billion by 2025. It has been assessed that growth in economic activity resulting from AI until 2030 will be of high value and significance. Currently, the US attracts ~50% of global VC funding, China ~39%, and 11% goes to Europe.</li>
				<li><strong class="bold">Big data</strong>: Data is exponentially <a id="_idIndexMarker028"/>growing in volume, velocity, veracity, and variety. For instance, observations suggest data growing in volume at 61% per annum in Europe, and it is anticipated that four times more data will be created by 2025 than exists today. Data is a requisite raw material for developing AI.</li>
				<li><strong class="bold">Infrastructural developments and adoption</strong>: Moore's law has been closely tracked and observed to have been realized prior to 2012. Post-2012, compute has been doubling every 3.4 months. </li>
				<li><strong class="bold">Increasing research and development</strong>: AI research has been prospering in quality and quantity. A prominent growth of 300% is observed in the volume of peer-reviewed AI papers from 1998 to 2018, summing up to 9% of published conference papers and 3% of peer-reviewed journal publications. </li>
				<li><strong class="bold">Industry</strong>: Based on a surveyed report, 47% of large companies have reported having adopted AI in at <a id="_idIndexMarker029"/>least one function or <a id="_idIndexMarker030"/>business unit. In 2019, it went up to 58% and is expected to increase.<p class="callout-heading">Information</p><p class="callout">These points have been sourced from policy and investment recommendations for trustworthy AI – European commission (<a href="https://ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence">https://ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence</a>) and AI Index 2019 (<a href="https://hai.stanford.edu/research/ai-index-2019">https://hai.stanford.edu/research/ai-index-2019</a>).</p></li>
			</ul>
			<p>All these developments indicate a strong push toward the industrialization of AI, and this is possible by bridging industry and research. MLOps will play a key role in the industrialization of AI. If you invest in learning this method, it will give you a headstart in your company or team and you could be a catalyst for operationalizing ML and industrializing AI. </p>
			<p>So far, we have learned about some challenges and developments in IT, software development, and AI. Next, we will delve into understanding MLOps conceptually and learn in detail about a generic MLOps workflow that can be used commonly for any use case. These fundamentals will help you get a firm grasp of MLOps.</p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor023"/>Understanding MLOps  </h1>
			<p>Software <a id="_idIndexMarker031"/>development is interdisciplinary and is evolving to facilitate ML. MLOps is an emerging method to fuse ML with software development by integrating multiple domains as MLOps combines ML, DevOps, and data engineering, which aims to build, deploy, and maintain ML systems in production reliably and efficiently. Thus, MLOps can be expounded by this <a id="_idIndexMarker032"/>intersection.</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B16572_01_008.jpg" alt="Figure 1.8 – MLOps intersection&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8 – MLOps intersection</p>
			<p>To make this intersection (MLOps) operational, I have designed a modular framework by following the systematic <em class="italic">design science method proposed by Wieringa</em> (<a href="https://doi.org/10.1007/978-3-662-43839-8">https://doi.org/10.1007/978-3-662-43839-8</a>) to develop a workflow to bring these three together (Data Engineering, Machine Learning, and <em class="italic">DevOps</em>). Design science goes with the application of design to problems and context. Design science is the design and investigation of artifacts in a context. The artifact in this case is the MLOps workflow, which is designed iteratively by interacting with problem contexts (industry use cases for the application of AI):</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B16572_01_009.jpg" alt="Figure 1.9 – Design science workflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.9 – Design science workflow</p>
			<p>In a structured and iterative approach, the implementation of two cycles (the design cycle and the empirical cycle) was done for qualitative and quantitative analysis for MLOps workflow design through iterations. As a result of these cycles, an MLOps workflow is developed and <a id="_idIndexMarker033"/>validated by applying it to multiple problem contexts, that is, tens of ML use cases (for example, anomaly detection, real-time trading, predictive maintenance, recommender systems, virtual assistants, and so on) across multiple industries (for example, finance, manufacturing, healthcare, retail, the automotive industry, energy, and so on). I have applied and validated this MLOps workflow successfully in various projects across multiple industries to operationalize ML. In the next section, we will go through the concepts of the MLOps workflow designed as a result of the design science process. </p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor024"/>Concepts and workflow of MLOps </h1>
			<p>In this section, we <a id="_idIndexMarker034"/>will learn about a generic MLOps workflow; it is the result of many design cycle iterations as <a id="_idIndexMarker035"/>discussed in the previous section. It brings together data engineering, ML, and DevOps in a streamlined fashion. <em class="italic">Figure 1.10</em> is a generic MLOps workflow; it is modular and flexible and can be used to build proofs of concept or to operationalize ML solutions in any business or industry:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B16572_01_010.jpg" alt="Figure 1.10 – MLOps workflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10 – MLOps workflow</p>
			<p>This workflow is segmented into two modules:  </p>
			<ul>
				<li><strong class="bold">MLOps pipeline</strong> (build, deploy, and <a id="_idIndexMarker036"/>monitor) – the upper <a id="_idIndexMarker037"/>layer</li>
				<li><strong class="bold">Drivers</strong>: Data, code, artifacts, middleware, and <a id="_idIndexMarker038"/>infrastructure – mid and lower layers </li>
			</ul>
			<p>The upper layer is the MLOps pipeline (build, deploy, and monitor), which is enabled by drivers such as data, code, artifacts, middleware, and infrastructure. The MLOps pipeline is powered by an array of services, drivers, middleware, and infrastructure, and it crafts ML-driven solutions. By using this pipeline, a business or individual(s) can do quick prototyping, testing, and validating and deploy the model(s) to production at scale frugally and efficiently. </p>
			<p>To understand the workings and implementation of the MLOps workflow, we will look at the implementation of each layer and step using a figurative business use case.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor025"/>Discussing a use case </h2>
			<p>In this use case, we are to operationalize (prototyping and deploying for production) an image classification service to <a id="_idIndexMarker039"/>classify cats and dogs in a pet park in Barcelona, Spain. The service will identify cats and dogs in real time from the inference data coming from a CCTV camera installed in the pet park.</p>
			<p>The pet park provide you access to the data and infrastructure needed to operationalize the service:</p>
			<ul>
				<li><strong class="bold">Data</strong>: The pet park has given you access to their data lake containing 100,000 labeled images of cats and dogs, which we will use for training the model. </li>
				<li><strong class="bold">Infrastructure</strong>: Public cloud (IaaS).  </li>
			</ul>
			<p>This use case resembles a real-life use case for operationalizing ML and is used to explain the workings and implementation of the MLOps workflow. Remember to look for an explanation for the implementation of this use case at every segment and step of the MLOps workflow. Now, let's look at the workings of every layer and step in detail. </p>
			<h3>The MLOps pipeline </h3>
			<p>The MLOps pipeline is the upper layer, which performs operations such as build, deploy, and monitor, which work modularly in sync with each other. Let's look into each module's functionality. </p>
			<h4>Build</h4>
			<p>The build module has the <a id="_idIndexMarker040"/>core ML pipeline, and this is purely for training, packaging, and versioning the ML models. It is powered by the required compute (for example, the CPU or GPU on the cloud or distributed computing) resources to run the ML training and pipeline:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B16572_01_11.jpg" alt="Figure 1.11 – MLOps – build pipeline &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11 – MLOps – build pipeline </p>
			<p>The pipeline works from left to right. Let's look at the functionality of each step in detail:  </p>
			<ul>
				<li><strong class="bold">Data ingestion</strong>: This step is a trigger step for the ML pipeline. It deals with the volume, velocity, veracity, and variety of data by extracting data from various data sources (for example, databases, data warehouses, or data lakes) and ingesting the required data for the model training step. Robust data pipelines connected to multiple <a id="_idIndexMarker041"/>data sources enable it to perform <strong class="bold">extract, transform, and load </strong>(<strong class="bold">ETL</strong>) operations to provide the necessary data for ML training purposes. In this step, we can split and version data for model training in the required <a id="_idIndexMarker042"/>format (for example, the training or test set). As a result of this step, any experiment (that is, model training) can be audited and is back-traceable. <p>For a better understanding of the data ingestion step, here is the previously described use case implementation: </p><p><em class="italic">Use case implementation</em></p><p>As you have access to the pet park's data lake, you can now procure data to get started. Using data pipelines (part of the data ingestion step), you do the following: </p><p>   1. Extract, transform, and load 100,000 images of cats and dogs. </p><p>   2. Split and version this data into a train and test split (with an 80% and 20% split).</p><p>Versioning this data will enable end-to-end traceability for trained models. </p><p>Congrats – now you are ready to start training and testing the ML model using this data.</p></li>
				<li><strong class="bold">Model training</strong>: After procuring the required data for ML model training in the previous step, this step will enable model training; it has modular scripts or code that perform <a id="_idIndexMarker043"/>all the traditional steps in ML, such as data preprocessing, feature engineering, and feature scaling before training or retraining any model. Following this, the ML model is trained while performing hyperparameter tuning to fit the model to the dataset (training set). This step can be done manually, but efficient and automatic solutions such as <strong class="bold">Grid Search</strong> or <strong class="bold">Random Search</strong> exist. As a result, all important steps of ML model training are executed with a ML model as the output of this step.<p><em class="italic">Use case implementation</em></p><p>In this step, we implement all the important steps to train the image classification model. The goal is to train a ML model to classify cats and dogs. For <a id="_idIndexMarker044"/>this case, we train a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong> – <a href="https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb">https://towardsdatascience.com/wtf-is-image-classification-8e78a8235acb</a>) for the image classification service. The following steps are implemented: data preprocessing, feature engineering, and feature scaling before training, followed by training the model with hyperparameter tuning. As a result, we have a CNN model to classify cats and dogs with 97% accuracy.</p></li>
				<li><strong class="bold">Model testing</strong>: In this step, we <a id="_idIndexMarker045"/>evaluate the trained model performance on a separated set of data points named test data (which was split and versioned in the data ingestion step). The inference of the trained model is evaluated according to selected metrics as per the use case. The output of this step is a report on the trained model's performance.<p><em class="italic">Use case implementation</em></p><p>We test the trained model on test data (we split data earlier in the <em class="italic">Data ingestion</em> step) to evaluate the trained model's performance. In this case, we look for precision and the recall score to validate the model's performance in classifying cats and dogs to assess false positives and true positives to get a realistic understanding of the <a id="_idIndexMarker046"/>model's pe<a id="_idTextAnchor026"/>rformance. If and when we are satisfied with the results, we can proceed to the next step, or else reiterate the previous steps to get a decent performing model for the pet park image classification service.</p></li>
				<li><strong class="bold">Model packaging</strong>: After the trained model has been tested in the previous step, the model can be serialized <a id="_idIndexMarker047"/>into a file or containerized (using Docker) to be exported to the production environment.<p><em class="italic">Use case implementation</em></p><p>The model we trained and tested in the previous steps is serialized to an ONNX file and is ready to be deployed in the production environment.</p></li>
				<li><strong class="bold">Model registering</strong>: In this step, the model that was serialized or containerized in the previous step is <a id="_idIndexMarker048"/>registered and stored in the model registry. A registered model is a logical collection or package of one or more files that assemble, represent, and execute your ML model. For example, multiple files can be registered as one model. For instance, a classification model can be comprised of a vectorizer, model weights, and serialized model files. All these files can be registered as one single model. After registering, the model (all files or a single file) can be downloaded and deployed as needed.<p><em class="italic">Use case implementation</em></p><p>The serialized model in the previous step is registered on the model registry and is available for quick deployment into the pet park production environment.</p><p>By implementing the preceding steps, we successfully execute the ML pipeline designed for our use case. As a result, we have trained models on the model registry ready to be deployed in the production setup. Next, we will look into the workings of the deployment pipeline. </p></li>
			</ul>
			<h4>Deploy</h4>
			<p>The deploy module enables operationalizing the ML models we developed in the previous <a id="_idIndexMarker049"/>module (build). In this module, we test our model performance and behavior in a production or production-like (test) environment to ensure the robustness and scalability of the ML model for production use. <em class="italic">Figure 1.12</em> depicts the deploy pipeline, which has two components – production testing and production release – and the deployment pipeline is enabled by streamlined CI/CD pipelines connecting the development to production environments:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B16572_01_12.jpg" alt="Figure 1.12 – MLOps – deploy pipeline &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12 – MLOps – deploy pipeline </p>
			<p>It works from left to right. Let's look at the functionality of each step in detail:  </p>
			<ul>
				<li><strong class="bold">Application testing</strong>: Before deploying an <a id="_idIndexMarker050"/>ML model to production, it is vital to test its robustness and performance via testing. Hence we have the "application testing" phase where we rigorously test all the trained models for robustness and performance in a <a id="_idIndexMarker051"/>production-like environment called a test environment. In the application testing phase, we deploy the models in the test environment (pre-production), which replicates the production environment. <p>The ML model for testing is deployed as an API or streaming service in the test environment to deployment targets such as Kubernetes clusters, container instances, or scalable virtual machines or edge devices as per the need and use case. After the model is deployed for testing, we perform predictions using test data (which is not used for training the model; test data is sample data from a production <a id="_idIndexMarker052"/>environment) for the deployed model, during which model inference in batch or periodically is done to test the model deployed in the test environment for robustness and performance. </p><p>The performance results are automatically or manually reviewed by a quality assurance expert. When the ML model's performance meets the standards, then it is approved to be deployed in the production environment where the model will be used to infer in batches or real time to make business decisions.</p><p><em class="italic">Use case implementation</em></p><p>We deploy the model as an API service on an on-premises computer in the pet park, which is set up for testing purposes. This computer is connected to a CCTV camera in the park to fetch real-time inference data to predict cats or dogs in the video frames. The model deployment is enabled by the CI/CD pipeline. In this step, we test the robustness of the model in a production-like environment, that is, whether the model is performing inference consistently, and an accuracy, fairness, and error analysis. At the end of this step, a quality assurance expert certifies the model if it meets the standards.</p></li>
				<li><strong class="bold">Production release</strong>: Previously tested <a id="_idIndexMarker053"/>and approved models are deployed in the production environment for model inference to generate business or operational value. This production release is deployed to the production environment enabled by CI/CD pipelines. <p><em class="italic">Use case implementation</em></p><p>We deploy a previously tested and approved model (by a quality assurance expert) as an API service on a computer connected to CCTV in the pet park (production setup). This <a id="_idIndexMarker054"/>deployed model performs ML inference on the incoming video data from the CCTV camera in the pet park to classify cats or dogs in real time.</p></li>
			</ul>
			<h4>Monitor</h4>
			<p>The monitor module works in <a id="_idIndexMarker055"/>sync with the deploy module. Using explainable monitoring (discussed later in detail, in <a href="B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206"><em class="italic">Chapter 11</em></a>, <em class="italic">Key Principles for Monitoring Your ML System</em>), we can monitor, analyze, and govern the deployed ML application (ML model and application). Firstly, we can monitor the performance of the ML model (using pre-defined metrics) and the deployed application (using telemetry data). Secondly, model performance can be analyzed using a pre-defined explainability framework, and lastly, the ML application can be governed using alerts and actions based on the model's quality assurance and control. This ensures a robust monitoring mechanism for the production system:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B16572_01_13.jpg" alt="Figure 1.13 – MLOps – monitor pipeline &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.13 – MLOps – monitor pipeline </p>
			<p>Let's see each of the abilities of the monitor module in detail:</p>
			<ul>
				<li><strong class="bold">Monitor</strong>: The monitoring module <a id="_idIndexMarker056"/>captures critical information to monitor data integrity, model drift, and application performance. Application performance can be monitored using telemetry data. It depicts the device performance of a production system over a period of time. With telemetry data such as accelerometer, gyroscope, humidity, magnetometer, pressure, and temperature we can keep a check on the production system's performance, health, and longevity. <p><em class="italic">Use case implementation</em></p><p>In real time, we will monitor three things – data integrity, model drift, and application performance – for the deployed API service on the park's computer. Metrics such as accuracy, F1 score, precision, and recall are tracked to data integrity and model drift. We monitor application performance by tracking the telemetry data of the production system (the on-premises computer in the park) running the deployed <a id="_idIndexMarker057"/>ML model to ensure the proper functioning of the production system. Telemetry data is monitored to foresee any anomalies or potential failures and fix them in advance. Telemetry data is logged and can be used to assess production system performance over time to check its health and longevity.</p></li>
				<li><strong class="bold">Analyze</strong>: It is critical to analyze the model performance of ML models deployed in production systems to ensure optimal performance and governance in correlation to <a id="_idIndexMarker058"/>business decisions or impact. We use model explainability techniques to measure the model performance in real time. Using this, we evaluate important aspects such as model fairness, trust, bias, transparency, and error analysis with the intention of improving the model in correlation to business. <p>Over time, the statistical properties of the target variable we are trying to predict can change in unforeseen ways. This <a id="_idIndexMarker059"/>change is called "model drift," for example, in a case where we have deployed a recommender system model to suggest suitable items for users. User behavior may change due to unforeseeable trends that could not be observed in historical data that was used for training the model. It is essential to consider such unforeseen factors to ensure deployed models provide the best and most relevant business value. When model drift is observed, then any of these actions should be performed:</p><p>a) The product owner or the quality assurance expert needs to be alerted. </p><p>b) The model needs to be switched or updated.</p><p>c) Re-training the pipeline should be triggered to re-train and update the model as per the latest data or needs.</p><p><em class="italic">Use case implementation</em></p><p>We monitor the deployed model's performance in the production system (a computer connected to the CCTV in the pet park). We will analyze the accuracy, precision, and recall scores <a id="_idIndexMarker060"/>for the model periodically (once a day) to ensure the model's performance does not deteriorate below the threshold. When the model performance deteriorates below the threshold, we initiate system governing mechanisms (for example, a trigger to retrain the model). </p></li>
				<li><strong class="bold">Govern</strong>: Monitoring and analyzing is done to govern the deployed application to drive optimal performance <a id="_idIndexMarker061"/>for the business (or the purpose of the ML system). After monitoring and analyzing the production data, we can generate certain alerts and actions to govern the system. For example, the product owner or the quality assurance expert gets alerted when model performance deteriorates (for example, low accuracy, high bias, and so on) below a pre-defined threshold. The product owner initiates a trigger to retrain and deploy an alternative model. Lastly, an important aspect of governance is "compliance" with the local and global laws and rules. For compliance, model explainability and transparency are vital. For this, model auditing and reporting are done to provide end-to-end traceability and explainability for production models.  <p><em class="italic">Use case implementation</em></p><p>We monitor and analyze the deployed model's performance in the production system (a computer connected to the CCTV in the pet park). Based on the analysis of accuracy, precision, and recall scores for the deployed model, periodically (once a day), alerts are generated when the model's performance deteriorates below the pre-defined threshold. The product owner of the park generates actions, and these actions are based on the alerts. For example, an alert is generated notifying the product <a id="_idIndexMarker062"/>owner that the production model is 30% biased to detect dogs more than cats. The product owner then triggers the model re-training pipeline to update the model using the latest data to reduce the bias, resulting in a fair and robust model in production. This way, the ML system at the pet park in Barcelona is well-governed to serve the business needs.</p></li>
			</ul>
			<p>This brings us to the end of the MLOps pipeline. All models trained, deployed, and monitored using the MLOps method are end-to-end traceable and their lineage is logged in order to trace the origins of the model, which includes the source code the model used to train, the data used to train and test the model, and parameters used to converge the model. Full lineage is useful to audit operations or to replicate the model, or when a blocker is hit, the logged ML model lineage is useful to backtrack the origins of the model or to observe and debug the cause of the blocker. As ML models generate data in production during inference, this data can be tied to the model training and deployment lineage to ensure the end-to-end lineage, and this is important for certain compliance requirements. Next, we will look into key drivers enabling the MLOps pipeline.  </p>
			<h3>Drivers</h3>
			<p>These are the key drivers for <a id="_idIndexMarker063"/>the MLOps pipeline: data, code, artifacts, middleware, and infrastructure. Let's look into each of the drivers to get an overview of how they enable the MLOps pipeline:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B16572_01_14.jpg" alt="Figure 1.14 – MLOps drivers &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.14 – MLOps drivers </p>
			<p>Each of the key drivers for the MLOps pipeline are defined as follows:</p>
			<ul>
				<li><strong class="bold">Data</strong>: Data can be in multiple forms, such as text, audio, video, and images. In traditional software <a id="_idIndexMarker064"/>applications, data quite often tends to be structured, whereas, for ML applications, it can be structured or unstructured. To manage data in ML applications, data is handled in these steps: data acquisition, data annotation, data cataloging, data preparation, data quality checking, data sampling, and data augmentation. Each step involves its own life cycle. This makes a whole new set of processes and tools necessary for ML applications. For efficient functioning of the ML pipeline, data is segmented and versioned into training data, testing data, and monitoring data (collected in production, for example, model inputs, outputs, and telemetry data). These data operations are part of the MLOps pipeline.</li>
				<li><strong class="bold">Code</strong>: There are three essential <a id="_idIndexMarker065"/>modules of code that drive the MLOps pipeline: training code, testing code, and application code. These scripts or code are executed using the CI/CD and data pipelines to ensure the robust working of the MLOps pipeline. The source code management system (for example, using Git or Mercurial) will enable orchestration and play a vital role in managing and integrating seamlessly with CI, CD, and data pipelines. All of the code is staged and versioned in the source code management setup (for example, Git).</li>
				<li><strong class="bold">Artifacts</strong>: The MLOps pipeline generates artifacts such as data, serialized models, code snippets, system logs, ML model training, and testing metrics information. All these <a id="_idIndexMarker066"/>artifacts are useful for the successful working of the MLOps pipeline, ensuring its traceability and sustainability. These artifacts are managed using middleware services such as the model registry, workspaces, logging services, source code management services, databases, and so on. </li>
				<li><strong class="bold">Middleware</strong>: Middleware is computer software that offers services to software applications that are more <a id="_idIndexMarker067"/>than those available from the operating systems. Middleware services ensure multiple applications to automate and orchestrate processes for the MLOps pipeline. We can use a diverse set of middleware software and services depending on the use cases, for example, Git for source code management, VNets to enable the required network configurations, Docker for containerizing our models, and Kubernetes for container orchestration to automate application deployment, scaling, and management.</li>
				<li><strong class="bold">Infrastructure</strong>: To ensure the successful working of the MLOps pipeline, we need essential compute and storage resources to train Test and deploy the ML models. Compute resources enable us to train, deploy and monitor our ML models. Two types of storages resources can facilitate ML operations, central storage and feature stores. A central storage stores the logs, artifacts, training, testing and monitoring data. A feature store is optional and complementary to central storage. It extracts, transforms and stores needed features for ML model training and inference using a feature pipeline. When it comes to the infrastructure, there are various options such as on-premises resources or <strong class="bold">infrastructure as a service (IaaS)</strong>, which is cloud services. These days, there are many cloud players providing IaaS, such as Amazon, Microsoft, Google, Alibaba, and so on. Having the right infrastructure for your use case will enable robust, efficient, and frugal operations for your team and company.<p>A fully automated workflow is achievable with smart optimization and synergy of all these drivers with the MLOps pipeline. Some direct advantages of implementing an automated MLOps workflow is a spike in IT teams' efficiency (by reducing the time <a id="_idIndexMarker068"/>spent by data scientists and developers on mundane and repeatable tasks) and the optimization of resources, resulting in cost reductions, and both of these are great for any business. </p></li>
			</ul>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor027"/>Summary</h1>
			<p>In this chapter, we have learned about the evolution of software development and infrastructure to facilitate ML. We delved into the concepts of MLOps, followed by getting acquainted with a generic MLOps workflow that can be implemented in a wide range of ML solutions across multiple industries. </p>
			<p>In the next chapter, you will learn how to characterize any ML problem into an MLOps-driven solution and start developing it using an MLOps workflow. </p>
		</div>
	</body></html>