["```py\nimport pandas as pd\n df_population_density = pd.read_csv('/home/nbuser/library/World_Population_Growth.csv')\n df_population_density.head(10)\n```", "```py\nfiltered_cells_df = df_population_density[['Location','Time','Births','Deaths','GrowthRate']].dropna(how=\"any\")\n filtered_cells_df\n```", "```py\ndf_population_pivot = filtered_cells_df.pivot_table('GrowthRate','Location','Time') df_population_pivot.head(100)\n```", "```py\nimport numpy as np import matplotlib.pyplot as plot plot.figure(figsize=(15,10),dpi = 80) plot.plot(df_population_pivot.ix[:,0:1], label=\"Net Growth rate, both sexes males and females\") plot.plot(df_population_pivot.ix[:,1:2], label=\"Net migration rate (per 1000 population distribution)\") plot.plot(df_population_pivot.ix[:,2:3],label=\"Population growth rate (%)\") plot.xlabel('Location') ...\n```", "```py\nimport cntk as Cognitive\n from cntk.train import Trainer\n from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs\n from cntk.learners import adadelta, learning_rate_schedule, UnitType\n from cntk.ops import RELU, element_times, constant\n from cntk.layers import Dense, Sequential, For, default_options\n from cntk.losses import cross_entropy_with_softmax\n from cntk.metrics import classification_error\n from cntk.train.training_session import *\n from cntk.logging import ProgressPrinter\n\n input_dimension = 784\n number_output_classes = 10\n number_hidden_layers = 2\n hidden_layers_dimension=200\n feature_val = Cognitive.input_variable(input_dimension)\n label_val = Cognitive.input_variable(number_output_classes)\n```", "```py\nfrom cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefspath =  \"C:\\\\Users\\\\CNTK\\\\Examples\\\\Image\\\\DataSets\\\\MNIST\\Train-28x28_cntk_text.txt\"reader_train_val = MinibatchSource(CTFDeserializer(path, StreamDefs( ...\n```", "```py\n # Instantiate the feed forward classification model\n scaled_input = element_times(constant(0.00390625), feature_val)\n```", "```py\nfrom cntk.layers import Dense, Sequential, For, default_options\n\nwith default_options(activation=relu, init=Cognitive.glorot_uniform()):\n z = Sequential([For(range(number_hidden_layers),\n\nlambda i: Dense(hidden_layers_dimension)),\n Dense(number_output_classes, activation=None)])(scaled_input)\n```", "```py\n\n def simple_mnist():\n input_dimension = 784\n number_output_classes = 10\n number_hidden_layers = 2\n hidden_layers_dimension = 200\n```", "```py\n# Instantiate the feedforward classification model\n scaled_input = element_times(constant(0.00390625), feature_val)\n\nwith default_options(activation=relu, init=Cognitive.glorot_uniform()):\n z = Sequential([For(range(number_hidden_layers),\n lambda i: Dense(hidden_layers_dimension)),\n Dense(number_output_classes, activation=None)])(scaled_input)\n```", "```py\nce = cross_entropy_with_softmax(z, label_val)\n```", "```py\npe = classification_error(z, label_val)\n```", "```py\ninput_map = {\n feature_val: reader_train_val.streams.features,\n label_val: reader_train_val.streams.labels\n }\n```", "```py\n# Instantiate progress writers.\n progress_writers_val = [ProgressPrinter(\n tag='Training',\n num_epochs=number_sweeps_to_train_with)]\n```", "```py\n#Instantiate the trainer object to drive the model training lr = learning_rate_schedule(1, UnitType.sample) trainer = Trainer(z, (ce, pe), [adadelta(z.parameters, lr)], progress_writers_val)  \n```", "```py\n# Training config\n minibatch_size_val = 64\n number_samples_per_sweep = 60000\n number_sweeps_to_train_with = 10\n\ntraining_session(\n trainer=trainer,\n mb_source=reader_train_val,\n mb_size=minibatch_size_val,\n model_inputs_to_streams=input_map,\n max_samples=number_samples_per_sweep * number_sweeps_to_train_with,\n progress_frequency=number_samples_per_sweep\n ).train()\n```", "```py\n# Load test data path = \"C:\\\\Users\\\\CNTK\\\\Examples\\\\Image\\\\DataSets\\\\MNIST\\\\Test-28x28_cntk_text.txt\"#Reading of data using MinibatchSourcereader_test_val = MinibatchSource(CTFDeserializer(path, StreamDefs( features=StreamDef(field='features', shape=input_dimension), labels=StreamDef(field='labels', shape=number_output_classes))))#mapping of input dataset using feature & labelinput_map = { feature_val: reader_test_val.streams.features, label_val: reader_test_val.streams.labels }\n```", "```py\ndocker pull microsoft/cntk \n```", "```py\ndocker run -d -p 8888:8888 --name cntk-jupyter-notebooks -t microsoft/cntk\n```", "```py\ndocker exec -it cntk-jupyter-notebooks bash -c \"source /cntk/activate-cntk andand jupyter-notebook --no-browser --port=8888 --ip=0.0.0.0 --notebook-dir=/cntk/Tutorials --allow-root\"\n```", "```py\n{    \"epsilon\": 1e-07, \"image_data_format\": ...\n```", "```py\ndocker run -d --name my-mmlsparkbook -p 8888:8888 -e ACCEPT_EULA=yes microsoft/MMLSpark\n```", "```py\n#Declare few TensorFlow operations\n d = tensorf.add(b, c, name='d')\n e = tensorf.add(c, 2, nname='e')\n a = tensorf.multiply(d, e, nname='a')\n```", "```py\n#setup the initialization of variables\n init_operation = ttensorf.global_variable_initializer()\n```", "```py\n#Start the Tensorflow Session\n with tensorf.Session() as tfsess:\n #initialize the variables\n tfsess.run(init_operation)\n #computation of the output from the graph\n a_graphOut = tfsess.run(a)\n print(\"Variable a is the value of {}\".format(a_graphOut))\n```", "```py\ndef run_simple_tensorGraph_multiple(): #first lets create a constant for TensorFlow constant = tensorf.constant(2.0, name=\"constant\")\n```", "```py\n#Next create those TensorFlow variables b = tensorf.placeholder(tensorf.float32,[None, 1], NameError='b') c = tensorf.Variable(1.0, name='c')\n```", "```py\ndef nn_example():\n mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n```", "```py\n# Python optimisation variables\n learning_rate = 0.5\n epochs = 10\n batch_size = 100\n```", "```py\n # declare the training data placeholders\n # input x - for 28 x 28 pixels = 784\n x = tensorf.placeholder(tensorf.float32, [None, 784])\n # now declare the output data placeholder - 10 digits\n y = tensorf.placeholder(tensorf.float32, [None, 10])\n```", "```py\n# now declare the weights connecting the input to the hidden layer\n W1 = tensorf.Variable(tensorf.random_normal([784, 300], stddev=0.03), name='W1')\n b1 = tensorf.Variable(tensorf.random_normal([300]), name='b1')\n # and the weights connecting the hidden layer to the output layer\n W2 = tensorf.Variable(tensorf.random_normal([300, 10], stddev=0.03), name='W2')\n b2 = tensorf.Variable(tensorf.random_normal([10]), name='b2') \n```", "```py\n# calculate the output of the hidden layer\n hidden_out = tensorf.add(tensorf.matmul(x, W1), b1)\n hidden_out = tensorf.nn.relu(hidden_out)\n```", "```py\n # now calculate the hidden layer output - in this case, let's use a softmax activated\n # output layer\n y_ = tensorf.nn.softmax(tensorf.add(tensorf.matmul(hidden_out, W2), b2))\n```", "```py\n# add an optimiser\n optimiser = tensorf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n```", "```py\n# finally setup the initialisation operator\n init_operation = tensorf.global_variables_initializer()\n# define an accuracy assessment operation\n correct_prediction = tensorf.equal(tensorf.argmax(y, 1), tensorf.argmax(y_, 1))\n accuracy = tensorf.reduce_mean(tensorf.cast(correct_prediction, tensorf.float32))\n```", "```py\n# add a summary to store the accuracy\n tensorf.summary.scalar('accuracy', accuracy)\n```", "```py\nmerged = tensorf.summary.merge_all()\n writer = tensorf.summary.FileWriter('c:\\\\users\\\\anbasa\\\\source\\\\repos')\n```", "```py\n# start the session with tensorf.Session() as tfsess: # initialise the variables tfsess.run(init_operation) total_batch = int(len(mnist.train.labels) / batch_size) for epoch in range(epochs): avg_cost = 0 for i ...\n```", "```py\ndocker run -d -p 8888:8888 -v /notebook:/notebook xblaster/TensorFlow-jupyter\n```", "```py\npython tf_cnn_benchmarks.py --local_parameter_device=gpu --num_gpus=2 --batch_size=128 --model=googlenet --variable_update=parameter_server ...\n```"]