- en: 3D Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last few chapters, we have discussed the extraction of an object and
    semantic information from images. We saw how good feature extraction leads to
    object detection, segmentation, and tracking. This information explicitly requires
    the geometry of the scene; in several applications, knowing the exact geometry
    of a scene plays a vital role.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will see a discussion leading to the three-dimensional aspects
    of an image. Here, we will begin by using a simple camera model to understand
    how pixel values and real-world points are linked correspondingly. Later, we will
    study methods for computing depth from images and also methods of computing the
    motion of a camera from a sequence of images.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: RGDB dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications to extract features from images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image formation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aligning of images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual odometry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual SLAM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset and libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will be using OpenCV for most of the applications. In the
    last section, for **Visual Simultaneous Localization and Mapping** (**vSLAM**)
    techniques, we will see the use of an open source repository; directions for its
    use are mentioned in the section. The dataset is the `RGBD` dataset, consisting
    of a sequence of images captured using RGB and a depth camera. To download this
    dataset, visit the following link and download the fr1/xyz tar file: [https://vision.in.tum.de/data/datasets/rgbd-dataset/download](https://vision.in.tum.de/data/datasets/rgbd-dataset/download).
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, use the following (Linux only) command in a Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While deep learning can extract good features for high-level applications,
    there are areas that require pixel level matching to compute geometric information
    from an image. Some of the applications that use this information are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Drones**: In commercial robots like drones, the image sequence is used to
    compute the motion of the camera mounted on them. This helps them to make robust
    motion estimations and, in addition to other **Inertial Measurement Units** (**IMU**)
    such as gyroscopes, accelerometers, and so on, the overall motion is estimated
    more accurately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image editing applications**: Smartphones and professional applications for
    image editing include tools like panorama creation, image stitching, and so on.
    These apps compute orientation from common pixels across image samples and align
    the images in one target orientation. The resulting image looks as if it has been
    stitched by joining the end of one image to another.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Satellites or space vehicles**: In the remote operation of satellites or
    robots, it is hard and erroneous to obtain orientation after a significant motion.
    If the robot moves along a path on the moon, it might get lost due to an error
    in its local GPS systems or inertial measurement units. In order to build more
    robust systems, an image-based orientation of the camera is also computed and
    fused other sensor data to obtain more robust motion estimates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Augmented Reality**: With the boom in smartphones and apps and the availability
    of better hardware, several computer vision algorithms that use geometry information
    can now run in real time. **Augmented Reality** (**AR**) apps and games use geometrical
    properties from a sequence of images. These further combine this information with
    other sensor data to create a seamless AR experience and we can see a virtual
    object as if it is a real object placed on the table. Tracking planar objects
    and computing the relative positions of objects and the camera is crucial in these
    applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image formation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The basic camera model is a pinhole camera, though the real-world cameras that
    we use are far more complex models. A pinhole camera is made up of a very small
    slit on a plane that allows the formation of an image as depicted in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/326a14a8-9a7f-47ef-8bf8-1b6897d80f00.png)'
  prefs: []
  type: TYPE_IMG
- en: This camera converts a point in the physical world, often termed the *real world*,
    to a pixel on an image plane. The conversion follows the transformation of the
    three-dimensional coordinate to two-dimensional coordinates. Here in the image
    plane, the coordinates are denoted as where ![](img/ef95d229-0447-4c95-922a-d5c95b7d3b06.png),
    *P[i]* is any point on an image. In the physical world, the same point is denoted
    by ![](img/e2f62f14-34eb-4b77-a17b-627cdf391a4a.png), where *P[w]* is any point
    in the physical world with a global reference frame.
  prefs: []
  type: TYPE_NORMAL
- en: '*P[i](x'', y'')* and *P[w](x, y, z)* can be related as, for an ideal pin hole
    camera:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b1b711b-8dfc-4770-994b-b61e4e047b74.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *f* is focal length of the camera.
  prefs: []
  type: TYPE_NORMAL
- en: For further discussion on geometry of image formation, it is necessary to introduce
    the homogeneous coordinate system. The physical world coordinate system is referred
    to as **Euclidean coordinate system**. In the image plane, a point *P'* with *(x,
    y)* coordinates is represented in homogeneous coordinate system as *(x, y, 1)*.
    Similarly a point *P[w]* with *(x, y, z)* in world coordinates can be represented
    in homogeneous coordinate system as (x, y, z, 1) .
  prefs: []
  type: TYPE_NORMAL
- en: To convert back from homogeneous to Euclidean, we need to divide by the last
    coordinate value. To convert a point on an image plane in homogeneous system as
    (x,y, w) to Euclidean system as (x/w, y/w) . Similarly, for a 3D point in a homogeneous
    system given by (x, y, z, w), the Euclidean coordinates are given by (x/w, y/w,
    z/ w). In this book, the use of homogeneous coordinate systems will be explicitly
    mentioned; otherwise we will see equations in the Euclidean coordinate system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image formation comes from transforming physical world coordinates to image
    plane coordinates but losing information about an extra dimension. This means
    that when we construct an image we are losing depth information for each pixel.
    As a result, converting back from image pixel coordinates to real-world coordinates
    is not possible. As shown in the following figure, for a point **P[I]** in the
    figure there can be an infinite number of points lying along the line. Points **P1**,
    **P2**, and **P3** have the same image pixel location, and therefore estimations
    of depth (distance from camera) are lost during image formation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/464d2f09-3ffa-4842-87c5-5a3781d1b612.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let us observe a point world from two images. If we know the optical center
    of a camera that constructs an image and the point location of two images, we
    can get much more information. The following figure explains **Epipolar Geometry**
    using two images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15a9a57e-f709-4eba-9dc7-3155994cf8c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous figure, the camera centers **O[1]** and **O[2]** are connected
    to point **P[w]** in the world, and the plane forming the line **P[w]**, **O[1]**,
    **O[2]** is the epipolar plane. The points where the camera''s center line O[1]O[2] 
    intersects with the image are epipoles for the image. These may or may not lie
    on the image. In cases where both the image planes are parallel, the epipoles
    will lie at infinity. Here, we can define an epipolar constraint, as if we know
    the transformation between camera center **O[1]** and **O[2]** as translation
    T and rotation R, we can compute the location of point **P1** in **Image 1** to
    the corresponding location in **Image 2**. Mathematically, this is written as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8137ecdd-e645-4387-bee2-1613721c4bee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Inversely, if know the location of corresponding points in two images, we would
    like to compute the rotation matrix R and translation matrix T between the two
    camera centers. Here, if the two cameras are different, the camera centers can
    be at the different distance from the image plane and, therefore, we would require
    camera intrinsic parameters too. Mathematically, this is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85ab1610-68d7-4026-8425-da6370aa5def.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *F* is called the **fundamental matrix** and *K* is our **camera intrinsic
    matrix** for each camera. Computing *F*, we can know the correct transformation
    between the two camera poses and can convert any point on one image plane to another.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see transformations between images and their applications.
  prefs: []
  type: TYPE_NORMAL
- en: Aligning images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image alignment is a problem for computing a transformation matrix so that on
    applying that transformation to an input image, it can be converted to the target
    image plane. As a result, the resulting images look like they are stitched together
    and form a continuous larger image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Panorama is one such example of aligning images, where we collect images of
    a scene with changing camera angles and the resulting image is a combination of
    images aligned. A resulting image is as shown, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f75b5df7-a580-47da-bfd8-cae9d1f85a5e.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding figure, an example of panorama creation is shown. Using a camera,
    we collect multiple images for the same scene by adding overlapping regions. As
    the camera is moved, often, the pose changes significantly, so therefore for different
    poses of the camera a transformation matrix is computed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started with a basic method to compute this transformation matrix.
    The following code works inside Jupyter notebook too. In the following block of
    code, we define a function to compute **oriented BRIEF** (**ORB**) keypoints.
    There is a descriptor for each keypoint also:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have feature keypoints, we match them using a brute force matcher,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is our main function for computing the fundamental matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will extend relative transformation between images to
    compute camera pose and also estimate the trajectory of the camera.
  prefs: []
  type: TYPE_NORMAL
- en: Visual odometry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Odometry is the process of incrementally estimating the position of a robot
    or device. In the case of a wheeled robot, it uses wheel motion or inertial measurement
    using tools such as gyroscopes or accelerometers to estimate the robot's position
    by summing over wheel rotations. Using **visual odometry** (**VO**), we can estimate
    the odometry of cameras using only image sequences by continuously estimating
    camera motion.
  prefs: []
  type: TYPE_NORMAL
- en: 'A major use of VO is in autonomous robots like drones, where gyroscopes and
    accelerometers are not robust enough for motion estimation. However, there are
    several assumptions and challenges in using VO:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, objects in the scene for the camera should be static. While the camera
    captures a sequence of the image, the only moving object should be the camera
    itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, during the estimation of VO, if there are significant illumination
    changes, like light source appearance, drastic changes to pixel values might occur
    in subsequent images. As a result, VO suffers from large errors or complete dysfunction.
    The same case applies to dark environments; due to the lack of illumination, VO
    is not able to estimate robust motion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The process of VO is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the starting position as the origin, for the frame of reference.
    All the subsequent motion estimation is done with respect to this frame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As an image arrives, compute features and match corresponding features with
    previous frames to get a transformation matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the historical transformation matrix between all subsequent frames to compute
    the trajectory of the camera.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This process is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2d2ae77-0f07-4849-923e-e3bdf08d3532.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **I[i]** is the i^(th) image received from the camera and **T[ij]** is
    the transformation matrix computed using feature matching between *i* and *j*
    images. The trajectory of camera motion is shown with stars, where *P[i]* is the
    estimated pose of the camera at the *i*^(th) image. This can be a two-dimensional
    pose with (*x*, *y*) angle as well as a three-dimensional pose. Each *P[j]* is
    computed as applying the transformation *T[ij]* on *P[i]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than the assumption mentioned earlier, there are a few limitations to
    VO estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: As more images are observed from the sequence, the errors in trajectory estimation
    are accumulated. This results in an overall drift in the computed track of camera
    motion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In cases of sudden motion in camera,  the image feature match between the corresponding
    two images will be significantly erroneous. As a result, the estimated transformation
    between the frames will also have huge errors and, therefore, the overall trajectory
    of camera motion gets highly distorted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visual SLAM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SLAM refers to Simultaneous Localization and Mapping and is one of the most
    common problems in robot navigation. Since a mobile robot does not have hardcoded
    information about the environment around itself, it uses sensors onboard to construct
    a representation of the region. The robot tries to estimate its position with
    respect to objects around it like trees, building, and so on. This is, therefore,
    a chicken-egg problem, where the robot first tries to localize itself using objects
    around it and then uses its obtained location to map objects around it; hence
    the term *Simultaneous  Localization and Mapping*. There are several methods for
    solving the SLAM problem. In this section, we will discuss special types of SLAM
    using a single RGB camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual SLAM methods extend visual odometry by computing a more robust camera
    trajectory as well as constructing a robust representation of the environment.
    An overview of Visual SLAM in action is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8abfbd3-a703-44ca-a782-c9728fe0ea75.png)'
  prefs: []
  type: TYPE_IMG
- en: This is an overview of a generic SLAM method composed of an undirected graph.
    Each node of the graph is composed of a keyframe which represents unique information
    about the world and also contains the camera pose (*x*,*y*, angle) for the location.
    In between, keyframes are frames that overlap significantly with the keyframes
    scene, however, they help in computing robust estimates of pose for the next frame.
    Here, a camera starts the process by initializing a keyframe at the origin. As
    the camera moves along a trajectory, the SLAM system updates the graph by adding
    keyframes or frames based on criteria. If the camera returns back to a previously
    seen area, it links up with the old frame, creating a cyclic structure in the
    graph. This is often called **loop closure** and helps correct the overall graph
    structure. The edges connecting nodes to another in the graph are usually weighted
    with a  transformation matrix between the pose of the two nodes. Overall, the
    graph structure is corrected by improving the position of keyframes. This is done
    by minimizing overall error. Once a graph is constructed, it can be saved and
    used for localizing a camera by matching to the nearest keyframe.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will see a popular robust method, ORB SLAM, using monocular
    cameras. This method constructs a graph structure similar to that which was shown
    previously to keep track of camera pose and works on RGB image frames from a simple
    camera. The steps involved can be summarized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: In the case of the monocular camera, the input is a single captured
    frame.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Initialization**: Once the process starts, a map is initialized with the
    origin, and the first node of a keyframe graph is constructed.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are three threads that run in parallel for the system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Tracking**: For each incoming frame, ORB features are extracted for matching.
    These features are matched with previously seen frames and are then used to compute
    the relative pose of the current frame. This also decides if the current frame
    is to be kept as a keyframe or used as a normal frame.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local mapping**: If a new keyframe is determined from tracking, the overall
    map is updated with the insertion of a new node in the graph. While a new connection
    between neighbourhood keyframes is formed, redundant connections are removed.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loop closure**: If there is a previously seen keyframe that matches the current
    keyframe, a loop is formed. This gives extra information about drift caused by
    the trajectory of the camera pose and as a result, all node poses in the graph
    map is corrected by an optimization algorithm.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will use an implementation of ORB SLAM2 from [https://github.com/raulmur/ORB_SLAM2](https://github.com/raulmur/ORB_SLAM2).
    This is not a Python-based implementation. The instruction provided there can
    be used to build the package and can be used to see visual SLAM. However, for
    demonstration purposes, we will use a Docker container version of it.
  prefs: []
  type: TYPE_NORMAL
- en: A Docker is a container platform that provides the distributed shipping of an
    environment as if they are packed inside a ship container,  as well as code to
    run applications. We need to install the Docker platform and pull an image of
    the environment, as well as the code. The environment inside the image is independent
    of the platform we use, as long as the Docker platform is installed. If you want
    to learn more about Docker and containers, the following website provides more
    details, as well as installation instructions: [https://www.docker.com/what-docker](https://www.docker.com/what-docker).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once Docker is installed, we can begin with the following steps for ORB SLAM
    2\. Let''s start by pulling a Docker image (this is similar to cloning a repository)
    for ORB SLAM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This will download the environment for the package and pre-build the ORB SLAM2
    repository so that we don't have to build it again. All the dependencies for this
    repository are already satisfied inside the Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: Once the Docker image is downloaded, let's get started with downloading the
    dataset. In this section, we will use the `TUM RGBD` dataset, which was collected
    specifically to evaluate SLAM and VO methods. Earlier in this chapter, under dataset
    and libraries, we saw how to download this dataset. We will use the extracted
    dataset in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Since this implementation of ORB SLAM uses a GUI interface to output the results,
    we will first add the GUI interface to the Docker image. The following code assumes
    a Linux environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the GUI output from ORB SLAM, add this as the first step, otherwise, visual
    SLAM will run but there will be an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s launch the downloaded image using the Docker platform, though with
    several parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `-e` and `-v` parameters are used to set the display environment
    for GUI. The dataset downloaded before is shared inside Docker using `- v $PATH_TO_DOWNLOADED_DATASET:$PATH_INSIDE_DOCKER`.
    Finally, the name of the image is `orb-slam: latest`*,* which we downloaded earlier
    using Docker pull, and we asked it to run bash inside Docker using `/bin/bash`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On running the previous command, we can see a change in the Terminal, as if
    we logging in to a new computer Terminal. Let''s go and run ORB-SLAM as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, the first parameter is to run Monocular Visual SLAM, as there are other
    methods too. The other parameters are to run the type of dataset that we had downloaded
    earlier. If there is any change in the dataset, these parameters are to be changed
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'On this command, after some time there will be two windows, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30ce9f93-8e72-400e-a603-0bcf7713d6d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the window on the right is the input dataset, with the keypoints detected
    in each frame. While the window on the left details the Visual SLAM happening.
    As we can see there are blue boxes that show the keyframe graph creation, with
    the current state of the position of the camera and its links with the historical
    position. As the camera in the dataset is moved, the graph is created and adjusted
    as more observations are found. The result is an accurate trajectory of the camera
    as well as adjusted keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the aim was to view computer vision from a geometrical point
    of view. Starting with understanding how an image is formed using a pinhole camera,
    there was a discussion on how to incorporate three-dimensional worlds using multi-image
    formation. We saw an explanation of Visual Odometry with an introduction to Visual
    SLAM. The various steps involved in SLAM were explained and a demo of using ORB-SLAM
    was also shown, so that we could see a SLAM operation as it happened. This is
    basic motivation to extend the SLAM solution for various other datasets, and so
    create interesting applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sturm Jürgen, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers.
    *A Benchmark for the Evaluation of RGB-D SLAM Systems*. In Intelligent Robots
    and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 573-580\. IEEE,
    2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mur-Artal Raul, Jose Maria Martinez Montiel, and Juan D. Tardos. *ORB-SLAM:
    A Versatile and Accurate Monocular SLAM System*. IEEE Transactions on Robotics 31,
    no. 5 (2015): 1147-1163.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rublee Ethan, Vincent Rabaud, Kurt Konolige, and Gary Bradski. *ORB: an efficient
    alternative to SIFT or SURF*. In Computer Vision (ICCV), 2011 IEEE international
    conference on, pp. 2564-2571\. IEEE, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
