- en: Machine Learning Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: Hello everyone, and welcome to *Machine Learning Using C# and .NET*. Our goal
    in this book is to expose you, a seasoned C# .NET developer, to the many open
    source machine learning frameworks that are available, as well as examples of
    using those packages. Along the way, we'll talk about logging, facial and motion
    detection, decision trees, image recognition, intuitive deep learning, quantum
    computing, and more. In many cases, you'll be up and running within minutes. It's
    my true hope that there is something for everyone in this series. Most importantly,
    having dealt with developers for 30 years now, here's why I wrote this book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，欢迎来到《使用C#和.NET进行机器学习》。本书的目标是向您，一位经验丰富的C# .NET开发者，介绍许多可用的开源机器学习框架，以及如何使用这些包的示例。在这个过程中，我们将讨论日志记录、面部和动作检测、决策树、图像识别、直观的深度学习、量子计算等内容。在许多情况下，你将在几分钟内就能启动并运行。我真诚地希望这个系列中总有一款适合每个人。最重要的是，作为一名已经与开发者打交道30年的作者，以下是为什么我写这本书的原因。
- en: As a lifelong Microsoft developer, I have often watched developers struggle
    to find the resources needed for everyday problems. Let's face it, none of us
    have the time to do things the way we like, and few of us are fortunate enough
    to work in a true research and development unit. We've made quite a journey over
    the years though, from those of us old enough to remember having the sentinel
    copy of the C programmers' reference and 50 other books on our desk, to now being
    able to type in a quick search into Google and get exactly (okay, sometimes exactly)
    what we are looking for. But now that the age of AI is among us, things take a
    bit of a different turn. As C# developers, Google search isn't always our best
    friend when it comes to machine learning because almost everything being used
    is Python, R, MATLAB, and Octave. We also have to remember that machine learning
    has been around for many years; it's just recently that corporate America has
    embraced it and we're seeing more and more people become involved. The computing
    power is now available, and the academia has made incredible strides and progress
    in bringing it out into the world. But the world, my friends, as you have no doubt
    heard, is a scary place! Where is a C# .NET developer to turn? Let's start answering
    this question with a short story in the next section, which, unfortunately, is
    as true as the sky is blue. At least here in sunny Florida!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名终身微软开发者，我经常看到开发者们为了解决日常问题而苦苦寻找所需的资源。让我们面对现实吧，我们中没有一个人有足够的时间按照自己的方式做事，而且我们中很少有人有幸在一个真正的研发部门工作。然而，多年来我们已经走了很长的路，从那些还记得我们桌上有C程序员参考书和50多本书的我们，到如今能够快速在谷歌上搜索并找到我们确切（好吧，有时是确切）想要的东西。但现在，随着人工智能时代的到来，事情发生了一些变化。作为C#开发者，当涉及到机器学习时，谷歌搜索并不总是我们的最佳朋友，因为几乎所有被使用的东西都是Python、R、MATLAB和Octave。我们还必须记住，机器学习已经存在很多年了；只是最近，美国企业才开始拥抱它，我们看到越来越多的人参与其中。现在，计算能力已经可用，学术界在将其推广到世界各地的过程中取得了惊人的进步。但朋友们，正如你们无疑已经听说的那样，世界是一个可怕的地方！C#
    .NET开发者该往哪里去呢？让我们在下一节中通过一个简短的故事来回答这个问题，不幸的是，这个故事的真实性就像天空一样。至少在这里阳光明媚的佛罗里达州是这样！
- en: 'In this chapter, we are going to learn the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Data mining
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据挖掘
- en: '**Artificial Intelligence** (**AI**) and bio-AI'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）和生物人工智能'
- en: Deep learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习
- en: Probability and statistics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率与统计
- en: Supervised learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习
- en: Unsupervised learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Reinforcement learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习
- en: Whether to buy, build or open source
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是购买、构建还是开源
- en: Introduction to machine learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习简介
- en: I once had a boss whom I told I was using machine learning to discover more
    about our data. His response was, *What do you think you can learn that I don't
    already know!* If you haven't encountered one of those in your career, congratulations.
    Also let me know if you have any openings! But you more than likely have, or will.
    Here's how it was handled. And no, I didn't quit!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我曾经有一个老板，我告诉他我正在使用机器学习来发现更多关于我们数据的信息。他的回应是：“你认为你能学到我不知道的东西吗！”如果你在你的职业生涯中没有遇到过这样的人，恭喜你。也请告诉我如果你有任何空缺！但你更有可能已经遇到了，或者将会遇到。下面是如何处理的。而且，我没有辞职！
- en: 'Me: "The goal is to learn more information and details about the funds that
    we have and how they may relate to what the user actually means."'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我说：“目标是了解更多关于我们拥有的基金以及它们可能如何与用户实际意图相关联的信息和细节。”
- en: 'Him: "But I already know all that. And machine learning is just a buzzword,
    it''s all data in the end, and we''re all just data stewards. The rest is all
    buzzwords. Why should we be doing this and how is it going to help me in the end."'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 他：“但我已经知道这一切了。机器学习只是一个时髦词，最终都是数据，我们都是数据管理员。其余的都是时髦词。我们为什么要做这件事，它最终将如何帮助我？”
- en: 'Me: "Well, let me ask you this. What do you think happens when you type a search
    for something in Google?"'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我：“好吧，让我问你这个问题。你认为你在谷歌上搜索某个东西时会发生什么？”
- en: 'Him: Deer-in-the headlights look with a slight hint of anger.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 他：眼神呆滞，带着一丝愤怒。
- en: 'Him: "What do you mean? Google obviously compares my search against other searches
    that have historically looked for the same thing."'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 他：“你是什么意思？显然，谷歌会把我搜索的内容与其他历史上搜索过相同内容的其他搜索进行对比。”
- en: 'Me: "OK, and how does that get done?"'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我：“好吧，那它是怎么完成的？”
- en: 'Him: A slightly bigger hint at anger and frustration.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 他：愤怒和挫败感略有加剧。
- en: 'Him: "Obviously its computers searching the web and matching up my search criteria
    against others." Me: "But did you ever think about how that search gets matched
    up amongst the billions of other searches going on, and how all the data behind
    the searches keeps getting updated? People obviously cannot be involved or it
    wouldn''t scale."'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 他：“显然是它的计算机在搜索网络，并将我的搜索条件与其他搜索进行匹配。” 我：“但你有没有想过，这个搜索是如何在数十亿其他搜索中匹配的，以及所有搜索背后的数据是如何不断更新的？显然，人们无法参与其中，否则就无法扩展。”
- en: 'Him: "Of course, algorithms are finely tuned and give the results we are looking
    for, or at least, recommendations."'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 他：“当然，算法经过精心调整，给出了我们想要的结果，或者至少，推荐。”
- en: 'Me: "Right, and it is machine learning that does just that." (not always but
    close enough!)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我：“没错，这正是机器学习所做的事情。”（不一定总是，但足够接近！）
- en: 'Him: "OK, well I don''t see what more I can learn from the data so let''s see
    how it goes."'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 他：“好吧，我觉得从数据中我学不到更多，让我们看看结果如何。”
- en: So, let's be honest folks. Sometimes, no amount of logic will override blinders
    or resistance to change, but the story has a much different and more important
    meaning behind it than a boss who defies everything we learned in biology. In
    the world of machine learning, it's a lot harder to prove/show what's going on,
    whether or not things are working, how they are working, why they are (or are
    not) working, and so on to someone who isn't in the day-to-day trenches of development
    like you are. And even then, it could be very difficult for you to understand
    what the algorithm is doing as well.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们坦诚一点，朋友们。有时候，无论逻辑多么严密，都无法克服盲点或对改变的抵抗，但这个故事背后有着截然不同且更为重要的意义，远非一个违背我们生物学所学的一切的老板。在机器学习的世界中，向一个不身处日常开发前线的人证明/展示正在发生的事情，无论事情是否在正常运作，它们是如何运作的，为什么它们（或不）在运作，等等，都要困难得多。即使如此，你也可能很难理解算法正在做什么。
- en: 'Here are just some of the questions you should be asking yourself when it comes
    to deciding whether or not machine learning is right for you:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定机器学习是否适合你时，以下是一些你应该问自己的问题：
- en: Are you just trying to be *buzzword compliant* (which might be what's really
    being asked for) or is there a true need for this type of solution?
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你只是在尝试符合时髦词（这可能就是真正被要求的东西）吗，或者你对这种解决方案有真正的需求？
- en: Do you have the data you need?
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有你需要的数据吗？
- en: Is the data clean enough for usage (more on that later)?
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否足够干净以供使用（关于这一点稍后还会讨论）？
- en: Do you know where, and whether, you can get data that you might be missing?
    More importantly, how do you know that data is in fact missing?
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你知道你能否获得可能缺失的数据，以及你是否知道数据实际上缺失了吗？
- en: Do you have a lot of data or just a small amount?
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你有大量数据还是只有少量数据？
- en: Is there another known and proven solution that already exists that we could
    use instead?
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否存在已知且经过验证的解决方案，我们可以用它来代替？
- en: Do you know what you are trying to accomplish?
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你知道你试图实现什么吗？
- en: Do you know how you are going to accomplish it?
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你知道你将如何实现它吗？
- en: How will you explain it to others?
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将如何向他人解释？
- en: How will you be able to prove what's going on under the hood when asked?
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当被问及时，你将如何证明引擎盖下正在发生的事情？
- en: These are just some of the many questions we will tackle together as we embark
    on our machine learning journey. It's all about developing what I call the *machine
    learning mindset*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是我们在开始机器学习之旅时将共同解决的问题的一部分。这全部关乎培养我所说的机器学习思维模式。
- en: Nowadays, it seems that if someone does a SQL query that returns more than one
    row, they call themselves a **data scientist**. Fair enough for the resume; everyone
    needs a pat on the back occasionally, even if it's self-provided. But are we really
    operating as data scientists, and what exactly does data scientist mean? Are we
    really doing machine learning, and what exactly does that mean? Well, by the end
    of this book, we'll hopefully have found the answers to all of that, or at the
    very least, created an environment where you can find the answers on your own!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，似乎如果有人执行了一个返回多行数据的SQL查询，他们就会自称是**数据科学家**。对于简历来说，这是公平的；每个人偶尔都需要得到一些认可，即使这是自我提供的。但我们真的在以数据科学家的身份运作吗？数据科学家究竟是什么意思？我们真的在做机器学习吗？那究竟是什么意思？好吧，到这本书的结尾，我们希望找到所有这些答案，或者至少创造一个环境，让您自己找到答案！
- en: Not all of us have the luxury of working in the research or academic world.
    Many of us have daily fires to fight, and the right solution just might be a tactical
    solution that has to be in place in 2 hours. That's what we, as C# developers,
    do. We sit behind our desks all day, headphones on if we're lucky, and type away.
    But do we ever really get the full time we want or need to develop a project the
    way we'd like? If we did, there wouldn't be as much technical debt in our projects
    as we have, right (you do track your technical debt, right)?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们中并非所有人都有在研究或学术界工作的奢侈。我们中的许多人每天都在应对各种挑战，而正确的解决方案可能就是一个必须在2小时内就位的具体战术解决方案。这就是我们作为C#开发者所做的事情。我们整天坐在办公桌后面，如果幸运的话，戴着耳机，敲击键盘。但我们真的得到了我们想要或需要的全部时间来以我们喜欢的方式开发项目吗？如果我们做到了，我们的项目中就不会有那么多技术债务，对吧（你真的在跟踪你的技术债务，对吧）？
- en: We need to be smart about how we can get ahead of the curve, and sometimes we
    do that by thinking more than we code, especially upfront. The academic side of
    things is invaluable; there's simply no replacement for knowledge. But most production
    code in corporate America isn't written in academic languages such as Python,
    R, Matlab and Octave. Even though all that academic wealth is available, it's
    not available in the form that suits us best to do our jobs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要聪明地考虑如何超越曲线，有时我们通过思考多于编码来实现这一点，尤其是在一开始。学术方面是无价的；知识根本无法替代。但美国企业中的大多数生产代码并不是用Python、R、Matlab和Octave这样的学术语言编写的。尽管所有这些学术财富都可用，但它们并不是以最适合我们工作的形式提供的。
- en: In the meantime, let's stop and praise those that contribute to the open source
    community. It is because of them that we have some excellent third-party open
    source solutions out there that we can leverage to get the job done. It's such
    a privilege that the open source community allows us to utilize what they have
    developed, and the objective of this book is to expose you to just some of those
    tools and show how you can use them. Along the way, we'll try and give you at
    least some of the basic behind-the-scenes knowledge that you should know, just
    so that everything isn't a black hole versus a black box!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，让我们停下来赞扬那些为开源社区做出贡献的人。正是因为他们，我们才有一些出色的第三方开源解决方案可以利用，来完成我们的工作。开源社区允许我们利用他们所开发的内容，这是一种特权。本书的目的是向您展示其中的一些工具，并展示您如何使用它们。在这个过程中，我们将尽力为您提供一些基本的后台知识，这样就不会让一切都变得像黑洞一样神秘！
- en: You've heard buzzwords everywhere. I used to have a 2-4 hour commute to and
    from work each day, and I can't remember the total number of billboards I would
    see that had the words **machine learning** or AI on them. They are everywhere,
    but what exactly does it all mean? AI, machine learning, data science, **Natural
    Language Processing** (**NLP**), data mining, neurons, phew! It seems like as
    soon as corporate America got involved, what was once a finely tuned art became
    a messy free-for-all, and micro-managed project with completely unreal expectations.
    I've even heard a prospective client say, *I'm not sure what it means, but I just
    don't want to be left behind!*
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您无处不在都能听到这些热门词汇。我过去每天都要花费2-4个小时的时间通勤，我记不清我看到了多少个广告牌上有“机器学习”或AI的字样。它们无处不在，但这一切究竟意味着什么呢？AI、机器学习、数据科学、**自然语言处理（NLP**）、数据挖掘、神经元，等等！似乎一旦美国企业介入，曾经精细调校的艺术变成了一个混乱的自由竞争，一个微观管理的项目，有着完全不切实际的目标。我甚至听到一个潜在客户说：“我不确定它是什么意思，但我不想被落下！”
- en: 'The first thing we must do is to learn the proper way to approach a machine
    learning project. Let''s start with some definitions:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须做的第一件事是学习正确的方法来处理机器学习项目。让我们从一些定义开始：
- en: 'Tom Mitchell has defined machine learning as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 托马斯·米切尔将机器学习定义为：
- en: '"A computer program is said to learn from experience E with respect to some
    class of tasks T and performance measure P if its performance at tasks in T, as
    measured by P, improves with experience E."'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: “如果一个计算机程序在经验E方面对任务T和性能度量P有所学习，那么它在T中的任务性能，按照P来衡量，会随着经验E的提高而提高。”
- en: 'Our definition is going to be just a bit different. It will hopefully be something
    that you can use when asked to defend your chosen path:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的定义将会略有不同。希望这能成为你在被要求捍卫你选择的道路时可以用到的东西：
- en: '"Machine learning is a collection of techniques which can be used to deal with
    large amounts of data in the most efficient and effective manner possible, which
    will derive actionable results and insight for us from that data."'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: “机器学习是一系列技术，可以以最有效和最有效的方式处理大量数据，从而从数据中为我们提供可操作的结果和洞察。”
- en: Now, what about those things we call **techniques**? Make no mistake; techniques
    such as probability, statistics, they are all there, just hidden under the covers.
    And the tools we're going to use to perform our examples will hide the details
    just like Python, R, and the rest of them do! That being said, it would be a complete
    disservice to you if we didn't at least make you aware of some of the basics,
    which we'll cover in a moment. I don't mean to lower the importance of any of
    them as they are all equally important, but our goal here is to get all C# developers
    up and running as quick as possible. We're going to give you enough information
    to make you buzzword compliant, and then you'll know more than just the block
    box API calls! I encourage each one of you to pursue as much academic knowledge
    as possible in this field. Machine Learning and Artificial Intelligence are changing
    daily it seems, so always keep up with the latest. The more you know, the better
    you will be at gaining acceptance for your project.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，关于我们称之为**技巧**的那些东西，不要误解；比如概率、统计学，它们都在那里，只是隐藏在表面之下。而我们用来执行示例的工具也会像Python、R和其他类似工具一样隐藏细节！话虽如此，如果我们不至少让你意识到一些基础知识，那将是对你的极大不公，这些基础知识我们稍后会涉及。我并不是要降低任何一项的重要性，因为它们都是同等重要的，但我们的目标是让所有C#开发者尽可能快地开始使用。我们将提供足够的信息，让你符合行业术语，然后你将知道的不仅仅是块状API调用！我鼓励你们每个人都尽可能在这个领域追求更多的学术知识。机器学习和人工智能似乎每天都在变化，所以请始终跟上最新的发展。你知道的越多，你在获得项目认可方面就会越出色。
- en: Since we brought up the topic of **buzzword compliant**, let's clear up a few
    terms right from the start. Data mining, machine learning, artificial intelligence,
    the list goes on and on. I'll only cover a few terms for now, but here's an easy
    way to think about it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们提到了**行业术语**，让我们从一开始就澄清一些术语。数据挖掘、机器学习、人工智能，等等。现在我只介绍几个术语，但这里有一个简单的方式来思考它。
- en: You're on a road trip with your family. Let's assume you have children, and
    let's put aside the *are we there yet* conversations! You are driving down the
    highway and one of your kids (a very young toddler), yells *TRUCK* and points
    out the window at a truck. This child is very young, so how did he know that particular
    vehicle was a truck (let's assume it really was!). They know it's a truck because
    every previous time they did the same thing you said *Yes* or *No*. That's machine
    learning. Then, when you told them *Yes* or *No*, that's reinforcement learning.
    If you said *Yes, that's a big truck*, that's adding context to the reinforcement,
    and that moves us down the road into deep learning. See what you've been teaching
    your children that you didn't even know about?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你和家人在旅行。假设你有孩子，我们先不考虑“我们到那里了吗”这样的对话！你正在高速公路上驾驶，其中一个孩子（一个非常小的幼儿）大声喊“卡车”并指向窗外的一辆卡车。这个孩子非常小，那么他是怎么知道那辆特定车辆是卡车的（让我们假设它真的是卡车！）他们知道那是卡车，因为每次他们做同样的事情时，你都会说“是的”或“不”。那就是机器学习。然后，当你告诉他们“是的，那是一辆大卡车”时，你就是在为强化学习添加上下文，这让我们进入了深度学习。你有没有注意到你一直在教给你的孩子你甚至不知道的事情？
- en: Hope that helped.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这有所帮助。
- en: Data mining
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据挖掘
- en: Data mining deals with searching large amounts of data for very specific information.
    You are searching through your data looking for something specific. For example,
    a credit card company would use data mining to learn about buyers habits by analyzing
    purchases and their locations. This information then becomes very useful for things
    such as targeted advertisements.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 数据挖掘涉及在大量数据中搜索非常具体的信息。您正在通过数据寻找特定的事物。例如，信用卡公司会通过分析购买行为及其位置来使用数据挖掘了解买家的习惯。这些信息随后变得非常有用，例如用于定向广告。
- en: Machine learning, on the other hand, focuses on performing the actual task of
    searching for that data using algorithms you have provided. Makes sense?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，机器学习专注于使用您提供的算法执行搜索该数据的实际任务。这说得通吗？
- en: 'Enough said for now, but here is an excellent link where you can learn more
    about data mining: [https://blog.udacity.com/2014/12/24-data-science-resources-keep-finger-pulse.html](https://blog.udacity.com/2014/12/24-data-science-resources-keep-finger-pulse.html)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就说到这里，但这里有一个非常棒的链接，您可以从中了解更多关于数据挖掘的信息：[https://blog.udacity.com/2014/12/24-data-science-resources-keep-finger-pulse.html](https://blog.udacity.com/2014/12/24-data-science-resources-keep-finger-pulse.html)
- en: Artificial Intelligence
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工智能
- en: Artificial Intelligence is a higher order of machine learning. Some people have
    defined it as when the machine appears as smart as or smarter than a human. As
    for me, the verdict is still out on that one. The more I watch the daily news,
    the more I wonder which intelligence it is that is artificial, and for that matter,
    what intelligence really is! There are so many definitions floating around, but
    in a nutshell, Artificial Intelligence is considered a machine doing things that
    a human could or should do in a manner such that any reasonable person would not
    be able to distinguish the machine from the human in its response. In any event,
    Artificial Intelligence is a very far-reaching subject, and unfortunately there
    are as many meanings as people using the term!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能是机器学习的高级形式。有些人将其定义为当机器表现得和人类一样聪明，甚至比人类更聪明的时候。至于我，对这个问题的结论仍然悬而未决。我越看每天的新闻，就越想知道哪一种智能是人工的，以及真正的智能究竟是什么！有如此多的定义在流传，但简而言之，人工智能被认为是做人类能够或应该做的事情的机器，以至于任何合理的人都不可能在机器的响应中将其与人类区分开来。无论如何，人工智能是一个非常广泛的主题，不幸的是，人们对这个术语的理解和定义也各不相同！
- en: Bio-AI
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物人工智能
- en: Bio-AI refers to putting a biological component alongside a computational component.
    Genotypes, phenotypes, neurons, mirror neurons, canonical neurons, synapses...
    you'll hear all that mentioned under this category, **Artificial Neural Networks**
    (**ANNs**)! Bio-AI is mostly used in the medical field. For now, we need not concern
    ourselves with this, but just know that the term exists and that biology is the
    basis for its incorporation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 生物人工智能指的是将生物成分与计算组件结合在一起。基因型、表型、神经元、镜像神经元、典型神经元、突触……您将在这一类别下听到所有这些提及，**人工神经网络**（**ANNs**）！生物人工智能主要应用于医疗领域。目前，我们不需要担心这一点，但只需知道这个术语存在，并且生物学是其结合的基础。
- en: Deep learning
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: 'For many years, it was believed that neural networks (using a concept known
    as hidden layers) only needed a single hidden layer to solve any problem. With
    the increase of computing power, decrease of computing hardware cost, and advances
    of neural network algorithms, it''s common to have hundreds or even thousands
    of **hidden layers** in your network. The increase in the number of hidden layers,
    among other things, is what deep learning is in a very small nutshell! Here''s
    a visual comparison that might help in making things clearer:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，人们认为神经网络（使用一个称为隐藏层概念）只需要一个隐藏层就能解决任何问题。随着计算能力的提高、计算硬件成本的降低和神经网络算法的进步，网络中拥有数百甚至数千个**隐藏层**是很常见的。隐藏层数量的增加，以及其他因素，正是深度学习在非常简短的概念中的核心！这里有一个视觉比较，可能有助于使事情更清晰：
- en: '![](img/c6217e09-d275-4785-91aa-73808208a6aa.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c6217e09-d275-4785-91aa-73808208a6aa.png)'
- en: No hidden layers
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 没有隐藏层
- en: As you can see in the following representational image there are several hidden
    layers in the network.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在下面的表示性图像中可以看到，网络中有几个隐藏层。
- en: '![](img/475dd46e-8d34-492e-abcf-85dae4347bf8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/475dd46e-8d34-492e-abcf-85dae4347bf8.png)'
- en: Many hidden layers (white circles)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 许多隐藏层（白色圆圈）
- en: Probability and statistics
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率与统计学
- en: Believe it or not, this is what you are doing; it's just very well abstracted
    from your view. But let me give you an incredibly, overly simplified, quick primer...
    just in case you are rusty!
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，这就是你正在做的事情；只是它从你的视角来看非常抽象。但让我给你一个极其、过于简化的、快速入门指南……以防你生疏了！
- en: You see a polar bear walking in the snow. You wonder what kind of footprints
    it makes. That's probability. Next, you see footprints in the snow and wonder
    if it's a polar bear. That's statistics. Kaboom! Now you're primed! You're also
    probably wondering what is wrong with this author, so maybe another example just
    in case!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到一只北极熊在雪地里行走。你好奇它会留下什么样的脚印。这就是概率。接下来，你看到雪地里的脚印，想知道那是不是北极熊的。这就是统计学。砰！现在你准备好了！你可能也在想这位作者有什么问题，所以也许再举一个例子以防万一！
- en: Probability deals with predicting the likelihood of future event(s).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率论涉及预测未来事件的可能性。
- en: Statistics deals with analyzing the frequency of past event(s).
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计学涉及分析过去事件的发生频率。
- en: Approaching your machine learning project
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接近你的机器学习项目
- en: Next, let's talk about how we're going to approach our machine learning project,
    and while doing so, continue to define/refine our machine learning mindset. Let's
    start by defining the basic steps that we need to use each time we approach one
    of these projects. Basically, we can break them down into the following categories.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们谈谈我们将如何接近我们的机器学习项目，并在做这件事的同时，继续定义/细化我们的机器学习思维模式。让我们从定义每次我们接近这些项目时需要使用的基本步骤开始。基本上，我们可以将它们分解为以下几类。
- en: Data collection
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据收集
- en: There are countless types of data at your disposal, from SQL and NoSQL databases,
    Excel files, Access databases, text files, and on and on. You need to decide where
    your data is located, how it is formatted, how you will import and refine it.
    You need to always keep in mind that there is no substitute for large amounts
    of testing and training data, as well as the quality of it. Garbage in, garbage
    out can get very messy in machine learning!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你有无数种类型的数据可供使用，从SQL和NoSQL数据库、Excel文件、Access数据库、文本文件等等。你需要决定你的数据在哪里，它的格式是什么，你将如何导入和精炼它。你需要始终记住，大量测试和训练数据以及其质量是没有替代品的。垃圾输入，垃圾输出在机器学习中可能会变得非常混乱！
- en: Data preparation
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'As we said previously, there is simply no substitute for data quality. Is there
    data that is missing, malformed, or incorrect? And let''s not forget about another
    term you''ll get familiar with, data outliers. Those are the nasty little pieces
    of data that simply don''t fit nicely with the rest of your data! Do you have
    those? If so, should they be there, and if so, how will they be treated? If you
    are not sure, here''s what a data outlier might look like if you are plotting
    your data:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所说，数据质量是没有替代品的。有没有缺失、格式不正确或错误的数据？而且别忘了你将熟悉的一个术语，数据离群值。那些是那些与你的其他数据不太匹配的讨厌的小数据片段！你有这些吗？如果有，它们应该在那里吗？如果是，它们将如何被处理？如果你不确定，这里是你绘制数据时可能看到的数据离群值的例子：
- en: '![](img/f5bdc3f4-92b4-45e2-acdb-118bc218bd32.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f5bdc3f4-92b4-45e2-acdb-118bc218bd32.png)'
- en: In statistics, an outlier is an observation point that is distant from other
    observations, sometimes very much so, sometimes not. The outlier itself may be
    due to variability in measurement, indicate an experiment defect, or it might
    in fact be valid. If you see outliers in your data, you need to understand why.
    They can indicate some form of measurement error, and the algorithm that you are
    using may not be robust enough to handle these outliers.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计学中，离群值是指与其他观察值距离较远的观察点，有时非常远，有时则不然。离群值本身可能是由于测量中的变化引起的，表明实验缺陷，或者实际上可能是有效的。如果你在你的数据中看到离群值，你需要了解原因。它们可能表明某种形式的测量错误，而你使用的算法可能不足以处理这些离群值。
- en: Model selection and training
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型选择和训练
- en: When creating and training a model, here are a few things that you need to consider.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建和训练模型时，这里有几点你需要考虑。
- en: 'You need to choose the appropriate machine learning algorithm for the task
    at hand, which will be representative of the data you are working with. You will
    then split this into 2-3 subsets of data: **training**, **validation**, and **test**.
    The rules for the correct proportions vary based upon the amount of data you are
    working with. For example, if you have 10,000 rows of data, then perhaps 20% to
    training and 80% to test is good. But if you have 10⁸ rows of data, perhaps 5%
    training and 95% test is better.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要为手头的任务选择合适的机器学习算法，这将代表你正在处理的数据。然后你将这个数据分成2-3个子集：**训练**、**验证**和**测试**。正确的比例规则取决于你处理的数据量。例如，如果你有10,000行数据，那么20%用于训练和80%用于测试可能很好。但如果你有10⁸行数据，可能5%用于训练和95%用于测试会更好。
- en: There is one rule that you must always follow to the letter. Whatever fractionality
    you decide to use for your test, train and validation sets, ***ALL THE DATA MUST
    COME FROM THE SAME DATASET***. This is so very important. You never want to take
    some data from one dataset to train on, and then data from a completely different
    dataset to test on. That will just lead to frustration. Always accumulate huge
    datasets to train, test and validate on!
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一个规则你必须始终严格遵守。无论你决定为你的测试、训练和验证集使用什么比例，***所有数据必须来自同一个数据集***。这一点非常重要。你永远不希望从某个数据集取一些数据来训练，然后从完全不同的数据集取数据来测试。那样只会导致挫败感。始终积累大量数据集来训练、测试和验证！
- en: 'Validation data can be used to validate your test data prior to using the test
    data set. Some people use it, some don''t. However you split your data up, you
    will always have a data set to train with, and a set to test with. The goal of
    your algorithm must be to be flexible enough to handle data it has not previously
    seen, and you can''t do that if you are testing with the same set of data you
    are developing against. Following are the two ways that the data can be split.
    The two approaches show how you can separate test and train sets (one with a cross
    validation set and the other without one):'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据可以在使用测试数据集之前用来验证你的测试数据。有些人使用它，有些人不使用。无论你如何划分你的数据，你总会有一个数据集来训练，一个数据集来测试。你的算法的目标必须是足够灵活，能够处理它之前未见过的数据，而你如果用你开发的数据集进行测试，就无法做到这一点。以下有两种数据划分的方法。这两种方法展示了你可以如何分离测试和训练集（一个包含交叉验证集，另一个不包含）：
- en: '![](img/c561157b-a0ac-405b-9f17-2bb76c34d21b.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c561157b-a0ac-405b-9f17-2bb76c34d21b.png)'
- en: Model evaluation
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型评估
- en: Once you have used your training data, you will move on to testing/evaluating
    your model using the test dataset you prepared earlier. This is where we find
    out how well our model works against data that it has not previously seen. If
    our model fails here, we return to go, do not collect $200, and refine our process!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你使用了你的训练数据，你将进入使用你之前准备的测试数据集来测试/评估你的模型。这就是我们了解我们的模型在之前未见过的数据上的表现如何。如果我们的模型在这里失败，我们就返回去，不要收集$200，并改进我们的流程！
- en: Model tuning
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型调优
- en: As you are evaluating your model, you may, at some point, determine that you
    need to choose a different model or introduce more features/variables/hyper-parameters
    to improve the efficiency and performance of your model. One good way of reducing
    your exposure here is to spend the extra time in the *Data collection* section
    and *Data preparation* section. As we said earlier, there is simply no substitute
    for a lot of correct data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当你评估你的模型时，你可能会在某些时候确定你需要选择一个不同的模型或引入更多的特征/变量/超参数来提高你模型的效率和性能。一个减少你暴露的好方法是在*数据收集*部分和*数据准备*部分花更多的时间。正如我们之前所说的，大量的正确数据是没有任何替代品的。
- en: 'If you have to tune your models, and you will, there are many approaches to
    doing so. Here are just a few:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你必须调整你的模型，你将会这样做，有许多方法可以做到这一点。这里只列举几个：
- en: Grid search
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格搜索
- en: Random search
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机搜索
- en: Bayesian optimization
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Gradient-based optimization
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于梯度的优化
- en: Evolutionary optimization
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进化优化
- en: Let's look at an example dataset—the infamous and always used Iris dataset.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个示例数据集——臭名昭著且总是被使用的Iris数据集。
- en: Iris dataset
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Iris数据集
- en: The Iris dataset is a dataset of flowers introduced by the biologist Mr. Ronald
    Fisher in 1936\. This dataset contains 50 samples from each of 3 species of the
    Iris flower (Iris setosa, Iris virginica, Iris versicolor). Each sample consists
    of four features (length of the sepal, length of the petal, width of the sepal,
    width of the pedal). Combined, this data produces a linearly discriminant model
    distinguishing one species from another.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 爱丽丝数据集是由生物学家罗纳德·费希尔先生在1936年引入的花卉数据集。这个数据集包含来自三种鸢尾花（鸢尾花、鸢尾花、鸢尾花）的50个样本。每个样本由四个特征组成（萼片长度、花瓣长度、萼片宽度、花瓣宽度）。结合这些数据，可以产生一个线性判别模型，区分不同的物种。
- en: 'So, how do we go from the flower to the data:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何从花朵转换到数据：
- en: '![](img/8624a43f-3814-47b7-9967-90e01f955c76.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8624a43f-3814-47b7-9967-90e01f955c76.png)'
- en: 'We need to now take what we know about the visual representation of what we
    are working with (the flower) and transform it into something the computer can
    understand. We do so by breaking down all the information we know about the flower
    into columns (features) and rows (data items) as you can see below:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将我们对我们正在处理的可视表示（花朵）的知识转化为计算机可以理解的东西。我们通过将我们对花朵的所有知识分解为列（特征）和行（数据项）来实现这一点，如下所示：
- en: '![](img/b000725c-378c-4804-aad2-407535d9294f.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b000725c-378c-4804-aad2-407535d9294f.png)'
- en: 'Now that all the measurements are in a format which the computer can understand,
    our first step should be to make sure we have no missing or malformed data, as
    that spells trouble. If you look at the yellow highlights in the previous screenshot,
    you can see that we are missing data. We need to ensure that this gets populated
    before we feed it to our application. Once the data is properly prepared and validated,
    we are ready to go. If we run the Iris validator from **Encog3[4]** our output
    should reflect that we have `150` datasets, which it does:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有的测量值都已经转换成计算机可以理解的形式，我们的第一步应该是确保我们没有缺失或格式不正确的数据，因为这会带来麻烦。如果你查看之前的截图中的黄色高亮部分，你可以看到我们缺少数据。我们需要确保在将其提供给应用程序之前，这些数据得到填充。一旦数据得到适当的准备和验证，我们就可以开始了。如果我们从**Encog3[4]**运行爱丽丝验证器，我们的输出应该反映出我们有`150`个数据集，它确实如此：
- en: '![](img/0074dade-c1bb-4d77-ba1e-f65f0b444782.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0074dade-c1bb-4d77-ba1e-f65f0b444782.png)'
- en: Types of Machine Learning
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的类型
- en: Now, let's briefly familiarize ourselves with the different types of machine
    learning which we will discuss throughout the book, starting with the next chapter.
    It is important that you are at least familiar with these terms as they surely
    will come up one day, and the more you know and understand, the better you will
    be able to approach your problem and explain it to others.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要地熟悉一下本书中将要讨论的不同类型的机器学习，从下一章开始。重要的是，你至少要熟悉这些术语，因为它们肯定会在某一天出现，而且你知道和理解得越多，你就能更好地处理你的问题并向他人解释。
- en: 'Here is a simple diagram which shows the three main categories of machine learning:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的图表，展示了机器学习的三个主要类别：
- en: '![](img/d9438b75-730d-4323-9f8e-379cc73851eb.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d9438b75-730d-4323-9f8e-379cc73851eb.png)'
- en: Supervised learning
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: These types of machine learning models are used to predict the outcome based
    upon the data presented to it. The instructions provided are explicit and detailed,
    or at least should be, which is what has garnered the label **supervised**. We
    are basically learning a function which maps an input to an output based upon
    input and output pairs. This function is inferred from training data which is
    called **labeled**, in that it specifically tells the function what it expects.
    In supervised learning, there is always an input and corresponding output (or
    more correctly, a desired output value). More formally, this type of algorithm
    uses a technique known as **inductive bias** to accomplish this, which basically
    means that there are a set of assumptions which the algorithm will use to predict
    the outputs given inputs it may or may not have previously seen.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型的机器学习模型用于根据呈现给它的数据预测结果。提供的指令是明确和详细的，或者至少应该是，这就是它获得了**监督学习**这一标签的原因。我们基本上是在学习一个函数，该函数根据输入和输出对将输入映射到输出。这个函数是从称为**标记**的训练数据中推断出来的，因为它具体告诉函数它期望什么。在监督学习中，始终有一个输入和相应的输出（或者更准确地说，是一个期望的输出值）。更正式地说，这类算法使用称为**归纳偏差**的技术来实现这一点，这基本上意味着有一组算法将用于预测给定输入的输出的假设。
- en: In supervised learning we typically have access to a set of *X* features (*X[1],
    X[2], X[3], ... X[x]*), measured on observations, and a response *Y*, also measured
    on those same *n* observations. We then try and predict *Y* using *X[1], X[2],
    X[3], ... X[n]*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，我们通常可以访问一组 *X* 特征（*X[1]，X[2]，X[3]，... X[x]*），这些特征是在观察中测量的，以及一个响应 *Y*，也是在相同的
    *n* 次观察中测量的。然后我们尝试使用 *X[1]，X[2]，X[3]，... X[n]* 来预测 *Y*。
- en: Models such as **Support Vector Machines** (**SVM**), linear regression, Naive
    Bayes, and tree-based methods are just a few examples of supervised learning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）、线性回归、朴素贝叶斯和基于树的算法只是监督学习的一些例子。'
- en: 'Next, let''s briefly discuss a few things which we need to concern ourselves
    with when it comes to supervised learning. They are, in no particular order:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们简要讨论一下在监督学习中我们需要关注的一些事情。它们没有特定的顺序：
- en: Bias-variance trade-off
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: Amount of training data
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据量
- en: Input space dimensionality
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入空间维度
- en: Incorrect output values
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不正确的输出值
- en: Data heterogeneity
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据异质性
- en: Bias-variance trade-off
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: Before we talk about the bias-variance trade-off, it only makes sense that we
    would first make sure you are familiar with the individual terms themselves.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论偏差-方差权衡之前，确保你对这些个别术语本身熟悉是很有意义的。
- en: When we talk about bias-variance trade-off, bias refers to an error from incorrect
    assumptions in the learning algorithm. High bias causes what is known as **under-fitting**,
    a phenomenon which causes the algorithm to miss relevant feature-output layer
    relationships in the data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论偏差-方差权衡时，偏差指的是学习算法中由于不正确的假设而产生的错误。高偏差会导致所谓的**欠拟合**，这种现象会导致算法在数据中错过相关的特征-输出层关系。
- en: Variance, on the other hand, is a sensitivity error to small fluctuations in
    the training set. High variance can cause your algorithm to model random noise
    rather than the actual intended outputs, a phenomenon known as **over-fitting**.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，方差是对训练集中微小波动的敏感错误。高方差可能导致你的算法模型随机噪声而不是实际预期的输出，这种现象称为**过拟合**。
- en: There is a trade-off between bias and variance that every machine learning developer
    needs to understand. It has a direct correlation to under and over fitting of
    your data. We say that a learning algorithm has a high variance for an input if
    it predicts a different output result when used on a different training set, and
    that of course is not good.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差和方差之间存在权衡，每个机器学习开发者都需要理解。它与数据的欠拟合和过拟合有直接关系。我们说，如果一个学习算法对不同的训练集预测不同的输出结果，那么它就有高方差，这当然是不好的。
- en: A machine learning algorithm with **low bias** must be flexible enough so that
    it can fit the data well. If the algorithm is designed too flexible, each training
    and test dataset will fit differently, resulting in high variance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有**低偏差**的机器学习算法必须足够灵活，以便它能很好地拟合数据。如果算法设计得太灵活，每个训练和测试数据集都会以不同的方式拟合，从而导致高方差。
- en: Your algorithm must be flexible enough to adjust this trade-off either by inherent
    algorithmic knowledge or a parameter which can be user adjusted.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你的算法必须足够灵活，可以通过固有的算法知识或用户可调整的参数来调整这种权衡。
- en: The following figure shows a simple model with high bias (to the left), and
    a more complex model with high variance (to the right).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一个具有高偏差（左侧）的简单模型和一个具有高方差（右侧）的更复杂模型。
- en: '![](img/5620b62b-2723-497e-a171-e372087c9cc3.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5620b62b-2723-497e-a171-e372087c9cc3.png)'
- en: Amount of training data
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据量
- en: As we have said repeatedly, there simply is no substitute for having enough
    data to get the job done correctly and completely. This directly correlates to
    the complexity of your learning algorithm. A less complex algorithm with high
    bias and low variance can learn better from a smaller amount of data. However,
    if your learning algorithm is complex (many input features, parameters, and so
    on), then you will need a much larger training set from which to learn from with
    low bias and high variance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们反复所说的，拥有足够的数据来完成工作，这是没有替代品的。这直接关联到你的学习算法的复杂性。一个复杂度较低、偏差高、方差低的算法可以从较少的数据中学习得更好。然而，如果你的学习算法复杂（许多输入特征、参数等），那么你需要一个更大的训练集，从中学习以获得低偏差和高方差。
- en: Input space dimensionality
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入空间维度
- en: With every learning problem our input is going to be in the form of a **vector**.
    The **feature vector**, meaning the features of the data itself, can affect the
    learning algorithm greatly. If the input feature vectors are very large, which
    is called high-dimensionality, then learning can be more difficult even if you
    only need just a few of those features. Sometimes, the extra dimensions confuse
    your learning algorithm, which results in high variance. This, in turn, means
    that you will have to tune your algorithm to have lower variance and higher bias.
    It is sometimes easier, if applicable, to remove the extra features from your
    data, thus improving your learning function accuracy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个学习问题，我们的输入都将以**向量**的形式存在。**特征向量**，即数据本身的特征，可以极大地影响学习算法。如果输入的特征向量非常大，这被称为高维性，那么即使你只需要其中的一小部分特征，学习也可能变得更加困难。有时，额外的维度会混淆你的学习算法，从而导致高方差。反过来，这意味着你将不得不调整你的算法以降低方差并提高偏差。如果适用，有时从你的数据中移除额外的特征会更容易，从而提高你的学习函数准确性。
- en: That being said, a popular technique known as **dimensionality reduction** is
    used by several machine learning algorithms. These algorithms will identify and
    remove irrelevant features.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，一种称为**降维**的流行技术被几个机器学习算法所使用。这些算法将识别并移除无关的特征。
- en: Incorrect output values
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不正确的输出值
- en: The question we ask ourselves here is how many errors exist in the desired output
    from our machine learning algorithm. If we experience this, the learning algorithm
    may be attempting to fit the data too well, resulting in something we mentioned
    previously, **over-fitting**. Over-fitting can result from incorrect data, or
    a learning algorithm which is too complex for the task at hand. If this happens,
    we need to either tune our algorithm or look for one which will provide us with
    higher bias and lower variance.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里问自己的问题是，我们的机器学习算法期望的输出中存在多少错误。如果我们遇到这种情况，学习算法可能正在尝试将数据拟合得太好，从而导致我们之前提到的问题，**过拟合**。过拟合可能源于错误的数据，或者对于当前任务来说过于复杂的学习算法。如果发生这种情况，我们需要调整我们的算法或者寻找一个能够提供更高偏差和更低方差的算法。
- en: Data heterogeneity
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据异质性
- en: 'Heterogeneity, according to Webster''s dictionary, means *the quality or state
    of consisting of dissimilar or diverse elements: the quality or state of being
    heterogeneous*. To us this means that the feature vectors include features of
    many different kinds. If this applies to our application, then it may be better
    for us to apply a different learning algorithm for the task. Some learning algorithms
    also require that our data is scaled to fit within certain ranges, such as *[0
    - 1], [-1 - 1]*, and so on. As we get into learning algorithms that utilize distance
    functions as their basis, such as nearest neighbor and support vector methods,
    you will see that they are exceptionally sensitive to this. On the other hand,
    algorithms such as those that are tree-based (decision trees, and so on) handle
    this phenomenon quite well.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 根据韦伯斯特词典，异质性意味着*由不同或多样化的元素组成的质量或状态：异质性的质量或状态*。对我们来说，这意味着特征向量包括许多不同种类的特征。如果这适用于我们的应用，那么我们可能需要为该任务应用不同的学习算法。一些学习算法还要求我们的数据被缩放到适合某些范围，例如
    *[0 - 1]，[-1 - 1]* 等。当我们深入研究利用距离函数作为其基础的学习算法时，例如最近邻和支持向量方法，你会看到它们对此非常敏感。另一方面，像基于树的算法（决策树等）处理这种现象相当好。
- en: We will end this discussion by saying that we should always start with the least
    complex, and most appropriate algorithm, and ensure our data is collected and
    prepared correctly. From there, we can always experiment with different learning
    algorithms and tune them to see which one works best for our situation. Make no
    mistake, tuning algorithms may not be a simple task, and in the end, consumes
    a lot more time than we have available. Always ensure the appropriate amount of
    data is available first!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以这句话结束这次讨论：我们应该始终从最简单、最合适的算法开始，并确保我们的数据被正确收集和准备。从那里，我们总是可以尝试不同的学习算法，并调整它们以查看哪个最适合我们的情况。不要误解，调整算法可能不是一项简单的任务，最终，它消耗的时间可能比我们可用的还要多。始终确保首先有适当数量的数据！
- en: Unsupervised learning
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: 'Contrary to supervised learning, unsupervised usually has more leeway in how
    the outcome is determined. The data is treated such that, to the algorithm, there
    is no single feature more important than any other in the dataset. These algorithms
    learn from datasets of input data without the expected output data being labeled.
    k-means clustering (cluster analysis) is an example of an unsupervised model.
    It is very good at finding patterns in the data that have meaning relative to
    the input data. The big difference between what we learned in the supervised section
    and here is that we now have *x* features *X[1], X[2], X[3], ... X[x]* measured
    on *n* observations. But we no longer interested in prediction of *Y* because
    we no longer have *Y*. Our only interest is to discover data patterns over the
    features that we have:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习相反，无监督学习在确定结果方面通常有更多的灵活性。数据被处理成，对于算法来说，数据集中没有哪个特征比其他特征更重要。这些算法从没有预期输出数据的标签的输入数据集中学习。k-means聚类（聚类分析）是一个无监督模型的例子。它非常擅长在数据中找到有意义的模式，这些模式与输入数据相关。我们在这里学到的与监督部分学到的最大区别是，我们现在有*x*个特征*X[1]，X[2]，X[3]，...
    X[x]*在*n*个观察上进行了测量。但我们不再对*Y*的预测感兴趣，因为我们不再有*Y*。我们唯一的兴趣是发现我们拥有的特征上的数据模式：
- en: '![](img/439c1ef0-6934-4d70-bfb0-16e052c5bdc7.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/439c1ef0-6934-4d70-bfb0-16e052c5bdc7.png)'
- en: In the previous diagram, you can see that data such as this lends itself much
    more to a non-linear approach, where the data appears to be in clusters relative
    to importance. It is **non-linear** because there is no way we will get a straight
    line to accurately separate and categorize the data. Unsupervised learning allows
    us to approach a problem with little to no idea what the results will, or should,
    look like. Structure is derived from the data itself versus supervised rules applied
    to output labels. This structure is usually derived by **clustering** relationships
    of data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的图中，你可以看到像这样的数据更适合采用非线性方法，其中数据似乎相对于重要性呈现出聚类状态。它是**非线性**的，因为我们无法得到一条直线来准确地区分和分类数据。无监督学习允许我们用一个几乎没有任何关于结果会是什么样或应该是什么样的想法来处理问题。结构是从数据本身中得出的，而不是应用在输出标签上的监督规则。这种结构通常是通过**聚类**数据之间的关系来得出的。
- en: For example, let's say we have 10⁸ genes from our genomic data science experiment.
    We would like to group this data into similar segments, such as hair color, lifespan,
    weight, and so on.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们从我们的基因组数据科学实验中有10⁸个基因。我们希望将这些数据分组到相似的片段中，比如发色、寿命、体重等等。
- en: The second example is what is famously known as the **cocktail party effect[3]**,
    which basically refers to the brains auditory ability to focus attention to one
    thing and filter out the **noise** around it.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个例子是广为人知的**鸡尾酒会效应[3]**，它基本上指的是大脑的听觉能力能够集中注意力在一件事上，并过滤掉它周围的**噪音**。
- en: Both examples can use clustering to accomplish their goals.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个例子都可以使用聚类来实现它们的目标。
- en: Reinforcement learning
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Reinforcement learning is a case where the machine is trained for a specific
    outcome with the sole purpose of maximizing efficiency and/or performance. The
    algorithm is **rewarded** for making the correct decisions, and **penalized**
    for making incorrect ones. Continual training is used to constantly improve performance.
    The continual learning process means less human intervention. Markov models are
    an example of reinforcement learning, and self-driving autonomous automobiles
    are a great example of just such an application. It constantly interacts with
    its environments, watches for obstacles, speed limits, distance, pedestrians,
    and so on to (hopefully) make the correct decisions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一个案例，其中机器被训练以实现特定的结果，唯一目的是最大化效率和/或性能。算法因做出正确的决策而**奖励**，因做出错误的决策而**惩罚**。持续训练用于不断改进性能。持续学习过程意味着更少的人为干预。马尔可夫模型是强化学习的一个例子，自动驾驶的自主汽车是这样一个应用的绝佳例子。它不断地与它的环境互动，观察障碍物、速度限制、距离、行人等等，以（希望）做出正确的决策。
- en: Our biggest difference with reinforcement learning is that we do not deal with
    correct input and output data. The focus here is on performance, meaning somehow
    finding a balance between unseen data and what the algorithms have already learned.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与强化学习最大的不同之处在于我们不处理正确的输入和输出数据。这里的重点是性能，意味着以某种方式在未见数据与算法已经学习到的内容之间找到平衡。
- en: The algorithm applies an action to its environment, receives a reward or a penalty
    based upon what it has done, repeats, and so on as shown in the following image.
    You can just imagine how many times per second this is happening in that nice
    little autonomous taxi that just picked you up at the hotel.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法对其环境执行操作，根据其行为获得奖励或惩罚，然后重复，如下面的图像所示。你可以想象一下，在那个可爱的小自动驾驶出租车把你从酒店接走的时候，每秒钟会发生多少次这样的操作。
- en: '![](img/d297881b-a00d-4759-ac14-596d9cae4454.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d297881b-a00d-4759-ac14-596d9cae4454.png)'
- en: Build, buy, or open source
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建、购买或开源
- en: Next, let's ask ourselves the ever-important question? Buy, build or open source?
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们自问一个始终重要的问题？购买、构建还是开源？
- en: It would be my recommendation, and of course one reason why I'm writing this
    book, to expose yourself to the open source world. I realize that many developers
    suffer from the '*it's not built here'* syndrome, but we should really be honest
    with ourselves before going down that path. Do we really think we have the expertise
    to do better, faster, and have it tested within our time constraints, compared
    to what is already out there? We should first try and see what is already out
    there that we can use. There are so many fabulous open source toolkits for us
    to use, and the developers of those have put tremendous amounts of hours and work
    into developing and testing them. Obviously open source is not a solution for
    everyone, every time, but even if you cannot use it in your application, there
    certainly is tremendous knowledge you can gain by using and experimenting with
    them.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是我的一些建议，当然也是我写这本书的原因之一，那就是让你接触开源世界。我意识到许多开发者都患有“这不是在这里建造的”综合症，但在我们走这条路之前，我们应该真的对自己诚实。我们真的认为我们有专业知识做得更好、更快，并且在我们的时间限制内进行测试，与已经存在的相比吗？我们应该首先尝试看看我们能否使用已经存在的。有如此多的开源工具包供我们使用，那些工具包的开发者已经投入了大量的时间和精力来开发和测试它们。显然，开源并不是每次都对每个人都是解决方案，但即使你无法在你的应用程序中使用它，通过使用和实验它们，你肯定可以从中获得大量的知识。
- en: Buying usually isn't an option. If you're lucky enough to find something to
    purchase, you probably won't get the approval as it will cost a pretty penny!
    And what happens if you need to modify the product to do something you need? Good
    luck getting access to the source or having the support team change their priorities
    just for you. Not going to happen, at least not as fast as we'll probably need
    it to!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 购买通常不是一种选择。如果你足够幸运能找到可以购买的东西，你可能不会得到批准，因为它会花费一大笔钱！如果你需要修改产品以完成你需要的事情，会发生什么？好运，得到访问源代码或让支持团队为了你改变他们的优先级。这不可能发生，至少不会像我们可能需要的那么快！
- en: And as for building it yourself, hey we're developers, it's what we all want
    to do, right? But before you fire up Visual Studio and take off, think long and
    hard about what you are getting into.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 至于自己构建，嘿，我们是开发者，这是我们所有人都想做的事情，对吧？但在你启动Visual Studio并起飞之前，仔细思考一下你将要进入的是什么。
- en: So open source should always be a first choice. You can bring it in house (assuming
    licensing allows you), adapt it to your standards if need be (code contacts, more
    unit tests, better documentation, and so on).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，开源应该始终是首选。你可以将其引入内部（假设许可允许你这样做），如果需要的话，适应你的标准（代码联系、更多单元测试、更好的文档等）。
- en: Additional reading
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外阅读
- en: Although the code is in Python and R, I encourage those interested in expanding
    upon what we have talked about in this chapter to visit Jason Brownlee's site,
    [https://machinelearningmastery.com/](https://machinelearningmastery.com/). The
    explanations and passion about machine learning are second to none and there is
    an incredible amount of information you can gain from his site. The explanations
    are clear, passionate and cover an incredible amount of depth. I highly recommend
    perusing his blog and site to learn as much as you can.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码是用Python和R编写的，但我鼓励那些对在本章中讨论的内容进行扩展感兴趣的人访问Jason Brownlee的网站，[https://machinelearningmastery.com/](https://machinelearningmastery.com/)。他对机器学习的解释和热情无与伦比，你可以从他的网站上获得难以置信的大量信息。解释清晰、充满激情，涵盖了难以置信的深度。我强烈推荐浏览他的博客和网站，尽可能多地学习。
- en: Summary
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed many aspects of machine learning with C#, different
    strategies for implementing your code—such as build, buy, or open source—as well
    as lightly touch upon some important definitions. I hope this got you ready for
    the chapters to come.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了使用C#进行机器学习的许多方面，不同的代码实现策略——如构建、购买或开源——以及简要地触及一些重要定义。我希望这让你为即将到来的章节做好了准备。
- en: 'Before we dive right into our source code and applications, I want to take
    some time to discuss with you something that is very near and dear to my heart:
    logging. It''s something that we all do (or should do), and there is a phenomenal
    tool out there that you need to know about if you do not already. We''ll be using
    it quite a bit in this book, so it''s definitely helpful to spend some time on
    it up front, starting in the next chapter.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入到源代码和应用之前，我想花些时间与你们讨论一下对我来说非常亲近和重要的事情：日志记录。这是我们每个人（或者应该做）的事情，而且如果你还不知道的话，这里有一个非常出色的工具你需要了解。在这本书中，我们将大量使用它，所以花些时间在前面了解它肯定是有帮助的，从下一章开始。
- en: References
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: By Nicoguaro - Own work, CC BY 4.0, [https://commons.wikimedia.org/w/index.php?curid=46257808](https://commons.wikimedia.org/w/index.php?curid=46257808)
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: By Nicoguaro - Own work, CC BY 4.0, [https://commons.wikimedia.org/w/index.php?curid=46257808](https://commons.wikimedia.org/w/index.php?curid=46257808)
- en: Creative Commons Attribution-ShareAlike 3.0 Unported
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创用CC署名-相同方式共享3.0未本地化版本
- en: '[https://en.wikipedia.org/wiki/Cocktail_party_effect](https://en.wikipedia.org/wiki/Cocktail_party_effect)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://en.wikipedia.org/wiki/Cocktail_party_effect](https://en.wikipedia.org/wiki/Cocktail_party_effect)'
- en: Encog framework is copyright of Jeff Heaton/Heaton research
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Encog框架版权属于Jeff Heaton/Heaton研究
