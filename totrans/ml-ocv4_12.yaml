- en: Using Deep Learning to Classify Handwritten Digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now return to supervised learning and discuss a family of algorithms known
    as **artificial neural networks**. Early studies of neural networks go back to
    the 1940s when Warren McCulloch and Walter Pitts first described how biological
    nerve cells (or neurons) in the brain might work. More recently, artificial neural
    networks have seen a revival under the buzzword deep learning, which powers state-of-the-art
    technologies such as Google's DeepMind and Facebook's DeepFace algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we want to wrap our heads around some simple versions of artificial
    neural networks, such as the McCulloch-Pitts neuron, the perceptron, and the multilayer
    perceptron. Once we have familiarized ourselves with the basics, we will be ready
    to implement a more sophisticated deep neural network to classify handwritten
    digits from the popular **MNIST database** (short for **Mixed National Institute
    of Standards and Technology database**). For this, we will be making use of Keras,
    a high-level neural network library, which is also frequently used by researchers
    and tech companies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along the way, we will address the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing perceptrons and multilayer perceptrons in OpenCV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differentiating stochastic and batch gradient descent, and how they fit in with
    backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the size of your neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Keras to build sophisticated deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Excited? Then let's go!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can refer the code for this chapter at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter09](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a short summary of the software and hardware requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python version 3.6 (any Python version 3.x will be fine).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anaconda Python 3 for installing Python and the required modules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use any operating system—macOS, Windows, or Linux-based—with this book.
    We recommend you have at least 4 GB RAM in your system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don't need to have a GPU to run the code provided with the book.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the McCulloch-Pitts neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 1943, Warren McCulloch and Walter Pitts published a mathematical description
    of neurons as they were believed to operate in the brain. A neuron receives input
    from other neurons through connections on its dendritic tree, which are integrated
    to produce an output at the cell body (or soma). The output is then communicated
    to other neurons via a long wire (or axon), which eventually branches out to make
    one or more connections (at axon terminals) on the dendritic tree of other neurons.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example neuron is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6e7d5f2-dde4-47cb-9ba1-dc48d0346dbd.png)'
  prefs: []
  type: TYPE_IMG
- en: McCulloch and Pitts described the inner workings of such a neuron as a simple
    logic gate that would be either on or off, depending on the input it received
    on its dendritic tree. Specifically, the neuron would sum up all of its inputs,
    and if the sum exceeded a certain threshold, an output signal would be generated
    and passed on by the axon.
  prefs: []
  type: TYPE_NORMAL
- en: However, today we know that real neurons are much more complicated than that.
    Biological neurons perform intricate nonlinear mathematical operations on thousands
    of inputs and can change their responsiveness dynamically depending on the context,
    importance, or novelty of the input signal. You can think of real neurons being
    as complex as computers and of the human brain being as complex as the internet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a simple artificial neuron that receives exactly two inputs,
    *x[0]* and *x[1]*. The job of the artificial neuron is to calculate a sum of the
    two inputs (usually in the form of a weighted sum), and if this sum exceeds a
    certain threshold (often zero), the neuron will be considered active and output
    a one; else it will be considered silent and output a minus one (or zero). In
    more mathematical terms, the output, *y*, of this McCulloch-Pitts neuron can be
    described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77e7cc8a-4619-4549-9be5-d20576ca9175.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, *w[0]* and *w[1]* are weight coefficients, which,
    together with *x[0]* and *x[1]*, make up the weighted sum. In textbooks, the two
    different scenarios where the output, *y*, is either *+1* and *-1* would often
    be masked by an activation function, *ϕ*, which could take on two different values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d494b0cc-0876-44a1-abf7-6dd5029c2724.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we introduce a new variable, *z* (the so-called **network input**), which
    is equivalent to the weighted sum: *z = w[0]x[0] + w[1]x[1]*. The weighted sum
    is then compared to a threshold, *θ*, to determine the value of *ϕ* and subsequently
    the value of *y*. Apart from that, these two equations say exactly the same thing
    as the preceding one.'
  prefs: []
  type: TYPE_NORMAL
- en: If these equations look strangely familiar, you might be reminded of [Chapter
    1](7ebeef44-67a4-4843-83e0-bd644fc19eea.xhtml), *A Taste of Machine Learning*,
    when we were talking about linear classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: And you are right, a McCulloch-Pitts neuron is essentially a linear, binary
    classifier!
  prefs: []
  type: TYPE_NORMAL
- en: 'You can think of it this way: *x[0]* and *x[1]* are the input features, *w[0]*
    and *w[1]* are weights to be learned, and the classification is performed by the
    activation function, *ϕ.* If we do a good job of learning the weights, which we
    would do with the help of a suitable training set, we could classify data as positive
    or negative samples. In this scenario, *ϕ(z)=θ* would act as the decision boundary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This might all make more sense with the help of the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/052ff7aa-3830-49eb-8b74-a7c2b22b90fd.png)'
  prefs: []
  type: TYPE_IMG
- en: On the left, you can see the neuron's activation function, *ϕ*, plotted against
    *z*. Remember that *z* is nothing more than the weighted sum of the two inputs
    *x[0]* and *x[1]*[.] The rule is that as long as the weighted sum is below some
    threshold, *θ*, the output of the neuron is -1; above *θ*, the output is +1.
  prefs: []
  type: TYPE_NORMAL
- en: On the right, you can see the decision boundary denoted by *ϕ(z)=θ*, which splits
    the data into two regimes, *ϕ(z)<θ* (where all data points are predicted to be
    negative samples) and *ϕ(z)>θ* (where all data points are predicted to be positive
    samples).
  prefs: []
  type: TYPE_NORMAL
- en: The decision boundary does not need to be vertical or horizontal, it can be
    tilted as shown in the preceding diagram. But in the case of a single McCulloch-Pitts
    neuron, the decision boundary will always be a straight line.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the magic lies with learning the weight coefficients, *w[0]* and
    *w[1]*, such that the decision boundary comes to lie right between all positive
    and all negative data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train a neural network, we generally need three things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training data**: It is no surprise to learn that we need some data samples
    with which the effectiveness of our classifier can be verified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost function (also known as loss function)**: A cost function provides a
    measure of how good the current weight coefficients are. There is a wide range
    of cost functions available, which we will talk about toward the end of this chapter.
    One solution is to count the number of misclassifications. Another one is to calculate
    the **sum of squared errors**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rule**: A learning rule specifies mathematically how we have to
    update the weight coefficients from one iteration to the next. This learning rule
    usually depends on the error (measured by the cost function) we observed on the
    training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is where the work of renowned researcher Frank Rosenblatt comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the 1950s, American psychologist and artificial intelligence researcher
    Frank Rosenblatt invented an algorithm that would automatically learn the optimal
    weight coefficients *w[0]* and *w[1]* needed to perform an accurate binary classification:
    the perceptron learning rule.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rosenblatt''s original perceptron algorithm can be summed up as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weights to zero or some small random numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each training sample, *s[i]*, perform the following steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the predicted target value, *ŷ**[i].*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compare *ŷ**[i]* to the ground truth, *y**[i]*, and update the weights accordingly:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the two are the same (correct prediction), skip ahead.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If the two are different (wrong prediction), push the weight coefficients, *w[0]*
    and
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing your first perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Perceptrons are easy enough to be implemented from scratch. We can mimic the
    typical OpenCV or scikit-learn implementation of a classifier by creating a perceptron
    object. This will allow us to initialize new perceptron objects that can learn
    from data via a `fit` method and make predictions via a separate `predict` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we initialize a new perceptron object, we want to pass a learning rate
    (`lr`, or *η* in the previous section) and the number of iterations after which
    the algorithm should terminate (`n_iter`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fit` method is where most of the work is done. This method should take
    as input some data samples (`X`) and their associated target labels (`y`). We
    will then create an array of weights (`self.weights`), one for each feature (`X.shape[1]`),
    initialized to zero. For convenience, we will keep the bias term (`self.bias`)
    separate from the weight vector and initialize it to zero as well. One of the
    reasons for initializing the bias to zero is because the small random numbers
    in the weights provide asymmetry breaking in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The `predict` method should take in a number of data samples (`X`) and, for
    each of them, return a target label, either +1 or -1\. In order to perform this
    classification, we need to implement *ϕ(z)>θ*. Here we will choose *θ = 0*, and
    the weighted sum can be computed with NumPy''s dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will calculate the Δ*w* terms for every data sample (`xi`, `yi`) in
    the dataset and repeat this step for a number of iterations (`self.n_iter`). For
    this, we need to compare the ground-truth label (`yi`) to the predicted label
    (aforementioned `self.predict(xi)`). The resulting delta term will be used to
    update both the weights and the bias term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: That's it!
  prefs: []
  type: TYPE_NORMAL
- en: Generating a toy dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will learn how to create and plot a toy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To test our perceptron classifier, we need to create some mock data. Let''s
    keep things simple for now and generate 100 data samples (`n_samples`) belonging
    to one of two blobs (`centers`), again relying on scikit-learn''s `make_blobs`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'One thing to keep in mind is that our perceptron classifier expects target
    labels to be either +1 or -1, whereas `make_blobs` returns `0` and `1`. An easy
    way to adjust the labels is with the following equation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the following code, we ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fitting the perceptron to data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will learn to fit a perceptron algorithm on the
    given data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can instantiate our perceptron object similar to other classifiers we encountered
    with OpenCV:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we chose a learning rate of 0.1 and told the perceptron to terminate after
    10 iterations. These values are chosen rather arbitrarily at this point, although
    we will come back to them in a little while.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an appropriate learning rate is critical, but it's not always clear
    what the most appropriate choice is. The learning rate determines how quickly
    or slowly we move toward the optimal weight coefficients. If the learning rate
    is too large, we might accidentally skip the optimal solution. If it is too small,
    we will need a large number of iterations to converge to the best values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the perceptron is set up, we can call the `fit` method to optimize the
    weight coefficients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Did it work? Let''s have a look at the learned weight values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And don''t forget to have a peek at the bias term:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If we plug these values into our equation for *ϕ*, it becomes clear that the
    perceptron learned a decision boundary of the form *2.2 x[1] - 0.48 x[2] + 0.2
    >= 0*.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the perceptron classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will be evaluating the trained perceptron on the
    test data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to find out how good our perceptron performs, we can calculate the
    accuracy score on all data samples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Perfect score!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the decision landscape by bringing back our `plot_decision_boundary`
    from the earlier chapters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Applying the perceptron to data that is not linearly separable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will learn to build a perceptron to separate a
    nonlinear data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the perceptron is a linear classifier, you can imagine that it would
    have trouble trying to classify data that is not linearly separable. We can test
    this by increasing the spread (`cluster_std`) of the two blobs in our toy dataset
    so that the two blobs start overlapping:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can plot the dataset again using matplotlib''s `scatter` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As is evident in the following screenshot, this data is no longer linearly
    separable because there is no straight line that perfectly separates the two blobs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a9a9a8d-1f6a-4fc8-992f-bb38f1192b49.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot shows an example of data that is not linearly separable.
    So what would happen if we applied the perceptron classifier to this dataset?
  prefs: []
  type: TYPE_NORMAL
- en: 'We can find an answer to this question by repeating the preceding steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we find an accuracy score of 81%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to find out which data points were misclassified, we can again visualize
    the decision landscape using our helper function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph makes the limitations of the perceptron classifier evident.
    Being a linear classifier, it tried to separate the data using a straight line
    but ultimately failed. The main reason it failed is because the data was not linearly
    separable even though we achieved 81% accuracy. However, from the following plot,
    it is clear that many of the red dots lie in the blue region and vice versa. So,
    unlike a perceptron, we need a nonlinear algorithm that can create not a straight
    but a nonlinear (circular) decision boundary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21aa1c5d-691d-412d-af9a-14d86495a613.png)'
  prefs: []
  type: TYPE_IMG
- en: Fortunately, there are ways to make the perceptron more powerful and ultimately
    create nonlinear decision boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multilayer perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to create nonlinear decision boundaries, we can combine multiple perceptrons
    to form a larger network. This is also known as a **multilayer perceptron** (**MLP**).
    MLPs usually consist of at least three layers, where the first layer has a node
    (or neuron) for every input feature of the dataset, and the last layer has a node
    for every class label. The layer in between is called the **hidden layer**.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of this feedforward neural network architecture is shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d824c0c2-9d1f-4ccd-a5ba-ac2010eee788.png)'
  prefs: []
  type: TYPE_IMG
- en: In this network, every circle is an artificial neuron (or, essentially, a perceptron),
    and the output of one artificial ...
  prefs: []
  type: TYPE_NORMAL
- en: Understanding gradient descent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we talked about the perceptron earlier in this chapter, we identified
    three of the essential ingredients needed for training: training data, a cost
    function, and a learning rule. While the learning rule worked great for a single
    perceptron, unfortunately, it did not generalize to MLPs, so people had to come
    up with a more general rule.'
  prefs: []
  type: TYPE_NORMAL
- en: If you think about how we measure the success of a classifier, we usually do
    so with the help of a cost function. A typical example is the number of misclassifications
    of the network or the mean squared error. This function (also known as a **loss
    function**) usually depends on the parameters we are trying to tweak. In neural
    networks, these parameters are the weight coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume a simple neural network has a single weight to tweak, *w*. Then
    we can visualize the cost as a function of the weight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f8fb5ba-35a2-48e9-8bed-606c50106a8a.png)'
  prefs: []
  type: TYPE_IMG
- en: At the beginning of training, at time zero, we may start out way on the left
    of this graph (*w[t=0]*). But from the graph, we know that there would be a better
    value for *w*, namely *w[optimal]*, which would minimize the cost function. The
    smallest cost means the lowest error, so it should be our highest goal to reach
    *w[optimal]* through learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is exactly what gradient descent does. You can think of the gradient as
    a vector that points up the hill. In gradient descent, we are trying to walk opposite
    of the gradient, effectively walking down the hill, from the peaks down to the
    valley:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccc693b1-b01c-46f0-869f-565c6304fe32.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you reach the valley, the gradient goes to zero, and that completes the
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to reach the valley—we could approach from the left,
    or we could approach from the right. The starting point of our descent is determined
    by the initial weight values. Furthermore, we have to be careful not to take too
    large a step, otherwise we might miss the valley:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc30f57b-3402-41f5-ba64-e7404f9072fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, in stochastic gradient descent (sometimes also called iterative or on-line
    gradient descent), the goal is to take small steps but to take them as often as
    possible. The effective step size is determined by the learning rate of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we would perform the following procedure over and over:'
  prefs: []
  type: TYPE_NORMAL
- en: Present a small number of training samples to the network (called the **batch
    size**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On this small batch of data, calculate the gradient of the cost function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weight coefficients by taking a small step in the opposite direction
    of the gradient, toward the valley.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1-3 until the weight cost no longer goes down. This is an indication
    that we have reached the valley.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some other ways to improve SGD are using the learning rate finder in the Keras
    framework, decreasing the step size (learning rate) in epochs, and, as discussed
    in the preceding point, using a batch size (or mini-batch), which will compute
    the weight update faster.
  prefs: []
  type: TYPE_NORMAL
- en: Can you think of an example where this procedure might fail?
  prefs: []
  type: TYPE_NORMAL
- en: 'One scenario that comes to mind is where the cost function has multiple valleys,
    some deeper than others, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60373859-3362-45bc-b11b-031aeeb28fda.png)'
  prefs: []
  type: TYPE_IMG
- en: If we start on the left, we should arrive at the same valley as before—no problem.
    But, if our starting point is all the way to the right, we might encounter another
    valley on the way. Gradient descent will lead us straight down to the valley,
    but it will not have any means to climb out of it.
  prefs: []
  type: TYPE_NORMAL
- en: This is also known as **getting stuck in a local minimum**. Researchers have
    come up with different ways to try and avoid this issue, one of them being to
    add noise to the process.
  prefs: []
  type: TYPE_NORMAL
- en: There is one piece left in the puzzle. Given our current weight coefficients,
    how do we know the slope of the cost function?
  prefs: []
  type: TYPE_NORMAL
- en: Training MLPs with backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is where backpropagation comes in, which is an algorithm for estimating
    the gradient of the cost function in neural networks. Some might say that it is
    basically a fancy word for the chain rule, which is a means to calculate the partial
    derivative of functions that depend on more than one variable. Nonetheless, it
    is a method that helped bring the field of artificial neural networks back to
    life, so we should be thankful for that.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding backpropagation involves quite a bit of calculus, so I will only
    give you a brief introduction here.
  prefs: []
  type: TYPE_NORMAL
- en: Let's remind ourselves that the cost function, and therefore its gradient, depends
    on the difference between the true output (*y[i]*) and the current output (*ŷ[i]*
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a MLP in OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementing an MLP in OpenCV uses the same syntax that we have seen at least
    a dozen times before. In order to see how an MLP compares to a single perceptron,
    we will operate on the same toy data as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Preprocessing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'However, since we are working with OpenCV, this time we want to make sure the
    input matrix is made up of 32-bit floating point numbers, otherwise the code will
    break:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, we need to think back to [Chapter 4](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml),
    *Representing Data and Engineering Features*, and remember how to represent categorical
    variables. We need to find a way to represent target labels, not as integers but
    with a one-hot encoding. The easiest way to achieve this is by using scikit-learn''s
    `preprocessing` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Creating an MLP classifier in OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The syntax to create an MLP in OpenCV is the same as for all the other classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: However, now we need to specify how many layers we want in the network and how
    many neurons there are per layer. We do this with a list of integers, which specify
    the number of neurons in each layer. Since the data matrix `X` has two features,
    the first layer should also have two neurons in it (`n_input`). Since the output
    has two different values, the last layer should also have two neurons in it (`n_output`).
  prefs: []
  type: TYPE_NORMAL
- en: 'In between these two layers, we can put as many hidden layers with as many
    neurons as we want. Let''s choose a single hidden layer with an arbitrary number
    of 10 neurons in it (`n_hidden`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Customizing the MLP classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we move on to training the classifier, we can customize the MLP classifier
    via a number of optional settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '`mlp.setActivationFunction`: This defines the activation function to be used
    for every neuron in the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp.setTrainMethod`: This defines a suitable training method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlp.setTermCriteria`: This sets the termination criteria of the training phase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Whereas our home-brewed perceptron classifier used a linear activation function,
    OpenCV provides two additional options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cv2.ml.ANN_MLP_IDENTITY`: This is the linear activation function, *f(x) =
    x*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv2.ml.ANN_MLP_SIGMOID_SYM`: This is the symmetrical sigmoid function (also
    known as **hyperbolic tangent**), *f(x) = β (1 -* exp*(-α x)) / (1 +* exp*(-α
    x))*. Whereas ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and testing the MLP classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is the easy part. Training the MLP classifier is the same as with all
    other classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The same goes for predicting target labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The easiest way to measure accuracy is by using scikit-learn''s helper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks like we were able to increase our performance from 81% with a single
    perceptron to 88% with an MLP consisting of 10 hidden-layer neurons and 2 output
    neurons. In order to see what changed, we can look at the decision boundary one
    more time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'However, there is a problem right here, in that `zz` is now a one-hot encoded
    matrix. In order to transform the one-hot encoding into a number that corresponds
    to the class label (zero or one), we can use NumPy''s `argmax` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then the rest stays the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can call the function like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7430fc52-df88-490d-972d-4d56c4ee0eb3.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding output shows a decision boundary of an MLP with one hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: And voila! The decision boundary is no longer a straight line. That being said,
    you got a great performance increase and might have expected a more drastic performance
    increase. But nobody said we have to stop here!
  prefs: []
  type: TYPE_NORMAL
- en: 'There are at least two different things we can try from here on out:'
  prefs: []
  type: TYPE_NORMAL
- en: We can add more neurons to the hidden layer. You can do this by replacing `n_hidden`
    on line 6 with a larger value and running the code again. Generally speaking,
    the more neurons you put in the network, the more powerful the MLP will be.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can add more hidden layers. It turns out that this is where neural networks
    really get their power from.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hence, this is where I should tell you about deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Getting acquainted with deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back when deep learning didn't have a fancy name yet, it was called artificial
    neural networks. So you already know a great deal about it!
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, interest in neural networks was rekindled in 1986 when David Rumelhart,
    Geoffrey Hinton, and Ronald Williams were involved in the (re)discovery and popularization
    of the aforementioned backpropagation algorithm. However, it was not until recently
    that computers became powerful enough so they could actually execute the backpropagation
    algorithm on large-scale networks, leading to a surge in deep learning research.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more information on the history and origin of deep learning in
    the following scientific article: Wang and Raj (2017), *On the Origin ...*'
  prefs: []
  type: TYPE_NORMAL
- en: Getting acquainted with Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The core data structure of Keras is a model, which is similar to OpenCV''s
    classifier object, except it focuses on neural networks only. The simplest type
    of model is the sequential model, which arranges the different layers of the neural
    network in a linear stack, just like we did for the MLP in OpenCV:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, different layers can be added to the model one by one. In Keras, layers
    do not just contain neurons, they also perform a function. Some core layer types
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dense**: This is a densely connected layer. This is exactly what we used
    when we designed our MLP: a layer of neurons that is connected to every neuron
    in the previous layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Activation**: This applies an activation function to an output. Keras provides
    a whole range of activation functions, including OpenCV''s identify function (`linear`),
    the hyperbolic tangent (`tanh`), a sigmoidal squashing function (`sigmoid`), a
    softmax function (`softmax`), and many more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reshape**: This reshapes an output to a certain shape.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are other layers that calculate arithmetic or geometric operations on
    their inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional layers**: These layers allow you to specify a kernel with which
    the input layer is convolved. This allows you to perform operations such as a
    Sobel filter or apply a Gaussian kernel in 1D, 2D, or even 3D.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pooling layers**: These layers perform a max pooling operation on their input,
    where the output neuron''s activity is given by the maximally active input neuron.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some other layers that are popular in deep learning are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropout**: This layer randomly sets a fraction of input units to zero at
    each update. This is a way to inject noise into the training process, making it
    more robust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embeddin****g**: This layer encodes categorical data, similar to some functions
    from scikit-learn''s `preprocessing` module.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gaussian noise**: This layer applies additive zero-centered Gaussian noise.
    This is another way of injecting noise into the training process, making it more
    robust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A perceptron similar to the preceding one could thus be implemented using a
    dense layer that has two inputs and one output. Staying true to our earlier example,
    we will initialize the weights to zero and use the hyperbolic tangent as an activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we want to specify the training method. Keras provides a number of
    optimizers, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stochastic gradient descent (SGD)**: This is what we discussed earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Root mean square propagation (RMSprop)**: This is a method in which the learning
    rate is adapted for each of the parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive moment estimation (Adam)**: This is an update to the root mean square
    propagation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, Keras also provides a number of different loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean squared error (mean_squared_error)**: This is what was discussed earlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hinge loss (hinge)**: This is a maximum-margin classifier often used with
    SVM, as discussed in [Chapter 6](419719a8-3340-483a-86be-1d9b94f4a682.xhtml),
    *Detecting Pedestrians with Support Vector Machines*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can see that there''s a plethora of parameters to be specified and methods
    to choose from. To stay true to our aforementioned perceptron implementation,
    we will choose SGD as an optimizer, the mean squared error as a cost function,
    and accuracy as a scoring function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to compare the performance of the Keras implementation to our home-brewed
    version, we will apply the classifier to the same dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, a Keras model is fit to the data with a very familiar syntax. Here,
    we can also choose how many iterations to train for (`epochs`), how many samples
    to present before we calculate the error gradient (`batch_size`), whether to shuffle
    the dataset (`shuffle`), and whether to output progress updates (`verbose`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'After the training completes, we can evaluate the classifier as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Here, the first reported value is the mean squared error, whereas the second
    value denotes accuracy. This means that the final mean squared error was 0.04,
    and we had 100% accuracy. Way better than our own implementation!
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information on Keras, source code documentation, and a number
    of tutorials at [http://keras.io](http://keras.io).
  prefs: []
  type: TYPE_NORMAL
- en: With these tools in hand, we are now ready to approach a real-world dataset!
  prefs: []
  type: TYPE_NORMAL
- en: Classifying handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we covered a lot of the theory around neural networks,
    which can be a little overwhelming if you are new to this topic. In this section,
    we will use the famous MNIST dataset, which contains 60,000 samples of handwritten
    digits along with their labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will train two different networks on it:'
  prefs: []
  type: TYPE_NORMAL
- en: An MLP using OpenCV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A deep neural network using Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The easiest way to obtain the MNIST dataset is by using Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This will download the data from Amazon Cloud (might take a while depending
    on your internet connection) and automatically split the data into training and
    test sets.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST provides its own predefined train-test split. This way, it is easier to
    compare the performance of different classifiers because they will all use the
    same data for training and the same data for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This data comes in a format that we are already familiar with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We should take note that the labels come as integer values between zero and
    nine (corresponding to the digits 0-9):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can have a look at some example digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The digits look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbf6668b-2d3f-4188-99c7-6363e034ca9f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In fact, the MNIST dataset is the successor to the NIST digits dataset provided
    by scikit-learn that we used before (`sklearn.datasets.load_digits`; refer to
    [Chapter 2](8b9a6f9b-32b4-4af5-9a8c-4e121341f292.xhtml), *Working with Data in
    OpenCV*). Some notable differences are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: MNIST images are significantly larger (28 x 28 pixels) than NIST images (8 x
    8 pixels), thus paying more attention to fine details such as distortions and
    individual differences between images of the same digit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MNIST dataset is much larger than the NIST dataset, providing 60,000 training
    and 10,000 test samples (as compared to a total of 5,620 NIST images).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we learned in [Chapter 4](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml), *Representing
    Data and Engineering Features*, there are a number of preprocessing steps we might
    like to apply here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Centering**: It is important that all the digits are centered in the image.
    For example, take a look at all the example images of the digit 1 in the preceding
    diagram, which are all made of an almost-vertical strike. If the images were misaligned,
    the strike could lie anywhere in the image, making it hard for the neural network
    to find commonalities in the training samples. Fortunately, images in MNIST are
    already centered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling**: The same is true for scaling the digits so that they all have
    the same size. This way, the location of strikes, curves, and loops are important.
    ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an MLP using OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can set up and train an MLP in OpenCV with the following recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a new MLP object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the size of every layer in the network. We are free to add as many
    layers as we want, but we need to make sure that the first layer has the same
    number of neurons as input features (`784` in our case), and that the last layer
    has the same number of neurons as class labels (`10` in our case), while there
    are two hidden layers each having `512` nodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify an activation function. Here we use the sigmoidal activation function
    from before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the training method. Here, we use the backpropagation algorithm described
    earlier. We also need to make sure that we choose a small enough learning rate.
    Since we have on the order of 10⁵ training samples, it is a good idea to set the
    learning rate to at most 10^(-5):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the termination criteria. Here, we use the same criteria as before:
    to run training for 10 iterations (`term_max_iter`) or until the error does no
    longer increase significantly (`term_eps`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the network on the training set (`X_train_pre`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Before you call `mlp.train`, here is a word of caution: this might take several
    hours to run, depending on your computer setup! For comparison, it took just under
    an hour on my own laptop. We are now dealing with a real-world dataset of 60,000
    samples: if we run 100 training epochs, we have to compute 6 million gradients!
    So beware.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the training completes, we can calculate the accuracy score on the training
    set to see how far we got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'But, of course, what really counts is the accuracy score we get on the held-out
    test data which was not taken into account in training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 91.7% accuracy is not bad at all if you ask me! The first thing you should try
    is to change the layer sizes in the preceding `In [10]` and see how the test score
    changes. As you add more neurons to the network, you should see the training score
    increase—and with it, hopefully, the test score. However, having *N* neurons in
    a single layer is not the same as having them spread out over several layers!
    Can you confirm this observation?
  prefs: []
  type: TYPE_NORMAL
- en: Training a deep neural network using Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we achieved a formidable score with the preceding MLP, our result does
    not hold up to state-of-the-art results. Currently, the best result has close
    to 99.8% accuracy—better than human performance! This is why, nowadays, the task
    of classifying handwritten digits is largely regarded as solved.
  prefs: []
  type: TYPE_NORMAL
- en: To get closer to the state-of-the-art results, we need to use state-of-the-art
    techniques. Thus, we return to Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing the MNIST dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will learn to preprocess the data before it is
    fed to the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure we get the same result every time we run the experiment, we will
    pick a random seed for NumPy''s random number generator. This way, shuffling the
    training samples from the MNIST dataset will always result in the same order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Keras provides a loading function similar to `train_test_split` from scikit-learn''s
    `model_selection` module. Its syntax might look strangely familiar to you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: In contrast to other datasets we have encountered so far, MNIST comes with a
    predefined train-test split. This allows the dataset to be used as a benchmark,
    as the test score reported by different algorithms will always apply to the same
    test samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The neural networks in Keras act on the feature matrix slightly differently
    than the standard OpenCV and scikit-learn estimators. Whereas the rows of a feature
    matrix in Keras still correspond to the number of samples (`X_train.shape[0]`
    in the following code), we can preserve the two-dimensional nature of the input
    images by adding more dimensions to the feature matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we have reshaped the feature matrix into a four-dimensional matrix with
    dimensions `n_features` x 28 x 28 x 1\. We also need to make sure we operate on
    32-bit floating point numbers between [0, 1], rather than unsigned integers in
    [0, 255]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can one-hot encode the training labels as we did before. This will
    make sure each category of target labels can be assigned to a neuron in the output
    layer. We could do this with scikit-learn''s `preprocessing`, but in this case,
    it is easier to use Keras'' own utility function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Creating a convolutional neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following steps, you will create a neural network and train on the data
    you preprocessed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have preprocessed the data, it is time to define the actual model.
    Here, we will once again rely on the `Sequential` model to define a feedforward
    neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'However, this time, we will be smarter about the individual layers. We will
    design our neural network around a convolutional layer, where the kernel is a
    3 x 3-pixel two-dimensional convolution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Model summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also visualize the model''s summary, which will list all the layers
    along with their respective dimensions and the number of weights each layer consists.
    It will also provide you with information about the total number of parameters
    (weights and biases) in your network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d014b7f0-d32b-4887-a660-1828cfc3e1cc.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see that there are in total 600,810 parameters that will be trained and
    will require a good amount of computation power! Please note that how we calculate
    the number of parameters in each layer is out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We fit the model as we do with all other classifiers (caution, this might take
    a while):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'After training completes, we can evaluate the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: And we achieved 99% accuracy! This is worlds apart from the MLP classifier we
    implemented before. And this is just one way to do things. As you can see, neural
    networks provide a plethora of tuning parameters, and it is not at all clear which
    ones will lead to the best performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we added a whole bunch of skills to our list as a machine
    learning practitioner. Not only did we cover the basics of artificial neural networks,
    including perceptrons and MLPs, we also got our hands on some advanced deep learning
    software. We learned how to build a simple perceptron from scratch and how to
    build state-of-the-art networks using Keras. Furthermore, we learned about all
    the details of neural nets: activation functions, loss functions, layer types,
    and training methods. All in all, this was probably the most intensive chapter
    yet.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know about most of the essential supervised learners, it is time
    to talk about how to combine different algorithms into a more powerful one. Thus,
    in the next chapter, we will talk about how to build ensemble classifiers.
  prefs: []
  type: TYPE_NORMAL
