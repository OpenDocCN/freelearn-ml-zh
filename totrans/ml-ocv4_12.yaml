- en: Using Deep Learning to Classify Handwritten Digits
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用深度学习来分类手写数字
- en: Let's now return to supervised learning and discuss a family of algorithms known
    as **artificial neural networks**. Early studies of neural networks go back to
    the 1940s when Warren McCulloch and Walter Pitts first described how biological
    nerve cells (or neurons) in the brain might work. More recently, artificial neural
    networks have seen a revival under the buzzword deep learning, which powers state-of-the-art
    technologies such as Google's DeepMind and Facebook's DeepFace algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到监督学习，并讨论一组被称为**人工神经网络**的算法。神经网络早期的研究可以追溯到20世纪40年代，当时沃伦·麦克洛奇（Warren McCulloch）和沃尔特·皮茨（Walter
    Pitts）首次描述了大脑中生物神经细胞（或神经元）可能的工作方式。最近，在深度学习的热潮下，人工神经网络得到了复兴，这推动了诸如谷歌的DeepMind和Facebook的DeepFace算法等最先进的技术。
- en: In this chapter, we want to wrap our heads around some simple versions of artificial
    neural networks, such as the McCulloch-Pitts neuron, the perceptron, and the multilayer
    perceptron. Once we have familiarized ourselves with the basics, we will be ready
    to implement a more sophisticated deep neural network to classify handwritten
    digits from the popular **MNIST database** (short for **Mixed National Institute
    of Standards and Technology database**). For this, we will be making use of Keras,
    a high-level neural network library, which is also frequently used by researchers
    and tech companies.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们想要了解一些简单的人工神经网络版本，例如麦克洛奇-皮茨神经元、感知器和多层感知器。一旦我们熟悉了基础知识，我们就可以准备实现一个更复杂的深度神经网络，用于从流行的**MNIST数据库**（简称**混合国家标准与技术研究院数据库**）中分类手写数字。为此，我们将使用
    Keras，这是一个高级神经网络库，也被研究人员和技术公司广泛使用。
- en: 'Along the way, we will address the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程中，我们将解决以下问题：
- en: Implementing perceptrons and multilayer perceptrons in OpenCV
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 OpenCV 中实现感知器和多层感知器
- en: Differentiating stochastic and batch gradient descent, and how they fit in with
    backpropagation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分随机梯度下降和批量梯度下降，以及它们如何与反向传播相结合
- en: Finding the size of your neural network
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定神经网络的大小
- en: Using Keras to build sophisticated deep neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 构建复杂的深度神经网络
- en: Excited? Then let's go!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 激动吗？那么，让我们开始吧！
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can refer the code for this chapter at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter09](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter09).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此链接中找到本章的代码：[https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter09](https://github.com/PacktPublishing/Machine-Learning-for-OpenCV-Second-Edition/tree/master/Chapter09)。
- en: 'Here is a short summary of the software and hardware requirements:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是软件和硬件要求的简要总结：
- en: OpenCV version 4.1.x (4.1.0 or 4.1.1 will both work just fine).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV 版本 4.1.x（4.1.0 或 4.1.1 都可以正常工作）。
- en: Python version 3.6 (any Python version 3.x will be fine).
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 版本 3.6（任何 3.x 版本的 Python 都可以）。
- en: Anaconda Python 3 for installing Python and the required modules.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Python 和所需模块的 Anaconda Python 3。
- en: You can use any operating system—macOS, Windows, or Linux-based—with this book.
    We recommend you have at least 4 GB RAM in your system.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用任何操作系统——macOS、Windows 或基于 Linux 的系统——使用本书。我们建议您的系统至少有 4 GB 的 RAM。
- en: You don't need to have a GPU to run the code provided with the book.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行本书提供的代码不需要 GPU。
- en: Understanding the McCulloch-Pitts neuron
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解麦克洛奇-皮茨神经元
- en: In 1943, Warren McCulloch and Walter Pitts published a mathematical description
    of neurons as they were believed to operate in the brain. A neuron receives input
    from other neurons through connections on its dendritic tree, which are integrated
    to produce an output at the cell body (or soma). The output is then communicated
    to other neurons via a long wire (or axon), which eventually branches out to make
    one or more connections (at axon terminals) on the dendritic tree of other neurons.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 1943年，沃伦·麦克洛奇（Warren McCulloch）和沃尔特·皮茨（Walter Pitts）发表了对神经元数学描述的文章，他们相信神经元在大脑中是这样运作的。神经元通过其树突树上的连接从其他神经元接收输入，这些输入在细胞体（或胞体）处综合产生输出。然后，通过一根长长的电线（或轴突）将输出传递给其他神经元，这根电线最终分支出来，在其他神经元的树突树上形成一个或多个连接（在轴突末端）。
- en: 'An example neuron is shown in the following diagram:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图中展示了示例神经元：
- en: '![](img/b6e7d5f2-dde4-47cb-9ba1-dc48d0346dbd.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b6e7d5f2-dde4-47cb-9ba1-dc48d0346dbd.png)'
- en: McCulloch and Pitts described the inner workings of such a neuron as a simple
    logic gate that would be either on or off, depending on the input it received
    on its dendritic tree. Specifically, the neuron would sum up all of its inputs,
    and if the sum exceeded a certain threshold, an output signal would be generated
    and passed on by the axon.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 麦克洛奇和皮茨描述了这种神经元的内部工作原理，将其视为一个简单的逻辑门，它要么开启，要么关闭，这取决于它在其树突上的输入。具体来说，神经元会将其所有输入相加，如果总和超过某个阈值，就会产生一个输出信号，并通过轴突传递。
- en: However, today we know that real neurons are much more complicated than that.
    Biological neurons perform intricate nonlinear mathematical operations on thousands
    of inputs and can change their responsiveness dynamically depending on the context,
    importance, or novelty of the input signal. You can think of real neurons being
    as complex as computers and of the human brain being as complex as the internet.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，今天我们知道，真实的神经元比这要复杂得多。生物神经元在数千个输入上执行复杂的非线性数学运算，并且可以根据上下文、重要性或输入信号的新颖性动态地改变其反应性。你可以将真实的神经元想象得像计算机一样复杂，将人脑想象得像互联网一样复杂。
- en: 'Let''s consider a simple artificial neuron that receives exactly two inputs,
    *x[0]* and *x[1]*. The job of the artificial neuron is to calculate a sum of the
    two inputs (usually in the form of a weighted sum), and if this sum exceeds a
    certain threshold (often zero), the neuron will be considered active and output
    a one; else it will be considered silent and output a minus one (or zero). In
    more mathematical terms, the output, *y*, of this McCulloch-Pitts neuron can be
    described as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的人工神经元，它恰好接收两个输入，*x[0]* 和 *x[1]*。人工神经元的任务是计算两个输入的总和（通常以加权总和的形式），如果这个总和超过某个阈值（通常是零），则该神经元将被认为是活跃的并输出一个一；否则，它将被认为是沉默的并输出一个负一（或零）。用更数学的话来说，这个麦克洛奇-皮茨神经元的输出
    *y* 可以描述如下：
- en: '![](img/77e7cc8a-4619-4549-9be5-d20576ca9175.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/77e7cc8a-4619-4549-9be5-d20576ca9175.png)'
- en: 'In the preceding equation, *w[0]* and *w[1]* are weight coefficients, which,
    together with *x[0]* and *x[1]*, make up the weighted sum. In textbooks, the two
    different scenarios where the output, *y*, is either *+1* and *-1* would often
    be masked by an activation function, *ϕ*, which could take on two different values:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，*w[0]* 和 *w[1]* 是权重系数，它们与 *x[0]* 和 *x[1]* 一起构成了加权总和。在教科书中，输出 *y* 要么是
    *+1* 要么是 *-1* 的两种不同情况，通常会被一个激活函数 *ϕ* 所掩盖，该函数可以取两个不同的值：
- en: '![](img/d494b0cc-0876-44a1-abf7-6dd5029c2724.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d494b0cc-0876-44a1-abf7-6dd5029c2724.png)'
- en: 'Here, we introduce a new variable, *z* (the so-called **network input**), which
    is equivalent to the weighted sum: *z = w[0]x[0] + w[1]x[1]*. The weighted sum
    is then compared to a threshold, *θ*, to determine the value of *ϕ* and subsequently
    the value of *y*. Apart from that, these two equations say exactly the same thing
    as the preceding one.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们引入一个新的变量 *z*（所谓的**网络输入**），它等同于加权总和：*z = w[0]x[0] + w[1]x[1]*。然后，加权总和与阈值
    *θ* 进行比较，以确定 *ϕ* 的值，进而确定 *y* 的值。除此之外，这两个方程与前面的方程完全相同。
- en: If these equations look strangely familiar, you might be reminded of [Chapter
    1](7ebeef44-67a4-4843-83e0-bd644fc19eea.xhtml), *A Taste of Machine Learning*,
    when we were talking about linear classifiers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些方程看起来很熟悉，你可能会想起我们在谈论线性分类器时提到的[第一章](7ebeef44-67a4-4843-83e0-bd644fc19eea.xhtml)，《机器学习的味道》。
- en: And you are right, a McCulloch-Pitts neuron is essentially a linear, binary
    classifier!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你说得对，一个麦克洛奇-皮茨神经元本质上是一个线性的、二进制分类器！
- en: 'You can think of it this way: *x[0]* and *x[1]* are the input features, *w[0]*
    and *w[1]* are weights to be learned, and the classification is performed by the
    activation function, *ϕ.* If we do a good job of learning the weights, which we
    would do with the help of a suitable training set, we could classify data as positive
    or negative samples. In this scenario, *ϕ(z)=θ* would act as the decision boundary.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以这样想：*x[0]* 和 *x[1]* 是输入特征，*w[0]* 和 *w[1]* 是需要学习的权重，分类是通过激活函数 *ϕ* 来执行的。如果我们能很好地学习权重，这通常需要借助合适的训练集，我们就能将数据分类为正样本或负样本。在这种情况下，*ϕ(z)=θ*
    将充当决策边界。
- en: 'This might all make more sense with the help of the following diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助理解以下图表可能会使这一切更加清晰：
- en: '![](img/052ff7aa-3830-49eb-8b74-a7c2b22b90fd.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/052ff7aa-3830-49eb-8b74-a7c2b22b90fd.png)'
- en: On the left, you can see the neuron's activation function, *ϕ*, plotted against
    *z*. Remember that *z* is nothing more than the weighted sum of the two inputs
    *x[0]* and *x[1]*[.] The rule is that as long as the weighted sum is below some
    threshold, *θ*, the output of the neuron is -1; above *θ*, the output is +1.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，你可以看到神经元的激活函数，*ϕ*，与 *z* 的关系图。记住，*z* 仅仅是两个输入 *x[0]* 和 *x[1]* 的加权总和。规则是，只要加权总和低于某个阈值，*θ*，神经元的输出为
    -1；高于 *θ*，输出为 +1。
- en: On the right, you can see the decision boundary denoted by *ϕ(z)=θ*, which splits
    the data into two regimes, *ϕ(z)<θ* (where all data points are predicted to be
    negative samples) and *ϕ(z)>θ* (where all data points are predicted to be positive
    samples).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，你可以看到由 *ϕ(z)=θ* 表示的决策边界，它将数据分为两个区域，*ϕ(z)<θ*（其中所有数据点都被预测为负样本）和 *ϕ(z)>θ*（其中所有数据点都被预测为正样本）。
- en: The decision boundary does not need to be vertical or horizontal, it can be
    tilted as shown in the preceding diagram. But in the case of a single McCulloch-Pitts
    neuron, the decision boundary will always be a straight line.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 决策边界不需要是垂直或水平的，它可以像前面图示那样倾斜。但在单个 McCulloch-Pitts 神经元的情况下，决策边界始终是一条直线。
- en: Of course, the magic lies with learning the weight coefficients, *w[0]* and
    *w[1]*, such that the decision boundary comes to lie right between all positive
    and all negative data points.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，魔法在于学习权重系数，*w[0]* 和 *w[1]*，使得决策边界正好位于所有正数据和所有负数据点之间。
- en: 'To train a neural network, we generally need three things:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个神经网络，我们通常需要三样东西：
- en: '**Training data**: It is no surprise to learn that we need some data samples
    with which the effectiveness of our classifier can be verified.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据**：了解到我们需要一些数据样本来验证我们分类器的有效性，这并不令人惊讶。'
- en: '**Cost function (also known as loss function)**: A cost function provides a
    measure of how good the current weight coefficients are. There is a wide range
    of cost functions available, which we will talk about toward the end of this chapter.
    One solution is to count the number of misclassifications. Another one is to calculate
    the **sum of squared errors**.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代价函数（也称为损失函数）**：代价函数提供了一个衡量当前权重系数好坏的指标。有各种各样的代价函数可供选择，我们将在本章末尾讨论。一个解决方案是计算误分类的数量。另一个解决方案是计算
    **平方误差之和**。'
- en: '**Learning rule**: A learning rule specifies mathematically how we have to
    update the weight coefficients from one iteration to the next. This learning rule
    usually depends on the error (measured by the cost function) we observed on the
    training data.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习规则**：学习规则从数学上指定了如何从一次迭代更新权重系数到下一次迭代。这个学习规则通常取决于我们在训练数据上观察到的错误（由损失函数衡量）。 '
- en: This is where the work of renowned researcher Frank Rosenblatt comes in.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是著名研究员弗兰克·罗森布拉特的工作所在。
- en: Understanding the perceptron
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解感知器
- en: 'In the 1950s, American psychologist and artificial intelligence researcher
    Frank Rosenblatt invented an algorithm that would automatically learn the optimal
    weight coefficients *w[0]* and *w[1]* needed to perform an accurate binary classification:
    the perceptron learning rule.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪50年代，美国心理学家和人工智能研究员弗兰克·罗森布拉特发明了一个算法，该算法可以自动学习执行准确二元分类所需的最佳权重系数 *w[0]* 和
    *w[1]*：感知器学习规则。
- en: 'Rosenblatt''s original perceptron algorithm can be summed up as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 罗森布拉特原始的感知器算法可以总结如下：
- en: Initialize the weights to zero or some small random numbers.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重初始化为零或一些小的随机数。
- en: 'For each training sample, *s[i]*, perform the following steps:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个训练样本，*s[i]*，执行以下步骤：
- en: Compute the predicted target value, *ŷ**[i].*
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算预测的目标值，*ŷ**[i].*
- en: 'Compare *ŷ**[i]* to the ground truth, *y**[i]*, and update the weights accordingly:'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *ŷ**[i]* 与真实值，*y**[i]*，进行比较，并相应地更新权重：
- en: If the two are the same (correct prediction), skip ahead.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两者相同（预测正确），则跳过。
- en: If the two are different (wrong prediction), push the weight coefficients, *w[0]*
    and
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两者不同（预测错误），则推动权重系数，*w[0]* 和
- en: Implementing your first perceptron
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现你的第一个感知器
- en: Perceptrons are easy enough to be implemented from scratch. We can mimic the
    typical OpenCV or scikit-learn implementation of a classifier by creating a perceptron
    object. This will allow us to initialize new perceptron objects that can learn
    from data via a `fit` method and make predictions via a separate `predict` method.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器足够简单，可以从头开始实现。我们可以通过创建感知器对象来模拟典型的 OpenCV 或 scikit-learn 分类器的实现。这将允许我们初始化新的感知器对象，通过
    `fit` 方法从数据中学习，并通过单独的 `predict` 方法进行预测。
- en: 'When we initialize a new perceptron object, we want to pass a learning rate
    (`lr`, or *η* in the previous section) and the number of iterations after which
    the algorithm should terminate (`n_iter`):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们初始化一个新的感知器对象时，我们希望传递一个学习率（`lr`，或前一部分中的*η*）以及算法应该在多少次迭代后终止的次数（`n_iter`）：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `fit` method is where most of the work is done. This method should take
    as input some data samples (`X`) and their associated target labels (`y`). We
    will then create an array of weights (`self.weights`), one for each feature (`X.shape[1]`),
    initialized to zero. For convenience, we will keep the bias term (`self.bias`)
    separate from the weight vector and initialize it to zero as well. One of the
    reasons for initializing the bias to zero is because the small random numbers
    in the weights provide asymmetry breaking in the network:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`方法是大部分工作的地方。该方法应接受一些数据样本（`X`）及其相关的目标标签（`y`）。然后我们将创建一个权重数组（`self.weights`），每个特征一个，初始化为零。为了方便，我们将偏差项（`self.bias`）与权重向量分开，并将其也初始化为零。初始化偏差为零的一个原因是因为权重中的小随机数在网络中提供了不对称性破坏：'
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `predict` method should take in a number of data samples (`X`) and, for
    each of them, return a target label, either +1 or -1\. In order to perform this
    classification, we need to implement *ϕ(z)>θ*. Here we will choose *θ = 0*, and
    the weighted sum can be computed with NumPy''s dot product:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict`方法应该接受多个数据样本（`X`），并为每个样本返回一个目标标签，即+1或-1。为了执行这种分类，我们需要实现*ϕ(z)>θ*。这里我们将选择*θ
    = 0*，加权求和可以使用NumPy的点积来计算：'
- en: '[PRE2]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we will calculate the Δ*w* terms for every data sample (`xi`, `yi`) in
    the dataset and repeat this step for a number of iterations (`self.n_iter`). For
    this, we need to compare the ground-truth label (`yi`) to the predicted label
    (aforementioned `self.predict(xi)`). The resulting delta term will be used to
    update both the weights and the bias term:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将计算数据集中每个数据样本（`xi`，`yi`）的Δ*w*项，并重复此步骤多次迭代（`self.n_iter`）。为此，我们需要将真实标签（`yi`）与预测标签（前面提到的`self.predict(xi)`）进行比较。产生的delta项将用于更新权重和偏差项：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That's it!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！
- en: Generating a toy dataset
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成玩具数据集
- en: 'In the following steps, you will learn how to create and plot a toy dataset:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，你将学习如何创建和绘制一个玩具数据集：
- en: 'To test our perceptron classifier, we need to create some mock data. Let''s
    keep things simple for now and generate 100 data samples (`n_samples`) belonging
    to one of two blobs (`centers`), again relying on scikit-learn''s `make_blobs`
    function:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试我们的感知器分类器，我们需要创建一些模拟数据。现在让我们保持简单，并生成100个数据样本（`n_samples`），属于两个blob之一（`centers`），再次依赖于scikit-learn的`make_blobs`函数：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'One thing to keep in mind is that our perceptron classifier expects target
    labels to be either +1 or -1, whereas `make_blobs` returns `0` and `1`. An easy
    way to adjust the labels is with the following equation:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 需要注意的一件事是，我们的感知器分类器期望目标标签为+1或-1，而`make_blobs`返回`0`和`1`。调整标签的一个简单方法是以下方程：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the following code, we ...
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将...
- en: Fitting the perceptron to data
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将感知器拟合到数据
- en: 'In the following steps, you will learn to fit a perceptron algorithm on the
    given data:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，你将学习如何在给定的数据上拟合感知器算法：
- en: 'We can instantiate our perceptron object similar to other classifiers we encountered
    with OpenCV:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以像使用OpenCV中遇到的其他分类器一样实例化我们的感知器对象：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here, we chose a learning rate of 0.1 and told the perceptron to terminate after
    10 iterations. These values are chosen rather arbitrarily at this point, although
    we will come back to them in a little while.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择了一个学习率为0.1，并告诉感知器在10次迭代后终止。这些值目前是相当任意选择的，尽管我们很快就会回到它们。
- en: Choosing an appropriate learning rate is critical, but it's not always clear
    what the most appropriate choice is. The learning rate determines how quickly
    or slowly we move toward the optimal weight coefficients. If the learning rate
    is too large, we might accidentally skip the optimal solution. If it is too small,
    we will need a large number of iterations to converge to the best values.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的学习率至关重要，但并不总是清楚最合适的选择是什么。学习率决定了我们以多快或多慢的速度向最优权重系数移动。如果学习率太大，我们可能会意外地跳过最优解。如果它太小，我们需要大量的迭代才能收敛到最佳值。
- en: 'Once the perceptron is set up, we can call the `fit` method to optimize the
    weight coefficients:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦感知器设置完成，我们可以调用`fit`方法来优化权重系数：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Did it work? Let''s have a look at the learned weight values:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它是否有效？让我们看看学习到的权重值：
- en: '[PRE8]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And don''t forget to have a peek at the bias term:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 并且不要忘记查看偏差项：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If we plug these values into our equation for *ϕ*, it becomes clear that the
    perceptron learned a decision boundary of the form *2.2 x[1] - 0.48 x[2] + 0.2
    >= 0*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些值代入我们的*ϕ*方程中，就可以清楚地看到感知器学习到了形如*2.2 x[1] - 0.48 x[2] + 0.2 >= 0*的决策边界。
- en: Evaluating the perceptron classifier
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估感知器分类器
- en: 'In the following steps, you will be evaluating the trained perceptron on the
    test data:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，你将对训练好的感知器在测试数据上进行评估：
- en: 'In order to find out how good our perceptron performs, we can calculate the
    accuracy score on all data samples:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了了解我们的感知器表现如何，我们可以在所有数据样本上计算准确率：
- en: '[PRE10]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Perfect score!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 完美得分！
- en: 'Let''s have a look at the decision landscape by bringing back our `plot_decision_boundary`
    from the earlier chapters:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过将之前章节中的`plot_decision_boundary`函数调回来，来看看决策景观：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Applying the perceptron to data that is not linearly separable
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将感知器应用于非线性可分的数据
- en: 'In the following steps, you will learn to build a perceptron to separate a
    nonlinear data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，你将学习如何构建一个感知器来分离非线性数据：
- en: 'Since the perceptron is a linear classifier, you can imagine that it would
    have trouble trying to classify data that is not linearly separable. We can test
    this by increasing the spread (`cluster_std`) of the two blobs in our toy dataset
    so that the two blobs start overlapping:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于感知器是一个线性分类器，你可以想象它会在尝试对非线性可分的数据进行分类时遇到困难。我们可以通过增加我们玩具数据集中两个团块的扩散（`cluster_std`）来测试这一点，使得两个团块开始重叠：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can plot the dataset again using matplotlib''s `scatter` function:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用matplotlib的`scatter`函数再次绘制数据集：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As is evident in the following screenshot, this data is no longer linearly
    separable because there is no straight line that perfectly separates the two blobs:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下截图所示，这些数据不再是线性可分的，因为没有一条直线可以完美地分离这两个团块：
- en: '![](img/6a9a9a8d-1f6a-4fc8-992f-bb38f1192b49.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6a9a9a8d-1f6a-4fc8-992f-bb38f1192b49.png)'
- en: The preceding screenshot shows an example of data that is not linearly separable.
    So what would happen if we applied the perceptron classifier to this dataset?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的截图展示了一个非线性可分的数据集的例子。那么，如果我们将感知器分类器应用于这个数据集，会发生什么呢？
- en: 'We can find an answer to this question by repeating the preceding steps:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过重复前面的步骤来找到这个问题的答案：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then we find an accuracy score of 81%:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们找到了一个准确率为81%的评分：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In order to find out which data points were misclassified, we can again visualize
    the decision landscape using our helper function:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了找出哪些数据点被错误分类，我们可以再次使用我们的辅助函数可视化决策景观：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following graph makes the limitations of the perceptron classifier evident.
    Being a linear classifier, it tried to separate the data using a straight line
    but ultimately failed. The main reason it failed is because the data was not linearly
    separable even though we achieved 81% accuracy. However, from the following plot,
    it is clear that many of the red dots lie in the blue region and vice versa. So,
    unlike a perceptron, we need a nonlinear algorithm that can create not a straight
    but a nonlinear (circular) decision boundary:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表清楚地展示了感知器分类器的局限性。作为一个线性分类器，它试图使用一条直线来分离数据，但最终失败了。它失败的主要原因是因为数据本身不是线性可分的，尽管我们达到了81%的准确率。然而，从以下图中可以看出，许多红色点位于蓝色区域，反之亦然。因此，与感知器不同，我们需要一个非线性算法，它可以创建的不是直线而是非线性（圆形）的决策边界：
- en: '![](img/21aa1c5d-691d-412d-af9a-14d86495a613.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21aa1c5d-691d-412d-af9a-14d86495a613.png)'
- en: Fortunately, there are ways to make the perceptron more powerful and ultimately
    create nonlinear decision boundaries.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有方法可以使感知器更强大，并最终创建非线性决策边界。
- en: Understanding multilayer perceptrons
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解多层感知器
- en: In order to create nonlinear decision boundaries, we can combine multiple perceptrons
    to form a larger network. This is also known as a **multilayer perceptron** (**MLP**).
    MLPs usually consist of at least three layers, where the first layer has a node
    (or neuron) for every input feature of the dataset, and the last layer has a node
    for every class label. The layer in between is called the **hidden layer**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建非线性决策边界，我们可以将多个感知器组合成更大的网络。这也被称为**多层感知器**（**MLP**）。MLP通常至少包含三个层，其中第一层为数据集的每个输入特征都有一个节点（或神经元），最后一层为每个类别标签都有一个节点。中间的层被称为**隐藏层**。
- en: 'An example of this feedforward neural network architecture is shown in the
    following diagram:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了这种前馈神经网络架构的示例：
- en: '![](img/d824c0c2-9d1f-4ccd-a5ba-ac2010eee788.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d824c0c2-9d1f-4ccd-a5ba-ac2010eee788.png)'
- en: In this network, every circle is an artificial neuron (or, essentially, a perceptron),
    and the output of one artificial ...
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个网络中，每一个圆圈都是一个人工神经元（或者说，本质上是一个感知器），一个人工神经元的输出 ...
- en: Understanding gradient descent
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解梯度下降
- en: 'When we talked about the perceptron earlier in this chapter, we identified
    three of the essential ingredients needed for training: training data, a cost
    function, and a learning rule. While the learning rule worked great for a single
    perceptron, unfortunately, it did not generalize to MLPs, so people had to come
    up with a more general rule.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面讨论感知器时，我们确定了训练所需的三个基本要素：训练数据、代价函数和学习规则。虽然学习规则对单个感知器效果很好，但不幸的是，它并没有推广到多层感知器（MLPs），因此人们必须提出一个更通用的规则。
- en: If you think about how we measure the success of a classifier, we usually do
    so with the help of a cost function. A typical example is the number of misclassifications
    of the network or the mean squared error. This function (also known as a **loss
    function**) usually depends on the parameters we are trying to tweak. In neural
    networks, these parameters are the weight coefficients.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑我们如何衡量分类器的成功，我们通常借助代价函数来衡量。一个典型的例子是网络的误分类数量或均方误差。这个函数（也称为**损失函数**）通常取决于我们试图调整的参数。在神经网络中，这些参数是权重系数。
- en: 'Let''s assume a simple neural network has a single weight to tweak, *w*. Then
    we can visualize the cost as a function of the weight:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个简单的神经网络有一个可以调整的权重，*w*。然后我们可以将代价视为权重的函数：
- en: '![](img/9f8fb5ba-35a2-48e9-8bed-606c50106a8a.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9f8fb5ba-35a2-48e9-8bed-606c50106a8a.png)'
- en: At the beginning of training, at time zero, we may start out way on the left
    of this graph (*w[t=0]*). But from the graph, we know that there would be a better
    value for *w*, namely *w[optimal]*, which would minimize the cost function. The
    smallest cost means the lowest error, so it should be our highest goal to reach
    *w[optimal]* through learning.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始时，在时间零，我们可能开始时位于这张图的左侧（*w[t=0]*）。但从图中我们知道，对于 *w* 来说，会有一个更好的值，即 *w[optimal]*，这将最小化代价函数。最小的代价意味着最低的错误，因此，通过学习达到
    *w[optimal]* 应该是我们的最高目标。
- en: 'This is exactly what gradient descent does. You can think of the gradient as
    a vector that points up the hill. In gradient descent, we are trying to walk opposite
    of the gradient, effectively walking down the hill, from the peaks down to the
    valley:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是梯度下降所做的。你可以把梯度想象成一个指向山上的向量。在梯度下降中，我们试图沿着梯度的反方向行走，实际上是在下山，从山顶走到山谷：
- en: '![](img/ccc693b1-b01c-46f0-869f-565c6304fe32.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ccc693b1-b01c-46f0-869f-565c6304fe32.png)'
- en: Once you reach the valley, the gradient goes to zero, and that completes the
    training.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦到达山谷，梯度变为零，这就完成了训练。
- en: 'There are several ways to reach the valley—we could approach from the left,
    or we could approach from the right. The starting point of our descent is determined
    by the initial weight values. Furthermore, we have to be careful not to take too
    large a step, otherwise we might miss the valley:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以到达山谷——我们可以从左侧接近，或者我们可以从右侧接近。我们下降的起点由初始权重值决定。此外，我们必须小心不要迈出太大的步子，否则我们可能会错过山谷：
- en: '![](img/dc30f57b-3402-41f5-ba64-e7404f9072fc.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dc30f57b-3402-41f5-ba64-e7404f9072fc.png)'
- en: Hence, in stochastic gradient descent (sometimes also called iterative or on-line
    gradient descent), the goal is to take small steps but to take them as often as
    possible. The effective step size is determined by the learning rate of the algorithm.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在随机梯度下降（有时也称为迭代或在线梯度下降）中，目标是采取小步，但要尽可能频繁地采取这些步子。有效的步长由算法的学习率决定。
- en: 'Specifically, we would perform the following procedure over and over:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们会反复执行以下程序：
- en: Present a small number of training samples to the network (called the **batch
    size**).
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向网络呈现少量训练样本（称为**批量大小**）。
- en: On this small batch of data, calculate the gradient of the cost function.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个小批量数据上，计算代价函数的梯度。
- en: Update the weight coefficients by taking a small step in the opposite direction
    of the gradient, toward the valley.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在梯度的反方向上迈一小步，朝着山谷的方向更新权重系数。
- en: Repeat steps 1-3 until the weight cost no longer goes down. This is an indication
    that we have reached the valley.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1-3，直到权重代价不再下降。这是我们已经到达山谷的迹象。
- en: Some other ways to improve SGD are using the learning rate finder in the Keras
    framework, decreasing the step size (learning rate) in epochs, and, as discussed
    in the preceding point, using a batch size (or mini-batch), which will compute
    the weight update faster.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 改进SGD的一些其他方法包括在Keras框架中使用学习率查找器，在epoch中减小步长（学习率），以及在前一个点中讨论的，使用批量大小（或小批量），这将更快地计算权重更新。
- en: Can you think of an example where this procedure might fail?
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你能想到一个这个流程可能会失败的情况吗？
- en: 'One scenario that comes to mind is where the cost function has multiple valleys,
    some deeper than others, as shown in the following diagram:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以想到的场景是成本函数有多个山谷，其中一些比其他更深，如下面的图所示：
- en: '![](img/60373859-3362-45bc-b11b-031aeeb28fda.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/60373859-3362-45bc-b11b-031aeeb28fda.png)'
- en: If we start on the left, we should arrive at the same valley as before—no problem.
    But, if our starting point is all the way to the right, we might encounter another
    valley on the way. Gradient descent will lead us straight down to the valley,
    but it will not have any means to climb out of it.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从左边开始，我们应该到达之前相同的山谷——没问题。但是，如果我们的起点在右边，我们可能会在途中遇到另一个山谷。梯度下降会直接把我们带到山谷，但它没有爬出来的方法。
- en: This is also known as **getting stuck in a local minimum**. Researchers have
    come up with different ways to try and avoid this issue, one of them being to
    add noise to the process.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这也被称为**陷入局部最小值**。研究人员已经想出不同的方法来尝试避免这个问题，其中之一就是在过程中添加噪声。
- en: There is one piece left in the puzzle. Given our current weight coefficients,
    how do we know the slope of the cost function?
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 拼图中还缺一块。给定我们当前的权重系数，我们如何知道成本函数的斜率？
- en: Training MLPs with backpropagation
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用反向传播训练MLP
- en: This is where backpropagation comes in, which is an algorithm for estimating
    the gradient of the cost function in neural networks. Some might say that it is
    basically a fancy word for the chain rule, which is a means to calculate the partial
    derivative of functions that depend on more than one variable. Nonetheless, it
    is a method that helped bring the field of artificial neural networks back to
    life, so we should be thankful for that.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是反向传播的作用，它是一种用于估计神经网络中成本函数梯度的算法。有些人可能会说，这基本上是链式法则的一个花哨的词，链式法则是计算依赖于多个变量的函数的偏导数的一种方法。尽管如此，它是一种帮助人工神经网络领域重生的方法，因此我们应该为此感到感激。
- en: Understanding backpropagation involves quite a bit of calculus, so I will only
    give you a brief introduction here.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 理解反向传播需要相当多的微积分知识，所以我只会在这里给你一个简要的介绍。
- en: Let's remind ourselves that the cost function, and therefore its gradient, depends
    on the difference between the true output (*y[i]*) and the current output (*ŷ[i]*
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提醒自己，成本函数及其梯度取决于真实输出(*y[i]*)和当前输出(*ŷ[i]*)之间的差异
- en: Implementing a MLP in OpenCV
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenCV中实现MLP
- en: 'Implementing an MLP in OpenCV uses the same syntax that we have seen at least
    a dozen times before. In order to see how an MLP compares to a single perceptron,
    we will operate on the same toy data as before:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中实现MLP使用我们之前至少见过一次的相同语法。为了了解MLP与单个感知器相比如何，我们将使用之前相同的玩具数据操作：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Preprocessing the data
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'However, since we are working with OpenCV, this time we want to make sure the
    input matrix is made up of 32-bit floating point numbers, otherwise the code will
    break:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们正在使用OpenCV，这次我们想确保输入矩阵由32位浮点数组成，否则代码会出错：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Furthermore, we need to think back to [Chapter 4](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml),
    *Representing Data and Engineering Features*, and remember how to represent categorical
    variables. We need to find a way to represent target labels, not as integers but
    with a one-hot encoding. The easiest way to achieve this is by using scikit-learn''s
    `preprocessing` module:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要回顾[第4章](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml)，*表示数据和工程特征*，并记住如何表示分类变量。我们需要找到一种方法来表示目标标签，而不是整数，而是使用独热编码。实现这一点最简单的方法是使用scikit-learn的`preprocessing`模块：
- en: '[PRE19]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Creating an MLP classifier in OpenCV
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在OpenCV中创建MLP分类器
- en: 'The syntax to create an MLP in OpenCV is the same as for all the other classifiers:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenCV中创建MLP的语法与所有其他分类器相同：
- en: '[PRE20]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: However, now we need to specify how many layers we want in the network and how
    many neurons there are per layer. We do this with a list of integers, which specify
    the number of neurons in each layer. Since the data matrix `X` has two features,
    the first layer should also have two neurons in it (`n_input`). Since the output
    has two different values, the last layer should also have two neurons in it (`n_output`).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现在我们需要指定网络中要包含多少层以及每层有多少个神经元。我们通过一个整数列表来完成这项工作，该列表指定了每层的神经元数量。由于数据矩阵 `X`
    有两个特征，第一层也应该有两个神经元（`n_input`）。由于输出有两个不同的值，最后一层也应该有两个神经元（`n_output`）。
- en: 'In between these two layers, we can put as many hidden layers with as many
    neurons as we want. Let''s choose a single hidden layer with an arbitrary number
    of 10 neurons in it (`n_hidden`):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两层之间，我们可以放置任意数量的隐藏层，每层包含任意数量的神经元。让我们选择一个包含任意数量 10 个神经元的单个隐藏层（`n_hidden`）：
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Customizing the MLP classifier
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定制 MLP 分类器
- en: 'Before we move on to training the classifier, we can customize the MLP classifier
    via a number of optional settings:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始训练分类器之前，我们可以通过一系列可选设置来定制 MLP 分类器：
- en: '`mlp.setActivationFunction`: This defines the activation function to be used
    for every neuron in the network.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp.setActivationFunction`: 这定义了网络中每个神经元要使用的激活函数。'
- en: '`mlp.setTrainMethod`: This defines a suitable training method.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp.setTrainMethod`: 这定义了一个合适的训练方法。'
- en: '`mlp.setTermCriteria`: This sets the termination criteria of the training phase.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp.setTermCriteria`: 这设置了训练阶段的终止标准。'
- en: 'Whereas our home-brewed perceptron classifier used a linear activation function,
    OpenCV provides two additional options:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 而我们的自制感知器分类器使用的是线性激活函数，OpenCV 提供了两个额外的选项：
- en: '`cv2.ml.ANN_MLP_IDENTITY`: This is the linear activation function, *f(x) =
    x*.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cv2.ml.ANN_MLP_IDENTITY`: 这是一个线性激活函数，*f(x) = x*。'
- en: '`cv2.ml.ANN_MLP_SIGMOID_SYM`: This is the symmetrical sigmoid function (also
    known as **hyperbolic tangent**), *f(x) = β (1 -* exp*(-α x)) / (1 +* exp*(-α
    x))*. Whereas ...'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cv2.ml.ANN_MLP_SIGMOID_SYM`: 这是一个对称的 Sigmoid 函数（也称为双曲正切），*f(x) = β (1 - exp(-α
    x)) / (1 + exp(-α x))*. 而 ...'
- en: Training and testing the MLP classifier
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和测试 MLP 分类器
- en: 'This is the easy part. Training the MLP classifier is the same as with all
    other classifiers:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分很简单。训练 MLP 分类器与所有其他分类器相同：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The same goes for predicting target labels:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测目标标签也是一样：
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The easiest way to measure accuracy is by using scikit-learn''s helper function:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 测量准确率最简单的方法是使用 scikit-learn 的辅助函数：
- en: '[PRE24]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It looks like we were able to increase our performance from 81% with a single
    perceptron to 88% with an MLP consisting of 10 hidden-layer neurons and 2 output
    neurons. In order to see what changed, we can look at the decision boundary one
    more time:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们能够将性能从单个感知器的 81% 提高到由 10 个隐藏层神经元和 2 个输出神经元组成的 MLP 的 88%。为了看到发生了什么变化，我们可以再次查看决策边界：
- en: '[PRE25]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'However, there is a problem right here, in that `zz` is now a one-hot encoded
    matrix. In order to transform the one-hot encoding into a number that corresponds
    to the class label (zero or one), we can use NumPy''s `argmax` function:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个问题，那就是 `zz` 现在是一个 one-hot 编码的矩阵。为了将 one-hot 编码转换为对应于类别标签（零或一）的数字，我们可以使用
    NumPy 的 `argmax` 函数：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then the rest stays the same:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后其余部分保持不变：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we can call the function like this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以这样调用函数：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output looks like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输出看起来是这样的：
- en: '![](img/7430fc52-df88-490d-972d-4d56c4ee0eb3.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7430fc52-df88-490d-972d-4d56c4ee0eb3.png)'
- en: The preceding output shows a decision boundary of an MLP with one hidden layer.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示了一个具有单个隐藏层的 MLP 的决策边界。
- en: And voila! The decision boundary is no longer a straight line. That being said,
    you got a great performance increase and might have expected a more drastic performance
    increase. But nobody said we have to stop here!
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！决策边界不再是直线了。话虽如此，你获得了很大的性能提升，可能还期望有更大的性能提升。但没有人说过我们必须止步于此！
- en: 'There are at least two different things we can try from here on out:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们可以尝试至少两种不同的方法：
- en: We can add more neurons to the hidden layer. You can do this by replacing `n_hidden`
    on line 6 with a larger value and running the code again. Generally speaking,
    the more neurons you put in the network, the more powerful the MLP will be.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在隐藏层中添加更多的神经元。你可以通过在第六行将 `n_hidden` 替换为更大的值并再次运行代码来实现这一点。一般来说，你放入网络中的神经元越多，MLP
    的能力就越强。
- en: We can add more hidden layers. It turns out that this is where neural networks
    really get their power from.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以添加更多的隐藏层。结果证明，这正是神经网络真正获得其力量的地方。
- en: Hence, this is where I should tell you about deep learning.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这就是我应该告诉你关于深度学习的地方。
- en: Getting acquainted with deep learning
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解深度学习
- en: Back when deep learning didn't have a fancy name yet, it was called artificial
    neural networks. So you already know a great deal about it!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习还没有一个响亮的名字之前，它被称为人工神经网络。所以你已经对它了解很多了！
- en: Eventually, interest in neural networks was rekindled in 1986 when David Rumelhart,
    Geoffrey Hinton, and Ronald Williams were involved in the (re)discovery and popularization
    of the aforementioned backpropagation algorithm. However, it was not until recently
    that computers became powerful enough so they could actually execute the backpropagation
    algorithm on large-scale networks, leading to a surge in deep learning research.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 1986年，当David Rumelhart、Geoffrey Hinton和Ronald Williams参与上述反向传播算法的（再）发现和普及时，对神经网络的研究兴趣再次被点燃。然而，直到最近，计算机才足够强大，能够在大型网络上实际执行反向传播算法，从而引发了深度学习研究的激增。
- en: 'You can find more information on the history and origin of deep learning in
    the following scientific article: Wang and Raj (2017), *On the Origin ...*'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下科学文章中找到有关深度学习的历史和起源的更多信息：王和拉吉（2017年），*关于起源...*
- en: Getting acquainted with Keras
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解Keras
- en: 'The core data structure of Keras is a model, which is similar to OpenCV''s
    classifier object, except it focuses on neural networks only. The simplest type
    of model is the sequential model, which arranges the different layers of the neural
    network in a linear stack, just like we did for the MLP in OpenCV:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Keras的核心数据结构是模型，它类似于OpenCV的分类器对象，但它只关注神经网络。最简单的模型类型是序列模型，它将神经网络的各个层线性堆叠起来，就像我们在OpenCV中对MLP所做的那样：
- en: '[PRE29]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, different layers can be added to the model one by one. In Keras, layers
    do not just contain neurons, they also perform a function. Some core layer types
    include the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以逐个将不同的层添加到模型中。在Keras中，层不仅包含神经元，还执行一个函数。一些核心层类型包括以下内容：
- en: '**Dense**: This is a densely connected layer. This is exactly what we used
    when we designed our MLP: a layer of neurons that is connected to every neuron
    in the previous layer.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集层**：这是一个密集连接层。这正是我们在设计MLP时使用的：一个与前一层的每个神经元都连接的神经元层。'
- en: '**Activation**: This applies an activation function to an output. Keras provides
    a whole range of activation functions, including OpenCV''s identify function (`linear`),
    the hyperbolic tangent (`tanh`), a sigmoidal squashing function (`sigmoid`), a
    softmax function (`softmax`), and many more.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活**：此操作对输出应用激活函数。Keras提供了一系列激活函数，包括OpenCV的恒等函数（`linear`）、双曲正切（`tanh`）、S形压缩函数（`sigmoid`）、softmax函数（`softmax`）等。'
- en: '**Reshape**: This reshapes an output to a certain shape.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重塑**：此操作将输出重塑为特定的形状。'
- en: 'There are other layers that calculate arithmetic or geometric operations on
    their inputs:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他层可以对它们的输入进行算术或几何运算：
- en: '**Convolutional layers**: These layers allow you to specify a kernel with which
    the input layer is convolved. This allows you to perform operations such as a
    Sobel filter or apply a Gaussian kernel in 1D, 2D, or even 3D.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层**：这些层允许你指定一个内核，与输入层进行卷积。这允许你执行诸如Sobel滤波器或应用1D、2D甚至3D的高斯核等操作。'
- en: '**Pooling layers**: These layers perform a max pooling operation on their input,
    where the output neuron''s activity is given by the maximally active input neuron.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池化层**：这些层对其输入执行最大池化操作，其中输出神经元的活性由最活跃的输入神经元给出。'
- en: 'Some other layers that are popular in deep learning are as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中流行的其他层如下：
- en: '**Dropout**: This layer randomly sets a fraction of input units to zero at
    each update. This is a way to inject noise into the training process, making it
    more robust.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：此层在每个更新时随机将一部分输入单元设置为0。这是将噪声注入训练过程的一种方式，使其更加鲁棒。'
- en: '**Embeddin****g**: This layer encodes categorical data, similar to some functions
    from scikit-learn''s `preprocessing` module.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入**：此层对分类数据进行编码，类似于scikit-learn的`preprocessing`模块中的某些函数。'
- en: '**Gaussian noise**: This layer applies additive zero-centered Gaussian noise.
    This is another way of injecting noise into the training process, making it more
    robust.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯噪声**：此层应用加性零均值高斯噪声。这是将噪声注入训练过程的一种方式，使其更加鲁棒。'
- en: 'A perceptron similar to the preceding one could thus be implemented using a
    dense layer that has two inputs and one output. Staying true to our earlier example,
    we will initialize the weights to zero and use the hyperbolic tangent as an activation
    function:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用具有两个输入和一个输出的密集层实现与前面类似的前馈感知器。保持与之前示例的一致性，我们将权重初始化为零，并使用双曲正切作为激活函数：
- en: '[PRE30]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, we want to specify the training method. Keras provides a number of
    optimizers, including the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们想要指定训练方法。Keras 提供了多种优化器，包括以下几种：
- en: '**Stochastic gradient descent (SGD)**: This is what we discussed earlier.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降（SGD）**：这是我们之前讨论过的。'
- en: '**Root mean square propagation (RMSprop)**: This is a method in which the learning
    rate is adapted for each of the parameters.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方根传播（RMSprop）**：这是一种为每个参数调整学习率的方法。'
- en: '**Adaptive moment estimation (Adam)**: This is an update to the root mean square
    propagation.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应动量估计（Adam）**：这是对均方根传播的更新。'
- en: 'In addition, Keras also provides a number of different loss functions:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Keras 还提供了一系列不同的损失函数：
- en: '**Mean squared error (mean_squared_error)**: This is what was discussed earlier.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差（mean_squared_error）**：这是我们之前讨论过的。'
- en: '**Hinge loss (hinge)**: This is a maximum-margin classifier often used with
    SVM, as discussed in [Chapter 6](419719a8-3340-483a-86be-1d9b94f4a682.xhtml),
    *Detecting Pedestrians with Support Vector Machines*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hinge 损失（hinge）**：这是一种最大间隔分类器，通常与 SVM 一起使用，如第 6 章[419719a8-3340-483a-86be-1d9b94f4a682.xhtml]中所述，*使用支持向量机检测行人*。'
- en: 'You can see that there''s a plethora of parameters to be specified and methods
    to choose from. To stay true to our aforementioned perceptron implementation,
    we will choose SGD as an optimizer, the mean squared error as a cost function,
    and accuracy as a scoring function:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到有许多参数需要指定和许多方法可以选择。为了保持与我们之前提到的感知器实现的一致性，我们将选择 SGD 作为优化器，均方误差作为损失函数，准确率作为评分函数：
- en: '[PRE31]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In order to compare the performance of the Keras implementation to our home-brewed
    version, we will apply the classifier to the same dataset:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较 Keras 实现与我们的自制版本的性能，我们将分类器应用于相同的数据集：
- en: '[PRE32]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, a Keras model is fit to the data with a very familiar syntax. Here,
    we can also choose how many iterations to train for (`epochs`), how many samples
    to present before we calculate the error gradient (`batch_size`), whether to shuffle
    the dataset (`shuffle`), and whether to output progress updates (`verbose`):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用非常熟悉的语法将 Keras 模型拟合到数据中。在这里，我们还可以选择训练的迭代次数（`epochs`）、在计算误差梯度之前展示的样本数量（`batch_size`）、是否打乱数据集（`shuffle`）以及是否输出进度更新（`verbose`）：
- en: '[PRE33]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After the training completes, we can evaluate the classifier as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以如下评估分类器：
- en: '[PRE34]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Here, the first reported value is the mean squared error, whereas the second
    value denotes accuracy. This means that the final mean squared error was 0.04,
    and we had 100% accuracy. Way better than our own implementation!
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个报告的值是均方误差，而第二个值表示准确率。这意味着最终的均方误差为 0.04，我们达到了 100% 的准确率。比我们自己的实现要好得多！
- en: You can find more information on Keras, source code documentation, and a number
    of tutorials at [http://keras.io](http://keras.io).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [http://keras.io](http://keras.io) 上找到更多关于 Keras、源代码文档和许多教程的信息。
- en: With these tools in hand, we are now ready to approach a real-world dataset!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这些工具在手，我们现在可以开始处理真实世界的数据集了！
- en: Classifying handwritten digits
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手写数字分类
- en: In the previous section, we covered a lot of the theory around neural networks,
    which can be a little overwhelming if you are new to this topic. In this section,
    we will use the famous MNIST dataset, which contains 60,000 samples of handwritten
    digits along with their labels.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了关于神经网络的大量理论，如果你对这个主题是新手，可能会觉得有点令人不知所措。在本节中，我们将使用著名的 MNIST 数据集，它包含
    60,000 个手写数字样本及其标签。
- en: 'We will train two different networks on it:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这个数据集上训练两个不同的网络：
- en: An MLP using OpenCV
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 OpenCV 的 MLP
- en: A deep neural network using Keras
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 的深度神经网络
- en: Loading the MNIST dataset
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载 MNIST 数据集
- en: 'The easiest way to obtain the MNIST dataset is by using Keras:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 MNIST 数据集最简单的方法是使用 Keras：
- en: '[PRE35]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This will download the data from Amazon Cloud (might take a while depending
    on your internet connection) and automatically split the data into training and
    test sets.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从 Amazon Cloud 下载数据（根据您的网络连接速度可能需要一段时间）并自动将数据分为训练集和测试集。
- en: MNIST provides its own predefined train-test split. This way, it is easier to
    compare the performance of different classifiers because they will all use the
    same data for training and the same data for testing.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST提供了自己的预定义的训练-测试分割。这样，比较不同分类器的性能更容易，因为它们将使用相同的数据进行训练和测试。
- en: 'This data comes in a format that we are already familiar with:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据以我们已熟悉的格式出现：
- en: '[PRE36]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We should take note that the labels come as integer values between zero and
    nine (corresponding to the digits 0-9):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该注意，标签是以零到九之间的整数形式出现的（对应于数字0-9）：
- en: '[PRE37]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can have a look at some example digits:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看一些示例数字：
- en: '[PRE38]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The digits look like this:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字看起来是这样的：
- en: '![](img/cbf6668b-2d3f-4188-99c7-6363e034ca9f.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbf6668b-2d3f-4188-99c7-6363e034ca9f.png)'
- en: 'In fact, the MNIST dataset is the successor to the NIST digits dataset provided
    by scikit-learn that we used before (`sklearn.datasets.load_digits`; refer to
    [Chapter 2](8b9a6f9b-32b4-4af5-9a8c-4e121341f292.xhtml), *Working with Data in
    OpenCV*). Some notable differences are as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，MNIST数据集是scikit-learn之前提供的NIST数字数据集的后继者，我们之前使用过（`sklearn.datasets.load_digits`；参见[第2章](8b9a6f9b-32b4-4af5-9a8c-4e121341f292.xhtml)，*在OpenCV中处理数据*）。以下是一些显著的不同点：
- en: MNIST images are significantly larger (28 x 28 pixels) than NIST images (8 x
    8 pixels), thus paying more attention to fine details such as distortions and
    individual differences between images of the same digit.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNIST图像的尺寸显著大于NIST图像（28 x 28像素），因此需要更加关注诸如扭曲和相同数字图像之间的个体差异等细微细节。
- en: The MNIST dataset is much larger than the NIST dataset, providing 60,000 training
    and 10,000 test samples (as compared to a total of 5,620 NIST images).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MNIST数据集比NIST数据集大得多，提供了60,000个训练样本和10,000个测试样本（相比之下，NIST总共有5,620个图像）。
- en: Preprocessing the MNIST dataset
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理MNIST数据集
- en: 'As we learned in [Chapter 4](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml), *Representing
    Data and Engineering Features*, there are a number of preprocessing steps we might
    like to apply here:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第4章](142fec63-a847-4cde-9de9-c34805d2bb84.xhtml)中学习的，*表示数据和工程特征*，我们可能希望应用以下预处理步骤：
- en: '**Centering**: It is important that all the digits are centered in the image.
    For example, take a look at all the example images of the digit 1 in the preceding
    diagram, which are all made of an almost-vertical strike. If the images were misaligned,
    the strike could lie anywhere in the image, making it hard for the neural network
    to find commonalities in the training samples. Fortunately, images in MNIST are
    already centered.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**居中**：所有数字都应居中在图像中。例如，看看前面图表中所有数字1的示例图像，它们几乎都是垂直打击。如果图像未对齐，打击点可以位于图像的任何位置，这使得神经网络难以在训练样本中找到共性。幸运的是，MNIST中的图像已经居中。'
- en: '**Scaling**: The same is true for scaling the digits so that they all have
    the same size. This way, the location of strikes, curves, and loops are important.
    ...'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放**：对数字进行缩放以使它们具有相同的大小也是一样的。这样，打击点、曲线和环的位置就很重要。 ...'
- en: Training an MLP using OpenCV
  id: totrans-248
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenCV训练MLP
- en: 'We can set up and train an MLP in OpenCV with the following recipe:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方法在OpenCV中设置和训练一个MLP：
- en: 'Instantiate a new MLP object:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个新的MLP对象：
- en: '[PRE39]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Specify the size of every layer in the network. We are free to add as many
    layers as we want, but we need to make sure that the first layer has the same
    number of neurons as input features (`784` in our case), and that the last layer
    has the same number of neurons as class labels (`10` in our case), while there
    are two hidden layers each having `512` nodes:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定网络中每一层的尺寸。我们可以添加尽可能多的层，但我们需要确保第一层有与输入特征相同的神经元数量（在我们的例子中是`784`），并且最后一层有与类别标签相同的神经元数量（在我们的例子中是`10`），同时有两个隐藏层，每个隐藏层有`512`个节点：
- en: '[PRE40]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Specify an activation function. Here we use the sigmoidal activation function
    from before:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定激活函数。在这里，我们使用之前使用的S型激活函数：
- en: '[PRE41]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Specify the training method. Here, we use the backpropagation algorithm described
    earlier. We also need to make sure that we choose a small enough learning rate.
    Since we have on the order of 10⁵ training samples, it is a good idea to set the
    learning rate to at most 10^(-5):'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定训练方法。在这里，我们使用之前描述的逆传播算法。我们还需要确保我们选择足够小的学习率。由于我们有大约10⁵个训练样本，将学习率设置为最多10^(-5)是一个好主意：
- en: '[PRE42]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Specify the termination criteria. Here, we use the same criteria as before:
    to run training for 10 iterations (`term_max_iter`) or until the error does no
    longer increase significantly (`term_eps`):'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定终止条件。在这里，我们使用之前相同的条件：运行 10 次迭代 (`term_max_iter`) 或直到错误不再显著增加 (`term_eps`)：
- en: '[PRE43]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Train the network on the training set (`X_train_pre`):'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集 (`X_train_pre`) 上训练网络：
- en: '[PRE44]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Before you call `mlp.train`, here is a word of caution: this might take several
    hours to run, depending on your computer setup! For comparison, it took just under
    an hour on my own laptop. We are now dealing with a real-world dataset of 60,000
    samples: if we run 100 training epochs, we have to compute 6 million gradients!
    So beware.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 `mlp.train` 之前，请注意：这可能会根据您的计算机配置需要几个小时才能运行！为了比较，在我的笔记本电脑上只需要不到一个小时。我们现在处理的是一个包含
    60,000 个样本的真实世界数据集：如果我们运行 100 个训练轮次，我们必须计算 600 万个梯度！所以请小心。
- en: 'When the training completes, we can calculate the accuracy score on the training
    set to see how far we got:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练完成后，我们可以在训练集上计算准确率分数，看看我们达到了什么程度：
- en: '[PRE45]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'But, of course, what really counts is the accuracy score we get on the held-out
    test data which was not taken into account in training process:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，当然，真正重要的是我们在未参与训练过程的保留测试数据上得到的准确率分数：
- en: '[PRE46]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 91.7% accuracy is not bad at all if you ask me! The first thing you should try
    is to change the layer sizes in the preceding `In [10]` and see how the test score
    changes. As you add more neurons to the network, you should see the training score
    increase—and with it, hopefully, the test score. However, having *N* neurons in
    a single layer is not the same as having them spread out over several layers!
    Can you confirm this observation?
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果问我，91.7% 的准确率绝对不错！你应该尝试的第一件事是更改前面 `In [10]` 中的层大小，看看测试分数如何变化。随着你向网络添加更多的神经元，你应该看到训练分数增加——希望随之而来的是测试分数的增加。然而，在单个层中有
    *N* 个神经元与它们分布在几个层中是不同的！你能证实这个观察结果吗？
- en: Training a deep neural network using Keras
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Keras 训练深度神经网络
- en: Although we achieved a formidable score with the preceding MLP, our result does
    not hold up to state-of-the-art results. Currently, the best result has close
    to 99.8% accuracy—better than human performance! This is why, nowadays, the task
    of classifying handwritten digits is largely regarded as solved.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用前面的 MLP 取得了令人印象深刻的分数，但我们的结果并不符合最先进的结果。目前，最佳结果接近 99.8% 的准确率——优于人类表现！这就是为什么现在，将手写数字分类的任务在很大程度上被认为已经解决。
- en: To get closer to the state-of-the-art results, we need to use state-of-the-art
    techniques. Thus, we return to Keras.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 为了接近最先进的结果，我们需要使用最先进的技术。因此，我们回到了 Keras。
- en: Preprocessing the MNIST dataset
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理 MNIST 数据集
- en: 'In the following steps, you will learn to preprocess the data before it is
    fed to the neural network:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，你将学习在将数据馈送到神经网络之前预处理数据：
- en: 'To make sure we get the same result every time we run the experiment, we will
    pick a random seed for NumPy''s random number generator. This way, shuffling the
    training samples from the MNIST dataset will always result in the same order:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保每次运行实验时都能得到相同的结果，我们将为 NumPy 的随机数生成器选择一个随机种子。这样，从 MNIST 数据集的随机训练样本将始终以相同的顺序进行洗牌：
- en: '[PRE47]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Keras provides a loading function similar to `train_test_split` from scikit-learn''s
    `model_selection` module. Its syntax might look strangely familiar to you:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keras 提供了一个类似于 scikit-learn 的 `model_selection` 模块中的 `train_test_split` 的加载函数。它的语法可能看起来很熟悉：
- en: '[PRE48]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In contrast to other datasets we have encountered so far, MNIST comes with a
    predefined train-test split. This allows the dataset to be used as a benchmark,
    as the test score reported by different algorithms will always apply to the same
    test samples.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们迄今为止遇到的其他数据集相比，MNIST 预定义了训练-测试分割。这使得数据集可以用作基准，因为不同算法报告的测试分数将始终适用于相同的测试样本。
- en: 'The neural networks in Keras act on the feature matrix slightly differently
    than the standard OpenCV and scikit-learn estimators. Whereas the rows of a feature
    matrix in Keras still correspond to the number of samples (`X_train.shape[0]`
    in the following code), we can preserve the two-dimensional nature of the input
    images by adding more dimensions to the feature matrix:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Keras 中的神经网络在处理特征矩阵时与标准的 OpenCV 和 scikit-learn 估计器略有不同。在 Keras 中，特征矩阵的行仍然对应于样本数量（以下代码中的
    `X_train.shape[0]`），我们可以通过向特征矩阵添加更多维度来保留输入图像的二维性质：
- en: '[PRE49]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Here, we have reshaped the feature matrix into a four-dimensional matrix with
    dimensions `n_features` x 28 x 28 x 1\. We also need to make sure we operate on
    32-bit floating point numbers between [0, 1], rather than unsigned integers in
    [0, 255]:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们将特征矩阵重塑为具有`n_features` x 28 x 28 x 1维度的四维矩阵。我们还需要确保我们在[0, 1]范围内的32位浮点数上操作，而不是[0,
    255]范围内的无符号整数：
- en: '[PRE50]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then, we can one-hot encode the training labels as we did before. This will
    make sure each category of target labels can be assigned to a neuron in the output
    layer. We could do this with scikit-learn''s `preprocessing`, but in this case,
    it is easier to use Keras'' own utility function:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以像之前一样对训练标签进行one-hot编码。这将确保每个目标标签类别都可以分配到输出层中的一个神经元。我们可以使用scikit-learn的`preprocessing`来完成这个任务，但在这个情况下，使用Keras自己的实用函数更容易：
- en: '[PRE51]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Creating a convolutional neural network
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建卷积神经网络
- en: 'In the following steps, you will create a neural network and train on the data
    you preprocessed earlier:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，你将创建一个神经网络，并使用你之前预处理的数据进行训练：
- en: 'Once we have preprocessed the data, it is time to define the actual model.
    Here, we will once again rely on the `Sequential` model to define a feedforward
    neural network:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们预处理了数据，就是时候定义实际模型了。在这里，我们再次依赖`Sequential`模型来定义一个前馈神经网络：
- en: '[PRE52]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'However, this time, we will be smarter about the individual layers. We will
    design our neural network around a convolutional layer, where the kernel is a
    3 x 3-pixel two-dimensional convolution:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，这次我们将更聪明地处理各个层。我们将围绕一个卷积层设计我们的神经网络，其中核是一个3 x 3像素的二维卷积：
- en: '[PRE53]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Model summary
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型摘要
- en: 'You can also visualize the model''s summary, which will list all the layers
    along with their respective dimensions and the number of weights each layer consists.
    It will also provide you with information about the total number of parameters
    (weights and biases) in your network:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以可视化模型的摘要，它将列出所有层及其相应的维度和每个层所包含的权重数量。它还将提供有关网络中总参数数（权重和偏差）的信息：
- en: '![](img/d014b7f0-d32b-4887-a660-1828cfc3e1cc.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d014b7f0-d32b-4887-a660-1828cfc3e1cc.png)'
- en: We can see that there are in total 600,810 parameters that will be trained and
    will require a good amount of computation power! Please note that how we calculate
    the number of parameters in each layer is out of the scope of this book.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，总共有600,810个参数将接受训练，并且需要相当大的计算能力！请注意，我们如何计算每个层的参数数量超出了本书的范围。
- en: Fitting the model
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型拟合
- en: 'We fit the model as we do with all other classifiers (caution, this might take
    a while):'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像处理所有其他分类器一样拟合模型（注意，这可能需要一段时间）：
- en: '[PRE54]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'After training completes, we can evaluate the classifier:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以评估分类器：
- en: '[PRE55]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: And we achieved 99% accuracy! This is worlds apart from the MLP classifier we
    implemented before. And this is just one way to do things. As you can see, neural
    networks provide a plethora of tuning parameters, and it is not at all clear which
    ones will lead to the best performance.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们达到了99%的准确率！这与我们之前实现的MLP分类器相去甚远。而且这只是做事情的一种方式。正如你所见，神经网络提供了大量的调整参数，而且并不清楚哪些参数将导致最佳性能。
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we added a whole bunch of skills to our list as a machine
    learning practitioner. Not only did we cover the basics of artificial neural networks,
    including perceptrons and MLPs, we also got our hands on some advanced deep learning
    software. We learned how to build a simple perceptron from scratch and how to
    build state-of-the-art networks using Keras. Furthermore, we learned about all
    the details of neural nets: activation functions, loss functions, layer types,
    and training methods. All in all, this was probably the most intensive chapter
    yet.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们为机器学习实践者的技能清单增加了很多。我们不仅涵盖了人工神经网络的基础，包括感知器和MLP，我们还接触了一些高级深度学习软件。我们学习了如何从头开始构建一个简单的感知器，以及如何使用Keras构建最先进的网络。此外，我们还了解了神经网络的所有细节：激活函数、损失函数、层类型和训练方法。总的来说，这可能是迄今为止最密集的一章。
- en: Now that you know about most of the essential supervised learners, it is time
    to talk about how to combine different algorithms into a more powerful one. Thus,
    in the next chapter, we will talk about how to build ensemble classifiers.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了大多数基本的有监督学习算法，现在是时候讨论如何将不同的算法组合成一个更强大的算法了。因此，在下一章中，我们将讨论如何构建集成分类器。
