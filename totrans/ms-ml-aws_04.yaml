- en: Classifying Twitter Feeds with Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) plays a major part in analyzing large datasets
    and extracting actionable insights from the data. ML algorithms perform tasks
    such as predicting outcomes, clustering data to extract trends, and building recommendation
    engines. Knowledge of ML algorithms helps data scientists to understand the nature
    of data they are dealing with and plan what algorithms should be applied to achieve
    desired outcomes from the data. Although multiple algorithms are available to
    perform any tasks, it is important for data scientists to know the pros and drawbacks
    of different ML algorithms. The decision to apply ML algorithms can be based on
    various factors, such as the size of the dataset, the budget for the clusters
    used for training and deployment of ML models, and the cost of error rates. Although
    AWS offers a large number of options in terms of selecting and deploying ML models,
    a data scientist has to knowledgeable in terms of what algorithms should be used
    in different situations.'
  prefs: []
  type: TYPE_NORMAL
- en: In this part of the book, we present various popular ML algorithms and examples
    of applications where they can be applied effectively. We will explain the advantages
    and disadvantages of each algorithm and situations when these algorithms should
    be selected in AWS. As this book is written with data science students and professionals
    in mind, we will present a simple example of how the algorithms can be implemented
    using simple Python libraries, and then deployed on AWS clusters using Spark and
    AWS SageMaker for larger datasets. These chapters should help data scientists
    to get familiar with the popular ML algorithms and help them understand the nuances
    of implementing these algorithms in big data environments on AWS clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter
    Feeds with Naive Bayes*, [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml),
    *Predicting House Value with Regression Algorithms*, [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting
    User Behavior with Tree-Based Methods*, and [Chapter 5](ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml), *Customer
    Segmentation Using Clustering Algorithms* present four classification algorithms
    that can be used to predict an outcome based on a feature set. [Chapter 6](c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml), *Analyzing
    Visitor Patterns to Make Recommendations*, explains clustering algorithms and
    demonstrates how they can be used for applications such as customer segmentation.
    [Chapter 7](c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml), *Implementing Deep Learning
    Algorithms*, presents a recommendation algorithm that can be used to recommend
    new items to users based on their purchase history.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce the basics of the Naive Bayes algorithm and present
    a text classification problem that will be addressed by the use of this algorithm
    and language models. We'll provide examples on how to apply it on `scikit-learn`,
    Apache Spark, and on SageMaker's BlazingText. Additionally, we'll explore how
    to further use the ideas behind Bayesian reasoning in more complex scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying text with language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes — pros and cons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the popular subsets of ML algorithms are the classification algorithms.
    They are also referred to as supervised learning algorithms. For this approach,
    we assume that we have a rich dataset of features and events associated with those
    features. The task of the algorithm is to predict an event given a set of features.
    The event is referred to as a class variable. For example, consider the following
    dataset of features related to weather and if it snowed on that day:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 1: Sample dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Temperature (in °F)** | **Sky condition** | **Wind Speed (in MPH)** | **Snowfall**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Less than 20 | Sunny | 30 | False |'
  prefs: []
  type: TYPE_TB
- en: '| 20-32 | Sunny | 6 | False |'
  prefs: []
  type: TYPE_TB
- en: '| 32-70 | Cloudy | 20 | False |'
  prefs: []
  type: TYPE_TB
- en: '| 70 and above | Cloudy | 0 | False |'
  prefs: []
  type: TYPE_TB
- en: '| 20-32 | Cloudy | 10 | True |'
  prefs: []
  type: TYPE_TB
- en: '| 32-70 | Sunny | 15 | False |'
  prefs: []
  type: TYPE_TB
- en: '| Less than 20 | Cloudy | 8 | True |'
  prefs: []
  type: TYPE_TB
- en: '| 32-70 | Sunny | 7 | False |'
  prefs: []
  type: TYPE_TB
- en: '| 20-32 | Cloudy | 11 | False |'
  prefs: []
  type: TYPE_TB
- en: '| Less than 20 | Sunny | 13 | True |'
  prefs: []
  type: TYPE_TB
- en: In the dataset, a weather station has information about the temperature, the
    sky condition, and the wind speed for the day. They also have records of when
    they received snowfall. The classification problem they are working on is to predict
    snowfall based on features such as temperature, sky condition, and wind speed.
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss some terminology that is used in ML datasets. For the example
    table, if the classification problem is to predict snowfall, then the snowfall
    feature is referred to as a **class** or **target** variable. Non-class values
    are referred to as attribute or feature variables. Each row in this dataset is
    referred to as an observation.
  prefs: []
  type: TYPE_NORMAL
- en: Feature types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are three types of features that are available in a classification dataset.
    The reason why data scientists need to be able to differentiate between different
    features is that not every ML algorithm supports each type of feature. So, if
    the type of feature set does not match the desired algorithm, then the features
    need to be preprocessed to transform the feature that the classification algorithm
    can process.
  prefs: []
  type: TYPE_NORMAL
- en: Nominal features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Nominal** or **categorical** features are features that can have a finite
    set of categorical values, and these values cannot be ordered in any specific
    order. In the example dataset, the **sky condition** feature is a nominal feature.
    In the table, the value of the nominal feature is either **Sunny** or **Cloudy**.
    Other examples of nominal features are gender and color. Nominal features can
    be converted into continuous variables by using techniques such as one-hot encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: Ordinal features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Ordinal** features, similar to nominal features, also have a finite set of
    categorical values. However, unlike nominal features, these categorical values
    can be put into a specific order. In the previous example, the **Temperature** feature is
    an ordinal feature. The labels in this category can be ordered from coldest to
    warmest. Ordinal features can be converted into continuous variables by interpolating
    the range values to a defined scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Continuous** features can have infinite possible values. Unlike nominal and
    ordinal features, which can only have a discrete set of values, continuous variables
    are numerical variables, and are not compatible with some ML algorithms. However,
    continuous features can be converted into ordinal features using a technique called
    **discretization**.'
  prefs: []
  type: TYPE_NORMAL
- en: Although we will not discuss techniques to transform features from one form
    to another here, we will demonstrate how it can be done in our example sections.
    We have selected example datasets in this book where feature transformation is
    required. You should not only learn about these various transformation techniques
    from this book, but also observe how a data scientist analyzes a dataset and uses
    specific feature transformation techniques based on the application. We have also
    provided examples to apply these techniques at scale in Python and AWS SageMaker.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naïve Bayes classifier is a ML algorithm based on Bayes' theorem. The algorithm
    is comparable to how a belief system evolves. Bayes' theorem was initially introduced
    by an English mathematician, Thomas Bayes, in 1776\. This algorithm has various
    applications, and has been used for many historic tasks for more than two centuries.
    One of the most famous applications of this algorithm was by Alan Turing during
    the Second World War, where he used Bayes' theorem to decrypt the German Enigma
    code. Bayes' theorem has also found an important place in ML for algorithms such
    as Bayesian Net and Naive Bayes algorithm. Naïve Bayes algorithm is very popular
    for ML due to its low complexity and transparency in why it makes the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will first introduce Bayes' theorem and demonstrate how
    it is applied in ML.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem calculates the probability of an event given a condition, such
    that we have prior knowledge about the event, the condition, and the probability
    of the condition when the event occurs. In our snow prediction example, the event
    is when snow occurs. A condition would be when the temperature is between 20°F
    and 32°F. And, based on the data, we can calculate the likelihood of temperature
    being 20°F and 32°F when it snows. Using this data, we can predict the probability
    of snow given the temperature being between 20°F and 32°F.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that we have a class variable *C* and a condition variable *x*. Bayes'
    theorem is presented in formula 1\. We also present a given simple way to remember
    different components of the algorithm in formula 2.
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula 1**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/761e081a-97d5-4bec-811c-c36b7bc1e555.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Formula 2 **'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/024ae2dd-62e4-410d-b8d1-f6fdf5c78ee4.png)'
  prefs: []
  type: TYPE_IMG
- en: There are four terms that you need to remember from this formula.
  prefs: []
  type: TYPE_NORMAL
- en: Posterior
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **posterior** probability is the chance of an event occurring given the
    existence of feature variable *x*.
  prefs: []
  type: TYPE_NORMAL
- en: Likelihood
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Likelihood** is the probability of a condition occurring for a given event.
    In our example, likelihood means what the probability is of the temperature being
    between 20°F to 32°F when it snows. Based on the data in the dataset, there is
    a 66.66% probability that the temperature is 20°F-30°F when it snows. Training
    data can be used to calculate the probability of each discrete value in the feature
    set.'
  prefs: []
  type: TYPE_NORMAL
- en: Prior probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **prior** probability is the overall probability of the event in the dataset.
    In our example, this would be the overall probability that it snows in the dataset.
    Prior probability is important in cases where the datasets are unbalanced, that
    is, the number of instances of one class variable in the dataset is significantly
    higher than the other. This leads to bias in the likelihood variable. Prior probabilities
    are used to renormalize these probabilities by taking the bias in the dataset
    into account. For example, in our dataset, the prior probability of a snow event
    is 30% and the prior probability of it not snowing is 70%. The probability of
    cloudy conditions when it snows is 66%, while the likelihood of cloudy conditions
    when it does not snow is 42.8%.
  prefs: []
  type: TYPE_NORMAL
- en: However, by taking the prior probabilities into account, although cloudy conditions
    are more likely when it snows than when it does not, after multiplying the priors,
    the posterior probability of snow when it is cloudy is 19% and the probability
    of not snowing when it is cloudy is 30%. By multiplying the prior probabilities
    to the likelihood events, we inform our posterior probability that there is a
    higher probability of it not snowing than snowing.
  prefs: []
  type: TYPE_NORMAL
- en: Evidence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **evidence** variable is the probability of a condition in the dataset.
    In our example, the probability of temperature being 70°F or above is only 10%.
    Rare events have low evidence probability. Evidence probabilities boost posterior
    probabilities of rare events. For the purpose of the Naïve Bayes classifier, we
    do not need to consider the evidence variable, since it is not dependent on the
    class variable.
  prefs: []
  type: TYPE_NORMAL
- en: So, Bayes' theorem is used to calculate the probability of an event given a
    single condition. However, when we train ML algorithms, we use one or more features
    to predict the probability of an event. In the next section, we will explain Naïve
    Bayes algorithm and how it utilizes posterior probabilities of multiple features
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: How the Naive Bayes algorithm works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Naive Bayes algorithm uses Bayes' theorem to calculate the posterior probability
    of every condition in the dataset and uses these probabilities to calculate the
    conditional probability of an event given a set of conditions. The Naive Bayes
    algorithm assumes that each conditional feature is independent of each other.
    This is an important assumption that helps simplify how the conditional probability
    is calculated. The independence assumption is the reason why the algorithm gets
    the name, Naive Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, instead of considering one *x* feature variable, we consider
    a vector of features,![](img/0905d81e-d283-452c-933e-1cc360270d18.png), where
    *n* is the number of feature variables used to calculate the class probability.
    We represent the conditional probability of a class variable for the *x* vector
    in formula 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula 3**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7156a8e-ff2a-4255-b184-d9ef34deec26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we have assumed that each feature variable is independent of each other,
    the conditional probability of a class variable can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula 4**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e7880e7-9ad5-4522-98d0-14d8be8c20c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on posterior probability calculations shown in the previous sections,
    this formula can be rewritten as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula 5**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6a3509a-c92f-4079-94dc-312c679f4663.png)'
  prefs: []
  type: TYPE_IMG
- en: Formula 5 explains how a probability of event *C* is calculated based on the ![](img/0905d81e-d283-452c-933e-1cc360270d18.png) feature
    variables. An interesting thing to note in this formula is how easy it is to calculate
    each element from the dataset. Also, since the evidence probability from Bayes'
    theorem is not dependent on the class variable, it is not used in the Naive Bayes
    formula.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Naive Bayes algorithm only requires one pass over the dataset during the
    training phase to calculate the probability of the value of a feature for each
    event. During the prediction phase, we calculate the probability of each event
    given the instance of the features and predict the event with the highest probability.
    Formula 6 shows how the prediction of a Naïve Bayes classifier is calculated when
    *k* events are possible. **Argmax** in the formula means that the event with maximum
    probability is selected as the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '** Formula 6**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb33aabc-cbc3-4a34-89f0-702644ccea50.png)'
  prefs: []
  type: TYPE_IMG
- en: Naïve Bayes classifier is a multiclass classifier that can be used to train
    on a dataset where two or more class variables need to be predicted. In the next
    chapters, we will present some examples of binary classifiers that only work with
    two class variables needs to be predicted. However, we will show you the methodologies
    of applying binary classifiers to multiclass problems.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying text with language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text classification is an application of classification algorithms. However,
    the text is a combination of words in a specific order. Hence, you can observe
    that a text document with a class variable is not similar to the dataset that
    we presented in table 1, in the *Classification algorithms* section.
  prefs: []
  type: TYPE_NORMAL
- en: A text dataset can be represented as shown in table 2.
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 2: Example of a Twitter dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tweet** | **Account** |'
  prefs: []
  type: TYPE_TB
- en: '| The simplest way to protect Americans from gun violence is to actually talk
    about common-sense gun laws. | Democrats |'
  prefs: []
  type: TYPE_TB
- en: '| This cannot be who we are as a country. We need to find out what happened
    and ensure it never happens again ([https://t.co/RiY7sjMfJK)](https://t.co/RiY7sjMfJK))  |
    Democrats |'
  prefs: []
  type: TYPE_TB
- en: '| Over the weekend, President Trump visited Arlington National Cemetery to
    honor fallen soldiers. | Republicans |'
  prefs: []
  type: TYPE_TB
- en: '| This President has made it clear that he will secure this country—`@SecNielsen`.
    | Republicans |'
  prefs: []
  type: TYPE_TB
- en: For this chapter, we have built a dataset based on tweets from two different
    accounts. We also have provided code in the following sections so that you can
    create your own datasets to try this example. Our purpose is to build a smart
    application that is capable of predicting the source of a tweet just by reading
    the tweet text. We will collect several tweets by the United States Republican
    Party (`@GOP`) and the Democratic Party (`@TheDemocrats`) to build a model that
    can predict which party wrote a given tweet. In order to do this, we will randomly
    select some tweets from each party and submit them through the model to check
    whether the prediction actually matched reality.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting the tweets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start by using the `Twython` library to access the Twitter API and collect
    a series of tweets, labeling them with the originating political party.
  prefs: []
  type: TYPE_NORMAL
- en: 'The details of the implementation can be found in our GitHub repository in
    the following Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '`chapter2/collect_tweets.ipynb`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to invoke the following method in the `Twython` library to save tweets
    from `@GOP` and `@TheDemocrats` onto some text files, `gop.txt` and `dems.txt`
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Each file contains 200 tweets. The following are some excerpts from the `dems.txt`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`This cannot be who we are as a country. We need to find out what happened
    and ensure it never happens again.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RT @AFLCIO: Scott Walker. Forever a national disgrace.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have the source data in text files, we need to convert it to a
    format that can be used as an input for a ML library. Most general-purpose ML
    packages, such as `scikit-learn` and Apache Spark, only accept a matrix of numbers
    as input. Hence, feature transformation is required for a text dataset. A common
    approach is to use language models such as **bag of words** (**BoW**). In this
    example, we build a BoW for each tweet and construct a matrix in which each row
    represents a tweet and each column signals the presence of a particular word.
    We also have a column for the label that can distinguish tweets from `Republicans`
    (`1`) or `Democrats` (`0`), as we can see in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Table 3: Converting text dataset to structured dataset**'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Immigration** | **Medicaid** | **Terrorism** | **Class** |'
  prefs: []
  type: TYPE_TB
- en: '| Tweet 1 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Tweet 2 | 1 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Tweet 3 | 0 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: Table 2 represents the matrix that can be derived from tweets. However, there
    are many points to remember when generating such a matrix. Due to the number of
    terms in the language lexicon, the number of columns in the matrix can be very
    high. This poses a problem in ML known as the **curse of dimensionality** (see
    section *X*). There are several ways to tackle this problem; however, as our example
    is fairly small in terms of data, we will only briefly discuss methods to reduce
    the number of columns.
  prefs: []
  type: TYPE_NORMAL
- en: '**Stopwords**: Certain common words might add no value to our task (for example,
    the words **the**, **for**, or **as**). We call these words stopwords, and we
    shall remove these words from `dems.txt` and `gop.txt`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stemming**: There may be many variants of a word that are used in the text.
    For example, argue, argued, argues, and arguing all stem from the word **argue**.
    Techniques such as stemming and lemmatization can be used to find the stem of
    the word and replace variants of that word with the stem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenization**: Tokenization can be used to combine various words into phrases
    so that the number of features can be reduced. For example, **tea party** has
    a totally different meaning, politically, than the two words alone. We won''t
    consider this for our simple example, but tokenization techniques help in finding
    such phrases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another issue to consider is that words appearing more than once in a tweet
    have equal importance on a training row. There are ways to utilize this information
    by using multinomial or term frequency-inverse document frequency (TFIDF) models.
    Since tweets are relatively short text, we will not consider this aspect in our
    implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The table 2 matrix describes the words you would find for each class (that
    is each political party). However, when we want to predict the source of the tweet,
    the inverse problem is posed. Given a specific bag of words, we''re interested
    in assessing how likely it is that the terms are used by one party or another.
    In other words, we know the probability of a bag of words given a particular party,
    and we are interested in the reverse: the probability of a tweet being written
    by a party given a bag of words. This is where the Naive Bayes algorithm is applied.'
  prefs: []
  type: TYPE_NORMAL
- en: Building a Naive Bayes model through SageMaker notebooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get started with SageMaker notebooks. This tool will help us run the code
    that will train our model. SageMaker, among other things, allows us to create
    notebook instances that host Jupyter Notebooks. Jupyter is a web UI that allows
    a data scientist or programmer to code interactively by creating paragraphs of
    code that are executed on demand. It works as an IDE, but with the additional
    ability to render the output of the code in visually relevant forms (for example,
    charts, tables, and markdown), and also supports writing paragraphs in different
    languages within the same notebook. We will use notebooks extensively throughout
    this book, and we recommend its use as a way to share and present data science
    findings. It allows users to achieve reproducible research, as the code necessary
    for a particular research objective can be validated and reproduced by re-running
    the code paragraphs in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more on SageMaker's AWS console page at [https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/dashboard](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/dashboard).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at what Sagemaker''s AWS console page looks in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89acc840-4b2c-4737-9743-852afedb151e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Add repository, choose your authentication mechanism and add the repository
    found at [https://github.com/mg-um/mastering-ml-on-aws](https://github.com/mg-um/mastering-ml-on-aws):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e93d7ec-84f0-4db1-9faf-a88ff6164296.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Before creating the notebook instance, it is possible that you would want to
    attach a Git repository so that the notebooks available with this book are attached
    to the notebook, and so are made available immediately as you will see later:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8af5a0cb-2ad3-4f82-bdd2-8b8a71f5cdcc.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now proceed to launch a notebook instance. There are several options
    to configure the hardware, networking, and security of the server that will host
    the notebook. However, we will not go into much detail for now, and will accept
    the defaults. The AWS documentation is an excellent resource if you want to limit
    the access or power-up your machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we attached the Git repository, once you open Jupyter, you should see
    the notebooks we created for this book, and you can re-run them, modify them,
    or improve them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2aedb20-c8a9-4ccb-94c6-a5d0d253f08b.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we focus on the `train_scikit` Python notebook and go over
    code snippets to explain how we can build and test a model for out tweet classification
    problem. We encourage you to run all the paragraphs of this notebook to get an
    idea of the purpose of this notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we will do is load the stopwords and the two sets of tweets into
    variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then proceed to use the utilities in `scikit-learn` to construct our
    matrix. In order to do that, we will use a `CountVectorizer` class, which is a
    class that knows how to allocate the different words into columns while at the
    same time filtering the stopwords. We will consider both sets of tweets; for our
    example, we''ll just use the first `1200` words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Through `vectorizer` we can now construct two matrices, one for republican
    party tweets and one for democratic party tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'These two bag-of-words matrices (`dem_bow` and `gop_bow`) are represented in
    a sparse data structure to minimize memory usage, but can be examined by converting
    them to arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to train our model, we need to provide two arrays. The BoWs matrix
    (for both parties), which we will call `x`, and the labels (class variables) for
    each of the tweets. To construct this, we will vertically stack both matrices
    (for each party):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To construct the labels vector, we will just assemble a vector with `ones`
    for `Democrat` positions and `zeros` for `Republican` positions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we train our models, we will split the tweets (rows on our `x` matrix)
    randomly, so that some are used to build a model and others are used to check
    whether the model predicts the correct political party (label):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our training and testing datasets, we proceed to train our
    model using Naive Bayes (a Bernoulli Naive Bayes, since our matrices are ones
    or zeros):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the preceding code, it is very simple to fit a Naive Bayes
    model. We need to provide the training matrices and the labels. A model is now
    capable of predicting the label (political party) of arbitrary tweets (as long
    as we have them as a BoWs matrix representation). Fortunately, we had separated
    some of the tweets for testing, so we can run these through the model and see
    how often the model predicts the right label (note that we know the actual party
    that wrote the tweet for every tweet in the testing dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the predictions it''s as simple as invoking the `predict` method of
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can see how many of the predictions match the ground truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The output score of the code block is `0.95`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we are using accuracy as an evaluation metric. Accuracy can
    be calculated using formula 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Formula 7**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b450b51-a65b-4f29-b4dd-a7447a36b262.png)'
  prefs: []
  type: TYPE_IMG
- en: There are various evaluation metrics that a data scientist can use to evaluate
    ML algorithm. We will present evaluation measures such as precision, recall, F1
    measure, **root mean squared error** (**RMSE**), and **area under curve** (**AUC**) in
    our next chapters for different examples. Evaluation metrics should be selected
    based on the business need of implementing an algorithm, and should indicate whether
    or not the ML algorithm is performing at the standards required to achieve a task.
  prefs: []
  type: TYPE_NORMAL
- en: Since this is the first example we are working on, we will use the simplest
    evaluation measure, which is accuracy. As specified in formula 7, accuracy is
    the ratio of correct predictions to the total number of predictions made by the
    classifier. It turns out that our Naive Bayes model is very accurate, with an
    accuracy of 95%. It is possible that some words, such as the names of members
    of each party, can quickly make the model give a correct prediction. We will explore
    this using decision trees in [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml),
    *Predicting User Behavior with Tree-Based Methods*.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, during this process, we had to prepare and transform the data in
    order to fit a model. This process is very common, and both `scikit-learn` and
    Spark support the concept of pipelines, which allow the data scientist to declare
    the necessary transformations needed to build a model without having to manually
    obtain intermediary results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code snippet, we can see an alternative way to produce the
    same model by creating a pipeline with the following two stages:'
  prefs: []
  type: TYPE_NORMAL
- en: Count vectorizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naive Bayes trainer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This allows our modeling to be a bit more concise and declarative. By calling
    the `pipeline.fit()` method, the library applies any necessary transformations
    or estimations necessary. Note that, in this case, we split the raw texts (rather
    than the matrices) as the `fit()` method now receives the raw input. As we shall
    see in the next section, pipelines can contain two kinds of stages, Transformers
    and Estimators, depending on whether the stage needs to compute a model out of
    the data, or simply transform the data declaratively.
  prefs: []
  type: TYPE_NORMAL
- en: Naïve Bayes model on SageMaker notebooks using Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section *Classifying text with language models*, we saw how
    you can train a model with `scikit-learn` on a SageMaker notebook instance. This
    is feasible for examples as small as the ones we collected from Twitter. What
    if, instead, we had hundreds of terabytes worth of tweet data? For starters, we
    would not be able to store the data in a single machine. Even if we could, it
    would probably take too long to train on such large dataset. Apache Spark solves
    this problem for us by implementing ML algorithms that can read data from distributed
    datasets (such as AWS S3) and can distribute the computing across many machines.
    AWS provides a product called **Elastic MapReduce** (**EMR**) that is capable
    of launching and managing clusters on which we can perform ML at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the ML algorithms require several passes over the data (although this
    is not the case for Naive Bayes). Apache Spark provides a way to cache the datasets
    in memory, so that one can efficiently run algorithms that require several passes
    over the data (such as **logistic regression** or **decision trees**, which we
    will see in the following chapters). We will show how to launch EMR clusters in
    [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting User Behavior
    with Tree-Based Methods*, however, in this section, we will present how similar
    it is to work with Apache Spark compared to `scikit-learn`. In fact, many of the
    interfaces in Apache Spark (such as pipelines, Transformers, and Estimators) were
    inspired by `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark supports four main languages: R, Python, Scala, and Java. In this
    book we will use the Python flavor, also called PySpark. Even though our spark
    code will run on a single machine (that is, will run on our SageMaker notebook
    instance), it could run on multiple machines without any code changes if our data
    was larger and we had a Spark Cluster (in [Chapter 4](https://cdp.packtpub.com/mastering_machine_learning_on_aws/wp-admin/post.php?post=25&action=edit#post_27), *Predicting
    User Behavior with Tree-Based Methods*, we will dive into creating Spark Clusters
    with EMR).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Spark, the first thing we need to do is to create a Spark session. We do
    this by first creating a Spark context, and then creating a session for SQL-like
    manipulation of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Since we will run Spark locally (on a single machine) we specify `local`. However,
    if we were to run this on a cluster, we would need to specify the master address
    of the cluster instead. Spark works with abstractions called DataFrames that allow
    us to manipulate huge tables of data using SQL-like operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first task will be to define DataFrames for our raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first two lines, we create DataFrames out of our raw tweets. We also
    create `corpus_df`, which contains both sources of tweets, and add the label by
    creating a column with a literal of `1` for Democrats and `0` for `Republicans`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Spark works in a lazy fashion, so, even though we defined and unioned the DataFrame,
    no actual processing will happen until we perform the first operation on the data.
    In our case, this will be the splitting of the DataFrame into testing and training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to train our model. Spark supports the same concept of pipelines.
    We will build a pipeline with the necessary transformations for our model. It''s
    very similar to our previous example, except that Spark has two separate stages
    for tokenization and stop words remover:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: A Spark ML pipeline consists of a series of stages. Each stage can be a Transformer
    or an Estimator. Transformers apply a well-defined transformation on a dataset,
    while Estimators have the added capability of producing models by traversing the
    dataset. `NaiveBayes` and `CountVectorizer` are examples of Estimators, while
    tokenizer and `StopWordsRemover` are examples of Transformers. Models, in turn,
    are Transformers, because they can provide predictions for all elements in a dataset
    as a transformation.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding code, we defined a pipeline with all the necessary
    stages to clean the data. Each stage will transform the original DataFrame (which
    only has two columns value, which are the raw tweet text and label) and add more
    columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, the relevant columns used at training time are the features
    (a sparse vector representing the BoWs exactly like our `scikit-learn` example)
    and the label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying these columns to the `NaiveBayes` classifier we can train a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is a transformer that can provide predictions for each row in our
    training DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to our previous example, we can evaluate the accuracy of our models.
    By using the `MulticlassClassificationEvaluator` class and specifying the actual
    and predicted labels, we can obtain `accuracy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The output is 0.93, which is similar to the results we had on `scikit-learn`.
  prefs: []
  type: TYPE_NORMAL
- en: Using SageMaker's BlazingText built-in ML service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw how to perform ML tasks using `scikit-learn` and Apache Spark libraries.
    However, sometimes it's more appropriate to use a ML service*. *SageMaker provides
    ways for us to create, tune, and deploy models supporting a variety of built-in
    ML algorithms by just invoking a service. In a nutshell, you need to place the
    data in S3 (an Amazon service to store large amounts of data) and call the SageMaker
    service providing all the necessary details (actual ML algorithm, the location
    of the data, which kind and how many machines should be used for training). In
    this section, we go through the process of training our model for predicting tweets
    through SageMaker's BlazingText ML service. BlazingText is an algorithm that supports
    text classification using word2vec, which is a way to transform words into vectors
    in a way that captures precise syntactic and semantic word relationships. We won't
    dive into the details of SageMaker's architecture yet, but we will present the
    reader how we would use this AWS service as an alternative to `scikit-learn` or
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by importing the SakeMaker libraries, creating a session, and
    obtaining a role (which is the role that the notebook instance is using (see [https://aws.amazon.com/blogs/aws/iam-roles-for-ec2-instances-simplified-secure-access-to-aws-service-apies-from-ec2](https://aws.amazon.com/blogs/aws/iam-roles-for-ec2-instances-simplified-secure-access-to-aws-service-apies-from-ec2)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we specify the S3 bucket we will be using to store all our data
    and models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to put some data in S3 for training. The expected format for
    BlazingText is to have each line in the `__label__X TEXT ` format. In our case,
    this means prefixing each tweet by a label representing the originating party:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To do that, we perform some preprocessing of our tweets and prefix the right
    label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed to create the sets for training and testing as text files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have our training and validation text files, we upload them into `S3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed to instantiate `Estimator`, by specifying all the necessary
    details: the type and amount of machines to be used for training, as well as the
    location of the path in `S3` where the models will be stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'As we discussed in the previous section *Naive Bayes model on SageMaker notebooks
    using Apache Spark* section, an estimator is capable of creating models by processing
    training data. The next step will be to fit the model providing the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Before we train the model we need to specify the hyperparameters. We won't go
    into much detail about this algorithm in this section, but the reader can find
    the details in [https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'This particular algorithm also takes the validation data, as it runs over the
    data several times (epochs) to improve the error. Once we fit the model, we can
    deploy the model as a web service so that applications can use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'In our case, we will just hit the endpoint to get the predictions and evaluate
    the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the preceding code, each prediction comes along with a probability
    (which we will ignore for now). Next, we compute how many of these labels matched
    the original one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then run the next line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the following output from the previous code block, some of
    the labels matched the actual while some don''t:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run the following code to build a boolean vector containing true or
    false depending on whether the actual matches the predicted result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After we run the preceding output, we will run the following code to calculate
    the ratio of cases that match out of the total instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output from the previous block shows the accuracy score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the accuracy is lower than in our previous examples. This is
    for many reasons. For starters, we did not invest too much in data preparation
    in this case (for example, no stopwords are used in this case). However, the main
    reason for the lower accuracy is due to the fact we're using such little data.
    These models work best on larger datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes – pros and cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we present the advantages and disadvantages in selecting the
    Naive Bayes algorithm for classification problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training time**:Naive Bayes algorithm only requires one pass on the entire
    dataset to calculate the posterior probabilities for each value of the feature
    in the dataset. So, when we are dealing with large datasets or low-budget hardware,
    Naive Bayes algorithm is a feasible choice for most data scientists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction time**: Since all the probabilities are pre-computed in the Naive
    Bayes algorithm, the prediction time of this algorithm is very efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency**: Since the predictions of Naive Bayes algorithms are based
    on the posterior probability of each conditional feature, it is easy to understand
    which features are influencing the predictions. This helps users to understand
    the predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction accuracy**: The prediction accuracy of the Naive Bayes algorithm
    is lower than other algorithms we will discuss in the book. Algorithm prediction
    accuracy is dataset dependent, a lot of research works have proved that algorithms
    such as random forest, **support vector machines** (**SVMs**), and **deep neural
    networks** (**DNNs**) outperform Naive Bayes algorithm in terms of classification
    accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Assumption of independence**: Since we assume that each feature is independent
    of each other, this algorithm may lose information for features that are dependent
    on each other. Other advanced algorithms do use this dependence information when
    calculating predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced you to why ML is a crucial tool in a data scientist's
    repository. We discussed what a structured ML dataset looks like and how to identify
    the types of features in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We took a deep dive into Naive Bayes classification algorithm, and studied how
    Bayes' theorem is used in Naive Bayes algorithm. Using Bayes' theorem, we can
    predict the probability of an event occurring based on the values of each feature,
    and select the event that has the highest probability.
  prefs: []
  type: TYPE_NORMAL
- en: We also presented an example of a Twitter dataset. We hope that you learned
    how to think about a text classification problem, and  how to build a Naive Bayes
    classification model to predict the source of a tweet. We also presented how the
    algorithm can be implemented in SageMaker, and how it can also be implemented
    using Apache Spark. This code base should help you tackle any text classification
    problems in the future. As the implementation is presented using SageMaker services
    and Spark, it can scale to datasets that can be gigabytes or terabytes in size.
  prefs: []
  type: TYPE_NORMAL
- en: We will look at how to deploy the ML models on actual production clusters in
    later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bayes' Theorem is not only useful for the Naive Bayes algorithm, but is also
    used for other purposes. Find two more algorithms where Bayes' theorem is applied,
    and explain how they are different than the Naive Bayes algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this chapter, we presented an example of a binary classifier. Based on our
    code to download tweets, create a new dataset where you download tweets from five
    different sources and build a Naive Bayes model that can predict the source of
    each tweet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify scenarios for when you would use `scikit-learn`, Apache Spark, or SageMaker
    services for a particular problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
