- en: Classifying Twitter Feeds with Naive Bayes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯分类Twitter流
- en: '**Machine learning** (**ML**) plays a major part in analyzing large datasets
    and extracting actionable insights from the data. ML algorithms perform tasks
    such as predicting outcomes, clustering data to extract trends, and building recommendation
    engines. Knowledge of ML algorithms helps data scientists to understand the nature
    of data they are dealing with and plan what algorithms should be applied to achieve
    desired outcomes from the data. Although multiple algorithms are available to
    perform any tasks, it is important for data scientists to know the pros and drawbacks
    of different ML algorithms. The decision to apply ML algorithms can be based on
    various factors, such as the size of the dataset, the budget for the clusters
    used for training and deployment of ML models, and the cost of error rates. Although
    AWS offers a large number of options in terms of selecting and deploying ML models,
    a data scientist has to knowledgeable in terms of what algorithms should be used
    in different situations.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（ML）在分析大数据集和从数据中提取可操作见解方面发挥着重要作用。ML算法执行预测结果、聚类数据以提取趋势和构建推荐引擎等任务。了解ML算法有助于数据科学家了解他们处理的数据的性质，并计划应用哪些算法以从数据中获得期望的结果。尽管有多种算法可用于执行任何任务，但数据科学家了解不同ML算法的优缺点非常重要。应用ML算法的决定可以基于各种因素，例如数据集的大小、用于训练和部署ML模型的集群预算以及错误率成本。尽管AWS在选择和部署ML模型方面提供了大量选项，但数据科学家必须了解在不同情况下应使用哪些算法。'
- en: In this part of the book, we present various popular ML algorithms and examples
    of applications where they can be applied effectively. We will explain the advantages
    and disadvantages of each algorithm and situations when these algorithms should
    be selected in AWS. As this book is written with data science students and professionals
    in mind, we will present a simple example of how the algorithms can be implemented
    using simple Python libraries, and then deployed on AWS clusters using Spark and
    AWS SageMaker for larger datasets. These chapters should help data scientists
    to get familiar with the popular ML algorithms and help them understand the nuances
    of implementing these algorithms in big data environments on AWS clusters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，我们介绍了各种流行的ML算法以及它们可以有效地应用的示例。我们将解释每个算法的优点和缺点，以及在这些算法应该在AWS中选择的情况。鉴于本书是为数据科学学生和专业人士编写的，我们将通过一个简单的示例展示如何使用简单的Python库实现这些算法，然后使用Spark和AWS
    SageMaker在更大的数据集上部署。这些章节应有助于数据科学家熟悉流行的ML算法，并帮助他们了解在AWS集群的大数据环境中实现这些算法的细微差别。
- en: '[Chapter 2](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml), *Classifying Twitter
    Feeds with Naive Bayes*, [Chapter 3](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml),
    *Predicting House Value with Regression Algorithms*, [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting
    User Behavior with Tree-Based Methods*, and [Chapter 5](ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml), *Customer
    Segmentation Using Clustering Algorithms* present four classification algorithms
    that can be used to predict an outcome based on a feature set. [Chapter 6](c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml), *Analyzing
    Visitor Patterns to Make Recommendations*, explains clustering algorithms and
    demonstrates how they can be used for applications such as customer segmentation.
    [Chapter 7](c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml), *Implementing Deep Learning
    Algorithms*, presents a recommendation algorithm that can be used to recommend
    new items to users based on their purchase history.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '[第二章](9163133d-07bc-43a6-88e6-c79b2187e257.xhtml)，*使用朴素贝叶斯分类Twitter流*，[第三章](eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml)，*使用回归算法预测房屋价值*，[第四章](af506fc8-f482-453e-8162-93a676b2e737.xhtml)，*使用基于树的算法预测用户行为*，以及[第五章](ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml)，*使用聚类算法进行客户细分*，介绍了四种分类算法，这些算法可以根据特征集预测结果。[第六章](c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml)，*分析访问模式以提供建议*，解释了聚类算法，并展示了它们如何用于客户细分等应用。[第七章](c832a5c1-d877-4c90-bfb5-e3a0fe99d19a.xhtml)，*实现深度学习算法*，介绍了一种推荐算法，可以根据用户的购买历史向用户推荐新项目。'
- en: This chapter will introduce the basics of the Naive Bayes algorithm and present
    a text classification problem that will be addressed by the use of this algorithm
    and language models. We'll provide examples on how to apply it on `scikit-learn`,
    Apache Spark, and on SageMaker's BlazingText. Additionally, we'll explore how
    to further use the ideas behind Bayesian reasoning in more complex scenarios.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍朴素贝叶斯算法的基础知识，并展示一个将通过使用此算法和语言模型来解决的问题。我们将提供如何在 `scikit-learn`、Apache Spark
    和 SageMaker 的 BlazingText 上应用它的示例。此外，我们还将探讨如何在更复杂的场景中进一步使用贝叶斯推理背后的思想。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Classification algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类算法
- en: Naive Bayes classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: Classifying text with language models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语言模型进行文本分类
- en: Naive Bayes — pros and cons
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯 — 优点和缺点
- en: Classification algorithms
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类算法
- en: 'One of the popular subsets of ML algorithms are the classification algorithms.
    They are also referred to as supervised learning algorithms. For this approach,
    we assume that we have a rich dataset of features and events associated with those
    features. The task of the algorithm is to predict an event given a set of features.
    The event is referred to as a class variable. For example, consider the following
    dataset of features related to weather and if it snowed on that day:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法中流行的一个子集是分类算法。它们也被称为监督学习算法。对于这种方法，我们假设我们有一个丰富的特征和事件数据集。算法的任务是根据一组特征预测一个事件。事件被称为类别变量。例如，考虑以下与天气相关的特征数据集，以及那天是否下雪：
- en: '**Table 1: Sample dataset**'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 1：样本数据集**'
- en: '| **Temperature (in °F)** | **Sky condition** | **Wind Speed (in MPH)** | **Snowfall**
    |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **温度 (华氏度)** | **天空状况** | **风速 (英里/小时)** | **降雪量** |'
- en: '| Less than 20 | Sunny | 30 | False |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 小于 20 | 晴朗 | 30 | 否 |'
- en: '| 20-32 | Sunny | 6 | False |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 20-32 度 | 晴朗 | 6 | 否 |'
- en: '| 32-70 | Cloudy | 20 | False |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 32-70 度 | 多云 | 20 | 否 |'
- en: '| 70 and above | Cloudy | 0 | False |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 70 度以上 | 多云 | 0 | 否 |'
- en: '| 20-32 | Cloudy | 10 | True |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 20-32 度 | 多云 | 10 | 是 |'
- en: '| 32-70 | Sunny | 15 | False |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 32-70 度 | 晴朗 | 15 | 否 |'
- en: '| Less than 20 | Cloudy | 8 | True |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 小于 20 | 多云 | 8 | 是 |'
- en: '| 32-70 | Sunny | 7 | False |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 32-70 度 | 晴朗 | 7 | 否 |'
- en: '| 20-32 | Cloudy | 11 | False |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 20-32 度 | 多云 | 11 | 否 |'
- en: '| Less than 20 | Sunny | 13 | True |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 小于 20 | 晴朗 | 13 | 是 |'
- en: In the dataset, a weather station has information about the temperature, the
    sky condition, and the wind speed for the day. They also have records of when
    they received snowfall. The classification problem they are working on is to predict
    snowfall based on features such as temperature, sky condition, and wind speed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，气象站有关于该日温度、天空状况和风速的信息。他们还有关于何时收到降雪的记录。他们正在工作的分类问题是根据温度、天空状况和风速等特征预测降雪。
- en: Let's discuss some terminology that is used in ML datasets. For the example
    table, if the classification problem is to predict snowfall, then the snowfall
    feature is referred to as a **class** or **target** variable. Non-class values
    are referred to as attribute or feature variables. Each row in this dataset is
    referred to as an observation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下在机器学习数据集中使用的术语。对于示例表，如果分类问题是预测降雪，那么降雪特征被称为**类别**或**目标**变量。非类别值被称为属性或特征变量。数据集中的每一行被称为一个观测值。
- en: Feature types
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征类型
- en: There are three types of features that are available in a classification dataset.
    The reason why data scientists need to be able to differentiate between different
    features is that not every ML algorithm supports each type of feature. So, if
    the type of feature set does not match the desired algorithm, then the features
    need to be preprocessed to transform the feature that the classification algorithm
    can process.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类数据集中有三种类型的特征可用。数据科学家需要能够区分不同特征的原因是，并非每个机器学习算法都支持每种类型的特征。因此，如果特征集的类型与期望的算法不匹配，那么特征就需要进行预处理，以转换成分类算法可以处理的特征。
- en: Nominal features
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 名义特征
- en: '**Nominal** or **categorical** features are features that can have a finite
    set of categorical values, and these values cannot be ordered in any specific
    order. In the example dataset, the **sky condition** feature is a nominal feature.
    In the table, the value of the nominal feature is either **Sunny** or **Cloudy**.
    Other examples of nominal features are gender and color. Nominal features can
    be converted into continuous variables by using techniques such as one-hot encoding.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**名义**或**分类**特征是具有有限个分类值的特征，这些值不能按任何特定顺序排列。在示例数据集中，**天空状况**特征是一个名义特征。在表中，名义特征的值要么是**晴朗**，要么是**多云**。其他名义特征的例子包括性别和颜色。名义特征可以通过使用如独热编码等技术转换为连续变量。'
- en: Ordinal features
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 序列特征
- en: '**Ordinal** features, similar to nominal features, also have a finite set of
    categorical values. However, unlike nominal features, these categorical values
    can be put into a specific order. In the previous example, the **Temperature** feature is
    an ordinal feature. The labels in this category can be ordered from coldest to
    warmest. Ordinal features can be converted into continuous variables by interpolating
    the range values to a defined scale.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列**特征，类似于名义特征，也具有有限个分类值。然而，与名义特征不同，这些分类值可以按特定顺序排列。在先前的例子中，**温度**特征是一个序列特征。这个类别的标签可以从最冷到最暖排列。序列特征可以通过将范围值插值到定义的尺度来转换为连续变量。'
- en: Continuous features
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续特征
- en: '**Continuous** features can have infinite possible values. Unlike nominal and
    ordinal features, which can only have a discrete set of values, continuous variables
    are numerical variables, and are not compatible with some ML algorithms. However,
    continuous features can be converted into ordinal features using a technique called
    **discretization**.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续**特征可以有无穷多个可能的值。与只能有离散值集合的名义和序列特征不同，连续变量是数值变量，并且与某些机器学习算法不兼容。然而，可以使用称为**离散化**的技术将连续特征转换为序列特征。'
- en: Although we will not discuss techniques to transform features from one form
    to another here, we will demonstrate how it can be done in our example sections.
    We have selected example datasets in this book where feature transformation is
    required. You should not only learn about these various transformation techniques
    from this book, but also observe how a data scientist analyzes a dataset and uses
    specific feature transformation techniques based on the application. We have also
    provided examples to apply these techniques at scale in Python and AWS SageMaker.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这里不会讨论将特征从一种形式转换为另一种形式的技巧，但我们将通过示例部分展示如何实现。在这本书中，我们选择了需要特征转换的示例数据集。您不仅应该从这本书中了解这些不同的转换技巧，还应该观察数据科学家如何分析数据集，并根据应用使用特定的特征转换技巧。我们还提供了在Python和AWS
    SageMaker中大规模应用这些技巧的示例。
- en: Naive Bayes classifier
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: Naïve Bayes classifier is a ML algorithm based on Bayes' theorem. The algorithm
    is comparable to how a belief system evolves. Bayes' theorem was initially introduced
    by an English mathematician, Thomas Bayes, in 1776\. This algorithm has various
    applications, and has been used for many historic tasks for more than two centuries.
    One of the most famous applications of this algorithm was by Alan Turing during
    the Second World War, where he used Bayes' theorem to decrypt the German Enigma
    code. Bayes' theorem has also found an important place in ML for algorithms such
    as Bayesian Net and Naive Bayes algorithm. Naïve Bayes algorithm is very popular
    for ML due to its low complexity and transparency in why it makes the prediction.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于贝叶斯定理的朴素贝叶斯分类器是一种机器学习算法。该算法与信念系统的发展方式相似。贝叶斯定理最初由英国数学家托马斯·贝叶斯在1776年提出。这个算法有各种应用，并且被用于超过两个世纪的许多历史任务。这个算法最著名的应用之一是艾伦·图灵在第二次世界大战期间的应用，他使用贝叶斯定理来解密德国恩尼格玛密码。贝叶斯定理在机器学习中也为诸如贝叶斯网络和朴素贝叶斯算法等算法找到了一个重要的位置。朴素贝叶斯算法因其低复杂性和预测原因的透明度而在机器学习中非常受欢迎。
- en: Bayes' theorem
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: In this section, we will first introduce Bayes' theorem and demonstrate how
    it is applied in ML.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍贝叶斯定理，并展示它在机器学习中的应用。
- en: Bayes' theorem calculates the probability of an event given a condition, such
    that we have prior knowledge about the event, the condition, and the probability
    of the condition when the event occurs. In our snow prediction example, the event
    is when snow occurs. A condition would be when the temperature is between 20°F
    and 32°F. And, based on the data, we can calculate the likelihood of temperature
    being 20°F and 32°F when it snows. Using this data, we can predict the probability
    of snow given the temperature being between 20°F and 32°F.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理计算在给定条件下事件发生的概率，这样我们就有关于事件、条件和事件发生时条件概率的先验知识。在我们的雪预测例子中，事件是下雪。条件是温度在20°F到32°F之间。根据数据，我们可以计算下雪时温度在20°F和32°F之间的似然。使用这些数据，我们可以预测在温度在20°F到32°F之间时下雪的概率。
- en: Assume that we have a class variable *C* and a condition variable *x*. Bayes'
    theorem is presented in formula 1\. We also present a given simple way to remember
    different components of the algorithm in formula 2.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个类别变量 *C* 和一个条件变量 *x*。贝叶斯定理在公式1中给出。我们还在公式2中提供了一个简单的方法来记住算法的不同组成部分。
- en: '**Formula 1**'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**公式1**'
- en: '![](img/761e081a-97d5-4bec-811c-c36b7bc1e555.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/761e081a-97d5-4bec-811c-c36b7bc1e555.png)'
- en: '**Formula 2 **'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**公式2 **'
- en: '![](img/024ae2dd-62e4-410d-b8d1-f6fdf5c78ee4.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/024ae2dd-62e4-410d-b8d1-f6fdf5c78ee4.png)'
- en: There are four terms that you need to remember from this formula.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个公式中，你需要记住四个术语。
- en: Posterior
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后验
- en: The **posterior** probability is the chance of an event occurring given the
    existence of feature variable *x*.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**后验**概率是在特征变量 *x* 存在的情况下事件发生的概率。'
- en: Likelihood
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 似然
- en: '**Likelihood** is the probability of a condition occurring for a given event.
    In our example, likelihood means what the probability is of the temperature being
    between 20°F to 32°F when it snows. Based on the data in the dataset, there is
    a 66.66% probability that the temperature is 20°F-30°F when it snows. Training
    data can be used to calculate the probability of each discrete value in the feature
    set.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**似然**是指给定事件发生时某个条件出现的概率。在我们的例子中，似然指的是在下雪时温度在20°F到32°F之间的概率。根据数据集中的数据，下雪时温度在20°F-30°F之间的概率是66.66%。训练数据可以用来计算特征集中每个离散值的概率。'
- en: Prior probability
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 先验概率
- en: The **prior** probability is the overall probability of the event in the dataset.
    In our example, this would be the overall probability that it snows in the dataset.
    Prior probability is important in cases where the datasets are unbalanced, that
    is, the number of instances of one class variable in the dataset is significantly
    higher than the other. This leads to bias in the likelihood variable. Prior probabilities
    are used to renormalize these probabilities by taking the bias in the dataset
    into account. For example, in our dataset, the prior probability of a snow event
    is 30% and the prior probability of it not snowing is 70%. The probability of
    cloudy conditions when it snows is 66%, while the likelihood of cloudy conditions
    when it does not snow is 42.8%.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**先验**概率是数据集中事件的总体概率。在我们的例子中，这将是数据集中下雪的总体概率。先验概率在数据集不平衡的情况下很重要，即数据集中一个类别变量的实例数量显著高于另一个。这会导致似然变量的偏差。先验概率通过考虑数据集中的偏差来重新规范化这些概率。例如，在我们的数据集中，雪事件的先验概率是30%，不下雪的先验概率是70%。下雪时多云的概率是66%，而不下雪时多云的似然是42.8%。'
- en: However, by taking the prior probabilities into account, although cloudy conditions
    are more likely when it snows than when it does not, after multiplying the priors,
    the posterior probability of snow when it is cloudy is 19% and the probability
    of not snowing when it is cloudy is 30%. By multiplying the prior probabilities
    to the likelihood events, we inform our posterior probability that there is a
    higher probability of it not snowing than snowing.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑到先验概率，尽管下雪时多云的条件比不下雪时更可能，但在乘以先验概率后，多云时的后验概率为19%，多云时不下雪的概率为30%。通过将先验概率与似然事件相乘，我们告知我们的后验概率是它不下雪的概率比下雪的概率更高。
- en: Evidence
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 证据
- en: The **evidence** variable is the probability of a condition in the dataset.
    In our example, the probability of temperature being 70°F or above is only 10%.
    Rare events have low evidence probability. Evidence probabilities boost posterior
    probabilities of rare events. For the purpose of the Naïve Bayes classifier, we
    do not need to consider the evidence variable, since it is not dependent on the
    class variable.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**证据**变量是数据集中一个条件的概率。在我们的例子中，温度为70°F或以上的概率仅为10%。罕见事件具有低的证据概率。证据概率会提高罕见事件的后验概率。对于
    Naïve Bayes 分类器来说，我们不需要考虑证据变量，因为它不依赖于类变量。'
- en: So, Bayes' theorem is used to calculate the probability of an event given a
    single condition. However, when we train ML algorithms, we use one or more features
    to predict the probability of an event. In the next section, we will explain Naïve
    Bayes algorithm and how it utilizes posterior probabilities of multiple features
    variables.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，贝叶斯定理用于计算给定单个条件的事件的概率。然而，当我们训练机器学习算法时，我们使用一个或多个特征来预测事件的概率。在下一节中，我们将解释 Naive
    Bayes 算法以及它是如何利用多个特征变量的后验概率的。
- en: How the Naive Bayes algorithm works
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Naive Bayes 算法的工作原理
- en: The Naive Bayes algorithm uses Bayes' theorem to calculate the posterior probability
    of every condition in the dataset and uses these probabilities to calculate the
    conditional probability of an event given a set of conditions. The Naive Bayes
    algorithm assumes that each conditional feature is independent of each other.
    This is an important assumption that helps simplify how the conditional probability
    is calculated. The independence assumption is the reason why the algorithm gets
    the name, Naive Bayes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Naive Bayes 算法使用贝叶斯定理来计算数据集中每个条件的后验概率，并使用这些概率来计算给定一组条件的事件的条件概率。Naive Bayes 算法假设每个条件特征是相互独立的。这是一个重要的假设，有助于简化条件概率的计算方式。独立性假设是算法得名
    Naive Bayes 的原因。
- en: 'In this section, instead of considering one *x* feature variable, we consider
    a vector of features,![](img/0905d81e-d283-452c-933e-1cc360270d18.png), where
    *n* is the number of feature variables used to calculate the class probability.
    We represent the conditional probability of a class variable for the *x* vector
    in formula 3:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们不是考虑一个 *x* 特征变量，而是考虑一个特征向量，![图片](img/0905d81e-d283-452c-933e-1cc360270d18.png)，其中
    *n* 是用于计算类概率的特征变量的数量。我们在公式3中表示 *x* 向量的类变量的条件概率：
- en: '**Formula 3**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**公式 3**'
- en: '![](img/e7156a8e-ff2a-4255-b184-d9ef34deec26.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e7156a8e-ff2a-4255-b184-d9ef34deec26.png)'
- en: 'As we have assumed that each feature variable is independent of each other,
    the conditional probability of a class variable can be calculated as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们假设每个特征变量是相互独立的，因此可以按以下方式计算类变量的条件概率：
- en: '**Formula 4**'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**公式 4**'
- en: '![](img/4e7880e7-9ad5-4522-98d0-14d8be8c20c2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4e7880e7-9ad5-4522-98d0-14d8be8c20c2.png)'
- en: 'Based on posterior probability calculations shown in the previous sections,
    this formula can be rewritten as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前几节中所示的后验概率计算，此公式可以重写如下：
- en: '**Formula 5**'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**公式 5**'
- en: '![](img/c6a3509a-c92f-4079-94dc-312c679f4663.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c6a3509a-c92f-4079-94dc-312c679f4663.png)'
- en: Formula 5 explains how a probability of event *C* is calculated based on the ![](img/0905d81e-d283-452c-933e-1cc360270d18.png) feature
    variables. An interesting thing to note in this formula is how easy it is to calculate
    each element from the dataset. Also, since the evidence probability from Bayes'
    theorem is not dependent on the class variable, it is not used in the Naive Bayes
    formula.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 公式5解释了如何根据![图片](img/0905d81e-d283-452c-933e-1cc360270d18.png)特征变量计算事件 *C* 的概率。在这个公式中值得注意的是，如何轻松地从数据集中计算每个元素。此外，由于贝叶斯定理中的证据概率不依赖于类变量，因此它没有用于
    Naive Bayes 公式中。
- en: 'The Naive Bayes algorithm only requires one pass over the dataset during the
    training phase to calculate the probability of the value of a feature for each
    event. During the prediction phase, we calculate the probability of each event
    given the instance of the features and predict the event with the highest probability.
    Formula 6 shows how the prediction of a Naïve Bayes classifier is calculated when
    *k* events are possible. **Argmax** in the formula means that the event with maximum
    probability is selected as the prediction:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Naive Bayes 算法在训练阶段只需要遍历一次数据集，就可以计算每个事件的特征值的概率。在预测阶段，我们根据特征实例计算每个事件的概率，并预测概率最高的那个事件。公式
    6 展示了当有 *k* 个可能事件时，Naive Bayes 分类器的预测是如何计算的。公式中的 **Argmax** 表示选择概率最大的事件作为预测：
- en: '** Formula 6**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '** 公式 6**'
- en: '![](img/bb33aabc-cbc3-4a34-89f0-702644ccea50.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb33aabc-cbc3-4a34-89f0-702644ccea50.png)'
- en: Naïve Bayes classifier is a multiclass classifier that can be used to train
    on a dataset where two or more class variables need to be predicted. In the next
    chapters, we will present some examples of binary classifiers that only work with
    two class variables needs to be predicted. However, we will show you the methodologies
    of applying binary classifiers to multiclass problems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Naive Bayes 分类器是一种多类分类器，可以用于在需要预测两个或更多类变量的数据集上进行训练。在下一章中，我们将展示一些仅适用于需要预测两个类变量的二元分类器的示例。然而，我们将向您展示将二元分类器应用于多类问题的方法。
- en: Classifying text with language models
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用语言模型进行文本分类
- en: Text classification is an application of classification algorithms. However,
    the text is a combination of words in a specific order. Hence, you can observe
    that a text document with a class variable is not similar to the dataset that
    we presented in table 1, in the *Classification algorithms* section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类是分类算法的应用。然而，文本是按特定顺序组合的单词。因此，您可以看到，具有类变量的文本文档与我们之前在 *分类算法* 部分中展示的表 1 中的数据集并不相似。
- en: A text dataset can be represented as shown in table 2.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据集可以表示如表 2 所示。
- en: '**Table 2: Example of a Twitter dataset**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 2：Twitter 数据集示例**'
- en: '| **Tweet** | **Account** |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **推文** | **账户** |'
- en: '| The simplest way to protect Americans from gun violence is to actually talk
    about common-sense gun laws. | Democrats |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 保护美国人民免受枪支暴力的最简单方法就是真正讨论常识性的枪支法律。 | 民主党 |'
- en: '| This cannot be who we are as a country. We need to find out what happened
    and ensure it never happens again ([https://t.co/RiY7sjMfJK)](https://t.co/RiY7sjMfJK))  |
    Democrats |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 这不能是我们作为一个国家的样子。我们需要找出发生了什么，并确保它永远不会再次发生 ([https://t.co/RiY7sjMfJK)](https://t.co/RiY7sjMfJK))  |
    民主党 |'
- en: '| Over the weekend, President Trump visited Arlington National Cemetery to
    honor fallen soldiers. | Republicans |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 周末，特朗普总统访问了阿灵顿国家公墓，向阵亡士兵致敬。 | 共和党 |'
- en: '| This President has made it clear that he will secure this country—`@SecNielsen`.
    | Republicans |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 这位总统已经明确表示他将保护这个国家——`@SecNielsen`。 | 共和党 |'
- en: For this chapter, we have built a dataset based on tweets from two different
    accounts. We also have provided code in the following sections so that you can
    create your own datasets to try this example. Our purpose is to build a smart
    application that is capable of predicting the source of a tweet just by reading
    the tweet text. We will collect several tweets by the United States Republican
    Party (`@GOP`) and the Democratic Party (`@TheDemocrats`) to build a model that
    can predict which party wrote a given tweet. In order to do this, we will randomly
    select some tweets from each party and submit them through the model to check
    whether the prediction actually matched reality.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们基于两个不同账户的推文构建了一个数据集。我们还在以下章节中提供了代码，以便您可以创建自己的数据集来尝试这个示例。我们的目的是构建一个智能应用程序，能够仅通过阅读推文文本就能预测推文的来源。我们将收集美国共和党（`@GOP`）和民主党（`@TheDemocrats`）的多个推文来构建一个模型，可以预测哪个政党撰写了给定的推文。为了做到这一点，我们将从每个政党随机选择一些推文并通过模型提交，以检查预测是否与实际情况相符。
- en: Collecting the tweets
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 收集推文
- en: We will start by using the `Twython` library to access the Twitter API and collect
    a series of tweets, labeling them with the originating political party.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用 `Twython` 库访问 Twitter API 并收集一系列推文，并给它们标注起源的政治党派。
- en: 'The details of the implementation can be found in our GitHub repository in
    the following Jupyter Notebook:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的细节可以在以下 Jupyter Notebook 中的我们的 GitHub 仓库中找到：
- en: '`chapter2/collect_tweets.ipynb`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`chapter2/collect_tweets.ipynb`'
- en: 'We need to invoke the following method in the `Twython` library to save tweets
    from `@GOP` and `@TheDemocrats` onto some text files, `gop.txt` and `dems.txt`
    respectively:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在 `Twython` 库中调用以下方法来将来自 `@GOP` 和 `@TheDemocrats` 的推文分别保存到一些文本文件中，`gop.txt`
    和 `dems.txt`：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Each file contains 200 tweets. The following are some excerpts from the `dems.txt`
    file:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件包含 200 条推文。以下是从 `dems.txt` 文件中的一些摘录：
- en: '`This cannot be who we are as a country. We need to find out what happened
    and ensure it never happens again.`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`这不能代表我们作为一个国家的样子。我们需要找出发生了什么，并确保它不再发生。`'
- en: '`RT @AFLCIO: Scott Walker. Forever a national disgrace.`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RT @AFLCIO: Scott Walker. 永远是国家耻辱。`'
- en: Preparing the data
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Now that we have the source data in text files, we need to convert it to a
    format that can be used as an input for a ML library. Most general-purpose ML
    packages, such as `scikit-learn` and Apache Spark, only accept a matrix of numbers
    as input. Hence, feature transformation is required for a text dataset. A common
    approach is to use language models such as **bag of words** (**BoW**). In this
    example, we build a BoW for each tweet and construct a matrix in which each row
    represents a tweet and each column signals the presence of a particular word.
    We also have a column for the label that can distinguish tweets from `Republicans`
    (`1`) or `Democrats` (`0`), as we can see in the following table:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将源数据保存在文本文件中，我们需要将其转换为可以用于机器学习库输入的格式。大多数通用机器学习包，如 `scikit-learn` 和 Apache
    Spark，只接受数字矩阵作为输入。因此，需要对文本数据集进行特征转换。一种常见的方法是使用语言模型，如**词袋模型**（**BoW**）。在这个例子中，我们为每条推文构建一个
    BoW，并构建一个矩阵，其中每一行代表一条推文，每一列表示特定单词的存在。我们还有一个用于区分来自 `共和党`（`1`）或 `民主党`（`0`）的推文的标签列，如下表所示：
- en: '**Table 3: Converting text dataset to structured dataset**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**表 3：将文本数据集转换为结构化数据集**'
- en: '|  | **Immigration** | **Medicaid** | **Terrorism** | **Class** |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | **移民** | **医疗补助** | **恐怖主义** | **阶级** |'
- en: '| Tweet 1 | 0 | 1 | 0 | 0 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 推文 1 | 0 | 1 | 0 | 0 |'
- en: '| Tweet 2 | 1 | 0 | 1 | 1 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 推文 2 | 1 | 0 | 1 | 1 |'
- en: '| Tweet 3 | 0 | 0 | 1 | 0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 推文 3 | 0 | 0 | 1 | 0 |'
- en: Table 2 represents the matrix that can be derived from tweets. However, there
    are many points to remember when generating such a matrix. Due to the number of
    terms in the language lexicon, the number of columns in the matrix can be very
    high. This poses a problem in ML known as the **curse of dimensionality** (see
    section *X*). There are several ways to tackle this problem; however, as our example
    is fairly small in terms of data, we will only briefly discuss methods to reduce
    the number of columns.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2 表示可以从推文中导出的矩阵。然而，在生成此类矩阵时有许多需要注意的点。由于语言词汇表中的术语数量，矩阵的列数可能非常高。这给机器学习带来了一个称为**维度诅咒**的问题（见第
    *X* 节）。有几种方法可以解决这个问题；然而，由于我们的示例在数据量上相对较小，我们只会简要讨论减少列数的方法。
- en: '**Stopwords**: Certain common words might add no value to our task (for example,
    the words **the**, **for**, or **as**). We call these words stopwords, and we
    shall remove these words from `dems.txt` and `gop.txt`.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**停用词**：某些常见的单词可能对我们的任务没有价值（例如，单词 **the**、**for** 或 **as**）。我们称这些单词为停用词，我们将从
    `dems.txt` 和 `gop.txt` 中删除这些单词。'
- en: '**Stemming**: There may be many variants of a word that are used in the text.
    For example, argue, argued, argues, and arguing all stem from the word **argue**.
    Techniques such as stemming and lemmatization can be used to find the stem of
    the word and replace variants of that word with the stem.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词干提取**：文本中可能有许多单词的变体。例如，argue、argued、argues 和 arguing 都来自单词 **argue**。可以使用词干提取和词形还原等技术来找到单词的词干，并用该词的变体替换词干。'
- en: '**Tokenization**: Tokenization can be used to combine various words into phrases
    so that the number of features can be reduced. For example, **tea party** has
    a totally different meaning, politically, than the two words alone. We won''t
    consider this for our simple example, but tokenization techniques help in finding
    such phrases.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：分词可以将各种单词组合成短语，从而减少特征的数量。例如，“tea party”在政治上与单独的两个词有完全不同的含义。我们不会在我们的简单示例中考虑这一点，但分词技术有助于找到这样的短语。'
- en: Another issue to consider is that words appearing more than once in a tweet
    have equal importance on a training row. There are ways to utilize this information
    by using multinomial or term frequency-inverse document frequency (TFIDF) models.
    Since tweets are relatively short text, we will not consider this aspect in our
    implementation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的问题是，在推文中出现多次的单词在训练行中具有同等的重要性。可以通过使用多项式或词频-逆文档频率（TFIDF）模型来利用这些信息。由于推文相对较短，我们不会在我们的实现中考虑这个方面。
- en: 'The table 2 matrix describes the words you would find for each class (that
    is each political party). However, when we want to predict the source of the tweet,
    the inverse problem is posed. Given a specific bag of words, we''re interested
    in assessing how likely it is that the terms are used by one party or another.
    In other words, we know the probability of a bag of words given a particular party,
    and we are interested in the reverse: the probability of a tweet being written
    by a party given a bag of words. This is where the Naive Bayes algorithm is applied.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2 矩阵描述了每个类别（即每个政党）中可能找到的单词。然而，当我们想要预测推文的来源时，就提出了逆问题。给定一个特定的词袋，我们感兴趣的是评估这些术语被一个政党或另一个政党使用的可能性。换句话说，我们知道给定一个特定政党，词袋的概率，而我们感兴趣的是相反的情况：给定一个词袋，推文被一个政党撰写的概率。这就是朴素贝叶斯算法应用的地方。
- en: Building a Naive Bayes model through SageMaker notebooks
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 SageMaker 笔记本构建朴素贝叶斯模型
- en: Let's get started with SageMaker notebooks. This tool will help us run the code
    that will train our model. SageMaker, among other things, allows us to create
    notebook instances that host Jupyter Notebooks. Jupyter is a web UI that allows
    a data scientist or programmer to code interactively by creating paragraphs of
    code that are executed on demand. It works as an IDE, but with the additional
    ability to render the output of the code in visually relevant forms (for example,
    charts, tables, and markdown), and also supports writing paragraphs in different
    languages within the same notebook. We will use notebooks extensively throughout
    this book, and we recommend its use as a way to share and present data science
    findings. It allows users to achieve reproducible research, as the code necessary
    for a particular research objective can be validated and reproduced by re-running
    the code paragraphs in the notebook.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始使用 SageMaker 笔记本。这个工具将帮助我们运行训练模型的代码。SageMaker 除了其他功能外，还允许我们创建笔记本实例，这些实例托管
    Jupyter 笔记本。Jupyter 是一个 Web UI，允许数据科学家或程序员通过创建代码段落进行交互式编码，这些代码段落按需执行。它作为一个 IDE，但具有将代码输出以视觉相关形式（例如图表、表格和
    Markdown）呈现的附加功能，并且还支持在同一个笔记本中用不同语言编写段落。我们将在这本书中广泛使用笔记本，并推荐将其用作分享和展示数据科学发现的方法。它允许用户实现可重复研究，因为特定研究目标所需的代码可以通过重新运行笔记本中的代码段落进行验证和重现。
- en: You can learn more on SageMaker's AWS console page at [https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/dashboard](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/dashboard).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 SageMaker 的 AWS 控制台页面了解更多信息，请访问[https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/dashboard](https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/dashboard)。
- en: 'Let''s look at what Sagemaker''s AWS console page looks in the following screenshot:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 SageMaker 的 AWS 控制台页面在以下截图中的样子：
- en: '![](img/89acc840-4b2c-4737-9743-852afedb151e.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/89acc840-4b2c-4737-9743-852afedb151e.png)'
- en: 'Click on Add repository, choose your authentication mechanism and add the repository
    found at [https://github.com/mg-um/mastering-ml-on-aws](https://github.com/mg-um/mastering-ml-on-aws):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“添加仓库”，选择您的身份验证机制，并添加位于[https://github.com/mg-um/mastering-ml-on-aws](https://github.com/mg-um/mastering-ml-on-aws)的仓库：
- en: '![](img/1e93d7ec-84f0-4db1-9faf-a88ff6164296.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1e93d7ec-84f0-4db1-9faf-a88ff6164296.png)'
- en: 'Before creating the notebook instance, it is possible that you would want to
    attach a Git repository so that the notebooks available with this book are attached
    to the notebook, and so are made available immediately as you will see later:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建笔记本实例之前，您可能希望附加一个 Git 仓库，以便本书提供的笔记本可以附加到笔记本上，并立即可用，正如您稍后将会看到的：
- en: '![](img/8af5a0cb-2ad3-4f82-bdd2-8b8a71f5cdcc.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8af5a0cb-2ad3-4f82-bdd2-8b8a71f5cdcc.png)'
- en: We can now proceed to launch a notebook instance. There are several options
    to configure the hardware, networking, and security of the server that will host
    the notebook. However, we will not go into much detail for now, and will accept
    the defaults. The AWS documentation is an excellent resource if you want to limit
    the access or power-up your machine.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续启动笔记本实例。为将要托管笔记本的服务器配置硬件、网络和安全有多种选项。然而，我们现在不会深入细节，而是接受默认设置。如果你想要限制访问或增强你的机器，AWS
    文档是一个极好的资源。
- en: 'Since we attached the Git repository, once you open Jupyter, you should see
    the notebooks we created for this book, and you can re-run them, modify them,
    or improve them:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们附加了 Git 仓库，一旦你打开 Jupyter，你应该能看到我们为这本书创建的笔记本，你可以重新运行它们、修改它们或改进它们：
- en: '![](img/c2aedb20-c8a9-4ccb-94c6-a5d0d253f08b.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c2aedb20-c8a9-4ccb-94c6-a5d0d253f08b.png)'
- en: In this section, we focus on the `train_scikit` Python notebook and go over
    code snippets to explain how we can build and test a model for out tweet classification
    problem. We encourage you to run all the paragraphs of this notebook to get an
    idea of the purpose of this notebook.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们关注 `train_scikit` Python 笔记本，并回顾代码片段来解释我们如何构建和测试用于推文分类问题的模型。我们鼓励你运行这个笔记本的所有段落，以了解这个笔记本的目的。
- en: 'The first thing we will do is load the stopwords and the two sets of tweets into
    variables:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将加载停用词和两组推文到变量中：
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will then proceed to use the utilities in `scikit-learn` to construct our
    matrix. In order to do that, we will use a `CountVectorizer` class, which is a
    class that knows how to allocate the different words into columns while at the
    same time filtering the stopwords. We will consider both sets of tweets; for our
    example, we''ll just use the first `1200` words:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将接着使用 `scikit-learn` 中的工具来构建我们的矩阵。为了做到这一点，我们将使用一个 `CountVectorizer` 类，这是一个知道如何将不同的单词分配到列同时过滤掉停用词的类。我们将考虑两组推文；在我们的例子中，我们将只使用前
    `1200` 个单词：
- en: '[PRE2]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Through `vectorizer` we can now construct two matrices, one for republican
    party tweets and one for democratic party tweets:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `vectorizer` 我们现在可以构建两个矩阵，一个用于共和党推文，另一个用于民主党推文：
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'These two bag-of-words matrices (`dem_bow` and `gop_bow`) are represented in
    a sparse data structure to minimize memory usage, but can be examined by converting
    them to arrays:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个词袋矩阵（`dem_bow` 和 `gop_bow`）以稀疏数据结构表示，以最小化内存使用，但可以通过将它们转换为数组来检查：
- en: '[PRE4]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In order to train our model, we need to provide two arrays. The BoWs matrix
    (for both parties), which we will call `x`, and the labels (class variables) for
    each of the tweets. To construct this, we will vertically stack both matrices
    (for each party):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型，我们需要提供两个数组。一个是 BoWs 矩阵（针对双方），我们将称之为 `x`，另一个是每条推文的标签（类别变量）。为了构建这个，我们将垂直堆叠两个矩阵（针对每个政党）：
- en: '[PRE5]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To construct the labels vector, we will just assemble a vector with `ones`
    for `Democrat` positions and `zeros` for `Republican` positions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建标签向量，我们只需组装一个向量，其中 `Democrat` 位置为 `ones`，而 `Republican` 位置为 `zeros`：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Before we train our models, we will split the tweets (rows on our `x` matrix)
    randomly, so that some are used to build a model and others are used to check
    whether the model predicts the correct political party (label):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练我们的模型之前，我们将随机分割推文（`x` 矩阵的行），以便一些用于构建模型，而另一些用于检查模型是否正确预测政治党派（标签）：
- en: '[PRE7]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that we have our training and testing datasets, we proceed to train our
    model using Naive Bayes (a Bernoulli Naive Bayes, since our matrices are ones
    or zeros):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练集和测试集，我们继续使用朴素贝叶斯（由于我们的矩阵是 ones 或 zeros，因此是伯努利朴素贝叶斯）来训练我们的模型：
- en: '[PRE8]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see in the preceding code, it is very simple to fit a Naive Bayes
    model. We need to provide the training matrices and the labels. A model is now
    capable of predicting the label (political party) of arbitrary tweets (as long
    as we have them as a BoWs matrix representation). Fortunately, we had separated
    some of the tweets for testing, so we can run these through the model and see
    how often the model predicts the right label (note that we know the actual party
    that wrote the tweet for every tweet in the testing dataset).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，拟合朴素贝叶斯模型非常简单。我们需要提供训练矩阵和标签。现在，模型能够预测任意推文的标签（政治党派）（只要我们以 BoWs 矩阵表示形式拥有它们）。幸运的是，我们已经为测试分离了一些推文，因此我们可以将这些推文通过模型，看看模型预测正确标签的频率（注意，我们知道测试数据集中每条推文的实际党派）。
- en: 'To get the predictions it''s as simple as invoking the `predict` method of
    the model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取预测结果，只需调用模型的`predict`方法：
- en: '[PRE9]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we can see how many of the predictions match the ground truth:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到有多少预测结果与真实值相匹配：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The output score of the code block is `0.95`.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块的输出分数为`0.95`。
- en: 'In this example, we are using accuracy as an evaluation metric. Accuracy can
    be calculated using formula 7:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用准确率作为评估指标。准确率可以使用公式7计算：
- en: '**Formula 7**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**公式7**'
- en: '![](img/0b450b51-a65b-4f29-b4dd-a7447a36b262.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b450b51-a65b-4f29-b4dd-a7447a36b262.png)'
- en: There are various evaluation metrics that a data scientist can use to evaluate
    ML algorithm. We will present evaluation measures such as precision, recall, F1
    measure, **root mean squared error** (**RMSE**), and **area under curve** (**AUC**) in
    our next chapters for different examples. Evaluation metrics should be selected
    based on the business need of implementing an algorithm, and should indicate whether
    or not the ML algorithm is performing at the standards required to achieve a task.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家可以使用各种评估指标来评估机器学习算法。在下一章中，我们将介绍如精度、召回率、F1度量、**均方根误差**（**RMSE**）和**曲线下面积**（**AUC**）等评估指标，以不同的示例进行展示。评估指标应根据实现算法的业务需求进行选择，并应表明机器学习算法是否达到了完成任务所需的标准。
- en: Since this is the first example we are working on, we will use the simplest
    evaluation measure, which is accuracy. As specified in formula 7, accuracy is
    the ratio of correct predictions to the total number of predictions made by the
    classifier. It turns out that our Naive Bayes model is very accurate, with an
    accuracy of 95%. It is possible that some words, such as the names of members
    of each party, can quickly make the model give a correct prediction. We will explore
    this using decision trees in [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml),
    *Predicting User Behavior with Tree-Based Methods*.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是我们正在处理的第一个例子，我们将使用最简单的评估指标，即准确率。根据公式7，准确率是正确预测数与分类器做出的总预测数之比。结果证明，我们的朴素贝叶斯模型非常准确，准确率为95%。可能有些词，如每个党派的成员名称，可以迅速使模型给出正确的预测。我们将使用决策树在[第4章](af506fc8-f482-453e-8162-93a676b2e737.xhtml)，*基于树的预测用户行为方法*中探讨这一点。
- en: Note that, during this process, we had to prepare and transform the data in
    order to fit a model. This process is very common, and both `scikit-learn` and
    Spark support the concept of pipelines, which allow the data scientist to declare
    the necessary transformations needed to build a model without having to manually
    obtain intermediary results.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个过程中，我们必须准备和转换数据以适应模型。这个过程非常常见，`scikit-learn`和Spark都支持管道的概念，允许数据科学家声明构建模型所需的必要转换，而无需手动获取中间结果。
- en: 'In the following code snippet, we can see an alternative way to produce the
    same model by creating a pipeline with the following two stages:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们可以看到通过创建以下两个阶段的管道来产生相同模型的一种替代方法：
- en: Count vectorizer
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数向量器
- en: Naive Bayes trainer
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯训练器
- en: '[PRE11]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This allows our modeling to be a bit more concise and declarative. By calling
    the `pipeline.fit()` method, the library applies any necessary transformations
    or estimations necessary. Note that, in this case, we split the raw texts (rather
    than the matrices) as the `fit()` method now receives the raw input. As we shall
    see in the next section, pipelines can contain two kinds of stages, Transformers
    and Estimators, depending on whether the stage needs to compute a model out of
    the data, or simply transform the data declaratively.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得我们的建模更加简洁和声明性。通过调用`pipeline.fit()`方法，库应用任何必要的转换或估计。请注意，在这种情况下，我们分割了原始文本（而不是矩阵），因为`fit()`方法现在接收的是原始输入。正如我们将在下一节中看到的，管道可以包含两种类型的阶段，Transformers和Estimators，这取决于该阶段是否需要从数据中计算模型，或者只是声明性地转换数据。
- en: Naïve Bayes model on SageMaker notebooks using Apache Spark
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在SageMaker笔记本中使用Apache Spark的朴素贝叶斯模型
- en: In the previous section *Classifying text with language models*, we saw how
    you can train a model with `scikit-learn` on a SageMaker notebook instance. This
    is feasible for examples as small as the ones we collected from Twitter. What
    if, instead, we had hundreds of terabytes worth of tweet data? For starters, we
    would not be able to store the data in a single machine. Even if we could, it
    would probably take too long to train on such large dataset. Apache Spark solves
    this problem for us by implementing ML algorithms that can read data from distributed
    datasets (such as AWS S3) and can distribute the computing across many machines.
    AWS provides a product called **Elastic MapReduce** (**EMR**) that is capable
    of launching and managing clusters on which we can perform ML at scale.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节*使用语言模型对文本进行分类*中，我们看到了如何在SageMaker笔记本实例上使用`scikit-learn`训练模型。对于像我们从Twitter收集的例子那样小的例子来说，这是可行的。如果我们有数百TB的推文数据呢？首先，我们无法在单个机器上存储这些数据。即使我们能够做到，在如此大的数据集上训练可能也需要太长时间。Apache
    Spark通过实现可以读取分布式数据集（如AWS S3）的ML算法并可以在多台机器上分配计算来解决我们的问题。AWS提供了一个名为**弹性映射减少**（**EMR**）的产品，它能够启动和管理我们可以在其上执行大规模ML的集群。
- en: Many of the ML algorithms require several passes over the data (although this
    is not the case for Naive Bayes). Apache Spark provides a way to cache the datasets
    in memory, so that one can efficiently run algorithms that require several passes
    over the data (such as **logistic regression** or **decision trees**, which we
    will see in the following chapters). We will show how to launch EMR clusters in
    [Chapter 4](af506fc8-f482-453e-8162-93a676b2e737.xhtml), *Predicting User Behavior
    with Tree-Based Methods*, however, in this section, we will present how similar
    it is to work with Apache Spark compared to `scikit-learn`. In fact, many of the
    interfaces in Apache Spark (such as pipelines, Transformers, and Estimators) were
    inspired by `scikit-learn`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 许多ML算法需要对数据进行多次遍历（尽管朴素贝叶斯不是这种情况）。Apache Spark提供了一种将数据集缓存到内存中的方法，这样就可以高效地运行需要多次遍历数据的算法（如**逻辑回归**或**决策树**，我们将在下一章中看到）。我们将在[第4章](af506fc8-f482-453e-8162-93a676b2e737.xhtml)，*使用基于树的算法预测用户行为*中展示如何启动EMR集群，然而，在本节中，我们将展示与Apache
    Spark相比，与`scikit-learn`一起工作的相似性。事实上，Apache Spark中的许多接口（如管道、转换器和估计器）都受到了`scikit-learn`的启发。
- en: 'Apache Spark supports four main languages: R, Python, Scala, and Java. In this
    book we will use the Python flavor, also called PySpark. Even though our spark
    code will run on a single machine (that is, will run on our SageMaker notebook
    instance), it could run on multiple machines without any code changes if our data
    was larger and we had a Spark Cluster (in [Chapter 4](https://cdp.packtpub.com/mastering_machine_learning_on_aws/wp-admin/post.php?post=25&action=edit#post_27), *Predicting
    User Behavior with Tree-Based Methods*, we will dive into creating Spark Clusters
    with EMR).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark支持四种主要语言：R、Python、Scala和Java。在这本书中，我们将使用Python版本，也称为PySpark。尽管我们的Spark代码将在单个机器上运行（即在SageMaker笔记本实例上运行），但如果我们的数据更大，并且我们有Spark集群（在[第4章](https://cdp.packtpub.com/mastering_machine_learning_on_aws/wp-admin/post.php?post=25&action=edit#post_27)，*使用基于树的算法预测用户行为*中，我们将深入了解使用EMR创建Spark集群），它可以在不更改任何代码的情况下在多台机器上运行。
- en: 'In Spark, the first thing we need to do is to create a Spark session. We do
    this by first creating a Spark context, and then creating a session for SQL-like
    manipulation of data:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，我们首先需要做的是创建一个Spark会话。我们通过首先创建一个Spark上下文，然后创建一个用于类似SQL数据操作会话来完成此操作：
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Since we will run Spark locally (on a single machine) we specify `local`. However,
    if we were to run this on a cluster, we would need to specify the master address
    of the cluster instead. Spark works with abstractions called DataFrames that allow
    us to manipulate huge tables of data using SQL-like operations.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将本地运行Spark（在单个机器上），我们指定`local`。然而，如果我们要在集群上运行此操作，我们需要指定集群的主机地址。Spark使用称为DataFrame的抽象，允许我们使用类似SQL的操作来操作大量数据表。
- en: 'Our first task will be to define DataFrames for our raw data:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一项任务将是为我们的原始数据定义DataFrame：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the first two lines, we create DataFrames out of our raw tweets. We also
    create `corpus_df`, which contains both sources of tweets, and add the label by
    creating a column with a literal of `1` for Democrats and `0` for `Republicans`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两行中，我们将原始推文创建为DataFrame。我们还创建了`corpus_df`，它包含推文的两个来源，并通过创建一个包含`1`的列来标记民主党人，`0`来标记`共和党人`：
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Spark works in a lazy fashion, so, even though we defined and unioned the DataFrame,
    no actual processing will happen until we perform the first operation on the data.
    In our case, this will be the splitting of the DataFrame into testing and training:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 以惰性方式工作，因此，尽管我们定义并联合了 DataFrame，但在我们执行数据上的第一个操作之前，实际上不会发生任何处理。在我们的例子中，这将是将
    DataFrame 分割为测试集和训练集：
- en: '[PRE15]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we are ready to train our model. Spark supports the same concept of pipelines.
    We will build a pipeline with the necessary transformations for our model. It''s
    very similar to our previous example, except that Spark has two separate stages
    for tokenization and stop words remover:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好训练我们的模型。Spark 支持相同的管道概念。我们将构建一个包含模型所需转换的管道。它与我们的上一个例子非常相似，只是 Spark
    有两个单独的阶段用于分词和停用词去除：
- en: '[PRE16]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A Spark ML pipeline consists of a series of stages. Each stage can be a Transformer
    or an Estimator. Transformers apply a well-defined transformation on a dataset,
    while Estimators have the added capability of producing models by traversing the
    dataset. `NaiveBayes` and `CountVectorizer` are examples of Estimators, while
    tokenizer and `StopWordsRemover` are examples of Transformers. Models, in turn,
    are Transformers, because they can provide predictions for all elements in a dataset
    as a transformation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML 管道由一系列阶段组成。每个阶段可以是 Transformer 或 Estimator。Transformers 在数据集上应用一个定义良好的转换，而
    Estimators 通过遍历数据集具有生成模型的能力。`NaiveBayes` 和 `CountVectorizer` 是 Estimator 的例子，而分词器和
    `StopWordsRemover` 是 Transformer 的例子。反过来，模型也是 Transformers，因为它们可以提供数据集中所有元素的预测作为转换。
- en: As you can see in the preceding code, we defined a pipeline with all the necessary
    stages to clean the data. Each stage will transform the original DataFrame (which
    only has two columns value, which are the raw tweet text and label) and add more
    columns.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码所示，我们定义了一个包含所有必要阶段的管道来清理数据。每个阶段都将转换原始 DataFrame（仅包含两个列 value，分别是原始推文文本和标签）并添加更多列。
- en: 'In the following code, the relevant columns used at training time are the features
    (a sparse vector representing the BoWs exactly like our `scikit-learn` example)
    and the label:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，训练时使用的相关列是特征（一个稀疏向量，表示与我们的 `scikit-learn` 示例中完全相同的 BoWs）和标签：
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'By specifying these columns to the `NaiveBayes` classifier we can train a model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通过指定这些列到 `NaiveBayes` 分类器，我们可以训练一个模型：
- en: '[PRE18]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The model is a transformer that can provide predictions for each row in our
    training DataFrame:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是一个 Transformer，可以为我们的训练 DataFrame 中的每一行提供预测：
- en: '[PRE19]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Similar to our previous example, we can evaluate the accuracy of our models.
    By using the `MulticlassClassificationEvaluator` class and specifying the actual
    and predicted labels, we can obtain `accuracy`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的上一个例子类似，我们可以评估我们模型的准确性。通过使用 `MulticlassClassificationEvaluator` 类并指定实际和预测标签，我们可以获得
    `accuracy`：
- en: '[PRE20]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The output is 0.93, which is similar to the results we had on `scikit-learn`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果是 0.93，这与我们在 `scikit-learn` 上得到的结果相似。
- en: Using SageMaker's BlazingText built-in ML service
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker 的 BlazingText 内置 ML 服务
- en: We saw how to perform ML tasks using `scikit-learn` and Apache Spark libraries.
    However, sometimes it's more appropriate to use a ML service*. *SageMaker provides
    ways for us to create, tune, and deploy models supporting a variety of built-in
    ML algorithms by just invoking a service. In a nutshell, you need to place the
    data in S3 (an Amazon service to store large amounts of data) and call the SageMaker
    service providing all the necessary details (actual ML algorithm, the location
    of the data, which kind and how many machines should be used for training). In
    this section, we go through the process of training our model for predicting tweets
    through SageMaker's BlazingText ML service. BlazingText is an algorithm that supports
    text classification using word2vec, which is a way to transform words into vectors
    in a way that captures precise syntactic and semantic word relationships. We won't
    dive into the details of SageMaker's architecture yet, but we will present the
    reader how we would use this AWS service as an alternative to `scikit-learn` or
    Spark.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何使用`scikit-learn`和Apache Spark库执行ML任务。然而，有时使用ML服务更合适。SageMaker提供了创建、调整和部署模型的方法，这些模型支持多种内置的ML算法，只需调用服务即可。简而言之，您需要将数据放在S3（一个用于存储大量数据的亚马逊服务）中，并调用SageMaker服务，提供所有必要的详细信息（实际的ML算法、数据的位置、用于训练的机器的类型和数量）。在本节中，我们将通过SageMaker的BlazingText
    ML服务的过程来训练我们的模型，以预测推文。BlazingText是一种支持使用word2vec进行文本分类的算法，word2vec是一种将单词转换为向量的方式，可以捕捉精确的语法和语义单词关系。我们不会在本节中深入探讨SageMaker架构的细节，但我们将向读者展示如何使用这个AWS服务作为`scikit-learn`或Spark的替代方案。
- en: We will start by importing the SakeMaker libraries, creating a session, and
    obtaining a role (which is the role that the notebook instance is using (see [https://aws.amazon.com/blogs/aws/iam-roles-for-ec2-instances-simplified-secure-access-to-aws-service-apies-from-ec2](https://aws.amazon.com/blogs/aws/iam-roles-for-ec2-instances-simplified-secure-access-to-aws-service-apies-from-ec2)).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先导入SakeMaker库，创建一个会话，并获取一个角色（这是笔记本实例正在使用的角色（见[https://aws.amazon.com/blogs/aws/iam-roles-for-ec2-instances-simplified-secure-access-to-aws-service-apies-from-ec2]））。
- en: 'Additionally, we specify the S3 bucket we will be using to store all our data
    and models:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们指定我们将使用的S3存储桶来存储所有我们的数据和模型：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The next step is to put some data in S3 for training. The expected format for
    BlazingText is to have each line in the `__label__X TEXT ` format. In our case,
    this means prefixing each tweet by a label representing the originating party:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将一些数据放入S3进行训练。BlazingText期望的格式是每行以`__label__X TEXT `格式存在。在我们的例子中，这意味着在每个推文前加上代表原始方的标签：
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To do that, we perform some preprocessing of our tweets and prefix the right
    label:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们对我们的推文进行一些预处理，并添加正确的标签：
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We then proceed to create the sets for training and testing as text files:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将创建用于训练和测试的集合，以文本文件的形式：
- en: '[PRE24]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once we have our training and validation text files, we upload them into `S3`:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了训练和验证的文本文件，我们将它们上传到`S3`：
- en: '[PRE25]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We then proceed to instantiate `Estimator`, by specifying all the necessary
    details: the type and amount of machines to be used for training, as well as the
    location of the path in `S3` where the models will be stored:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来通过指定所有必要的详细信息来实例化`Estimator`：用于训练的机器的类型和数量，以及模型将在S3中存储的路径位置：
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As we discussed in the previous section *Naive Bayes model on SageMaker notebooks
    using Apache Spark* section, an estimator is capable of creating models by processing
    training data. The next step will be to fit the model providing the training data:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在上一节中讨论的*在SageMaker笔记本上使用Apache Spark的朴素贝叶斯模型*部分，estimator能够通过处理训练数据来创建模型。下一步将是提供训练数据来拟合模型：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Before we train the model we need to specify the hyperparameters. We won't go
    into much detail about this algorithm in this section, but the reader can find
    the details in [https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练模型之前，我们需要指定超参数。我们不会在本节中详细介绍这个算法，但读者可以在[https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html]中找到详细信息。
- en: 'This particular algorithm also takes the validation data, as it runs over the
    data several times (epochs) to improve the error. Once we fit the model, we can
    deploy the model as a web service so that applications can use it:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的算法也会使用验证数据，因为它会在数据上运行多次（epochs）以减少错误。一旦我们拟合了模型，我们就可以将模型部署为Web服务，以便应用程序可以使用它：
- en: '[PRE28]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In our case, we will just hit the endpoint to get the predictions and evaluate
    the accuracy:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们只需点击端点即可获取预测结果并评估准确性：
- en: '[PRE29]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After running the preceding code we get the following output:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们得到以下输出：
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As you can see in the preceding code, each prediction comes along with a probability
    (which we will ignore for now). Next, we compute how many of these labels matched
    the original one:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前面代码中看到的，每个预测都伴随着一个概率（我们现在将忽略它）。接下来，我们计算有多少这样的标签与原始标签匹配：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'After running the preceding code we get the following output:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们得到以下输出：
- en: '[PRE32]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then run the next line of code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行下一行代码：
- en: '[PRE33]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As you can see in the following output from the previous code block, some of
    the labels matched the actual while some don''t:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在以下代码块输出的结果中可以看到，一些标签与实际匹配，而一些则没有匹配：
- en: '[PRE34]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we run the following code to build a boolean vector containing true or
    false depending on whether the actual matches the predicted result:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们运行以下代码来构建一个布尔向量，该向量包含真实或假，取决于实际是否与预测结果匹配：
- en: '[PRE35]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After running the preceding code we get the following output:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的代码后，我们得到以下输出：
- en: '[PRE36]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After we run the preceding output, we will run the following code to calculate
    the ratio of cases that match out of the total instances:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行前面的输出后，我们将运行以下代码来计算匹配的案例与总实例的比例：
- en: '[PRE37]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following output from the previous block shows the accuracy score:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从前一个代码块输出的准确性分数：
- en: '[PRE38]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We can see that the accuracy is lower than in our previous examples. This is
    for many reasons. For starters, we did not invest too much in data preparation
    in this case (for example, no stopwords are used in this case). However, the main
    reason for the lower accuracy is due to the fact we're using such little data.
    These models work best on larger datasets.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，准确性低于我们之前的示例。这有很多原因。首先，在这个案例中，我们没有在数据准备上投入太多（例如，这个案例中没有使用停用词）。然而，准确性较低的主要原因是我们使用了如此少的数据。这些模型在大型数据集上表现最佳。
- en: Naive Bayes – pros and cons
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Naive Bayes – 优点和缺点
- en: 'In this section, we present the advantages and disadvantages in selecting the
    Naive Bayes algorithm for classification problems:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了选择朴素贝叶斯算法进行分类问题的优缺点：
- en: '**Pros**'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**'
- en: '**Training time**:Naive Bayes algorithm only requires one pass on the entire
    dataset to calculate the posterior probabilities for each value of the feature
    in the dataset. So, when we are dealing with large datasets or low-budget hardware,
    Naive Bayes algorithm is a feasible choice for most data scientists.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练时间**：朴素贝叶斯算法只需要在整个数据集上遍历一次来计算数据集中每个特征值的后续概率。因此，当我们处理大型数据集或预算有限的硬件时，朴素贝叶斯算法对于大多数数据科学家来说是一个可行的选择。'
- en: '**Prediction time**: Since all the probabilities are pre-computed in the Naive
    Bayes algorithm, the prediction time of this algorithm is very efficient.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测时间**：由于朴素贝叶斯算法中所有概率都是预先计算的，因此该算法的预测时间非常高效。'
- en: '**Transparency**: Since the predictions of Naive Bayes algorithms are based
    on the posterior probability of each conditional feature, it is easy to understand
    which features are influencing the predictions. This helps users to understand
    the predictions.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明度**：由于朴素贝叶斯算法的预测基于每个条件特征的后续概率，因此很容易理解哪些特征正在影响预测。这有助于用户理解预测。'
- en: '**Cons**'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**'
- en: '**Prediction accuracy**: The prediction accuracy of the Naive Bayes algorithm
    is lower than other algorithms we will discuss in the book. Algorithm prediction
    accuracy is dataset dependent, a lot of research works have proved that algorithms
    such as random forest, **support vector machines** (**SVMs**), and **deep neural
    networks** (**DNNs**) outperform Naive Bayes algorithm in terms of classification
    accuracy.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测准确性**：朴素贝叶斯算法的预测准确性低于本书中将要讨论的其他算法。算法的预测准确性依赖于数据集，许多研究工作已经证明，例如随机森林、**支持向量机**（**SVMs**）和**深度神经网络**（**DNNs**）在分类准确性方面优于朴素贝叶斯算法。'
- en: '**Assumption of independence**: Since we assume that each feature is independent
    of each other, this algorithm may lose information for features that are dependent
    on each other. Other advanced algorithms do use this dependence information when
    calculating predictions.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立性假设**：由于我们假设每个特征相互独立，因此此算法可能会丢失相互依赖的特征的信息。其他高级算法在计算预测时确实会使用这种依赖信息。'
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced you to why ML is a crucial tool in a data scientist's
    repository. We discussed what a structured ML dataset looks like and how to identify
    the types of features in the dataset.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们向您介绍了为什么机器学习是数据科学家工具箱中的关键工具。我们讨论了结构化机器学习数据集的外观以及如何识别数据集中的特征类型。
- en: We took a deep dive into Naive Bayes classification algorithm, and studied how
    Bayes' theorem is used in Naive Bayes algorithm. Using Bayes' theorem, we can
    predict the probability of an event occurring based on the values of each feature,
    and select the event that has the highest probability.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们深入研究了朴素贝叶斯分类算法，并研究了贝叶斯定理在朴素贝叶斯算法中的应用。利用贝叶斯定理，我们可以根据每个特征值预测事件发生的概率，并选择概率最高的那个事件。
- en: We also presented an example of a Twitter dataset. We hope that you learned
    how to think about a text classification problem, and  how to build a Naive Bayes
    classification model to predict the source of a tweet. We also presented how the
    algorithm can be implemented in SageMaker, and how it can also be implemented
    using Apache Spark. This code base should help you tackle any text classification
    problems in the future. As the implementation is presented using SageMaker services
    and Spark, it can scale to datasets that can be gigabytes or terabytes in size.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还提供了一个Twitter数据集的示例。我们希望您学会了如何思考文本分类问题，以及如何构建朴素贝叶斯分类模型来预测推文的来源。我们还展示了如何在SageMaker中实现该算法，以及如何使用Apache
    Spark实现。这个代码库应该能帮助您在未来解决任何文本分类问题。由于实现使用了SageMaker服务和Spark，它可以扩展到可以容纳吉字节或太字节规模的数据集。
- en: We will look at how to deploy the ML models on actual production clusters in
    later chapters.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中探讨如何在实际生产集群上部署机器学习模型。
- en: Exercises
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Bayes' Theorem is not only useful for the Naive Bayes algorithm, but is also
    used for other purposes. Find two more algorithms where Bayes' theorem is applied,
    and explain how they are different than the Naive Bayes algorithm.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 贝叶斯定理不仅对朴素贝叶斯算法有用，还被用于其他目的。找出另外两个应用贝叶斯定理的算法，并解释它们与朴素贝叶斯算法的不同之处。
- en: In this chapter, we presented an example of a binary classifier. Based on our
    code to download tweets, create a new dataset where you download tweets from five
    different sources and build a Naive Bayes model that can predict the source of
    each tweet.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章中，我们提供了一个二元分类器的示例。基于我们下载推文的代码，创建一个新的数据集，从五个不同的来源下载推文，并构建一个可以预测每个推文来源的朴素贝叶斯模型。
- en: Identify scenarios for when you would use `scikit-learn`, Apache Spark, or SageMaker
    services for a particular problem.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定何时使用`scikit-learn`、Apache Spark或SageMaker服务解决特定问题的场景。
