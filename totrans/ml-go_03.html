<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Evaluation and Validation</h1>
                </header>
            
            <article>
                
<p class="mce-root">In order to have sustainable, responsible machine learning workflows and develop machine learning applications that produce true value, we need to be able to measure how well our machine learning models perform. We also need to ensure that our machine learning models generalize to data that they will see in production. If we don't do these things, we are basically shooting in the dark. We will have no understanding of the expected behavior of our models and we won't be able to improve them over time.</p>
<p>The process of measuring how a model is performing (with respect to certain data) is called <strong>evaluation</strong>. The process of ensuring that our model generalizes to data that we might expect to encounter is called <strong>validation</strong>. Both processes need to be present in every machine learning workflow and application, and we will cover both in this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluation</h1>
                </header>
            
            <article>
                
<p>A basic tenet of science is measurement, and the science of machine learning is not an exception. We need to be able to measure, or evaluate, how well our models are performing, so we can continue to improve on them, compare one model to another, and detect when our models are behaving poorly.</p>
<p>There's only one problem. How do we evaluate how our models are performing? Should we measure how fast they can be trained or make inferences? Should we measure how many times they get the right answer? How do we know what the right answer is? Should we measure how far we deviated from the observed values? How do we measure that distance?</p>
<p>As you can see, there are a lot of decisions to make around how we evaluate our models. What really matters is the context. In some cases, efficiency definitely matters, but every machine learning context requires us to measure how our predictions, inferences, or results match the ideal predictions, inferences, or results. Thus, measuring this comparison between computed results and ideal results should always take priority over speed optimizations.</p>
<p>Generally, there are some types of results that we will need to evaluate:</p>
<ul>
<li><strong>Continuous</strong>: Results such as total sales, stock price, and temperature that can take any continuous numerical value ($12102.21, 92 degrees, and so on)</li>
<li><strong>Categorical</strong>: Results such as fraud/not fraud, activity, and name that can take one of a finite number of categories (fraud, standing, Frank, and so on)</li>
</ul>
<div class="packt_tip">Each of these types of results have corresponding evaluation metrics that will be covered here. However, remember that your choice of evaluation metric depends on what you are trying to achieve with your machine learning model. There is no one-size-fits-all metric, and in some cases, you may even need to create your own metric.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Continuous metrics</h1>
                </header>
            
            <article>
                
<p>Let's say that we have a model that is supposed to predict some continuous value, like a stock price. Suppose that we have accumulated some predicted values that we can compare to actual observed values:</p>
<pre>observation,prediction
22.1,17.9
10.4,9.1
9.3,7.8
18.5,14.2
12.9,15.6
7.2,7.4
11.8,9.7<br/>...</pre>
<p>Now, how do we measure the performance of this model? Well, the first step would be taking the difference between the observed and predicted values to get an <kbd>error</kbd>:</p>
<pre>observation,prediction,error
22.1,17.9,4.2
10.4,9.1,1.3
9.3,7.8,1.5
18.5,14.2,4.3
12.9,15.6,-2.7
7.2,7.4,-0.2
11.8,9.7,2.1<br/>...</pre>
<p>The error gives us a general idea of <em>how far off we were</em> from the value that we were supposed to predict. However, it's not really feasible or practical to look at all the error values individually, especially when there is a lot of data. There could be a million or more of these error values. Thus, we need a way to understand the errors in aggregate.</p>
<p>The <strong>mean squared error</strong> (<strong>MSE</strong>) and <strong>mean absolute error</strong> (<strong>MAE</strong>) provide us with a view on errors in aggregate:</p>
<ul>
<li>MSE or <strong>mean squared deviation</strong> (<strong>MSD</strong>) is the average of the squares of all the errors</li>
<li>MAE is the average of the absolute values of all the errors</li>
</ul>
<p>Both MSE and MAE give us a good overall picture of how good our predictions are, but they do have differences. As the MSE takes the squares of the errors, large error values (for example, corresponding to outliers) are emphasized more than in the MAE. In other words, MSE is more sensitive to outliers. MAE, on the other hand, maintains the same units as the variable that we are trying to predict, and is thus directly comparable to these values.</p>
<p>For this dataset, we can parse the observed and predicted values and calculate the MAE and MSE as follows:</p>
<pre>// Open the continuous observations and predictions.
f, err := os.Open("continuous_data.csv")
if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a new CSV reader reading from the opened file.
reader := csv.NewReader(f)

// observed and predicted will hold the parsed observed and predicted values
// form the continuous data file.<br/>var observed []float64
var predicted []float64
<br/>// line will track row numbers for logging.<br/>line := 1<br/>// Read in the records looking for unexpected types in the columns.
for {

    // Read in a row. Check if we are at the end of the file.
    record, err := reader.Read()
    if err == io.EOF {
        break
    }

    // Skip the header.
    if line == 1 {
        line++
        continue
    }

    // Read in the observed and predicted values.
    observedVal, err := strconv.ParseFloat(record[0], 64)
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    predictedVal, err := strconv.ParseFloat(record[1], 64)
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    // Append the record to our slice, if it has the expected type.
    observed = append(observed, observedVal)
    predicted = append(predicted, predictedVal)
    line++<br/>}<br/><br/>// Calculate the mean absolute error and mean squared error.
var mAE float64<br/>var mSE float64
for idx, oVal := range observed {
    mAE += math.Abs(oVal-predicted[idx]) / float64(len(observed))<br/>    mSE += math.Pow(oVal-predicted[idx], 2) / float64(len(observed))
}

// Output the MAE and MSE value to standard out.
fmt.Printf("\nMAE = %0.2f\n", mAE)<br/>fmt.Printf("\nMSE = %0.2f\n\n", mSE)</pre>
<p>For our example data, this results in the following:</p>
<pre><strong>$ go build
$ ./myprogram 

MAE = 2.55

MSE = 10.51</strong></pre>
<p>To judge if these are good values or not, we need to compare them to the values in our observed data. In particular, the MAE is <kbd>2.55</kbd> and the mean of our observed values is 14.0, so our MAE is about 20% of our mean value. Not very good, depending on the context.</p>
<p>Along with the MSE and MAE, you will likely see <strong>R-squared</strong> (also known as <strong>R<sup>2</sup></strong> or <strong>R2</strong>), or the <strong>coefficient of determination</strong>, used as an evaluation metric for continuous variable models. R-squared also gives us a general idea about the deviations of our predictions, but the idea of R-squared is slightly different.</p>
<p>R-squared measures the proportion of the variance in the observed values that we capture in the predicted values. Remember that the values that we are trying to predict have some variability. For example, we might be trying to predict stock prices, interest rates, or disease progressions, which, by their very nature, aren't all the same. We are attempting to create a model that can predict this variability in the observed values, and the percentage of the variation that we capture is represented by R-squared.</p>
<p>Conveniently, <kbd><span>gonum.org/v1/gonum/stat</span></kbd> <span>has a built-in function to calculate R-squared:</span></p>
<pre>// Calculate the R^2 value.
rSquared := stat.RSquaredFrom(observed, predicted, nil)
<br/>// Output the R^2 value to standard out.
fmt.Printf("\nR^2 = %0.2f\n\n", rSquared)</pre>
<p>Running the preceding code for our example dataset results in the following:</p>
<pre><strong>$ go build
$ ./myprogram     

R^2 = 0.37</strong></pre>
<p>So, is this a good or bad R-squared? Remember that R-squared is a percentage and higher percentages are better. Here, we are capturing about 37% of the variance in the variable that we are trying to predict. Not very good.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Categorical metrics</h1>
                </header>
            
            <article>
                
<p>Let's say that we have a model that is supposed to predict some discrete value, such as fraud/not fraud, standing/sitting/walking, approved/not approved, and so on. Our data might look something like the following:</p>
<pre>observed,predicted
0,0
0,1
2,2
1,1
1,1
0,0
2,0
0,0<br/>...</pre>
<p>The observed values could take any one of a finite number of values (in this case 1, 2, or 3). Each of these values represents one of the discrete categories in our data (class 1 might correspond to a fraudulent transaction, class 2 might correspond to a transaction that is not fraudulent, and class 3 might correspond to an invalid transaction, for example). The predicted values could also take one of these discrete values. In evaluating our predictions, we want to somehow measure how right we were in making those discrete predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Individual evaluation metrics for categorical variables</h1>
                </header>
            
            <article>
                
<p>Actually, there are a huge number of ways to evaluate discrete predictions with metrics, including accuracy, precision, recall, specificity, sensitivity, fallout, false omission rate, and many more. As with continuous variables, there is no one-size-fits-all metric for evaluation. Each time you approach a problem, you need to determine the metric that fits the problem and matches the goals of the project. You don't want to optimize for the wrong things and then waste a bunch of time reimplementing your model based on other metrics.</p>
<p>To understand these metrics and determine which is appropriate for our use case, we need to realize that there are a number of different scenarios that could occur when we are making discrete predictions:</p>
<ul>
<li><strong>True Positive</strong> (<strong>TP</strong>): We predicted a certain category, and the observation was actually that category (for example, we predicted fraud and the observation was fraud)</li>
<li><strong>False Positive</strong> (<strong>FP</strong>): We predicted a certain category, but the observation was actually another category (for example, we predicted fraud but the observation was not fraud)</li>
<li><strong>True Negative</strong> (<strong>TN</strong>): We predicted that the observation wasn't a certain category, and the observation was not that category (for example, we predicted not fraud and the observation was not fraud)</li>
<li><strong>False Negative</strong> (<strong>FN</strong>): We predicted that the observation wasn't a certain category, but the observation was actually that category (for example, we predicted not fraud but the observation was fraud)</li>
</ul>
<p>You can see that there are a number of ways we can combine, aggregate, and measure these scenarios. In fact, we could even aggregate/measure them in some sort of unique way related to our specific problem. However, there are some pretty standard ways of aggregating and measuring these scenarios that result in the following common metrics:</p>
<ul>
<li><strong>Accuracy</strong>: The percentage of predictions that were right, or <em>(TP + TN)/(TP + TN + FP + FN)</em></li>
<li><strong>Precision</strong>: The percentage of positive predictions that were actually positive, or <em>TP/(TP + FP)</em></li>
<li><strong>Recall</strong>: The percentage of positive predictions that were identified as positive, or <em>TP/(TP + FN)</em></li>
</ul>
<div class="packt_infobox">Even though I'm going to emphasize these here, you should take a look at other common metrics and their implications. A good overview can be found at <a href="https://en.wikipedia.org/wiki/Precision_and_recall">https://en.wikipedia.org/wiki/Precision_and_recall</a>.</div>
<p>The following is an example that parses our data and calculates accuracy. First, we read in our <kbd>labeled.csv</kbd> file, create a CSV reader, and initialize two slices that will hold our parsed observed/predicted values:</p>
<pre class="mce-root">// Open the binary observations and predictions.<br/>f, err := os.Open("labeled.csv")<br/>if err != nil {<br/>    log.Fatal(err)<br/>}<br/>defer f.Close()<br/><br/>// Create a new CSV reader reading from the opened file.<br/>reader := csv.NewReader(f)<br/><br/>// observed and predicted will hold the parsed observed and predicted values
// form the labeled data file.
var observed []int
var predicted []int</pre>
<p>Then we will iterate over the records in the CSV parsing the values, and we will compare the observed and predicted values to calculate accuracy:</p>
<pre class="mce-root">// line will track row numbers for logging.
line := 1

// Read in the records looking for unexpected types in the columns.
for {

    // Read in a row. Check if we are at the end of the file.
    record, err := reader.Read()
    if err == io.EOF {
        break
    }

    // Skip the header.
    if line == 1 {
        line++
        continue
    }

    // Read in the observed and predicted values.
    observedVal, err := strconv.Atoi(record[0])
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    predictedVal, err := strconv.Atoi(record[1])
    if err != nil {
        log.Printf("Parsing line %d failed, unexpected type\n", line)
        continue
    }

    // Append the record to our slice, if it has the expected type.
    observed = append(observed, observedVal)
    predicted = append(predicted, predictedVal)
    line++
}

// This variable will hold our count of true positive and
// true negative values.
var truePosNeg int

// Accumulate the true positive/negative count.
for idx, oVal := range observed {
    if oVal == predicted[idx] {
        truePosNeg++
    }
}

// Calculate the accuracy (subset accuracy).
accuracy := float64(truePosNeg) / float64(len(observed))

// Output the Accuracy value to standard out.
fmt.Printf("\nAccuracy = %0.2f\n\n", accuracy)</pre>
<p>Running this results in the following:</p>
<pre><strong>$ go build
$ ./myprogram

Accuracy = 0.97</strong></pre>
<p>97%! That's pretty good. That means we were right 97% of the time.</p>
<p>We can similarly calculate precision and recall. However, you may have noticed that there are a couple of ways we can do this when we have more than two categories or classes. We could consider class 1 as positive and the other classes as negative, class 2 as positive and the other classes as negative, and so on. That is, we could calculate a precision or recall for each of our classes, as shown in the following code sample:</p>
<pre>// classes contains the three possible classes in the labeled data.
classes := []int{0, 1, 2}

// Loop over each class.
for _, class := range classes {

    // These variables will hold our count of true positives and
    // our count of false positives.
    var truePos int
    var falsePos int
    var falseNeg int

    // Accumulate the true positive and false positive counts.
    for idx, oVal := range observed {

        switch oVal {

        // If the observed value is the relevant class, we should check to
        // see if we predicted that class.
        case class:
            if predicted[idx] == class {
                truePos++
                continue
            }

            falseNeg++

        // If the observed value is a different class, we should <br/>        // check to see if we predicted a false positive.
        default:
            if predicted[idx] == class {
                falsePos++
            }
        }
    }

    // Calculate the precision.
    precision := float64(truePos) / float64(truePos+falsePos)

    // Calculate the recall.
    recall := float64(truePos) / float64(truePos+falseNeg)

    // Output the precision value to standard out.
    fmt.Printf("\nPrecision (class %d) = %0.2f", class, precision)
    fmt.Printf("\nRecall (class %d) = %0.2f\n\n", class, recall)
}</pre>
<p>Running this code results in the following:</p>
<pre><strong>$ go build
$ ./myprogram

Precision (class 0) = 1.00
Recall (class 0) = 1.00


Precision (class 1) = 0.96
Recall (class 1) = 0.94


Precision (class 2) = 0.94
Recall (class 2) = 0.96</strong></pre>
<p>Notice that the precision and recall are slightly different metrics and have different implications. If we wanted to get an overall precision or recall, we could average the per-class precisions and recalls. In fact, if certain classes were more important than other classes, we could take a weighted average of these and use that as our evaluation metric.</p>
<p>You can see that a couple of the metrics are 100%. This seems good, but it might actually indicate a problem, as we will further discuss.</p>
<div class="packt_tip">In some cases, such as finance and banking, false positives or other cases may be very costly for certain classes. For example, a mislabeling a transaction as fraudulent might result in significant losses. On the other hand, certain results for other classes might be negligible. These scenario might warrant the use of a custom metric or cost function that weights certain classes, certain results, or certain combinations of results as more important than others.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Confusion matrices, AUC, and ROC</h1>
                </header>
            
            <article>
                
<p>In addition to calculating individual numerical metrics for our models, there are a variety of techniques to combine various metrics into a form that gives you a more complete representation of model performance. These include, but are certainly not limited to, <strong>confusion matrices</strong> and <strong>area under the curve</strong> (<strong>AUC</strong>)/<strong>Receiver Operating Characteristic</strong> (<strong>ROC</strong>) <strong>curves</strong>.</p>
<p>Confusion matrices allow us to visualize the various <strong>TP</strong>, <strong>TN</strong>, <strong>FP</strong>, and <strong>FN</strong> values that we predict in a two-dimensional format. A confusion matrix has rows corresponding to the categories that you were supposed to predict, and columns corresponding to categories that were predicted. Then, the value of each element is the corresponding count:</p>
<div class="CDPAlignCenter CDPAlign"><img height="112" width="279" class="image-border" src="assets/56626b65-b9f3-4207-8ddf-f69de05aa45f.png"/></div>
<p>As you can see, the ideal situation is that your confusion matrix only has entries on the diagonal (<strong>TP</strong>, <strong>TN</strong>). The diagonal elements represent predicting a certain category and the observation actually being in that category. The off-diagonal elements include counts for predictions that were incorrect.</p>
<p>This type of confusion matrix can be especially useful for problems that have more than two categories. For example, you may be trying to predict various activities based on mobile accelerator and position data. These activities may include more than two categories, such as standing, sitting, running, driving, and so on. The confusion matrix for this problem with be larger than 2 x 2 and would allow you to quickly gauge the overall performance of your model on all categories, and identify categories in which your model is performing poorly.</p>
<p>In addition to confusion matrices, ROC curves are commonly used to get an overall picture of the performance of binary classifiers (or models that are trained to predict one of two categories). ROC curves plot the recall versus false positive rate (<em>FP/(FP + TN)</em>) for every possible classification threshold.</p>
<p>The thresholds used in an ROC curve represent various boundaries or rankings in which you are separating the two categories of your classification. That is, the model that is evaluated by the ROC curve must make a prediction for the two classes based on probability, ranking, or score (referred to as a score in the following image). In every example mentioned earlier, a score is classified one way, and vise versa:</p>
<div class="CDPAlignCenter CDPAlign"><img height="217" width="285" class="image-border" src="assets/d9847ec0-35b3-40fb-b4bf-f58bc3ba01ee.png"/></div>
<p>To generate an ROC curve, we plot a point for each score or rank in our testing examples (recall, false positive rate). We can then connect these to form a curve. In many cases, you will see a straight line plotted down the diagonal of the ROC curve plot. This straight line is a reference line for a classifier, with approximately random predictive power:</p>
<div class="CDPAlignCenter CDPAlign"><img height="181" width="365" class="alignnone size-full wp-image-584 image-border" src="assets/1d839efe-add3-4dc7-ab73-e408fc5a7523.png"/></div>
<p>A good ROC curve is one that is in the upper left section of the plot, which means that our model has better than random predictive power. The more that the ROC curve hugs the upper left hand side of the plot, the better. This means that good ROC curves have more AUC; AUC for ROC curves is also used as an evaluation metric. Refer to the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="202" width="264" class="image-border" src="assets/72073033-e4d7-40f2-96f2-ba2453df4935.png"/></div>
<p><kbd>gonum.org/v1/gonum/stat</kbd> has some built-in functions and types that help you build ROC curves and AUC metrics:</p>
<pre>func ROC(n int, y []float64, classes []bool, weights []float64) (tpr, fpr []float64)</pre>
<p>Here is a quick example that calculates the AUC for an ROC curve with gonum:</p>
<pre>// Define our scores and classes.<br/>scores := []float64{0.1, 0.35, 0.4, 0.8}
classes := []bool{true, false, true, false}
<br/>// Calculate the true positive rates (recalls) and<br/>// false positive rates.
tpr, fpr := stat.ROC(0, scores, classes, nil)<br/><span class="com">// Compute the Area Under Curve.</span>
auc := integrate.Trapezoidal(fpr, tpr)<br/><br/>// Output the results to standard out.
fmt.Printf("true positive rate: %v\n", tpr)
fmt.Printf("false positive rate: %v\n", fpr)
fmt.Printf("auc: %v\n", auc)</pre>
<p>Running this code results in the following:</p>
<pre><strong>$ go build
$ ./myprogram 
true positive rate: [0 0.5 0.5 1 1]
false positive rate: [0 0 0.5 0.5 1]
auc: 0.75</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Validation</h1>
                </header>
            
            <article>
                
<p>So, now we know some ways to measure how well our model is performing. In fact, if we wanted to, we could create a super sophisticated, complicated model that could predict every observation without error. For example, we could create a model that would take the index of the row of the observation and return the exact answer for each of those rows. It might be a really big function with a lot of parameters, but it would return the correct answers.</p>
<p>So, what's the problem with this? Well, the problem is that it would not generalize to new data. Our complicated model would predict really well for the data that we would expose it to, but once we try some new input data (that isn't part of our training dataset), the model would likely perform poorly.</p>
<p>We call this type of model (that doesn't generalize) a model that has been <strong>overfit</strong>. That is, our process of making the model more and more complicated based on the data that was available to us was overfitting the model.</p>
<p>Overfitting can happen when predicting continuous values or discrete/categorical values:</p>
<div class="CDPAlignCenter CDPAlign"><img height="231" width="466" class="image-border" src="assets/b61c8e32-cce8-4f5f-bc80-632475a1f966.png"/></div>
<p>To prevent overfitting, we need to validate our model. There are multiple ways to perform validation, and we will cover a couple of these here.</p>
<div class="packt_tip">Every time you are productionizing a model, you need to ensure that you have validated your model and understand how it will generalize to new data.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training and test sets</h1>
                </header>
            
            <article>
                
<p>The first method to help prevent overfitting is to train or fit your model on a portion of your dataset and then test or evaluate your model on a different portion of your dataset. Training your model generally consists of parameterizing one or more functions that make up your model, such that the functions that predict what you are trying to predict. Then, you can evaluate this trained model using one or more of the evaluation metrics that we discussed previously. The important thing here is that you do not want to test/evaluate your model on the same data that is used to train your model.</p>
<p>By reserving part of your data for testing, you are simulating the scenario in which your model sees new data. That is, the model is making predictions based on data that was not used in parameterizing the model.</p>
<div class="CDPAlignCenter CDPAlign"><img height="125" width="255" class="image-border" src="assets/d041cb25-7cdc-4f3d-b862-81f27c597e7e.png"/></div>
<p>Many people start by splitting 80% of their data into a training data set and 20% into a test set (an 80/20 split). However, you will see different people splitting their datasets up in different proportions. The proportion of test to training data depends a little bit on the type and amount of data that you have and the model that you are trying to train. Generally, you want to ensure that both your training data and test data are a fairly accurate representation of your data on a large scale.</p>
<p>For example, if you are trying to predict one of a few different categories, A, B, and C, you wouldn't want your training data to include observations that only correspond to A and B. A model trained on such a dataset would likely only be able to predict the A and B categories. Likewise, you wouldn't want your test set to include some subset of the categories, or artificially weighted proportions of the categories. This could very easily happen, depending on how your data was generated.</p>
<p>In addition, you want to make sure that you have enough training data to reduce the variability in your determined parameters as they are computed over and over. If you have too few training data points, or poorly sampled training data points, your model training may produce parameters with a lot of variability, or it may not even be able to converge numerically. These are indications that your model lacks predictive power.</p>
<p>Typically, as you increase the complexity of your model, you will be able to improve the evaluation metric that you are using for your training data, but at some point, the evaluation metric will start getting worse for your test data. When the evaluation metric starts to get worse for your test data, you are starting to overfit your model. The ideal scenario is when you are able to increase your model complexity up to the inflection point, where the test evaluation metric starts to degrade. Another way of putting this (which fits very well into our general philosophy for model building in this book) is that we want the most interpretable model (or simplistic model) that can produce valuable results.</p>
<div class="CDPAlignCenter CDPAlign"><img height="249" width="422" class="image-border" src="assets/82f10fbd-6a15-46dc-bcff-829c780a6f2c.png"/></div>
<p>One way to quickly split a dataset into training and test sets is with <kbd>github.com/kniren/gota/dataframe</kbd>. Let's demonstrate this using a dataset, which includes a bunch of anonymized information about medical patients and a corresponding indication of the progression of disease and diabetes:</p>
<pre>age,sex,bmi,map,tc,ldl,hdl,tch,ltg,glu,y
0.0380759064334,0.0506801187398,0.0616962065187,0.021872354995,-0.0442234984244,-0.0348207628377,-0.043400845652,-0.00259226199818,0.0199084208763,-0.0176461251598,151.0
-0.00188201652779,-0.044641636507,-0.0514740612388,-0.0263278347174,-0.00844872411122,-0.0191633397482,0.0744115640788,-0.0394933828741,-0.0683297436244,-0.0922040496268,75.0
0.0852989062967,0.0506801187398,0.0444512133366,-0.00567061055493,-0.0455994512826,-0.0341944659141,-0.0323559322398,-0.00259226199818,0.00286377051894,-0.0259303389895,141.0
-0.0890629393523,-0.044641636507,-0.0115950145052,-0.0366564467986,0.0121905687618,0.0249905933641,-0.0360375700439,0.0343088588777,0.0226920225667,-0.00936191133014,206.0
0.00538306037425,-0.044641636507,-0.0363846922045,0.021872354995,0.00393485161259,0.0155961395104,0.00814208360519,-0.00259226199818,-0.0319914449414,-0.0466408735636,135.0
...</pre>
<p>You can retrieve this data set here: <a href="https://archive.ics.uci.edu/ml/datasets/diabetes">https://archive.ics.uci.edu/ml/datasets/diabetes</a>.</p>
<p>To split this data with <kbd>github.com/kniren/gota/dataframe</kbd>, we can do the following (where we save the training and test splits to respective CSV files):</p>
<pre>// Open the diabetes dataset file.
f, err := os.Open("diabetes.csv")<br/>if err != nil {
    log.Fatal(err)
}
defer f.Close()

// Create a dataframe from the CSV file.
// The types of the columns will be inferred.
diabetesDF := dataframe.ReadCSV(f)

// Calculate the number of elements in each set.
trainingNum := (4 * diabetesDF.Nrow()) / 5
testNum := diabetesDF.Nrow() / 5
if trainingNum+testNum &lt; diabetesDF.Nrow() {
    trainingNum++
}

// Create the subset indices.
trainingIdx := make([]int, trainingNum)
testIdx := make([]int, testNum)

// Enumerate the training indices.
for i := 0; i &lt; trainingNum; i++ {
    trainingIdx[i] = i
}

// Enumerate the test indices.
for i := 0; i &lt; testNum; i++ {
    testIdx[i] = trainingNum + i
}

// Create the subset dataframes.
trainingDF := diabetesDF.Subset(trainingIdx)
testDF := diabetesDF.Subset(testIdx)

// Create a map that will be used in writing the data
// to files.
setMap := map[int]dataframe.DataFrame{
    0: trainingDF,
    1: testDF,
}

// Create the respective files.
for idx, setName := range []string{"training.csv", "test.csv"} {

    // Save the filtered dataset file.
    f, err := os.Create(setName)
    if err != nil {
        log.Fatal(err)
    }

    // Create a buffered writer.
    w := bufio.NewWriter(f)

    // Write the dataframe out as a CSV.
    if err := setMap[idx].WriteCSV(w); err != nil {
        log.Fatal(err)
    }
}</pre>
<p>Running this results in the following:</p>
<pre><strong>$ go build
$ ./myprogram 
$ wc -l *.csv
   443 diabetes.csv
    89 test.csv
   355 training.csv
   887 total</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Holdout set</h1>
                </header>
            
            <article>
                
<p>We are making progress to ensure that our models generalize using training and test sets. However, imagine the following scenario:</p>
<ol>
<li>We develop a first version of our model based on our training set.</li>
<li>We test this first version of our model on the test set.</li>
<li>We aren't satisfied with the result on the test set, so we loop back to step 1 and repeat.</li>
</ol>
<p>This process might seem logical, but you are probably already seeing a problem that can result from this procedure. We can actually overfit our model on the test data by iteratively exposing the model to our test set.</p>
<p>There are a couple of ways to deal with this extra level of overfitting. The first is by simply creating another split of our data called a <strong>holdout set</strong> (also known as a <strong>validation set</strong>). So, now we would have a training set, test set, and holdout set. This is sometimes called the three dataset validation, for obvious reasons.</p>
<div class="CDPAlignCenter CDPAlign"><img height="137" width="280" class="image-border" src="assets/9421ffa1-cc2d-4dbf-911a-8a474792dc25.png"/></div>
<p>Keep in mind that, to truly get an idea about the general performance of your model, your holdout set must never be used in training and testing. You should reserve this dataset for validation after you have gone through the process of training your model, making adjustments to the model, and getting an acceptable performance on the test dataset.</p>
<div class="packt_tip">You might be wondering how you can manage this splitting of data over time and recover different sets of data used to train or test certain models. This "provenance" of data is crucial when trying to maintain integrity in your machine learning workflows. This is also exactly what Pachyderm's data versioning (introduced in Chapter 1, <em>Gathering and Organizing Data</em>) was created to handle. We will see exactly how this plays out at scale later in Chapter 9, <em>Deploying and distributing Analyses and Models</em>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cross validation</h1>
                </header>
            
            <article>
                
<p>In addition to reserving a holdout set for validation, cross validation is a common technique to validate the generality of a model. In cross validation, or k-fold cross validation, you actually perform <em>k</em> random splits of your dataset into different training and test combinations. Think of these as <em>k</em> experiments.</p>
<p>Once you have performed each split, you train your model on the training data for that split, and then evaluate it on the test data for that split. This process results in an evaluation metric result for each random split of your data. You can then average these evaluation metrics to get an overall evaluation metric that is a more general representation of model performance than any one of the individual evaluation metrics by themselves. You can also look at the variance in the evaluation metrics to get an idea about the stability of your various experiments. This process is illustrated in the following image:</p>
<div class="CDPAlignCenter CDPAlign"><img height="303" width="339" class="image-border" src="assets/6c333632-adaf-4adc-b750-2693dc863aa9.png"/></div>
<p>Some advantages of using cross validation, in comparison to dataset validation, are as follows:</p>
<ul>
<li>You are making use of your entire dataset, and thus, are actually exposing your model to more training examples and more testing examples</li>
<li>There are some convenience functions and packaging already written for cross validation</li>
<li>It helps prevent the biases that may result from choosing a single validation set</li>
</ul>
<p>The <kbd>github.com/sjwhitworth/golearn</kbd> is one Go package that provides some convenience functions for cross validation. Actually, <kbd>github.com/sjwhitworth/golearn</kbd> includes a bunch of machine learning functionality that we will cover later on in the book, but for now, let's just look at what functionality is available for cross validation.</p>
<p>If you look at the <kbd>github.com/sjwhitworth/golearn/evaluation</kbd> package Godocs, you will see the following function that is available for cross validation:</p>
<pre>func GenerateCrossFoldValidationConfusionMatrices(data base.FixedDataGrid, cls base.Classifier, folds int) ([]ConfusionMatrix, error)</pre>
<p>This function can actually be used with a variety of models, but here is an example using a decision tree model (don't worry about the details of the model here):</p>
<pre>// Define the decision tree model.
tree := trees.NewID3DecisionTree(param)

// Perform the cross validation.
cfs, err := evaluation.GenerateCrossFoldValidationConfusionMatrices(myData, tree, 5)
if err != nil {
    panic(err)
}<br/><br/>// Calculate the metrics.
mean, variance := evaluation.GetCrossValidatedMetric(cfs, evaluation.GetAccuracy)
stdev := math.Sqrt(variance)

// Output the results to standard out.
fmt.Printf("%0.2f\t\t%.2f (+/- %.2f)\n", param, mean, stdev*2)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>Evaluation:</p>
<ul>
<li>Essay on overfitting: <a href="http://scott.fortmann-roe.com/docs/MeasuringError.html">http://scott.fortmann-roe.com/docs/MeasuringError.html</a></li>
<li>Essay on bias-variance trade-off: <a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">http://scott.fortmann-roe.com/docs/BiasVariance.html</a></li>
<li>Comparison of categorical evaluation metrics: <a href="https://en.wikipedia.org/wiki/Precision_and_recall">https://en.wikipedia.org/wiki/Precision_and_recall</a></li>
<li><span><kbd>gonum.org/v1/gonum/stat</kbd> docs: <a href="https://godoc.org/gonum.org/v1/gonum/stat">https://godoc.org/gonum.org/v1/gonum/stat</a><a href="https://godoc.org/github.com/gonum/stat"/></span></li>
<li><kbd>github.com/sjwhitworth/golearn/evaluation</kbd> docs: <a href="https://godoc.org/github.com/sjwhitworth/golearn/evaluation">https://godoc.org/github.com/sjwhitworth/golearn/evaluation</a></li>
</ul>
<p>Validation:</p>
<ul>
<li><kbd>github.com/kniren/gota/dataframe</kbd> docs: <a href="https://godoc.org/github.com/kniren/gota/dataframe">https://godoc.org/github.com/kniren/gota/dataframe</a></li>
<li><kbd>github.com/sjwhitworth/golearn/evaluation</kbd> docs: <a href="https://godoc.org/github.com/sjwhitworth/golearn/evaluation">https://godoc.org/github.com/sjwhitworth/golearn/evaluation</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Choosing an appropriate evaluation metric and laying out a procedure for evaluation/validation are essential parts of any machine learning project. You have learned about a variety of relevant evaluation metrics and how to avoid overfitting using holdout sets and/or cross validation. In the next chapter, we will start looking at machine learning models and we will build our first model using linear regression!</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </body></html>