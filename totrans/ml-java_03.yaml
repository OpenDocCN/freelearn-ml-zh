- en: Basic Algorithms - Classification, Regression, and Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we reviewed the key Java libraries that are used for
    machine learning and what they bring to the table. In this chapter, we will finally
    get our hands dirty. We will take a closer look at the basic machine learning
    tasks, such as classification, regression, and clustering. Each of the topics
    will introduce basic algorithms for classification, regression, and clustering.
    The example datasets will be small, simple, and easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering attributes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building classification, regression, and clustering models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before you start
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you start, download the latest stable version of Weka (Weka 3.8 at the
    time of writing) from [http://www.cs.waikato.ac.nz/ml/weka/downloading.html](http://www.cs.waikato.ac.nz/ml/weka/downloading.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple download options available. You''ll want to use Weka as
    a library in your source code, so make sure that you skip the self-extracting
    executables and download the ZIP archive, as shown in the following screenshot.
    Unzip the archive and locate `weka.jar` within the extracted archive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7db1f2c9-1619-4251-9a3b-ef6a59eb9538.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll use the Eclipse IDE to show examples; follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start a new Java project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Right-click on the project properties, select Java Build Path, click on the
    Libraries tab, and select Add External JARs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Navigate to extract the Weka archive and select the `weka.jar` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's it; we are ready to implement the basic machine learning techniques!
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start with the most commonly used machine learning technique: classification.
    As we reviewed in the first chapter, the main idea is to automatically build a
    mapping between the input variables and the outcome. In the following sections,
    we will look at how to load the data, select features, implement a basic classifier
    in Weka, and evaluate its performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this task, we will take a look at the `ZOO` database. The database contains
    101 data entries of animals described with 18 attributes, as shown in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| animal | aquatic | fins |'
  prefs: []
  type: TYPE_TB
- en: '| hair | predator | legs |'
  prefs: []
  type: TYPE_TB
- en: '| feathers | toothed | tail |'
  prefs: []
  type: TYPE_TB
- en: '| eggs | backbone | domestic |'
  prefs: []
  type: TYPE_TB
- en: '| milk | breathes | cat size |'
  prefs: []
  type: TYPE_TB
- en: '| airborne | venomous | type |'
  prefs: []
  type: TYPE_TB
- en: 'An example entry in the dataset is a lion, with the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`animal`: lion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hair`: true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feathers`: false'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eggs`: false'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`milk`: true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`airborne`: false'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aquatic`: false'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predator`: true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`toothed`: true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`backbone`: true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`breathes`: true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`venomous`: false'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fins`: false'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`legs`: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tail`: true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`domestic`: false'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`catsize`: true'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`type`: mammal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our task will be to build a model to predict the outcome variable, `animal`,
    given all of the other attributes as input.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we start the analysis, we will load the data in Weka's **Attribute-Relation
    File Format** (**ARFF**) and print the total number of loaded instances. Each
    data sample is held within a `DataSource` object, while the complete dataset,
    accompanied by meta-information, is handled by the `Instances` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the input data, we will use the `DataSource` object that accepts a
    variety of file formats and converts them into `Instances`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will provide the number of loaded instances as output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We can also print the complete dataset by calling the `data.toString()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Our task is to learn a model that is able to predict the `animal` attribute
    in the future examples for which we know the other attributes, but do not know
    the `animal` label. Hence, we will remove the `animal` attribute from the training
    set. We will accomplish this by filtering out the animal attribute, using the
    `Remove()` filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we set a string table of parameters, specifying that the first attribute
    must be removed. The remaining attributes are used as our dataset for training
    a classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we call the `Filter.useFilter(Instances, Filter)` static method to
    apply the filter on the selected dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As introduced in [Chapter 1](11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml), *Applied
    Machine Learning Quick Start*, one of the preprocessing steps is focused on feature
    selection, also known as **attribute selection**. The goal is to select a subset
    of relevant attributes that will be used in a learned model. Why is feature selection
    important? A smaller set of attributes simplifies the models and makes them easier
    for users to interpret. This usually requires shorter training and reduces overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Attribute selection can take the class value into account or it cannot. In the
    first case, an attribute selection algorithm evaluates the different subsets of
    features and calculates a score that indicates the quality of selected attributes.
    We can use different searching algorithms, such as exhaustive search and best-first
    search, and different quality scores, such as information gain, the Gini index,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weka supports this process with an `AttributeSelection` object, which requires
    two additional parameters: an evaluator, which computes how informative an attribute
    is, and a ranker, which sorts the attributes according to the score assigned by
    the evaluator.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following steps to perform selection:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we will use information gain as an evaluator, and we will
    rank the features by their information gain score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will initialize an `AttributeSelection` object and set the evaluator, ranker,
    and data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We will print an order list of attribute `indices`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This process will provide the following result as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The most informative attributes are `12` (fins), `3` (eggs), `7` (aquatic),
    `2` (hair), and so on. Based on this list, we can remove additional, non-informative
    features in order to help the learning algorithms achieve more accurate and faster
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: What would make the final decision about the number of attributes to keep? There's
    no rule of thumb related to an exact number; the number of attributes depends
    on the data and the problem. The purpose of attribute selection is to choose attributes
    that serve your model better, so it is best to focus on whether the attributes
    are improving the model.
  prefs: []
  type: TYPE_NORMAL
- en: Learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have loaded our data and selected the best features, and we are ready to
    learn some classification models. Let's begin with basic decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: In Weka, a decision tree is implemented within the `J48` class, which is a reimplementation
    of Quinlan's famous C4.5 decision tree learner (Quinlan, 1993).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will make a decision tree by using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize a new `J48` decision tree learner. We can pass additional parameters
    with a string table—for instance, the tree pruning that controls the model complexity
    (refer to [Chapter 1](11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml), *Applied Machine
    Learning Quick Start*). In our case, we will build an un-pruned tree; hence, we
    will pass a single `-U` parameter, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will call the `buildClassifier(Instances)` method to initialize the learning
    process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The built model is now stored in a `tree` object. We can provide the entire
    `J48` unpruned tree by calling the `toString()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The tree in the output has `17` nodes in total and `9` of them are terminal
    (`Leaves`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to present the tree is to leverage the built-in `TreeVisualizer`
    tree viewer, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code results in the following output frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10ddbded-4f22-4f20-a2d5-eefd3158464e.png)'
  prefs: []
  type: TYPE_IMG
- en: The decision process starts at the top node, also known as the root node. The
    node label specifies the attribute value that will be checked. In our example,
    first, we check the value of the `feathers` attribute. If the feather is present,
    we follow the right-hand branch, which leads us to the leaf labeled `bird`, indicating
    that there are `20` examples supporting this outcome. If the feather is not present,
    we follow the left-hand branch, which leads us to the `milk` attribute. We check
    the value of the attribute again, and we follow the branch that matches the attribute
    value. We repeat the process until we reach a leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build other classifiers by following the same steps: initialize a classifier,
    pass the parameters controlling the model complexity, and call the `buildClassifier(Instances)`
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to use a trained model to assign a class
    label to a new example whose label is unknown.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying new data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose that we record attributes for an animal whose label we do not know;
    we can predict its label from the learned classification model. We will use the
    following animal for this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f4f4365-22d9-44d1-9cfc-942e707c67cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we construct a feature vector describing the new specimen, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we call the `classify(Instance)` method on the model, in order to obtain
    the class value. The method returns the label index, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This will provide the `mammal` class label as output.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation and prediction error metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We built a model, but we do not know if it can be trusted. To estimate its performance,
    we can apply a cross-validation technique that was explained in [Chapter 1](11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml),
    *Applied Machine Learning Quick Start*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weka offers an `Evaluation` class for implementing cross-validation. We pass
    the model, data, number of folds, and an initial random seed, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The evaluation results are stored in the `Evaluation` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'A mix of the most common metrics can be invoked by calling the `toString()`
    method. Note that the output does not differentiate between regression and classification,
    so make sure to pay attention to the metrics that make sense, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the classification, we are interested in the number of correctly/incorrectly
    classified instances.
  prefs: []
  type: TYPE_NORMAL
- en: The confusion matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Furthermore, we can inspect where a particular misclassification has been made
    by examining the confusion matrix. The confusion matrix shows how a specific class
    value was predicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting confusion matrix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The column names in the first row correspond to the labels assigned by the classification
    node. Each additional row then corresponds to an actual true class value. For
    instance, the second row corresponds to instances with the `mammal` true class
    label. In the column line, we read that all mammals were correctly classified
    as mammals. In the fourth row, `reptiles`, we notice that three were correctly
    classified as `reptiles`, while one was classified as `fish` and one as `insect`.
    The confusion matrix gives us insight into the kinds of errors that our classification
    model can make.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a classification algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Naive Bayes is one of the most simple, efficient, and effective inductive algorithms
    in machine learning. When features are independent, which is rarely true in the
    real world, it is theoretically optimal and, even with dependent features, its
    performance is amazingly competitive (Zhang, 2004). The main disadvantage is that
    it cannot learn how features interact with each other; for example, despite the
    fact that you like your tea with lemon or milk, you hate a tea that has both of
    them at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The main advantage of the decision tree is that it is a model that is easy to
    interpret and explain, as we studied in our example. It can handle both nominal
    and numeric features, and you don't have to worry about whether the data is linearly
    separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some other examples of classification algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`weka.classifiers.rules.ZeroR`: This predicts the majority class and is considered
    a baseline; that is, if your classifier''s performance is worse than the average
    value predictor, it is not worth considering it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.trees.RandomTree`: This constructs a tree that considers
    *K* randomly chosen attributes at each node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.trees.RandomForest`: This constructs a set (forest) of random
    trees and uses majority voting to classify a new instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.lazy.IBk`: This is the k-nearest neighbors classifier that
    is able to select an appropriate value of neighbors, based on cross-validation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.functions.MultilayerPerceptron`: This is a classifier based
    on neural networks that uses backpropagation to classify instances. The network
    can be built by hand, or created by an algorithm, or both.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.bayes.NaiveBayes`: This is a Naive Bayes classifier that
    uses estimator classes, where numeric estimator precision values are chosen based
    on the analysis of the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.meta.AdaBoostM1`: This is the class for boosting a nominal
    class classifier by using the `AdaBoost M1` method. Only nominal class problems
    can be tackled. This often dramatically improves the performance, but sometimes,
    it overfits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weka.classifiers.meta.Bagging`: This is the class for bagging a classifier
    to reduce the variance. This can perform classification and regression, depending
    on the base learner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification using Encog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, you saw how to use a Weka library for classification.
    In this section, we will quickly look at how the same can be achieved by using
    the Encog library. Encog requires us to build a model to do the classification.
    Download the Encog library from [https://github.com/encog/encog-java-core/releases](https://github.com/encog/encog-java-core/releases).
    Once downloaded, add the `.jar` file in the Eclipse project, as explained at the
    beginning of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will use the `iris` dataset, which is available in `.csv`
    format; it can be downloaded from [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris).
    From the download path, copy the `iris.data.csv` file into your data directory.
    This file contains the data of 150 different flowers. It contains four different
    measurements about the flowers, and the last column is a label.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now perform the classification, using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `VersatileMLDataSet` method to load the file and define all
    four columns. The next step is to call the `analyze` method that will read the
    entire file and find the statistical parameters, such as the mean, the standard
    deviation, and many more:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to define the output column. Then, it''s time to normalize
    the data; but before that, we need to decide on the model type according to which
    the data will be normalized, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to fit the model on a training set, leaving a test set aside.
    We will hold 30% of the data, as specified by the first argument, `0.3`; the next
    argument specifies that we want to shuffle the data in randomly. `1001` says that
    there is a seed value of 1001, so we use a `holdBackValidation` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, it''s time to train the model and classify the data, according to the
    measurements and labels. The cross-validation breaks the training dataset into
    five different combinations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to display the results of each fold and the errors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will start to use the model to predict the values, using the following
    code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will yield an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c6cad63-e1ef-499b-8b39-74d333a78d85.png)'
  prefs: []
  type: TYPE_IMG
- en: Encog supports many other options in `MLMethodFactory`, such as SVM, PNN, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Classification using massive online analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Massive Online Analysis** (**MOA**), as discussed in [Chapter 2](6fd557d7-2807-4a6d-8f93-d7c4ca094b7e.xhtml), *Java
    Libraries and Platforms for Machine Learning,* is another library that can be
    used to achieve classification. It is mainly designed to work with the stream.
    If it is working with the stream, a lot of data will be there; so, how do we evaluate
    the model? In the traditional batch learning mode, we usually divide the data
    into training and test sets and cross-validation is preferred if the data is limited.
    In stream processing, where the data seems to be unlimited, cross-validation proves
    to be expensive. Two approaches that we can use are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Holdout**: This is useful when the data is already divided into two parts,
    which are predefined. It gives the estimation of the current classifier, if it
    is similar to the current data. This similarity is hard to guarantee between the
    holdout set and the current data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interleaved test-then-train, or prequential**: In this method, the model
    is tested on the example before it is used for training. So, the model is always
    tested for the data that it has never seen. In this, no holdout scheme is needed.
    It uses the available data. Over time, this approach will improve the accuracy
    of classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MOA provides various ways to generate the stream of data. First, download the
    MOA library from [https://moa.cms.waikato.ac.nz/downloads/](https://moa.cms.waikato.ac.nz/downloads/).
    Add the downloaded `.jar` files to Eclipse, like we did for Weka at the beginning
    of this chapter. We will be using the GUI tool provided by MOA to see how to use
    MOA for streams. To launch the GUI, make sure `moa.jar` and `sizeofag.jar` are
    in the current path; then, run the following command in Command Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'It will display the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3ba39b4e-10ac-424d-b2a1-187bdee96629.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that it has options for classification, regression, clustering,
    outliers, and more. Clicking on the Configure button will display the screen used
    to make your classifier. It provides various learners and streams to work with,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16dcc58d-a17e-42bf-a59e-fa4a28af752c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is an example of running `RandomTreeGenerator` with `NaiveBayes`
    and `HoeffdingTree`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0662400-373d-4b18-9d2d-ac392275f3e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Evaluation is the next important task, after the model has been developed.
    It lets you decide whether the model is performing on the given dataset well and
    ensures that it will be able to handle data that it has never seen. The evaluation
    framework mostly uses the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Error estimation**: This uses holdout or interleaved test-and-train methods
    to estimate the errors. K-fold cross-validation is also used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance measures**: The Kappa statistics are used, which are more sensitive
    towards streaming classifiers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical validation**: When comparing evaluating classifiers, we must
    look at the differences in random and non-random experiments. The McNemar''s test
    is the most popular test in streaming, used to access the statistical significance
    of differences in two classifiers. If we are working with one classifier, the
    confidence intervals of parameter estimates indicate the reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The cost measure of the process**: As we are dealing with streaming data,
    which may require access to third-party or cloud-based solutions to get and process
    the data, the cost per hour of usage and memory is considered for evaluation purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baseline classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Batch learning has led to the development of many classifiers in different paradigms,
    such as divide and conquer, lazy learners, kernel methods, graphics models, and
    so on. Now, if we move to a stream for the same, we need to understand how to
    make them incremental and fast for the large datasets in the streams. We have
    to think in terms of the complexity of the model versus the speed of the model
    update, and this is the main trade-off that needs to be taken care of.
  prefs: []
  type: TYPE_NORMAL
- en: The **majority class algorithm** is one of the simplest classifiers, and it
    is used as a baseline. It is also used as a default classifier for decision tree
    leaves. Another is the **no**-**change classifier**, which predicts the labels
    for new instances. The Naive Bayes algorithm is known for its low cost in terms
    of computational power and simplicity. It's an incremental algorithm and is best
    suited for streams.
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The decision tree is a very popular classifier technique, making it easy to
    interpret and visualize the models. It is based on trees. It divides or splits
    the nodes on the basis of the attribute value and the leaves of the tree usually
    fall to the majority class classifier. In streaming data, the Hoeffding tree is
    a very fast algorithm for decision trees; it waits for new instances, instead
    of reusing instances. It builds a tree for large data. The **Concept-adapting
    Very Fast Decision Tree** (**CVFDT**) deals with the concept of drift, which maintains
    a model consistency with the instances in a sliding window. The other trees are
    the **Ultra Fast Forest of Trees** (**UFFT**), the Hoeffding adaptive tree, the
    exhaustive binary tree, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the streaming context, **k-nearest neighbor** (**KNN**) is the most convenient
    batch method. A sliding window is used to determine the KNN for a new instance
    that is not yet classified. It normally uses the 1,000 most recent instances for
    the sliding window. As the sliding window slides, it handles the concept drift,
    too.
  prefs: []
  type: TYPE_NORMAL
- en: Active learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We all know that classifiers work well with labeled data, but that is not always
    the case with stream data. For example, the data from a stream may come unlabeled.
    Labeling data is costly, because it requires human intervention to label the unlabeled
    data. We understand that the streams generate large amounts of data. Active learning
    algorithms only do the labeling for selective data. The data to be labeled is
    decided on from historical data suited for pool-based settings. Regular retraining
    is required to decide whether a label is required for incoming instances. A simple
    strategy for labeling data is to use a random strategy. It is also called a baseline
    strategy, and it asks for a label for each incoming instance with probability
    of a budget for labelling. Another strategy is to ask for a label for the instance
    for which the current classifier is least confident. This may work fine, but soon,
    the classifier will exhaust its budget or reach its threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will explore basic regression algorithms through an analysis of an energy
    efficiency dataset (Tsanas and Xifara, 2012). We will investigate the heating
    and cooling load requirements of the buildings based on their construction characteristics,
    such as surface, wall, and roof area; height; glazing area; and compactness. The
    researchers have used a simulator to design 12 different house configurations,
    while varying 18 building characteristics. In total, 768 different buildings were
    simulated.
  prefs: []
  type: TYPE_NORMAL
- en: Our first goal is to systematically analyze the impact that each building characteristic
    has on the target variable, that is, the heating or cooling load. The second goal
    is to compare the performance of a classical linear regression model against other
    methods, such as SVM regression, random forests, and neural networks. For this
    task, we will use the Weka library.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Download the energy efficiency dataset from [https://archive.ics.uci.edu/ml/datasets/Energy+efficiency](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency).
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset is in Excel''s XLSX format, which cannot be read by Weka. We can
    convert it into a **comma-separated value** (**CSV**) format by clicking on File
    | Save As and picking `.csv` in the saving dialog, as shown in the following screenshot.
    Confirm to save only the active sheet (since all of the others are empty), and
    confirm to continue, to lose some formatting features. Now, the file is ready
    to be loaded by Weka:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c14075de-336e-4f70-a82a-bc3405293095.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Open the file in a text editor and inspect whether the file was correctly transformed.
    There might be some minor issues that could cause problems. For instance, in my
    export, each line ended with a double semicolon, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To remove the doubled semicolon, you can use the Find and Replace function:
    find `;;` and replace it with `;`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second problem was that my file had a long list of empty lines at the end
    of the document, which can be deleted, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to load the data. Let''s open a new file and write a simple
    data import function by using Weka''s converter for reading files in a CSV format,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The data is loaded! Let's move on.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing attributes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we analyze the attributes, let''s try to understand what we are dealing
    with. In total, there are eight attributes describing building characteristics,
    and there are also two target variables, the heating and cooling load, as shown
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attribute** | **Attribute name** |'
  prefs: []
  type: TYPE_TB
- en: '| `X1` | Relative compactness |'
  prefs: []
  type: TYPE_TB
- en: '| `X2` | Surface area |'
  prefs: []
  type: TYPE_TB
- en: '| `X3` | Wall area |'
  prefs: []
  type: TYPE_TB
- en: '| `X4` | Roof area |'
  prefs: []
  type: TYPE_TB
- en: '| `X5` | Overall height |'
  prefs: []
  type: TYPE_TB
- en: '| `X6` | Orientation |'
  prefs: []
  type: TYPE_TB
- en: '| `X7` | Glazing area |'
  prefs: []
  type: TYPE_TB
- en: '| `X8` | Glazing area distribution |'
  prefs: []
  type: TYPE_TB
- en: '| `Y1` | Heating load |'
  prefs: []
  type: TYPE_TB
- en: '| `Y2` | Cooling load |'
  prefs: []
  type: TYPE_TB
- en: Building and evaluating the regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start by learning a model for the heating load by setting the class
    attribute at the feature position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The second target variable, the cooling load, can now be removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will start with a basic linear regression model, implemented with the `LinearRegression`
    class. Similar to the classification example, we will initialize a new model instance,
    pass the parameters and data, and invoke the `buildClassifier(Instances)` method,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The learned model, which is stored in the object, can be provided by calling
    the `toString()` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The linear regression model constructs a function that linearly combines the
    input variables to estimate the heating load. The number in front of the feature
    explains the feature''s impact on the target variable: the sign corresponds to
    the positive/negative impact, while the magnitude corresponds to its significance.
    For instance, the relative compactness of the feature `X1` is negatively correlated
    with heating load, while the glazing area is positively correlated. These two
    features also significantly impact the final heating load estimate. The model''s
    performance can similarly be evaluated with the cross-validation technique.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ten-fold cross-validation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can provide the common evaluation metrics, including the correlation, the
    mean absolute error, the relative absolute error, and so on, as output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Linear regression using Encog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will quickly look at how Encog can be used to make a regression model.
    We will be using the dataset that we used in a previous section, *Loading the
    data*. The following steps show how to make the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load the data, we will use the `VersatileMLDataSet` function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have two pieces of output, `Y1` and `Y2`, they can be added by using
    the `defineMultipleOutputsOthersInput` function, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to develop a simple regression model by using the `FEEDFORWARD` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our regression model is ready. The last few lines of the output are given
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a260d2b-211b-40d0-884b-21b04ec6ac34.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression using MOA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using MOA for regression requires us to use the GUI. You can download the dataset
    from [http://www.cs.waikato.ac.nz/~bernhard/halifax17/census.arff.gz](http://www.cs.waikato.ac.nz/~bernhard/halifax17/census.arff.gz).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps show how to perform regression:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch the MOA GUI by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the Regression tab and click on Configure, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/afc20bde-fc01-44f6-864f-07b531de5c92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will use the downloaded `.arff` file for regression. When we click on Configure
    in the preceding step, it will display the Configure task window, as shown in
    the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/62d0e7ee-fbcd-41bc-aa15-5632716435ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the stream option, click on Edit and select the ArffFileStream; select the
    `.arff` file that we downloaded, as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c679a3fd-fadd-410d-9635-c3bf2fd7c871.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In classIndex, specify `-1`, which sets the first attribute as the target.
    Click on OK in all pop-up windows and click on Run. It will take some time, as
    the census file has a large amount of data to process, as shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/82fc5070-7875-4ad8-a5e7-3860ed59c498.png)'
  prefs: []
  type: TYPE_IMG
- en: Regression trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another approach is to construct a set of regression models, each on its own
    part of the data. The following diagram shows the main difference between a regression
    model and a regression tree. A regression model constructs a single model that
    best fits all of the data. A regression tree, on the other hand, constructs a
    set of regression models, each modeling a part of the data, as shown on the right-hand
    side. Compared to the regression model, the regression tree can better fit the
    data, but the function is a piece-wise linear plot, with jumps between modeled
    regions, as seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/05ac04c7-3b94-4b84-ad85-107afa117db8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A regression tree in Weka is implemented within the `M5` class. The model construction
    follows the same paradigm: initialize the model, pass the parameters and data,
    and invoke the `buildClassifier(Instances)` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The induced model is a tree with equations in the leaf nodes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The tree has `13` leaves, each corresponding to a linear equation. The preceding
    output is visualized in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afb3b685-9ce9-4bcc-b3eb-9ba448748986.png)'
  prefs: []
  type: TYPE_IMG
- en: The tree can be read similarly to a classification tree. The most important
    features are at the top of the tree. The terminal node, the leaf, contains a linear
    regression model explaining the data that reaches this part of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: 'An evaluation will provide the following results as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Tips to avoid common regression problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we have to use prior studies and domain knowledge to figure out which
    features to include in regression. Check literature, reports, and previous studies
    on what kinds of features work and some reasonable variables for modeling your
    problem. Suppose that you have a large set of features with random data; it is
    highly likely that several features will be correlated to the target variable
    (even though the data is random).
  prefs: []
  type: TYPE_NORMAL
- en: We have to keep the model simple, in order to avoid overfitting. The Occam's
    razor principle states that you should select a model that best explains your
    data, with the least assumptions. In practice, the model can be as simple as having
    two to four predictor features.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to a supervised classifier, the goal of clustering is to identify intrinsic
    groups in a set of unlabeled data. It can be applied to identifying representative
    examples of homogeneous groups, finding useful and suitable groupings, or finding
    unusual examples, such as outliers.
  prefs: []
  type: TYPE_NORMAL
- en: We'll demonstrate how to implement clustering by analyzing a bank dataset. The
    dataset consists of 11 attributes, describing 600 instances, with age, sex, region,
    income, marital status, children, car ownership status, saving activity, current
    activity, mortgage status, and PEP. In our analysis, we will try to identify the
    common groups of clients by applying the **expectation maximization** (**EM**)
    clustering.
  prefs: []
  type: TYPE_NORMAL
- en: 'EM works as follows: given a set of clusters, EM first assigns each instance
    with a probability distribution of belonging to a particular cluster. For example,
    if we start with three clusters—namely, A, B, and C—an instance might get the
    probability distribution of 0.70, 0.10, and 0.20, belonging to the A, B, and C
    clusters, respectively. In the second step, EM re-estimates the parameter vector
    of the probability distribution of each class. The algorithm iterates these two
    steps until the parameters converge or the maximum number of iterations is reached.'
  prefs: []
  type: TYPE_NORMAL
- en: The number of clusters to be used in EM can be set either manually or automatically
    by cross-validation. Another approach to determining the number of clusters in
    a dataset includes the elbow method. This method looks at the percentage of variance
    that is explained with a specific number of clusters. The method suggests increasing
    the number of clusters until the additional cluster does not add much information,
    that is, it explains little additional variance.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process of building a cluster model is quite similar to the process of
    building a classification model, that is, loading the data and building a model.
    Clustering algorithms are implemented in the `weka.clusterers` package, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The model identified the following six clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The table can be read as follows: the first line indicates six clusters, while
    the first column shows the attributes and their ranges. For example, the attribute
    `age` is split into three ranges: `0-34`, `35-51`, and `52-max`. The columns on
    the left indicate how many instances fall into the specific range in each cluster;
    for example, clients in the `0-34` years age group are mostly in cluster 2 (122
    instances).'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A clustering algorithm''s quality can be estimated by using the `logLikelihood`
    measure, which measures how consistent the identified clusters are. The dataset
    is split into multiple folds, and clustering is run with each fold. The motivation
    is that, if the clustering algorithm assigns a high probability to similar data
    that wasn''t used to fit parameters, then it has probably done a good job of capturing
    the data structure. Weka offers the `CluterEvaluation` class to estimate it, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'It provides the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Clustering using Encog
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Encog supports k-means clustering. Let''s consider a very simple example, with
    the data shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'To make `BasicMLDataSet` from this data, a simple `for` loop is used, which
    will add data to the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `KMeansClustering` function, let''s clusters the dataset into two
    clusters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Clustering using ELKI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ELKI supports many clustering algorithms. A few are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Affinity propagation clustering algorithm**: This is a cluster analysis that
    uses affinity propagation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DBSCAN**: This is density based clustering especially for the applications
    with noise; it finds the sets in the database on the basis of density.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**EM**: This algorithm creates clusters based on the expectation maximization
    algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AGNES**: **Hierarchical agglomerative clustering (HAC),** or **agglomerative
    nesting (AGNES),** is a classic hierarchical clustering algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SLINK**: This is the single link algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CLINK**: This is used for complete linkage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HDBSCAN**: This is an extracting cluster hierarchy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, KMeansSort, KMeansCompare, KMedianLloyd, KMediodsEM, KMeansBisecting,
    and so on, are some examples from the family of KMean.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed list of clustering algorithms, with all of the algorithms supported
    by ELKI, can be found at [https://elki-project.github.io/algorithms/](https://elki-project.github.io/algorithms/).
  prefs: []
  type: TYPE_NORMAL
- en: We need to get the required `.jar` file from [https://elki-project.github.io/releases/](https://elki-project.github.io/releases/).
    Download the executable archive, and download the mouse dataset from [https://elki-project.github.io/datasets/](https://elki-project.github.io/datasets/).
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Terminal or Command Prompt, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad7aa6a0-9316-4d68-ac6c-3fd4ad9f7bba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see two options, in an orange color: `dbc.in` and `algorithm`. We need
    to specify the value. In `dbc.in`, click on the dots (`...`) to select the `mouse.csv`
    file that we downloaded. In `algorithm`, select `k-Mean Clustering algorithm`
    by clicking on the plus sign (`+`), find `kmean.k`, and fill it with the value
    `3`. Click on the Run Task button, which is now enabled. It will generate the
    following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff63ada1-1a48-4ca8-bbfd-e32a2e6e34f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned how to implement basic machine learning tasks
    with Weka: classification, regression, and clustering. We briefly discussed the
    attribute selection process and trained models and evaluated their performance.'
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will focus on how to apply these techniques to solving real-life
    problems, such as customer retention.
  prefs: []
  type: TYPE_NORMAL
