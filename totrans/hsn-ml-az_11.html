<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Integration with Other Azure Services</h1>
                </header>
            
            <article>
                
<p>In addition to using Azure AI services directly, Azure also provides options for using these services from other non-AI services. Many Azure AI services provide REST API interfaces that can be consumed from other services. AI services can thus be used as subcomponents of other apps to provide insights and predictions. Many non-AI services in Azure have built-in integration with AI services, so that AI components can often be added to apps with a few clicks.</p>
<p>Some AI services do not include any automation features. Recurring tasks, such as retraining ML models or running batch workloads, require integration with other services that offer these features. In the following sections, we will present various options for launching AI jobs automatically. In addition to traditional time-scheduled workloads, Azure services also provide objects called triggers to launch tasks after a certain event has occurred. Triggers allow services to react to events in a specific way, for example, processing the contents of a blob file after it has been created or modified.</p>
<p>In this chapter, we will learn how to integrate Azure AI services with four non-AI services:</p>
<ul>
<li>Logic Apps</li>
<li>Azure Functions</li>
<li>Data Lake Analytics</li>
<li>Data Factory</li>
</ul>
<p>With these services, it is possible to build complex application pipelines where the ML model is just a part of the solution. In Azure, integration between different services is made as easy as possible, without compromising security. Getting results is therefore quick and efficient, and developing Azure applications can be a lot of fun!</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Logic Apps</h1>
                </header>
            
            <article>
                
<p>Azure Logic Apps is a graphical tool for automating various types of tasks, such as getting data from a Web API and saving it in cloud storage. Logic Apps can be developed without writing a single line of code, so no programming skills are required. However, Logic Apps provides some basic functionality for programming languages, such as conditional execution and iterative loops.</p>
<p>Logic Apps is meant for light-weight tasks that do not require complex logic or lightning-fast performance. Such tasks could include sending an email when a SharePoint list is modified, or copying files between Dropbox and OneDrive if the files have been modified.</p>
<p>For AI development, Logic Apps provides a number of basic functionalities. There are built-in ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Triggers and actions</h1>
                </header>
            
            <article>
                
<p>Logic Apps is based on two main concepts: triggers and actions. Triggers are listeners that are constantly waiting for a specific kind of event. These events could be created when a new file is created in a <kbd>Blob</kbd> folder, or they could occur every day at the same time to automate daily tasks. Actions are routines that are executed every time the trigger fires. Actions usually take some input data, process it, and finally, save or send the result somewhere.</p>
<p>Logic Apps supports all Azure Data Storage services: Data Lake Storage, Blob Storage, Cosmos DB, and so on. For example, the <strong>Blob Storage</strong> trigger executes Logic Apps every time a new file is added to a directory or a file in the directory is modified. We can then take the file as input to Logic Apps, process the contents of the file, and save the results back to the Blob.</p>
<p>The Logic Apps Designer contains many Cognitive Services actions: the <span class="packt_screen">Computer Vision</span> <span class="packt_screen">API</span>, <span class="packt_screen">Custom Vision</span>, <span class="packt_screen">Text Analytics</span>, and so on. To see the full list, search for the cognitive actions when creating an action, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-879 image-border" src="Images/7680f7ee-2d1d-4ad0-ac61-e2f4a58ee4a8.png" style="width:38.83em;height:42.58em;" width="890" height="977"/></p>
<p class="mce-root"/>
<p>There are also a few Azure Machine Learning Studio actions. These can be used to score examples or retrain ML models, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/7622647c-2a3b-44f5-a0d0-d6bb6467806d.png" style="width:41.58em;height:37.08em;" width="910" height="811"/></p>
<p>The connectors might have been updated since the time of writing, so check the latest information in the official documentation from Microsoft.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Twitter sentiment analysis</h1>
                </header>
            
            <article>
                
<p>Social media has become an everyday tool for many organizations. In addition to advertising themselves on these platforms, monitoring discussions about a company brand provides important information about how customers see its products and the image of the company.</p>
<p>In this section, we'll show how to create an app that reads tweets containing the keyword <em>Azure</em> and saves those tweets in cloud storage. This is a real-world data acquisition scenario: once the tweets get stored in the cloud permanently, we have started to collect history data about the company's brand. As a next step, we could start creating analytics and machine learning models based on this data. These models could include topic analysis or looking ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Adding language detection</h1>
                </header>
            
            <article>
                
<p>In the previous example, we created a workflow that reads tweets with a specific keyword, performs sentiment analysis on them, and saves the results in Blob Storage. If you completed the example and looked at the results more carefully, you might have noticed that the tweets can contain many different languages, because our keyword (Azure) is not specific to any language. In this example, we show how to add language detection to the pipeline and filter out all tweets that are not in a particular language.</p>
<ol>
<li>To begin the example, create a new Logic App in the Azure portal, open the Logic Apps Designer, and choose the same trigger as a starting point as we did in the previous example (when a new tweet appears).</li>
</ol>
<ol start="2">
<li>Add the Detect Language module from the Text Analytics API. For the <span class="packt_screen">Text</span> field, choose the <span class="packt_screen">Tweet text</span> parameter to be analyzed by the Text Analytics API. The output of this module will contain the list of languages detected from the tweet. Note that the default number of detected languages is 1. This can be changed in the advanced options for the module, if the text is expected to contain more than one language:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-880 image-border" src="Images/1a2ce9f1-19b5-4228-8e47-630586c251b3.png" style="width:42.08em;height:28.08em;" width="466" height="311"/></p>
<ol start="3">
<li>Once the tweet language has been detected, we need to add a module that chooses only tweets that are in a certain language. For this purpose, create a new step in the pipeline, search for <kbd>Control</kbd> actions and choose the <kbd>Condition</kbd> action.</li>
</ol>
<ol start="4">
<li>When the <kbd>Condition</kbd> module has been added to the pipeline, we must specify the filter condition using the three fields in the module. The first field indicates which value we would like to compare, the second field specifies the condition type, and the last field contains the value against which to compare. For the first field, choose the <kbd>detectedLanguages</kbd> item. Note that the detect language module returns the languages as a list, even if we are only detecting one language. The <kbd>detectedLanguages</kbd> item refers to each item in this list. Therefore, the Logic Apps Designer will add a new <kbd>For each</kbd> control module to the pipeline and the previously created <kbd>Condition</kbd> module will be placed inside the <kbd>For each</kbd> module. The <kbd>For each</kbd> module iterates through all the values in the <kbd>detectedLanguages</kbd> list, if there is more than one language detected.</li>
<li>Now that our <kbd>Condition</kbd> module is placed inside the <kbd>For each</kbd> module, we can change the first field of the <kbd>Condition</kbd> module to <kbd>Language code</kbd>. The second field should be set to <kbd>is equal to</kbd> and the last field should contain the two-letter language code. To keep English language tweets only, we set the parameter to <kbd>en</kbd>. Note that the language detection module might not be able to detect all languages, so check the list of available languages in the latest Microsoft documentation.</li>
<li>After the condition module, the pipeline now branches into two directions, depending on whether the condition is <kbd>true</kbd> or not. In the <kbd>true</kbd> branch, we can add any processing steps for the English language tweets. In the <kbd>false</kbd> branch, we can specify what to do with the rest of the tweets. In this example, we use the sentiment detection pipeline that was developed in the previous example to process the English language tweets. We are not interested in the non-English tweets, so we can just ignore them by leaving the false branch empty. After completing all these steps, the pipeline appears as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="Images/a75abd3f-8d0a-4d23-a880-5f6d9212cd8b.png" style="width:60.08em;height:39.33em;" width="978" height="640"/></p>
<p class="mce-root">Once you save the app, the trigger will start to run and the tweets will be saved to the Blob. Since we used the same modules for sentiment detection as we did in the previous example, the format of the files should be identical, with the exception that only English tweets are saved to the Blob.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Functions</h1>
                </header>
            
            <article>
                
<p>While Logic Apps provides a fast way to automate tasks, its collection of actions is limited to pre-selected options that cannot be customized. Moreover, the programmability of Logic Apps is quite limited, and the development of more complex programs is not necessarily any easier than writing code. If more flexibility is needed, it might be more productive to develop applications with Azure Functions. Functions can be developed with multiple programming languages familiar to web developers.</p>
<p>Azure Functions is a serverless coding platform in the cloud, where the underlying operating system has been virtualized. This means that many maintenance tasks, such as updating the operating system or language versions, is managed by ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Triggers</h1>
                </header>
            
            <article>
                
<p>Azure Functions triggers work with the same principle as the Logic Apps triggers introduced in the previous section: they listen to a certain event and initiate the function every time the event occurs. For example, an HTTP trigger executes the function every time the web URL of the function is called. A schedule trigger can execute the function periodically, for example, once a day. One of the biggest strengths of Azure Functions is the trigger collection. The trigger collection allows you to respond to many different types of events, and the code-based approach makes it possible to react to those events with the full flexibility of programming languages.</p>
<p class="mce-root"/>
<p>For data-related tasks, the most useful triggers are the storage triggers. Similar to how  Logic Apps can be triggered on blob events, Azure Functions can be triggered when files are added or updated in storage services such as Blob Storage or Cosmos DB. Consider a scheme where raw data is stored in the cloud, for example, JSON files that are produced by a web app, and we wish to convert those files into a columnar format to add them to a relational database, for example. Using Azure Functions with Blob Storage triggers, this task can be fully automated without any external services. Thus, Azure Functions provides another way to perform such format conversions, among other things.</p>
<p>One of the most interesting applications for Azure Functions is processing data from <strong>Internet of Things</strong> (<strong>IoT</strong>) devices. With event hub and IoT hub triggers, Azure Functions can be used as a downstream processing engine for data sent by IoT devices. The event hub and IoT Hub resources in Azure are data ingestion services that are capable of handling a large number of requests in a short time interval. They are designed for continuous data streams where the size of each message is small, but the number of messages can be large. Therefore, they are ideal services for receiving data from an array of IoT sensors that send their measurements in short intervals. The event hub and IoT hub triggers are set to fire every time a new event is received from the IoT devices, so we can use Azure Functions to define a processing routine for each event. This routine could include a scoring call to a machine learning service and saving the results in a database, for example.</p>
<p>For most trigger types, the trigger also passes some information about the event that fired the trigger. For example, a Blob Storage trigger passes the contents of the file that was created or changed, or a HTTP trigger passes the contents of a POST request. Handling different types of input data is made very simple in Azure Functions: depending on the source, a handle to the input data is passed as a parameter to the main function. In the following section, we will show an example how to read the contents of a blob file after the blob trigger has been fired.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Blob-triggered function</h1>
                </header>
            
            <article>
                
<p>In this example, we'll demonstrate how to create a simple function that reads the contents of a blob file. We will not do any processing on the file; instead, we'll just print its contents in a log file.</p>
<p>When you create an Azure Functions service in the portal, it will create a number of resources in addition to the functions app itself. A storage account is needed to store logs produced by the functions app. The app service plan is a container resource that determines the pricing and the resource scaling of the app. Optionally, you can also create an app insights resource for monitoring your app usage. This is particularly useful for error analysis and tracking how often your app is triggered.</p>
<p>To begin development, ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Data Lake Analytics</h1>
                </header>
            
            <article>
                
<p><strong>Azure Data Lake</strong> (<strong>ADL</strong>) is Microsoft's storage and analytics service for big data. It is capable of storing data on a petabyte scale and making efficient queries on the stored data. The storage and the analytics services are separate in Azure and the ADL service actually consists of two different products: <strong>Azure Data Lake Storage</strong> (<strong>ADLS</strong>) and <strong>Azure Data Lake Analytics</strong> (<strong>ADLA</strong>). In this section, we will focus on ADLA, but we will also touch on ADLS where appropriate.</p>
<p class="mce-root"/>
<p>Data Lake Storage is a file-based storage, with files organized into directories. This type of storage is called schemaless, since there are no constraints on what type of data can be stored in the Data Lake. Directories can contain text files and images, and the data type is specified only when the data is read out from the Data Lake. This is particularly useful in big data scenarios where the amount of data written to the Data Lake is large and running the data validation steps on the fly would be too resource-consuming. The data validation steps can be incorporated in the queries later when the data is read out, or they can be run periodically in batches.</p>
<p>ADLA is the query engine that enables you to make efficient queries against the ADLS. An ADLA account always requires an ADLS account behind it. This primary account is the default source for queries. ADLA queries are written with U-SQL, an SQL-like language with some programming features borrowed from C#. Some examples of U-SQL scripts that query data from ADLS and Storage Blobs follow.</p>
<p>In addition to the primary ADLS source, the ADLA instance can have secondary sources as well. You can add other ADLS accounts or Azure Storage Blobs from the <span class="packt_screen">Data sources</span> tab in the portal, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/cae0e7ca-dd32-4a97-96c7-6ebb26417e7b.png" style="width:60.25em;height:26.92em;" width="871" height="389"/></p>
<p>Once an account has been registered as a data source, it can be used in queries similar to the primary ADLS account. By using multiple data sources, you can avoid moving the data to a single Data Lake if your data resides in multiple Storage accounts.</p>
<p class="mce-root"/>
<p>As a side note, it is always possible to create an ADL Storage account without associating an ADLA account with it. But the ADLS would only act as a file storage in that case, without the ability to make queries against the file contents. Also, nothing would prevent you from creating multiple ADLA accounts that all use the same ADLS account as their primary source. This flexibility is possible because ADLS and ADLA are separate products in the Azure portal.</p>
<p>One of the biggest strengths of ADL is its user permission controls. Permissions can be set separately for each file and directory. If some data in the Data Lake is confidential and should not be visible to all users, it can be placed in a separate directory and the directory can be made visible only to a select group of users. Data Lake is also integrated with Azure AD, and permissions can be assigned to Azure AD groups instead of specific users. Role-based access control will not be discussed further here, as the focus is on analytics. To add users to the ADLA account, they should be granted a Data Lake Analytics Developer role using the Add User Wizard in the ADLA instance in the Azure portal.</p>
<p>The material in this section concerns Data Lake Storage Gen1 (Data Lake Store). ADLS Gen2 is in private preview at the time of writing. There might be changes in how ADLA works with Gen2, so check the latest information in the Microsoft documentation.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Developing with U-SQL</h1>
                </header>
            
            <article>
                
<p>As mentioned previously, ADLA queries are written in U-SQL, a query language developed by Microsoft specifically for big data. Similar to many other Azure services, queries can be written and executed in the Azure portal, which is useful for small and quick tasks. For advanced development, there are extensions for VS Code and Visual Studio IDEs that offer more functionality.</p>
<p>Although writing U-SQL queries is in many ways similar to writing SQL queries, there are some differences as well. ADLA does not produce interactive output to the query editor as it does in SQL Server Management Studio, for example. The results are always directed to a certain output, which is usually a file in ADLS. To check the results of the query, ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">U-SQL databases</h1>
                </header>
            
            <article>
                
<p>Although ADLS is a file-based storage system, it also includes features that are familiar from relational databases, such as tables and views. An ADLA database is a collection of these objects. ADLA databases are useful for managing data in Data Lake Store, since they provide some useful features such as table indexes. On the other hand, ADLA database tables have strict schemas in place, so data must be validated before it can be entered into a table. Then, we have lost one of the biggest strengths of the Data Lake, namely the schemaless storage principle that was discussed at the beginning of the section.</p>
<p>When an ADLA resource is created, a master database is also created. This database is used as the default database for U-SQL queries. New databases can be created with the <kbd>CREATE DATABASE</kbd> command and a database can be changed with the <kbd>USE DATABASE</kbd> command. The contents of databases can be viewed in the catalog inside the ADLA Data Explorer.</p>
<p>There are two kinds of tables in U-SQL databases: managed and external tables. Managed tables are in many ways similar to SQL tables: a table consists of metadata (for example, table schema) and the data itself. In practice, the data is stored as structured files in a Data Lake Storage directory. Managed tables enforce a schema-on-write, meaning that it is not possible to enter data to the table unless it conforms to the table schema.</p>
<p class="mce-root"/>
<p>External tables are in some ways similar to SQL views: only the metadata is stored in the database, but the data itself can reside outside of Data Lake. External tables can refer to various Azure services: Azure SQL Database, Azure SQL Data Warehouse, or SQL Servers on virtual machines. In contrast to managed tables, external tables enforce a schema-on-read: the format of the underlying data can change after the schema is defined, because the schema is not enforced at the time of writing. If the schema of the data changes, the external table definition can be altered to match the new data.</p>
<p>Using a managed table instead of storing the data as files in directories has some advantages. For example, if your data is stored in a huge CSV file, it might be difficult to append new values to the file. This operation is possible with managed tables using the <kbd>INSERT</kbd> statement. (Note that the managed table is append only, without the possibility to update values.) Managed tables also have a clustered index defined, which can be used to make more effective queries.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Simple format conversion for blobs</h1>
                </header>
            
            <article>
                
<p>In this section, we'll demonstrate the execution of U-SQL scripts in the Azure portal and integration with secondary data sources (Blob Storage). We will read a <strong>CSV</strong> (short for <strong>comma-separated value</strong>) file from a Blob Storage and write the whole contents of the file to another blob file in a <strong>TSV</strong> (short for <strong>tab-separated values</strong>) format. In other words, we'll perform a file format conversion from CSV to TSV. Such format conversions are common in data management, since different applications often require data in different formats.</p>
<p>The requirements for running this example are a Blob Storage account (with some data), and ADLS and ADLA accounts.</p>
<p>To read the input data, the U-SQL module <kbd>Extractors</kbd> must be used. ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Integration with Cognitive Services</h1>
                </header>
            
            <article>
                
<p>In previous sections, we saw how to integrate Logic Apps with Cognitive Services. We used the Text Analytics API to score the sentiment of each tweet and stored the results in Blob Storage. In this example, we'll show how to implement the same steps with ADLA. In addition to getting the sentiment score, we'll also extract key phrases for each tweet to get more insights.</p>
<p>Completing this example requires a Cognitive Services Text Analytics API account.</p>
<p>Before Cognitive Services can be used in U-SQL scripts, the ADLA instance must be registered with Cognitive extensions. This can be done in the Azure portal by opening the ADLA instance and choosing the <span class="packt_screen">Sample scripts</span> tab, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/00917094-bc9e-4891-943c-ddc409c12f67.png" style="width:41.08em;height:22.08em;" width="1697" height="912"/></p>
<p class="mce-root"/>
<p>Once installation is successful, Cognitive extensions are ready to be used in U-SQL scripts. This means that the <kbd>REFERENCE ASSEMBLY</kbd> statement can now be used to import the Cognitive Services modules.</p>
<p>In this example, the input comes from a directory that contains multiple files. The query can be limited to match only some of the files in the directory, according to the filename. In this case, all CSV files in the input directory are chosen. The input files have the same format as the files that were produced by the Logic App in a previous example:</p>
<pre>2018-09-22 11:55:20|Nadeem_ahamed_r|RT @msdev: .@JimBobBennett would like to help you build #Azure #CognitiveServices speech API into your Android app in this three part serie…|1</pre>
<p>Create a new job and enter the following script:</p>
<pre>DECLARE @inputFiles string = "HandsOnAI/twitter/2018-09-21/{*}.csv";<br/>DECLARE @sentimentOutput string = "HandsOnAI/twitter/sentiment.csv";<br/><br/>REFERENCE ASSEMBLY [TextSentiment];<br/><br/>@query =<br/>    EXTRACT Timestamp DateTime,<br/>            Username string,<br/>            Text string,<br/>            SentimentScore float<br/>    FROM @inputFiles<br/>    USING Extractors.Text(delimiter:'|', quoting:false, silent:true);<br/><br/>// Extract the sentiment for each tweet<br/>@sentiment =<br/>    PROCESS @query<br/>    PRODUCE Username,<br/>            Text,<br/>            Sentiment string,<br/>            Conf double<br/>    READONLY Username, Text<br/>    USING new Cognition.Text.SentimentAnalyzer();<br/><br/>OUTPUT @sentiment<br/>    TO @sentimentOutput<br/>    USING Outputters.Csv();</pre>
<p class="mce-root"/>
<p>The preceding script reads the input data using the <kbd>Extractors.Text()</kbd> module. The module options state that the values are separated by the pipe <kbd>|</kbd> character, the text values are not quoted, and that the <kbd>Extractor</kbd> should ignore all erroneous data instead of throwing an error and exiting (silent mode). This last property is useful if we don't need to worry about the errors.</p>
<p>The Cognitive Services modules are accessed through the <kbd>PROCESS</kbd> command. Since we referenced the assembly <kbd>TextSentiment</kbd> in the beginning of the script, the <kbd>Cognition.Text.SentimentAnalyzer</kbd> module is now available to be used for processing.</p>
<p>Finally, the username, tweet text, tweet sentiment, and the sentiment score are saved to the output (Data Lake directory). The output file should look like the following:</p>
<pre>"treyjohnson","Delta got me this far.  Waiting on the MARTA train and then a quick UBER to #SQLSatATLBI.  Feels like an episode of the Amazing Race! #Evangelism #Azure #BusinessIntelligence @ZAP_Data","Positive",0.74410326366140189<br/>"LouSimonetti","RT @fincooper: Getting ready for #MSIgnite? Check out my sessions BRK3267 and THR2104 in the schedule builder: https://t.co/NYiEWCAfr5 . I'…","Neutral",0</pre>
<p>Note that, in the preceding script, the output schema is defined after the <kbd>PRODUCE</kbd> statement. In this case, some fields in the input schema are ignored (<kbd>Timestamp</kbd>, <kbd>SentimentScore</kbd>). The output is enriched with the new sentiment and confidence values returned by Cognitive Services. The new confidence score can take values between <kbd>-1</kbd> and <kbd>1</kbd>.</p>
<p>In addition to sentiment detection, the Text Analytics API includes a number of other features. We can extract the most relevant key phrases from a text sample, in this case the tweet text. The <kbd>Cognition.Text.KeyPhraseProcessor</kbd> module can be used to extract the key phrases from the <kbd>Text</kbd> column, as in the following script:</p>
<pre>// Add this to the beginning of the script<br/>DECLARE @keyphrasesOutput string = "HandsOnAI/split_keyphrases.csv";<br/>REFERENCE ASSEMBLY [TextKeyPhrase];<br/><br/>// Extract key phrases for each tweet<br/>@keyphrase =<br/>    PROCESS @query<br/>    PRODUCE Username,<br/>            Text,<br/>            KeyPhrase SQL.ARRAY&lt;string&gt;<br/>    READONLY Username, Text<br/>    USING new Cognition.Text.KeyPhraseProcessor();<br/>    <br/>// Tokenize the key phrases<br/>@split_keyphrases =<br/>    SELECT Username,<br/>           Text,<br/>           T.KeyPhrase<br/>    FROM @keyphrase<br/>        CROSS APPLY EXPLODE (KeyPhrase) AS T(KeyPhrase);<br/><br/>OUTPUT @split_keyphrases<br/>    TO @keyphrasesOutput<br/>    USING Outputters.Csv();</pre>
<p>The preceding script uses the <kbd>CROSS APPLY</kbd> statement to produce a new row for every item in the key phrase list.</p>
<p>By merging the preceding script with the previous script, we have created a simple data pipeline that reads tweets from CSV files, calls the Cognitive Services Text Analytics API to find the tweet sentiment and key phrases, and finally, saves the results in ADLS.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Azure Data Factory</h1>
                </header>
            
            <article>
                
<p><strong>Azure Data Factory</strong> (<strong>ADF</strong>) is a cloud data integration platform that allows you to automate various data-related tasks, such as copying data between data stores, running analytical workloads, and retraining machine learning models. It supports a wide range of different data stores, including products from other vendors. Via its integration runtime model, ADF can also connect to on-premises locations such as self-hosted SQL databases.</p>
<p>ADF can make use of many different types of computing resources in the Azure catalogue. These include Machine Learning Studio, ADLA, Databricks, and HDInsight. ADF can also make requests to any service that exposes a REST API, such as Cognitive Services.</p>
<p>Data Factory is developed with ADF Visual ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Datasets, pipelines, and linked services</h1>
                </header>
            
            <article>
                
<p>The three main concepts in ADF are datasets, pipelines, and linked services. Datasets represent data that is stored in a specific location, such as a file in Blob Storage or a table in an SQL database. Pipelines are procedures that copy or modify data between datasets. Pipelines consist of a sequence of activities that make transformations to the input dataset and produce the output dataset as a result. The most simple pipeline consists of two datasets, the input and output datasets, and a copy activity between them. This simple pipeline could be used to move data between data stores, for example, from an on-premises SQL database to an Azure SQL database.</p>
<p>The dataset definition itself contains information only about the format and the schema of data, not about the location of data. All the connection information is separated to modules called <strong>linked services</strong>. Linked ssrvices contain all the information needed for integration, such as connection strings, server addresses, and passwords. Every dataset must be associated to a linked service, otherwise, ADF would not know where the data resides. The linked services collection defines all the data sources that ADF can connect to. This collection includes many Azure, on-premises, and third-party products and services. The full list can be found in the ADF documentation and you can also request more connectors through the Azure feedback system, if your favorite service is missing.</p>
<p>Activities are tasks that move and transform data within the pipeline. These include control statements, such as loops and conditional execution, and computational tasks using various Azure services. The computational services include the Azure Batch service and many Machine Learning services, such as AML Studio, Azure Databricks, and Azure HDInsight. Therefore, Data Factory provides a great way to automate many AI-related tasks, including retraining ML models and running periodic analytical workloads. Similar to datasets, computational activities require a linked service definition that points to an existing computational resource in Azure.</p>
<p>It is also good to note that the use of storage and computational resources is not included in the Data Factory billing model. Data Factory billing includes data management only, and data transfer costs and computational costs are billed per service on top of the Data Factory bill. As general advice, it is always good to test pipelines with very small datasets and keep an eye on the costs as the amount of data grows.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">File format conversion</h1>
                </header>
            
            <article>
                
<p>Here, we'll show yet another way to perform a file format conversion in the cloud. To begin developing Data Factory pipelines, open the Data Factory Visual Tools portal and choose the <em>Author</em> tab from the left-hand menu. Create and configure the linked services for your Storage account(s), where the input and output data resides. After the linked services have been created successfully, create the datasets for the input files and output files and attach these datasets to the linked services.</p>
<p>When creating the dataset, pay attention that the configuration matches the format of your data. Also, make sure that you specify the data schema correctly.</p>
<p>When the datasets are configured, add a new pipeline to the Data Factory. ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Automate U-SQL scripts</h1>
                </header>
            
            <article>
                
<p class="mce-root">In the previous sections, we developed U-SQL scripts to transfer and transform data in the cloud. The ADL Analytics engine does not include any automation functionality in itself, so we must use external services to automate such data workflows. ADF has the ability to trigger Data Lake Analytics jobs at regular intervals, so it is a good choice for automating U-SQL scripts.</p>
<p>Completing this example requires ADLS and <span>ADLA account</span>, and a U-SQL script to run. The first step is to create an ADLS linked service. The ADLS linked service is used to store the U-SQL script to run. The U-SQL script should be uploaded to ADLS so that it can be read by Data Factory during runtime.</p>
<p>There are two ways to authenticate with the ADLS instance: <strong>Managed Service Identity</strong> (<strong>MSI</strong>) and <strong>Service Principal</strong>. In this example, we will use the MSI authentication. The instructions for using the service principal authentication can be found in the documentation. When creating the linked service, take note of the service identity application ID, which is displayed on the screen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/e4f0064f-2d41-4581-80ca-00d3c1c72712.png" style="width:44.17em;height:18.42em;" width="627" height="262"/></p>
<p class="mce-root"/>
<p>In order to modify the Data Lake contents, the Data Factory instance must have the correct access rights to the Data Lake. To grant these access rights, copy the application ID onto the clipboard and navigate to the Data Lake Storage in the Azure portal. From the <span class="packt_screen">Data Explorer</span> view, open the directory where you want to store your data. Click on <span class="packt_screen">Access</span> and add new permissions using the <span class="packt_screen">+<span class="packt_screen">Add</span></span> button, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/f04ea11c-a300-4c90-be59-b248d6426a91.png" style="width:65.83em;height:7.83em;" width="1274" height="151"/></p>
<p class="mce-root"/>
<p>Use the service identity application ID to find the Data Factory account and grant all access rights (<kbd>rwx</kbd>) to this account. This gives Data Factory the rights to read and write files to the specified directory.</p>
<p>In addition to the destination directory, Data Factory also needs permissions for the parent directories. In this case, the <kbd>x</kbd> permission is sufficient. The easiest way to grant these permissions is to go to the root directory and grant the <kbd>x</kbd> access rights to the root and all its children directories. This means that the service account can navigate to all the subdirectories in the Data Lake, but it cannot read or write any files in these directories unless the <kbd>rw</kbd> rights are granted to these directories specifically. If you want Data Factory to be able to modify all files in the Data Lake, you can just grant <kbd>rwx</kbd> rights to the root directory and all its children directories at once. This way, you do not need to worry about the permissions anymore, since Data Factory has all the necessary permissions by default.</p>
<p>Next, create an ADLA linked service. For details about configuring the service principal authentication for ADLA, refer to the documentation.</p>
<p>The last step is to create a new pipeline and add a U-SQL activity to the pipeline. Configure the activity to use the previously created linked services for ADLS and ADLA, and specify the path where the U-SQL script can be found. Here is an example configuration:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-883 image-border" src="Images/caaede63-35a5-4a34-ad2a-c835d681ea34.png" style="width:50.08em;height:31.75em;" width="1950" height="1236"/></p>
<p>Note that the U-SQL module does not have any datasets associated with it. The input data and output data is specified in the U-SQL script, as explained in the section <em>Azure Data Lake Analytics</em>. It is possible to pass parameters to the U-SQL script, however. These can be specified under the advanced properties in the <span class="packt_screen">Script</span> tab, and they will be available as variables in the U-SQL script. This is useful for passing filenames or table names for input and output data, for example.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Running Databricks jobs</h1>
                </header>
            
            <article>
                
<p>Azure Databricks is a cloud-based machine learning service that is able to process heavy workloads with very high efficiency. Based on Apache Spark, it is designed for handling big data and high-throughput streams in real time. With ADF, it is possible to schedule Databricks jobs to run batch workloads periodically.</p>
<p>To complete this example, you need access to a Databricks Workspace. If you don't have an existing Databricks account, create one first in the Azure portal.</p>
<p>Once you have access to a Databricks Workspace, open Data Factory Visual Tools and create a Linked Service for Databricks. The  Databricks linked service is of compute type. Choose the Databricks account to connect to and configure the linked services ...</p></article></section></div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>As we have seen in this chapter, integrating Azure AI services with other non-AI services is easy and configuring these integrations can be done in a few simple steps. For codeless approach, Logic Apps and Data Factory provide tools to automate many data-related tasks. By leveraging AI services such as Cognitive Services or ML Studio Web Services, the incoming data can be enriched with insights and predictions produced by the AI services.</span></p>
<p><span>The trigger-based event handling system allows you to react to different kinds of events, for example when a new file is created or modified in cloud storage. The triggers can be used to launch data processing pipelines in scenarios where data moves infrequently and schedule-based data processing might introduce lags, since the system must wait for the scheduled time to lapse. With storage-based triggers, the data pipeline can be initiated automatically every time the source data is updated.</span></p>
<p>Data Lake Analytics is a batch processing engine that can make efficient queries against huge quantities of data. The U-SQL language combines SQL-style queries and C# commands as a highly flexible query language for Big Data. While the Data Lake Analytics engine does not contain an automation service in itself, Data Lake Analytics jobs can be launched from Data Factory by using the U-SQL module. This way all the triggers available in Data Factory can be used to automate Data Lake Analytics jobs.</p>
<p>In the next chapter will learn about <span>Azure Machine Learning service launched by Microsoft.</span></p>


            </article>

            
        </section>
    </div>



  </body></html>