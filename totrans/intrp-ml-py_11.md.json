["```py\nimport math\nimport os\nimport mldatasets\nimport pandas as pd\nimport numpy as np\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection, tree\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom aif360.datasets import BinaryLabelDataset\nfrom aif360.metrics import BinaryLabelDatasetMetric,\\\n                           ClassificationMetric\nfrom aif360.algorithms.preprocessing import Reweighing,\\\n                                           DisparateImpactRemover\nfrom aif360.algorithms.inprocessing import ExponentiatedGradientReduction, GerryFairClassifier\nfrom aif360.algorithms.postprocessing.\\\n                      calibrated_eq_odds_postprocessing \\\n                            import CalibratedEqOddsPostprocessing\nfrom aif360.algorithms.postprocessing.eq_odds_postprocessing\\\n                            import EqOddsPostprocessing\nfrom econml.dr import LinearDRLearner\nimport dowhy\nfrom dowhy import CausalModel\nimport xai\nfrom networkx.drawing.nx_pydot import to_pydot\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n```", "```py\nccdefault_all_df = mldatasets.load(\"cc-default\", prepare=True) \n```", "```py\nccdefault_all_df.info() \n```", "```py\nInt64Index: 30000 entries, 1 to 30000\nData columns (total 31 columns):\n#   Column            Non-Null Count  Dtype  \n---  ------            --------------  -----  \n0   CC_LIMIT_CAT      30000 non-null  int8   \n1   EDUCATION         30000 non-null  int8   \n2   MARITAL_STATUS    30000 non-null  int8   \n3   GENDER            30000 non-null  int8   \n4   AGE_GROUP         30000 non-null  int8   \n5   pay_status_1      30000 non-null  int8   \n6   pay_status_2      30000 non-null  int8   \n7   pay_status_3      30000 non-null  int8   \n8   pay_status_4      30000 non-null  int8   \n9   pay_status_5      30000 non-null  int8   \n10  pay_status_6      30000 non-null  int8   \n11  paid_pct_1        30000 non-null  float64\n12  paid_pct_2        30000 non-null  float64\n13  paid_pct_3        30000 non-null  float64\n14  paid_pct_4        30000 non-null  float64\n15  paid_pct_5        30000 non-null  float64\n16  paid_pct_6        30000 non-null  float64\n17  bill1_over_limit  30000 non-null  float64\n18  IS_DEFAULT        30000 non-null  int8   \n19  _AGE              30000 non-null  int16  \n20  _spend            30000 non-null  int32  \n21  _tpm              30000 non-null  int16  \n22  _ppm              30000 non-null  int16  \n23  _RETAIL           30000 non-null  int8   \n24  _URBAN            30000 non-null  int8   \n25  _RURAL            30000 non-null  int8   \n26  _PREMIUM          30000 non-null  int8   \n27  _TREATMENT        30000 non-null  int8   \n28  _LTV              30000 non-null  float64\n29  _CC_LIMIT         30000 non-null  int32  \n30  _risk_score       30000 non-null  float64 \n```", "```py\nccdefault_all_df._TREATMENT.value_counts() \n```", "```py\n-1    28904\n3      274\n2      274\n1      274\n0      274 \n```", "```py\nccdefault_bias_df = ccdefault_all_df[\n    ccdefault_all_df._TREATMENT < 1\n]\nccdefault_causal_df =ccdefault_all_df[\n    ccdefault_all_df._TREATMENT >= 0\n] \n```", "```py\nccdefault_bias_df[\n    ccdefault_bias_df.**IS_DEFAULT==1**\n].**GENDER.value_counts()**/ccdefault_bias_df.**GENDER.value_counts()** \n```", "```py\n2    0.206529\n1    0.241633 \n```", "```py\nmldatasets.**plot_prob_progression(**\n    ccdefault_bias_df.**_AGE**,\n    ccdefault_bias_df.**IS_DEFAULT**,\n    **x_intervals**=8,\n    use_quantiles=True,\n    xlabel='Age',\n    **mean_line**=True,\n    title='Probability of Default by Age'\n) \n```", "```py\nmldatasets.**plot_prob_progression(**\n    ccdefault_bias_df.**AGE_GROUP**.**replace**({0:'21-25,48+',1:'26-47'}),\n    ccdefault_bias_df.**IS_DEFAULT**,\n    xlabel='Age Group',\n    title='Probability of Default by Age Group',\n    **mean_line**=True\n) \n```", "```py\nmldatasets.plot_prob_contour_map(\n    ccdefault_bias_df.**GENDER**.replace({1:'Male',2:'Female'}),\n    ccdefault_bias_df.**AGE_GROUP**.replace({0:'21-25,48+',1:'26-47'}),\n    ccdefault_bias_df.**IS_DEFAULT**,\n    xlabel='Gender',\n    ylabel='Age Group',\n    annotate=True,\n    plot_type='grid',\n    title='Probability of Default by Gender/Age Group'\n) \ngroup is 26- 47-year-old females, followed by their male counterparts at about 3-4% apart. The same happens with the underprivileged age group:\n```", "```py\ncols_bias_l = ccdefault_all_df.columns[\n    **~ccdefault_all_df.columns.str.startswith('_')**\n].tolist()\ncols_causal_l = [**'AGE_GROUP'**,**'IS_DEFAULT'**] +\\\n    ccdefault_all_df.columns[\n        **ccdefault_all_df.columns.str.startswith('_')**\n    ].tolist()\nccdefault_bias_df = ccdefault_bias_df[**cols_bias_l**]\nccdefault_causal_df = ccdefault_causal_df[**cols_causal_l**] \n```", "```py\nrand = 9\nos.environ['PYTHONHASHSEED']=str(rand)\nnp.random.seed(rand)\ny = ccdefault_bias_df[**'IS_DEFAULT'**]\nX = ccdefault_bias_df.drop([**'IS_DEFAULT'**], axis=1).copy()\nX_train, X_test, y_train, y_test = model_selection.**train_test_split**(\n    X, y, test_size=0.25, random_state=rand\n) \n```", "```py\ntrain_ds = **BinaryLabelDataset**(\n    df=**X_train.join(y_train)**,\n    label_names=['IS_DEFAULT'],\n    protected_attribute_names=['AGE_GROUP', 'GENDER'],\n    favorable_label=0,\n    unfavorable_label=1\n)\ntest_ds = **BinaryLabelDataset**(\n    **df=X_test.join(y_test)**,\n    label_names=['IS_DEFAULT'],\n    protected_attribute_names=['AGE_GROUP', 'GENDER'],\n    favorable_label=0, unfavorable_label=1\n) \n```", "```py\nunderprivileged_groups=[**{'AGE_GROUP': 0}**]\nprivileged_groups=[**{'AGE_GROUP': 1}**]\nmetrics_train_ds = **BinaryLabelDatasetMetric**(\n    train_ds,\n    unprivileged_groups=underprivileged_groups,\n    privileged_groups=privileged_groups\n)\nprint('Statistical Parity Difference (SPD): %.4f' %\n      metrics_train_ds.**statistical_parity_difference**())\nprint('Disparate Impact (DI): %.4f' % \n      metrics_train_ds.**disparate_impact**())\nprint('Smoothed Empirical Differential Fairness (SEDF): %.4f' %\\\n      metrics_train_ds.**smoothed_empirical_differential_fairness**()) \n```", "```py\nStatistical Parity Difference (SPD):               -0.0437\nDisparate Impact (DI):                              0.9447\nSmoothed Empirical Differential Fairness (SEDF):    0.3514 \n```", "```py\ncls_mdls = {}\nlgb_params = {\n    'learning_rate': 0.4,\n    'reg_alpha': 21,\n    'reg_lambda': 1,\n    **'scale_pos_weight'**: 1.8\n}\nlgb_base_mdl = lgb.LGBMClassifier(\n    random_seed=rand,\n    max_depth=6,\n    num_leaves=33,\n    **lgb_params\n)\nlgb_base_mdl.fit(X_train, y_train)\n**cls_mdls**['lgb_0_base'] = mldatasets.**evaluate_class_mdl**(\n    lgb_base_mdl,\n    X_train,\n    X_test,\n    y_train,\n    y_test,\n    plot_roc=False,\n    plot_conf_matrix=True,\n    show_summary=True,\n    ret_eval_dict=True\n) \nFigure 11.4. The scale_pos_weight parameter ensures a healthier balance between false positives in the top-right corner and false negatives at the bottom left. As a result, precision and recall arenâ€™t too far off from each other. We favor high precision for a problem such as this one because we want to maximize true positives, but, not at the great expense of recall, so a balance between both is critical. While hyperparameter tuning, the F1 score, and the Matthews correlation coefficient (MCC) are useful metrics to use to this end. The evaluation of the LightGBM base model is shown here:\n```", "```py\ntest_pred_ds = test_ds.copy(deepcopy=True)\ntest_pred_ds.labels =\\\n    cls_mdls['lgb_0_base']['preds_test'].reshape(-1,1)\ntest_pred_ds.scores = \\\n    cls_mdls['lgb_0_base']['probs_test'].reshape(-1,1)\nmetrics_test_dict, metrics_test_cls = \\\n    mldatasets.**compute_aif_metrics**(\n        test_ds,\n        test_pred_ds,\n        unprivileged_groups=underprivileged_groups,\n        privileged_groups=privileged_groups\n    )\ncls_mdls['lgb_0_base'].**update**(metrics_test_dict)\nprint('Statistical Parity Difference (SPD): %.4f' %\n      metrics_test_cls.**statistical_parity_difference**())\nprint('Disparate Impact (DI): %.4f' %\n      metrics_test_cls.**disparate_impact**())\nprint('Average Odds Difference (AOD): %.4f' %\n      metrics_test_cls.**average_odds_difference**())\nprint('Equal Opportunity Difference (EOD): %.4f' %\n      metrics_test_cls.**equal_opportunity_difference**())\nprint('Differential Fairness Bias Amplification(DFBA): %.4f' % \\\n    metrics_test_cls.**differential_fairness_bias_amplification**()) \n```", "```py\nStatistical Parity Difference (SPD):               -0.0679\nDisparate Impact (DI):                                      0.9193\nAverage Odds Difference (AOD):                        -0.0550\nEqual Opportunity Difference (EOD):                 -0.0265\nDifferential Fairness Bias Amplification (DFBA):    0.2328 \n```", "```py\nreweighter= **Reweighing**(\n    unprivileged_groups=underprivileged_groups,\n    privileged_groups=privileged_groups\n)\nreweighter.**fit**(train_ds)\ntrain_rw_ds = reweighter.**transform**(train_ds) \n```", "```py\nmetrics_train_rw_ds = **BinaryLabelDatasetMetric**(\n    train_rw_ds,\n    unprivileged_groups=underprivileged_groups,\n    privileged_groups=privileged_groups\n)\nprint('Statistical Parity Difference (SPD): %.4f' %\n      metrics_train_rw_ds.**statistical_parity_difference**())\nprint('Disparate Impact (DI): %.4f' %\n       metrics_train_rw_ds.**disparate_impact**())\nprint('Smoothed Empirical Differential Fairness(SEDF): %.4f'%\nmetrics_train_rw_ds.**smoothed_empirical_differential_fairness**()) \n```", "```py\nStatistical Parity Difference (SPD):                    -0.0000\nDisparate Impact (DI):                                   1.0000\nSmoothed Empirical Differential Fairness (SEDF):    0.1942 \n```", "```py\nnp.**abs**(train_ds.**instance_weights.mean**() -\\\n       train_rw_ds.**instance_weights.mean**()) < 1e-6 \n```", "```py\nlgb_rw_mdl = lgb.LGBMClassifier(\n    random_seed=rand,\n    max_depth=6,\n    num_leaves=33,\n    **lgb_params\n)\nlgb_rw_mdl.fit(\n    X_train,\n    y_train,\n    **sample_weight**=train_rw_ds.instance_weights\n) \n```", "```py\ncls_mdls['lgb_1_rw'] = mldatasets.**evaluate_class_mdl**(\n    lgb_rw_mdl,\n    train_rw_ds.features,\n    X_test,\n    train_rw_ds.labels,\n    y_test,\n    plot_roc=False,\n    plot_conf_matrix=True,\n    show_summary=True,\n    ret_eval_dict=True\n)\ntest_pred_rw_ds = test_ds.copy(deepcopy=True)\ntest_pred_rw_ds.labels = cls_mdls['lgb_1_rw']['preds_test'\n    ].reshape(-1,1)\ntest_pred_rw_ds.scores = cls_mdls['lgb_1_rw']['probs_test'\n    ].reshape(-1,1)\nmetrics_test_rw_dict, _ = mldatasets.**compute_aif_metrics**(\n    test_ds,\n    test_pred_rw_ds,\n    unprivileged_groups=underprivileged_groups,\n    privileged_groups=privileged_groups\n)\ncls_mdls['lgb_1_rw'].update(metrics_test_rw_dict) \nFigure 11.5:\n```", "```py\nprotected_index = train_ds.feature_names.index('AGE_GROUP') \n```", "```py\ndi = np.array([])\ntrain_dir_ds = None\ntest_dir_ds = None\nlgb_dir_mdl = None\nX_train_dir = None\nX_test_dir = None\nlevels = np.hstack(\n    [np.linspace(0., 0.1, 41), np.linspace(0.2, 1, 9)]\n)\nfor level in tqdm(levels):\n    di_remover = **DisparateImpactRemover**(repair_level=level)\n    train_dir_ds_i = di_remover.**fit_transform**(train_ds)\n    test_dir_ds_i = di_remover.**fit_transform**(test_ds)\n    X_train_dir_i = np.**delete**(\n        train_dir_ds_i.features,\n        protected_index,\n        axis=1\n    )\n    X_test_dir_i = np.**delete**(\n        test_dir_ds_i.features,\n        protected_index,\n        axis=1\n    )\n    lgb_dir_mdl_i = lgb.**LGBMClassifier**(\n        random_seed=rand,\n        max_depth=5,\n        num_leaves=33,\n        **lgb_params\n    )\n    lgb_dir_mdl_i.**fit**(X_train_dir_i, train_dir_ds_i.labels)\n    test_dir_ds_pred_i = test_dir_ds_i.copy()\n    test_dir_ds_pred_i.labels = lgb_dir_mdl_i.predict(\n        X_test_dir_i\n    )\n    metrics_test_dir_ds = **BinaryLabelDatasetMetric**(\n        test_dir_ds_pred_i,\n        unprivileged_groups=underprivileged_groups,\n        privileged_groups=privileged_groups\n    )\n    di_i = metrics_test_dir_ds.disparate_impact()\n    if (di.shape[0]==0) or (np.min(np.abs(di-1)) >= abs(di_i-1)):\n        print(abs(di_i-1))\n        train_dir_ds = train_dir_ds_i\n        test_dir_ds = test_dir_ds_i\n        X_train_dir = X_train_dir_i\n        X_test_dir = X_test_dir_i\n        lgb_dir_mdl = lgb_dir_mdl_i\n    di = np.append(np.array(di), di_i) \n```", "```py\nplt.plot(**levels**, **di**, marker='o') \n```", "```py\ncls_mdls['lgb_1_dir'] = mldatasets.**evaluate_class_mdl**(\n    lgb_dir_mdl,\n    X_train_dir,\n    X_test_dir,\n    train_dir_ds.labels,\n    test_dir_ds.labels,\n    plot_roc=False,\n    plot_conf_matrix=False,\n    show_summary=False,\n    ret_eval_dict=True\n)\ntest_pred_dir_ds = test_ds.copy(deepcopy=True)\ntest_pred_dir_ds.labels = cls_mdls['lgb_1_dir']['preds_test'\n].reshape(-1,1)\nmetrics_test_dir_dict, _ = mldatasets.**compute_aif_metrics**(\n    test_ds,\n    test_pred_dir_ds,\n    unprivileged_groups=underprivileged_groups,\n    privileged_groups=privileged_groups\n)\ncls_mdls['lgb_1_dir'].**update**(metrics_test_dir_dict) \n```", "```py\nlgb_egr_mdl = ExponentiatedGradientReduction(\n    estimator=lgb_base_mdl,\n    max_iter=50,\n    constraints='DemographicParity'\n)\nlgb_egr_mdl.fit(train_ds) \n```", "```py\ntrain_pred_egr_ds = lgb_egr_mdl.**predict**(train_ds)\ntest_pred_egr_ds = lgb_egr_mdl.**predict**(test_ds)\ncls_mdls['lgb_2_egr'] = mldatasets.**evaluate_class_metrics_mdl**(\n    lgb_egr_mdl,\n    train_pred_egr_ds.labels,\n    test_pred_egr_ds.scores,\n    test_pred_egr_ds.labels,\n    y_train,\n    y_test\n)\nmetrics_test_egr_dict, _ = mldatasets.**compute_aif_metrics**(\n    test_ds,\n    test_pred_egr_ds,\n    unprivileged_groups=underprivileged_groups,\n    privileged_groups=privileged_groups\n)\ncls_mdls['lgb_2_egr'].**update**(metrics_test_egr_dict) \n```", "```py\ndt_gf_mdl = **GerryFairClassifier**(\n    C=100,\n    gamma=.005,\n    max_iters=50,\n    **fairness_def**='FN',\n    printflag=True,\n    **predictor**=tree.DecisionTreeRegressor(max_depth=3)\n)\ndt_gf_mdl.**fit**(train_ds, early_termination=True) \n```", "```py\ntrain_pred_gf_ds = dt_gf_mdl.**predict**(train_ds, threshold=False)\ntest_pred_gf_ds = dt_gf_mdl.**predict**(test_ds, threshold=False)\ncls_mdls['dt_2_gf'] = mldatasets.evaluate_class_metrics_mdl(\n    dt_gf_mdl,\n    train_pred_gf_ds.labels,\n    None,\n    test_pred_gf_ds.labels,\n    y_train,\n    y_test\n)\nmetrics_test_gf_dict, _ = mldatasets.**compute_aif_metrics**(\n    test_ds,\n    test_pred_gf_ds,\n    unprivileged_groups=underprivileged_groups,\n    privileged_groups=privileged_groups\n)\ncls_mdls['dt_2_gf'].**update**(metrics_test_gf_dict) \n```", "```py\nepp = **EqOddsPostprocessing**(\n    privileged_groups=privileged_groups,\n    unprivileged_groups=underprivileged_groups,\n    seed=rand\n)\nepp = epp.**fit**(test_ds, test_pred_ds)\ntest_pred_epp_ds = epp.**predict**(test_pred_ds) \n```", "```py\ncls_mdls['lgb_3_epp'] = mldatasets.**evaluate_class_metrics_mdl**(\n    lgb_base_mdl,\n    cls_mdls['lgb_0_base']['preds_train'],\n    test_pred_epp_ds.scores,\n    test_pred_epp_ds.labels,\n    y_train,\n    y_test\n)\nmetrics_test_epp_dict, _ = mldatasets.**compute_aif_metrics**(\n    test_ds,\n    test_pred_epp_ds,\n    unprivileged_groups=underprivileged_groups,\n    privileged_groups=privileged_groups\n)\ncls_mdls['lgb_3_epp'].**update**(metrics_test_epp_dict) \n```", "```py\ncpp = **CalibratedEqOddsPostprocessing**(\n    privileged_groups=privileged_groups,\n    unprivileged_groups=underprivileged_groups,\n    **cost_constraint**=\"fpr\",\n    seed=rand\n)\ncpp = cpp.**fit**(test_ds, test_pred_ds)\ntest_pred_cpp_ds = cpp.**predict**(test_pred_ds)\ncls_mdls['lgb_3_cpp'] = mldatasets.**evaluate_class_metrics_mdl**(\n    lgb_base_mdl,\n    cls_mdls['lgb_0_base']['preds_train'],\n    test_pred_cpp_ds.scores,\n    test_pred_cpp_ds.labels,\n    y_train,\n    y_test\n)\nmetrics_test_cpp_dict, _ = mldatasets.**compute_aif_metrics**(\n    test_ds,\n    test_pred_cpp_ds,\n    unprivileged_groups=underprivileged_groups,\n    privileged_groups=privileged_groups\n)\ncls_mdls['lgb_3_cpp'].**update**(metrics_test_cpp_dict) \n```", "```py\ncls_metrics_df = pd.DataFrame.from_dict(cls_mdls, 'index')[\n    [\n        'accuracy_train',\n        'accuracy_test',\n        'f1_test',\n        'mcc_test',\n        'SPD',\n        'DI',\n        'AOD',\n        'EOD',\n        'DFBA'\n    ]\n]\nmetrics_fmt_dict = dict(\n    zip(cls_metrics_df.columns,['{:.1%}']*3+ ['{:.3f}']*6)\n)\ncls_metrics_df.sort_values(\n    by='accuracy_test',\n    ascending=False\n).style.format(metrics_fmt_dict) \n```", "```py\ntest_df = ccdefault_bias_df.loc[**X_test.index**]\ntest_df['AGE_GROUP'] = test_df.AGE_GROUP.**replace**(\n    {0:'underprivileged', 1:'privileged'}\n)\ncat_cols_l = ccdefault_bias_df.dtypes[**lambda x: x==np.int8**\n                                      ].index.tolist()\n_ = xai.**metrics_plot**(\n    y_test,cls_mdls['lgb_3_epp']['probs_test'],\n    df=test_df, cross_cols=['AGE_GROUP'],\n    categorical_cols=cat_cols_l\n)\n_ = xai.**roc_plot**(\n    y_test, cls_mdls['lgb_3_epp']['probs_test'],\n    df=test_df, cross_cols=['AGE_GROUP'],\n    categorical_cols=cat_cols_l\n)\n_ = xai.**pr_plot**(\n    y_test,\n    cls_mdls['lgb_3_epp']['probs_test'],\n    df=test_df, cross_cols=['AGE_GROUP'],\n    categorical_cols=cat_cols_l\n) \nFigure 11.8. The first one shows that even the fairest model still has some disparities between both groups, especially between precision and recall and, by extension, F1 score, which is their average. However, the ROC curve shows how close both groups are from an FPR versus a TPR standpoint. The third plot is where the disparities in precision and recall become even more evident. This all demonstrates how hard it is to keep a fair balance on all fronts! Some methods are best for making one aspect perfect but nothing else, while others are pretty good on a handful of aspects but nothing else. Despite the shortcomings of the methods, most of them achieved a sizable improvement. Ultimately, choosing methods will depend on what you most care about, and combining them is also recommended for maximum effect! The output is shown here:\n```", "```py\ntreatment_names = [\n    'Lower Credit Limit',\n    'Payment Plan',\n    'Payment Plan &Credit Limit'\n]\nall_treatment_names = np.array([\"None\"] + treatment_names) \n```", "```py\npct_s = ccdefault_causal_df[\n    ccdefault_causal_df.IS_DEFAULT==1]\n    .groupby(['_TREATMENT'])\n    .size()\n    /ccdefault_causal_df.groupby(['_TREATMENT']).size()\nltv_s = ccdefault_causal_df.groupby(\n    ['_TREATMENT'])['_LTV'].sum()/1000\nplot_df = pd.DataFrame(\n    {'% Defaulted':pct_s,\n     'Total LTV, K$':ltv_s}\n)\nplot_df.index = all_treatment_names\nax = plot_df.plot(secondary_y=['Total LTV, K$'], figsize=(8,5))\nax.get_legend().set_bbox_to_anchor((0.7, 0.99))\nplt.grid(False) \nFigure 11.9. It can be inferred that all treatments fare better than the control group. The lowering of the credit limit on its own decreases the default rate by over 12% and more than doubles the estimated LTV, while the payment plan only decreases the defaults by 3% and increases the LTV by about 85%. However, both policies combined quadrupled the control groupâ€™s LTV and reduced the default rate by nearly 15%! The output can be seen here:\n```", "```py\nsns.**scatterplot**(\n    x=ccdefault_causal_df['_CC_LIMIT'].values,\n    y=ccdefault_causal_df['_risk_score'].values,\n    hue=all_treatment_names[ccdefault_causal_df['_TREATMENT'].values],\n    hue_order = all_treatment_names\n) \n```", "```py\nsns.**displot**(\n    ccdefault_causal_df,\n    x=\"_CC_LIMIT\",\n    hue=\"_TREATMENT\",\n    kind=\"kde\",\n    fill=True\n)\nsns.**displot**(\n    ccdefault_causal_df,\n    x=\"_LTV\",\n    hue=\"_TREATMENT\",\n    kind=\"kde\", fill=True\n) \nFigure 11.11. We can easily tell how far apart all four distributions are for both plots, mostly regarding treatment #3 (Payment Plan & Lower Credit Limit), which tends to be centered significantly more to the right and has a longer and fatter right tail. You can view the output here:\n```", "```py\n**W** = ccdefault_causal_df[\n    [\n      '_spend','_tpm', '_ppm', '_RETAIL','_URBAN', '_RURAL',\n      '_PREMIUM'\n    ]\n]\n**X** = ccdefault_causal_df[['_CC_LIMIT']]\n**T** = ccdefault_causal_df[['_TREATMENT']]\n**Y** = ccdefault_causal_df[['_LTV']] \n```", "```py\ndrlearner = **LinearDRLearner**(\n    model_regression=xgb.XGBRegressor(learning_rate=0.1),\n    model_propensity=xgb.XGBClassifier(learning_rate=0.1,\n    max_depth=2,\n    objective=\"multi:softmax\"),\n    random_state=rand\n) \n```", "```py\ncausal_mdl = drlearner.dowhy.fit(\n    Y,\n    T,\n    X=X,\n    W=W,\n    outcome_names=Y.columns.to_list(),\n    treatment_names=T.columns.to_list(),\n    feature_names=X.columns.to_list(),\n    confounder_names=W.columns.to_list(),\n    target_units=X.iloc[:550].values\n) \n```", "```py\ntry:\n    display(**Image**(to_pydot(causal_mdl._graph._graph).create_png()))\nexcept:\n    causal_mdl.**view_model**() \n```", "```py\nidentified_ate = causal_mdl.identified_estimand_\nprint(identified_ate)\ndrlearner_estimate = causal_mdl.estimate_\nprint(drlearner_estimate) \n the estimand expression for identified_estimand_, which is a derivation of the expected value for , with some assumptions. Then, the causal-realized estimate_ returns the ATE for treatment #1, as illustrated in the following code snippet:\n```", "```py\nEstimand type: nonparametric-ate\n### Estimand : 1\nEstimand name: backdoor1 (Default)\nEstimand expression:\n      d \nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€(E[_LTV|_RETAIL,_URBAN,_PREMIUM,_RURAL,_CC_LIMIT,\nd[_TREATMENT]\n])\nEstimand assumption 1, Unconfoundedness: If Uâ†’{_TREATMENT} and Uâ†’_LTV then \\ P(_LTV|_TREATMENT,_RETAIL,_URBAN,_PREMIUM,_RURAL,_CC_LIMIT,_spend,_ppm,_tpm,U) = \\ P(_LTV|_TREATMENT,_RETAIL,_URBAN,_PREMIUM,_RURAL,_CC_LIMIT,_spend,_ppm,_tpm)\n*** Causal Estimate ***\n## Identified estimand\nEstimand type: nonparametric-ate\n## Realized estimand\nb:_LTV ~ _TREATMENT + _RETAIL + _URBAN + _PREMIUM + _RURAL + \\ _CC_LIMIT + _spend + _ppm + _tpm | _CC_LIMIT\nTarget units:\n## Estimate\nMean value: 7227.904763676559\nEffect estimates: [6766.07978487 7337.39526574 7363.36013004\n                   7224.20893104 7500.84310705 7221.40328496] \n```", "```py\nfor i in range(causal_mdl._d_t[0]):\n    print(\"Treatment: %s\" % treatment_names[i])\n     display(econml_mdl.**summary**(T=i+1)) \n```", "```py\nidxs = np.arange(0, causal_mdl._d_t[0])\ncoefs = np.hstack([causal_mdl.**coef_**(T=i+1) for i in idxs])\nintercepts = np.hstack(\n    [causal_mdl.**intercept_**(T=i+1)for i in idxs]\n)\ncoefs_err = np.hstack(\n    [causal_mdl.**coef__interval**(T=i+1) for i in idxs]\n)\ncoefs_err[0, :] = coefs - coefs_err[0, :]\ncoefs_err[1, :] = coefs_err[1, :] - coefs\nintercepts_err = np.vstack(\n    [causal_mdl.**intercept__interval**(T=i+1) for i in idxs]\n).Tintercepts_err[0, :] = intercepts - intercepts_err[0, :]\nintercepts_err[1, :] = intercepts_err[1, :] - intercepts \n```", "```py\nax1 = plt.subplot(2, 1, 1)\nplt.errorbar(**idxs**, **coefs**, **coefs_err**, fmt=\"o\")\nplt.xticks(idxs, treatment_names)\nplt.setp(ax1.get_xticklabels(), visible=False)\nplt.title(\"Coefficients\")\nplt.subplot(2, 1, 2)\nplt.errorbar(**idxs**, **intercepts**, **intercepts_err**, fmt=\"o\")\nplt.xticks(idxs, treatment_names)\nplt.title(\"Intercepts\") \n```", "```py\n**cost_fn** = lambda X: np.repeat(\n    np.array([[0, 1000, 1000]]),\n    X.shape[0], axis=0) + (np.repeat(np.array([[72, 0, 72]]),\n    X.shape[0], axis=0)\n    *X._ppm.values.reshape(-1,1)\n)\n**treatment_effect_minus_costs** = causal_mdl.const_marginal_effect(\n    X=X.values) - **cost_fn**(ccdefault_causal_df)\ntreatment_effect_minus_costs = np.hstack(\n    [\n        np.zeros(X.shape),\n        **treatment_effect_minus_costs**\n    ]\n)\nrecommended_T = np.**argmax**(treatment_effect_minus_costs, axis=1) \n```", "```py\nsns.scatterplot(\n    x=ccdefault_causal_df['_CC_LIMIT'].values,\n    y=ccdefault_causal_df['_ppm'].values,\n    hue=all_treatment_names[recommended_T],\n    hue_order=all_treatment_names\n)\nplt.title(\"Optimal Credit Policy by Customer\")\nplt.xlabel(\"Original Credit Limit\")\nplt.ylabel(\"Payments/month\") \n```", "```py\nccdefault_causal_df['recommended_T'] = recommended_T\nplot_df = ccdefault_causal_df.groupby(\n    ['recommended_T','AGE_GROUP']).size().reset_index()\nplot_df['AGE_GROUP'] = plot_df.AGE_GROUP.**replace**(\n    {0:'underprivileged', 1:'privileged'}\n)\nplot_df = plot_df.pivot(\n    columns='AGE_GROUP',\n    index='recommended_T',\n    values=0\n)\nplot_df.index = treatment_names\nplot_df = plot_df.apply(lambda r: **r/r.sum()*100**, axis=1)\nplot_df.plot.bar(stacked=True, rot=0)\nplt.xlabel('Optimal Policy')\nplt.ylabel('%') \n```", "```py\nref_random = causal_mdl.refute_estimate(\n    method_name=\"random_common_cause\"\n)\nprint(ref_random) \n```", "```py\nRefute: Add a Random Common Cause\nEstimated effect:7227.904763676559\nNew effect:7241.433599647397 \n```", "```py\nref_placebo = causal_mdl.refute_estimate(\n    method_name=\"placebo_treatment_refuter\",\n    placebo_type=\"permute\", num_simulations=20\n)\nprint(ref_placebo) \n```", "```py\nRefute: Use a Placebo Treatment\nEstimated effect:7227.904763676559\nNew effect:381.05420029741083\np value:0.32491556283289624 \n```"]