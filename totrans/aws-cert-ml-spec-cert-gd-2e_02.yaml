- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: AWS Services for Data Storage
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS数据存储服务
- en: AWS provides a wide range of services to store your data safely and securely.
    There are various storage options available on AWS, such as block storage, file
    storage, and object storage. It is expensive to manage on-premises data storage
    due to the higher investment in hardware, admin overheads, and managing system
    upgrades. With AWS storage services, you just pay for what you use, and you don’t
    have to manage the hardware. You will also learn about various storage classes
    offered by Amazon S3 for intelligent access to data and to reduce costs. You can
    expect questions in the exam on storage classes. As you continue through this
    chapter, you will master the **single-AZ** and **multi-AZ** instances, and **Recovery
    Time Objective** (**RTO**) and **Recovery Point Objective** (**RPO**) concepts
    of Amazon RDS.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了一系列服务来安全地存储你的数据。在AWS上提供了各种存储选项，例如块存储、文件存储和对象存储。由于硬件投资较高、管理开销和系统升级管理，本地数据存储管理成本较高。使用AWS存储服务，你只需为所使用的付费，无需管理硬件。你还将了解Amazon
    S3提供的各种存储类别，以实现数据的智能访问和降低成本。你可以期待在考试中关于存储类别的相关问题。随着你继续阅读本章，你将掌握Amazon RDS的**单可用区**和**多可用区**实例，以及**恢复时间目标**（**RTO**）和**恢复点目标**（**RPO**）的概念。
- en: 'In this chapter, you will learn about storing your data securely for further
    analytical purposes throughout the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何在以下几节中安全地存储你的数据，以便进行进一步的分析：
- en: Storing data on Amazon S3
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon S3上存储数据
- en: Controlling access on S3 buckets and objects
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制S3存储桶和对象的访问
- en: Protecting data on Amazon S3
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护Amazon S3上的数据
- en: Securing S3 objects at rest and in transit
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在静止和传输过程中保护S3对象
- en: Using other types of data stores
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其他类型的数据存储
- en: '**Relational Database** **Services** (**RDSes**)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系数据库** **服务**（**RDS**）'
- en: Managing failover in Amazon RDS
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理Amazon RDS中的故障转移
- en: Taking automatic backup, RDS snapshots, and restore and read replicas
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动备份、RDS快照、恢复和读取副本
- en: Writing to Amazon Aurora with multi-master capabilities
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有多主能力的Amazon Aurora进行写入
- en: Storing columnar data on Amazon Redshift
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon Redshift上存储列式数据
- en: Amazon DynamoDB for NoSQL databases as a service
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为服务的NoSQL数据库Amazon DynamoDB
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'All you will need for this chapter is an AWS account and the AWS CLI configured.
    The steps to configure the AWS CLI for your account are explained in detail by
    Amazon here: [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的所有内容只是一个AWS账户和配置好的AWS CLI。配置AWS CLI的步骤在Amazon这里有详细的解释：[https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)。
- en: 'You can download the code examples from GitHub, here: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从GitHub下载代码示例，这里：[https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02)。
- en: Storing Data on Amazon S3
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Amazon S3上存储数据
- en: S3 is Amazon’s cloud-based object storage service, and it can be accessed from
    anywhere via the internet. It is an ideal storage option for large datasets. It
    is region-based, as your data is stored in a particular region until you move
    the data to a different region. Your data will never leave that region until it
    is configured to do so. In a particular region, data is replicated in the availability
    zones of that region; this makes S3 regionally resilient. If any of the availability
    zones fail in a region, then other availability zones will serve your requests.
    S3 can be accessed via the AWS console UI, AWS CLI, AWS API requests, or via standard
    HTTP methods.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: S3是Amazon基于云的对象存储服务，可以通过互联网从任何地方访问。它是大型数据集的理想存储选项。它是基于区域的，因为你的数据存储在特定的区域，直到你将数据移动到不同的区域。你的数据将永远不会离开该区域，直到它被配置为这样做。在特定区域，数据在该区域的可用区中进行复制；这使得S3在区域上具有弹性。如果该区域中的任何可用区失败，则其他可用区将处理你的请求。S3可以通过AWS控制台UI、AWS
    CLI、AWS API请求或通过标准HTTP方法访问。
- en: 'S3 has two main components: **buckets** and **objects**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: S3有两个主要组件：**存储桶**和**对象**。
- en: Buckets are created in a specific AWS region. Buckets can contain objects but
    cannot contain other buckets.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储桶是在特定的AWS区域中创建的。存储桶可以包含对象，但不能包含其他存储桶。
- en: Objects have two main attributes. One is the **key**, and the other is the **value**.
    The value is the content being stored, and the key is the name. The maximum size
    of an object can be 5 TB. As per the Amazon S3 documentation ([https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html)),
    objects also have a version ID, metadata, access control information, and sub-resources.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象有两个主要属性。一个是**键**，另一个是**值**。值是存储的内容，键是名称。对象的最大大小为5 TB。根据亚马逊S3文档（[https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html)），对象还有版本ID、元数据、访问控制信息和子资源。
- en: Important note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: As per Amazon’s docs, S3 provides read-after-write consistency for PUTs of new
    objects, which means that if you upload a new object or create a new object and
    you immediately try to read the object using its key, then you get the exact data
    that you just uploaded. However, for overwrites and deletes, it behaves in an
    **eventually consistent manner**. This means that if you read an object straight
    after the delete or overwrite operation, then you may read an old copy or a stale
    version of the object. It takes some time to replicate the content of the object
    across three Availability Zones.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据亚马逊的文档，S3为新的PUT操作提供了写后一致性读取，这意味着如果你上传了一个新的对象或者创建了一个新的对象，然后立即尝试使用其键读取该对象，那么你会得到你刚刚上传的确切数据。然而，对于覆盖和删除操作，它以**最终一致性**的方式表现。这意味着如果你在删除或覆盖操作后立即读取一个对象，那么你可能会读取到旧副本或过时的对象版本。将对象的内容复制到三个可用区需要一些时间。
- en: 'A folder structure can be maintained logically by using a prefix. Take an example
    where an image is uploaded into a bucket, `bucket-name-example`, with the prefix
    `folder-name` and the object name `my-image.jpg`. The entire structure looks like
    this: `/bucket-name-example/folder-name/my-image.jpg`.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用前缀来逻辑上维护文件夹结构。以一个例子来说明，一个图片被上传到一个名为`bucket-name-example`的桶中，前缀为`folder-name`，对象名为`my-image.jpg`。整个结构看起来是这样的：`/bucket-name-example/folder-name/my-image.jpg`。
- en: The content of the object can be read by using the bucket name of `bucket-name-example`
    and the key of `/folder-name/my-image.jpg`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`bucket-name-example`的桶名和`/folder-name/my-image.jpg`的键来读取对象的内容。
- en: 'There are several storage classes offered by Amazon for objects stored in S3:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊为存储在S3中的对象提供了几种存储类别：
- en: '**Standard Storage (S3 Standard):** This is the storage class for frequently
    accessed objects and for quick access. S3 Standard has a millisecond first-byte
    latency and objects can be made publicly available.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准存储（S3 Standard）**：这是频繁访问的对象和快速访问的存储类别。S3标准具有毫秒级的首字节延迟，并且对象可以被公开访问。'
- en: '**Standard Infrequent Access (S3 Standard-IA):** This option is used when you
    need data to be returned quickly, but not for frequent access. The object size
    has to be a minimum of 128 KB. The minimum storage timeframe is 30 days. If the
    object is deleted before 30 days, you are still charged for 30 days. Standard-IA
    objects are resilient to the loss of Availability Zones.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准低频访问（S3 Standard-IA）**：当需要快速返回数据但不是频繁访问时，使用此选项。对象大小必须至少为128 KB。最小存储时间为30天。如果对象在30天内被删除，你仍然会被收取30天的费用。Standard-IA对象对可用区的丢失具有弹性。'
- en: '**One Zone Infrequent Access (S3 One Zone-IA):** Objects in this storage class
    are stored in just one Availability Zone, which makes it cheaper than **Standard-IA**.
    The minimum object size and storage timeframe are the same as Standard-IA. Objects
    from this storage class are less available and less resilient. This storage class
    is used when you have another copy, or if the data can be recreated. A **One Zone-IA**
    storage class should be used for long-lived data that is non-critical and replaceable,
    and where access is infrequent.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单区低频访问（S3 One Zone-IA）**：这个存储类别的对象只存储在一个可用区，这使得它比**Standard-IA**更便宜。最小对象大小和存储时间与Standard-IA相同。这个存储类别的对象可用性和弹性较低。当你有另一个副本，或者数据可以被重新创建时，使用这个存储类别。应该为长期存储的非关键且可替换的数据使用**One
    Zone-IA**存储类别，并且访问频率较低。'
- en: '**Amazon S3 Glacier Flexible Retrieval (formerly S3 Glacier):** This option
    is used for long-term archiving and backup. It can take anything from minutes
    to hours to retrieve objects in this storage class. The minimum storage timeframe
    is 90 days. For archived data that doesn’t need to be accessed right away but
    requires the ability to retrieve extensive data sets without incurring additional
    charges, like in backup or disaster-recovery scenarios, S3 Glacier Flexible Retrieval
    (formerly known as S3 Glacier) is the perfect storage option.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon S3 Glacier Flexible Retrieval（之前称为S3 Glacier）**：此选项用于长期存档和备份。在此存储类别中检索对象可能需要从几分钟到几小时不等。最小存储时间为90天。对于不需要立即访问但需要能够检索大量数据集而不产生额外费用的存档数据，例如在备份或灾难恢复场景中，S3
    Glacier Flexible Retrieval（之前称为S3 Glacier）是完美的存储选项。'
- en: '**Amazon S3 Glacier Instant Retrieval:** This storage class offers cost-effective,
    high-speed storage for seldom-accessed, long-term data. Compared to S3 Standard-Infrequent
    Access, it can cut storage expenses by up to 68% when data is accessed once per
    quarter. This storage class is perfect for swiftly retrieving archive data like
    medical images, news media assets, or user-generated content archives. You can
    upload data directly or use S3 Lifecycle policies to move it from other S3 storage
    classes.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon S3 Glacier Instant Retrieval**：此存储类别为不常访问的长期数据提供经济高效、高速的存储。与S3 Standard-Infrequent
    Access相比，如果数据每季度访问一次，可以降低高达68%的存储费用。此存储类别非常适合快速检索存档数据，如医学图像、新闻媒体资产或用户生成内容存档。您可以直接上传数据或使用S3生命周期策略将其从其他S3存储类别移动过来。'
- en: '**Glacier Deep Archive:** The minimum storage duration of this class is 180
    days. This is the least expensive storage class and has a default retrieval time
    of 12 hours.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Glacier Deep Archive**：此类别的最小存储时间为180天。这是最经济的存储类别，默认检索时间为12小时。'
- en: '**S3 Intelligent-Tiering:** This storage class is designed to reduce operational
    overheads. Users pay a monitoring fee and AWS selects a storage class between
    Standard (a frequent-access tier) and Standard-IA (a lower-cost, infrequent-access
    tier) based on the access pattern of an object. This option is designed for long-lived
    data with unknown or unpredictable access patterns.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S3 Intelligent-Tiering**：此存储类别旨在降低运营成本。用户支付监控费用，AWS根据对象的访问模式在Standard（频繁访问层）和Standard-IA（成本较低、不频繁访问层）之间选择存储类别。此选项适用于具有未知或不可预测访问模式的长期数据。'
- en: Through sets of rules, the transition between storage classes and deletion of
    the objects can be managed easily and are referred to as **S3 Lifecycle configurations**.
    These rules consist of actions. These can be applied to a bucket or a group of
    objects in that bucket defined by prefixes or tags. Actions can either be **transition
    actions** or **expiration actions**. Transition actions define the storage class
    transition of the objects following the creation of *a user-defined* number of
    days. Expiration actions configure the deletion of versioned objects, or the deletion
    of delete markers or incomplete multipart uploads. This is very useful for managing
    costs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一系列规则，可以轻松管理存储类别之间的转换和对象的删除，这些规则被称为**S3生命周期配置**。这些规则包括操作。这些操作可以应用于一个存储桶或该存储桶中由前缀或标签定义的一组对象。操作可以是**转换操作**或**过期操作**。转换操作定义了在创建*用户定义的*天数后对象的存储类别转换。过期操作配置了版本化对象的删除，或删除标记或未完成的分片上传。这对于管理成本非常有用。
- en: 'An illustration is given in *Figure 2**.1*. You can find more details here:
    [https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图2.1*中给出了说明。更多详细信息请见：[https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)。
- en: '![Figure 2.1 – A comparison table of S3 Storage classes](img/B21197_02_01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – S3存储类别的比较表](img/B21197_02_01.jpg)'
- en: Figure 2.1 – A comparison table of S3 Storage classes
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – S3存储类别的比较表
- en: Creating buckets to hold data
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建存储桶以存储数据
- en: 'Now, you will see how to create a bucket, upload an object, and read the object
    using the AWS CLI:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将了解如何使用AWS CLI创建存储桶、上传对象以及读取对象：
- en: 'In the first step, check whether you have any buckets created by using the
    `aws s3` `ls` command:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一步，检查你是否已经使用 `aws s3 ls` 命令创建了任何存储桶：
- en: '[PRE0]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This command returns nothing here. So, create a bucket now by using the `mb`
    argument. Let’s say the bucket name is `demo-bucket-baba` in the `us-east-1` Region:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此命令在此处不返回任何内容。因此，现在请使用 `mb` 参数创建一个存储桶。假设存储桶名称为 `demo-bucket-baba`，位于 `us-east-1`
    区域：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you have created a bucket now, your next step is to copy a file to your
    bucket using the `cp` argument, as shown in the following code:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于您已经创建了一个存储桶，您的下一步是使用 `cp` 参数将文件复制到您的存储桶中，如下面的代码所示：
- en: '[PRE8]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To validate the file upload operation via the AWS console, please log in to
    your AWS account and go to the AWS S3 console to see the same. The AWS S3 console
    lists the result as shown in *Figure 2**.2*. The console may have changed by the
    time you are reading this book!
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要通过 AWS 控制台验证文件上传操作，请登录您的 AWS 账户并转到 AWS S3 控制台查看相同的内容。AWS S3 控制台将结果列示为 *图 2.2*。请注意，当您阅读这本书时，控制台可能已经发生了变化！
- en: '![Figure 2.2 – AWS S3 listing your files](img/B21197_02_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – AWS S3 列出您的文件](img/B21197_02_02.jpg)'
- en: Figure 2.2 – AWS S3 listing your files
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – AWS S3 列出您的文件
- en: 'You can also list the files in your S3 bucket from the command line, as shown
    here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以从命令行列出您的 S3 存储桶中的文件，如下所示：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you want to upload your filesystem directories and files to the S3 bucket,
    then `--recursive` will do the job for you:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想将您的文件系统目录和文件上传到 S3 存储桶，那么 `--recursive` 参数将为您完成工作：
- en: '[PRE13]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The contents of one bucket can be copied/moved to another bucket via the `cp`
    command and the `--recursive` parameter. To achieve this, you will have to create
    two buckets, `demo-bucket-baba-copied` and `demo-bucket-baba-moved`. The steps
    are as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个存储桶的内容可以通过 `cp` 命令和 `--recursive` 参数复制/移动到另一个存储桶。要实现这一点，您必须创建两个存储桶，`demo-bucket-baba-copied`
    和 `demo-bucket-baba-moved`。步骤如下：
- en: '[PRE18]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: If all the commands are run successfully, then the original bucket should be
    empty at the end (as all the files have now been moved).
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果所有命令都成功运行，那么原始存储桶最终应该是空的（因为所有文件现在都已移动）。
- en: Note
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the certification exam, you will not find many questions on bucket- and object-level
    operations. However, it is always better to know the basic operations and the
    required steps.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在认证考试中，您不会在存储桶和对象级别操作中找到很多问题。然而，了解基本操作和所需步骤总是更好的。
- en: 'The buckets must be deleted to avoid costs as soon as the hands-on work is
    finished. The bucket has to be empty before you run the `rb` command:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦实际操作完成，必须删除存储桶以避免费用。在运行 `rb` 命令之前，存储桶必须为空：
- en: '[PRE27]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `demo-bucket-baba-moved` bucket is not empty, so you couldn’t remove the
    bucket. In such scenarios, use the `--force` parameter to delete the entire bucket
    and all its contents, as shown here:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`demo-bucket-baba-moved` 存储桶不为空，因此您无法删除该存储桶。在这种情况下，请使用 `--force` 参数删除整个存储桶及其所有内容，如下所示：'
- en: '[PRE30]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s take an example of a bucket, `test-bucket`, that has a prefix, `images`.
    This prefix contains four image files named `animal.jpg`, `draw-house.jpg`, `cat.jpg`,
    and `human.jpg`.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们以一个具有前缀 `images` 的存储桶 `test-bucket` 为例。此前缀包含四个名为 `animal.jpg`、`draw-house.jpg`、`cat.jpg`
    和 `human.jpg` 的图片文件。
- en: 'Now, to delete the contents inside the images, the command will be as follows:
    `aws s3 rm` `s3://test-bucket/images –recursive`'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，要删除图片内的内容，命令如下：`aws s3 rm` `s3://test-bucket/images –recursive`
- en: The bucket should now be empty.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在存储桶应该是空的。
- en: In the next section, you are going to learn about object tags and object metadata.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习关于对象标签和对象元数据的内容。
- en: Distinguishing between object tags and object metadata
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分对象标签和对象元数据
- en: 'Let’s compare these two terms:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较这两个术语：
- en: '**Object tag**: An object tag is a **key-value** pair. AWS S3 object tags can
    help you filter analytics and metrics, categorize storage, secure objects based
    on certain categorizations, track costs based on certain categorization of objects,
    and much more besides. Object tags can be used to create life cycle rules to move
    objects to cheaper storage tiers. You can have a maximum of 10 tags added to an
    object and 50 tags to a bucket. A tag key can contain 128 Unicode characters,
    while a tag value can contain 256 Unicode characters.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象标签**：对象标签是一个 **键值对**。AWS S3 对象标签可以帮助您过滤分析指标，对存储进行分类，根据某些分类对对象进行安全保护，根据对象的某些分类跟踪成本，以及更多。对象标签可以用来创建生命周期规则，将对象移动到更便宜的存储层。您可以为对象添加最多
    10 个标签，为存储桶添加最多 50 个标签。标签键可以包含 128 个 Unicode 字符，而标签值可以包含 256 个 Unicode 字符。'
- en: '**Object metadata**: Object metadata is descriptive data describing an object.
    It consists of **name-value** pairs. Object metadata is returned as HTTP headers
    on objects. They are of two types: one is **system metadata**, and the other is
    **user-defined metadata**. User-defined metadata is a custom name-value pair added
    to an object by the user. The name must begin with **x-amz-meta**. You can change
    all system metadata such as storage class, versioning, and encryption attributes
    on an object. Further details are available here: [https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象元数据**：对象元数据是描述对象的描述性数据。它由**名称-值**对组成。对象元数据作为对象的HTTP头返回。它们有两种类型：一种是**系统元数据**，另一种是**用户定义元数据**。用户定义元数据是用户添加到对象中的自定义名称-值对。名称必须以**x-amz-meta**开头。您可以更改对象上的所有系统元数据，如存储类、版本控制和加密属性。更多详细信息请参阅此处：[https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html)。'
- en: Important note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Metadata names are case-insensitive, whereas tag names are case-sensitive.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据名称不区分大小写，而标签名称区分大小写。
- en: In the next section, you are going to learn about controlling access to buckets
    and objects on Amazon S3 through different policies, including the resource policy
    and the identity policy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何通过不同的策略来控制Amazon S3中存储桶和对象的访问，包括资源策略和身份策略。
- en: Controlling access to buckets and objects on Amazon S3
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制对Amazon S3中存储桶和对象的访问
- en: Once the object is stored in the bucket, the next major step is to manage access.
    S3 is private by default, and access is given to other users, groups, or resources
    via several methods. This means that access to the objects can be managed via
    **Access Control Lists (ACLs)**, **Public Access Settings**, **Identity Policies**,
    and **Bucket Policies**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦对象存储在存储桶中，下一步重要的步骤就是管理访问。S3默认是私有的，并且通过多种方法向其他用户、组或资源提供访问权限。这意味着可以通过**访问控制列表（ACLs）**、**公共访问设置**、**身份策略**和**存储桶策略**来管理对象的访问权限。
- en: Let’s look at some of these in detail.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看其中的一些。
- en: S3 bucket policy
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: S3存储桶策略
- en: 'An `Principal` is rendered `*`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`Principal`被渲染为`*`：'
- en: '[PRE31]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: By default, everything in S3 is private to the owner. If you want to make a
    prefix public to the world, then `Resource` changes to `arn:aws:s3:::my-bucket/some-prefix/*`,
    and similarly, if it is intended for a specific IAM user or IAM group, then those
    details go in the principal part in the policy.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，S3中的所有内容对所有者都是私有的。如果您想将前缀公开给全世界，那么`Resource`将变为`arn:aws:s3:::my-bucket/some-prefix/*`，同样地，如果它是为特定的IAM用户或IAM组设计的，那么这些详细信息将包含在策略的主体部分。
- en: 'There can be conditions added to the bucket policy too. Let’s examine a use
    case where the organization wants to keep a bucket public and whitelist particular
    IP addresses. The policy would look something like this:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在存储桶策略中添加条件。让我们考察一个组织希望保持存储桶公开并白名单特定IP地址的用例。该策略可能看起来像这样：
- en: '[PRE43]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'More examples are available in the AWS S3 developer guide, which can be found
    here: [https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 更多示例可以在AWS S3开发者指南中找到，该指南可在此处找到：[https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html)。
- en: '**Block public access** is a separate setting given to the bucket owner to
    avoid any kind of mistakes in bucket policy. In a real-world scenario, the bucket
    can be made public through bucket policy by mistake; to avoid such mistakes, or
    data leaks, AWS has provided this setting. It provides a further level of security,
    irrespective of the bucket policy. You can choose this while creating a bucket,
    or it can be set after creating a bucket.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**阻止公共访问**是分配给存储桶所有者的一个独立设置，以避免在存储桶策略中犯任何错误。在现实场景中，存储桶可能会由于策略错误而公开；为了避免此类错误或数据泄露，AWS提供了这个设置。它提供了比存储桶策略更高的安全级别。您可以在创建存储桶时选择此设置，也可以在创建存储桶后设置。'
- en: '`us-east-1` in this example):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`us-east-1`在此示例中）：'
- en: '[PRE58]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '**ACLs** are used to grant high-level permissions, typically for granting access
    to other AWS accounts. ACLs are one of the **sub-resources** of a bucket or an
    object. A bucket or object can be made public quickly via ACLs. AWS doesn’t suggest
    doing this, and you shouldn’t expect questions about this on the test. It is good
    to know about this, but it is not as flexible as the **S3** **bucket policy**.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**ACLs**用于授予高级权限，通常用于授予对其他AWS账户的访问权限。ACLs是存储桶或对象的**子资源**之一。可以通过ACLs快速使存储桶或对象公开。AWS不建议这样做，您也不应该期望在测试中遇到关于此的问题。了解这一点是好的，但它不如**S3**
    **存储桶策略**灵活。'
- en: Now, let’s learn about the methods to protect our data in the next section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在下一节中了解保护我们数据的方法。
- en: Protecting data on Amazon S3
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护Amazon S3上的数据
- en: In this section, you will learn how to record every version of an object. Along
    with durability, Amazon provides several techniques to secure the data in S3\.
    Some of those techniques involve enabling versioning and encrypting the objects.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何记录对象的每个版本。除了耐用性之外，Amazon还提供了几种技术来保护S3中的数据。其中一些技术涉及启用版本控制和加密对象。
- en: Versioning helps you to roll back to a previous version if any problem occurs
    with the current object during update, delete, or put operations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制可以帮助您在更新、删除或put操作期间，如果当前对象出现问题，回滚到之前的版本。
- en: Through encryption, you can control the access of an object. You need the appropriate
    key to read and write an object. You will also learn **Multi-Factor Authentication
    (MFA**) for delete operations. Amazon also allows **Cross-Region Replication (CRR)**
    to maintain a copy of an object in another Region, which can be used for data
    backup during any disaster, for further redundancy, or for the enhancement of
    data access speed in different Regions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通过加密，您可以控制对象的访问。您需要适当的密钥来读取和写入对象。您还将学习用于删除操作的**多因素认证（MFA**）。Amazon还允许**跨区域复制（CRR**），在另一个区域中维护对象的副本，这可以在任何灾难期间用于数据备份，以提供额外的冗余，或者用于提高不同区域的数据访问速度。
- en: Applying bucket versioning
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用存储桶版本控制
- en: 'Let’s now understand how you can enable bucket versioning with the help of
    some hands-on examples. Bucket versioning can be applied while creating a bucket
    from the AWS S3 console:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在通过一些实际示例了解如何使用帮助启用存储桶版本控制。在创建存储桶时，可以从AWS S3控制台应用存储桶版本控制：
- en: 'To enable versioning on a bucket from the command line, a bucket must be created
    first and then versioning can be enabled, as shown in the following example. In
    this example, I have created a bucket, `version-demo-mlpractice`, and enabled
    versioning through the `put-bucket-versioning` command:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从命令行启用存储桶的版本控制，首先必须创建一个存储桶，然后才能启用版本控制，如下面的示例所示。在这个示例中，我创建了一个名为`version-demo-mlpractice`的存储桶，并通过`put-bucket-versioning`命令启用了版本控制：
- en: '[PRE73]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'You have not created this bucket with any kind of encryption. So, if you run
    **aws s3api get-bucket-encryption --bucket version-demo-mlpractice**, then it
    will output an error that says the following:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您没有为此存储桶创建任何类型的加密。因此，如果您运行**aws s3api get-bucket-encryption --bucket version-demo-mlpractice**，那么它将输出一个错误，说明如下：
- en: '[PRE79]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '`put-bucket-encryption` API. The command will look like this:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`put-bucket-encryption` API。命令看起来像这样：'
- en: '[PRE80]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'This can be verified using the following command: `aws s3api get-bucket-encryption
    --``bucket version-demo-mlpractice`.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这可以通过以下命令进行验证：`aws s3api get-bucket-encryption --bucket version-demo-mlpractice`。
- en: You will learn more about encryption in the next section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在下一节中了解更多关于加密的内容。
- en: Applying encryption to buckets
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用存储桶加密
- en: 'You also need to understand how enabling versioning on a bucket would help.
    There are use cases where a file is updated regularly, and versions will be created
    for the same file. To simulate this scenario, try the following example:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要了解如何启用存储桶的版本控制会有什么帮助。有一些用例中，文件会定期更新，并为同一文件创建版本。为了模拟这种场景，请尝试以下示例：
- en: 'In this example, you will create a file with versions written in it. You will
    overwrite it and retrieve it to check the versions in that file:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本例中，您将创建一个包含版本信息的文件。您将覆盖它并检索它以检查该文件中的版本：
- en: '[PRE82]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Upon retrieval, you got the latest version of the file, in other words, `Version-2`
    in this case. To check each of the versions and the latest one of them, S3 provides
    the `list-object-versions` API, as shown here. From the JSON results, you can
    deduce the latest version:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检索时，您将获得文件的最新版本，换句话说，在本例中是`Version-2`。要检查每个版本及其最新版本，S3提供了`list-object-versions`
    API，如下所示。从JSON结果中，您可以推断出最新版本：
- en: '[PRE94]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'There may be a situation where you have to roll back to the earlier version
    of the current object. In the preceding example, the latest one is `Version-2.`
    You can make any desired version the latest or current version by parsing the
    `VersionId` sub-resource to the `get-object` API call and uploading that object
    again. The other way is to delete the current or latest version by passing `versionId`
    to the `–version-id` parameter in the `delete-object` API request. More details
    about the API are available here: [https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html](https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html).'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能会有这样的情况，您需要回滚到当前对象的早期版本。在上面的示例中，最新的一个是`Version-2.`您可以通过将`VersionId`子资源解析到`get-object`
    API调用中并重新上传该对象，将任何所需的版本设置为最新或当前版本。另一种方法是，通过在`delete-object` API请求中将`versionId`传递给`–version-id`参数来删除当前或最新版本。有关API的更多详细信息，请参阅此处：[https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html](https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html)。
- en: 'When you delete an object in a versioning-enabled bucket, it does not delete
    the object from the bucket. It just creates a marker called `DeleteMarker`. It
    looks like this:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您在启用了版本控制的存储桶中删除对象时，并不会从存储桶中删除该对象。它只是创建了一个名为`DeleteMarker`的标记。它看起来像这样：
- en: '[PRE126]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'This means that the object is not deleted. You can list it by using this command:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这意味着对象没有被删除。您可以使用此命令列出它：
- en: '[PRE130]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Now the bucket has no objects as `version-doc.txt`, and you can verify this
    using the `aws s3 ls` command because that marker became the current version of
    the object with a new ID. If you try to retrieve an object that is deleted, which
    means a delete marker is serving the current version of the object, then you will
    get a `VersionId`, as shown in the following example commands. A simple delete
    request `{`
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在存储桶中没有名为`version-doc.txt`的对象，您可以使用`aws s3 ls`命令来验证这一点，因为该标记已成为具有新ID的对象的当前版本。如果您尝试检索已删除的对象，这意味着删除标记正在提供对象的当前版本，那么您将获得一个`VersionId`，如下面的示例命令所示。一个简单的删除请求`{`
- en: '[PRE131]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'Upon listing the bucket now, the older objects can be seen:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在列出存储桶时，可以看到较旧的对象：
- en: '[PRE134]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: As you have already covered the exam topics and practiced most of the required
    concepts, you should delete the objects in the bucket and then delete the bucket
    to save on costs. This step deletes the versions of the object and, in turn, removes
    the object permanently.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于您已经涵盖了考试主题并练习了大多数所需的概念，您应该删除存储桶中的对象，然后删除存储桶以节省成本。这一步删除了对象的版本，从而永久删除了对象。
- en: 'Here, the latest version is deleted by giving the version ID to it, followed
    by the other version ID:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，通过给出版本ID来删除最新版本，然后是另一个版本ID：
- en: '[PRE136]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: You can clearly see the empty bucket now.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您现在可以清楚地看到空存储桶。
- en: Important note
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'AWS best practices suggest adding another layer of protection through **MFA
    delete.** Accidental bucket deletions can be prevented, and the security of the
    objects in the bucket is ensured. MFA delete can be enabled or disabled via the
    console and CLI. As documented in AWS docs, MFA delete requires two forms of authentication
    together: your security credentials, and the concatenation of a valid serial number,
    a space, and the six-digit code displayed on an approved authentication device.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: AWS最佳实践建议通过**多因素认证删除**添加另一层保护。可以防止意外删除存储桶，并确保存储桶中对象的安全性。可以通过控制台和CLI启用或禁用多因素认证删除。如AWS文档所述，多因素认证删除需要两种认证方式同时进行：您的安全凭证，以及一个有效序列号、一个空格和显示在批准的认证设备上的六位数字代码的组合。
- en: 'CRR helps you to separate data between different geographical Regions. A typical
    use case is the maintenance business-as-usual activities during a disaster. If
    a Region goes down, then another Region can support the users if CRR is enabled.
    This improves the availability of the data. Another use case is to reduce latency
    if the same data is used by another compute resource, such as EC2 or AWS Lambda
    being launched in another Region. You can also use CRR to copy objects to another
    AWS account that belongs to a different owner. There are a few important points
    that are worth noting down for the certification exam:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: CRR可以帮助您在不同地理区域之间分离数据。一个典型的用例是在灾难期间维护常规业务活动。如果一个区域关闭，那么如果启用了CRR，另一个区域可以支持用户。这提高了数据可用性。另一个用例是如果相同的数据被另一个计算资源使用，例如在另一个区域启动的EC2或AWS
    Lambda，可以减少延迟。您还可以使用CRR将对象复制到属于不同所有者的另一个AWS账户。对于认证考试，以下是一些重要要点值得记录：
- en: In order to use CRR, versioning has to be enabled on both the source and destination
    bucket.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使用 CRR，必须在源和目标存储桶上启用版本控制。
- en: Replication is enabled on the source bucket by adding rules. As the source,
    either an entire bucket, a prefix, or tags can be replicated.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过添加规则，可以在源存储桶上启用复制。作为源，可以是整个存储桶、前缀或标签进行复制。
- en: Encrypted objects can also be replicated by assigning an appropriate encryption
    key.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分配适当的加密密钥，也可以通过复制加密对象。
- en: The destination bucket can be in the same account or in another account. You
    can change the storage type and ownership of the object in the destination bucket.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标存储桶可以位于同一账户或另一个账户中。你可以更改目标存储桶中对象的存储类型和所有权。
- en: For CRR, an existing role can be chosen or a new IAM role can be created too.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 CRR，可以选择现有的角色，也可以创建新的 IAM 角色。
- en: There can be multiple replication rules on the source bucket, with priority
    accorded to it. Rules with higher priority override rules with lower priority.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源存储桶上可以有多个复制规则，并赋予其优先级。优先级较高的规则会覆盖优先级较低的规则。
- en: When you add a replication rule, only new versions of an object that are created
    after the rules are enabled get replicated.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你添加复制规则时，只有规则启用后创建的新版本对象才会被复制。
- en: If versions are deleted from the source bucket, then they are not deleted from
    the destination bucket.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果从源存储桶中删除了版本，则它们不会被从目标存储桶中删除。
- en: When you delete an object from the source bucket, it creates a delete marker
    in said source bucket. That delete marker is not replicated to the destination
    bucket by S3.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你从源存储桶中删除对象时，会在该源存储桶中创建一个删除标记。S3 不会将此删除标记复制到目标存储桶。
- en: In the next section, you will cover the concept of securing S3 objects.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解保护 S3 对象的概念。
- en: Securing S3 objects at rest and in transit
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护静止和传输中的 S3 对象
- en: 'In the previous section, you learned about bucket default encryption, which
    is completely different from object-level encryption. Buckets are not encrypted,
    whereas objects are. A question may arise here: *what is the default bucket encryption?*
    You will learn these concepts in this section. Data during transmission can be
    protected by using **Secure Socket Layer (SSL)** or **Transport Layer Security
    (TLS)** for the transfer of HTTPS requests. The next step is to protect the data,
    where the authorized person can encode and decode the data.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学习了存储桶默认加密的概念，这与对象级加密完全不同。存储桶没有被加密，而对象被加密。这里可能会出现一个问题：*默认存储桶加密是什么？* 你将在本节中学习这些概念。在传输数据时，可以使用
    **安全套接字层 (SSL**) 或 **传输层安全性 (TLS**) 来保护 HTTPS 请求的传输。下一步是保护数据，授权人员可以编码和解码数据。
- en: 'It is possible to have different encryption settings on different objects in
    the same bucket. S3 supports **Client-Side Encryption (CSE)** and **Server-Side
    Encryption (SSE)** for objects at rest:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个存储桶中，可以为不同的对象设置不同的加密设置。S3 支持对静止对象使用 **客户端加密 (CSE**) 和 **服务器端加密 (SSE)**：
- en: '**CSE**: A client uploads the object to S3 via the S3 endpoint. In CSE, the
    data is encrypted by the client before uploading to S3\. Although the transit
    between the user and the S3 endpoint happens in an encrypted channel, the data
    in the channel is already encrypted by the client and can’t be seen. In transit,
    encryption takes place by default through HTTPS. So, AWS S3 stores the encrypted
    object and cannot read the data in any format at any point in time. In CSE, the
    client takes care of encrypting the object’s content. So, control stays with the
    client in terms of key management and the encryption-decryption process. This
    leads to a huge amount of CPU usage. S3 is only used for storage.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CSE**：客户端通过 S3 端点将对象上传到 S3。在 CSE 中，数据在上传到 S3 之前由客户端加密。尽管用户和 S3 端点之间的传输发生在加密通道中，但通道中的数据已经被客户端加密，无法被看到。在传输过程中，默认通过
    HTTPS 进行加密。因此，AWS S3 存储加密对象，在任何时候都无法以任何格式读取数据。在 CSE 中，客户端负责加密对象的内容。因此，密钥管理和加密解密过程始终由客户端控制。这导致大量的
    CPU 使用。S3 仅用于存储。'
- en: '**SSE**: A client uploads the object to S3 via the S3 endpoint. Even though
    the data in transit is through an encrypted channel that uses HTTPS, the objects
    themselves are not encrypted inside the channel. Once the data hits S3, then it
    is encrypted by the S3 service. In SSE, you trust S3 to perform encryption-decryption,
    object storage, and key management. There are three types of SSE techniques available
    for S3 objects:'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSE-C
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SSE-S3
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SSE-KMS
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PUT` operation, the user has to provide a key and an object to S3\. S3 encrypts
    the object using the key provided and attaches the hash (a cipher text) to the
    object. As soon as the object is stored, S3 discards the encryption key. This
    generated hash is one-way and cannot be used to generate a new key. When the user
    provides a `GET` operation request along with the decryption key, the hash identifies
    whether the specific key was used for encryption. Then, S3 decrypts and discards
    the key.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PUT` operation, the user just provides the unencrypted object. S3 creates
    a master key to be used for the encryption process. No one can change anything
    on this master key as this is created, rotated internally, and managed by S3 from
    end to end. This is a unique key for the object. It uses the AES-256 algorithm
    by default.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSE with Customer Master Keys stored in AWS Key Management Service (SSE-KMS):**
    AWS Key Management Service (KMS) manages the Customer Master Key (CMK). AWS S3
    collaborates with AWS KMS and generates an AWS-managed CMK. This is the default
    master key used for SSE-KMS. Every time an object is uploaded, S3 uses a dedicated
    key to encrypt that object, and that key is a **Data Encryption Key (DEK).** The
    DEK is generated by KMS using the CMK. S3 is provided with both a plain-text version
    and an encrypted version of the DEK. The plain-text version of DEK is used to
    encrypt the object and then discarded. The encrypted version of DEK is stored
    along with the encrypted object. When you are using SSE-KMS, it is not necessary
    to use the default CMK that is created by S3\. You can create and use a customer-managed
    CMK, which means you can control the permission on it as well as the rotation
    of the key material. So, if you have a regulatory board in your organization that
    is concerned with the rotation of the key or the separation of roles between encryption
    users and decryption users, then SSE-KMS is the solution. Logging and auditing
    are also possible on SSE-KMS to track the API calls made against keys.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PUT` operation).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, you will learn about some of the data stores used with
    EC2 instances.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Using other types of data stores
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Elastic Block Store (EBS)** is used to create volumes in an Availability
    Zone. The volume can only be attached to an EC2 instance in the same Availability
    Zone. Amazon EBS provides both **Solid-State Drive (SSD)** and **Hard Disk Drive
    (HDD)** types of volumes. For SSD-based volumes, the dominant performance attribute
    is **Input-Output Per Second (IOPS)**, and for HDD it is throughput, which is
    generally measured as MiB/s. You can choose between different volume types, such
    as General Purpose SSD (gp2), Provisioned IOPS SSD (io1), or Throughput Optimized
    HDD (st1), depending on your requirements. Provisioned IOPS volumes are often
    used for high-performance workloads, such as deep learning training, where low
    latency and high throughput are critical. *Table 2.1* provides an overview of
    the different volumes and types:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '| **Volume Types** | **Use cases** |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
- en: '| General Purpose SSD (gp2) | Useful for maintaining balance between price
    and performance. Good for most workloads, system boot volumes, dev, and test environments
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| Provisioned IOPS SSD (io2, io1) | Useful for mission-critical, high-throughput
    or low-latency workloads. For example, I/O intensive database workloads like MongoDB,
    Cassandra, Oracle |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| Throughput Optimized HDD (st1) | Useful for frequently accessed, throughput-intensive
    workloads. For example, big data processing, data warehouses, log processing |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| Cold HDD (sc1) | Useful for less frequently accessed workloads |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Different volumes and their use cases
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '**EBS** is designed to be resilient within an **Availability Zone (AZ)**. If,
    for some reason, an AZ fails, then the volume cannot be accessed. To prevent such
    scenarios, **snapshots** can be created from the EBS volumes, and they are stored
    in S3\. Once the snapshot arrives in S3, the data in the snapshot becomes Region-resilient.
    The first snapshot is a full copy of data on the volume and, from then onward,
    snapshots are incremental. Snapshots can be used to clone a volume. As the snapshot
    is stored in S3, a volume can be cloned in any AZ in that Region. Snapshots can
    be shared between Regions and volumes can be cloned from them during disaster
    recovery. Even after the EC2 instance is stopped/terminated, EBS volumes can retain
    data through an easy restoration process from backed-up snapshots.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Multiple EC2 instances can be attached via **EBS Multi-Attach** for concurrent
    EBS volume access. If the use case demands multiple instances to access the training
    dataset simultaneously (distributed training scenarios), then EBS Multi-Attach
    will provide the solution with improved performance and scalability.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: AWS KMS manages the CMK. AWS KMS uses an AWS-managed CMK for EBS, or AWS KMS
    can use a customer-managed CMK. The CMK is used by EBS when an encrypted volume
    is created. The CMK is used to create an encrypted DEK, which is stored with the
    volume on the physical disk. This DEK can only be decrypted using KMS, assuming
    the entity has access to decrypt. When a snapshot is created from the encrypted
    volume, the snapshot is encrypted with the same DEK. Any volume created from this
    snapshot also uses that DEK.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance Store volumes are the block storage devices physically connected to
    the EC2 instance. They provide the highest performance, as the ephemeral storage
    attached to the instance is from the same host where the instance is launched.
    EBS can be attached to the instance at any time, but the instance store must be
    attached to the instance at the time of its launch; it cannot be attached once
    the instance is launched. If there is an issue on the underlying host of an EC2
    instance, then the same instance will be launched on another host with a new instance
    store volume and the earlier instance store (ephemeral storage) and old data will
    be lost. The size and capabilities of the attached volumes depend on the instance
    types and can be found in more detail here: [https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '**Elastic File System (EFS)** provides a network-based filesystem that can
    be mounted within Linux EC2 instances and can be used by multiple instances at
    once. It is an implementation of **NFSv4**. It can be used in general-purpose
    mode, max I/O performance mode (for scientific analysis or parallel computing),
    bursting mode, and provisioned throughput mode. This makes it ideal for scenarios
    where multiple instances need to train on large datasets or share model artifacts.
    With EFS, you can store training datasets, pre-trained models, and other data
    centrally, ensuring consistency and reducing data duplication. Additionally, EFS
    provides high throughput and low-latency access, enabling efficient data access
    during training and inference processes. By leveraging EFS with SageMaker, machine
    learning developers can seamlessly scale their workloads, collaborate effectively,
    and accelerate model development and training.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'As you know, in the case of instance stores, the data is volatile. As soon
    as the instance is lost, the data is lost from the instance store. That is not
    the case for EFS. EFS is separate from the EC2 instance storage. EFS is a file
    store and is accessed by multiple EC2 instances via mount targets inside a VPC.
    On-premises systems can access EFS storage via hybrid networking to the VPC, such
    as **VPN** or **Direct Connect**. EFS also supports two types of storage classes:
    Standard and Infrequent Access. Standard is used for frequently accessed data.
    Infrequent Access is the cost-effective storage class for long-lived, less frequently
    accessed data. Lifecycle policies can be used for the transition of data between
    storage classes. EFS offers a pay-as-you-go pricing model, where you only pay
    for the storage capacity you use. It eliminates the need to provision and manage
    separate storage volumes for each instance, reducing storage costs and simplifying
    storage management for your machine learning workloads.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: An instance store is preferred for max I/O requirements and if the data is replaceable
    and temporary.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Relational Database Service (RDS)
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of the most commonly featured topics in AWS exams. You should have
    sufficient knowledge prior to the exam. In this section, you will learn about
    Amazon’s RDS.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: AWS provides several relational databases as a service to its users. Users can
    run their desired database on EC2 instances, too. The biggest drawback is that
    the instance is only available in one Availability Zone in a Region. The EC2 instance
    has to be administered and monitored to avoid any kind of failure. Custom scripts
    will be required to maintain a data backup over time. Any database major or minor
    version update would result in downtime. Database instances running on an EC2
    instance cannot be easily scaled if the load increases on the database as replication
    is not an easy task.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: RDS provides managed database instances that can themselves hold one or more
    databases. Imagine a database server running on an EC2 instance that you do not
    have to manage or maintain. You need only access the server and create databases
    in it. AWS will manage everything else, such as the security of the instance,
    the operating system running on the instance, the database versions, and high
    availability of the database server. RDS supports multiple engines, such as MySQL,
    Microsoft SQL Server, MariaDB, Amazon Aurora, Oracle, and PostgreSQL. You can
    choose any of these based on your requirements.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The foundation of Amazon RDS is a database instance, which can support multiple
    engines and can have multiple databases created by the user. One database instance
    can be accessed only by using the database DNS endpoint (the CNAME, which is an
    alias for the canonical name in a domain name system database) of the primary
    instance. RDS uses standard database engines. So, accessing the database using
    some sort of tool in a self-managed database server is the same as accessing Amazon
    RDS.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: As you have now understood the requirements of Amazon RDS, let’s understand
    the failover process in Amazon RDS. You will cover what services Amazon offers
    if something goes wrong with the RDS instance.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Managing failover in Amazon RDS
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RDS instances can be **Single-AZ** or **Multi-AZ**. In Multi-AZ, multiple instances
    work together, similar to an active-passive failover design.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: For a Single-AZ RDS instance, storage can be allocated for that instance to
    use. In a nutshell, a Single-AZ RDS instance has one attached block store (EBS
    storage) available in the same Availability Zone. This makes the databases and
    the storage of the RDS instance vulnerable to Availability Zone failure. The storage
    allocated to the block storage can be SSD (gp2 or io1) or magnetic. To secure
    the RDS instance, it is advised to use a security group and provide access based
    on requirements.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: Multi-AZ is always the best way to design the architecture to prevent failures
    and keep the applications highly available. With Multi-AZ features, a standby
    replica is kept in sync synchronously with the primary instance. The standby instance
    has its own storage in the assigned Availability Zone. A standby replica cannot
    be accessed directly, because all RDS access is via a single database DNS endpoint
    (CNAME). You can’t access the standby unless a failover happens. The standby provides
    no performance benefit, but it does constitute an improvement in terms of the
    availability of the RDS instance. It can only happen in the same Region, another
    AZ’s subnet in the same Region inside the VPC. When a Multi-AZ RDS instance is
    online, you can take a backup from the standby replica without affecting the performance.
    In a Single-AZ instance, availability and performance issues can be significant
    during backup operation.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: To understand the workings of Multi-AZ, let’s take an example of a Single-AZ
    instance and expand it to Multi-AZ.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have an RDS instance running in Availability Zone `AZ-A` of the
    `us-east-1` Region inside a VPC named `db-vpc`. This becomes a primary instance
    in a Single-AZ design of an RDS instance. In this case, there will be storage
    allocated to the instance in the `AZ-A` Availability Zone. Once you opt for Multi-AZ
    deployment in another Availability Zone called `AZ-B`, AWS creates a standby instance
    in Availability Zone `AZ-B` of the `us-east-1` Region inside the `db-vpc` VPC
    and allocates storage for the standby instance in `AZ-B` of the `us-east-1` Region.
    Along with that, RDS will enable **synchronous replication** from the primary
    instance to the standby replica. As you learned earlier, the only way to access
    our RDS instance is via the database CNAME, hence, the access request goes to
    the RDS primary instance. As soon as a write request comes to the endpoint, it
    writes to the primary instance. Then it writes the data to the hardware, which
    is the block storage attached to the primary instance. At the same time, the primary
    instance replicates the same data to the standby instance. Finally, the standby
    instance commits the data to its block storage.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: The primary instance writes the data into the hardware and replicates the data
    to the standby instance in parallel, so there is a minimal time lag (almost nothing)
    between the data commit operations in their respective hardware. If an error occurs
    with the primary instance, then RDS detects this and changes the database endpoint
    to the standby instance. The clients accessing the database may experience a very
    short interruption with this. This failover occurs within 60-120 seconds. It does
    not provide a fault-tolerant system because there will be some impact during the
    failover operation.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: You should now understand failover management on Amazon RDS. Let’s now learn
    about taking automatic RDS backups and using snapshots to restore in the event
    of a failure, and read replicas in the next section.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Taking automatic backups, RDS snapshots, and restore and read replicas
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will see how RDS **automatic backups** and **manual snapshots**
    work. These features come with Amazon RDS.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a database that is scheduled to take a backup at 5 A.M. every
    day. If the application fails at 11 A.M., then it is possible to restart the application
    from the backup taken at 11 A.M. with the loss of 6 hours’ worth of data. This
    is called a 6-hour **Recovery Point Objective (RPO)**. The RPO is defined as the
    time between the most recent backup and the incident, and this determines the
    amount of data loss. If you want to reduce this, then you have to schedule more
    incremental backups, which increases the cost and backup frequency. If your business
    demands a lower RPO value, then the business must spend more to provide the necessary
    technical solutions.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: Now, according to our example, an engineer was assigned the task of bringing
    the system back online as soon as the disaster occurred. The engineer managed
    to bring the database online at 2 P.M. on the same day by adding a few extra hardware
    components to the current system and installing some updated versions of the software.
    This is called a 3-hour **Recovery Time Objective (RTO).** The RTO is determined
    as the time between the disaster recovery and full recovery. RTO values can be
    reduced by having spare hardware and documenting the restoration process. If the
    business demands a lower RTO value, then your business must spend more money on
    spare hardware and an effective system setup to perform the restoration process.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: In RDS, the RPO and RTO play an important role in the selection of automatic
    backups and manual snapshots. Both of these backup services use AWS-managed S3
    buckets, which means they cannot be visible in the user’s AWS S3 console. They
    areRegion-resilient because the backup is replicated into multiple Availability
    Zones in the AWS Region. In the case of a Single-AZ RDS instance, the backup happens
    from the single available data store, and for a Multi-AZ enabled RDS instance,
    the backup happens from the standby data store (the primary store remains untouched
    as regards the backup).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: The snapshots are manual for RDS instances, and they are stored in the AWS-managed
    S3 bucket. The first snapshot of an RDS instance is a full copy of the data and
    the onward snapshots are incremental, reflecting the change in the data. In terms
    of the time taken for the snapshot process, it is high for the first one and,
    from then on, the incremental backup is quicker. When any snapshot occurs, it
    can impact the performance of the Single-AZ RDS instance, but not the performance
    of a Multi-AZ RDS instance as this happens on the standby data storage. Manual
    snapshots do not expire, have to be cleared automatically, and live past the termination
    of an RDS instance. When you delete an RDS instance, it suggests making one final
    snapshot on your behalf and it will contain all the databases inside your RDS
    instance (there is not just a single database in an RDS instance). When you restore
    from a manual snapshot, you restore to a single point in time, and that affects
    the RPO.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: To automate this entire process, you can choose a time window when these snapshots
    can be taken. This is called an automatic backup. These time windows can be managed
    carefully to essentially lower the RPO value of the business. Automatic backups
    have a retention period of 0 to 35 days, with 0 being disabled and the maximum
    is 35 days. To quote AWS documentation, retained automated backups contain system
    snapshots and transaction logs from a database instance. They also include database
    instance properties such as allocated storage and a database instance class, which
    are required to restore it to an active instance. Databases generate transaction
    logs, which contain the actual change in data in a particular database. These
    transaction logs are also written to S3 every 5 minutes by RDS. Transaction logs
    can also be replayed on top of the snapshots to restore to a point in time of
    5 minutes’ granularity. Theoretically, the RPO can be a 5-minute point in time.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: When you perform a restore, RDS creates a new RDS instance, which means a new
    database endpoint to access the instance. The applications using the instances
    have to point to the new address, which significantly affects the RTO. This means
    that the restoration process is not very fast, which affects the RTO. To minimize
    the RTO during a failure, you may consider replicating the data. With replicas,
    there is a high chance of replicating the corrupted data. The only way to overcome
    this is to have snapshots and restore an RDS instance to a particular point in
    time prior to the corruption. **Amazon RDS Read Replicas** are unlike the Multi-AZ
    replicas. In Multi-AZ RDS instances, the standby replicas cannot be used directly
    for anything unless a primary instance fails, whereas **Read Replicas** can be
    used directly, but only for read operations. Read replicas have their own database
    endpoints and read-heavy applications can directly point to this address. They
    are kept in sync **asynchronously** with the primary instance. Read Replicas can
    be created in the same Region as the primary instance or in a different Region.
    Read Replicas in other Regions are called **Cross-Region Read Replicas** and this
    improves the global performance of the application.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: As per AWS documentation, five direct Read Replicas are allowed per database
    instance and this helps to scale out the read performances. Read Replicas have
    a very low RPO value due to asynchronous replication. They can be promoted to
    a read-write database instance in the case of a primary instance failure. This
    can be done quickly and it offers a fairly low RTO value.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn about Amazon’s database engine, Amazon Aurora.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Writing to Amazon Aurora with multi-master capabilities
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Aurora is the most reliable relational database engine developed by Amazon
    to deliver speed in a simple and cost-effective manner. Aurora uses a cluster
    of single primary instances and zero or more replicas. Aurora’s replicas can give
    you the advantage of both read replicas and Multi-AZ instances in RDS. Aurora
    uses a shared cluster volume for storage and is available to all compute instances
    of the cluster (a maximum of 64 TiB). This allows the Aurora cluster to provision
    faster and improves availability and performance. Aurora uses SSD-based storage,
    which provides high IOPS and low latency. Aurora does not ask you to allocate
    storage, unlike other RDS instances; it is based on the storage that you use.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Aurora clusters have multiple endpoints, including the **cluster endpoint**
    **and reader endpoint.** If there are zero replicas, then the cluster endpoint
    is the same as the reader endpoint. If there are replicas available, then the
    reader endpoint is load-balanced across the reader endpoints. Cluster endpoints
    are used for reading/writing, while reader endpoints are intended for reading
    from the cluster. If you add more replicas, then AWS manages load balancing under
    the hood for the new replicas.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: When failover occurs, the replicas are promoted to read/write mode, and this
    takes some time. This can be prevented in a **Multi-Master** mode of an Aurora
    cluster. This allows multiple instances to perform reads and writes at the same
    time.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Storing columnar data on Amazon Redshift
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Redshift is not used for real-time transactions, but it is used for data
    warehouse purposes. It is designed to support huge volumes of data at a petabyte
    scale. It is a column-based database used for analytics, long-term processing,
    tending, and aggregation. **Redshift Spectrum** can be used to query data on S3
    without loading data to the Redshift cluster (a Redshift cluster is required,
    though). It’s not an OLTP, but an OLAP. **AWS QuickSight** can be integrated with
    Redshift for visualization, with a SQL-like interface that allows you to connect
    using JDBC/ODBC connections to query the data.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Redshift uses a clustered architecture in one AZ in a VPC with faster network
    connectivity between the nodes. It is not high availability by design as it is
    tightly coupled to the AZ. A Redshift cluster has a leader node, and this node
    is responsible for all the communication between the client and the computing
    nodes of the cluster, query planning, and aggregation. Compute nodes are responsible
    for running the queries submitted by the leader lode and for storing the data.
    By default, Redshift uses a public network for communicating with external services
    or any AWS services. With **enhanced VPC routing**, it can be controlled via customized
    networking settings.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: By combining Redshift with SageMaker, data scientists and analysts can leverage
    the scalability and computational power of Redshift to preprocess and transform
    data before training machine learning models. They can utilize Redshift’s advanced
    SQL capabilities to perform aggregations, joins, and filtering operations, enabling
    efficient feature engineering and data preparation. The processed data can then
    be seamlessly fed into SageMaker for model training, hyperparameter tuning, and
    evaluation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Amazon DynamoDB for NoSQL Database-as-a-Service
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon DynamoDB is a NoSQL database-as-a-service product within AWS. It’s a
    fully managed key/value and document database. Accessing DynamoDB is easy via
    its endpoint. The input and output throughputs can be managed or scaled manually
    or automatically. It also supports data backup, point-in-time recovery, and data
    encryption.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: One example where Amazon DynamoDB can be used with Amazon SageMaker in a cost-efficient
    way is for real-time prediction applications. DynamoDB can serve as a storage
    backend for storing and retrieving input data for prediction models built using
    SageMaker. Instead of continuously running and scaling an inference endpoint,
    which can be costlier, you can leverage DynamoDB’s low-latency access and scalability
    to retrieve the required input data on demand.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: In this setup, the input data for predictions can be stored in DynamoDB tables,
    where each item represents a unique data instance. When a prediction request is
    received, the application can use DynamoDB’s efficient querying capabilities to
    retrieve the required input data item(s) based on specific attributes or conditions.
    Once the data is retrieved, it can be passed to the SageMaker endpoint for real-time
    predictions.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: By using DynamoDB in this way, you can dynamically scale your application’s
    read capacity based on the incoming prediction requests, ensuring that you only
    pay for the read capacity you actually need. This approach offers a cost-efficient
    solution as it eliminates the need for running and managing a continuously running
    inference endpoint, which may incur high costs even during periods of low prediction
    demand. With DynamoDB and SageMaker working together, you can achieve scalable
    and cost-efficient real-time prediction applications while maintaining low latency
    and high availability.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'You will not cover the DynamoDB table structure or key structure in this chapter
    as this is not required for the certification exam. However, it is good to have
    a basic knowledge of them. For more details, please refer to the AWS docs available
    here: [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about various data storage services from Amazon,
    and how to secure data through various policies and use these services. If you
    are working on machine learning use cases, then you may encounter such scenarios
    where you have to choose an effective data storage service for your requirements.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about the migration and processing of stored
    data.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH02](https://packt.link/MLSC01E2_CH02).
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 2**.3*):'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.3 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_02_03.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – QR code that opens Chapter Review Questions for logged-in users
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 2**.4*:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Chapter Review Questions for Chapter 2](img/B21197_02_04.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Chapter Review Questions for Chapter 2
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – Sample timing practice drills on the online platform
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
