- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: AWS Services for Data Storage
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS数据存储服务
- en: AWS provides a wide range of services to store your data safely and securely.
    There are various storage options available on AWS, such as block storage, file
    storage, and object storage. It is expensive to manage on-premises data storage
    due to the higher investment in hardware, admin overheads, and managing system
    upgrades. With AWS storage services, you just pay for what you use, and you don’t
    have to manage the hardware. You will also learn about various storage classes
    offered by Amazon S3 for intelligent access to data and to reduce costs. You can
    expect questions in the exam on storage classes. As you continue through this
    chapter, you will master the **single-AZ** and **multi-AZ** instances, and **Recovery
    Time Objective** (**RTO**) and **Recovery Point Objective** (**RPO**) concepts
    of Amazon RDS.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供了一系列服务来安全地存储你的数据。在AWS上提供了各种存储选项，例如块存储、文件存储和对象存储。由于硬件投资较高、管理开销和系统升级管理，本地数据存储管理成本较高。使用AWS存储服务，你只需为所使用的付费，无需管理硬件。你还将了解Amazon
    S3提供的各种存储类别，以实现数据的智能访问和降低成本。你可以期待在考试中关于存储类别的相关问题。随着你继续阅读本章，你将掌握Amazon RDS的**单可用区**和**多可用区**实例，以及**恢复时间目标**（**RTO**）和**恢复点目标**（**RPO**）的概念。
- en: 'In this chapter, you will learn about storing your data securely for further
    analytical purposes throughout the following sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何在以下几节中安全地存储你的数据，以便进行进一步的分析：
- en: Storing data on Amazon S3
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon S3上存储数据
- en: Controlling access on S3 buckets and objects
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制S3存储桶和对象的访问
- en: Protecting data on Amazon S3
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保护Amazon S3上的数据
- en: Securing S3 objects at rest and in transit
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在静止和传输过程中保护S3对象
- en: Using other types of data stores
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其他类型的数据存储
- en: '**Relational Database** **Services** (**RDSes**)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关系数据库** **服务**（**RDS**）'
- en: Managing failover in Amazon RDS
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理Amazon RDS中的故障转移
- en: Taking automatic backup, RDS snapshots, and restore and read replicas
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动备份、RDS快照、恢复和读取副本
- en: Writing to Amazon Aurora with multi-master capabilities
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有多主能力的Amazon Aurora进行写入
- en: Storing columnar data on Amazon Redshift
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Amazon Redshift上存储列式数据
- en: Amazon DynamoDB for NoSQL databases as a service
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为服务的NoSQL数据库Amazon DynamoDB
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'All you will need for this chapter is an AWS account and the AWS CLI configured.
    The steps to configure the AWS CLI for your account are explained in detail by
    Amazon here: [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章所需的所有内容只是一个AWS账户和配置好的AWS CLI。配置AWS CLI的步骤在Amazon这里有详细的解释：[https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)。
- en: 'You can download the code examples from GitHub, here: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从GitHub下载代码示例，这里：[https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02)。
- en: Storing Data on Amazon S3
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Amazon S3上存储数据
- en: S3 is Amazon’s cloud-based object storage service, and it can be accessed from
    anywhere via the internet. It is an ideal storage option for large datasets. It
    is region-based, as your data is stored in a particular region until you move
    the data to a different region. Your data will never leave that region until it
    is configured to do so. In a particular region, data is replicated in the availability
    zones of that region; this makes S3 regionally resilient. If any of the availability
    zones fail in a region, then other availability zones will serve your requests.
    S3 can be accessed via the AWS console UI, AWS CLI, AWS API requests, or via standard
    HTTP methods.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: S3是Amazon基于云的对象存储服务，可以通过互联网从任何地方访问。它是大型数据集的理想存储选项。它是基于区域的，因为你的数据存储在特定的区域，直到你将数据移动到不同的区域。你的数据将永远不会离开该区域，直到它被配置为这样做。在特定区域，数据在该区域的可用区中进行复制；这使得S3在区域上具有弹性。如果该区域中的任何可用区失败，则其他可用区将处理你的请求。S3可以通过AWS控制台UI、AWS
    CLI、AWS API请求或通过标准HTTP方法访问。
- en: 'S3 has two main components: **buckets** and **objects**.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: S3有两个主要组件：**存储桶**和**对象**。
- en: Buckets are created in a specific AWS region. Buckets can contain objects but
    cannot contain other buckets.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储桶是在特定的AWS区域中创建的。存储桶可以包含对象，但不能包含其他存储桶。
- en: Objects have two main attributes. One is the **key**, and the other is the **value**.
    The value is the content being stored, and the key is the name. The maximum size
    of an object can be 5 TB. As per the Amazon S3 documentation ([https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html)),
    objects also have a version ID, metadata, access control information, and sub-resources.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象有两个主要属性。一个是**键**，另一个是**值**。值是存储的内容，键是名称。对象的最大大小为5 TB。根据亚马逊S3文档（[https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html)），对象还有版本ID、元数据、访问控制信息和子资源。
- en: Important note
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: As per Amazon’s docs, S3 provides read-after-write consistency for PUTs of new
    objects, which means that if you upload a new object or create a new object and
    you immediately try to read the object using its key, then you get the exact data
    that you just uploaded. However, for overwrites and deletes, it behaves in an
    **eventually consistent manner**. This means that if you read an object straight
    after the delete or overwrite operation, then you may read an old copy or a stale
    version of the object. It takes some time to replicate the content of the object
    across three Availability Zones.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根据亚马逊的文档，S3为新的PUT操作提供了写后一致性读取，这意味着如果你上传了一个新的对象或者创建了一个新的对象，然后立即尝试使用其键读取该对象，那么你会得到你刚刚上传的确切数据。然而，对于覆盖和删除操作，它以**最终一致性**的方式表现。这意味着如果你在删除或覆盖操作后立即读取一个对象，那么你可能会读取到旧副本或过时的对象版本。将对象的内容复制到三个可用区需要一些时间。
- en: 'A folder structure can be maintained logically by using a prefix. Take an example
    where an image is uploaded into a bucket, `bucket-name-example`, with the prefix
    `folder-name` and the object name `my-image.jpg`. The entire structure looks like
    this: `/bucket-name-example/folder-name/my-image.jpg`.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用前缀来逻辑上维护文件夹结构。以一个例子来说明，一个图片被上传到一个名为`bucket-name-example`的桶中，前缀为`folder-name`，对象名为`my-image.jpg`。整个结构看起来是这样的：`/bucket-name-example/folder-name/my-image.jpg`。
- en: The content of the object can be read by using the bucket name of `bucket-name-example`
    and the key of `/folder-name/my-image.jpg`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`bucket-name-example`的桶名和`/folder-name/my-image.jpg`的键来读取对象的内容。
- en: 'There are several storage classes offered by Amazon for objects stored in S3:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊为存储在S3中的对象提供了几种存储类别：
- en: '**Standard Storage (S3 Standard):** This is the storage class for frequently
    accessed objects and for quick access. S3 Standard has a millisecond first-byte
    latency and objects can be made publicly available.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准存储（S3 Standard）**：这是频繁访问的对象和快速访问的存储类别。S3标准具有毫秒级的首字节延迟，并且对象可以被公开访问。'
- en: '**Standard Infrequent Access (S3 Standard-IA):** This option is used when you
    need data to be returned quickly, but not for frequent access. The object size
    has to be a minimum of 128 KB. The minimum storage timeframe is 30 days. If the
    object is deleted before 30 days, you are still charged for 30 days. Standard-IA
    objects are resilient to the loss of Availability Zones.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准低频访问（S3 Standard-IA）**：当需要快速返回数据但不是频繁访问时，使用此选项。对象大小必须至少为128 KB。最小存储时间为30天。如果对象在30天内被删除，你仍然会被收取30天的费用。Standard-IA对象对可用区的丢失具有弹性。'
- en: '**One Zone Infrequent Access (S3 One Zone-IA):** Objects in this storage class
    are stored in just one Availability Zone, which makes it cheaper than **Standard-IA**.
    The minimum object size and storage timeframe are the same as Standard-IA. Objects
    from this storage class are less available and less resilient. This storage class
    is used when you have another copy, or if the data can be recreated. A **One Zone-IA**
    storage class should be used for long-lived data that is non-critical and replaceable,
    and where access is infrequent.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单区低频访问（S3 One Zone-IA）**：这个存储类别的对象只存储在一个可用区，这使得它比**Standard-IA**更便宜。最小对象大小和存储时间与Standard-IA相同。这个存储类别的对象可用性和弹性较低。当你有另一个副本，或者数据可以被重新创建时，使用这个存储类别。应该为长期存储的非关键且可替换的数据使用**One
    Zone-IA**存储类别，并且访问频率较低。'
- en: '**Amazon S3 Glacier Flexible Retrieval (formerly S3 Glacier):** This option
    is used for long-term archiving and backup. It can take anything from minutes
    to hours to retrieve objects in this storage class. The minimum storage timeframe
    is 90 days. For archived data that doesn’t need to be accessed right away but
    requires the ability to retrieve extensive data sets without incurring additional
    charges, like in backup or disaster-recovery scenarios, S3 Glacier Flexible Retrieval
    (formerly known as S3 Glacier) is the perfect storage option.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon S3 Glacier Flexible Retrieval（之前称为S3 Glacier）**：此选项用于长期存档和备份。在此存储类别中检索对象可能需要从几分钟到几小时不等。最小存储时间为90天。对于不需要立即访问但需要能够检索大量数据集而不产生额外费用的存档数据，例如在备份或灾难恢复场景中，S3
    Glacier Flexible Retrieval（之前称为S3 Glacier）是完美的存储选项。'
- en: '**Amazon S3 Glacier Instant Retrieval:** This storage class offers cost-effective,
    high-speed storage for seldom-accessed, long-term data. Compared to S3 Standard-Infrequent
    Access, it can cut storage expenses by up to 68% when data is accessed once per
    quarter. This storage class is perfect for swiftly retrieving archive data like
    medical images, news media assets, or user-generated content archives. You can
    upload data directly or use S3 Lifecycle policies to move it from other S3 storage
    classes.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon S3 Glacier Instant Retrieval**：此存储类别为不常访问的长期数据提供经济高效、高速的存储。与S3 Standard-Infrequent
    Access相比，如果数据每季度访问一次，可以降低高达68%的存储费用。此存储类别非常适合快速检索存档数据，如医学图像、新闻媒体资产或用户生成内容存档。您可以直接上传数据或使用S3生命周期策略将其从其他S3存储类别移动过来。'
- en: '**Glacier Deep Archive:** The minimum storage duration of this class is 180
    days. This is the least expensive storage class and has a default retrieval time
    of 12 hours.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Glacier Deep Archive**：此类别的最小存储时间为180天。这是最经济的存储类别，默认检索时间为12小时。'
- en: '**S3 Intelligent-Tiering:** This storage class is designed to reduce operational
    overheads. Users pay a monitoring fee and AWS selects a storage class between
    Standard (a frequent-access tier) and Standard-IA (a lower-cost, infrequent-access
    tier) based on the access pattern of an object. This option is designed for long-lived
    data with unknown or unpredictable access patterns.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**S3 Intelligent-Tiering**：此存储类别旨在降低运营成本。用户支付监控费用，AWS根据对象的访问模式在Standard（频繁访问层）和Standard-IA（成本较低、不频繁访问层）之间选择存储类别。此选项适用于具有未知或不可预测访问模式的长期数据。'
- en: Through sets of rules, the transition between storage classes and deletion of
    the objects can be managed easily and are referred to as **S3 Lifecycle configurations**.
    These rules consist of actions. These can be applied to a bucket or a group of
    objects in that bucket defined by prefixes or tags. Actions can either be **transition
    actions** or **expiration actions**. Transition actions define the storage class
    transition of the objects following the creation of *a user-defined* number of
    days. Expiration actions configure the deletion of versioned objects, or the deletion
    of delete markers or incomplete multipart uploads. This is very useful for managing
    costs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一系列规则，可以轻松管理存储类别之间的转换和对象的删除，这些规则被称为**S3生命周期配置**。这些规则包括操作。这些操作可以应用于一个存储桶或该存储桶中由前缀或标签定义的一组对象。操作可以是**转换操作**或**过期操作**。转换操作定义了在创建*用户定义的*天数后对象的存储类别转换。过期操作配置了版本化对象的删除，或删除标记或未完成的分片上传。这对于管理成本非常有用。
- en: 'An illustration is given in *Figure 2**.1*. You can find more details here:
    [https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图2.1*中给出了说明。更多详细信息请见：[https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html)。
- en: '![Figure 2.1 – A comparison table of S3 Storage classes](img/B21197_02_01.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – S3存储类别的比较表](img/B21197_02_01.jpg)'
- en: Figure 2.1 – A comparison table of S3 Storage classes
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – S3存储类别的比较表
- en: Creating buckets to hold data
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建存储桶以存储数据
- en: 'Now, you will see how to create a bucket, upload an object, and read the object
    using the AWS CLI:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您将了解如何使用AWS CLI创建存储桶、上传对象以及读取对象：
- en: 'In the first step, check whether you have any buckets created by using the
    `aws s3` `ls` command:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一步，检查你是否已经使用 `aws s3 ls` 命令创建了任何存储桶：
- en: '[PRE0]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This command returns nothing here. So, create a bucket now by using the `mb`
    argument. Let’s say the bucket name is `demo-bucket-baba` in the `us-east-1` Region:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此命令在此处不返回任何内容。因此，现在请使用 `mb` 参数创建一个存储桶。假设存储桶名称为 `demo-bucket-baba`，位于 `us-east-1`
    区域：
- en: '[PRE4]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you have created a bucket now, your next step is to copy a file to your
    bucket using the `cp` argument, as shown in the following code:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于您已经创建了一个存储桶，您的下一步是使用 `cp` 参数将文件复制到您的存储桶中，如下面的代码所示：
- en: '[PRE8]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To validate the file upload operation via the AWS console, please log in to
    your AWS account and go to the AWS S3 console to see the same. The AWS S3 console
    lists the result as shown in *Figure 2**.2*. The console may have changed by the
    time you are reading this book!
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要通过 AWS 控制台验证文件上传操作，请登录您的 AWS 账户并转到 AWS S3 控制台查看相同的内容。AWS S3 控制台将结果列示为 *图 2.2*。请注意，当您阅读这本书时，控制台可能已经发生了变化！
- en: '![Figure 2.2 – AWS S3 listing your files](img/B21197_02_02.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – AWS S3 列出您的文件](img/B21197_02_02.jpg)'
- en: Figure 2.2 – AWS S3 listing your files
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – AWS S3 列出您的文件
- en: 'You can also list the files in your S3 bucket from the command line, as shown
    here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以从命令行列出您的 S3 存储桶中的文件，如下所示：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you want to upload your filesystem directories and files to the S3 bucket,
    then `--recursive` will do the job for you:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想将您的文件系统目录和文件上传到 S3 存储桶，那么 `--recursive` 参数将为您完成工作：
- en: '[PRE13]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The contents of one bucket can be copied/moved to another bucket via the `cp`
    command and the `--recursive` parameter. To achieve this, you will have to create
    two buckets, `demo-bucket-baba-copied` and `demo-bucket-baba-moved`. The steps
    are as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个存储桶的内容可以通过 `cp` 命令和 `--recursive` 参数复制/移动到另一个存储桶。要实现这一点，您必须创建两个存储桶，`demo-bucket-baba-copied`
    和 `demo-bucket-baba-moved`。步骤如下：
- en: '[PRE18]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: If all the commands are run successfully, then the original bucket should be
    empty at the end (as all the files have now been moved).
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果所有命令都成功运行，那么原始存储桶最终应该是空的（因为所有文件现在都已移动）。
- en: Note
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the certification exam, you will not find many questions on bucket- and object-level
    operations. However, it is always better to know the basic operations and the
    required steps.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在认证考试中，您不会在存储桶和对象级别操作中找到很多问题。然而，了解基本操作和所需步骤总是更好的。
- en: 'The buckets must be deleted to avoid costs as soon as the hands-on work is
    finished. The bucket has to be empty before you run the `rb` command:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦实际操作完成，必须删除存储桶以避免费用。在运行 `rb` 命令之前，存储桶必须为空：
- en: '[PRE27]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `demo-bucket-baba-moved` bucket is not empty, so you couldn’t remove the
    bucket. In such scenarios, use the `--force` parameter to delete the entire bucket
    and all its contents, as shown here:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`demo-bucket-baba-moved` 存储桶不为空，因此您无法删除该存储桶。在这种情况下，请使用 `--force` 参数删除整个存储桶及其所有内容，如下所示：'
- en: '[PRE30]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Let’s take an example of a bucket, `test-bucket`, that has a prefix, `images`.
    This prefix contains four image files named `animal.jpg`, `draw-house.jpg`, `cat.jpg`,
    and `human.jpg`.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们以一个具有前缀 `images` 的存储桶 `test-bucket` 为例。此前缀包含四个名为 `animal.jpg`、`draw-house.jpg`、`cat.jpg`
    和 `human.jpg` 的图片文件。
- en: 'Now, to delete the contents inside the images, the command will be as follows:
    `aws s3 rm` `s3://test-bucket/images –recursive`'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，要删除图片内的内容，命令如下：`aws s3 rm` `s3://test-bucket/images –recursive`
- en: The bucket should now be empty.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在存储桶应该是空的。
- en: In the next section, you are going to learn about object tags and object metadata.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习关于对象标签和对象元数据的内容。
- en: Distinguishing between object tags and object metadata
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分对象标签和对象元数据
- en: 'Let’s compare these two terms:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较这两个术语：
- en: '**Object tag**: An object tag is a **key-value** pair. AWS S3 object tags can
    help you filter analytics and metrics, categorize storage, secure objects based
    on certain categorizations, track costs based on certain categorization of objects,
    and much more besides. Object tags can be used to create life cycle rules to move
    objects to cheaper storage tiers. You can have a maximum of 10 tags added to an
    object and 50 tags to a bucket. A tag key can contain 128 Unicode characters,
    while a tag value can contain 256 Unicode characters.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象标签**：对象标签是一个 **键值对**。AWS S3 对象标签可以帮助您过滤分析指标，对存储进行分类，根据某些分类对对象进行安全保护，根据对象的某些分类跟踪成本，以及更多。对象标签可以用来创建生命周期规则，将对象移动到更便宜的存储层。您可以为对象添加最多
    10 个标签，为存储桶添加最多 50 个标签。标签键可以包含 128 个 Unicode 字符，而标签值可以包含 256 个 Unicode 字符。'
- en: '**Object metadata**: Object metadata is descriptive data describing an object.
    It consists of **name-value** pairs. Object metadata is returned as HTTP headers
    on objects. They are of two types: one is **system metadata**, and the other is
    **user-defined metadata**. User-defined metadata is a custom name-value pair added
    to an object by the user. The name must begin with **x-amz-meta**. You can change
    all system metadata such as storage class, versioning, and encryption attributes
    on an object. Further details are available here: [https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html).'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对象元数据**：对象元数据是描述对象的描述性数据。它由**名称-值**对组成。对象元数据作为对象的HTTP头返回。它们有两种类型：一种是**系统元数据**，另一种是**用户定义元数据**。用户定义元数据是用户添加到对象中的自定义名称-值对。名称必须以**x-amz-meta**开头。您可以更改对象上的所有系统元数据，如存储类、版本控制和加密属性。更多详细信息请参阅此处：[https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html)。'
- en: Important note
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Metadata names are case-insensitive, whereas tag names are case-sensitive.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据名称不区分大小写，而标签名称区分大小写。
- en: In the next section, you are going to learn about controlling access to buckets
    and objects on Amazon S3 through different policies, including the resource policy
    and the identity policy.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将学习如何通过不同的策略来控制Amazon S3中存储桶和对象的访问，包括资源策略和身份策略。
- en: Controlling access to buckets and objects on Amazon S3
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制对Amazon S3中存储桶和对象的访问
- en: Once the object is stored in the bucket, the next major step is to manage access.
    S3 is private by default, and access is given to other users, groups, or resources
    via several methods. This means that access to the objects can be managed via
    **Access Control Lists (ACLs)**, **Public Access Settings**, **Identity Policies**,
    and **Bucket Policies**.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦对象存储在存储桶中，下一步重要的步骤就是管理访问。S3默认是私有的，并且通过多种方法向其他用户、组或资源提供访问权限。这意味着可以通过**访问控制列表（ACLs）**、**公共访问设置**、**身份策略**和**存储桶策略**来管理对象的访问权限。
- en: Let’s look at some of these in detail.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看其中的一些。
- en: S3 bucket policy
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: S3存储桶策略
- en: 'An `Principal` is rendered `*`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`Principal`被渲染为`*`：'
- en: '[PRE31]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: By default, everything in S3 is private to the owner. If you want to make a
    prefix public to the world, then `Resource` changes to `arn:aws:s3:::my-bucket/some-prefix/*`,
    and similarly, if it is intended for a specific IAM user or IAM group, then those
    details go in the principal part in the policy.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，S3中的所有内容对所有者都是私有的。如果您想将前缀公开给全世界，那么`Resource`将变为`arn:aws:s3:::my-bucket/some-prefix/*`，同样地，如果它是为特定的IAM用户或IAM组设计的，那么这些详细信息将包含在策略的主体部分。
- en: 'There can be conditions added to the bucket policy too. Let’s examine a use
    case where the organization wants to keep a bucket public and whitelist particular
    IP addresses. The policy would look something like this:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在存储桶策略中添加条件。让我们考察一个组织希望保持存储桶公开并白名单特定IP地址的用例。该策略可能看起来像这样：
- en: '[PRE43]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'More examples are available in the AWS S3 developer guide, which can be found
    here: [https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 更多示例可以在AWS S3开发者指南中找到，该指南可在此处找到：[https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html)。
- en: '**Block public access** is a separate setting given to the bucket owner to
    avoid any kind of mistakes in bucket policy. In a real-world scenario, the bucket
    can be made public through bucket policy by mistake; to avoid such mistakes, or
    data leaks, AWS has provided this setting. It provides a further level of security,
    irrespective of the bucket policy. You can choose this while creating a bucket,
    or it can be set after creating a bucket.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**阻止公共访问**是分配给存储桶所有者的一个独立设置，以避免在存储桶策略中犯任何错误。在现实场景中，存储桶可能会由于策略错误而公开；为了避免此类错误或数据泄露，AWS提供了这个设置。它提供了比存储桶策略更高的安全级别。您可以在创建存储桶时选择此设置，也可以在创建存储桶后设置。'
- en: '`us-east-1` in this example):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`us-east-1`在此示例中）：'
- en: '[PRE58]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '**ACLs** are used to grant high-level permissions, typically for granting access
    to other AWS accounts. ACLs are one of the **sub-resources** of a bucket or an
    object. A bucket or object can be made public quickly via ACLs. AWS doesn’t suggest
    doing this, and you shouldn’t expect questions about this on the test. It is good
    to know about this, but it is not as flexible as the **S3** **bucket policy**.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**ACLs**用于授予高级权限，通常用于授予对其他AWS账户的访问权限。ACLs是存储桶或对象的**子资源**之一。可以通过ACLs快速使存储桶或对象公开。AWS不建议这样做，您也不应该期望在测试中遇到关于此的问题。了解这一点是好的，但它不如**S3**
    **存储桶策略**灵活。'
- en: Now, let’s learn about the methods to protect our data in the next section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在下一节中了解保护我们数据的方法。
- en: Protecting data on Amazon S3
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护Amazon S3上的数据
- en: In this section, you will learn how to record every version of an object. Along
    with durability, Amazon provides several techniques to secure the data in S3\.
    Some of those techniques involve enabling versioning and encrypting the objects.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何记录对象的每个版本。除了耐用性之外，Amazon还提供了几种技术来保护S3中的数据。其中一些技术涉及启用版本控制和加密对象。
- en: Versioning helps you to roll back to a previous version if any problem occurs
    with the current object during update, delete, or put operations.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制可以帮助您在更新、删除或put操作期间，如果当前对象出现问题，回滚到之前的版本。
- en: Through encryption, you can control the access of an object. You need the appropriate
    key to read and write an object. You will also learn **Multi-Factor Authentication
    (MFA**) for delete operations. Amazon also allows **Cross-Region Replication (CRR)**
    to maintain a copy of an object in another Region, which can be used for data
    backup during any disaster, for further redundancy, or for the enhancement of
    data access speed in different Regions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通过加密，您可以控制对象的访问。您需要适当的密钥来读取和写入对象。您还将学习用于删除操作的**多因素认证（MFA**）。Amazon还允许**跨区域复制（CRR**），在另一个区域中维护对象的副本，这可以在任何灾难期间用于数据备份，以提供额外的冗余，或者用于提高不同区域的数据访问速度。
- en: Applying bucket versioning
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用存储桶版本控制
- en: 'Let’s now understand how you can enable bucket versioning with the help of
    some hands-on examples. Bucket versioning can be applied while creating a bucket
    from the AWS S3 console:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在通过一些实际示例了解如何使用帮助启用存储桶版本控制。在创建存储桶时，可以从AWS S3控制台应用存储桶版本控制：
- en: 'To enable versioning on a bucket from the command line, a bucket must be created
    first and then versioning can be enabled, as shown in the following example. In
    this example, I have created a bucket, `version-demo-mlpractice`, and enabled
    versioning through the `put-bucket-versioning` command:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从命令行启用存储桶的版本控制，首先必须创建一个存储桶，然后才能启用版本控制，如下面的示例所示。在这个示例中，我创建了一个名为`version-demo-mlpractice`的存储桶，并通过`put-bucket-versioning`命令启用了版本控制：
- en: '[PRE73]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'You have not created this bucket with any kind of encryption. So, if you run
    **aws s3api get-bucket-encryption --bucket version-demo-mlpractice**, then it
    will output an error that says the following:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您没有为此存储桶创建任何类型的加密。因此，如果您运行**aws s3api get-bucket-encryption --bucket version-demo-mlpractice**，那么它将输出一个错误，说明如下：
- en: '[PRE79]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '`put-bucket-encryption` API. The command will look like this:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`put-bucket-encryption` API。命令看起来像这样：'
- en: '[PRE80]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'This can be verified using the following command: `aws s3api get-bucket-encryption
    --``bucket version-demo-mlpractice`.'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这可以通过以下命令进行验证：`aws s3api get-bucket-encryption --bucket version-demo-mlpractice`。
- en: You will learn more about encryption in the next section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在下一节中了解更多关于加密的内容。
- en: Applying encryption to buckets
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用存储桶加密
- en: 'You also need to understand how enabling versioning on a bucket would help.
    There are use cases where a file is updated regularly, and versions will be created
    for the same file. To simulate this scenario, try the following example:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要了解如何启用存储桶的版本控制会有什么帮助。有一些用例中，文件会定期更新，并为同一文件创建版本。为了模拟这种场景，请尝试以下示例：
- en: 'In this example, you will create a file with versions written in it. You will
    overwrite it and retrieve it to check the versions in that file:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本例中，您将创建一个包含版本信息的文件。您将覆盖它并检索它以检查该文件中的版本：
- en: '[PRE82]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '[PRE89]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '[PRE90]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '[PRE93]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Upon retrieval, you got the latest version of the file, in other words, `Version-2`
    in this case. To check each of the versions and the latest one of them, S3 provides
    the `list-object-versions` API, as shown here. From the JSON results, you can
    deduce the latest version:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在检索时，您将获得文件的最新版本，换句话说，在本例中是`Version-2`。要检查每个版本及其最新版本，S3提供了`list-object-versions`
    API，如下所示。从JSON结果中，您可以推断出最新版本：
- en: '[PRE94]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: '[PRE95]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE102]'
- en: '[PRE103]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '[PRE104]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE104]'
- en: '[PRE105]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE105]'
- en: '[PRE106]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE107]'
- en: '[PRE108]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE109]'
- en: '[PRE110]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE111]'
- en: '[PRE112]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '[PRE114]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '[PRE118]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE119]'
- en: '[PRE120]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '[PRE121]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE122]'
- en: '[PRE123]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'There may be a situation where you have to roll back to the earlier version
    of the current object. In the preceding example, the latest one is `Version-2.`
    You can make any desired version the latest or current version by parsing the
    `VersionId` sub-resource to the `get-object` API call and uploading that object
    again. The other way is to delete the current or latest version by passing `versionId`
    to the `–version-id` parameter in the `delete-object` API request. More details
    about the API are available here: [https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html](https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html).'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能会有这样的情况，您需要回滚到当前对象的早期版本。在上面的示例中，最新的一个是`Version-2.`您可以通过将`VersionId`子资源解析到`get-object`
    API调用中并重新上传该对象，将任何所需的版本设置为最新或当前版本。另一种方法是，通过在`delete-object` API请求中将`versionId`传递给`–version-id`参数来删除当前或最新版本。有关API的更多详细信息，请参阅此处：[https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html](https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html)。
- en: 'When you delete an object in a versioning-enabled bucket, it does not delete
    the object from the bucket. It just creates a marker called `DeleteMarker`. It
    looks like this:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您在启用了版本控制的存储桶中删除对象时，并不会从存储桶中删除该对象。它只是创建了一个名为`DeleteMarker`的标记。它看起来像这样：
- en: '[PRE126]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'This means that the object is not deleted. You can list it by using this command:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这意味着对象没有被删除。您可以使用此命令列出它：
- en: '[PRE130]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Now the bucket has no objects as `version-doc.txt`, and you can verify this
    using the `aws s3 ls` command because that marker became the current version of
    the object with a new ID. If you try to retrieve an object that is deleted, which
    means a delete marker is serving the current version of the object, then you will
    get a `VersionId`, as shown in the following example commands. A simple delete
    request `{`
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在存储桶中没有名为`version-doc.txt`的对象，您可以使用`aws s3 ls`命令来验证这一点，因为该标记已成为具有新ID的对象的当前版本。如果您尝试检索已删除的对象，这意味着删除标记正在提供对象的当前版本，那么您将获得一个`VersionId`，如下面的示例命令所示。一个简单的删除请求`{`
- en: '[PRE131]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE133]'
- en: 'Upon listing the bucket now, the older objects can be seen:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在列出存储桶时，可以看到较旧的对象：
- en: '[PRE134]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE135]'
- en: As you have already covered the exam topics and practiced most of the required
    concepts, you should delete the objects in the bucket and then delete the bucket
    to save on costs. This step deletes the versions of the object and, in turn, removes
    the object permanently.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于您已经涵盖了考试主题并练习了大多数所需的概念，您应该删除存储桶中的对象，然后删除存储桶以节省成本。这一步删除了对象的版本，从而永久删除了对象。
- en: 'Here, the latest version is deleted by giving the version ID to it, followed
    by the other version ID:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，通过给出版本ID来删除最新版本，然后是另一个版本ID：
- en: '[PRE136]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE137]'
- en: '[PRE138]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE138]'
- en: You can clearly see the empty bucket now.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您现在可以清楚地看到空存储桶。
- en: Important note
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'AWS best practices suggest adding another layer of protection through **MFA
    delete.** Accidental bucket deletions can be prevented, and the security of the
    objects in the bucket is ensured. MFA delete can be enabled or disabled via the
    console and CLI. As documented in AWS docs, MFA delete requires two forms of authentication
    together: your security credentials, and the concatenation of a valid serial number,
    a space, and the six-digit code displayed on an approved authentication device.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: AWS最佳实践建议通过**多因素认证删除**添加另一层保护。可以防止意外删除存储桶，并确保存储桶中对象的安全性。可以通过控制台和CLI启用或禁用多因素认证删除。如AWS文档所述，多因素认证删除需要两种认证方式同时进行：您的安全凭证，以及一个有效序列号、一个空格和显示在批准的认证设备上的六位数字代码的组合。
- en: 'CRR helps you to separate data between different geographical Regions. A typical
    use case is the maintenance business-as-usual activities during a disaster. If
    a Region goes down, then another Region can support the users if CRR is enabled.
    This improves the availability of the data. Another use case is to reduce latency
    if the same data is used by another compute resource, such as EC2 or AWS Lambda
    being launched in another Region. You can also use CRR to copy objects to another
    AWS account that belongs to a different owner. There are a few important points
    that are worth noting down for the certification exam:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: CRR可以帮助您在不同地理区域之间分离数据。一个典型的用例是在灾难期间维护常规业务活动。如果一个区域关闭，那么如果启用了CRR，另一个区域可以支持用户。这提高了数据可用性。另一个用例是如果相同的数据被另一个计算资源使用，例如在另一个区域启动的EC2或AWS
    Lambda，可以减少延迟。您还可以使用CRR将对象复制到属于不同所有者的另一个AWS账户。对于认证考试，以下是一些重要要点值得记录：
- en: In order to use CRR, versioning has to be enabled on both the source and destination
    bucket.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使用 CRR，必须在源和目标存储桶上启用版本控制。
- en: Replication is enabled on the source bucket by adding rules. As the source,
    either an entire bucket, a prefix, or tags can be replicated.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过添加规则，可以在源存储桶上启用复制。作为源，可以是整个存储桶、前缀或标签进行复制。
- en: Encrypted objects can also be replicated by assigning an appropriate encryption
    key.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过分配适当的加密密钥，也可以通过复制加密对象。
- en: The destination bucket can be in the same account or in another account. You
    can change the storage type and ownership of the object in the destination bucket.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标存储桶可以位于同一账户或另一个账户中。你可以更改目标存储桶中对象的存储类型和所有权。
- en: For CRR, an existing role can be chosen or a new IAM role can be created too.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 CRR，可以选择现有的角色，也可以创建新的 IAM 角色。
- en: There can be multiple replication rules on the source bucket, with priority
    accorded to it. Rules with higher priority override rules with lower priority.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源存储桶上可以有多个复制规则，并赋予其优先级。优先级较高的规则会覆盖优先级较低的规则。
- en: When you add a replication rule, only new versions of an object that are created
    after the rules are enabled get replicated.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你添加复制规则时，只有规则启用后创建的新版本对象才会被复制。
- en: If versions are deleted from the source bucket, then they are not deleted from
    the destination bucket.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果从源存储桶中删除了版本，则它们不会被从目标存储桶中删除。
- en: When you delete an object from the source bucket, it creates a delete marker
    in said source bucket. That delete marker is not replicated to the destination
    bucket by S3.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你从源存储桶中删除对象时，会在该源存储桶中创建一个删除标记。S3 不会将此删除标记复制到目标存储桶。
- en: In the next section, you will cover the concept of securing S3 objects.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解保护 S3 对象的概念。
- en: Securing S3 objects at rest and in transit
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护静止和传输中的 S3 对象
- en: 'In the previous section, you learned about bucket default encryption, which
    is completely different from object-level encryption. Buckets are not encrypted,
    whereas objects are. A question may arise here: *what is the default bucket encryption?*
    You will learn these concepts in this section. Data during transmission can be
    protected by using **Secure Socket Layer (SSL)** or **Transport Layer Security
    (TLS)** for the transfer of HTTPS requests. The next step is to protect the data,
    where the authorized person can encode and decode the data.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学习了存储桶默认加密的概念，这与对象级加密完全不同。存储桶没有被加密，而对象被加密。这里可能会出现一个问题：*默认存储桶加密是什么？* 你将在本节中学习这些概念。在传输数据时，可以使用
    **安全套接字层 (SSL**) 或 **传输层安全性 (TLS**) 来保护 HTTPS 请求的传输。下一步是保护数据，授权人员可以编码和解码数据。
- en: 'It is possible to have different encryption settings on different objects in
    the same bucket. S3 supports **Client-Side Encryption (CSE)** and **Server-Side
    Encryption (SSE)** for objects at rest:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一个存储桶中，可以为不同的对象设置不同的加密设置。S3 支持对静止对象使用 **客户端加密 (CSE**) 和 **服务器端加密 (SSE)**：
- en: '**CSE**: A client uploads the object to S3 via the S3 endpoint. In CSE, the
    data is encrypted by the client before uploading to S3\. Although the transit
    between the user and the S3 endpoint happens in an encrypted channel, the data
    in the channel is already encrypted by the client and can’t be seen. In transit,
    encryption takes place by default through HTTPS. So, AWS S3 stores the encrypted
    object and cannot read the data in any format at any point in time. In CSE, the
    client takes care of encrypting the object’s content. So, control stays with the
    client in terms of key management and the encryption-decryption process. This
    leads to a huge amount of CPU usage. S3 is only used for storage.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CSE**：客户端通过 S3 端点将对象上传到 S3。在 CSE 中，数据在上传到 S3 之前由客户端加密。尽管用户和 S3 端点之间的传输发生在加密通道中，但通道中的数据已经被客户端加密，无法被看到。在传输过程中，默认通过
    HTTPS 进行加密。因此，AWS S3 存储加密对象，在任何时候都无法以任何格式读取数据。在 CSE 中，客户端负责加密对象的内容。因此，密钥管理和加密解密过程始终由客户端控制。这导致大量的
    CPU 使用。S3 仅用于存储。'
- en: '**SSE**: A client uploads the object to S3 via the S3 endpoint. Even though
    the data in transit is through an encrypted channel that uses HTTPS, the objects
    themselves are not encrypted inside the channel. Once the data hits S3, then it
    is encrypted by the S3 service. In SSE, you trust S3 to perform encryption-decryption,
    object storage, and key management. There are three types of SSE techniques available
    for S3 objects:'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SSE**：客户端通过 S3 端点将对象上传到 S3。尽管传输中的数据通过使用 HTTPS 的加密通道，但对象本身在通道内并未加密。一旦数据到达
    S3，S3 服务就会对其进行加密。在 SSE 中，你信任 S3 执行加密解密、对象存储和密钥管理。S3 对象有三种 SSE 技术可供选择：'
- en: SSE-C
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSE-C
- en: SSE-S3
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSE-S3
- en: SSE-KMS
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSE-KMS
- en: '`PUT` operation, the user has to provide a key and an object to S3\. S3 encrypts
    the object using the key provided and attaches the hash (a cipher text) to the
    object. As soon as the object is stored, S3 discards the encryption key. This
    generated hash is one-way and cannot be used to generate a new key. When the user
    provides a `GET` operation request along with the decryption key, the hash identifies
    whether the specific key was used for encryption. Then, S3 decrypts and discards
    the key.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PUT` 操作，用户必须向 S3 提供一个密钥和一个对象。S3 使用提供的密钥加密对象，并将哈希（加密文本）附加到对象上。一旦对象存储完毕，S3 就会丢弃加密密钥。这个生成的哈希是一次的，不能用来生成新的密钥。当用户提供带有解密密钥的
    `GET` 操作请求时，哈希可以识别是否使用了特定的密钥进行加密。然后，S3 解密并丢弃密钥。'
- en: '`PUT` operation, the user just provides the unencrypted object. S3 creates
    a master key to be used for the encryption process. No one can change anything
    on this master key as this is created, rotated internally, and managed by S3 from
    end to end. This is a unique key for the object. It uses the AES-256 algorithm
    by default.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PUT` 操作，用户只需提供未加密的对象。S3 创建一个用于加密过程的主密钥。由于这个主密钥在创建后，内部轮换并由 S3 从头到尾管理，因此没有人可以更改它。这是对象的唯一密钥。它默认使用
    AES-256 算法。'
- en: '**SSE with Customer Master Keys stored in AWS Key Management Service (SSE-KMS):**
    AWS Key Management Service (KMS) manages the Customer Master Key (CMK). AWS S3
    collaborates with AWS KMS and generates an AWS-managed CMK. This is the default
    master key used for SSE-KMS. Every time an object is uploaded, S3 uses a dedicated
    key to encrypt that object, and that key is a **Data Encryption Key (DEK).** The
    DEK is generated by KMS using the CMK. S3 is provided with both a plain-text version
    and an encrypted version of the DEK. The plain-text version of DEK is used to
    encrypt the object and then discarded. The encrypted version of DEK is stored
    along with the encrypted object. When you are using SSE-KMS, it is not necessary
    to use the default CMK that is created by S3\. You can create and use a customer-managed
    CMK, which means you can control the permission on it as well as the rotation
    of the key material. So, if you have a regulatory board in your organization that
    is concerned with the rotation of the key or the separation of roles between encryption
    users and decryption users, then SSE-KMS is the solution. Logging and auditing
    are also possible on SSE-KMS to track the API calls made against keys.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用存储在 AWS 密钥管理服务（SSE-KMS）中的客户主密钥（SSE-KMS）的 SSE**：AWS 密钥管理服务（KMS）管理客户主密钥（CMK）。AWS
    S3 与 AWS KMS 协作并生成一个 AWS 管理的 CMK。这是 SSE-KMS 使用的默认主密钥。每次上传对象时，S3 都会使用一个专用的密钥来加密该对象，而这个密钥是一个**数据加密密钥（DEK）**。DEK
    由 KMS 使用 CMK 生成。S3 提供了 DEK 的明文版本和加密版本。DEK 的明文版本用于加密对象然后丢弃。DEK 的加密版本与加密对象一起存储。当你使用
    SSE-KMS 时，不需要使用 S3 创建的默认 CMK。你可以创建并使用客户管理的 CMK，这意味着你可以控制其上的权限以及密钥材料的轮换。因此，如果你的组织有一个关注密钥轮换或加密用户与解密用户之间角色分离的监管委员会，那么
    SSE-KMS 就是解决方案。在 SSE-KMS 上还可以进行日志记录和审计，以跟踪针对密钥的 API 调用。'
- en: '`PUT` operation).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PUT` 操作）。'
- en: In the next section, you will learn about some of the data stores used with
    EC2 instances.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解一些与 EC2 实例一起使用的数据库。
- en: Using other types of data stores
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用其他类型的数据存储
- en: '**Elastic Block Store (EBS)** is used to create volumes in an Availability
    Zone. The volume can only be attached to an EC2 instance in the same Availability
    Zone. Amazon EBS provides both **Solid-State Drive (SSD)** and **Hard Disk Drive
    (HDD)** types of volumes. For SSD-based volumes, the dominant performance attribute
    is **Input-Output Per Second (IOPS)**, and for HDD it is throughput, which is
    generally measured as MiB/s. You can choose between different volume types, such
    as General Purpose SSD (gp2), Provisioned IOPS SSD (io1), or Throughput Optimized
    HDD (st1), depending on your requirements. Provisioned IOPS volumes are often
    used for high-performance workloads, such as deep learning training, where low
    latency and high throughput are critical. *Table 2.1* provides an overview of
    the different volumes and types:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性块存储 (EBS)** 用于在可用区中创建卷。卷只能附加到同一可用区中的 EC2 实例。Amazon EBS 提供了 **固态硬盘 (SSD)**
    和 **硬盘驱动器 (HDD)** 类型的卷。对于基于 SSD 的卷，主导的性能属性是 **每秒输入输出 (IOPS)**，而对于 HDD 是吞吐量，通常以
    MiB/s 来衡量。您可以根据需求选择不同的卷类型，例如通用型 SSD (gp2)、预配置 IOPS SSD (io1) 或吞吐量优化 HDD (st1)。预配置
    IOPS 卷通常用于高性能工作负载，如深度学习训练，其中低延迟和高吞吐量至关重要。*表 2.1* 提供了不同卷和类型的概述：'
- en: '| **Volume Types** | **Use cases** |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| **卷类型** | **用例** |'
- en: '| General Purpose SSD (gp2) | Useful for maintaining balance between price
    and performance. Good for most workloads, system boot volumes, dev, and test environments
    |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 通用型 SSD (gp2) | 适用于在价格和性能之间保持平衡。适用于大多数工作负载、系统启动卷、开发和测试环境 |'
- en: '| Provisioned IOPS SSD (io2, io1) | Useful for mission-critical, high-throughput
    or low-latency workloads. For example, I/O intensive database workloads like MongoDB,
    Cassandra, Oracle |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 预配置 IOPS SSD (io2, io1) | 适用于关键任务、高吞吐量或低延迟的工作负载。例如，I/O 密集型数据库工作负载，如 MongoDB、Cassandra、Oracle
    |'
- en: '| Throughput Optimized HDD (st1) | Useful for frequently accessed, throughput-intensive
    workloads. For example, big data processing, data warehouses, log processing |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 吞吐量优化 HDD (st1) | 适用于频繁访问、吞吐量密集型的工作负载。例如，大数据处理、数据仓库、日志处理 |'
- en: '| Cold HDD (sc1) | Useful for less frequently accessed workloads |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 冷 HDD (sc1) | 适用于访问频率较低的工作负载 |'
- en: Table 2.1 – Different volumes and their use cases
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1 – 不同的卷及其用例
- en: '**EBS** is designed to be resilient within an **Availability Zone (AZ)**. If,
    for some reason, an AZ fails, then the volume cannot be accessed. To prevent such
    scenarios, **snapshots** can be created from the EBS volumes, and they are stored
    in S3\. Once the snapshot arrives in S3, the data in the snapshot becomes Region-resilient.
    The first snapshot is a full copy of data on the volume and, from then onward,
    snapshots are incremental. Snapshots can be used to clone a volume. As the snapshot
    is stored in S3, a volume can be cloned in any AZ in that Region. Snapshots can
    be shared between Regions and volumes can be cloned from them during disaster
    recovery. Even after the EC2 instance is stopped/terminated, EBS volumes can retain
    data through an easy restoration process from backed-up snapshots.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**EBS** 设计为在 **可用区 (AZ)** 内具有弹性。如果由于某种原因，一个 AZ 失败，那么卷将无法访问。为了防止此类情况，可以从 EBS
    卷创建 **快照**，并且它们存储在 S3 中。一旦快照到达 S3，快照中的数据就具有区域弹性。第一个快照是卷上数据的完整副本，从那时起，快照是增量式的。快照可以用来克隆卷。由于快照存储在
    S3 中，因此可以在该区域的任何 AZ 中克隆卷。快照可以在区域之间共享，并且可以在灾难恢复期间从它们克隆卷。即使在 EC2 实例停止/终止后，EBS 卷也可以通过从备份快照的简单恢复过程保留数据。'
- en: Multiple EC2 instances can be attached via **EBS Multi-Attach** for concurrent
    EBS volume access. If the use case demands multiple instances to access the training
    dataset simultaneously (distributed training scenarios), then EBS Multi-Attach
    will provide the solution with improved performance and scalability.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过 **EBS 多附加** 将多个 EC2 实例附加，以实现并发 EBS 卷访问。如果用例需要多个实例同时访问训练数据集（分布式训练场景），那么
    EBS 多附加将提供具有改进性能和可伸缩性的解决方案。
- en: AWS KMS manages the CMK. AWS KMS uses an AWS-managed CMK for EBS, or AWS KMS
    can use a customer-managed CMK. The CMK is used by EBS when an encrypted volume
    is created. The CMK is used to create an encrypted DEK, which is stored with the
    volume on the physical disk. This DEK can only be decrypted using KMS, assuming
    the entity has access to decrypt. When a snapshot is created from the encrypted
    volume, the snapshot is encrypted with the same DEK. Any volume created from this
    snapshot also uses that DEK.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: AWS KMS管理CMK。AWS KMS可以使用AWS管理的CMK来管理EBS，或者AWS KMS可以使用客户管理的CMK。当创建加密卷时，EBS会使用CMK。CMK用于创建加密的DEK，该DEK与卷一起存储在物理磁盘上。此DEK只能通过KMS解密，前提是该实体有权解密。当从加密卷创建快照时，快照会使用相同的DEK进行加密。从该快照创建的任何卷也将使用该DEK。
- en: 'Instance Store volumes are the block storage devices physically connected to
    the EC2 instance. They provide the highest performance, as the ephemeral storage
    attached to the instance is from the same host where the instance is launched.
    EBS can be attached to the instance at any time, but the instance store must be
    attached to the instance at the time of its launch; it cannot be attached once
    the instance is launched. If there is an issue on the underlying host of an EC2
    instance, then the same instance will be launched on another host with a new instance
    store volume and the earlier instance store (ephemeral storage) and old data will
    be lost. The size and capabilities of the attached volumes depend on the instance
    types and can be found in more detail here: [https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 实例存储卷是物理连接到EC2实例的块存储设备。由于实例附加的临时存储来自实例启动的主机，因此它们提供最高性能。EBS可以在任何时间附加到实例，但实例存储必须在实例启动时附加；一旦实例启动，就不能再附加。如果EC2实例的底层主机存在问题，则相同的实例将在另一个主机上启动，并带有新的实例存储卷，之前的实例存储（临时存储）和旧数据将丢失。附加卷的大小和能力取决于实例类型，更详细的信息请在此处查看：[https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/)。
- en: '**Elastic File System (EFS)** provides a network-based filesystem that can
    be mounted within Linux EC2 instances and can be used by multiple instances at
    once. It is an implementation of **NFSv4**. It can be used in general-purpose
    mode, max I/O performance mode (for scientific analysis or parallel computing),
    bursting mode, and provisioned throughput mode. This makes it ideal for scenarios
    where multiple instances need to train on large datasets or share model artifacts.
    With EFS, you can store training datasets, pre-trained models, and other data
    centrally, ensuring consistency and reducing data duplication. Additionally, EFS
    provides high throughput and low-latency access, enabling efficient data access
    during training and inference processes. By leveraging EFS with SageMaker, machine
    learning developers can seamlessly scale their workloads, collaborate effectively,
    and accelerate model development and training.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**弹性文件系统（EFS）**提供了一种基于网络的文件系统，可以在Linux EC2实例内部挂载，并且可以一次由多个实例使用。它是**NFSv4**的实现。它可以用于通用模式、最大I/O性能模式（用于科学分析或并行计算）、突发模式和预配吞吐量模式。这使得它在多个实例需要在大数据集上训练或共享模型工件的情况下非常理想。使用EFS，您可以集中存储训练数据集、预训练模型和其他数据，确保一致性并减少数据重复。此外，EFS提供高吞吐量和低延迟访问，使训练和推理过程中的数据访问效率更高。通过利用EFS与SageMaker结合，机器学习开发者可以无缝扩展其工作负载，有效协作，并加速模型开发和训练。'
- en: 'As you know, in the case of instance stores, the data is volatile. As soon
    as the instance is lost, the data is lost from the instance store. That is not
    the case for EFS. EFS is separate from the EC2 instance storage. EFS is a file
    store and is accessed by multiple EC2 instances via mount targets inside a VPC.
    On-premises systems can access EFS storage via hybrid networking to the VPC, such
    as **VPN** or **Direct Connect**. EFS also supports two types of storage classes:
    Standard and Infrequent Access. Standard is used for frequently accessed data.
    Infrequent Access is the cost-effective storage class for long-lived, less frequently
    accessed data. Lifecycle policies can be used for the transition of data between
    storage classes. EFS offers a pay-as-you-go pricing model, where you only pay
    for the storage capacity you use. It eliminates the need to provision and manage
    separate storage volumes for each instance, reducing storage costs and simplifying
    storage management for your machine learning workloads.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，在实例存储的情况下，数据是易变的。一旦实例丢失，数据就会从实例存储中丢失。EFS的情况并非如此。EFS与EC2实例存储是分开的。EFS是一个文件存储，通过VPC内部的挂载目标被多个EC2实例访问。本地系统可以通过混合网络访问EFS存储，例如**VPN**或**直接连接**。EFS还支持两种存储类别：标准存储和频繁访问存储。标准存储用于频繁访问的数据。频繁访问存储是长期存储、不频繁访问数据的成本效益存储类别。生命周期策略可用于在存储类别之间转换数据。EFS提供按使用付费的定价模式，您只需为使用的存储容量付费。它消除了为每个实例配置和管理单独存储卷的需求，从而降低了存储成本并简化了机器学习工作负载的存储管理。
- en: Important note
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: An instance store is preferred for max I/O requirements and if the data is replaceable
    and temporary.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 当最大I/O需求且数据可替换且临时时，实例存储是首选。
- en: Relational Database Service (RDS)
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关系型数据库服务（RDS）
- en: This is one of the most commonly featured topics in AWS exams. You should have
    sufficient knowledge prior to the exam. In this section, you will learn about
    Amazon’s RDS.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这是AWS考试中最常见的主题之一。您应该在考试前具备足够的知识。在本节中，您将了解亚马逊的RDS。
- en: AWS provides several relational databases as a service to its users. Users can
    run their desired database on EC2 instances, too. The biggest drawback is that
    the instance is only available in one Availability Zone in a Region. The EC2 instance
    has to be administered and monitored to avoid any kind of failure. Custom scripts
    will be required to maintain a data backup over time. Any database major or minor
    version update would result in downtime. Database instances running on an EC2
    instance cannot be easily scaled if the load increases on the database as replication
    is not an easy task.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: AWS为用户提供了几种关系型数据库作为服务。用户也可以在EC2实例上运行他们想要的数据库。最大的缺点是实例在一个区域的可用区中仅可用。EC2实例必须被管理和监控以避免任何类型的故障。需要自定义脚本来维护数据备份。任何数据库主版本或次版本更新都会导致停机。在EC2实例上运行的数据库实例在数据库负载增加时无法轻松扩展，因为复制不是一项容易的任务。
- en: RDS provides managed database instances that can themselves hold one or more
    databases. Imagine a database server running on an EC2 instance that you do not
    have to manage or maintain. You need only access the server and create databases
    in it. AWS will manage everything else, such as the security of the instance,
    the operating system running on the instance, the database versions, and high
    availability of the database server. RDS supports multiple engines, such as MySQL,
    Microsoft SQL Server, MariaDB, Amazon Aurora, Oracle, and PostgreSQL. You can
    choose any of these based on your requirements.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: RDS提供可管理的数据库实例，这些实例可以包含一个或多个数据库。想象一下，在一个EC2实例上运行的数据库服务器，你不需要管理或维护它。你只需要访问服务器并在其中创建数据库。AWS将管理其他一切，例如实例的安全性、实例上运行的操作系统、数据库版本和数据库服务器的高可用性。RDS支持多种引擎，如MySQL、Microsoft
    SQL Server、MariaDB、Amazon Aurora、Oracle和PostgreSQL。你可以根据自己的需求选择这些中的任何一种。
- en: The foundation of Amazon RDS is a database instance, which can support multiple
    engines and can have multiple databases created by the user. One database instance
    can be accessed only by using the database DNS endpoint (the CNAME, which is an
    alias for the canonical name in a domain name system database) of the primary
    instance. RDS uses standard database engines. So, accessing the database using
    some sort of tool in a self-managed database server is the same as accessing Amazon
    RDS.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon RDS 的基础是一个数据库实例，它可以支持多个引擎，并且用户可以创建多个数据库。只能通过使用主实例的数据库 DNS 端点（CNAME，它是域名系统数据库中规范名称的别名）来访问一个数据库实例。RDS
    使用标准数据库引擎。因此，使用某种工具在自管理数据库服务器上访问数据库与访问 Amazon RDS 相同。
- en: As you have now understood the requirements of Amazon RDS, let’s understand
    the failover process in Amazon RDS. You will cover what services Amazon offers
    if something goes wrong with the RDS instance.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 既然您已经了解了 Amazon RDS 的需求，让我们了解 Amazon RDS 中的故障转移过程。您将了解如果 RDS 实例出现问题，Amazon 提供哪些服务。
- en: Managing failover in Amazon RDS
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon RDS 中管理故障转移
- en: RDS instances can be **Single-AZ** or **Multi-AZ**. In Multi-AZ, multiple instances
    work together, similar to an active-passive failover design.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: RDS 实例可以是 **Single-AZ** 或 **Multi-AZ**。在 Multi-AZ 中，多个实例协同工作，类似于主动-被动故障转移设计。
- en: For a Single-AZ RDS instance, storage can be allocated for that instance to
    use. In a nutshell, a Single-AZ RDS instance has one attached block store (EBS
    storage) available in the same Availability Zone. This makes the databases and
    the storage of the RDS instance vulnerable to Availability Zone failure. The storage
    allocated to the block storage can be SSD (gp2 or io1) or magnetic. To secure
    the RDS instance, it is advised to use a security group and provide access based
    on requirements.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Single-AZ RDS 实例，可以为该实例分配存储空间。简而言之，Single-AZ RDS 实例在同一可用区中有一个附加的块存储（EBS 存储）可用。这使得
    RDS 实例的数据库和存储容易受到可用区故障的影响。分配给块存储的存储可以是 SSD（gp2 或 io1）或磁性存储。为了确保 RDS 实例的安全，建议使用安全组并根据需求提供访问权限。
- en: Multi-AZ is always the best way to design the architecture to prevent failures
    and keep the applications highly available. With Multi-AZ features, a standby
    replica is kept in sync synchronously with the primary instance. The standby instance
    has its own storage in the assigned Availability Zone. A standby replica cannot
    be accessed directly, because all RDS access is via a single database DNS endpoint
    (CNAME). You can’t access the standby unless a failover happens. The standby provides
    no performance benefit, but it does constitute an improvement in terms of the
    availability of the RDS instance. It can only happen in the same Region, another
    AZ’s subnet in the same Region inside the VPC. When a Multi-AZ RDS instance is
    online, you can take a backup from the standby replica without affecting the performance.
    In a Single-AZ instance, availability and performance issues can be significant
    during backup operation.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Multi-AZ 总是设计架构以防止故障并保持应用程序高度可用的最佳方式。使用 Multi-AZ 功能，一个备用副本会与主实例同步。备用实例在其分配的可用区中有自己的存储。备用副本不能直接访问，因为所有
    RDS 访问都是通过单个数据库 DNS 端点（CNAME）。除非发生故障转移，否则无法访问备用实例。备用实例不提供性能优势，但它确实在 RDS 实例的可用性方面构成了一种改进。它只能在同一区域发生，或者在
    VPC 内同一区域的另一个可用区的子网中。当 Multi-AZ RDS 实例在线时，您可以从备用副本中备份，而不会影响性能。在 Single-AZ 实例中，备份操作期间可能会出现可用性和性能问题。
- en: To understand the workings of Multi-AZ, let’s take an example of a Single-AZ
    instance and expand it to Multi-AZ.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解 Multi-AZ 的工作原理，让我们以一个 Single-AZ 实例为例，并将其扩展到 Multi-AZ。
- en: Imagine you have an RDS instance running in Availability Zone `AZ-A` of the
    `us-east-1` Region inside a VPC named `db-vpc`. This becomes a primary instance
    in a Single-AZ design of an RDS instance. In this case, there will be storage
    allocated to the instance in the `AZ-A` Availability Zone. Once you opt for Multi-AZ
    deployment in another Availability Zone called `AZ-B`, AWS creates a standby instance
    in Availability Zone `AZ-B` of the `us-east-1` Region inside the `db-vpc` VPC
    and allocates storage for the standby instance in `AZ-B` of the `us-east-1` Region.
    Along with that, RDS will enable **synchronous replication** from the primary
    instance to the standby replica. As you learned earlier, the only way to access
    our RDS instance is via the database CNAME, hence, the access request goes to
    the RDS primary instance. As soon as a write request comes to the endpoint, it
    writes to the primary instance. Then it writes the data to the hardware, which
    is the block storage attached to the primary instance. At the same time, the primary
    instance replicates the same data to the standby instance. Finally, the standby
    instance commits the data to its block storage.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一个在名为 `db-vpc` 的 VPC 内的 `us-east-1` 区域的可用区 `AZ-A` 中运行的 RDS 实例。这成为了一个 RDS
    实例单可用区设计的主体实例。在这种情况下，将在 `AZ-A` 可用区为该实例分配存储。一旦您选择在另一个名为 `AZ-B` 的可用区中进行多可用区部署，AWS
    将在 `db-vpc` VPC 内的 `us-east-1` 区域的 `AZ-B` 可用区创建一个备用实例，并在 `us-east-1` 区域的 `AZ-B`
    为备用实例分配存储。除此之外，RDS 还将从主体实例到备用副本启用 **同步复制**。如您之前所学的，访问我们的 RDS 实例的唯一方式是通过数据库 CNAME，因此，访问请求会发送到
    RDS 主体实例。一旦写入请求到达端点，它就会写入主体实例。然后它将数据写入硬件，这是附加到主体实例的块存储。同时，主体实例将相同的数据复制到备用实例。最后，备用实例将数据提交到其块存储。
- en: The primary instance writes the data into the hardware and replicates the data
    to the standby instance in parallel, so there is a minimal time lag (almost nothing)
    between the data commit operations in their respective hardware. If an error occurs
    with the primary instance, then RDS detects this and changes the database endpoint
    to the standby instance. The clients accessing the database may experience a very
    short interruption with this. This failover occurs within 60-120 seconds. It does
    not provide a fault-tolerant system because there will be some impact during the
    failover operation.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 主体实例将数据写入硬件，并并行地将数据复制到备用实例，因此在它们各自的硬件中的数据提交操作之间有最小的时间延迟（几乎为零）。如果主体实例发生错误，那么
    RDS 会检测到这一点，并将数据库端点更改为备用实例。访问数据库的客户端可能会经历非常短暂的中断。这种故障转移在 60-120 秒内发生。它不提供容错系统，因为在故障转移操作期间会有一些影响。
- en: You should now understand failover management on Amazon RDS. Let’s now learn
    about taking automatic RDS backups and using snapshots to restore in the event
    of a failure, and read replicas in the next section.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该已经理解了 Amazon RDS 的故障转移管理。现在让我们学习如何进行自动 RDS 备份，以及在发生故障时使用快照进行恢复，以及在下节中学习读取副本。
- en: Taking automatic backups, RDS snapshots, and restore and read replicas
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行自动备份、RDS 快照、恢复和读取副本
- en: In this section, you will see how RDS **automatic backups** and **manual snapshots**
    work. These features come with Amazon RDS.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解 RDS 的 **自动备份** 和 **手动快照** 如何工作。这些功能是 Amazon RDS 的一部分。
- en: Let’s consider a database that is scheduled to take a backup at 5 A.M. every
    day. If the application fails at 11 A.M., then it is possible to restart the application
    from the backup taken at 11 A.M. with the loss of 6 hours’ worth of data. This
    is called a 6-hour **Recovery Point Objective (RPO)**. The RPO is defined as the
    time between the most recent backup and the incident, and this determines the
    amount of data loss. If you want to reduce this, then you have to schedule more
    incremental backups, which increases the cost and backup frequency. If your business
    demands a lower RPO value, then the business must spend more to provide the necessary
    technical solutions.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个每天早上 5 点进行备份的数据库。如果应用程序在上午 11 点失败，那么可以从上午 11 点的备份中重新启动应用程序，但会丢失 6 小时的数据。这被称为
    6 小时的 **恢复点目标 (RPO**)。RPO 定义为最近一次备份和事件之间的时间，这决定了数据丢失的数量。如果您想减少这个时间，那么您必须安排更多的增量备份，这会增加成本和备份频率。如果您的业务需要更低的
    RPO 值，那么业务必须投入更多资金来提供必要的技术解决方案。
- en: Now, according to our example, an engineer was assigned the task of bringing
    the system back online as soon as the disaster occurred. The engineer managed
    to bring the database online at 2 P.M. on the same day by adding a few extra hardware
    components to the current system and installing some updated versions of the software.
    This is called a 3-hour **Recovery Time Objective (RTO).** The RTO is determined
    as the time between the disaster recovery and full recovery. RTO values can be
    reduced by having spare hardware and documenting the restoration process. If the
    business demands a lower RTO value, then your business must spend more money on
    spare hardware and an effective system setup to perform the restoration process.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据我们的示例，一位工程师被分配了在灾难发生时尽快恢复系统的任务。工程师通过向现有系统添加一些额外的硬件组件并安装一些更新的软件版本，在当天下午2点成功将数据库上线。这被称为3小时的**恢复时间目标（RTO）**。RTO是灾难恢复和完全恢复之间的时间。通过拥有备用硬件和记录恢复过程，可以降低RTO值。如果业务需求较低的RTO值，那么您的业务必须在备用硬件和有效的系统设置上投入更多资金以执行恢复过程。
- en: In RDS, the RPO and RTO play an important role in the selection of automatic
    backups and manual snapshots. Both of these backup services use AWS-managed S3
    buckets, which means they cannot be visible in the user’s AWS S3 console. They
    areRegion-resilient because the backup is replicated into multiple Availability
    Zones in the AWS Region. In the case of a Single-AZ RDS instance, the backup happens
    from the single available data store, and for a Multi-AZ enabled RDS instance,
    the backup happens from the standby data store (the primary store remains untouched
    as regards the backup).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在RDS中，RPO（恢复点目标）和RTO（恢复时间目标）在自动备份和手动快照的选择中起着重要作用。这两种备份服务都使用AWS管理的S3存储桶，这意味着它们在用户的AWS
    S3控制台中不可见。它们具有区域弹性，因为备份被复制到AWS区域内的多个可用区。对于单AZ RDS实例，备份是从单个可用的数据存储进行的，而对于已启用多AZ的RDS实例，备份是从备用数据存储进行的（主存储在备份方面保持不变）。
- en: The snapshots are manual for RDS instances, and they are stored in the AWS-managed
    S3 bucket. The first snapshot of an RDS instance is a full copy of the data and
    the onward snapshots are incremental, reflecting the change in the data. In terms
    of the time taken for the snapshot process, it is high for the first one and,
    from then on, the incremental backup is quicker. When any snapshot occurs, it
    can impact the performance of the Single-AZ RDS instance, but not the performance
    of a Multi-AZ RDS instance as this happens on the standby data storage. Manual
    snapshots do not expire, have to be cleared automatically, and live past the termination
    of an RDS instance. When you delete an RDS instance, it suggests making one final
    snapshot on your behalf and it will contain all the databases inside your RDS
    instance (there is not just a single database in an RDS instance). When you restore
    from a manual snapshot, you restore to a single point in time, and that affects
    the RPO.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RDS实例，快照是手动的，并且存储在AWS管理的S3存储桶中。RDS实例的第一个快照是数据的完整副本，后续的快照是增量副本，反映了数据的变化。在快照过程所需的时间方面，第一个快照较高，从那时起，增量备份更快。当发生任何快照时，它可能会影响单AZ
    RDS实例的性能，但不会影响多AZ RDS实例的性能，因为这是在备用数据存储上发生的。手动快照不会过期，需要自动清除，并且会超过RDS实例的终止。当您删除RDS实例时，它会建议为您创建一个最后的快照，并将包含您RDS实例中的所有数据库（RDS实例中不仅仅只有一个数据库）。当您从手动快照恢复时，您将恢复到某个特定的时间点，这会影响RPO。
- en: To automate this entire process, you can choose a time window when these snapshots
    can be taken. This is called an automatic backup. These time windows can be managed
    carefully to essentially lower the RPO value of the business. Automatic backups
    have a retention period of 0 to 35 days, with 0 being disabled and the maximum
    is 35 days. To quote AWS documentation, retained automated backups contain system
    snapshots and transaction logs from a database instance. They also include database
    instance properties such as allocated storage and a database instance class, which
    are required to restore it to an active instance. Databases generate transaction
    logs, which contain the actual change in data in a particular database. These
    transaction logs are also written to S3 every 5 minutes by RDS. Transaction logs
    can also be replayed on top of the snapshots to restore to a point in time of
    5 minutes’ granularity. Theoretically, the RPO can be a 5-minute point in time.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 要自动化整个流程，你可以选择一个时间窗口来捕捉这些快照。这被称为自动备份。这些时间窗口可以精心管理，从而本质上降低业务的RPO（恢复点目标）值。自动备份的保留期为0到35天，0表示禁用，最大为35天。根据AWS文档，保留的自动备份包含数据库实例的系统快照和事务日志。它们还包括数据库实例属性，如分配的存储和数据库实例类别，这些是将其恢复为活动实例所必需的。数据库生成事务日志，这些日志包含特定数据库中的实际数据变化。这些事务日志每5分钟由RDS写入S3。事务日志也可以在快照之上重新播放，以恢复到5分钟粒度的时间点。理论上，RPO可以是一个5分钟的时间点。
- en: When you perform a restore, RDS creates a new RDS instance, which means a new
    database endpoint to access the instance. The applications using the instances
    have to point to the new address, which significantly affects the RTO. This means
    that the restoration process is not very fast, which affects the RTO. To minimize
    the RTO during a failure, you may consider replicating the data. With replicas,
    there is a high chance of replicating the corrupted data. The only way to overcome
    this is to have snapshots and restore an RDS instance to a particular point in
    time prior to the corruption. **Amazon RDS Read Replicas** are unlike the Multi-AZ
    replicas. In Multi-AZ RDS instances, the standby replicas cannot be used directly
    for anything unless a primary instance fails, whereas **Read Replicas** can be
    used directly, but only for read operations. Read replicas have their own database
    endpoints and read-heavy applications can directly point to this address. They
    are kept in sync **asynchronously** with the primary instance. Read Replicas can
    be created in the same Region as the primary instance or in a different Region.
    Read Replicas in other Regions are called **Cross-Region Read Replicas** and this
    improves the global performance of the application.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 当你执行恢复操作时，RDS会创建一个新的RDS实例，这意味着一个新的数据库端点来访问该实例。使用这些实例的应用程序必须指向新的地址，这会显著影响RTO。这意味着恢复过程并不非常快，这会影响RTO。为了在故障期间最小化RTO，你可能考虑复制数据。使用副本，复制损坏数据的高概率很高。克服这一点的唯一方法是拥有快照，并在损坏之前将RDS实例恢复到特定的时间点。**Amazon
    RDS读取副本**与多AZ副本不同。在多AZ RDS实例中，备用副本除非主实例故障，否则不能直接用于任何操作，而**读取副本**可以直接使用，但仅限于读取操作。读取副本有自己的数据库端点，读取密集型应用程序可以直接指向这个地址。它们与主实例**异步**保持同步。读取副本可以在与主实例相同的区域或不同区域创建。其他区域的读取副本称为**跨区域读取副本**，这提高了应用程序的全球性能。
- en: As per AWS documentation, five direct Read Replicas are allowed per database
    instance and this helps to scale out the read performances. Read Replicas have
    a very low RPO value due to asynchronous replication. They can be promoted to
    a read-write database instance in the case of a primary instance failure. This
    can be done quickly and it offers a fairly low RTO value.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 根据AWS文档，每个数据库实例允许有五个直接读取副本，这有助于扩展读取性能。由于异步复制，读取副本具有非常低的RPO值。在主实例故障的情况下，它们可以被提升为读写数据库实例。这可以快速完成，并且提供了相当低的RTO（恢复时间目标）值。
- en: In the next section, you will learn about Amazon’s database engine, Amazon Aurora.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解Amazon的数据库引擎，Amazon Aurora。
- en: Writing to Amazon Aurora with multi-master capabilities
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用具有多主能力的Amazon Aurora进行写入
- en: Amazon Aurora is the most reliable relational database engine developed by Amazon
    to deliver speed in a simple and cost-effective manner. Aurora uses a cluster
    of single primary instances and zero or more replicas. Aurora’s replicas can give
    you the advantage of both read replicas and Multi-AZ instances in RDS. Aurora
    uses a shared cluster volume for storage and is available to all compute instances
    of the cluster (a maximum of 64 TiB). This allows the Aurora cluster to provision
    faster and improves availability and performance. Aurora uses SSD-based storage,
    which provides high IOPS and low latency. Aurora does not ask you to allocate
    storage, unlike other RDS instances; it is based on the storage that you use.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Aurora 是亚马逊开发的最可靠的数据库引擎，以简单且经济高效的方式提供速度。Aurora 使用单个主实例和零个或多个副本的集群。Aurora
    的副本可以为您提供 RDS 中读取副本和多区域实例的优势。Aurora 使用共享集群卷进行存储，可供集群中所有计算实例（最多 64 TiB）使用。这允许 Aurora
    集群更快地提供资源，并提高可用性和性能。Aurora 使用基于 SSD 的存储，提供高 IOPS 和低延迟。Aurora 不会要求您分配存储，与其他 RDS
    实例不同；它是基于您使用的存储。
- en: Aurora clusters have multiple endpoints, including the **cluster endpoint**
    **and reader endpoint.** If there are zero replicas, then the cluster endpoint
    is the same as the reader endpoint. If there are replicas available, then the
    reader endpoint is load-balanced across the reader endpoints. Cluster endpoints
    are used for reading/writing, while reader endpoints are intended for reading
    from the cluster. If you add more replicas, then AWS manages load balancing under
    the hood for the new replicas.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Aurora 集群具有多个端点，包括 **集群端点** 和 **读取端点**。如果没有副本，则集群端点与读取端点相同。如果有可用的副本，则读取端点在读取端点之间进行负载均衡。集群端点用于读取/写入，而读取端点旨在从集群中读取。如果您添加更多副本，则
    AWS 在幕后为新副本管理负载均衡。
- en: When failover occurs, the replicas are promoted to read/write mode, and this
    takes some time. This can be prevented in a **Multi-Master** mode of an Aurora
    cluster. This allows multiple instances to perform reads and writes at the same
    time.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生故障转移时，副本被提升到读取/写入模式，这需要一些时间。这可以在 Aurora 集群的 **多主** 模式下防止。这允许多个实例同时执行读取和写入操作。
- en: Storing columnar data on Amazon Redshift
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon Redshift 上存储列式数据
- en: Amazon Redshift is not used for real-time transactions, but it is used for data
    warehouse purposes. It is designed to support huge volumes of data at a petabyte
    scale. It is a column-based database used for analytics, long-term processing,
    tending, and aggregation. **Redshift Spectrum** can be used to query data on S3
    without loading data to the Redshift cluster (a Redshift cluster is required,
    though). It’s not an OLTP, but an OLAP. **AWS QuickSight** can be integrated with
    Redshift for visualization, with a SQL-like interface that allows you to connect
    using JDBC/ODBC connections to query the data.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 不用于实时事务，但用于数据仓库目的。它设计用于支持以拍字节规模的大量数据。它是一个基于列的数据库，用于分析、长期处理、维护和聚合。**Redshift
    Spectrum** 可以用于查询 S3 上的数据，而无需将数据加载到 Redshift 集群中（尽管需要 Redshift 集群）。它不是 OLTP，而是
    OLAP。**AWS QuickSight** 可以与 Redshift 集成以进行可视化，具有类似 SQL 的界面，允许您使用 JDBC/ODBC 连接连接以查询数据。
- en: Redshift uses a clustered architecture in one AZ in a VPC with faster network
    connectivity between the nodes. It is not high availability by design as it is
    tightly coupled to the AZ. A Redshift cluster has a leader node, and this node
    is responsible for all the communication between the client and the computing
    nodes of the cluster, query planning, and aggregation. Compute nodes are responsible
    for running the queries submitted by the leader lode and for storing the data.
    By default, Redshift uses a public network for communicating with external services
    or any AWS services. With **enhanced VPC routing**, it can be controlled via customized
    networking settings.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: Redshift 在 VPC 中的一个 AZ 中使用集群架构，节点之间具有更快的网络连接。它不是设计为高可用性，因为它与 AZ 紧密耦合。Redshift
    集群有一个主节点，该节点负责客户端与集群计算节点之间的所有通信、查询规划和聚合。计算节点负责运行由主节点提交的查询以及存储数据。默认情况下，Redshift
    使用公共网络与外部服务或任何 AWS 服务进行通信。通过 **增强 VPC 路由**，可以通过自定义网络设置进行控制。
- en: By combining Redshift with SageMaker, data scientists and analysts can leverage
    the scalability and computational power of Redshift to preprocess and transform
    data before training machine learning models. They can utilize Redshift’s advanced
    SQL capabilities to perform aggregations, joins, and filtering operations, enabling
    efficient feature engineering and data preparation. The processed data can then
    be seamlessly fed into SageMaker for model training, hyperparameter tuning, and
    evaluation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合 Redshift 和 SageMaker，数据科学家和分析人员可以利用 Redshift 的可伸缩性和计算能力在训练机器学习模型之前预处理和转换数据。他们可以利用
    Redshift 的高级 SQL 功能执行聚合、连接和过滤操作，从而实现高效的特征工程和数据准备。处理后的数据可以无缝地输入到 SageMaker 中进行模型训练、超参数调整和评估。
- en: Amazon DynamoDB for NoSQL Database-as-a-Service
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Amazon DynamoDB 作为 NoSQL 数据库即服务
- en: Amazon DynamoDB is a NoSQL database-as-a-service product within AWS. It’s a
    fully managed key/value and document database. Accessing DynamoDB is easy via
    its endpoint. The input and output throughputs can be managed or scaled manually
    or automatically. It also supports data backup, point-in-time recovery, and data
    encryption.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon DynamoDB 是 AWS 中的 NoSQL 数据库即服务产品。它是一个完全管理的键/值和文档数据库。通过其端点访问 DynamoDB
    很容易。输入和输出吞吐量可以手动或自动管理或扩展。它还支持数据备份、时间点恢复和数据加密。
- en: One example where Amazon DynamoDB can be used with Amazon SageMaker in a cost-efficient
    way is for real-time prediction applications. DynamoDB can serve as a storage
    backend for storing and retrieving input data for prediction models built using
    SageMaker. Instead of continuously running and scaling an inference endpoint,
    which can be costlier, you can leverage DynamoDB’s low-latency access and scalability
    to retrieve the required input data on demand.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是，Amazon DynamoDB 可以以成本效益的方式与 Amazon SageMaker 结合使用，用于实时预测应用。DynamoDB 可以作为存储后端，用于存储和检索使用
    SageMaker 构建的预测模型的输入数据。您不必持续运行和扩展推理端点，这可能会更昂贵，而是可以利用 DynamoDB 的低延迟访问和可伸缩性，按需检索所需的输入数据。
- en: In this setup, the input data for predictions can be stored in DynamoDB tables,
    where each item represents a unique data instance. When a prediction request is
    received, the application can use DynamoDB’s efficient querying capabilities to
    retrieve the required input data item(s) based on specific attributes or conditions.
    Once the data is retrieved, it can be passed to the SageMaker endpoint for real-time
    predictions.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在此配置中，预测的输入数据可以存储在 DynamoDB 表中，其中每个条目代表一个独特的数据实例。当接收到预测请求时，应用程序可以使用 DynamoDB
    的高效查询功能，根据特定的属性或条件检索所需的输入数据项。一旦检索到数据，就可以将其传递到 SageMaker 端点进行实时预测。
- en: By using DynamoDB in this way, you can dynamically scale your application’s
    read capacity based on the incoming prediction requests, ensuring that you only
    pay for the read capacity you actually need. This approach offers a cost-efficient
    solution as it eliminates the need for running and managing a continuously running
    inference endpoint, which may incur high costs even during periods of low prediction
    demand. With DynamoDB and SageMaker working together, you can achieve scalable
    and cost-efficient real-time prediction applications while maintaining low latency
    and high availability.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式使用 DynamoDB，您可以根据传入的预测请求动态扩展应用程序的读取能力，确保您只为实际需要的读取能力付费。这种方法提供了一种成本效益的解决方案，因为它消除了运行和管理持续运行的推理端点的需求，即使在预测需求较低期间，这也可能产生高昂的成本。通过
    DynamoDB 和 SageMaker 的协同工作，您可以在保持低延迟和高可用性的同时，实现可伸缩和成本效益的实时预测应用。
- en: 'You will not cover the DynamoDB table structure or key structure in this chapter
    as this is not required for the certification exam. However, it is good to have
    a basic knowledge of them. For more details, please refer to the AWS docs available
    here: [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html).'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会涵盖 DynamoDB 表结构或键结构，因为这对于认证考试不是必需的。然而，了解它们的基本知识是好的。更多详情，请参阅此处可用的 AWS 文档：[https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html)。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about various data storage services from Amazon,
    and how to secure data through various policies and use these services. If you
    are working on machine learning use cases, then you may encounter such scenarios
    where you have to choose an effective data storage service for your requirements.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了关于亚马逊的各种数据存储服务以及如何通过各种策略来保护数据并使用这些服务。如果你正在处理机器学习用例，那么你可能会遇到这样的场景，你必须为你的需求选择一个有效的数据存储服务。
- en: In the next chapter, you will learn about the migration and processing of stored
    data.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将了解存储数据的迁移和处理。
- en: Exam Readiness Drill – Chapter Review Questions
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考试准备练习 - 章节复习题
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对关键概念有扎实的理解外，能够在时间压力下快速思考是一项帮助你通过认证考试的关键技能。这就是为什么在学习的早期阶段就培养这些技能至关重要。
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 章节复习题旨在随着你学习并复习每个章节的内容，逐步提高你的应试技巧，同时同时复习章节中关键概念的理解。你将在每个章节的末尾找到这些内容。
- en: How To Access These Resources
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 如何访问这些资源
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何访问这些资源，请转到名为[*第 11 章*](B21197_11.xhtml#_idTextAnchor1477)的章节，*访问在线* *实践资源*。
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开本章的章节复习题，请执行以下步骤：
- en: Click the link – [https://packt.link/MLSC01E2_CH02](https://packt.link/MLSC01E2_CH02).
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击链接 – [https://packt.link/MLSC01E2_CH02](https://packt.link/MLSC01E2_CH02)。
- en: 'Alternatively, you can scan the following **QR code** (*Figure 2**.3*):'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你可以扫描以下**二维码**（*图 2.3*）：
- en: '![Figure 2.3 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_02_03.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 为登录用户打开章节复习题的二维码](img/B21197_02_03.jpg)'
- en: Figure 2.3 – QR code that opens Chapter Review Questions for logged-in users
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 为登录用户打开章节复习题的二维码
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 2**.4*:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，你会看到一个类似于*图 2.4*所示的页面：
- en: '![Figure 2.4 – Chapter Review Questions for Chapter 2](img/B21197_02_04.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 第 2 章的章节复习题](img/B21197_02_04.jpg)'
- en: Figure 2.4 – Chapter Review Questions for Chapter 2
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 第 2 章的章节复习题
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备就绪后，开始以下练习，多次重新尝试测验。
- en: Exam Readiness Drill
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考试准备练习
- en: For the first three attempts, don’t worry about the time limit.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前三次尝试，不要担心时间限制。
- en: ATTEMPT 1
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试 1
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次，目标至少达到**40%**。查看你答错的答案，并再次阅读章节中的相关部分，以修复你的学习差距。
- en: ATTEMPT 2
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试 2
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次，目标至少达到**60%**。查看你答错的答案，并再次阅读章节中的相关部分，以修复任何剩余的学习差距。
- en: ATTEMPT 3
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试 3
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 第三次，目标至少达到**75%**。一旦得分达到 75% 或更高，你就可以开始练习时间了。
- en: Tip
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要超过**三次**尝试才能达到 75%。没关系。只需复习章节中的相关部分，直到达到那里。
- en: Working On Timing
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习时间
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 目标：你的目标是保持分数不变，同时尽可能快地回答这些问题。以下是你下一次尝试应该看起来像什么：
- en: '| **Attempt** | **Score** | **Time Taken** |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| **尝试** | **分数** | **用时** |'
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 尝试 5 | 77% | 21 分钟 30 秒 |'
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 尝试 6 | 78% | 18 分钟 34 秒 |'
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 尝试 7 | 76% | 14 分钟 44 秒 |'
- en: Table 2.2 – Sample timing practice drills on the online platform
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 – 在线平台上的样本时间练习
- en: Note
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 上表中显示的时间限制只是示例。根据网站上的测验时间限制，每次尝试时自行设定你的时间限制。
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 每次新的尝试，你的得分应该保持在**75%**以上，同时完成所需的时间应该“减少”。你可以重复尽可能多的尝试，直到你觉得自己能够自信地应对时间压力。
