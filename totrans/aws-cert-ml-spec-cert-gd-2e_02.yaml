- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Services for Data Storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS provides a wide range of services to store your data safely and securely.
    There are various storage options available on AWS, such as block storage, file
    storage, and object storage. It is expensive to manage on-premises data storage
    due to the higher investment in hardware, admin overheads, and managing system
    upgrades. With AWS storage services, you just pay for what you use, and you don’t
    have to manage the hardware. You will also learn about various storage classes
    offered by Amazon S3 for intelligent access to data and to reduce costs. You can
    expect questions in the exam on storage classes. As you continue through this
    chapter, you will master the **single-AZ** and **multi-AZ** instances, and **Recovery
    Time Objective** (**RTO**) and **Recovery Point Objective** (**RPO**) concepts
    of Amazon RDS.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about storing your data securely for further
    analytical purposes throughout the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Storing data on Amazon S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling access on S3 buckets and objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting data on Amazon S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Securing S3 objects at rest and in transit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using other types of data stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relational Database** **Services** (**RDSes**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing failover in Amazon RDS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking automatic backup, RDS snapshots, and restore and read replicas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing to Amazon Aurora with multi-master capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing columnar data on Amazon Redshift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon DynamoDB for NoSQL databases as a service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All you will need for this chapter is an AWS account and the AWS CLI configured.
    The steps to configure the AWS CLI for your account are explained in detail by
    Amazon here: [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the code examples from GitHub, here: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter02).'
  prefs: []
  type: TYPE_NORMAL
- en: Storing Data on Amazon S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: S3 is Amazon’s cloud-based object storage service, and it can be accessed from
    anywhere via the internet. It is an ideal storage option for large datasets. It
    is region-based, as your data is stored in a particular region until you move
    the data to a different region. Your data will never leave that region until it
    is configured to do so. In a particular region, data is replicated in the availability
    zones of that region; this makes S3 regionally resilient. If any of the availability
    zones fail in a region, then other availability zones will serve your requests.
    S3 can be accessed via the AWS console UI, AWS CLI, AWS API requests, or via standard
    HTTP methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'S3 has two main components: **buckets** and **objects**.'
  prefs: []
  type: TYPE_NORMAL
- en: Buckets are created in a specific AWS region. Buckets can contain objects but
    cannot contain other buckets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Objects have two main attributes. One is the **key**, and the other is the **value**.
    The value is the content being stored, and the key is the name. The maximum size
    of an object can be 5 TB. As per the Amazon S3 documentation ([https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingObjects.html)),
    objects also have a version ID, metadata, access control information, and sub-resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: As per Amazon’s docs, S3 provides read-after-write consistency for PUTs of new
    objects, which means that if you upload a new object or create a new object and
    you immediately try to read the object using its key, then you get the exact data
    that you just uploaded. However, for overwrites and deletes, it behaves in an
    **eventually consistent manner**. This means that if you read an object straight
    after the delete or overwrite operation, then you may read an old copy or a stale
    version of the object. It takes some time to replicate the content of the object
    across three Availability Zones.
  prefs: []
  type: TYPE_NORMAL
- en: 'A folder structure can be maintained logically by using a prefix. Take an example
    where an image is uploaded into a bucket, `bucket-name-example`, with the prefix
    `folder-name` and the object name `my-image.jpg`. The entire structure looks like
    this: `/bucket-name-example/folder-name/my-image.jpg`.'
  prefs: []
  type: TYPE_NORMAL
- en: The content of the object can be read by using the bucket name of `bucket-name-example`
    and the key of `/folder-name/my-image.jpg`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several storage classes offered by Amazon for objects stored in S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Standard Storage (S3 Standard):** This is the storage class for frequently
    accessed objects and for quick access. S3 Standard has a millisecond first-byte
    latency and objects can be made publicly available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard Infrequent Access (S3 Standard-IA):** This option is used when you
    need data to be returned quickly, but not for frequent access. The object size
    has to be a minimum of 128 KB. The minimum storage timeframe is 30 days. If the
    object is deleted before 30 days, you are still charged for 30 days. Standard-IA
    objects are resilient to the loss of Availability Zones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One Zone Infrequent Access (S3 One Zone-IA):** Objects in this storage class
    are stored in just one Availability Zone, which makes it cheaper than **Standard-IA**.
    The minimum object size and storage timeframe are the same as Standard-IA. Objects
    from this storage class are less available and less resilient. This storage class
    is used when you have another copy, or if the data can be recreated. A **One Zone-IA**
    storage class should be used for long-lived data that is non-critical and replaceable,
    and where access is infrequent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon S3 Glacier Flexible Retrieval (formerly S3 Glacier):** This option
    is used for long-term archiving and backup. It can take anything from minutes
    to hours to retrieve objects in this storage class. The minimum storage timeframe
    is 90 days. For archived data that doesn’t need to be accessed right away but
    requires the ability to retrieve extensive data sets without incurring additional
    charges, like in backup or disaster-recovery scenarios, S3 Glacier Flexible Retrieval
    (formerly known as S3 Glacier) is the perfect storage option.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon S3 Glacier Instant Retrieval:** This storage class offers cost-effective,
    high-speed storage for seldom-accessed, long-term data. Compared to S3 Standard-Infrequent
    Access, it can cut storage expenses by up to 68% when data is accessed once per
    quarter. This storage class is perfect for swiftly retrieving archive data like
    medical images, news media assets, or user-generated content archives. You can
    upload data directly or use S3 Lifecycle policies to move it from other S3 storage
    classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Glacier Deep Archive:** The minimum storage duration of this class is 180
    days. This is the least expensive storage class and has a default retrieval time
    of 12 hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**S3 Intelligent-Tiering:** This storage class is designed to reduce operational
    overheads. Users pay a monitoring fee and AWS selects a storage class between
    Standard (a frequent-access tier) and Standard-IA (a lower-cost, infrequent-access
    tier) based on the access pattern of an object. This option is designed for long-lived
    data with unknown or unpredictable access patterns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through sets of rules, the transition between storage classes and deletion of
    the objects can be managed easily and are referred to as **S3 Lifecycle configurations**.
    These rules consist of actions. These can be applied to a bucket or a group of
    objects in that bucket defined by prefixes or tags. Actions can either be **transition
    actions** or **expiration actions**. Transition actions define the storage class
    transition of the objects following the creation of *a user-defined* number of
    days. Expiration actions configure the deletion of versioned objects, or the deletion
    of delete markers or incomplete multipart uploads. This is very useful for managing
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'An illustration is given in *Figure 2**.1*. You can find more details here:
    [https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/storage-class-intro.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – A comparison table of S3 Storage classes](img/B21197_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – A comparison table of S3 Storage classes
  prefs: []
  type: TYPE_NORMAL
- en: Creating buckets to hold data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, you will see how to create a bucket, upload an object, and read the object
    using the AWS CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first step, check whether you have any buckets created by using the
    `aws s3` `ls` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command returns nothing here. So, create a bucket now by using the `mb`
    argument. Let’s say the bucket name is `demo-bucket-baba` in the `us-east-1` Region:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you have created a bucket now, your next step is to copy a file to your
    bucket using the `cp` argument, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To validate the file upload operation via the AWS console, please log in to
    your AWS account and go to the AWS S3 console to see the same. The AWS S3 console
    lists the result as shown in *Figure 2**.2*. The console may have changed by the
    time you are reading this book!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.2 – AWS S3 listing your files](img/B21197_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – AWS S3 listing your files
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also list the files in your S3 bucket from the command line, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to upload your filesystem directories and files to the S3 bucket,
    then `--recursive` will do the job for you:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The contents of one bucket can be copied/moved to another bucket via the `cp`
    command and the `--recursive` parameter. To achieve this, you will have to create
    two buckets, `demo-bucket-baba-copied` and `demo-bucket-baba-moved`. The steps
    are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If all the commands are run successfully, then the original bucket should be
    empty at the end (as all the files have now been moved).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In the certification exam, you will not find many questions on bucket- and object-level
    operations. However, it is always better to know the basic operations and the
    required steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'The buckets must be deleted to avoid costs as soon as the hands-on work is
    finished. The bucket has to be empty before you run the `rb` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `demo-bucket-baba-moved` bucket is not empty, so you couldn’t remove the
    bucket. In such scenarios, use the `--force` parameter to delete the entire bucket
    and all its contents, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s take an example of a bucket, `test-bucket`, that has a prefix, `images`.
    This prefix contains four image files named `animal.jpg`, `draw-house.jpg`, `cat.jpg`,
    and `human.jpg`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, to delete the contents inside the images, the command will be as follows:
    `aws s3 rm` `s3://test-bucket/images –recursive`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bucket should now be empty.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, you are going to learn about object tags and object metadata.
  prefs: []
  type: TYPE_NORMAL
- en: Distinguishing between object tags and object metadata
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s compare these two terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Object tag**: An object tag is a **key-value** pair. AWS S3 object tags can
    help you filter analytics and metrics, categorize storage, secure objects based
    on certain categorizations, track costs based on certain categorization of objects,
    and much more besides. Object tags can be used to create life cycle rules to move
    objects to cheaper storage tiers. You can have a maximum of 10 tags added to an
    object and 50 tags to a bucket. A tag key can contain 128 Unicode characters,
    while a tag value can contain 256 Unicode characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Object metadata**: Object metadata is descriptive data describing an object.
    It consists of **name-value** pairs. Object metadata is returned as HTTP headers
    on objects. They are of two types: one is **system metadata**, and the other is
    **user-defined metadata**. User-defined metadata is a custom name-value pair added
    to an object by the user. The name must begin with **x-amz-meta**. You can change
    all system metadata such as storage class, versioning, and encryption attributes
    on an object. Further details are available here: [https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingMetadata.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Metadata names are case-insensitive, whereas tag names are case-sensitive.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you are going to learn about controlling access to buckets
    and objects on Amazon S3 through different policies, including the resource policy
    and the identity policy.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling access to buckets and objects on Amazon S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the object is stored in the bucket, the next major step is to manage access.
    S3 is private by default, and access is given to other users, groups, or resources
    via several methods. This means that access to the objects can be managed via
    **Access Control Lists (ACLs)**, **Public Access Settings**, **Identity Policies**,
    and **Bucket Policies**.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: S3 bucket policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An `Principal` is rendered `*`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: By default, everything in S3 is private to the owner. If you want to make a
    prefix public to the world, then `Resource` changes to `arn:aws:s3:::my-bucket/some-prefix/*`,
    and similarly, if it is intended for a specific IAM user or IAM group, then those
    details go in the principal part in the policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'There can be conditions added to the bucket policy too. Let’s examine a use
    case where the organization wants to keep a bucket public and whitelist particular
    IP addresses. The policy would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'More examples are available in the AWS S3 developer guide, which can be found
    here: [https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html](https://docs.aws.amazon.com/AmazonS3/latest/dev/example-bucket-policies.html).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Block public access** is a separate setting given to the bucket owner to
    avoid any kind of mistakes in bucket policy. In a real-world scenario, the bucket
    can be made public through bucket policy by mistake; to avoid such mistakes, or
    data leaks, AWS has provided this setting. It provides a further level of security,
    irrespective of the bucket policy. You can choose this while creating a bucket,
    or it can be set after creating a bucket.'
  prefs: []
  type: TYPE_NORMAL
- en: '`us-east-1` in this example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '**ACLs** are used to grant high-level permissions, typically for granting access
    to other AWS accounts. ACLs are one of the **sub-resources** of a bucket or an
    object. A bucket or object can be made public quickly via ACLs. AWS doesn’t suggest
    doing this, and you shouldn’t expect questions about this on the test. It is good
    to know about this, but it is not as flexible as the **S3** **bucket policy**.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s learn about the methods to protect our data in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting data on Amazon S3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how to record every version of an object. Along
    with durability, Amazon provides several techniques to secure the data in S3\.
    Some of those techniques involve enabling versioning and encrypting the objects.
  prefs: []
  type: TYPE_NORMAL
- en: Versioning helps you to roll back to a previous version if any problem occurs
    with the current object during update, delete, or put operations.
  prefs: []
  type: TYPE_NORMAL
- en: Through encryption, you can control the access of an object. You need the appropriate
    key to read and write an object. You will also learn **Multi-Factor Authentication
    (MFA**) for delete operations. Amazon also allows **Cross-Region Replication (CRR)**
    to maintain a copy of an object in another Region, which can be used for data
    backup during any disaster, for further redundancy, or for the enhancement of
    data access speed in different Regions.
  prefs: []
  type: TYPE_NORMAL
- en: Applying bucket versioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now understand how you can enable bucket versioning with the help of
    some hands-on examples. Bucket versioning can be applied while creating a bucket
    from the AWS S3 console:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable versioning on a bucket from the command line, a bucket must be created
    first and then versioning can be enabled, as shown in the following example. In
    this example, I have created a bucket, `version-demo-mlpractice`, and enabled
    versioning through the `put-bucket-versioning` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You have not created this bucket with any kind of encryption. So, if you run
    **aws s3api get-bucket-encryption --bucket version-demo-mlpractice**, then it
    will output an error that says the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`put-bucket-encryption` API. The command will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This can be verified using the following command: `aws s3api get-bucket-encryption
    --``bucket version-demo-mlpractice`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will learn more about encryption in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Applying encryption to buckets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You also need to understand how enabling versioning on a bucket would help.
    There are use cases where a file is updated regularly, and versions will be created
    for the same file. To simulate this scenario, try the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, you will create a file with versions written in it. You will
    overwrite it and retrieve it to check the versions in that file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon retrieval, you got the latest version of the file, in other words, `Version-2`
    in this case. To check each of the versions and the latest one of them, S3 provides
    the `list-object-versions` API, as shown here. From the JSON results, you can
    deduce the latest version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There may be a situation where you have to roll back to the earlier version
    of the current object. In the preceding example, the latest one is `Version-2.`
    You can make any desired version the latest or current version by parsing the
    `VersionId` sub-resource to the `get-object` API call and uploading that object
    again. The other way is to delete the current or latest version by passing `versionId`
    to the `–version-id` parameter in the `delete-object` API request. More details
    about the API are available here: [https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html](https://docs.aws.amazon.com/cli/latest/reference/s3api/delete-object.html).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When you delete an object in a versioning-enabled bucket, it does not delete
    the object from the bucket. It just creates a marker called `DeleteMarker`. It
    looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This means that the object is not deleted. You can list it by using this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now the bucket has no objects as `version-doc.txt`, and you can verify this
    using the `aws s3 ls` command because that marker became the current version of
    the object with a new ID. If you try to retrieve an object that is deleted, which
    means a delete marker is serving the current version of the object, then you will
    get a `VersionId`, as shown in the following example commands. A simple delete
    request `{`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon listing the bucket now, the older objects can be seen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you have already covered the exam topics and practiced most of the required
    concepts, you should delete the objects in the bucket and then delete the bucket
    to save on costs. This step deletes the versions of the object and, in turn, removes
    the object permanently.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here, the latest version is deleted by giving the version ID to it, followed
    by the other version ID:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can clearly see the empty bucket now.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS best practices suggest adding another layer of protection through **MFA
    delete.** Accidental bucket deletions can be prevented, and the security of the
    objects in the bucket is ensured. MFA delete can be enabled or disabled via the
    console and CLI. As documented in AWS docs, MFA delete requires two forms of authentication
    together: your security credentials, and the concatenation of a valid serial number,
    a space, and the six-digit code displayed on an approved authentication device.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CRR helps you to separate data between different geographical Regions. A typical
    use case is the maintenance business-as-usual activities during a disaster. If
    a Region goes down, then another Region can support the users if CRR is enabled.
    This improves the availability of the data. Another use case is to reduce latency
    if the same data is used by another compute resource, such as EC2 or AWS Lambda
    being launched in another Region. You can also use CRR to copy objects to another
    AWS account that belongs to a different owner. There are a few important points
    that are worth noting down for the certification exam:'
  prefs: []
  type: TYPE_NORMAL
- en: In order to use CRR, versioning has to be enabled on both the source and destination
    bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication is enabled on the source bucket by adding rules. As the source,
    either an entire bucket, a prefix, or tags can be replicated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encrypted objects can also be replicated by assigning an appropriate encryption
    key.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The destination bucket can be in the same account or in another account. You
    can change the storage type and ownership of the object in the destination bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For CRR, an existing role can be chosen or a new IAM role can be created too.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be multiple replication rules on the source bucket, with priority
    accorded to it. Rules with higher priority override rules with lower priority.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you add a replication rule, only new versions of an object that are created
    after the rules are enabled get replicated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If versions are deleted from the source bucket, then they are not deleted from
    the destination bucket.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you delete an object from the source bucket, it creates a delete marker
    in said source bucket. That delete marker is not replicated to the destination
    bucket by S3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, you will cover the concept of securing S3 objects.
  prefs: []
  type: TYPE_NORMAL
- en: Securing S3 objects at rest and in transit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, you learned about bucket default encryption, which
    is completely different from object-level encryption. Buckets are not encrypted,
    whereas objects are. A question may arise here: *what is the default bucket encryption?*
    You will learn these concepts in this section. Data during transmission can be
    protected by using **Secure Socket Layer (SSL)** or **Transport Layer Security
    (TLS)** for the transfer of HTTPS requests. The next step is to protect the data,
    where the authorized person can encode and decode the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to have different encryption settings on different objects in
    the same bucket. S3 supports **Client-Side Encryption (CSE)** and **Server-Side
    Encryption (SSE)** for objects at rest:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CSE**: A client uploads the object to S3 via the S3 endpoint. In CSE, the
    data is encrypted by the client before uploading to S3\. Although the transit
    between the user and the S3 endpoint happens in an encrypted channel, the data
    in the channel is already encrypted by the client and can’t be seen. In transit,
    encryption takes place by default through HTTPS. So, AWS S3 stores the encrypted
    object and cannot read the data in any format at any point in time. In CSE, the
    client takes care of encrypting the object’s content. So, control stays with the
    client in terms of key management and the encryption-decryption process. This
    leads to a huge amount of CPU usage. S3 is only used for storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSE**: A client uploads the object to S3 via the S3 endpoint. Even though
    the data in transit is through an encrypted channel that uses HTTPS, the objects
    themselves are not encrypted inside the channel. Once the data hits S3, then it
    is encrypted by the S3 service. In SSE, you trust S3 to perform encryption-decryption,
    object storage, and key management. There are three types of SSE techniques available
    for S3 objects:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSE-C
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SSE-S3
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SSE-KMS
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PUT` operation, the user has to provide a key and an object to S3\. S3 encrypts
    the object using the key provided and attaches the hash (a cipher text) to the
    object. As soon as the object is stored, S3 discards the encryption key. This
    generated hash is one-way and cannot be used to generate a new key. When the user
    provides a `GET` operation request along with the decryption key, the hash identifies
    whether the specific key was used for encryption. Then, S3 decrypts and discards
    the key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PUT` operation, the user just provides the unencrypted object. S3 creates
    a master key to be used for the encryption process. No one can change anything
    on this master key as this is created, rotated internally, and managed by S3 from
    end to end. This is a unique key for the object. It uses the AES-256 algorithm
    by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SSE with Customer Master Keys stored in AWS Key Management Service (SSE-KMS):**
    AWS Key Management Service (KMS) manages the Customer Master Key (CMK). AWS S3
    collaborates with AWS KMS and generates an AWS-managed CMK. This is the default
    master key used for SSE-KMS. Every time an object is uploaded, S3 uses a dedicated
    key to encrypt that object, and that key is a **Data Encryption Key (DEK).** The
    DEK is generated by KMS using the CMK. S3 is provided with both a plain-text version
    and an encrypted version of the DEK. The plain-text version of DEK is used to
    encrypt the object and then discarded. The encrypted version of DEK is stored
    along with the encrypted object. When you are using SSE-KMS, it is not necessary
    to use the default CMK that is created by S3\. You can create and use a customer-managed
    CMK, which means you can control the permission on it as well as the rotation
    of the key material. So, if you have a regulatory board in your organization that
    is concerned with the rotation of the key or the separation of roles between encryption
    users and decryption users, then SSE-KMS is the solution. Logging and auditing
    are also possible on SSE-KMS to track the API calls made against keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PUT` operation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, you will learn about some of the data stores used with
    EC2 instances.
  prefs: []
  type: TYPE_NORMAL
- en: Using other types of data stores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Elastic Block Store (EBS)** is used to create volumes in an Availability
    Zone. The volume can only be attached to an EC2 instance in the same Availability
    Zone. Amazon EBS provides both **Solid-State Drive (SSD)** and **Hard Disk Drive
    (HDD)** types of volumes. For SSD-based volumes, the dominant performance attribute
    is **Input-Output Per Second (IOPS)**, and for HDD it is throughput, which is
    generally measured as MiB/s. You can choose between different volume types, such
    as General Purpose SSD (gp2), Provisioned IOPS SSD (io1), or Throughput Optimized
    HDD (st1), depending on your requirements. Provisioned IOPS volumes are often
    used for high-performance workloads, such as deep learning training, where low
    latency and high throughput are critical. *Table 2.1* provides an overview of
    the different volumes and types:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Volume Types** | **Use cases** |'
  prefs: []
  type: TYPE_TB
- en: '| General Purpose SSD (gp2) | Useful for maintaining balance between price
    and performance. Good for most workloads, system boot volumes, dev, and test environments
    |'
  prefs: []
  type: TYPE_TB
- en: '| Provisioned IOPS SSD (io2, io1) | Useful for mission-critical, high-throughput
    or low-latency workloads. For example, I/O intensive database workloads like MongoDB,
    Cassandra, Oracle |'
  prefs: []
  type: TYPE_TB
- en: '| Throughput Optimized HDD (st1) | Useful for frequently accessed, throughput-intensive
    workloads. For example, big data processing, data warehouses, log processing |'
  prefs: []
  type: TYPE_TB
- en: '| Cold HDD (sc1) | Useful for less frequently accessed workloads |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Different volumes and their use cases
  prefs: []
  type: TYPE_NORMAL
- en: '**EBS** is designed to be resilient within an **Availability Zone (AZ)**. If,
    for some reason, an AZ fails, then the volume cannot be accessed. To prevent such
    scenarios, **snapshots** can be created from the EBS volumes, and they are stored
    in S3\. Once the snapshot arrives in S3, the data in the snapshot becomes Region-resilient.
    The first snapshot is a full copy of data on the volume and, from then onward,
    snapshots are incremental. Snapshots can be used to clone a volume. As the snapshot
    is stored in S3, a volume can be cloned in any AZ in that Region. Snapshots can
    be shared between Regions and volumes can be cloned from them during disaster
    recovery. Even after the EC2 instance is stopped/terminated, EBS volumes can retain
    data through an easy restoration process from backed-up snapshots.'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple EC2 instances can be attached via **EBS Multi-Attach** for concurrent
    EBS volume access. If the use case demands multiple instances to access the training
    dataset simultaneously (distributed training scenarios), then EBS Multi-Attach
    will provide the solution with improved performance and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: AWS KMS manages the CMK. AWS KMS uses an AWS-managed CMK for EBS, or AWS KMS
    can use a customer-managed CMK. The CMK is used by EBS when an encrypted volume
    is created. The CMK is used to create an encrypted DEK, which is stored with the
    volume on the physical disk. This DEK can only be decrypted using KMS, assuming
    the entity has access to decrypt. When a snapshot is created from the encrypted
    volume, the snapshot is encrypted with the same DEK. Any volume created from this
    snapshot also uses that DEK.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance Store volumes are the block storage devices physically connected to
    the EC2 instance. They provide the highest performance, as the ephemeral storage
    attached to the instance is from the same host where the instance is launched.
    EBS can be attached to the instance at any time, but the instance store must be
    attached to the instance at the time of its launch; it cannot be attached once
    the instance is launched. If there is an issue on the underlying host of an EC2
    instance, then the same instance will be launched on another host with a new instance
    store volume and the earlier instance store (ephemeral storage) and old data will
    be lost. The size and capabilities of the attached volumes depend on the instance
    types and can be found in more detail here: [https://aws.amazon.com/ec2/instance-types/](https://aws.amazon.com/ec2/instance-types/).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Elastic File System (EFS)** provides a network-based filesystem that can
    be mounted within Linux EC2 instances and can be used by multiple instances at
    once. It is an implementation of **NFSv4**. It can be used in general-purpose
    mode, max I/O performance mode (for scientific analysis or parallel computing),
    bursting mode, and provisioned throughput mode. This makes it ideal for scenarios
    where multiple instances need to train on large datasets or share model artifacts.
    With EFS, you can store training datasets, pre-trained models, and other data
    centrally, ensuring consistency and reducing data duplication. Additionally, EFS
    provides high throughput and low-latency access, enabling efficient data access
    during training and inference processes. By leveraging EFS with SageMaker, machine
    learning developers can seamlessly scale their workloads, collaborate effectively,
    and accelerate model development and training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you know, in the case of instance stores, the data is volatile. As soon
    as the instance is lost, the data is lost from the instance store. That is not
    the case for EFS. EFS is separate from the EC2 instance storage. EFS is a file
    store and is accessed by multiple EC2 instances via mount targets inside a VPC.
    On-premises systems can access EFS storage via hybrid networking to the VPC, such
    as **VPN** or **Direct Connect**. EFS also supports two types of storage classes:
    Standard and Infrequent Access. Standard is used for frequently accessed data.
    Infrequent Access is the cost-effective storage class for long-lived, less frequently
    accessed data. Lifecycle policies can be used for the transition of data between
    storage classes. EFS offers a pay-as-you-go pricing model, where you only pay
    for the storage capacity you use. It eliminates the need to provision and manage
    separate storage volumes for each instance, reducing storage costs and simplifying
    storage management for your machine learning workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: An instance store is preferred for max I/O requirements and if the data is replaceable
    and temporary.
  prefs: []
  type: TYPE_NORMAL
- en: Relational Database Service (RDS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is one of the most commonly featured topics in AWS exams. You should have
    sufficient knowledge prior to the exam. In this section, you will learn about
    Amazon’s RDS.
  prefs: []
  type: TYPE_NORMAL
- en: AWS provides several relational databases as a service to its users. Users can
    run their desired database on EC2 instances, too. The biggest drawback is that
    the instance is only available in one Availability Zone in a Region. The EC2 instance
    has to be administered and monitored to avoid any kind of failure. Custom scripts
    will be required to maintain a data backup over time. Any database major or minor
    version update would result in downtime. Database instances running on an EC2
    instance cannot be easily scaled if the load increases on the database as replication
    is not an easy task.
  prefs: []
  type: TYPE_NORMAL
- en: RDS provides managed database instances that can themselves hold one or more
    databases. Imagine a database server running on an EC2 instance that you do not
    have to manage or maintain. You need only access the server and create databases
    in it. AWS will manage everything else, such as the security of the instance,
    the operating system running on the instance, the database versions, and high
    availability of the database server. RDS supports multiple engines, such as MySQL,
    Microsoft SQL Server, MariaDB, Amazon Aurora, Oracle, and PostgreSQL. You can
    choose any of these based on your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The foundation of Amazon RDS is a database instance, which can support multiple
    engines and can have multiple databases created by the user. One database instance
    can be accessed only by using the database DNS endpoint (the CNAME, which is an
    alias for the canonical name in a domain name system database) of the primary
    instance. RDS uses standard database engines. So, accessing the database using
    some sort of tool in a self-managed database server is the same as accessing Amazon
    RDS.
  prefs: []
  type: TYPE_NORMAL
- en: As you have now understood the requirements of Amazon RDS, let’s understand
    the failover process in Amazon RDS. You will cover what services Amazon offers
    if something goes wrong with the RDS instance.
  prefs: []
  type: TYPE_NORMAL
- en: Managing failover in Amazon RDS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RDS instances can be **Single-AZ** or **Multi-AZ**. In Multi-AZ, multiple instances
    work together, similar to an active-passive failover design.
  prefs: []
  type: TYPE_NORMAL
- en: For a Single-AZ RDS instance, storage can be allocated for that instance to
    use. In a nutshell, a Single-AZ RDS instance has one attached block store (EBS
    storage) available in the same Availability Zone. This makes the databases and
    the storage of the RDS instance vulnerable to Availability Zone failure. The storage
    allocated to the block storage can be SSD (gp2 or io1) or magnetic. To secure
    the RDS instance, it is advised to use a security group and provide access based
    on requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-AZ is always the best way to design the architecture to prevent failures
    and keep the applications highly available. With Multi-AZ features, a standby
    replica is kept in sync synchronously with the primary instance. The standby instance
    has its own storage in the assigned Availability Zone. A standby replica cannot
    be accessed directly, because all RDS access is via a single database DNS endpoint
    (CNAME). You can’t access the standby unless a failover happens. The standby provides
    no performance benefit, but it does constitute an improvement in terms of the
    availability of the RDS instance. It can only happen in the same Region, another
    AZ’s subnet in the same Region inside the VPC. When a Multi-AZ RDS instance is
    online, you can take a backup from the standby replica without affecting the performance.
    In a Single-AZ instance, availability and performance issues can be significant
    during backup operation.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the workings of Multi-AZ, let’s take an example of a Single-AZ
    instance and expand it to Multi-AZ.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine you have an RDS instance running in Availability Zone `AZ-A` of the
    `us-east-1` Region inside a VPC named `db-vpc`. This becomes a primary instance
    in a Single-AZ design of an RDS instance. In this case, there will be storage
    allocated to the instance in the `AZ-A` Availability Zone. Once you opt for Multi-AZ
    deployment in another Availability Zone called `AZ-B`, AWS creates a standby instance
    in Availability Zone `AZ-B` of the `us-east-1` Region inside the `db-vpc` VPC
    and allocates storage for the standby instance in `AZ-B` of the `us-east-1` Region.
    Along with that, RDS will enable **synchronous replication** from the primary
    instance to the standby replica. As you learned earlier, the only way to access
    our RDS instance is via the database CNAME, hence, the access request goes to
    the RDS primary instance. As soon as a write request comes to the endpoint, it
    writes to the primary instance. Then it writes the data to the hardware, which
    is the block storage attached to the primary instance. At the same time, the primary
    instance replicates the same data to the standby instance. Finally, the standby
    instance commits the data to its block storage.
  prefs: []
  type: TYPE_NORMAL
- en: The primary instance writes the data into the hardware and replicates the data
    to the standby instance in parallel, so there is a minimal time lag (almost nothing)
    between the data commit operations in their respective hardware. If an error occurs
    with the primary instance, then RDS detects this and changes the database endpoint
    to the standby instance. The clients accessing the database may experience a very
    short interruption with this. This failover occurs within 60-120 seconds. It does
    not provide a fault-tolerant system because there will be some impact during the
    failover operation.
  prefs: []
  type: TYPE_NORMAL
- en: You should now understand failover management on Amazon RDS. Let’s now learn
    about taking automatic RDS backups and using snapshots to restore in the event
    of a failure, and read replicas in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Taking automatic backups, RDS snapshots, and restore and read replicas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will see how RDS **automatic backups** and **manual snapshots**
    work. These features come with Amazon RDS.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a database that is scheduled to take a backup at 5 A.M. every
    day. If the application fails at 11 A.M., then it is possible to restart the application
    from the backup taken at 11 A.M. with the loss of 6 hours’ worth of data. This
    is called a 6-hour **Recovery Point Objective (RPO)**. The RPO is defined as the
    time between the most recent backup and the incident, and this determines the
    amount of data loss. If you want to reduce this, then you have to schedule more
    incremental backups, which increases the cost and backup frequency. If your business
    demands a lower RPO value, then the business must spend more to provide the necessary
    technical solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, according to our example, an engineer was assigned the task of bringing
    the system back online as soon as the disaster occurred. The engineer managed
    to bring the database online at 2 P.M. on the same day by adding a few extra hardware
    components to the current system and installing some updated versions of the software.
    This is called a 3-hour **Recovery Time Objective (RTO).** The RTO is determined
    as the time between the disaster recovery and full recovery. RTO values can be
    reduced by having spare hardware and documenting the restoration process. If the
    business demands a lower RTO value, then your business must spend more money on
    spare hardware and an effective system setup to perform the restoration process.
  prefs: []
  type: TYPE_NORMAL
- en: In RDS, the RPO and RTO play an important role in the selection of automatic
    backups and manual snapshots. Both of these backup services use AWS-managed S3
    buckets, which means they cannot be visible in the user’s AWS S3 console. They
    areRegion-resilient because the backup is replicated into multiple Availability
    Zones in the AWS Region. In the case of a Single-AZ RDS instance, the backup happens
    from the single available data store, and for a Multi-AZ enabled RDS instance,
    the backup happens from the standby data store (the primary store remains untouched
    as regards the backup).
  prefs: []
  type: TYPE_NORMAL
- en: The snapshots are manual for RDS instances, and they are stored in the AWS-managed
    S3 bucket. The first snapshot of an RDS instance is a full copy of the data and
    the onward snapshots are incremental, reflecting the change in the data. In terms
    of the time taken for the snapshot process, it is high for the first one and,
    from then on, the incremental backup is quicker. When any snapshot occurs, it
    can impact the performance of the Single-AZ RDS instance, but not the performance
    of a Multi-AZ RDS instance as this happens on the standby data storage. Manual
    snapshots do not expire, have to be cleared automatically, and live past the termination
    of an RDS instance. When you delete an RDS instance, it suggests making one final
    snapshot on your behalf and it will contain all the databases inside your RDS
    instance (there is not just a single database in an RDS instance). When you restore
    from a manual snapshot, you restore to a single point in time, and that affects
    the RPO.
  prefs: []
  type: TYPE_NORMAL
- en: To automate this entire process, you can choose a time window when these snapshots
    can be taken. This is called an automatic backup. These time windows can be managed
    carefully to essentially lower the RPO value of the business. Automatic backups
    have a retention period of 0 to 35 days, with 0 being disabled and the maximum
    is 35 days. To quote AWS documentation, retained automated backups contain system
    snapshots and transaction logs from a database instance. They also include database
    instance properties such as allocated storage and a database instance class, which
    are required to restore it to an active instance. Databases generate transaction
    logs, which contain the actual change in data in a particular database. These
    transaction logs are also written to S3 every 5 minutes by RDS. Transaction logs
    can also be replayed on top of the snapshots to restore to a point in time of
    5 minutes’ granularity. Theoretically, the RPO can be a 5-minute point in time.
  prefs: []
  type: TYPE_NORMAL
- en: When you perform a restore, RDS creates a new RDS instance, which means a new
    database endpoint to access the instance. The applications using the instances
    have to point to the new address, which significantly affects the RTO. This means
    that the restoration process is not very fast, which affects the RTO. To minimize
    the RTO during a failure, you may consider replicating the data. With replicas,
    there is a high chance of replicating the corrupted data. The only way to overcome
    this is to have snapshots and restore an RDS instance to a particular point in
    time prior to the corruption. **Amazon RDS Read Replicas** are unlike the Multi-AZ
    replicas. In Multi-AZ RDS instances, the standby replicas cannot be used directly
    for anything unless a primary instance fails, whereas **Read Replicas** can be
    used directly, but only for read operations. Read replicas have their own database
    endpoints and read-heavy applications can directly point to this address. They
    are kept in sync **asynchronously** with the primary instance. Read Replicas can
    be created in the same Region as the primary instance or in a different Region.
    Read Replicas in other Regions are called **Cross-Region Read Replicas** and this
    improves the global performance of the application.
  prefs: []
  type: TYPE_NORMAL
- en: As per AWS documentation, five direct Read Replicas are allowed per database
    instance and this helps to scale out the read performances. Read Replicas have
    a very low RPO value due to asynchronous replication. They can be promoted to
    a read-write database instance in the case of a primary instance failure. This
    can be done quickly and it offers a fairly low RTO value.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn about Amazon’s database engine, Amazon Aurora.
  prefs: []
  type: TYPE_NORMAL
- en: Writing to Amazon Aurora with multi-master capabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Aurora is the most reliable relational database engine developed by Amazon
    to deliver speed in a simple and cost-effective manner. Aurora uses a cluster
    of single primary instances and zero or more replicas. Aurora’s replicas can give
    you the advantage of both read replicas and Multi-AZ instances in RDS. Aurora
    uses a shared cluster volume for storage and is available to all compute instances
    of the cluster (a maximum of 64 TiB). This allows the Aurora cluster to provision
    faster and improves availability and performance. Aurora uses SSD-based storage,
    which provides high IOPS and low latency. Aurora does not ask you to allocate
    storage, unlike other RDS instances; it is based on the storage that you use.
  prefs: []
  type: TYPE_NORMAL
- en: Aurora clusters have multiple endpoints, including the **cluster endpoint**
    **and reader endpoint.** If there are zero replicas, then the cluster endpoint
    is the same as the reader endpoint. If there are replicas available, then the
    reader endpoint is load-balanced across the reader endpoints. Cluster endpoints
    are used for reading/writing, while reader endpoints are intended for reading
    from the cluster. If you add more replicas, then AWS manages load balancing under
    the hood for the new replicas.
  prefs: []
  type: TYPE_NORMAL
- en: When failover occurs, the replicas are promoted to read/write mode, and this
    takes some time. This can be prevented in a **Multi-Master** mode of an Aurora
    cluster. This allows multiple instances to perform reads and writes at the same
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Storing columnar data on Amazon Redshift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon Redshift is not used for real-time transactions, but it is used for data
    warehouse purposes. It is designed to support huge volumes of data at a petabyte
    scale. It is a column-based database used for analytics, long-term processing,
    tending, and aggregation. **Redshift Spectrum** can be used to query data on S3
    without loading data to the Redshift cluster (a Redshift cluster is required,
    though). It’s not an OLTP, but an OLAP. **AWS QuickSight** can be integrated with
    Redshift for visualization, with a SQL-like interface that allows you to connect
    using JDBC/ODBC connections to query the data.
  prefs: []
  type: TYPE_NORMAL
- en: Redshift uses a clustered architecture in one AZ in a VPC with faster network
    connectivity between the nodes. It is not high availability by design as it is
    tightly coupled to the AZ. A Redshift cluster has a leader node, and this node
    is responsible for all the communication between the client and the computing
    nodes of the cluster, query planning, and aggregation. Compute nodes are responsible
    for running the queries submitted by the leader lode and for storing the data.
    By default, Redshift uses a public network for communicating with external services
    or any AWS services. With **enhanced VPC routing**, it can be controlled via customized
    networking settings.
  prefs: []
  type: TYPE_NORMAL
- en: By combining Redshift with SageMaker, data scientists and analysts can leverage
    the scalability and computational power of Redshift to preprocess and transform
    data before training machine learning models. They can utilize Redshift’s advanced
    SQL capabilities to perform aggregations, joins, and filtering operations, enabling
    efficient feature engineering and data preparation. The processed data can then
    be seamlessly fed into SageMaker for model training, hyperparameter tuning, and
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon DynamoDB for NoSQL Database-as-a-Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amazon DynamoDB is a NoSQL database-as-a-service product within AWS. It’s a
    fully managed key/value and document database. Accessing DynamoDB is easy via
    its endpoint. The input and output throughputs can be managed or scaled manually
    or automatically. It also supports data backup, point-in-time recovery, and data
    encryption.
  prefs: []
  type: TYPE_NORMAL
- en: One example where Amazon DynamoDB can be used with Amazon SageMaker in a cost-efficient
    way is for real-time prediction applications. DynamoDB can serve as a storage
    backend for storing and retrieving input data for prediction models built using
    SageMaker. Instead of continuously running and scaling an inference endpoint,
    which can be costlier, you can leverage DynamoDB’s low-latency access and scalability
    to retrieve the required input data on demand.
  prefs: []
  type: TYPE_NORMAL
- en: In this setup, the input data for predictions can be stored in DynamoDB tables,
    where each item represents a unique data instance. When a prediction request is
    received, the application can use DynamoDB’s efficient querying capabilities to
    retrieve the required input data item(s) based on specific attributes or conditions.
    Once the data is retrieved, it can be passed to the SageMaker endpoint for real-time
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: By using DynamoDB in this way, you can dynamically scale your application’s
    read capacity based on the incoming prediction requests, ensuring that you only
    pay for the read capacity you actually need. This approach offers a cost-efficient
    solution as it eliminates the need for running and managing a continuously running
    inference endpoint, which may incur high costs even during periods of low prediction
    demand. With DynamoDB and SageMaker working together, you can achieve scalable
    and cost-efficient real-time prediction applications while maintaining low latency
    and high availability.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will not cover the DynamoDB table structure or key structure in this chapter
    as this is not required for the certification exam. However, it is good to have
    a basic knowledge of them. For more details, please refer to the AWS docs available
    here: [https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/SQLtoNoSQL.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about various data storage services from Amazon,
    and how to secure data through various policies and use these services. If you
    are working on machine learning use cases, then you may encounter such scenarios
    where you have to choose an effective data storage service for your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about the migration and processing of stored
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Exam Readiness Drill – Chapter Review Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  prefs: []
  type: TYPE_NORMAL
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: How To Access These Resources
  prefs: []
  type: TYPE_NORMAL
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the link – [https://packt.link/MLSC01E2_CH02](https://packt.link/MLSC01E2_CH02).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Alternatively, you can scan the following **QR code** (*Figure 2**.3*):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.3 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – QR code that opens Chapter Review Questions for logged-in users
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 2**.4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Chapter Review Questions for Chapter 2](img/B21197_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Chapter Review Questions for Chapter 2
  prefs: []
  type: TYPE_NORMAL
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exam Readiness Drill
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the first three attempts, don’t worry about the time limit.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  prefs: []
  type: TYPE_NORMAL
- en: ATTEMPT 3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  prefs: []
  type: TYPE_NORMAL
- en: Working On Timing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attempt** | **Score** | **Time Taken** |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  prefs: []
  type: TYPE_TB
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  prefs: []
  type: TYPE_TB
- en: Table 2.2 – Sample timing practice drills on the online platform
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  prefs: []
  type: TYPE_NORMAL
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  prefs: []
  type: TYPE_NORMAL
