["```py\npip install pandas matpotlib \n```", "```py\n    # Importing necessary libraries\n    import pandas as pd\n    # URL of the raw CSV file on GitHub\n    data_url = \"https://raw.githubusercontent.com/yourusername/yourrepository/master/data.csv\"\n    # Read the dataset into a pandas DataFrame\n    df = pd.read_csv(data_url, encoding='latin1')\n    # Display the shape of the DataFrame (rows, columns)\n    print(\"Shape of the DataFrame:\", df.shape)\n    # Display the first few rows of the DataFrame\n    print(\"First few rows of the DataFrame:\")\n    print(df.head()) \n    ```", "```py\n    # Display the column names and data types\n    print(\"Column names and data types:\")\n    print(df.dtypes)\n    # Check for any missing values in each column\n    print(\"\\nMissing values per column:\")\n    print(df.isnull().sum()) \n    ```", "```py\n    # Calculate summary statistics for all columns\n    summary_stats = df.describe(include='all')\n    # Transpose the summary statistics for better readability\n    summary_stats = summary_stats.transpose()\n    # Display summary statistics\n    print(\"\\nSummary Statistics:\")\n    print(summary_stats) \n    ```", "```py\nShape of the DataFrame: (541909, 8)  \nFirst few rows of the DataFrame:  \nInvoiceNo StockCode                          Description  Quantity  \\  \n536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6     \n536365     71053                  WHITE METAL LANTERN         6     \n536365    84406B       CREAM CUPID HEARTS COAT HANGER         8     \n536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6       \n536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6      \nInvoiceDate  UnitPrice  CustomerID         Country    \n12/1/2010 8:26       2.55     17850.0  United Kingdom    \n12/1/2010 8:26       3.39     17850.0  United Kingdom    \n12/1/2010 8:26       2.75     17850.0  United Kingdom \n```", "```py\n# Step 4: Data Preprocessing\n# 1\\. Remove Missing Values in the CustomerID column\ndf_cleaned = df.dropna(subset=['CustomerID'])\n# 2\\. Remove Negative Values in the Quantity and UnitPrice columns\ndf_cleaned = df_cleaned[(df_cleaned['Quantity'] >= 0) & (df_cleaned['UnitPrice'] >= 0)]\n# 3\\. Remove Unused Columns: 'StockCode', 'InvoiceDate', 'Description', 'Country'\ncolumns_to_drop = ['StockCode', 'InvoiceDate', 'Description', 'Country']\ndf_cleaned = df_cleaned.drop(columns=columns_to_drop)\n# Displaying the shape of the cleaned DataFrame\nprint(\"Shape of the cleaned DataFrame:\", df_cleaned.shape)\n# Displaying the first few rows of the cleaned DataFrame\nprint(\"\\nFirst few rows of the cleaned DataFrame:\")\nprint(df_cleaned.head()) \n```", "```py\n# Step 5: Feature Engineering  \n\n# 1\\. Create Amount Column: Multiply Quantity and UnitPrice columns df_cleaned['Amount'] = df_cleaned['Quantity'] * df_cleaned['UnitPrice']   \n# 2\\. Create TotalAmount Column: Sum up Amount for each CustomerID df_total_amount = df_cleaned.groupby('CustomerID')['Amount'].sum().reset_index() df_total_amount.rename(columns={'Amount': 'TotalAmount'}, inplace=True)   \n# 3\\. Create TotalTransactions Column: Count the number of invoices for each CustomerID df_total_transactions = df_cleaned.groupby('CustomerID') \n['InvoiceNo'].nunique().reset_index() df_total_transactions.rename(columns={'InvoiceNo': 'TotalTransactions'}, inplace=True)  \n\n# 4\\. Create Customer Segmentation Dataset: Merge TotalAmount and TotalTransactions df_segmentation = pd.merge(df_total_amount, df_total_transactions, on='CustomerID')  \n\n# Displaying the first few rows of the Customer Segmentation DataFrame print(\"Customer Segmentation DataFrame:\") print(df_segmentation.head()) \n```", "```py\n# Step 6: Checking for Outliers\n# Drop the CustomerID column from df_segmentation\ndf_segmentation.drop('CustomerID', axis=1, inplace=True)\n# Checking for outliers using box plots\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 5))\nplt.boxplot([df_segmentation['TotalAmount'], df_segmentation['TotalTransactions']], labels=['TotalAmount', 'TotalTransactions'])\nplt.title('Box Plot of TotalAmount and TotalTransactions')\nplt.ylabel('Values')\nplt.show() \n```", "```py\n# Step 7: Removing Outliers from TotalAmount column\n# Calculate the IQR for TotalAmount column\nQ1 = df_segmentation['TotalAmount'].quantile(0.25)\nQ3 = df_segmentation['TotalAmount'].quantile(0.75)\nIQR = Q3 - Q1\n# Define the lower and upper bounds for outlier detection\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\n# Remove outliers from the TotalAmount column\ndf_segmentation = df_segmentation[(df_segmentation['TotalAmount'] >= lower_bound) & (df_segmentation['TotalAmount'] <= upper_bound)]\n# Display the updated shape of the DataFrame after removing outliers\nprint(\"Shape of DataFrame after removing outliers:\", df_segmentation.shape) \n```", "```py\n# Step 8: Data Scaling using Standardization\nfrom sklearn.preprocessing import StandardScaler\n# Initialize the StandardScaler\nscaler = StandardScaler()\n# Scale the dataset (TotalAmount and TotalTransactions)\ndf_scaled = scaler.fit_transform(df_segmentation)\n# Convert the scaled array back to a DataFrame\ndf_scaled = pd.DataFrame(df_scaled, columns=df_segmentation.columns)\n# Display the first few rows of the scaled DataFrame\nprint(\"Scaled DataFrame:\")\nprint(df_scaled.head()) \n```", "```py\n Scaled DataFrame:  \n       TotalAmount  TotalTransactions  \n1.099421           0.425738  \n1.051512          -0.745491  \n-0.666340          -0.745491  \n1.954997           1.987377  \n-0.962557          -0.745491 \n```", "```py\n    from sklearn.cluster import KMeans import matplotlib.pyplot as plt  \n\n    # Create a list to store the WCSS values for different numbers of clusters wcss = []  \n\n    # Try different values of K (number of clusters) from 1 to a reasonable maximum for k in range(1, 11):     kmeans = KMeans(n_clusters=k, random_state=42)     kmeans.fit(df_scaled)     wcss.append(kmeans.inertia_)  # inertia_ contains the WCSS value   \n    # Plot the Elbow Method graph plt.figure(figsize=(8, 5))\n    plt.plot(range(1, 11), wcss, marker='o')\n    plt.xlabel('Number of Clusters (K)')\n    plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n    plt.title('Elbow Method')\n    plt.grid(True)\n    plt.show() \n    ```", "```py\n    from sklearn.metrics import silhouette_score\n    # Create a list to store the Silhouette Scores for different numbers of clusters\n    silhouette_scores = []\n    # Try different values of K (number of clusters) from 2 to a reasonable maximum\n    for k in range(2, 11):\n        kmeans = KMeans(n_clusters=k, random_state=42)\n        kmeans.fit(df_scaled)\n        silhouette_scores.append(silhouette_score(df_scaled, kmeans.labels_))\n    # Plot the Silhouette Scores\n    plt.figure(figsize=(8, 5))\n    plt.plot(range(2, 11), silhouette_scores, marker='o')\n    plt.xlabel('Number of Clusters (K)')\n    plt.ylabel('Average Silhouette Score')\n    plt.title('Silhouette Score')\n    plt.grid(True)\n    plt.show() \n    ```", "```py\nfrom sklearn_extra.cluster import KMeansExtra\n# Calculate Gap Statistics\nfrom sklearn_extra.cluster import KMeansExtra\nkmeans_gap = KMeansExtra(\n    n_clusters_max=10,\n    random_state=42,\n    n_init=10,\n    max_iter=300,\n    metric=\"gap\"\n)\nkmeans_gap.fit(df_scaled)\n# Plot Gap Statistics\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, 11), kmeans_gap.gap_values_, marker='o')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Gap Value')\nplt.title('Gap Statistics')\nplt.grid(True)\nplt.show() \n```", "```py\nfrom sklearn.cluster import KMeans\n# Number of clusters\nnum_clusters = 6\n# Create the KMeans clustering model\nkmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n# Fit the model to the scaled data\nkmeans_model.fit(df_scaled)\n# Add the cluster labels to the DataFrame\ndf_segmentation['Cluster'] = kmeans_model.labels_\n# Display the first few rows of the DataFrame with cluster assignments\nprint(\"Customer Segmentation DataFrame with Cluster Assignments:\")\nprint(df_segmentation.head()) \n```", "```py\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Scatter plot with cluster assignments\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='TotalAmount', y='TotalTransactions', hue='Cluster', data=df_segmentation, palette='viridis', s=50)\nplt.xlabel('Total Amount')\nplt.ylabel('Total Transactions')\nplt.title('Customer Segmentation by K-Means Clustering')\nplt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.show() \n```", "```py\nimport pandas as pd\n# Load the data from the CSV file using Latin1 encoding\ndata_url = \"../Datasets/data.csv\"\ndf = pd.read_csv(data_url, encoding='latin1')\n# Display basic information about the dataset\nprint(df.info())\n# Display the first few rows of the dataset\nprint(df.head()) \n```", "```py\n# Drop rows with missing 'Description' values\ndf = df.dropna(subset=['Description'])\n# Display basic statistics of numerical columns\nprint(df.describe())\n# Display unique values in the 'Description' column\nprint(df['Description'].nunique())\n# Display the top 10 most common descriptions\nprint(df['Description'].value_counts().head(10)) \n```", "```py\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n# Download NLTK resources (only needed once)\nnltk.download('punkt')\nnltk.download('stopwords')\n# Load the data from the CSV file using Latin1 encoding\ndata_url = \"../Datasets/data.csv\"\ndf = pd.read_csv(data_url, encoding='latin1')\n# Drop rows with missing 'Description' values\ndf = df.dropna(subset=['Description'])\n# Drop duplicate 'Description' entries\ndf = df.drop_duplicates(subset=['Description'])\n# Text preprocessing and feature engineering\n# Convert descriptions to lowercase\ndf['Description'] = df['Description'].str.lower()\n# Tokenization and removal of punctuation\ndf['Description'] = df['Description'].apply(word_tokenize)\n# Remove stopwords\nstop_words = set(stopwords.words('english'))\ndf['Description'] = df['Description'].apply(lambda x: [word for word in x if word not in stop_words])\n# Convert tokenized descriptions back to strings\ndf['Description'] = df['Description'].apply(' '.join)\n# TF-IDF vectorization\ntfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust the number of features\ntfidf_matrix = tfidf_vectorizer.fit_transform(df['Description'])\n# Convert TF-IDF matrix to a DataFrame\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n# Display the TF-IDF DataFrame\nprint(tfidf_df.head()) \n```", "```py\nimport matplotlib.pyplot as plt\n# Calculate the sum of TF-IDF scores for each feature\nfeature_sum = tfidf_df.sum()\n# Get the top 100 features by frequency\ntop_features = feature_sum.nlargest(100)\n# Create a bar chart\nplt.figure(figsize=(10, 12))\ntop_features.plot(kind='barh')\nplt.xlabel('TF-IDF Score Sum')\nplt.ylabel('Words')\nplt.title('Top 100 Most Frequent Features from TF-IDF Matrix')\nplt.tight_layout()\nplt.show() \n```", "```py\ncolor_words = [\n    'black', 'white', 'grey', 'gray', 'red', 'blue', 'green', 'yellow',\n    'orange', 'purple', 'pink', 'brown', 'beige', 'gold', 'silver',\n    'indigo', 'violet', 'turquoise', 'teal', 'aqua', 'navy', 'olive',\n    'maroon', 'coral', 'plum', 'salmon', 'magenta', 'cyan', 'khaki',\n    'ivory', 'chartreuse', 'crimson', 'fuchsia', 'lavender', 'lime',\n    'tan', 'sienna', 'orchid', 'periwinkle', 'peach', 'thistle'\n] \n```", "```py\n# Remove color-related words from descriptions\ndf['Description'] = df['Description'].apply(lambda x: [word for word in x if word not in color_words]) \n```", "```py\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\n# Download NLTK resources (only needed once)\nnltk.download('punkt')\nnltk.download('stopwords')\n# List of color-related words\ncolor_words = [\n    'black', 'white', 'grey', 'gray', 'red', 'blue', 'green', 'yellow',\n    'orange', 'purple', 'pink', 'brown', 'beige', 'gold', 'silver',\n    'indigo', 'violet', 'turquoise', 'teal', 'aqua', 'navy', 'olive',\n    'maroon', 'coral', 'plum', 'salmon', 'magenta', 'cyan', 'khaki',\n    'ivory', 'chartreuse', 'crimson', 'fuchsia', 'lavender', 'lime',\n    'tan', 'sienna', 'orchid', 'periwinkle', 'peach', 'thistle'\n]\ndef preprocess_and_vectorize_data(data):\n    # Drop rows with missing 'Description' values\n    data = data.dropna(subset=['Description'])\n    # Drop duplicate 'Description' entries\n    data = data.drop_duplicates(subset=['Description'])\n    # Convert descriptions to lowercase\n    data['Description'] = data['Description'].str.lower()\n    # Tokenization and removal of punctuation\n    data['Description'] = data['Description'].apply(word_tokenize)\n    # Remove stopwords and color-related words\n    stop_words = set(stopwords.words('english'))\n    data['Description'] = data['Description'].apply(lambda x: [word for word in x if word not in stop_words and len(word) > 2 and word not in color_words])\n    # Convert tokenized descriptions back to strings\n    data['Description'] = data['Description'].apply(' '.join)\n    # TF-IDF vectorization\n    tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust the number of features\n    tfidf_matrix = tfidf_vectorizer.fit_transform(data['Description'])\n    # Convert TF-IDF matrix to a DataFrame\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n\n    return tfidf_df\n# Load the data from the CSV file using Latin1 encoding\ndata_url = \"../Datasets/data.csv\"\ndf = pd.read_csv(data_url, encoding='latin1')\n# Preprocess and vectorize the data\ntfidf_df = preprocess_and_vectorize_data(df)\n# Calculate the sum of TF-IDF scores for each feature\nfeature_sum = tfidf_df.sum()\n# Get the top 100 features by frequency\ntop_features = feature_sum.nlargest(100)\n# Create a bar chart\nplt.figure(figsize=(10, 12))\ntop_features.plot(kind='barh')\nplt.xlabel('TF-IDF Score Sum')\nplt.ylabel('Words')\nplt.title('Top 100 Most Frequent Features from TF-IDF Matrix')\nplt.tight_layout()\nplt.show() \n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(\n    max_features=1000,\n    max_df=0.8,\n    min_df=0.05,\n    ngram_range=(1, 2),\n    stop_words='english',\n    use_idf=True,\n    smooth_idf=True,\n    sublinear_tf=True\n) \n```", "```py\ndef preprocess_and_vectorize_data(data, min_df):\n    # Drop rows with missing 'Description' values\n    data = data.dropna(subset=['Description'])\n    # Drop duplicate 'Description' entries\n    data = data.drop_duplicates(subset=['Description'])\n    # Convert descriptions to lowercase\n    data['Description'] = data['Description'].str.lower()\n    # Tokenization and removal of punctuation\n    data['Description'] = data['Description'].apply(word_tokenize)\n    # Remove stopwords and color-related words\n    stop_words = set(stopwords.words('english'))\n    data['Description'] = data['Description'].apply(lambda x: [word for word in x if word not in stop_words and len(word) > 2 and word not in color_words])\n    # Convert tokenized descriptions back to strings\n    data['Description'] = data['Description'].apply(' '.join)\n    # TF-IDF vectorization\n    tfidf_vectorizer = TfidfVectorizer(min_df=min_df)\n    tfidf_matrix = tfidf_vectorizer.fit_transform(data['Description'])\n    # Convert TF-IDF matrix to a DataFrame\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n\n    return tfidf_df\n# Load the data from the CSV file using Latin1 encoding\ndata_url = \"../Datasets/data.csv\"\ndf = pd.read_csv(data_url, encoding='latin1')\n# Different values for min_df\nmin_df_values = [5, 10, 20, 30]\n# Store silhouette scores\nsilhouette_scores = []\n# Perform clustering and calculate silhouette scores for different min_df values\nfor min_df in min_df_values:\n    # Preprocess and vectorize the data\n    tfidf_df = preprocess_and_vectorize_data(df, min_df)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=10, random_state=42)\n    cluster_labels = kmeans.fit_predict(tfidf_df)\n\n    # Calculate silhouette score\n    silhouette_scores.append(silhouette_score(tfidf_df, cluster_labels))\n# Visualize the silhouette scores\nplt.plot(min_df_values, silhouette_scores, marker='o')\nplt.xlabel('min_df')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score vs. min_df')\nplt.xticks(min_df_values)\nplt.grid()\nplt.show() \n```", "```py\n# Different values for min_df and num_clusters\nmin_df_values = [5, 10, 20, 30]\nnum_clusters = [3, 4, 5, 6, 7]\n# Store silhouette scores for each combination\nsilhouette_scores = []\n# Perform clustering and calculate silhouette scores for different min_df and num_clusters combinations\nfor min_df, n_clusters in itertools.product(min_df_values, num_clusters):\n    # Preprocess and vectorize the data\n    tfidf_df = preprocess_and_vectorize_data(df, min_df)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(tfidf_df)\n\n    # Calculate silhouette score\n    silhouette_scores.append((min_df, n_clusters, silhouette_score(tfidf_df, cluster_labels)))\n# Convert silhouette scores to a DataFrame for easier visualization\nsilhouette_scores_df = pd.DataFrame(silhouette_scores, columns=['min_df', 'num_clusters', 'silhouette_score'])\n# Plot the silhouette scores\nplt.figure(figsize=(10, 6))\nfor min_df in min_df_values:\n    subset_df = silhouette_scores_df[silhouette_scores_df['min_df'] == min_df]\n    plt.plot(subset_df['num_clusters'], subset_df['silhouette_score'], marker='o', label=f'min_df = {min_df}')\n\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score vs. Number of Clusters for Different min_df Values')\nplt.legend()\nplt.grid()\nplt.show() \n```", "```py\n# ...\n# Store silhouette scores for each combination\nsilhouette_scores = []\n# Perform clustering and calculate silhouette scores for different min_df and num_clusters combinations\nfor min_df, n_clusters in itertools.product(min_df_values, num_clusters):\n    # Preprocess and vectorize the data\n    tfidf_df = preprocess_and_vectorize_data(df, min_df)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(tfidf_df)\n\n    # Calculate silhouette score\n    silhouette_scores.append((min_df, n_clusters, silhouette_score(tfidf_df, cluster_labels)))\n# Convert silhouette scores to a DataFrame for easier visualization\nsilhouette_scores_df = pd.DataFrame(silhouette_scores, columns=['min_df', 'num_clusters', 'silhouette_score'])\n# Plot the silhouette scores\nplt.figure(figsize=(10, 6))\nfor min_df in min_df_values:\n    subset_df = silhouette_scores_df[silhouette_scores_df['min_df'] == min_df]\n\n    # Get the number of features for the current min_df value\n    num_features = preprocess_and_vectorize_data(df, min_df).shape[1]\n\n    plt.plot(subset_df['num_clusters'], subset_df['silhouette_score'], marker='o', label=f'min_df = {min_df}, num_features = {num_features}')\n\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score vs. Number of Clusters for Different min_df Values')\nplt.legend()\nplt.grid()\nplt.show() \n```", "```py\n# ...\n# Chosen min_df and num_clusters values\nmin_df_value = 20\nnum_clusters_values = [3, 4, 5, 6]\n# Store silhouette scores and number of products per cluster\nresults = []\n# Perform clustering and calculate silhouette scores for chosen min_df and num_clusters values\nfor n_clusters in num_clusters_values:\n    # Preprocess and vectorize the data\n    tfidf_df = preprocess_and_vectorize_data(df, min_df_value)\n\n    # Perform KMeans clustering\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    cluster_labels = kmeans.fit_predict(tfidf_df)\n\n    # Calculate silhouette score\n    silhouette = silhouette_score(tfidf_df, cluster_labels)\n\n    # Count number of products in each cluster\n    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n\n    results.append((n_clusters, silhouette, cluster_counts))\n# Convert results to a DataFrame for easier visualization\nresults_df = pd.DataFrame(results, columns=['num_clusters', 'silhouette_score', 'cluster_counts'])\n# Plot the silhouette scores\nplt.figure(figsize=(15, 6))\n# Silhouette Score plot\nplt.subplot(1, 2, 1)\nplt.plot(results_df['num_clusters'], results_df['silhouette_score'], marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Score vs. Number of Clusters')\nplt.grid()\n# Products per Cluster plot\nplt.subplot(1, 2, 2)\nfor n_clusters, cluster_counts in results_df[['num_clusters', 'cluster_counts']].values:\n    plt.plot(range(1, n_clusters + 1), cluster_counts, marker='o', label=f'num_clusters = {n_clusters}')\nplt.xlabel('Cluster')\nplt.ylabel('Number of Products')\nplt.title('Number of Products per Cluster')\nplt.legend()\nplt.grid()\nplt.tight_layout()\nplt.show() \n```", "```py\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D  # Required for 3D plotting\n# Selected min_df and num_clusters values\nmin_df_value = 20\nnum_clusters = 5\n# Preprocess data and perform clustering\ntfidf_matrix, tfidf_vectorizer = preprocess_and_vectorize_data(df['Description'], min_df_value)\nkmeans = KMeans(n_clusters=num_clusters, random_state=42)\ncluster_labels = kmeans.fit_predict(tfidf_matrix)\n# Apply PCA with 3 components\npca = PCA(n_components=3)\npca_result = pca.fit_transform(tfidf_matrix.toarray())\n# Create a DataFrame for PCA results\npca_df = pd.DataFrame(data=pca_result, columns=['PC1', 'PC2', 'PC3'])\npca_df['Cluster'] = cluster_labels\n# Seaborn color palette for cluster colors\ncolor_palette = sns.color_palette(\"Set1\", n_colors=num_clusters)\n# Plot 3D scatter plot\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nfor cluster_id in range(num_clusters):\n    cluster_points = pca_df[pca_df['Cluster'] == cluster_id]\n    ax.scatter(cluster_points['PC1'], cluster_points['PC2'], cluster_points['PC3'], color=color_palette[cluster_id], label=f'Cluster {cluster_id}')\nax.set_xlabel('PC1')\nax.set_ylabel('PC2')\nax.set_zlabel('PC3')\nax.set_title('Product Clusters in 3D')\nax.legend()\nplt.show() \n```", "```py\nfrom wordcloud import WordCloud\n# Create a DataFrame for clustering results\nclustering_results = pd.DataFrame(data={'Description': df['Description'], 'Cluster': cluster_labels})\n# Function to generate word clouds for each cluster\ndef generate_wordclouds(dataframe, num_clusters):\n    for cluster_id in range(num_clusters):\n        cluster_data = dataframe[dataframe['Cluster'] == cluster_id]\n        cluster_text = ' '.join(cluster_data['Description'])\n\n        # Create a WordCloud object\n        wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(cluster_text)\n\n        # Plot the WordCloud\n        plt.figure(figsize=(10, 6))\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.title(f'Cluster {cluster_id} - Most Frequent Words')\n        plt.axis('off')\n        plt.show()\n# Generate word clouds for each cluster\ngenerate_wordclouds(clustering_results, num_clusters) \n```", "```py\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n# Sample data\ndata = {\n'Product': ['Red Shirt', 'Blue Jeans', 'Green Hat', 'Black Shoes'],\n'Category': ['Clothing', 'Clothing', 'Accessories', 'Footwear']\n}\ndf = pd.DataFrame(data)\n# Feature extraction\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['Product'])\ny = df['Category']\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Train the model\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n# Predict and evaluate\ny_pred = model.predict(X_test)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred)}') \n```", "```py\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Sample data\ndata = {\n    'Invoice': ['Invoice for Red Shirt', 'Invoice for Blue Jeans', 'Invoice for Green Hat', 'Invoice for Black Shoes'],\n    'Category': ['Clothing', 'Clothing', 'Accessories', 'Footwear']\n}\ndf = pd.DataFrame(data)\n# Feature extraction\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['Invoice'])\ny = df['Category']\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Model training\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\n# Model evaluation\ny_pred = model.predict(X_test)\nprint(classification_report(y_test, y_pred))\n# Confusion matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n# Hyperparameter tuning\nparam_grid = {'alpha': [0.1, 0.5, 1.0]}\ngrid_search = GridSearchCV(MultinomialNB(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(f'Best parameters: {grid_search.best_params_}') \n```"]