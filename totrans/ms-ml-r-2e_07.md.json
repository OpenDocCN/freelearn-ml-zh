["```py\n    > sigmoid = function(x) {\n     1 / ( 1 + exp(-x) )\n     }\n\n```", "```py\n > x <- seq(-5, 5, .1) > plot(sigmoid(x))\n\n```", "```py\n > library(ggplot2)\n > s <- sigmoid(x)\n > t <- tanh(x)\n > z <- data.frame(cbind(x, s, t))\n > ggplot(z, aes(x)) +\n geom_line(aes(y = s, color = \"sigmoid\")) +\n geom_line(aes(y = t, color = \"tanh\")) \n\n```", "```py\n > library(caret)\n > library(MASS)\n > library(neuralnet)\n > library(vcd) \n\n```", "```py\n    > data(shuttle)\n    > str(shuttle)\n    'data.frame':256 obs. of  7 variables:\n     $ stability: Factor w/ 2 levepicels \"stab\",\"xstab\": 2 2 2 2 2 2 2        \n       2 2 2 ...\n     $ error    : Factor w/ 4 levels \"LX\",\"MM\",\"SS\",..: 1 1 1 1 1 1 1 1        \n       1 1 ...\n     $ sign     : Factor w/ 2 levels \"nn\",\"pp\": 2 2 2 2 2 2 1 1 1 1 ...\n     $ wind     : Factor w/ 2 levels \"head\",\"tail\": 1 1 1 2 2 2 1 1 1 2        \n       ...\n     $ magn     : Factor w/ 4 levels \"Light\",\"Medium\",..: 1 2 4 1 2 4 1        \n       2 4 1 ...\n     $ vis      : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 1 1 1 1 1 1 1        \n       ...\n     $ use      : Factor w/ 2 levels \"auto\",\"noauto\": 1 1 1 1 1 1 1 1 1        \n       1 ...\n\n```", "```py\n    > table(shuttle$use)\n    auto noauto \n       145    111\n\n```", "```py\n    > table1 <- structable(wind + magn ~ use, shuttle)\n    > table1\n    wind  head                    tail \n           magn Light Medium Out Strong Light Medium Out Strong\n    use \n    auto           19     19  16     18    19     19  16     19\n    noauto         13     13  16     14    13     13  16     13\n\n```", "```py\n    > mosaic(table1, shading = T)\n\n```", "```py\n    > mosaic(use ~ error + vis, shuttle)\n\n```", "```py\n    > table(shuttle$use, shuttle$stability)\n             stab xstab\n      auto     81    64\n      noauto   47    64\n    > prop.table(table(shuttle$use, shuttle$stability))\n                  stab     xstab\n      auto   0.3164062 0.2500000\n      noauto 0.1835938 0.2500000\n\n```", "```py\n    > chisq.test(shuttle$use, shuttle$stability)\n    Pearson's Chi-squared test with Yates' continuity\n    correction\n    data:  shuttle$use and shuttle$stability\n    X-squared = 4.0718, df = 1, p-value = 0.0436\n\n```", "```py\n    > dummies <- dummyVars(use ~ .,shuttle, fullRank = T)\n    > dummies\n    Dummy Variable Object\n    Formula: use ~ .\n    7 variables, 7 factors\n    Variables and levels will be separated by '.'\n    A full rank encoding is used\n\n```", "```py\n    > shuttle.2 = as.data.frame(predict(dummies, newdata=shuttle))\n\n    > names(shuttle.2)\n    [1] \"stability.xstab\" \"error.MM\"        \"error.SS\" \n    [4] \"error.XL\"        \"sign.pp\"         \"wind.tail\" \n    [7] \"magn.Medium\"     \"magn.Out\"        \"magn.Strong\" \n    [10] \"vis.yes\" \n\n    > head(shuttle.2)\n      stability.xstab error.MM error.SS error.XL sign.pp wind.tail\n    1               1        0        0        0       1         0\n    2               1        0        0        0       1         0\n    3               1        0        0        0       1         0\n    4               1        0        0        0       1         1\n    5               1        0        0        0       1         1\n    6               1        0        0        0       1         1\n      magn.Medium magn.Out magn.Strong vis.yes\n    1           0        0           0       0\n    2           1        0           0       0\n    3           0        0           1       0\n    4           0        0           0       0\n    5           1        0           0       0\n    6           0        0           1       0\n\n```", "```py\n    > shuttle.2$use <- ifelse(shuttle$use == \"auto\", 1, 0)\n    > table(shuttle.2$use)\n    0   1 \n    111 145\n\n```", "```py\n    > set.seed(123)\n    > trainIndex <- createDataPartition(shuttle.2$use, p = .7, list =       \n       FALSE)\n\n```", "```py\n    > shuttleTrain <- shuttle.2[trainIndex, ]\n    > shuttleTest  <- shuttle.2[-trainIndex, ]\n\n```", "```py\n    > n <- names(shuttleTrain)\n    > form <- as.formula(paste(\"use ~\", paste(n[!n %in% \"use\"], \n      collapse = \" + \")))\n    > form\n    use ~ stability.xstab + error.MM + error.SS + error.XL + sign.pp +       \n      wind.tail \n       + magn.Medium + magn.Out + magn.Strong + vis.yes\n\n```", "```py\n    > fit <- neuralnet(form, data = shuttleTrain, err.fct = \"ce\", \n      linear.output = FALSE)\n\n```", "```py\n    > fit$result.matrix\n      1\n    error                         0.009928587504\n    reached.threshold             0.009905188403\n    steps                       660.000000000000\n    Intercept.to.1layhid1        -4.392654985479\n    stability.xstab.to.1layhid1   1.957595172393\n    error.MM.to.1layhid1         -1.596634090134\n    error.SS.to.1layhid1         -2.519372079568\n    error.XL.to.1layhid1         -0.371734253789\n    sign.pp.to.1layhid1          -0.863963659357\n    wind.tail.to.1layhid1         0.102077456260\n    magn.Medium.to.1layhid1      -0.018170137582\n    magn.Out.to.1layhid1          1.886928834123\n    magn.Strong.to.1layhid1       0.140129588700\n    vis.yes.to.1layhid1           6.209014123244\n    Intercept.to.use             30.721652703205\n    1layhid.1.to.use            -65.084168998463\n\n```", "```py\n    > head(fit$generalized.weights[[1]])\n          [,1]             [,2]         [,3]         [,4] \n 1 -4.374825405  3.568151106  5.630282059 0.8307501368 \n 2 -4.301565756  3.508399808  5.535998871 0.8168386187 \n 6 -5.466577583  4.458595039  7.035337605 1.0380665866 \n 9 -10.595727733 8.641980909 13.636415225 2.0120579565 \n 10 -10.270199330 8.376476707 13.217468969 1.9502422861 \n 11 -10.117466745 8.251906491 13.020906259 1.9212393878\n\n```", "```py\n    > plot(fit)\n\n```", "```py\n    > par(mfrow = c(1, 2))\n    > gwplot(fit, selected.covariate = \"vis.yes\")\n    > gwplot(fit, selected.covariate = \"wind.tail\")\n\n```", "```py\n    > resultsTrain <- compute(fit, shuttleTrain[, 1:10])\n    > predTrain <- resultsTrain$net.result\n\n```", "```py\n    > predTrain <- ifelse(predTrain >= 0.5, 1, 0) \n    > table(predTrain, shuttleTrain$use)  \n    predTrain  0  1\n            0 81  0\n            1  0 99\n\n```", "```py\n > resultsTest <- compute(fit, shuttleTest[,1:10]) > predTest <- resultsTest$net.result > predTest <- ifelse(predTest >= 0.5, 1, 0) > table(predTest, shuttleTest$use)\n predTest  0  1\n 0 29  0\n 1  1 46 \n\n```", "```py\n    > which(predTest == 1 & shuttleTest$use == 0)\n    [1] 62\n\n```", "```py\n # The following two commands remove any previously installed H2O      \n      packages for \n     R.\n if (\"package:h2o\" %in% search()) { detach(\"package:h2o\",     \n      unload=TRUE) }\n if (\"h2o\" %in% rownames(installed.packages())) {     \n      remove.packages(\"h2o\") }\n\n # Next, we download packages that H2O depends on.\n if (! (\"methods\" %in% rownames(installed.packages()))) {     \n    install.packages(\"methods\") }\n if (! (\"statmod\" %in% rownames(installed.packages()))) {   \n    install.packages(\"statmod\") }\n if (! (\"stats\" %in% rownames(installed.packages()))) { \n    install.packages(\"stats\") }\n if (! (\"graphics\" %in% rownames(installed.packages()))) { \n    install.packages(\"graphics\") }\n if (! (\"RCurl\" %in% rownames(installed.packages()))) {  \n    install.packages(\"RCurl\") }\n if (! (\"jsonlite\" %in% rownames(installed.packages()))) { \n    install.packages(\"jsonlite\") }\n if (! (\"tools\" %in% rownames(installed.packages()))) { \n    install.packages(\"tools\") }\n if (! (\"utils\" %in% rownames(installed.packages()))) { \n    install.packages(\"utils\") }\n\n # Now we download, install and initialize the H2O package for R.\n install.packages(\"h2o\", type=\"source\", repos=(c(\"http://h2o-   \n    release.s3.amazonaws.com/h2o/rel-tverberg/5/R\")))\n\n```", "```py\n > library(h2o)\n    > path <- \"C:/.../bank_DL.csv\" \n\n```", "```py\n    > localH2O = h2o.init(nthreads = -1)\n\n```", "```py\n    > bank <- h2o.uploadFile(path = path)\n      |=========================================================| 100%\n\n```", "```py\n > class(bank)\n [1] \"H2OFrame\" \n\n```", "```py\n > str(bank)\n Class 'H2OFrame' <environment: 0x0000000032d02e80> \n - attr(*, \"op\")= chr \"Parse\"\n - attr(*, \"id\")= chr \"bank_DL_sid_95ad_2\"\n - attr(*, \"eval\")= logi FALSE\n - attr(*, \"nrow\")= int 4521\n - attr(*, \"ncol\")= int 64\n - attr(*, \"types\")=List of 64 \n\n```", "```py\n > h2o.table(bank$y)\n y Count\n 1 no  4000\n 2 yes  521\n [2 rows x 2 columns]\n\n```", "```py\n    > rand <- h2o.runif(bank, seed = 123)\n\n```", "```py\n    > train <- bank[rand <= 0.7, ]\n    > train <- h2o.assign(train, key = \"train\")\n    > test <- bank[rand > 0.7, ]\n    > test <- h2o.assign(test, key = \"test\")\n\n```", "```py\n > h2o.table(train[, 64])\n y Count\n 1  no  2783\n 2 yes   396\n [2 rows x 2 columns] \n > h2o.table(test[, 64])\n y Count\n 1  no  1217\n 2 yes   125\n [2 rows x 2 columns]\n\n```", "```py\n    > args(h2o.deeplearning)\n\n```", "```py\n > hyper_params <- list(\n activation = c(\"Tanh\", \"TanhWithDropout\"),\n hidden = list(c(20,20),c(30, 30),c(30, 30, 30)),\n input_dropout_ratio = c(0, 0.05),\n rate = c(0.01, 0.25)\n )\n\n```", "```py\n > search_criteria = list(\n strategy = \"RandomDiscrete\", max_runtime_secs = 420,\n max_models = 100, seed = 123, stopping_rounds = 5,\n stopping_tolerance = 0.01\n )\n\n```", "```py\n > randomSearch <- h2o.grid(\n algorithm = \"deeplearning\",\n grid_id = \"randomSearch\",\n training_frame = train,\n validation_frame = test, \n x = 1:63, \n y = 64,\n epochs = 1,\n stopping_metric = \"misclassification\",\n hyper_params = hyper_params,\n search_criteria = search_criteria\n )\n |===================================================================| 100% \n\n```", "```py\n > grid <- h2o.getGrid(\"randomSearch\",sort_by = \"auc\", decreasing = \n       FALSE)\n > grid\n H2O Grid Details\n ================\n\n Grid ID: randomSearch \n Used hyper parameters: \n - activation \n - hidden \n - input_dropout_ratio \n - rate \n Number of models: 71 \n    Number of failed models: 0 \n\n    Hyper-Parameter Search Summary: ordered by decreasing auc\n           activation       hidden input_dropout_ratio rate\n    1 TanhWithDropout [30, 30, 30]                0.05 0.25\n    2 TanhWithDropout [20, 20]                    0.05 0.01\n    3 TanhWithDropout [30, 30, 30]                0.05 0.25\n    4 TanhWithDropout [40, 40]                    0.05 0.01\n    5 TanhWithDropout [30, 30, 30]                0.0  0.25\n                  model_ids                 auc\n    1 randomSearch_model_57  0.8636778964667214\n    2 randomSearch_model_8   0.8623894823336072\n    3 randomSearch_model_10  0.856568611339359\n    4 randomSearch_model_39  0.8565258833196385\n    5 randomSearch_model_3   0.8544026294165982\n\n```", "```py\n > best_model <- h2o.getModel(grid@model_ids[[1]])\n > h2o.confusionMatrix(best_model, valid = T)\n Confusion Matrix (vertical: actual; across: predicted) for max f1 @ \n      threshold = 0.0953170555399435:\n no yes    Error      Rate\n no     1128  89 0.073131 = 89/1217\n yes      60  65 0.480000 =  60/125\n Totals 1188 154 0.111028 = 149/1342 \n\n```", "```py\n > dlmodel <- h2o.deeplearning(\n x = 1:63,\n y = 64, \n training_frame = train,\n hidden = c(30, 30, 30),\n epochs = 3,\n nfolds = 5,\n fold_assignment = \"Stratified\",\n balance_classes = T,\n activation = \"TanhWithDropout\",\n seed = 123,\n adaptive_rate = F, \n input_dropout_ratio = 0.05,\n stopping_metric = \"misclassification\",\n variable_importances = T\n ) \n\n```", "```py\n > dlmodel\n Model Details:\n ==============\n    AUC:  0.8571054599\n    Gini: 0.7142109198\n\n    Confusion Matrix (vertical: actual; across: predicted) for F1-optimal  \n      threshold:\n             no yes    Error       Rate\n    no     2492 291 0.104563 = 291/2783\n    yes     160 236 0.404040 =  160/396\n    Totals 2652 527 0.141869 = 451/3179 \n\n```", "```py\n > perf <- h2o.performance(dlmodel, test)\n > perf\n H2OBinomialMetrics: deeplearning\n MSE:                  0.07237450145\n RMSE:                 0.2690250945\n LogLoss:              0.2399027004\n Mean Per-Class Error: 0.2326113394\n AUC:                  0.8319605588\n Gini:                 0.6639211175\n\n Confusion Matrix (vertical: actual; across: predicted) for F1-\n      optimal \n    threshold:\n no yes    Error      Rate\n no 1050 167 0.137223 = 167/1217\n yes   41  84 0.328000 =  41/125\n Totals 1091 251 0.154993 = 208/1342\n\n Maximum Metrics: Maximum metrics at their respective thresholds\n metric                      threshold    value idx\n 1 max f1                       0.323529 0.446809  62\n 2 max f2                       0.297121 0.612245 166\n 3 max f0point5                 0.323529 0.372011  62\n 4 max accuracy                 0.342544 0.906110   0\n 5 max precision                0.323529 0.334661  62\n 6 max recall                   0.013764 1.000000 355\n 7 max specificity              0.342544 0.999178   0\n 8 max absolute_mcc             0.297121 0.411468 166\n 9 max min_per_class_accuracy   0.313356 0.799507 131\n 10 max mean_per_class_accuracy  0.285007 0.819730 176\n\n```", "```py\n > dlmodel@model$variable_importances\n Variable Importances: \n variable relative_importance scaled_importance percentage\n 1 duration                    1.000000          1.000000   0.147006\n 2 poutcome_success            0.806309          0.806309   0.118532\n 3 month_oct                   0.329299          0.329299   0.048409\n 4 month_mar                   0.223847          0.223847   0.032907\n 5 poutcome_failure            0.199272          0.199272   0.029294 \n\n```"]