- en: Chapter 7. Detecting and Recognizing Objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will introduce the concept of detecting and recognizing objects,
    which is one of the most common challenges in computer vision. You've come this
    far in the book, so at this stage, you're wondering how far are you from mounting
    a computer in your car that will give you information about cars and people surrounding
    you through the use of a camera. Well, You're not too far from your goal, actually.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will expand on the concept of object detection, which we
    initially explored when talking about recognizing faces, and adapt it to all sorts
    of real-life objects, not just faces.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection and recognition techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We made a distinction in [Chapter 5](part0043.xhtml#aid-190861 "Chapter 5. Detecting
    and Recognizing Faces"), *Detecting and Recognizing Faces*, which we''ll reiterate
    for clarity: detecting an object is the ability of a program to determine if a
    certain region of an image contains an unidentified object, and recognizing is
    the ability of a program to identify this object. Recognizing normally only occurs
    in areas of interest where an object has been detected, for example, we have attempted
    to recognize faces on the areas of an image that contained a face in the first
    place.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When it comes to recognizing and detecting objects, there are a number of techniques
    used in computer vision, which we''ll be examining:'
  prefs: []
  type: TYPE_NORMAL
- en: Histogram of Oriented Gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image pyramids
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sliding windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike feature detection algorithms, these are not mutually exclusive techniques,
    rather, they are complimentary. You can perform a **Histogram of Oriented Gradients**
    (**HOG**) while applying the sliding windows technique.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's take a look at HOG first and understand what it is.
  prefs: []
  type: TYPE_NORMAL
- en: HOG descriptors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HOG is a feature descriptor, so it belongs to the same family of algorithms,
    such as SIFT, SURF, and ORB.
  prefs: []
  type: TYPE_NORMAL
- en: It is used in image and video processing to detect objects. Its internal mechanism
    is really clever; an image is divided into portions and a gradient for each portion
    is calculated. We've observed a similar approach when we talked about face recognition
    through LBPH.
  prefs: []
  type: TYPE_NORMAL
- en: HOG, however, calculates histograms that are not based on color values, rather,
    they are based on gradients. As HOG is a feature descriptor, it is capable of
    delivering the type of information that is vital for feature matching and object
    detection/recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before diving into the technical details of how HOG works, let''s first take
    a look at how HOG *sees* the world; here is an image of a truck:'
  prefs: []
  type: TYPE_NORMAL
- en: '![HOG descriptors](img/image00231.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is its HOG version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![HOG descriptors](img/image00232.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You can easily recognize the wheels and the main structure of the vehicle. So,
    what is HOG *seeing*? First of all, you can see how the image is divided into
    cells; these are 16x16 pixels cells. Each cell contains a visual representation
    of the calculated gradients of color in eight directions (N, NW, W, SW, S, SE,
    E, and NE).
  prefs: []
  type: TYPE_NORMAL
- en: 'These eight values contained in each cell are the famous histograms. Therefore,
    a single cell gets a unique *signature*, which you can mentally visualize to be
    somewhat like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![HOG descriptors](img/image00233.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The extrapolation of histograms into descriptors is quite a complex process.
    First, local histograms for each cell are calculated. The cells are grouped into
    larger regions called blocks. These blocks can be made of any number of cells,
    but Dalal and Triggs found that 2x2 cell blocks yielded the best results when
    performing people detection. A block-wide vector is created so that it can be
    normalized, accounting for variations in illumination and shadowing (a single
    cell is too small a region to detect such variations). This improves the accuracy
    of detection as it reduces the illumination and shadowing difference between the
    sample and the block being examined.
  prefs: []
  type: TYPE_NORMAL
- en: Simply comparing cells in two images would not work unless the images are identical
    (both in terms of size and data).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main problems to resolve:'
  prefs: []
  type: TYPE_NORMAL
- en: Location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The scale issue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine, for example, if your sample was a detail (say, a bike) extrapolated
    from a larger image, and you're trying to compare the two pictures. You would
    not obtain the same gradient signatures and the detection would fail (even though
    the bike is in both pictures).
  prefs: []
  type: TYPE_NORMAL
- en: The location issue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we''ve resolved the scale problem, we have another obstacle in our path:
    a potentially detectable object can be anywhere in the image, so we need to scan
    the entire image in portions to make sure we can identify areas of interest, and
    within these areas, try to detect objects. Even if a sample image and object in
    the image are of identical size, there needs to be a way to instruct OpenCV to
    locate this object. So, the rest of the image is discarded and a comparison is
    made on potentially matching regions.'
  prefs: []
  type: TYPE_NORMAL
- en: To obviate these problems, we need to familiarize ourselves with the concepts
    of image pyramid and sliding windows.
  prefs: []
  type: TYPE_NORMAL
- en: Image pyramid
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many of the algorithms used in computer vision utilize a concept called **pyramid**.
  prefs: []
  type: TYPE_NORMAL
- en: 'An image pyramid is a multiscale representation of an image. This diagram should
    help you understand this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image pyramid](img/image00234.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A multiscale representation of an image, or an image pyramid, helps you resolve
    the problem of detecting objects at different scales. The importance of this concept
    is easily explained through real-life hard facts, such as it is extremely unlikely
    that an object will appear in an image at the exact scale it appeared in our sample
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, you will learn that object classifiers (utilities that allow you to
    detect objects in OpenCV) need *training*, and this training is provided through
    image databases made up of positive matches and negative matches. Among the positives,
    it is again unlikely that the object we want to identify will appear in the same
    scale throughout the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We've got it, Joe. We need to take scale out of the equation, so now let's examine
    how an image pyramid is built.
  prefs: []
  type: TYPE_NORMAL
- en: 'An image pyramid is built through the following process:'
  prefs: []
  type: TYPE_NORMAL
- en: Take an image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Resize (smaller) the image using an arbitrary scale parameter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Smoothen the image (using Gaussian blurring).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the image is larger than an arbitrary minimum size, repeat the process from
    step 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Despite exploring image pyramids, scale ratio, and minimum sizes only at this
    stage of the book, you've already dealt with them. If you recall [Chapter 5](part0043.xhtml#aid-190861
    "Chapter 5. Detecting and Recognizing Faces"), *Detecting and Recognizing Faces*,
    we used the `detectMultiScale` method of the `CascadeClassifier` object.
  prefs: []
  type: TYPE_NORMAL
- en: Straight away, `detectMultiScale` doesn't sound so obscure anymore; in fact,
    it has become self-explanatory. The cascade classifier object attempts at detecting
    an object at different scales of an input image. The second piece of information
    that should become much clearer is the `scaleFactor` parameter of the `detectMultiScale()`
    method. This parameter represents the ratio at which the image will be resampled
    to a smaller size at each step of the pyramid.
  prefs: []
  type: TYPE_NORMAL
- en: The smaller the `scaleFactor` parameter, the more layers in the pyramid, and
    the slower and more computationally intensive the operation will be, although—to
    an extent—more accurate in results.
  prefs: []
  type: TYPE_NORMAL
- en: So, by now, you should have an understanding of what an image pyramid is, and
    why it is used in computer vision. Let's now move on to sliding windows.
  prefs: []
  type: TYPE_NORMAL
- en: Sliding windows
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '**Sliding windows** is a technique used in computer vision that consists of
    examining the shifting portions of an image (sliding windows) and operating detection
    on those using image pyramids. This is done so that an object can be detected
    at a multiscale level.'
  prefs: []
  type: TYPE_NORMAL
- en: Sliding windows resolves location issues by scanning smaller regions of a larger
    image, and then repeating the scanning on different scales of the same image.
  prefs: []
  type: TYPE_NORMAL
- en: With this technique, each image is decomposed into portions, which allows discarding
    portions that are unlikely to contain objects, while the remaining portions are
    classified.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is one problem that emerges with this approach, though: **overlapping
    regions**.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's expand a little bit on this concept to clarify the nature of the problem.
    Say, you're operating face detection on an image and are using sliding windows.
  prefs: []
  type: TYPE_NORMAL
- en: Each window slides off a few pixels at a time, which means that a sliding window
    happens to be a positive match for the same face in four different positions.
    Naturally, we don't want to report four matches, rather only one; furthermore,
    we're not interested in the portion of the image with a good score, but simply
    in the portion with the highest score.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s where non-maximum suppression comes into play: given a set of overlapping
    regions, we can suppress all the regions that are not classified with the maximum
    score.'
  prefs: []
  type: TYPE_NORMAL
- en: Non-maximum (or non-maxima) suppression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Non-maximum (or non-maxima) suppression is a technique that suppresses all the
    results that relate to the same area of an image, which are not the maximum score
    for a particular area. This is because similarly colocated windows tend to have
    higher scores and overlapping areas are significant, but we are only interested
    in the window with the best result, and discarding overlapping windows with lower
    scores.
  prefs: []
  type: TYPE_NORMAL
- en: When examining an image with sliding windows, you want to make sure to retain
    the best window of a bunch of windows, all overlapping around the same subject.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, you determine that all the windows with more than a threshold, *x*,
    in common will be thrown into the non-maximum suppression operation.
  prefs: []
  type: TYPE_NORMAL
- en: This is quite complex, but it's also not the end of this process. Remember the
    image pyramid? We're scanning the image at smaller scales iteratively to make
    sure to detect objects in different scales.
  prefs: []
  type: TYPE_NORMAL
- en: This means that you will obtain a series of windows at different scales, then,
    compute the size of a window obtained in a smaller scale as if it were detected
    in the original scale, and, finally, throw this window into the original mix.
  prefs: []
  type: TYPE_NORMAL
- en: It does sound a bit complex. Thankfully, we're not the first to come across
    this problem, which has been resolved in several ways. The fastest algorithm in
    my experience was implemented by Dr. Tomasz Malisiewicz at [http://www.computervisionblog.com/2011/08/blazing-fast-nmsm-from-exemplar-svm.html](http://www.computervisionblog.com/2011/08/blazing-fast-nmsm-from-exemplar-svm.html).
    The example is in MATLAB, but in the application example, we will obviously use
    a Python version of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general approach behind non-maximum suppression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Once an image pyramid has been constructed, scan the image with the sliding
    window approach for object detection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect all the current windows that have returned a positive result (beyond
    a certain arbitrary threshold), and take a window, `W`, with the highest response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Eliminate all windows that overlap `W` significantly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Move on to the next window with the highest response and repeat the process
    for the current scale.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When this process is complete, move up the next scale in the image pyramid and
    repeat the preceding process. To make sure windows are correctly represented at
    the end of the entire non-maximum suppression process, be sure to compute the
    window size in relation to the original size of the image (for example, if you
    detect a window at 50 percent scale of the original size in the pyramid, the detected
    window will actually be four times the size in the original image).
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this process, you will have a set of maximum scored windows. Optionally,
    you can check for windows that are entirely contained in other windows (like we
    did for the people detection process at the beginning of the chapter) and eliminate
    those.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how do we determine the score of a window? We need a classification system
    that determines whether a certain feature is present or not and a confidence score
    for this classification. This is where **support vector machines** (**SVM**) comes
    into play.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector machines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Explaining in detail what an SVM is and does is beyond the scope of this book,
    but suffice it to say, SVM is an algorithm that—given labeled training data–enables
    the classification of this data by outputting an optimal *hyperplane*, which,
    in plain English, is the optimal plane that divides differently classified data.
    A visual representation will help you understand this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support vector machines](img/image00235.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Why is it so helpful in computer vision and object detection in particular?
    This is due to the fact that finding the optimal division line between pixels
    that belong to an object and those that don't is a vital component of object detection.
  prefs: []
  type: TYPE_NORMAL
- en: The SVM model has been around since the early 1960s; however, the current form
    of its implementation originates in a 1995 paper by Corinna Cortes and Vadimir
    Vapnik, which is available at [http://link.springer.com/article/10.1007/BF00994018](http://link.springer.com/article/10.1007/BF00994018).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good understanding of the concepts involved in object detection,
    we can start looking at a few examples. We will start from built-in functions
    and evolve into training our own custom object detectors.
  prefs: []
  type: TYPE_NORMAL
- en: People detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenCV comes with `HOGDescriptor` that performs people detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a pretty straightforward example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After the usual imports, we define two very simple functions: `is_inside` and
    `draw_person`, which perform two minimal tasks, namely, determining whether a
    rectangle is fully contained in another rectangle, and drawing rectangles around
    detected people.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then load the image and create `HOGDescriptor` through a very simple and
    self-explanatory code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: After this, we specify that `HOGDescriptor` will use a default people detector.
  prefs: []
  type: TYPE_NORMAL
- en: This is done through the `setSVMDetector()` method, which—after our introduction
    to SVM—sounds less obscure than it may have if we hadn't introduced SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we apply `detectMultiScale` on the loaded image. Interestingly, unlike
    all the face detection algorithms, we don't need to convert the original image
    to grayscale before applying any form of object detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The detection method will return an array of rectangles, which would be a good
    enough source of information for us to start drawing shapes on the image. If we
    did this, however, you would notice something strange: some of the rectangles
    are entirely contained in other rectangles. This clearly indicates an error in
    detection, and we can safely assume that a rectangle entirely inside another one
    can be discarded.'
  prefs: []
  type: TYPE_NORMAL
- en: This is precisely the reason why we defined an `is_inside` function, and why
    we iterate through the result of the detection to discard false positives.
  prefs: []
  type: TYPE_NORMAL
- en: If you run the script yourself, you will see rectangles around people in the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and training an object detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using built-in features makes it easy to come up with a quick prototype for
    an application, and we're all very grateful to the OpenCV developers for making
    great features, such as face detection or people detection readily available (truly,
    we are).
  prefs: []
  type: TYPE_NORMAL
- en: However, whether you are a hobbyist or a computer vision professional, it's
    unlikely that you will only deal with people and faces.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, if you're like me, you wonder how the people detector feature was
    created in the first place and if you can improve it. Furthermore, you may also
    wonder whether you can apply the same concepts to detect the most diverse type
    of objects, ranging from cars to goblins.
  prefs: []
  type: TYPE_NORMAL
- en: In an enterprise environment, you may have to deal with very specific detection,
    such as registration plates, book covers, or whatever your company may deal with.
  prefs: []
  type: TYPE_NORMAL
- en: So, the question is, how do we come up with our own classifiers?
  prefs: []
  type: TYPE_NORMAL
- en: The answer lies in SVM and bag-of-words technique.
  prefs: []
  type: TYPE_NORMAL
- en: We've already talked about HOG and SVM, so let's take a closer look at bag-of-words.
  prefs: []
  type: TYPE_NORMAL
- en: Bag-of-words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Bag-of-words** (**BOW**) is a concept that was not initially intended for
    computer vision, rather, we use an evolved version of this concept in the context
    of computer vision. So, let''s first talk about its basic version, which—as you
    may have guessed— originally belongs to the field of language analysis and information
    retrieval.'
  prefs: []
  type: TYPE_NORMAL
- en: 'BOW is the technique by which we assign a count weight to each word in a series
    of documents; we then rerepresent these documents with vectors that represent
    these set of counts. Let''s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Document 1**: `I like OpenCV and I like Python`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 2**: `I like C++ and Python`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document 3**: `I don''t like artichokes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These three documents allow us to build a dictionary (or codebook) with these
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We have eight entries. Let''s now rerepresent the original documents using
    eight-entry vectors, each vector containing all the words in the dictionary with
    values representing the count for each term in the document. The vector representation
    of the preceding three sentences is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This kind of representation of documents has many effective applications in
    the real world, such as spam filtering.
  prefs: []
  type: TYPE_NORMAL
- en: These vectors can be conceptualized as a histogram representation of documents
    or as a feature (the same way we extracted features from images in previous chapters),
    which can be used to train classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a grasp of the basic concept of BOW or **bag of visual words**
    (**BOVW**) in computer vision, let's see how this applies to the world of computer
    vision.
  prefs: []
  type: TYPE_NORMAL
- en: BOW in computer vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are by now familiar with the concept of image features. We've used feature
    extractors, such as SIFT, and SURF, to extract features from images so that we
    could match these features in another image.
  prefs: []
  type: TYPE_NORMAL
- en: We've also familiarized ourselves with the concept of codebook, and we know
    about SVM, a model that can be fed a set of features and utilizes complex algorithms
    to classify train data, and can predict the classification of new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the implementation of a BOW approach will involve the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Take a sample dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each image in the dataset, extract descriptors (with SIFT, SURF, and so
    on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add each descriptor to the BOW trainer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster the descriptors to *k* clusters (okay, this sounds obscure, but bear
    with me) whose centers (centroids) are our visual words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, we have a dictionary of visual words ready to be used. As you
    can imagine, a large dataset will help make our dictionary richer in visual words.
    Up to an extent, the more words, the better!
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, we are ready to test our classifier and attempt detection. The
    good news is that the process is very similar to the one outlined previously:
    given a test image, we can extract features and quantize them based on their distance
    to the nearest centroid to form a histogram.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on this, we can attempt to recognize visual words and locate them in
    the image. Here''s a visual representation of the BOW process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![BOW in computer vision](img/image00236.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is the point in the chapter when you have built an appetite for a practical
    example, and are rearing to code. However, before proceeding, I feel that a quick
    digression into the theory of the k-means clustering is necessary so that you
    can fully understand how visual words are created, and gain a better understanding
    of the process of object detection using BOW and SVM.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means clustering
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The k-means clustering is a method of vector quantization to perform data analysis.
    Given a dataset, *k* represents the number of clusters in which the dataset is
    going to be divided. The term "means" refers to the mathematical concept of mean,
    which is pretty basic, but for the sake of clarity, it's what people commonly
    refer to as average; when visually represented, the mean of a cluster is its **centroid**
    or the geometrical center of points in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Clustering** refers to the grouping of points in a dataset into clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the classes we will be using to perform object detection is called `BagOfWordsKMeansTrainer`;
    by now, you should able to deduce what the responsibility of this class is to
    create:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"`kmeans()` -based class to train a visual vocabulary using the bag-of-words
    approach"*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is as per the OpenCV documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a representation of a k-means clustering operation with five clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The k-means clustering](img/image00237.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: After this long theoretical introduction, we can look at an example, and start
    training our object detector.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting cars
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no virtual limit to the type of objects you can detect in your images
    and videos. However, to obtain an acceptable level of accuracy, you need a sufficiently
    large dataset, containing train images that are identical in size.
  prefs: []
  type: TYPE_NORMAL
- en: This would be a time consuming operation if we were to do it all by ourselves
    (which is entirely possible).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can avail of ready-made datasets; there are a number of them freely downloadable
    from various sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The University** **of Illinois**: [http://l2r.cs.uiuc.edu/~cogcomp/Data/Car/CarData.tar.gz](http://l2r.cs.uiuc.edu/~cogcomp/Data/Car/CarData.tar.gz)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stanford** **University**: [http://ai.stanford.edu/~jkrause/cars/car_dataset.html](http://ai.stanford.edu/~jkrause/cars/car_dataset.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that training images and test images are available in separate files.
  prefs: []
  type: TYPE_NORMAL
- en: I'll be using the UIUC dataset in my example, but feel free to explore the Internet
    for other types of datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: What did we just do?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is quite a lot to assimilate, so let''s go through what we''ve done:'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, our usual imports are followed by the declaration of the base
    path of our training images. This will come in handy to avoid rewriting the base
    path every time we process an image in a particular folder on our computer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After this, we declare a function, `path`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**More on the path function**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This function is a utility method: given the name of a class (in our case,
    we have two classes, `pos` and `neg`) and a numerical index, we return the full
    path to a particular testing image. Our car dataset contains images named in the
    following way: `pos-x.pgm` and `neg-x.pgm`, where `x` is a number.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Immediately, you will find the usefulness of this function when iterating through
    a range of numbers (say, 20), which will allow you to load all images from `pos-0.pgm`
    to `pos-20.pgm`, and the same goes for the negative class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next up, we''ll create two SIFT instances: one to extract keypoints, the other
    to extract features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Whenever you see SIFT involved, you can be pretty sure some feature matching
    algorithm will be involved too. In our case, we''ll create an instance for a FLANN
    matcher:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that currently, the `enum` values for FLANN are missing from the Python
    version of OpenCV 3, so, number `1`, which is passed as the algorithm parameter,
    represents the `FLANN_INDEX_KDTREE` algorithm. I suspect the final version will
    be `cv2.FLANN_INDEX_KDTREE`, which is a little more helpful. Make sure to check
    the `enum` values for the correct flags.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we mention the BOW trainer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This BOW trainer utilizes 40 clusters. After this, we''ll initialize the BOW
    extractor. This is the BOW class that will be fed a vocabulary of visual words
    and will try to detect them in the test image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To extract the SIFT features from an image, we build a utility method, which
    takes the path to the image, reads it in grayscale, and returns the descriptor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: At this stage, we have everything we need to start training the BOW trainer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s read eight images per class (eight positives and eight negatives) from
    our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To create the vocabulary of visual words, we''ll call the `cluster()` method
    on the trainer, which performs the k-means classification and returns the said
    vocabulary. We''ll assign this vocabulary to `BOWImgDescriptorExtractor` so that
    it can extract descriptors from test images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In line with other utility functions declared in this script, we''ll declare
    a function that takes the path to an image and returns the descriptor as computed
    by the BOW descriptor extractor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create two arrays to accommodate the train data and labels, and populate
    them with the descriptors generated by `BOWImgDescriptorExtractor`, associating
    labels to the positive and negative images we''re feeding (`1` stands for a positive
    match, `-1` for a negative):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s create an instance of an SVM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, train it by wrapping the train data and labels into the NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We're all set with a trained SVM; all that is left to do is to feed the SVM
    a couple of sample images and see how it behaves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first define another utility method to print the result of our `predict`
    method and return it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define two sample image paths and read them as the NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll pass these images to the trained SVM, and get the result of the prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Naturally, we're hoping that the car image will be detected as a car (result
    of `predict()` should be `1.0`), and that the other image will not (result should
    be `-1.0`), so we will only add text to the images if the result is the expected
    one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'At last, we''ll present the images on the screen, hoping to see the correct
    caption on each:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding operation produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What did we just do?](img/image00238.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'It also results in this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What did we just do?](img/image00239.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: SVM and sliding windows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having detected an object is an impressive achievement, but now we want to
    push this to the next level in these ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting multiple objects of the same kind in an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining the position of a detected object in an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To accomplish this, we will use the sliding windows approach. If it''s not
    already clear from the previous explanation of the concept of sliding windows,
    the rationale behind the adoption of this approach will become more apparent if
    we take a look at a diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![SVM and sliding windows](img/image00240.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Observe the movement of the block:'
  prefs: []
  type: TYPE_NORMAL
- en: We take a region of the image, classify it, and then move a predefined step
    size to the right-hand side. When we reach the rightmost end of the image, we'll
    reset the *x* coordinate to `0` and move down a step, and repeat the entire process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each step, we'll perform a classification with the SVM that was trained with
    BOW.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep a track of all the blocks that have *passed* the SVM predict test.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you've finished classifying the entire image, scale the image down and
    repeat the entire sliding windows process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Continue rescaling and classifying until you get to a minimum size.
  prefs: []
  type: TYPE_NORMAL
- en: This gives you the chance to detect objects in several regions of the image
    and at different scales.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage, you will have collected important information about the content
    of the image; however, there''s a problem: it''s most likely that you will end
    up with a number of overlapping blocks that give you a positive score. This means
    that your image may contain one object that gets detected four or five times,
    and if you were to report the result of the detection, your report would be quite
    inaccurate, so here''s where non-maximum suppression comes into play.'
  prefs: []
  type: TYPE_NORMAL
- en: Example – car detection in a scene
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are now ready to apply all the concepts we learned so far to a real-life
    example, and create a car detector application that scans an image and draws rectangles
    around cars.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s summarize the process before diving into the code:'
  prefs: []
  type: TYPE_NORMAL
- en: Obtain a train dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a BOW trainer and create a visual vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train an SVM with the vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attempt detection using sliding windows on an image pyramid of a test image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply non-maximum suppression to overlapping boxes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Output the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's also take a look at the structure of the project, as it is a bit more
    complex than the classic standalone script approach we've adopted until now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The project structure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The main program is in `car_sliding_windows.py`, and all the utilities are contained
    in the `car_detector` folder. As we're using Python 2.7, we'll need an `__init__.py`
    file in the folder for it to be detected as a module.
  prefs: []
  type: TYPE_NORMAL
- en: 'The four files in the `car_detector` module are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The SVM training model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The non-maximum suppression function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The image pyramid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sliding windows function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s examine them one by one, starting from the image pyramid:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This module contains two function definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: Resize takes an image and resizes it by a specified factor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pyramid takes an image and returns a resized version of it until the minimum
    constraints of width and height are reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You will notice that the image is not returned with the `return` keyword but
    with the `yield` keyword. This is because this function is a so-called generator.
    If you are not familiar with generators, take a look at [https://wiki.python.org/moin/Generators](https://wiki.python.org/moin/Generators).
  prefs: []
  type: TYPE_NORMAL
- en: This will allow us to obtain a resized image to process in our main program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up is the sliding windows function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, this is a generator. Although a bit deep-nested, this mechanism is very
    simple: given an image, return a window that moves of an arbitrary sized step
    from the left margin towards the right, until the entire width of the image is
    covered, then goes back to the left margin but down a step, covering the width
    of the image repeatedly until the bottom right corner of the image is reached.
    You can visualize this as the same pattern used for writing on a piece of paper:
    start from the left margin and reach the right margin, then move onto the next
    line from the left margin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last utility is non-maximum suppression, which looks like this (Malisiewicz/Rosebrock''s
    code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This function simply takes a list of rectangles and sorts them by their score.
    Starting from the box with the highest score, it eliminates all boxes that overlap
    beyond a certain threshold by calculating the area of intersection and determining
    whether it is greater than a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Examining detector.py
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now, let's examine the heart of this program, which is `detector.py`. This a
    bit long and complex; however, everything should appear much clearer given our
    newfound familiarity with the concepts of BOW, SVM, and feature detection/extraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Let's go through it. First, we'll import our usual modules, and then set a path
    for the training images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we''ll define a number of utility functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This function returns the path to an image given a base path and a class name.
    In our example, we're going to use the `neg-` and `pos-` class names, because
    this is what the training images are called (that is, `neg-1.pgm`). The last argument
    is an integer used to compose the final part of the image path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll define a utility function to obtain a FLANN matcher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Again, it's not that the integer, `1`, passed as an algorithm argument represents
    `FLANN_INDEX_KDTREE`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next two functions return the SIFT feature detectors/extractors and a BOW
    trainer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The next utility is a function used to return features from an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A SIFT detector detects features, while a SIFT extractor extracts and returns
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll also define a similar utility function to extract the BOW features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `main car_detector` function, we''ll first create the necessary object
    used to perform feature detection and extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll add features taken from training images to the trainer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: For each class, we'll add a positive image to the trainer and a negative image.
  prefs: []
  type: TYPE_NORMAL
- en: After this, we'll instruct the trainer to cluster the data into *k* groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The clustered data is now our vocabulary of visual words, and we can set the
    `BOWImgDescriptorExtractor` class'' vocabulary in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Associating training data with classes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'With a visual vocabulary ready, we can now associate train data with classes.
    In our case, we have two classes: `-1` for negative results and `1` for positive
    ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s populate two arrays, `traindata` and `trainlabels`, containing extracted
    features and their corresponding labels. Iterating through the dataset, we can
    quickly set this up with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that at each cycle, we'll add one positive and one negative
    image, and then populate the labels with a `1` and a `-1` value to keep the data
    synchronized with the labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Should you wish to train more classes, you could do that by following this
    pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: For example, you could train a detector to detect cars and people and perform
    detection on these in an image containing both cars and people.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we''ll train the SVM with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two parameters in particular that I''d like to focus your attention
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: '**C**: With this parameter, you could conceptualize the strictness or severity
    of the classifier. The higher the value, the less chances of misclassification,
    but the trade-off is that some positive results may not be detected. On the other
    hand, a low value may over-fit, so you risk getting false positives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel**: This parameter determines the nature of the classifier: `SVM_LINEAR`
    indicates a linear *hyperplane*, which, in practical terms, works very well for
    a binary classification (the test sample either belongs to a class or it doesn''t),
    while `SVM_RBF` (**radial basis function**) separates data using the Gaussian
    functions, which means that the data is split into several kernels defined by
    these functions. When training the SVM to classify for more than two classes,
    you will have to use RBF.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we'll pass the `traindata` and `trainlabels` arrays into the SVM `train`
    method, and return the SVM and BOW extractor object. This is because in our applications,
    we don't want to have to recreate the vocabulary every time, so we expose it for
    reuse.
  prefs: []
  type: TYPE_NORMAL
- en: Dude, where's my car?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are ready to test our car detector!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first create a simple program that loads an image, and then operates
    detection using the sliding windows and image pyramid techniques, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The notable part of the program is the function within the pyramid/sliding
    window loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Here, we extract the features of the **region of interest** (**ROI**), which
    corresponds to the current sliding window, and then we call `predict` on the extracted
    features. The `predict` method has an optional parameter, `flags`, which returns
    the score of the prediction (contained at the `[0][0]` value).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A word on the score of the prediction: the lower the value, the higher the
    confidence that the classified element really belongs to the class.'
  prefs: []
  type: TYPE_NORMAL
- en: So, we'll set an arbitrary threshold of `-1.0` for classified windows, and all
    windows with less than `-1.0` are going to be taken as good results. As you experiment
    with your SVMs, you may tweak this to your liking until you find a golden mean
    that assures best results.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we add the computed coordinates of the sliding window (meaning, we
    multiply the current coordinates by the scale of the current layer in the image
    pyramid so that it gets correctly represented in the final drawing) to the array
    of rectangles.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s one last operation we need to perform before drawing our final result:
    non-maximum suppression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We turn the rectangles array into a NumPy array (to allow certain kind of operations
    that are only possible with NumPy), and then apply NMS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we proceed with displaying all our results; for the sake of convenience,
    I''ve also printed the score obtained for all the remaining windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dude, where''s my car?](img/image00241.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This is a remarkably accurate result!
  prefs: []
  type: TYPE_NORMAL
- en: 'A final note on SVM: you don''t need to train a detector every time you want
    to use it, which would be extremely impractical. You can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: You can subsequently reload it with a load method and feed it test images or
    frames.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we talked about numerous object detection concepts, such as
    HOG, BOW, SVM, and some useful techniques, such as image pyramid, sliding windows,
    and non-maximum suppression.
  prefs: []
  type: TYPE_NORMAL
- en: We introduced the concept of machine learning and explored the various approaches
    used to train a custom detector, including how to create or obtain a training
    dataset and classify data. Finally, we put this knowledge to good use by creating
    a car detector from scratch and verifying its correct functioning.
  prefs: []
  type: TYPE_NORMAL
- en: All these concepts form the foundation of the next chapter, in which we will
    utilize object detection and classification techniques in the context of making
    videos, and learn how to track objects to retain information that can potentially
    be used for business or application purposes.
  prefs: []
  type: TYPE_NORMAL
