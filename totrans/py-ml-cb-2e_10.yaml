- en: Analyzing Image Content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Operating on images using OpenCV-Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting edges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram equalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting corners
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting SIFT feature points
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Star feature detector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating features using Visual Codebook and vector quantization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an image classifier using Extremely Random Forests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an object recognizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LightGBM for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To go through the recipes in this chapter, you need the following files (available
    on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '`operating_on_images.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`capri.jpg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`edge_detector.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chair.jpg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`histogram_equalizer.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sunrise.jpg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`corner_detector.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`box.png`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`feature_detector.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`table.jpg`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`star_detector.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`trainer.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`object_recognizer.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LightgbmClassifier.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Computer vision** is a field that studies how to process, analyze, and understand
    the contents of visual data. In image content analysis, we use a lot of computer
    vision algorithms to build our understanding of the objects in the image. Computer
    vision covers various aspects of image analysis, such as object recognition, shape
    analysis, pose estimation, 3D modeling, visual search, and so on. Humans are really
    good at identifying and recognizing things around them! The ultimate goal of computer
    vision is to accurately model the human vision system using computers.'
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision consists of various levels of analysis. In low-level vision,
    we deal with pixel-processing tasks, such as **edge detection**, **morphological
    processing**, and **optical flow**. In middle-level and high-level vision, we
    deal with things such as **object recognition**, **3D modeling**, **motion analysis**,
    and various other aspects of visual data. As we go higher, we tend to delve deeper
    into the conceptual aspects of our visual system and try to extract a description
    of visual data, based on activities and intentions. One thing to note is that
    higher levels tend to rely on the outputs of the lower levels for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the most common questions here is this: how is computer vision different
    than image processing? **Image processing** studies image transformations at the
    pixel level. Both the input and output of an image processing system are images.
    Some common examples are edge detection, **histogram equalization**, and **image
    compression**. Computer vision algorithms heavily rely on image processing algorithms
    to perform their duties. In computer vision, we deal with more complex things
    that include understanding the visual data at a conceptual level. The reason for
    this is that we want to construct meaningful descriptions of the objects in the
    images. The output of a computer vision system is an interpretation of the 3D
    scene in the given image. This interpretation can come in various forms, depending
    on the task at hand.'
  prefs: []
  type: TYPE_NORMAL
- en: Operating on images using OpenCV-Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will use a library called **Open Source Computer Vision
    Library** (**OpenCV**), to analyze images. OpenCV is the world's most popular
    library for computer vision. As it has been highly optimized for many different
    platforms, it has become the de facto standard in the industry. Before you proceed,
    make sure that you install the library with Python support. You can download and
    install OpenCV at [http://opencv.org](http://opencv.org). For detailed installation
    instructions on various operating systems, you can refer to the documentation
    section on the website.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will take a look at how to operate on images using OpenCV-Python.
    In this recipe, we will look at how to load and display an image. We will also
    look at how to crop, resize, and save an image to an output file.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can operate on images using OpenCV-Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `operating_on_images.py` file that is provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Specify the input image as the first argument to the file, and read it using
    the image read function. We will use the `forest.jpg` file that is provided to
    you, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the input image, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now crop this image. Extract the height and width of the input image,
    and then specify the boundaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Crop the image using NumPy-style slicing and display it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Resize the image to `1.3` times its original size and display it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous method will uniformly scale the image on both dimensions. Let''s
    assume that we want to skew the image based on specific output dimensions. We
    will use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the image to an output file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `waitKey()` function displays the images until you hit a key on the keyboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will run the code in a Terminal window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following four images on the screen (*Capri''s Faraglioni
    (Italy)*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7512e226-cd6e-4f44-8a58-f924d96d324e.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we learned how to operate on images using the OpenCV-Python
    library. The following tasks were performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading and displaying an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cropping an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resizing an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenCV is a free software library that was originally developed by Intel and
    the Nizhny Novgorod research center in Russia. Later, it was maintained by Willow
    Garage and is now maintained by Itseez. The programming language that's mainly
    used to develop with this library is C ++, but it is also possible to interface
    through C, Python, and Java.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Refer to the official documentation of the OpenCV library at [http://opencv.org](http://opencv.org)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to the OpenCV tutorials at [https://docs.opencv.org/2.4/opencv_tutorials.pdf](https://docs.opencv.org/2.4/opencv_tutorials.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting edges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Edge detection** is one of the most popular techniques in computer vision.
    It is used as a preprocessing step in many applications. With edge detection,
    you can mark points in a digital image where light intensity suddenly changes.
    The sudden changes in the properties of an image want to highlight important events
    or changes in the physical world of which the images are representations. These
    changes identify, for example, surface orientation discontinuities, depth discontinuities,
    and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use different edge detectors to detect
    edges in the input image.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can detect edges:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `edge_detector.py` file that is provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the input image. We will use `chair.jpg`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the height and width of the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The **Sobel filter** is a type of edge detector that uses a 3 x 3 kernel to
    detect horizontal and vertical edges separately:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the vertical Sobel detector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The **Laplacian edge detector** detects edges in both directions. We use it
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though Laplacian addresses the shortcomings of Sobel, the output is still
    very noisy. The **Canny edge detector** outperforms all of them because of the
    way it treats the problem. It is a multistage process, and it uses hysteresis
    to come up with clean edges:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Display all the output images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We will run the code in the terminal window using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following five images on the screen (*The ancient theatre
    of Siracusa (Italy)*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/308073ff-013c-4bb3-a8a5-cb2b1cac353e.png)'
  prefs: []
  type: TYPE_IMG
- en: At the top of the screenshot is the original image, the horizontal Sobel edge
    detector output, and the vertical Sobel edge detector output. Note how the detected
    lines tend to be vertical. This is due to the fact that it's a horizontal edge
    detector, and it tends to detect changes in this direction. At the bottom of the
    screenshot is the Laplacian edge detector output and the Canny edge detector,
    which detects all the edges nicely.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Sobel operator is a differential operator, which calculates an approximate
    value of the gradient of a function that represents the brightness of the image.
    At each point in the image, the Sobel operator can correspond to the gradient
    vector or to the norm of that vector. The algorithm that's used by the Sobel operator
    is based on the convolution of the image with a filter, separated and of integer
    value, applied both in the vertical and horizontal direction, and is therefore
    economical in terms of the calculation power required.
  prefs: []
  type: TYPE_NORMAL
- en: The Laplacian edge detector is part of the zero-crossing methods that look for
    points where the second-order derivative goes through zero, which is usually the
    Laplacian function or a differential expression of a non-linear function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Canny algorithm uses a multi-stage calculation method to find outlines of
    many of the types that are normally present in real images. To do this, the algorithm
    must identify and mark as many contours as possible in the image good location.
    Furthermore, the marked contours must be as close as possible to the real contours
    of the image. Finally, a given image contour must be marked only once, and if
    possible, the noise that's present in the image must not cause the detection of
    false contours.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Sobel Operator*: [http://www.tutorialspoint.com/dip/sobel_operator.htm](http://www.tutorialspoint.com/dip/sobel_operator.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Laplacian edge detector*: [http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm](http://homepages.inf.ed.ac.uk/rbf/HIPR2/log.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Canny edge detector*: [http://homepages.inf.ed.ac.uk/rbf/HIPR2/canny.htm](http://homepages.inf.ed.ac.uk/rbf/HIPR2/canny.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Most Common Edge Detectors* (from the University of Minnesota): [http://me.umn.edu/courses/me5286/vision/Notes/2015/ME5286-Lecture7.pdf](http://me.umn.edu/courses/me5286/vision/Notes/2015/ME5286-Lecture7.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Histogram equalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Histogram equalization** is the process of modifying the intensities of the
    image pixels to enhance the image''s contrast. The human eye likes contrast! This
    is the reason why almost all camera systems use histogram equalization to make
    images look nice.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The interesting thing is that the histogram equalization process is different
    for grayscale and color images. There's a catch when dealing with color images,
    and we'll see it in this recipe. Let's see how to do it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can perform histogram equalization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `histogram_equalizer.py` file that is provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the input image. We will use the `sunrise.jpg` image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the image into `grayscale` and display it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Equalize the histogram of the `grayscale` image and display it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenCV loads images in the `BGR` format by default, so let''s convert it from
    `BGR` into `YUV` first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Equalize the Y channel, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert it back into `BGR`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the input and output images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We will run the code in a terminal window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following four images on the screen (*the medieval city of
    Gubbio (Italy)*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e409691b-5e34-4d71-92a1-08eaa439ba53.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Histogram equalization is a digital image processing method with which you can
    calibrate the contrast using the image histogram. Histogram equalization increases
    the general contrast of many images, particularly when the usable image data is
    represented by very close intensity values. With this adaptation, intensities
    can be better distributed on the histogram. In this way, the areas with low local
    contrast obtain a greater contrast. The equalization of the histogram is achieved
    by spreading most of the values of frequent intensity.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To equalize the histogram of the color images, we need to follow a different
    procedure. Histogram equalization only applies to the intensity channel. An RGB
    image consists of three color channels, and we cannot apply the histogram equalization
    process on these channels separately. We need to separate the intensity information
    from the color information before we do anything. So, we convert it into a YUV
    colorspace first, equalize the Y channel, and then convert it back into RGB to
    get the output.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Contrast Enhancement* (from the Polytechnic University, Brooklyn): [http://eeweb.poly.edu/~yao/EL5123/lecture3_contrast_enhancement.pdf](http://eeweb.poly.edu/~yao/EL5123/lecture3_contrast_enhancement.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*YUV Colorspace*: [http://softpixel.com/~cwright/programming/colorspace/yuv/](http://softpixel.com/~cwright/programming/colorspace/yuv/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting corners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Corner detection** is an important process in computer vision. It helps us
    identify the salient points in the image. This was one of the earliest feature
    extraction techniques that was used to develop image analysis systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to detect the corner of a box by placing markers
    at the points that are identified.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can detect corners:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `corner_detector.py` file that is provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the input image. We will use `box.png`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the image into `grayscale` and cast it to floating-point values. We
    need the floating-point values for the corner detector to work:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the `Harris corner detector` function on the `grayscale` image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To mark the corners, we need to dilate the image, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s threshold the image to display the important points:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the output image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We will run the code in a terminal window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following two images on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb0530af-e029-465d-b6a7-900e99e5a7b3.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Corner detection is an approach that's used in computer vision to extract types
    of features and infer the contents of the image. It is often used in motion detection,
    image recording, video tracking, image mosaicization, image panoramas creation,
    3D modeling, and object recognition. It is a topic similar to the detection of
    points of interest.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Corner detection methods can be subdivided into two groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques based on the extraction of the contours and the subsequent identification
    of the points corresponding to the maximum curvature, or where the edge segments
    intersect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Algorithms that search for corners directly from the intensity of the gray levels
    of the image pixels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Harris Corner Detector* (from Penn State University): [http://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf](http://www.cse.psu.edu/~rtc12/CSE486/lecture06.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the Harris corner detector: [https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html](https://docs.opencv.org/3.0-beta/doc/py_tutorials/py_feature2d/py_features_harris/py_features_harris.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting SIFT feature points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Scale invariant feature transform** (**SIFT**) is one of the most popular
    features in the field of computer vision. David Lowe first proposed this in his
    seminal paper. It has since become one of the most effective features to use for
    image recognition and content analysis. It is robust against scale, orientation,
    intensity, and so on. This forms the basis of our object recognition system.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to detect SIFT feature points.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can detect SIFT feature points:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `feature_detector.py` file that is provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the input image. We will use `table.jpg`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert this image into grayscale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the SIFT detector object and extract the keypoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The keypoints are the salient points, but they are not the features. This basically
    gives us the location of the salient points. SIFT also functions as a very effective
    feature extractor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Draw the keypoints on top of the input image, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the input and output images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We will run this in a terminal window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following two images on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d879a540-7db8-4d25-bdfb-e9fcd02cef02.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For each object in an image, some interesting points are extracted to provide
    a description of the characteristics of the object. This feature, obtained from
    an image selected for training, is used to identify the object when trying to
    locate it in a test image that contains many other objects. To obtain a reliable
    recognition, the features that are extracted from the training image must be detectable,
    even with scale variations, noise, and lighting. These points are usually placed
    in high-contrast regions of the image, such as object contours.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Lowe's method, the key points of the SIFT objects are extracted from a set
    of reference images in the first phase and then they are stored in a database.
    The recognition of the object in a new image takes place by individually comparing
    each characteristic of the new image with the database that was obtained previously
    and looking for features based on the Euclidean distance of their feature vectors.
    From the complete set of matches in the new image, subsets of key points are identified that
    agree with the object and its position, scale, and orientation to filter the best
    matches.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Introduction to SIFT (Scale-Invariant Feature Transform)*: [https://docs.opencv.org/3.4/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html](https://docs.opencv.org/3.4/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation of the `OpenCV.xfeatures2d.SIFT` function: [https://docs.opencv.org/3.4/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html](https://docs.opencv.org/3.4/d5/d3c/classcv_1_1xfeatures2d_1_1SIFT.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Distinctive Image Features from Scale-Invariant Keypoints* (by David G Lowe
    from the University of British Columbia): [https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Star feature detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SIFT feature detector is good in many cases. However, when we build object
    recognition systems, we may want to use a different feature detector before we
    extract features using SIFT. This will give us the flexibility to cascade different
    blocks to get the best possible performance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the** S****tar** **feature detector** to detect
    features from an image.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can build a Star feature detector:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `star_detector.py` file that is provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a class to handle all the functions that are related to Star feature
    detection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to run the detector on the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the input image in the `main` function. We will use `table.jpg`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the image into grayscale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Detect features using the Star feature detector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw keypoints on top of the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the output image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We will run the code in a terminal window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following image on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f32de3b7-9417-4bc7-808e-ed23ada7d6d2.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we learned how to use the OpenCV-Python library to build a
    Star feature detector. The following tasks were performed:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting to grayscale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting features using the Star feature detector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drawing keypoints and displaying the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Star function detector** is based on **CenSurE** (**Center Surrounded
    Extrema**). The differences between the two detectors lie in the choice of polygons:'
  prefs: []
  type: TYPE_NORMAL
- en: CenSurE uses square, hexagons, and octagons as an alternative to the circle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Star approximates the circle with two superimposed squares: one vertical and
    one rotated 45 degrees'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of OpenCV: [https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_feature_detectors.html?highlight=fast%20feature#StarFeatureDetector%20:%20public%20FeatureDetector](https://docs.opencv.org/2.4/modules/features2d/doc/common_interfaces_of_feature_detectors.html?highlight=fast%20feature#StarFeatureDetector%20:%20public%20FeatureDetector)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CenSurE: Center Surround Extremas for Realtime Feature Detection and Matching*,
    in Computer Vision–ECCV 2008 (pp. 102-115). Springer Berlin Heidelberg: [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.1117&rep=rep1&type=pdf](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.465.1117&rep=rep1&type=pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating features using Visual Codebook and vector quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To build an object recognition system, we need to extract feature vectors from
    each image. Each image needs to have a signature that can be used for matching.
    We use a concept called **V****isual Codebook** to build image signatures. This
    codebook is basically the dictionary that we will use to come up with a representation
    for the images in our training data image signatures set. We use **vector quantization**
    to cluster many feature points and come up with **centroids**. These centroids
    will serve as the elements of our Visual Codebook.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will create features using Visual Codebook and vector quantization. To
    build a robust object recognition system, you need tens of thousands of images.
    There is a dataset called `Caltech256` that's very popular in this field! It contains
    256 classes of images, where each class contains thousands of samples.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can create features using Visual Codebook and vector quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a lengthy recipe, so we will only look at the important functions.
    The full code is given in the `build_features.py` file that is already provided
    for you. Let''s look at the class that''s defined to extract features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a method to extract features from the input image. We will use the Star
    detector to get the keypoints and then use SIFT to extract descriptors from these
    locations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to extract centroids from all the descriptors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Each image will give rise to a large number of descriptors. We will just use
    a small number of images because the centroids won''t change much after this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The print progress is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the current label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Read the image and resize it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Use vector quantization to quantize the feature points. Vector quantizationis
    the *N*-dimensional version of rounding off:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the class to handle the bag-of-words model and vector quantization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a method to quantize the datapoints. We will use **k-means clustering**
    to achieve this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the centroids, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a method to normalize the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a method to get the feature vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a histogram and normalize it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a method, and then extract the SIFT features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned earlier, please refer to `build_features.py` for the complete
    code. You should run the code in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: This will generate two files called `codebook.pkl` and `feature_map.pkl`. We
    will use these files in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used a visual textbook as a dictionary, which we then used
    to create a representation for images in our image signatures, which are contained
    in the training set. So, we used vector quantization to group many characteristic
    points and create centroids. These centroids are served as elements of our visual
    textbook.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We extract the features from various points in the image, counting the frequency
    of the values of the extracted features and classifying the image based on the
    frequency found, which is a technique that's similar to the representation of
    a document in a vector space. It is a vector quantization process with which I
    create a dictionary to discretize the possible values of the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Visual Codebook* (by Tae-Kyun Kim, from Sidney Sussex College): [http://mi.eng.cam.ac.uk/~cipolla/lectures/PartIB/old/IB-visualcodebook.pdf](http://mi.eng.cam.ac.uk/~cipolla/lectures/PartIB/old/IB-visualcodebook.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Caltech-256 images repository* (from the California Institute of Technology):
    [http://www.vision.caltech.edu/Image_Datasets/Caltech256/](http://www.vision.caltech.edu/Image_Datasets/Caltech256/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Vector Quantization Overview* (from Binghamton University): [http://www.ws.binghamton.edu/fowler/fowler%20personal%20page/EE523_files/Ch_10_1%20VQ%20Description%20(PPT).pdf](http://www.ws.binghamton.edu/fowler/fowler%20personal%20page/EE523_files/Ch_10_1%20VQ%20Description%20(PPT).pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an image classifier using Extremely Random Forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An object recognition system uses an image classifier to classify the images
    into known categories. **Extremely Random Forests** (**ERFs**) are very popular
    in the field of machine learning because of their speed and accuracy. This algorithm
    is based on decision trees. Their differences compared to classical decision trees
    are in the choice of the points of division of the tree. The best division to
    separate the samples of a node into two groups is done by creating random subdivisions
    for each of the randomly selected features and choosing the best division between
    those.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use ERFs to train our image classifier. We basically
    construct decision trees based on our image signatures, and then train the forest
    to make the right decision.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can train an image classifier using ERFs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `trainer.py` file that is provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Define an argument parser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a class to handle ERF training. We will use a label encoder to encode
    our training labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Encode the labels and train the classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to encode the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to classify an unknown datapoint:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `main` function and parse the input arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the feature map that we created in the previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the feature vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the ERF, which is based on the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the trained ERF model, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you should run the code in the Terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: This will generate a file called `erf.pkl`. We will use this file in the next
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used ERFs to train our image classifier. First, we defined
    an argument parser function and a class to handle ERF training. We used a label
    encoder to encode our training labels. Then, we loaded the feature map we obtained
    in the *Creating features using Visual Codebook and vector quantization* recipe.
    So, we extracted feature vectors and the labels, and finally we trained the ERF
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To train the image classifier, the `sklearn.ensemble.ExtraTreesClassifier` function
    was used. This function builds an extremely randomized tree classifier.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The official documentation of the `sklearn.ensemble.ExtraTreesClassifier` function:
    [https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html#sklearn.tree.ExtraTreeClassifier)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Random Forests* (by Leo Breiman and Adele Cutler, from the University of California,
    Berkeley): [https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Extremely randomized trees* (by Pierre Geurts, Damien Ernst, and Louis Wehenkel,
    form *Machine learning Journal - Springer)*: [https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf](https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an object recognizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, *Training an image classifier using Extremely Random
    Forests*, we used ERFs to train our image classifier. Now that we have trained
    an ERF model, let's go ahead and build an object recognizer that can recognize
    the content of unknown images.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use a trained ERF model to recognize the
    content of unknown images.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can build an object recognizer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `object_recognizer.py` file that is provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the argument parser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a class to handle the image tag extraction functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to predict the output using the trained ERF model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the `main` function and load the input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Scale the image appropriately, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Print the output on the terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you should run the code in the following way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used a trained ERF model to recognize the content of unknown
    images. To do this, the algorithms discussed in the two previous recipes were
    used, namely, *Creating features using Visual Codebook and vector quantization* and *Training
    an image classifier using Extremely Random Forests.*
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **random forest** is an aggregate classifier that is made up of many decision
    trees and outputs the class that corresponds to the output of the individual tree
    classes. The algorithm for inducing a random forest was developed by Leo Breiman
    and Adele Cutler. It is based on the creation of a broad set of classifying trees,
    each of which is proposed to classify a single plant whose characteristics of
    any nature have been evaluated. Comparing the classification proposals provided
    by each tree in the forest shows the class to attribute the plant to it is the
    one that received the most indications or votes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Intro to random forest* (by Anthony Anh Quoc Doan, from California State University,
    Long Beach): [https://web.csulb.edu/~tebert/teaching/lectures/551/random_forest.pdf](https://web.csulb.edu/~tebert/teaching/lectures/551/random_forest.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Light GBM for image classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosting is used in regression and classification problems to produce
    a predictive model in the form of a set of weak predictive models, typically decision
    trees. This methodology is similar to the boosting methods and generalizes them,
    allowing for the optimization of an arbitrary differentiable `loss` function.
  prefs: []
  type: TYPE_NORMAL
- en: The **Light Gradient Boosting Machine** (**LightGBM**) is a particular variation
    of gradient boosting, with some modifications that make it particularly advantageous.
    It is based on classification trees, but the choice of splitting the leaf at each
    step is done more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to use LightGBM to classify handwritten digits.
    To do this, the **Modified National Institute of Standards and Technology** (**MNIST**)
    dataset will be used. This is a large database of handwritten digits. It has a
    set of 70,000 examples of data. It is a subset of NIST's larger dataset. The digits
    are of a 28 x 28 pixel resolution and are stored in a matrix of 70,000 rows and
    785 columns; 784 columns form each pixel value from the 28 x 28 matrix, and one
    value is the actual digit. The digits have been size-normalized and centered in
    a fixed-size image.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see how we can use LightGBM for image classification:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a new Python file and import the following packages (the full code is
    given in the `LightgbmClassifier.py` file that is provided for you):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'To import the `mnist` dataset, the following code must be used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The following tuples are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '`XTrain`, `XTest`: A `uint8` array of grayscale image data with the (`num_samples,`
    `28, 28`) shape'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`YTrain`, `YTest`: A `uint8` array of digit labels (integers in the range 0-9)
    with the (`num_samples`) shape'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, each sample image consists of a 28 x 28 matrix. To reduce the dimensionality,
    we will flatten the 28 x 28 images into vectors of size 784:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will extract from the dataset that contains the digits from 0 to 9,
    but only the first two (0 and 1) because we want to build a binary classifier.
    To do this, we will use the `numpy.where` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a dataset for `lightgbm`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to specify the model parameters as a dictionary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Our model is now ready, so we can use it to classify the handwritten digits
    automatically. To do this, we will use the `predict()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can evaluate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'To analyze the errors that were made in the binary classification in more detail,
    we need to compute the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'The following results are returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will calculate the model''s accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'The following result is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: The model is therefore able to classify images of handwritten digits with a
    high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we used LightGBM to classify handwritten digits. LightGBM is
    a particular variation of gradient boosting, with some modifications that make
    it particularly advantageous. It is based on classification trees, but the choice
    of splitting the leaf at each step is done more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'While boosting operates a tree growth in depth, LightGBM makes this choice
    by combining two criteria:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimization based on gradient descent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To avoid overfitting problems, a limit for the maximum depth is set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This type of growth is called **leaf-wise**.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Light GBM has many advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: The procedure that's presented is, on average, an order of magnitude faster
    than similar algorithms. This is because it does not completely grow trees, and
    it also makes use of the binning of variables (a procedure that divides these
    into sub-groups, both to speed up the calculations and as a regularization method).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More economical use of memory: the binning procedure involves less intensive
    use of memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Better accuracy compared to the usual boosting algorithms: since it uses a
    leaf-wise procedure, the obtained trees are more complex. At the same time, to
    avoid overfitting, a limit is placed on the maximum depth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is easily parallelizable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The official documentation of the `LightGBM` library: [https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Gentle Introduction to Gradient Boosting* (from Northeastern University):
    [http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf](http://www.ccs.neu.edu/home/vip/teach/MLcourse/4_boosting/slides/gradient_boosting.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LightGBM: A Highly Efficient Gradient Boosting Decision Tree* (by Guolin Ke
    and others): [https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
