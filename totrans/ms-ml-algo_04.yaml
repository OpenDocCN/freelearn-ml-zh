- en: Bayesian Networks and Hidden Markov Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to introduce the basic concepts of Bayesian models,
    which allow working with several scenarios where it's necessary to consider uncertainty
    as a structural part of the system. The discussion will focus on static (time-invariant)
    and dynamic methods that can be employed where necessary to model time sequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, the chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem and its applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling from a Bayesian network using direct methods and **Markov chain Monte
    Carlo** (**MCMC**) ones (Gibbs and Metropolis-Hastings samplers)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling a Bayesian network with PyMC3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden Markov Models** (**HMMs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examples with hmmlearn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conditional probabilities and Bayes' theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we have a probability space *S* and two events *A* and *B*, the probability
    of *A* given *B* is called **conditional probability**, and it''s defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/352da6fc-79b9-478c-acef-eac9462996be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As *P(A, B)* = *P(B, A)*, it''s possible to derive **Bayes'' theorem**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c34fa3ad-5cf8-4d3d-80f1-768034c0d1a3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This theorem allows expressing a conditional probability as a function of the
    opposite one and the two marginal probabilities *P(A)* and *P(B)*. This result
    is fundamental to many machine learning problems, because, as we''re going to
    see in this and in the next chapters, normally it''s easier to work with a conditional
    probability in order to get the opposite, but it''s hard to work directly from
    the latter. A common form of this theorem can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7d71973-5b65-4bbc-8eef-433b4cd5950d.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's suppose that we need to estimate the probability of an event *A* given
    some observations *B*, or using the standard notation, **the posterior probability
    of A**; the previous formula expresses this value as proportional to the term
    *P(A)*, which is the marginal probability of *A*, called **prior probability**,
    and the conditional probability of the observations *B* given the event *A*. *P(B|A)*
    is called **likelihood**, and defines how event *A* is likely to determine *B*.
    Therefore, we can summarize the relation as *posterior probability ∝ likelihood ·
    prior probability*. The proportion is not a limitation, because the term *P(B)*
    is always a normalizing constant that can be omitted. Of course, the reader must
    remember to normalize *P(A|B)* so that its terms always sum up to one.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a key concept of Bayesian statistics, where we don''t directly trust
    the prior probability, but we reweight it using the likelihood of some observations.
    As an example, we can think to toss a coin 10 times (event *A*). We know that
    *P(A) = 0.5* if the coin is fair. If we''d like to know what the probability is
    to get 10 heads, we could employ the Binomial distribution obtaining *P(10 heads)
    = 0.5^k*; however, let''s suppose that we don''t know whether the coin is fair
    or not, but we suspect it''s loaded with a prior probability *P(Loaded) = 0.7*
    in favor of tails. We can define a complete prior probability *P(Coin status)*
    using the indicator functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d017c7ab-85b8-4687-8cea-ce1f6222e7c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *P(Fair) = 0.5* and *P(Loaded) = 0.7*, the indicator *I[Coin=Fair]* is
    equal to 1 only if the coin is fair, and 0 otherwise. The same happens with *I[Coin=Loaded]* when
    the coin is loaded. Our goal now is to determine the posterior probability *P(Coin
    status|B[1], B[2], ..., **B[n])* to be able to confirm or to reject our hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s imagine to observe *n = 10* events with *B[1] = Head* and *B[2], ..., B[n] =
    Tail*. We can express the probability using the binomial distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/845c235e-2666-4685-88fe-65cdb967a8ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After simplifying the expression, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48d546d0-7d6d-4bb8-8824-202451d938da.png)'
  prefs: []
  type: TYPE_IMG
- en: We still need to normalize by dividing both terms by 0.083 (the sum of the two
    terms), so we get the final posterior probability *P(Coin status|B[1], B[2], ..., **Bn)
    = 0.04I[Fair] + 0.96I[Loaded]*. This result confirms and strengthens our hypothesis.
    The probability of a loaded coin is now about 96%, thanks to the sequence of nine
    tail observations after one head.
  prefs: []
  type: TYPE_NORMAL
- en: This example was presented to show how the data (observations) is plugged into
    the Bayesian framework. If the reader is interested in studying these concepts
    in more detail, in *Introduction to Statistical Decision Theory*, *Pratt J.*,
    *Raiffa H.*, *Schlaifer R.*, *The MIT Press*,it's possible to find many interesting
    examples and explanations; however, before introducing Bayesian networks, it's
    useful to define two other essential concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first concept is called **conditional independence**, and it can be formalized
    considering two variables *A* and *B*, which are conditioned to a third one, *C*.
    We say that *A* and *B* are conditionally independent given *C* if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36bb9203-b429-4355-a8dc-85614c716870.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s suppose we have an event *A* that is conditioned to a series of
    causes *C[1], C[2], ..., C[n]*; the conditional probability is, therefore, *P(A|C[1],
    C[2], ..., C[n])*. Applying Bayes'' theorem, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe21f59c-5b35-43b8-8e36-a87618f446de.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If there is conditional independence, the previous expression can be simplified
    and rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/981fd6f3-7f95-411f-8c54-81cb045249d8.png)'
  prefs: []
  type: TYPE_IMG
- en: This property is fundamental in Naive Bayes classifiers, where we assume that
    the effect produced by a cause does not influence the other causes. For example,
    in a spam detector, we could say that the length of the mail and the presence
    of some particular keywords are independent events, and we only need to compute
    *P(Length|Spam)* and *P(Keywords|Spam)* without considering the joint probability
    *P(Length, Keywords|Spam)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important element is the **chain rule** of probabilities. Let''s suppose
    we have the joint probability *P(X[1], X[2], ..., X[n])*. It can be expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/107285da-ab3d-4be3-a4b0-d680ff35d2cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Repeating the procedure with the joint probability on the right side, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fe5a7d2-8909-4da8-9322-2582f45b29a0.png)'
  prefs: []
  type: TYPE_IMG
- en: In this way, it's possible to express a full joint probability as the product
    of hierarchical conditional probabilities, until the last term, which is a marginal
    distribution. We are going to use this concept extensively in the next paragraph
    when exploring Bayesian networks.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **Bayesian network** is a probabilistic model represented by a direct acyclic
    graph *G = {V, E}*, where the vertices are random variables *X[i]*, and the edges
    determine a conditional dependence among them. In the following diagram, there''s
    an example of simple Bayesian networks with four variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cae2f7b1-4b9f-4cc2-b225-0e12772b4866.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of Bayesian network
  prefs: []
  type: TYPE_NORMAL
- en: 'The variable *x[4]* is dependent on *x[3]*, which is dependent on *x[1]* and *x[2]*. To
    describe the network, we need the marginal probabilities *P(x[1])* and *P(x[2])*
    and the conditional probabilities *P(x[3]|x[1],x[2])* and *P(x[4]|x[3])*. In fact,
    using the chain rule, we can derive the full joint probability as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c97c045a-48c4-48c0-88f0-bd904ce463c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous expression shows an important concept: as the graph is direct
    and acyclic, each variable is conditionally independent of all other variables
    that are not successors given its predecessors. To formalize this concept, we
    can define the function *Predecessors(x[i])*, which returns the set of nodes that
    influence *x[i]* directly, for example, *Predecessors(x[3]) = {x[1],x[2]}* (we
    are using lowercase letters, but we are considering the random variable, not a
    sample). Using this function, it''s possible to write a general expression for
    the full joint probability of a Bayesian network with *N* nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/303e7bb6-83c4-48be-afea-573888168eef.png)'
  prefs: []
  type: TYPE_IMG
- en: The general procedure to build a Bayesian network should always start with the
    first causes, adding their effects one by one, until the last nodes are inserted
    into the graph. If this rule is not respected, the resulting graph can contain
    useless relations that can increase the complexity of the model. For example,
    if *x[4]* is caused indirectly by both *x[1]* and *x[2]*, therefore adding the
    edges *x[1] → x[4]* and *x[2] → x[4]* could seem a good modeling choice; however,
    we know that the final influence on *x[4]* is determined only by the values of
    *x*[*3*, ]whose probability must be conditioned on *x[1]* and *x[2]*, hence we
    can remove the spurious edges. I suggest reading *Introduction to Statistical
    Decision Theory, Pratt J., Raiffa H., Schlaifer R., The MIT Press*to learn many
    best practices that should be employed in this procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling from a Bayesian network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performing a direct inference on a Bayesian network can be a very complex operation
    when the number of variables and edges is high. For this reason, several sampling
    methods have been proposed. In this paragraph, we are going to show how to determine
    the full joint probability sampling from a network using a direct approach, and
    two MCMC algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start considering the previous network and, for simplicity, let''s assume
    to have only *Bernoulli* distributions. *X[1]* and *X[2]* are modeled as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb8c8451-da95-42e4-923a-d096f4041bc0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The conditional distribution *X[3]* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/789d0fe3-5573-429b-a2e3-37a4043b7fed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While the conditional distribution *X[4]* is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c964debc-3b61-4b6d-9b5a-7d68f7f4a514.png)'
  prefs: []
  type: TYPE_IMG
- en: We can now use a direct sampling to estimate the full joint probability *P(x[1],
    x[2], x[3], x[4])* using the chain rule previously introduced.
  prefs: []
  type: TYPE_NORMAL
- en: Direct sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With **direct sampling**, our goal is to approximate the full joint probability
    through a sequence of samples drawn from each conditional distribution. If we
    assume that the graph is well-structured (without unnecessary edges) and we have
    *N* variables, the algorithm is made up of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the variable *N[Samples]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a vector *S* with shape *(N, N[Samples])*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a frequency vector *F[Samples]* with shape *(N, N[Samples])*. In
    Python, it's better to employ a dictionary where the key is a combination *(x[1],
    x[2], x[3], ..., x[N])*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *t=1* to *N*[*Samples*:]
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample from *P(X[i]|Predecessors(X[i]))*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the sample in *S[i, t]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *F[Samples]* contains the sampled tuple *S[:, t]*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**F[Samples][S[:, t]] += 1**'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Else:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*F[Samples][**S[:, t]] = 1* (both these operations are immediate with Python
    dictionaries)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a vector *P[Sampled]* with shape *(**N, 1)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *P[Sampled][i, 0] = F[Samples][**i]/N*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From a mathematical viewpoint, we are first creating a frequency vector *F[Samples](x[1],
    x[2], x[3], ..., x[N]; N[Samples]**)* and then we approximate the full joint probability
    considering *N[Samples] → ∞*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66ba24b0-7188-465c-925c-17d0b4d179e2.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of direct sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can now implement this algorithm in Python. Let''s start by defining the
    sample methods using the NumPy function `np.random.binomial(1, p)`, which draws
    a sample from a *Bernoulli* distribution with probability `p`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can implement the main cycle. As the variables are Boolean,
    the total number of probabilities is 16, so we set `Nsamples` to `5000` (smaller
    values are also acceptable):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When the sampling is complete, it''s possible to extract the full joint probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also query the model. For example, we could be interested in *P(X[4]=True)*.
    We can do this by looking for all the elements where *X[4]=True*, and summing
    up the relative probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This value is coherent with the definition of *X[4]*, which is always *p >=
    0.5*. The reader can try to change the values and repeat the simulation.
  prefs: []
  type: TYPE_NORMAL
- en: A gentle introduction to Markov chains
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to discuss the MCMC algorithms, it's necessary to introduce the concept
    of Markov chains. In fact, while the direct sample method draws samples without
    any particular order, the MCMC strategies draw a sequence of samples according
    to a precise transition probability from a sample to the following one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider a time-dependent random variable *X(t)*, and let''s assume
    a discrete time sequence **X[1]**, **X[2]**, ..., **X[t]**, **X[t+1]**, ... where
    **X[t]** represents the value assumed at time *t*. In the following diagram, there''s
    a schematic representation of this sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69b55183-7574-40b2-9b1e-96e2d4cf5a3e.png)'
  prefs: []
  type: TYPE_IMG
- en: Structure of a generic Markov chain
  prefs: []
  type: TYPE_NORMAL
- en: 'We can suppose to have *N* different states *s[i]* for *i=1..N*, therefore
    it''s possible to consider the probability *P(X[t]=s[i]|X[t-1]=s[j], ..., X[1]=s[p])*.
    *X(t)* is defined as a **first-order Markov process** if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4414956f-285c-4f5d-bf7a-7de1a5f60c52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, in a Markov process (from now on, we omit *first-order*, even
    if there are cases when it''s useful to consider more previous states), the probability
    that *X(t)* is in a certain state depends only on the state assumed in the previous
    time instant. Therefore, we can define a **transition probability **for every
    couple *i*, *j*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a16ad93b-a206-445b-9ffb-42f3407edb16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering all the couples *(i, j)*, it''s also possible to build a transition
    probability matrix *T(i, j) = P(i → j)*. The marginal probability that *X[t]=s[i]*
    using a standard notation is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17cc4835-7ef7-4686-a2e4-124c42e4a982.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, it''s easy to prove (**Chapman-Kolmogorov** equation) that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee16d959-98d0-4b92-9ceb-7a345491283c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous expression, in order to compute *π[i](t+1)*, we need to sum
    over all possible previous states, considering the relative transition probability.
    This operation can be rewritten in matrix form, using a vector *π(t)* containing
    all states and the transition probability matrix *T* (the uppercase superscript
    *T* means that the matrix is transposed). The evolution of the chain can be computed
    recursively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d9526323-5850-4106-aa9d-b2bddc9343fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For our purposes, it''s important to consider Markov chains that are able to
    reach a *stationary distribution* *π[s]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66b29160-1d3f-4c7c-ae04-157fcd2ef634.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the state does not depend on the initial condition *π(1)*, and
    it's no longer able to change. The stationary distribution is unique if the underlying
    Markov process is *ergodic*. This concept means that the process has the same
    properties if averaged over time (which is often impossible), or averaged vertically (freezing
    the time) over the states (which is simpler in the majority of cases).
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of ergodicity for Markov chains is assured by two conditions. The
    first is aperiodicityfor all states, which means that it is impossible to find
    a positive number *p* so that the chain returns in the same state sequence after
    a number of instants equal to a multiple of *p*. The second condition is that
    all states must be positive recurrent: this means that, given a random variable *N[instants]**(i)*,
    describing the number of time instants needed to return to the state *s[i]*, *E[N[instants](i)]
    < ∞*; therefore, potentially, all the states can be revisited in a finite time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The reason why we need the ergodicity condition, and hence the existence of
    a unique stationary distribution, is that we are considering the sampling processes
    modeled as Markov chains, where the next value is sampled according to the current
    state. The transition from one state to another is done in order to find better
    samples, as we''re going to see in the Metropolis-Hastings sampler, where we can
    also decide to reject a sample and keep the chain in the same state. For this
    reason, we need to be sure that the algorithms converge to the unique stable distribution
    (that approximates the real full joint distribution of our Bayesian network).
    It''s possible to prove that a chain always reaches a stationary distribution
    if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ec1e6164-4511-457d-9598-93c25d184f80.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous equation is called detailed balance, and implies the reversibility
    of the chain. Intuitively, it means that the probability of finding the chain
    in the state *A* times the probability of a transition to the state *B* is equal
    to the probability of finding the chain in the state *B* times the probability
    of a transition to *A*.
  prefs: []
  type: TYPE_NORMAL
- en: For both methods that we are going to discuss, it's possible to prove that they
    satisfy the previous condition, and therefore their convergence is assured.
  prefs: []
  type: TYPE_NORMAL
- en: Gibbs sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s suppose that we want to obtain the full joint probability for a Bayesian
    network *P(x[1], x[2], x[3], ..., x[N]**)*; however, the number of variables is
    large and there''s no way to solve this problem easily in a closed form. Moreover,
    imagine that we would like to get some marginal distribution, such as *P(x[2])*,
    but to do so we should integrate the full joint probability, and this task is
    even harder. Gibbs sampling allows approximating of all marginal distributions
    with an iterative process. If we have *N* variables, the algorithm proceeds with
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the variable *N[Iterations]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a vector *S* with shape *(N, N[Iterations])*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly initialize *x[1]^((0)), x[2]^((0)), ..., x[N]^((0))* (the superscript
    index is referred to the iteration)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *t=1* to *N[Iterations]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *x[1]^((t))* from *p(x[1]|x[2]^((t-1)), x[3]^((t-1)), ..., x[N]^((t-1)))*
    and store it in *S[0, t]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *x[2]^((t))* from *p(**x[2]|x[1]^((t)), x[3]^((t-1)), ..., x[N]^((t-1))**)* and
    store it in *S[1, t]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *x[3]^((t))* from *p(**x[3]|x[1]^((t)), x[2]^((t)), ..., x[N]^((t-1))**)* and
    store it in *S[2, t]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '...'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *x[N]^((t))* from *p(**x[N]|x[1]^((t)), x[2]^((t)), ..., x[N-1]^((t))**)* and
    store it in *S[N-1, t]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of the iterations, vector *S* will contain *N*[*Iterations* ]samples
    for each distribution. As we need to determine the probabilities, it's necessary
    to proceed like in the direct sampling algorithm, counting the number of single
    occurrences and normalizing dividing by *N[Iterations]*. If the variables are
    continuous, it's possible to consider intervals, counting how many samples are
    contained in each of them.
  prefs: []
  type: TYPE_NORMAL
- en: For small networks, this procedure is very similar to direct sampling, except
    that when working with very large networks, the sampling process could become
    slow; however, the algorithm can be simplified after introducing the concept of
    the Markov blanket of *X[i]*, which is the set of random variables that are predecessors,
    successors, and successors' predecessors of *X[i]* (in some books, they use the
    terms *parents* and *children*). In a Bayesian network, a variable *X[i]* is a
    conditional independent of all other variables given its Markov blanket. Therefore,
    if we define the function *MB(X[i])*, which returns the set of variables in the
    blanket, the generic sampling step can be rewritten as *p(x[i]|MB(X[i]))*, and
    there's no more need to consider all the other variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this concept, let''s consider the network shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f40b1fa-6e12-401a-a769-3bd4b0438600.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayesian network for the Gibbs sampling example
  prefs: []
  type: TYPE_NORMAL
- en: 'The Markov blankets are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*MB(X[1])* = *{ X[2], X[3] }*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MB(X[2])* = *{ X[1, ]X[3], X[4] }*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MB(X[3])* = *{ X[1, ]X[2], X[4], X[5] }*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MB(X[4])* = *{ X[3] }*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MB(X[5])* = *{ X[3] }*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MB(X[6])* = *{ X[2] }*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, if *N* is very large, the cardinality of *|MB(X[i])| << N*, thus
    simplifying the process (the *vanilla* Gibbs sampling needs *N-1* conditions for
    each variable). We can prove that the Gibbs sampling generates samples from a
    Markov chain that is in detailed balance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30105cca-76a4-4cad-bf32-b2c541feed6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the procedure converges to the unique stationary distribution. This
    algorithm is quite simple; however, its performance is not excellent, because
    the random walks are not tuned up in order to explore the right regions of the
    state-space, where the probability to find good samples is high. Moreover, the
    trajectory can also return to bad states, slowing down the whole process. An alternative
    (also implemented by PyMC3 for continuous random variables) is the **No-U-Turn**
    algorithm, which we don''t discuss in this book. The reader interested in this
    topic can find a full description in *The* *No-U-Turn Sampler: Adaptively Setting
    Path Lengths in Hamiltonian Monte Carlo*, *Hoffmann M. D.*, *Gelman A.*, *arXiv:1111.4246*.'
  prefs: []
  type: TYPE_NORMAL
- en: Metropolis-Hastings sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen that the full joint probability distribution of a Bayesian network
    *P(x[1], x[2], x[3], ..., x[N]**)* can become intractable when the number of variables
    is large. The problem can become even harder when it's needed to marginalize it
    in order to obtain, for example, *P(x[i])*, because it's necessary to integrate
    a very complex function. The same problem happens when applying the Bayes' theorem
    in simple cases. Let's suppose we have the expression *p(A|B) = K · P(B|A)P(A)*.
    I've expressly inserted the normalizing constant *K*, because if we know it, we
    can immediately obtain the posterior probability; however, finding it normally
    requires integrating *P(B|A)P(A)*, and this operation can be impossible in closed
    form.
  prefs: []
  type: TYPE_NORMAL
- en: The Metropolis-Hastings algorithm can help us in solving this problem. Let's
    imagine that we need to sample from *P(x[1], x[2], x[3], ..., x[N]**)*, but we
    know this distribution up to a normalizing constant, so *P(x[1], x[2], x[3], ...,
    x[N]) ∝ g(x[1], x[2], x[3], ..., x[N])*. For simplicity, from now on we collapse
    all variables into a single vector, so *P(x) ∝ g(x)*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take another distribution *q(x'|x^((i-1)))*, which is called **candidate-generating
    distribution**. There are no particular restrictions on this choice, only that
    *q* is easy to sample. In some situations, *q* can be chosen as a function very
    similar to the distribution *p(x)*, which is our target, while in other cases,
    it's possible to use a normal distribution with mean equal to *x^((i-1))*. As
    we're going to see, this function acts as a proposal-generator, but we're not
    obliged to accept all the samples drawn from it therefore, potentially any distribution
    with the same domain of *P(X)* can be employed. When a sample is accepted, the
    Markov chain transitions to the next state, otherwise it remains in the current
    one. This decisional process is based on the idea that the sampler must explore
    the most important state-space regions and discard the ones where the probability
    to find good samples is low.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm proceeds with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the variable *N[Iterations]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize *x^((0))* randomly
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *t=1* to *N**[Iterations]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a candidate sample *x'* from *q(x'|x^((i-1))**)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the following value:![](img/e2dfa594-aaac-4af2-a7b0-133f55e1fd30.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *α ≥ 1*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Accept the sample *x^((t)) = x'*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Else if *0 < α < 1*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Accept the sample *x^((t))** = x'* with probability *α*; or
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Reject the sample *x'* setting *x^((t)) = x^((t-1))* with probability *1 - **α*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: It's possible to prove (the proof will be omitted, but it's available in *Markov
    Chain Monte Carlo and Gibbs Sampling, **Walsh B., Lecture Notes for EEB 596z*)
    that the transition probability of the Metropolis-Hastings algorithm satisfies
    the detailed balance equation, and therefore the algorithm converges to the true
    posterior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Example of Metropolis-Hastings sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can implement this algorithm to find the posterior distribution *P(A|B)*
    given the product of *P(B|A)* and *P(A)*, without considering the normalizing
    constant that requires a complex integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54a4353b-48d1-4357-90cd-01824204939f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the resulting *g(x)* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c55f8193-4471-4ed9-907c-e98992b1a0f5.png)'
  prefs: []
  type: TYPE_IMG
- en: To solve this problem, we adopt the random walk Metropolis-Hastings, which consists
    of choosing *q ∼ Normal(μ=x^((t-1)))*. This choice allows simplifying the value
    *α*, because the two terms *q(x^((t-1))|x')* and *q(x'|x^((t-1)))* are equal (thanks
    to the symmetry around the vertical axis passing through *x[mean]*) and can be
    canceled out, so *α* becomes the ratio between *g(x')* and *g(x^((t-1))*).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing is defining the functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can start our sampling process with 100,000 iterations and *x^((0))
    = 1.0*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a representation of the posterior distribution, we need to create a
    histogram through the NumPy function `np.histogram()`, which accepts an array
    of values and the number of desired intervals (`bins`); in our case, we set `100`
    intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting plot of *p(x)* is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d6fb2c3-0a41-45e3-93e8-2ca302dbbf90.png)'
  prefs: []
  type: TYPE_IMG
- en: Sampled probability density function
  prefs: []
  type: TYPE_NORMAL
- en: Sampling example using PyMC3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PyMC3** is a powerful Python Bayesian framework that relies on Theano to
    perform high-speed computations (see the information box at the end of this paragraph
    for the installation instructions). It implements all the most important continuous
    and discrete distributions, and performs the sampling process mainly using the
    No-U-Turn and Metropolis-Hastings algorithms. For all the details about the API
    (distributions, functions, and plotting utilities), I suggest visiting the documentation
    home page [http://docs.pymc.io/index.html](http://docs.pymc.io/index.html), where
    it''s also possible to find some very intuitive tutorials.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The example we want to model and simulate is based on this scenario: a daily
    flight from London to Rome has a scheduled departure time at 12:00 am, and a standard
    flight time of two hours. We need to organize the operations at the destination
    airport, but we don''t want to allocate resources when the plane hasn''t landed
    yet. Therefore, we want to model the process using a Bayesian network and considering
    some common factors that can influence the arrival time. In particular, we know
    that the onboarding process can be longer than expected, as well as the refueling
    one, even if they are carried out in parallel. London air traffic control can
    also impose a delay, and the same can happen when the plane is approaching Rome.
    We also know that the presence of rough weather can cause another delay due to
    a change of route. We can summarize this analysis with the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db234ff4-d869-4c51-80bb-7d28a56e0783.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayesian network representing the air traffic control problem
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering our experience, we decide to model the random variables using the
    following distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Passenger onboarding ∼ Wald(μ=0.5, λ=0.2)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Refueling ∼ Wald(μ=0.25, λ=0.5)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Departure traffic delay ∼ Wald(μ=0.1, λ=0.2)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Arrival traffic delay ∼ Wald(μ=0.1, λ=0.2)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Departure time = 12 + Departure traffic delay + max(Passenger onboarding,
    Refueling)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rough weather ∼ Bernoulli(p=0.35)*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Flight time ∼ Exponential(λ=0.5 - (0.1 · Rough weather))* (The output of a
    Bernoulli distribution is *0* or *1* corresponding to False and True)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Arrival time = Departure time + Flight time + Arrival traffic delay*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The probability density functions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b14d7f2-e3a1-4226-b874-59a38946bbd5.png)'
  prefs: []
  type: TYPE_IMG
- en: '`Departure Time` and `Arrival Time` are functions of random variables, and
    the parameter λ of `Flight Time` is also a function of `Rough Weather`.'
  prefs: []
  type: TYPE_NORMAL
- en: Even if the model is not very complex, the direct inference is rather inefficient,
    and therefore we want to simulate the process using PyMC3.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create a `model` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'From now on, all operations must be performed using the context manager provided
    by the `model` variable. We can now set up all the random variables of our Bayesian
    network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We have imported two namespaces, `pymc3.distributions.continuous` and `pymc3.distributions.discrete`,
    because we are using both kinds of variable. Wald and exponential are continuous
    distributions, while `Bernoulli` is discrete. In the first three rows, we declare
    the variables `passenger_onboarding`, `refueling`, and `departure_traffic_delay`.
    The structure is always the same: we need to specify the class corresponding to
    the desired distribution, passing the name of the variable and all the required
    parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: The `departure_time` variable is declared as `pm.Deterministic`. In PyMC3, this
    means that, once all the random elements have been set, its value becomes completely
    determined. Indeed, if we sample from `departure_traffic_delay`, `passenger_onboarding`,
    and `refueling`, we get a determined value for `departure_time`. In this declaration,
    we've also used the utility function `pmm.switch`, which operates a binary choice
    based on its first parameter (for example, if *A > B*, return *A*, else return
    *B*).
  prefs: []
  type: TYPE_NORMAL
- en: The other variables are very similar, except for `flight_time`, which is an
    exponential variable with a parameter *λ*, which is a function of another variable
    (`rough_weather`). As a Bernoulli variable outputs *1* with probability *p* and
    *0* with probability *1 - p*, *λ = 0.4* if there's rough weather, and *0.5* otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been set up, it''s possible to simulate it through a sampling
    process. PyMC3 picks the best sampler automatically, according to the type of
    variables. As the model is not very complex, we can limit the process to `500`
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output can be analyzed using the built-in `pm.traceplot()` function, which
    generates the plots for each of the sample''s variables. The following graph shows
    the detail of one of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a2fae86-4a34-4d33-8fcb-d95901ceb486.png)'
  prefs: []
  type: TYPE_IMG
- en: Distribution and samples for the arrival time random variable
  prefs: []
  type: TYPE_NORMAL
- en: 'The right column shown the samples generated for the random variable (in this
    case, the arrival time), while the left column shows the relative frequencies.
    This plot can be useful to have a visual confirmation of our initial ideas; in
    fact, the arrival time has the majority of its mass concentrated in the interval
    14:00 to 16:00 (the numbers are always decimal, so it''s necessary to convert
    the times); however, we should integrate to get the probabilities. Instead, through
    the `pm.summary()` function, PyMC3 provides a statistical summary that can help
    us in making the right decisions. In the following snippet, the output containing
    the summary of a single variable is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For each variable, it contains mean, standard deviation, Monte Carlo error,
    95% highest posterior density interval, and the posterior quantiles. In our case,
    we know that the plane will land at about 15:10 (`15.174`).
  prefs: []
  type: TYPE_NORMAL
- en: This is only a very simple example to show the power of Bayesian networks. For
    deep insight, I suggest the book *Introduction to Statistical Decision Theory*,
    *Pratt J.*, *Raiffa H.*, *Schlaifer R.*, *The MIT Press*, where it's possible
    to study different Bayesian applications that are out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: PyMC3 ([http://docs.pymc.io/index.html](http://docs.pymc.io/index.html)) can
    be installed using the `pip install -U pymc3` command. As it requires Theano (which
    is installed automatically), it's also necessary to provide it with a C/C++ compiler.
    I suggest using distributions such as Anaconda ([https://www.anaconda.com/download/](https://www.anaconda.com/download/)),
    which allows installing MinGW through the `conda install -c anaconda mingw` command.
    For any problems, on the website you can find detailed installation instructions.
    For further information on how to configure Theano to work with GPU support (the
    default installation is based on CPU NumPy algorithms), please visit this page: [http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/).
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Markov Models (HMMs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a stochastic process *X(t)* that can assume *N* different states:
    *s[1], s[2], ..., s[N]* with first-order Markov chain dynamics. Let''s also suppose
    that we cannot observe the state of *X(t)*, but we have access to another process
    *O(t)*, connected to *X(t)*, which produces observable outputs (often known as
    **emissions**). The resulting process is called a **Hidden Markov Model** (**HMM**),
    and a generic schema is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f054b58-c9c5-4832-b900-76d03dd95007.png)'
  prefs: []
  type: TYPE_IMG
- en: Structure of a generic Hidden Markov Model
  prefs: []
  type: TYPE_NORMAL
- en: 'For each hidden state *s[i]*, we need to define a transition probability *P(i →
    j)*, normally represented as a matrix if the variable is discrete. For the Markov
    assumption, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae2d037e-d1ac-4fbf-a67a-6fd10b65dcf1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Moreover, given a sequence of observations *o[1], o[2], ..., o[M]*, we also
    assume the following assumption about the independence of the **emission probability**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/592ba42f-5436-4409-8576-8f415fd3cb99.png)'
  prefs: []
  type: TYPE_IMG
- en: In other words, the probability of the observation *o[i]* (in this case, we
    mean the value at time *i*) is conditioned only by the state of the hidden variable
    at time *i (x[i])*. Conventionally, the first state *x[0]* and the last one *x[Ending]*
    are never emittied, and therefore all the sequences start with the index *1* and
    end with an extra timestep corresponding to the final state.
  prefs: []
  type: TYPE_NORMAL
- en: HMMs can be employed in all those contexts where it's impossible to measure
    the state of a system (we can only model it as a stochastic variable with a known
    transition probability), but it's possible to access some data connected to it.
    An example can be a complex engine that is made up of a large number of parts.
    We can define some internal states and learn a transition probability matrix (we're
    going to learn how to do that), but we can only receive measures provided by specific
    sensors.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, even if not extremely realistic, but it's useful to include the Markov
    assumption and the emission probability independence into our model. The latter
    can be justified considering that we can sample all the *peak* emissions corresponding
    to precise states and, as the random process *O(t)* is implicitly dependent on
    *X(t)*, it's not unreasonable to think of it like a *pursuer* of *X(t)*.
  prefs: []
  type: TYPE_NORMAL
- en: The Markov assumption holds for many real-life processes if either they are
    naturally first-order Markov ones, or if the states contain all the history needed
    to justify a transition. In other words, in many cases, if the state is *A*, then
    there's a transit to *B* and finally to *C*. We assume that when in *C*, the system
    moved from a state *(B)* that carries a part of the information provided by *A*.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we are filling a tank, we can measure the level (the state of
    our system) at time *t*, *t+1*, ... If the water flow is modeled by a random variable
    because we don't have a stabilizer, we can find the probability that the water
    has reached a certain level at time *t*, *p(L[t]=x|L[t-1])*. Of course, it doesn't
    make sense to condition over all the previous states, because if the level is,
    for example, 80 m at time t-1, all the information needed to determine the probability
    of a new level (state) at time *t* is already contained in this state (80 m).
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can start analyzing how to train a hidden Markov model, and
    how to determine the most likely hidden states given a sequence of observations.
    For simplicity, we call *A* the transition probability matrix, and *B* the matrix
    containing all *P(o[i]|x[t])*. The resulting model can be determined by the knowledge
    of those elements: *HMM = { A, B }*.'
  prefs: []
  type: TYPE_NORMAL
- en: Forward-backward algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **forward-backward algorithm** is a simple but effective method to find
    the transition probability matrix *T* given a sequence of observations *o[1],
    o[2], ..., o[t]*. The first step is called the *forward phase*, and consists of
    determining the probability of a sequence of observations *P(o[1], o[2], ...,
    o[Sequence Length]|A, B)*. This piece of information can be directly useful if
    we need to know the likelihood of a sequence and it's necessary, together with
    the *backward phase*, to estimate the structure (*A* and *B*) of the underlying
    HMM.
  prefs: []
  type: TYPE_NORMAL
- en: Both algorithms are based on the concept of dynamic programming, which consists
    of splitting a complex problem into sub-problems that can be easily solved, and
    reusing the solutions to solve more complex steps in a recursive/iterative fashion.
    For further information on this, please refer to *Dynamic Programming and Markov
    Process, Ronald A. Howard, The MIT Press*.
  prefs: []
  type: TYPE_NORMAL
- en: Forward phase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If we call *p[ij]* the transition probability *P(i → j)*, we define a recursive
    procedure considering the following probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa938322-d572-4c1e-98fc-f027a2787715.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The variable *f[t]^i* represents the probability that the HMM is in the state
    *i* (at time *t*) after *t* observations (from *1* to *t*). Considering the HMM
    assumptions, we can state that *f[t]*^(*i* )depends on all possible *f[t-1]^j*.
    More precisely, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e46b6f9b-3b8a-4129-b8fb-c0ceb4d53ece.png)'
  prefs: []
  type: TYPE_IMG
- en: With this process, we are considering that the HMM can reach any of the states
    at time *t-1* (with the first *t-1* observations), and transition to the state *i*
    at time *t* with probability *p[ji]*. We need also to consider the emission probability
    for the final state *o[t]* conditioned to each of the possible previous states.
  prefs: []
  type: TYPE_NORMAL
- en: 'For definition, the initial and ending states are not emitting. It means that
    we can write any sequence of observations as *0, o[1], o[2], ..., o[Sequence Length],
    0*, where the first and the final values are null. The procedure starts with computing
    the forward message at time *1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f825279c-c495-40c8-866c-4a7279c157a2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The non-emitting ending state must be also considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c62826ee-9eab-404a-b9b6-e3656e23da6f.png)'
  prefs: []
  type: TYPE_IMG
- en: The expression for the last state *x*[*Ending* ]is interpreted here as the index
    of the ending state in both *A* and *B* matrices. For example, we indicate *p[ij]*
    as *A[i, j]*, meaning the transition probability at a generic time instant from
    the state *x[t] = i* to the state *x[t+1] = j*. In the same way, *p[i][Ending]*
    is represented as *A[i, x[Ending]]*, meaning the transition probability from the
    penultimate state *x[Sequence Length-1] = i* to the ending one*x[Sequence Length]
    = Ending State*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Forward algorithm can, therefore, be summarized in the following steps
    (we assume to have *N* states, hence we need to allocate *N+2* positions, considering
    the initial and the ending states):'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization of a *Forward* vector with shape (*N + 2*, *Sequence Length*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialization of *A* (transition probability matrix) with shape (*N, N*). Each
    element is *P(x[i]|x[j]**)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialization of *B* with shape (*Sequence Length*, *N*). Each element is *P(o[i]|x[j]**)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *Forward[i, 1]* = *A[0, i] · B[1, i]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *t=2* to *Sequence Length-1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *S = 0*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *j=1* to *N*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *S = S + Forward[j, t-1] · A[j, i] · B[t, i]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *Forward[i, t] = S*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *S = 0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *S = S + Forward[i, Sequence Length] · A[i, x[Ending]]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *Forward[x[Ending], Sequence Length] = S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now it should be clear that the name **forward** derives from the procedure
    to propagate the information from the previous step to the next one, until the
    ending state, which is not emittied.
  prefs: []
  type: TYPE_NORMAL
- en: Backward phase
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'During the **backward phase**, we need to compute the probability of a sequence
    starting at time *t+1: o[t+1], o[t+2], ..., o[Sequence Length]*, given that the
    state at time *t* is *i*. Just like we have done before, we define the following
    probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c024525-9838-4223-a54f-c158fb18e0fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The backward algorithm is very similar to the forward one, but in this case,
    we need to move in the opposite direction, assuming we know that the state at
    time *t* is *i*. The first state to consider is the last one *x[Ending]*, which
    is not emitting, like the initial state; therefore we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9cae7840-24b7-4382-aad0-91eb97efa387.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We terminate the recursion with the initial state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d62d9350-f901-4766-a303-43d0ec77f12c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The steps are the following ones:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization of a vector *Backward* with shape *(N + 2, Sequence Length)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialization of *A* (transition probability matrix) with shape *(N, N)*. Each
    element is *P(x[i]|x[j]**)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialization of *B* with shape *(Sequence Length, N)*. Each element is* P(o[i]|x[j]**)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *Backward[x[Endind], Sequence Length] = A[i, x[Endind]]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *t=Sequence Length-1* to *1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *S = 0*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For *j=1* to *N*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *S = S + Backward[j, t+1] · A[j, i] · B[t+1, i]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *Backward[i, t] = S*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *S = 0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *S = S + Backward[i, 1] · A[0, i] · B[1, i]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *Backward[0, 1] = S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: HMM parameter estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have defined both the forward and the backward algorithms, we can
    use them to estimate the structure of the underlying HMM. The procedure is an
    application of the Expectation-Maximization algorithm, which will be discussed
    in the next chapter, [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM
    Algorithm and Applications*, and its goal can be summarized as defining how we
    want to estimate the values of *A* and *B*. If we define *N(i, j)* as the number
    of transitions from the state *i* to the state *j*, and *N(i)* the total number
    of transitions from the state *i*, we can approximate the transition probability
    *P(i → j)* with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/599b50d2-75d5-4c85-bc1f-e36280247077.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the same way, if we define *M(i, p)* the number of times we have observed
    the emission *o[p]* in the state *i*, we can approximate the emission probability *P(o[p]|x[i])*
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0efa14ed-4579-43c4-8458-49df67a3689a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start with the estimation of the transition probability matrix *A*.
    If we consider the probability that the HMM is in the state *i* at time *t*, and
    in the state *j* at time *t+1* given the observations, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c0c449d-33da-43c9-b894-c5c211bf45c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can compute this probability using the forward and backward algorithms,
    given a sequence of observations *o[1], o[2], ..., o[Sequence Length]*. In fact,
    we can use both the forward message *f[t]^i*, which is the probability that the
    HMM is in the state *i* after *t* observations, and the backward message *b[t+1]^j*,
    which is the probability of a sequence *o[t+1], o[t+1], ..., o[Sequence Length]*
    starting at time *t+1*, given that the HMM is in state *j* at time *t+1*. Of course,
    we need also to include the emission probability and the transition probability
    *p[ij]*, which is what we are estimating. The algorithm, in fact, starts with
    a random hypothesis and iterates until the values of *A* become stable. The estimation *α[ij]*
    at time *t* is equal to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24edfb52-1fa3-405e-a9a3-768e2ef640c7.png)'
  prefs: []
  type: TYPE_IMG
- en: In this context, we are omitting the full proof due to its complexity; however,
    the reader can find it in *A tutorial on hidden Markov models and selected applications
    in speech recognition,* *Rabiner L. R., Proceedings of the IEEE 77.2**.*
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the emission probabilities, it''s easier to start with the probability
    of being in the state *i* at time *t* given the sequence of observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d28be2d-6fc3-499d-86af-6ee4523a4625.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, the computation is immediate, because we can multiply the forward
    and backward messages computed at the same time *t* and state *i* (remember that
    considering the observations, the backward message is conditioned to *x[t] = i*,
    while the forward message computes the probability of the observations joined
    with *x[t] = i*. Hence, the multiplication is the unnormalized probability of
    being in the state *i* at time *t*). Therefore, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75ac8487-2059-47de-9904-5f7f0e9ed4fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The proof of how the normalizing constant is obtained can be found in the aforementioned
    paper. We can now plug these expressions to the estimation of *a[ij]* and *b[ip]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f1a3d0a-b4f0-4249-b65a-07c68b0a4ccf.png)'
  prefs: []
  type: TYPE_IMG
- en: In the numerator of the second formula, we adopted the indicator function (it's
    *1* only if the condition is true, *0* otherwise) to limit the sum only where
    those elements are *o[t] = p*. During an iteration *k*, *p[ij]* is the estimated
    value *a[ij]* found in the previous iteration *k-1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is based on the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly initialize the matrices *A* and *B*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a tolerance variable *Tol* (for example, *Tol = 0.001*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *Norm(A^k - A^(k-1)) > Tol* and *Norm(B^k - B^(k-1)**) > Tol* (*k* is
    the iteration index):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *t=1* to *Sequence Length-1*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *j=1* to *N*:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *α^t*[*ij*]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute *β^t[i]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the estimations of *a[ij]* and *b[ip]* and store them in *A^k*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Alternatively, it's possible to fix the number of iterations, even if the best
    solution is using both a tolerance and a maximum number of iterations, to terminate
    the process when the first condition is met.
  prefs: []
  type: TYPE_NORMAL
- en: Example of HMM training with hmmlearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this example, we are going to use hmmlearn, which is a package for HMM computations
    (see the information box at the end of this section for further details). For
    simplicity, let's consider the airport example discussed in the paragraph about
    the Bayesian networks, and let's suppose we have a single hidden variable that
    represents the weather (of course, this is not a real hidden variable!), modeled
    as a multinomial distribution with two components (good and rough).
  prefs: []
  type: TYPE_NORMAL
- en: We observe the arrival time of our flight London-Rome (which partially depends
    on the weather conditions), and we want to train an HMM to infer future states
    and compute the posterior probability of hidden states corresponding to a given
    sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'The schema for our example is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f88ad311-0701-48b9-9f86-024a2edf0510.png)'
  prefs: []
  type: TYPE_IMG
- en: HMM for the weather-arrival delay problem
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by defining our observation vector. As we have two states, its
    values will be `0` and `1`. Let''s assume that `0` means **On-time** and `1` means **Delay**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We have 35 consecutive observations whose values are either `0` or `1`.
  prefs: []
  type: TYPE_NORMAL
- en: To build the HMM, we are going to use the `MultinomialHMM` class, with `n_components=2`, `n_iter=100`,
    and `random_state=1000` (it's important to always use the same seed to avoid differences
    in the results). The number of iterations is sometimes hard to determine; for
    this reason, hmmlearn provides a utility `ConvergenceMonitor` class which can
    be checked to be sure that the algorithm has successfully converged.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can train our model using the `fit()` method, passing as argument the
    list of observations (the array must be always bidimensional with shape *Sequence
    Length × N[Components]*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The process is very fast, and the monitor (available as instance variable `monitor`)
    has confirmed the convergence. If the model is very big and needs to be retrained,
    it''s also possible to check smaller values of `n_iter`). Once the model is trained,
    we can immediately visualize the transition probability matrix, which is available
    as an instance variable `transmat_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can interpret these values as saying that the probability to transition from
    `0` (good weather) to `1` (rough weather) is higher (*p[01]* is close to *1*)
    than the opposite, and it's more likely to remain in state `1` than in state `0`
    (*p[00]* is almost null). We could deduce that the observations have been collected
    during the winter period! After explaining the Viterbi algorithmin the next paragraph,
    we can also check, given some observations, what the most likely hidden state
    sequence is.
  prefs: []
  type: TYPE_NORMAL
- en: hmmlearn ([http://hmmlearn.readthedocs.io/en/latest/index.html](http://hmmlearn.readthedocs.io/en/latest/index.html))
    is a framework originally built to be a part of Scikit-Learn. It supports multinomial
    and Gaussian HMM, and allows training and inferring using the most common algorithms.
    It can be installed using the `pip install hmmlearn` command.
  prefs: []
  type: TYPE_NORMAL
- en: Viterbi algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **Viterbi algorithm** is one of most common decoding algorithms for HMM.
    Its goal is to find the most likely hidden state sequence corresponding to a series
    of observations. The structure is very similar to the forward algorithm, but instead
    of computing the probability of a sequence of observations joined with the state
    at the last time instant, this algorithm looks for:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ba7094a-03d5-4453-9755-557392e2bffd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The variable *v[t]^i* represents that maximum probability of the given observation
    sequence joint with *x[t] = i*, considering all possible hidden state paths (from
    time instant *1* to *t-1*). We can compute *v[t]^i* recursively by evaluating
    all the *v[t-1]^j* multiplied by the corresponding transition probabilities *p[ji]*
    and emission probability *P(o[t]|x[i])*, and always picking the maximum overall possible
    values of *j*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09735a93-2d90-4f2f-b9fe-125a6626c989.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The algorithm is based on a backtracking approach, using a backpointer *bp[t]^i*
    whose recursive expression is the same as *v[t]^i*, but with the *argmax* function
    instead of *max*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/613be74e-74c1-4af3-b1ce-a250c35c1a21.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, *bp[t]*^(*i* )represents the partial sequence of hidden states *x**[1],
    x[2], ..., x*[*t-1* ]that maximizes* v[t]^i*. During the recursion, we add the
    timesteps one by one, so the previous path could be invalidated by the last observation.
    That's why we need to backtrack the partial result and replace the sequence built
    at time *t* that doesn't maximize *v[t+1]^i* anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm is based on the following steps (like in the other cases, the
    initial and ending states are not emitting):'
  prefs: []
  type: TYPE_NORMAL
- en: Initialization of a vector *V* with shape *(N + 2, Sequence Length)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialization of a vector *BP* with shape *(N + 2, Sequence Length)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialization of *A* (transition probability matrix) with shape *(N, N)*. Each
    element is *P(x[i]|x[j]**)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialization of *B* with shape *(Sequence Length, N)*. Each element is *P(o[i]|x[j]**)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *V[i, 1]* = *A[i, 0] **· B[1, i]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*BP[i, 1]* = Null (or any other value that cannot be interpreted as a state)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *t=1* to *Sequence Length*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *N*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *V[i, t] = max[j] V[j, t-1] · A[j, i] · B[t, i]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *BP[i, t] = argmax[j] V[j, t-1] · A[j, i] · B[t, i]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *V[x[Endind], Sequence Length] = max[j] V[j, Sequence Length] **· A[j, x[Endind]]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *BP**[x[Endind], Sequence Length] = argmax[j] V[j, Sequence Length] **·
    A[j, x[Endind]]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reverse *BP*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output of the Viterbi algorithm is a tuple with the most likely sequence
    *BP*, and the corresponding probabilities *V*.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the most likely hidden state sequence with hmmlearn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we can continue with the previous example, using our model to
    find the most likely hidden state sequence given a set of possible observations.
    We can use either the `decode()` method or the `predict()` method. The first one
    returns the log probability of the whole sequence and the sequence itself; however,
    they all use the Viterbi algorithm as a default decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The sequence is coherent with the transition probability matrix; in fact, it''s
    more likely the persistence of rough weather (`1`) than the opposite. As a consequence,
    the transition from `1` to X is less likely than the one from `0` to `1`. The
    choice of state is made by selecting the highest probability; however, in some
    cases, the differences are minimal (in our example, it can happen to have *p =
    [0.49, 0.51]*, meaning that there''s a high error chance), so it''s useful to
    check the posterior probabilities for all the states in the sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In our case, there are a couple of states that have *p ∼ [0.495, 0.505]*, so
    even if the output state is *1* (rough weather), it's also useful to consider
    a moderate probability to observe good weather. In general, if a sequence is coherent
    with the transition probability previously learned (or manually input), those
    cases are not very common. I suggest trying different configurations and observations
    sequences, and to also assess the probabilities for the *strangest* situations
    (like a sequence of zero second). At that point, it's possible to retrain the
    model and recheck the new evidence has been correctly processed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have introduced Bayesian networks, describing their structure
    and relations. We have seen how it's possible to build a network to model a probabilistic
    scenario where some elements can influence the probability of others. We have
    also described how to obtain the full joint probability using the most common
    sampling methods, which allow reducing the computational complexity through an
    approximation.
  prefs: []
  type: TYPE_NORMAL
- en: The most common sampling methods belong to the family of MCMC algorithms, which
    model the transition probability from a sample to another one as a first-order
    Markov chain. In particular, the Gibbs sampler is based on the assumption that
    it's easier to sample from conditional distribution than work directly with the
    full joint probability. The method is very easy to implement, but it has some
    performance drawbacks that can be avoided by adopting more complex strategies.
    The Metropolis-Hastings sampler, instead, works with a candidate-generating distribution
    and a criterion to accept or reject the samples. Both methods satisfy the detailed
    balance equation, which guarantees the convergence (the underlying Markov chain
    will reach the unique stationary distribution).
  prefs: []
  type: TYPE_NORMAL
- en: In the last part of the chapter, we introduced HMMs, which allow modeling time
    sequences based on observations corresponding to a series of hidden states. The
    main concept of such models, in fact, is the presence of unobservable states that
    condition the emission of a particular observation (which is observable). We have
    discussed the main assumptions and how to build, train, and infer from a model.
    In particular, the Forward-Backward algorithm can be employed when it's necessary
    to learn the transition probability matrix and the emission probabilities, while
    the Viterbi algorithm is adopted to find the most likely hidden state sequence
    given a set of consecutive observations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 5](8d541a43-8790-4a91-a79b-e48496f75d90.xhtml), *EM
    Algorithm and Applications*, we're going to briefly discuss the Expectation-Maximization
    algorithm, focusing on some important applications based on the **Maximum Likelihood
    Estimation** (**MLE**) approach.
  prefs: []
  type: TYPE_NORMAL
