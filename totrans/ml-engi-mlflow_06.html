<html><head></head><body>
		<div id="_idContainer058">
			<h1 id="_idParaDest-66"><a id="_idTextAnchor081"/><em class="italic">Chapter 4</em>: Experiment Management in MLflow</h1>
			<p>In this chapter, we will give you practical experience with stock predictions by creating different models and comparing metrics of different runs in MLflow. You will be guided in terms of how to use the MLflow experiment method so that different machine learning practitioners can share metrics and improve on the same model.</p>
			<p>Specifically, we will look at the following topics in this chapter:</p>
			<ul>
				<li>Getting started with the experiments module</li>
				<li>Defining the experiment</li>
				<li>Adding experiments</li>
				<li>Comparing different models</li>
				<li>Tuning your model with hyperparameter optimization</li>
			</ul>
			<p>At this stage, we currently have a baseline pipeline that acts based on a naïve heuristic. In this chapter, we will add to our set of skills the ability to experiment with multiple models and tune one specific model using MLflow.</p>
			<p>We will be delving into our <strong class="bold">Psystock</strong> company use case of a stock trading machine learning platform introduced in <a href="B16783_02_Final_SB_epub.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>, <em class="italic">Your Machine Learning Project</em>. In this chapter, we will add to our platform to compare multiple models and run experiments in the benchmark to be able to create a predictor for a specific stock and ticker.</p>
			<p>In data science functions, a common methodology is to develop a model for a specific model that involves the following three steps: creating baseline models with different model types, identifying the best performant model, and predicting with the best model.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor082"/>Technical requirements</h1>
			<p>For this chapter, you will need the following prerequisites:</p>
			<ul>
				<li>The latest version of Docker installed on your machine. If you don't already have it installed, please follow the instructions at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
				<li>The latest version of Docker Compose installed. Please follow the instructions at <a href="https://docs.docker.com/compose/install/">https://docs.docker.com/compose/install/</a>.</li>
				<li>Access to Git in the command line and installed as described in <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>.</li>
				<li>Access to a bash terminal (Linux or Windows).</li>
				<li>Access to a browser.</li>
				<li>Python 3.5+ installed.</li>
				<li>The latest version of your machine learning installed locally and described in <a href="B16783_03_Final_SB_epub.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a>, <em class="italic">Your</em> <em class="italic">Data Science Workbench</em>.</li>
			</ul>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor083"/>Getting started with the experiments module</h1>
			<p>To get started with the technical modules, you will need to get started with the environment prepared for this chapter in the following folder: <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter04">https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter04</a></p>
			<p>You should be able, at this stage, to execute the <strong class="source-inline">make</strong> command to build up your workbench with the dependencies needed to follow along with this chapter. You need next to type the following command to move to the right directory:</p>
			<p class="source-code">$ cd Chapter04/gradflow/</p>
			<p>To start the environment, you need to run the following command:</p>
			<p class="source-code">$ make</p>
			<p>The entry point to start managing experimentation in <strong class="bold">MLflow</strong> is the experiments interface illustrated in <em class="italic">Figure 4.1</em>:</p>
			<div>
				<div id="_idContainer040">
					<p>2</p>
				</div>
			</div>
			<div>
				<div id="_idContainer041">
					<p> 1</p>
				</div>
			</div>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B16783_04_01.jpg" alt="Figure 4.1 – The Experiments interface in MLflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 4.1 – The Experiments interface in MLflow</p>
			<p>On the left pane (1), you can <a id="_idIndexMarker136"/>manage and create experiments, and on the right (2), you can query details of a specific experiment.</p>
			<p>To create a new experiment, you need to click on the <strong class="bold">+</strong> button on the left pane and add the details of your experiment, as illustrated by <em class="italic">Figure 4.2</em>:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B16783_04_02.jpg" alt="Figure 4.2 – Creating new experiments&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – Creating new experiments</p>
			<p>Having introduced <a id="_idIndexMarker137"/>at a high level the tracking server and the experiment management features, we will now proceed to use the features available on our workbench to tackle the challenges of the current chapter.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor084"/><a id="_idTextAnchor085"/>Defining the experiment</h1>
			<p>Using the <a id="_idIndexMarker138"/>machine learning problem framing methodology, we will now define the main components of our stock price prediction problem as defined for the chapter:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B16783_04_Table_(1).jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B16783_04_Table_(2).jpg" alt="Table 4.1 – Machine learning problem framing recap&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 4.1 – Machine learning problem framing recap</p>
			<p>The <strong class="bold">F-score</strong> metric in <a id="_idIndexMarker139"/>machine learning is a measure of accuracy for binary <a id="_idIndexMarker140"/>classifiers and provides a good balance and trade-off between misclassifications (false positives or false negatives). Further details can be found on the Wikipedia page: <a href="https://en.wikipedia.org/wiki/F-score">https://en.wikipedia.org/wiki/F-score</a>.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor086"/>Exploring the dataset</h2>
			<p>As specified <a id="_idIndexMarker141"/>in our machine learning problem framing, we will use as input data the market observations for the period January-December 2020, as provided by the Yahoo data API.</p>
			<p>The following code excerpt, which uses the <strong class="source-inline">pandas_datareader</strong> module available in our workbench, allows us to easily retrieve the data that we want. The complete working notebook is available at <a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter04/gradflow/notebooks/retrieve_training_data.ipynb">https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter04/gradflow/notebooks/retrieve_training_data.ipynb</a>:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import datetime</p>
			<p class="source-code">import pandas_datareader.data as web</p>
			<p class="source-code">from pandas import Series, DataFrame</p>
			<p class="source-code">start = datetime.datetime(2014, 1, 1)</p>
			<p class="source-code">end = datetime.datetime(2020, 12, 31)</p>
			<p class="source-code">btc_df = web.DataReader("BTC-USD", 'yahoo', start, end)</p>
			<p>For this particular problem, we will retrieve data from 2014 up to the end of 2020, as represented in the table provided in <em class="italic">Figure 4.3</em>. The table provides value information about High, Low, Open, and Close for the BTC stock of the trading section. This data will be used to train the models in the current chapter:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B16783_04_03.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Listing the data retrieved from the source (Yahoo Finance)</p>
			<p>This data <a id="_idIndexMarker142"/>can easily be plotted by plotting one of the variables just to illustrate the continuous nature of the data:</p>
			<p class="source-code">btc_df['Open'].plot()</p>
			<p>To illustrate a bit more about the nature of the data, we can plot an excerpt of the data:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B16783_04_04.jpg" alt="Figure 4.4 – Plot of one of the variables BTC Open retrieved from the source (Yahoo Finance)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Plot of one of the variables BTC Open retrieved from the source (Yahoo Finance)</p>
			<p>Having defined precisely what we will be experimenting with in this section, we will move to add new models to enable us to run experiments and compare among them.</p>
			<p>The data for <a id="_idIndexMarker143"/>the required range was conveniently saved in a file under <strong class="source-inline">Chapter04/gradflow/notebooks/training_data.csv</strong>, for the period ranging from 2014 to 2020 inclusive, so it can be easily retrieved during the modeling phase.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor087"/>Adding experiments</h1>
			<p>So, in this section, we will use the experiments module in <strong class="bold">MLflow</strong> to track the different runs of <a id="_idIndexMarker144"/>different models and post them in our workbench database so that the performance results can be compared side by side.</p>
			<p>The experiments can actually be done by different model developers as long as they are all pointing to a shared MLflow infrastructure.</p>
			<p>To create our first, we will pick a set of model families and evaluate our problem on each of the cases. In broader terms, the major families for classification can be tree-based models, linear models, and neural networks. By looking at the metric that performs better on each of the cases, we can then direct tuning to the best model and use it as our initial model in production.</p>
			<p>Our choice for this section includes the following:</p>
			<ul>
				<li><strong class="bold">Logistic Classifier</strong>: Part of <a id="_idIndexMarker145"/>the family of linear-based models and a commonly used baseline.</li>
				<li><strong class="bold">Xgboost</strong>: This belongs <a id="_idIndexMarker146"/>to the family of tree boosting algorithms where many weak tree classifiers are assembled to produce a stronger model. </li>
				<li><strong class="bold">Keras</strong>: This type <a id="_idIndexMarker147"/>of model belongs to the neural network's family and is generally indicated for situations where there is a lot of data available and relations are not linear between the features.</li>
			</ul>
			<p>The steps to set up a new model are quite common and there will be overlapping and repeated code for each of the models. We will start next with a logistic regression-based classifier.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor088"/>Steps for setting up a logistic-based classifier</h2>
			<p>In this sub-section, we will implement a logistic regression classifier in <strong class="source-inline">scikit-learn</strong> and train a model with our input data.</p>
			<p>The <a id="_idIndexMarker148"/>complete notebook <a id="_idIndexMarker149"/>for this model is available in the book's repository and can be used to follow along in the <strong class="source-inline">Chapter04/gradflow/notebooks/mlflow_run_logistic_regression.ipynb</strong> file:</p>
			<ol>
				<li><strong class="bold">Importing dependencies</strong>: This section has the most important dependencies, apart from the foundational ones (pandas, NumPy, and MLflow), that we need to import the <strong class="source-inline">SKLearn</strong> model, <strong class="source-inline">LogisticRegression</strong>, and the metrics functionality, <strong class="source-inline">f1_score</strong>, that will enable us to calculate the performance:<p class="source-code">import pandas</p><p class="source-code">import numpy as np</p><p class="source-code">import mlflow</p><p class="source-code">import tensorflow</p><p class="source-code">from tensorflow import keras</p><p class="source-code">import mlflow.keras</p><p class="source-code">from sklearn.metrics import f1_score,confusion_matrix</p><p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li><strong class="bold">Setting up training data</strong>: The data is read from the <strong class="source-inline">training_data.csv</strong> file:<p class="source-code">pandas_df = pandas.read_csv("training_data.csv")</p><p class="source-code">X=pandas_df.iloc[:,:-1]</p><p class="source-code">Y=pandas_df.iloc[:,-1]</p><p class="source-code">X_train, X_test, y_train, y_test = \</p><p class="source-code">train_test_split(X, Y, test_size=0.33, </p><p class="source-code">                 random_state=4284, stratify=Y)</p><p>The data is split into training and testing using the <strong class="source-inline">train_test_split</strong> function, which takes one-third of the data for testing, with the remainder being used for training.</p></li>
				<li><strong class="bold">Setting up the experiment</strong>: To set the experiment in <strong class="bold">MLflow</strong> programmatically, you use the <strong class="source-inline">mlflow.set_experiment</strong> method. This will create an experiment if it does not exist or associate your current run with an experiment. We use <strong class="source-inline">mlflow.sklearn.autolog()</strong> to enable the automated capabilities of MLflow to capture the metrics of our experiment:<p class="source-code">mlflow.set_experiment("Baseline_Predictions")</p><p class="source-code">mlflow.sklearn.autolog()</p></li>
				<li><strong class="bold">Running the experiment</strong>: To run your experiment, you will have to enclose it in a run using the scope keyword, <strong class="source-inline">with</strong>. The <strong class="source-inline">mlflow.start_run</strong> function is used to take care of registering your run with a specific <strong class="source-inline">run_name</strong> so that it can be identified and encloses the <strong class="source-inline">fit</strong> model, with evaluation code used to calculate the performance metrics of the <strong class="source-inline">f1_score</strong> experiment:<p class="source-code">with mlflow.start_run(run_name='logistic_regression_model_baseline') as run:</p><p class="source-code">    model = LogisticRegression()</p><p class="source-code">    model.fit(X_train, y_train)</p><p class="source-code">    preds = model.predict(X_test)</p><p class="source-code">    y_pred = np.where(preds&gt;0.5,1,0)</p><p class="source-code">    f1 = f1_score(y_test, y_pred)</p><p class="source-code">    mlflow.log_metric(key="f1_experiment_score", </p><p class="source-code">                      value=f1)</p><p>Additionally, we need to log our specific metric, <strong class="source-inline">f1_experiment_score</strong>, with the <strong class="source-inline">mlflow.log_metric</strong> function. The main reason for adding our specific method is that for each model, the autologging functionality in <strong class="bold">MLflow</strong> uses the default metric used by each underlying framework and generally, these metrics don't match.</p></li>
			</ol>
			<p>After executing all the steps relating to model development, we can now navigate to our run and <a id="_idIndexMarker150"/>visualize the log of the experiment. In <em class="italic">Figure 4.5</em>, you can see the specific parameters associated with logistic regression, durations, and all the parameters used on your run:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B16783_04_05.jpg" alt="Figure 4.5 – Logistic regression model details &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Logistic regression model details </p>
			<p>For <strong class="source-inline">SKLearn</strong> models, <strong class="bold">MLflow</strong> automatically logs confusion matrices and precision and recall curves that are very useful in detecting how well the model performed on training data. For instance, the <em class="italic">Figure 4.6</em> report will be stored in the artifacts of your run:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B16783_04_06.jpg" alt="Figure 4.6 – Confusion matrix metrics&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Confusion matrix metrics</p>
			<p>MLflow <a id="_idIndexMarker151"/>provides built-in metrics for Sklearn, providing better visibility of the model produced during training without the developer needing to produce extra code.</p>
			<h3>Steps for setting up an XGBoost-based classifier</h3>
			<p>We will <a id="_idIndexMarker152"/>now implement a gradient tree-based algorithm using the <strong class="source-inline">XGBoost</strong> library.</p>
			<p>The <a id="_idIndexMarker153"/>complete notebook for this model is available in the book's repository and can be used to follow along in the <strong class="source-inline">Chapter04/gradflow/notebooks/mlflow_run_xgboost.ipynb</strong> file:</p>
			<p><strong class="bold">Importing dependencies</strong>: The XGBoost library is imported alongside the metrics function:</p>
			<p class="source-code">import pandas</p>
			<p class="source-code">import mlflow</p>
			<p class="source-code">import xgboost as xgb</p>
			<p class="source-code">from sklearn.metrics import f1_score</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<ol>
				<li value="1"><strong class="bold">Retrieving data</strong>: This step remains the same as we are splitting data and reading the data from the <strong class="source-inline">training_data.csv</strong> file.</li>
				<li><strong class="bold">Setting up the experiment</strong>: The experiment is still the same, <strong class="source-inline">Baseline_Predictions</strong>, and we need to give MLflow the instruction to automatically <a id="_idIndexMarker154"/> log the model through <strong class="source-inline">mlflow.xgboost.autolog</strong>:<p class="source-code">mlflow.set_experiment("Baseline_Predictions")</p><p class="source-code">mlflow.xgboost.autolog()</p></li>
				<li><strong class="bold">Running the experiment</strong>: This experiment is very similar to the previous case where <a id="_idIndexMarker155"/>we run the model and evaluate the metrics through <strong class="source-inline">f1_score</strong>:<p class="source-code">with mlflow.start_run(</p><p class="source-code">  run_name='xgboost_model_baseline') as run:</p><p class="source-code">    model=xgb.train(dtrain=dtrain,params={})</p><p class="source-code">    preds = model.predict(dtest)</p><p class="source-code">    y_bin = [1. if y_cont &gt; threshold else 0. for y_cont in preds]</p><p class="source-code">    f1= f1_score(y_test,y_bin)</p><p class="source-code">    mlflow.log_metric(key="f1_experiment_score", </p><p class="source-code">                      value=f1)</p></li>
			</ol>
			<p>After executing all the steps relating to model development, we can now navigate to our run and visualize the log of the experiment. In <em class="italic">Figure 4.7</em>, you can see the specific parameters associated with <strong class="source-inline">xgboost_model_baseline</strong>, durations, and all the parameters used on your run:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B16783_04_07.jpg" alt="Figure 4.7 – XGBoost classifier details in MLflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – XGBoost classifier details in MLflow</p>
			<p>For XGBoost models, <strong class="bold">MLflow</strong> automatically logs feature information and importance. We can <a id="_idIndexMarker156"/>see in <em class="italic">Figure 4.8</em> the ranking of our features in the model stored in the <em class="italic">Artifacts</em> section of the workbench:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B16783_04_08.jpg" alt="Figure 4.8 – XGBoost feature importance on MLflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – XGBoost feature importance on MLflow</p>
			<p>The feature importance graph in <em class="italic">Figure 4.8</em> allows the developer to have some insights into the <a id="_idIndexMarker157"/>internals of the model ascertained from the data. In this particular case, it seems that the second and seventh days of the 14 days in the input vector are the top two meaningful features. We will next implement a deep learning-based model.</p>
			<h3>Steps for setting up a deep learning-based classifier</h3>
			<p>In this <a id="_idIndexMarker158"/>section, we will <a id="_idIndexMarker159"/>implement a neural network algorithm to solve our classification problem.</p>
			<p>The complete notebook for this model is available in the book's repository and can be used to follow along in the Chapter04/gradflow/notebooks/mlflow_run_keras.ipynb file:</p>
			<ol>
				<li value="1"><strong class="bold">Importing dependencies</strong>: The salient dependency in this step is <strong class="source-inline">tensorflow</strong>, as we are using it as a backend for <strong class="source-inline">keras</strong>:<p class="source-code">import pandas</p><p class="source-code">import numpy as np</p><p class="source-code">import mlflow</p><p class="source-code">import tensorflow</p><p class="source-code">from tensorflow import keras</p><p class="source-code">import mlflow.keras</p><p class="source-code">from sklearn.metrics import f1_score,confusion_matrix</p><p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li><strong class="bold">Retrieving data</strong>: Refer to Step 2 in the <em class="italic">Steps for setting up an XGBoost-based classifier</em> section.</li>
				<li><strong class="bold">Setting up the experiment</strong>: The experiment is still the same, <strong class="source-inline">Baseline_Predictions</strong>, and we need to give MLflow the instruction to automatically <a id="_idIndexMarker160"/>log the model through <strong class="source-inline">mlflow.tensorflow.autolog</strong>:<p class="source-code"> mlflow.set_experiment("Baseline_Predictions")</p><p class="source-code"> mlflow.tensorflow.autolog()</p><p><strong class="bold">Creating the model</strong>: One of the big differences compared with the neural-based model is that the creation of the model is a bit more involved than <strong class="source-inline">Sklearn</strong> or XGBoost classifiers, so we need to define the layers and architecture of the network. In this particular case, the <strong class="source-inline">Sequential</strong> architecture and the model need to be compiled as required by Tensorflow:</p><p class="source-code">model = keras.Sequential([</p><p class="source-code">  keras.layers.Dense(</p><p class="source-code">    units=36,</p><p class="source-code">    activation='relu',</p><p class="source-code">    input_shape=(X_train.shape[-1],)</p><p class="source-code">  ),</p><p class="source-code">  keras.layers.BatchNormalization(),</p><p class="source-code">  keras.layers.Dense(units=1, activation='sigmoid'),</p><p class="source-code">])</p><p class="source-code">model.compile(</p><p class="source-code">  optimizer=keras.optimizers.Adam(lr=0.001),</p><p class="source-code">  loss="binary_crossentropy",</p><p class="source-code">  metrics="Accuracy"</p><p class="source-code">)</p></li>
				<li><strong class="bold">Running the model</strong>: Running the model involves the same steps as specifying <strong class="source-inline">run_name</strong> and fitting <a id="_idIndexMarker161"/>the model followed by calculating the <strong class="source-inline">f1_score</strong> metrics:<p class="source-code">with mlflow.start_run(</p><p class="source-code">  run_name='keras_model_baseline') as run:</p><p class="source-code">    model.fit(</p><p class="source-code">        X_train,</p><p class="source-code">        y_train,</p><p class="source-code">        epochs=20,</p><p class="source-code">        validation_split=0.05,</p><p class="source-code">        shuffle=True,</p><p class="source-code">        verbose=0</p><p class="source-code">    )</p><p class="source-code">    preds = model.predict(X_test)</p><p class="source-code">    y_pred = np.where(preds&gt;0.5,1,0)</p><p class="source-code">    f1 = f1_score(y_test, y_pred)</p><p class="source-code">    mlflow.log_metric(key="f1_experiment_score", </p><p class="source-code">                      value=f1)</p><p>For <strong class="source-inline">keras</strong> models, <strong class="bold">MLflow</strong> automatically logs a myriad of neural network-related data, namely, regarding optimizers and epoch and batch sizes, as well as other <a id="_idIndexMarker162"/>relevant information that can be seen in <em class="italic">Figure 4.9</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B16783_04_09.jpg" alt="Figure 4.9 – Keras classifier model details&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – Keras classifier model details</p>
			<p>Additionally, <strong class="bold">TensorFlow</strong> logs can be hooked into a TensorBoard. This is a TensorFlow built-in tool to provide visualizations and metrics for the machine learning workflow. Interfaces are created so that the model developer can leverage the native TensorFlow instrumentation and specialized visualization tooling.</p>
			<p>Having set up our classifiers in our platform, in the next section, we are ready to compare the performance of the different classifiers developed using MLflow. </p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor089"/>Comparing different models</h1>
			<p>We have run the experiments in this section for each of the models covered and verified all the different <a id="_idIndexMarker163"/>artifacts. Just by looking at our baseline experiment table, and by selecting the common custom metric, <strong class="source-inline">f1_experiment_score</strong>, we can see that the best performing model is the logistic regression-based model, with an F-score of 0.66:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B16783_04_10.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Comparing different model performance in terms of the goal metric</p>
			<p>Metrics can also be compared side by side, as shown in the excerpt in <em class="italic">Figure 4.11</em>. On the left side, we have the <strong class="source-inline">SKlearn</strong> model, and on the right the XGBoost model, with the custom metrics of <strong class="source-inline">f1_experiment_score</strong>. We can see that the metrics provided by both are different and, hence, the reason for custom metrics when we have different models:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B16783_04_11.jpg" alt="Figure 4.11 – Metrics of the Sklearn model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – Metrics of the Sklearn model</p>
			<p>After comparing <a id="_idIndexMarker164"/>the metrics, it becomes clear that the best model is logistic regression. To improve the model, in the next section, we will optimize its parameters with state-of-the-art techniques and use MLflow experiment features to achieve that.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor090"/>Tuning your model with hyperparameter optimization</h1>
			<p>Machine learning models have many parameters that allow the developer to improve performance <a id="_idIndexMarker165"/>and control the model that they are using, providing leverage to better fit the <a id="_idIndexMarker166"/>data and production use cases. Hyperparameter optimization is the systematic and automated process of identifying the optimal parameters for your machine learning model and is critical for the successful deployment of such a system.</p>
			<p>In the previous section, we identified the best family (in other words, <strong class="source-inline">LogisticRegression</strong>) model for our problem, so now it's time to identify the right parameters for our model with MLflow. You can follow along in the following notebook in the project repository, Chapter04/gradflow/notebooks/hyperopt_optimization_logistic_regre<a id="_idTextAnchor091"/>ssion_mlflow.ipynb:</p>
			<ol>
				<li value="1"><strong class="bold">Importing dependencies</strong>: We will use the <strong class="source-inline">hyperopt</strong> library, which contains multiple algorithms to help us carry out model tuning:<p class="source-code">from hyperopt import tpe</p><p class="source-code">from hyperopt import STATUS_OK</p><p class="source-code">from hyperopt import Trials</p><p class="source-code">from hyperopt import hp</p><p class="source-code">from hyperopt import fmin</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.model_selection import cross_val_score</p><p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li><strong class="bold">Defining an objective function</strong>: The objective function is the most important step of the process, essentially defining what we want to achieve with <a id="_idIndexMarker167"/>our optimization. In our particular case, we want to optimize the <strong class="source-inline">f1_score</strong> metric in our model. The way optimization works in <strong class="source-inline">hyperopt</strong> is through <a id="_idIndexMarker168"/>minimization, but in our case, we want the maximum possible <strong class="source-inline">f1_score</strong> metric. So, the way we define our loss (the function to minimize) is as the inverse of our <strong class="source-inline">f1_score</strong> metric, as in <strong class="source-inline">loss = 1-fscore</strong>, so the minimization of this function will represent the best <strong class="source-inline">f1_score</strong> metric. For each run of the model's parameters, we will enclose it in an <strong class="source-inline">mlflow.start_run(nested=True)</strong> in such a way that each optimization iteration will be logged as a sub run of the main job, providing multiple advantages in terms of comparing metrics across runs:<p class="source-code">N_FOLDS = 3</p><p class="source-code">MAX_EVALS = 10</p><p class="source-code">def objective(params, n_folds = N_FOLDS):</p><p class="source-code">    # Perform n_fold cross validation with </p><p class="source-code">    #hyperparameters</p><p class="source-code">    # Use early stopping and evaluate based on ROC AUC</p><p class="source-code">    mlflow.sklearn.autolog()</p><p class="source-code">    with mlflow.start_run(nested=True):</p><p class="source-code">        clf = LogisticRegression(**params,</p><p class="source-code">                                 random_state=0,</p><p class="source-code">                                 verbose =0)</p><p class="source-code">        scores = cross_val_score(clf, X_train, </p><p class="source-code">                                 y_train, cv=5, </p><p class="source-code">                                 scoring='f1_macro')</p><p class="source-code">        # Extract the best score</p><p class="source-code">        best_score = max(scores)</p><p class="source-code">        # Loss must be minimized</p><p class="source-code">        loss = 1 - best_score</p><p class="source-code">        # Dictionary with information for evaluation</p><p class="source-code">        return {'loss': loss, 'params': params, </p><p class="source-code">                'status': STATUS_OK}</p></li>
				<li><strong class="bold">Running optimization trials</strong>: The trials step allows us to run multiple experiments with the logistic regression algorithm and help us identify the best possible <a id="_idIndexMarker169"/>configuration for our model, which will be stored under the <strong class="source-inline">best</strong> variable. The <a id="_idIndexMarker170"/>core function is the minimization represented by <strong class="source-inline">fmin(fn = objective, space = space, algo = tpe.suggest, max_evals = MAX_EVALS, trials = bayes_trials)</strong>, where we provide the parameter space and objective function as previously defined:<p class="source-code"># Algorithm</p><p class="source-code">tpe_algorithm = tpe.suggest</p><p class="source-code"># Trials object to track progress</p><p class="source-code">bayes_trials = Trials()</p><p class="source-code">mlflow.set_experiment("Bayesian_param_tuning")</p><p class="source-code">with mlflow.start_run():</p><p class="source-code">    best = fmin(fn = objective, space = space, </p><p class="source-code">                algo = tpe.suggest, </p><p class="source-code">                max_evals = MAX_EVALS, </p><p class="source-code">                trials = bayes_trials)</p></li>
				<li>After running the experiment for a few minutes, we can now review the experiments in <strong class="bold">MLflow</strong>. <em class="italic">Figure 4.12</em> represents the experiment and the nested hierarchy of the multiple experiments run under the umbrella of the <strong class="source-inline">Hyperopt_Optimization</strong> experiment:<div id="_idContainer055" class="IMG---Figure"><img src="image/B16783_04_12.jpg" alt="Figure 4.12 – Listing all the nested runs of the hyperparameter tuning&#13;&#10;"/></div><p class="figure-caption">Figure 4.12 – Listing all the nested runs of the hyperparameter tuning</p></li>
				<li>By clicking <a id="_idIndexMarker171"/>on the <strong class="bold">compare</strong> option, we have the results displayed in <em class="italic">Figure 4.13</em>. You can <a id="_idIndexMarker172"/>analyze multiple runs of the optimization of parameters in sequence, reviewing the implication of specific parameters in relation to performance metrics such as <strong class="source-inline">training_f1_score</strong> and the solver:<div id="_idContainer056" class="IMG---Figure"><img src="image/B16783_04_13.jpg" alt=""/></div><p class="figure-caption">Figure 4.13 – Listing all the nested runs of the hyperparameter tuning</p></li>
				<li>We can <a id="_idIndexMarker173"/>easily compare in the same interface the different solvers and implications <a id="_idIndexMarker174"/>for our performance metric, providing further insights into our modeling phase:</li>
			</ol>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B16783_04_14.jpg" alt="Figure 4.14 – Listing all the nested runs of the hyperparameter tuning&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14 – Listing all the nested runs of the hyperparameter tuning</p>
			<p>We <a id="_idIndexMarker175"/>concluded this section by optimizing the parameters of the most performant model for our <a id="_idIndexMarker176"/>current problem. In the next chapter of the book, we will be using the information provided by the best model to delve into the life cycle of the model management in <strong class="bold">MLflow</strong>.</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor092"/>Summary</h1>
			<p>In this chapter, we introduced the experiments component of MLflow. We got to understand the logging metrics and artifacts in MLflow. We detailed the steps to track experiments in MLflow.</p>
			<p>In the final sections, we explored the use case of hyperparameter optimization using the concepts learned in the chapter.</p>
			<p>In the next chapter, we will focus on managing models with MLflow using the models developed in this chapter.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor093"/>Further reading</h1>
			<p>To consolidate your knowledge further, you can consult the documentation available at the following links:</p>
			<ul>
				<li>https://www.mlflow.org/docs/latest/tracking.html</li>
				<li>h<a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">ttps://en.wikipedia.org/wiki/Hyperparameter_optimization</a></li>
			</ul>
		</div>
	</body></html>