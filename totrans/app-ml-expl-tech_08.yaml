- en: '*Chapter 6*: Model Interpretability Using SHAP'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：使用SHAP进行模型可解释性'
- en: In the previous two chapters, we explored model-agnostic local explainability
    using the **LIME** framework to explain black-box models. We also discussed certain
    limitations of the LIME approach, even though it remains one of the most popular
    **Explainable AI** (**XAI**) algorithms. In this chapter, we will cover **SHapley
    Additive exPlanation** (**SHAP**), which is another popular XAI framework that
    can provide model-agnostic local explainability for tabular, image, and text datasets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们探讨了使用**LIME**框架进行模型无关局部可解释性，以解释黑盒模型。尽管LIME仍然是最受欢迎的**可解释人工智能**（**XAI**）算法之一，但我们还讨论了LIME方法的一些局限性。在本章中，我们将介绍**SHapley
    Additive exPlanation**（**SHAP**），这是另一个流行的XAI框架，可以为表格、图像和文本数据集提供模型无关的局部可解释性。
- en: '**SHAP** is based on **Shapley values**, which is a concept popularly used
    in **Game Theory** ([https://c3.ai/glossary/data-science/shapley-values/](https://c3.ai/glossary/data-science/shapley-values/)).
    Although the mathematical understanding of Shapley values can be complicated,
    I will provide a simple, intuitive understanding of Shapley values and SHAP and
    focus more on the practical aspects of the framework. Similar to LIME, SHAP also
    has its pros and cons, which we are going to discuss in this chapter. This chapter
    will cover one practical tutorial that will explain regression models using SHAP.
    Later, in [*Chapter 7*](B18216_07_ePub.xhtml#_idTextAnchor128), *Practical Exposure
    to Using SHAP in ML*, we will cover other practical applications of the SHAP framework.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**SHAP**基于**Shapley值**，这是一个在**博弈论**（[https://c3.ai/glossary/data-science/shapley-values/](https://c3.ai/glossary/data-science/shapley-values/)）中广泛使用的概念。尽管Shapley值的数学理解可能很复杂，但我将提供一个简单、直观的Shapley值和SHAP的理解，并更多地关注框架的实践方面。与LIME类似，SHAP也有其优缺点，我们将在本章中讨论。本章将涵盖一个实践教程，将使用SHAP解释回归模型。稍后，在第7章[*使用SHAP在机器学习中实际应用*](B18216_07_ePub.xhtml#_idTextAnchor128)中，我们将涵盖SHAP框架的其他实际应用。'
- en: 'So, here is a list of the main topics of discussion for this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以下是本章讨论的主要主题列表：
- en: An intuitive understanding of the SHAP and Shapley values
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对SHAP和Shapley值的直观理解
- en: Model explainability approaches using SHAP
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SHAP的模型可解释性方法
- en: Advantages and limitations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Using SHAP to explain regression models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SHAP解释回归模型
- en: Now, let's begin!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code tutorial with the necessary resources can be downloaded or cloned
    from the GitHub repository for this chapter: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter06](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter06).
    Python and Jupyter notebooks are used to implement the practical application of
    the theoretical concepts that are covered in this chapter. But I recommend that
    you only run the notebooks after you go through this chapter for a better understanding.
    Additionally, please look at the *SHAP Errata* section before proceeding with
    the practical tutorial part of this chapter: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/SHAP_ERRATA/ReadMe.md](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/SHAP_ERRATA/ReadMe.md).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章相关的代码教程和必要资源可以从GitHub仓库下载或克隆：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter06](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter06)。Python和Jupyter笔记本用于实现本章涵盖的理论概念的实际应用。但我建议您在深入理解本章内容之后再运行笔记本。此外，在开始本章的实践教程部分之前，请先查看*SHAP勘误表*部分：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/SHAP_ERRATA/ReadMe.md](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/SHAP_ERRATA/ReadMe.md)。
- en: An intuitive understanding of the SHAP and Shapley values
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对SHAP和Shapley值的直观理解
- en: As discussed in [*Chapter 1*](B18216_01_ePub.xhtml#_idTextAnchor014), *Foundational
    Concepts of Explainability Techniques*, explaining black-box models is a necessity
    for increasing AI adoption. Algorithms that are model-agnostic and can provide
    local explainability with a global perspective are the ideal choice of explainability
    technique in **machine learning (ML)** . That is why LIME is a popular choice
    in XAI. SHAP is another popular choice of explainability technique in ML and,
    in certain scenarios, is more effective than LIME. In this section, we will discuss
    about the intuitive understanding of the SHAP framework along with how it functions
    for providing model explainability.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 [*第 1 章*](B18216_01_ePub.xhtml#_idTextAnchor014) 中所讨论的，*可解释性技术的根本概念*，解释黑盒模型对于增加人工智能的采用是必要的。那些模型无关且能够从全局视角提供局部可解释性的算法是机器学习
    (ML) 中可解释性技术的理想选择。这就是为什么 LIME 在 XAI 中如此受欢迎。SHAP 是机器学习中另一种流行的可解释性技术，在特定场景下，它比 LIME
    更有效。在本节中，我们将讨论 SHAP 框架的直观理解以及它是如何提供模型可解释性的。
- en: Introduction to SHAP and Shapley values
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP 和 Shapley 值的介绍
- en: The SHAP framework was introduced by *Scott Lundberg* and *Su-In Lee* in their
    research work, *A Unified Approach of Interpreting Model Predictions* ([https://arxiv.org/abs/1705.07874](https://arxiv.org/abs/1705.07874)).
    This was published in 2017\. SHAP is based on the concept of Shapley values from
    cooperative game theory, but unlike the LIME framework, it considers *additive
    feature importance*. By definition, the Shapley value is *the mean marginal contribution
    of each feature value across all possible values in the feature space*. The mathematical
    understanding of Shapley values is complicated and might confuse most readers.
    That said, if you are interested in getting an in-depth mathematical understanding
    of Shapley values, we recommend that you take a look at the research paper called
    *"A Value for n-Person Games." Contributions to the Theory of Games 2.28 (1953),
    by Lloyd S. Shapley*. In the next section, we will gain an intuitive understanding
    of Shapley values with a very simple example.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 框架是由 *Scott Lundberg* 和 *Su-In Lee* 在他们的研究工作中引入的，该研究工作为 *《解释模型预测的统一方法》*
    ([https://arxiv.org/abs/1705.07874](https://arxiv.org/abs/1705.07874))。这项研究于 2017
    年发表。SHAP 基于合作博弈论中的 Shapley 值概念，但与 LIME 框架不同，它考虑了 *加性特征重要性*。根据定义，Shapley 值是 *在特征空间中所有可能值上每个特征值的平均边际贡献*。Shapley
    值的数学理解比较复杂，可能会让大多数读者感到困惑。话虽如此，如果你对 Shapley 值的深入数学理解感兴趣，我们建议你阅读名为 *"A Value for
    n-Person Games." Contributions to the Theory of Games 2.28 (1953), by Lloyd S.
    Shapley* 的研究论文。在下文中，我们将通过一个非常简单的例子来获得 Shapley 值的直观理解。
- en: What are Shapley values?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 Shapley 值？
- en: In this section, I will explain Shapley values using a very simple and easy-to-understand
    example. Let's suppose that Alice, Bob, and Charlie are three friends who are
    taking part, as a team, in a Kaggle competition to solve a given problem with
    ML, for a certain cash prize. Their collective goal is to win the competition
    and get the prize money. All three of them are equally not good in all areas of
    ML and, therefore, have contributed in different ways. Now, if they win the competition
    and earn their prize money, *how will they ensure a fair distribution of the prize
    money considering their individual contributions?* *How will they measure their
    individual contributions for the same goal?* The answer to these questions can
    be given by Shapley values, which were introduced in 1951 by Lloyd Shapley.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将通过一个非常简单且易于理解的例子来解释 Shapley 值。假设 Alice、Bob 和 Charlie 是三位朋友，他们作为一个团队参加
    Kaggle 竞赛，使用机器学习解决给定问题以赢得一定的奖金。他们的共同目标是赢得比赛并获得奖金。他们在机器学习的所有领域都不擅长，因此以不同的方式做出了贡献。现在，如果他们赢得比赛并获得奖金，*他们将如何确保奖金的公平分配，考虑到他们的个人贡献？*
    *他们将如何衡量他们为同一目标所做的个人贡献？* 这些问题的答案可以通过 Shapley 值给出，这些值是由 Lloyd Shapley 在 1951 年引入的。
- en: 'The following diagram gives us a visual illustration of the scenario:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表为我们提供了该场景的视觉说明：
- en: '![Figure 6.1 – Visual illustration of the scenario discussed in the What are
    Shapley values? section'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.1 – "什么是 Shapley 值？"部分讨论的场景的视觉说明'
- en: '](img/B18216_06_001.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_06_001.jpg)'
- en: Figure 6.1 – Visual illustration of the scenario discussed in the What are Shapley
    values? section
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – "什么是 Shapley 值？"部分讨论的场景的视觉说明
- en: So, in this scenario, Alice, Bob, and Charlie are part of the same team, playing
    the same game (which is the Kaggle competition). In game theory, this is referred
    to as a **Coalition Game**. The prize money for the competition is their *payout*.
    So, Shapley values tell us the average contribution of each player to the payout
    ensuring a fair distribution. But *why not just equally distribute the prize money
    between all the players*? Well, since the contributions are not equal, it is not
    *fair* to distribute the money equally.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，Alice、Bob和Charlie是同一队的成员，在玩同一款游戏（即Kaggle比赛）。在博弈论中，这被称为**联盟博弈**。比赛的奖金是他们的**奖金分配**。因此，Shapley值告诉我们每个玩家对奖金分配的平均贡献，以确保公平分配。但**为什么不是将奖金平均分配给所有玩家**呢？嗯，由于贡献并不相等，所以平均分配奖金是不**公平**的。
- en: Deciding the payouts
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决定奖金分配
- en: Now, how do we decide the fairest way to distribute the payout? One way is to
    assume that Alice, Bob, and Charlie joined the game in a sequence in which Alice
    started first, followed by Bob, and then followed by Charlie. Let's suppose that
    if Alice, Bob, and Charlie had participated alone, they would have gained 10 points,
    20 points, and 25 points, respectively. But if only Alice and Bob teamed up, they
    might have received 40 points. While Alice and Charlie together could get 30 points,
    Bob and Charlie together could get 50 points. When all three of them collaborate
    together, only then do they get 90 points, which is sufficient for them to win
    the competition.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何决定分配奖金的最公平方式呢？一种方法是假设Alice、Bob和Charlie按照Alice首先加入，然后是Bob，最后是Charlie的顺序加入游戏。假设如果Alice、Bob和Charlie单独参加，他们分别可以获得10分、20分和25分。但如果是只有Alice和Bob组队，他们可能获得40分。而Alice和Charlie一起可以得到30分，Bob和Charlie一起可以得到50分。当三人共同合作时，他们才能获得90分，这足以让他们赢得比赛。
- en: '*Figure 6.2* illustrates the point values for each condition. We will make
    use of these values to calculate the average marginal contribution of each player:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.2*展示了每种情况下的得分点数。我们将利用这些值来计算每个玩家的平均边际贡献：'
- en: '![Figure 6.2 – The contribution values for all possible combinations of all
    the players'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2 – 所有可能玩家组合的贡献值'
- en: '](img/B18216_06_002.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_06_002.jpg)'
- en: Figure 6.2 – The contribution values for all possible combinations of all the
    players
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 所有可能玩家组合的贡献值
- en: 'Mathematically, if we assume that there are *N* players, where *S* is the coalition
    subset of players and ![](img/B18216_06_001.png) is the total value of *S* players,
    then by the formula of Shapley values, the marginal contribution of player *i*
    is given as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，如果我们假设有 *N* 名玩家，其中 *S* 是玩家的联盟子集，而 ![](img/B18216_06_001.png) 是 *S* 玩家的总价值，那么根据Shapley值公式，玩家
    *i* 的边际贡献如下所示：
- en: '![](img/B18216_06_0021.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18216_06_0021.jpg)'
- en: The equation of Shapley value might look complicated, but let's simplify this
    with our example. Please note that the order in which each player starts the game
    is important to consider as Shapley values try to account for the order of each
    player to calculate the marginal contribution.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Shapley值的方程可能看起来很复杂，但让我们用我们的例子来简化它。请注意，每个玩家开始游戏的顺序很重要，因为Shapley值试图考虑每个玩家的顺序来计算边际贡献。
- en: Now, for our example, the contribution of Alice can be calculated by calculating
    the difference that Alice can cause to the final score. So, the contribution is
    calculated by taking the difference in the points scored when Alice is in the
    game and when she is not. Also, when Alice is playing, she can either play alone
    or team up with others. When Alice is playing, the value that she can create can
    be represented as ![](img/B18216_06_003.png). Likewise, ![](img/B18216_06_004.png)
    and ![](img/B18216_06_005.png) denote individual values created by Bob and Charlie.
    Now, when Alice and Bob are teaming up, we can calculate only Alice's contribution
    by removing Bob's contribution from the overall contribution. This can be represented
    as ![](img/B18216_06_006.png). And if all three are playing together, Alice's
    contribution is given as ![](img/B18216_06_007.png).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于我们的例子，爱丽丝的贡献可以通过计算爱丽丝对最终分数造成的影响来计算。因此，贡献是通过计算爱丽丝在游戏中和不在游戏中的得分差异来计算的。此外，当爱丽丝在游戏中时，她可以单独玩或者与其他人组队。当爱丽丝在游戏中时，她可以创造的价值可以表示为![img/B18216_06_003.png](img/B18216_06_003.png)。同样，![img/B18216_06_004.png](img/B18216_06_004.png)和![img/B18216_06_005.png](img/B18216_06_005.png)分别表示鲍勃和查理创造的个体价值。现在，当爱丽丝和鲍勃组队时，我们只能通过从整体贡献中减去鲍勃的贡献来计算爱丽丝的贡献。这可以表示为![img/B18216_06_006.png](img/B18216_06_006.png)。如果三个人一起玩，爱丽丝的贡献给出为![img/B18216_06_007.png](img/B18216_06_007.png)。
- en: 'Considering all possible permutations of the sequences by which Alice, Bob,
    and Charlie play the game, the marginal contribution of Alice is the average of
    her individual contributions in all possible scenarios. This is illustrated in
    *Figure 6.3*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到爱丽丝、鲍勃和查理玩游戏的所有可能排列顺序，爱丽丝的边际贡献是在所有可能场景中她个体贡献的平均值。这如图*6.3*所示：
- en: '![Figure 6.3 – The Shapley value of Alice is her marginal contribution considering
    all possible scenarios'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 – 考虑所有可能场景的爱丽丝的Shapley值'
- en: '](img/B18216_06_003.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_06_003.jpg)'
- en: Figure 6.3 – The Shapley value of Alice is her marginal contribution considering
    all possible scenarios
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 考虑所有可能场景的爱丽丝的Shapley值
- en: 'So, the overall contribution of Alice will be her marginal contribution across
    all possible scenarios, which also happens to be the Shapley value. For Alice,
    the Shapley value is *20.83*. Similarly, we can calculate the marginal contribution
    for Bob and Charlie, as shown in *Figure 6.4*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，爱丽丝的整体贡献将是她在所有可能场景中的边际贡献，这也恰好是Shapley值。对于爱丽丝来说，Shapley值是**20.83**。同样，我们可以计算出鲍勃和查理的边际贡献，如图*6.4*所示：
- en: '![Figure 6.4 – Marginal contribution for Alice, Bob, and Charlie'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4 – 爱丽丝、鲍勃和查理的边际贡献'
- en: '](img/B18216_06_004.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_06_004.jpg)'
- en: Figure 6.4 – Marginal contribution for Alice, Bob, and Charlie
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 爱丽丝、鲍勃和查理的边际贡献
- en: I hope this wasn't too difficult to understand! One thing to note is that the
    sum of marginal contributions of Alice, Bob, and Charlie should be equal to the
    total contribution made by all three of them together. Now, let's try to understand
    Shapley values in the context of ML.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这不难理解！有一点需要注意，即爱丽丝、鲍勃和查理的边际贡献之和应该等于他们三个人共同做出的总贡献。现在，让我们尝试在机器学习的背景下理解Shapley值。
- en: Shapley values in ML
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习中的Shapley值
- en: In order to understand the importance of Shapley values in ML to explain model
    predictions, we will try to modify the example about Alice, Bob, and Charlie that
    we used for understanding Shapley values. We can consider Alice, Bob, and Charlie
    to be *three different features present in a dataset used for training a model*.
    So, in this case, the *player contributions* will be the *contribution of each
    feature*. *The game* or the Kaggle competition will be the *black-box ML model*
    and the *payout* will be the *prediction*. So, if we want to know the *contribution
    of each feature toward the model prediction*, we will use *Shapley values*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解Shapley值在机器学习中对解释模型预测的重要性，我们将尝试修改我们用于理解Shapley值的关于爱丽丝、鲍勃和查理的例子。我们可以将爱丽丝、鲍勃和查理视为在用于训练模型的**数据集中存在的三个不同的特征**。在这种情况下，**玩家贡献**将是**每个特征的贡献**。**游戏**或Kaggle比赛将是**黑盒机器学习模型**，**收益**将是**预测**。因此，如果我们想了解**每个特征对模型预测的贡献**，我们将使用**Shapley值**。
- en: 'The modification of *Figure 6.1* to represent Shapley values in the context
    of ML is illustrated in *Figure 6.5*:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 将*图6.1*的修改用于表示机器学习背景下的Shapley值，如图*6.5*所示：
- en: '![Figure 6.5 – Understanding Shapley values in the context of ML'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.5 – 在机器学习背景下理解Shapley值'
- en: '](img/B18216_06_005.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_06_005.jpg)'
- en: Figure 6.5 – Understanding Shapley values in the context of ML
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 在机器学习背景下理解Shapley值
- en: Therefore, Shapley values help us to understand the collective contribution
    of each feature toward the outcome predicted by black-box ML models. By using
    Shapley values, we can explain the working of black-box models by estimating the
    feature contributions.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Shapley值帮助我们理解每个特征对黑盒机器学习模型预测结果的集体贡献。通过使用Shapley值，我们可以通过估计特征贡献来解释黑盒模型的工作原理。
- en: Properties of Shapley values
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Shapley值的性质
- en: 'Now that we have an intuitive understanding of Shapley values and we have learned
    how to calculate Shapley values, we should also gain an understanding of the properties
    of Shapley values:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对Shapley值有了直观的理解，并且我们已经学习了如何计算Shapley值，我们还应该了解Shapley值的性质：
- en: '**Efficiency**: The total sum of Shapley values or the marginal contribution
    of each feature should be equal to the value of the total coalition. For example,
    in *Figure 6.4*, we can see that sum of individual Shapley values for Alice, Bob,
    and Charlie are equal to the total coalition value obtained when Alice, Bob, and
    Charlie team up together.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**效率**：Shapley值的总和或每个特征的边际贡献应该等于总联盟的价值。例如，在*图6.4*中，我们可以看到Alice、Bob和Charlie的Shapley值总和等于当Alice、Bob和Charlie一起组队时获得的总联盟价值。'
- en: '**Symmetry**: Each player has a fair chance of joining the game in any order.
    in *Figure 6.4*, we can see that all permutations of the sequences for all the
    players are considered.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对称性**：每个玩家都有公平的机会以任何顺序加入游戏。在*图6.4*中，我们可以看到考虑了所有玩家的序列的所有排列。'
- en: '**Dummy**: If a particular feature does not change the predicted value regardless
    of the coalition group, then the Shapley value for the feature is *0*.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟玩家**：如果一个特定的特征无论联盟组如何都不会改变预测值，那么该特征的Shapley值为*0*。'
- en: '**Additivity**: For any game with a combined payout, the Shapley values are
    also combined. This is denoted as ![](img/B18216_06_008.png), then ![](img/B18216_06_009.png).
    For example, for the random forest algorithm in ML, Shapley values can be calculated
    for a particular feature by calculating it for each individual tree and then averaging
    them to find the additive Shapley value for the entire random forest.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可加性**：对于任何具有合并支付的游戏，Shapley值也会合并。这表示为![](img/B18216_06_008.png)，然后![](img/B18216_06_009.png)。例如，对于机器学习中的随机森林算法，可以通过计算每个单独的树并取平均值来计算特定特征的Shapley值，从而找到整个随机森林的加性Shapley值。'
- en: So, these are the important properties of Shapley values. Next, let's discuss
    the SHAP framework and understand how it is much more than just the usage of Shapley
    values.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些是Shapley值的重要性质。接下来，让我们讨论SHAP框架，了解它远不止是Shapley值的使用。
- en: The SHAP framework
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP框架
- en: 'Previously, we discussed what Shapley values are and how they are used in ML.
    Now, let''s cover the SHAP framework. Although SHAP is popularly used as an XAI
    tool for providing local explainability to individual predictions, SHAP can also
    provide a global explanation by aggregating the individual predictions. Additionally,
    SHAP is *model-agnostic*, which means that it does not make any assumptions about
    the algorithm used in black-box models. The creators of the framework broadly
    came up with two model-agnostic approximation methods, which are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了Shapley值是什么以及它们如何在机器学习中使用。现在，让我们介绍SHAP框架。虽然SHAP通常用作XAI工具，为个别预测提供局部可解释性，但SHAP还可以通过汇总个别预测提供全局解释。此外，SHAP是*模型无关的*，这意味着它不对黑盒模型中使用的算法做出任何假设。该框架的创造者广泛提出了两种模型无关的近似方法，如下所示：
- en: '**SHAP Explainer**: This is based on *Shapley sampling values*.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SHAP解释器**：这是基于*Shapley采样值*。'
- en: '**KernelSHAP Explainer**: This is based on the *LIME approach*.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KernelSHAP解释器**：这是基于*LIME方法*。'
- en: 'The framework also includes *model-specific* explainability methods such as
    the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架还包括以下*特定模型*的可解释性方法：
- en: '**Linear SHAP**: This is for linear models with independent features.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性SHAP**：这是针对具有独立特征的线性模型。'
- en: '**Tree SHAP**: This is an algorithm that is faster than SHAP explainers to
    compute SHAP values for tree algorithms and tree-based ensemble learning algorithms.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tree SHAP**：这是一个比SHAP解释器更快的算法，用于计算树算法和基于树的集成学习算法的SHAP值。'
- en: '**Deep SHAP**: This is an algorithm that is faster than SHAP explainers to
    compute SHAP values for deep learning models.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Deep SHAP**：这是一个比SHAP解释器更快的算法，用于计算深度学习模型的SHAP值。'
- en: Apart from these approaches, SHAP also uses interesting visualization methods
    to explain AI models. We will cover these methods in more detail in the next section.
    But one point to note is that the calculation of Shapley values is computationally
    very expensive, and the algorithm is of the order of ![](img/B18216_06_010.png),
    where *n* is the number of features. So, if the dataset has many features, calculating
    Shapley values might take forever! However, the SHAP framework uses an approximation
    technique to calculate Shapley values efficiently. The explanation provided by
    SHAP is more robust as compared to the LIME framework. Let's proceed to the next
    section, where we will discuss the various model explainability approaches used
    by SHAP on various types of data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些方法之外，SHAP还使用有趣的可视化方法来解释AI模型。我们将在下一节中更详细地介绍这些方法。但有一点需要注意，Shapley值的计算在计算上非常昂贵，算法的复杂度为![](img/B18216_06_010.png)，其中*n*是特征的数量。所以，如果数据集有大量特征，计算Shapley值可能需要很长时间！然而，SHAP框架使用近似技术来有效地计算Shapley值。与LIME框架相比，SHAP提供的解释更加稳健。让我们进入下一节，我们将讨论SHAP在多种类型数据上使用的各种模型可解释性方法。
- en: Model explainability approaches using SHAP
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SHAP的模型可解释性方法
- en: After reading the previous section, you have gained an understanding of SHAP
    and Shapley values. In this section, we will discuss various model-explainability
    approaches using SHAP. Data visualization is an important method to explain the
    working of complex algorithms. SHAP makes use of various interesting data visualization
    techniques to represent the approximated Shapley values to explain black-box models.
    So, let's discuss some of the popular visualization methods used by the SHAP framework.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读上一节之后，您已经对SHAP和Shapley值有了了解。在本节中，我们将讨论使用SHAP的各种模型可解释性方法。数据可视化是解释复杂算法工作原理的重要方法。SHAP利用各种有趣的数据可视化技术来表示近似的Shapley值，以解释黑盒模型。因此，让我们讨论一些SHAP框架中使用的流行可视化方法。
- en: Visualizations in SHAP
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP中的可视化
- en: As mentioned previously, SHAP can be used for both the global interpretability
    of the model and the local interpretability of the inference data instance. Now,
    the values generated by the SHAP algorithm are quite difficult to understand unless
    we make use of intuitive visualizations. The choice of visualization depends on
    the choice of global interpretability or local interpretability, which we will
    cover in this section.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，SHAP可以用于模型的全球可解释性和推理数据实例的局部可解释性。现在，除非我们使用直观的视觉表示，否则SHAP算法生成的值很难理解。可视化方法的选择取决于全球可解释性或局部可解释性的选择，我们将在本节中介绍。
- en: Global interpretability with feature importance bar plots
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用特征重要性条形图的全球可解释性
- en: Analyzing the most influential features present in a dataset always helps us
    to understand the functioning of an algorithm with respect to the underlying data.
    SHAP provides an effective way in which to find feature importance using Shapley
    values. So, the feature importance bar plot displays the important features in
    descending order of their importance. Additionally, SHAP provides a unique way
    to show feature interactions using **hierarchical clustering** ([https://www.displayr.com/what-is-hierarchical-clustering/](https://www.displayr.com/what-is-hierarchical-clustering/)).
    These feature clustering methods help us to visualize a group of features that
    collectively impacts the model's outcome. This is very interesting since one of
    the core benefits of using Shapley values is to analyze the additive influence
    of multiple features together. However, there is one drawback of the feature importance
    plot for global interpretability. Since this method only considers mean absolute
    Shapley values to estimate feature importance, it doesn't show whether collectively
    certain features are impacting the model in a negative way or not.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 分析数据集中存在的最具影响力的特征总是有助于我们理解算法相对于底层数据的工作原理。SHAP提供了一种有效的方法，通过Shapley值来找到特征重要性。因此，特征重要性条形图按重要性降序显示重要特征。此外，SHAP提供了一种独特的方式来使用**层次聚类**（[https://www.displayr.com/what-is-hierarchical-clustering/](https://www.displayr.com/what-is-hierarchical-clustering/)）展示特征交互。这些特征聚类方法帮助我们可视化一组共同影响模型结果的特征。这非常有趣，因为使用Shapley值的核心好处之一就是分析多个特征组合的加性影响。然而，对于全球可解释性，特征重要性图有一个缺点。因为这个方法只考虑平均绝对Shapley值来估计特征重要性，所以它不显示某些特征是否以负面的方式共同影响模型。
- en: 'The following diagram illustrates the visualizations for a feature importance
    plot and a feature clustering plot using SHAP:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表说明了使用SHAP进行特征重要性图和特征聚类图可视化的示例：
- en: '![Figure 6.6 –A feature importance plot for global interpretability (left-hand
    side) and a feature clustering plot for global interpretability (right-hand side)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.6 –A feature importance plot for global interpretability (left-hand
    side) and a feature clustering plot for global interpretability (right-hand side)'
- en: '](img/B18216_06_006.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_06_006.jpg](img/B18216_06_006.jpg)'
- en: Figure 6.6 –A feature importance plot for global interpretability (left-hand
    side) and a feature clustering plot for global interpretability (right-hand side)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 用于全局可解释性的特征重要性图（左侧）和用于全局可解释性的特征聚类图（右侧）
- en: Next, let's explore SHAP Cohort plots.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索SHAP队列图。
- en: Global interpretability with the Cohort plot
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用队列图进行全局可解释性
- en: Sometimes, analyzing subgroups of data is an important part of data analysis.
    SHAP provides a very interesting way of grouping the data into certain defined
    cohorts to analyze feature importance. I found this to be a unique option in SHAP,
    which can be really helpful! This is an extension of the existing feature importance
    visualization, and it highlights feature importance for each of the cohorts for
    a better comparison.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，分析数据子集是数据分析的重要部分。SHAP提供了一种非常有趣的方法，将数据分组到某些定义好的队列中，以分析特征重要性。我发现这是SHAP中的一个独特选项，非常有帮助！这是现有特征重要性可视化的扩展，并突出了每个队列的特征重要性，以便更好地比较。
- en: '*Figure 6.7* shows us the cohort plot to compare two cohorts that are defined
    from the data:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.7* 展示了从数据中定义的两个队列的比较队列图：'
- en: '![Figure 6.7 – A cohort plot visualization to compare feature importance between
    two cohorts'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.7 – A cohort plot visualization to compare feature importance between
    two cohorts'
- en: '](img/B18216_06_007.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_06_007.jpg](img/B18216_06_007.jpg)'
- en: Figure 6.7 – A cohort plot visualization to compare feature importance between
    two cohorts
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 用于比较两个队列特征重要性的队列图可视化
- en: Next, we will explore SHAP heatmap plots.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索SHAP热图图。
- en: Global interpretability with heatmap plots
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用热图图进行全局可解释性
- en: To understand the overall impact of all features on the model at a more granular
    level, heatmap visualizations are extremely useful. The SHAP heatmap visualization
    shows how every feature value can positively or negatively impact the outcome.
    Additionally, the plot also includes a line plot to show how the model prediction
    varies with the positive or negative impact of feature values. However, for non-technical
    users, this visualization can be really challenging to interpret. This is one
    of the drawbacks of this visualization method.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在更细粒度的层面上理解所有特征对模型的整体影响，热图可视化非常有用。SHAP热图可视化显示了每个特征值如何积极或消极地影响结果。此外，该图还包括一个折线图，以显示模型预测如何随着特征值的积极或消极影响而变化。然而，对于非技术用户来说，这种可视化可能真的很难解释。这是这种可视化方法的缺点之一。
- en: '*Figure 6.8* illustrates a *SHAP heatmap visualization*:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.8* 说明了 *SHAP热图可视化*：'
- en: '![Figure 6.8 – A SHAP heatmap plot'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.8 – A SHAP heatmap plot'
- en: '](img/B18216_06_008.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_06_008.jpg](img/B18216_06_008.jpg)'
- en: Figure 6.8 – A SHAP heatmap plot
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – SHAP热图图
- en: Another popular choice of visualization for global interpretability using SHAP
    is summary plots. Let's discuss summary plots in the next section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SHAP进行全局可解释性的另一种流行的可视化选择是摘要图。让我们在下一节讨论摘要图。
- en: Global interpretability with summary plots
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用摘要图进行全局可解释性
- en: A summary plot is another visualization method in SHAP for providing global
    explainability of black-box models. It is a good replacement for the feature importance
    plot, which not only includes the important features but also the range of effects
    of these features present in the dataset. The color bar indicates the impact of
    the features. The features that influence the model's outcome in a positive way
    are highlighted in a particular color, whereas the features that impact the model's
    outcome negatively are represented in another contrasting color. The horizontal
    violin plot for each feature shows the distribution of the Shapley values of the
    features for each data instance.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要图是SHAP提供的一种用于提供黑盒模型全局可解释性的可视化方法。它是特征重要性图的良好替代品，不仅包括重要的特征，还包括数据集中这些特征的影响范围。颜色条表示特征的影响。以特定颜色突出显示对模型结果产生积极影响的特征，而以另一种对比颜色表示对模型结果产生负面影响的特征。每个特征的横向小提琴图显示了每个数据实例的特征Shapley值的分布。
- en: 'The following screenshot illustrates a *SHAP summary plot*:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图说明了 *SHAP摘要图*：
- en: '![Figure 6.9 – A SHAP violin summary plot'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.9 – SHAP小提琴总结图'
- en: '](img/B18216_06_009.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_06_009.jpg)'
- en: Figure 6.9 – A SHAP violin summary plot
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – SHAP小提琴总结图
- en: In the next section, we will discuss SHAP dependence plots.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论SHAP依赖性图。
- en: Global interpretability with dependence plots
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用依赖性图进行全局可解释性
- en: In certain scenarios, it is important to analyze interactions between the features
    and how this interaction influences the model outcome. So, SHAP feature dependence
    plots show the variation of the model outcome by specific features. These plots
    are similar to the *partial dependence plots*, which were covered in [*Chapter
    2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability Methods*. This
    plot can help to pick up interesting interaction patterns or trends between the
    feature values. The features used for selecting the color map are automatically
    picked up by the algorithm, based on the interaction with a specific selected
    feature.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，分析特征之间的相互作用以及这种相互作用如何影响模型结果是很重要的。因此，SHAP特征依赖性图显示了模型结果随特定特征的变化。这些图类似于在[*第2章*](B18216_02_ePub.xhtml#_idTextAnchor033)“模型可解释性方法”中介绍的*部分依赖性图*。此图可以帮助发现特征值之间的有趣交互模式或趋势。用于选择颜色图的特征由算法自动根据与特定选定特征的交互选择。
- en: '*Figure 6.10* illustrates a SHAP dependence plot:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.10* 展示了一个SHAP依赖性图：'
- en: '![Figure 6.10 – A SHAP dependence plot for the pH feature'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.10 – pH特征的SHAP依赖性图'
- en: '](img/B18216_06_010.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_06_010.jpg)'
- en: Figure 6.10 – A SHAP dependence plot for the pH feature
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – pH特征的SHAP依赖性图
- en: In this example, the selected feature is *pH*, and the feature used for selecting
    the colormap is *alcohol*. So, the plot tells us that with an increase in *pH*,
    the *alcohol* value also increases. This will be covered in greater detail in
    the next section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，选定的特征是*pH*，用于选择颜色图的特征是*alcohol*。因此，该图告诉我们，随着*pH*的增加，*alcohol*值也会增加。这将在下一节中更详细地介绍。
- en: In the next section, let's explore the SHAP visualization methods used for local
    explainability.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨用于局部可解释性的SHAP可视化方法。
- en: Local interpretability with bar plots
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用条形图进行局部可解释性
- en: So far, we have covered various visualization techniques offered by SHAP for
    providing a global overview of the model. However, similar to LIME, SHAP is also
    model-agnostic that is designed to provide local explainability. SHAP provides
    certain visualization methods that can be applied to inference data for local
    explainability. Local feature importance using SHAP bar plots is one such local
    explainability method. This plot can help us analyze the positive and negative
    impact of the features that are present in the data. The features that impact
    the model's outcome positively are highlighted in one color (pinkish-red by default),
    and the features having a negative impact on the model outcome are represented
    using another color (blue by default). Also, as we have discussed before, if the
    Shapley value is zero for any feature, this indicates that the feature does not
    influence the model outcome at all. Additionally, the bar plot centers at zero
    to show the contribution of the features present in the data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了SHAP提供的各种可视化技术，用于提供模型的全球概述。然而，与LIME类似，SHAP也是模型无关的，旨在提供局部可解释性。SHAP提供了一些可视化方法，可以应用于推理数据以实现局部可解释性。使用SHAP条形图进行局部特征重要性分析是此类局部可解释性方法之一。此图可以帮助我们分析数据中存在的特征的正负影响。对模型结果产生正面影响的特征以一种颜色（默认为粉红色）突出显示，而对模型结果产生负面影响的特征则使用另一种颜色（默认为蓝色）表示。此外，正如我们之前讨论的，如果任何特征的总和值为零，这表明该特征根本不影响模型结果。此外，条形图以零为中心，以显示数据中存在的特征的贡献。
- en: 'The following diagram shows a *SHAP feature importance bar plot* for local
    interpretability:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了用于局部可解释性的*SHAP特征重要性条形图*：
- en: '![Figure 6.11 – A SHAP feature importance bar plot for local interpretability'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.11 – 用于局部可解释性的SHAP特征重要性条形图'
- en: '](img/B18216_06_011.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_06_011.jpg)'
- en: Figure 6.11 – A SHAP feature importance bar plot for local interpretability
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 用于局部可解释性的SHAP特征重要性条形图
- en: Next, let's cover another SHAP visualization that is used for local interpretability.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们介绍另一种用于局部可解释性的SHAP可视化。
- en: Local interpretability with waterfall plots
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用瀑布图进行局部可解释性
- en: Bar charts are not the only visualization provided by SHAP for local interpretability.
    The same information can be displayed using a waterfall plot, which might look
    more attractive. Perhaps, the only difference is that waterfall plots are not
    centered at zero, whereas bar plots are centered at zero. Otherwise, we get the
    same feature importance based on Shapley values and the positive or the negative
    impact of the specific features on the model outcome.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 条形图并不是SHAP为局部可解释性提供的唯一可视化方式。相同的信息可以使用瀑布图来显示，这可能看起来更吸引人。也许，唯一的区别是瀑布图不是以零为中心，而条形图是以零为中心。否则，我们得到基于Shapley值的相同特征重要性，以及特定特征对模型结果的正向或负向影响。
- en: '*Figure 6.12* illustrates a *SHAP waterfall plot* for local interpretability:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.12* 展示了用于局部可解释性的 *SHAP瀑布图*：'
- en: '![Figure 6.12 – A SHAP waterfall plot for local interpretability'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.12 – 用于局部可解释性的SHAP瀑布图'
- en: '](img/B18216_06_012.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_06_012.jpg](img/B18216_06_012.jpg)'
- en: Figure 6.12 – A SHAP waterfall plot for local interpretability
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – 用于局部可解释性的SHAP瀑布图
- en: Next, we will discuss force plot visualization in SHAP.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论SHAP中的力图可视化。
- en: Local interpretability with force plot
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用力图的局部可解释性
- en: We can also use force plots instead of waterfall or bar plots to explain local
    inference data. With force plots, we can see the model prediction, which is denoted
    by *f(x)*, as shown in *Figure 6.13*. The *base value* in the following diagram
    represents the average predicted outcome of the model. The base value is actually
    used when the features present in the local data instance are not considered.
    So, using the force plot, we can also see how far the predicted outcome is from
    the base value. Additionally, we can see the feature impacts as the visual highlights
    certain features that try to increase the model prediction (which is represented
    in pink in *Figure 6.13*) along with other important features that have a negative
    influence on the model as it tries to lower the prediction value (which is represented
    in green in *Figure 6.13*).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用力图来代替瀑布图或条形图来解释局部推理数据。使用力图，我们可以看到模型预测，用 *f(x)* 表示，如图 *图6.13* 所示。以下图中的
    *基础值* 代表模型的平均预测结果。当局部数据实例中不包含特征时，实际上使用的是基础值。因此，使用力图，我们还可以看到预测结果与基础值之间的距离。此外，我们还可以看到特征影响，如图中某些试图增加模型预测（在
    *图6.13* 中用粉色表示）的特征的视觉高亮，以及那些试图降低预测值（在 *图6.13* 中用绿色表示）并对模型有负面影响的其他重要特征。
- en: 'So, *Figure 6.13* illustrates a sample force plot visualization in SHAP:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*图6.13* 展示了SHAP中的样本力图可视化：
- en: '![Figure 6.13 – A SHAP force plot for local interpretability'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.13 – 用于局部可解释性的SHAP力图'
- en: '](img/B18216_06_013.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_06_013.jpg](img/B18216_06_013.jpg)'
- en: Figure 6.13 – A SHAP force plot for local interpretability
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 用于局部可解释性的SHAP力图
- en: Although force plots might visually look very interesting, we would recommend
    using bar plots or waterfall plots if the dataset contains many features affecting
    the model outcome in a positive or a negative way.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管力图在视觉上可能看起来非常有趣，但如果数据集中包含许多以正向或负向方式影响模型结果的特征，我们建议使用条形图或瀑布图。
- en: Local interpretability with decision plots
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用决策图的局部可解释性
- en: The easiest way to explain something is by comparing it with a reference value.
    So far, in bar plots, waterfall plots, and even force plots, we do not see any
    reference values for the underlying features used. However, in order to find out
    whether the feature values are positively or negatively influencing the model
    outcome, the algorithm is actually trying to compare the feature values of the
    inference data with the mean of the feature values of the trained model. So, this
    is the reference value that is not displayed in the three local explainability
    visualization plots that we covered. But SHAP decision plots help us to compare
    the feature values of the local data instance with the mean feature values of
    the training dataset. Additionally, decision plots show the deviation of the feature
    values, the model prediction, and the direction of deviation of features from
    the reference values. If the direction of deviation is toward the right, this
    indicates that the feature is positively influencing the model outcome; if the
    direction of deviation is toward the left, this represents the negative influence
    of the feature on the model outcome. Different colors are used to highlight positive
    or negative influences. If there is no deviation, then the features are actually
    not influencing the model outcome.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 解释某事最简单的方法就是将其与参考值进行比较。到目前为止，在条形图、瀑布图甚至力图中，我们没有看到任何用于底层特征的参考值。然而，为了找出特征值是正向还是负向影响模型结果，算法实际上是在尝试将推理数据的特征值与训练模型特征值的平均值进行比较。因此，这是我们未在所涵盖的三个局部可解释性可视化图中显示的参考值。但SHAP决策图帮助我们比较局部数据实例的特征值与训练数据集的平均特征值。此外，决策图显示了特征值的偏差、模型预测以及特征值与参考值偏差的方向。如果偏差方向向右，这表明特征正向影响模型结果；如果偏差方向向左，这表示特征对模型结果的负向影响。使用不同的颜色突出显示正向或负向影响。如果没有偏差，则实际上特征不会影响模型结果。
- en: 'The following diagram illustrates the use of decision plots to compare two
    different data instances for providing local explainability:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表说明了使用决策图比较两个不同的数据实例以提供局部可解释性的用法：
- en: '![Figure 6.14 – SHAP decision plots for local interpretability'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.14 – SHAP决策图用于局部可解释性](img/B18216_06_014.jpg)'
- en: '](img/B18216_06_014.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_06_014.jpg](img/B18216_06_014.jpg)'
- en: Figure 6.14 – SHAP decision plots for local interpretability
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – SHAP决策图用于局部可解释性
- en: So far, you have seen the variety of visualization methods provided in SHAP
    for the global and local explainability of ML models. Now, let's discuss the various
    types of explainers in SHAP.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了SHAP为ML模型的全球和局部可解释性提供的各种可视化方法。现在，让我们讨论SHAP中提供的各种解释器类型。
- en: Explainers in SHAP
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP中的解释器
- en: In the previous section, we looked at how the data visualization techniques
    that are available in SHAP can be used to provide explainability. But the choice
    of the visualization method might also depend on the choice of the explainer algorithm.
    As we discussed earlier, SHAP provides both model-specific and model-agnostic
    explainability. But the framework has multiple explainer algorithms that can be
    applied with different models and with different types of datasets. In this section,
    we will cover the various explainer algorithms provided in SHAP.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了如何使用SHAP中可用的数据可视化技术来提供可解释性。但可视化方法的选择也可能取决于解释器算法的选择。如我们之前讨论的，SHAP提供特定于模型和模型无关的可解释性。但该框架具有多个解释器算法，可以应用于不同的模型和不同类型的数据集。在本节中，我们将介绍SHAP提供的各种解释器算法。
- en: TreeExplainer
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TreeExplainer
- en: TreeExplainer is a fast implementation of the **Tree SHAP algorithm** ([https://arxiv.org/pdf/1802.03888.pdf](https://arxiv.org/pdf/1802.03888.pdf))
    for computing Shapley values for trees and tree-based ensemble learning algorithms.
    The algorithm makes many diverse possible assumptions about the feature dependence
    of the features present in the dataset. Only tree-based algorithms are supported
    such as *Random Forest*, *XGBoost*, *LightGBM*, and *CatBoost*. The algorithm
    relies on fast C++ implementations in either the local compiled C extension or
    inside an external model package, but it is faster than conventional Shapley value-based
    explainers. Generally, it is used for tree-based models trained on structured
    data for both classification and regression problems.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: TreeExplainer 是 **Tree SHAP 算法** ([https://arxiv.org/pdf/1802.03888.pdf](https://arxiv.org/pdf/1802.03888.pdf))
    的快速实现，用于计算树和基于树的集成学习算法的 Shapley 值。该算法对数据集中存在的特征之间的特征依赖性做出了许多不同的假设。仅支持基于树的算法，如
    *随机森林*、*XGBoost*、*LightGBM* 和 *CatBoost*。该算法依赖于在本地编译的 C 扩展或外部模型包内部的快速 C++ 实现，但它比传统的基于
    Shapley 值的解释器更快。通常，它用于训练在结构化数据上用于分类和回归问题的基于树的模型。
- en: DeepExplainer
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepExplainer
- en: Similar to LIME, SHAP can also be applied to deep learning models trained on
    unstructured data, such as images and texts. SHAP uses DeepExplainer, which is
    based on the **Deep SHAP algorithm** for explaining deep learning models. The
    DeepExplainer algorithm is designed for deep learning models to approximate SHAP
    values. The algorithm is a modified version of the **DeepLIFT algorithm** ([https://arxiv.org/abs/1704.02685](https://arxiv.org/abs/1704.02685)).
    The developers of the framework have mentioned that the implementation of the
    Deep SHAP algorithm differs slightly from the original DeepLIFT algorithm. It
    uses a distribution of background samples rather than a single reference value.
    Additionally, the Deep SHAP algorithm also uses Shapley equations to linearize
    computations such as products, division, max, softmax, and more. The framework
    mostly supports deep learning frameworks such as TensorFlow, Keras, and PyTorch.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与 LIME 类似，SHAP 也可以应用于在非结构化数据上训练的深度学习模型，例如图像和文本。SHAP 使用基于 **Deep SHAP 算法** 的
    DeepExplainer 来解释深度学习模型。DeepExplainer 算法是为深度学习模型近似 SHAP 值而设计的。该算法是 **DeepLIFT
    算法** ([https://arxiv.org/abs/1704.02685](https://arxiv.org/abs/1704.02685)) 的修改版。框架的开发者提到，Deep
    SHAP 算法的实现与原始 DeepLIFT 算法略有不同。它使用背景样本的分布而不是单个参考值。此外，Deep SHAP 算法还使用 Shapley 方程来线性化乘法、除法、最大值、softmax
    等计算。该框架主要支持 TensorFlow、Keras 和 PyTorch 等深度学习框架。
- en: GradientExplainer
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GradientExplainer
- en: DeepExplainer is not the only explainer in SHAP that can be used with deep learning
    models. GradientExplainer can also work with deep learning models. The algorithm
    explains models using the concept of **expected gradients**. The expected gradient
    is an extension of **Integrated Gradients** ([https://arxiv.org/abs/1703.01365](https://arxiv.org/abs/1703.01365)),
    SHAP, and **SmoothGrad** ([https://arxiv.org/abs/1706.03825](https://arxiv.org/abs/1706.03825)),
    which combines the ideas of these algorithms into a single expected value equation.
    Consequently, similar to DeepExplainer, the entire dataset can be used as the
    background distribution sample instead of a single reference sample. This allows
    the model to be approximated with a linear function between individual samples
    of the data and the current input data instance that is to be explained. Since
    the input features are assumed to be independent, the expected gradients will
    calculate the approximate SHAP values.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: DeepExplainer 并不是 SHAP 中唯一可以与深度学习模型一起使用的解释器。GradientExplainer 也可以与深度学习模型一起工作。该算法使用
    **预期梯度** 的概念来解释模型。预期梯度是 **集成梯度** ([https://arxiv.org/abs/1703.01365](https://arxiv.org/abs/1703.01365))、SHAP
    和 **SmoothGrad** ([https://arxiv.org/abs/1706.03825](https://arxiv.org/abs/1706.03825))
    的扩展，这些算法将这些想法结合成一个单一的预期值方程。因此，与 DeepExplainer 类似，可以使用整个数据集作为背景分布样本，而不是单个参考样本。这允许模型通过数据单个样本和要解释的当前输入数据实例之间的线性函数进行近似。由于假设输入特征是独立的，预期梯度将计算近似的
    SHAP 值。
- en: For model explainability, the feature values with higher SHAP values are highlighted,
    as these features have a positive contribution toward the model's outcome. For
    unstructured data such as images, pixel positions that have the maximum contribution
    toward the model prediction are highlighted. Usually, GradientExplainer is slower
    than DeepExplainer as it makes different approximation assumptions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型可解释性，具有更高 SHAP 值的特征值会被突出显示，因为这些特征对模型的输出有积极的贡献。对于如图像这样的非结构化数据，对模型预测贡献最大的像素位置会被突出显示。通常，GradientExplainer
    比 DeepExplainer 慢，因为它做出了不同的近似假设。
- en: 'The following diagram shows a sample GradientExplainer visualization for the
    local explainability of a classification model trained on images:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了在图像上训练的分类模型的局部可解释性的一个样本 GradientExplainer 可视化：
- en: '![Figure 6.15 – Visualizations used by SHAP GradientExplainers for local explainability'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.15 – SHAP GradientExplainers 用于局部可解释性所使用的可视化'
- en: '](img/B18216_06_015.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_06_015.jpg](img/B18216_06_015.jpg)'
- en: Figure 6.15 – Visualizations used by SHAP GradientExplainers for local explainability
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – SHAP GradientExplainers 用于局部可解释性所使用的可视化
- en: Next, let's discuss SHAP KernelExplainers.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论 SHAP KernelExplainers。
- en: KernelExplainer
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KernelExplainer
- en: KernelExplainers in SHAP use the **Kernel SHAP** method to provide model-agnostic
    explainability. To estimate the SHAP values for any model, the Kernel SHAP algorithm
    utilizes a specifically weighted local linear regression approach to compute feature
    importance. The approach is similar to the LIME algorithm that we have discussed
    in [*Chapter 4*](B18216_04_ePub.xhtml#_idTextAnchor076), *LIME for Model Interpretability*.
    The major difference between Kernel SHAP and LIME is the approach that is adopted
    to assign weights to the instances in a regression model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 中的 KernelExplainers 使用 **Kernel SHAP** 方法提供模型无关的可解释性。为了估计任何模型的 SHAP 值，Kernel
    SHAP 算法利用一个特别加权的局部线性回归方法来计算特征重要性。这种方法与我们在 [*第 4 章*](B18216_04_ePub.xhtml#_idTextAnchor076)
    中讨论的 LIME 算法类似，即 *LIME for Model Interpretability*。Kernel SHAP 和 LIME 之间的主要区别在于它们采用的方法来为回归模型中的实例分配权重。
- en: In LIME, the weights are assigned based on how close the local data instances
    are to the original instance. Whereas in Kernel SHAP, the weights are assigned
    based on the estimated Shapley values of the coalition of features used. In simple
    words, LIME assigns weights based on isolated features, whereas SHAP considers
    the combined effect of the features for assigning the weights. KernelExplainer
    is slower than model-specific algorithms as it does not make any assumptions about
    the model type.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LIME 中，权重是根据局部数据实例与原始实例的接近程度来分配的。而在 Kernel SHAP 中，权重是根据所使用特征的联盟的 Shapley 估计值来分配的。简单来说，LIME
    是基于孤立特征分配权重，而 SHAP 考虑了特征的组合效应来分配权重。KernelExplainer 比特定模型算法慢，因为它不对模型类型做出任何假设。
- en: LinearExplainer
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LinearExplainer
- en: SHAP LinearExplainer is designed for computing SHAP values for linear models
    to analyze inter-feature correlations. LinearExplainer also supports the estimation
    of the feature covariance matrix for coalition feature importance. However, finding
    feature correlations for a high-dimensional dataset can be computationally expensive.
    But LinearExplainers are fast and efficient as they use sampling to estimate a
    transformation. This is then used for explaining any outcome of linear models.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP LinearExplainer 是为计算线性模型的 SHAP 值而设计的，以分析特征间的相关性。LinearExplainer 还支持联盟特征重要性的特征协方差矩阵的估计。然而，对于高维数据集，找到特征相关性可能会非常耗费计算资源。但
    LinearExplainers 由于使用采样来估计一个转换，因此它们既快又高效。然后，这个转换被用来解释线性模型的任何结果。
- en: Therefore, we have discussed the theoretical aspect of various explainers in
    SHAP. For more information about these explainers, I do recommend checking out
    [https://shap-lrjball.readthedocs.io/en/docs_update/api.html](https://shap-lrjball.readthedocs.io/en/docs_update/api.html).
    In the next chapter, we will cover the practical implementation of the SHAP explainers
    using the code tutorials on GitHub in which we will implement the SHAP explainers
    for explaining models trained on different types of datasets. In the next section,
    we will cover a practical tutorial on how to use SHAP for explaining regression
    models to give you a glimpse of how to apply SHAP for model explainability.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经讨论了 SHAP 中各种解释器的理论方面。有关这些解释器的更多信息，我强烈建议您查看[https://shap-lrjball.readthedocs.io/en/docs_update/api.html](https://shap-lrjball.readthedocs.io/en/docs_update/api.html)。在下一章中，我们将通过
    GitHub 上的代码教程来介绍 SHAP 解释器的实际应用，我们将实现 SHAP 解释器来解释在不同类型数据集上训练的模型。在下一节中，我们将介绍一个关于如何使用
    SHAP 解释回归模型的实际教程，让您了解如何应用 SHAP 来提高模型的可解释性。
- en: Using SHAP to explain regression models
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SHAP 解释回归模型
- en: 'In the previous section, we learned about different visualizations and explainers
    in SHAP for explaining ML models. Now, I will give you practical exposure to using
    SHAP for providing model explainability. The framework is available as an open
    source project on GitHub: [https://github.com/slundberg/shap](https://github.com/slundberg/shap).
    You can get the API documentation at [https://shap-lrjball.readthedocs.io/en/docs_update/index.html](https://shap-lrjball.readthedocs.io/en/docs_update/index.html).
    The complete tutorial is provided in the GitHub repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/Intro_to_SHAP.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/Intro_to_SHAP.ipynb).
    I strongly recommend that you read this section and execute the code side by side.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了 SHAP 中用于解释机器学习模型的不同可视化和解释器。现在，我将向您展示如何使用 SHAP 提供模型可解释性的实际操作。该框架作为开源项目在
    GitHub 上提供：[https://github.com/slundberg/shap](https://github.com/slundberg/shap)。您可以在[https://shap-lrjball.readthedocs.io/en/docs_update/index.html](https://shap-lrjball.readthedocs.io/en/docs_update/index.html)获取
    API 文档。完整的教程在 GitHub 仓库[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/Intro_to_SHAP.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/Intro_to_SHAP.ipynb)中提供。我强烈建议您阅读这一部分并边读边执行代码。
- en: Setting up SHAP
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 SHAP
- en: 'Installing SHAP in Python can be done easily using the pip installer by using
    the following command in your console:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中安装 SHAP 可以通过使用以下命令在您的控制台中通过 pip 安装器轻松完成：
- en: '[PRE0]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Since the tutorial requires you to have other Python frameworks installed,
    you can also try the following command to install all the necessary modules for
    the tutorial from the Jupyter notebook itself if it is not installed already:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于教程需要您安装其他 Python 框架，如果尚未安装，您也可以尝试以下命令从 Jupyter 笔记本本身安装教程所需的全部必要模块：
- en: '[PRE1]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s import SHAP and check its version:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们导入 SHAP 并检查其版本：
- en: '[PRE2]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The version that I have used for this tutorial is *0.40.0*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这个教程中使用的版本是 *0.40.0*。
- en: Please note that with different versions, there can be different changes to
    the API or different errors that you might face. So, I will recommend you to look
    at the latest documentation of the framework if you encounter any such issues.
    I have also added a **SHAP Errata** ([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter06/SHAP_ERRATA](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter06/SHAP_ERRATA))
    in the repository for providing solutions to existing known issues with the SHAP
    framework.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于版本不同，API 可能会有所变化，或者您可能会遇到不同的错误。因此，如果您遇到此类问题，我建议您查看框架的最新文档。我还在仓库中添加了**SHAP
    错误列表**([https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter06/SHAP_ERRATA](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter06/SHAP_ERRATA))，以提供对
    SHAP 框架已知问题的解决方案。
- en: Inspecting the dataset
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查数据集
- en: 'For this tutorial, we will use the *Red Wine Quality Dataset* from *Kaggle*:
    [https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009).
    The dataset has already been added to the code repository so that it is easy for
    you to access the data. This particular dataset contains information about the
    red variant of the Portuguese *Vinho Verde* wine, and it is derived from the original
    UCI source at [https://archive.ics.uci.edu/ml/datasets/wine+quality](https://archive.ics.uci.edu/ml/datasets/wine+quality).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Wine Quality Data Set
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The credit for this dataset goes to *P. Cortez, A. Cerdeira, F. Almeida, T.
    Matos and J. Reis. Modeling wine preferences by data mining from physicochemical
    properties.*
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use this dataset to solve a regression problem. We will load the data
    as a pandas DataFrame and perform an initial inspection:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure 6.16* illustrates a snapshot of the data:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Snapshot of the pandas DataFrame of the Wine Quality dataset'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_06_016.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.16 – Snapshot of the pandas DataFrame of the Wine Quality dataset
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check some quick information about the dataset using the following command:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This gives the following output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As you can see, our dataset consists of *11 numerical features* and *1,599*
    records of data. The target outcome that the regression model will be learning
    is the *quality* of the wine, which is an *integer feature*. Although we are using
    this dataset for solving a regression problem, the same problem can be viewed
    as a classification problem and the same underlying data can be used.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose of the tutorial is not for building an extremely efficient model,
    but rather to consider any model and use SHAP to explain the workings of the model.
    So, we will skip the EDA, data normalization, outlier detection, and even feature
    engineering steps, which are, otherwise, crucial steps for building a robust ML
    model. But missing values in the dataset can create problems for the SHAP algorithm.
    Therefore, I would suggest doing a quick check of missing values at the very least:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding lines of code will result in the following plot as its output:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – The missing plot visualization for the dataset'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_06_017.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.17 – The missing plot visualization for the dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, the dataset doesn't have any missing values; otherwise, we might
    have to handle this before proceeding further. But we are good to proceed with
    the modeling step as there are no significant issues with the data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since I do not have any pretrained model for this dataset, I thought of building
    a simple random forest model. We can divide the model into a training and a testing
    set using the 80:20 split ratio:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'To use the random forest algorithm, we would need to import this algorithm
    from the scikit-learn module and then fit the regression model on the training
    data:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Important Note
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Considering the objective of this notebook, we are not doing an extensive hyperparameter
    tuning process. But I would strongly recommend you to perform all necessary best
    practices like *EDA, feature engineering, hyperparameter tuning, cross-validation*,
    and others for your use cases.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这个笔记本的目标，我们并没有进行广泛的超参数调整过程。但我强烈建议您为您的用例执行所有必要的最佳实践，如*数据探索性分析（EDA）、特征工程、超参数调整、交叉验证*等。
- en: 'Once the trained model is ready, we will do a quick evaluation of the model
    using the metric of **coefficient of determination** (**R2 coefficient**):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们将使用**确定系数**（**R2系数**）的指标对模型进行快速评估：
- en: '[PRE43]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The model score obtained is just around 0.5, which indicates that the model
    is not very efficient. So, model explainability is even more important for such
    models. Now, let's use SHAP to explain the model.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的模型评分大约为0.5，这表明模型效率不高。因此，对于此类模型，模型可解释性尤为重要。现在，让我们使用SHAP来解释模型。
- en: Application of SHAP
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP的应用
- en: 'Applying SHAP is very easy and can be done in a few lines of code. First, we
    will use the Shapley value-based explainer on the test dataset:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 应用SHAP非常简单，只需几行代码即可完成。首先，我们将使用基于Shapley值的解释器对测试数据集进行处理：
- en: '[PRE44]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Then, we can use the SHAP values for the various visualization techniques discussed
    earlier. The choice of the visualizations depends on whether we want to go for
    global explainability or local explainability. For example, for the summary plot
    shown in *Figure 6.9*, we can use the following code:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用SHAP值来应用之前讨论的各种可视化技术。选择可视化方式取决于我们是否希望追求全局可解释性或局部可解释性。例如，对于*图6.9*中显示的摘要图，我们可以使用以下代码：
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To provide local explainability, if we want to use the decision plot shown
    in *Figure 6.14*, we can try the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要使用*图6.14*中显示的决策图来提供局部可解释性，我们可以尝试以下代码：
- en: '[PRE48]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'To use a different explainer algorithm, we just need to select the appropriate
    explainer. For Tree Explainers, we can try the following code:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用不同的解释器算法，我们只需选择合适的解释器。对于树形解释器，我们可以尝试以下代码：
- en: '[PRE51]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The usage of this framework to explain the regression model while considering
    various aspects has already been covered in the notebook tutorial: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/Intro_to_SHAP.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/Intro_to_SHAP.ipynb).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本教程中已经涵盖了使用此框架解释回归模型并考虑各种方面的用法：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/Intro_to_SHAP.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter06/Intro_to_SHAP.ipynb)。
- en: In the next chapter, we will cover more interesting use cases. Next, let's discuss
    some advantages and disadvantages of this framework.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍更多有趣的应用案例。接下来，让我们讨论这个框架的一些优势和劣势。
- en: Advantages and limitations of SHAP
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SHAP的优势和局限性
- en: In the previous section, we discussed the practical application of SHAP for
    explaining a regression model with just a few lines of code. However, since SHAP
    is not the only explainability framework, we should be aware of the specific advantages
    and disadvantages of SHAP, too.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了使用SHAP仅用几行代码解释回归模型的实际应用。然而，由于SHAP不是唯一的可解释性框架，我们也应该了解SHAP的具体优势和劣势。
- en: Advantages
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优势
- en: 'The following is a list of some of the advantages of SHAP:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些SHAP的优势：
- en: '**Local explainability**: Since SHAP provides local explainability to inference
    data, it enables users to analyze key factors that are positively or negatively
    affecting the model''s decision-making process. As SHAP provides local explainability,
    it is useful for production-level ML systems, too.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部可解释性**：由于SHAP为推理数据提供了局部可解释性，它使用户能够分析影响模型决策过程的关键因素，这些因素是正面还是负面影响的。由于SHAP提供了局部可解释性，因此它对生产级机器学习系统也很有用。'
- en: '**Global explainability**: Global explainability provided in SHAP helps to
    extract key information about the model and the training data, especially from
    the collective feature importance plots. I think SHAP is better than LIME for
    getting a global perspective on the model. SP-LIME in LIME is good for getting
    an example-driven global perspective of the model, but I think SHAP provides a
    generalized global understanding of trained models.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局可解释性**：SHAP提供的全局可解释性有助于提取有关模型和训练数据的键信息，特别是从集体特征重要性图中。我认为SHAP在获得模型的全局视角方面优于LIME。LIME中的SP-LIME对于获得以示例驱动的模型全局视角很好，但我认为SHAP提供了对训练模型的通用全局理解。'
- en: '**Model-agnostic and model-specific**: SHAP can be model-agnostic and model-specific.
    So, it can work with black-box models and also work with complex deep learning
    models to provide explainability.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型无关性和模型特异性**：SHAP可以是模型无关的，也可以是模型特定的。因此，它可以与黑盒模型一起工作，也可以与复杂的深度学习模型一起工作，以提供可解释性。'
- en: '**Theoretical robustness**: The concept of using Shapley values for model explainability,
    which is based on the principles of coalition game theory, captures feature interaction
    very well. Also, the properties of SHAP regarding *efficiency*, *symmetry*, *dummy*,
    and *additivity* are formulated on a robust theoretical foundation. Unlike SHAP,
    LIME is not based on a solid theory as it assumes ML models will behave linearly
    for some local data points. But there is not much theoretical evidence that proves
    why this assumption is true for all cases. That is why I would say SHAP is based
    on ideas that are theoretically more robust than LIME.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理论稳健性**：基于联盟博弈论原则的Shapley值用于模型可解释性的概念，很好地捕捉了特征交互。此外，SHAP关于*效率*、*对称性*、*虚拟性*和*可加性*的性质是在稳健的理论基础上制定的。与SHAP不同，LIME没有建立在坚实的理论基础之上，因为它假设ML模型对于某些局部数据点将表现出线性行为。但是，没有多少理论证据证明为什么这个假设对所有情况都成立。这就是为什么我会说SHAP基于比LIME更稳健的理论思想。'
- en: These advantages make SHAP one of the most popular choices of the XAI framework.
    Unfortunately, applying SHAP can be really challenging for high-dimensional datasets
    as it does not provide actionable explanations. Let's look at some of the limitations
    of SHAP.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这些优势使SHAP成为XAI框架中最受欢迎的选择之一。不幸的是，对于高维数据集，应用SHAP可能真的具有挑战性，因为它不提供可操作的解释。让我们看看SHAP的一些局限性。
- en: Limitations
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: 'Here is a list of some of the limitations of SHAP:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是SHAP的一些局限性的列表：
- en: '**SHAP is not the preferred choice for high-dimensional data**: Computing Shapley
    values on high-dimensional data can be computationally more challenging, as the
    time complexity of the algorithm is ![](img/B18216_06_011.png), where *n* is the
    total number of features in the dataset.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SHAP不是高维数据的首选选择**：在高维数据上计算Shapley值可能更具计算挑战性，因为算法的时间复杂度为![](img/B18216_06_011.png)，其中*n*是数据集中特征的总数。'
- en: '**Shapley values are ineffective for selective explanation**: Shapley values
    try to consider all the features for providing explainability. The explanations
    can be incorrect for sparse explanations, in which only selected features are
    considered. But usually, human-friendly explanations consider selective features.
    So, I would say that LIME is better than SHAP when you seek a selective explanation.
    However, more recent versions of the SHAP framework do include the same ideas
    as LIME and can be almost equally effective for sparse explanations.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Shapley值对于选择性解释无效**：Shapley值试图考虑所有特征以提供可解释性。对于只考虑所选特征的稀疏解释，解释可能是错误的。但通常，人类友好的解释会考虑选择性特征。因此，当您寻求选择性解释时，我认为LIME比SHAP更好。然而，SHAP框架的最新版本确实包括与LIME相同的思想，并且对于稀疏解释可以几乎同样有效。'
- en: '**SHAP cannot be used for prescriptive insights**: SHAP computes the Shapley
    values for each feature and does not build a prediction model like LIME. So, it
    cannot be used for analyzing any *what-if scenario* or for providing any *counter-factual
    example* for suggesting actionable insights.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SHAP不能用于提供规范性见解**：SHAP计算每个特征的Shapley值，并不像LIME那样构建预测模型。因此，它不能用于分析任何*假设情景*或提供任何*反事实示例*以提供可操作的见解。'
- en: '**KernelSHAP can be slow**: Although KernelSHAP is model-agnostic, it can be
    very slow and, thus, might not be suitable for production-level ML systems for
    models trained on high-dimensional data.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KernelSHAP可能很慢**：尽管KernelSHAP是模型无关的，但它可能非常慢，因此可能不适合用于在训练高维数据上训练的模型的生产级ML系统。'
- en: '**Not extremely human-friendly**: Apart from analyzing feature importance through
    feature interactions, SHAP visualizations can be complicated to interpret for
    any non-technical user. Often, non-technical users prefer simple selective actionable
    insights, recommendations, or justifications from ML models. Unfortunately, SHAP
    requires another layer of abstraction for human-friendly explanations when used
    in production systems.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不太适合人类使用**：除了通过特征交互分析特征重要性之外，SHAP的可视化对于任何非技术用户来说可能都难以解释。通常，非技术用户更喜欢从机器学习模型中获得简单、有针对性的见解、建议或理由。不幸的是，当在生产系统中使用时，SHAP需要另一层抽象来为人类提供友好的解释。'
- en: As we can see from the points discussed in this section, SHAP might not be the
    most ideal framework for explainability, and there is a lot of space for improvement
    to make it more human-friendly. However, it is indeed an important and very useful
    framework for explaining black-box algorithms, especially for technical users.
    This brings us to the end of the chapter. Let's summarize what we have learned
    in the chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 从本节讨论的要点来看，SHAP可能不是最理想的解释性框架，而且还有很大的改进空间，使其更加适合人类使用。然而，它确实是一个重要的、非常有用的框架，用于解释黑盒算法，尤其是对于技术用户来说。这使我们来到了本章的结尾。让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on understanding the importance of the SHAP framework
    for model explainability. By now, you have a good understanding of Shapley values
    and SHAP. We have covered how to use SHAP for model explainability through a variety
    of visualization and explainer methods. Also, we have covered a code walk-through
    for using SHAP to explain regression models. Finally, we discussed some advantages
    and limitations of the framework.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于理解SHAP框架对于模型解释性的重要性。到目前为止，你对Shapley值和SHAP有了很好的理解。我们介绍了如何通过各种可视化和解释方法使用SHAP进行模型解释性分析。我们还介绍了使用SHAP解释回归模型的代码示例。最后，我们讨论了该框架的一些优点和局限性。
- en: In the next chapter, we will cover more interesting practical use cases for
    applying SHAP on different types of datasets.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍更多关于在不同类型的数据集上应用SHAP的有趣的实际用例。
- en: References
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For additional information, please refer to the following resources:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参阅以下资源：
- en: '*Shapley, Lloyd S. "A Value for n-Person Games." Contributions to the Theory
    of Games 2.28 (1953)*: [https://doi.org/10.1515/9781400881970-018](https://doi.org/10.1515/9781400881970-018)'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Shapley, Lloyd S. "A Value for n-Person Games." Contributions to the Theory
    of Games 2.28 (1953)*：[https://doi.org/10.1515/9781400881970-018](https://doi.org/10.1515/9781400881970-018)'
- en: 'The Red Wine Quality dataset from Kaggle: [https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaggle上的红葡萄酒质量数据集：[https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)
- en: 'The SHAP GitHub project: [https://github.com/slundberg/shap](https://github.com/slundberg/shap)'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SHAP GitHub项目：[https://github.com/slundberg/shap](https://github.com/slundberg/shap)
- en: 'The official SHAP documentation: [https://shap.readthedocs.io/en/latest/index.html](https://shap.readthedocs.io/en/latest/index.html)'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 官方SHAP文档：[https://shap.readthedocs.io/en/latest/index.html](https://shap.readthedocs.io/en/latest/index.html)
