- en: Boosting Model Performance with Boosting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用提升算法提升模型性能
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Introduction to boosting
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升算法简介
- en: Implementing AdaBoost for disease risk prediction using scikit-learn
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现AdaBoost进行疾病风险预测
- en: Implementing gradient boosting for disease risk prediction using scikit-learn
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现梯度提升进行疾病风险预测
- en: Implementing extreme gradient boosting for glass identification using XGBoost
    with scikit-learn
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用XGBoost和scikit-learn实现极端梯度提升进行玻璃识别
- en: Introduction to boosting
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升算法简介
- en: A boosting algorithm is an ensemble technique that helps to improve model performance
    and accuracy by taking a group of weak learners and combining them to form a strong
    learner. The idea behind boosting is that predictors should learn from mistakes
    that have been made by previous predictors.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法是一种集成技术，通过将一组弱学习器组合起来形成一个强学习器，从而帮助提高模型性能和准确性。提升背后的理念是预测器应该从先前预测器所犯的错误中学习。
- en: 'Boosting algorithms have two key characteristics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 提升算法有两个关键特性：
- en: First, they undergo multiple iterations
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，它们经历多次迭代
- en: Second, each iteration focuses on the instances that were wrongly classified
    by previous iterations
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，每次迭代都专注于先前迭代错误分类的实例
- en: When an input is misclassified by a hypothesis, its weight is altered in the
    next iteration so that the next hypothesis can classify it correctly. More weight
    will be given to those that provide better performance on the training data. This
    process, through multiple iterations, converts weak learners into a collection
    of strong learners, thereby improving the model's performance.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个输入被假设错误分类时，其权重将在下一次迭代中改变，以便下一个假设可以正确分类它。将更多的权重给予在训练数据上提供更好性能的那些。通过多次迭代，这个过程将弱学习器转换为一组强学习器，从而提高模型性能。
- en: In bagging, no bootstrap sample depends on any other bootstrap, so they all
    run in parallel. Boosting works in a sequential manner and does not involve bootstrap
    sampling. Both bagging and boosting reduce the variance of a single estimate by
    combining several estimates from different models into a single estimate. However,
    it is important to note that boosting does not help significantly if the single
    model is overfitting. Bagging would be a better option if the model overfits.
    On the other hand, boosting tries to reduce bias, while bagging rarely improves
    bias.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在袋装法中，没有自助样本依赖于任何其他自助样本，因此它们都并行运行。提升算法以顺序方式进行，不涉及自助采样。袋装法和提升法都通过将不同模型中的多个估计组合成一个单一估计来减少单个估计的方差。然而，需要注意的是，如果单个模型过拟合，提升算法并不能显著帮助。如果模型过拟合，袋装法可能是一个更好的选择。另一方面，提升算法试图减少偏差，而袋装法很少能改善偏差。
- en: In this chapter, we will introduce different boosting algorithms such as **Adaptive
    Boosting** (**AdaBoost**), gradient boosting, and **e****xtreme gradient boosting**
    (**XGBoost**).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍不同的提升算法，例如**自适应提升**（**AdaBoost**）、梯度提升和**极端梯度提升**（**XGBoost**）。
- en: Implementing AdaBoost for disease risk prediction using scikit-learn
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现AdaBoost进行疾病风险预测
- en: AdaBoost is one of the earliest boosting algorithms that was used for binary
    classification. It was proposed by Freund and Schapire in 1996\. Many other boosting-based
    algorithms have since been developed on top of AdaBoost.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost是用于二元分类的最早提升算法之一。它由Freund和Schapire于1996年提出。此后，基于AdaBoost的许多其他提升算法已被开发出来。
- en: Another variation of adaptive boosting is known as **AdaBoost-abstain**. AdaBoost-abstain allows each
    baseline classifier to abstain from voting if its dependent feature is missing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种自适应提升的变体被称为**AdaBoost-abstain**。AdaBoost-abstain允许每个基线分类器在其相关特征缺失时放弃投票。
- en: 'AdaBoost focuses on combining a set of weak learners into a strong learner.
    The process of an AdaBoost classifier is as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost专注于将一组弱学习器组合成一个强学习器。AdaBoost分类器的过程如下：
- en: Initially, a short decision tree classifier is fitted onto the data. The decision
    tree can just have a single split, which is known as a **decision stump**. The
    overall errors are evaluated. This is the first iteration.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始时，将一个短决策树分类器拟合到数据上。决策树可以只有一个分割，这被称为**决策树桩**。评估整体误差。这是第一次迭代。
- en: In the second iteration, whatever data is correctly classified will be given
    lowerweights, while higher weights will be given to the misclassified classes.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二次迭代中，正确分类的数据将被赋予较低的权重，而错误分类的类别将被赋予较高的权重。
- en: In the third iteration, another decision stump will be fitted to the data and
    the weights will be changed again in the next iteration.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第三次迭代中，将再次对数据进行拟合，并在下一次迭代中更改权重。
- en: Once these iterations are over, the weights are automatically calculated for
    each classifier at each iteration based on the error rates to come up with a strong
    classifier.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦这些迭代完成，每个分类器在每个迭代中都会根据错误率自动计算权重，以得出一个强分类器。
- en: 'The following screenshot shows how AdaBoost works:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 AdaBoost 的工作原理：
- en: '![](img/659ca5ea-7b32-4ead-9a6f-6c082501bb48.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/659ca5ea-7b32-4ead-9a6f-6c082501bb48.png)'
- en: The concept behind this algorithm is to distribute the weights to the training
    example and select the classifier with the lowest weighted error. Finally, it
    constructs a strong classifier as a linear combination of these weak learners.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法背后的概念是将权重分配给训练示例，并选择具有最低加权错误的分类器。最后，它通过这些弱学习者的线性组合构建一个强分类器。
- en: 'The general equation for an AdaBoost as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 的一般公式如下：
- en: '![](img/6ad74a62-127d-4778-897c-2841ac0d91e3.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6ad74a62-127d-4778-897c-2841ac0d91e3.png)'
- en: Here, **F(x)** represents a strong classifier, **![](img/141fe725-e4a9-44d6-93f6-b61bfb296f09.png)** represent
    the weights, and **f(x)** represents a weak classifier.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**F(x)** 代表一个强分类器，**![图片](img/141fe725-e4a9-44d6-93f6-b61bfb296f09.png)**
    代表权重，而 **f(x)** 代表一个弱分类器。
- en: 'The AdaBoost classifier takes various parameters. The important ones are explained
    as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost 分类器接受各种参数。以下解释了重要的参数：
- en: '`base_estimator`: The learning algorithm that is used to train the models.
    If a value is not provided for this parameter, the base estimator is `DecisionTreeClassifier
    (max_depth=1)`.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`base_estimator`：用于训练模型的算法。如果未提供此参数的值，则基本估计器为 `DecisionTreeClassifier (max_depth=1)`。'
- en: '`n_estimators`: The number of models to iteratively train.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：迭代训练的模型数量。'
- en: '`learning_rate`:The contribution of each model to the weights. By default,
    `learning_rate` has a value of `1`. A lower value for the learning rate forces
    the model to train slower but might result in better performance scores.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：每个模型对权重的贡献。默认情况下，`learning_rate` 的值为 `1`。较低的 `learning_rate`
    值会使模型训练得更慢，但可能会得到更好的性能分数。'
- en: Getting ready
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'To start with, import the `os` and the `pandas` packages and set your working
    directory according to your requirements:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入 `os` 和 `pandas` 包，并根据您的需求设置工作目录：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Download the `breastcancer.csv` dataset from GitHub and copy it to your working
    directory. Read the dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从 GitHub 下载 `breastcancer.csv` 数据集并将其复制到您的当前工作目录。读取数据集：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Take a look at the first few rows with the `head()` function:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `head()` 函数查看前几行：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Notice that the `diagnosis` variable has values such as M and B, representing
    Malign and Benign, respectively. We will perform label encoding on the `diagnosis`
    variable so that we can convert the M and B values into numeric values.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`diagnosis` 变量具有 M 和 B 等值，分别代表恶性和良性。我们将对 `diagnosis` 变量进行标签编码，以便将 M 和 B 值转换为数值。
- en: 'We use `head()` to see the changes:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `head()` 来查看变化：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then check whether the dataset has any null values:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们检查数据集是否有任何空值：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We check the shape of the dataset with `shape()`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `shape()` 检查数据集的形状：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now separate our target and feature set. We also split our dataset into
    training and testing subsets:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将我们的目标集和特征集分开。我们还把数据集分成训练集和测试集：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, we will move on to building our model using the `AdaBoost` algorithm.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续使用 `AdaBoost` 算法构建我们的模型。
- en: It is important to note that the accuracy and AUC scores may differ because
    of random splits and other randomness factors.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，准确率和 AUC 分数可能因随机分割和其他随机因素而不同。
- en: How to do it...
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We will now look at how to use an AdaBoost to train our model:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨如何使用 AdaBoost 训练我们的模型：
- en: 'Before we build our first `AdaBoost` model, let''s train our model using the `DecisionTreeClassifier`:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们构建第一个 `AdaBoost` 模型之前，让我们使用 `DecisionTreeClassifier` 训练我们的模型：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see our accuracy and **Area Under the Curve** (**AUC**) with the following
    code:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码看到我们的准确率和 **曲线下面积**（**AUC**）：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We get an accuracy score and an AUC value of 91.81% and 0.91, respectively.
    Note that these values might be different for different users due to randomness.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了准确率分数和 AUC 值分别为 91.81% 和 0.91。请注意，由于随机性，这些值可能因用户而异。
- en: 'Now, we will build our AdaBoost model using the scikit-learn library. We will
    use the `AdaBoostClassifier` to build our `AdaBoost` model. `AdaBoost` uses `dtree`
    as the base classifier by default:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用scikit-learn库构建我们的AdaBoost模型。我们将使用`AdaBoostClassifier`构建我们的`AdaBoost`模型。`AdaBoost`默认使用`dtree`作为基础分类器：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We check the accuracy and AUC value of the model on our test data:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们检查了模型在测试数据上的准确率和AUC值：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We notice that we get an accuracy score of 92.82% and an AUC value of 0.97\.
    Both of these metrics are higher than the decision tree model we built in *Step
    1*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到我们得到了92.82%的准确率分数和0.97的AUC值。这两个指标都高于我们在*步骤1*中构建的决策树模型。
- en: 'Then, we must fine-tune our hyperparameters. We set `n_estimators` to `100`
    and `learning_rate` to `0.4`:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须微调我们的超参数。我们将`n_estimators`设置为`100`，将`learning_rate`设置为`0.4`：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we will check the accuracy and AUC values of our new model on our test
    data:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将检查我们的新模型在测试数据上的准确率和AUC值：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We notice the accuracy drops to 92.39%, but that we get an improved AUC value
    of 0.98.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到准确率下降到92.39%，但我们得到了改进的AUC值，为0.98。
- en: How it works...
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we used the `DecisionTreeClassifier` to build our model. In *Step
    2*, we noticed that our mean accuracy and the AUC score were 91.81% and 0.91,
    respectively. We aimed to improve this using the `AdaBoost` algorithm.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们使用`DecisionTreeClassifier`构建了我们的模型。在*步骤2*中，我们注意到我们的平均准确率和AUC分数分别为91.81%和0.91。我们希望通过使用`AdaBoost`算法来提高这一点。
- en: Note that the `AdaBoost` algorithm uses a decision tree as the base classifier
    by default. In *Step 3*, we trained our model using `AdaBoost` with the default
    base learner. We set `n_estimators` to `100` and the `learning_rate` to `0.1`.
    We checked our mean accuracy and AUC value in *Step 4*. We noticed that we got
    a decent improvement in the mean accuracy and the AUC as they jumped to 93.57%
    and 0.977, respectively.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`AdaBoost`算法默认使用决策树作为基础分类器。在*步骤3*中，我们使用默认的基础学习器`AdaBoost`训练了我们的模型。我们将`n_estimators`设置为`100`，将`learning_rate`设置为`0.1`。我们在*步骤4*中检查了我们的平均准确率和AUC值。我们注意到平均准确率和AUC值分别提高了93.57%和0.977。
- en: In *Step 5*, we fine-tuned some of the hyperparameters for our `AdaBoost` algorithm,
    which used a decision tree as the base classifier. We set the `n_estimators` to
    `100` and the `learning_rate` to `0.4`. *Step 6* gave us the accuracy and AUC
    values for the model we built in *Step 5*. We saw that the accuracy dropped to
    93.56% and that the AUC stayed similar at 0.981.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤5*中，我们微调了`AdaBoost`算法的一些超参数，该算法使用决策树作为基础分类器。我们将`n_estimators`设置为`100`，将`learning_rate`设置为`0.4`。*步骤6*给出了我们在*步骤5*中构建的模型的准确率和AUC值。我们看到准确率下降到93.56%，而AUC值保持在0.981左右。
- en: There's more...
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Here, we will showcase training a model using AdaBoost with a **support vector
    machine** (**SVM**) as the base learner.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将展示如何使用AdaBoost训练一个模型，其中基础学习器为**支持向量机**（**SVM**）：
- en: 'By default, AdaBoost uses a decision tree as the base learner. We can use different
    base learners as well. In the following example, we have used an SVM as our base
    learner with the `AdaBoost` algorithm. We use `SVC` with `rbf` as the kernel:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，AdaBoost使用决策树作为基础学习器。我们也可以使用不同的基础学习器。在以下示例中，我们使用SVM作为我们的基础学习器，并使用`AdaBoost`算法。我们使用`SVC`并使用`rbf`作为核：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can check the accuracy and the AUC values of our AdaBoost model with **support
    vector classifier** (**SVC**) as the base learner:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用**支持向量分类器**（**SVC**）作为基础学习器来检查我们的AdaBoost模型的准确率和AUC值：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We noticed that the accuracy and AUC values fall to 62.57 and 0.92, respectively.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到准确率和AUC值分别下降到62.57和0.92。
- en: 'Now, we will rebuild our AdaBoost model with SVC. This time, we will use a
    linear kernel:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用SVC重新构建我们的AdaBoost模型。这次，我们将使用线性核：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We now get a mean accuracy of 90.64% and a decent AUC value of 0.96.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们得到了90.64%的平均准确率和0.96的合理AUC值。
- en: 'We will now plot a graph to compare the AUC value of each model using the following
    code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用以下代码绘制一个图表来比较每个模型的AUC值：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This gives us the following plot:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下图表：
- en: '![](img/ccb24245-d84c-46b3-a7e0-46377e766863.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ccb24245-d84c-46b3-a7e0-46377e766863.png)'
- en: 'We can also plot the accuracy of all the models with the following code:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用以下代码绘制所有模型的准确率：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This gives us the following output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '![](img/3c274f0f-7286-4468-b1a2-07b246aa9cd5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3c274f0f-7286-4468-b1a2-07b246aa9cd5.png)'
- en: See also
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'We can also use grid search with AdaBoost:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用网格搜索与AdaBoost：
- en: '[PRE18]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the preceding code, we performed a grid search with the `n_estimators` set
    to `10`, `30`, `40`, and `100`, and `learning_rate` set to `0.1`, `0.2`, and `0.3`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们进行了网格搜索，将`n_estimators`设置为`10`、`30`、`40`和`100`，将`learning_rate`设置为`0.1`、`0.2`和`0.3`。
- en: Implementing a gradient boosting machine for disease risk prediction using scikit-learn
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn实现梯度提升机进行疾病风险预测
- en: Gradient boosting is a machine learning technique that works on the principle
    of boosting, where weak learners iteratively shift their focus toward error observations
    that were difficult to predict in previous iterations and create an ensemble of
    weak learners, typically decision trees.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升是一种机器学习技术，它基于提升原理，其中弱学习器迭代地将注意力转向在先前迭代中难以预测的错误观测值，并创建一个弱学习者的集成，通常是决策树。
- en: 'Gradient boosting trains models in a sequential manner, and involves the following
    steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升以顺序方式训练模型，并涉及以下步骤：
- en: Fitting a model to the data
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到数据中
- en: Fitting a model to the residuals
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到残差中
- en: Creating a new model
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的模型
- en: While the AdaBoost model identifies errors by using weights that have been assigned
    to the data points, gradient boosting does the same by calculating the gradients
    in the loss function. The loss function is a measure of how a model is able to
    fit the data on which it is trained and generally depends on the type of problem
    being solved. If we are talking about regression problems, mean squared error
    may be used, while in classification problems, the logarithmic loss can be used.
    The gradient descent procedure is used to minimize loss when adding trees one
    at a time. Existing trees in the model remain the same.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然AdaBoost模型通过使用分配给数据点的权重来识别错误，但梯度提升通过在损失函数中计算梯度来完成同样的工作。损失函数是衡量模型能够拟合其训练数据的程度的一个指标，通常取决于所解决的问题的类型。如果我们谈论回归问题，可能使用均方误差，而在分类问题中，可以使用对数损失。梯度下降过程用于在逐个添加树时最小化损失。模型中现有的树保持不变。
- en: 'There are a handful of hyperparameters that may be tuned for this:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些超参数可以调整：
- en: '`N_estimators`:This represents the number of trees in the model. Usually, the
    higher it is, the better the model learns the data.'
  id: totrans-101
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N_estimators`：这表示模型中的树的数量。通常，它越高，模型学习数据的能力越好。'
- en: '`max_depth`: This signifies how deep our tree is. It is used to control overfitting.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：这表示我们的树有多深。它用于控制过拟合。'
- en: '`min_samples_split`: This is the minimum number of samples required to split
    an internal node. Values that are too high can prevent the model from learning
    relations.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：这是分割内部节点所需的最小样本数。过高的值可能会阻止模型学习关系。'
- en: '`learning_rate`: This controls the magnitude of change in the estimates. Lower
    values with a higher number of trees are generally preferred.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：这控制估计值的变化幅度。通常，较低值与较高的树的数量更受青睐。'
- en: '`loss`: This refers to the loss function that is minimized in each split. `deviance`
    is used in the algorithm as the default parameter, while the other is `exponential`.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`loss`：这指的是在每次分割中最小化的损失函数。算法中默认参数使用`deviance`，另一个是`exponential`。'
- en: '`max_features`: This represents the number of features we have to consider
    when looking for the best split.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：这表示在寻找最佳分割时需要考虑的特征数量。'
- en: '`criterion`:This function measures the quality of the split and supports `friedman_mse`
    and `mae` to evaluate the performance of the model.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`：这个函数衡量分割的质量，并支持`friedman_mse`和`mae`来评估模型的性能。'
- en: '`subsample`:This represents the fraction of samples to be used for fitting
    the individual base learners. Choosing a subsample that is less than 1.0 leads
    to a reduction of variance and an increase in bias.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subsample`：这表示用于拟合单个基学习者的样本的分数。选择小于1.0的子样本会导致方差减少和偏差增加。'
- en: '`min_impurity_split`: This is represented as a threshold to stop tree growth
    early.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_impurity_split`：这表示一个阈值，用于提前停止树的生长。'
- en: Getting ready
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will take the same dataset that we used for training our AdaBoost model.
    In this example, we will see how we can train our model using gradient boosting
    machines. We will also look at a handful of hyperparameters that can be tuned
    to improve the model's performance.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与训练我们的AdaBoost模型相同的数据集。在这个例子中，我们将看到如何使用梯度提升机来训练我们的模型。我们还将查看一些可以调整以改进模型性能的超参数。
- en: 'First, we must import all the required libraries:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须导入所有必需的库：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Then, we read our data and label encode our target variables to 1 and 0:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们读取我们的数据并将目标变量编码为1和0：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, separate our target and feature variables. We split our data into train
    and test subsets:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将我们的目标变量和特征变量分开。我们将数据分为训练集和测试集：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This is the same code that we used in the *Getting ready* section of the `AdaBoost`
    example.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在`AdaBoost`示例的*准备就绪*部分中使用的代码相同。
- en: How to do it...
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'We will now look at how to use a Gradient Boosting Machines to train our model:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨如何使用梯度提升机来训练我们的模型：
- en: 'We imported `GradientBoostingClassifier` from `sklearn.ensemble` in the last
    section,* Getting ready*. We trained our model using `GradieBoostingClassfier`:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一节*准备就绪*中，我们导入了`GradientBoostingClassifier`从`sklearn.ensemble`。我们使用`GradieBoostingClassfier`训练我们的模型：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here, we must pass our test data to the `predict()` function to make the predictions
    using the model we built in *Step 1*:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们必须将我们的测试数据传递给`predict()`函数，使用我们在*步骤 1*中构建的模型进行预测：
- en: '[PRE23]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we use  `classification_report` to see the following metrics:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们使用`classification_report`查看以下指标：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`classification_report` gives us the following output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`classification_report`提供了以下输出：'
- en: '![](img/ad04d7fa-7696-4168-bd2f-8bee27c56d4a.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad04d7fa-7696-4168-bd2f-8bee27c56d4a.png)'
- en: 'We will use `confusion_matrix()` to generate the confusion matrix. We then
    pass the output of the `confusion_matrix` to our predefined function, that is, `plot_confusion_matrix()`,
    to plot the matrix:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`confusion_matrix()`生成混淆矩阵。然后，我们将`confusion_matrix`的输出传递给我们的预定义函数，即`plot_confusion_matrix()`，以绘制矩阵：
- en: '![](img/905c7c1c-03c9-453b-b2d6-2931f578c79f.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/905c7c1c-03c9-453b-b2d6-2931f578c79f.png)'
- en: We can check the test accuracy and the AUC value with `accuracy_score()` and `roc_auc_score()`.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`accuracy_score()`和`roc_auc_score()`来检查测试准确率和AUC值。
- en: 'Note that `accuracy_score` and `roc_auc_score` have been imported from `sklearn.metrics`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`accuracy_score`和`roc_auc_score`已从`sklearn.metrics`导入：
- en: '![](img/e2061b05-4ac1-4159-a970-4e4c9f32c13c.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2061b05-4ac1-4159-a970-4e4c9f32c13c.png)'
- en: How it works...
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we trained a gradient boosting classifier model. In *Step 2*, we
    used the `predict()` method to make predictions on our test data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 1*中，我们训练了一个梯度提升分类器模型。在*步骤 2*中，我们使用`predict()`方法对我们的测试数据进行预测。
- en: In *Step 3*, we used `classification_report()` to see various metrics such as
    `precision`, `recall`, and `f1-score` for each class, as well as the average of
    each of the metrics. The `classification_report()` reports the averages for the
    total true positives, false negatives, false positives, unweighted mean per label,
    and support-weighted mean per label. It also reports a sample average for multi-label
    classification.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 3*中，我们使用了`classification_report()`来查看每个类别的各种指标，如`precision`、`recall`和`f1-score`，以及每个指标的均值。`classification_report()`报告了总真实正例、假负例、假正例、未加权的每个标签的平均值和支持加权的每个标签的平均值。它还报告了多标签分类的样本平均值。
- en: Precision refers to the classifier's ability not to label an instance that is
    negative as positive, while recall refers to the ability of the classifier to
    find all positive instances. The f[1] score is a weighted harmonic mean of precision
    and recall. The best `f[1] score` is 1.0 and the worst is 0.0\. The support is
    the number of observations of each class.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 精度指的是分类器不将负例标记为正例的能力，而召回率指的是分类器找到所有正例的能力。f[1]分数是精度和召回率的加权调和平均值。最佳的`f[1]分数`是1.0，最差的是0.0。支持是每个类别的观测数。
- en: In *Step 4*, we used `confusion_matrix()` to generate the confusion matrix to
    see the true positives, true negatives, false positives, and false negatives.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 4*中，我们使用`confusion_matrix()`生成混淆矩阵，以查看真实正例、真实负例、假正例和假负例。
- en: In *Step 5*, we looked at the accuracy and the AUC values of our test data using
    the `accuracy_score()` and `roc_auc_score()` functions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们使用`accuracy_score()`和`roc_auc_score()`函数查看了测试数据的准确率和AUC值。
- en: In the next section, we will tune our hyperparameters using a grid search to
    find the optimal model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用网格搜索调整超参数，以找到最佳模型。
- en: There's more...
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'We will now look at how to fine-tune the hyperparameters for gradient boosting
    machines:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨如何微调梯度提升机的超参数：
- en: 'First, we import `GridSearchCV` from `sklearn.model_selection`:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从`sklearn.model_selection`导入`GridSearchCV`：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We set the grid parameters to a variable:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将网格参数设置为一个变量：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We use `GridSeacrhCV`, which lets us combine an estimator with a grid search
    to tune the hyperparameters. The `GridSeacrhCV` method selects the optimal parameter
    from the grid values and uses it with the estimator:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`GridSeacrhCV`，它允许我们将估计量与网格搜索相结合以调整超参数。`GridSeacrhCV`方法从网格值中选择最佳参数，并将其与估计量一起使用：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then, we can view the optimal parameters:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以查看最佳参数：
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Take a look at the following screenshot:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的截图：
- en: '![](img/e01f81c8-036d-4083-b101-61505564fa5e.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e01f81c8-036d-4083-b101-61505564fa5e.png)'
- en: 'We pass our test data to the `predict` method to get the predictions:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将测试数据传递给`predict`方法以获取预测结果：
- en: '[PRE29]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Again, we can see the metrics that are provided by `classification_report`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们可以看到`classification_report`提供的指标：
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This gives us the following output. We notice that the average `precision`
    and `f1-score` improved from the previous case:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出。我们注意到平均`precision`和`f1-score`比之前的案例有所提高：
- en: '![](img/ee2e913d-849e-4077-991b-93d071a4080f.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee2e913d-849e-4077-991b-93d071a4080f.png)'
- en: 'Now, we will take a look at the confusion matrix and plot it, like we did earlier:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将查看混淆矩阵并绘制它，就像我们之前做的那样：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We get the following plot from the preceding code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们得到以下图表：
- en: '![](img/77d312c3-89a3-4a46-b046-920e8dcfd943.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/77d312c3-89a3-4a46-b046-920e8dcfd943.png)'
- en: 'Now, we will look at the accuracy and AUC values again:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们再次查看准确率和AUC值：
- en: '[PRE32]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We notice that the accuracy remains the same but that the AUC improves from
    0.96 to 0.97:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到准确率保持不变，但AUC从0.96提高到0.97：
- en: '![](img/8df0c134-5624-4f39-afce-466c64cbff69.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8df0c134-5624-4f39-afce-466c64cbff69.png)'
- en: Implementing the extreme gradient boosting method for glass identification using
    XGBoost with scikit-learn
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用scikit-learn和XGBoost实现玻璃识别的极端梯度提升方法
- en: XGBoost stands for extreme gradient boosting. It is a variant of the gradient
    boosting machine that aims to improve performance and speed. The XGBoost library
    in Python implements the gradient boosting decision tree algorithm. The name gradient
    boosting comes from its us of the gradient descent algorithm to minimize loss
    when adding new models. XGBoost can handle both regression and classification
    tasks.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost代表极端梯度提升。它是梯度提升机的变体，旨在提高性能和速度。Python中的XGBoost库实现了梯度提升决策树算法。梯度提升的名字来源于它在添加新模型时使用梯度下降算法来最小化损失。XGBoost可以处理回归和分类任务。
- en: XGBoost is the algorithm of choice among those participating in Kaggle competitions
    because of its performance and speed of execution in difficult machine learning
    problems.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在困难的机器学习问题中性能和执行速度出色，XGBoost成为了Kaggle竞赛中参赛者的首选算法。
- en: 'Some of the important parameters that are used in XGBoost are as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在XGBoost中使用的一些重要参数如下：
- en: '`n_estimators`/`ntrees`:This specifies the number of trees to build. The default
    value is 50.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`/`ntrees`：这指定了要构建的树的数量。默认值是50。'
- en: '`max_depth`:This specifies the maximum tree depth. The default value is 6\.
    Higher values will make the model more complex and may lead to overfitting. Setting
    this value to 0 specifies no limit.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：这指定了最大树深度。默认值是6。更高的值会使模型更加复杂，可能会导致过拟合。将此值设置为0表示没有限制。'
- en: '`min_rows`: Thisspecifies the minimum number of observations for a leaf. The
    default value is 1.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_rows`：这指定了叶子的最小观测数。默认值是1。'
- en: '`learn_rate`: This specifies the learning rate by which to shrink the feature
    weights. Shrinking feature weights after each boosting step makes the boosting
    process more conservative and prevents overfitting. The range is 0.0 to 1.0\.
    The default value is 0.3.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learn_rate`：这指定了用于缩小特征权重的学习率。在每次提升步骤之后缩小特征权重可以使提升过程更加保守，并防止过拟合。范围是0.0到1.0。默认值是0.3。'
- en: '`sample_rate`:This specifies the row sampling ratio of the training instance
    (the *x* *axis*). For example, setting this value to 0.5 tells XGBoost to randomly
    collect half of the data instances to grow trees. The default value is 1 and the
    range is 0.0 to 1.0\. Higher values may improve training accuracy.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_rate`：这指定了训练实例（*x*轴）的行采样比率。例如，将此值设置为0.5表示XGBoost随机收集一半的数据实例来生长树。默认值是1，范围是0.0到1.0。更高的值可能会提高训练准确率。'
- en: '`col_sample_rate`: This specifies the column sampling rate (the *y axis*) for
    each split in each level. The default value is 1.0 and the range is from 0 to
    1.0\. Higher values may improve training accuracy.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`col_sample_rate`：这指定了每个级别的每个分割的列采样率（*y轴*）。默认值是1.0，范围是从0到1.0。更高的值可能会提高训练准确率。'
- en: Getting ready...
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备中...
- en: 'You will need the `XGBoost` library installed to continue with this recipe.
    You can use the `pip` command to install the `XGBoost` library as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装`XGBoost`库才能继续此食谱。您可以使用以下`pip`命令安装`XGBoost`库：
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Import the required libraries:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE34]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Set your working folder and read your data:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 设置您的当前工作文件夹并读取您的数据：
- en: '[PRE35]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This data has been taken from the UCI ML repository. The column names have
    been changed according to the data description that''s provided at the following
    link: [https://bit.ly/2EZX6IC](https://bit.ly/2EZX6IC).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据来自UCI ML存储库。列名已根据以下链接提供的数据描述进行更改：[https://bit.ly/2EZX6IC](https://bit.ly/2EZX6IC)。
- en: 'We take a look at the data:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来看看数据：
- en: '[PRE36]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We split our data into a target and feature set, and verify it. Note that we
    ignore the ID column:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据分为目标和特征集，并验证它。请注意，我们忽略了ID列：
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We confirm that there are no missing values:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确认没有缺失值：
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We split our dataset into train and test subsets:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集分为训练集和测试集：
- en: '[PRE39]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: How to do it...
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Now, we will proceed to build our first XGBoost model:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将继续构建我们的第一个XGBoost模型：
- en: 'First, we fit our train data into the XGBoost classifier:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将训练数据拟合到XGBoost分类器：
- en: '[PRE40]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can visualize a single XGBoost decision tree from our trained model. Visualizing
    decision trees can provide insight into the gradient boosting process:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以可视化训练模型中的单个XGBoost决策树。可视化决策树可以提供梯度提升过程的洞察：
- en: '[PRE41]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This gives us the following output:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '![](img/76e980da-6fd8-4498-9b40-d45f6a6996ac.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](img/76e980da-6fd8-4498-9b40-d45f6a6996ac.png)'
- en: With `num_trees=0`, we get the first boosted tree. We can view the other boosted
    trees by setting the index value to the `num_trees` parameter.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 当`num_trees=0`时，我们得到第一个提升树。我们可以通过将索引值设置为`num_trees`参数来查看其他提升树。
- en: 'We set `num_trees=5` in the following example:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将`num_trees`设置为`5`：
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The following screenshot shows us the 6th boosted tree:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了第6个提升树：
- en: '![](img/21e57f28-d9e9-47d4-8583-92582c2b6d1c.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21e57f28-d9e9-47d4-8583-92582c2b6d1c.png)'
- en: You will need the `graphviz` library installed on your system to plot the boosted
    trees.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在您的系统上安装`graphviz`库来绘制提升树。
- en: 'We will now use `predict()` on our test data to get the predicted values. We
    can see our test accuracy with `accuracy_score()`:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在将在测试数据上使用`predict()`来获取预测值。我们可以使用`accuracy_score()`看到我们的测试准确率：
- en: '[PRE43]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: By executing this code, we can see the test accuracy is 69.23%.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行此代码，我们可以看到测试准确率为69.23%。
- en: 'We can see our confusion matrix by using the following code:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码查看我们的混淆矩阵：
- en: '[PRE44]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can then use a predefined function, `plot_confusion_matrix()`, which we
    have sourced from [https://scikit-learn.org](https://scikit-learn.org):'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用预定义的函数`plot_confusion_matrix()`，该函数来自[https://scikit-learn.org](https://scikit-learn.org)：
- en: '[PRE45]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We then look at the `unique` values of our target variable to set the names
    of each level of our target variable:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们查看目标变量的`unique`值以设置目标变量每个级别的名称：
- en: '[PRE46]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'In the following code block, we can see the `target_names` values as `1`, `2`,
    `3`, `5`, `6`, and `7`. We set the names to each level of our target variable
    accordingly:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们可以看到`target_names`的值为`1`、`2`、`3`、`5`、`6`和`7`。我们相应地为目标变量的每个级别设置名称：
- en: '[PRE47]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can now visualize the confusion matrix, as shown in the following screenshot:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以可视化混淆矩阵，如下面的截图所示：
- en: '![](img/94cf908f-30aa-49c6-ad21-e09a5d42b239.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/94cf908f-30aa-49c6-ad21-e09a5d42b239.png)'
- en: How it works...
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we fit the `XGBoostClassfier` to our train data. In *Step 2* and
    *Step 3*, we visualized the individual boosted trees. To do this, we used the `plot_tree()` function.
    We passed our `XGBoost` model to the `plot_tree()` and set the index of the tree
    by setting the `num_trees` parameter. The `rankdir='LR'` parameter plotted the
    tree from left to right. Setting `rankdir` to UT would plot a vertical tree.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们将`XGBoostClassfier`拟合到我们的训练数据。在*步骤2*和*步骤3*中，我们可视化了单个提升树。为此，我们使用了`plot_tree()`函数。我们将我们的`XGBoost`模型传递给`plot_tree()`，并通过设置`num_trees`参数来设置树的索引。`rankdir='LR'`参数从左到右绘制树。将`rankdir`设置为UT将绘制垂直树。
- en: In *Step 4*, we passed our test subset to `predict()` to get the test accuracy.
    *Step 5* gave us the confusion matrix. In *Step 6*, we sourced a predefined function, `plot_confusion_matrix()`,
    from [scikit-learn.org](https://scikit-learn.org/stable/). We used this function
    to plot our confusion matrix. In *Step 7*, we looked at the unique values of our
    target variable so that we could set the names for each class of our confusion
    matrix plot. We then plotted our confusion matrix to evaluate our model.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 4* 中，我们将测试子集传递给 `predict()` 以获取测试准确率。*步骤 5* 给出了混淆矩阵。在 *步骤 6* 中，我们从 [scikit-learn.org](https://scikit-learn.org/stable/)
    中获取了一个预定义的函数，`plot_confusion_matrix()`。我们使用此函数来绘制我们的混淆矩阵。在 *步骤 7* 中，我们查看目标变量的唯一值，以便我们可以为混淆矩阵图中的每个类别设置名称。然后我们绘制了混淆矩阵以评估我们的模型。
- en: There's more...
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多...
- en: In this section, we will look at how we can check feature importance and perform
    feature selection based on that. We will also look at how we can evaluate the
    performance of our XGBoost model using cross-validation.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何检查特征重要性并根据该重要性进行特征选择。我们还将探讨如何使用交叉验证评估我们的 XGBoost 模型的性能。
- en: 'We can check feature importance with `model.feature_importances_`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `model.feature_importances_` 检查特征重要性：
- en: '[PRE48]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can also visualize feature importance using `plot_importance()`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `plot_importance()` 可视化特征重要性：
- en: Note that we have imported `plot_importance` from the `xgboost` library.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经从 `xgboost` 库中导入了 `plot_importance`。
- en: '[PRE49]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'After executing the preceding code, we get to see the following chart, which
    shows feature importance in descending order of importance:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码后，我们可以看到以下图表，它显示了按重要性降序排列的特征重要性：
- en: '![](img/c9e25012-3c0d-4e66-b74a-1b944c24b59c.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9e25012-3c0d-4e66-b74a-1b944c24b59c.png)'
- en: Feature importance can be used for feature selection using `SelectFromModel`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `SelectFromModel` 进行特征选择：
- en: The `SelectFromModel` class is imported from `sklearn.feature_selection`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`SelectFromModel` 类是从 `sklearn.feature_selection` 导入的。'
- en: In the following example, the `SelectFromModel` takes the pretrained `XGBoost`
    model and provides a subset from our dataset with the selected features. It decides
    on the selected features based on a threshold value.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，`SelectFromModel` 使用预训练的 `XGBoost` 模型，并从我们的数据集中提供具有选定特征的子集。它根据阈值值决定选定的特征。
- en: 'Features that have an importance that is greater than or equal to the threshold
    value are kept, while any others are discarded:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 重要性大于或等于阈值值的特征将被保留，而其他任何特征都将被丢弃：
- en: '[PRE50]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'From the preceding code, we get to see the following output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以看到以下输出：
- en: '![](img/2ad16bc9-18cf-45ec-88ca-7fb8be176527.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ad16bc9-18cf-45ec-88ca-7fb8be176527.png)'
- en: We notice that the performance of the model fluctuates with the number of selected
    features. Based on the preceding output, we decide to opt for five features that
    give us an accuracy value of 72%. Also, if we use the Occam's razor principle,
    we can probably opt for a simpler model with four features that gives us a slightly
    lower accuracy of 71%.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到模型的性能随着选定特征数量的增加而波动。根据前面的输出，我们决定选择五个特征，这些特征给我们带来了 72% 的准确率。此外，如果我们使用奥卡姆剃刀原则，我们可能可以选择一个具有四个特征且略微降低的
    71% 准确率的更简单模型。
- en: We can also evaluate our models using cross-validation. To perform k-fold cross-validation,
    we must import the `KFold` class from `sklearn.model_selection`.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用交叉验证来评估我们的模型。要执行 k 折交叉验证，我们必须从 `sklearn.model_selection` 中导入 `KFold`
    类。
- en: 'First, we create the `KFold` object and mention the number of splits that we
    would like to have:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建 `KFold` 对象并说明我们希望拥有的拆分数量：
- en: '[PRE51]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: With `cross_val_score()`, we evaluate our model, which gives us the mean and
    standard deviation classification accuracy. We notice that we get a mean accuracy
    of 77.92% and a standard deviation of 22.33%.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `cross_val_score()`，我们评估我们的模型，它给出了平均和标准差分类准确率。我们注意到我们得到了 77.92% 的平均准确率和 22.33%
    的标准差。
- en: In our case, we have a target variable with six classes.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们有一个具有六个类别的目标变量。
- en: 'If you have many classes for a multi-class classification task, you may use stratified
    folds when performing cross-validation:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对于多类分类任务有多个类别，在执行交叉验证时，您可以使用分层折：
- en: '[PRE52]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: With `StratifiedKFold()`, we get an improved mean accuracy of 81.18% and a reduced
    standard deviation of 21.37%.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `StratifiedKFold()`，我们得到了改进的平均准确率 81.18% 和降低的标准差 21.37%。
- en: Note that `n_splits` cannot be greater than the number of members in each class.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`n_splits` 不能大于每个类别中的成员数量。
- en: See also
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: LightGBM is an open source software for the gradient boosting framework that
    was developed by Microsoft. It uses the tree-based algorithm differently to other
    **Gradient Boosting Machines** (**GBMs**): [https://bit.ly/2QW53jH](https://bit.ly/2QW53jH)
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LightGBM 是由微软开发的开源软件，用于梯度提升框架。它使用基于树的算法，与其他 **梯度提升机** (**GBMs**) 不同：[https://bit.ly/2QW53jH](https://bit.ly/2QW53jH)
