- en: Boosting Model Performance with Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing AdaBoost for disease risk prediction using scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing gradient boosting for disease risk prediction using scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing extreme gradient boosting for glass identification using XGBoost
    with scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A boosting algorithm is an ensemble technique that helps to improve model performance
    and accuracy by taking a group of weak learners and combining them to form a strong
    learner. The idea behind boosting is that predictors should learn from mistakes
    that have been made by previous predictors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boosting algorithms have two key characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: First, they undergo multiple iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, each iteration focuses on the instances that were wrongly classified
    by previous iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an input is misclassified by a hypothesis, its weight is altered in the
    next iteration so that the next hypothesis can classify it correctly. More weight
    will be given to those that provide better performance on the training data. This
    process, through multiple iterations, converts weak learners into a collection
    of strong learners, thereby improving the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: In bagging, no bootstrap sample depends on any other bootstrap, so they all
    run in parallel. Boosting works in a sequential manner and does not involve bootstrap
    sampling. Both bagging and boosting reduce the variance of a single estimate by
    combining several estimates from different models into a single estimate. However,
    it is important to note that boosting does not help significantly if the single
    model is overfitting. Bagging would be a better option if the model overfits.
    On the other hand, boosting tries to reduce bias, while bagging rarely improves
    bias.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce different boosting algorithms such as **Adaptive
    Boosting** (**AdaBoost**), gradient boosting, and **e****xtreme gradient boosting**
    (**XGBoost**).
  prefs: []
  type: TYPE_NORMAL
- en: Implementing AdaBoost for disease risk prediction using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AdaBoost is one of the earliest boosting algorithms that was used for binary
    classification. It was proposed by Freund and Schapire in 1996\. Many other boosting-based
    algorithms have since been developed on top of AdaBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Another variation of adaptive boosting is known as **AdaBoost-abstain**. AdaBoost-abstain allows each
    baseline classifier to abstain from voting if its dependent feature is missing.
  prefs: []
  type: TYPE_NORMAL
- en: 'AdaBoost focuses on combining a set of weak learners into a strong learner.
    The process of an AdaBoost classifier is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, a short decision tree classifier is fitted onto the data. The decision
    tree can just have a single split, which is known as a **decision stump**. The
    overall errors are evaluated. This is the first iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second iteration, whatever data is correctly classified will be given
    lowerweights, while higher weights will be given to the misclassified classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the third iteration, another decision stump will be fitted to the data and
    the weights will be changed again in the next iteration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once these iterations are over, the weights are automatically calculated for
    each classifier at each iteration based on the error rates to come up with a strong
    classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot shows how AdaBoost works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/659ca5ea-7b32-4ead-9a6f-6c082501bb48.png)'
  prefs: []
  type: TYPE_IMG
- en: The concept behind this algorithm is to distribute the weights to the training
    example and select the classifier with the lowest weighted error. Finally, it
    constructs a strong classifier as a linear combination of these weak learners.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general equation for an AdaBoost as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ad74a62-127d-4778-897c-2841ac0d91e3.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **F(x)** represents a strong classifier, **![](img/141fe725-e4a9-44d6-93f6-b61bfb296f09.png)** represent
    the weights, and **f(x)** represents a weak classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The AdaBoost classifier takes various parameters. The important ones are explained
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`base_estimator`: The learning algorithm that is used to train the models.
    If a value is not provided for this parameter, the base estimator is `DecisionTreeClassifier
    (max_depth=1)`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_estimators`: The number of models to iteratively train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`:The contribution of each model to the weights. By default,
    `learning_rate` has a value of `1`. A lower value for the learning rate forces
    the model to train slower but might result in better performance scores.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To start with, import the `os` and the `pandas` packages and set your working
    directory according to your requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Download the `breastcancer.csv` dataset from GitHub and copy it to your working
    directory. Read the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the first few rows with the `head()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the `diagnosis` variable has values such as M and B, representing
    Malign and Benign, respectively. We will perform label encoding on the `diagnosis`
    variable so that we can convert the M and B values into numeric values.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use `head()` to see the changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We then check whether the dataset has any null values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the shape of the dataset with `shape()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We now separate our target and feature set. We also split our dataset into
    training and testing subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will move on to building our model using the `AdaBoost` algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that the accuracy and AUC scores may differ because
    of random splits and other randomness factors.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now look at how to use an AdaBoost to train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we build our first `AdaBoost` model, let''s train our model using the `DecisionTreeClassifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see our accuracy and **Area Under the Curve** (**AUC**) with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We get an accuracy score and an AUC value of 91.81% and 0.91, respectively.
    Note that these values might be different for different users due to randomness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will build our AdaBoost model using the scikit-learn library. We will
    use the `AdaBoostClassifier` to build our `AdaBoost` model. `AdaBoost` uses `dtree`
    as the base classifier by default:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We check the accuracy and AUC value of the model on our test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We notice that we get an accuracy score of 92.82% and an AUC value of 0.97\.
    Both of these metrics are higher than the decision tree model we built in *Step
    1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we must fine-tune our hyperparameters. We set `n_estimators` to `100`
    and `learning_rate` to `0.4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will check the accuracy and AUC values of our new model on our test
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We notice the accuracy drops to 92.39%, but that we get an improved AUC value
    of 0.98.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we used the `DecisionTreeClassifier` to build our model. In *Step
    2*, we noticed that our mean accuracy and the AUC score were 91.81% and 0.91,
    respectively. We aimed to improve this using the `AdaBoost` algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `AdaBoost` algorithm uses a decision tree as the base classifier
    by default. In *Step 3*, we trained our model using `AdaBoost` with the default
    base learner. We set `n_estimators` to `100` and the `learning_rate` to `0.1`.
    We checked our mean accuracy and AUC value in *Step 4*. We noticed that we got
    a decent improvement in the mean accuracy and the AUC as they jumped to 93.57%
    and 0.977, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we fine-tuned some of the hyperparameters for our `AdaBoost` algorithm,
    which used a decision tree as the base classifier. We set the `n_estimators` to
    `100` and the `learning_rate` to `0.4`. *Step 6* gave us the accuracy and AUC
    values for the model we built in *Step 5*. We saw that the accuracy dropped to
    93.56% and that the AUC stayed similar at 0.981.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will showcase training a model using AdaBoost with a **support vector
    machine** (**SVM**) as the base learner.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, AdaBoost uses a decision tree as the base learner. We can use different
    base learners as well. In the following example, we have used an SVM as our base
    learner with the `AdaBoost` algorithm. We use `SVC` with `rbf` as the kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the accuracy and the AUC values of our AdaBoost model with **support
    vector classifier** (**SVC**) as the base learner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We noticed that the accuracy and AUC values fall to 62.57 and 0.92, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will rebuild our AdaBoost model with SVC. This time, we will use a
    linear kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We now get a mean accuracy of 90.64% and a decent AUC value of 0.96.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now plot a graph to compare the AUC value of each model using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ccb24245-d84c-46b3-a7e0-46377e766863.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also plot the accuracy of all the models with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c274f0f-7286-4468-b1a2-07b246aa9cd5.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can also use grid search with AdaBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we performed a grid search with the `n_estimators` set
    to `10`, `30`, `40`, and `100`, and `learning_rate` set to `0.1`, `0.2`, and `0.3`.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a gradient boosting machine for disease risk prediction using scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosting is a machine learning technique that works on the principle
    of boosting, where weak learners iteratively shift their focus toward error observations
    that were difficult to predict in previous iterations and create an ensemble of
    weak learners, typically decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradient boosting trains models in a sequential manner, and involves the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Fitting a model to the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fitting a model to the residuals
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a new model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While the AdaBoost model identifies errors by using weights that have been assigned
    to the data points, gradient boosting does the same by calculating the gradients
    in the loss function. The loss function is a measure of how a model is able to
    fit the data on which it is trained and generally depends on the type of problem
    being solved. If we are talking about regression problems, mean squared error
    may be used, while in classification problems, the logarithmic loss can be used.
    The gradient descent procedure is used to minimize loss when adding trees one
    at a time. Existing trees in the model remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a handful of hyperparameters that may be tuned for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`N_estimators`:This represents the number of trees in the model. Usually, the
    higher it is, the better the model learns the data.'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`: This signifies how deep our tree is. It is used to control overfitting.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_split`: This is the minimum number of samples required to split
    an internal node. Values that are too high can prevent the model from learning
    relations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate`: This controls the magnitude of change in the estimates. Lower
    values with a higher number of trees are generally preferred.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss`: This refers to the loss function that is minimized in each split. `deviance`
    is used in the algorithm as the default parameter, while the other is `exponential`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`: This represents the number of features we have to consider
    when looking for the best split.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`criterion`:This function measures the quality of the split and supports `friedman_mse`
    and `mae` to evaluate the performance of the model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subsample`:This represents the fraction of samples to be used for fitting
    the individual base learners. Choosing a subsample that is less than 1.0 leads
    to a reduction of variance and an increase in bias.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_impurity_split`: This is represented as a threshold to stop tree growth
    early.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will take the same dataset that we used for training our AdaBoost model.
    In this example, we will see how we can train our model using gradient boosting
    machines. We will also look at a handful of hyperparameters that can be tuned
    to improve the model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import all the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we read our data and label encode our target variables to 1 and 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, separate our target and feature variables. We split our data into train
    and test subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This is the same code that we used in the *Getting ready* section of the `AdaBoost`
    example.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now look at how to use a Gradient Boosting Machines to train our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We imported `GradientBoostingClassifier` from `sklearn.ensemble` in the last
    section,* Getting ready*. We trained our model using `GradieBoostingClassfier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we must pass our test data to the `predict()` function to make the predictions
    using the model we built in *Step 1*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we use  `classification_report` to see the following metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`classification_report` gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad04d7fa-7696-4168-bd2f-8bee27c56d4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will use `confusion_matrix()` to generate the confusion matrix. We then
    pass the output of the `confusion_matrix` to our predefined function, that is, `plot_confusion_matrix()`,
    to plot the matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/905c7c1c-03c9-453b-b2d6-2931f578c79f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can check the test accuracy and the AUC value with `accuracy_score()` and `roc_auc_score()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that `accuracy_score` and `roc_auc_score` have been imported from `sklearn.metrics`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2061b05-4ac1-4159-a970-4e4c9f32c13c.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we trained a gradient boosting classifier model. In *Step 2*, we
    used the `predict()` method to make predictions on our test data.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 3*, we used `classification_report()` to see various metrics such as
    `precision`, `recall`, and `f1-score` for each class, as well as the average of
    each of the metrics. The `classification_report()` reports the averages for the
    total true positives, false negatives, false positives, unweighted mean per label,
    and support-weighted mean per label. It also reports a sample average for multi-label
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Precision refers to the classifier's ability not to label an instance that is
    negative as positive, while recall refers to the ability of the classifier to
    find all positive instances. The f[1] score is a weighted harmonic mean of precision
    and recall. The best `f[1] score` is 1.0 and the worst is 0.0\. The support is
    the number of observations of each class.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we used `confusion_matrix()` to generate the confusion matrix to
    see the true positives, true negatives, false positives, and false negatives.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 5*, we looked at the accuracy and the AUC values of our test data using
    the `accuracy_score()` and `roc_auc_score()` functions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will tune our hyperparameters using a grid search to
    find the optimal model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now look at how to fine-tune the hyperparameters for gradient boosting
    machines:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import `GridSearchCV` from `sklearn.model_selection`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the grid parameters to a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `GridSeacrhCV`, which lets us combine an estimator with a grid search
    to tune the hyperparameters. The `GridSeacrhCV` method selects the optimal parameter
    from the grid values and uses it with the estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can view the optimal parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e01f81c8-036d-4083-b101-61505564fa5e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We pass our test data to the `predict` method to get the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, we can see the metrics that are provided by `classification_report`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output. We notice that the average `precision`
    and `f1-score` improved from the previous case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee2e913d-849e-4077-991b-93d071a4080f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will take a look at the confusion matrix and plot it, like we did earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following plot from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77d312c3-89a3-4a46-b046-920e8dcfd943.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will look at the accuracy and AUC values again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We notice that the accuracy remains the same but that the AUC improves from
    0.96 to 0.97:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8df0c134-5624-4f39-afce-466c64cbff69.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementing the extreme gradient boosting method for glass identification using
    XGBoost with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: XGBoost stands for extreme gradient boosting. It is a variant of the gradient
    boosting machine that aims to improve performance and speed. The XGBoost library
    in Python implements the gradient boosting decision tree algorithm. The name gradient
    boosting comes from its us of the gradient descent algorithm to minimize loss
    when adding new models. XGBoost can handle both regression and classification
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost is the algorithm of choice among those participating in Kaggle competitions
    because of its performance and speed of execution in difficult machine learning
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the important parameters that are used in XGBoost are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`/`ntrees`:This specifies the number of trees to build. The default
    value is 50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_depth`:This specifies the maximum tree depth. The default value is 6\.
    Higher values will make the model more complex and may lead to overfitting. Setting
    this value to 0 specifies no limit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_rows`: Thisspecifies the minimum number of observations for a leaf. The
    default value is 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learn_rate`: This specifies the learning rate by which to shrink the feature
    weights. Shrinking feature weights after each boosting step makes the boosting
    process more conservative and prevents overfitting. The range is 0.0 to 1.0\.
    The default value is 0.3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_rate`:This specifies the row sampling ratio of the training instance
    (the *x* *axis*). For example, setting this value to 0.5 tells XGBoost to randomly
    collect half of the data instances to grow trees. The default value is 1 and the
    range is 0.0 to 1.0\. Higher values may improve training accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col_sample_rate`: This specifies the column sampling rate (the *y axis*) for
    each split in each level. The default value is 1.0 and the range is from 0 to
    1.0\. Higher values may improve training accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need the `XGBoost` library installed to continue with this recipe.
    You can use the `pip` command to install the `XGBoost` library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Set your working folder and read your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This data has been taken from the UCI ML repository. The column names have
    been changed according to the data description that''s provided at the following
    link: [https://bit.ly/2EZX6IC](https://bit.ly/2EZX6IC).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We take a look at the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We split our data into a target and feature set, and verify it. Note that we
    ignore the ID column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We confirm that there are no missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We split our dataset into train and test subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, we will proceed to build our first XGBoost model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we fit our train data into the XGBoost classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize a single XGBoost decision tree from our trained model. Visualizing
    decision trees can provide insight into the gradient boosting process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76e980da-6fd8-4498-9b40-d45f6a6996ac.png)'
  prefs: []
  type: TYPE_IMG
- en: With `num_trees=0`, we get the first boosted tree. We can view the other boosted
    trees by setting the index value to the `num_trees` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We set `num_trees=5` in the following example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows us the 6th boosted tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21e57f28-d9e9-47d4-8583-92582c2b6d1c.png)'
  prefs: []
  type: TYPE_IMG
- en: You will need the `graphviz` library installed on your system to plot the boosted
    trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use `predict()` on our test data to get the predicted values. We
    can see our test accuracy with `accuracy_score()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: By executing this code, we can see the test accuracy is 69.23%.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see our confusion matrix by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use a predefined function, `plot_confusion_matrix()`, which we
    have sourced from [https://scikit-learn.org](https://scikit-learn.org):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We then look at the `unique` values of our target variable to set the names
    of each level of our target variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code block, we can see the `target_names` values as `1`, `2`,
    `3`, `5`, `6`, and `7`. We set the names to each level of our target variable
    accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now visualize the confusion matrix, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94cf908f-30aa-49c6-ad21-e09a5d42b239.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Step 1*, we fit the `XGBoostClassfier` to our train data. In *Step 2* and
    *Step 3*, we visualized the individual boosted trees. To do this, we used the `plot_tree()` function.
    We passed our `XGBoost` model to the `plot_tree()` and set the index of the tree
    by setting the `num_trees` parameter. The `rankdir='LR'` parameter plotted the
    tree from left to right. Setting `rankdir` to UT would plot a vertical tree.
  prefs: []
  type: TYPE_NORMAL
- en: In *Step 4*, we passed our test subset to `predict()` to get the test accuracy.
    *Step 5* gave us the confusion matrix. In *Step 6*, we sourced a predefined function, `plot_confusion_matrix()`,
    from [scikit-learn.org](https://scikit-learn.org/stable/). We used this function
    to plot our confusion matrix. In *Step 7*, we looked at the unique values of our
    target variable so that we could set the names for each class of our confusion
    matrix plot. We then plotted our confusion matrix to evaluate our model.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at how we can check feature importance and perform
    feature selection based on that. We will also look at how we can evaluate the
    performance of our XGBoost model using cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check feature importance with `model.feature_importances_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also visualize feature importance using `plot_importance()`:'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we have imported `plot_importance` from the `xgboost` library.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the preceding code, we get to see the following chart, which
    shows feature importance in descending order of importance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9e25012-3c0d-4e66-b74a-1b944c24b59c.png)'
  prefs: []
  type: TYPE_IMG
- en: Feature importance can be used for feature selection using `SelectFromModel`.
  prefs: []
  type: TYPE_NORMAL
- en: The `SelectFromModel` class is imported from `sklearn.feature_selection`.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, the `SelectFromModel` takes the pretrained `XGBoost`
    model and provides a subset from our dataset with the selected features. It decides
    on the selected features based on a threshold value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Features that have an importance that is greater than or equal to the threshold
    value are kept, while any others are discarded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we get to see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ad16bc9-18cf-45ec-88ca-7fb8be176527.png)'
  prefs: []
  type: TYPE_IMG
- en: We notice that the performance of the model fluctuates with the number of selected
    features. Based on the preceding output, we decide to opt for five features that
    give us an accuracy value of 72%. Also, if we use the Occam's razor principle,
    we can probably opt for a simpler model with four features that gives us a slightly
    lower accuracy of 71%.
  prefs: []
  type: TYPE_NORMAL
- en: We can also evaluate our models using cross-validation. To perform k-fold cross-validation,
    we must import the `KFold` class from `sklearn.model_selection`.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the `KFold` object and mention the number of splits that we
    would like to have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: With `cross_val_score()`, we evaluate our model, which gives us the mean and
    standard deviation classification accuracy. We notice that we get a mean accuracy
    of 77.92% and a standard deviation of 22.33%.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we have a target variable with six classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have many classes for a multi-class classification task, you may use stratified
    folds when performing cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: With `StratifiedKFold()`, we get an improved mean accuracy of 81.18% and a reduced
    standard deviation of 21.37%.
  prefs: []
  type: TYPE_NORMAL
- en: Note that `n_splits` cannot be greater than the number of members in each class.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LightGBM is an open source software for the gradient boosting framework that
    was developed by Microsoft. It uses the tree-based algorithm differently to other
    **Gradient Boosting Machines** (**GBMs**): [https://bit.ly/2QW53jH](https://bit.ly/2QW53jH)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
