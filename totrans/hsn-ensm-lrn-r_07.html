<html><head></head><body>
<div class="book" title="Chapter&#xA0;7.&#xA0;The General Ensemble Technique" id="1GKCM1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>Chapter 7. The General Ensemble Technique</h1></div></div></div><p class="calibre7">The previous four chapters have dealt with the ensembling techniques for decision trees. In each of the topics discussed in those chapters, the base learner was a decision tree and, consequently, we delved into the homogenous ensembling technique. In this chapter, we will demonstrate that the base learner can be any statistical or machine learning technique and their ensemble will lead to improved precision in predictions. An important requirement will be that the base learner should be better than a random guess. Through R programs, we will discuss and clarify the different possible cases in which ensembling will work. Voting is an important trait of the classifiers – we will state two different methods for this and illustrate them in the context of bagging and random forest ensemblers. The averaging technique is an ensembler for regression variables, which will follow the discussion of classification methods. The chapter will conclude with a detailed discussion of stacking methods, informally introduced in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>. The topic flow unfolds as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem">Why does ensembling work?</li><li class="listitem">Ensembling by voting</li><li class="listitem">Ensembling by averaging</li><li class="listitem">Stack ensembles</li></ul></div></div>

<div class="book" title="Chapter&#xA0;7.&#xA0;The General Ensemble Technique" id="1GKCM1-2006c10fab20488594398dc4871637ee">
<div class="book" title="Technical requirements"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec54" class="calibre1"/>Technical requirements</h1></div></div></div><p class="calibre7">The libraries that will be used in this chapter are as follows:</p><div class="book"><ul class="itemizedlist"><li class="listitem"><code class="literal">rpart</code></li><li class="listitem"><code class="literal">randomForest</code></li></ul></div></div></div>
<div class="book" title="Why does ensembling work?"><div class="book" id="1HIT82-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec55" class="calibre1"/>Why does ensembling work?</h1></div></div></div><p class="calibre7">When using the bagging method, we combine the result of many decision trees and produce a single output/prediction by taking a majority count. Under a different sampling mechanism, the results<a id="id282" class="calibre1"/> had been combined to produce a single prediction for the random forests. Under a sequential error reduction method for decision trees, the boosting method also provides improved answers. Although we are dealing with uncertain data, which involves probabilities, we don't intend to have methodologies that give results out of a black box and behave without consistent solutions. A theory should explain the working and we need an assurance that the results will be consistent and there is no black magic about it. Arbitrary and uncertain answers are completely unwanted. In this section, we will look at how and why the ensembling solutions work, as well as scenarios where they will not work.</p><p class="calibre7">Ensembling methods have strong mathematical and statistical underpinnings that explain why they give the solutions that they do. We will consider the classification problem first. We will begin with a simplified setup and assume that we have <span class="strong"><em class="calibre9">T</em></span> classifiers that are independent of each other, and that the accuracy associated with each of them is the same as <span class="strong"><img src="../images/00304.jpeg" alt="Why does ensembling work?" class="calibre15"/></span>. This is one of the simplest cases, and we will generalize the scenario later. Now, if we have <span class="strong"><em class="calibre9">T</em></span> classifiers and each of them votes on observations such as +1 or -1, it begs the question, what will the overall accuracy be? Since the number of correct classifications of the <span class="strong"><em class="calibre9">T</em></span> classifiers must outnumber the misclassifications, we would need at least <span class="strong"><img src="../images/00305.jpeg" alt="Why does ensembling work?" class="calibre15"/></span> classifiers to vote the correct outcome. Here, <span class="strong"><img src="../images/00306.jpeg" alt="Why does ensembling work?" class="calibre15"/></span> denotes the greatest integer that is less than the given fractional number. The majority classification is correct whenever <span class="strong"><img src="../images/00307.jpeg" alt="Why does ensembling work?" class="calibre15"/></span> or a higher number of classifiers vote for the correct class.</p><p class="calibre7">To clarify, it is important to note that when we say a classifier has an accuracy <span class="strong"><em class="calibre9">p</em></span>, we don't mean that the probability of the classifier marking the observation as +1 is <span class="strong"><em class="calibre9">p</em></span>. Rather, what we mean here is that if the classifier makes 100 predictions, the predictions can be any combination of +1 and -1; 100*p predictions are correctly identified by the classifier. The accuracy is independent of the distribution of +1 and -1 in the population.</p><p class="calibre7">Under this setup, the probability of the number of classifiers marking a correct observation follows a binomial distribution with <span class="strong"><em class="calibre9">n = T</em></span> and a probability of <span class="strong"><em class="calibre9">p</em></span>. Thus, the probability of the majority vote getting the correct prediction is as follows:</p><div class="mediaobject"><img src="../images/00308.jpeg" alt="Why does ensembling work?" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Since we have mentioned that the<a id="id283" class="calibre1"/> classifier must be better than a random guess, we will need the classifier accuracy to be in excess of 0.5. We will then increment the accuracy over multiple points and see how the increase in the number of classifiers impacts the probability of a majority vote:</p><div class="informalexample"><pre class="programlisting">&gt; source("Utilities.R")
&gt; windows(height=100,width=100)
&gt; # Ensembling in Classification
&gt; # Illustrating the ensemble accuracy with same accuracy for each classifier
&gt; # Different p's and T's with p &gt; 0.5
&gt; classifiers &lt;- seq(9,45,2) # Number of classifiers 
&gt; accuracy &lt;- seq(0.55,0.85,.05)
&gt; plot(0,type='n',xlim=range(classifiers),ylim=c(0.6,1),
+      xlab="Number of Classifiers",ylab="Probability of Majority Voting")
&gt; for(i in 1:length(accuracy)){
+   Prob_MV &lt;- NULL
+   for(j in 1:length(classifiers)){
+     Prob_MV[j] &lt;- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],
+        prob=accuracy[i],size=classifiers[j]))
+   }
+   points(classifiers,Prob_MV,col=i,"l")
+ }
&gt; title("Classifiers with Accuracy Better Than Random Guess")</pre></div><p class="calibre7">The <code class="literal">seq</code> function sets up an odd-numbered sequence of the number of classifiers in the <code class="literal">classifiers R</code> numeric vector. The accuracy percentage varies from <code class="literal">0.55</code> to <code class="literal">0.85</code> in the <code class="literal">accuracy</code> vector. To kick off the proceedings, we set up an empty <code class="literal">plot</code> of sorts, with appropriate <span class="strong"><em class="calibre9">x</em></span> and <span class="strong"><em class="calibre9">y</em></span> axis labels. Now, for each accuracy value, we will calculate the probability of the majority vote for range <code class="literal">floor(classifiers[j]/2+1):classifiers[j]</code>. The <code class="literal">floor(./2+1)</code> ensures that we select the correct starting point. For example, if the number of classifiers is nine, then the value of <code class="literal">floor(./2+1)</code> is <code class="literal">5</code>. Furthermore, when we have nine classifiers, we need a minimum of five votes in favor of the event of interest. On the other hand, for an even number of classifiers (for example, eight) the value of <code class="literal">floor(./2+1)</code> is <code class="literal">5</code>. The <code class="literal">dbinom</code> function calculates the probability of that specific value for the given size and probability. Over the range of <code class="literal">floor(classifiers[j]/2+1): classifiers[j]</code>, it gives the probability of the majority vote, or the accuracy of the majority vote. The output of the preceding code is presented in <span class="strong"><em class="calibre9">Figure 1</em></span>. We can <a id="id284" class="calibre1"/>see from the result that as the number of classifiers increases (each with the same accuracy and better than the random guess), the accuracy of the majority voting also increases:</p><div class="mediaobject"><img src="../images/00309.jpeg" alt="Why does ensembling work?" class="calibre10"/><div class="caption"><p class="calibre14">Figure 1: Why should ensembling work?</p></div></div><p class="calibre11"> </p><p class="calibre7">It would help us to see <code class="literal">Prob_MV</code> for one choice of the accuracy – for example, 0.65. We will run the loop with index <code class="literal">j</code> separately for <code class="literal">prob=0.65</code> and look at how the accuracy of the majority vote increases as the number of classifiers increases:</p><div class="informalexample"><pre class="programlisting">&gt; Prob_MV &lt;- NULL
&gt; for(j in 1:length(classifiers)){
+   Prob_MV[j] &lt;- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],
+                            prob=0.65,size=classifiers[j]))
+ }
&gt; Prob_MV
 [1] 0.8282807 0.8513163 0.8705318 0.8867689 0.9006211 0.9125264 0.9228185
 [8] 0.9317586 0.9395551 0.9463770 0.9523633 0.9576292 0.9622714 0.9663716
[15] 0.9699991 0.9732133 0.9760651 0.9785984 0.9808513</pre></div><p class="calibre7">Consequently, as the number of classifiers with equal accuracy increases, we can see that the accuracy of the majority vote also increases. Also, what is noteworthy here is that even though each of our classifiers had an accuracy of a mere <code class="literal">0.65</code>, the ensemble has way higher accuracy and almost becomes a perfect classifier. This is the main advantage of ensemble.</p><p class="calibre7">Will ensembling help any sort <a id="id285" class="calibre1"/>of classifier? If we have classifiers whose accuracy is worse than the random guess and hence is less than <code class="literal">0.5</code>, then we will search in the same way that we did with the previous case. For a host of a number of classifiers and accuracies less than <code class="literal">0.5</code>, we will compute the accuracy of the majority vote classifier:</p><div class="informalexample"><pre class="programlisting">&gt; # When p &lt; 0.5, ensemble accuracy goes to zero
&gt; classifiers &lt;- seq(6,50,2)
&gt; accuracy &lt;- seq(0.45,0.05,-0.05)
&gt; plot(0,type='n',xlim=range(classifiers),ylim=c(0,0.3),
+      xlab="Number of Classifiers",ylab="Probability of Majority Voting")
&gt; for(i in 1:length(accuracy)){
+   Prob_MV &lt;- NULL
+   for(j in 1:length(classifiers)){
+     Prob_MV[j] &lt;- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],
+                              prob=accuracy[i],size=classifiers[j]))
+   }
+   points(classifiers,Prob_MV,col=i,"l")
+   }
&gt; title("Classifiers with Accuracy Worse Than Random Guess")</pre></div><p class="calibre7">The result of the preceding R program is shown in <span class="strong"><em class="calibre9">Figure 2.</em></span> Now, the first observation should be that it does not matter whether the accuracy is closer to <code class="literal">0.5</code> or to <code class="literal">0</code>, the probability/accuracy of the majority vote classifier is on a decline and this adversely affects the performance. In every case, we see that the accuracy will eventually approach zero. The changes in the block of R code are the classifier sequence <code class="literal">seq(6,50,2)</code>, and the accuracy levels decrease from <code class="literal">0.45</code> to <code class="literal">0.05</code> in <code class="literal">seq(0.45,0.05,-0.05)</code>. Now, consider the case of accuracy being slightly less than <code class="literal">0.5</code>. For example, let's keep it to <code class="literal">0.4999</code>. Will we be lucky enough to see a performance improvement now?</p><div class="mediaobject"><img src="../images/00310.jpeg" alt="Why does ensembling work?" class="calibre10"/><div class="caption"><p class="calibre14">Figure 2: Ensemble is not alchemy!</p></div></div><p class="calibre11"> </p><div class="informalexample"><pre class="programlisting">&gt; classifiers &lt;- seq(10,200,10)
&gt; Prob_MV &lt;- NULL
&gt; for(j in 1:length(classifiers)){
+   Prob_MV[j] &lt;- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],
+                            prob=0.4999,size=classifiers[j]))
+ }
&gt; Prob_MV
 [1] 0.3767071 0.4115491 0.4273344 0.4368132 0.4433011 0.4480955 0.4518222
 [8] 0.4548247 0.4573097 0.4594096 0.4612139 0.4627854 0.4641698 0.4654011
[15] 0.4665053 0.4675025 0.4684088 0.4692370 0.4699975 0.4706989</pre></div><p class="calibre7">Again, it turns out that we can't match<a id="id286" class="calibre1"/> the accuracy of a single classifier. Consequently, we have the important and crucial condition that the classifier must be better than a random guess. What about the random guess itself? It is not at all difficult to pretend that we have a host of classifiers which are all random guesses. If the performance of the ensemble improves with the random guesses, we don't typically have to build any of the statistical or machine learning techniques. Given a set of random guesses, we can always improve the<a id="id287" class="calibre1"/> accuracy. Let's check this out.</p><p class="calibre7">There are two cases – an odd number of classifiers and an even number of classifiers –  and we provide the program for both scenarios:</p><div class="informalexample"><pre class="programlisting">&gt; accuracy &lt;- 0.5
&gt; classifiers &lt;- seq(5,45,2)
&gt; Prob_MV &lt;- NULL
&gt; for(j in 1:length(classifiers)){
+   Prob_MV[j] &lt;- sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],
+                            prob=accuracy,size=classifiers[j]))
+   }
&gt; Prob_MV
 [1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5
[19] 0.5 0.5 0.5
&gt; classifiers &lt;- seq(10,50,2)
&gt; Prob_MV &lt;- NULL
&gt; for(j in 1:length(classifiers)){
+   Prob_MV[j] &lt;- (sum(dbinom(floor(classifiers[j]/2):classifiers[j],
+                             prob=accuracy,size=classifiers[j]))+
+                    sum(dbinom(floor(classifiers[j]/2+1):classifiers[j],
+                               prob=accuracy,size=classifiers[j])))/2
+   }
&gt; Prob_MV
 [1] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5
[19] 0.5 0.5 0.5</pre></div><p class="calibre7">This is interesting! An ensemble of random guesses remains the same irrespective of the number of classifiers. Here, there is neither any improvement nor deterioration. Consequently, for ensembling purposes, we always need classifiers that are better than random guesses.</p><p class="calibre7">It is good to follow your intuition when it comes to understanding how ensembling works. We began with an oversimplified assumption that all models have the same accuracy, but such an assumption does not work well if we deal with models with varying accuracies. As a result, we need to consider cases in which we may have different accuracies for different classifiers. We will first consider a case where each classifier has an accuracy of higher than 0.5, or where each of them is better than a random guess. The approach to finding the accuracy of the majority vote is to evaluate the probabilities for each possible combination of the classifiers' outcomes. We<a id="id288" class="calibre1"/> consider the simpler case when the number of classifiers is an odd number.</p><p class="calibre7">Suppose we have <span class="strong"><em class="calibre9">T</em></span> number of classifiers, and the accuracy of each classifier is <span class="strong"><img src="../images/00311.jpeg" alt="Why does ensembling work?" class="calibre15"/></span>. Note that <span class="strong"><img src="../images/00312.jpeg" alt="Why does ensembling work?" class="calibre15"/></span>, as these correspond to different measures.</p><p class="calibre7">The steps involved in evaluating the probability of a majority vote with unequal accuracies is given in the following steps:</p><div class="book"><ul class="itemizedlist"><li class="listitem">
List all possible elementary events. If each classifier votes TRUE or FALSE for a given case, this means that it has two possible outcomes, and <span class="strong"><em class="calibre9">T</em></span> number of classifiers. List the <span class="strong"><img src="../images/00313.jpeg" alt="Why does ensembling work?" class="calibre15"/></span> possible outcomes:
<div class="book"><ul class="itemizedlist1"><li class="listitem">Example: If we have three classifiers, there would be eight possible cases, as follows:<div class="informalexample"><table border="1" class="calibre16"><colgroup class="calibre17"><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/></colgroup><thead class="calibre19"><tr class="calibre20"><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 1</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 2</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 3</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre20"><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td></tr></tbody></table></div></li></ul></div></li><li class="listitem">Compute the probability of each possible event. Since each classifier has a different accuracy, the probabilities would then be different for each possible outcome:<div class="book"><ul class="itemizedlist1"><li class="listitem">Example: If the accuracies of the three classifiers (for TRUE) are 0.6, 0.7, and 0.8, then the probabilities of FALSE are, respectively, 0.4, 0.3, and 0.2, and the probabilities of the preceding table would be as follows:<div class="informalexample"><table border="1" class="calibre16"><colgroup class="calibre17"><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/></colgroup><thead class="calibre19"><tr class="calibre20"><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 1</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 2</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 3</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.6</p>
</td><td class="calibre25">
<p class="calibre22">0.7</p>
</td><td class="calibre25">
<p class="calibre22">0.8</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.4</p>
</td><td class="calibre25">
<p class="calibre22">0.7</p>
</td><td class="calibre25">
<p class="calibre22">0.8</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.6</p>
</td><td class="calibre25">
<p class="calibre22">0.3</p>
</td><td class="calibre25">
<p class="calibre22">0.8</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.4</p>
</td><td class="calibre25">
<p class="calibre22">0.3</p>
</td><td class="calibre25">
<p class="calibre22">0.8</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.6</p>
</td><td class="calibre25">
<p class="calibre22">0.7</p>
</td><td class="calibre25">
<p class="calibre22">0.2</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.4</p>
</td><td class="calibre25">
<p class="calibre22">0.7</p>
</td><td class="calibre25">
<p class="calibre22">0.2</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.6</p>
</td><td class="calibre25">
<p class="calibre22">0.3</p>
</td><td class="calibre25">
<p class="calibre22">0.2</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.4</p>
</td><td class="calibre25">
<p class="calibre22">0.3</p>
</td><td class="calibre25">
<p class="calibre22">0.2</p>
</td></tr></tbody></table></div></li></ul></div></li><li class="listitem">In the next step, obtain the probability of the elementary event, which will be the product of the numbers in each column:<div class="informalexample"><table border="1" class="calibre16"><colgroup class="calibre17"><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/></colgroup><thead class="calibre19"><tr class="calibre20"><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 1</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 2</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 3</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Probability</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.6</p>
</td><td class="calibre25">
<p class="calibre22">0.7</p>
</td><td class="calibre25">
<p class="calibre22">0.8</p>
</td><td class="calibre25">
<p class="calibre22">0.336</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.4</p>
</td><td class="calibre25">
<p class="calibre22">0.7</p>
</td><td class="calibre25">
<p class="calibre22">0.8</p>
</td><td class="calibre25">
<p class="calibre22">0.224</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.6</p>
</td><td class="calibre25">
<p class="calibre22">0.3</p>
</td><td class="calibre25">
<p class="calibre22">0.8</p>
</td><td class="calibre25">
<p class="calibre22">0.144</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.4</p>
</td><td class="calibre25">
<p class="calibre22">0.3</p>
</td><td class="calibre25">
<p class="calibre22">0.8</p>
</td><td class="calibre25">
<p class="calibre22">0.096</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.6</p>
</td><td class="calibre25">
<p class="calibre22">0.7</p>
</td><td class="calibre25">
<p class="calibre22">0.2</p>
</td><td class="calibre25">
<p class="calibre22">0.084</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.4</p>
</td><td class="calibre25">
<p class="calibre22">0.7</p>
</td><td class="calibre25">
<p class="calibre22">0.2</p>
</td><td class="calibre25">
<p class="calibre22">0.056</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.6</p>
</td><td class="calibre25">
<p class="calibre22">0.3</p>
</td><td class="calibre25">
<p class="calibre22">0.2</p>
</td><td class="calibre25">
<p class="calibre22">0.036</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">0.4</p>
</td><td class="calibre25">
<p class="calibre22">0.3</p>
</td><td class="calibre25">
<p class="calibre22">0.2</p>
</td><td class="calibre25">
<p class="calibre22">0.024</p>
</td></tr></tbody></table></div></li><li class="listitem">Find the events which have a majority <a id="id289" class="calibre1"/>count. In this case, this refers to a sum greater than or equal to 2: <div class="informalexample"><table border="1" class="calibre16"><colgroup class="calibre17"><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/><col class="calibre18"/></colgroup><thead class="calibre19"><tr class="calibre20"><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 1</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 2</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Classifier 3</p>
</th><th valign="bottom" class="calibre21">
<p class="calibre22">Vote Count</p>
</th></tr></thead><tbody class="calibre24"><tr class="calibre20"><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">3</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">2</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">2</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">1</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">2</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">1</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">TRUE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">1</p>
</td></tr><tr class="calibre20"><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">FALSE</p>
</td><td class="calibre25">
<p class="calibre22">0</p>
</td></tr></tbody></table></div></li><li class="listitem">The probability of the majority vote is then simply the sum of the probability in cases where the vote count is greater than or equal to 2. This is the sum of the entries in rows 1, 2, 3, and 5 of the Probability column, as 0.336 + 0.224 + 0.144 + 0.084 = 0.788.</li></ul></div><p class="calibre7">We need to define a function here called <code class="literal">Get_Prob</code>, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; Get_Prob &lt;- function(Logical,Probability){
+   return(t(ifelse(Logical,Probability,1-Probability)))
+ }</pre></div><p class="calibre7">Given a logical vector and a<a id="id290" class="calibre1"/> vector of corresponding probabilities, the <code class="literal">Get_Prob</code> function will return a vector that consists of the probability that the logical condition is <code class="literal">TRUE</code>. If the logical value is <code class="literal">FALSE</code>, the complement (1 – probability) is returned.</p><p class="calibre7">The preceding steps are put in an R program, and are listed as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Different accuracies T's illustration
&gt; # For simplicity, we set the number of classifiers at odd number
&gt; # Each p_i's greater than 0.5
&gt; accuracy &lt;- c(0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9)
&gt; NT &lt;- length(accuracy) # Number of classifiers 
&gt; APC &lt;- expand.grid(rep(list(c(TRUE,FALSE)),NT)) # All possible combinations
&gt; head(APC)
   Var1  Var2  Var3 Var4 Var5 Var6 Var7 Var8 Var9
1  TRUE  TRUE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE
2 FALSE  TRUE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE
3  TRUE FALSE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE
4 FALSE FALSE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE
5  TRUE  TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE
6 FALSE  TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE
&gt; Elements_Prob &lt;- t(apply(APC,1,Get_Prob,Probability=accuracy))
&gt; head(Elements_Prob)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]  0.5 0.55  0.6 0.65  0.7 0.75  0.8 0.85  0.9
[2,]  0.5 0.55  0.6 0.65  0.7 0.75  0.8 0.85  0.9
[3,]  0.5 0.45  0.6 0.65  0.7 0.75  0.8 0.85  0.9
[4,]  0.5 0.45  0.6 0.65  0.7 0.75  0.8 0.85  0.9
[5,]  0.5 0.55  0.4 0.65  0.7 0.75  0.8 0.85  0.9
[6,]  0.5 0.55  0.4 0.65  0.7 0.75  0.8 0.85  0.9
&gt; Events_Prob &lt;- apply(Elements_Prob,1,prod)
&gt; Majority_Events &lt;- (rowSums(APC)&gt;NT/2)
&gt; sum(Events_Prob*Majority_Events)
[1] 0.9112646</pre></div><p class="calibre7">Given a numeric vector with accuracies, named <code class="literal">accuracy</code>, with an odd number of classifiers, we first find the number of classifiers in it with the <code class="literal">length</code> function and store it in <code class="literal">NT</code>. All possible combinations of <code class="literal">APC</code> are then generated using the <code class="literal">expand.grid</code> function, where the <code class="literal">rep</code> function will repeat the vector <code class="literal">(TRUE, FALSE) NT</code> number of times. Each element of the column of the <code class="literal">APC</code> object will then generate a column where the <code class="literal">TRUE</code> and <code class="literal">FALSE</code> condition will be replaced by the corresponding classifier's accuracy as well as the appropriate complement by using the <code class="literal">Get_Prob</code> function. Since we consider an odd number of classifiers, the majority vote is attended in cases when the number of <code class="literal">TRUE</code> in that elementary event is greater than 50 percent of the number of classifiers (that is, greater than <code class="literal">NT/2</code>). The rest of the computations are easier to follow. If the accuracy of the nine classifiers is 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, and 0.9, then the computations show that the accuracy of the ensemble would be 0.9113, higher than the most accurate classifier here, which is 0.9. However, we<a id="id291" class="calibre1"/> must remember that each of the eight classifiers is less accurate than 0.9. Despite this, the ensemble accuracy is higher than the highest classifier we have on hand. To verify that the computations are working fine, we apply this approach to the example given on page 74 of Zhou (2012), and confirm the final majority vote probability at 0.933:</p><div class="informalexample"><pre class="programlisting">&gt; accuracy &lt;- c(0.7,0.7,0.7,0.9,0.9)
&gt; NT &lt;- length(accuracy) # Number of classifiers
&gt; APC &lt;- expand.grid(rep(list(c(TRUE,FALSE)),NT)) # All possible combinations
&gt; Elements_Prob &lt;- t(apply(APC,1,Get_Prob,Probability=accuracy))
&gt; Events_Prob &lt;- apply(Elements_Prob,1,prod)
&gt; Majority_Events &lt;- (rowSums(APC)&gt;NT/2)
&gt; sum(Events_Prob*Majority_Events)
[1] 0.93268</pre></div><p class="calibre7">What happens to the case when each classifier is worse than a random guess? We will simply turn out the accuracies of the nine classifier scenarios and repeat the program to get the following answer:</p><div class="informalexample"><pre class="programlisting">&gt; # Each p_i's lesser than 0.5
&gt; accuracy &lt;- 1-c(0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9)
&gt; NT &lt;- length(accuracy) # Number of classifiers
&gt; APC &lt;- expand.grid(rep(list(c(TRUE,FALSE)),NT)) # All possible combinations
&gt; head(APC)
   Var1  Var2  Var3 Var4 Var5 Var6 Var7 Var8 Var9
1  TRUE  TRUE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE
2 FALSE  TRUE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE
3  TRUE FALSE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE
4 FALSE FALSE  TRUE TRUE TRUE TRUE TRUE TRUE TRUE
5  TRUE  TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE
6 FALSE  TRUE FALSE TRUE TRUE TRUE TRUE TRUE TRUE
&gt; Elements_Prob &lt;- t(apply(APC,1,Get_Prob,Probability=accuracy))
&gt; head(Elements_Prob)
     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]
[1,]  0.5 0.45  0.4 0.35  0.3 0.25  0.2 0.15  0.1
[2,]  0.5 0.45  0.4 0.35  0.3 0.25  0.2 0.15  0.1
[3,]  0.5 0.55  0.4 0.35  0.3 0.25  0.2 0.15  0.1
[4,]  0.5 0.55  0.4 0.35  0.3 0.25  0.2 0.15  0.1
[5,]  0.5 0.45  0.6 0.35  0.3 0.25  0.2 0.15  0.1
[6,]  0.5 0.45  0.6 0.35  0.3 0.25  0.2 0.15  0.1
&gt; Events_Prob &lt;- apply(Elements_Prob,1,prod)
&gt; Majority_Events &lt;- (rowSums(APC)&gt;NT/2)
&gt; sum(Events_Prob*Majority_Events)
[1] 0.08873544</pre></div><p class="calibre7">When each of the classifiers is<a id="id292" class="calibre1"/> worse than a random guess, the majority vote classifier gives horrible results in the case of ensembling. This leaves us with the final case. What if we have a mixture of classifiers of which some are better than the random guess classifier and some are worse than the random guess classifier? We will put the computing code block in a function known as <code class="literal">Random_Accuracy</code>. The accuracies in the classifiers then become randomly generated numbers in the unit interval. The function <code class="literal">Random_Accuracy</code> is then run over ten times to generate the following output:</p><div class="informalexample"><pre class="programlisting">&gt; # Mixture of p_i's, some &gt; 0.5, and some &lt; 0.5
&gt; Random_Accuracy &lt;- function() {
+   accuracy &lt;- runif(9)
+   NT &lt;- length(accuracy) 
+   APC &lt;- expand.grid(rep(list(c(TRUE,FALSE)),NT)) 
+   Elements_Prob &lt;- t(apply(APC,1,Get_Prob,Probability=accuracy))
+   Events_Prob &lt;- apply(Elements_Prob,1,prod)
+   Majority_Events &lt;- (rowSums(APC)&gt;NT/2)
+   return(sum(Events_Prob*Majority_Events))
+ }
&gt; Random_Accuracy()
[1] 0.3423631
&gt; Random_Accuracy()
[1] 0.3927145
&gt; Random_Accuracy()
[1] 0.5341844
&gt; Random_Accuracy()
[1] 0.1624876
&gt; Random_Accuracy()
[1] 0.4065803
&gt; Random_Accuracy()
[1] 0.4687087
&gt; Random_Accuracy()
[1] 0.7819835
&gt; Random_Accuracy()
[1] 0.3124515
&gt; Random_Accuracy()
[1] 0.6842173
&gt; Random_Accuracy()
[1] 0.2531727</pre></div><p class="calibre7">A mixed bag of results. As a result, if we need to get reasonable accuracy and performance from the ensembling method, it is imperative to ensure that each classifier is better than the random guess. A central assumption<a id="id293" class="calibre1"/> in our analysis thus far has been that the classifiers are independent of each other. This assumption is seldom true in practical settings, as the classifiers are built using the same dataset. However, this topic will be dealt with in the following chapter.</p><p class="calibre7">We will now move on to the problem of ensembling by voting.</p></div>

<div class="book" title="Ensembling by voting" id="1IHDQ1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec56" class="calibre1"/>Ensembling by voting</h1></div></div></div><p class="calibre7">Ensembling by voting can <a id="id294" class="calibre1"/>be used efficiently for classification problems. We now have a set of classifiers, and we need to use them to predict the class of an unknown case. The combining of the predictions of the classifiers can proceed in multiple ways. The two options that we will consider are majority voting, and weighted voting.</p></div>

<div class="book" title="Ensembling by voting" id="1IHDQ1-2006c10fab20488594398dc4871637ee">
<div class="book" title="Majority voting"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec33" class="calibre1"/>Majority voting</h2></div></div></div><p class="calibre7">Ideas related to voting <a id="id295" class="calibre1"/>will be illustrated through an ensemble based on the homogeneous base learners of decision trees, as used in the development of bagging and random forests. First, we will create 500 base learners using the <code class="literal">randomForest</code> function and repeat the program in the first block, as seen in <a class="calibre1" title="Chapter 4. Random Forests" href="part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee">Chapter 4</a>, <span class="strong"><em class="calibre9">Random Forests</em></span>. Ensembling has already been performed in that chapter, and we will elaborate on those steps here. First, the code block for setting up the random forest is given here:</p><div class="informalexample"><pre class="programlisting">&gt; load("../Data/GC2.RData")
&gt; set.seed(12345)
&gt; Train_Test &lt;- sample(c("Train","Test"),nrow(GC2),
+ replace = TRUE,prob = c(0.7,0.3))
&gt; GC2_Train &lt;- GC2[Train_Test=="Train",]
&gt; GC2_TestX &lt;- within(GC2[Train_Test=="Test",],rm(good_bad))
&gt; GC2_TestY &lt;- GC2[Train_Test=="Test","good_bad"]
&gt; GC2_Formula &lt;- as.formula("good_bad~.")
&gt; # RANDOM FOREST ANALYSIS
&gt; GC2_RF &lt;- randomForest(GC2_Formula,data=GC2_Train,keep.inbag=TRUE,
+                        ntree=500)</pre></div><p class="calibre7">Next, we will use the standard <code class="literal">predict</code> function to predict the class for the <code class="literal">GC2_TestX</code> data, and then, using the option of <code class="literal">predict.all=TRUE</code>, obtain the prediction for each tree generated in the random forest:</p><div class="informalexample"><pre class="programlisting">&gt; # New data voting
&gt; GC2_RF_Test_Margin &lt;- predict(GC2_RF,newdata = GC2_TestX,
+                          type="class")
&gt; GC2_RF_Test_Predict &lt;- predict(GC2_RF,newdata=GC2_TestX,
+                           type="class",predict.all=TRUE
+                           )</pre></div><p class="calibre7">The predicted <code class="literal">GC2_RF_Test_Predict</code> object will consist of further <code class="literal">individual</code> objects, which will have the predictions for each decision tree. We will first define a function called <code class="literal">Row_Count_Max</code>, which will return the prediction whose count is a maximum in the forest. The rudimentary voting method is<a id="id296" class="calibre1"/> then compared with the <code class="literal">predict</code> function's outcomes in the following code block:</p><div class="informalexample"><pre class="programlisting">&gt; Row_Count_Max &lt;- function(x) names(which.max(table(x))) 
&gt; # Majority Voting
&gt; Voting_Predict &lt;- apply(GC2_RF_Test_Predict$individual,1,
+ Row_Count_Max)
&gt; head(Voting_Predict);tail(Voting_Predict)
     1      2      3      4      9     10 
"good"  "bad" "good"  "bad" "good"  "bad" 
   974    980    983    984    988    996 
 "bad"  "bad" "good" "good" "good" "good" 
&gt; all(Voting_Predict==GC2_RF_Test_Predict$aggregate)
[1] TRUE
&gt; all(Voting_Predict==GC2_RF_Test_Margin)
[1] TRUE
&gt; sum(Voting_Predict==GC2_TestY)/313
[1] 0.7795527</pre></div><p class="calibre7">Consequently, we can see that the <code class="literal">predict</code> function implements the majority count technique. Next, we will quickly illustrate the ideas and thinking behind weighted voting.</p></div></div>

<div class="book" title="Ensembling by voting" id="1IHDQ1-2006c10fab20488594398dc4871637ee">
<div class="book" title="Weighted voting"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec34" class="calibre1"/>Weighted voting</h2></div></div></div><p class="calibre7">An implicit assumption in the use of simple voting is that all classifiers are equally accurate, or that all classifiers<a id="id297" class="calibre1"/> have equal voting power. Consider the simpler case in which we have five classifiers, three of them with an accuracy of 0.51 and the remaining two with an accuracy of 0.99. If the less accurate classifier votes an observation as a negative case (-1) and the two more accurate classifiers as a positive case (+1), then the simple voting method will call the observation (-1). With this voting pattern, the probability of the observation being -1 is <span class="strong"><img src="../images/00314.jpeg" alt="Weighted voting" class="calibre15"/></span>, while that of it being +1 is <span class="strong"><img src="../images/00315.jpeg" alt="Weighted voting" class="calibre15"/></span>. Thus, we can't pretend that all classifiers should have the same voting power. This is where we will make good use of the weighted voting method.</p><p class="calibre7">In this analysis, we will take the accuracy of the classifiers over the training dataset as the weights. We will treat <span class="strong"><img src="../images/00316.jpeg" alt="Weighted voting" class="calibre15"/></span> as the weight associated with the classifier <span class="strong"><img src="../images/00317.jpeg" alt="Weighted voting" class="calibre15"/></span>. An important<a id="id298" class="calibre1"/> characteristic of the weights is that they should be non-negative and should add up to 1, that is, <span class="strong"><img src="../images/00318.jpeg" alt="Weighted voting" class="calibre15"/></span>. We will normalize the accuracy of the classifiers to satisfy this constraint.</p><p class="calibre7">We will continue the analysis with the German Credit dataset. First, we will obtain the predictions for the 500 trees over the training dataset, and then obtain the accuracies: </p><div class="informalexample"><pre class="programlisting">&gt; # Analyzing Accuracy of Trees of the Fitted Forest
&gt; GC2_RF_Train_Predict &lt;- predict(GC2_RF,newdata=GC2_Train[,-20],
+                                 type="class",predict.all=TRUE)
&gt; head(GC2_RF_Train_Predict$individual[,c(1:5,496:500)])  
   [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]   [,10] 
5  "bad"  "bad"  "bad"  "bad"  "good" "bad"  "bad"  "bad"  "bad"  "bad" 
6  "good" "good" "good" "good" "good" "good" "bad"  "bad"  "bad"  "good"
7  "good" "good" "good" "good" "good" "good" "good" "good" "good" "good"
8  "good" "good" "good" "good" "good" "bad"  "good" "bad"  "good" "good"
11 "bad"  "bad"  "bad"  "bad"  "bad"  "bad"  "bad"  "bad"  "bad"  "bad" 
12 "good" "bad"  "bad"  "bad"  "bad"  "good" "bad"  "bad"  "bad"  "bad" 
&gt; RF_Tree_Train_Accuracy &lt;- NULL
&gt; for(i in 1:GC2_RF$ntree){
+   RF_Tree_Train_Accuracy[i] &lt;- sum(GC2_RF_Train_Predict$individual[,i]==
+                                   GC2_Train$good_bad)/nrow(GC2_Train)
+ }
&gt; headtail(sort(RF_Tree_Train_Accuracy),10)
 [1] 0.8340611 0.8369723 0.8384279 0.8398836 0.8398836 0.8413392 0.8413392
 [8] 0.8413392 0.8413392 0.8427948 0.8908297 0.8908297 0.8908297 0.8908297
[15] 0.8922853 0.8922853 0.8937409 0.8937409 0.8966521 0.8981077</pre></div><p class="calibre7">What is the <code class="literal">headtail</code> function? It is available in the <code class="literal">Utilities.R</code> file. The analysis is repeated with the bagging ensemble as follows:</p><div class="informalexample"><pre class="programlisting">&gt; # Bagging ANALYSIS
&gt; GC2_Bagg &lt;- randomForest(GC2_Formula,data=GC2_Train,keep.inbag=TRUE,
+                          mtry=ncol(GC2_TestX),ntree=500)
&gt; GC2_Bagg_Test_Predict &lt;- predict(GC2_Bagg,newdata=GC2_TestX,
+                                 type="class",predict.all=TRUE)
&gt; GC2_Bagg_Train_Predict &lt;- predict(GC2_Bagg,newdata=GC2_Train[,-20],
+                                 type="class",predict.all=TRUE)
&gt; Bagg_Tree_Train_Accuracy &lt;- NULL
&gt; for(i in 1:GC2_Bagg$ntree){
+   Bagg_Tree_Train_Accuracy[i] &lt;- sum(GC2_Bagg_Train_Predict$individual[,i]==
+                                   GC2_Train$good_bad)/nrow(GC2_Train)
+ }
&gt; headtail(sort(Bagg_Tree_Train_Accuracy),10)
 [1] 0.8369723 0.8384279 0.8413392 0.8457060 0.8457060 0.8471616 0.8471616
 [8] 0.8471616 0.8471616 0.8486172 0.8966521 0.8966521 0.8966521 0.8966521
[15] 0.8966521 0.8981077 0.8995633 0.8995633 0.9024745 0.9097525</pre></div><p class="calibre7">Next, we normalize the weights and<a id="id299" class="calibre1"/> calculate the weighted votes for the observations in the test samples, as shown in the following code: </p><div class="informalexample"><pre class="programlisting">&gt; # Weighted Voting with Random Forest
&gt; RF_Weights &lt;- RF_Tree_Train_Accuracy/sum(RF_Tree_Train_Accuracy)
&gt; Bagg_Weights &lt;- Bagg_Tree_Train_Accuracy/sum(Bagg_Tree_Train_Accuracy)
&gt; RF_Weighted_Vote &lt;- data.frame(matrix(0,nrow(GC2_TestX),ncol=3))
&gt; names(RF_Weighted_Vote) &lt;- c("Good_Weight","Bad_Weight","Prediction")
&gt; for(i in 1:nrow(RF_Weighted_Vote)){
+   RF_Weighted_Vote$Good_Weight[i] &lt;- 
+     sum((GC2_RF_Test_Predict$individual[i,]=="good")*RF_Weights)
+   RF_Weighted_Vote$Bad_Weight[i] &lt;- 
+     sum((GC2_RF_Test_Predict$individual[i,]=="bad")*RF_Weights)
+   RF_Weighted_Vote$Prediction[i] &lt;- c("good","bad")[which.max(RF_Weighted_Vote[i,1:2])]
+ }
&gt; head(RF_Weighted_Vote,10)
   Good_Weight Bad_Weight Prediction
1    0.8301541 0.16984588       good
2    0.3260033 0.67399668        bad
3    0.8397035 0.16029651       good
4    0.4422527 0.55774733        bad
5    0.9420565 0.05794355       good
6    0.2378956 0.76210442        bad
7    0.4759756 0.52402435        bad
8    0.7443038 0.25569624       good
9    0.8120180 0.18798195       good
10   0.7799587 0.22004126       good</pre></div><p class="calibre7">The weighted voting analysis is<a id="id300" class="calibre1"/> repeated for the <code class="literal">bagging</code> objects, as shown here:</p><div class="informalexample"><pre class="programlisting">&gt; # Weighted Voting with Bagging
&gt; Bagg_Weights &lt;- Bagg_Tree_Train_Accuracy/sum(Bagg_Tree_Train_Accuracy)
&gt; Bagg_Weights &lt;- Bagg_Tree_Train_Accuracy/sum(Bagg_Tree_Train_Accuracy)
&gt; Bagg_Weighted_Vote &lt;- data.frame(matrix(0,nrow(GC2_TestX),ncol=3))
&gt; names(Bagg_Weighted_Vote) &lt;- c("Good_Weight","Bad_Weight","Prediction")
&gt; for(i in 1:nrow(Bagg_Weighted_Vote)){
+   Bagg_Weighted_Vote$Good_Weight[i] &lt;- 
+     sum((GC2_Bagg_Test_Predict$individual[i,]=="good")*Bagg_Weights)
+   Bagg_Weighted_Vote$Bad_Weight[i] &lt;- 
+     sum((GC2_Bagg_Test_Predict$individual[i,]=="bad")*Bagg_Weights)
+   Bagg_Weighted_Vote$Prediction[i] &lt;- c("good","bad")[which.max(Bagg_Weighted_Vote[i,1:2])]
+ }
&gt; head(Bagg_Weighted_Vote,10)
   Good_Weight Bad_Weight Prediction
1    0.9279982 0.07200181       good
2    0.1634505 0.83654949        bad
3    0.8219618 0.17803818       good
4    0.4724477 0.52755226        bad
5    0.9619528 0.03804725       good
6    0.1698628 0.83013718        bad
7    0.4540574 0.54594265        bad
8    0.7883772 0.21162281       good
9    0.8301772 0.16982283       good
10   0.7585720 0.24142804       good</pre></div><p class="calibre7">Now, with the voting mechanisms behind us, we turn our attention to regression problems.</p></div></div>

<div class="book" title="Ensembling by averaging"><div class="book" id="1JFUC2-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec57" class="calibre1"/>Ensembling by averaging</h1></div></div></div><p class="calibre7">Within the context of regression models, the predictions are the numeric values of the variables of interest. Combining<a id="id301" class="calibre1"/> the predictions of the output due to the various ensemblers is rather straightforward; because of the ensembling mechanism, we simply interpret the average of the predicted values across the ensemblers as the predicted value. Within the <a id="id302" class="calibre1"/>context of the classification problem, we can carry out simple averaging and weighted averaging. In the previous section, the ensemble had homogeneous base learners. However, in this section, we will deal with heterogeneous base learners.</p><p class="calibre7">We will now consider a regression problem that is dealt with in detail in <a class="calibre1" title="Chapter 8. Ensemble Diagnostics" href="part0057_split_000.html#1MBG21-2006c10fab20488594398dc4871637ee">Chapter 8</a>, <span class="strong"><em class="calibre9">Ensemble Diagnostics</em></span>. The problem is the prediction of housing prices based on over 60 explanatory variables. We have the dataset in training and testing partitions, and load them to kick off the proceedings:</p><div class="informalexample"><pre class="programlisting">&gt; # Averaging for Regression Problems
&gt; load("../Data/ht_imp_author.Rdata") # returns ht_imp object
&gt; load("../Data/htest_imp_author.Rdata") # returns htest_imp
&gt; names(ht_imp)[69] &lt;- "SalePrice"
&gt; dim(ht_imp)
[1] 1460   69
&gt; dim(htest_imp)
[1] 1459   68</pre></div><p class="calibre7">Consequently, we have many observations to build our models. The <code class="literal">SalePrice</code> is the variable of interest here. First, we create a <code class="literal">formula</code> and build a linear model; four regression trees with different depths; four neural networks with a different number of hidden neurons; and a support vector machine model in the following code block:</p><div class="informalexample"><pre class="programlisting">&gt; hf &lt;- as.formula("SalePrice~.")
&gt; SP_lm &lt;- lm(hf,data=ht_imp)
&gt; SP_rpart2 &lt;- rpart(hf,data=ht_imp,maxdepth=2)
&gt; SP_rpart4 &lt;- rpart(hf,data=ht_imp,maxdepth=4)
&gt; SP_rpart6 &lt;- rpart(hf,data=ht_imp,maxdepth=6)
&gt; SP_rpart8 &lt;- rpart(hf,data=ht_imp,maxdepth=8)
&gt; SP_nn2 &lt;- nnet(hf,data=ht_imp,size=2,linout=TRUE)
# weights:  267
initial  value 56996872361441.906250 
final  value 9207911334609.976562 
converged
&gt; SP_nn3 &lt;- nnet(hf,data=ht_imp,size=3,linout=TRUE)
# weights:  400
initial  value 56997125121706.257812 
final  value 9207911334609.960938 
converged
&gt; SP_nn4 &lt;- nnet(hf,data=ht_imp,size=4,linout=TRUE)
# weights:  533
initial  value 56996951452602.304687 
iter  10 value 19328028546738.226562
iter  20 value 19324281941793.617187
final  value 9080312934601.205078 
converged
&gt; SP_nn5 &lt;- nnet(hf,data=ht_imp,size=5,linout=TRUE)
# weights:  666
initial  value 56997435951836.507812 
final  value 9196060713131.609375 
converged
&gt; SP_svm &lt;- svm(hf,data=ht_imp)</pre></div><p class="calibre7">We have the required<a id="id303" class="calibre1"/> setup here to consider the heterogeneous ensemble.</p></div>

<div class="book" title="Ensembling by averaging">
<div class="book" title="Simple averaging"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec35" class="calibre1"/>Simple averaging</h2></div></div></div><p class="calibre7">We built ten models <a id="id304" class="calibre1"/>using the training dataset, and we will now make the predictions for these models on the training dataset using the <code class="literal">predict</code> function, as shown in the following:</p><div class="informalexample"><pre class="programlisting">&gt; # Simple Averaging
&gt; SP_lm_pred &lt;- predict(SP_lm,newdata=htest_imp)
Warning message:
In predict.lm(SP_lm, newdata = htest_imp) :
  prediction from a rank-deficient fit may be misleading
&gt; SP_rpart2_pred &lt;- predict(SP_rpart2,newdata=htest_imp)
&gt; SP_rpart4_pred &lt;- predict(SP_rpart4,newdata=htest_imp)
&gt; SP_rpart6_pred &lt;- predict(SP_rpart6,newdata=htest_imp)
&gt; SP_rpart8_pred &lt;- predict(SP_rpart8,newdata=htest_imp)
&gt; SP_nn2_pred &lt;- predict(SP_nn2,newdata=htest_imp)
&gt; SP_nn3_pred &lt;- predict(SP_nn3,newdata=htest_imp)
&gt; SP_nn4_pred &lt;- predict(SP_nn4,newdata=htest_imp)
&gt; SP_nn5_pred &lt;- predict(SP_nn5,newdata=htest_imp)
&gt; SP_svm_pred &lt;- predict(SP_svm,newdata=htest_imp)</pre></div><p class="calibre7">When it comes to classification problems, the predictions are either based on the class labels or the probability of the class of interest. Consequently, we don't come across <span class="strong"><em class="calibre9">bad predictions </em></span>in terms of the magnitude of predictions, though we need to at least check if the predictions give a mixture of +1s or -1s. If the classifiers predict only +1 or -1, such classifiers can then be discarded from further analysis. For the regression problems, we need to see if the models can make reasonable predictions in terms of the magnitude, and we will simply obtain a plot of the magnitude of the predictions, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; windows(height=300,width=400)
&gt; par(mfrow=c(2,5))
&gt; plot.ts(SP_lm_pred,col=1)
&gt; plot.ts(SP_rpart2_pred,col=2)
&gt; plot.ts(SP_rpart4_pred,col=3)
&gt; plot.ts(SP_rpart6_pred,col=4)
&gt; plot.ts(SP_rpart8_pred,col=5)
&gt; plot.ts(SP_nn2_pred,col=6)
&gt; plot.ts(SP_nn3_pred,col=7)
&gt; plot.ts(SP_nn4_pred,col=8)
&gt; plot.ts(SP_nn5_pred,col=9)
&gt; plot.ts(SP_svm_pred,col=10)</pre></div><p class="calibre7">The result of the preceding code block is shown in the following figure:</p><div class="mediaobject"><img src="../images/00319.jpeg" alt="Simple averaging" class="calibre10"/><div class="caption"><p class="calibre14">Figure 3: A simple plot of the predictions for the ten heterogeneous base learners</p></div></div><p class="calibre11"> </p><p class="calibre7">We can see<a id="id305" class="calibre1"/> that the predictions related to the neural network models with two or three hidden neurons produce no variation in the predictions. Consequently, we delete these two models from further analysis. The ensemble prediction is simply the average of the predictions across the remaining eight models:</p><div class="informalexample"><pre class="programlisting">&gt; Avg_Ensemble_Prediction &lt;- rowMeans(cbind(SP_lm_pred,SP_rpart2_pred,
+     SP_rpart4_pred,SP_rpart6_pred,
+                SP_rpart8_pred,SP_nn4_pred,SP_nn5_pred,SP_svm_pred))
&gt; plot.ts(Avg_Ensemble_Prediction)</pre></div><div class="mediaobject"><img src="../images/00320.jpeg" alt="Simple averaging" class="calibre10"/><div class="caption"><p class="calibre14">Figure 4: Ensemble predictions for the housing dataset</p></div></div><p class="calibre11"> </p><p class="calibre7">As with the extension of simple<a id="id306" class="calibre1"/> voting to weighted voting, we will now look at weighted averaging.</p></div></div>

<div class="book" title="Ensembling by averaging">
<div class="book" title="Weight averaging"><div class="book"><div class="book"><div class="book"><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec36" class="calibre1"/>Weight averaging</h2></div></div></div><p class="calibre7">In the case of classifiers, the <a id="id307" class="calibre1"/>weights were chosen from the accuracies of the classifiers for the training dataset. In this instance, we need unifying measures like this. A regressor model is preferred if it has less residual variance, and we will select the variance as a measure of accuracy. Suppose that the estimated residual variance for a weak base model <span class="strong"><em class="calibre9">i</em></span> is <span class="strong"><img src="../images/00321.jpeg" alt="Weight averaging" class="calibre15"/></span>. In the context of ensemble neural networks, Perrone and Cooper (1993) claim that the optimal weight for the <span class="strong"><em class="calibre9">ith </em></span>weak base model can be obtained using the following equation:</p><div class="mediaobject"><img src="../images/00322.jpeg" alt="Weight averaging" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre7">Since the proportional constants do not matter, we will simply substitute <span class="strong"><img src="../images/00323.jpeg" alt="Weight averaging" class="calibre15"/></span> with the mean of residual squares. In this direction, we will first obtain the <span class="strong"><img src="../images/00324.jpeg" alt="Weight averaging" class="calibre15"/></span> up to a constant, by simply calculating <code class="literal">mean(residuals(model)^2)</code> for the eight models considered in the context of simple averaging, as shown:</p><div class="informalexample"><pre class="programlisting">&gt; # Weighted Averaging
&gt; SP_lm_sigma &lt;- mean(residuals(SP_lm)^2)
&gt; SP_rp2_sigma &lt;- mean(residuals(SP_rpart2)^2)
&gt; SP_rp4_sigma &lt;- mean(residuals(SP_rpart4)^2)
&gt; SP_rp6_sigma &lt;- mean(residuals(SP_rpart6)^2)
&gt; SP_rp8_sigma &lt;- mean(residuals(SP_rpart8)^2)
&gt; SP_nn4_sigma &lt;- mean(residuals(SP_nn4)^2)
&gt; SP_nn5_sigma &lt;- mean(residuals(SP_nn5)^2)
&gt; SP_svm_sigma &lt;- mean(residuals(SP_svm)^2)</pre></div><p class="calibre7">Next, we simply<a id="id308" class="calibre1"/> implement the formula of weights <span class="strong"><img src="../images/00325.jpeg" alt="Weight averaging" class="calibre15"/></span>as follows:</p><div class="informalexample"><pre class="programlisting">&gt; sigma_sum &lt;- SP_lm_sigma + SP_rp2_sigma + SP_rp4_sigma +
+   SP_rp6_sigma + SP_rp8_sigma + SP_nn4_sigma +
+   SP_nn5_sigma + SP_svm_sigma
&gt; sigma_sum
[1] 20727111061
&gt; SP_lm_wts &lt;- SP_lm_sigma/sigma_sum
&gt; SP_rp2_wts &lt;- SP_rp2_sigma/sigma_sum
&gt; SP_rp4_wts &lt;- SP_rp4_sigma/sigma_sum
&gt; SP_rp6_wts &lt;- SP_rp6_sigma/sigma_sum
&gt; SP_rp8_wts &lt;- SP_rp8_sigma/sigma_sum
&gt; SP_nn4_wts &lt;- SP_nn4_sigma/sigma_sum
&gt; SP_nn5_wts &lt;- SP_nn5_sigma/sigma_sum
&gt; SP_svm_wts &lt;- SP_svm_sigma/sigma_sum</pre></div><p class="calibre7">The <code class="literal">rowMeans</code> and <code class="literal">cbind</code> functions simply give away the weighted averaging predictions:</p><div class="informalexample"><pre class="programlisting">&gt; Weighted_Ensemble_Prediction &lt;- rowMeans(cbind(SP_lm_wts*SP_lm_pred,
+                                           SP_rp2_wts*SP_rpart2_pred,
+                                           SP_rp4_wts*SP_rpart4_pred,
+                                           SP_rp6_wts*SP_rpart6_pred,
+                                           SP_rp8_wts*SP_rpart8_pred,
+                                           SP_nn4_wts*SP_nn4_pred,
+                                           SP_nn5_wts*SP_nn5_pred,
+                                           SP_svm_wts*SP_svm_pred))
&gt; plot.ts(Weighted_Ensemble_Prediction)</pre></div><p class="calibre7">The output for the preceding code is as follows:</p><div class="mediaobject"><img src="../images/00326.jpeg" alt="Weight averaging" class="calibre10"/><div class="caption"><p class="calibre14">Figure 5: Weighted averaging predictions for the housing price</p></div></div><p class="calibre11"> </p></div></div>
<div class="book" title="Stack ensembling" id="1KEEU1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec58" class="calibre1"/>Stack ensembling</h1></div></div></div><p class="calibre7">An introductory<a id="id309" class="calibre1"/> and motivational example of the stacked regression was provided in <a class="calibre1" title="Chapter 1. Introduction to Ensemble Techniques" href="part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee">Chapter 1</a>, <span class="strong"><em class="calibre9">Introduction to Ensemble Techniques</em></span>. Here, we will continue the discussion of stacked ensembles for a regression problem which has not been previously developed.</p><p class="calibre7">With stacked ensembling, the outputs of several weak models are given as an input variable, along with the covariates used to build the earlier models, to build a stack model. The form of the stack model might be one of these, or it can be a different model. Here, we will simply use the eight regression models (used in previous sections) as weak models. The stacking regression model is selected as the gradient boosting model, and it will be given the original input variables and predictions of the new models, as follows:</p><div class="informalexample"><pre class="programlisting">&gt; SP_lm_train &lt;- predict(SP_lm,newdata=ht_imp)
Warning message:
In predict.lm(SP_lm, newdata = ht_imp) :
  prediction from a rank-deficient fit may be misleading
&gt; SP_rpart2_train &lt;- predict(SP_rpart2,newdata=ht_imp)
&gt; SP_rpart4_train &lt;- predict(SP_rpart4,newdata=ht_imp)
&gt; SP_rpart6_train &lt;- predict(SP_rpart6,newdata=ht_imp)
&gt; SP_rpart8_train &lt;- predict(SP_rpart8,newdata=ht_imp)
&gt; SP_nn4_train &lt;- predict(SP_nn4,newdata=ht_imp)
&gt; SP_nn5_train &lt;- predict(SP_nn5,newdata=ht_imp)
&gt; SP_svm_train &lt;- predict(SP_svm,newdata=ht_imp)
&gt; 
&gt; ht_imp2 &lt;- cbind(ht_imp[,-69],SP_lm_train,SP_rpart2_train,SP_rpart4_train,
+                           SP_rpart6_train,SP_rpart8_train,SP_nn4_train,SP_nn5_train,
+                           SP_svm_train,ht_imp[,69])
&gt; names(ht_imp2)[77] &lt;- "SalePrice"
&gt; SP_gbm &lt;- gbm(hf,data=ht_imp2,distribution = "gaussian",n.trees=200)
&gt; headtail(predict(SP_gbm,n.trees=100),20)
 [1] 180260.6 177793.3 181836.9 177793.3 191927.7 177793.3 191927.7 182237.3
 [9] 177793.3 177793.3 177793.3 191927.7 177793.3 187520.7 177793.3 177793.3
[17] 177793.3 177793.3 177793.3 177793.3 177908.2 177793.3 191927.7 177793.3
[25] 177793.3 177793.3 177793.3 191927.7 177793.3 177793.3 177793.3 191927.7
[33] 177793.3 177793.3 177793.3 177793.3 179501.7 191927.7 177793.3 177793.3</pre></div><p class="calibre7">This concludes our simple discussion of stacked ensemble regressions.</p></div>
<div class="book" title="Summary" id="1LCVG1-2006c10fab20488594398dc4871637ee"><div class="book"><div class="book"><div class="book"><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec59" class="calibre1"/>Summary</h1></div></div></div><p class="calibre7">In this chapter, we looked at why ensemble works in the context of classification problems. A series of detailed programs illustrated the point that each classifier must be better than a random guess. We considered scenarios where all the classifiers have the same accuracy, different accuracy, and finally a scenario with completely arbitrary accuracies. Majority and weighted voting was illustrated within the context of the random forest and bagging methods. For the regression problem, we used a different choice of base learners and allowed them to be heterogeneous. Simple and weighted averaging methods were illustrated in relation to the housing sales price data. A simple illustration of stacked regression ultimately concluded the technical section of this chapter.</p><p class="calibre7">In the following chapter, we will look at ensembling diagnostics.</p></div></body></html>