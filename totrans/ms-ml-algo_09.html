<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Neural Networks for Machine Learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">This chapter is the introduction to the world of deep learning, whose methods make it possible to achieve the state-of-the-art performance in many classification and regression fields often considered extremely difficult to manage (such as image segmentation, automatic translation, voice synthesis, and so on). The goal is to provide the reader with the basic instruments to understand the structure of a fully connected neural network and model it using the Python tool Keras (employing all the modern techniques to speed the training process and prevent overfitting).</p>
<p>In particular, the topics covered in the chapter are as follows:</p>
<ul>
<li>The structure of a basic artificial neuron</li>
<li>Perceptrons, linear classifiers, and their limitations</li>
<li>Multilayer perceptrons with the most important activation functions (such as ReLU)</li>
<li>Back-propagation algorithms based on <strong>stochastic gradient descent</strong> (<strong>SGD</strong>) optimization method</li>
<li>Optimized SGD algorithms (Momentum, RMSProp, Adam, AdaGrad, and AdaDelta)</li>
<li>Regularization and dropout</li>
<li>Batch normalization</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The basic artificial neuron</h1>
                </header>
            
            <article>
                
<p>The building block of a neural network is the abstraction of a biological neuron, a quite simplistic but powerful computational unit that was proposed for the first time by F. Rosenblatt in 1957, to make up the simplest neural architecture, called a perceptron, that we are going to analyze in the next section. Contrary to Hebbian Learning, which is more biologically plausible but has some strong limitations, the artificial neuron has been designed with a pragmatic viewpoint and, of course, only its structure is based on a few elements characterizing a biological cell. However, recent deep learning research activities have unveiled the enormous power of this kind of architecture. Even if there are more complex and specialized computational cells, the basic artificial neuron can be summarized as the conjunction of two blocks, which are clearly shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cac7847b-851e-4a7e-81a8-6bb81174669a.png" style="width:28.92em;height:21.67em;"/></div>
<p>The input of a neuron is a real-valued vector <em>x ∈ ℜ<sup>n</sup></em>, while the output is a scalar <em>y ∈ ℜ</em>. The first operation is linear:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0963dba1-7a8a-43f1-9f8c-00dbc8866620.png" style="width:7.08em;height:1.50em;"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">The vector <em>w ∈ ℜ<sup>n</sup></em> is called <strong>weight-vector</strong> (or <strong>synaptic weight vector</strong>, because, analogously to a biological neuron, it reweights the input values), while the scalar term <em>b ∈ ℜ</em> is a constant called <strong>bias</strong>. In many cases, it's easier to consider only the weight vector. It's possible to get rid of the bias by adding an extra input feature equal to 1 and a corresponding weight:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5cf0597d-6932-4e7b-b990-94e3981c2441.png" style="width:13.25em;height:1.58em;"/></div>
<p>In this way, the only element that must be learned is the weight vector. The following block is called an <strong>activation function</strong>, and it's responsible for remapping the input into a different subset. If the function is <em>f<sub>a</sub>(z) = z</em>, the neuron is called linear and the transformation can be omitted. The first experiments were based on linear neurons that are much less powerful than non-linear ones, and this was a reason that led many researchers to consider the perceptron as a failure, but, at the same time, this limitation opened the door for a new architecture that, instead, showed its excellent abilities. Let's now start this analysis with the first neural network ever proposed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Perceptron</h1>
                </header>
            
            <article>
                
<p><strong>Perceptron</strong> was the name that Frank Rosenblatt gave to the first neural model in 1957. A perceptron is a neural network with a single layer of input linear neurons, followed by an output unit based on the <em>sign(•)</em> function (alternatively, it's possible to consider a bipolar unit whose output is -1 and 1). The architecture of a perceptron is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/45010c23-8b99-4016-ad8d-cc0490489977.png" style="width:36.50em;height:25.50em;"/></div>
<p>Even if the diagram can appear as quite complex, a perceptron can be summarized by the following equation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/49fbeabc-88f8-4bfa-bc16-2a1efb7a151c.png" style="width:30.67em;height:1.83em;"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">All the vectors are conventionally column-vectors; therefore, the dot product <em>w<sup>T</sup>x<sub>i</sub></em> transforms the input into a scalar, then the bias is added, and the binary output is obtained using the step function, which outputs 1 when <em>z &gt; 0</em> and 0 otherwise. At this point, a reader could object that the step function is non-linear; however, a non-linearity applied to the output layer is only a filtering operation that has no effect on the actual computation. Indeed, the output is already decided by the linear block, while the step function is employed only to impose a binary threshold. Moreover, in this analysis, we are considering only single-value outputs (even if there are multi-class variants) because our goal is to show the dynamics and also the limitations, before moving to more generic architectures that can be used to solve extremely complex problems.</p>
<p>A perceptron can be trained with an online algorithm (even if the dataset is finite) but it's also possible to employ an offline approach that repeats for a fixed number of iterations or until the total error becomes smaller than a predefined threshold. The procedure is based on the squared error loss function (remember that, conventionally, the term <em>loss</em> is applied to single samples, while the term <em>cost</em> refers to the sum/average of every single loss):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2bf91875-302e-4805-8124-d040e18149ee.png" style="width:51.67em;height:3.25em;"/></div>
<p class="mce-root">When a sample is presented, the output is computed, and if it is wrong, a weight correction is applied (otherwise the step is skipped). For simplicity, we don't consider the bias, as it doesn't affect the procedure. Our goal is to correct the weights so as to minimize the loss. This can be achieved by computing the partial derivatives with respect to <em>w<sub>i</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ef339462-311a-4e70-8e75-2577230ae0d7.png" style="width:13.58em;height:3.33em;"/></div>
<p class="mce-root">Let's suppose that <em>w<sup>(0)</sup> = (0, 0)</em> (ignoring the bias) and the sample, <em>x = (1, 1)</em>, has <em>y = 1</em>. The perceptron misclassifies the sample, because <em>sign(w<sup>T</sup>x) = 0</em>. The partial derivatives are both equal to -1; therefore, if we subtract them from the current weights, we obtain <em><span>w</span><sup>(1)</sup></em> <span><em>= (1, 1)</em> and now the sample is correctly classified because</span> <em>sign(w<sup>T</sup>x) = 1</em><span>. Therefore, including a learning rate <em>η</em>, the weight update rule becomes as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5c35fd06-e3ac-406f-be6b-786b945c9a02.png" style="width:32.50em;height:4.33em;"/></div>
<p>When a sample is misclassified, the weights are corrected proportionally to the difference between actual linear output and true label. This is a variant of a learning rule called the <strong>delta rule</strong>, which represented the first step toward the most famous training algorithm, employed in almost any supervised deep learning scenario (we're going to discuss it in the next sections). The algorithm has been proven to converge to a stable solution in a finite number of states as the dataset is linearly separable. The formal proof is quite tedious and very technical, but the reader who is interested can find it in <em>Perceptrons, Minsky M. L., Papert S. A., The MIT Press</em>.</p>
<p>In this chapter, the role of the learning rate becomes more and more important, in particular when the update is performed after the evaluation of a single sample (like in a perceptron) or a small batch. In this case, a high learning rate (that is, one greater than 1.0) can cause an instability in the convergence process because of the magnitude of the single corrections. When working with neural networks, it's preferable to use a small learning rate and repeat the training session for a fixed number of epochs. In this way, the single corrections are limited, and only if they are <em>confirmed</em> by the majority of samples/batches, they can become stable, driving the network to converge to an optimal solution. If, instead, the correction is the consequence of an outlier, a small learning rate can limit its action, avoiding destabilizing the whole network only for a few noisy samples. We are going to discuss this problem in the next sections.</p>
<p>Now, we can describe the full perceptron algorithm and close the paragraph with some important considerations:</p>
<ol>
<li>Select a value for the learning rate <span><em>η</em> (such as <kbd>0.1</kbd>).</span></li>
<li>Append a constant column (set to <kbd>1.0</kbd>) to the sample vector <em>X</em>. Therefore <em>X<sub>b</sub> ∈ ℜ<sup>M × (n+1)</sup></em>.</li>
<li>Initialize the weight vector <em>w <span>∈ ℜ</span><sup>n+1</sup></em> with random values sampled from a normal distribution with a small variance (such as <kbd>0.05</kbd>).</li>
<li>Set an error threshold <kbd>Thr</kbd> (such as <kbd>0.0001</kbd>).</li>
<li>Set a maximum number of iterations <em>N<sub>i</sub></em>.</li>
<li>Set <kbd>i = 0</kbd>.</li>
<li>Set <kbd>e = 1.0</kbd>.</li>
</ol>
<ol start="8">
<li>While <em>i &lt; N<sub>i</sub></em> and <em>e &gt; Thr</em>:
<ol>
<li>Set <kbd>e = 0.0</kbd>.</li>
<li>For <em>k=1</em> to <em>M</em>:
<ol>
<li>Compute the linear output <em>l<sub>k</sub> = w<sup>T</sup>x<sub>k</sub></em> and the threshold one <em>t<sub>k</sub> = sign(l<sub>k</sub>)</em>.</li>
<li>If <em>t<sub>k</sub> != y</em><sub><em>k</em>:</sub>
<ol>
<li>Compute <em>Δw<sub>j</sub> = η(l<sub>k</sub> - y<sub>k</sub>)x<sub>k</sub><sup>(j)</sup></em>.</li>
<li>Update the weight vector.</li>
</ol>
</li>
<li>Set <em>e += <span>(l</span><sub>k</sub> <span>- y</span><sub>k</sub><span>)</span><sup>2</sup></em> (alternatively it's possible to use the absolute value <em><span>|l</span><sub>k</sub> <span>- y</span><sub>k</sub>|</em>).</li>
</ol>
</li>
<li>Set <kbd>e /= M</kbd>.</li>
</ol>
</li>
</ol>
<p>The algorithm is very simple, and the reader should have noticed an analogy with a logistic regression. Indeed, this method is based on a structure that can be considered as a perceptron with a sigmoid output activation function (that outputs a real value that can be considered as a probability). The main difference is the training strategy—in a logistic regression, the correction is performed after the evaluation of a cost function based on the negative log likelihood:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2476673b-93c2-4fba-a649-9af13799e415.png" style="width:62.33em;height:10.00em;"/></div>
<p class="mce-root">This cost function is the well-known cross-entropy and, in the first chapter, we showed that minimizing it is equivalent to reducing the Kullback-Leibler divergence between the true and predicted distribution. In almost all deep learning classification tasks, we are going to employ it, thanks to its robustness and convexity (this is a convergence guarantee in a logistic regression, but unfortunately the property is normally lost in more complex architectures).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of a perceptron with Scikit-Learn</h1>
                </header>
            
            <article>
                
<p class="mce-root">Even if the algorithm is very simple to implement from scratch, I prefer to employ the Scikit-Learn implementation <kbd>Perceptron</kbd>, so as to focus the attention on the limitations that led to non-linear neural networks. The <em>historical</em> problem that showed the main weakness of a perceptron is based on the XOR dataset. Instead of explaining, it's better to build it and visualize the structure:</p>
<pre>import numpy as np<br/><br/>from sklearn.preprocessing import StandardScaler<br/>from sklearn.utils import shuffle<br/><br/>np.random.seed(1000)<br/><br/>nb_samples = 1000<br/>nsb = int(nb_samples / 4)<br/><br/>X = np.zeros((nb_samples, 2))<br/>Y = np.zeros((nb_samples, ))<br/><br/>X[0:nsb, :] = np.random.multivariate_normal([1.0, -1.0], np.diag([0.1, 0.1]), size=nsb)<br/>Y[0:nsb] = 0.0<br/><br/>X[nsb:(2 * nsb), :] = np.random.multivariate_normal([1.0, 1.0], np.diag([0.1, 0.1]), size=nsb)<br/>Y[nsb:(2 * nsb)] = 1.0<br/><br/>X[(2 * nsb):(3 * nsb), :] = np.random.multivariate_normal([-1.0, 1.0], np.diag([0.1, 0.1]), size=nsb)<br/>Y[(2 * nsb):(3 * nsb)] = 0.0<br/><br/>X[(3 * nsb):, :] = np.random.multivariate_normal([-1.0, -1.0], np.diag([0.1, 0.1]), size=nsb)<br/>Y[(3 * nsb):] = 1.0<br/><br/>ss = StandardScaler()<br/>X = ss.fit_transform(X)<br/><br/>X, Y = shuffle(X, Y, random_state=1000)</pre>
<p>The plot showing the true labels is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f047a4fc-fff1-4018-b6ea-566443d4ca3e.png" style="width:34.67em;height:32.92em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Example of XOR dataset</div>
<p>As it's possible to see, the dataset is split into four blocks that are organized as the output of a logical XOR operator. Considering that the separation hypersurface of a two-dimensional perceptron (as well as the one of a logistic regression) is a line; it's easy to understand that any possible final configuration can achieve an accuracy that is about 50% (a random guess). To have a confirmation, let's try to solve this problem:</p>
<pre>import numpy as np<br/><br/>from multiprocessing import cpu_count<br/><br/>from sklearn.linear_model import Perceptron<br/>from sklearn.model_selection import cross_val_score<br/><br/>pc = Perceptron(penalty='l2', alpha=0.1, max_iter=1000, n_jobs=cpu_count(), random_state=1000)<br/>print(np.mean(cross_val_score(pc, X, Y, cv=10)))<br/>0.498</pre>
<p>The Scikit-Learn implementation offers the possibility to add a regularization term (see <a href="acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml">Chapter 1</a>, <em>Machine Learning Models Fundamentals</em>) through the parameter <kbd>penalty</kbd> (it can be <kbd>'l1'</kbd>, <kbd>'l2'</kbd> or <kbd>'elasticnet'</kbd>) to avoid overfitting and improve the convergence speed (the strength can be specified using the parameter <kbd>alpha</kbd>). This is not always necessary, but as the algorithm is offered in a production-ready package, the designers decided to add this feature. Nevertheless, the average cross-validation accuracy is slightly higher than 0.5 (the reader is invited to test any other possible hyperparameter configuration). The corresponding plot (that can change with different random states or subsequent experiments) is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b082166a-bf90-4f69-95d5-3102b88ebf13.png" style="width:36.92em;height:35.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">XOR dataset labeled using a perceptron</div>
<p>It's obvious that a perceptron is another linear model without specific peculiarities, and its employment is discouraged in favor of other algorithms like logistic regression or SVM. After 1957, for a few years, many researchers didn't hide their delusion and considered the neural network like a promise never fulfilled. It was necessary to wait until a simple modification to the architecture, together with a powerful learning algorithm, opened officially the door of a new fascinating machine learning branch (later called <strong>deep learning</strong>).</p>
<div class="packt_infobox">In Scikit-Learn &gt; 0.19, the class <kbd>Perceptron</kbd> allows adding <kbd>max_iter</kbd> or <kbd>tol</kbd> (tolerance) parameters. If not specified, a warning will be issued to inform the reader about the future behavior. This piece of information doesn't affect the actual results.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multilayer perceptrons</h1>
                </header>
            
            <article>
                
<p>The main limitation of a perceptron is its linearity. How is it possible to exploit this kind of architecture by removing such a constraint? The solution is easier than any speculation. Adding at least a non-linear layer between input and output leads to a highly non-linear combination, parametrized with a larger number of variables. The resulting architecture, called <strong>Mu</strong><strong>ltilayer Perceptron</strong> (<strong>MLP</strong>) and containing a single (only for simplicity) <strong>Hidden Layer</strong><em>,</em> is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7a6cfef0-a339-4c00-8abb-e469a2fb4b31.png" style="width:25.25em;height:23.75em;"/></div>
<p>This is a so-called <strong>feed-forward network</strong>, meaning that the flow of information begins in the first layer, proceeds always in the same direction and ends at the output layer. Architectures that allow a partial feedback (for example, in order to implement a local memory) are called <strong>recurrent</strong> <strong>networks</strong> and will be analyzed in the next chapter.</p>
<p>In this case, there are two weight matrices, <em>W</em> and <em>H</em>, and two corresponding bias vectors, <em>b</em> and <em>c</em>. If there are <em>m</em> hidden neurons, <em>x<sub>i</sub> ∈ ℜ<sup>n × 1</sup></em> (column vector), and <em>y<sub>i</sub> ∈ ℜ<sup>k × 1</sup></em>, the dynamics are defined by the following transformations:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a844411b-2b60-42ba-9292-5d9fde341e6e.png" style="width:31.92em;height:4.17em;"/></div>
<p class="mce-root">A fundamental condition for any MLP is that at least one hidden-layer activation function <em>f<sub>h</sub>(•)</em> is non-linear. It's straightforward to prove that <em>m</em> linear hidden layers are equivalent to a single linear network and, hence, an MLP falls back into the case of a standard perceptron. Conventionally, the activation function is fixed for a given layer, but there are no limitations in their combinations. In particular, the output activation is normally chosen to meet a precise requirement (such as multi-label classification, regression, image reconstruction, and so on). That's why the first step of this analysis concerns the most common activation functions and their features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activation functions</h1>
                </header>
            
            <article>
                
<p>In general, any continuous and differentiable function could be employed as activation; however, some of them have particular properties that allow achieving a good accuracy while improving the learning process speed. They are commonly used in the state-of-the-art models, and it's important to understand their properties in order to make the most reasonable choice.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sigmoid and hyperbolic tangent</h1>
                </header>
            
            <article>
                
<p>These two activations are very similar but with an important difference. Let's start defining them:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/818ed54f-f955-4666-aa7c-ee6feb633b3c.png" style="width:30.25em;height:3.33em;"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">The corresponding plots are shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/33cd940d-83ba-4471-b926-c6159dfcf26b.png" style="width:67.17em;height:45.08em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Sigmoid and hyperbolic tangent plots</div>
<p>A sigmoid <em>σ(x)</em> is bounded between 0 and 1, with two asymptotes (<span><em>σ(x) → 0 when x → -∞</em> and <em>σ(x) → 1 when x → ∞</em>). Similarly, the hyperbolic tangent (</span><kbd>tanh</kbd><span>) is bounded between -1 and 1 with two asymptotes corresponding to the extreme values. Analyzing the two plots, we can discover that both functions are almost linear in a short range (about <em>[-2, 2]</em>), and they become almost flat immediately after. This means that the gradient is high and about constant when <em>x</em> has small values around 0 and it falls down to about 0 for larger absolute values. A sigmoid perfectly represents a probability or a set of weights that must be bounded between 0 and 1, and therefore, it can be a good choice for some output layers. However, the hyperbolic tangent is completely symmetric, and, for optimization purposes, it's preferable because the performances are normally superior. This activation function is often employed in intermediate layers, whenever the input is normally small. The reason will be clear when the back-propagation algorithm is analyzed; however, it's obvious that large absolute inputs lead to almost constant outputs, and as the gradient is about 0, the weight correction can become extremely slow (this problem is formally known as <strong>vanishing gradients</strong>). For this reason, in many real-world applications, the next family of activation functions is often employed.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Rectifier activation functions</h1>
                </header>
            
            <article>
                
<p>These functions are all linear (or quasi-linear for Swish) when <em>x &gt; 0</em>, while they differ when <em>x &lt; 0</em>. Even if some of them are differentiable when <em>x = 0</em>, the derivative is set to <kbd>0</kbd> in this case. The most common functions are as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/87aa3fd6-522b-4ebb-9acd-2325e25a7102.png" style="width:55.75em;height:5.67em;"/></div>
<p class="mce-root">The corresponding plots are shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/da31e807-da0a-4604-bf32-c89f3ac08674.png" style="width:67.17em;height:45.08em;"/></div>
<p>The basic function (and also the most commonly employed) is the ReLU, which has a constant gradient when <em>x &gt; 0</em>, while it is null for <em>x &lt; 0</em>. This function is very often employed in visual processing when the input is normally greater than 0 and has the extraordinary advantage to mitigate the vanishing gradient problem, as a correction based on the gradient is always possible. On the other side, ReLU is null (together with its first derivative) when <em>x &lt; 0</em>, therefore every negative input doesn't allow any modification. In general, this is not an issue, but there are some deep networks that perform much better when a small negative gradient was allowed. This consideration drove to the other variants, which are characterized by the presence of the hyperparameter <em>α</em>, that controls the strength of the <em>negative tail</em>. Common values between 0.01 and 0.1 allow a behavior that is almost identical to ReLU, but with the possibility of a small weight update when <em>x &lt; 0</em>. The last function, called Swish and proposed in <em>Searching for Activation Functions, Ramachandran P., Zoph P., Le V. L., arXiv:1710.05941 [cs.NE]</em>, is based on the sigmoid and offers the extra advantage to converge to 0 when <em>x → 0</em>, so the <em>non-null effect</em> is limited to a short region bounded between <em>[-b, 0]</em> with <em>b &gt; 0</em>. This function can improve the performance of some particular visual processing deep networks, as discussed in the aforementioned paper. However, I always suggest starting the analysis with ReLU (that is very robust and computationally inexpensive) and switch to an alternative only if no other techniques can improve the performance of a model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Softmax</h1>
                </header>
            
            <article>
                
<p>This function characterized the output layer of almost all classification networks, as it can immediately represent a discrete probability distribution. If there are <em>k</em> outputs <em>y<sub>i</sub></em>, the softmax is computed as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1271b390-d7d7-4eee-a438-565e3ddf6ee6.png" style="width:14.83em;height:4.17em;"/></div>
<p>In this way, the output of a layer containing <em>k</em> neurons is normalized so that the sum is always 1. It goes without saying that, in this case, the best cost function is the cross-entropy. In fact, if all true labels are represented with a one-hot encoding, they implicitly become probability vectors with 1 corresponding to the true class. The goal of the classifier is hence to reduce the discrepancy between the training distribution of its output by minimizing the function (see <a href="acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml">Chapter 1</a>, <em>Machine Learning Models Fundamentals</em>, for further information):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2f10de73-0460-4cde-9e68-370467df39b4.png" style="width:29.92em;height:4.00em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Back-propagation algorithm</h1>
                </header>
            
            <article>
                
<p>We can now discuss the training approach employed in an MLP (and almost all other neural networks). This algorithm is more a methodology than an actual one; therefore I prefer to define the main concepts without focusing on a particular case. The reader who is interested in implementing it will be able to apply the same techniques to different kinds of networks with minimal effort (assuming that all requirements are met).</p>
<p>The goal of a training process using a deep learning model is normally achieved by minimizing a cost function. Let's suppose to have a network parameterized with a global vector θ, the cost function (using the same notation for loss and cost but with different parameters to disambiguate) is defined as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5d0a5dfb-1bf8-4371-816c-06620a147ce3.png" style="width:14.75em;height:4.08em;"/></div>
<p>We have already explained that the minimization of the previous expression (which is the empirical risk) is a way to minimize the real expected risk and, therefore, to maximize the accuracy. Our goal is, hence, to find an optimal parameter set so that the following applies:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d6088201-8503-45cc-939a-ebb7c87122b0.png" style="width:11.33em;height:1.58em;"/></div>
<p>If we consider a single loss function (associated with a sample x<sub>i</sub> and a true label <em>y<sub>i</sub></em>), we know that such a function can be expressed with an explicit dependence on the predicted value:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2ed57317-d234-4ff0-baec-8270e920a653.png" style="width:12.58em;height:1.58em;"/></div>
<p>Now, the parameters have been <em>embedded</em> into the prediction. From calculus (without an excessive mathematical rigor that can be found in many books about optimization techniques), we know that the gradient of <em>L</em>, a scalar function, computed at any point (we are assuming the <em>L</em> is differentiable) is a vector with components:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3686bcc1-3894-41fd-88ef-97a7cb63df4f.png" style="width:16.00em;height:3.17em;"/></div>
<p>As <em>∇L</em> points always in the direction of the closest maximum, so the negative gradient points in the direction of the closest minimum. Hence, if we compute the gradient of <em>L</em>, we have a ready-to-use piece of information that can be used to minimize the cost function. Before proceeding, it's useful to expose an important mathematical property called the <strong>chain rule of derivatives</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/12aa74d3-62b3-489d-ae4e-da19eace1537.png" style="width:20.00em;height:2.83em;"/></div>
<p class="mce-root">Now, let's consider a single step in an MLP (starting from the bottom) and let's exploit the chain rule:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9bfe13f3-ac2c-4200-bf7e-3043d56a51cb.png" style="width:10.25em;height:1.92em;"/></div>
<p class="mce-root">Each component of the vector <em>y</em> is independent of the others, so we can simplify the example by considering only an output value:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2707a46b-ef56-45ca-9655-91d2b2494f8b.png" style="width:15.42em;height:4.75em;"/></div>
<p>In the previous expression (discarding the bias), there are two important elements—the weights, <em>h<sub>j</sub></em> (that are the columns of <em>H</em>), and the expression, <em>z<sub>j</sub></em>, which is a function of the previous weights. As <em>L</em> is, in turn, a function of all predictions, <em>y<sub>i</sub></em>, applying the chain rule (using the variable <em>t</em> as the generic argument of the activation functions), we get the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/977e1cdb-f888-4a18-80a7-e78f83a6f59c.png" style="width:22.33em;height:3.08em;"/></div>
<p>As we normally cope with vectorial functions, it's easier to express this concept using the gradient operator. Simplifying the transformations performed by a generic layer, we can express the relations (with respect to a row of <em>H</em>, so to a weight vector <em>h<sub>i</sub></em> corresponding to a hidden unit, <em>z<sub>i</sub></em>) as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/907957cb-c2f1-427c-bfdd-b5e7cd4eb261.png" style="width:17.42em;height:4.00em;"/></div>
<p>Employing the gradient and considering the vectorial output <em>y</em> can be written as <em>y = (y<sub>1</sub>, y<sub>2</sub>, ..., y<sub>m</sub>)</em>, we can derive the following expression:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cc908237-c2dc-4d74-ae2c-b2cf8669681e.png" style="width:41.00em;height:9.00em;"/></div>
<p>In this way we get all the components of the gradient of <em>L</em> computed with respect to the weightvectors, <em>h<sub>i</sub></em>. If we move back, we can derive the expression of <em>z<sub>j</sub></em>:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/edac1e1f-6f2d-46e9-9e4f-d3ffe2230e4e.png" style="width:16.50em;height:4.75em;"/></div>
<p>Reapplying the chain rule, we can compute the partial derivative of <em>L</em> with respect to <em>w<sub>pj</sub></em> (to avoid confusion, the argument of the prediction <em>y<sub>i</sub></em> is called <em>t<sub>1</sub></em>, while the argument of <em>z<sub>j</sub></em> is called <em>t<sub>2</sub></em>):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/351d0b2a-0865-4cf6-b84b-71a637a5c261.png" style="width:24.92em;height:3.25em;"/></div>
<p class="mce-root">Observing this expression (that can be easily rewritten using the gradient) and comparing it with the previous one, it's possible to understand the philosophy of the <strong>back-propagation algorithm</strong>, presented for the first time in <em>Learning representations by back-propagating errors, Rumelhart D. E., Hinton G. E., Williams R. J., Nature 323/1986</em>. The samples are fed into the network and the cost function is computed. At this point, the process starts from the bottom, computing the gradients with respect to the closest weights and reusing a part of the calculation δ<sub>i</sub> (proportional to the error) to move back until the first layer is reached.</p>
<p class="mce-root">The correction is indeed propagated from the source (the cost function) to the origin (the input layer), and the effect is proportional to the responsibility of each different weight (and bias).</p>
<p class="mce-root">Considering all the possible different architectures, I think that writing all the equations for a single example is useless. The methodology is conceptually simple, and it's purely based on the chain rule of derivatives. Moreover, all existing frameworks, such as Tensorflow, Caffe, CNTK, PyTorch, Theano, and so on, can compute the gradients for all weights of a complete network with a single operation, so as to allow the user to focus attention on more pragmatic problems (like finding the best way to avoid overfitting and improving the training process).</p>
<p class="mce-root">A very important phenomenon that is worth considering was already outlined in the previous section and now it should be clearer: the chain rule is based on multiplications; therefore, when the gradients start to become smaller than 1, the multiplication effect forces the last values to be close to 0. This problem is known as <strong>vanishing gradients</strong> and can really stop the training process of very deep models that use saturating activation functions (like <kbd>sigmoid</kbd> or <kbd>tanh</kbd>). Rectifier units provide a good solution to many specific issues, but sometimes when functions like hyperbolic tangent are necessary, other methods, like normalization, must be employed to mitigate the phenomenon. We are going to discuss some specific techniques in this chapter and in the next one, but a generic best practice is to work always with normalized datasets and, if necessary, also testing the effect of whitening.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stochastic gradient descent</h1>
                </header>
            
            <article>
                
<p>Once the gradients have been computed, the cost function can be <em>moved</em> in the direction of its minimum. However, in practice, it is better to perform an update after the evaluation of a fixed number of training samples (batch). Indeed, the algorithms that are normally employed don't compute the global cost for the whole dataset, because this operation could be very computationally expensive. An approximation is obtained with partial steps, limited to the experience accumulated with the evaluation of a small subset. According to some literature, the expression <strong>stochastic gradient descent</strong> (<strong>SGD</strong>) should be used only when the update is performed after every single sample. When this operation is carried out on every <em>k</em> sample, the algorithm is also known as <strong>mini-batch gradient descent</strong>; however, conventionally SGD is referred to all batches containing <em>k ≥ 1</em> samples, and we are going to use this expression from now on.</p>
<p>The process can be expressed considering a partial cost function computed using a batch containing <em>k</em> samples:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dea3d8af-284f-4464-b8e0-17fa793615dd.png" style="width:15.58em;height:4.50em;"/></div>
<p>The algorithm performs a gradient descent by updating the weights according to the following rule:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d787dbc1-4a87-49bc-b7d7-6ecdfc897b97.png" style="width:12.92em;height:2.17em;"/></div>
<p>If we start from an initial configuration <em>θ<sub>start</sub></em>, the stochastic gradient descent process can be imagined like the path shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b8c4f4f1-9db4-40be-8714-64da4b095753.png" style="width:38.42em;height:23.92em;"/></div>
<p>The weights are moved towards the minimum <em><span>θ</span></em><sub><em>opt</em>,</sub> with many subsequent corrections that could also be wrong considering the whole dataset. For this reason, the process must be repeated several times (epochs), until the validation accuracy reaches its maximum. In a perfect scenario, with a convex cost function L, this simple procedure converges to the optimal configuration. Unfortunately, a deep network is a very complex and non-convex function where plateaus and saddle points are quite common (see <a href="acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml">Chapter 1</a>, <em>Machine Learning Models Fundamentals</em>). In such a scenario, a <em>vanilla</em> SGD wouldn't be able to find the global optimum and, in many cases, could not even find a close point. For example, in flat regions, the gradients can become so small (also considering the numerical imprecisions) as to slow down the training process until no change is possible (so <em>θ<sup>(t+1)</sup> ≈ θ<sup>(t)</sup></em>). In the next section, we are going to present some common and powerful algorithms that have been developed to mitigate this problem and dramatically accelerate the convergence of deep models.</p>
<p>Before moving on, it's important to mark two important elements. The first one concerns the learning rate, <em>η</em>. This hyperparameter plays a fundamental role in the learning process. As also shown in the figure, the algorithm proceeds jumping from a point to another one (which is not necessarily closer to the optimum). Together with the optimization algorithms, it's absolutely important to correctly tune up the learning rate. A high value (such as 1.0) could move the weights too rapidly increasing the instability. In particular, if a batch contains a few outliers (or simply non-dominant samples), a large <span><em>η</em> will consider them as representative elements, correcting the weights so to minimize the error. However, subsequent batches could better represent the data generating process, and, therefore, the algorithm must partially <em>revert</em> its modifications in order to compensate the wrong update. For this reason, the learning rate is usually quite small with common values bounded between 0.0001 and 0.01 (in some particular cases, <em>η = 0.1</em> can be also a valid choice). On the other side, a very small learning rate leads to minimum corrections, slowing down the training process. A good trade-off, which is often the best practice, is to let the learning rate decay as a function of the epoch. In the beginning, <em>η</em> can be higher, because the probability to be close to the optimum is almost null; so, larger jumps can be easily adjusted. While the training process goes on, the weights are progressively moved towards their final configuration and, hence, the corrections become smaller and smaller. In this case, large jumps should be avoided, preferring a fine-tuning. That's why the learning rate is decayed. Common techniques include the exponential decay or a linear one. In both cases, the initial and final values must be chosen according to the specific problem (testing different configurations) and the optimization algorithm. In many cases, the ratio between the start and end value is about 10 or even larger.</span></p>
<p>Another important hyperparameter is the batch size. There are no silver bullets that allow us to automatically make the right choice, but some considerations can be made. As SGD is an approximate algorithm, larger batches drive to corrections that are probably more similar to the ones obtained considering the whole dataset. However, when the number of samples is extremely high, we don't expect the deep model to map them with a one-to-one association, but instead our efforts are directed to improving the generalization ability. This feature can be re-expressed saying that the network must learn a smaller number of abstractions and reuse them in order to classify new samples. A batch, if sampled correctly, contains a part of these <em>abstract elements</em> and part of the corrections automatically improve the evaluation of a subsequent batch. You can imagine a waterfall process, where a new training step never starts from scratch. However, the algorithm is also called mini-batch gradient descent, because the usual batch size normally ranges from 16 to 512 (larger sizes are uncommon, but always possible), which are values smaller than the number of total samples (in particular in deep learning contexts). A reasonable default value could be 32 samples, but I always invite the reader to test larger values, comparing the performances in terms of training speed and final accuracy.</p>
<div class="packt_infobox">When working with deep neural networks, all the values (number of neurons in a layer, batch size, and so on) are normally powers of two. This is not a constraint, but only an optimization tip (above all when using GPUs), as the memory can be more efficiently filled when the blocks are based on a <em>2<sup>N</sup></em> elements. However, this is only a suggestion, whose benefits could also be negligible; so, don't be afraid to test architectures with different values. For example, in many papers, the batch size is 100 or some layers have 1,000 neurons.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weight initialization</h1>
                </header>
            
            <article>
                
<p>A very important element is the initial configuration of a neural network. How should the weights be initialized? Let's imagine we that have set them all to zero. As all neurons in a layer receive the same input, if the weights are 0 (or any other common, constant number), the output will be equal. When applying the gradient correction, all neurons will be treated in the same way; so, the network is equivalent to a sequence of single neuron layers. It's clear that the initial weights must be different to achieve a goal called symmetry breaking, but which is the best choice?</p>
<p>If we knew (also approximately) the final configuration, we could set them to easily reach the optimal point in a few iterations, but, unfortunately, we have no idea where the minimum is located. Therefore, some empirical strategies have been developed and tested, with the goal of minimizing the training time (obtaining state-of-the-art accuracies). A general rule of thumb is that the weights should be small (compared to the input sample variance). Large values lead to large outputs that negatively impact on saturating functions <span>(such as <kbd>tanh</kbd> and <kbd>sigmoid</kbd>)</span>, while <span>small values can be more easily optimized because the corresponding gradients are larger and the corrections have a stronger effect.</span> The same is true also for rectifier units because the maximum efficiency is achieved by working in a segment crossing the origin (where the non-linearity is actually <em>located</em>). For example, when coping with images, if the values are positive and large, a ReLU neuron becomes almost a linear unit, losing a lot of its advantages (that's why images are normalized, so as to bound each pixel value between 0 and 1 or -1 and 1).</p>
<p>At the same time, ideally, the activation variances should remain almost constant throughout the network, as well as the weight variances after every back-propagation step. These two conditions are fundamental in order to improve the convergence process and to avoid the vanishing and exploding <span>gradient problems</span> (the latter, which is the opposite of vanishing gradients, will be discussed in the section dedicated to recurrent network architectures).</p>
<p>A very common strategy considers the number of neurons in a layer and initializes the weights as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/84f592fa-d6d8-402f-811c-a1c7fd37492b.png" style="width:10.50em;height:3.83em;"/></div>
<p class="mce-root">This method is called <strong>variance scaling</strong> and can be applied using the number of input units (Fan-In), the number of output units (Fan-Out), or their average. The idea is very intuitive: if the number of incoming or outgoing connections is large, the weights must be smaller, so as to avoid large outputs. In the degenerate case of a single neuron, the variance is set to <kbd>1.0</kbd>, which is the maximum value allowed(in general, all methods keep the initial values for the biases equal to 0.0 because it's not necessary to initialize them with a random value).</p>
<p>Other variations have been proposed, even if they all share the same basic ideas. <strong>LeCun</strong> proposed initializing the weights as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b7564827-d12d-451a-9d80-a0de48e25214.png" style="width:22.58em;height:4.58em;"/></div>
<p>Another method called <strong>Xavier initialization</strong> (presented in <em>Understanding the difficulty of training deep feedforward neural networks, Glorot X., Bengio Y., Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</em>), is similar to <strong>LeCun initialization</strong>, but it's based on the average between the number of units of two consecutive layers (to mark the sequentiality, we have substituted the terms Fan-In and Fan-Out with explicit indices):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8a4ca55c-0f06-4619-8a46-940e4083553c.png" style="width:24.75em;height:4.50em;"/></div>
<p>This is a more robust variant, as it considers both the incoming connections and also the outgoing ones (which are in turn incoming connections). The goal (widely discussed by the authors in the aforementioned papers) is trying to meet the two previously presented requirements. The first one is to avoid oscillations in the variance of the activations of each layer (ideally, this condition can avoid saturation). The second one is strictly related to the back-propagation algorithm, and it's based on the observation that, w<span>hen employing a variance scaling (or an equivalent uniform distribution),</span> the variance of a weight matrix is proportional to the reciprocal of <em>3n<sub>k</sub></em>.</p>
<p>Therefore, the averages of Fan-In and Fan-Out are multiplied by three, trying to avoid large variations in the weights after the updates. Xavier initialization has been proven to be very effective in many deep architectures, and it's often the default choice.</p>
<p>Other methods are based on a different way to measure the variance during both the feed-forward and back-propagation phases and trying to correct the values to minimize residual oscillations in specific contexts. For example, <span>He, Zhang, Ren, and Sun</span> (in <em>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification, He K., Zhang X., Ren S., Sun J., arXiv:1502.01852 [cs.CV]</em>) analyzed the initialization problem in the context of convolutional networks (we are going to discuss them in the next chapter) based on ReLU or variable Leaky-ReLU activations (also known as PReLU, parametric ReLU), deriving an optimal criterion (often called the <strong>He initializer</strong>), which is slightly different from the Xavier initializer:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1562931a-8ac3-493b-b49b-a82aa418792e.png" style="width:23.25em;height:4.75em;"/></div>
<p>All these methods share some common principles and, in many cases, they are interchangeable. As already mentioned, Xavier is one of the most robust and, in the majority of real-life problems, there's no need to look for other methods; however, the reader should be always aware that the complexity of deep models must be often faced using empirical methods based on sometimes simplistic mathematical assumptions. Only the validation with real dataset can confirm if a hypothesis is correct or it's better to continue the investigation in another direction.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of MLP with Keras</h1>
                </header>
            
            <article>
                
<p>Keras (<a href="https://keras.io">https://keras.io</a>) is a powerful Python toolkit that allows modeling and training complex deep learning architectures with minimum effort. It relies on low-level frameworks, such as Tensorflow, Theano, or CNTK, and provides high-level blocks to build the single layers of a model. In this book, we need to be very pragmatic because there's no room for a complete explanation; however, all the examples will be structured to allow the reader to try different configurations and options without a full knowledge (for further details, I suggest the book <em>Deep Learning with Keras, Gulli A, Pal S., Packt Publishing</em>).</p>
<p>In this example, we want to build a small MLP with a single hidden layer to solve the XOR problem (the dataset is the same created in the previous example). The simplest and most common way is to instantiate the class <kbd>Sequential</kbd>, which defines an <em>empty container</em> for an indefinite model. In this initial part, the fundamental method is <kbd>add()</kbd>, which allows adding a layer to the model. For our example, we want to employ four hidden layers with hyperbolic tangent activation and two softmax output layers. The following snippet defines the MLP:</p>
<pre>from keras.models import Sequential<br/>from keras.layers import Dense, Activation<br/><br/>model = Sequential()<br/><br/>model.add(Dense(4, input_dim=2))<br/>model.add(Activation('tanh'))<br/><br/>model.add(Dense(2))<br/>model.add(Activation('softmax'))</pre>
<p>The <kbd>Dense</kbd> class defines a fully connected layer (a <em>classical</em> MLP layer), and the first parameter is used to declare the number of desired units. The first layer must declare the <kbd>input_shape</kbd> or <kbd>input_dim</kbd>, which specify the dimensions (or the shape) of a single sample (the batch size is omitted as it's dynamically set by the framework). All the subsequent layers compute the dimensions automatically. One of the strengths of Keras is the possibility to avoid setting many parameters (like weight initializers), as they will be automatically configured using the most appropriate default values (for example, the default weight initializer is Xavier). In the next examples, we are going to explicitly set some of them, but I suggest that the reader checks the official documentation to get acquainted with all the possibilities and features. The other layer involved in this experiment is <kbd>Activation</kbd>, which specifies the desired activation function (it's also possible to declare it using the parameter <kbd>activation</kbd> implemented by almost all layers, but I prefer to decouple the operations to emphasize the single roles, and also because some techniques—such as batch normalization—are normally applied to the linear output, before the activation).</p>
<p>At this point, we must ask Keras to compile the model (using the preferred backend):</p>
<pre>model.compile(optimizer='adam',<br/>              loss='categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>The parameter <kbd>optimizer</kbd> defines the stochastic gradient descent algorithm that we want to employ. Using <kbd>optimizer='sgd'</kbd>, it's possible to implement a standard version (as described in the previous paragraph). In this case, we are employing Adam (with the default parameters), which is a much more performant variant that will be discussed in the next section. The parameter <kbd>loss</kbd> is used to define the cost function (in this case, cross-entropy) and <kbd>metrics</kbd> is a list of all the evaluation score we want to be computed (<kbd>'accuracy'</kbd> is enough for many classification tasks). Once the model is compiled, it's possible to train it:</p>
<pre>from keras.utils import to_categorical<br/><br/>from sklearn.model_selection import train_test_split<br/><br/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1000)<br/><br/>model.fit(X_train, <br/>          to_categorical(Y_train, num_classes=2), <br/>          epochs=100, <br/>          batch_size=32,<br/>          validation_data=(X_test, to_categorical(Y_test, num_classes=2)))<br/><br/>Train on 700 samples, validate on 300 samples
Epoch 1/100
700/700 [==============================] - 1s 2ms/step - loss: 0.7227 - acc: 0.4929 - val_loss: 0.6943 - val_acc: 0.5933
Epoch 2/100
700/700 [==============================] - 0s 267us/step - loss: 0.7037 - acc: 0.5371 - val_loss: 0.6801 - val_acc: 0.6100
Epoch 3/100
700/700 [==============================] - 0s 247us/step - loss: 0.6875 - acc: 0.5871 - val_loss: 0.6675 - val_acc: 0.6733<br/><br/>...<br/><br/>Epoch 98/100
700/700 [==============================] - 0s 236us/step - loss: 0.0385 - acc: 0.9986 - val_loss: 0.0361 - val_acc: 1.0000
Epoch 99/100
700/700 [==============================] - 0s 261us/step - loss: 0.0378 - acc: 0.9986 - val_loss: 0.0355 - val_acc: 1.0000
Epoch 100/100
700/700 [==============================] - 0s 250us/step - loss: 0.0371 - acc: 0.9986 - val_loss: 0.0347 - val_acc: 1.0000</pre>
<p>The operations are quite simple. We have split the dataset into training and test/validation sets (in deep learning, cross-validation is seldom employed) and, then, we have trained the model setting <kbd>batch_size=32</kbd> and <kbd>epochs=100</kbd>. The dataset is automatically shuffled at the beginning of each epoch, unless setting <kbd>shuffle=False</kbd>. In order to convert the discrete labels into one-hot encoding, we have used the utility function <kbd>to_categorical</kbd>. In this case, the label 0 becomes (1, 0) and the label 1 (0, 1). The model converges before reaching 100 epochs; therefore, I invite the reader to optimize the parameters as an exercise. However, at the end of the process, the training accuracy is about 0.999 and the validation accuracy is 1.0.</p>
<p>The final classification plot is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e3be515d-06ce-43dc-ba43-ac43ff49332c.png" style="width:32.00em;height:30.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">MLP classification of the XOR dataset</div>
<p>Only three points have been misclassified, but it's clear that the MLP successfully separated the XOR dataset. To have a confirmation of the generalization ability, we've plotted the decision surfaces for a hyperbolic tangent hidden layer and ReLU one:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d8bf3ec2-26b9-470b-8f3e-bfa3bcbcce76.png" style="width:66.92em;height:34.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">MLP decision surfaces with Tanh (left) and ReLU (right) hidden layer</div>
<p>In both cases, the MLPs delimited the areas in a reasonable way. However, while a <kbd>tanh</kbd> hidden layer seems to be overfitted (this is not true in our case, as the dataset represents exactly the data generating process), the ReLU layer generates less smooth boundaries with an apparent lower variance (in particular for considering the outliers of a class). We know that the final validation accuracies confirm an almost perfect fit, and the decision plots (which is easy to create with two dimensions) show in both cases acceptable boundaries, but this simple exercise is useful to understand the complexity and the sensitivity of a deep model. For this reason, it's absolutely necessary to select a valid training set (representing the ground-truth) and employ all possible techniques to avoid the overfitting (as we're going to discuss later). The easiest way to detect such a situation is checking the validation loss. A good model should reduce both training and validation loss after each epoch, reaching a plateau for the latter. If, after <em>n</em> epochs, the validation loss (and, consequently, the accuracy) begins to increase, while the training loss keeps decreasing, it means that the model is overfitting the training set.</p>
<p>Another empirical indicator that the training process is evolving correctly is that, at least at the beginning, the validation accuracy should be higher than the training one. This can seem strange, but we need to consider that the validation set is slightly smaller and less complex than the training set; therefore, if the capacity of the model is not saturated with training samples, the probability of misclassification is higher for the training set than for the validation set. When this trend is inverted, the model is very likely to overfit after a few epochs. To verify these concepts, I invite the reader to repeat the exercise using a large number of hidden neurons (so as to increase dramatically the capacity), but they will be clearer when working with much more complex and unstructured datasets.</p>
<div class="packt_infobox">Keras can be installed using the command <kbd>pip install -U keras</kbd>. The default framework is Theano with CPU support. In order to use other frameworks (such as Tensorflow GPU), I suggest reading the instructions reported on the home page <a href="https://keras.io">https://keras.io</a>. As also suggested by the author, the best backend is Tensorflow, which is available for Linux, Mac OSX, and Windows. To install it (together with all dependencies), please follow the instructions on the following page: <a href="https://www.tensorflow.org/install/">https://www.tensorflow.org/install/</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Optimization algorithms</h1>
                </header>
            
            <article>
                
<p>When discussing the back-propagation algorithm, we have shown how the SGD strategy can be easily employed to train deep networks with large datasets. This method is quite robust and effective; however, the function to optimize is generally non-convex and the number of parameters is extremely large. These conditions increase dramatically the probability to find saddle points (instead of local minima) and can slow down the training process when the surface is almost flat.</p>
<p>A common result of applying a <em>vanilla</em> SGD algorithm to these systems is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/762c2102-e0ea-48f5-8b10-13ea786aea2e.png" style="width:48.50em;height:30.00em;"/></div>
<p>Instead of reaching the optimal configuration, <em>θ<sub>opt</sub></em>, the algorithm reaches a sub-optimal parameter configuration, <em><span>θ</span></em><sub><em>subopt</em>,</sub> and loses the ability to perform further corrections. To mitigate all these problems and their consequences, many SGD optimization algorithms have been proposed, with the purpose of speeding up the convergence (also when the gradients become extremely small) and avoiding the instabilities of ill-conditioned systems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient perturbation</h1>
                </header>
            
            <article>
                
<p>A common problem arises when <span>the hypersurface is flat (plateaus)</span> the gradients become close to zero. A very simple way to mitigate this problem is based on adding a small homoscedastic noise component to the gradients:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5322d2e3-a850-48af-a649-b73657a318e8.png" style="width:27.00em;height:1.58em;"/></div>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">The covariance matrix is normally diagonal with all elements set to <em>σ<sup>2</sup>(t)</em>, and this value is decayed during the training process to avoid perturbations when the corrections are very small. This method is conceptually reasonable, but its implicit randomness can yield undesired effects when the noise component is dominant. As it's very difficult to tune up the variances in deep models, other (more deterministic) strategies have been proposed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Momentum and Nesterov momentum</h1>
                </header>
            
            <article>
                
<p>A more robust way to improve the performance of SGD when plateaus are encountered is based on the idea of momentum (analogously to physical momentum). More formally, a momentum is obtained employing the weighted moving average of subsequent gradient estimations instead of the punctual value:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fa994b7f-3b44-4c19-9ad7-8a5e5086d2a7.png" style="width:14.08em;height:2.08em;"/></div>
<p>The new vector <em>v</em><sup><em>(t)</em>,</sup> contains a component which is based on the past history (and weighted using the parameter <em>μ</em> which is a forgetting factor) and a term referred to the current gradient estimation (multiplied by the learning rate). With this approach, abrupt changes become more difficult, and when the exploration leaves a sloped region to enter a plateau, the momentum doesn't become immediately null (but for a time proportional to <span><em>μ</em></span>)<span> a portion of the previous gradients will be kept, making it possible to traverse flat regions. The value assigned to the hyperparameter μ is normally bounded between 0 and 1. Intuitively, small values imply a short memory as the first term decays very quickly, while values close to 1.0 (for example, 0.9) allow a longer memory, less influenced by local oscillations. Like for many other hyperparameters, μ needs to be tuned according to the specific problem, considering that a high momentum is not always the best choice. High values could slow down the convergence when very small adjustments are needed, but, at the same time, values close to 0.0 are normally ineffective because the memory contribution decays too early. Using momentum, the update rule becomes as follows:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2c0a39b1-8da8-4ade-9ec5-7906ffdcde6b.png" style="width:13.00em;height:2.00em;"/></div>
<p>A variant is provided by <strong>Nesterov momentum</strong>, which is based on the results obtained in the field of mathematical optimization by Nesterov that have been proven to speed up the convergence of many algorithms. The idea is to determine a temporary parameter update based on the current momentum and then apply the gradient to this vector to determine the next momentum (it can be interpreted as a <em>look-ahead</em> gradient evaluation aimed to mitigate the risk of a wrong correction considering the moving history of each parameter):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2a57de8e-bed1-45a2-8559-e80c36c48a2a.png" style="width:19.50em;height:5.08em;"/></div>
<p class="mce-root">This algorithm showed a performance improvement in several deep models; however, its usage is still limited because the next algorithms very soon outperformed the standard SGD with momentum, and they became the first choice in almost any real-life task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">SGD with momentum in Keras</h1>
                </header>
            
            <article>
                
<p>When using Keras, it's possible to customize the SGD optimizer by directly instantiating the <kbd>SGD</kbd> class and using it while compiling the model:</p>
<pre>from keras.optimizers import SGD<br/><br/>...<br/><br/>sgd = SGD(lr=0.0001, momentum=0.8, nesterov=True)<br/><br/>model.compile(optimizer=sgd,<br/>              loss='categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>The class SGD accepts the parameter <kbd>lr</kbd> (the learning rate <em><span class="underline">η</span></em> with a default set to <kbd>0.01</kbd>), <kbd>momentum</kbd> (the parameter <em>μ</em>), <kbd>nesterov</kbd> (a boolean indicating whether employing the Nesterov momentum), and an optional <kbd>decay</kbd> parameter to indicate whether the learning rate must be decayed over the updates with the following formula:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/62f05832-fcb8-4175-b271-6729257cf8cd.png" style="width:10.67em;height:3.42em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RMSProp</h1>
                </header>
            
            <article>
                
<p><strong>RMSProp</strong> was proposed by Hinton as an adaptive algorithm, partially based on the concept of momentum. Instead of considering the whole gradient vector, it tries to optimize each parameter separately to increase the corrections of slowly changing weights (that probably need more drastic modifications) and decreasing the update magnitudes of quickly changing ones (which are normally the more unstable). The algorithm computes the exponentially weighted moving average of the <em>changing speed</em> of every parameter considering the square of the gradient (which is insensitive to the sign):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c15efc5f-3c65-472d-a5dd-a893063b7008.png" style="width:22.58em;height:2.08em;"/></div>
<p>The weight update is then performed, as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dfd09daa-c05f-4729-9c29-e9469ea11fd2.png" style="width:22.92em;height:4.17em;"/></div>
<p class="mce-root">The parameter <em>δ</em> is a small constant (such as 10<sup>-6</sup>) that is added to avoid numerical instabilities when the changing speed becomes null. The previous expression could be rewritten in a more compact way:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9e4d44d1-dec1-44c9-bafc-fd5f783de5e5.png" style="width:16.50em;height:2.00em;"/></div>
<p>Using this notation, it is clear that the role of RMSProp is adapting the learning rate for every parameter so it can increase it when necessary (almost <em>frozen</em> weights) and decrease it when the risk of oscillations is higher. In a practical implementation, the learning rate is always decayed over the epochs using an exponential or linear function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RMSProp with Keras</h1>
                </header>
            
            <article>
                
<p>The following snippet shows the usage of RMSProp with Keras:</p>
<pre>from keras.optimizers import RMSprop<br/><br/>...<br/><br/>rms_prop = RMSprop(lr=0.0001, rho=0.8, epsilon=1e-6, decay=1e-2)<br/><br/>model.compile(optimizer=rms_prop,<br/>              loss='categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>The learning rate and decay are the same as SGD. The parameter <kbd>rho</kbd> corresponds to the exponential moving average weight, μ, and <kbd>epsilon</kbd> is the constant added to the changing speed to improve the stability. As with any other algorithm, if the user wants to use the default values, it's possible to declare the optimizer without instantiating the class (for example, <kbd>optimizer='rmsprop'</kbd>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adam</h1>
                </header>
            
            <article>
                
<p><strong>Adam</strong> (the contraction of Adaptive Moment Estimation) is an algorithm proposed by Kingma and Ba (in <em>Adam: A Method for Stochastic Optimization, Kingma D. P., Ba J., arXiv:1412.6980 [cs.LG]</em><em>)</em> to further improve the performance of RMSProp. The algorithm determines an adaptive learning rate by computing the exponentially weighted averages of both the gradient and its square for every parameter:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d1e00807-fed7-4348-8cff-503e85644b7e.png" style="width:26.58em;height:4.08em;"/></div>
<p>In the aforementioned paper, the authors suggest to unbias the two estimations (which concern the first and second moment) by dividing them by <em>1 - μ<sub>i</sub></em>, so the new moving averages become as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/88d60b62-7a70-4dcb-b672-472d1614e2c1.png" style="width:14.42em;height:5.92em;"/></div>
<p>The weight update rule for Adam is as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/56b028cf-f88d-48ad-aadd-19d3cd2e7bd0.png" style="width:17.75em;height:4.75em;"/></div>
<p class="mce-root">Analyzing the previous expression, it is possible to understand why this algorithm is often called RMSProp with momentum. In fact, the term <em>g(•)</em> acts just like the standard momentum, computing the moving average of the gradient for each parameter (with all the advantages of this procedure), while the denominator acts as an adaptive term with the same exact semantics of RMSProp. For this reason, Adam is very often one of the most widely employed algorithms, even if, in many complex tasks, its performances are comparable to a standard RMSProp. The choice must be made considering the extra complexity due to the presence of two forgetting factors. In general, the default values (0.9) are acceptable, but sometimes it's better to perform an analysis of several scenarios before deciding on a specific configuration. Another important element to remember is that all momentum based methods can lead to instabilities (oscillations) when training some deep architectures. That's why RMSProp is very diffused in almost any research paper; however, don't consider this statement as a limitation, because Adam has shown outstanding performances in many tasks. It's helpful to remember that, whenever the training process seems unstable also with low learning rates, it's preferable to employ methods that are not based on momentum (the inertial term, in fact, can slow down the fast modifications necessary to avoid oscillations).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adam with Keras</h1>
                </header>
            
            <article>
                
<p>The following snippet shows the usage of Adam with Keras:</p>
<pre>from keras.optimizers import Adam<br/><br/>...<br/><br/>adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.9, epsilon=1e-6, decay=1e-2)<br/><br/>model.compile(optimizer=adam,<br/>              loss='categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>The forgetting factors, <em>μ<sub>1</sub></em> and <em><span>μ</span></em><sub><em>2</em>,</sub> <span>are represented by the parameters <kbd>beta_1</kbd> and <kbd>beta_2</kbd>. All the other elements are the same as the other algorithms.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaGrad</h1>
                </header>
            
            <article>
                
<p>This algorithm has been proposed by Duchi, Hazan, and Singer (in <em>Adaptive Subgradient Methods for Online Learning and Stochastic Optimizatioln, Duchi J., Hazan E., Singer Y., Journal of Machine Learning Research 12/2011).</em> The idea is very similar to RMSProp, but, in this case, the whole history of the squared gradients is taken into account:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/16165cf0-6ea2-4abc-9eee-0767a05d6e2a.png" style="width:19.67em;height:2.08em;"/></div>
<p>The weights are updated exactly like in RMSProp:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/be1c1511-841d-4fa0-8be7-7f3ede17e84f.png" style="width:22.00em;height:4.00em;"/></div>
<p>However, as the squared gradients are non-negative, the implicit sum <em>v<sup>(t)</sup>(•) → ∞</em> when <em>t → ∞</em>. As the growth continues until the gradients are non-null, there's no way to keep the contribution stable while the training process proceeds. The effect is normally quite strong at the beginning, but vanishes after a limited number of epochs, yielding a null learning rate. <strong>AdaGrad</strong> keeps on being a powerful algorithm when the number of epochs is very limited, but it cannot be a first-choice solution for the majority of deep models (the next algorithm has been proposed to solve this problem).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaGrad with Keras</h1>
                </header>
            
            <article>
                
<p>The following snippet shows the use of AdaGrad with Keras:</p>
<pre>from keras.optimizers import Adagrad<br/><br/>...<br/><br/>adagrad = Adagrad(lr=0.0001, epsilon=1e-6, decay=1e-2)<br/><br/>model.compile(optimizer=adagrad,<br/>              loss='categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>The AdaGrad implementation has no other parameters but the common ones.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaDelta</h1>
                </header>
            
            <article>
                
<p><strong>AdaDelta</strong> is an algorithm (proposed in <em>ADADELTA: An Adaptive Learning Rate Method, Zeiler M. D., arXiv:1212.5701 [cs.LG]</em>) in order to address the main issue of AdaGrad, which arises to managing the whole squared gradient history. First of all, instead of the accumulator, AdaDelta employs an exponentially weighted moving average, like RMSProp:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/33648fdf-1fcd-4796-b41e-6f551e2e8172.png" style="width:21.67em;height:2.00em;"/></div>
<p>However, the main difference with RMSProp is based on the analysis of the update rule. When we consider the operation <em>x + Δx</em>, we assume that both terms have the same unit; however, the author noticed that the adaptive learning rate <em>η(θ<sub>i</sub>)</em> obtained with RMSProp (as well as AdaGrad) is unitless (instead of having the unit of <em><span>θ</span><sub>i</sub></em>). In fact, as the gradient is split into partial derivatives that can be approximated as <em>ΔL/Δθ<sub>i</sub></em> and the cost function <em>L</em> is assumed to be unitless, we obtain the following relations:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7346ef49-3082-45b3-bc08-aaee221b0bf7.png" style="width:44.67em;height:6.67em;"/></div>
<p>Therefore, Zeiler proposed to apply a correction term proportional to the unit of each weight <em>θ<sub>i</sub></em>. This factor is obtained by considering the exponentially weighted moving average of every squared difference:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/042bebb2-417b-46a5-9539-5353244a5832.png" style="width:22.33em;height:2.00em;"/></div>
<p>The resulting updated rule hence becomes as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e911cca3-2e71-43ef-b075-221043212d02.png" style="width:22.25em;height:5.67em;"/></div>
<p>This approach is indeed more similar to RMSProp than AdaGrad, but the boundaries between the two algorithms are very thin, in particular when the history is limited to a finite sliding window. AdaDelta is a powerful algorithm, but it can outperform Adam or RMSProp only in very particular tasks. My suggestion is to employ a method and, before moving to another one, try to optimize the hyperparameters until the accuracy reaches its maximum. If the performances keep on being bad and the model cannot be improved in any other way, it's a good idea to test other optimization algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AdaDelta with Keras</h1>
                </header>
            
            <article>
                
<p>The following snippet shows the usage of AdaDelta with Keras:</p>
<pre>from keras.optimizers import Adadelta<br/><br/>...<br/><br/>adadelta = Adadelta(lr=0.0001, rho=0.9, epsilon=1e-6, decay=1e-2)<br/><br/>model.compile(optimizer=adadelta,<br/>              loss='categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>The forgetting factor, <em>μ</em>, is represented by the parameter <kbd>rho</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regularization and dropout</h1>
                </header>
            
            <article>
                
<p>Overfitting is a common issue in deep models. Their extremely high capacity can often become problematic even with very large datasets because the ability to learn the structure of the training set is not always related to the ability to generalize. A deep neural network can easily become an associative memory, but the final internal configuration couldn't be the most suitable to manage samples belonging to the same distribution but was never presented during the training process. It goes without saying that this behavior is proportional to the complexity of the separation hypersurface. A linear classifier has a minimum chance to overfit, and a polynomial classifier is incredibly more prone to do it. A combination of hundreds, thousands, or more non-linear functions yields a separation hypersurface, which is beyond any possible analysis. In 1991, Hornik (in <em>Approximation Capabilities of Multilayer Feedforward Networks,Hornik K., Neural Networks, 4/2</em>) generalized a very important result obtained two years before by the mathematician Cybenko (and published in <em>Approximations by Superpositions of Sigmoidal Functions, Cybenko G., Mathematics of Control, Signals, and Systems, 2 /4</em>). Without any mathematical detail (which is, however, not very complex), the theorem states that an MLP (not the most complex architecture!) can approximate any function that is continuous in a compact subset of <em>ℜ<sup>n</sup></em>. It's clear that such a result formalized what almost any researcher already intuitively knew, but its <em>power</em> goes beyond the first impact, because the MLP is a finite system (not a mathematical series) and the theorem assumes a finite number of layers and neurons. Obviously, the precision is proportional to the complexity; however, there are no unacceptable limitations for almost any problem. However, our goal is not learning an existing continuous function, but managing samples drawn from an unknown data generating process with the purpose to maximize the accuracy when a new sample is presented. There are no guarantees that the function is continuous or that the domain is a compact subset.</p>
<p>In <a href="acff0775-f21c-4b6d-8ef2-c78713e21364.xhtml">Chapter 1</a>, <em>Machine Learning Models Fundamentals</em>, we have presented the main regularization techniques based on a slightly modified cost function:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c2be5c48-adfd-4fd3-87c8-d3f7a8a19ce1.png" style="width:23.08em;height:4.17em;"/></div>
<p class="mce-root">The additional term <em>g(θ)</em> is a non-negative function of the weights (such as L2 norm) that forces the optimization process to keep the parameters as small as possible. When working with saturating functions (such as <kbd>tanh</kbd>), regularization methods based on the L2 norm try to limit the operating range of the function to the linear part, reducing <em>de facto</em> its capacity. Of course, the final configuration won't be the optimal one (that could be the result of an overfitted model) but the suboptimal trade-off between training and validation accuracy (alternatively, we can say between bias and variance). A system with a bias close to 0 (and a training accuracy close to 1.0) could be extremely rigid in the classification, succeeding only when the samples are very similar to ones evaluated during the training process. That's why this <em>price</em> is often paid considering the advantages obtained when working with new samples. L2 regularization can be employed with any kind of activation function, but the effect could be different. For example, ReLU units have an increased probability to become linear (or constantly null) when the weights are very large. Trying to keep them close to 0.0 means forcing the function to exploit its non-linearity without the risk of extremely large outputs (that can negatively affect very deep architectures). This result can sometimes be more useful, because it allows training bigger models in a smoother way, obtaining better final performances. In general, it's almost impossible to decide whether a regularization can improve the result without several tests, but there are some scenarios where it's very common to introduce a dropout (we discuss this approach in the next paragraph) and tune up its hyperparameter. This is more an empirical choice than a precise architectural decision because many real-life examples (including state-of-the-art models) obtained outstanding results employing this regularization technique. I suggest the reader prefer a rational skepticism to blind trust and double-checking its models before picking a specific solution. Sometimes, an extremely high-performing network turns to being ineffective when a different (but analogous) dataset is chosen. That's why testing different alternatives can provide the best experience in order to solve specific problem classes.</p>
<p>Before moving on, I want to show how it's possible to implement an L1 (useful to enforce sparsity), L2, or ElasticNet (the combination of L1 and L2) regularization using Keras. The framework provides a fine-grained approach that allows imposing a different constraint to each layer. For example, the following snippet shows how to add a <kbd>l2</kbd> constraint with the strength parameter set to <kbd>0.05</kbd> to a generic fully connected layer:</p>
<pre>from keras.layers import Dense<br/>from keras.regularizers import l2<br/><br/>...<br/><br/>model.add(Dense(128, kernel_regularizer=l2(0.05)))</pre>
<p>The <kbd>keras.regularizers</kbd> package contains the functions <kbd>l1()</kbd>, <kbd>l2()</kbd>, and <kbd>l1_l2()</kbd>, which can be applied to <kbd>Dense</kbd> and convolutional layers (we're going to discuss them in the next chapter). These layers allow us to impose a regularization on the weights (<kbd>kernel_regularizer</kbd>), on the bias (<kbd>bias_regularizer</kbd>), and on the activation output (<kbd>activation_regularizer</kbd>), even if the first one is normally the most widely employed.</p>
<p>Alternatively, it's possible to impose specific constraints on the weights and biases that in a more selective way. The following snippet shows how to set a maximum norm (equal to <kbd>1.5</kbd>) on the weights of a layer:</p>
<pre>from keras.layers import Dense<br/>from keras.constraints import maxnorm<br/><br/>...<br/><br/>model.add(Dense(128, kernel_constraint=maxnorm(1.5)))</pre>
<p>Keras, in the <kbd>keras.constraints</kbd> package, provides some functions that can be used to impose a maximum norm on the weights or biases <kbd>maxnorm()</kbd>, a unit norm along an axis <kbd>unit_norm()</kbd>, non-negativity <kbd>non_neg()</kbd>, and upper and lower bounds for the norm <kbd>min_max_norm()</kbd>. The difference between this approach and regularization is that it is applied only if necessary. Considering the previous example, imposing an L2 regularization always has an effect, while a constraint on the maximum norm is inactive until the value is lower than the predefined threshold.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dropout</h1>
                </header>
            
            <article>
                
<p>This method has been proposed by Hinton and co. (in <em>Improving neural networks by preventing co-adaptation of feature detectors, Hinton G. E., Srivastava N., Krizhevsky A., Sutskever I., Salakhutdinov R. R., arXiv:1207.0580 [cs.NE]</em>) as an alternative to prevent overfitting and allow bigger networks to explore more regions of the sample space. The idea is rather simple—during every training step, given a predefined percentage <em>n<sub>d</sub></em>, a <strong>dropout</strong> layer randomly selects <em>n<sub>d</sub>N</em> incoming units and sets them to <kbd>0.0</kbd> (the operation is only active during the training phase, while it's completely removed when the model is employed for new predictions).</p>
<p>This operation can be interpreted in many ways. When more dropout layers are employed, the result of their selection is a sub-network with a reduced capacity that can, with more difficultly, overfit the training set. The overlap of many trained sub-networks makes up an implicit ensemble whose prediction is an average over all models. If the dropout is applied on input layers, it works like a weak data augmentation, by adding a random noise to the samples (setting a few units to zero can lead to potential corrupted patterns). At the same time, employing several dropout layers allows exploring several potential configurations that are continuously combined and refined.</p>
<p>This strategy is clearly probabilistic, and the result can be affected by many factors that are impossible to anticipate; however, several tests confirmed that the employment of a dropout is a good choice when the networks are very deep because the resulting sub-networks have a residual capacity that allows them to model a wide portion of the samples, without driving the whole network to <em>freeze</em> its configuration overfitting the training set. On the other hand, this method is not very effective when the networks are shallow or contain a small number of neurons (in these cases, L2 regularization is probably a better choice).</p>
<p>According to the authors, dropout layers should be used in conjunction with high learning rates and maximum norm constraints on the weights. In this way, in fact, the model can easily learn more potential configurations that would be avoided when the learning rate is kept very small. However, this is not an absolute rule because many state-of-the-art models use a dropout together with optimization algorithms, such as RMSProp or Adam, and not excessively high learning rates.</p>
<p>The main drawback of a dropout is that it slows down the training process and can lead to an unacceptable sub-optimality. The latter problem can be mitigated by adjusting the percentages of dropped units, but, in general, it's very difficult to solve it completely. For this reason, some new image-recognition models (like residual networks) avoid the dropout and employ more sophisticated techniques to train very deep convolutional networks that overfit both training and validation sets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of dropout with Keras</h1>
                </header>
            
            <article>
                
<p>We cannot test the effectiveness of the dropout with a more challenging classification problem. The dataset is the <em>classical</em> MNIST handwritten digits, but Keras allows downloading and working with the original version that is made up of 70 thousand (60 thousand training and 10 thousand test) 28 × 28 grayscale images. Even if this is not the best strategy, because a convolutional network should be the first choice to manage images, we want to try to classify the digits considering them as flattened 784-dimensional arrays.</p>
<p>The first step is loading and normalizing the dataset so that each value becomes a float bounded between 0 and 1:</p>
<pre>import numpy as np<br/><br/>from keras.datasets import mnist<br/>from keras.utils import to_categorical<br/><br/>(X_train, Y_train), (X_test, Y_test) = mnist.load_data()<br/><br/>width = height = X_train.shape[1]<br/><br/>X_train = X_train.reshape((X_train.shape[0], width * height)).astype(np.float32) / 255.0<br/>X_test = X_test.reshape((X_test.shape[0], width * height)).astype(np.float32) / 255.0<br/><br/>Y_train = to_categorical(Y_train, num_classes=10)<br/>Y_test = to_categorical(Y_test, num_classes=10)</pre>
<p>At this point, we can start testing a model without dropout. The structure, which is common to all experiments, is based on three fully connected ReLU layers (2048-1024-1024) followed by a softmax layer with 10 units. Considering the problem, we can try to train the model using an Adam optimizer with <em>η = 0.0001</em> and a decay set to <em>10<sup>-6</sup></em>:</p>
<pre>from keras.models import Sequential<br/>from keras.layers import Dense, Activation<br/>from keras.optimizers import Adam<br/><br/>model = Sequential()<br/><br/>model.add(Dense(2048, input_shape=(width * height, )))<br/>model.add(Activation('relu'))<br/><br/>model.add(Dense(1024))<br/>model.add(Activation('relu'))<br/><br/>model.add(Dense(1024))<br/>model.add(Activation('relu'))<br/><br/>model.add(Dense(10))<br/>model.add(Activation('softmax'))<br/><br/>model.compile(optimizer=Adam(lr=0.0001, decay=1e-6),<br/>             loss='categorical_crossentropy',<br/>             metrics=['accuracy'])</pre>
<p>The model is trained for <kbd>200</kbd> epochs with a batch size of <kbd>256</kbd> samples:</p>
<pre>history = model.fit(X_train, Y_train,<br/>                    epochs=200,<br/>                    batch_size=256,<br/>                    validation_data=(X_test, Y_test))<br/><br/>Train on 60000 samples, validate on 10000 samples
Epoch 1/200
60000/60000 [==============================] - 11s 189us/step - loss: 0.4026 - acc: 0.8980 - val_loss: 0.1601 - val_acc: 0.9523
Epoch 2/200
60000/60000 [==============================] - 7s 116us/step - loss: 0.1338 - acc: 0.9621 - val_loss: 0.1062 - val_acc: 0.9669
Epoch 3/200
60000/60000 [==============================] - 7s 124us/step - loss: 0.0872 - acc: 0.9744 - val_loss: 0.0869 - val_acc: 0.9732<br/><br/>...<br/><br/>Epoch 199/200
60000/60000 [==============================] - 7s 114us/step - loss: 1.1935e-07 - acc: 1.0000 - val_loss: 0.1214 - val_acc: 0.9838
Epoch 200/200
60000/60000 [==============================] - 7s 116us/step - loss: 1.1935e-07 - acc: 1.0000 - val_loss: 0.1214 - val_acc: 0.9840</pre>
<p>Even without a further analysis, we can immediately notice that the model is overfitted. After 200 epochs, the training accuracy is 1.0 with a loss close to 0.0, while the validation accuracy is reasonably high, but with a validation loss slightly lower than the one obtained at the end of the second epoch.</p>
<p>To better understand what happened, it's useful to plot both accuracy and loss during the training process:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/61e814cc-f0c0-4603-b83b-5cbf451d941c.png" style="width:77.92em;height:30.17em;"/></div>
<p>As it's possible to see, the validation loss reached a minimum during the first 10 epochs and immediately restarted to grow (this is sometimes called a U-curve because of its shape). At the same moment, the training accuracy reached 1.0. From that epoch on, the model started overfitting, learning a perfect structure of the training set, but losing the generalization ability. In fact, even if the final validation accuracy is rather high, the loss function indicates a lack of robustness when new samples are presented. As the loss is a categorical cross-entropy, the result can be interpreted as saying that the model has learned a distribution that partially mismatches the validation set one. As our goal is to use the model to predict new samples, this configuration could not be acceptable. Therefore, we try again, using some dropout layers. As suggested by the authors, we also increment the learning rate to 0.1 (switching to a Momentum SGD optimizer in order to avoid <em>explosions</em> due to adaptivity of RMSProp or Adam), initialize the weight with a uniform distribution (<em>-0.05, 0.05</em>), and impose a maximum norm constraint set to 2.0. This choice allows the exploration of more sub-configurations without the risk of excessively high weights. The dropout is applied to the 25% of input units and to all ReLU fully connected layers with a percentage set to 50%:</p>
<pre>from keras.constraints import maxnorm<br/>from keras.models import Sequential<br/>from keras.layers import Dense, Activation, Dropout<br/>from keras.optimizers import SGD<br/><br/>model = Sequential()<br/><br/>model.add(Dropout(0.25, input_shape=(width * height, ), seed=1000))<br/><br/>model.add(Dense(2048, kernel_initializer='uniform', kernel_constraint=maxnorm(2.0)))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5, seed=1000))<br/><br/>model.add(Dense(1024, kernel_initializer='uniform', kernel_constraint=maxnorm(2.0)))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5, seed=1000))<br/><br/>model.add(Dense(1024, kernel_initializer='uniform', kernel_constraint=maxnorm(2.0)))<br/>model.add(Activation('relu'))<br/>model.add(Dropout(0.5, seed=1000))<br/><br/>model.add(Dense(10))<br/>model.add(Activation('softmax'))<br/><br/>model.compile(optimizer=SGD(lr=0.1, momentum=0.9),<br/>              loss='categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>The training process is performed with the same parameters:</p>
<pre>history = model.fit(X_train, Y_train,<br/>                    epochs=200,<br/>                    batch_size=256,<br/>                    validation_data=(X_test, Y_test))<br/><br/>Train on 60000 samples, validate on 10000 samples
Epoch 1/200
60000/60000 [==============================] - 11s 189us/step - loss: 0.4964 - acc: 0.8396 - val_loss: 0.1592 - val_acc: 0.9511
Epoch 2/200
60000/60000 [==============================] - 6s 97us/step - loss: 0.2300 - acc: 0.9300 - val_loss: 0.1081 - val_acc: 0.9645
Epoch 3/200
60000/60000 [==============================] - 6s 93us/step - loss: 0.1867 - acc: 0.9435 - val_loss: 0.0941 - val_acc: 0.9713<br/><br/>...<br/><br/>Epoch 199/200
60000/60000 [==============================] - 6s 99us/step - loss: 0.0184 - acc: 0.9939 - val_loss: 0.0473 - val_acc: 0.9884
Epoch 200/200
60000/60000 [==============================] - 6s 101us/step - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0484 - val_acc: 0.9883</pre>
<p>The final condition is dramatically changed. The model is no longer overfitted (even if it's possible to improve it in order to increase the validation accuracy) and the validation loss is lower than the initial one. To have a confirmation, let's analyze the accuracy/loss plots:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bfd4857e-5576-4626-98a0-f628019f210a.png" style="width:79.67em;height:31.08em;"/></div>
<p>The result shows some imperfections because the validation loss is almost flat for many epochs; however, the same model, with a higher learning rate and a weaker algorithm achieved a better final performance (0.988 validation accuracy) and a superior generalization ability. State-of-the-art models can<span> also</span> reach a validation accuracy equal to 0.995, but our goal was to show the effect of dropout layers in preventing the overfitting and, moreover, yielding a final configuration that is much more robust to new samples or noisy ones. I invite the reader to repeat the experiment with different parameters, bigger or smaller networks, and other optimization algorithms, trying to further reduce the final validation loss.</p>
<p>Keras also implements two additional dropout layers: <kbd>GaussianDropout</kbd>, which multiplies the input samples by a Gaussian noise:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d7859906-f6f3-4ce4-b4ff-c748763d1f5b.png" style="width:21.25em;height:3.33em;"/></div>
<p>The value for the constant ρ can be set through the parameter <kbd>rate</kbd> (bounded between 0 and 1). When <span><em>ρ → 1</em>, <em>σ<sup>2</sup> → ∞</em>, while small values yield a null effect as <em>n ≈ 1</em>.</span> This layer can be very useful as input one, in order to simulate a random data augmentation process. The other class is <kbd>AlphaDropout</kbd>, which works like the previous one, but renormalizing the output to keep the original mean and variance (this effect is very similar to the one obtained employing the technique described in the next paragraph together with noisy layers).</p>
<div class="packt_tip">When working with probabilistic layers (such as dropout), I always suggest setting the random seed (<kbd>np.random.seed(...)</kbd> and <kbd>tf.set_random_seed(...)</kbd> when Tensorflow backend is used). In this way, it's possible to repeat the experiments comparing the results without any bias. If the random seed is not explicitly set, every new training process will be different and it's not easy to compare the performances, for example, after a fixed number of epochs.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Batch normalization</h1>
                </header>
            
            <article>
                
<p>Let's consider a mini-batch of <em>k</em> samples:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8ffd163f-345d-4367-bb5c-8051d63d3911.png" style="width:13.75em;height:1.75em;"/></div>
<p>Before traversing the network, we can measure a mean and a variance:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/87aea2d8-f5ec-4982-84f2-032efed1a32b.png" style="width:26.75em;height:4.08em;"/></div>
<p>After the first layer (for simplicity, let's suppose that the activation function, <em>f(•)</em>, is the always the same), the batch is transformed into the following:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5ebe382c-ae47-4a90-b01c-cccfd04593d3.png" style="width:36.08em;height:2.17em;"/></div>
<p>In general, there's no guarantee that the new mean and variance are the same. On the contrary, it's easy to observe a modification that increases throughout the network. This phenomenon is called <strong>covariate shift</strong>, and it's responsible for a progressive training speed decay due to the different adaptations needed in each layer. Ioffe and Szegedy (in <em>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, Ioffe S., Szegedy C., arXiv:1502.03167 [cs.LG]</em>) proposed a method to mitigate this problem, which has been called <strong>batch normalization</strong> (<strong>BN</strong>).</p>
<p>The idea is to renormalize the linear output of a layer (before or after applying the activation function), so that the batch has null mean and unit variance. Therefore, the first task of a BN layer is to compute:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a0de4a53-1db8-4ac3-a6be-c0728afdeb5a.png" style="width:34.00em;height:4.25em;"/></div>
<p>Then each sample is transformed into a normalized version (the parameter <em>δ</em> is included to improve the numerical stability):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0f936eb0-339a-41fd-9364-4dd0e875d1fc.png" style="width:13.67em;height:4.33em;"/></div>
<p>However, as the batch normalization has no computational purposes other than speeding up the training process, the transformation must always be an identity (in order to avoid to distort and bias the data); therefore, the actual output will be obtained by applying the linear operation:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/129065a7-5bb8-4131-b0e2-290e84efd55c.png" style="width:12.42em;height:2.17em;"/></div>
<p>The two parameters <em>α<sup>(j)</sup></em> and <em>β<sup>(j)</sup></em> are variables optimized by the SGD algorithm; therefore, each transformation is guaranteed not to alter the scale and the position of data. These layers are active only during the training phase (like dropout), but, contrary to other algorithms, they cannot be simply discarded when the model is used to make predictions on new samples because the output would be constantly biased. To avoid this problem, the authors suggest approximating both mean and variance of <em>X</em> by averaging over the batches (assuming that there are <em>N<sub>b</sub></em> batches with <em>k</em> samples):</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a4d8ac24-285f-4aeb-8fc3-6e0c6da16b89.png" style="width:30.42em;height:4.08em;"/></div>
<p>Using these values, the batch normalization layers can be transformed into the following linear operations:</p>
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8b298eb1-53a3-440c-9bae-1cdf0d9622bb.png" style="width:22.58em;height:3.58em;"/></div>
<p class="mce-root">It's not difficult to prove that this approximation becomes more and more accurate when the number of batches increases and that the error is normally negligible. However, when the batch size is very small, the statistics can be quite inaccurate; therefore, this method should be used considering the <em>representativeness</em> of a batch. If the data generating process is simple, even a small batch can be enough to describe the actual distribution. When, instead, <em>p<sub>data</sub></em> is more complex, batch normalization requires larger batches to avoid wrong adjustments (a feasible strategy is to compare global mean and variance with the ones computed sampling some batches and trying to set the batch size that minimizes the discrepancy). However, this simple process can dramatically reduce the covariate shift and improve the convergence speed of very deep networks (including the famous residual networks). Moreover, it allows employing higher learning rates as the layers are implicitly <em>saturated</em> and can never <em>explode</em>. Additionally, it has been proven that batch normalization has also a secondary regularization effect even if it doesn't work on the weights. The reason is not very different from the one proposed for L2, but, in this case, there's a residual effect due to the transformation itself (partially caused by the variability of the parameters <em><span>α</span><sup>(j)</sup></em> <span>and <em>β</em></span><em><sup>(j)</sup></em>) that can encourage the exploration of different regions of the sample space. However, this is not the primary effect, and it's not a good practice employing this method as a regularizer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of batch normalization with Keras</h1>
                </header>
            
            <article>
                
<p>In order to show the feature of this technique, let's repeat the previous example using an MLP without dropout but applying a batch normalization after each fully connected layer before the ReLU activation. The example is very similar to the first one, but, in this case, we increase the Adam learning rate to 0.001 keeping the same decay:</p>
<pre>from keras.models import Sequential<br/>from keras.layers import Dense, Activation, BatchNormalization<br/>from keras.optimizers import Adam<br/><br/>model = Sequential()<br/><br/>model.add(Dense(2048, input_shape=(width * height, )))<br/>model.add(BatchNormalization())<br/>model.add(Activation('relu'))<br/><br/>model.add(Dense(1024))<br/>model.add(BatchNormalization())<br/>model.add(Activation('relu'))<br/><br/>model.add(Dense(1024))<br/>model.add(BatchNormalization())<br/>model.add(Activation('relu'))<br/><br/>model.add(Dense(10))<br/>model.add(BatchNormalization())<br/>model.add(Activation('softmax'))<br/><br/>model.compile(optimizer=Adam(lr=0.001, decay=1e-6),<br/>              loss='categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>We can now train using the same parameters again:</p>
<pre>history = model.fit(X_train, Y_train,<br/>                    epochs=200,<br/>                    batch_size=256,<br/>                    validation_data=(X_test, Y_test))<br/><br/>Train on 60000 samples, validate on 10000 samples
Epoch 1/200
60000/60000 [==============================] - 16s 274us/step - loss: 0.3848 - acc: 0.9558 - val_loss: 0.3338 - val_acc: 0.9736
Epoch 2/200
60000/60000 [==============================] - 8s 139us/step - loss: 0.1977 - acc: 0.9844 - val_loss: 0.1904 - val_acc: 0.9789
Epoch 3/200
60000/60000 [==============================] - 8s 137us/step - loss: 0.1292 - acc: 0.9903 - val_loss: 0.1397 - val_acc: 0.9835<br/><br/>...<br/><br/>Epoch 199/200
60000/60000 [==============================] - 8s 132us/step - loss: 4.7805e-05 - acc: 1.0000 - val_loss: 0.0599 - val_acc: 0.9877
Epoch 200/200
60000/60000 [==============================] - 8s 133us/step - loss: 2.6056e-05 - acc: 1.0000 - val_loss: 0.0593 - val_acc: 0.9879</pre>
<p>The model is again overfitted, but now the final validation accuracy is only slightly higher than the one achieved using the dropout layers. Let's plot accuracy and loss to better analyze the training process:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/be30b058-1319-423c-8c2e-9a3dde35d229.png" style="width:80.33em;height:31.08em;"/></div>
<p>The effect of the batch normalization improved the performances and slowed down the overfitting. At the same time, the elimination of the covariate shift avoided the U-curve keeping a quite low validation loss. Moreover, the model reached a validation accuracy of about 0.99 during the epochs 135-140 with a residual positive trend. Analogously to the previous example, this solution is imperfect, but it's a good starting point for further optimization. It would be a good idea to continue the training process for a larger number of epochs, monitoring both the validation loss and accuracy. Moreover, it's possible to mix dropout and batch normalization or experiment with the Keras AlphaDropout layer. However, if, in the first example (without dropout), the climax of training accuracy was associated with a starting positive trend for the validation loss, in this case, the learned distribution doesn't seem to be very different from the validation set one. In other words, batch normalization is not preventing overfitting the training set, but it's avoiding a decay in the generalization ability (observed when there was no batch normalization). I suggest repeating the test with other hyperparameter and architectural configurations in order to decide whether this model can be used for prediction purposes or it's better to look for other solutions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we started the exploration of the deep learning world by introducing the basic concepts that led the first researchers to improve the algorithms until they achieved the top results we have nowadays. The first part explained the structure of a basic artificial neuron, which combines a linear operation followed by an optional non-linear scalar function. A single layer of linear neurons was initially proposed as the first neural network, with the name of the perceptron.</p>
<p>Even though it was quite powerful for many problems, this model soon showed its limitations when working with non-linear separable datasets. A perceptron is not very different from a logistic regression, and there's no concrete reason to employ it. Nevertheless, this model opened the doors to a family of extremely powerful models obtained combining multiple non-linear layers. The multilayer perceptron, which has been proven to be a universal approximator, is able to manage almost any kind of dataset, achieving high-level performances when other methods fail.</p>
<p>In the next section, we analyzed the building bricks of an MLP. We started with the activation functions, describing their structure and features, and focusing on the reasons they lead the choice for specific problems. Then, we discussed the training process, considering the basic idea behind the back-propagation algorithm and how it can be implemented using the stochastic gradient descent method. Even if this approach is quite effective, it can be slow when the complexity of the network is very high. For this reason, many optimization algorithms were proposed. In this chapter, we analyzed the role of momentum and how it's possible to manage adaptive corrections using RMSProp. Then, we combined both, momentum and RMSProp to derive a very powerful algorithm called Adam. In order to provide a complete vision, we also presented two slightly different adaptive algorithms, called AdaGrad and AdaDelta.</p>
<p>In the next sections, we discussed the regularization methods and how they can be plugged into a Keras model. An important section was dedicated to a very diffused technique called dropout, which consists in setting to zero (dropping) a fixed percentage of samples through a random selection. This method, although very simple, prevents the overfitting of very deep networks and encourages the exploration of different regions of the sample space,  obtaining a result not very dissimilar to the ones analyzed in <a href="78baef9c-5391-4898-91bf-8df25330a163.xhtml" target="_blank">Chapter 8</a>, <em>Ensemble Learning</em>. The last topic was the batch normalization technique, which is a method to reduce the mean and variance shift (called covariate shift) caused by subsequent neural transformations. This phenomenon can slow down the training process as each layer requires different adaptations and it's more difficult to move all the weights in the best direction. Applying batch normalization means very deep networks can be trained in a shorter time, thanks also to the possibility of employing higher learning rates.</p>
<p>In the next chapter, we are going to continue this exploration, analyzing very important advanced layers like convolutions (that achieve extraordinary performances in image-oriented tasks) and recurrent units (for the processing of time series) and discussing some practical applications that can be experimented on and readapted using Keras and Tensorflow.</p>


            </article>

            
        </section>
    </body></html>