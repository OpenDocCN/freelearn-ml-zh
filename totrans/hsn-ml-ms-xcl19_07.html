<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Correlations and the Importance of Variables</h1>
                </header>
            
            <article>
                
<p>Correlation between variables, in general, means that a change in one variable reflects on the other. However, it does not mean that the change in one variable is <em>caused</em> by the change in the correlated variable. For example, the selling price of a product is correlated to its manufacturing cost, but the price increase is not totally caused by it, since there are other factors such as transportation and inflation to take into account.</p>
<p>Not every variable or feature in a dataset is useful for the analysis that we are planning and, sometimes, many of them are redundant. Strong correlations between pairs of variables tell us which ones can be discarded and which ones are important to predict or explain the target variable.</p>
<p>Different correlation calculations can be performed in Excel and used to determine the relative importance of the input features. We will show some of them in this chapter, together with graphical methods.</p>
<p>The dataset that will be used in this chapter has been taken from the StatLib library, which is maintained at Carnegie Mellon University, and relates the different variables of a car to its fuel consumption.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Building a scatter diagram</li>
<li>Calculating the covariance</li>
<li>Calculating the Pearson's coefficient of correlation</li>
<li>Studying the Spearman's correlation</li>
<li>Understanding least squares</li>
<li>Focusing on feature selection</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="mce-root">You will need to download the <kbd>auto-mpg.xlsx</kbd> file from the GitHub repository at <a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter05">https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter05</a><a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter05">.</a><a href="https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a scatter diagram</h1>
                </header>
            
            <article>
                
<p>First, load the <kbd><span>auto-mpg</span>.xlsx</kbd> file. We will use the data in it to illustrate different aspects of this chapter. The meaning of the variables are described in the Excel file and in its references.</p>
<p>The simplest way of assessing correlations between variables is to create a scatter diagram, taking all features in pairs. If we plot, for example, the <kbd>Cylinders</kbd> variable in the <em>x </em>axis against the <kbd>Displacement</kbd> variable in the <em>y </em>axis, we will see a <em>positive</em> <em>correlation</em> (that is, the greater the number of cylinders the higher the displacement value). This is to be expected, since the calculation of the engine displacement, here expressed in cubic inches, is linearly dependent on the number of cylinders. </p>
<p>The scatter diagram can be seen in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cbb435b0-cc7e-46b7-b0c1-756db3eb7a9f.png" style="width:31.17em;height:18.50em;"/></p>
<p>If we, instead, look at the relationship between fuel consumption and car weight, the diagram will be similar to the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7290f2f2-a217-4d79-b998-542b38460ff8.png" style="width:30.92em;height:18.50em;"/></p>
<p>In this case, there is a negative correlation between the weight of the car and the number of miles per fuel gallon (that is, the heavier the car, the fewer number of miles it can run on a gallon of fuel). We also notice that the correlation is non-linear, meaning that a straight line will not describe the relationship between these variables.</p>
<p>So, what if we plot two uncorrelated variables? Could there be any correlation between, for example, the number of cylinders in the engine and the year that the car was manufactured? Let's take a look at the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b9267c13-4116-4d9c-81da-a133e48cbeae.png" style="width:29.33em;height:17.25em;"/></p>
<p>Here, we notice that cars with <strong>3</strong> or <strong>5</strong> cylinders were not very common in the time period that we are analyzing, as there are only a few examples of them. Years <strong>78</strong> to <strong>80</strong> seem to be the <strong>5</strong>-cylinder engine period, but, apart from these facts, there were also <strong>4</strong>-, <strong>6</strong>-, and <strong>8</strong>-cylinder engines being produced for each year of our dataset. There is no clear correlation between these two variables, and one of them cannot give us any information about the other.</p>
<p>This method of finding correlations in scatter diagrams is fine if we have a few variables, but the number of diagrams needs scales fast. In fact, if the number of variables is <em>N<sub>v</sub></em>, then the number of combinations needed to see all correlations is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em><span>N</span><sub>v </sub>* (<span>N</span><sub>v </sub>-1)</em></p>
<p>Even with a small dataset such as ours, to include 8 numerical variables, we need 28 diagrams to cover all the possible combinations. If we were to have hundreds of variables, then the task of finding which variables are correlated by eye is simply impossible. In the next section, we will describe methods to automatically calculate correlations, making it possible to deal with big datasets and a large number of features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the covariance</h1>
                </header>
            
            <article>
                
<p>We need to define a statistical method that quantitatively <span>measures </span>the degree of association between two features. The covariance of two variables does exactly that, so let's see how it is calculated. If there are two variables, <em>x</em> and <em>y</em>, we first center their values around their mean values, <img class="fm-editor-equation" src="assets/69efd6ca-3339-4487-b25a-d98481e9c20a.png" style="width:0.83em;height:1.25em;"/> and <img class="fm-editor-equation" src="assets/11a5b30d-ca86-4e2c-8eae-20607818ddf9.png" style="width:0.75em;height:1.50em;"/>; then, we multiply the new values and take the mean of the product:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/de5b5ebf-f471-42fa-9bb1-ebd671867b85.png" style="width:20.00em;height:1.50em;"/></p>
<p>This definition implies that if both variables increase or decrease at the same time, then the covariance is positive, whereas if they move in opposite directions, then the covariance is negative. If there is no correlation, the covariance value will be small, that is, close to zero.</p>
<p>It is also clear from the definition that, since the variables keep their scale, it is difficult to compare features that have very different mean values and it is impossible to compare two covariances. </p>
<p>It is easy to calculate covariances in Excel by using the <span class="packt_screen">Data analysis</span> add-in (we explain how to activate it in the Appendix).</p>
<p><span>To calculate the </span><span>covariances, perform the f</span>ollowing steps<span>:</span></p>
<ol>
<li>Open the data file.</li>
<li>Navigate to <span class="packt_screen"><span class="packt_screen">Data</span> | Data Analysis</span>.</li>
<li>In the pop-up window, select <span class="packt_screen">Covariance</span>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3188dae5-2b12-4074-86b5-6c369067ca59.png" style="width:34.75em;height:16.58em;"/></p>
<ol start="4">
<li>Select the data range; in this case, it is the whole table except the last column, which contains the car name and is non-numeric:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b25c1e26-4f31-45d5-996d-9413910fc748.png" style="width:33.67em;height:19.08em;"/></p>
<p>The result is the following table:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d68062e0-ddfb-4091-a3f1-999cecaf14d8.png" style="width:56.25em;height:15.67em;"/></p>
<p>We can see a positive covariance value between <kbd>displacement</kbd> and <kbd>cylinders</kbd>, and a negative value between <kbd>weight</kbd> and <kbd>mpg</kbd>. We cannot say much more, since comparing the values is impossible, as we explained previously. The big change here is that we calculated all that values at the same time, and so we don't need to watch the diagrams one by one. The matrix is symmetrical, so only one half is shown.</p>
<p>There is a way to quantify correlations and compare them, and it was developed by Karl Pearson in the 1880s. Let's explore it in more detail in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calculating the Pearson's coefficient of correlation</h1>
                </header>
            
            <article>
                
<p>The Pearson's coefficient is most commonly used when comparing two variables and it works by measuring the linear relationship between them. The original definition given by Pearson is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2171e11a-435a-4b49-96bd-f7cbcb0ce262.png" style="width:19.33em;height:4.17em;"/></p>
<p>The numerator is proportional to the covariance, and the denominator is the product of the standard deviations (σ) of the centered variables. This normalization ensures that the limits in the possible values of <em>ρ</em> are <em>-1</em> and <em>1</em>.</p>
<p>We can repeat the steps outlined in the <em>Calculating the covariance</em> section to calculate the Pearson correlation in Excel by selecting <span class="packt_screen">Correlation</span> in the pop-up window.</p>
<p>The resulting table is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8ea53abc-e1dc-4de7-849a-817a21ec4bcd.png" style="width:54.75em;height:15.33em;"/></p>
<p>The cells containing a value of <kbd>1</kbd> represent the linear relationship between itself and each variable. A negative correlation implies, again, that one variable increases while the other decreases, while a positive correlation implies that both variables change in the same direction.</p>
<p>Pearson's coefficient is good for comparing feature relationships. For example, we can see that <kbd>cylinders</kbd> and <kbd>displacement</kbd> are more linearly correlated (by the definition of displacement, in fact) than <kbd>weight</kbd> and <kbd>mpg</kbd>, even when the ones in the second pair are related. </p>
<p>Another definition for the Pearson coefficient is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/94bae1ba-6a8f-4afe-8364-eac87f254abe.png" style="width:7.25em;height:3.25em;"/></p>
<p>Here, <em>b</em> is the slope of the linear regression that best fits <em>x</em> versus <em>y</em>, and <em>σ<sub>i</sub></em> are the standard deviations of <em>x</em> and <em>y</em>. This definition clearly shows that the coefficient measures the linearity of the relationship and, at the same time, how much the two features can vary.</p>
<p>So, what if the relationship is not linear? In the next section, we can discuss another coefficient that will help us calculate a non-linear correlation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Studying the Spearman's correlation</h1>
                </header>
            
            <article>
                
<p>To calculate the Spearman's coefficient, we need to first rank the values of each variable, that is, the order of the values when we sort them from <span>highest to </span>lowest. Once we have the new table, we will calculate Pearson's <em>ρ</em> on it. </p>
<p>In a new sheet, we define the following formula in a cell:</p>
<p class="CDPAlignCenter CDPAlign"><em>=RANK.AVG(Data!A2;auto_mpg[mpg])</em></p>
<p>Here, we are asking Excel to write in that cell the ranking corresponding to the first cell of the <kbd>mpg</kbd> column in our data table, taking into account the full range of the column. We copy the formula to the cells on the right until we complete the number of columns of the data table (8 columns). It doesn't matter if you copy the formula to an extra cell – you will just get an error message since you are out of the data table range. In a similar way, we can copy the formulas to the remaining rows until we get to row <strong>399</strong> (the vertical range of the data table). We can even add a title to the new columns by using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><em>=CONCAT("Rank_";auto_mpg[[#Headers];[mpg]])</em></p>
<p>Then, we copy it to all the cells in the first row. </p>
<p>A sample of the table that we obtain is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b8b0b6ca-d128-4561-a8c5-5bf4a2206932.png" style="width:56.92em;height:39.33em;"/></p>
<p>Because <kbd>horsepower</kbd><em> </em>is missing some values, they cannot be ranked and so appear as <kbd>#N/A</kbd><em>.</em> Since there are only a few of them, we can remove them manually. This will avoid errors when calculating the Pearson coefficient in the next step, <span>exactly as we did before; the result is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/33243ad4-e948-4605-bacf-f5af6f1fe46b.png"/></p>
<p>We get similar values to that of the Pearson's coefficients, but they are slightly higher when there is a non-linear but strong correlation.</p>
<p>Spearman is only close to <strong>1</strong> if the correlation is monotonically increasing or decreasing. This is better shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/942e1f3a-667e-410d-8b06-41966fc144d5.png"/></p>
<p>In the first diagram, Pearson is high, since the relationship can be adjusted by a straight line, even if it is not the best fit. Spearman is <strong>1</strong> since there is a relationship and it is monotonically increasing. The second diagram shows a relationship with an abrupt change, giving a small value for both coefficients. The third diagram shows a quadratic relationship between variables, which is neither linear nor monotonic. Looking at the three examples, we can understand that coefficients don't always give all the necessary information about the correlation between variables, but they are useful to get a general idea.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding least squares</h1>
                </header>
            
            <article>
                
<p>In some instances, we might want to prove that there is a functional relationship between two variables and, hence, just use one of them in our model – since the other can be easily approximated by an expression. In this case, it is useful to rely on the least squares method. Given a set of points (<em>x<sub>i</sub>,y<sub>i</sub></em>) and a function such as <em>y'<sub>i</sub> = f(x<sub>i</sub>)</em>, this method minimizes the square of the differences between <em>y'<sub>i</sub></em> and <em>y<sub>i</sub></em>. The general expression for the minimization that we are calculating is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e1580832-57ab-4e80-9900-c20bd1c646d9.png" style="width:12.08em;height:3.58em;"/></p>
<p>We will use two columns from our data table, namely <kbd>weight</kbd> and <kbd>mpg</kbd>:</p>
<ol>
<li>Create a new table in a new sheet.</li>
<li>Copy the values of the <kbd>weight</kbd> and <kbd>mpg</kbd> columns.</li>
</ol>
<ol start="3">
<li>Order the rows by the value of <kbd>weight</kbd>; the resulting table is as follows:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0e981124-8900-4e46-9cdb-c7cae4f4f009.png" style="width:10.08em;height:29.50em;"/></p>
<ol start="4">
<li>Insert a line chart<span> to see what the functional relationship looks like, as follows:</span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/406a31b6-bcca-4f3e-babb-ea52ab3d1997.png" style="width:29.33em;height:17.75em;"/></p>
<p style="padding-left: 60px">Let's say that we assume that <em>mpg = A*weight^(-b)</em> and try to find the constants, <em>a</em> and <em>b</em>.</p>
<ol start="5">
<li>Create a new column, <kbd>prediction</kbd>, using the following formula:</li>
</ol>
<p style="padding-left: 90px" class="CDPAlignLeft CDPAlign"><em>=$H$2*POWER([@weight];$H$3)</em></p>
<p style="padding-left: 60px">The resulting table is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/47292ee2-d5eb-41a2-b63e-84b02b6efbff.png" style="width:12.58em;height:23.67em;"/></p>
<ol start="6">
<li>In order to fill the table, we choose the initial values of <em>a = 60</em> (in cell <em>H2</em>) and <em>b = -0.5</em> (in cell <em>H3</em>). These will be the starting points of the least squares method.</li>
<li>The quantity to minimize is the sum of the squares of the errors. To calculate it, we create a new column, <kbd>Squared error</kbd>, with the following formula:</li>
</ol>
<p style="padding-left: 90px"><em>=([@mpg]-[@prediction])^2</em></p>
<ol start="8">
<li>Then, use the following formula to sum all the values in that column in a cell:</li>
</ol>
<p style="padding-left: 90px"><em>=SUM(Table9[Squared error])</em></p>
<ol start="9">
<li>Navigate to <span class="packt_screen">Data</span> | <span class="packt_screen">Solver</span>; if you cannot see this option, please refer to the Appendix for instructions on how to activate <span class="packt_screen">Solver</span>. You will see the following window pop up on your screen:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7aef1ec0-6598-4927-a310-1f6e1b31b3a2.png" style="width:42.25em;height:40.25em;"/></p>
<ol start="10">
<li>The <span class="packt_screen">Set Objective</span> option is filled with the cell ID where we calculated the sum of the squared errors, and the <span class="packt_screen">By Changing Variable Cells</span> option is filled with the ID of the two cells containing the values of <em>a</em> and <em>b</em>. We can leave the rest of the parameters as their default value settings.</li>
<li>Click on <span class="packt_screen">Solve</span>; if the regression converges, then you will see the following window:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0684e405-de49-43f1-b240-b8ba8e9df0a9.png" style="width:40.00em;height:26.42em;"/></p>
<ol start="12">
<li>Choose <span class="packt_screen">Keep Solver Solution</span> to replace the values of <em>a</em> and <em>b</em> by the ones calculated, and get new values for all predictions.</li>
</ol>
<p style="padding-left: 60px">If we include the real values and the prediction in the same diagram, you should see something similar to the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/21a796f8-d911-490e-a168-808e667456bb.png" style="width:30.00em;height:25.25em;"/></p>
<p>So, the adjusted function of the data points is approximately <em>mpg = 68564/weight</em>.</p>
<p>This adjustment is not precise due to the large dispersion in the y variable, but it could be used as a quick estimation of the fuel consumption, given the weight of the car.</p>
<p>We have explored a number of methods for finding correlated variables. This is useful for understanding which ones are related and which ones are redundant. The next section explains how to use this knowledge to simplify the input to our machine learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Focusing on feature selection</h1>
                </header>
            
            <article>
                
<p>As we mentioned previously, none of these described methods will tell us precisely how to choose the input features by themselves. It is true that in some particular cases, if the correlations are strong enough, we could discard one or more features and just keep the ones that represent them by correlation. In general, feature engineering is a long and time-consuming task that became almost a separate field of study within machine learning. </p>
<p>There are automatic techniques to perform feature engineering, which are part of what is generically called <strong>Automatic Machine Learning</strong> (<strong>AutoML</strong>). The method consists of letting the computer try different feature sets, including combinations of them, and test the results until the best set is found. In spite of this, there is no general recipe for selecting features, and each problem has to be analyzed—in particular, finding the set of features that lead to a better model training and predictive power. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we described the most widely used methods to establish correlations between variables, which will later be used as features in a machine learning model. This is a long and difficult task, but is the basis of a good predictive model.</p>
<p>No method can be used alone to determine which features are important and which can be discarded. A combination of methods, plus a deep knowledge of the dataset, are fundamental to complete this task.</p>
<p>In the next chapter, we will leave the preliminary tasks and start focusing on some real use cases of the machine learning models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What method would be better to find a correlation between a numerical and a categorical variable?</li>
<li>Build some other plot graphs between a pair of variables and study the correlations and the logic behind them.</li>
<li>Does a negative Pearson coefficient value imply that one of the variables has negative values?</li>
<li>The table of the Pearson's coefficient can be colored or have bars added to it in order to better compare the different values. Explore these options in <span class="packt_screen">Quick Analysis</span> | <span class="packt_screen">Formatting</span>.</li>
<li>The quality of the least square regression is usually measured by the value of R<sup>2</sup>. Calculate this value for the function that was adjusted in the <kbd>mpg</kbd> column versus the <kbd>weight</kbd> data value (hint: you only need to calculate one more sum of values – refer to the literature for more information).</li>
<li>The value that was calculated in the previous question should be close to 0.7, which is not good enough to prove that the function reproduces the data well. Try a different function and see what the result is.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li class="a-spacing-none"><span class="a-size-extra-large"><em>Statistics: A Gentle Introduction</em>, written by Frederick L. Coolidge (refer to Chapter 6 and the references within)</span></li>
</ul>


            </article>

            
        </section>
    </body></html>