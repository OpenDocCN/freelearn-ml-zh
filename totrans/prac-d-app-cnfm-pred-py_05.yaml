- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Types of Conformal Predictors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter describes different families of conformal predictors, exploring
    various approaches to quantifying uncertainty. Through practical examples, we
    provide an intermediate-level understanding of these techniques and how they can
    be applied to real-world situations.
  prefs: []
  type: TYPE_NORMAL
- en: Here are examples of how companies are using conformal prediction.
  prefs: []
  type: TYPE_NORMAL
- en: At a high-profile AI developer conference called GTC 2023 ([https://www.nvidia.com/gtc/](https://www.nvidia.com/gtc/)),
    Bill Dally, NVIDIA’s chief scientist and SVP of research, offered insights into
    one of NVIDIA’s R&D primary focuses, which is in conformal prediction ([https://www.hpcwire.com/2023/03/28/whats-stirring-in-nvidias-rd-lab-chief-scientist-bill-dally-provides-a-peek/](https://www.hpcwire.com/2023/03/28/whats-stirring-in-nvidias-rd-lab-chief-scientist-bill-dally-provides-a-peek/)).
  prefs: []
  type: TYPE_NORMAL
- en: Traditional machine learning models for autonomous vehicles output a single
    classification (e.g., pedestrian or no pedestrian on the road) and position estimate
    for detected objects. However, NVIDIA wants to produce a set of potential outputs
    with probabilities; for example, an object could be a pedestrian (80% probability)
    or cyclist (20% probability) at a position of 20 +/- 1 meters.
  prefs: []
  type: TYPE_NORMAL
- en: This allows the vehicle’s planner to guarantee safe actions accounting for multiple
    possible outcomes. Rather than just the most likely label and position, conformal
    prediction provides a range of plausible options, such as “pedestrian at 19–21
    meters” with 80% confidence.
  prefs: []
  type: TYPE_NORMAL
- en: NVIDIA uses a nonconformity function to calculate probabilities that measure
    how strange or different each potential label and position is compared to the
    training data. This generates a multi-modal predictive distribution reflecting
    uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: Conformal prediction gives NVIDIA’s vehicles a reliable way to quantify uncertainty
    and consider multiple interpretations of the environment. By planning for the
    entire set of plausible outcomes rather than just the single most likely one,
    conformal prediction improves robustness and safety.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of machine learning, quantifying uncertainty and providing reliable
    predictions is of significant importance. Conformal prediction is an innovative
    technique that allows us to construct prediction sets (in classification) and
    prediction intervals (in regression), offering a measure of confidence in our
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter aims to provide a deeper understanding of the different types of
    conformal predictors and their respective approaches to quantifying uncertainty.
    Through practical examples, we will illustrate how these techniques can be applied
    to various machine learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Foundations of classical and inductive conformal predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Examining algorithmic descriptions of conformal predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mathematical formulations and practical examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantages and limitations of conformal predictors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidelines for choosing the most suitable conformal predictor for specific problem
    domains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with the classical conformal predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding classical predictors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we deep dive into the intricacies of conformal predictors, let’s briefly
    recap the key concepts from the previous chapters. Conformal prediction is a framework
    that enables creating confidence regions for our predictions while controlling
    the error rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is especially beneficial in situations where a measure of uncertainty
    is essential, such as in medical diagnosis, self-driving cars, or financial risk
    management. The framework encompasses two main types of conformal predictors:
    **classical** and **inductive**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classical transductive conformal prediction** (**TCP**) is the original form
    of conformal prediction developed by the inventors of Conformal prediction. It
    forms the basis for understanding the general principles of conformal predictors.
    Classical Conformal prediction was developed to construct prediction regions that
    conform to a specified confidence level. The critical aspect of classical Conformal
    prediction is its distribution-free nature, meaning it makes no assumptions about
    the data distribution. Thus, it can be applied to any machine learning algorithm,
    making it algorithm-agnostic.'
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the widely used inductive conformal prediction, classical TCP
    does not require a separate calibration set, enabling a more efficient utilization
    of the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, for smaller datasets, it can generate more accurate predictions.
    This approach allows statistical, machine learning, and deep learning models to
    fully capitalize on all available data, potentially leading to more efficient
    (narrower) prediction sets and intervals.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s discuss how classical TCP can be used in classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Applying TCP for classification problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In classification tasks, not only do we seek to assign labels, but we aim to
    do so confidently and accurately. This is where classical TCP shines. As we delve
    into its use in classification, we will cover its unique approach and advantages
    over traditional techniques. Ready to explore? Let’s dive into the nuances of
    TCP for classification!
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapters, we discussed the core concept of the conformal prediction
    framework, which involves assigning a nonconformity measure (or strangeness) to
    each object in the dataset. This measure is utilized to rank the objects within
    the dataset. Subsequently, when predicting a new object, a prediction region is
    created that encompasses values linked to a specific proportion of the dataset
    objects, determined by their strangeness scores. This proportion corresponds to
    the desired confidence level for the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the more popular inductive conformal prediction method, which
    relies on a calibration set to rank objects based on their nonconformity scores,
    classical TCP employs the entire dataset in conjunction with the features of the
    new object to establish the prediction region.
  prefs: []
  type: TYPE_NORMAL
- en: While this approach can be computationally expensive, it allows you to fully
    leverage the whole dataset to capture changes in the data distribution, providing
    more accurate prediction regions.
  prefs: []
  type: TYPE_NORMAL
- en: However, classical conformal prediction has some limitations, too. For instance,
    it may not be feasible for large datasets or real-time applications because it
    requires retraining the underlying point prediction model for each new prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classical conformal prediction is a process that involves several key steps.
    Here, we outline these steps to provide a clear understanding of the procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dataset preparation**: Divide the dataset into training and test sets. The
    training set is used to train the machine learning model, while the test set is
    used to evaluate the performance of the conformal predictor.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model training**: Train the underlying machine learning model using the training
    dataset. This point prediction model will generate point predictions for new objects.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Nonconformity measure calculation**: Define a nonconformity (strangeness)
    measure that quantifies how different an object is from the other objects in the
    dataset. For each object in the training dataset, calculate its nonconformity
    score using the trained model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**New object nonconformity score**: When a new object (without its label) is
    introduced, calculate its nonconformity score using the same nonconformity measure
    and the trained point prediction model.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Ranking**: Based on calculated nonconformity scores, rank all objects, including
    the objects in the training dataset and the new object.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction region**: Determine the desired confidence level for the prediction.
    Identify the proportion of objects in the ranked set corresponding to this confidence
    level. Form a prediction set that includes the values associated with these objects.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s clarify these concepts using a practical example with the hinge loss nonconformity
    measure, which we discussed in the previous chapters. As a reminder, hinge loss
    (also known as inverse probability or LAC loss) is a nonconformity measure calculated
    as *1-P(y|x)*, where *P(y|x)* represents the class score produced by the underlying
    model for the actual class.
  prefs: []
  type: TYPE_NORMAL
- en: The hinge loss nonconformity measure intuitively measures the difference between
    the probability score generated by an ideal classifier for the correct class (which
    should ideally be 1) and the classification score produced by the classifier model.
    It quantifies how far the model’s prediction is from the perfect classification,
    with larger nonconformity scores indicating a more significant discrepancy between
    the ideal and actual predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To compute the inverse probability (hinge) nonconformity score, consider an
    example where your classifier generates two scores: *class_0 = 0.6* and *class_1
    = 0.4,* with the actual label *y=1*. To determine the nonconformity score, subtract
    the probability of the true class (in this case, 0.4 from 1).'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting inverse probability (hinge) nonconformity score is 0.6.
  prefs: []
  type: TYPE_NORMAL
- en: The hinge (inverse probability) score is lower when the underlying machine learning
    classification model performs better. This performance is influenced by a range
    of factors, such as the size and complexity of the dataset, the type of machine
    learning model employed, and the quality of the model’s construction. In other
    words, a well-built model using an appropriate machine learning technique for
    the given dataset will generally yield lower hinge scores, indicating better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: The training process is the critical difference between TCP and **inductive
    conformal prediction** (**ICP**). In ICP, the underlying classifier is trained
    only once on the training set and the calibration of the conformal prediction
    model happens on the calibration dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, with TCP, the classifier is trained by appending each test point
    to the training set twice, each time assigning potential labels 0 and 1\. This
    procedure is repeated for every point in the test set. As a result, the underlying
    classifier model is trained *2 x m* times, where m is the number of points in
    your test set. This may become computationally expensive for large datasets, and
    for such datasets, using ICP might be a more suitable choice.
  prefs: []
  type: TYPE_NORMAL
- en: However, the computational cost is typically manageable for medium and small
    datasets. To obtain potentially better point predictions and narrower probability
    intervals, you might consider TCP, which achieves better prediction intervals
    by training the classifier model *2 x m* times. Many algorithms, such as logistic
    regression, are fast and well suited for this approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall methodology for training TCP remains fundamentally unchanged. The
    TCP algorithm process is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the underlying classifier on the entire training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Append each test point to the training set with each possible class label one
    class label at a time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each appended test point with a postulated label, retrain the classifier
    and compute the nonconformity score for the test point given the postulated label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the p-values for each postulated label, comparing the test point’s
    nonconformity score to the scores of the points in the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each test point and each postulated label, include the postulated label
    in the prediction set if its p-value is greater than or equal to the chosen significance
    level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will illustrate the TCP approach with a practical classification task example
    using the German credit dataset ([https://www.openml.org/d/31](https://www.openml.org/d/31)),
    a classical dataset describing good and bad credit risk based on features such
    as loan duration, credit history, employment, property, age, housing, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the GitHub repo for the book, you will find a notebook ([https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_05_TCP.ipynb](https://github.com/PacktPublishing/Practical-Guide-to-Applied-Conformal-Prediction/blob/main/Chapter_05_TCP.ipynb))
    describing how TCP works that you can work through to understand the key concepts
    of TCP in practice.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – German credit dataset](img/B19925_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – German credit dataset
  prefs: []
  type: TYPE_NORMAL
- en: For clarity, let’s examine the first test point with the original index `30`,
    which has now been appended to the end of the training set. We will use this extended
    training set we created to train the classical transductive conformal predictor.
    This new dataset created using the code in the notebook incorporates all points
    from the original training set and the single test point.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have a feature set to train two classification models: one with an assumed
    label of the test point of 0 and another with an assumed label of 1\. We train
    two models using any classifier (in this case, *Logistic Regression* from scikit-learn)
    and calculate nonconformity scores using the described procedure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Distribution of nonconformity scores](img/B19925_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Distribution of nonconformity scores
  prefs: []
  type: TYPE_NORMAL
- en: From the distribution of nonconformity scores, we observe that the nonconformity
    score for *label 0* (represented by the green vertical line) is relatively typical
    (more conforming) to the training set. In contrast, the nonconformity score for
    the potential *label 1* (represented by the red vertical line) is in a low-density
    probability area.
  prefs: []
  type: TYPE_NORMAL
- en: This suggests that the test object is likelier to be assigned a label of 0,
    while label 1 is less probable. However, conformal prediction is a robust mathematical
    machine learning framework, so we must quantify and statistically test this decision.
    This is where p-values come into play.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a moment to revisit the conventional process of calculating p-values,
    which we previously explored in [*Chapter 3*](B19925_03.xhtml#_idTextAnchor033),
    using the formula from Vovk’s book *Algorithmic Learning in a* *Random World*.
  prefs: []
  type: TYPE_NORMAL
- en: 'p-values can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'p = (|z i : α i ≥ : α T| + 1) / (n + 1)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the nonconformity score of a new test point is compared with the nonconformity
    scores of points in the training set. Essentially, the nonconformity score quantifies
    the *strangeness* or novelty of the new test object compared to the previously
    encountered objects in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: According to the formula, what we need to do is to check (for each test point
    and each potential value of label 0 and 1) how many objects in the set of training
    data appended with the test point using the postulated label have nonconformity
    values that are larger or equal to the nonconformity score of the test point.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then divide it by the number of training points *(n+1)* (+1 accounts for
    the test point that we appended to the training set). As a result, we obtain two
    p-values for each test point: one for class 0 and one for class 1.'
  prefs: []
  type: TYPE_NORMAL
- en: The central concept of conformal prediction revolves around utilizing nonconformity
    values for each test point to evaluate how well it aligns with the training set.
    By computing p-values based on this evaluation, we can conduct robust statistical
    tests to determine if each potential label value should be included in the prediction
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we have a postulated label (either 0 or 1). If there are sufficient
    instances in the training set with nonconformity values equal to or greater than
    the test point’s nonconformity value, then we infer that this postulated label
    aligns well with the observed data. As a result, we incorporate this label into
    our prediction set. Conversely, if the postulated label does not correspond well
    with the observed data, we refrain from including it in the prediction set.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, this procedure echoes the principles of statistical hypothesis testing.
    For each hypothesized label value, we establish a null hypothesis, asserting that
    the label could be part of the prediction set if its associated p-value exceeds
    a pre-defined significance level. If the p-value falls short of this threshold,
    we discard the null hypothesis. This implies that the proposed label doesn’t adequately
    match the pattern found in the training data, leading us to exclude it from our
    prediction set.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say we have calculated two p-values for the first test object:'
  prefs: []
  type: TYPE_NORMAL
- en: Assume for label 0 that the p-value is 0.55\. Since the p-value is larger than
    the significance level (0.05), we include the hypothesized label (0 in this case)
    in the prediction set for this test point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now assume for label 1 that the p-value is 0.002\. Since the p-value is smaller
    than the significance level (0.05), we cannot include the hypothesized label (1
    in this case) in the prediction set for this test point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Thus, the final prediction set for this point is 0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the context of TCP, the key distinction between binary and multiclass classification
    lies in the number of potential labels taken into account for each test point.
    In a binary classification scenario, only two labels exist (0 and 1). In contrast,
    multiclass classification involves a greater number of classes (for instance,
    C1, C2, C3, ...).
  prefs: []
  type: TYPE_NORMAL
- en: The main difference from the binary classification is that we will have to repeat
    the process for each possible class label, increasing computational complexity
    as you need to retrain the classifier for each test point and each potential label.
    However, the overall method for obtaining the prediction set remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: After delving into the nuances of TCP for classification, let’s pivot our focus.
    Next up, we’ll explore the intricacies of employing TCP in regression contexts.
    This approach offers unique challenges and benefits, so let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Applying TCP for regression problems
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TCP can also be applied to regression problems. The process for TCP in regression
    is similar to the one used for classification, with some differences in computing
    nonconformity scores and prediction intervals. Here is an algorithmic description
    of the TCP for regression problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the underlying regression model on the entire training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each test point, create a grid of potential target values. The granularity
    of this grid depends on the desired precision and the problem’s nature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each test point and each potential target value on the grid, append the
    test point to the training set with the associated target value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each appended test point with a postulated target value, retrain the regression
    model and compute the nonconformity score for the given postulated target value.
    The nonconformity score can be computed as the absolute difference between the
    predicted value and the true value of the appended point, or you can calculate
    it by using other error metrics such as **mean squared** **error** (**MSE**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the p-values for each postulated target value by comparing the test
    point’s nonconformity score for each value on the grid of potential target values
    to the scores of the points in the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each test point, include the postulated target value in the prediction interval
    if its p-value is greater than or equal to the chosen significance level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The prediction set for a regression problem will be an interval rather than
    a set of discrete labels, as in classification. The main difference from the classification
    is that you will have to repeat the process for each potential target value in
    the grid, which could increase computational complexity. However, the overall
    method for obtaining the prediction interval remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: We conclude the section about TCP by summarizing the advantages and limitations
    of TCP.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TCP has several advantages over alternative methods of uncertainty quantification:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Distribution-free**: Transductive conformal predictors do not make any assumptions
    about the distribution of the data, making them suitable for various types of
    data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validity**: They provide prediction intervals with a guaranteed coverage
    probability, allowing for a reliable measure of uncertainty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptability**: Conformal predictors can be applied to various machine learning
    models, making them versatile and easily adaptable to different settings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better prediction intervals**: Transductive conformal predictors generally
    produce more precise prediction intervals compared to inductive conformal predictors
    since they fully utilize the dataset for training the underlying point prediction
    model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But there are a few limitations as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational expense**: TCP requires retraining the model for each test
    point and for each potential class label (in classification) or each potential
    target value on the grid regression, making it computationally expensive, particularly
    for large datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Not ideal for online learning**: Due to the computational expense, transductive
    conformal predictors are not well suited for online learning scenarios where models
    must be continuously updated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity**: Implementing transductive conformal predictors can be more
    complicated than traditional machine learning models, potentially posing a barrier
    to widespread adoption'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transductive conformal predictors offer several advantages in providing reliable,
    distribution-free prediction intervals. However, their computational expense and
    scalability limitations should be considered, particularly for large-scale or
    online learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building upon our exploration of TCP, it’s time to turn our attention to another
    intriguing variant: inductive conformal predictors. Differing from its classical
    counterpart in key ways, this approach brings a new set of strategies and benefits
    to the table. Ready to delve into the mechanics and merits of inductive conformal
    predictors? Let’s embark on this enlightening journey!'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding inductive conformal predictors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ICP is a variant of conformal prediction that provides valid predictive regions
    under the same assumptions as classical conformal prediction and has the added
    benefit of improved computational efficiency, which is particularly useful when
    dealing with large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: ICPs present a highly efficient and effective solution within the realm of machine
    learning. They provide a form of conformal prediction that caters to larger datasets,
    making it highly suitable for real-world applications that involve extensive data
    volumes. ICPs divide the dataset into training and calibration sets during the
    model-building process. The training set is used to develop the model, while the
    calibration set helps calculate the nonconformity scores. This two-step process
    optimizes computation and delivers precise prediction regions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Inductive conformal prediction](img/B19925_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Inductive conformal prediction
  prefs: []
  type: TYPE_NORMAL
- en: A predictive model, such as a neural network or a decision tree, is first trained
    on the proper training set. Then, the nonconformity of each example in the calibration
    set is computed using the trained model. The nonconformity measure is a real-valued
    function that describes how much an example contradicts the rest of the data.
    The nonconformity scores of the calibration set are then used to determine the
    size of the prediction region for new examples.
  prefs: []
  type: TYPE_NORMAL
- en: The inductive approach offers a significant computational advantage, particularly
    for large datasets. By creating the predictive model only once, ICP reduces the
    algorithm’s time complexity, unlike TCP, which requires retraining the model for
    each new prediction. However, it’s important to note that ICP assumes the data
    are exchangeable, meaning the data’s order doesn’t carry any information.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of applications, inductive conformal predictors can be used for both
    classification (binary and multiclass) and regression tasks. The method offers
    a flexible and efficient way of providing a measure of uncertainty associated
    with predictions, a valuable feature in many practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'ICP involves several steps, most of which center around the calculation of
    nonconformity scores. Here’s a rough outline of the algorithm along with the associated
    mathematical formulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data partitioning**: Split the initial dataset *D* into a proper training
    set *D_train*, and a calibration set *D_cal*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model training**: Train a predictive model *M* on *D_train*. This model is
    used to generate predictions on new instances. The type of model (e.g., SVM, decision
    tree, linear regression, etc.) depends on the problem at hand.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Nonconformity measure calculation**: Use the trained model *M* to predict
    outcomes for instances in the calibration set *D_cal*. For each instance of, *(x_i,
    y_i)* in *D_cal*, compute a nonconformity score *α_i*, representing the *strangeness*
    or *abnormality* of the instance. The nonconformity measure *α* is generally problem-specific.
    For instance, classification tasks could be hinge loss *1 - p_yi,* where *p_yi*
    is the predicted probability of the correct class *y_i* according to the model
    *M*. For regression, it could be the absolute error *|y_i - y_hat_i|*, where *y_hat_i*
    is the model’s prediction for *x_i*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`|{i: α_i ≥ α_x}|` denotes the number of instances in the calibration set with
    nonconformity scores greater than or equal to *α_x*. This p-value represents how
    often we expect to observe a nonconformity score at least as large as *α_x* by
    chance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prediction output**: Using the calculated p-value, create a prediction set
    *Γ(x)* for the new test point *x*. For classification, the prediction set contains
    all classes *y* for which the p-value is at least the chosen significance level
    *ε*: Γ(x) = {y: p_y ≥ ε}. For regression, an interval prediction (*y_lower,* *y_upper*)
    is typically outputted, where *y_lower* and *y_upper* are the lowest and highest
    values, respectively, for which the p-value is at least as large as the chosen
    significant level.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please note that this is a high-level description of the algorithm and mathematical
    formulation. The exact details may vary based on the specific form of ICP used
    and the type of problem (classification, regression, etc.) being addressed.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve unpacked the complexities and capabilities of both classical and inductive
    approaches, it’s now essential to discern how to choose the optimal method for
    a given situation. Let’s navigate the factors and guidelines that will guide you
    in selecting the best-fit conformal predictor for your specific needs in the upcoming
    section, *Choosing the right* *conformal predictor*.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right conformal predictor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both classical and inductive conformal predictors offer valuable approaches
    to building reliable machine learning models. However, they each come with unique
    strengths and weaknesses.
  prefs: []
  type: TYPE_NORMAL
- en: Classical transductive conformal predictors are highly adaptable and do not
    make any assumptions about data distribution. However, they tend to be computationally
    expensive, requiring the model’s retraining for each new prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Inductive conformal predictors, conversely, are computationally more efficient,
    as they only require the model to be trained once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing the right conformal predictor largely depends on the specific requirements
    of the problem at hand. Some considerations might include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computation resources**: If computation resources or time are a concern,
    inductive conformal predictors might be more suitable due to their reduced computational
    cost'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data size**: For smaller datasets, classical conformal predictors might be
    more suitable, while for larger datasets, inductive conformal predictors are usually
    the preferred choice due to computational efficiency'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality**: If data quality is high, inductive conformal predictors can
    be a good choice'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time requirements**: If the model needs to make real-time predictions,
    inductive conformal predictors might be more suitable due to their one-time training
    process'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here are real-life scenarios illustrating when one might opt for transductive
    or inductive conformal predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Transductive conformal predictors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'medical diagnostics with limited data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario**: A hospital uses machine learning to diagnose a rare disease but
    only has a limited dataset of past patients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: Given the smaller dataset and the critical nature of accurate
    predictions, classical TCP is favored. Its adaptability and distribution-free
    nature might lead to more accurate predictions, even if it requires more computational
    power per prediction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inductive conformal predictors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'e-commerce recommendation systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario**: A large e-commerce platform wants to provide real-time product
    recommendations to millions of its users based on their browsing habits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reasoning**: Due to the massive scale, the system can’t afford to retrain
    models for every recommendation. ICP’s one-time training process, combined with
    its computational efficiency for larger datasets, makes it a suitable choice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To effectively choose the appropriate type of conformal predictor, it’s essential
    to gain a deep understanding of both classical and inductive conformal predictors,
    their working principles, and their strengths and weaknesses. Furthermore, understanding
    the nature and requirements of the problem domain, such as the specific characteristics
    of the data, the computational resources available, the need for real-time predictions,
    and the importance of model interpretability, can significantly aid in making
    an informed choice. Always remember that the best conformal predictor is the one
    that best meets the needs of your specific problem domain.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored the fascinating world of conformal predictors, their types,
    and their distinctive features. The key concepts and skills we touched upon include
    covering the foundational principles of conformal prediction and its application
    in machine learning. It also highlighted the differences between classical transductive
    and inductive conformal predictors. We also covered how to effectively choose
    the appropriate type of conformal predictor based on the specific requirements
    of the problem. Finally, the practical applications of conformal predictors in
    binary classification, multiclass classification, and regression were also included.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter also provided a detailed algorithmic description and mathematical
    formulation of classical and inductive conformal predictors, adding to our theoretical
    understanding. To deepen our learning, we also took a hands-on approach, looking
    at practical examples in Python.
  prefs: []
  type: TYPE_NORMAL
- en: For those interested in further exploring conformal predictors, several avenues
    exist for you to consider. A more detailed study of the mathematical underpinnings
    of conformal prediction could be pursued, along with implementing conformal predictors
    in more complex machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the advanced versions of conformal predictors, such as Mondrian conformal
    predictors, or understanding how conformal prediction can be integrated with other
    machine learning techniques, such as neural networks and ensemble learning, are
    also exciting areas for further research.
  prefs: []
  type: TYPE_NORMAL
- en: In closing, we hope this chapter has given a solid grounding in the principles
    and applications of conformal prediction. Moving into the next chapter, we’ll
    delve deeper into conformal prediction for classification problems. As always,
    keep exploring, keep learning, and enjoy the journey!
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Applications of Conformal Prediction'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will provide more details about conformal prediction for classification
    problems. It will introduce the calibration concept and illustrate how conformal
    prediction compares with other calibration methods, explaining how it can quantify
    uncertainty in regression to produce well-calibrated prediction intervals. This
    part will also explain how conformal prediction can produce prediction intervals
    for point forecasting models, illustrate applications using open source libraries,
    and detail recent innovations in conformal prediction for NLP. Finally, this part
    will explain how conformal prediction can be applied to produce state-of-the-art
    uncertainty quantification for NLP and illustrate applications using open source
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B19925_06.xhtml#_idTextAnchor058), *Conformal Prediction for
    Classification*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B19925_07.xhtml#_idTextAnchor073), *Conformal Prediction for
    Regression*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B19925_08.xhtml#_idTextAnchor090), *Conformal Prediction for
    Time Series and Forecasting*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B19925_09.xhtml#_idTextAnchor111), *Conformal Prediction for
    Computer Vision*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19925_10.xhtml#_idTextAnchor130), *Conformal Prediction for
    Natural Language Processing*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
