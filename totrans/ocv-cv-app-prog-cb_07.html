<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07" class="calibre1"/>Chapter 7. Extracting Lines, Contours, and Components</h1></div></div></div><p class="calibre8">In this chapter, we will cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Detecting image contours with the Canny operator</li><li class="listitem">Detecting lines in images with the Hough transform</li><li class="listitem">Fitting a line to a set of points</li><li class="listitem">Extracting the components' contours</li><li class="listitem">Computing components' shape descriptors</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch07lvl1sec46" class="calibre1"/>Introduction</h1></div></div></div><p class="calibre8">In order to perform content-based analysis of an image, it is necessary to extract meaningful features from the collection of pixels that constitute the image. Contours, lines, blobs, and so on, are fundamental image primitives that can be used to describe the elements contained in an image. This chapter will teach you how to extract some of these important image features.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec47" class="calibre1"/>Detecting image contours with the Canny operator</h1></div></div></div><p class="calibre8">In the <a id="id580" class="calibre1"/>previous chapter, we learned how it is possible to detect the edges of an image. In particular, we showed you that by applying a threshold on the gradient magnitude, a binary map<a id="id581" class="calibre1"/> of the main edges of an image can be obtained. Edges carry important visual information since they delineate the image elements. For this reason, they can be used, for example, in object recognition. However, simple binary edge maps suffer from two main drawbacks. First, the edges that are detected are unnecessarily thick; this makes the object's limit more difficult to identify. Second, and more importantly, it is often impossible to find a threshold that is sufficiently low in order to detect all important edges of an image and is, at the same time, sufficiently high in order to not include too many insignificant edges. This is a trade-off problem that the <a id="id582" class="calibre1"/>
<strong class="calibre2">Canny</strong> algorithm tries to solve.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec134" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The Canny algorithm<a id="id583" class="calibre1"/> is implemented in OpenCV by the <a id="id584" class="calibre1"/>
<code class="email">cv::Canny</code> function. As will be explained, this algorithm requires the specification of two thresholds. The call to the function is, therefore, as follows:</p><div><pre class="programlisting">   // Apply Canny algorithm
   cv::Mat contours;
   cv::Canny(image,    // gray-level image
             contours, // output contours
             125,      // low threshold
             350);     // high threshold</pre></div><p class="calibre8">Take a look at the following screenshot:</p><div><img src="img/00103.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">When the algorithm is applied on the preceding screenshot, the result is as follows:</p><div><img src="img/00104.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Note <a id="id585" class="calibre1"/>that in order to obtain an image like the one shown in the preceding screenshot, we had to invert the black and white values since the normal result represents contours by nonzero pixels. The displayed image, then, is simply 255-contours.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec135" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">The Canny operator <a id="id586" class="calibre1"/>is generally based on the Sobel operator that was presented in <a class="calibre1" title="Chapter 6. Filtering the Images" href="part0047_split_000.html#page">Chapter 6</a>, <em class="calibre9">Filtering the Images</em>, although other gradient operators can also be used. The key idea here is to use two different thresholds in order to determine which point should belong to a contour: a low and a high threshold.</p><p class="calibre8">The low threshold should be chosen in a way that it includes all edge pixels that are considered to belong to a significant image contour. For example, using the low-threshold value specified in the example of the preceding section and applying it on the result of a Sobel operator, the following edge map is obtained:</p><div><img src="img/00105.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As can be seen, the edges that delineate the road are very well defined. However, because a<a id="id587" class="calibre1"/> permissive threshold was used, more edges than what is ideally needed are also detected. The role of the second threshold, then, is to define the edges that belong to all important contours. It should exclude all edges considered as outliers. For example, the Sobel edge map that corresponds to the high threshold used in our example is as follows:</p><div><img src="img/00106.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We now have an image that contains broken edges, but the ones that are visible certainly belong to the <a id="id588" class="calibre1"/>significant contours of the scene. The Canny algorithm combines these two edge maps in order to produce an <em class="calibre9">optimal</em> map of contours. It operates by keeping only the edge points of the low-threshold edge map for which a continuous path of edges exists, linking those edge points to an edge that belongs to the high-threshold edge map. Consequently, all edge points of the high-threshold map are kept, while all isolated chains of edge points in the low-threshold map are removed. The solution that is obtained constitutes a good compromise, allowing good quality contours to be obtained as long as appropriate threshold values are specified. This strategy, based on the use of two thresholds to obtain a binary map, is called <strong class="calibre2">hysteresis thresholding</strong><a id="id589" class="calibre1"/>, and can be used in any context where a binary map needs to be obtained from a thresholding operation. However, this is done at the cost of higher computational complexity.</p><p class="calibre8">In addition, the Canny algorithm uses an extra strategy to improve the quality of the edge map. Prior to the application of the hysteresis thresholding, all edge points for which the gradient magnitude is not a maximum in the gradient direction are removed. Recall that the gradient orientation is always perpendicular to the edge. Therefore, the local maximum of the gradient<a id="id590" class="calibre1"/> in this direction corresponds to the point of maximum strength of the contour. This explains why thin edges are obtained in the Canny contour maps.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec136" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The classic article by J. Canny, <em class="calibre9">A computational approach to edge detection, IEEE Transactions on Pattern Analysis and Image Understanding, vol. 18, issue 6, 1986</em></li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec48" class="calibre1"/>Detecting lines in images with the Hough transform</h1></div></div></div><p class="calibre8">In our human-made world, planar and linear structures abound. As a result, straight lines are frequently visible in <a id="id591" class="calibre1"/>images. These<a id="id592" class="calibre1"/> are meaningful features that play an important role in object recognition and image understanding. The <strong class="calibre2">Hough transform</strong><a id="id593" class="calibre1"/> is a classic algorithm that is often used to detect these particular features in images. It was initially developed to detect lines in images and, as we will see, it can also be extended to detect other simple image structures.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec137" class="calibre1"/>Getting ready</h2></div></div></div><p class="calibre8">With the Hough transform, lines are represented using the following equation:</p><div><img src="img/00107.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The ρ parameter<a id="id594" class="calibre1"/> is the distance between the line and the image origin (the upper-left corner), and θ<a id="id595" class="calibre1"/> is the angle of the perpendicular to the line. Under this representation, the lines visible in an image have a <em class="calibre9">θ</em> angle between <em class="calibre9">0</em> and <em class="calibre9">π</em> radians, while the <em class="calibre9">ρ</em> radius can have a maximum value that equals the length of the image diagonal. Consider, for example, the following set of lines:</p><div><img src="img/00108.jpeg" alt="Getting ready" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">A vertical line such as line <strong class="calibre2">1</strong> has a <em class="calibre9">θ</em> angle value equal to zero, while a horizontal line (for example, line <strong class="calibre2">5</strong>) has its <em class="calibre9">θ</em> value equal to <em class="calibre9">π/2</em>. Therefore, line <strong class="calibre2">3</strong> has an angle <em class="calibre9">θ</em> equal to <em class="calibre9">π/4</em>, and line <strong class="calibre2">4</strong> is at <em class="calibre9">0.7π</em> approximately. In order to be able to represent all possible lines with <em class="calibre9">θ</em> in the <em class="calibre9">[0, π]</em> interval, the radius value can be made negative. This is the case of line <strong class="calibre2">2</strong>, which has a θ value equal to <em class="calibre9">0.8π</em> with a negative value for <em class="calibre9">ρ</em>.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec138" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">OpenCV offers<a id="id596" class="calibre1"/> two implementations of the Hough transform for line detection. The basic version is <code class="email">cv::HoughLines</code>. Its input<a id="id597" class="calibre1"/> is a binary map that contains a set of points (represented by nonzero pixels), some of which are aligned to form lines. Usually, this is an edge map obtained, for example, from the Canny operator. The output of the <code class="email">cv::HoughLines</code> function<a id="id598" class="calibre1"/> is a vector of the <code class="email">cv::Vec2f</code> elements, each of them being a pair of floating point values, which represents the parameters of a detected line, <em class="calibre9">(ρ, θ)</em>. The following is an example of using this function where we first apply the Canny operator to obtain the image contours and then detect the lines using the Hough transform:</p><div><pre class="programlisting">   // Apply Canny algorithm
   cv::Mat contours;
   cv::Canny(image,contours,125,350);
   // Hough transform for line detection
   std::vector&lt;cv::Vec2f&gt; lines;
   cv::HoughLines(test,lines,
        1,PI/180,  // step size
        60);       // minimum number of votes</pre></div><p class="calibre8">Parameters 3 and 4 correspond to the step size for the line search. In our example, the function will search for lines of all possible radii by steps of <code class="email">1</code> and all possible angles by steps of <em class="calibre9">π/180</em>. The role of the last parameter will be explained in the next section. With this particular choice of parameter values, 15 lines are detected on the road image of the preceding recipe. In order to visualize the result of the detection, it is interesting to draw these lines on the <a id="id599" class="calibre1"/>original image. However, it is <a id="id600" class="calibre1"/>important to note that this algorithm detects lines in an image and not line segments, since the endpoints of each line are not given. Consequently, we will draw lines that traverse the entire image. To do this, for a vertically-oriented line, we calculate its intersection with the horizontal limits of the image (that is, the first and last rows) and draw a line between these two points. We proceed similarly with horizontally-oriented lines but using the first and last columns. Lines are drawn using the <a id="id601" class="calibre1"/>
<code class="email">cv::line</code> function. Note that this function works well even with point coordinates outside the image limits. Therefore, there is no need to check whether the computed intersection points fall within the image. Lines are then drawn by iterating over the line vector as follows:</p><div><pre class="programlisting">   std::vector&lt;cv::Vec2f&gt;::const_iterator it= lines.begin();
   while (it!=lines.end()) {

      float rho= (*it)[0];   // first element is distance rho
      float theta= (*it)[1]; // second element is angle theta
      
      if (theta &lt; PI/4. 
           || theta &gt; 3.*PI/4.) { // ~vertical line
      
         // point of intersection of the line with first row
         cv::Point pt1(rho/cos(theta),0);        
         // point of intersection of the line with last row
         cv::Point pt2((rho-result.rows*sin(theta))/
                                  cos(theta),result.rows);
         // draw a white line
         cv::line( image, pt1, pt2, cv::Scalar(255), 1); 

      } else { // ~horizontal line

         // point of intersection of the 
         // line with first column
         cv::Point pt1(0,rho/sin(theta));        
         // point of intersection of the line with last column
         cv::Point pt2(result.cols,
                 (rho-result.cols*cos(theta))/sin(theta));
         // draw a white line
         cv::line(image, pt1, pt2, cv::Scalar(255), 1); 
      }

      ++it;
   }</pre></div><p class="calibre8">The following result is obtained:</p><div><img src="img/00109.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As can be seen, the Hough transform simply looks for an alignment of edge pixels across the image. This<a id="id602" class="calibre1"/> can potentially create some false detections due to incidental pixel alignments or multiple detections when several lines pass through the same alignment of pixels.</p><p class="calibre8">To overcome some <a id="id603" class="calibre1"/>of these problems, and to allow line segments to be detected (that is, with endpoints), a variant of the transform has been proposed. This is the Probabilistic Hough transform, and it is implemented in OpenCV as the<a id="id604" class="calibre1"/> <code class="email">cv::HoughLinesP</code> function. We use it here to create our <code class="email">LineFinder</code> class that encapsulates the function parameters:</p><div><pre class="programlisting">class LineFinder {

  private:

     // original image
     cv::Mat img;

     // vector containing the endpoints 
     // of the detected lines
     std::vector&lt;cv::Vec4i&gt; lines;

     // accumulator resolution parameters
     double deltaRho;
     double deltaTheta;

     // minimum number of votes that a line 
     // must receive before being considered
     int minVote;

     // min length for a line
     double minLength;

     // max allowed gap along the line
     double maxGap;

  public:

     // Default accumulator resolution is 1 pixel by 1 degree
     // no gap, no minimum length
     LineFinder() : deltaRho(1), deltaTheta(PI/180), 
                    minVote(10), minLength(0.), maxGap(0.) {}</pre></div><p class="calibre8">Take a<a id="id605" class="calibre1"/> look <a id="id606" class="calibre1"/>at the corresponding setter methods:</p><div><pre class="programlisting">     // Set the resolution of the accumulator
     void setAccResolution(double dRho, double dTheta) {

        deltaRho= dRho;
        deltaTheta= dTheta;
     }

     // Set the minimum number of votes
     void setMinVote(int minv) {

        minVote= minv;
     }

     // Set line length and gap
     void setLineLengthAndGap(double length, double gap) {

        minLength= length;
        maxGap= gap;
     }</pre></div><p class="calibre8">With the preceding method, the method that performs Hough line segment detection is as follows:</p><div><pre class="programlisting">     // Apply probabilistic Hough Transform
     std::vector&lt;cv::Vec4i&gt; findLines(cv::Mat&amp; binary) {

        lines.clear();
        cv::HoughLinesP(binary,lines,
                        deltaRho, deltaTheta, minVote, 
                        minLength, maxGap);

        return lines;
     }</pre></div><p class="calibre8">This <a id="id607" class="calibre1"/>method returns a vector of <code class="email">cv::Vec4i</code>, <a id="id608" class="calibre1"/>which contains the start and endpoint coordinates of each detected segment. The detected lines can then be drawn on an image with the following method:</p><div><pre class="programlisting">     // Draw the detected lines on an image
     void drawDetectedLines(cv::Mat &amp;image, 
                cv::Scalar color=cv::Scalar(255,255,255)) {
   
        // Draw the lines
        std::vector&lt;cv::Vec4i&gt;::const_iterator it2= 
                                           lines.begin();
   
        while (it2!=lines.end()) {
      
           cv::Point pt1((*it2)[0],(*it2)[1]);        
           cv::Point pt2((*it2)[2],(*it2)[3]);

           cv::line( image, pt1, pt2, color);
      
           ++it2;   
        }
     }</pre></div><p class="calibre8">Now, using the same input image, lines can be detected with the following sequence:</p><div><pre class="programlisting">   // Create LineFinder instance
   LineFinder finder;

   // Set probabilistic Hough parameters
   finder.setLineLengthAndGap(100,20);
   finder.setMinVote(60);

   // Detect lines and draw them
   std::vector&lt;cv::Vec4i&gt; lines= finder.findLines(contours);
   finder.drawDetectedLines(image);
   cv::namedWindow("Detected Lines with HoughP");
   cv::imshow("Detected Lines with HoughP",image);</pre></div><p class="calibre8">The <a id="id609" class="calibre1"/>preceding <a id="id610" class="calibre1"/>code gives the following result:</p><div><img src="img/00110.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec139" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">The objective of the Hough transform is to find all lines in a binary image that pass through a sufficient number of points. It proceeds by considering each individual pixel point in the input binary map and identifying all possible lines that pass through it. When the same line passes through many points, it means that this line is significant enough to be considered.</p><p class="calibre8">The Hough transform uses a two-dimensional accumulator in order to count how many times a given line is identified. The size of this accumulator is defined by the specified step sizes (as mentioned in the preceding section) of the <em class="calibre9">(ρ, θ)</em> parameters of the adopted line representation. To illustrate the functioning of the transform, let's create a 180 by 200 matrix (corresponding to a step size of <em class="calibre9">π/180</em> for <em class="calibre9">θ</em> and <em class="calibre9">1</em> for <em class="calibre9">ρ</em>):</p><div><pre class="programlisting">   // Create a Hough accumulator
   // here a uchar image; in practice should be ints
   cv::Mat acc(200,180,CV_8U,cv::Scalar(0));</pre></div><p class="calibre8">This accumulator is a<a id="id611" class="calibre1"/> mapping of different <em class="calibre9">(ρ, θ)</em> values. Therefore, each entry of this matrix corresponds to one particular line. Now, if we <a id="id612" class="calibre1"/>consider one point, let's say one at coordinate <code class="email">(50,30)</code>, then it is possible to identify all lines that pass through this point by looping over all possible <em class="calibre9">θ</em> angles (with a step size of π/180) and computing the corresponding (rounded) <em class="calibre9">ρ</em> value:</p><div><pre class="programlisting">   // Choose a point
   int x=50, y=30;
   // loop over all angles
   for (int i=0; i&lt;180; i++) {

      double theta= i*PI/180.;

      // find corresponding rho value 
      double rho= x*std::cos(theta)+y*std::sin(theta);
      // j corresponds to rho from -100 to 100
      int j= static_cast&lt;int&gt;(rho+100.5);

      std::cout &lt;&lt; i &lt;&lt; "," &lt;&lt; j &lt;&lt; std::endl;

      // increment accumulator
      acc.at&lt;uchar&gt;(j,i)++;
   }</pre></div><p class="calibre8">The entries of the accumulator corresponding to the computed <em class="calibre9">(ρ, θ)</em> pairs are then incremented, signifying that all of these lines pass through one point of the image (or, to say it another way, each point votes for a set of possible candidate lines). If we display the accumulator as an image (inverted and multiplied by <code class="email">100</code> to make the count of <code class="email">1</code> visible), we obtain the following:</p><div><img src="img/00111.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The preceding <a id="id613" class="calibre1"/>curve represents the set of all <a id="id614" class="calibre1"/>lines that pass through the considered point. Now, if we repeat the same exercise with, let's say, point <code class="email">(30,10)</code>, we now have the following accumulator:</p><div><img src="img/00112.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As can be seen, the two resulting curves intersect at one point: the point that corresponds to the line that passes through these two points. The corresponding entry of the accumulator receives two votes, indicating that two points pass through this line. If the same process is repeated for all points of a binary map, then points aligned along a given line will increase a common entry of the accumulator many times. At the end, you just need to identify the local maxima in this accumulator that receives a significant number of votes in order to detect the lines (that is, point alignments) in the image. The last parameter specified in the <code class="email">cv::HoughLines</code> function corresponds to the minimum number of votes that a line must receive to be considered as detected. For example, we lower this value at <code class="email">50</code>, as follows:</p><div><pre class="programlisting">   cv::HoughLines(test,lines,1,PI/180,50);</pre></div><p class="calibre8">As a result <a id="id615" class="calibre1"/>of the previous code, more lines <a id="id616" class="calibre1"/>will be accepted for the example of the preceding section, as shown in the following screenshot:</p><div><img src="img/00113.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The Probabilistic Hough transform adds a few modifications to the basic algorithm. First, instead of systematically scanning the image row-by-row, points are chosen in random order in the binary map. Whenever an entry of the accumulator reaches the specified minimum value, the image is scanned along the corresponding line and all points that pass through it are removed (even if they have not voted yet). This scanning also determines the length of the segments that will be accepted. For this, the algorithm defines two additional parameters. One is the minimum length for a segment to be accepted, and the other is the maximum pixel gap that is permitted to form a continuous segment. This additional step increases the complexity of the algorithm, but this is partly compensated by the fact that fewer points will be involved in the voting process as some of them are eliminated by the line-scanning process.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch07lvl2sec140" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">The Hough transform can also be used to detect other geometrical entities. In fact, any entity that can be represented by a parametric equation is a good candidate for the Hough transform.</p><div><div><div><div><h3 class="title2"><a id="ch07lvl3sec33" class="calibre1"/>Detecting circles</h3></div></div></div><p class="calibre8">In the <a id="id617" class="calibre1"/>case of circles, the corresponding parametric equation is as follows:</p><div><img src="img/00114.jpeg" alt="Detecting circles" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">This equation includes three parameters (the circle radius and center coordinates), which means that a three-dimensional accumulator would be required. However, it is generally found that the Hough transform becomes less reliable as the dimensionality of its accumulator increases. Indeed, in this case, a large number of entries of the accumulator will be incremented for each point and, as a consequence, the accurate localization of local peaks becomes more difficult. Different strategies have been proposed in order to overcome this problem. The strategy used in the OpenCV implementation of the Hough circle detection uses two passes. During the first pass, a two-dimensional accumulator is used to find candidate circle locations. Since the gradient of points on the circumference of a circle should point in the direction of the radius, for each point, only the entries in the accumulator along the gradient direction are incremented (based on predefined minimum and maximum radius values). Once a possible circle center is detected (that is, has received a predefined number of votes), a 1D histogram of a possible radius is built during the second pass. The peak value in this histogram corresponds to the radius of the detected circles.</p><p class="calibre8">The <code class="email">cv::HoughCircles</code> function<a id="id618" class="calibre1"/> that implements the preceding strategy integrates both the Canny detection and the Hough transform. It is called as follows:</p><div><pre class="programlisting">   cv::GaussianBlur(image,image,cv::Size(5,5),1.5);
   std::vector&lt;cv::Vec3f&gt; circles;
   cv::HoughCircles(image, circles, CV_HOUGH_GRADIENT, 
      2,   // accumulator resolution (size of the image / 2) 
      50,  // minimum distance between two circles
      200, // Canny high threshold 
      100, // minimum number of votes 
      25, 100); // min and max radius</pre></div><p class="calibre8">Note that it is always recommended that you smooth the image before calling the <code class="email">cv::HoughCircles</code> function in order to reduce the image noise that could cause several false circle detections. The result of the detection is given in a vector of <code class="email">cv::Vec3f</code> instances. The first two values are the circle center coordinates and the third is the radius.</p><p class="calibre8">The <code class="email">CV_HOUGH_GRADIENT</code> argument was the only option available at the time of writing this book. It corresponds to the two-pass circle detection method. The fourth parameter defines the accumulator resolution. It is a divider factor; specifying a value of 2, for example, makes the accumulator half the size of the image. The next parameter is the minimum <a id="id619" class="calibre1"/>distance in pixels between two detected circles. The other parameter corresponds to the high threshold of the Canny edge detector. The low-threshold value is always set at half this value. The seventh parameter is the minimum number of votes that a center location must receive during the first pass to be considered as a candidate circle for the second pass. Finally, the last two parameters are the minimum and maximum radius values for the circles to be detected. As can be seen, the function includes many parameters that make it difficult to tune.</p><p class="calibre8">Once the vector of detected circles is obtained, these circles can be drawn on the image by iterating over the vector and calling<a id="id620" class="calibre1"/> the <code class="email">cv::circle</code> drawing function with the found parameters:</p><div><pre class="programlisting">   std::vector&lt;cv::Vec3f&gt;::
          const_iterator itc= circles.begin();
   
   while (itc!=circles.end()) {
      
     cv::circle(image, 
        cv::Point((*itc)[0], (*itc)[1]), // circle centre
        (*itc)[2],       // circle radius
        cv::Scalar(255), // color 
        2);              // thickness
      
     ++itc;   
   }</pre></div><p class="calibre8">The following is the result obtained on a test image with the chosen arguments:</p><div><img src="img/00115.jpeg" alt="Detecting circles" class="calibre10"/></div><p class="calibre11"> </p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch07lvl2sec141" class="calibre1"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem">The article <em class="calibre9">Gradient-based Progressive Probabilistic Hough Transform by C. Galambos, J. Kittler, and J. Matas, IEE Vision Image and Signal Processing, vol. 148 no 3, pp. 158-165, 2002</em>, is one of the numerous references on the Hough transform and describes the probabilistic algorithm implemented in OpenCV</li><li class="listitem">The article <em class="calibre9">Comparative Study of Hough Transform Methods for Circle Finding, Image and Vision Computing, vol. 8 no 1, pp. 71-77, 1990</em>, by H.K. Yuen, J. Princen, J. Illingworth, and J Kittler, describes different strategies for circle detection using the Hough transform</li></ul></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec49" class="calibre1"/>Fitting a line to a set of points</h1></div></div></div><p class="calibre8">In some <a id="id621" class="calibre1"/>applications, it could be important to not only detect lines in an image, but also to obtain an <a id="id622" class="calibre1"/>accurate estimate of the line's position and orientation. This recipe will show you how to find the line that best fits a given set of points.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec142" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">The first thing<a id="id623" class="calibre1"/> to do is to identify points in an image that seem to be aligned along a straight line. Let's use one of the lines we detected in the preceding recipe. The lines detected using <code class="email">cv::HoughLinesP</code> are contained in <code class="email">std::vector&lt;cv::Vec4i&gt;</code> called <code class="email">lines</code>. To extract the set of points that seem to belong to, let's say, the first of these lines, we can proceed as follows. We draw a white line on a black image <a id="id624" class="calibre1"/>and intersect it with the Canny image of contours used to detect our lines. This is simply achieved by the following statements:</p><div><pre class="programlisting">   int n=0; // we select line 0 
   // black image
   cv::Mat oneline(contours.size(),CV_8U,cv::Scalar(0));
   // white line
   cv::line(oneline, 
            cv::Point(lines[n][0],lines[n][1]),
            cv::Point(lines[n][2],lines[n][3]),
            cv::Scalar(255),
            3); // line width
   // contours AND white line
   cv::bitwise_and(contours,oneline,oneline);</pre></div><p class="calibre8">The result is an image that contains only the points that could be associated with the specified line. In order to introduce some tolerance, we draw a line of a certain thickness (here, 3). All points inside the defined neighborhood are, therefore, accepted. The following is the image that is obtained (inverted for better viewing):</p><div><img src="img/00116.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The coordinates<a id="id625" class="calibre1"/> of the points<a id="id626" class="calibre1"/> in this set can then be inserted in <code class="email">std::vector</code> of<a id="id627" class="calibre1"/> the <code class="email">cv::Point</code> objects (floating point <a id="id628" class="calibre1"/>coordinates, that is, <code class="email">cv::Point2f</code>, can also be used) by the following double loop:</p><div><pre class="programlisting">   std::vector&lt;cv::Point&gt; points;

   // Iterate over the pixels to obtain all point positions
   for( int y = 0; y &lt; oneline.rows; y++ ) {    
      // row y
    
      uchar* rowPtr = oneline.ptr&lt;uchar&gt;(y);
    
      for( int x = 0; x &lt; oneline.cols; x++ ) {
         // column x 

         // if on a contour
         if (rowPtr[x]) {

            points.push_back(cv::Point(x,y));
         }
      }
    }</pre></div><p class="calibre8">The best fitting line is easily found by calling the <a id="id629" class="calibre1"/>
<code class="email">cv::fitLine</code> OpenCV function:</p><div><pre class="programlisting">   cv::Vec4f line;
   cv::fitLine(points,line,
               CV_DIST_L2, // distance type
               0,          // not used with L2 distance 
               0.01,0.01); // accuracy</pre></div><p class="calibre8">The preceding <a id="id630" class="calibre1"/>code gives us the parameters of the line equation in the form of a unit-directional vector (the first two values of <code class="email">cv::Vec4f</code>) and the coordinates <a id="id631" class="calibre1"/>of one point on the line (the last two values of <code class="email">cv::Vec4f</code>). For our example, these values are <code class="email">(0.83, 0.55)</code> for the directional vector and <code class="email">(366.1, 289.1)</code> for the point coordinates. The last two parameters specify the requested accuracy for the line parameters.</p><p class="calibre8">In general, the line equation will be used in the calculation of some properties (calibration is a good example where precise parametric representation is required). As an illustration, and to make sure we calculated the right line, let's draw the estimated line on the image. Here, we simply draw an arbitrary black segment that has a length of <code class="email">100</code> pixels and a thickness of <code class="email">3</code> pixels:</p><div><pre class="programlisting">   int x0= line[2];        // a point on the line
   int y0= line[3];
   int x1= x0+100*line[0]; // add a vector of length 100
   int y1= y0+100*line[1]; // using the unit vector
   // draw the line
   cv::line(image,cv::Point(x0,y0),cv::Point(x1,y1),
            0,3); // color and thickness</pre></div><p class="calibre8">The result can be seen in the following screenshot:</p><div><img src="img/00117.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec143" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">Fitting lines <a id="id632" class="calibre1"/>to a set of points is a classic problem in mathematics. The <a id="id633" class="calibre1"/>OpenCV implementation proceeds by minimizing the sum of the distances from each point to the line. Several distance functions are proposed, and the fastest option is to use the Euclidean distance, which is specified by <code class="email">CV_DIST_L2</code>. This choice corresponds to the standard least-squares line fitting. When outliers (that is, points that don't belong to the line) are included in the point set, other distance functions that give less influence to far points can be selected. The minimization is based on the M-estimator technique that iteratively solves a weighted least-squares problem with weights that are inversely proportional to the distance from the line.</p><p class="calibre8">Using this function, it is also possible to fit a line to a 3D point set. The input is, in this case, a set of <code class="email">cv::Point3i</code> or <code class="email">cv::Point3f</code> objects and the output is a <a id="id634" class="calibre1"/>
<code class="email">std::Vec6f</code> instance.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec144" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">The <a id="id635" class="calibre1"/>
<code class="email">cv::fitEllipse</code> function fits an ellipse to a set of 2D points. This returns a rotated rectangle (a <code class="email">cv::RotatedRect</code> instance) inside which the ellipse is inscribed. In this case, you would write the following:</p><div><pre class="programlisting">   cv::RotatedRect rrect= cv::fitEllipse(cv::Mat(points));
   cv::ellipse(image,rrect,cv::Scalar(0));</pre></div><p class="calibre8">The <code class="email">cv::ellipse</code> function<a id="id636" class="calibre1"/> is the one you would use to draw the computed ellipse.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec50" class="calibre1"/>Extracting the components' contours</h1></div></div></div><p class="calibre8">Images <a id="id637" class="calibre1"/>generally contain representations of objects. One of the goals of image analysis is to identify and extract these objects. In object detection/recognition applications, the first step is often to produce a binary image that shows you where certain objects of interest could be located. No matter how this binary map is obtained (for example, from the histogram back projection we did in <a class="calibre1" title="Chapter 4. Counting the Pixels with Histograms" href="part0032_split_000.html#page">Chapter 4</a>, <em class="calibre9">Counting the Pixels with Histograms</em>, or from motion analysis as we will learn in <a class="calibre1" title="Chapter 11. Processing Video Sequences" href="part0072_split_000.html#page">Chapter 11</a>, <em class="calibre9">Processing Video Sequences</em>), the next step is to extract the objects that are contained in this collection of 1s and 0s.</p><p class="calibre8">Consider, for example, the image of buffaloes in a binary form that we manipulated in <a class="calibre1" title="Chapter 5. Transforming Images with Morphological Operations" href="part0040_split_000.html#page">Chapter 5</a>, <em class="calibre9">Transforming Images with Morphological Operations</em>, as shown in the following figure:</p><div><img src="img/00118.jpeg" alt="Extracting the components' contours" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We obtained this image from a simple thresholding operation followed by the application of open and close morphological filters. This recipe will show you how to extract the objects of such images. More specifically, we will extract the <a id="id638" class="calibre1"/>
<strong class="calibre2">connected components</strong>, that is, shapes made of a set of connected pixels in a binary image.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec145" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">OpenCV offers a simple function that extracts the contours of the connected components of an image. This is the <a id="id639" class="calibre1"/>
<code class="email">cv::findContours</code> function:</p><div><pre class="programlisting">   // the vector that will contain the contours
   std::vector&lt;std::vector&lt;cv::Point&gt;&gt; contours;
   cv::findContours(image, 
      contours, // a vector of contours 
      CV_RETR_EXTERNAL, // retrieve the external contours
      CV_CHAIN_APPROX_NONE); // all pixels of each contours</pre></div><p class="calibre8">The input <a id="id640" class="calibre1"/>is obviously the binary image. The output is a vector of contours, each contour being represented by a vector of <code class="email">cv::Point</code> objects. This explains why the output parameter is defined as a <code class="email">std::vector</code> instance<a id="id641" class="calibre1"/> of the <code class="email">std::vector</code> instances. In addition, two flags are specified. The first one indicates that only the external contours are required, that is, holes in an object will be ignored (the <em class="calibre9">There's more…</em> section will discuss the other options). The second flag is there to specify the format of the contour. With the current option, the vector will list all of the points in the contour. With the <code class="email">CV_CHAIN_APPROX_SIMPLE</code> flag, only the endpoints for horizontal, vertical, or diagonal contours will be included. Other flags would give a more sophisticated chain approximation of the contours in order to obtain a more compact representation. With the preceding image, nine connected components are obtained as given by <code class="email">contours.size()</code>.</p><p class="calibre8">Fortunately, there is a very convenient function that can draw the contours of those components on an image (here, a white image):</p><div><pre class="programlisting">   // draw black contours on a white image
   cv::Mat result(image.size(),CV_8U,cv::Scalar(255));
   cv::drawContours(result,contours,
      -1, // draw all contours
       0, // in black
       2);// with a thickness of 2</pre></div><p class="calibre8">If the third parameter of this function is a negative value, then all contours are drawn. Otherwise, it is possible to specify the index of the contour to be drawn. The result is shown in the following screenshot:</p><div><img src="img/00119.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec146" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">The contours<a id="id642" class="calibre1"/> are extracted by a simple algorithm that consists of systematically scanning the image until a component is hit. From this starting point on the component, its contour is followed, marking the pixels on its border. When the contour is completed, the scanning resumes at the last position until a new component is found.</p><p class="calibre8">The identified connected components can then be individually analyzed. For example, if some prior knowledge is available about the expected size of the objects of interest, it becomes possible to eliminate some of the components. Let's then use a minimum and a maximum value for the perimeter of the components. This is done by iterating over the vector of contours and eliminating the invalid components:</p><div><pre class="programlisting">   // Eliminate too short or too long contours
   int cmin= 50;  // minimum contour length
   int cmax= 1000; // maximum contour length
   std::vector&lt;std::vector&lt;cv::Point&gt;&gt;::
              iterator itc= contours.begin();
   // for all contours
   while (itc!=contours.end()) {

      // verify contour size
      if (itc-&gt;size() &lt; cmin || itc-&gt;size() &gt; cmax)
         itc= contours.erase(itc);
      else 
         ++itc;
   }</pre></div><p class="calibre8">Note that this loop <a id="id643" class="calibre1"/>could have been made more efficient since each erasing operation in a <code class="email">std::vector</code> instance is O(N). However, considering the small size of this vector, the overall cost is not too high. This time, we draw the remaining contours on the original image and obtain the following result:</p><div><img src="img/00120.jpeg" alt="How it works..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">We were lucky enough to find a simple criterion that allowed us to identify all objects of interest in this image. In more complex situations, a more refined analysis of the components' properties is required. This is the object of the next recipe.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec147" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">With the <a id="id644" class="calibre1"/>
<code class="email">cv::findContours</code> function, it is also possible to include all closed contours in the binary map, including the ones formed by holes in the components. This is done by specifying another flag in the function call:</p><div><pre class="programlisting">   cv::findContours(image, 
      contours, // a vector of contours 
      CV_RETR_LIST, // retrieve all contours
      CV_CHAIN_APPROX_NONE); // all pixels of each contours</pre></div><p class="calibre8">With this call, the following contours are obtained:</p><div><img src="img/00121.jpeg" alt="There's more..." class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Notice the extra contours that were added in the background forest. It is also possible to have these contours organized into a hierarchy. The main component is the parent, holes in it are its children, and if there are components inside these holes, they become the children of the previous children, and so on. This hierarchy is obtained by using the <code class="email">CV_RETR_TREE</code> flag, as follows:</p><div><pre class="programlisting">   std::vector&lt;cv::Vec4i&gt; hierarchy;
   cv::findContours(image, 
      contours, // a vector of contours
      hierarchy, // hierarchical representation 
      CV_RETR_TREE, // retrieve all contours in tree format
      CV_CHAIN_APPROX_NONE); // all pixels of each contours</pre></div><p class="calibre8">In this case, each contour has a corresponding hierarchy element at the same index, made of four integers. The first two integers give you the index of the next and the previous contours of the same level, and the next two integers give you the index of the first child and the parent of this contour. A negative index indicates the end of a contour list. The <code class="email">CV_RETR_CCOMP</code> flag is similar but limits the hierarchy at two levels.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch07lvl1sec51" class="calibre1"/>Computing components' shape descriptors</h1></div></div></div><p class="calibre8">A connected <a id="id645" class="calibre1"/>component often corresponds to the image of an object in a pictured scene. To identify this object, or to compare it with other image elements, it can be useful to perform some measurements on the component in order to extract some of its characteristics. In this recipe, we will look at some of the shape descriptors available in OpenCV that can be used to describe the shape of a connected component.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch07lvl2sec148" class="calibre1"/>How to do it...</h2></div></div></div><p class="calibre8">Many OpenCV functions are available when it comes to shape description. We will apply some of them on the components that we have extracted in the preceding recipe. In particular, we will use our vector of four contours corresponding to the four buffaloes we previously identified. In the following code snippets, we compute a shape descriptor on the contours (<code class="email">contours[0]</code> to <code class="email">contours[3]</code>) and draw the result (with a thickness of <code class="email">2</code>) over the image of the contours (with a thickness of <code class="email">1</code>). This image is shown at the end of this section.</p><p class="calibre8">The first one is the bounding box, which is applied to the bottom-right component:</p><div><pre class="programlisting">  // testing the bounding box 
  cv::Rect r0= cv::boundingRect(contours[0]);
  // draw the rectangle
  cv::rectangle(result,r0, 0, 2);</pre></div><p class="calibre8">The minimum enclosing circle is similar. It is applied on the upper-right component:</p><div><pre class="programlisting">   // testing the enclosing circle 
   float radius;
   cv::Point2f center;
   cv::minEnclosingCircle(contours[1],center,radius);
   // draw the circle
   cv::circle(result,center,
              static_cast&lt;int&gt;(radius),cv::Scalar(0),2);</pre></div><p class="calibre8">The polygonal approximation of a component's contour is computed as follows (on the left-hand side component):</p><div><pre class="programlisting">   // testing the approximate polygon
   std::vector&lt;cv::Point&gt; poly;
   cv::approxPolyDP(contours[2],poly,5,true);
   // draw the polygon
   cv::polylines(result, poly, true, 0, 2);</pre></div><p class="calibre8">Notice the <a id="id646" class="calibre1"/>polygon drawing<a id="id647" class="calibre1"/> function, <code class="email">cv::polylines</code>. This operates similarly to the other drawing functions. The third Boolean parameter is used to indicate whether the contour is closed or not (if yes, the last point is linked to the first one).</p><p class="calibre8">The convex hull is another form of polygonal approximation (on the second component from the left):</p><div><pre class="programlisting">   // testing the convex hull
   std::vector&lt;cv::Point&gt; hull;
   cv::convexHull(contours[3],hull);
   // draw the polygon
   cv::polylines(result, hull, true, 0, 2);</pre></div><p class="calibre8">Finally, the computation of the moments is another powerful descriptor (the center of mass is drawn inside all components):</p><div><pre class="programlisting">   // testing the moments
   // iterate over all contours
   itc= contours.begin();
   while (itc!=contours.end()) {

      // compute all moments
      cv::Moments mom= cv::moments(cv::Mat(*itc++));

      // draw mass center
      cv::circle(result,
         // position of mass center converted to integer
         cv::Point(mom.m10/mom.m00,mom.m01/mom.m00),
         2,cv::Scalar(0),2); // draw black dot
   }</pre></div><p class="calibre8">The resulting image is as follows:</p><div><img src="img/00122.jpeg" alt="How to do it..." class="calibre10"/></div><p class="calibre11"> </p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch07lvl2sec149" class="calibre1"/>How it works...</h2></div></div></div><p class="calibre8">The<a id="id648" class="calibre1"/> <strong class="calibre2">bounding box</strong> of a <a id="id649" class="calibre1"/>component is probably the most compact way to represent and localize a component in an image. It is defined as the upright rectangle of minimum size that completely contains the shape. Comparing the height and width of the box gives you an indication about the vertical or horizontal dimension of the object (for example, one could use a height-to-width ratio in order to distinguish the image of a car from one of a pedestrian). The <strong class="calibre2">minimum enclosing circle</strong><a id="id650" class="calibre1"/> is generally used when only the approximate component size and location is required.</p><p class="calibre8">The <strong class="calibre2">polygonal approximation</strong><a id="id651" class="calibre1"/> of a component is useful when one wants to manipulate a more compact representation that resembles the component's shape. It is created by specifying an accuracy parameter, giving you the maximal acceptable distance between a shape and its simplified polygon. It is the fourth parameter in the <a id="id652" class="calibre1"/>
<code class="email">cv::approxPolyDP</code> function. The result is a vector of <code class="email">cv::Point</code>, which corresponds to the vertices of the polygon. To draw this polygon, we need to iterate over the vector and link each point with the next one by drawing a line between them.</p><p class="calibre8">The <a id="id653" class="calibre1"/>
<strong class="calibre2">convex hull</strong>, or convex envelope, of a shape is the minimal convex polygon that encompasses a shape. It can be visualized as the shape that an elastic band would take if placed around the component. As can be seen, the convex hull contour will deviate from the original one at the concave locations of the shape contour.</p><p class="calibre8">These locations are often designated as convexity defects, and a special OpenCV function is available to identify them: the <a id="id654" class="calibre1"/>
<code class="email">cv::convexityDefects</code> function. It is called as follows:</p><div><pre class="programlisting">  std::vector&lt;cv::Vec4i&gt; defects;
  cv::convexityDefects(contour, hull, defects);</pre></div><p class="calibre8">The <code class="email">contour</code> and <code class="email">hull</code> arguments are, respectively, the original and the convex hull contours (both represented with <code class="email">std::vector&lt;cv::Point&gt;</code> instances). The output is a vector <a id="id655" class="calibre1"/>of four integer elements. The first two integers are the indices of the points on the contour, delimitating the defect; the third integer corresponds to the farthest point inside the concavity, and finally, the last integer corresponds to the distance between this farthest point and the convex hull.</p><p class="calibre8">
<strong class="calibre2">Moments</strong><a id="id656" class="calibre1"/> are commonly used mathematical entities in the structural analysis of shapes. OpenCV has defined a data structure that encapsulates all computed moments of a shape. It is the object returned by the <a id="id657" class="calibre1"/>
<code class="email">cv::moments</code> function. Together, the moments represent a compact description of the shape of an object. They are commonly used, for example, in character recognition. We simply use this structure to obtain the mass center of each component that is computed from the first three spatial moments here.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch07lvl2sec150" class="calibre1"/>There's more...</h2></div></div></div><p class="calibre8">Other structural properties can be computed using the available OpenCV functions. The <code class="email">cv::minAreaRect</code> function<a id="id658" class="calibre1"/> computes the minimum enclosed rotated rectangle (this was used in <a class="calibre1" title="Chapter 5. Transforming Images with Morphological Operations" href="part0040_split_000.html#page">Chapter 5</a>, <em class="calibre9">Transforming Images with Morphological Operations</em>, in the <em class="calibre9">Extracting distinctive regions using MSER</em> recipe). The <code class="email">cv::contourArea</code> function<a id="id659" class="calibre1"/> estimates the area of (the number of pixels inside) a contour. The<a id="id660" class="calibre1"/> <code class="email">cv::pointPolygonTest</code> function determines whether a point is inside or outside a contour, and <code class="email">cv::matchShapes</code> measures the resemblance between two contours. All these property measures can be advantageously combined in order to perform more advanced structural analysis.</p><div><div><div><div><h3 class="title2"><a id="ch07lvl3sec34" class="calibre1"/>Quadrilateral detection</h3></div></div></div><p class="calibre8">The MSER features presented in <a class="calibre1" title="Chapter 5. Transforming Images with Morphological Operations" href="part0040_split_000.html#page">Chapter 5</a>, <em class="calibre9">Transforming Images with Morphological Operations</em>, constitutes an efficient tool to <a id="id661" class="calibre1"/>extract shapes in an image. Considering the MSER result obtained in this preceding chapter, we will now build an algorithm to detect quadrilateral components in an image. In the case of the current image, this detection will allow us to identify the building's windows. A binary version of the MSER image is easily obtained as follows:</p><div><pre class="programlisting">  // create a binary version
  components= components==255;
  // open the image (white background)
  cv::morphologyEx(components,components,
                    cv::MORPH_OPEN,cv::Mat(),
                    cv::Point(-1,-1),3);</pre></div><p class="calibre8">In addition, we cleaned<a id="id662" class="calibre1"/> the image with a morphological filter. The image is then as follows:</p><div><img src="img/00123.jpeg" alt="Quadrilateral detection" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">The next step is to obtain the contours:</p><div><pre class="programlisting">  //invert image (background must be black)
  cv::Mat componentsInv= 255-components;
  // Get the contours of the connected components
  cv::findContours(componentsInv, 
    contours, // a vector of contours 
    CV_RETR_EXTERNAL, // retrieve the external contours
    CV_CHAIN_APPROX_NONE); </pre></div><p class="calibre8">Finally, we go over all the contours and roughly approximate them with a polygon:</p><div><pre class="programlisting">  // white image
  cv::Mat quadri(components.size(),CV_8U,255);

  // for all contours
  std::vector&lt;std::vector&lt;cv::Point&gt;&gt;::iterator 
                              it= contours.begin();
  while (it!= contours.end()) {
    poly.clear();
    // approximate contour by polygon
    cv::approxPolyDP(*it,poly,10,true);
  
    // do we have a quadrilateral?
    if (poly.size()==4) {
          // draw it
      cv::polylines(quadri, poly, true, 0, 2);
    }
    ++it;
  }</pre></div><p class="calibre8">The quadrilaterals <a id="id663" class="calibre1"/>are those polygons that have four edges. The detected ones are the following:</p><div><img src="img/00124.jpeg" alt="Quadrilateral detection" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">To detect rectangles, you can simply measure the angles between adjacent edges and reject the quadrilaterals that have angles that deviate too much from 90 degrees.</p></div></div></div></body></html>