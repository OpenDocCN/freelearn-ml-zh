<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Image Filters and Morphological Operators</h1></div></div></div><p>After learning the basics of setting up OpenCV for Java and dealing with a graphical user interface, it is time to explore some of the core operators in image processing. Some of them come from signal processing and we call them filters, as they usually help you to get away with noise from images. It is important to know that several digital filters have their optical counterparts. Other operators play a useful role when dealing with binary images, such as the <a id="id103" class="indexterm"/>morphological operators, which will help you to isolate regions or glue some of them together. We will also cover, in detail, the famous <strong>bucket fill tool</strong>, which is very useful in segmentation. When dealing with large images, it is important to know how image pyramids can help you decrease your image size without losing important information and by achieving performance. We will finish this chapter with one of the simplest and most useful techniques for segmentation, which is applying a threshold to separate regions as well as studying a dynamic threshold that will not suffer much from lighting problems.</p><p>In this chapter, we will cover:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Smoothing</li><li class="listitem" style="list-style-type: disc">Morphological operators</li><li class="listitem" style="list-style-type: disc">Flood filling</li><li class="listitem" style="list-style-type: disc">Image pyramids</li><li class="listitem" style="list-style-type: disc">Thresholding</li></ul></div><p>By the end of this chapter, you will be able to perform several filtering procedures over an image, such as removing noise, growing, shrinking and filling some areas, as well as deciding whether some pixels fit or not in accordance with a given criteria.</p><div><div><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Smoothing</h1></div></div></div><p>Just like in one-dimensional signals, we are always susceptible to receiving some noise in our images and we generally apply some preprocessing filters to them before we perform our main work on the images. We can consider noise as a random variation of color or brightness information that is not present in the imaged object, which can take place undesirably due to a sensor and circuitry of a digital camera or scanner. This section uses the ideas of low-pass filter kernels to smoothen our images. These filters remove high frequency content, such as edges and noises, although some techniques allow edges not to be blurred. We <a id="id104" class="indexterm"/>will cover the four main image filters available in OpenCV: averaging, Gaussian, median filtering, and bilateral filtering.</p><div><div><h3 class="title"><a id="note07"/>Note</h3><p><strong>2D Kernel </strong><a id="id105" class="indexterm"/>
<strong>Convolution</strong> is a form of mathematical convolution. An output image is calculated by sweeping each of the pixels of a given image and applying a kernel operator to them, yielding an output pixel for each resulting operation. For instance, the kernel operator can be a 3 x 3 matrix of 1s divided by 9. This way, each output pixel will be the average value of the 9 neighbor pixels for each pixel in the input image, yielding an average output image.</p></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec10"/>Averaging</h2></div></div></div><p>Most of the blurring <a id="id106" class="indexterm"/>techniques will use a 2D kernel convolution to filter images. The simplest idea is to have a 3 x 3 kernel that has a total of 9 pixels. Suppose we want to have the average value of 9 pixels, we will only need to add them and divide by 9. This is accomplished by the convolution with the following kernel:</p><div><img src="img/3972OS_03_16.jpg" alt="Averaging"/></div><p>In order to apply this transformation, we will use Imgproc's<code class="literal"> blur</code> function. Its syntax is as follows:</p><div><pre class="programlisting">public static void <strong>blur</strong>(Mat src, Mat dst, Size ksize)</pre></div><p>The parameters are, simply, the source image, destination, and the kernel size, which is as simple as <code class="literal">new Size(3.0, 3.0)</code> for our 3 x 3 kernel. You can optionally add the <code class="literal">Point</code> anchor parameter, shown as follows:</p><div><pre class="programlisting">public static void blur(Mat src, Mat dst, Size ksize, Point anchor, int borderType)</pre></div><p>The preceding line will let you position the anchor as well as an <code class="literal">int borderType</code> integer variable outside the center point. This <code class="literal">borderType</code> parameter lets you define how you want the behavior when part of the kernel is inside and outside the image. Note that in the first row, the preceding kernel will look for values that will be on top of the row, so OpenCV will need to extrapolate them. There are a few options available to extrapolate borders. From the <a id="id107" class="indexterm"/>documentation, we have the following types of borders, all available from <code class="literal">Core</code> constants, for instance: <code class="literal">Core.BORDER_REPLICATE</code>. For example, consider <code class="literal">|</code> as one of the image borders and <code class="literal">abcdefgh</code> as pixel values:</p><div><pre class="programlisting">BORDER_REPLICATE:     aaaaaa|abcdefgh|hhhhhhh
BORDER_REFLECT:       fedcba|abcdefgh|hgfedcb
BORDER_REFLECT_101:   gfedcb|abcdefgh|gfedcba
BORDER_WRAP:          cdefgh|abcdefgh|abcdefg
BORDER_CONSTANT:      000000|abcdefgh|0000000</pre></div><p>The default value is <code class="literal">Core.BORDER_DEFAULT</code> that maps to<code class="literal"> Core.BORDER_REFLECT_101</code>. For more information on how to use this function, look for the source code of this chapter's <code class="literal">imageFilter</code> project. The following is a screenshot of the main application, which lets you try out each of these filters:</p><div><img src="img/3972OS_03_01.jpg" alt="Averaging"/></div><p>Note that this application also provides some simple Gaussian noise, whose probability density function is equal <a id="id108" class="indexterm"/>to that of the normal distribution, to see the benefits of each filter.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec11"/>Gaussian</h2></div></div></div><p>The idea behind <a id="id109" class="indexterm"/>Gaussian is the same as average filtering except for the fact that instead of using the same weight for each of the pixels, a two-dimensional Gaussian function is used for the kernel that gives the highest weightage to the pixel in the center. The following graph displays the behavior of a 2D Gaussian curve:</p><div><img src="img/3972OS_03_02.jpg" alt="Gaussian"/></div><p>In order to use this function, employ the following basic signature:</p><div><pre class="programlisting">public static void <strong>GaussianBlur</strong>(Mat src,
                                Mat dst,
                                Size ksize,
                                double sigmaX [, double sigmaY])</pre></div><p>The <code class="literal">Mat src</code> and <code class="literal">Mat dst</code> parameters are straightforward since they describe the input and output images. The <code class="literal">Size ksize</code> parameter describes the kernel's width and height. Hence, if you want to <a id="id110" class="indexterm"/>set its size, this parameter must be positive and odd, so that the kernel can be symmetrical and have a center. In case you set the parameter to zero, the size will be calculated from <code class="literal">double sigmaX</code>. Sigma is its standard deviation, which is roughly <em>half width at half max</em> of the Gaussian value, which means that it is half the width of the Gaussian value when its height is half the highest Gaussian value. Optionally, you can also provide the fifth parameter as <code class="literal">sigmaY</code>, which is the standard deviation for the <em>y</em> axis. In case you don't use this parameter, <code class="literal">sigmaY</code> will be equal to <code class="literal">sigmaX</code>. Also, if both <code class="literal">sigmaX</code>, and <code class="literal">sigmaY</code> are zero, they are computed from the kernel's width and height. The <code class="literal">getGaussianKernel</code> function returns all the Gaussian coefficients in case they are required. A sixth parameter can also be given to the <code class="literal">GaussianBlur</code> function, which is how borders will behave. This parameters works just like the <code class="literal">int borderType</code> parameter from the<em> Averaging</em> section.</p><p>An example of how to use <code class="literal">GaussianBlur</code> can be taken from the sample <code class="literal">imageFilter</code> project from this chapter:</p><div><pre class="programlisting">Imgproc.<strong>GaussianBlur</strong>(image, output,  new Size(3.0, 3.0), 0);</pre></div><p>The preceding line sets sigma to <code class="literal">0</code> and makes the function calculate it from the kernel's size by using the following <a id="id111" class="indexterm"/>formula:</p><div><pre class="programlisting">sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8</pre></div><p>Here, <code class="literal">ksize</code> is the kernel's aperture size, which would be <code class="literal">3</code> for our example.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec12"/>Median filtering</h2></div></div></div><p>Another idea to make a <a id="id112" class="indexterm"/>filter is to select the median pixel in a kernel instead of the mean value, which is to select the pixel that would be in the middle of a line <a id="id113" class="indexterm"/>of intensity-sorted pixels. This is accomplished by using the following function:</p><div><pre class="programlisting">public static void <strong>medianBlur</strong>(Mat src,
                              Mat dst,
                              int ksize)</pre></div><p>The <code class="literal">Mat src</code> and <code class="literal">dst</code> parameters are the input and output images, respectively, while<code class="literal"> int ksize</code> is the kernel's aperture size, which must be odd and greater than 1.</p><p>Sometimes, the image noise is very high and it can appear as large isolated outlier points, which would cause a noticeable average shift. In order to overcome these problems, a median filter can be used to ignore these outliers.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec13"/>Bilateral filtering</h2></div></div></div><p>While median, Gaussian, and averaging filters tend to smoothen noise and edges, the main advantage of using <a id="id114" class="indexterm"/>bilateral filtering is the fact that it will preserve <a id="id115" class="indexterm"/>them, since they present important information, such as, the boundary of a cell in some medical imaging, which should not be filtered out. The tricky part of this filter is that it considers both the spatial distance and pixel intensity difference when calculating the average, which means that it will not include pixels that have intensity differences above a given threshold when calculating the output image. Note the effect of bilateral filtering in a marble checkboard using the <code class="literal">imageFilter</code> sample project from this chapter:</p><div><img src="img/3972OS_03_03.jpg" alt="Bilateral filtering"/></div><p>The right-hand image shows a filtered marble while preserving the edges, something that does not happen when you use other filters. One of the drawbacks of this method is that soft texture details tend to be removed, like in the white square of the third line and second column of <a id="id116" class="indexterm"/>the previous image. The function signature is as follows:</p><div><pre class="programlisting">public static void bilateralFilter(Mat src, Mat dst, int d,
                                   double sigmaColor,
                                   double sigmaSpace,
                                   [int borderType])</pre></div><p>While <code class="literal">Mat src</code> and <code class="literal">Mat dst</code> are the input and output images, respectively, the <code class="literal">int d</code> parameter is the diameter of the considered neighborhood. If it is non-positive, the diameter will be calculated from the <code class="literal">sigmaSpace</code> parameter. The filter sigma in color space is defined by the <a id="id117" class="indexterm"/>
<code class="literal">double sigmaColor</code> parameter, which means that for higher values, farther colors in the neighborhood will be considered when calculating the output color of a pixel, creating a watercolor effect. <code class="literal">Double sigmaSpace</code> is the sigma value in the coordinate space, which means that as long as colors are not skipped because of <code class="literal">sigmaColor</code>, they will have pretty much the same average component as in Gaussian. Remember that the watercolor effect can be very useful as a first step when segmenting images. If you need control over the border type, the <code class="literal">int borderType</code> parameter can be added as the last one, like in the previous filters.</p><p>When considering intensity differences to calculate the new average value of a pixel, another Gaussian function is used. Note that because of this additional step, bilateral filtering should be used with smaller kernel sizes (for instance, 5) when dealing with real-time images, while a kernel of size 9 might be good enough for offline applications. Note that when using a 3 x 3 neighborhood for a kernel of size 3, only 9 pixels are verified in the convolution of each <a id="id118" class="indexterm"/>pixel. On the other hand, when using a kernel of <a id="id119" class="indexterm"/>size 9, 9 x 9 pixels are verified, which makes the algorithm search for around 81 pixels. This could take 9 times longer.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Morphological operators</h1></div></div></div><p>Some image <a id="id120" class="indexterm"/>operations are called morphological operations, since they change the shape of an underlying object. We will discuss erosion and dilation, which are some very useful morphological transformations in this section as well as some derived transformations. They usually appear in the context of isolating elements, removing noise, and joining distanced elements in an image.</p><p>These operators work through the convolution of a given kernel with the image. This kernel is described with an anchor point, which is the one that is probed against a region of pixels, depending on its shape:</p><div><img src="img/3972OS_03_17.jpg" alt="Morphological operators"/></div><p>The preceding image shows a bright region on the image, which we will call <strong>A</strong>. Note that the complement region is completely dark. Our kernel is made of a 3 x 3 block with an anchor at its center, described as <strong>B</strong>. The <strong>C</strong> region is the result of applying the erosion morphological transformation over the image. Note that this operation takes place when you scan each pixel of the image, center the kernel anchor on each of these pixels, and then retrieve the local minimum over the kernel area. Note that erosion will reduce the bright areas.</p><p>The opposite operation is called dilation and the difference between these two is that in dilation, instead of computing the local minimum over the kernel area it will compute the local maximum over that area. This operation will expand a bright region of 3 x 3 square blocked kernels.</p><p>In order to get a better picture of how these operators work, a good idea is to try the <code class="literal">morphology</code> project from this chapter's source code. It is basically OpenCV's official C++ <code class="literal">morphology2</code> example translated to Java with some minor GUI enhancements. Note that in case of <a id="id121" class="indexterm"/>multichannel images, each channel is processed independently. The following screenshot shows the running application:</p><div><img src="img/3972OS_03_05.jpg" alt="Morphological operators"/></div><p>Note that our kernel bounding box is 2 times the kernel size slider parameter plus 1, so, if the kernel size parameter is selected as 1, we will have a 3 x 3 kernel bounding box. We also described our example in terms of a square kernel, but it could be of any shape, so the shape parameter is also there for us to choose from. In order to create these kernels easily, Imgproc's <code class="literal">getStructuringElement</code> function is used. This function will take the kernel's shape, its size, and <a id="id122" class="indexterm"/>zero indexed anchor position as its parameters. The kernel shape can be <code class="literal">Imgproc.CV_SHAPE_RECT</code> (for rectangles), <code class="literal">Imgproc.CV_SHAPE_ELLIPSE</code> (for ellipses), or <code class="literal">Imgproc.CV_SHAPE_CROSS</code> (for a cross-shaped kernel).</p><p>We have put all image operations in the <code class="literal">ImageProcessor</code> class, which we will highlight in the following code:</p><div><pre class="programlisting">public Mat <strong>erode</strong>(Mat input, int elementSize, int elementShape){
  Mat outputImage = new Mat();
  Mat element = getKernelFromShape(elementSize, elementShape);
  <strong>Imgproc.erode(input,outputImage, element);</strong>
  return outputImage;
}

public Mat <strong>dilate</strong>(Mat input, int elementSize, int elementShape) {
  Mat outputImage = new Mat();
  Mat element = getKernelFromShape(elementSize, elementShape);
  <strong>Imgproc.dilate(input,outputImage, element);</strong>
  return outputImage;
}

public Mat <strong>open</strong>(Mat input, int elementSize, int elementShape) {
  Mat outputImage = new Mat();
  Mat element = getKernelFromShape(elementSize, elementShape);
  <strong>Imgproc.morphologyEx(input,outputImage, Imgproc.MORPH_OPEN, element);</strong>
  return outputImage;
}

public Mat <strong>close</strong>(Mat input, int elementSize, int elementShape) {
  Mat outputImage = new Mat();
  Mat element = getKernelFromShape(elementSize, elementShape);
  <strong>Imgproc.morphologyEx(input,outputImage, Imgproc.MORPH_CLOSE, element);</strong>
  return outputImage;
}

private Mat <strong>getKernelFromShape</strong>(int elementSize, int elementShape) {
  <strong>return Imgproc.getStructuringElement(elementShape, new Size(elementSize*2+1, elementSize*2+1), new Point(elementSize, elementSize) );</strong>
}</pre></div><p>As all our methods create a kernel in the same way, we have extracted the <code class="literal">getKernelFromShape</code> method, which will simply call the <code class="literal">getStructuringElement</code> function with the size described in the preceding code. As we have a custom kernel, we will call the overloaded <a id="id123" class="indexterm"/>
<code class="literal">Imgproc.erode</code> function with the input image, output image, and kernel as a third parameter. The following screenshot is a result of the erosion function over a given input image:</p><div><img src="img/3972OS_03_06.jpg" alt="Morphological operators"/></div><p>Note that this operator is frequently used to remove speckle noise from an image, as it will be eroded to nothing, while larger regions that contain important information will practically not be affected. Note that smoothing filters will not completely remove speckle noise as they tend to decrease its amplitude. Also pay attention that these operations are sensitive to kernel size, so a size adjustment and some experimenting is required. We can also check out the result of applying dilation in the following screenshot:</p><div><img src="img/3972OS_03_07.jpg" alt="Morphological operators"/></div><p>Note that besides making areas thicker, the dilate morphological transformation is also very useful in searching for connected components, which are large regions of similar pixel intensity. It might be necessary when a large region is broken into smaller ones because of shadows, noise, or other effects, as can be seen in the lower part of the image in the preceding screenshot. Applying dilation will make them link together to a bigger element.</p><p>We also derived <a id="id124" class="indexterm"/>morphological transformations, which are <strong>open</strong> and <strong>close</strong>. Open is defined by erosion followed by a dilation, while in a close operation, the dilation happens first. The following is a screenshot of an open transform:</p><div><img src="img/3972OS_03_08.jpg" alt="Morphological operators"/></div><p>This operation is generally used while counting regions from a binary image. For example, we might use it to separate regions that are too near each other before counting them. Note that in the bottom part of our example, only larger areas have survived the operation while preserving <a id="id125" class="indexterm"/>the non-connectedness between the large areas that were apart. On the other hand, we can see the effects of applying the close operation to the same image, as shown in the following screenshot:</p><div><img src="img/3972OS_03_09.jpg" alt="Morphological operators"/></div><p>Check whether this tends to connect nearby regions. Depending on the kernel size, it might be useful in connected component algorithms to reduce segments generated by noise. Unlike erosion and dilation, both open and close morphological transformations tend to preserve the areas of their regions of interest.</p></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Flood filling</h1></div></div></div><p>Another very important <a id="id126" class="indexterm"/>algorithm for segmentation is flood fill, also known as region growing. Most of you who have already worked with popular computer graphic programs, such as Microsoft Paint or GIMP will have probably used the bucket fill or paint bucket tool, which fills an area with a color. Although it might look like a very simple algorithm at first sight, it has a very interesting implementation and has several parameters that can make it work well to segment images.</p><p>The idea behind the algorithm is to check for connected components, which are the areas with similar color or brightness, starting from a given point—the so-called seed point—and then examining this particular point's neighbors. These can include either 4 (north, south, east, and west) or 8 neighbors (north, north-east, east, south-east, south, south-west, west, and north-west) that check for a condition and then recursively, call the same procedure on each of the neighbors in case they have passed that condition. It will, naturally, add that point to the given connected component in case the condition is true. We generally seek for pixels that are either like the seed point or like their neighbor points, depending on which mode of flood fill will operate. We call it a fixed range when pixels are compared against the seed point and we call it a floating range when pixels are compared against neighbor <a id="id127" class="indexterm"/>pixels. This condition also accepts lower difference <em>loDiff</em> and higher difference <em>upDiff</em> parameters, which enter in the condition according to the <em>src(x',y') – loDiff &lt; src (x,y) &lt; src(x',y') + upDiff</em> equation. In this equation, <em>src(x,y)</em> is the value of the pixel at the <em>x</em>, <em>y</em> coordinates that are tested to check whether it belongs to the same domain as the seed point, while <em>src(x',y')</em> is the value of one of the pixels that is already known to belong to that component in case of a grayscale image operating in a floating range. In case we have a fixed range flood fill, the equation turns into <em>src(seed.x,seed.y) – loDiff &lt; src (x,y) &lt; src(seed.x,seed.y) + upDiff</em>, where <em>seed.x</em> and <em>seed.y</em> are the seed's coordinates. Also note that in case of a colored image, each of the pixel's components are tested against the condition, while <em>loDiff</em> and <em>highDiff</em> are tridimensional scalars. All in all, a new pixel will be added to the domain in case its brightness or color is close enough to one of its neighbors that already belongs to the connected component in case of a floating range flood fill or close enough to the seed's properties in the case of a fixed range one.</p><p>The flood fill's signature is as follows:</p><div><pre class="programlisting">public static int floodFill(Mat image,
                            Mat mask,
                            Point seedPoint,
                            Scalar newVal,
                            Rect rect,
                            Scalar loDiff,
                            Scalar upDiff,
                            int flags)</pre></div><p>The <code class="literal">Mat image</code> parameter is the input/output <code class="literal">Mat</code> containing the image to perform the flood fill, while <code class="literal">Mat mask</code> is a single channel 8-bit mat 2 rows taller and 2 columns wider than <code class="literal">Mat image</code>, for performance reasons. The <code class="literal">Point seedpoint</code> parameter contains the coordinates of the seed point, while <code class="literal">Rect rect</code> is an output rectangle with the smallest bounding box that contains the segmented area. The <code class="literal">Scalar loDiff</code> and <code class="literal">upDiff</code> parameters are discussed in the preceding condition. The <code class="literal">int flags</code> parameter contains options for the operating mode of the algorithm. The source code containing a <em>façade</em> class for the <code class="literal">floodFill</code> method is available in the <code class="literal">floodfill</code> project in this chapter. The following is a screenshot of the application:</p><div><img src="img/3972OS_03_10.jpg" alt="Flood filling"/></div><p>On the left-hand side of the preceding screenshot, there is a <code class="literal">JLabel</code> like the one explained in <a class="link" href="ch02.html" title="Chapter 2. Handling Matrices, Files, Cameras, and GUIs">Chapter 2</a>, <em>Handling Matrices, Files, Cameras, and GUIs</em>, used to load images, but this one has <code class="literal">MouseListener</code> that sends the captured clicks to the <code class="literal">FloodFillFacade</code> class. On the right-hand side of the preceding screenshot, the mask is shown in case the <strong>Mask</strong> <a id="id128" class="indexterm"/>radio button is turned on. The algorithm operation mode is chosen through the <code class="literal">Range radio</code> buttons, which will be relative (checks the conditions against neighbors), fixed (the condition is probed against the seed), or null (when <code class="literal">loDiff</code> and <code class="literal">hiDiff</code> are both zero). A radio button for connectivity is also available for 4 or 8 neighbors, while the lower and upper thresholds refer to the <code class="literal">loDiff</code> and <code class="literal">hiDiff</code> parameters, respectively.</p><p>While most fields from <code class="literal">FloodFillFacade</code> are just <code class="literal">getters</code> and <code class="literal">setters</code>, the flag configuration is something that you need to pay attention to. Note that a <em>façade</em> is just an object that creates a <a id="id129" class="indexterm"/>simplified interface to a larger part of code, making it easier to use. Here are some important pieces of <code class="literal">FloodFillFacade</code>:</p><div><pre class="programlisting">public class FloodFillFacade {

  public static final int NULL_RANGE = 0;
  public static final int FIXED_RANGE = 1;
  public static final int FLOATING_RANGE = 2;
  private boolean colored = true;
  private boolean masked = true;
  private int range = FIXED_RANGE;
  private Random random = new Random();
  private int connectivity = 4;
  private int newMaskVal = 255;
  private int lowerDiff = 20;
  private int upperDiff = 20;
  
  public int fill(Mat image, Mat mask, int x, int y) {
    <strong>Point seedPoint = new Point(x,y);</strong>

    int b = random.nextInt(256);
    int g = random.nextInt(256);
    int r = random.nextInt(256);
    Rect rect = new Rect();

    <strong>Scalar newVal = isColored() ? new Scalar(b, g, r) : new Scalar(r*0.299 + g*0.587 + b*0.114);</strong>

    Scalar lowerDifference = new Scalar(lowerDiff,lowerDiff,lowerDiff);
    Scalar upperDifference = new Scalar(upperDiff,upperDiff,upperDiff);
    if(range == NULL_RANGE){
      lowerDifference = new Scalar (0,0,0);
      upperDifference = new Scalar (0,0,0);
    }
    <strong>int flags = connectivity + (newMaskVal &lt;&lt; 8) + </strong>
      <strong>(range == FIXED_RANGE ? Imgproc.FLOODFILL_FIXED_RANGE : 0);</strong>
    int area = 0;
    if(masked){
      <strong>area = Imgproc.floodFill(image, mask, seedPoint, newVal, rect, lowerDifference, upperDifference, flags);</strong>
    }
    else{
      <strong>area = Imgproc.floodFill(image, new Mat(), seedPoint, newVal, rect, lowerDifference, upperDifference, flags);</strong>
    }
    return area;
  }
...
}</pre></div><p>Here, firstly, <code class="literal">newVal</code> is created as the new color that is to be filled in the connected component. Java random classes are used to generate the color and in case it's a grayscale image, it is converted to grayscale. Then, we set the <code class="literal">lowerDifference</code> and <code class="literal">higherDifference</code> scalars, which will be used in accordance with the equations described previously. Then, the <code class="literal">flags</code> variable is defined. Note that connectivity is set on lower bits, while <code class="literal">newMaskVal</code> is shifted to the left 8 times. This parameter is the color used to fill the mask in case it's being used. Then, in case a fixed range is required for flood fill, its flag is set. We <a id="id130" class="indexterm"/>are then able to chose from the masked or unmasked version of flood fill. Pay attention to <code class="literal">new Mat()</code>, which is passed when does not use a mask. Observe that the <code class="literal">seedPoint</code> parameter is built from the given coordinates from our <code class="literal">MouseListener</code>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Image pyramids</h1></div></div></div><p>Image pyramids are simply a <a id="id131" class="indexterm"/>collection of images obtained by downsampling an original image, so that each image is one-fourth the area of its predecessor. It is mainly used in image segmentation, since it can generate a very meaningful representation of the image in low resolution, so that a time consuming algorithm can run on it. This makes it easy for us to map this result back to a higher resolution image in the pyramid and makes it possible to refine the results there. Besides, an approximation to a Laplacian, by means of difference of Gaussians, can be generated. Note that a Laplacian image is the one that will show its edges.</p><p>In order to produce the downsample image, which we will call the layer <code class="literal">i+1</code> in the Gaussian pyramid (<code class="literal">Gi+1</code>), we first convolve <code class="literal">Gi</code> with a Gaussian kernel, just like in Gaussian filtering, followed by removing every even numbered row and column. Then, we yield an image with one quarter of the area of the above layer. Averaging before downsampling is important because this way, information from odd numbered columns and rows gets captured. The function to get a downsampled image has the following signature:</p><div><pre class="programlisting">public static void pyrDown(Mat src, Mat dst ,[Size dstsize, int borderType])</pre></div><p>The <code class="literal">Mat src</code> and <code class="literal">Mat dst</code> parameters are the input and output images. Note that the output image will have a width of <code class="literal">(src.width+1)/2</code> and a height of <code class="literal">(src.height+1)/2</code>, where <code class="literal">/</code> denotes an integer division. You should be careful when working with odd dimensions, since an upsampled image generated from a downsampled one will not have the <a id="id132" class="indexterm"/>same dimensions. Take for instance, an 11 x 11 image. When you use <code class="literal">pyrDown</code>, it will become a 6 x 6 image. In case you upsample it, it will become a 12 x 12 image, so you can't add or subtract it from the original image. Note that when using <code class="literal">pyrDown</code>, a 5 x 5 Gaussian kernel is used. In case you want, the <code class="literal">pyrDown</code> function is overloaded with the <code class="literal">Size dstsize</code> and <code class="literal">int borderType</code> properties. The <code class="literal">dstsize</code> property will allow you to define the output image size, but you must satisfy the following conditions:</p><div><pre class="programlisting">|dstsize.width  * 2 – src.cols| &lt; 2
|dstsize.height * 2 – src.rows| &lt; 2</pre></div><p>This means that you won't have much freedom when deciding the output image size. Also, <code class="literal">borderType</code> follows the same considerations as those are given in the <em>Smoothing</em> section.</p><p>On the other hand, the <code class="literal">pyrUp</code> function will upsample an image and then blur it. First, it will inject zero rows and columns on even locations and then, it convolve with the same kernel from the pyramid down operation. Note that <code class="literal">pyrDown</code> is a transformation that loses information, so <code class="literal">pyrUp</code> won't be able to recover the original image. Its usage is as follows:</p><div><pre class="programlisting">public static void pyrUp(Mat src, Mat dst)</pre></div><p>Also, its parameters are just like the <code class="literal">pyrDown</code> parameters.</p><p>In case you want to build the Laplacian, just note that it can be achieved by using the following equation:</p><div><img src="img/3972OS_03_18.jpg" alt="Image pyramids"/></div><p><code class="literal">UP</code> is the upsampling operation and <code class="literal">⊗G5</code> is the convolution with a 5 x 5 Gaussian kernel. Since <code class="literal">pyrUp</code> has already been implemented as an upsampling followed by a Gaussian blurring, all we need to do is downsample the original image, upsample it, and then subtract it from the original image. This can be accomplished by using the following code, as it appears in this chapter's <code class="literal">imagePyramid</code> sample:</p><div><pre class="programlisting">Mat gp1 = new Mat();
Imgproc.pyrDown(image, gp1);
Imgproc.pyrUp(gp1, gp1);
Core.subtract(image, gp1, gp1);</pre></div><p>In the preceding code, we assume that <code class="literal">image</code> is the image we are working on. Be careful when upsampling and then subtracting an image, since if the original image dimension is odd, they <a id="id133" class="indexterm"/>will have different dimensions. The <code class="literal">Core.subtract</code> function simply subtracts one image from another, as shown in the following screenshot:</p><div><img src="img/3972OS_03_11.jpg" alt="Image pyramids"/></div><p>In order to see some code working with pyramids, consider checking out this chapter's <code class="literal">imagePyramid</code> project. The preceding screenshot shows the application running the Laplacian filter. Also, play with the buttons to get a feeling of how pyramids work.</p></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec28"/>Thresholding</h1></div></div></div><p>One of the simplest <a id="id134" class="indexterm"/>methods of segmenting a grayscale image is using the threshold technique. It will basically set pixels below a given value as belonging to the interested object and the other pixels as not being part of it. Although it might suffer from illumination issues as well as problems that arise from variation inside the object, this can be enough when segmenting text in a page scan for OCR or to find a checkboard when calibrating the camera. Besides, some more interesting approaches, such as the adaptive threshold, can also yield good results in images that suffer from non-homogeneous lightning.</p><p>Basic thresholding is accomplished by means of Imgproc's <code class="literal">threshold</code> function, whose signature is as follows:</p><div><pre class="programlisting">public static double <strong>threshold</strong>(Mat src,
                               Mat dst,
                               double thresh,
                               double maxval,
                               int type)</pre></div><p>The <code class="literal">Mats src</code> and <code class="literal">dst</code> parameters are the input and output matrices, while <code class="literal">thresh</code> is the level used to threshold the <a id="id135" class="indexterm"/>image. double maxval is only used in the <code class="literal">Binary</code> and <code class="literal">Binary_Inv</code> modes and this will be explained in the following table. The <code class="literal">type</code> are Imgproc's constants used to describe the thresholding type, as in the following table, when tested in the next condition, the source pixel value is greater than the given threshold:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Thresholding type</p>
</th><th style="text-align: left" valign="bottom">
<p>Output when true</p>
</th><th style="text-align: left" valign="bottom">
<p>Output when false</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">CV_THRESH_BINARY</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">maxval</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CV_THRESH_BINARY_INV</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">maxval</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CV_THRESH_BINARY</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">threshold</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">source value</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CV_TOZERO</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">source value</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CV_TOZERO_INV</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">source value</code></p>
</td></tr></tbody></table></div><p>The following diagram will help you to easily understand the preceding table:</p><div><img src="img/3972OS_03_12.jpg" alt="Thresholding"/></div><p>When thresholding, it is important to experiment with several values using, for instance, a slider bar. The sample <a id="id136" class="indexterm"/>project <code class="literal">threshold</code> from this chapter makes it really easy to change the function's arguments and test the results. A screenshot of the project is shown as follows:</p><div><img src="img/3972OS_03_13.jpg" alt="Thresholding"/></div><p>Note that although the apple might pose a simple problem for segmentation, when applying the binary thresholding method, the apple is almost completely identified, except for the lighting spot above the middle line, which clearly has pixels above the 205 level, since they are <a id="id137" class="indexterm"/>almost pure white, which would be the 255 level. Besides, the shadow area under the apple is also identified as belonging to it. Aside from these minor problems, it is simple to use and will generally be part of one of the steps in any computer vision application.</p><p>Another interesting approach to this type of segmentation is related to the use of a dynamic threshold value. Instead of using a given value, the threshold is calculated as a mean of a square block around each pixel minus a given constant. This method is implemented in OpenCV through the <code class="literal">adaptiveThreshold</code> function, which has the following signature:</p><div><pre class="programlisting">public static void <strong>adaptiveThreshold</strong>(Mat src,
                                     Mat dst,
                                     double maxValue,
                                     int adaptiveMethod,
                                     int thresholdType,
                                     int blockSize,
                                     double C)</pre></div><p>The <code class="literal">Mat src </code>and <code class="literal">dst</code> parameters are the input and output matrices, respectively. <code class="literal">Maxvalue</code> is used the same way as the ordinary threshold function, which is described in the preceding section. The adaptive method can either be <code class="literal">ADAPTIVE_THRESH_MEAN_C</code> or <code class="literal">ADAPTIVE_THRESH_GAUSSIAN_C</code>. The first one will calculate the mean as the pixel value sum divided by the <a id="id138" class="indexterm"/>number of pixels in the block, while the latter will use Gaussian weighting for the average. <code class="literal">BlockSize</code> is the square <code class="literal">blockSize</code> by the <code class="literal">blockSize</code> region used for the mean whose value must be odd and greater than 1. The <code class="literal">C</code> constant is the value subtracted from the mean to compose the dynamic threshold. Note the result obtained for the same image with the adaptive threshold using <code class="literal">blocksize</code> of <code class="literal">13</code> and a constant <code class="literal">C</code> of <code class="literal">6</code>:</p><div><img src="img/3972OS_03_15.jpg" alt="Thresholding"/></div><p>Note that the shadow area is now much better, although the irregular texture from the apple can cause other <a id="id139" class="indexterm"/>problems. The sample code uses a binary and <code class="literal">ADAPTIVE_THRESH_MEAN_C</code> adaptive thresholding, but changing it for Gaussian is just a matter of changing the type parameter.</p></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Summary</h1></div></div></div><p>This chapter explained the theory and practice of basic image processing operations that will be required in any computer vision project. We started with filters that work with simple average or using a Gaussian weighting as well as a median and discussed the interesting bilateral filter, which maintains edges. Then, we explored the important morphological operators, such as erosion, dilation, opening, and closing, which appear in the context of isolating elements, removing noise, and joining distanced elements in an image. We followed this with the well-known paint bucket operation through flood filling. Then, we explored time and processing saving image pyramids, which make segmentation faster in higher levels by decreasing the image area to one quarter in each layer. We finally explained the important image segmentation technique called thresholding and tested the adaptive thresholding as well.</p><p>In the next chapter, we will focus on important image transforms, which will allow us to find edges, lines, and circles in images. Then, you will learn stretch, shrink, warp, and rotate operations, which will be followed by the Fourier transform, which is a nice tool to change image from the spatial domain to the frequency domain. Finally, we will check out integral images, which boost some face-tracking algorithms.</p></div></body></html>