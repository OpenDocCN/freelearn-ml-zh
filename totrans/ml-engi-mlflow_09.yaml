- en: '*Chapter 6*: Introducing ML Systems Architecture'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about general principles of **Machine Learning**
    (**ML**) systems architecture in the broader context of **Software Engineering**
    (**SWE**) and common issues with deploying models in production in a reliable
    way. You will also have the opportunity to follow along with architecting our
    ML systems. We will briefly look at how with MLflow, in conjunction with other
    relevant tools, we can build reliable and scalable ML platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will look at the following sections in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding challenges with ML systems and projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surveying state-of-the-art ML platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecting the PsyStock ML platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will follow a process of understanding the problem, studying different solutions
    from lead companies in the industry, and then developing your own relevant architecture.
    This three-step approach is transferrable to any future ML system that you want
    to develop.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need to meet the following prerequisites:'
  prefs: []
  type: TYPE_NORMAL
- en: The latest version of Docker installed on your machine. If you don't already
    have it installed, please follow the instructions at [https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of `docker-compose` installed. Please follow the instructions
    at [https://docs.docker.com/compose/install/](https://docs.docker.com/compose/install/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to Git in the command line and installed as described at [https://git-scm.com/book/en/v2/Getting-Started-Installing-Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a Bash terminal (Linux or Windows).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a browser.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.5+ installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latest version of your ML platform installed locally as described in [*Chapter
    3*](B16783_03_Final_SB_epub.xhtml#_idTextAnchor066), *Your Data Science Workbench*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding challenges with ML systems and projects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing a product leveraging ML can be a laborious task as some new concepts
    need to be introduced in the book around best practices of ML systems architecture.
  prefs: []
  type: TYPE_NORMAL
- en: So far in this book, we have shown how MLflow can enable the everyday model
    developer to have a platform to manage the ML life cycle from iteration on model
    development up to storing their models on the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, at this stage, we have managed to create a platform for the model
    developer to craft their models and publish the models in a central repository.
    This is the ideal stage to start unlocking potential in the business value of
    the models created. In an ML system, to make the leap from model development to
    a model in production, a change of mindset and approach is needed. After unlocking
    the value and crafting models, the exploitation phase begins, which is where having
    an ML systems architecture can set the tone of the deployments and operations
    of your models.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML systems are a specialization of the traditional SWE area and so we can and
    should leverage the body of knowledge in the SWE realm to architect our systems.
    Relevant concepts in SWE to our context are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Separation of concerns**: A complete system should be separated into different
    components. Each of the components of the system should be independent and focused
    on doing one thing well. For instance, a training component should be specialized
    in training and not doing scoring at the same time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autonomy**: Each component of the system should stand as an independent autonomous
    unit and be deployable independently. For example, the deployment of your API
    system should be independent of the deployment of the training system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resilience**: One consequence of the separation of concerns and modularity
    is that we must make sure that if one component of the wider system is faulty,
    this doesn''t affect independent components. If a batch scoring mechanism of a
    machine platform is broken, it shouldn''t affect the real-time system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: We should be able to scale the different components of our
    system independently and in accordance with its workload.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testability**: This represents the ability of a system to being tested and
    its functionality being verified against a set of representative inputs. In ML
    systems, this is particularly hard given the non-deterministic nature of models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous deployment/delivery**: This represents the ability to deploy systems
    in shorter cycles with almost no friction between a change in the code, configuration,
    models, or data, in the ML case, to have the new version of the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Composability**: We should be able to reuse the components of our systems
    in future projects to increase the return on investment. So, an ML engineer needs
    to be sure that the code and components being developed are easily reusable and/or
    interoperable with other systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintainability**: This is the ease at which a system can be modified, fixed,
    and improved to meet and adapt to the demands of a changing environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this stage, we can briefly introduce and refine our use case, of stock prediction,
    to develop our ML platform in the PsyStock company.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the work done so far in prototyping models to **predict the price of
    Bitcoin**, the business development department of the company decided to start
    its first product as a **Prediction API for cryptocurrencies** as they are becoming
    a popular technology in the corporate world. A team was assembled that decided
    to investigate challenges and state-of-the-art platforms, and then architect the
    company's own platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'An ML project generally involves many departments of a company. Imagining the
    hypothetical case of PsyStock, a typical ML project team involves the following
    stakeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data science team**: Responsible for building and developing the model with
    the goal of achieving the highest accuracy on their prediction of cryptocurrency
    prices and market movements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML/data engineering team**: Responsible for the engineering components, including
    data acquisition, preparation, training, and deployment, and is interested in
    the system being correctly deployed and running on spec in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Infrastructure team**: Responsible for providing compute and infrastructure
    resources. Expects that the system will not cause an operational load to the team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Product team**: Provides integration with the web platforms and the overall
    software of the company and drives the feature creation, ensuring a speedy inference
    speed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Business development/marketing team**: Packages and markets the product and
    monitors the business performance of the product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will understand general challenges in ML systems and
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: ML is an important application of technology to help unlock value in organizations
    using data. In the application of ML in the business world, there are no standard
    practices defined and a big number of organizations struggle to get products backed
    by ML in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the real world, a naive way to move models into production would consist
    of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data scientist produces a model in a notebook environment and implements the
    code in R.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data scientist shares the notebook with the engineering team, signaling
    that they're ready to send their model to production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The engineering team reimplements the training process in a language that they
    can understand, in this case, Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A long process of trial and error until the data science team and engineering
    team are in agreement that the model produced by the bespoke training system is
    producing equivalent outputs to the original one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new system is created and developed to score systems and the engineering team
    notes a high latency. The model is sent to redevelopment as it can't be redeveloped
    in the current status.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The situation described in the previous paragraph is more common than you might
    imagine. It is described in detail in the paper by *D. Sculley et al., Hidden
    Technical Debt in Machine Learning Systems (2015)*. The following risk factors
    and technical debt related to implementing ML platforms naively were identified
    by a team at Google:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Boundary erosion**: ML systems by their nature mix signals of different logical
    domains. Maintaining clear logic of business domains as is possible in SWE is
    challenging. Another natural issue is the temptation of using a model output as
    input of a third model, *A*, and changes might have unexpected effects in model
    *B*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Costly data dependencies**: Fresh, accurate, and dependable data is the most
    important ingredient of an ML system. For example, in the cryptocurrency case,
    in order to be able to predict, an external API might be consulted in combination
    with social network sentiment signals. At a given point, one of the data signals
    might be unavailable, making one of the components unavailable. Data distributions
    in the real world can change, causing the model inference to be irrelevant in
    the real world.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feedback loops**: In some contexts, the model influences the data selected
    for training. Credit scoring is a good example of such a case. A model that decides
    who gets credit for the next re-training of the model will select training data
    from the population that was affected by the model. Analyzing the effect of the
    model on the ground data is important to take into consideration when developing
    your model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**System-level anti patterns**: ML systems are notoriously known for harboring
    glue code with different packages and without proper abstractions. In some cases,
    multiple languages are used to implement in the library given the iterative nature
    of developing code in a notebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Configuration management**: Generally left as an afterthought in ML systems,
    information about the configuration that yielded a particular result is paramount
    for the post-analysis of models and deployment. Not using established configuration
    management practices can introduce errors in ML pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and testing**: Integration testing and unit testing are common
    patterns in SWE projects that due to the stochastic nature of ML projects are
    harder to implement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One important practice to tackle the challenges in ML systems is to add extensive
    tests on critical parts of the process, on your code, during model training and
    when running on your system, as shown in *Figure 6.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image0012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Testing in ML systems extracted from https://research.google/pubs/pub46555/
  prefs: []
  type: TYPE_NORMAL
- en: What *Figure 6.1* illustrates is one approach to address technical debt by testing
    the different parts of the system through standard software practices with the
    addition of specialized monitoring for data predictions. The important new additions
    are tests on data and tests on the model, so testing incoming data and training
    data and at the same time being able to monitor these tests and decide whether
    the system passes the relevant criteria is critical.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow as a platform addresses some of the issues referred to in this section
    as problems for ML systems. MLflow is focused on a specific set of dimensions
    of the ML technical debt and is a good pillar component to create an ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at some examples of state-of-the-art robust
    ML engineering systems, to guide our development.
  prefs: []
  type: TYPE_NORMAL
- en: Surveying state-of-the-art ML platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At a high level, a mature ML system has the components outlined in *Figure
    6.2*. These components are ideally independent and responsible for one particular
    feature of the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image0023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Components of an ML platform
  prefs: []
  type: TYPE_NORMAL
- en: 'Following the lead from SWE modularization, these general components allow
    us to compare different ML platforms and also specify our PsyStock requirements
    for each of the components. The components that we choose to use as a reference
    for architecture comparison are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data and feature management**: The component of data and feature management
    is responsible for data acquisition, feature generation, storing, and serving
    the modules upstream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training infrastructure**: The component that handles the process of the
    training of models, scheduling, consuming features, and producing a final model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment and inference**: The responsibility of this unit is for the deployment
    inference and batch scoring of a model. It is the external face of the system
    and is accessible to external systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance and monitoring**: A component that handles observability, metrics
    posted by different systems, and monitoring systems in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model management**: Manages model artifact versions and the life cycle of
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflow management**: The component responsible for orchestrating batch
    workflows and processing pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After having described the different components of an ML platform, we will look
    at some examples starting with Uber's Michelangelo.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know Michelangelo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Uber was one of the first companies to document widely their realization that
    an ML platform was critical to unlocking value on the data produced.
  prefs: []
  type: TYPE_NORMAL
- en: 'The internal motivations at Uber to build the platform were the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Limited impact of ML due to huge resources needed when translating a local model
    into production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unreliable ML and data pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineering teams had to create custom serving containers and systems for the
    systems at hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inability to scale ML projects.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following *Figure 6.3* (retrieved from [https://eng.uber.com/michelangelo-machine-learning-platform](https://eng.uber.com/michelangelo-machine-learning-platform))
    shows the different components of Michelangelo. One significant component is the
    data component of the Uber infrastructure decoupling real-time data infrastructure
    with streaming systems such as Kafka to acquire data from the outside from where
    the data flows to a training process, and from there to scoring in both real-time
    and offline mode. Distinctive features are a separation of the batch world and
    real-time world and the existence of generic prediction services for API and batch
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image0033.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure.6.3 – Michelangelo architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The components that we choose to use as a reference for architecture comparison
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data and feature management**: It consists of a centralized data store with
    all the features that are needed to serve models and train models. The feature
    data store can be accessed in real time and in batch. For the batch scenario,
    they use a database system called Hive and for real time, they use Cassandra.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training infrastructure**: Distributed training infrastructure with a tool
    called **Horovod** ([https://github.com/horovod/horovod](https://github.com/horovod/horovod))
    with specialized and bespoke components and enhanced reporting. It provides custom
    metrics for each type of model (deep learning, models, feature importance, and
    so on). The output of the training job is the model repo using as a backend the
    Cassandra database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment and inference**: The systems deployed through standard SWE practices
    (CI/CD, rollbacks on metrics monitoring, and so on), generally compiled as artifacts
    served over Uber data centers. A prediction service that receives a request and
    based on header information routes pre-loads the right model and feeds the prediction
    vector, using an internal DSL that is able to query for further data augmentation
    on the serving layer of the feature store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance and monitoring**: It leverages the general centralized logging
    system of the company. For monitoring predictions, metrics are produced of predictions
    and real-world values and differences are logged. The errors of the model can
    in this way be analyzed and monitored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model management**: Models are compiled as artifacts and stored in a Cassandra
    data store.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflow management**: Provides an API for wiring the pipelines. It contains
    a management plane with a UI that allows the management of models and deployments.
    Workflow management is API-driven and can be managed in either Python or Java
    from the outside.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The clear advantage for a company such as Uber to have built their own system
    is agility and the ability to cater to their very specific use case.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know Kubeflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubeflow in some way is an open source platform for the ML life cycle for **Kubernetes**
    environments. It's basically an ecosystem of tools that work together to provide
    the main components of an ML platform. Kubeflow was initially developed at Google
    and it's currently a very active open source project.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes** is one of the leading open source computational environments
    that allows flexibility in allocating computing and storage resources for containerized
    workloads. It was created originally at Google. In order to understand Kubeflow,
    a basic understanding of Kubernetes is needed. The following official documentation
    link contains the prerequisites to understand the basics: [https://kubernetes.io/docs/concepts/overview/](https://kubernetes.io/docs/concepts/overview/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in *Figure 6.4*, it uses the foundation of Kubernetes and provides
    a set of applications for the ML workflow where different tools compatible with
    the standards set by Kubeflow can be coalesced to provide a coherent set of services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image0043.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Retrieved from https://www.kubeflow.org/docs/started/kubeflow-overview/
  prefs: []
  type: TYPE_NORMAL
- en: 'The components that we choose to use as a reference for architecture comparison
    are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data and feature management**: Kubeflow provides integration with big data
    tools such as Spark and others. A component of the ecosystem used for data and
    feature management is called Feast, an open source feature for ML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training infrastructure**: Kubeflow provides specific types of Kubeflow operators
    for common models such as, for instance, TensorFlow, PyTorch, and custom-made
    ones. The training jobs will basically be specific Kubernetes jobs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment and inference**: Kubeflow provides multiple integrations with
    third-party tools such as TensorFlow Serving, Seldon Core, and KFServing with
    different trade-offs and maturity levels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Performance and monitoring**: Prometheus is a general tool used for monitoring
    within the Kubernetes environment and can be leveraged in this context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model management**: Not a specific tool for managing models but tools such
    as MLflow can be added to cover the model management life cycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workflow management**: Workflow management is leveraged through a specific
    tool called Kubeflow Pipelines built on top of a generic pipeline tool for Kubernetes
    called Argo Workflows. It allows multi-step pipelines to be built in code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After looking at reference architectures, we will now spend time crafting our
    own, armed with the state-of-the-art knowledge available in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: Architecting the PsyStock ML platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a set of desirable tenets that we can define for our ML platform based
    on a distillation of the research on best practices and example reference architectures.
    The main tenets that we want to maintain in our platform are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Leverage** **open systems and standards**: Using open systems such as the
    ones available in MLflow allows longevity and flexibility to leverage the open
    source community advances and power to extend the company ML platform at a lower
    cost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Favor** **scalable solutions**: A company needs to be prepared for a future
    surge in growth; although this is the first version, the ability to surge on-demand
    from training and perspective needs to be in place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Integrated** **reliable data life cycle**: Data is the center of gravity
    of the ML platform and should be managed in a reliable and traceable manner at
    scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Follow** **SWE best practices**: For example, separation of concerns, testability,
    CI/CD, observability, and modularity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Maintain** **vendor and cloud independence**: PsyStock being a start-up is
    operating in a very dynamic environment and in different geographies with access
    to different clouds and, in some cases, with compliance requirements of not moving
    the data from the given geography. So, being cloud-agnostic and being able to
    have workloads in different environments is a competitive advantage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tenets will allow us to frame our systems architecture within an open
    and low-cost solution for the company and allow the flexibility of running on
    the different systems on-premises, in the cloud, or local.
  prefs: []
  type: TYPE_NORMAL
- en: We have previously defined the business requirements of the prediction use cases,
    namely detection of the movement of cryptocurrency and value prediction. To leverage
    this and other use cases, the creation of an ML platform is critical to the company.
  prefs: []
  type: TYPE_NORMAL
- en: Now, armed with the knowledge from the research and description of state-of-the-art
    systems, we will next define eliciting the features of our ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Describing the features of the ML platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Creating a specification of features is extremely important to keep the development
    efforts focused on a narrow set of features that unlock value to users of the
    platform. In this section, we will elicit the features that will realize the best
    value of an ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our system, we want to be able to have the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature: Schedule training jobs**: A data scientist needs to be able to schedule
    training jobs for their models using configuration or equivalent code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature: Deploy seamlessly different models developed from the data science
    workbench**: The company already has a data science workbench developed in *Chapter
    3*, *Your Data Science Workbench*. We want to be able to leverage all the work
    previously done so models developed on the platform can be deployed in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature: Allow recalibration of models in presence of new data**: When new
    data arrives in a specific location, a new model needs to be generated automatically
    and stored in a model registry accessible to systems and humans of the platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature: Submit and configure batch scoring jobs**: The platform should allow
    the relevant users to configure and schedule batch jobs in the presence of new
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature: Efficient inference API-based scoring for the following APIs**:
    Given a model it should be a feature of the platform the creation of matching
    API using the model schema.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After discussing the ideal features of an ML system, we will start in the next
    section with architecting the system at a high level.
  prefs: []
  type: TYPE_NORMAL
- en: High-level systems architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now focus on defining the building blocks of our architecture and the
    different data flows between the different components.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the features specified and tenets of the previous section, our ML platform
    and solution should contain the following components as described by the architecture
    diagram in *Figure 6.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/image0053.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure.6.5 – Architectural diagram of an ML platform
  prefs: []
  type: TYPE_NORMAL
- en: This diagram is specifically agnostic from technology choices as this will be
    done in the respective upcoming chapters where each of the components' engineering
    will be explored fully.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data and feature management**: This is executed by the **Feature / Data Layer**,
    which receives feature registrations from the workbench and allows the registration
    of datasets from the workbench. The data layer provides data to the training environment.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Training infrastructure**: The training infrastructure component allows the
    schedule of training of jobs based on a request from the data science workbench.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deployment and inference**: The deployment environment consumes models to
    execute either in batch or in real time prompted either by data in the data layer
    or by requests via production systems.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Performance and monitoring**: This is accomplished through the central component
    of monitoring and metrics that all the systems surrounding the component publish
    metrics into.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model management**: Encapsulated by the component of **Model Registry**,
    which contains a store and the associated life cycles projects. The input comes
    primarily from the training jobs and the data science workbench.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Workflow management**: This is a component that allows the orchestration
    of the different systems. For example, it allows scheduling jobs and dependency
    management, enforcing the order of execution. For example, an inference batch
    job can only be executed after a training job. This can be achieved through the
    operating system using a Cron system or through more sophisticated workflow tools
    such as Airflow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will next briefly touch on how we will realize the ideas outlined in this
    section with **MLflow**.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow and other ecosystem tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MLflow is a tool that was created by the open source community to address a
    gap in open systems for ML systems, focused on reproducibility, model management,
    and deployment. MLflow is by no means a complete tool; it needs other components
    and is a central part of an ML solution when its strengths are leveraged.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, systems such as **Kubeflow** have been emerging in the Kubernetes
    world to help manage the infrastructure side of ML systems and being the actual
    deployment environment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Minio** is a storage system that ships with Kubeflow that will be used as
    an agnostic storage mechanism for metadata and datasets and provides an abstraction
    for storage on both cloud and local environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Having identified best practices in the industry with regards to ML platforms,
    outlining our requirements, and, in this section, architecting our ML platform,
    we will spend the next four chapters of the book building each of the components
    of our platform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the concepts involved in architecting ML systems,
    mapped stakeholders, identified common issues and best practices, and outlined
    the initial architecture. We identified critical building blocks of an ML systems
    architecture on the data layer and modeling and inference layer. The interconnection
    between the components was stressed and a specification of features was outlined.
  prefs: []
  type: TYPE_NORMAL
- en: We also addressed how MLflow can be leveraged in your ML platform and the shortcomings
    that can be complemented by other reference tools.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapters and section of the book, we will focus on applying the
    concepts learned so far to real-life systems and we will practice by implementing
    the architecture of the PsyStock ML platform. We will have one chapter dedicated
    to each component, starting from specification up to the implementation of the
    component with practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to further your knowledge, you can consult the documentation at the
    following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.mlflow.org/docs/latest/models.html](https://www.mlflow.org/docs/latest/models.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High interest of technical debt – [https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf](https://papers.nips.cc/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CS 329S**: ML systems design, *Chip Huyen* – [https://cs329s.stanford.edu](https://cs329s.stanford.edu),
    2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
