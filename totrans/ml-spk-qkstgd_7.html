<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep Learning Using Apache Spark</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will go on a hands-on exploration of the exciting and cutting-edge world of deep learning! We will use third-party deep learning libraries in conjunction with Apache Spark's <kbd>MLlib</kbd> to perform accurate <strong>optical character recognition</strong> (<strong>OCR</strong>) and automatically recognize and classify images via the following types of artificial neural networks and machine learning algorithms:</p>
<ul>
<li>Multilayer perceptrons</li>
<li>Convolutional neural networks</li>
<li>Transfer learning</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Artificial neural networks</h1>
                </header>
            
            <article>
                
<p>As we studied in <a href="a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml" target="_blank">Chapter 3</a>, <em>Artificial Intelligence and Machine Learning</em>, an <strong>artificial neural network</strong> (<strong>ANN</strong>) is a connected group of artificial neurons that is aggregated into three types of linked neural layers—the input layer, zero or more hidden layers, and the output layer. A <strong>monolayer</strong> ANN consists of just <em>one</em> layer of links between the input nodes and output nodes, while <strong>multilayer</strong> ANNs are characterized by the segmentation of artificial neurons across multiple linked layers.</p>
<p>An ANN where signals are propagated in one direction only—that is, the signals are received by the input layer and forwarded to the next layer for processing—are called <strong>feedforward</strong> networks. ANNs where a signal may be propagated back to artificial neurons or neural layers that have already processed that signal are called <strong>feedback</strong> networks.</p>
<p><strong>Backwards propagation</strong> is a supervised learning process by which multilayer ANNs can learn—that is, derive an optimal set of weight coefficients. First, all weights are initially set as random and the output from the network is calculated. If the predicted output does not match the desired output, the total error at the output nodes is propagated back through the entire network in an effort to readjust all weights in the network so that the error is reduced in the output layer. In other words, backwards propagation seeks to minimize the difference between the actual output and the desired output via an iterative weight adjustment process.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multilayer perceptrons</h1>
                </header>
            
            <article>
                
<p class="mce-root">A <strong>single-layer perceptron</strong> (<strong>SLP</strong>) is a basic type of ANN that consists of just two layers of nodes—an input layer containing input nodes and an output layer containing output nodes. A <strong>multilayer perceptron</strong> (<strong>MLP</strong>), however, introduces one or more hidden layers between the input and output layers, giving them the ability to learn nonlinear functions, as illustrated in <em>Figure 7.1</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-584 image-border" src="Images/2ca10ecb-d7ec-4d8c-9814-6a71948f7819.png" style="width:21.17em;height:26.58em;" width="845" height="1068"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.1: Multilayer perceptron neural architecture</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">MLP classifier</h1>
                </header>
            
            <article>
                
<p>Apache Spark's machine learning library, <kbd>MLlib</kbd>, provides an out-of-the-box <strong>multilayer perceptron classifier</strong> (<strong>MLPC</strong>) that can be applied to classification problems where we are required to predict from <em>k</em> possible classes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Input layer</h1>
                </header>
            
            <article>
                
<p>In <kbd>MLlib</kbd>'s MLPC, the nodes in the input layer represent the input data. Let's denote this input data as a vector, <em>X</em>, with <em>m</em> features, as follows:</p>
<p style="padding-left: 240px"><img src="Images/5e5fdf3c-d101-4692-8748-4c03a602c366.png" style="width:9.25em;height:1.17em;" width="1430" height="180"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Hidden layers</h1>
                </header>
            
            <article>
                
<p>The input data is then passed to the hidden layers. For the sake of simplicity, let's say that we have only one hidden layer, <em>h<sup>1</sup></em>, and that within this one hidden layer, we have <em>n</em> neurons, as follows:</p>
<p style="padding-left: 210px"><img src="Images/f3905ed0-3ff0-4448-886c-950002a5b445.png" style="width:11.25em;height:1.33em;" width="2110" height="250"/></p>
<p>The net input, <em>z</em>, into the activation function for each of these hidden neurons is then the input data set vector, <em>X</em>, multiplied by a weight set vector, <em>W</em><sup>n</sup> (corresponding to the weight sets assigned to the <em>n</em> neurons in the hidden layer), where each weight set vector, <em>W</em><sup>n</sup>, contains <em>m</em> weights (corresponding to the <em>m</em> features in our input data set vector <em>X</em>), as follows:</p>
<p style="padding-left: 210px"><img src="Images/b9f36ad6-5367-4754-aa2d-e09a06d96ae3.png" style="width:11.00em;height:1.50em;" width="1680" height="220"/></p>
<p>In linear algebra, the product of multiplying one vector by another is called the <strong>dot product</strong>, and it outputs a scalar (that is, a number) represented by <em>z</em>, as follows:</p>
<p style="padding-left: 90px"><img src="Images/a12eea96-9a2e-49c2-a62b-31d49b582111.png" style="width:30.00em;height:3.50em;" width="4630" height="540"/></p>
<p>The <strong>bias</strong>, as illustrated in <a href="a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml" target="_blank">Chapter 3</a>, <em>Artificial Intelligence and Machine Learning</em>, and shown in <em>Figure 3.5</em>, is a <em>stand-alone</em> constant analogous to the intercept term in a regression model, and may be added to non-output layers in feedforward neural networks. It is called standalone because bias nodes are not connected to preceding layers. By introducing a constant, we allow for the output of an activation function to be shifted left or right by that constant, thereby increasing the flexibility of an ANN to learn patterns more effectively by providing the ability to shift decision boundaries based on the data.</p>
<p>Note that in a single hidden layer containing <em>n</em> hidden neurons, <em>n</em> dot product calculations will be computed, as illustrated in <em>Figure 7.2</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-585 image-border" src="Images/5ea4e211-6519-4181-951a-55aab034be62.png" style="width:31.25em;height:32.08em;" width="1052" height="1084"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.2: Hidden layer net input and output</div>
<p>In <kbd>MLlib</kbd>'s MLPC, the hidden neurons use the <strong>sigmoid</strong> activation function, as shown in the following formula:</p>
<p style="padding-left: 210px"><img src="Images/b83c9aed-746c-4a14-8e42-c1e1c0b0d976.png" style="width:11.00em;height:2.58em;" width="1830" height="420"/></p>
<p>As we saw in <a href="a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml" target="_blank">Chapter 3</a>, <em>Artificial Intelligence and Machine Learning</em>, the sigmoid <span>(or logistic) </span><span>function</span><span> </span><span>is bounded between 0 and 1, and is smoothly defined for all real input values. By using the sigmoid activation function, the nodes in the hidden layers actually correspond to a logistic regression model. If we study the sigmoid curve, as shown in <em>Figure 7.3</em>, we can state that if the net input, </span><em>z</em><span>, is a large positive number, then the output of the sigmoid function, and hence the activation function for our hidden neurons, will be close to 1. Conversely, if the net input, z, is a negative number with a large absolute value, then the output of the sigmoid function will be close to 0:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-586 image-border" src="Images/d1c9a1b6-f7d5-4fa2-b3f3-e56f9ec4e9d8.png" style="width:35.17em;height:27.92em;" width="1058" height="843"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.3: Sigmoid function</div>
<p>In all cases, each hidden neuron will take the net input, <em>z</em>, which is the dot product of the input data, <em>X</em>, and the weight set, <em>W<sup>n</sup></em>, plus a bias, and apply that to the sigmoid function, finally outputting a number between 0 and 1. After all hidden neurons have computed the result of their activation function, we will then have <em>n</em> hidden outputs from our hidden layer <em>h<sup>1</sup></em>, as follows:</p>
<p style="padding-left: 180px"><img src="Images/75bb784d-640c-4f8c-84b8-a7d203e8ff67.png" style="width:12.42em;height:1.42em;" width="2190" height="250"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Output layer</h1>
                </header>
            
            <article>
                
<p>The hidden layer outputs are then used as inputs to calculate the final outputs in the output layer. In our case, we only have a single hidden layer, <em>h<sup>1</sup></em>, with outputs <img class="fm-editor-equation" src="Images/bcbb9457-7f00-4b1b-baa1-f65957755e17.png" style="width:6.00em;height:1.33em;" width="1080" height="240"/>. These then become <em>n</em> inputs into the output layer.</p>
<p>The net input into the activation function for the output layer neurons is then these <em>n</em> inputs computed by the hidden layer and multiplied by a weight set vector, <em>W<sup>h</sup></em>, where each weight set vector, <em>W<sup>h</sup></em>, contains <em>n</em> weights (corresponding to the <em>n</em> hidden layer inputs). For the sake of simplicity, let's assume that we only <span>have</span><span> </span><span>one output neuron in our output layer. The weight set vector for this neuron is therefore the following:</span></p>
<p style="padding-left: 180px"><img src="Images/ea76a3ab-cd53-432f-8ff0-7529a5ffecf0.png" style="width:11.67em;height:1.58em;" width="1830" height="250"/></p>
<p>Again, since we are multiplying vectors together, we use the dot product calculation, which will compute the following scalar representing our net input, <em>z</em>:</p>
<p style="padding-left: 180px"><img src="Images/30abd217-cdd9-48e5-9c85-9918a5d4770c.png" style="width:10.92em;height:3.50em;" width="1680" height="540"/></p>
<p>In <kbd>MLlib</kbd>'s MLPC, the output neurons use the softmax function as the activation function, which extends logistic regression by predicting <em>k</em> classes instead of a standard binary classification. This function takes the following form:</p>
<p style="padding-left: 180px"><img src="Images/87f99f9f-51bc-4666-82a6-b603514c8cb6.png" style="width:12.67em;height:3.25em;" width="1950" height="500"/></p>
<p>Therefore, the number of nodes in the output layer corresponds to the number of possible classes that you wish to predict from. For example, if your use case has five possible classes, then you would train an MLP with five nodes in the output layer. The final output from the activation function is therefore the prediction that the output neuron in question makes, as illustrated in <em>Figure 7.4</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-587 image-border" src="Images/d50b70a5-c32c-4e22-b6e1-884a0b11d36b.png" style="width:41.50em;height:32.42em;" width="1385" height="1080"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.4: Output layer net input and output</div>
<p>Note that <em>Figure 7.4</em> illustrates the initial <strong>forward propagation</strong> of the MLP, whereby input data is propagated to the hidden layer and the output from the hidden layer is propagated to the output layer where the final output is computed. <kbd>MLlib</kbd>'s MLPC thereafter uses <strong>backwards propagation</strong> to train the neural network and learn the model where the difference between the actual output and the desired output is minimized via an iterative weight adjustment process. MLPC achieves this by seeking to minimize a <strong>loss function</strong>. A loss function calculates a measure of the price paid for inaccurate predictions regarding classification problems. The specific loss function that MLPC employs is the <strong>logistic loss function</strong>, where predictions made with a high value of confidence are penalized less. To learn more about loss functions, please visit <a href="https://en.wikipedia.org/wiki/Loss_functions_for_classification">https://en.wikipedia.org/wiki/Loss_functions_for_classification</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study 1 – OCR</h1>
                </header>
            
            <article>
                
<p>A great real-world use case to demonstrate the power of MLPs is that of OCR. In OCR, the challenge is to recognize human writing, classifying each handwritten symbol as a letter. In the case of the English alphabet, there are 26 letters. Therefore, when applied to the English language, <span>OCR</span><span> </span><span>is actually a classification problem that has <em>k </em>= 26 possible classes!</span></p>
<div class="packt_infobox">The dataset that we will be using has been derived from the <strong>University of California's</strong> (<strong>UCI</strong>) Machine Learning Repository, which is found at <a href="https://archive.ics.uci.edu/ml/index.php">https://archive.ics.uci.edu/ml/index.php</a>. The specific letter recognition dataset that we will use, available from both the GitHub repository accompanying this book and from <a href="https://archive.ics.uci.edu/ml/datasets/letter+recognition">https://archive.ics.uci.edu/ml/datasets/letter+recognition</a>, was created by David J. Slate at Odesta Corporation; 1890 Maple Ave; Suite 115; Evanston, IL 60201, and was used in the paper <span><em>Letter Recognition Using Holland-style Adaptive Classifiers</em> by </span><span>P. W. Frey and D. J. Slate (from Machine Learning Vol 6 #2 March 91)</span><span>.</span></div>
<p class="mce-root"><em>Figure 7.5</em> <span>provides an example illustration of this dataset rendered visually. We will train an MLP classifier to recognize and classify each of the symbols, such as those shown in</span> <em>Figure 7.5</em><span>, as a letter of the English alphabet:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/3c356df0-4c5d-4c5e-a656-bd2a295c0b1e.png" style="width:8.00em;height:10.00em;" width="476" height="600"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.5: Letter recognition dataset</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Input data</h1>
                </header>
            
            <article>
                
<p>Before we delve further into the schema of our specific dataset, let's first understand how a MLP will actually help us with this problem. Firstly, as we saw in <a href="eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml" target="_blank">Chapter 5</a>, <em>Unsupervised Learning Using Apache Spark</em>, when studying image segmentation, images can be broken down into a matrix of either pixel-intensity values (for grayscale images) or pixel RGB values (for images with color). A single vector containing (<em>m</em> x <em>n</em>) numerical elements can then be generated, corresponding to the pixel height (<em>m</em>) and width (<em>n</em>) of the image.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Training architecture</h1>
                </header>
            
            <article>
                
<p>Now, imagine that we want to train an MLP using our entire letter recognition dataset, as illustrated in <em>Figure 7.6</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-589 image-border" src="Images/18a703a1-9640-48e9-934e-228916131d45.png" style="width:23.33em;height:30.92em;" width="879" height="1164"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.6: Multilayer perceptron for letter recognition</div>
<p>In our MLP, we have <em>p</em> (= <em>m</em> x <em>n</em>) neurons in our input layer that represent the <em>p</em> pixel-intensity values from our image. A single hidden layer has <em>n</em> neurons, and the output layer has 26 neurons that represent the 26 possible classes or letters in the English alphabet. When training this neural network, since we do not know initially what weights should be assigned to each layer, we initialize the weights randomly and perform a first iteration of forward propagation. We then iteratively employ backwards propagation to train the neural network, resulting in a set of weights that have been optimized so that the predictions/classifications made by the output layer are as accurate as possible.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Detecting patterns in the hidden layer</h1>
                </header>
            
            <article>
                
<p>The job of the neurons in the hidden layer is to learn to detect patterns within the input data. In our case, the neurons in the hidden layer(s) will detect the presence of certain substructures that constitute a wider symbol. This is illustrated in <em>Figure 7.7</em>, where we assume that the first three neurons in the hidden layer learn to recognize forward slash, back slash and horizontal line type patterns respectively:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-590 image-border" src="Images/94bcd340-a59d-41b5-a205-4ef7066b99eb.png" style="width:30.67em;height:33.42em;" width="1066" height="1165"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.7: Neurons in the hidden layer detect patterns and substructures</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Classifying in the output layer</h1>
                </header>
            
            <article>
                
<p>In our neural network, the first neuron in the output layer is trained to decide whether a given symbol is the uppercase English letter <em>A</em>. Assuming that the first three neurons in the hidden layer fire, we would expect the first neuron in the output layer to fire and the remaining 25 neurons not to fire. Our MLP would then classify this symbol as the letter <em>A</em>!</p>
<p>Note that our training architecture employed only a single hidden layer, which would only be able to learn very simple patterns. By adding more hidden layers, an ANN can learn more complicated patterns at the cost of computational complexity, resources, and training runtime. However, with the advent of distributed storage and processing technologies, as discussed in <a href="337b904f-87f1-4741-bd75-d7fd983185f6.xhtml" target="_blank">Chapter 1</a>, <em>The Big Data Ecosystem</em>, where huge volumes of data may be stored in memory and a large number of calculations may be processed on that data in a distributed manner, today we are able to train extremely complex neural networks with architecture that may contain large numbers of hidden layers and hidden neurons. Such complex neural networks are currently being applied to a broad range of applications, including facial recognition, speech recognition, real-time threat detection, image-based searching, fraud detection, and advances in healthcare.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">MLPs in Apache Spark</h1>
                </header>
            
            <article>
                
<p>Let's return to our dataset and train an MLP in Apache Spark to recognize and classify letters from the English alphabet. If you open <kbd>ocr-data/letter-recognition.data</kbd> in any text editor, from either the GitHub repository accompanying this book or from UCI's machine learning repository, you will find 20,000 rows of data, described by the following schema:</p>
<table style="border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px">
<p><strong>Column name</strong></p>
</td>
<td>
<p><strong>Data type</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>lettr</kbd></p>
</td>
<td>
<p><kbd>String</kbd></p>
</td>
<td>
<p>English letter (one of 26 values, from A to Z)</p>
</td>
</tr>
<tr>
<td>
<p><kbd>x-box</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Horizontal position of box</p>
</td>
</tr>
<tr>
<td>
<p><kbd>y-box</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Vertical position of box</p>
</td>
</tr>
<tr>
<td>
<p><kbd>width</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Width of box</p>
</td>
</tr>
<tr>
<td>
<p><kbd>high</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Height of box</p>
</td>
</tr>
<tr>
<td>
<p><kbd>onpix</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Total number of on pixels</p>
</td>
</tr>
<tr>
<td>
<p><kbd>x-bar</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Mean <em>x</em> of on pixels in the box</p>
</td>
</tr>
<tr>
<td>
<p><kbd>y-bar</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Mean <em>y</em> of on pixels in the box</p>
</td>
</tr>
<tr>
<td>
<p><kbd>x2bar</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Mean <em>x</em> variance</p>
</td>
</tr>
<tr>
<td>
<p><kbd>y2bar</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Mean <em>y</em> variance</p>
</td>
</tr>
<tr>
<td>
<p><kbd>xybar</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Mean <em>x y</em> correlation</p>
</td>
</tr>
<tr>
<td>
<p><kbd>x2ybr</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Mean of <em>x</em> * <em>x</em> * <em>y</em></p>
</td>
</tr>
<tr>
<td>
<p><kbd>xy2br</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Mean of <em>x</em> * <em>y</em> * <em>y</em></p>
</td>
</tr>
<tr>
<td>
<p><kbd>x-ege</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Mean edge count left to right</p>
</td>
</tr>
<tr>
<td>
<p><kbd>xegvy</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Correlation of <kbd>x-ege</kbd> with <em>y</em></p>
</td>
</tr>
<tr>
<td>
<p><kbd>y-ege</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Mean edge count, bottom to top</p>
</td>
</tr>
<tr>
<td>
<p><kbd>yegvx</kbd></p>
</td>
<td>
<p><kbd>Integer</kbd></p>
</td>
<td>
<p>Correlation of <kbd>y-ege</kbd> with <em>x</em></p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>This dataset describes 16 numerical attributes representing statistical features of the pixel distribution based on scanned character images, such as those illustrated in <em>Figure 7.5</em>. These attributes have been standardized and scaled linearly to a range of integer values from 0 to 15. For each row, a label column called <kbd>lettr</kbd> denotes the letter of the English alphabet that it represents, where no feature vector maps to more than one class—that is, each feature vector maps to only one letter in the English alphabet.</p>
<div class="packt_infobox">You will have noticed that we are not using the pixel data from the <em>raw</em> images themselves, but rather statistical features derived from the distribution of the pixels. However, using what we have learned from <a href="eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml" target="_blank">Chapter 5</a>, <em>Unsupervised Learning Using Apache Spark</em>, when we looked at specifically converting images into numerical feature vectors, the exact same steps that we will look at in a moment may be followed to train an MLP classifier using the raw images themselves. </div>
<p>Let's now use this dataset to train an MLP classifier to recognize symbols and classify them as letters from the English alphabet:</p>
<div class="mce-root packt_infobox">The following subsections describe each of the pertinent cells in the corresponding Jupyter notebook for this use case, called <kbd>chp07-01-multilayer-perceptron-classifier.ipynb</kbd>. This notebook can be found in the GitHub repository accompanying this book.</div>
<ol>
<li>First, we import the prerequisite PySpark libraries as normal, including <kbd>MLlib</kbd>'s <kbd>MultilayerPerceptronClassifier</kbd> classifier and <kbd>MulticlassClassificationEvaluator</kbd> evaluator respectively, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">import findspark<br/>findspark.init()<br/>from pyspark import SparkContext, SparkConf<br/>from pyspark.sql import SQLContext<br/>from pyspark.ml.feature import VectorAssembler<br/>from pyspark.ml.classification import MultilayerPerceptronClassifier<br/>from pyspark.ml.evaluation import MulticlassClassificationEvaluator</pre>
<ol start="2">
<li>After instantiating a Spark context, we are now ready to ingest our dataset into a Spark dataframe. Note that in our case, we have preprocessed the dataset into CSV format, where we have converted the <kbd>lettr</kbd> column from a <kbd>string</kbd> datatype to a <kbd>numeric</kbd> datatype representing one of the 26 characters in the English alphabet. This preprocessed CSV file is available in the GitHub repository accompanying this book. Once we have ingested this CSV file into a Spark dataframe, we then generate feature vectors using <kbd>VectorAssembler</kbd>, comprising the 16 feature columns, as usual. The resulting Spark dataframe, called <kbd>vectorised_df</kbd>, therefore contains two columns—the numeric <kbd>label</kbd> column, representing one of the 26 characters in the English alphabet, and the <kbd>features</kbd> column, containing our feature vectors:</li>
</ol>
<pre style="padding-left: 60px">letter_recognition_df = sqlContext.read<br/>   .format('com.databricks.spark.csv')<br/>   .options(header = 'true', inferschema = 'true')<br/>   .load('letter-recognition.csv')<br/>feature_columns = ['x-box','y-box','width','high','onpix','x-bar',<br/>   'y-bar','x2bar','y2bar','xybar','x2ybr','xy2br','x-ege','xegvy',<br/>   'y-ege','yegvx']<br/>vector_assembler = VectorAssembler(inputCols = feature_columns,<br/>   outputCol = 'features')<br/>vectorised_df = vector_assembler.transform(letter_recognition_df)<br/>   .withColumnRenamed('lettr', 'label').select('label', 'features')</pre>
<ol start="3">
<li>Next, we split our dataset into training and test datasets with a ratio of 75% to 25% respectively, using the following code:</li>
</ol>
<pre style="padding-left: 60px">train_df, test_df = vectorised_df<br/>   .randomSplit([0.75, 0.25], seed=12345)</pre>
<ol start="4">
<li>We are now ready to train our MLP classifier. First, we must define the size of the respective layers of our neural network. We do this by defining a Python list with the following elements<span>:<br/></span><br/>
<ul>
<li>The first element defines the size of the input layer. In our case, we have 16 features in our dataset, and so we set this element to <kbd>16</kbd>.</li>
<li>The next elements define the sizes of the intermediate hidden layers. We shall define two hidden layers of sizes <kbd>8</kbd> and <kbd>4</kbd> respectively.</li>
<li>The final element defines the size of the output layer. In our case, we have 26 possible classes representing the 26 letters of the English alphabet, and so we set this element to <kbd>26</kbd>:</li>
</ul>
</li>
</ol>
<pre style="padding-left: 60px">layers = [16, 8, 4, 26]</pre>
<ol start="5">
<li>Now that we have defined the architecture of our neural network, we can train an MLP using <kbd>MLlib</kbd>'s <kbd>MultilayerPerceptronClassifier</kbd> classifier and fit it to the training dataset, as shown in the following code. Remember that <kbd>MLlib</kbd>'s <kbd>MultilayerPerceptronClassifier</kbd> classifier uses the sigmoid activation function for hidden neurons and the softmax activation function for output neurons:</li>
</ol>
<pre style="padding-left: 60px">multilayer_perceptron_classifier = MultilayerPerceptronClassifier(<br/>   maxIter = 100, layers = layers, blockSize = 128, seed = 1234)<br/>multilayer_perceptron_classifier_model = <br/>   multilayer_perceptron_classifier.fit(train_df)</pre>
<ol start="6">
<li>We can now apply our trained MLP classifier to the test dataset in order to predict which of the 26 letters of the English alphabet the 16 numerical pixel-related features represent, as follows:</li>
</ol>
<pre style="padding-left: 60px">test_predictions_df = multilayer_perceptron_classifier_model<br/>   .transform(test_df)<br/>print("TEST DATASET PREDICTIONS AGAINST ACTUAL LABEL: ")<br/>test_predictions_df.select("label", "features", "probability",<br/>   "prediction").show()<br/><br/><strong>TEST DATASET PREDICTIONS AGAINST ACTUAL LABEL: </strong><br/><strong>+-----+--------------------+--------------------+----------+</strong><br/><strong>|label| features| probability|prediction|</strong><br/><strong>+-----+--------------------+--------------------+----------+</strong><br/><strong>| 0|[1.0,0.0,2.0,0.0,...|[0.62605849526384...| 0.0|</strong><br/><strong>| 0|[1.0,0.0,2.0,0.0,...|[0.62875656935176...| 0.0|</strong><br/><strong>| 0|[1.0,0.0,2.0,0.0,...|[0.62875656935176...| 0.0|</strong><br/><strong>+-----+--------------------+--------------------+----------+</strong></pre>
<ol start="7">
<li>Next, we compute the accuracy of our trained MLP classifier on the test dataset using the following code. In our case, it performs very poorly, with an accuracy rate of only 34%. We can conclude from this that an MLP with two hidden layers of sizes 8 and 4 respectively performs very poorly in recognizing and classifying letters from scanned images in the case of our dataset:</li>
</ol>
<pre style="padding-left: 60px">prediction_and_labels = test_predictions_df<br/>   .select("prediction", "label")<br/>accuracy_evaluator = MulticlassClassificationEvaluator(<br/>   metricName = "accuracy")<br/>precision_evaluator = MulticlassClassificationEvaluator(<br/>   metricName = "weightedPrecision")<br/>recall_evaluator = MulticlassClassificationEvaluator(<br/>   metricName = "weightedRecall")<br/>print("Accuracy on Test Dataset = %g" % accuracy_evaluator<br/>   .evaluate(prediction_and_labels))<br/>print("Precision on Test Dataset = %g" % precision_evaluator<br/>   .evaluate(prediction_and_labels))<br/>print("Recall on Test Dataset = %g" % recall_evaluator<br/>   .evaluate(prediction_and_labels))<br/><br/>Accuracy on Test Dataset = 0.339641<br/>Precision on Test Dataset = 0.313333<br/>Recall on Test Dataset = 0.339641</pre>
<ol start="8">
<li>How can we increase the accuracy of our neural classifier? To answer this question, we must revisit our definition of what the hidden layers do. Remember that the job of the neurons in the hidden layers is to learn to detect patterns within the input data. Therefore, defining more hidden neurons in our neural architecture should increase the ability of our neural network to detect more patterns at greater resolutions. To test this hypothesis, we shall increase the number of neurons in our two hidden layers to 16 and 12 respectively, as shown in the following code. Then, we retrain our MLP classifier and reapply it to the test dataset. This results in a far better performing model, with an accuracy rate of 72%:</li>
</ol>
<pre style="padding-left: 60px">new_layers = [16, 16, 12, 26]<br/>new_multilayer_perceptron_classifier = <br/>   MultilayerPerceptronClassifier(maxIter = 400, <br/>      layers = new_layers, blockSize = 128, seed = 1234)<br/>new_multilayer_perceptron_classifier_model = <br/>   new_multilayer_perceptron_classifier.fit(train_df)<br/>new_test_predictions_df = <br/>   new_multilayer_perceptron_classifier_model.transform(test_df)<br/>print("New Accuracy on Test Dataset = %g" % accuracy_evaluator<br/>   .evaluate(new_test_predictions_df<br/>   .select("prediction", "label")))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p>We have seen how MLPs, which receive a single input vector that is then transformed through one or more intermediate hidden layers, can be used to recognize and classify small images such as letters and digits in OCR. However, one limitation of MLPs is their ability to scale with larger images, taking into account not just individual pixel intensity or RGB values, but the height, width, and depth of the image itself.</p>
<p><strong>Convolutional neural networks</strong> (<strong>CNNs</strong>) assume that the input data is of a grid-like topology, and so they are predominantly used to recognize and classify objects in images since an image can be represented as a grid of pixels.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">End-to-end neural architecture</h1>
                </header>
            
            <article>
                
<p>The end-to-end architecture of a convolutional neural network is illustrated in <em>Figure</em> <em>7.8</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-591 image-border" src="Images/87bcf4e7-4543-4898-a2cf-0aec91618213.png" style="width:159.92em;height:36.50em;" width="1919" height="438"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.8: Convolutional neural network architecture</div>
<p>In the following subsections, we will describe each of the layers and transformations that constitute a CNN.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Input layer</h1>
                </header>
            
            <article>
                
<p>Given that CNNs are predominantly used to classify images, the input data into CNNs consists of image matrices of the dimensions <em>h</em> (height in pixels), <em>w</em> (width in pixels) and <em>d</em> (depth). In the case of RGB images, the depth would be three corresponding, to the three color channels, <strong>red</strong>, <strong>green</strong>, and <strong>blue</strong> (<strong>RGB</strong>). This is illustrated in <em>Figure 7.9</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-677 image-border" src="Images/c5b71f9a-c7c7-443c-b12f-67c8e6a5a0a7.png" style="width:22.00em;height:18.08em;" width="934" height="769"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.9: Image matrix dimensions</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Convolution layers</h1>
                </header>
            
            <article>
                
<p>The next transformations that occur in the CNN are processed in <em>convolution</em> layers. The purpose of the convolution layers is to detect features in the image, which is achieved through the use of <strong>filters</strong> (also called kernels). Imagine taking a magnifying glass and looking at an image, starting at the top-left of the image. As we move the magnifying glass from left to right and top to bottom, we detect the different features in each of the locations that our magnifying glass moves over. At a high level, this is the job of the convolution layers, where the magnifying glass represents the filter or kernel and the size of each step that the filter takes, normally pixel by pixel, is referred to as the <strong>stride</strong> size. The output of a convolution layer is called a <strong>feature map</strong>.</p>
<p>Let's look at an example to understand the processes undertaken within a convolution layer better. Imagine that we have an image that is 3 pixels (height) by 3 pixels (width). For the sake of simplicity, we will overlook the third dimension representing the image depth in our example, but note that real-world convolutions are computed in three dimensions for RGB images. Next, imagine that our filter is a matrix of 2 pixels (height) by 2 pixels (width) and that our stride size is 1 pixel.</p>
<p>These respective matrices are illustrated in <em>Figure 7.10</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-593 image-border" src="Images/a8e86093-7fa5-40c9-b751-c1f09993f6b0.png" style="width:27.25em;height:11.42em;" width="740" height="311"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.10: Image matrix and filter matrix</div>
<p>First, we place our filter matrix at the top-left corner of our image matrix and perform a <strong>matrix multiplication</strong> of the two at that location. We then move the filter matrix to the right by our stride size—1 pixel—and perform a matrix multiplication at that location. We continue this process until the filter matrix has traversed the entire image matrix. The resulting feature map matrix is illustrated in <em>Figure 7.11</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-594 image-border" src="Images/d4c664c9-11eb-4df2-81e2-aae3ff9594f1.png" style="width:30.33em;height:18.42em;" width="1076" height="656"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.11: Feature map</div>
<p>Note that the feature map is smaller in its dimensions than the input matrix of the convolution layer. To ensure that the dimensions of the output match the dimensions of the input, a layer of zero-value pixels is added in a process called <strong>padding</strong>. Also note that the filter must have the same number of channels as the input image—so in the case of RGB images, the filter must also have three channels.</p>
<p>So, how do convolutions help a neural network to learn? To answer this question, we must revisit the concept of filters. Filters themselves are matrices of <em>weights</em> that are trained to detect specific patterns within an image, and different filters can be used to detect different patterns, such as edges and other features. For example, if we use a filter that has been pretrained to detect simple edges, as we pass this filter over an image, the convolution computation will output a high-valued real number (as a result of matrix multiplication and summation) if an edge is present and a low-valued real number if an edge is not present.</p>
<p>As the filter finishes traversing the entire image, the output is a feature map matrix that represents the convolutions of this filter over all parts of the image. By using different filters during different convolutions per layer, we get different feature maps, which form the output of the convolution layer.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Rectified linear units</h1>
                </header>
            
            <article>
                
<p>As with other neural networks, an activation function defines the output of a node and is used so that our neural network can learn nonlinear functions. Note that our input data (<span>the RGB pixels making up the images) is </span><span>itself nonlinear, so we need a nonlinear activation function.</span> <strong>Rectified linear units</strong> <span>(</span><strong>ReLUs</strong><span>) are commonly used in CNNs, and are defined as follows:</span></p>
<p style="padding-left: 180px" class="CDPAlignLeft CDPAlign"><img src="Images/81dc3852-c5cb-41a3-88cb-a7d51b7bb209.png" style="width:11.58em;height:1.25em;" width="1990" height="220"/></p>
<p class="mce-root">In other words, the ReLU function returns 0 for every negative value in its input data, and returns the value itself for every positive value in its input data. This is shown in <em>Figure 7.12</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-595 image-border" src="Images/c50602e9-747c-4a87-968f-2c7aee8fe3eb.png" style="width:35.42em;height:13.67em;" width="1101" height="423"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.12: ReLU function</div>
<p class="mce-root">The ReLU function can be plotted as shown in <em>Figure 7.13</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-596 image-border" src="Images/48fcb0e4-f861-4487-bfbe-44e896a65b00.png" style="width:25.50em;height:20.25em;" width="942" height="747"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.13: ReLU function graph</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Pooling layers</h1>
                </header>
            
            <article>
                
<p>The next transformations that occur in the CNN are processed in <em>pooling</em> layers. The goal of the pooling layers is to reduce the dimensionality of the feature maps output by the convolution layers (but not their depth) while preserving the spatial variance of the original input data. In other words, the size of the data is reduced in order to reduce computational complexity, memory requirements, and training times while overcoming over fitting so that patterns detected during training can be detected in test data even if their appearance varies. There are various pooling algorithms available, given a specified window size, including the following:</p>
<ul>
<li><strong>Max pooling</strong>: Takes the maximum value in each window</li>
<li><strong>Average pooling</strong>: Takes the average value across each window</li>
<li><strong>Sum pooling</strong>: Takes the sum of values in each window</li>
</ul>
<p><em>Figure 7.14</em> shows the effect of performing max pooling on a 4 x 4 feature map using a 2 x 2 window size:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-597 image-border" src="Images/df33f184-e0c5-471f-8dca-97add6d76529.png" style="width:28.50em;height:13.33em;" width="1093" height="515"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.14: Max pooling on a 4 x 4 feature map using a 2 x 2 window</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Fully connected layer</h1>
                </header>
            
            <article>
                
<p>After the 3-D input data has been transformed through a series of convolution and pooling layers, a fully connected layer flattens the feature maps output by the last convolution and pooling layer into a long 1-D feature vector, which is then used as the input data for a regular ANN in which all of the neurons in each layer are connected to all of the neurons in the previous layer.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Output layer</h1>
                </header>
            
            <article>
                
<p>The output neurons in this <span><span>ANN</span></span> then use an activation function such as the softmax function (as seen in the MLP classifier) to classify the outputs and thereby recognize and classify the objects contained in the input image data!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study 2 – image recognition</h1>
                </header>
            
            <article>
                
<p>In this case study, we will use a pretrained <span><span>CNN</span></span> to recognize and classify objects in images that it has never encountered before.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">InceptionV3 via TensorFlow</h1>
                </header>
            
            <article>
                
<p>The pretrained CNN that we will use is called <strong>Inception-v3</strong>. This deep CNN has been trained on the <strong>ImageNet</strong> image database (an academic benchmark for computer vision algorithms containing a vast library of labelled images covering a wide range of nouns) and can classify entire images into 1,000 classes found in everyday life, such as "pizza", "plastic bag", "red wine", "desk", "orange", and "basketball", to name just a few.</p>
<p>The Inception-v3 deep CNN was developed and trained by <strong>TensorFlow</strong> <sup>TM</sup>, an open source machine learning framework and software library for high-performance numerical computation originally developed within Google's AI organization.</p>
<p>To learn more about TensorFlow, Inception-v3, and ImageNet, please visit the following links:</p>
<ul>
<li><strong><span>ImageNet:</span></strong> <a href="http://www.image-net.org/">http://www.image-net.org/</a></li>
<li><strong><span>TensorFlow:</span></strong> <a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a></li>
<li><strong><span>Inception-v3:</span></strong> <a href="https://www.tensorflow.org/tutorials/images/image_recognition">https://www.tensorflow.org/tutorials/images/image_recognition</a></li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep learning pipelines for Apache Spark</h1>
                </header>
            
            <article>
                
<p>In this case study, we will access the Inception-v3 TensorFlow deep CNN via a third-party Spark package called <kbd>sparkdl</kbd>. This Spark package has been developed by Databricks, a company formed by the original creators of Apache Spark, and provides high-level APIs for scalable deep learning within Apache Spark.</p>
<p class="mce-root">To learn more about Databricks and <kbd>sparkdl</kbd>, please visit the following links:</p>
<div style="margin-left: 2em">
<ul>
<li class="mce-root"><strong>Databricks</strong>: <a href="https://databricks.com/">https://databricks.com/</a></li>
<li class="mce-root"><strong>sparkdl</strong>: <a href="https://github.com/databricks/spark-deep-learning">https://github.com/databricks/spark-deep-learning</a></li>
</ul>
</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image library</h1>
                </header>
            
            <article>
                
<p>The images that we will use to test the pretrained Inception-v3 deep CNN have been selected from the <strong>Open Images v4</strong> dataset, a collection of over 9 million images that have been released under the Creative Common Attribution license, and which may be found at <a href="https://storage.googleapis.com/openimages/web/index.html">https://storage.googleapis.com/openimages/web/index.html</a>.</p>
<p>In the GitHub repository accompanying this book, you can find 30 images of birds (<kbd>image-recognition-data/birds</kbd>) and 30 images of planes (<kbd>image-recognition-data/planes</kbd>) respectively. <em>Figure 7.15</em> shows a couple of examples of the images that you might find in these test datasets:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-598 image-border" src="Images/b455f360-046b-4640-bc0f-cc260e7eba3c.png" style="width:41.42em;height:16.42em;" width="1112" height="443"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.15: Example images from the Open Images v4 dataset</div>
<p>Our goal in this case study will be to apply the pretrained Inception-v3 deep CNN to these test images and quantify the accuracy of a trained classifier model when it comes to distinguishing between images of birds and planes within a single test dataset.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">PySpark image recognition application</h1>
                </header>
            
            <article>
                
<p>Note that for the purposes of this case study, we will not be using Jupyter notebooks for development but rather standard Python code files with the <kbd>.py</kbd> file extension. This case study provides a first glimpse into how a production-grade pipeline should be developed and executed; rather than instantiating a <kbd>SparkContext</kbd> explicitly within our code, we will instead submit our code and all its dependencies to <kbd>spark-submit</kbd> (including any third-party Spark packages, such as <kbd>sparkdl</kbd>) via the Linux command line.</p>
<p>Let's now take a look at how we can use the Inception-v3 deep CNN via PySpark to classify test images. <span>In our Python-based image-recognition application, we perform the following steps (numbered to correspond to the numbered comments in our Python code file):</span></p>
<div class="packt_infobox">The following Python code file, called <kbd>chp07-02-convolutional-neural-network-transfer-learning.py</kbd>, can be found in the GitHub repository accompanying this book.</div>
<ol start="1">
<li>First, using the following code, we import the required Python dependencies, including the relevant modules from the third-party <kbd>sparkdl</kbd> package and the <kbd>LogisticRegression</kbd> classifier native to <kbd>MLlib</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from sparkdl import DeepImageFeaturizer<br/>from pyspark.sql.functions import *<br/>from pyspark.sql import SparkSession<br/>from pyspark.ml.image import ImageSchema<br/>from pyspark.ml import Pipeline<br/>from pyspark.ml.classification import LogisticRegression<br/>from pyspark.ml.evaluation import MulticlassClassificationEvaluator</pre>
<ol start="2">
<li>Unlike our Jupyter notebook case studies, there is no need to instantiate a <kbd>SparkContext</kbd>, as this will be done for us when we execute our PySpark application via <kbd>spark-submit</kbd> on the command line. In this case study, we will create a <kbd>SparkSession</kbd>, as shown in the following code, that acts as an entry point into the Spark execution environment (even if it is already running) that subsumes SQLContext. We can therefore use <kbd>SparkSession</kbd> to undertake the same SQL-like operations over data that we have seen previously while still using the Spark Dataset/DataFrame API:</li>
</ol>
<pre style="padding-left: 60px">spark = SparkSession.builder.appName("Convolutional Neural Networks - Transfer Learning - Image Recognition").getOrCreate()</pre>
<ol start="3">
<li>As of Version 2.3, Spark provides native support for image data sources via its <kbd>MLlib</kbd> API. In this step, we invoke the <kbd>readImages</kbd> method on <kbd>MLlib</kbd>'s <kbd>ImageSchema</kbd> class to load our bird and plane test images from the local filesystem into Spark dataframes called <kbd>birds_df</kbd> and <kbd>planes_df</kbd> respectively. We then label all images of birds with the <kbd><span><span>0</span></span></kbd> literal and label all images of planes with the <kbd>1</kbd> literal, as follows:</li>
</ol>
<pre style="padding-left: 60px">path_to_img_directory = 'chapter07/data/image-recognition-data'<br/>birds_df = ImageSchema.readImages(path_to_img_directory + "/birds")<br/>   .withColumn("label", lit(0))</pre>
<pre style="padding-left: 60px">planes_df = ImageSchema.readImages(path_to_img_directory + <br/>   "/planes").withColumn("label", lit(1))</pre>
<ol start="4">
<li>Now that we have loaded our test images into separate Spark dataframes differentiated by their label, we consolidate them into single training and test dataframes accordingly. We achieve this by using the <kbd>unionAll</kbd> method via the Spark dataframe API, which simply appends one dataframe onto another, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">planes_train_df, planes_test_df = planes_df<br/>   .randomSplit([0.75, 0.25], seed=12345)<br/>birds_train_df, birds_test_df = birds_df<br/>   .randomSplit([0.75, 0.25], seed=12345)<br/>train_df = planes_train_df.unionAll(birds_train_df)<br/>test_df = planes_test_df.unionAll(birds_test_df)</pre>
<ol start="5">
<li>As with previous case studies, we need to generate feature vectors from our input data. However, rather than training a deep CNN from scratch—which could take many days, even with distributed technologies—we will take advantage of the pretrained Inception-v3 deep CNN. To do this, we will use a process called <strong>transfer learning</strong>. In this process, knowledge gained while solving one machine learning problem is applied to a different but related problem. To use transfer learning in our case study, we employ the <kbd>DeepImageFeaturizer</kbd> module of the third-party <kbd>sparkdl</kbd> Spark package. The <kbd>DeepImageFeaturizer</kbd> not only transforms our images into numeric features, it also performs fast transfer learning by peeling off the last layer of a pretrained neural network and then uses the output from all the previous layers as features for a standard classification algorithm. In our case, the <kbd>DeepImageFeaturizer</kbd> will be peeling off the last layer of the pretrained Inception-v3 deep CNN, as follows:</li>
</ol>
<pre style="padding-left: 60px">featurizer = DeepImageFeaturizer(inputCol = "image", <br/>   outputCol = "features", modelName = "InceptionV3")</pre>
<ol start="6">
<li>Now that we have the features from all previous layers of the pretrained Inception-v3 deep CNN extracted via transfer learning, we input them into a classification algorithm. In our case, we will use <kbd>MLlib</kbd>'s <kbd>LogisticRegression</kbd> classifier, as follows:</li>
</ol>
<pre style="padding-left: 60px">logistic_regression = LogisticRegression(maxIter = 20, <br/>   regParam = 0.05, elasticNetParam = 0.3, labelCol = "label")</pre>
<ol start="7">
<li>To execute the transfer learning and logistic regression model training, we build a standard <kbd>pipeline</kbd> and <kbd>fit</kbd> that pipeline to our training dataframe, as follows:</li>
</ol>
<pre style="padding-left: 60px">pipeline = Pipeline(stages = [featurizer, logistic_regression])<br/>model = pipeline.fit(train_df)</pre>
<ol start="8">
<li>Now that we have a trained classification model, using the features derived by the Inception-v3 deep CNN, we apply our trained logistic regression model to our test dataframe to make predictions as normal, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">test_predictions_df = model.transform(test_df)<br/>test_predictions_df.select("image.origin", "prediction")<br/>   .show(truncate=False)</pre>
<ol start="9">
<li>Finally, we quantify the accuracy of our model on the test dataframe using <kbd>MLlib</kbd>'s <kbd>MulticlassClassificationEvaluator</kbd>, as follows:</li>
</ol>
<pre style="padding-left: 60px">accuracy_evaluator = MulticlassClassificationEvaluator(<br/>   metricName = "accuracy")<br/>print("Accuracy on Test Dataset = %g" % accuracy_evaluator<br/>   .evaluate(test_predictions_df.select("label", "prediction")))</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Spark submit</h1>
                </header>
            
            <article>
                
<p>We are now ready to run our image recognition application! Since it is a Spark application, we can execute it via <kbd>spark-submit</kbd> on the Linux command line. To do this, navigate to the directory where we installed Apache Spark (see <a href="673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml" target="_blank">Chapter 2</a>, <em>Setting Up a Local Development Environment</em>). Then, we can execute the <kbd>spark-submit</kbd> program by passing it the following command-line arguments:</p>
<ul>
<li><kbd>--master</kbd>: The Spark Master URL.</li>
<li><kbd>--packages</kbd>: The third-party libraries and dependencies required for the Spark application to work. In our case, our image-recognition application is dependent on the availability of the <kbd>sparkdl</kbd> third-party library.</li>
</ul>
<ul>
<li><kbd>--py-files</kbd>: Since our image-recognition application is a PySpark application, we pass the filesystem paths to any Python code files that our application is dependent on. In our case, since our image-recognition application is self-contained within a single code file, there are no further dependencies to pass to <kbd>spark-submit</kbd>.</li>
<li>The final argument is the path to the Python code file containing our Spark driver program, namely <kbd>chp07-02-convolutional-neural-network-transfer-learning.py</kbd>.</li>
</ul>
<p>The final commands to execute, therefore, look as follows:</p>
<pre><strong>&gt; cd {SPARK_HOME}</strong><br/><strong>&gt; bin/spark-submit --master spark://192.168.56.10:7077 --packages databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11 chapter07/chp07-02-convolutional-neural-network-transfer-learning.py</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image-recognition results</h1>
                </header>
            
            <article>
                
<p>Assuming that the image-recognition application ran successfully, you should see the following results output to the console:</p>
<table style="border-color: #000000;border-collapse: collapse;width: 394px" border="1">
<tbody>
<tr>
<td style="padding: 5px;width: 274px">
<p class="CDPAlignCenter CDPAlign"><strong>Origin</strong></p>
</td>
<td style="padding: 5px;width: 114px" class="CDPAlignCenter CDPAlign">
<p><strong>Prediction</strong></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>planes/plane-005.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>1.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>planes/plane-008.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>1.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>planes/plane-009.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>1.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>planes/plane-016.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>1.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>planes/plane-017.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>0.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>planes/plane-018.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>1.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>birds/bird-005.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>0.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>birds/bird-008.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>0.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>birds/bird-009.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>0.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>birds/bird-016.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>0.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>birds/bird-017.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>0.0</kbd></p>
</td>
</tr>
<tr>
<td style="width: 274px">
<p><kbd>birds/bird-018.jpg</kbd></p>
</td>
<td style="width: 114px">
<p><kbd>0.0</kbd></p>
</td>
</tr>
</tbody>
</table>
<p>The <kbd>Origin</kbd> column refers to the absolute filesystem path of the image, and the value in the <kbd>Prediction</kbd> column is <kbd>1.0</kbd> if our model predicts that the object in the image is a plane and <kbd>0.0</kbd> if our model predicts that the object in the image is a bird. Our model has an astonishingly high accuracy of 92% when run on the test dataset. The only mistake that our model made was on <kbd>plane-017.jpg</kbd>, illustrated in <em>Figure</em> <em>7.16</em>, which was incorrectly classified as a bird when it was in fact a plane:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-599 image-border" src="Images/677136f7-68b4-4b1c-8202-81f483f1fda0.png" style="width:28.50em;height:16.00em;" width="1069" height="601"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.16: Incorrect classification of plane-017.jpg</div>
<p>If we look at <kbd>plane-017.jpg</kbd> in <em>Figure 7.16</em>, we can quickly understand why the model made this mistake. Though it is a man-made plane, it has been physically modeled to look like a bird for increased efficiency and aerodynamic purposes.</p>
<p>In this case study, we used a pretrained CNN to featurize images. We then passed the resulting features to a standard logistic regression algorithm to predict whether a given image is a bird or a plane.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Case study 3 – image prediction</h1>
                </header>
            
            <article>
                
<p>In case study 2 (image recognition), we still explicitly labelled our test images before training our final logistic regression classifier. In this case study, we will simply send random images to the pretrained Inception-v3 deep CNN without labeling them and let the CNN itself classify the objects contained within the images. Again, we will take advantage of the third-party <kbd>sparkdl</kbd> Spark package to access the pretrained Inception-v3 CNN.</p>
<p>The assortment of random images that we will use have again been downloaded from the <strong>Open Images v4 dataset</strong>, and may be found in the GitHub repository accompanying this book under <kbd>image-recognition-data/assorted</kbd>. <em>Figure 7.17</em> shows a couple of typical images that you may find in this test dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-600 image-border" src="Images/b763134a-ece7-47ff-b8d4-888928a49752.png" style="width:24.50em;height:17.67em;" width="870" height="629"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 7.17: Assortment of random images</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">PySpark image-prediction application</h1>
                </header>
            
            <article>
                
<p>In our Python-based image-prediction application, we go through the following steps (numbered to correspond to the numbered comments in our Python code file):</p>
<div class="packt_infobox"><span>The following Python code file, called </span><kbd>chp07-03-convolutional-neural-network-image-predictor.py</kbd><span>, can be found in the GitHub repository accompanying this book.</span></div>
<ol>
<li>First, we import the required Python dependencies as usual, including the <kbd>DeepImagePredictor</kbd> class from the third-party <kbd>sparkdl</kbd> Spark package, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">from sparkdl import DeepImagePredictor<br/>from pyspark.sql import SparkSession<br/>from pyspark.ml.image import ImageSchema</pre>
<ol start="2">
<li>Next, we create a <kbd>SparkSession</kbd> that acts as an entry point into the Spark execution environment, as follows:</li>
</ol>
<pre style="padding-left: 60px">spark = SparkSession.builder.appName("Convolutional Neural Networks - Deep Image Predictor").getOrCreate()</pre>
<ol start="3">
<li>We then load our assortment of random images into a Spark dataframe using the <kbd>readImages</kbd> method of the <kbd>ImageSchema</kbd> class that we first encountered in the previous case study, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">assorted_images_df = ImageSchema.readImages(<br/>   "chapter07/data/image-recognition-data/assorted")</pre>
<ol start="4">
<li>Finally, we pass our Spark dataframe containing our assortment of random images to <kbd>sparkdl</kbd>'s <kbd>DeepImagePredictor</kbd>, which will apply a specified pretrained neural network to the images in an effort to classify the objects found within them. In our case, we will be using the pretrained Inception-v3 deep CNN. We also tell the <kbd>DeepImagePredictor</kbd> to return the top 10 (<kbd>topK=10</kbd>) predicted classifications for each image in descending order of confidence, as follows:</li>
</ol>
<pre style="padding-left: 60px">deep_image_predictor = DeepImagePredictor(inputCol = "image", <br/>   outputCol = "predicted_label", modelName = "InceptionV3", <br/>   decodePredictions = True, topK = 10)<br/>predictions_df = deep_image_predictor.transform(assorted_images_df)<br/>predictions_df.select("image.origin", "predicted_label")<br/>   .show(truncate = False)</pre>
<p>To run this PySpark image-prediction application, we again invoke <kbd>spark-submit</kbd> via the command line, as follows:</p>
<pre><strong>&gt; cd {SPARK_HOME}</strong><br/><strong>&gt; bin/spark-submit --master spark://192.168.56.10:7077 --packages databricks:spark-deep-learning:1.2.0-spark2.3-s_2.11 chapter07/chp07-03-convolutional-neural-network-image-predictor.py</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image-prediction results</h1>
                </header>
            
            <article>
                
<p>Assuming that the image-prediction application ran successfully, you should see the following results output to the console:</p>
<table style="border-color: #000000;border-collapse: collapse" border="1">
<tbody>
<tr>
<td style="padding: 5px">
<p><strong>Origin</strong></p>
</td>
<td style="padding: 5px">
<p><strong>First Predicted Label</strong></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/snowman.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Teddy</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/bicycle.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Mountain Bike</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/house.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Library</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/bus.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Trolley Bus</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/banana.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Banana</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/pizza.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Pizza</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/toilet.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Toilet Seat</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/knife.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Cleaver</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/apple.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Granny Smith (Apple)</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/pen.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Ballpoint</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/lion.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Lion</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/saxophone.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Saxophone</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/zebra.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Zebra</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/fork.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd>Spatula</kbd></p>
</td>
</tr>
<tr>
<td style="padding: 5px">
<p><kbd>assorted/car.jpg</kbd></p>
</td>
<td style="padding: 5px">
<p><kbd><span>C</span><span>onvertible</span></kbd></p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"><span>As you can see, the pretrained Inception-v3 deep CNN has an astonishing ability to recognize and classify the objects found in images. Though the images provided in this case study were relatively simple, the Inception-v3 CNN has a top-five error rate— how often the model fails to predict the correct answer as one of its top five guesses—of just 3.46% on the ImageNet image database. Remember that the Inception-v3 CNN attempts to classify en</span><span>tire images into</span> 1,000 <span>classes, hence a top-5 error rate of just 3.46% is truly impressive, and clearly demonstrates the learning ab</span><span>ility and power of not only convolution neural networks but ANNs in general when detecting and learning patterns!</span></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we went on a hands-on exploration through the exciting and cutting-edge world of deep learning. We developed applications to recognize and classify objects in images with astonishingly high rates of accuracy, and demonstrated the truly impressive learning ability of ANNs to detect and learn patterns in input data.</p>
<p>In the next chapter, we will extend our deployment of machine learning models beyond batch processing in order to learn from data and make predictions in real time!</p>


            </article>

            
        </section>
    </div>



  </body></html>