<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer024">
<h1 class="chapter-number" id="_idParaDest-15"><a id="_idTextAnchor015"/>1</h1>
<h1 id="_idParaDest-16"><a id="_idTextAnchor016"/>Foundations</h1>
<p><strong class="bold">Emotions</strong> play a key role in our daily lives. Some people define them as the reactions that we as human beings experience as a response to events or situations, some describe them simply as a class of feelings, and others say they describe physiological states and are generated subconsciously. Psychologists describe emotions as “<em class="italic">a complex state of feeling that results in physical and psychological changes that influence thought and behavior.</em>” So, it appears that although we feel emotions, they are much harder <span class="No-Break">to describe.</span></p>
<p>Our brains play a crucial role when creating and processing emotions. Historically, it was believed that each emotion was located in a specific part of the brain. However, research has shown that there is no single region of the brain that’s responsible for processing emotions – several brain regions are activated when emotions are being processed. Furthermore, different parts of the brain can generate the same emotion and different parts can also contribute to generating <span class="No-Break">an emotion.</span></p>
<p>The reality may even be that <em class="italic">emotion</em> and <em class="italic">sentiment</em> are experiences that result from combined influences of biological, cognitive, and social aspects. Whatever the case, emotions matter because they help us decide what actions to do, how to negotiate tricky situations, and, at a basic level, how to survive. Different emotions rule our everyday lives; for example, we make decisions based on whether we are happy, angry, or sad, and we choose our daily pastimes and routines based on the emotions they facilitate. So, emotions are important, and understanding them may make our <span class="No-Break">lives easier.</span></p>
<p>In this chapter, you will learn about the main concepts and differences between sentiment analysis and emotion analysis, and also understand why emotion analysis is important in the modern world. By combining this with a basic introduction to <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) and machine learning, we will lay the foundations for successfully using these techniques for <span class="No-Break">emotion analysis.</span></p>
<p>In this chapter, we’ll cover the <span class="No-Break">following topics:</span></p>
<ul>
<li><span class="No-Break">Emotions</span></li>
<li><span class="No-Break">Sentiment</span></li>
<li>Why is emotion <span class="No-Break">analysis important?</span></li>
<li>Introduction to natural <span class="No-Break">language processing</span></li>
<li>Introduction to <span class="No-Break">machine learning</span></li>
</ul>
<h1 id="_idParaDest-17"><a id="_idTextAnchor017"/>Emotions</h1>
<p>This book is about writing<a id="_idIndexMarker000"/> programs that can detect emotions expressed in texts, particularly informal texts. Emotions play a crucial role in our daily lives. They impact how we feel, how we think, and how we behave. Consequently, it stands to reason that they impact the decisions we make. If this is the case, then being able to detect emotions from written text (for example, social media posts) is a useful thing to do because the impact it would have on many practical everyday applications in sectors such as marketing, industry, health, and security would <span class="No-Break">be huge.</span></p>
<p>However, while it is clear that we all experience emotions and that they play a significant role in our plans and actions, it is much less clear what they <em class="italic">are</em>. Given that we are about to embark on a detailed study of how to write programs to detect them, it is perhaps worth beginning by investigating the notion of what an emotion is and looking at the various theories that attempt to pin them down. This is a topic that has fascinated philosophers and psychologists from antiquity to the present day, and it is still far from settled. We will briefly look at a number of the most prominent theories and approaches. This overview will not lead us to a definitive view, but before we start trying to identify them in written texts, we should at least become aware of the problems that people still have in pinning <span class="No-Break">them down.</span></p>
<p>Darwin believed that emotions allowed humans and animals to survive and reproduce. He argued that they evolved, were adaptive, and that all humans, and even other animals, expressed emotion through similar behaviors. He believed that emotions had an evolutionary history that could be traced across cultures and species. Today, psychologists agree that emotions such as fear, surprise, disgust, happiness, and sadness can be regarded as universal regardless <span class="No-Break">of culture.</span></p>
<p>The James-Lange theory proposes that it is our physical responses that are responsible for emotions. For example, if someone jumps out at you from behind a bush, your heart rate will increase, and it is this increase that causes the individual to feel fear. The facial-feedback theory builds on this idea and suggests that physical activity is responsible for influencing emotion, for example, if you smile, likely, you will automatically feel happier than if you did not smile. However, Cannon-Bard’s theory refutes James-Lange, instead suggesting that people experience emotional and physical responses simultaneously. The Schachter-Singer theory is a cognitive theory of emotion that suggests that it is our thoughts that are responsible for emotions, and similarly, cognitive appraisal theory suggests that thinking must come before experiencing an emotion. For instance, the brain might understand <a id="_idIndexMarker001"/>a situation as threatening, and hence fear <span class="No-Break">is experienced.</span></p>
<p>To try to obtain a deeper understanding of emotions, let’s look at the three main theories <span class="No-Break">of emotion:</span></p>
<ul>
<li><strong class="bold">Physiological</strong>: Psychologists<a id="_idIndexMarker002"/> have the view that emotions are<a id="_idIndexMarker003"/> formed when a bodily response is triggered by a stimulus, so as the individual experiences physiological changes, this is also experienced as <span class="No-Break">an emotion</span></li>
<li><strong class="bold">Neurological</strong>: Biologists <a id="_idIndexMarker004"/>claim that hormones (for example, estrogen, progesterone, and testosterone) that are produced by the body’s glands impact the chemistry and circuitry of the brain and these lead to <span class="No-Break">emotional responses</span></li>
<li><strong class="bold">Cognitive</strong>: Cognitive<a id="_idIndexMarker005"/> scientists believe that thoughts and other mental activities play a crucial role in <span class="No-Break">forming emotions</span></li>
</ul>
<p>In all likelihood, all three theories are probably valid to some extent. It has also been postulated that instead of thinking of these as mutually exclusive, it is more likely that they are complementary and that each explains and accounts for a different aspect of what we think of as <span class="No-Break">an emotion.</span></p>
<p>Although emotions have been studied for many decades, it is probably still true that we still do not fully <span class="No-Break">understand emotions.</span></p>
<p>Humans can experience a huge number of emotions, but only a handful are considered basic. However, the number of emotions considered in emotion analysis research is not always limited to just these basic emotions. Furthermore, it is not straightforward to demarcate emotions, and hence boundaries are very rarely <span class="No-Break">clearly defined.</span></p>
<p>We will now consider what are known as the <em class="italic">primary emotions</em>. These have been described as a reaction to an event or situation, or the immediate strong first reaction experienced when something happens. There has been much research on identifying these primary emotions, but there is still no general agreement, and different models have been suggested by eminent researchers such as Ekman, Plutchik, and Parrot. Some emotions such as anger, fear, joy, and surprise are universally agreed upon. However, the same is not true for other emotions, with disagreements on the emotions that constitute the basic emotions and the number of these emotions. Although there is, again, no consensus on <a id="_idIndexMarker006"/>which model is best at covering basic emotions, the models proposed by Ekman and Plutchik are most commonly used. There are two popular approaches: <strong class="bold">categorical</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">dimensional</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-18"><a id="_idTextAnchor018"/>Categorical</h2>
<p>Ekman is an advocate <a id="_idIndexMarker007"/>of the categorical theory, which suggests that emotions arise from separate neural systems. This approach also suggests that there are a limited number of primary, distinct emotions, such as anger, anxiety, joy, and sadness. Ekman suggested that primary emotions must have a distinct facial expression that is recognizable across all cultures. For example, the corners of the lips being turned down demonstrates sadness – and this facial expression is recognized universally as portraying sadness. Similarly, smiling with teeth exposed and the corners of the mouth pointing upwards is universally recognized <span class="No-Break">as joy.</span></p>
<p>Amazingly, people blind from birth use the same facial expressions when expressing sadness and joy. They have never seen these facial expressions, so it is impossible that these expressions were learned. It is much more likely that these are an integral part of human nature. Using this understanding of distinct, universal facial expressions, Ekman proposed six primary e<a id="_idTextAnchor019"/>motions (<span class="No-Break">Ekman, 1993):</span></p>
<ul>
<li><span class="No-Break">Anger</span></li>
<li><span class="No-Break">Disgust</span></li>
<li><span class="No-Break">Fear</span></li>
<li><span class="No-Break">Joy</span></li>
<li><span class="No-Break">Sadness</span></li>
<li><span class="No-Break">Surprise</span></li>
</ul>
<p>Ekman suggested that these <em class="italic">basic</em> emotions were biologically primitive and have evolved to increase the reproductive fitness of animals and that all other emotions were combinations of these eight primary emotions. Later, Eckman expanded this list to include other emotions that he considered basic, such as embarrassment, excitement, contempt, shame, pride, satisfaction, <span class="No-Break">and amusement.</span></p>
<p>Another of the most influential works in the area of emotions is Plutchik’s psychoevolutionary theory of emotion. Plutchik proposed eight primary e<a id="_idTextAnchor020"/>motions (<span class="No-Break">Plutchik, 2001):</span></p>
<ul>
<li><span class="No-Break">Anger</span></li>
<li><span class="No-Break">Anticipation</span></li>
<li><span class="No-Break">Disgust</span></li>
<li><span class="No-Break">Fear</span></li>
<li><span class="No-Break">Joy</span></li>
<li><span class="No-Break">Sadness</span></li>
<li><span class="No-Break">Surprise</span></li>
<li><span class="No-Break">Trust</span></li>
</ul>
<p>From this theory, Plutchik<a id="_idIndexMarker008"/> developed a Wheel of Emotions (see <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em>). This wheel was developed to help understand the nuances of emotion and how emotions contrast. It has eight sectors representing the eight emotions. Emotions intensify as they move from outside toward the center of the wheel. For example, annoyance increases to anger and then further increases to outright rage. Each sector of the circle has an opposite emotion that is placed directly opposite in the wheel. For example, the opposite of sadness is joy, and the opposite of anger is fear. It also shows how different emotions can <span class="No-Break">be combined.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 1.1 – Plutchik’s Wheel of Emotions" height="1489" src="image/B18714_01_01.jpg" width="1552"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Plutchik’s Wheel of Emotions</p>
<p>Although Ekman and Plutchik’s theories are the most common, there are other works, but there is little agreement <a id="_idIndexMarker009"/>on what the basic emotions are. However, in the area of emotion analysis research, Ekman and Plutchik’s models are the most often used <span class="No-Break">classification schemes.</span></p>
<h2 id="_idParaDest-19"><a id="_idTextAnchor021"/>Dimensional</h2>
<p>The dimensional <a id="_idIndexMarker010"/>approach posits that to understand emotional experiences, the fundamental dimensions of valence (the <em class="italic">goodness</em> and <em class="italic">badness</em> of the emotion) and arousal (the <em class="italic">intensity</em> of the emotion) are vital. This approach suggests that a common and interconnected neurophysiological system is responsible for all affective states. Every emotion can then be defined in terms of these two measures, so the plane can be viewed as a continuous two-dimensional space, with dimensions of valence and arousal, and each point in the place corresponds to a separate <span class="No-Break">emotion state.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 1.2 – Russell’s circumplex model" height="1498" src="image/B18714_01_02.jpg" width="1661"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Russell’s circumplex model</p>
<p>The most common <a id="_idIndexMarker011"/>dimensional model is Russell’s circumpl<a id="_idTextAnchor022"/>ex model ((Russell, 1980): see <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em>). The model posits that emotions are made up of two core dimensions: valence and arousal. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em> shows that valence ranges from −1 (unpleasant) to 1 (pleasant), and arousal also ranges from −1 (calm) to 1 (excited). Each emotion is then a linear combination of these two dimensions. For example, anger is an unpleasant emotional state (a negative valence) with a high intensity (a positive arousal). Other basic emotions can be seen in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em> with their approximate positions in the <span class="No-Break">two-dimensional space.</span></p>
<p>Some emotions have similar arousal and valence (for example, grief and rage). Hence, a third dimension (control) has also been suggested that can be used to distinguish between these. Control ranges from <em class="italic">no control</em> to <em class="italic">full control</em>. So, the entire range of human emotions can be represented as a set of points in the three-dimensional space using these <span class="No-Break">three dimensions.</span></p>
<p>The dimensional model has a poorer resolution of emotions; that is, it is harder to distinguish between ambiguous emotions. The categorical model is simpler to understand, but some emotions are not part of the set of <span class="No-Break">basic emotions.</span></p>
<p>Most emotion <a id="_idIndexMarker012"/>analysis research uses a categorical perspective; there seems to be a lack of research using the <span class="No-Break">dimensional approach.</span></p>
<h1 id="_idParaDest-20"><a id="_idTextAnchor023"/>Sentiment</h1>
<p>There is a second <a id="_idIndexMarker013"/>closely-related term known as <strong class="bold">sentiment</strong>. The terms sentiment and emotion seem to be used in an ad hoc manner, with different writers using them almost interchangeably. Given the difficulty we have found in working out what emotions are, and in deciding exactly how many emotions there are, having yet another ill-defined term is not exactly helpful. To try to clarify the situation, note that when people work on sentiment mining, they generally make use of a simple, limited system of classification using <em class="italic">positive</em>, <em class="italic">negative</em>, and <em class="italic">neutral</em> cases. This is a much simpler scheme to process and ascertain, and yields results that are also easier to understand. In some ways, emotion analysis may be regarded as an <em class="italic">upgrade</em> to sentiment analysis; a more complex solution that analyzes much more than the simple positive and negative markers and instead tries to determine specific emotions (anger, joy, sadness). This may be more useful but also involves much more effort, time, and cost. Emotion and sentiment are, thus, not the same. An emotion is a complex psychological state, whereas a sentiment is a mental attitude that is created through the very existence of <span class="No-Break">the emotion.</span></p>
<p>For us, sentiment refers exclusively to an expressed opinion that is positive, negative, or neutral. There is some degree of overlap here because, for example, emotions such as joy and love could both be considered positive sentiments. It may be that the terms simply have different granularity – in the same way that ecstasy, joy, and contentment provide a fine-grained classification of a single generic emotion class that we might call happiness, happiness and love are a fine-grained classification of the general notion of feeling positive. Alternatively, it may be that sentiment is the name for one of the axes in the dimensional model – for example, the valence axis in Russell’s analysis. Given the range of theories of emotion, it seems best to just avoid having another term for much the same thing. In this book, we will stick to the term emotion; we will take an entirely pragmatic approach by accepting some set of labels from an existing theory such as Plutchik’s or Russell’s as denoting emotions, without worrying too much about what it is that they denote. We can all agree that <em class="italic">I hate the people who did that and I wish they were all dead</em> expresses hate and anger, and that it is overall negative, even if we’re not sure what hate and anger are or what the scale from negative to positive <span class="No-Break">actually measures.</span></p>
<p>Now that we<a id="_idIndexMarker014"/> know a bit more about what emotion is and how it is categorized and understood, it is essential to understand why emotion analysis is an <span class="No-Break">important topic.</span></p>
<h1 id="_idParaDest-21"><a id="_idTextAnchor024"/>Why emotion analysis is important</h1>
<p>The amount of data <a id="_idIndexMarker015"/>generated daily from online sources such as social media and blogs is staggering. In 2019, Forbes estimated this to be around 2.5 quintillion bytes of data, though this figure <span class="No-Break">is more</span></p>
<p>than likely even higher now. Due to this, much research has focused on using this data for analysis and for gaining hitherto unknown insights (for example, predicting flu trends and disease outbreaks using Twitter (now known as “<span class="No-Break">X”) data).</span></p>
<p>Similarly, people are also increasingly expressing their opinions online – and many of these opinions are, explicitly or implicitly, highly emotional (for example, <em class="italic">I love summer</em>). Nowadays, social network platforms such as Facebook, LinkedIn, and Twitter are at the hub of everything we do. Twitter is one of the most popular social network platforms, with more than 300 million users using Twitter actively every month. Twitter is used by people from all walks of life; celebrities, movie stars, politicians, sports stars, and everyday people. Users post short messages, known <a id="_idIndexMarker016"/>as <strong class="bold">tweets</strong>, and, every day, millions share their opinions about themselves, news, sports, movies, and other topics. Consequently, this makes platforms such as Twitter rich sources of data for public opinion mining and <span class="No-Break">sentiment analysis.</span></p>
<p>As we have seen, emotions play an important role in human intelligence, decision-making, social interaction, perception, memory, learning, creativity, and much <span class="No-Break">much more.</span></p>
<p>Emotion analysis is the process of recognizing the emotions that are expressed through texts (for example, social media posts). It is a complex task because user-generated content, such as tweets, is typically understood <span class="No-Break">as follows:</span></p>
<ul>
<li>Written in <span class="No-Break">natural language</span></li>
<li>Often unstructured, informal, <span class="No-Break">and misspelled</span></li>
<li>Can contain slang and <span class="No-Break">made-up words</span></li>
<li>Can contain emojis and emoticons where their usage does not always correspond to the reason for their original creation (for example, using the pizza emoji to <span class="No-Break">express love)</span></li>
</ul>
<p>Furthermore, it is also entirely possible to express emotion without using any obvious <span class="No-Break">emotion markers.</span></p>
<p>One of the big unsolved problems in emotion analysis is detecting emotions such as anticipation, pessimism, and sarcasm. Consider the <span class="No-Break">following tweet:</span></p>
<p><em class="italic">We lost </em><span class="No-Break"><em class="italic">again. Great.</em></span></p>
<p>We humans are<a id="_idIndexMarker017"/> fairly knowledgeable when it comes to drilling down to the true meaning implied, and would understand that the user was being sarcastic. We know that a team losing again is not a good thing. Hence, by making use of this understanding, we can easily identify the <span class="No-Break">implied meaning.</span></p>
<p>The problem is that simply considering each word that has sentiment in isolation will not do a good job. Instead, further rules must be applied to understand the context of the word. These rules will help the analyzer differentiate between sentences that might contain similar words but have completely different meanings. However, even with these rules, analyzers will still <span class="No-Break">make mistakes.</span></p>
<p>Social media is now viewed as a valuable resource, so organizations are showing an increased interest in social media monitoring to analyze massive, free-form, short, user-generated text <span class="No-Break">from social</span></p>
<p>media sites. Exploiting these allows organizations to gain insights into understanding their customer’s opinions, concerns, and needs about their products <span class="No-Break">and services.</span></p>
<p>Due to its real-time nature, governments are also interested in using social media to identify threats and monitor and analyze public responses to <span class="No-Break">current events.</span></p>
<p>Emotion analysis has many <span class="No-Break">interesting applications:</span></p>
<ul>
<li><strong class="bold">Marketing</strong>: Lots of Twitter <a id="_idIndexMarker018"/>users follow brands (for example, Nike), so there are many marketing opportunities. Twitter can help spread awareness of a brand, generate leads, drive traffic to sites, build a customer base, and more. Some of the biggest marketing campaigns of previous years include <strong class="source-inline">#ShareACoke</strong> by Coca-Cola, <strong class="source-inline">#WantAnR8</strong> by Audi, and <strong class="source-inline">#BeTheFastest</strong> by <span class="No-Break">Virgin Media.</span></li>
<li><strong class="bold">Stock markets</strong>: Academics have attempted to use Twitter to anticipate trends in financial markets. In 2013, the Associated Press Twitter account posted a (false) tweet stating that there had been explosions in the White House and that Obama was injured. The post was debunked very quickly but the stock markets still took a nosedive, resulting in hundreds of billions of dollars <span class="No-Break">changing hands.</span></li>
<li><strong class="bold">Social studies</strong>: Millions of people regularly interact with the world by tweeting, providing invaluable insights into their feelings, actions, routines, emotions, and behavior. This vast amount of public communication can be used to generate forecasts of various types of events. For example, large-scale data analysis of social media has demonstrated that not only did Brexit supporters have a more powerful, emotional message, but they were also more effective in the use of social media. They routinely outmuscled their rivals and had more vocal and active supporters<a id="_idIndexMarker019"/> across nearly all social media platforms. This led to the activation of a greater number of Leave supporters and enabled them to dominate social media platforms – thus influencing many <span class="No-Break">undecided voters.</span></li>
</ul>
<p>Gaining an <a id="_idIndexMarker020"/>understanding of emotions is also important for organizations to gain insights into public opinion about their products and services. However, it is also important to automate this process so that decisions can be made and actions can be taken in real-time. For example, analysis techniques can automatically analyze and process thousands of reviews about a particular product and extract insights that show whether consumers are satisfied with the product or service. This can be sentiment or emotion, although emotion may be more useful due to it being <span class="No-Break">more granular.</span></p>
<p>Research has shown that tweets posted by dissatisfied users are shared more often and spread faster and wider than other types of tweets. Therefore, organizations have to provide customer services beyond the old-fashioned agent at the end of the phone line. Due to this, many organizations today also provide social media-based customer support in an attempt to head-off bad reviews and give a good impression. Nowadays, there is so much consumer choice, and it is so much easier for customers to switch to competitors, that it is vitally important for organizations to retain and increase their customer base. Hence, the quicker an organization reacts to a bad post, the better chance <span class="No-Break">they have</span></p>
<p>of retaining the customer. Furthermore, there is no better advertising than word of mouth – such as that generated by happy customers. Emotion analysis is one way to quickly analyze hundreds of tweets, find the ones where customers are unhappy, and use this to drive other processes that attempt to resolve the problem before the customer becomes too unhappy and decides to take their business elsewhere. Emotion analysis not only requires data – it also generates a lot of data. This data can be further analyzed to determine, for example, what the top items on user wishlists are, or what the top user gripes are. These can then be used to drive the next iteration or version of the product <span class="No-Break">or service.</span></p>
<p>Although sentiment analysis and emotion analysis are not mutually exclusive and can be used in conjunction, the consensus is that sentiment analysis is not adequate for classifying something as complex, multi-layered, and nuanced as emotion. Simply taking the whole range of emotions and considering them as only positive, negative, or neutral runs the considerable risk of missing out on deeper insights <span class="No-Break">and understandings.</span></p>
<p>Emotion analysis also provides more in-depth insights. Understanding why someone ignored or liked a post<a id="_idIndexMarker021"/> needs more than just a sentiment score. Furthermore, gaining <em class="italic">actionable</em> insights also requires more than just a <span class="No-Break">sentiment score.</span></p>
<p>Emotion analysis is a sub-field of NLP, so it makes sense to gain a better understanding of <span class="No-Break">that next.</span></p>
<h1 id="_idParaDest-22"><a id="_idTextAnchor025"/>Introduction to NLP</h1>
<p>Sentiment mining is<a id="_idIndexMarker022"/> about finding the sentiments that are expressed by natural language texts – often quite short texts such as tweets and online reviews, but also larger items such as newspaper articles. There are many other ways of getting computers to do useful things with natural language texts and spoken language: you can write programs that can have conversations (with people or with each other), you can write programs to extract facts and events from articles and stories, you can write programs to translate from one language to another, and so on. These applications all share some basic notions and techniques, but they each lay more emphasis on some topics and less on others. In <a href="B18714_04.xhtml#_idTextAnchor093"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Preprocessing – Stemming, Tagging, and Parsing</em>, we will look at the things that matter most for sentiment mining, but we will give a brief overview of the main principles of NLP here. As noted, not all of the stages outlined here are needed for every application, but it is nonetheless useful to have a picture of how everything fits together when considering specific <span class="No-Break">subtasks later.</span></p>
<p>We will start with a couple of <span class="No-Break">basic observations:</span></p>
<ul>
<li>Natural language is <em class="italic">linear</em>. The <a id="_idIndexMarker023"/>fundamental form of language is speech, which is necessarily linear. You make one sound, and then you make another, and then you make another. There may be some variation in the way you make each sound – louder or softer, with a higher pitch or a lower one, quicker or slower – and this may be used to overlay extra information on the basic message, but fundamentally, spoken language is made up of a <em class="italic">sequence</em> of identifiable units, namely sounds; and since written language is just a way of representing spoken language, it too must be made up of a sequence of <span class="No-Break">identifiable units.</span></li>
<li>Natural language<a id="_idIndexMarker024"/> is hierarchical. Smaller units are grouped into larger units, which are grouped<a id="_idIndexMarker025"/> into larger units, which are grouped into larger units, and so on. Consider the sentence smaller units are grouped into larger units. In the written form of English, for instance, the smallest units are characters; these are grouped into morphemes (meaning-bearing word-parts), as small er unit s are group ed into large er unit s, which are grouped into words (small-er unit-s are group-ed into large-er unit-s), which are grouped into base-level phrases ([small-er unit-s] [are group-ed] [into] [large-er unit-s]), which are grouped into higher-level phrases ([[small-er unit-s] [[are group-ed] [[into] [<span class="No-Break">large-er unit-s]]]]]).</span></li>
</ul>
<p>These two properties hold for all natural languages. All natural languages were spoken before they were written (some widely spoken languages have no universally accepted written form!), and hence are fundamentally linear. But they all express complex hierarchical relations, and hence to understand them, you have to be able to find the ways that smaller units are grouped into <span class="No-Break">larger ones.</span></p>
<p>What the bottom-level units are like, and how they are grouped, differs from language to language. The sounds of a language are made by moving your articulators (tongue, teeth, lips, vocal cords, and various other things) around while trying to expel air from your lungs. The sound that you get by closing and then opening your lips with your vocal cords tensed (/b/, as in the English word <em class="italic">bat</em>) is different from the sound you get by doing the same things with your lips while your vocal cords are relaxed (/p/, as in <em class="italic">pat</em>). Different languages use different combinations – Arabic doesn’t use /p/ and English doesn’t use the sound you get by closing the exit from the chamber containing the vocal cords (a <strong class="bold">glottal stop</strong>): the<a id="_idIndexMarker026"/> combinations that are used in a particular language are <a id="_idIndexMarker027"/>called its <strong class="bold">phonemes</strong>. Speakers of a language that don’t use a particular combination find it hard to distinguish words that use it from ones that use a very similar combination, and very hard to produce that combination when they learn a language <span class="No-Break">that does.</span></p>
<p>To make matters worse, the relationship between the bottom-level units in spoken language and written language can vary from language to language. The phonemes of a language can be represented in the written form of that language in a wide variety of ways. The written form may make<a id="_idIndexMarker028"/> use of <strong class="bold">graphemes</strong>, which are combinations of ways of making a shape out of strokes and marks (so, <span class="CharOverride-1">A</span><span class="CharOverride-2">A</span><span class="CharOverride-3">A</span><span class="CharOverride-4">A</span><span class="CharOverride-5">A</span><span class="CharOverride-6">A</span> are all written by producing two near-vertical <a id="_idIndexMarker029"/>more-or-less-straight lines joined at the top with a cross-piece about half-way up), just as phonemes are combinations of ways of making a sound; a single phoneme may be represented by one grapheme (the short vowel /a/ from <em class="italic">pat</em> is represented in English by the character <em class="italic">a</em>) or by a combination of graphemes (the sound /sh/ from <em class="italic">should</em> is represented by the pair of graphemes <em class="italic">s</em> and <em class="italic">h</em>); a sound may have no representation in the written form (Arabic text omits short vowels and some other distinctions between phonemes); or there may simply be no connection between the written form and the way it is pronounced (written Chinese, Japanese kanji symbols). Given that we are going to be largely looking at text, we can at least partly ignore the wide variety of ways that written and spoken language are related, but we will still have to be aware that different languages combine the basic elements of the written forms in completely different ways to make <span class="No-Break">up words.</span></p>
<p>The bottom-level units of a language, then, are either identifiable sounds or identifiable marks. These are combined into groups that carry<a id="_idIndexMarker030"/> meaning – <strong class="bold">morphemes</strong>. A morpheme can carry quite a lot of meaning; for example, <em class="italic">cat</em> (made out of the graphemes <em class="italic">c</em>, <em class="italic">a</em>, and <em class="italic">t</em>) denotes a small mammal with pointy ears and an inscrutable outlook on life, whereas <em class="italic">s</em> just says that you’ve got more than <span class="No-Break">one item</span> of the kind you are thinking about, so <em class="italic">cats</em> denotes a group of several small mammals with pointy ears and an opaque view of the world. Morphemes of the first kind are sometimes called <strong class="bold">lexemes</strong>, with<a id="_idIndexMarker031"/> a single lexeme combining with one or more other morphemes to express a concept (so, the French lexeme <em class="italic">noir</em> (<em class="italic">black</em>) might combine with <em class="italic">e</em> (feminine) and <em class="italic">s</em> (plural) to make <em class="italic">noires</em> – several black female things). Morphemes that add information to a lexeme, such as about how many things were involved or when an event happened, are <a id="_idIndexMarker032"/>called <strong class="bold">inflectional</strong> morphemes, whereas ones that radically change their meaning (for example an <em class="italic">incomplete</em> solution to a problem is <em class="italic">not</em> complete) are<a id="_idIndexMarker033"/> called <strong class="bold">derivational</strong> morphemes, since they derive a new concept from the original. Again, most languages make use of inflectional and derivational morphemes to enrich the basic set of lexemes, but exactly how this works varies from language to language. We will revisit this at some length in <a href="B18714_05.xhtml#_idTextAnchor116"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> , <em class="italic">Sentiment Lexicons and Vector Space Models</em> since finding the core lexemes can be significant when we are trying to assign emotions <span class="No-Break">to texts.</span></p>
<p>A lexeme plus a suitable set of morphemes is often referred to<a id="_idIndexMarker034"/> as a <strong class="bold">word</strong>. Words are typically grouped into larger tree-like structures, with the way that they are grouped carrying a substantial part of the message conveyed by the text. In the sentence <em class="italic">John believes that Mary expects Peter to marry Susan</em>, for instance, <em class="italic">Peter to marry Susan</em> is a group that describes a particular kind of event, <em class="italic">Mary expects [Peter to marry Susan]</em> is a group that describes Mary’s attitude to this event, and <em class="italic">John believes [that Mary expected [Peter to marry Susan]]</em> is a group that describes John’s view of <span class="No-Break">Mary’s expectation.</span></p>
<p>Yet again, different languages carry out this kind of grouping in different ways, and there are numerous ways of approaching the task of analyzing the grouping in particular cases. This is not the place for a review of all the grammatical theories that have ever been proposed to <a id="_idIndexMarker035"/>analyze the ways that words get grouped together or of all the algorithms that have ever been proposed for applying those theories to specific <a id="_idIndexMarker036"/>cases (<strong class="bold">parsers</strong>), but there are a few general observations that are <span class="No-Break">worth making.</span></p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor026"/>Phrase structure grammar versus dependency grammar</h2>
<p>In some<a id="_idIndexMarker037"/> languages, groups<a id="_idIndexMarker038"/> are <a id="_idIndexMarker039"/>mainly formed by merging adjacent groups. The previous sentence, for instance, can be analyzed if we group it <span class="No-Break">as follows:</span></p>
<p><em class="italic">In some languages groups are mainly formed by merging </em><span class="No-Break"><em class="italic">adjacent groups</em></span></p>
<p><em class="italic">In [some languages]</em><span class="subscript">np</span><em class="italic"> groups are mainly formed by merging [</em><span class="No-Break"><em class="italic">adjacent groups]</em></span><span class="No-Break"><span class="subscript">np</span></span></p>
<p><em class="italic">[In [some languages]]</em><span class="subscript">pp</span><em class="italic"> groups are mainly formed by [merging [</em><span class="No-Break"><em class="italic">adjacent groups]]</em></span><span class="No-Break"><span class="subscript">vp</span></span></p>
<p><em class="italic">[In [some languages]]</em><span class="subscript">pp</span><em class="italic"> groups are mainly formed [by [merging [</em><span class="No-Break"><em class="italic">adjacent groups]]]</em></span><span class="No-Break"><span class="subscript">pp</span></span></p>
<p><em class="italic">[In [some languages]]</em><span class="subscript">pp</span><em class="italic"> groups are mainly [formed [by [merging [</em><span class="No-Break"><em class="italic">adjacent groups]]]]</em></span><span class="No-Break"><span class="subscript">vp</span></span></p>
<p><em class="italic">[In [some languages]]</em><span class="subscript">pp</span><em class="italic"> groups are [mainly [formed [by [merging [</em><span class="No-Break"><em class="italic">adjacent groups]]]]]</em></span><span class="No-Break"><span class="subscript">vp</span></span></p>
<p><em class="italic">[In [some languages]]</em><span class="subscript">pp</span><em class="italic"> groups [are [mainly [formed [by [merging [</em><span class="No-Break"><em class="italic">adjacent groups]]]]]]</em></span><span class="No-Break"><span class="subscript">vp</span></span></p>
<p><em class="italic">[In [some languages]]</em><span class="subscript">pp</span><em class="italic"> [groups [are [mainly [formed [by [merging [</em><span class="No-Break"><em class="italic">adjacent groups]]]]]]]</em></span><span class="No-Break"><span class="subscript">s</span></span></p>
<p><em class="italic">[[In [some languages]][groups [are [mainly [formed [by [merging [</em><span class="No-Break"><em class="italic">adjacent groups]]]]]]]]</em></span><span class="No-Break"><span class="subscript">s</span></span></p>
<p>This tends to work well for languages where word order is largely fixed – no languages have completely fixed word order (for example, the preceding sentence could be rewritten as <em class="italic">Groups are mainly formed by merging adjacent groups in some languages</em> with very little change in meaning), but some languages allow more freedom than others. For languages such as English, analyzing the relationships between words in terms of adjacent phrases, such as using a <strong class="bold">phrase structure grammar</strong>, works <span class="No-Break">quite well.</span></p>
<p>For languages where words and phrases are allowed to move around fairly freely, it can be more convenient to record pairwise relationships between words. The following tree describes the same sentence using a <strong class="bold">dependency grammar</strong> – that is, by assigning a parent word to every word (apart from the full stop, which we are taking to be the root of <span class="No-Break">the tree):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 1.3 – Analysis of “In some languages, groups are mainly formed by merging adjacent groups” using a rule-based dependency parser" height="1474" src="image/B18714_01_03.jpg" width="973"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Analysis of “In some languages, groups are mainly formed by merging adjacent groups” using a rule-based dependency parser</p>
<p>There are many <a id="_idIndexMarker040"/>variations <a id="_idIndexMarker041"/>of phrase <a id="_idIndexMarker042"/>structure grammar and many variations of dependency grammar. Roughly speaking, dependency grammar provides an easier handle on languages where words can move around very freely, while phrase structure grammar makes it easier to deal with <em class="italic">invisible</em> items such as the subject of <em class="italic">merging</em> in the preceding example. The difference between the two is, in any case, less clear than it might seem from the preceding figure: a dependency tree can easily be transformed into a phrase structure tree by treating each subtree as a phrase, and a phrase structure tree can be transformed into a dependency tree if you can specify which item in a phrase <a id="_idIndexMarker043"/>is its <strong class="bold">head</strong> – for example, in the preceding phrase structure tree, the head of a group labeled as <strong class="bold">nn</strong> is its noun and the head of a <a id="_idIndexMarker044"/>group <a id="_idIndexMarker045"/>labeled as <strong class="bold">np</strong> is the head <a id="_idIndexMarker046"/><span class="No-Break">of </span><span class="No-Break"><strong class="bold">nn</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor027"/>Rule-based parsers versus data-driven parsers</h2>
<p>As well as having <a id="_idIndexMarker047"/>a theory of how to describe the structure of a piece of text, you need a program that applies that theory to specific texts – a <strong class="bold">parser</strong>. There<a id="_idIndexMarker048"/> are two ways to approach the development of <span class="No-Break">a parser:</span></p>
<ul>
<li><strong class="bold">Rule-based</strong>: You can try to devise a set of rules that describe the way that a particular<a id="_idIndexMarker049"/> language <a id="_idIndexMarker050"/>works (a <strong class="bold">grammar</strong>), and then implement a program that tries to apply these rules to the texts you want analyzed. Devising such rules is difficult and time-consuming, and programs that try to apply them tend to be slow and fail if the target text does not obey <span class="No-Break">the rules.</span></li>
<li><strong class="bold">Data-driven</strong>: You can somehow <a id="_idIndexMarker051"/>produce a set of analyses of a large number of texts (a <strong class="bold">treebank</strong>), and <a id="_idIndexMarker052"/>then implement a program that extracts patterns from these analyses. Producing a treebank is difficult and time-consuming – you need hundreds of thousands of examples, and the trees all have to be consistently annotated, which means that if this is to be done by people, then they have to be given consistent guidelines that cover every example they will see (which is, in effect, a grammar) (and if it is not done by people then you must already have an automated way of doing it, that is, <span class="No-Break">a parser!).</span></li>
</ul>
<p>Both approaches have advantages and disadvantages: when considering whether to use a dependency grammar or a phrase structure grammar and then when considering whether to follow a rule-based approach or a data-driven one, there are several criteria to be considered. Since <em class="italic">no</em> existing system optimizes all of these, you should think about which ones matter most for your application and then decide which way <span class="No-Break">to go:</span></p>
<ul>
<li><strong class="bold">Speed</strong>: The first <a id="_idIndexMarker053"/>criterion to consider is the speed at which the<a id="_idIndexMarker054"/> parser runs. Some parsers can become very slow when faced with long sentences. The worst-case complexity of the <a id="_idIndexMarker055"/>standard <strong class="bold">chart-parsing</strong> algorithm for rule-based approaches is O(N<span class="superscript">3</span>), where <em class="italic">N</em> is the length of the sentence, which means that for long sentences, the algorithm can take a <em class="italic">very</em> long time. Some other algori<a id="_idTextAnchor028"/>thms have much better complex<a id="_idTextAnchor029"/>ity than this (the MALT (Nivre et al., 2006) and MST (McDonald et al., 2005) parsers, for instance, are linear in the length of the sentence), while others have much worse. If two parsers are equally good according to all the other criteria, then the faster one will be preferable, but there will be situations where one (or more) of the other criteria is <span class="No-Break">more important.</span></li>
<li><strong class="bold">Robustness</strong>: Some parsers, particularly rule-based ones, can fail to produce any analysis at all for some sentences. This will happen if the input is ungrammatical, but it <a id="_idIndexMarker056"/>will<a id="_idIndexMarker057"/> also happen if the rules are not a complete description of the language. A parser that fails to produce a perfectly grammatical input sentence is less useful than one that can analyze every grammatically correct sentence of the target language. It is less clear that parsers that will do something with every input sentence are necessarily more useful than ones that will reject some sentences as being ungrammatical. In some applications, detecting ungrammaticality is a crucial part of the task (for example, in language learning programs), but in any case, assigning an analysis to an ungrammatical sentence cannot be either right or wrong, and hence any program that makes use of such an analysis cannot be sure that it is doing the <span class="No-Break">right thing.</span></li>
<li><strong class="bold">Accuracy</strong>: A parser that assigns the <em class="italic">right</em> analysis to every input text will generally be more useful than one that does not. This does, of course, beg the question of how to decide what the right analysis is. For data-driven parsers, it is impossible to say what the right analysis of a sentence that does not appear in the treebank is. For rule-based parsers, any analysis that is returned will be right in the sense that it obeys the rules. So, if an analysis looks odd, you have to work out how the rules led to it and revise <span class="No-Break">them accordingly.</span></li>
</ul>
<p>There is a trade-off between accuracy and robustness. A parser that fails to return any analysis at all in complex cases will produce fewer wrong analyses than one that tries to find some way of interpreting every input text: the one that simply rejects some sentences will have lower recall but may have higher precision, and that can be a good thing. It may be better to have a system that says <em class="italic">Sorry, I didn’t quite understand what you just said</em> than one that goes ahead with whatever it is supposed to be doing based on an <span class="No-Break">incorrect interpretation.</span></p>
<ul>
<li><strong class="bold">Sensitivity and consistency</strong>: Sometimes, sentences <a id="_idIndexMarker058"/>that look <a id="_idIndexMarker059"/>superficially similar have different underlying structures. Consider the <span class="No-Break">following examples:</span><ol><li class="Paragraph-Style-1">	a) I want to see <span class="No-Break">the queen</span>
b) I went to see <span class="No-Break">the queen</span></li></ol></li>
</ul>
<p>1(a) is the answer to <em class="italic">What do you want?</em> and 2(b) is the answer to <em class="italic">Why did you go?</em> If the <a id="_idIndexMarker060"/>structures<a id="_idIndexMarker061"/> that are <a id="_idIndexMarker062"/>assigned to these two sentences do not reflect the different roles for <em class="italic">to see the queen</em>, then it will be impossible to make <span class="No-Break">this distinction:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 1.4 – Trees for 1(a) and 1(b) from the Stanford dependency parser (Dozat et al., 2017)" height="941" src="image/B18714_01_04.jpg" width="1276"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – T<a id="_idTextAnchor030"/>rees for 1(a) and 1(b) from the Stanford dependency parser (Dozat et al., 2017)</p>
<ol>
<li class="Paragraph-Style-1" value="2">	a) One of <a id="_idIndexMarker063"/>my <a id="_idIndexMarker064"/>best friends is watching <span class="No-Break">old m<a id="_idTextAnchor031"/>ovies</span>
b) One of my favorite pastimes is watching <span class="No-Break">old movies</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 1.5 – Trees for 2(a) and 2(b) from the Stanford dependency parser" height="765" src="image/B18714_01_05.jpg" width="1658"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Trees for 2(a) and 2(b) from the Stanford dependency parser</p>
<p>The <strong class="bold">Stanford dependency parser</strong> (<strong class="bold">SDP</strong>) trees<a id="_idIndexMarker065"/> both say that the subject (<em class="italic">One of my best friends</em>, <em class="italic">One of my favorite pastimes</em>) is carrying out the action of <a id="_idIndexMarker066"/>watching old movies – it is sitting in its most comfortable armchair with the curtains drawn and the TV on. The first of these makes sense, but the second doesn’t: pastimes don’t watch old movies. What we need is an equational analysis that says that <em class="italic">One of my favorite pastimes</em> and <em class="italic">watching old movies</em> are the same thing, as in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="Figure 1.6 – Equational analysis of “One of my favorite pastimes is watching old movies”" height="1109" src="image/B18714_01_06.jpg" width="1548"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Equational analysis of “One of my favorite pastimes is watching old movies”</p>
<p>Spotting that 2(b) requires an analysis like this, where my favorite pastime is the predication<a id="_idIndexMarker067"/> in <a id="_idIndexMarker068"/>an <a id="_idIndexMarker069"/>equational use of <em class="italic">be</em> rather than the agent of a watching-old-movies event, requires more detail about the words in question than is usually embodied in <span class="No-Break">a treebank.</span></p>
<p>It can also happen that sentences that look superficially different have very similar <span class="No-Break">underlying structures:</span></p>
<ol>
<li class="Paragraph-Style-1" value="3">	a) Few great tenors <span class="No-Break">are poor</span>
b) Most great tenors <span class="No-Break">are rich</span></li>
</ol>
<p>This time, the SDP assigns quite different structures to the <span class="No-Break">two sentences:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 1.7 – Trees for 3(a) and 3(b) from the SDP" height="985" src="image/B18714_01_07.jpg" width="1189"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – Trees for 3(a) and 3(b) from the SDP</p>
<p>The <a id="_idIndexMarker070"/>analysis <a id="_idIndexMarker071"/>of 3(a) assigns <em class="italic">most</em> as<a id="_idIndexMarker072"/> a modifier of <em class="italic">great</em>, whereas the analysis of 3(b) assigns <em class="italic">few</em> as a modifier of <em class="italic">tenors</em>. <em class="italic">Most</em> can indeed be used for modifying adjectives, as in <em class="italic">He is the most annoying person I know</em>, but in 3(a), it is acting as something more like a determiner, just as <em class="italic">few</em> is <span class="No-Break">in 3(b).</span></p>
<ol>
<li class="Paragraph-Style-1" value="4">	a) There are great tenors who <span class="No-Break">are rich</span>
b) Are there great tenors who <span class="No-Break">are rich?</span></li>
</ol>
<p>It is clear that 4(a) and 4(b) should have almost identical analyses – 4(b) is just 4(a) turned into a question. Again, this can cause problems for <span class="No-Break">treebank-based parsers:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 1.8 – Trees for 4(a) and 4(b) from MALTParser" height="905" src="image/B18714_01_08.jpg" width="1660"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Trees for 4(a) and 4(b) from MALTParser</p>
<p>The analysis in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.8</em> for 4(a) makes <em class="italic">are</em> the head of the tree, with <em class="italic">there</em>, <em class="italic">great tenors who are rich</em>, and as daughters, whereas 4(b) is given <em class="italic">tenors</em> as its head and <em class="italic">are</em>, <em class="italic">there</em>, <em class="italic">great</em>, <em class="italic">who are rich</em>, and <em class="italic">?</em> as daughters. It would be difficult, given these analyses, to see that 4(a) is the answer <span class="No-Break">to 4(b)!</span></p>
<p>Treebank-based parsers frequently fail to cope with issues of the kind raised by the examples given here. The problem is that the treebanks on which they are trained tend not to include detailed information about the words that appear in them – that <em class="italic">went</em> is an intransitive verb and <em class="italic">want</em> requires a sentential complement, that friends are human and can therefore watch old movies while pastimes are events, and can therefore be equated with the activity of watching something, or that <em class="italic">most</em> can be used in a wide variety <span class="No-Break">of ways.</span></p>
<p>It is <a id="_idIndexMarker073"/>not <a id="_idIndexMarker074"/>possible<a id="_idIndexMarker075"/> to say that all treebank-based parsers suffer from these problems, but several very widely used ones (the SDP<a id="_idTextAnchor032"/>, the version of MALT distribute<a id="_idTextAnchor033"/>d with the NLTK, the EasyCCG parser (Lewis &amp; Steedman, 2014), spaCy (Kitaev &amp; Klein, 2018)) do. Some of these issues are fairly widespread (the failure to distinguish 1(a) and 1(b)), and some arise because of specific properties of either the treebank or the parsing algorithm. Most of the pre-trained models for parsers such as MALT and SPACY are trained on the well-known Wall Street Journal corpus, and since this treebank does not distinguish between sentences such as 1(a) and 1(b), it is impossible for parsers trained on it to do so. All the parsers listed previously assign different structures to 3(a) and 3(b), which may be a characteristic of the treebank or it may be some property of the training algorithms. It is worth evaluating the output of any such parser to check that it does give distinct analyses for obvious cases such as 1(a) and 1(b) and does give parallel analyses for obvious cases such as 4(a) <span class="No-Break">and 4(b).</span></p>
<p>So, when choosing a parser, you have to weigh up a range of factors. Do you care if it sometimes makes mistakes? Do you want it to assign different trees to texts whose underlying representations are different (this isn’t quite the same as accuracy because it could happen that what the parser produces isn’t wrong, it just doesn’t contain all the information you need, as in 1(a) and 1(b))? Do you want it to always produce a tree, even for texts that don’t conform to any of the rules of normal language (should it produce a parse for <em class="italic">#anxious don’t know why ................. #worry</em> 😶 <em class="italic">slowly going #mad hahahahahahahahaha</em>)? Does it matter if it takes 10 or 20 seconds to parse some <a id="_idIndexMarker076"/>sentences? Whatever<a id="_idIndexMarker077"/> you do, <em class="italic">do not trust what anyone says about a parser</em>: try<a id="_idIndexMarker078"/> it for yourself, on the data that you are intending to use it on, and check that its output matches <span class="No-Break">your needs.</span></p>
<h2 id="_idParaDest-25"><a id="_idTextAnchor034"/>Semantics (the study of meaning)</h2>
<p>As we’ve seen, finding<a id="_idIndexMarker079"/> words, assigning them to<a id="_idIndexMarker080"/> categories, and finding the relationships between them is quite hard work. There would be no point in doing this work unless you had some application in mind that could make use of it. The key here is that the choice of words and the relationships between them are what allow language to carry messages, to have meaning. That’s why language is important; because it carries messages. Almost all application programs that do anything with natural language are concerned with the message carried by the input text, so almost all such programs have to identify the words that are present and the way they <span class="No-Break">are arranged.</span></p>
<p>The study of how language encodes messages is known as semantics. As just noted, the message is encoded by the words that <a id="_idIndexMarker081"/>are present (<strong class="bold">lexical semantics</strong>) and<a id="_idIndexMarker082"/> the <a id="_idIndexMarker083"/>way they are<a id="_idIndexMarker084"/> arranged (<strong class="bold">compositional semantics</strong>). They are both crucial: you can’t understand the difference between <em class="italic">John loves Mary</em> and <em class="italic">John hates Mary</em> if you don’t know what <em class="italic">loves</em> and <em class="italic">hates</em> mean, and you can’t understand the difference between <em class="italic">John loves Mary</em> and <em class="italic">Mary loves John</em> if you don’t know how being the subject or object of a verb encodes the relationship between the things denoted by <em class="italic">John</em> and <em class="italic">Mary</em> and the event denoted <span class="No-Break">by </span><span class="No-Break"><em class="italic">loves</em></span><span class="No-Break">.</span></p>
<p>The key test for a theory of semantics is the ability to carry out inference between sets of natural language texts. If you can’t do the inferences in 1–7 (where P1, …, Pn |- Q means that Q can be inferred f<a id="_idTextAnchor035"/>rom the premises P1, …, Pn), then you <a id="_idTextAnchor036"/>cannot be said to <span class="No-Break">understand English:</span></p>
<ol>
<li>John hates Mary |- John <span class="No-Break">dislikes Mary</span></li>
<li>(a) John and Mary are divorced |- John and Mary are <span class="No-Break">not<a id="_idTextAnchor037"/> married</span></li>
<li>(b) John and Mary are divorced |- <a id="_idTextAnchor038"/>John and Mary used to <span class="No-Break">be married</span></li>
<li>I saw a man with a big nose |- I s<a id="_idTextAnchor039"/>aw <span class="No-Break">a man</span></li>
<li>Every woman distrusts John, Mary is a woman |- Mar<a id="_idTextAnchor040"/>y <span class="No-Break">distrusts John</span></li>
<li>I saw more than three pigeons |- I saw at least <span class="No-Break">four birds</span></li>
<li>I doubt that she saw anyone |- I do not believe she saw a <span class="No-Break">fat man</span></li>
</ol>
<p>These are very simple inferences. If someone said that the conclusions didn’t follow from the premises, you<a id="_idIndexMarker085"/> would have to say that they just don’t understand English properly. They <a id="_idIndexMarker086"/>involve a range of different kinds of knowledge – simple entailment relationships between words (<em class="italic">hates</em> entails <em class="italic">dislikes</em> (1)); more complex relationships between words (getting divorced means canceling an existing marriage (2), so if John and Mary are divorced, then they are not now married but at one time they were); the fact that <em class="italic">a man with a big nose</em> is something that is a man and has a big nose plus the fact that <em class="italic">A and </em><em class="italic">B</em> entails <em class="italic">A </em>(3); an understanding of how quantifiers work ((4) and (5)); combinations of all of these (6) – but they are all inferences that anyone who understands English would <span class="No-Break">agree with.</span></p>
<p>Some of this information can be fairly straightforwardly extracted from corpora. There is a great deal of work, for instance, on calculating the similarity between pairs of words, though extending that to cover entailments between words has proved more difficult. Some of it is much more difficult to find using data-driven methods – the relationships between <em class="italic">more than</em> and <em class="italic">at least</em>, for instance, cannot easily be found in corpora, and the complex concepts that lie behind the word <em class="italic">divorce</em> would also be difficult to extract unsupervised from <span class="No-Break">a corpus.</span></p>
<p>Furthermore, some of it can be applied by using tree-matching algorithms of various kinds, from simple algorithms that just compute whether one tree is a subtree of another to more complex approaches that pay attention to polarity (that <em class="italic">doubt</em> flicks a switch that turns the direction of the matching algorithm round – <em class="italic">I know she loves him</em> |<em class="italic">- I know she likes him, I doubt she likes him</em> |- <em class="italic">I doubt she loves him</em>) and to the<a id="_idTextAnchor041"/> relationships between qu<a id="_idTextAnchor042"/>antifiers (<em class="italic">the</em> |- <em class="italic">some, more than N</em> |- <em class="italic">at least N-1</em>) (Alabbas &amp; Ramsay, 2013) (MacCartney &amp; Manning, 2014). Some of it requires more complex strategies, in particular examples with multiple premises (4), but all but the very simplest (for example, just treating a sentence as a bag of words) require accurate, or at least <span class="No-Break">consistent, trees.</span></p>
<p>Exactly how much of this machinery you need depends on your ultimate application. Fortunately for us, sentiment mining can be done reasonably effectively with fairly shallow approaches, but it should not be forgotten that there is a great deal more to understanding a text than simply knowing lexical relationships such as similarity or subsumption <span class="No-Break">between words.</span></p>
<p>Before wrapping <a id="_idIndexMarker087"/>up this chapter, we will spend some time <a id="_idIndexMarker088"/>learning about machine learning, looking at various machine learning models, and then working our way through a sample project <span class="No-Break">using Python.</span></p>
<h1 id="_idParaDest-26"><a id="_idTextAnchor043"/>Introduction to machine learning</h1>
<p>Before discussing <a id="_idIndexMarker089"/>machine learning, it makes sense to properly understand the term artificial intelligence. Broadly speaking, artificial intelligence is a branch of computer science and is the idea that machines can be made to think and act just like us humans, without explicit <span class="No-Break">programming instructions.</span></p>
<p>There is a common misconception that artificial intelligence is a <em class="italic">new thing</em>. The term is widely considered to have been coined in 1956 by assistant Professor of Mathematics John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. We are now in an AI boom – but it was not always so; artificial intelligence has a somewhat chequered history. Following on from the 1956 conference, funding flowed generously and rapid progress was made as researchers developed systems that could play chess and solve mathematical problems. Optimism was high, but progress stalled because promises made earlier about artificial intelligence were not able to be fulfilled, and hence the funding dried up; this cycle was repeated in the 1980s. The current boom we are experiencing is due to <a id="_idIndexMarker090"/>the timely advances and emergence of three <span class="No-Break">key technologies:</span></p>
<ul>
<li><strong class="bold">Big data</strong>: Giving us the amounts of data required to be able to do <span class="No-Break">artificial intelligence</span></li>
<li><strong class="bold">High-speed high-capacity storage devices</strong>: Giving us the ability to store <span class="No-Break">the data</span></li>
<li><strong class="bold">GPUs</strong>: Giving us the ability to process <span class="No-Break">the data</span></li>
</ul>
<p>Nowadays, AI is everywhere. Here <a id="_idIndexMarker091"/>are some examples <span class="No-Break">of AI:</span></p>
<ul>
<li>Chatbots (for example, customer <span class="No-Break">service chatbots)</span></li>
<li>Amazon Alexa, Apple’s Siri, and other <span class="No-Break">smart assistants</span></li>
<li><span class="No-Break">Autonomous vehicles</span></li>
<li><span class="No-Break">Spam filters</span></li>
<li><span class="No-Break">Recommendation engines</span></li>
</ul>
<p>According to experts, there are four types <span class="No-Break">of AI:</span></p>
<ul>
<li><strong class="bold">Reactive</strong>: This is the simplest<a id="_idIndexMarker092"/> type and involves machines programmed to always respond in the same predictable manner. They <span class="No-Break">cannot learn.</span></li>
<li><strong class="bold">Limited memory</strong>: This is the <a id="_idIndexMarker093"/>most common type of AI in use today. It combines pre-programmed information with historical data to <span class="No-Break">perform tasks.</span></li>
<li><strong class="bold">Theory of mind</strong>: This is a technology we may see in the future. The idea here is that a machine with a<a id="_idIndexMarker094"/> theory of mind AI will understand emotions, and then alter its own behavior accordingly as it interacts <span class="No-Break">with humans.</span></li>
<li><strong class="bold">Self-aware</strong>: This is<a id="_idIndexMarker095"/> the most advanced type of AI. Machines that are self-aware of their own emotions, and the emotions of those around them, will have a level of intelligence like human beings and will be able to make assumptions, inferences, and deductions. This is certainly one for the future as the technology for this doesn’t exist <span class="No-Break">just yet.</span></li>
</ul>
<p>Machine learning is one way to exploit AI. Writing software programs to cater to all situations, occurrences, and eventualities is time-consuming, requires effort, and, in some cases, is not even possible. Consider the task of recognizing pictures of people. We humans can handle this task easily, but the same is not true for computers. Even more difficult is programming a computer to do this task. Machine learning tackles this problem by getting the machine to program itself by learning <span class="No-Break">through experiences.</span></p>
<p>There is no universally agreed-upon definition of machine learning that everyone subscribes to. Some attempts include <span class="No-Break">the following:</span></p>
<ul>
<li>A branch of computer science that focuses on the use of data and algorithms to imitate the way that <span class="No-Break">humans learn</span></li>
<li>The capability of machines to imitate intelligent <span class="No-Break">human behavior</span></li>
<li>A subset of AI that allows machines to learn from data without being <span class="No-Break">programmed explicitly</span></li>
</ul>
<p>Machine learning needs data – and sometimes lots and lots <span class="No-Break">of it.</span></p>
<p>Lack of data is a significant weak spot in AI. Without a reasonable amount of data, machines cannot<a id="_idIndexMarker096"/> perform and generate sensible results. Indeed, in some ways, this is just like how we humans operate – we look and learn and then apply that knowledge in new, <span class="No-Break">unknown situations.</span></p>
<p>And, if we think about it, everyone has data. From the smallest sole trader to the largest organization, everyone will have sales data, purchase data, customer data, and more. The format of this data may differ between different organizations, but it is all useful data that can be used in machine learning. This data can be collected and processed and can be used to build machine <a id="_idIndexMarker097"/>learning models. Typically, this data is split into the <span class="No-Break">following sets:</span></p>
<ul>
<li><strong class="bold">Training set</strong>: This is <a id="_idIndexMarker098"/>always the largest of the datasets (typically 80%) and is the data that is used to train the machine <span class="No-Break">learning models.</span></li>
<li><strong class="bold">Development set</strong>: This <a id="_idIndexMarker099"/>dataset (10%) is used to tweak and try new parameters to find the ones that work the best for <span class="No-Break">the model.</span></li>
<li><strong class="bold">Test set</strong>: This is <a id="_idIndexMarker100"/>used to test (validate) the model (10%). The model has already seen the training data, so it cannot be used to test the model, hence this dataset is required. This dataset also allows you to determine whether the model is working well or requires <span class="No-Break">more training.</span></li>
</ul>
<p>It is good practice to have both development and test datasets. The process of building models involves finding the best set of parameters that give the best results. These parameters are determined by making use of the development set. Without the development set, we would be reduced to using the same datasets for training, testing, and evaluation. This is undesirable, but it can also present further problems unless handled carefully. For example, the datasets should be constructed such that the original dataset class proportions are<a id="_idIndexMarker101"/> preserved across the test and training sets. Furthermore, as a general point, training data should be checked for <span class="No-Break">the following:</span></p>
<ul>
<li>It is relevant to <span class="No-Break">the problem</span></li>
<li>It is large enough such that all use cases of the model <span class="No-Break">are covered</span></li>
<li>It is unbias<a id="_idTextAnchor044"/>ed and contains no imbalance toward any <span class="No-Break">particular category</span></li>
</ul>
<p>Modern toolkits such as <strong class="source-inline">sklearn</strong> (Pedregosa et al., 2011) provide ready-made functions that will easily split your dataset <span class="No-Break">for you:</span></p>
<pre class="source-code">
res = train_test_split(data, labels,    train_size=0.8,
    test_size=0.2,
    random_state=42,
    stratify=labels)</pre>
<p>However, there are times when the data scientist will not have enough data available to be able to warrant splitting it multiple ways – for example, there is no data relevant to the problem, or the process to collect the data is too difficult, expensive, or time-consuming. This is known<a id="_idIndexMarker102"/> as <strong class="bold">data scarcity</strong> and it can be responsible for poor model performance. In such cases, various solutions may help alleviate <span class="No-Break">the problem:</span></p>
<ul>
<li><strong class="bold">Augmentation</strong>: For example, taking<a id="_idIndexMarker103"/> an image and performing processing (for example, rotation, scaling, and modifying the colors) so that new instances are <span class="No-Break">slightly different</span></li>
<li><strong class="bold">Synthetic data</strong>: Data that is <a id="_idIndexMarker104"/>artificially generated using <span class="No-Break">computer programs</span></li>
</ul>
<p>To evaluate models where data is scarce, a technique known as k-fold cross-validation is used. This is discussed more fully in <a href="B18714_02.xhtml#_idTextAnchor061"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>, briefly the dataset is split into a number (<em class="italic">k</em>) of groups; then, in turn, each group is taken as the test dataset with the remaining groups as the training dataset, and the model is fit and evaluated. This is repeated for each group, hence each member of the original dataset is used in the test dataset exactly once and in a training dataset k-1 times. Finally, the model accuracy is calculated by using the results from the <span class="No-Break">individual evaluations.</span></p>
<p>This poses an interesting question about how much data is needed. There are no hard-and-fast rules but, generally<a id="_idIndexMarker105"/> speaking, the more the better. However, regardless of the amount of data, there are typically other issues that need to <span class="No-Break">be addressed:</span></p>
<ul>
<li><span class="No-Break">Missing values</span></li>
<li><span class="No-Break">Inconsistencies</span></li>
<li><span class="No-Break">Duplicate values</span></li>
<li><span class="No-Break">Ambiguity</span></li>
<li><span class="No-Break">Inaccuracies</span></li>
</ul>
<p>Machine learning is important. It has many real-world applications that can allow businesses and individuals to save time, money, and effort by, for example, automating business processes. Consider a customer service center where staff are required to take calls, answer queries, and help customers. In such a scenario, machine learning can be used to handle some of the more simple repetitive tasks, hence relieving burden from staff and getting things done more quickly <span class="No-Break">and efficiently.</span></p>
<p>Machine learning has dramatically altered the traditional ways of doing things over the past few years. However, in many aspects, it still lags far behind human levels of performance. Often, the best solutions are hybrid human-in-the-loop solutions where humans are needed to perform final verification of <span class="No-Break">the outcome.</span></p>
<p>There are several types of <span class="No-Break">machine learning:</span></p>
<ul>
<li><span class="No-Break">Supervised learning</span></li>
<li><span class="No-Break">Unsupervised learning</span></li>
<li><span class="No-Break">Semi-supervised learning</span></li>
<li><span class="No-Break">Reinforcement learning</span></li>
</ul>
<p>Supervised learning models must be <a id="_idIndexMarker106"/>trained with <strong class="bold">labeled</strong> data. Hence, both the inputs and the outputs of the model are specified. For example, a machine learning model could be trained with human-labeled images of apples and other fruits, labeled as <em class="italic">apple</em> and <em class="italic">non-apple</em>. This would allow the machine to learn the best way to identify pictures of apples. Supervised machine learning is the most common type of machine learning used today. In some ways, this matches how we humans function; we look and learn from experiences and then apply that knowledge in unknown, new situations to work out an answer. Technically speaking, there are two main types of supervised <span class="No-Break">learning problems:</span></p>
<ul>
<li><strong class="bold">Classification</strong>: Problems <a id="_idIndexMarker107"/>that involve predicting labels (for <span class="No-Break">example, </span><span class="No-Break"><em class="italic">apple</em></span><span class="No-Break">)</span></li>
<li><strong class="bold">Regression</strong>: Problems<a id="_idIndexMarker108"/> that involve predicting a numerical value (for example, a <span class="No-Break">house price)</span></li>
</ul>
<p>Both of these<a id="_idIndexMarker109"/> types of problems can have any number of inputs of any type. These problems are known<a id="_idIndexMarker110"/> as <strong class="bold">supervised</strong> from the idea that the output is supplied by a teacher that shows the system what <span class="No-Break">to do.</span></p>
<p>Unsupervised learning is a type of machine learning that, opposite to supervised learning, involves training<a id="_idIndexMarker111"/> algorithms on data that is <strong class="bold">unlabeled</strong>. Unsupervised<a id="_idIndexMarker112"/> algorithms examine datasets looking for meaningful patterns or<a id="_idIndexMarker113"/> trends that would not otherwise be apparent – that is, the target is for the algorithm to find the structure in the data on its own. For example, unsupervised machine learning algorithms can examine sales data and pinpoint the different types of products being purchased. However, the problem with this is that although these models can perform more complex tasks than their supervised counterparts, they are also much more unpredictable. Some use cases that adopt this approach are <span class="No-Break">as follows:</span></p>
<ul>
<li><strong class="bold">Dimensionality reduction</strong>: The process <a id="_idIndexMarker114"/>of reducing the number of inputs into a model by identifying the key (<em class="italic">principal</em>) components that capture the majority of the data without losing <span class="No-Break">key information.</span></li>
<li><strong class="bold">Association rules</strong>: The <a id="_idIndexMarker115"/>process of finding associations between different inputs in the input dataset by discovering the probabilities of the co-occurrence of items. For example, when people buy ice cream, they also typically <span class="No-Break">buy sunglasses.</span></li>
<li><strong class="bold">Clustering</strong>: Finds <a id="_idIndexMarker116"/>hidden patterns in a dataset based on similarities or differences and groups the data into clusters or groups. Unsupervised learning can be used to perform clustering when the exact details of the clusters <span class="No-Break">are unknown.</span></li>
</ul>
<p>Semi-supervised<a id="_idIndexMarker117"/> learning is, unsurprisingly, a combination of supervised and unsupervised learning. A small amount of labeled data and a large amount of unlabeled data is used. This has the benefits of both unsupervised and supervised learning but at the same time avoids the challenges of requiring large amounts of labeled data. Consequently, models can be trained to label data without requiring huge amounts of labeled <span class="No-Break">training data.</span></p>
<p>Reinforcement <a id="_idIndexMarker118"/>learning is about learning the best behavior so that the maximum reward is achieved. This behavior is learned by interacting with the environment and observing how it responds. In other words, the sequence of actions that maximize the reward must be independently discovered via a trial-and-error process. In this way, the model can learn the actions that result in success in an <span class="No-Break">unseen environment.</span></p>
<p>Briefly, here are the typical steps that are followed in a machine <span class="No-Break">learning project:</span></p>
<ol>
<li><strong class="bold">Data collection</strong>: Data can<a id="_idIndexMarker119"/> come from a database, Excel, or text file – essentially it can come <span class="No-Break">from anywhere.</span></li>
<li><strong class="bold">Data preparation</strong>: The quality <a id="_idIndexMarker120"/>of the data used is crucial. Hence, time must be spent fixing issues such as missing data and duplicates. Initial <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) is <a id="_idIndexMarker121"/>performed on the data to discover patterns, spot anomalies, and test theories about the data by using <span class="No-Break">visual techniques.</span></li>
<li><strong class="bold">Model training</strong>: An appropriate<a id="_idIndexMarker122"/> algorithm and model is chosen to represent the data. The data is split into training data for developing the model and test data for testing <span class="No-Break">the model.</span></li>
<li><strong class="bold">Evaluation</strong>: To test <a id="_idIndexMarker123"/>the accuracy, the test data <span class="No-Break">is used.</span></li>
<li><strong class="bold">Improve performance</strong>: Here, a<a id="_idIndexMarker124"/> different model may be chosen, or other inputs may <span class="No-Break">be used.</span></li>
</ol>
<p>Let’s start with the <span class="No-Break">technical requirements.</span></p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor045"/>Technical requirements</h2>
<p>This book describes<a id="_idIndexMarker125"/> a series of experiments with machine learning algorithms – some standard algorithms, some developed especially for this book. These algorithms, along with various worked examples, are available as Python programs at <a href="https://github.com/PacktPublishing/Machine-Learning-for-Emotion-Analysis/tree/main">https://github.com/PacktPublishing/Machine-Learning-for-Emotion-Analysis/tree/main</a>, split into directories corresponding to the chapters in which the specific algorithms will <span class="No-Break">be discussed.</span></p>
<p>One of the reasons why we implemented these programs in Python is that there is a huge amount of useful material to build upon. In particular, there are good -quality, efficient implementations of several standard machine learning algorithms, and using these helps us be confident that where an algorithm doesn’t work as well as expected on some dataset, it is because the algorithm isn’t very well suited to that dataset, rather than that we just haven’t implemented it very well. Some of the programs in the repository use very particular libraries, but there are several packages that we will use throughout this book. These are listed here. If you are going to use the code in the repository – which we hope you will because looking at what actual programs do is one of the best ways of learning – you will need to install these libraries. Most of them can be installed very easily, either by using the built-in package installer <strong class="source-inline">pip</strong> or by following the directions on the <span class="No-Break">relevant website:</span></p>
<ul>
<li><strong class="bold">pandas</strong>: This is one of<a id="_idIndexMarker126"/> the most commonly used libraries and is used primarily for cleaning and preparing data, as well as analyzing tabular data. It provides tools to explore, clean, manipulate, and analyze all types of structured data. Typically, machine learning libraries and projects use <strong class="source-inline">pandas</strong> structures as inputs. You can install it by typing the following command in the <span class="No-Break">command prompt:</span><pre class="source-code">
pip install pandas</pre></li> <li>Or you can go to <a href="https://pandas.pydata.org/docs/getting_started/install.xhtml">https://pandas.pydata.org/docs/getting_started/install.xhtml</a> for <span class="No-Break">other options.</span></li>
<li><strong class="bold">NumPy</strong>: This is used <a id="_idIndexMarker127"/>primarily for its support of <em class="italic">N</em>-dimensional arrays. It has functions for linear algebra and matrices and is also used by other libraries. Python provides several collection classes that can be used to represent arrays, notably as lists, but they are computationally slow to work with – NumPy provides objects that are up to 50 times faster than Python lists. To install it, run the following command in the <span class="No-Break">command prompt:</span><pre class="source-code">
pip install numpy</pre></li> </ul>
<p>Alternatively, you can refer to the documentation for more <span class="No-Break">options: </span><a href="https://numpy.org/install/"><span class="No-Break">https://numpy.org/install/</span></a><span class="No-Break">.</span></p>
<ul>
<li><strong class="bold">SciPy</strong>: This provides a<a id="_idIndexMarker128"/> range of scientific functions built on top of NumPy, including ways of representing sparse arrays (arrays where most elements are 0) that can be <a id="_idIndexMarker129"/>manipulated thousands of times faster than standard NumPy arrays if the vast majority of elements are 0. You can install it using the <span class="No-Break">following command:</span><pre class="source-code">
pi<a id="_idTextAnchor046"/>p install scipy</pre></li> </ul>
<p>You can also refer to the SciPy documentation for more <span class="No-Break">details: </span><a href="https://scipy.org/install/"><span class="No-Break">https://scipy.org/install/</span></a><span class="No-Break">.</span></p>
<ul>
<li><strong class="bold">scikit-learn (Pedregosa et al., 2011)</strong>: This is <a id="_idIndexMarker130"/>used to build machine learning models as it has functions for building supervised and unsupervised machine learning models, analysis, and dimensionality reduction. A large part of this book is about investigating how well various standard machine learning algorithms work on particular datasets, and it is useful to have reliable good-quality implementations of the most widely used algorithms so that we are not distracted by issues due to the way we have <span class="No-Break">implemented them.</span></li>
</ul>
<p>scikit-learn is also known<a id="_idTextAnchor047"/> as <strong class="source-inline">sklearn</strong> – when you want to import it into a program, you should refer to it as sklearn. You can install it <span class="No-Break">as follows:</span></p>
<pre class="source-code">
pip install scikit-learn</pre> <p>Refer to the documentation for more <span class="No-Break">information: </span><a href="https://scikit-learn.org/stable/install.xhtml"><span class="No-Break">https://scikit-learn.org/stable/install.xhtml</span></a><span class="No-Break">.</span></p>
<p>The <strong class="source-inline">sklearn</strong> implementations of the various algorithms generally make the internal representations of the data available to other programs. This can be particularly <a id="_idIndexMarker131"/>valuable when you are trying to understand the behavior of some algorithm on a given dataset and is something we will use extensively as we carry out <span class="No-Break">our experiments.</span></p>
<ul>
<li><strong class="bold">TensorFlow</strong>: This is a popular library for building neural networks as well as performing other<a id="_idIndexMarker132"/> tasks. It uses <em class="italic">tensors</em> (multi-dimensional arrays) to perform operations. It is built to take advantage of parallelism, so it is used to train neural networks in a highly efficient manner. Again, it makes sense to reuse a reliable good-quality implementation when testing neural network models on our data so that we know that any poor performances arise because of problems with the algorithm rather than with our implementation of it. As ever, you can just install it <a id="_idIndexMarker133"/><span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">:</span><pre class="source-code">
pip install tensorflow</pre></li> </ul>
<p>For more information, refer to the TensorFlow <span class="No-Break">documentation: </span><a href="https://www.tensorflow.org/install"><span class="No-Break">https://www.tensorflow.org/install</span></a><span class="No-Break">.</span></p>
<p>You will not benefit from its use of parallelism unless you have a GPU or other hardware accelerator built into your machine, and training complex models is likely to be intolerably slow. We will consider how to use remote facilities such as Google Colab to obtain better performance in <a href="B18714_09.xhtml#_idTextAnchor172"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Exploring</em> <em class="italic">Transformers</em>. For now, just be aware that running <strong class="source-inline">tensorflow</strong> on a standard computer without any kind of hardware<a id="_idIndexMarker134"/> accelerator probably won’t do anything within a <span class="No-Break">reasonable period.</span></p>
<ul>
<li><strong class="bold">Keras</strong>: This is also <a id="_idIndexMarker135"/>used for building neural networks. It is built on top of TensorFlow. It creates computational graphs to represent machine learning algorithms, so it is slow compared to other libraries. Keras comes as part of TensorFlow, so there is no need to install anything beyond <span class="No-Break">TensorFlow itself.</span></li>
<li><strong class="bold">Matplotlib</strong>: This is an<a id="_idIndexMarker136"/> interactive library for plotting graphs, charts, plots, and visualizing data. It comes with a wide range of plots that help data scientists understand trends and patterns. Matplotlib is extremely powerful and allows users to create almost any visualization imaginable. Use the following command to <span class="No-Break">install </span><span class="No-Break"><strong class="source-inline">matplotlib</strong></span><span class="No-Break">:</span><pre class="source-code">
pip install matplotlib</pre></li> </ul>
<p>Refer to the documentation for more <span class="No-Break">information: </span><a href="https://matplotlib.org/stable/users/installing/index.xhtml"><span class="No-Break">https://matplotlib.org/stable/users/installing/index.xhtml</span></a><span class="No-Break">.</span></p>
<p>Matplotlib may install NumPy if you do not have it already installed, but it is more sensible to install them separately (<span class="No-Break">NumPy first).</span></p>
<ul>
<li><strong class="bold">Seaborn</strong>: This is built on <a id="_idIndexMarker137"/>the top of Matplotlib, and is another library for creating visualizations. It is useful for making attractive plots and helps users explore and understand data. Seaborn makes it easy for users to switch between different visualizations. You can easily install Seaborn by running the <span class="No-Break">following command:</span><pre class="source-code">
pip install seaborn</pre></li> </ul>
<p>For more installation options, please refer <span class="No-Break">to </span><a href="https://seaborn.pydata.org/installing.xhtml"><span class="No-Break">https://seaborn.pydata.org/installing.xhtml</span></a><span class="No-Break">.</span></p>
<p>We will use these <a id="_idIndexMarker138"/>libraries throughout this book, so we advise you to install them now, before trying out any of the programs and examples that we’ll discuss as we go along. You only have to install them once so that they will be available whenever you need them. We will specify any other libraries that the examples depend on as we go along, but from now on, we will assume that you have at least <span class="No-Break">these ones.</span></p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor048"/>A sample project</h2>
<p>The best way to <a id="_idIndexMarker139"/>learn is by doing! In this section, we will discover how to complete a small machine learning project in Python. Completing, and understanding, this project will allow you to become familiar with machine learning concepts <span class="No-Break">and techniques.</span></p>
<p>Typically, the first step in developing any Python program is to import the modules that are going to be needed using the <span class="No-Break"><strong class="source-inline">import</strong></span><span class="No-Break"> statement:</span></p>
<pre class="source-code">
import sklearnimport pandas as pd</pre>
<p class="callout-heading">Note</p>
<p class="callout">Other imports are needed for this exercise; these can be found in the <span class="No-Break">GitHub repository.</span></p>
<p>The next step is to load the data that is needed to build the model. Like most tutorials, we will use the famous Iris dataset. The Iris dataset contains data on the length and width of sepals and petals. We will use <strong class="source-inline">pandas</strong> to load the dataset. The dataset can be downloaded from the internet and read from your local filesystem, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
df = pd.read_csv("c:\iris.csv")</pre> <p>Alternatively, <strong class="source-inline">pandas</strong> can read it directly from <span class="No-Break">a URL:</span></p>
<pre class="source-code">
df = pd.read_csv("https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv")</pre> <p>The <strong class="source-inline">read_csv</strong> command returns a DataFrame. It is probably the most commonly used <strong class="source-inline">pandas</strong> object and is simply a two-dimensional data structure with rows and columns, just like <span class="No-Break">a spreadsheet.</span></p>
<p>Since we will be using <strong class="source-inline">sklearn</strong>, it is interesting to see that <strong class="source-inline">sklearn</strong> also makes it easy to access <span class="No-Break">the dataset:</span></p>
<pre class="source-code">
from sklearn import datasetsiris = datasets.load_iris()
df = iris.data</pre>
<p>We can now check that<a id="_idIndexMarker140"/> the dataset has been successfully loaded by using the <span class="No-Break"><strong class="source-inline">describe</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
df.describe()</pre> <p>The <strong class="source-inline">describe</strong> function returns a descriptive summary of a DataFrame reporting values such as the mean, count, and <span class="No-Break">standard deviation:</span></p>
<pre class="source-code">
sepal.length sepal.width petal.length petal.widthcount 150.000000 150.000000 150.000000 150.000000
mean 5.843333 3.057333 3.758000 1.199333
std 0.828066 0.435866 1.765298 0.762238
min 4.300000 2.000000 1.000000 0.100000
25% 5.100000 2.800000 1.600000 0.300000
50% 5.800000 3.000000 4.350000 1.300000
75% 6.400000 3.300000 5.100000 1.800000
max 7.900000 4.400000 6.900000 2.500000</pre>
<p>This function is useful to check that the data has been loaded correctly but also to provide a first glance at some interesting attributes of <span class="No-Break">the data.</span></p>
<p>Some other useful commands tell us more about <span class="No-Break">the DataFrame:</span></p>
<ul>
<li>This shows the first five elements in <span class="No-Break">the DataFrame:</span><pre class="source-code">
df.head(5)</pre></li> <li>This shows the last five elements in <span class="No-Break">the DataFrame:</span><pre class="source-code">
df.tail(5)</pre></li> <li>This describes the columns of <span class="No-Break">the DataFrame:</span><pre class="source-code">
df.columns</pre></li> <li>This describes the number of rows and columns in <span class="No-Break">the DataFrame:</span><pre class="source-code">
df.shape</pre></li> </ul>
<p>It is usually a good idea to use these functions to check that the dataset has been successfully and correctly loaded into the DataFrame and that everything looks as <span class="No-Break">it should.</span></p>
<p>It is also important <a id="_idIndexMarker141"/>to ensure that the dataset is balanced – that is, there are relatively equal numbers of <span class="No-Break">each class.</span></p>
<p>The majority of machine learning algorithms have been developed with the assumption that there are equal numbers of instances of each class. Consequently, imbalanced datasets present a big problem for machine learning models as this results in models with poor <span class="No-Break">predictive performance.</span></p>
<p>In the Iris example, this means that we have to check that we have equal numbers of each type of flower. This can be verified by running the <span class="No-Break">following command:</span></p>
<pre class="source-code">
df['variety'].value_counts()</pre> <p>This prints the <span class="No-Break">following output:</span></p>
<pre class="source-code">
Setosa 50Versicolor 50
Virginica 50
Name: variety, dtype: int64</pre>
<p>We can see that there are 50 examples of each variety. The next step is to create some visualizations. Although we used the <strong class="source-inline">describe</strong> function to get an idea of the statistical properties of the dataset, it is much easier to observe these in a visual form as opposed to in <span class="No-Break">a table.</span></p>
<p>Box plots (see <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.9</em>) plot the distribution of data based on the sample minimum, the lower quartile, the median, the upper quartile, and the sample maximum. This helps us analyze the data to<a id="_idIndexMarker142"/> establish any outliers and the data variation to better understand <span class="No-Break">each attribute:</span></p>
<pre class="source-code">
import matplotlib.pyplot as pltattributes = df[['sepal.length', 'sepal.width',
    'petal.length', 'petal.width']]
attributes.boxplot()
plt.show()</pre>
<p>This outputs the <span class="No-Break">following plot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 1.9 – Box plot" height="805" src="image/B18714_01_09.jpg" width="1048"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – Box plot</p>
<p>Heatmaps are <a id="_idIndexMarker143"/>useful for understanding the relationships between attributes. Heatmaps are an important tool for data scientists to explore and visualize data. They represent the data in a two-dimensional format and allow the data to be summarized visually as a colored graph. Although we can use <strong class="source-inline">matplotlib</strong> to create heatmaps, it is much easier in <strong class="source-inline">seaborn</strong> and requires significantly fewer lines of code – something <span class="No-Break">we like!</span></p>
<pre class="source-code">
import seaborn as snssns.heatmap(iris.corr(), annot=True)
plt.show()</pre>
<p>This outputs the <span class="No-Break">following heatmap:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 1.10 – Heatmap" height="819" src="image/B18714_01_10.jpg" width="1041"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – Heatmap</p>
<p>The squares in the <a id="_idIndexMarker144"/>heatmap represent the correlation (a measure that shows how much two variables are related) between the variables. The correlation values range from -1 <span class="No-Break">to +1:</span></p>
<ul>
<li>The closer the value is to 1, the more positively correlated they are – that is, as one increases, so does <span class="No-Break">the other</span></li>
<li>Conversely, the closer the value is to -1, the more negatively correlated they are – that is, as one variable decreases, the other <span class="No-Break">will increase</span></li>
<li>Values close to 0 indicate that there is no linear trend between <span class="No-Break">the variables</span></li>
</ul>
<p>In <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.10</em>, the diagonals are all 1. This is because, in those squares, the two variables are the same and hence the correlation is to itself. For the remainder, the scale shows that the lighter the color (toward the top of the scale), the higher the correlation. For example, the petal length and petal width are highly correlated, whereas petal length and sepal width are not. Finally, it can also be seen that the plot is symmetrical on both sides of the diagonal. This is because the same set of variables are paired in the squares that are <span class="No-Break">the same.</span></p>
<p>We can now build a model using the data and estimate the accuracy of the model on data that it has not seen previously. Let’s start by separating the data and the labels from each other by <span class="No-Break">using Python:</span></p>
<pre class="source-code">
data = df.iloc[:, 0:4]labels = df.iloc[:, 4]</pre>
<p>Before we can train a machine learning model, it is necessary to split the data and labels into testing and training data. As discussed previously, we can use the <strong class="source-inline">train_test_split</strong> function <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">sklearn</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
from sklearn.model_selection import train_test_splitX_train,X_test,y_train,y_test = train_test_split(data,
    labels, test_size=0.2)</pre>
<p>The capital <strong class="source-inline">X</strong> and lowercase <strong class="source-inline">y</strong> are a nod to math notation, where it is common practice to write matrix variable names in uppercase and vector variable names using lowercase letters. This has no special Python function and these conventions can be ignored if desired. For now, note that <strong class="source-inline">X</strong> refers to data, and <strong class="source-inline">y</strong> refers to the associated labels. Hence, <strong class="source-inline">X_train</strong> can be understood to refer to an object that contains the <span class="No-Break">training data.</span></p>
<p>Before we can begin<a id="_idIndexMarker145"/> to work on the machine learning model, we must <em class="italic">normalize</em> the data. Normalization is a scaling technique that updates the numeric columns to use a common scale. This helps improve the performance, reliability, and accuracy of the model. The two most common normalization techniques are min-max scaling and <span class="No-Break">standardization scaling:</span></p>
<ul>
<li><strong class="bold">Min-max scaling</strong>: This method<a id="_idIndexMarker146"/> uses the minimum and maximum<a id="_idIndexMarker147"/> values for scaling and rescales the values so that they end up in the range 0 to 1 or -1 to 1. It is most useful when the features are of different scales. It is typically used when the feature distribution is unknown, such as in k-NN or neural <span class="No-Break">network models.</span></li>
<li><strong class="bold">Standardization scaling</strong>: This <a id="_idIndexMarker148"/>method uses the mean and the standard deviation to rescale values so that they have a mean of 0 and a variance of 1. The resultant scaled values are not confined to a <a id="_idIndexMarker149"/>specific range. It is typically used when the feature distribution <span class="No-Break">is normal.</span></li>
</ul>
<p>It is uncommon to come across datasets that perfectly follow a certain specific distribution. Typically, every dataset will need to be standardized. For the Iris dataset, we will use sklearn’s <strong class="source-inline">StandardScaler</strong> to scale the data by making the mean of the data 0 and the standard <span class="No-Break">deviation 1:</span></p>
<pre class="source-code">
from sklearn.preprocessing import StandardScalerfrom sklearn.model_selection import cross_val_score
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)</pre>
<p>Now that the data is <a id="_idIndexMarker150"/>ready, <strong class="source-inline">sklearn</strong> makes it easy for us to test and compare various machine learning models. A brief explanation of each model has been provided but don’t worry – we explain these models in more detail later in <span class="No-Break">later chapters.</span></p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor049"/>Logistic regression</h2>
<p><strong class="bold">Logistic regression</strong> is one of the<a id="_idIndexMarker151"/> most popular machine<a id="_idIndexMarker152"/> learning techniques. It is used to predict a categorical dependent variable using a set of independent variables and makes use of a <em class="italic">sigmoid</em> function. The <a id="_idIndexMarker153"/>sigmoid is a mathematical function that has values from 0 to 1 and asymptotes both values. This makes it useful for binary classification with 0 and 1 as potential <span class="No-Break">output values:</span></p>
<pre class="source-code">
from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()
lr.fit(X_train, y_train)
score = lr.score(X_train, y_train)
print(f"Training data accuracy {score}")
score = lr.score(X_test, y_test)
print(f"Testing data accuracy {score}")
<strong class="bold">Training data accuracy 0.9666666666666667</strong>
<strong class="bold">Testing data accuracy 0.9666666666666667</strong></pre>
<p class="callout-heading">Note</p>
<p class="callout">There is also a technique called linear regression but, as its name suggests, this is used for regression problems, whereas the current Iris problem is a <span class="No-Break">classification problem.</span></p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor050"/>Support vector machines (SVMs)</h2>
<p><strong class="bold">Support vector machine</strong> (<strong class="bold">SVM</strong>) is one <a id="_idIndexMarker154"/>of the<a id="_idIndexMarker155"/> best “out-of-the-box” classification techniques. SVM constructs a hyperplane that can then be used for classification. It works by calculating the distance between two observations and then determining a hyperplane that maximizes the distance between the closest members of separate classes. The linear <strong class="bold">support vector classifier</strong> (<strong class="bold">SVC</strong>) method (as <a id="_idIndexMarker156"/>used in the following example) applies a linear kernel function to <span class="No-Break">perform classification:</span></p>
<pre class="source-code">
from sklearn.svm import SVCsvm = SVC(random_state=0, gamma='auto', C=1.0)
svm.fit(X_train, y_train)
score = svm.score(X_train, y_train)
print(f"Training data accuracy {score}")
score = svm.score(X_test, y_test)
print(f"Testing data accuracy {score}")
<strong class="bold">data accuracy 0.9666666666666667</strong>
<strong class="bold">Testing data accuracy 0.9666666666666667</strong></pre>
<p>The following parameters <span class="No-Break">are used:</span></p>
<ul>
<li><strong class="source-inline">random_state</strong>: This controls the random number generation that is used to shuffle the data. In this example, a value hasn’t been set, hence a randomly initialized state is used. This means that results will vary <span class="No-Break">between runs.</span></li>
<li><strong class="source-inline">gamma</strong>: This controls how much influence a single data point has on the decision boundary. Low values mean “far” and high values mean “close.” In this example, gamma is set to “auto,” hence allowing it to automatically define its own value based on the characteristics of <span class="No-Break">the dataset.</span></li>
<li><strong class="source-inline">C</strong>: This controls<a id="_idIndexMarker157"/> the trade-off between maximizing<a id="_idIndexMarker158"/> the distance between classes and correctly classifying <span class="No-Break">the data.</span></li>
</ul>
<h2 id="_idParaDest-31"><a id="_idTextAnchor051"/>K-nearest neighbors (k-NN)</h2>
<p><strong class="bold">k-NN</strong> is another <a id="_idIndexMarker159"/>widely used classification <a id="_idIndexMarker160"/>technique. k-NN classifies objects based on the closest training examples in the feature space. It is a simple algorithm that stores all available cases and classifies new cases by a majority vote of its <em class="italic">k</em> neighbors. The case being assigned to the class is the most common among its k-NNs measured by a <span class="No-Break">distance function:</span></p>
<pre class="source-code">
from sklearn.neighbors import KNeighborsClassifierknn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train,y_train)
score = knn.score(X_train, y_train)
print(f"Training data accuracy {score}")
score = knn.score(X_test, y_test)
print(f"Testing data accuracy {score}")
<strong class="bold">Training data accuracy 0.9583333333333334</strong>
<strong class="bold">Testing data accuracy 0.9333333333333333</strong></pre>
<h2 id="_idParaDest-32"><a id="_idTextAnchor052"/>Decision trees</h2>
<p><strong class="bold">Decision trees</strong> attempt to <a id="_idIndexMarker161"/>create a tree-like model that predicts <a id="_idIndexMarker162"/>the value of a variable by learning simple decision rules that are inferred from the data features. Decision trees classify examples by sorting down the tree from the root to a leaf node, with the leaf node providing the classification for <span class="No-Break">our example:</span></p>
<pre class="source-code">
from sklearn import treedt = tree.DecisionTreeClassifier()
dt.fit(X_train, y_train)
score = dt.score(X_train, y_train)
print(f"Training data accuracy {score}")
score = dt.score(X_test, y_test)
print(f"Testing data accuracy {score}")
<strong class="bold">Training data accuracy 1.0</strong>
<strong class="bold">Testing data accuracy 0.9333333333333333</strong></pre>
<h2 id="_idParaDest-33"><a id="_idTextAnchor053"/>Random forest</h2>
<p><strong class="bold">Random forest</strong> builds decision <a id="_idIndexMarker163"/>trees using different samples and <a id="_idIndexMarker164"/>then takes the majority vote as the answer. In other words, random forest builds multiple decision trees and then merges them to get a more accurate prediction. Due to its simplicity, it is also one of the most commonly <span class="No-Break">used algorithms:</span></p>
<pre class="source-code">
from sklearn.ensemble import RandomForestClassifierrf = RandomForestClassifier()
rf.fit(X_train, y_train)
score = rf.score(X_train, y_train)
print(f"Training data accuracy {score}")
score = rf.score(X_test, y_test)
print(f"Testing data accuracy {score}")
<strong class="bold">Training data accuracy 1.0</strong>
<strong class="bold">Testing data accuracy 0.9666666666666667</strong></pre>
<h2 id="_idParaDest-34"><a id="_idTextAnchor054"/>Neural networks</h2>
<p><strong class="bold">Neural networks</strong> (also referred to<a id="_idIndexMarker165"/> as deep learning) are <a id="_idIndexMarker166"/>algorithms that are inspired by how the human brain works, and are designed to recognize numerical patterns. Neural networks consist of input and output <em class="italic">layers</em> and (optionally) hidden layers. These layers contain units (<em class="italic">neurons</em>) that transform the inputs into something useful for the output layer. These neurons are connected and work together. We will look at these in more detail later in <span class="No-Break">this book.</span></p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor055"/>Making predictions</h2>
<p>Once we have<a id="_idIndexMarker167"/> chosen and fit a machine learning model, it can easily be used to make predictions on new, unseen data – that is, take the final model and one or more data instances and then predict the classes for each of the data instances. The model is needed because the result classes are not known for the new data. The class for the unseen data can be predicted using scikit-learn’s <span class="No-Break"><strong class="source-inline">predict()</strong></span><span class="No-Break"> function.</span></p>
<p>First, the unseen data must be transformed into a pandas DataFrame, along with the <span class="No-Break">column names:</span></p>
<pre class="source-code">
df_predict = pd.DataFrame([[5.9, 3.0, 5.1, 1.8]],    columns = ['sepal.length', 'sepal.width',
    'petal.length', 'petal.width'])</pre>
<p>This DataFrame can then be passed to scikit-learn’s <strong class="source-inline">predict()</strong> function to predict the <span class="No-Break">class value:</span></p>
<pre class="source-code">
print (dt.predict(df_predict))<strong class="bold">['Virginica']</strong></pre>
<h2 id="_idParaDest-36"><a id="_idTextAnchor056"/>A sample text classification problem</h2>
<p>Given that this is a <a id="_idIndexMarker168"/>book on emotion classification and emotions are generally expressed in written form, it makes sense to take a look at how a text classification problem <span class="No-Break">is tackled.</span></p>
<p>We have all received spam emails. These are typically emails that are sent to huge numbers of email addresses, usually for marketing or phishing purposes. Often, these emails are sent by bots. They are of no interest to the recipients and have not been requested by them. Consequently, email servers will often automatically detect and remove these messages by looking for recognizable phrases and patterns, and sometimes placing them into special folders labeled <em class="italic">Junk</em> <span class="No-Break">or </span><span class="No-Break"><em class="italic">Spam</em></span><span class="No-Break">.</span></p>
<p>In this example, we will build a spam detector and use the machine learning abilities of scikit-learn to train the spam detector to detect and classify text as spam and non-spam. There are many labeled datasets available online (for example, from Kaggle); we chose to use the dataset <span class="No-Break">from </span><a href="https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download"><span class="No-Break">https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?resource=download</span></a><span class="No-Break">.</span></p>
<p>The dataset contains SMS messages that have been collected for spam research. It contains 5,574 SMS messages in English that are labeled as spam or non-spam (ham). The file contains one message per line, and each line has two columns; the label and the <span class="No-Break">message text.</span></p>
<p>We have seen some of the basic <strong class="source-inline">pandas</strong> commands already, so let’s load the file and split it into training and test sets, as we <span class="No-Break">did previously:</span></p>
<pre class="source-code">
import pandas as pdfrom sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
spam = pd.read_csv("spam.csv", encoding_errors="ignore")
labels = spam["v1"]
data = spam["v2"]
X_train,X_test,y_train,y_test = train_test_split(data,
    labels, test_size = 0.2)</pre>
<p class="callout-heading">Note</p>
<p class="callout">The file may have an encoding error; for now, we will ignore this as it is not relevant to the task <span class="No-Break">at hand.</span></p>
<p>A handy <a id="_idIndexMarker169"/>function called <strong class="source-inline">CountVectorizer</strong> is available in <strong class="source-inline">sklearn</strong>. This can be used to transform text into a vector of term-token counts. It is also able to preprocess the text data before generating the vector representations, hence it is an extremely useful function. <strong class="source-inline">CountVectorizer</strong> converts the raw text into a numerical vector representation, which makes it easy to use the text as inputs in machine <span class="No-Break">learning tasks:</span></p>
<pre class="source-code">
count_vectorizer = CountVectorizer()X_train_features = count_vectorizer.fit_transform(X_train)</pre>
<p>Essentially, it assigns a number, randomly, to each word and then counts the number of occurrences of each. For example, consider the <span class="No-Break">following sentence:</span></p>
<p><em class="italic">The quick brown fox jumps over the </em><span class="No-Break"><em class="italic">lazy dog.</em></span></p>
<p>This would be converted <span class="No-Break">as follows:</span></p>
<table class="T---Table" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">word</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">the</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">quick</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">brown</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">fox</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">jumps</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">over</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">lazy</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">dog</strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">index</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p>5</p>
</td>
<td class="T---Table T---Body T---Body">
<p>6</p>
</td>
<td class="T---Table T---Body T---Body">
<p>7</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">count</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p>Notice that there are eight unique words, hence eight columns. Each column represents a unique word in the vocabulary. Each count row represents the item or row in the dataset. The values in the cells are the word counts. Armed with this knowledge about the types and counts of common words that appear in spam, the model will be able to classify text as spam <span class="No-Break">or non-spam.</span></p>
<p>We will use the simple k-NN model <span class="No-Break">introduced earlier:</span></p>
<pre class="source-code">
knn = KNeighborsClassifier(n_neighbors = 5)</pre> <p>The <strong class="source-inline">fit()</strong> function, as we have seen earlier, trains the model with the vectorized counts from the training data and the training labels. It compares its predictions against the real answers in <strong class="source-inline">y_train</strong> and then tunes its hyperparameters until it achieves the best possible <a id="_idIndexMarker170"/>accuracy. Note how here, since this is a classification task, the labels must also be passed to the <strong class="source-inline">fit()</strong> function. The Iris example earlier was a regression task; there were no labels, so we did not pass them into the <span class="No-Break"><strong class="source-inline">fit()</strong></span><span class="No-Break"> function:</span></p>
<pre class="source-code">
knn.fit(X_train_features, y_train)</pre> <p>We now have a model that we can use on the test data to test <span class="No-Break">for accuracy:</span></p>
<pre class="source-code">
X_test_features = count_vectorizer.transform(X_test)score = knn.score(X_test_features, y_test)
print(f"Training data accuracy {score}")
<strong class="bold">Training data accuracy 0.9255605381165919</strong></pre>
<p>Note how this time, we use <strong class="source-inline">transform()</strong> instead of <strong class="source-inline">fit_transform()</strong>. The difference is subtle but important. The <strong class="source-inline">fit_transform()</strong> function does <strong class="source-inline">fit()</strong>, followed by <strong class="source-inline">transform()</strong> – that is, it calculates the initial parameters, uses these calculated values to modify the training data, and generates a term-count matrix. This is needed when a model is being trained. The <strong class="source-inline">transform()</strong> method, on the other hand, only generates and returns the term-count matrix. The <strong class="source-inline">score()</strong> function then scores the prediction of the<a id="_idIndexMarker171"/> test data term-count matrix against the actual labels in test data labels, <strong class="source-inline">y_test</strong>, and even using a simplistic model we can classify spam with high accuracy and obtain <span class="No-Break">reasonable results.</span></p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor057"/>Summary</h1>
<p>In this chapter, we started by examining emotion and sentiment, and their origins. Emotion is not the same as sentiment; emotion is more fine-grained and is much harder to quantify and work with. Hence, we learned about the three main theories of emotion, with psychologists, neurologists, and cognitive scientists each having slightly different views as to how emotions are formed. We explored the approaches of Ekman and Plutchik, and how the categorical and dimensional models are <span class="No-Break">laid out.</span></p>
<p>We also examined the reasons why emotion analysis is important but difficult due to the nuances and difficulty of working with content written in natural language, particularly the kind of informal language we are concerned with in this book. We looked at the basic issues in NLP and will return to the most relevant aspects of NLP in <a href="B18714_04.xhtml#_idTextAnchor093"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, <em class="italic">Preprocessing – Stemming, Tagging, and Parsing</em>. Finally, we introduced machine learning and worked through some <span class="No-Break">sample projects.</span></p>
<p>In the next chapter, we will explore where to find suitable data, the steps needed to make it fit for purpose, and how to construct a dataset suitable for <span class="No-Break">emotion analysis.</span></p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor058"/>References</h1>
<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources<a id="_idTextAnchor059"/>:</span></p>
<ul>
<li>Alabbas, M., &amp; Ramsay, A. M. (2013). <em class="italic">Natural language inference for Arabic using extendedtree edit distance with subtrees</em>. Journal of Artificial Intelligence Research, <span class="No-Break">48, 1–22.</span></li>
<li>Dozat, T., Qi, P., &amp; Manning, C. D. (2017). <em class="italic">Stanford’s Graph-based Neural Dependency Parser at the CoNLL 2017 Shared Task</em>. Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, <span class="No-Break">20–30. </span><a href="https://doi.org/10.18653/v1/K17-3002"><span class="No-Break">https://doi.org/10.18653/v1/K17-3002</span></a><span class="No-Break">.</span></li>
<li>Ekman, P. (1993). <em class="italic">Facial expression and emotion</em>. <em class="italic">American Psychologist</em>, <span class="No-Break"><em class="italic">48(4)</em></span><span class="No-Break">, 384.</span></li>
<li>Kitaev, N., &amp; Klein, D. (2018). <em class="italic">Constituency Parsing with a Self-Attentive Encoder</em>. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), <span class="No-Break">2,676–2,686. </span><a href="https://doi.org/10.18653/v1/P18-1249"><span class="No-Break">https://doi.org/10.18653/v1/P18-1249</span></a><span class="No-Break">.</span></li>
<li>Lewis, M., &amp; Steedman, M. (2014). <em class="italic">A* CCG Parsing with a Supertag-factored Model</em>. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), <span class="No-Break">990–1,000. </span><a href="https://doi.org/10.3115/v1/D14-1107"><span class="No-Break">https://doi.org/10.3115/v1/D14-1107</span></a><span class="No-Break">.</span></li>
<li>MacCartney, B., &amp; Manning, C. D. (2014). <em class="italic">Natural logic and natural language inference</em>. In Computing Meaning (pp. <span class="No-Break">129–147). Springer.</span></li>
<li>McDonald, R., Pereira, F., Ribarov, K., &amp; Hajič, J. (2005). <em class="italic">Non-Projective Dependency Parsing using Spanning Tree Algorithms</em>. Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, <span class="No-Break">523–530. </span><a href="https://aclanthology.org/H05-1066"><span class="No-Break">https://aclanthology.org/H05-1066</span></a><span class="No-Break">.</span></li>
<li>Nivre, J., Hall, J., &amp; Nilsson, J. (2006). MaltParser: <em class="italic">A data-driven parser-generator for dependency parsing</em>. Proceedings of the International Conference on Language Resources and Evaluation (LREC), <span class="No-Break">6, 2,216–2,219.</span></li>
<li>Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp; Duchesnay, E. (2011). <em class="italic">Scikit-learn: Machine Learning in Python</em>. Journal of Machine Learning Research, <span class="No-Break">12, 2,825–2,830.</span></li>
<li>Plutchik, R. (2001). <em class="italic">The Nature of Emotions: Human emotions have deep evolutionary roots, a fact that may explain their complexity and provide tools for clinical practice</em>. <em class="italic">American Scientist</em>, <span class="No-Break">89(4), 344–350.</span></li>
<li>Russell, J. A. (1980). <em class="italic">A circumplex model of affect</em>. Journal of Personality and Social Psychology, 39(6), <span class="No-Break">1,161–1,178. </span><a href="https://doi.org/10.1037/h0077714"><span class="No-Break">https://doi.org/10.1037/h0077714</span></a><span class="No-Break">.</span></li>
</ul>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer025">
<h1 id="_idParaDest-39" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor060"/>Part 2:Building and Using a Dataset</h1>
<p>The process of collecting data (e.g., tweets and news articles) is described in this part, followed by the preprocessing steps that are required to get good results to create <span class="No-Break">a corpus.</span></p>
<p>This part has the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B18714_02.xhtml#_idTextAnchor061"><em class="italic">Chapter 2</em></a>, <em class="italic">Building and Using a Dataset</em></li>
<li><a href="B18714_03.xhtml#_idTextAnchor077"><em class="italic">Chapter 3</em></a>, <em class="italic">Labeling Data</em></li>
<li><a href="B18714_04.xhtml#_idTextAnchor093"><em class="italic">Chapter 4</em></a>, <em class="italic">Preprocessing – Stemming, Tagging, and Parsing</em></li>
</ul>
</div>
<div>
<div id="_idContainer026">
</div>
</div>
<div>
<div id="_idContainer027">
</div>
</div>
<div>
<div id="_idContainer028">
</div>
</div>
<div>
<div id="_idContainer029">
</div>
</div>
<div>
<div id="_idContainer030">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer031">
</div>
</div>
<div>
<div id="_idContainer032">
</div>
</div>
<div>
<div id="_idContainer033">
</div>
</div>
</div></body></html>