- en: Chapter 5. Transforming Images with Morphological Operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Eroding and dilating images using morphological filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opening and closing images using morphological filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting edges and corners using morphological filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmenting images using watersheds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting distinctive regions using MSER
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting foreground objects with the GrabCut algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Mathematical morphology** is a theory that was developed in the 1960s for
    the analysis and processing of discrete images. It defines a series of operators
    that transform an image by probing it with a predefined shape element. The way
    this shape element intersects the neighborhood of a pixel determines the result
    of the operation. This chapter presents the most important morphological operators.
    It also explores the problems of image segmentation and feature detection using
    algorithms based on morphological operators.'
  prefs: []
  type: TYPE_NORMAL
- en: Eroding and dilating images using morphological filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Erosion and dilation are the most fundamental morphological operators. Therefore,
    we will present these in the first recipe. The fundamental component in mathematical
    morphology is the **structuring element**. A structuring element can be simply
    defined as a configuration of pixels (the square shape in the following figure)
    on which an origin is defined (also called an **anchor point**). Applying a morphological
    filter consists of probing each pixel of the image using this structuring element.
    When the origin of the structuring element is aligned with a given pixel, its
    intersection with the image defines a set of pixels on which a particular morphological
    operation is applied (the nine shaded pixels in the following figure). In principle,
    the structuring element can be of any shape, but most often, a simple shape such
    as a square, circle, or diamond with the origin at the center is used (mainly
    for efficiency reasons), as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Eroding and dilating images using morphological filters](img/00050.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As morphological filters often work on binary images, we will use the binary
    image that was created through thresholding in the first recipe of the previous
    chapter. However, since the convention is to have the foreground objects represented
    by high (white) pixel values and the background objects by low (black) pixel values
    in morphology, we have negated the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In morphological terms, the following image is said to be the complement of
    the image that was created in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/00051.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Erosion and dilation are implemented in OpenCV as simple functions, which are
    `cv::erode` and `cv::dilate`. Their usage is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The two images produced by these function calls are seen in the following screenshots.
    The first screenshot shows erosion:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The second screenshot shows the dilation result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As with all the other morphological filters, the two filters of this recipe
    operate on the set of pixels (or the neighborhood) around each pixel as defined
    by the structuring element. Recall that when applied to a given pixel, the anchor
    point of the structuring element is aligned with this pixel location, and all
    the pixels that intersect the structuring element are included in the current
    set. **Erosion** replaces the current pixel with the minimum pixel value found
    in the defined pixel set. **Dilation** is the complementary operator, and it replaces
    the current pixel with the maximum pixel value found in the defined pixel set.
    Since the input binary image contains only black (`0`) and white (`255`) pixels,
    each pixel is replaced by either a white or black pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 'A good way to picturize the effect of these two operators is to think in terms
    of background (black) and foreground (white) objects. With erosion, if the structuring
    element when placed at a given pixel location touches the background (that is,
    one of the pixels in the intersecting set is black), then this pixel will be sent
    to the background. In the case of dilation, if the structuring element on a background
    pixel touches a foreground object, then this pixel will be assigned a white value.
    This explains why the size of the objects has been reduced (the shape has been
    eroded) in the eroded image. Note how some of the small objects (which can be
    considered as "noisy" background pixels) have also been completely eliminated.
    Similarly, the dilated objects are now larger, and some of the "holes" inside
    them have been filled. By default, OpenCV uses a 3 x 3 square structuring element.
    This default structuring element is obtained when an empty matrix (that is, `cv::Mat()`)
    is specified as the third argument in the function call, as it was done in the
    preceding example. You can also specify a structuring element of the size (and
    shape) you want by providing a matrix in which the nonzero element defines the
    structuring element. In the following example, a 7 x 7 structuring element is
    applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The effect is much more destructive in this case, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00054.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another way to obtain the same result is to repetitively apply the same structuring
    element on an image. The two functions have an optional parameter to specify the
    number of repetitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The argument `cv::Point(-1,-1)` means that the origin is at the center of the
    matrix (default); it can be defined anywhere on the structuring element. The image
    that is obtained will be identical to the image we obtained with the 7 x 7 structuring
    element. Indeed, eroding an image twice is similar to eroding an image with a
    structuring element dilated with itself. This also applies to dilation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, since the notion of background/foreground is arbitrary, we can make
    the following observation (which is a fundamental property of the erosion/dilation
    operators). Eroding the foreground objects with a structuring element can be seen
    as a dilation of the background part of the image. In other words, we can make
    the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: The erosion of an image is equivalent to the complement of the dilation of the
    complement image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dilation of an image is equivalent to the complement of the erosion of the
    complement image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note that even though we applied our morphological filters on binary images
    here, these filters can be applied on gray-level or even color images with the
    same definitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, note that the OpenCV morphological functions support in-place processing.
    This means that you can use the input image as the destination image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: OpenCV will create the required temporary image for you for this to work properly.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Opening and closing images using morphological filters* recipe applies
    the erosion and dilation filters in cascade to produce new operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Detecting edges and corners using morphological filters* recipe applies
    morphological filters on gray-level images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Opening and closing images using morphological filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The previous recipe introduced you to the two fundamental morphological operators:
    dilation and erosion. From these, other operators can be defined. The next two
    recipes will present some of them. The opening and closing operators are presented
    in this recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to apply higher-level morphological filters, you need to use the `cv::morphologyEx`
    function with the appropriate function code. For example, the following call will
    apply the closing operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we used a 5 x 5 structuring element to make the effect of the filter
    more apparent. If we use the binary image of the preceding recipe as input, we
    will obtain an image similar to what''s shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00055.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, applying the morphological opening operator will result in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00056.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding image is obtained from the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The opening and closing filters are simply defined in terms of the basic erosion
    and dilation operations. **Closing** is defined as the erosion of the dilation
    of an image. **Opening** is defined as the dilation of the erosion of an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, one can compute the closing of an image using the following calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The opening filter can be obtained by reverting these two function calls. While
    examining the result of the closing filter, it can be seen that the small holes
    of the white foreground objects have been filled. The filter also connects several
    adjacent objects together. Basically, any holes or gaps that are too small to
    completely contain the structuring element will be eliminated by the filter.
  prefs: []
  type: TYPE_NORMAL
- en: Reciprocally, the opening filter eliminated several small objects from the scene.
    All the objects that were too small to contain the structuring element have been
    removed.
  prefs: []
  type: TYPE_NORMAL
- en: These filters are often used in object detection. The closing filter connects
    the objects erroneously fragmented into smaller pieces together, while the opening
    filter removes the small blobs introduced by the image noise. Therefore, it is
    advantageous to use them in a sequence. If our test binary image is successively
    closed and opened, we obtain an image that shows only the main objects in the
    scene, as shown in the following screenshot. You can also apply the opening filter
    before the closing filter if you wish to prioritize noise filtering, but this
    will be at the price of eliminating some fragmented objects.
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00057.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Note that applying the same opening (and similarly the closing) operator on
    an image several times has no effect. Indeed, as the holes have been filled by
    the first opening filter an additional application of the same filter will not
    produce any other changes to the image. In mathematical terms, these operators
    are said to be idempotent.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The opening and closing operators are often used to clean up an image before
    extracting its connected components as explained in the *Extracting the components'
    contours* recipe of [Chapter 7](part0052_split_000.html#page "Chapter 7. Extracting
    Lines, Contours, and Components"), *Extracting Lines, Contours, and Components*.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting edges and corners using morphological filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Morphological filters can also be used to detect specific features in an image.
    In this recipe, we will learn how to detect contours and corners in a gray-level
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, the following image will be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/00058.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The edges of an image can be detected by using the appropriate filter of the
    `cv::morphologyEx` function. Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image is obtained as the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00059.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to detect corners using morphology, we now define a class named `MorphoFeatures`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The detection of corners using morphological corners is a bit complex since
    it requires the successive application of several different morphological filters.
    This is a good example of the use of nonsquare structuring elements. Indeed, this
    requires four different structuring elements shaped as a square, diamond, cross,
    and X-shape to be defined in the constructor (all these structuring elements have
    a fixed 5 x 5 dimension for simplicity):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the detection of corner features, all these structuring elements are applied
    in a cascade to obtain the resulting corner map:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The corners are then detected on an image by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the image, the detected corners are displayed as circles, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00060.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A good way to understand the effect of morphological operators on a gray-level
    image is to consider an image as a topological relief in which the gray levels
    correspond to elevation (or altitude). Under this perspective, the bright regions
    correspond to mountains, while the dark areas correspond to the valleys of the
    terrain. Also, since edges correspond to a rapid transition between the dark and
    bright pixels, these can be pictured as abrupt cliffs. If an erosion operator
    is applied on such a terrain, the net result will be to replace each pixel by
    the lowest value in a certain neighborhood, thus reducing its height. As a result,
    cliffs will be "eroded" as the valleys expand. Dilation has the exact opposite
    effect; that is, cliffs will gain terrain over the valleys. However, in both cases,
    the plateaux (that is, the area of constant intensity) will remain relatively
    unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: These observations lead to a simple way to detect the edges (or cliffs) of an
    image. This can be done by computing the difference between the dilated and eroded
    images. Since these two transformed images differ mostly at the edge locations,
    the image edges will be emphasized by the subtraction. This is exactly what the
    `cv::morphologyEx` function does when the `cv::MORPH_GRADIENT` argument is inputted.
    Obviously, the larger the structuring element is, the thicker the detected edges
    will be. This edge detection operator is also called the **Beucher** gradient
    (the next chapter will discuss the concept of an image gradient in more detail).
    Note that similar results can also be obtained by simply subtracting the original
    image from the dilated one or the eroded image from the original. The resulting
    edges would be thinner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Corner detection is a bit more complex since it uses four different structuring
    elements. This operator is not implemented in OpenCV, but we present it here to
    demonstrate how the structuring elements of various shapes can be defined and
    combined. The idea is to close the image by dilating and eroding it with two different
    structuring elements. These elements are chosen such that they leave straight
    edges unchanged, but because of their respective effects, the edges at corner
    points will be affected. Let''s use the simple following image made of a single
    white square to better understand the effect of this asymmetrical closing operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00061.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The first square is the original image. When dilated with a cross-shaped structuring
    element, the square edges expand, except at the corner points where the cross
    shape does not hit the square. This is the result illustrated by the square in
    the middle. This dilated image is then eroded by a structuring element that has
    a diamond shape. This erosion brings back most edges to their original position
    but pushes the corners even further since they were not dilated. The rightmost
    square is then obtained, which (as it can be seen) has lost its sharp corners.
    The same procedure is repeated with the X-shaped and square-shaped structuring
    elements. These two elements are the rotated versions of the previous elements
    and will consequently capture the corners at a 45-degree orientation. Finally,
    differencing the two results will extract the corner features.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Applying directional filters to detect edges* recipe in [Chapter 6](part0047_split_000.html#page
    "Chapter 6. Filtering the Images"), *Filtering the Images* describes the other
    filters that perform edge detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 8](part0058_split_000.html#page "Chapter 8. Detecting Interest Points"),
    *Detecting Interest Points*, presents different operators that perform corner
    detection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article, *The Morphological gradients*, *J.-F. Rivest, P. Soille, and S.
    Beucher, ISET's symposium on electronic imaging science and technology, SPIE,
    Feb. 1992*, discusses the concept of morphological gradients in more detail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The article, *A modified regulated morphological corner detector*, *F.Y. Shih,
    C.-F. Chuang, and V. Gaddipati, Pattern Recognition Letters, volume 26, issue
    7, May 2005*, gives more information on morphological corner detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmenting images using watersheds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The watershed transformation is a popular image processing algorithm that is
    used to quickly segment an image into homogenous regions. It relies on the idea
    that when the image is seen as a topological relief, the homogeneous regions correspond
    to relatively flat basins delimited by steep edges. As a result of its simplicity,
    the original version of this algorithm tends to over-segment the image, which
    produces multiple small regions. This is why OpenCV proposes a variant of this
    algorithm that uses a set of predefined markers that guide the definition of the
    image segments.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The watershed segmentation is obtained through the use of the `cv::watershed`
    function. The input for this function is a 32-bit signed integer-marker image
    in which each nonzero pixel represents a label. The idea is to mark some pixels
    of the image that are known to belong to a given region. From this initial labeling,
    the watershed algorithm will determine the regions to which the other pixels belong.
    In this recipe, we will first create the marker image as a gray-level image and
    then convert it into an image of integers. We have conveniently encapsulated this
    step into a `WatershedSegmenter` class. Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The way these markers are obtained depends on the application. For example,
    some preprocessing steps might have resulted in the identification of some pixels
    that belong to an object of interest. The watershed would then be used to delimitate
    the complete object from that initial detection. In this recipe, we will simply
    use the binary image used throughout this chapter in order to identify the animals
    of the corresponding original image (this is the image shown at the beginning
    of [Chapter 4](part0032_split_000.html#page "Chapter 4. Counting the Pixels with
    Histograms"), *Counting the Pixels with Histograms*). Therefore, from our binary
    image, we need to identify the pixels that belong to the foreground (the animals)
    and the pixels that belong to the background (mainly the grass). Here, we will
    mark the foreground pixels with the label `255` and the background pixels with
    the label `128` (this choice is totally arbitrary; any label number other than
    `255` will work). The other pixels, that is, the ones for which the labeling is
    unknown are assigned the value `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of now, the binary image includes too many white pixels that belong to the
    various parts of the image. We will then severely erode this image in order to
    retain only the pixels that belong to the important objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00062.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that a few pixels that belong to the background (forest) are still present.
    Let''s keep them. Therefore, they will be considered to correspond to an object
    of interest. Similarly, we also select a few pixels of the background by a large
    dilation of the original binary image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting black pixels correspond to the background pixels. This is why
    the thresholding operation assigns the value `128` to these pixels immediately
    after the dilation. The following image is obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00063.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'These images are combined to form the marker image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we used the overloaded operator `+` here in order to combine the images.
    The following image will be used as the input to the watershed algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00064.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this input image, the white areas belong, for sure, to the foreground objects,
    the gray areas are a part of the background, and the black areas have an unknown
    label. The segmentation is then obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The marker image is then updated such that each zero pixel is assigned one
    of the input labels, while the pixels that belong to the found boundaries have
    a value `-1`. The resulting image of the labels is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00065.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The boundary image will be similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00066.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we did in the preceding recipes, we will use the topological map analogy
    in the description of the watershed algorithm. In order to create a watershed
    segmentation, the idea is to progressively flood the image starting at level 0\.
    As the level of "water" progressively increases (to levels 1, 2, 3, and so on),
    catchment basins are formed. The size of these basins also gradually increases
    and, consequently, the water of two different basins will eventually merge. When
    this happens, a watershed is created in order to keep the two basins separate.
    Once the level of water has reached its maximal level, the sets of these created
    basins and watersheds form the watershed segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the flooding process initially creates many small individual basins.
    When all of these are merged, many watershed lines are created, which results
    in an over-segmented image. To overcome this problem, a modification to this algorithm
    has been proposed in which the flooding process starts from a predefined set of
    marked pixels. The basins created from these markers are labeled in accordance
    with the values assigned to the initial marks. When two basins having the same
    label merge, no watersheds are created, thus preventing over-segmentation. This
    is what happens when the `cv::watershed` function is called. The input marker
    image is updated to produce the final watershed segmentation. Users can input
    a marker image with any number of labels and pixels of unknown labeling left to
    value `0`. The marker image is chosen to be an image of a 32-bit signed integer
    in order to be able to define more than `255` labels. It also allows the special
    value, `-1`, to be assigned to the pixels associated with a watershed. This is
    returned by the `cv::watershed` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate the display of the result, we have introduced two special methods.
    The first method returns an image of the labels (with watersheds at value `0`).
    This is easily done through thresholding, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the second method returns an image in which the watershed lines
    are assigned the value `0`, and the rest of the image is at `255`. This time the
    `cv::convertTo` method is used to achieve this result, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The linear transformation that is applied before the conversion allows the `-1`
    pixels to be converted into `0` (since *-1*255+255=0*).
  prefs: []
  type: TYPE_NORMAL
- en: Pixels with a value greater than `255` are assigned the value `255`. This is
    due to the saturation operation that is applied when signed integers are converted
    into unsigned characters.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Obviously, the marker image can be obtained in many different ways. For example,
    users can be interactively asked to paint areas on the objects and the background
    of a scene. Alternatively, in an attempt to identify an object located at the
    center of an image, one can also simply input an image with the central area marked
    with a certain label and the border of the image (where the background is assumed
    to be present) marked with another label. This marker image can be created as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If we superimpose this marker image on a test image, we will obtain the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/00067.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the resulting watershed image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/00068.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The article, *The viscous watershed transform*, *C. Vachier and F. Meyer, Journal
    of Mathematical Imaging and Vision, volume 22, issue 2-3, May 2005*, gives more
    information on the watershed transform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last recipe of this chapter, *Extracting foreground objects with the GrabCut
    algorithm*, presents another image segmentation algorithm that can also segment
    an image into background and foreground objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting distinctive regions using MSER
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, you learned how an image can be segmented into regions
    by gradually flooding it and creating watersheds. The **maximally stable extremal
    regions** (**MSER**) algorithm uses the same immersion analogy in order to extract
    meaningful regions in an image. These regions will also be created by flooding
    the image level by level, but this time, we will be interested in the basins that
    remain relatively stable for a period of time during the immersion process. It
    will be observed that these regions correspond to some distinctive parts of the
    scene objects pictured in the image.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basic class to compute the MSER of an image is `cv::MSER`. An instance
    of this class can be created by using the default empty constructor. In our case,
    we chose to initialize it by specifying a minimum and maximum size for the detected
    regions in order to limit their number. Then, our call will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the MSER can be obtained by a call to a functor, specifying the input
    image and an appropriate output data structure, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a vector of regions represented by the pixel points that compose
    each of them. In order to visualize the results, we create a blank image on which
    we will display the detected regions in different colors (which are randomly chosen).
    This is done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the MSER form a hierarchy of regions. Therefore, to make all of these
    visible, we have chosen to not overwrite the small regions when they are included
    in larger ones. If the MSER are detected on the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00069.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, the resulting image will be (refer to the book''s graphics PDF to view
    this image in color) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00070.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: These are the raw results of the detection. Nevertheless, it can be observed
    how this operator has been able to extract some meaningful regions (for example,
    the building's windows) from this image.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MSER uses the same mechanism as the watershed algorithm; that is, it proceeds
    by gradually flooding the image from level `0` to level `255`. As the level of
    water increases, you can observe that the sharply delimitated darker areas form
    the basins that have a relatively stable shape for a period of time (recall that
    under the immersion analogy, the water levels correspond to the intensity levels).
    These stable basins are the MSER. These are detected by considering the connected
    regions at each level and measuring their stability. This is done by comparing
    the current area of a region with the area it previously had when the level was
    down by a value of delta. When this relative variation reaches a local minimum,
    the region is identified as a MSER. The delta value that is used to measure the
    relative stability is the first parameter in the constructor of the `cv::MSER`
    class; its default value is `5`. In addition, to be considered, the size of a
    region must be within a certain predefined range. The acceptable minimum and maximum
    region sizes are the next two parameters of the constructor. We must also ensure
    that the MSER is stable (the fourth parameter), that is, the relative variation
    of its shape is small enough. The stable regions can be included in the larger
    regions (called parent regions).
  prefs: []
  type: TYPE_NORMAL
- en: To be valid, a parent MSER must be sufficiently different from its child; this
    is the diversity criterion, and it is specified by the fifth parameter of the
    `cv::MSER` constructor. In the example used in the previous section, the default
    value for these last two parameters were used. (The default values are `0.25`
    for the maximum allowable variation of a MSER and `0.2` for the minimum diversity
    of a parent MSER.)
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the MSER detector is a vector of point sets. Since we are generally
    more interested in a region as a whole rather than its individual pixel locations,
    it is common to represent a MSER by a simple geometrical shape that describes
    the MSER location and size. A bounding ellipse is a commonly used representation.
    In order to obtain these ellipses, we will make use of two convenient OpenCV functions.
    The first is the `cv::minAreaRect` function that finds the rectangle of minimum
    area that binds all the points in a set. This rectangle is described by a `cv::RotatedRect`
    instance. Once this bounding rectangle is found, it is possible to draw the inscribed
    ellipse on the image by using the `cv::ellipse` function. Let''s encapsulate this
    complete process in one class. The constructor of this class basically repeats
    the one of the `cv::MSER` class. Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: One extra parameter (`minAreaRatio`) has been added to eliminate the MSER for
    which the bounding rectangle has an area that differs greatly from the one of
    the MSER it represents. This is to remove the less interesting elongated shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list of representative bounding rectangles is computed by the following
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding ellipses are drawn on the image using the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The detection of the MSER is then obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'By applying this function to the previously used image, we will get the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/00071.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Comparing this result with the previous result should convince you that this
    later representation is easier to interpret. Note how the child and parent MSER
    are often represented by very similar ellipses. In some cases, it would then be
    interesting to apply a minimum variation criterion on these ellipses in order
    to eliminate these repeated representations.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Computing components' shape descriptors* recipe in [Chapter 7](part0052_split_000.html#page
    "Chapter 7. Extracting Lines, Contours, and Components"), *Extracting Lines, Contours,
    and Components* will show you how to compute other properties of connected point
    sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 8](part0058_split_000.html#page "Chapter 8. Detecting Interest Points"),
    *Detecting Interest Points*, will explain how to use MSER as an interest point
    detector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting foreground objects with the GrabCut algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenCV proposes the implementation of another popular algorithm for image segmentation:
    the **GrabCut** algorithm. This algorithm is not based on mathematical morphology,
    but we have presented it here since it shows some similarities in its use with
    the watershed segmentation algorithm presented earlier in this chapter. GrabCut
    is computationally more expensive than watershed, but it generally produces more
    accurate results. It is the best algorithm to use when you want to extract a foreground
    object in a still image (for example, to cut and paste an object from one picture
    to another).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `cv::grabCut` function is easy to use. You just need to input an image,
    and label some of its pixels as belonging to the background or to the foreground.
    Based on this partial labeling, the algorithm will then determine a foreground/background
    segmentation for the complete image.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to specify a partial foreground/background labeling for an input image
    is by defining a rectangle inside which the foreground object is included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![How to do it...](img/00072.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'All the pixels outside this rectangle will then be marked as the background.
    In addition to the input image and its segmentation image, calling the `cv::grabCut`
    function requires the definition of two matrices, which will contain the models
    built by the algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we specified that we are using the bounding rectangle mode using the
    `cv::GC_INIT_WITH_RECT` flag as the last argument of the function (the next section
    will discuss the other available mode). The input/output segmentation image can
    have one of the following four values:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cv::GC_BGD`: This is the value for the pixels that certainly belong to the
    background (for example, pixels outside the rectangle in our example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv::GC_FGD`: This is the value for the pixels that certainly belong to the
    foreground (there are none in our example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv::GC_PR_BGD`: This is the value for the pixels that probably belong to the
    background'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv::GC_PR_FGD`: This is the value for the pixels that probably belong to the
    foreground (that is, the initial value for the pixels inside the rectangle in
    our example)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We get a binary image of the segmentation by extracting the pixels that have
    a value equal to `cv::GC_PR_FGD`. Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To extract all the foreground pixels, that is, with values equal to `cv::GC_PR_FGD`
    or `cv::GC_FGD`, it is possible to check the value of the first bit, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This is possible because these constants are defined as values 1 and 3, while
    the other two (`cv::GC_BGD` and `cv::GC_PR_BGD`) are defined as 0 and 2\. In our
    example, the same result is obtained because the segmentation image does not contain
    the `cv::GC_FGD` pixels (only the `cv::GC_BGD` pixels have been inputted).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we obtain an image of the foreground objects (over a white background)
    by the following copy operation with a mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image is obtained as the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/00073.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding example, the GrabCut algorithm was able to extract the foreground
    objects by simply specifying a rectangle inside which these objects (the four
    animals) were contained. Alternatively, one could also assign the values `cv::GC_BGD`
    and `cv::GC_FGD` to some specific pixels of the segmentation image, which are
    provided as the second argument of the `cv::grabCut` function. You would then
    specify `GC_INIT_WITH_MASK` as the input mode flag. These input labels could be
    obtained, for example, by asking a user to interactively mark a few elements of
    the image. It is also possible to combine these two input modes.
  prefs: []
  type: TYPE_NORMAL
- en: Using this input information, the GrabCut algorithm creates the background/foreground
    segmentation by proceeding as follows. Initially, a foreground label (`cv::GC_PR_FGD`)
    is tentatively assigned to all the unmarked pixels. Based on the current classification,
    the algorithm groups the pixels into clusters of similar colors (that is, `K`
    clusters for the background and `K` clusters for the foreground). The next step
    is to determine a background/foreground segmentation by introducing boundaries
    between the foreground and background pixels. This is done through an optimization
    process that tries to connect pixels with similar labels, and that imposes a penalty
    for placing a boundary in the regions of relatively uniform intensity. This optimization
    problem can be efficiently solved using the **Graph Cuts** algorithm, a method
    that can find the optimal solution of a problem by representing it as a connected
    graph on which cuts are applied in order to compose an optimal configuration.
    The obtained segmentation produces new labels for the pixels. The clustering process
    can then be repeated, and a new optimal segmentation is found again, and so on.
    Therefore, the GrabCut algorithm is an iterative procedure that gradually improves
    the segmentation result. Depending on the complexity of the scene, a good solution
    can be found in more or less number of iterations (in easy cases, one iteration
    would be enough).
  prefs: []
  type: TYPE_NORMAL
- en: This explains the argument of the function where the user can specify the number
    of iterations to be applied. The two internal models maintained by the algorithm
    are passed as an argument of the function (and returned). Therefore, it is possible
    to call the function with the models of the last run again if one wishes to improve
    the segmentation result by performing additional iterations.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The article, *GrabCut: Interactive Foreground Extraction using Iterated Graph
    Cuts in ACM Transactions on Graphics (SIGGRAPH) volume 23, issue 3, August 2004,*
    *C. Rother, V. Kolmogorov, and A. Blake* describes the GrabCut algorithm in detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
