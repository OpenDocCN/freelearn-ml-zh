["```py\n$ cd Chapter04/gradflow/\n```", "```py\n$ make\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport pandas_datareader.data as web\nfrom pandas import Series, DataFrame\nstart = datetime.datetime(2014, 1, 1)\nend = datetime.datetime(2020, 12, 31)\nbtc_df = web.DataReader(\"BTC-USD\", 'yahoo', start, end)\n```", "```py\nbtc_df['Open'].plot()\n```", "```py\n    import pandas\n    import numpy as np\n    import mlflow\n    import tensorflow\n    from tensorflow import keras\n    import mlflow.keras\n    from sklearn.metrics import f1_score,confusion_matrix\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    pandas_df = pandas.read_csv(\"training_data.csv\")\n    X=pandas_df.iloc[:,:-1]\n    Y=pandas_df.iloc[:,-1]\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(X, Y, test_size=0.33, \n                     random_state=4284, stratify=Y)\n    ```", "```py\n    mlflow.set_experiment(\"Baseline_Predictions\")\n    mlflow.sklearn.autolog()\n    ```", "```py\n    with mlflow.start_run(run_name='logistic_regression_model_baseline') as run:\n        model = LogisticRegression()\n        model.fit(X_train, y_train)\n        preds = model.predict(X_test)\n        y_pred = np.where(preds>0.5,1,0)\n        f1 = f1_score(y_test, y_pred)\n        mlflow.log_metric(key=\"f1_experiment_score\", \n                          value=f1)\n    ```", "```py\nimport pandas\nimport mlflow\nimport xgboost as xgb\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n```", "```py\n    mlflow.set_experiment(\"Baseline_Predictions\")\n    mlflow.xgboost.autolog()\n    ```", "```py\n    with mlflow.start_run(\n      run_name='xgboost_model_baseline') as run:\n        model=xgb.train(dtrain=dtrain,params={})\n        preds = model.predict(dtest)\n        y_bin = [1\\. if y_cont > threshold else 0\\. for y_cont in preds]\n        f1= f1_score(y_test,y_bin)\n        mlflow.log_metric(key=\"f1_experiment_score\", \n                          value=f1)\n    ```", "```py\n    import pandas\n    import numpy as np\n    import mlflow\n    import tensorflow\n    from tensorflow import keras\n    import mlflow.keras\n    from sklearn.metrics import f1_score,confusion_matrix\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n     mlflow.set_experiment(\"Baseline_Predictions\")\n     mlflow.tensorflow.autolog()\n    ```", "```py\n    model = keras.Sequential([\n      keras.layers.Dense(\n        units=36,\n        activation='relu',\n        input_shape=(X_train.shape[-1],)\n      ),\n      keras.layers.BatchNormalization(),\n      keras.layers.Dense(units=1, activation='sigmoid'),\n    ])\n    model.compile(\n      optimizer=keras.optimizers.Adam(lr=0.001),\n      loss=\"binary_crossentropy\",\n      metrics=\"Accuracy\"\n    )\n    ```", "```py\n    with mlflow.start_run(\n      run_name='keras_model_baseline') as run:\n        model.fit(\n            X_train,\n            y_train,\n            epochs=20,\n            validation_split=0.05,\n            shuffle=True,\n            verbose=0\n        )\n        preds = model.predict(X_test)\n        y_pred = np.where(preds>0.5,1,0)\n        f1 = f1_score(y_test, y_pred)\n        mlflow.log_metric(key=\"f1_experiment_score\", \n                          value=f1)\n    ```", "```py\n    from hyperopt import tpe\n    from hyperopt import STATUS_OK\n    from hyperopt import Trials\n    from hyperopt import hp\n    from hyperopt import fmin\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import cross_val_score\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    N_FOLDS = 3\n    MAX_EVALS = 10\n    def objective(params, n_folds = N_FOLDS):\n        # Perform n_fold cross validation with \n        #hyperparameters\n        # Use early stopping and evaluate based on ROC AUC\n        mlflow.sklearn.autolog()\n        with mlflow.start_run(nested=True):\n            clf = LogisticRegression(**params,\n                                     random_state=0,\n                                     verbose =0)\n            scores = cross_val_score(clf, X_train, \n                                     y_train, cv=5, \n                                     scoring='f1_macro')\n            # Extract the best score\n            best_score = max(scores)\n            # Loss must be minimized\n            loss = 1 - best_score\n            # Dictionary with information for evaluation\n            return {'loss': loss, 'params': params, \n                    'status': STATUS_OK}\n    ```", "```py\n    # Algorithm\n    tpe_algorithm = tpe.suggest\n    # Trials object to track progress\n    bayes_trials = Trials()\n    mlflow.set_experiment(\"Bayesian_param_tuning\")\n    with mlflow.start_run():\n        best = fmin(fn = objective, space = space, \n                    algo = tpe.suggest, \n                    max_evals = MAX_EVALS, \n                    trials = bayes_trials)\n    ```"]