- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic Regression in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce logistic regression, covering its theoretical
    construct, connection with linear regression, and practical implementation. As
    it is an important classification model that is widely used in areas where interpretability
    matters, such as credit risk modeling, we will focus on its modeling process in
    different contexts, along with extensions such as adding regularization to the
    loss function and predicting more than two classes.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the fundamentals of the logistic
    regression model and its comparison with linear regression, including extended
    concepts such as the `sigmoid` function, odds ratio, and **cross-entropy loss**
    (**CEL**). You will also have grasped the commonly used evaluation metrics in
    the classification setting, as well as enhancements that deal with imbalanced
    datasets and multiple classes in the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing logistic regression with linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More on log odds and odds ratio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the cross-entropy loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating a logistic regression model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with an imbalanced dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Penalized logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extending to multi-class classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run the code in this chapter, you will need to have the latest versions
    of the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`caret` – 6.0.94'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tibble` – 3.2.1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dplyr` – 1.0.10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pROC` – 1.18.2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nnet` – 7.3.18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`glmnet` – 4.1.7'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The versions mentioned along with the packages in the preceding list are the
    latest ones while I am writing this book.
  prefs: []
  type: TYPE_NORMAL
- en: All the code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_13/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_13/working.R).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logistic regression is a binary classification model. It is still a linear model,
    but now the output is constrained to be a binary variable, taking the value of
    `0` or `1`, instead of modeling a continuous outcome as in the case of linear
    regression. In other words, we will observe and model the outcome y = 1 or y =
    0\. For example, in the case of credit risk modeling, y = 0 refers to a non-default
    loan application, while y = 1 indicates a default loan.
  prefs: []
  type: TYPE_NORMAL
- en: However, instead of directly predicting the binary outcome, the logistic regression
    model predicts the probability of y taking a specific value, such as P(y = 1).
    The probability of assuming the other category is P(y = 0) = 1 − P(y = 1), since
    the total probability should always sum to `1`. The final prediction would be
    the winner of the two, taking the value of `1` if P(y = 1) > P(y = 0), and `0`
    otherwise. In the credit risk example, P(y = 1) would be interpreted as the probability
    of a loan defaulting.
  prefs: []
  type: TYPE_NORMAL
- en: In logistic regression, the term *logistic* is related to *logit*, which refers
    to the log odds. The odds are another way to describe the probability; instead
    of specifying the individual P(y = 1) and P(y = 0), it refers to the ratio of
    P(y = 1) to P(y = 0). Thus the log odds are calculated via logP(y = 1) _ P(y =
    0). Therefore, we can simply use the term *odds* to describe the probability of
    an event happening (y = 1) over the probability of it not occurring (y = 0).
  prefs: []
  type: TYPE_NORMAL
- en: First, let us look at how the logistic regression model transforms a continuous
    output (as in linear regression) into a probability score, which is a number bounded
    between `0` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the sigmoid function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `sigmoid` function is the key ingredient that maps any continuous number
    (from negative infinity to positive infinity) to a probability. Also known as
    the logistic function, the `sigmoid` function is characterized by an S-shaped
    curve, taking any real number as input and mapping it into a score between `0`
    and `1`, which happens to be the range of a valid probability score.
  prefs: []
  type: TYPE_NORMAL
- en: 'The standard `sigmoid` function takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: f(x) =  1 _ 1 + e −x
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is a nonlinear function. That is, the input values will get disproportionate
    scaling when going through the transformation using this function. It is also
    a continuous function (thus differentiable) and monotone (f(x) will increase as
    x increases), thus enjoying high popularity as the go-to activation function to
    be used at the last layer of a typical neural network model for binary classification
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us try to visualize this function. In the following code snippet, we use
    `seq(-10, 10, by = 0.1)` to create a sequence of equally spaced numbers from -10
    to 10, with a step size of `0.1`. For each number, we calculate the corresponding
    output using the `sigmoid` function. Here, we directly pass all the numbers of
    the function, which then calculates all the output in a parallel mode called vectorization.
    Here, vectorization refers to the process of applying an operation to an entire
    vector simultaneously instead of looping over each element one by one as in a
    `for` loop. Finally, we plot the function to show the characteristic S-shaped
    curve of the `sigmoid` function and add the gridlines using the `grid()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Running the preceding code generates the output shown in *Figure 13**.1*. The
    plot shows a different level of steepness across the whole domain, where the function
    is more sensitive in the middle region and becomes more saturated at the two extremes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Visualizing the sigmoid function](img/B18680_13_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Visualizing the sigmoid function
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the `sigmoid` function, let us look at the mathematical
    construct of the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Grokking the logistic regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The logistic regression model is essentially a linear regression model generalized
    to the setting where the dependent outcome variable is binary. In other words,
    it is a linear regression model that models the log odds of the probability of
    an event.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see this, let us first recall the following linear regression model, where
    we use a total of p features to model the target output variable, z:'
  prefs: []
  type: TYPE_NORMAL
- en: z = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p
  prefs: []
  type: TYPE_NORMAL
- en: Here, z is interpreted as the log odds, or logit, of the event of y = 1\. We
    are interested in estimating the parameters from β 0 to β p.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we know that the z variable is unbounded, meaning it can vary from negative
    infinity to positive infinity. We need a way to bound this output and convert
    it into a probability score valued between 0 and 1\. This is achieved via an additional
    transformation using the `sigmoid` function, which happens to satisfy all our
    needs. Mathematically, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: P(y = 1) =  1 _ 1 + e −z
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugging in the definition of z gives the full logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: P(y = 1) =  1 ______________  1 + e −(β 0+β 1x 1+β 2x 2+…+β px p)
  prefs: []
  type: TYPE_NORMAL
- en: Here, P(y = 1) refers to the probability of having a success of y = 1 (this
    is a general statement), and correspondingly, P(y = 0) indicates the probability
    of having a failure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can equivalently express the model as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: log P(y = 1) _ P(y = 0)  = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p
  prefs: []
  type: TYPE_NORMAL
- en: Here, the term log P(y = 1) _ P(y = 0) stands for the log odds.
  prefs: []
  type: TYPE_NORMAL
- en: A key change here is the introduction of the `sigmoid` function. This makes
    the relationship between the predictors and the resulting probability no longer
    linear, but sigmoidal instead. To observe the subtlety here, we can look at the
    different regions of the `sigmoid` function across the domain. For example, when
    looking at the region around 0, a small change in the input would result in a
    relatively large change in the resulting probability output. However, the same
    change in the input will cause a very small change in the output when located
    on the two extreme sides of the function. Also, as the input becomes more extreme,
    either toward the negative or positive side, the resulting probability will gradually
    approach 0 or 1\. *Figure 13**.2* recaps the characteristics of the logistic regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Summarizing the logistic regression model](img/B18680_13_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – Summarizing the logistic regression model
  prefs: []
  type: TYPE_NORMAL
- en: Note that a logistic regression model shares similar assumptions with the linear
    regression model. Specifically, it assumes that the observations are independent
    of each other, and the target outcome follows a Bernoulli distribution parameterized
    by p – that is, y ∼ Bernoulli(p). Given this, we do not assume a linear regression
    between the output variable and the input predicts; instead, we use the logistic
    link function to transform and introduce nonlinearity to the input variables.
  prefs: []
  type: TYPE_NORMAL
- en: The following section further compares logistic regression with linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing logistic regression with linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will focus on a binary credit classification task using
    the *German Credit* dataset, which contains 1,000 observations and 20 columns.
    Each observation denotes a customer who had a loan application from the bank and
    is labeled as either good or bad in terms of credit risk. The dataset is available
    in the `caret` package in R.
  prefs: []
  type: TYPE_NORMAL
- en: For our study, we will attempt to predict the target binary variable, `Class`,
    based on `Duration`, and compare the difference in the prediction outcome between
    linear regression and logistic regression. We specifically choose one predictor
    only so that we can visualize and compare the decision boundaries of the resultant
    model in a two-dimensional plot.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 13.1 – comparing linear regression with logistic regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will demonstrate the advantage of using a logistic regression
    model in producing a probabilistic output compared to the unbounded output using
    a linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the *German Credit* dataset from the `caret` package. Convert the target
    variable (`Class`) to numeric:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we create a new target variable called `Class_num` to map the original
    `Class` variable to `1` if it takes on the value of `"Bad"`, and `0` otherwise.
    This is necessary as both linear regression and logistic regression models cannot
    accept a string-based variable as the target (or predictor).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build a linear regression model to regress `Class_num` against `Duration`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `lm()` function to build the linear regression model and `coefficients()`
    to extract the model coefficients, including the intercept and slope.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the prediction and the target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we plot the observed target variable as a scatter plot and use the `geom_abline()`
    function to plot the model as a straight line based on the estimated slope and
    intercept.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Running the preceding code generates *Figure 13**.3*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Visualizing the linear regression model](img/B18680_13_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Visualizing the linear regression model
  prefs: []
  type: TYPE_NORMAL
- en: Since all target values are `0` or `1`, we can think of the predictions as probabilities
    valued between `0` and `1`. However, as we zoom out, the problem of having an
    unbounded probability would surface, as shown in the following steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Re-plot the graph by zooming out to a wide domain of `(-30, 120)` for the *x*
    axis and `(-0.5, 1.5)` for the *y* axis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we enlarged the range of possible values for the *x* axis and *y* axis
    using the `xlim()` and `ylim()` functions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Running the preceding code generates the output shown in *Figure 13**.4*, which
    shows that the predicted values are outside the range of `[0,1]` when the value
    of `Duration` becomes more extreme, a situation called extrapolation beyond the
    observed range of values. This means that the predicted probabilities would be
    smaller than 0 or bigger than 1, which is obviously an invalid output. This calls
    for a generalized linear regression model called logistic regression, where the
    response will follow a logistic, S-shaped curve based on the transformation of
    the `sigmoid` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.4 – Visualizing the linear regression model with extended range](img/B18680_13_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – Visualizing the linear regression model with extended range
  prefs: []
  type: TYPE_NORMAL
- en: 'Build a logistic regression model using the `glm()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows the estimated intercept and slope, along with the residual
    deviance, a measure of goodness of fit for the logistic regression model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the estimated logistic curve on the previous plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the preceding code will generate the output shown in *Figure 13**.5*,
    which suggests a slight deviation between the linear regression line and the logistic
    regression curve.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.5 – Visualizing the logistic regression curve](img/B18680_13_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – Visualizing the logistic regression curve
  prefs: []
  type: TYPE_NORMAL
- en: Again, we can zoom out the figure and focus on the difference when going beyond
    the observed range of possible values in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Plot the logistic curve within a wider range of values beyond the observed
    range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we first extract the coefficients for the logistic regression model, followed
    by generating a sequence of values for the input and the corresponding output
    using the `sigmoid` function transformation. Lastly, we plot the logistic curve
    together with the linear fit in the same plot with the observed data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Running the preceding code generates the output shown in *Figure 13**.6*, which
    shows that the logistic regression curve gets gradually saturated as the input
    value becomes more extreme. In addition, all values are now bounded in the range
    of `[0,1]`, making it a valid candidate for interpretation as probabilities instead
    of an unbounded value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.6 – Visualizing the logistic regression model with extended range](img/B18680_13_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – Visualizing the logistic regression model with extended range
  prefs: []
  type: TYPE_NORMAL
- en: The next section looks at how to make predictions using a logistic regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions using the logistic regression model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in the previous section, the direct predictions from a logistic
    regression model take the form of probabilities valued between `0` and `1`. To
    convert them into binary predictions, we could take the most probable prediction
    by rounding the probability using a threshold of `0.5`. For example, if the predicted
    probability is P(y = 1) = 0.8, the rounding operation will lead to a final binary
    prediction of y = 1\. On the other hand, if P(y = 1) = 0.3, rounding will result
    in y = 0.
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through the following exercise to understand how to perform predictions
    using the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 13.2 – performing predictions using the logistic regression model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have seen how to perform predictions using the explicit sigmoid transformation
    after extracting the slope and intercept of the logistic regression model. In
    this exercise, we will explore a more convenient approach using the `predict()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate a sequence of `Duration` values ranging from `5` to `80` with a step
    size of `2` and predict the corresponding probabilities for the sequence using
    the `predict()` function based on the previous logistic regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we use the `seq()` function to create the equally spaced vector and store
    it in a `tibble` object called `pred_df`. We then use `predict()` to predict the
    corresponding probabilities by specifying `type="response"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the predicted probabilities together with the raw data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code will generate the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.7 – Visualizing the predicted probabilities](img/B18680_13_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – Visualizing the predicted probabilities
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the probabilities to binary outcomes using the `round()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we round the predicted probabilities using a default threshold of `0.5`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Add the binary outcomes as green points to the previous graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the preceding code generates the following output, which suggests that
    all predicted probabilities above `0.5` are converted to `1`, and those below
    `0.5` are converted to `0`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.8 – Visualizing the predicted binary outcomes](img/B18680_13_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – Visualizing the predicted binary outcomes
  prefs: []
  type: TYPE_NORMAL
- en: The next section discusses the log odds further.
  prefs: []
  type: TYPE_NORMAL
- en: More on log odds and odds ratio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall that the odds refer to the ratio of the probability of an event happening
    over its complement:'
  prefs: []
  type: TYPE_NORMAL
- en: odds =  probability of event happening   ________________________   probability
    of event not happening  =  p _ 1 − p  =  P(y = 1) _ P(y = 0)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the probability is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p = P(y = 1) =  1 _ 1 + e −z
  prefs: []
  type: TYPE_NORMAL
- en: 1 − p = 1 −  1 _ 1 + e −z  =  e −z _ 1 + e −z
  prefs: []
  type: TYPE_NORMAL
- en: 'Plugging in the definition of p and 1 − p gives us the following:'
  prefs: []
  type: TYPE_NORMAL
- en: odds =  p _ 1 − p  = e z
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of directly working with the odds, we often use the log odds or logit.
    This term is typically modeled as a linear combination of predictors in a logistic
    regression model via the following:'
  prefs: []
  type: TYPE_NORMAL
- en: log P(y = 1) _ P(y = 0)  = z = β 0 + β 1 x 1 + … + β p x p
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can interpret each coefficient β j as the expected change in the log
    odds for a one-unit increase in the jth predictor x j, while keeping all other
    predictors constant. This equation essentially says that the log odds of the target
    value y being `1` is linearly related to the input variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now suppose x i is a binary input variable, making x i = 1 or 0\. We can calculate
    the odds of x i = 1 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p 1 _ 1 − p 1  = e z 1
  prefs: []
  type: TYPE_NORMAL
- en: 'This measures the chance of an event for x i = 1, over the chance of a non-event.
    Similarly, we can calculate the odds of x i = 0 as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p 0 _ 1 − p 0  = e z 0
  prefs: []
  type: TYPE_NORMAL
- en: This measures the chance of an event for x i = 0 over the chance of a non-event.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then calculate the odds ratio of x i, which is the ratio of the odds
    x i = 1 to the odds of x i = 0:'
  prefs: []
  type: TYPE_NORMAL
- en: p 1 _ 1 − p 1 _  p 0 _ 1 − p 0  =  e β 0+β 1x 1+…β i*1+…+β px p ___________ e β 0+β 1x 1+…β i*0+…+β px p 
    = e β i
  prefs: []
  type: TYPE_NORMAL
- en: Here, e β i measures the quantified impact of the binary input variable, x i,
    on the odds of the outcome y being `1`, while all other input variables remain
    unchanged. This gives us a way to measure the impact of any predictor in a logistic
    regression model, covering both categorical and numerical input variables.
  prefs: []
  type: TYPE_NORMAL
- en: For categorical input variables, we may use gender (`0` for male and `1` for
    female) to predict whether insurance is purchased (`1` for yes and `0` for no).
    We set the base categorical as `0` for male. If the estimated coefficient β gender
    = 0.2 for gender, its odds ratio is calculated as e 0.2 ≈ 1.22\. Therefore, the
    odds of female customers purchasing the insurance is 1.22 times the odds of their
    male counterparts purchasing the insurance, assuming all other variables remain
    unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: For numerical input variables, we may use age to predict whether insurance is
    purchased. There is no need to set the base category in this case. If the estimated
    coefficient β age = 0.3, the corresponding odds ratio is calculated as e 0.3 ≈
    1.35\. This means that the odds of a client purchasing is 1.35 times the odds
    of similar people who are one year younger, assuming *ceteris paribus*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we can calculate the log odds using the predicted probabilities,
    as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The next section introduces more on the loss function of the logistic regression
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the cross-entropy loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The binary CEL, also called the **log loss**, is often used as the cost function
    in logistic regression. This is the loss that the logistic regression model will
    attempt to minimize by moving the parameters. This function takes the predicted
    probabilities and the corresponding targets as the input and outputs a scalar
    score, indicating the goodness of fit. For a single observation with a target
    of y i and predicted probability of p i, the loss is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Q i(y i, p i) = − [ y i logp i + (1 − y i)log(1 − p i)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Summing up all individual losses gives the total binary CEL:'
  prefs: []
  type: TYPE_NORMAL
- en: Q(y, p) =  1 _ N  ∑ i N Q i =  1 _ N  ∑ i=1 N − [ y i logp i + (1 − y i)log(1
    − p i)]
  prefs: []
  type: TYPE_NORMAL
- en: The binary CEL function is a suitable choice for binary classification problems
    because it heavily penalizes confident but incorrect predictions. For example,
    as p i approaches *0* or *1*, the resulting CEL will go to infinity if the prediction
    is incorrect. This property thus encourages the learning process to output probabilities
    that are close to the true probabilities of the targets.
  prefs: []
  type: TYPE_NORMAL
- en: More generally, we use the CEL to model a classification problem with two or
    more classes in the target variable. For the i th observation, *x* i, the classification
    function would produce a probability output, denoted as p i,k = f( *x* i; *w*),
    to indicate the likelihood of belonging to the k th class. When we have a classification
    task with a total of C classes, the CEL for *x* i is defined as Q i(*w*) = − ∑ k=1 C  y i,k
    log(p i,k), which essentially sums across all classes. Again, y i,k = 1 if the
    target label for the i th observation belongs to the k th class, and y i,k = 0
    otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: The summation means that the class-wise evaluation (i.e., the term y i,k log(p i,k))
    is performed for all classes and summed together to produce the total cost for
    the i th observation. For each observation, there are a total of C predictions
    corresponding to the respective probability of belonging to each class. The CEL
    thus aggregates the matrix of predicted probabilities by summing them into a single
    number. In addition, the result is negated to produce a positive number, since
    log(x) is negative if x is a probability between `0` and `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the target label in the CEL calculation needs to be one-hot encoded.
    This means that a single categorical label is converted into an array of binary
    numbers that contains `1` for the class label and `0` otherwise. For example,
    for a digit image on number `8`, instead of passing the original class as the
    target output, the resulting one-hot-encoded target `[0, 0, 0, 0, 0, 0, 0, 1,
    0, 0]` would be used, where the 8th position is activated (i.e., hot) and the
    rest disabled. The target array would also have the same length as the probability
    output array, whose elements correspond to each of the classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intuitively, we would hope for the predicted probability for the correct class
    to be close to `1` and for the incorrect class to be `0`. That is, the loss should
    increase as the predicted probabilities diverge from the actual class label. The
    CEL is designed to follow this intuition. Specifically, we can look at the following
    four scenarios for the i th observation:'
  prefs: []
  type: TYPE_NORMAL
- en: When the target label belongs to the k th class (i.e., y i,k = 1) and the predicted
    probability for the k th class is very strong (i.e., p i,k ≈ 1), the cost should
    be low.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the target label belongs to the k th class (i.e., y i,k = 1) and the predicted
    probability for the k th class is very weak (i.e., p i,k ≈ 0), the cost should
    be high.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the target label does not belong to the k th class (i.e., y i,k = 0) and
    the predicted probability for the k th class is very strong (i.e., p i,k ≈ 1),
    the cost should be high.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the target label does not belong to the k th class (i.e., y i,k = 0) and
    the predicted probability for the k th class is very weak (i.e., p i,k ≈ 0), the
    cost should be low.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the CEL, we first calculate the weighted sum between the vector
    of binary labels and the vector of predicted probabilities across all classes
    for each observation. The results of all observations are added together, followed
    by a minus sign to reverse the cost to a positive number. The CEL is designed
    to match the intuition for the cost: the cost would be low when the prediction
    and the target closely match, and high otherwise. In other words, to calculate
    the total cost, we would simply sum individual costs, leading to Q(w) = − ∑ i=1 N  ∑ k=1 C  y i,k
    log(p i,k). *Figure 13**.9* summarizes the preceding discussion on the CEL.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – Illustrating the CEL](img/B18680_13_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – Illustrating the CEL
  prefs: []
  type: TYPE_NORMAL
- en: Note that the last two scenarios do not contribute at all to the loss calculation
    since the target value is equal to 0; any value multiplied by 0 becomes 0\. Also,
    observe that the predicted probabilities of a specific observation for these two
    classes need to add up to 1, making it sufficient to only focus on the predicted
    probability of one class (mostly class `1`).
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces how to evaluate a logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a logistic regression model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are multiple metrics we can use to evaluate a logistic regression model.
    These are the metrics we use to determine the goodness of fit (over the test set),
    which needs to be differentiated from the CEL we use to train the model (over
    the training set).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list provides the commonly used metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy rate**: This is the proportion of the number of correctly predicted
    observations made by the model out of the count of all observations. Since a correct
    prediction can be either a true positive or a true negative, the accuracy is calculated
    by summing up the true positives and true negatives and dividing the total number
    of observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error rate**: This is the proportion of incorrectly predicted observations
    made by the model over the total observations. An incorrect prediction can be
    a false positive or a false negative. It is calculated as *1 - accuracy rate*;
    that is, the error rate is the complement of the accuracy rate. In other words,
    it is calculated as *(false positives + false negatives) /* *total observations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: Precision is the proportion of correct positive predictions
    among all positive predictions. This measure essentially tells us out of all predicted
    positive instances, how many of them are correct. Thus it indicates the model’s
    accuracy in predicting positive observations and is calculated as *true positives
    / (true positives + false positives)*. In the denominator, we note that among
    all positive instances, some are true positives and the rest are false positives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: Recall refers to the proportion of actual positive instances that
    the model correctly predicts. Also called sensitivity or **true positive rate**
    (**TPR**), the recall measures the model’s ability to detect positive observations.
    It is calculated as *true positives / (true positives + false negatives)*, where
    the formula differs in the denominator compared with precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity**: Also called **true negative rate** (**TNR**), specificity
    measures the proportion of actual negative instances correctly predicted by the
    model. This is the opposite of sensitivity, which focuses on the model’s ability
    to capture the true positives. In other words, specificity measures the model’s
    ability to identify negative instances or non-events correctly. It is calculated
    as *true negatives / (true negatives +* *false positives)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0` and `1`, indicating the degree of separability between two classes. A perfect
    model with 100% correct predictions has an AUC of `1`, while a model with completely
    wrong predictions has an AUC of `0`. A model that performs random guesses (choosing
    `0` or `1` with 50% probability) corresponds to an AUC of `0.5`, suggesting no
    class separation capacity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we would exercise the principle of parsimony upon assessing two models
    with equally good evaluation metrics. The principle of parsimony says that if
    two competing models provide a similar level of fit to the data, the one with
    fewer input variables should be picked, thus preferring simplicity over complexity.
    The underlying assumption is that the most accurate model is not necessarily the
    best model.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.10* describes the process of laying out the confusion matrix that
    captures the prediction results in different scenarios, along with the details
    on calculating the aforementioned evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.10 – Illustrating the confusion matrix and common evaluation metrics
    for binary classification tasks](img/B18680_13_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 – Illustrating the confusion matrix and common evaluation metrics
    for binary classification tasks
  prefs: []
  type: TYPE_NORMAL
- en: Note that we will only be able to calculate these metrics after selecting a
    threshold to cut off predicted probabilities. Specifically, an observation with
    a predicted probability greater than the cutoff threshold will be classified as
    positive, and negative otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall usually have an inverse relationship with respect to the
    adjustment of the classification threshold. Reviewing both precision and recall
    is useful for cases where there is a huge imbalance in the target variable’s values.
    As precision and recall are two related but different metrics, which should we
    optimize for?
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, we need to assess the relative impact of making a false
    negative prediction. This is measured by the false negative rate, which is the
    opposite (or complement) of recall. As *Figure 13**.11* suggests, failing to spot
    a spam email is less risky than missing a fraudulent transaction or a positive
    cancer patient. We should aim to optimize for precision (so that the model’s predictions
    are more precise and targeted) for the first case and recall (so that we minimize
    the chance of missing a potentially positive case) for the second case.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.11 – Three cases with different impacts of having a false negative
    prediction](img/B18680_13_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.11 – Three cases with different impacts of having a false negative
    prediction
  prefs: []
  type: TYPE_NORMAL
- en: As for the AUC, or area under the ROC curve, there is no need to select a specific
    threshold as it is calculated by evaluating a sequence of thresholds from `0`
    to `1`. The ROC curve plots the sensitivity on the *y* axis and *1 - specificity*
    on the *x* axis. This also corresponds to plotting TPR versus 1-TNR, or TPR versus
    FPR. As the classification threshold goes up, FPR goes down, leading to a leftward
    movement of the curve.
  prefs: []
  type: TYPE_NORMAL
- en: A perfect binary classifier has an AUC score of `1`. This means that FPR, or
    *1 – specificity*, is `0`. That is, there is no false positive, and all negative
    cases are not predicted as positive. In addition, the sensitivity, or TPR, is
    `1`, meaning all positive cases are predicted as positive correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.12* illustrates three different AUC curves. The topmost curve
    (in green) corresponds to a better model due to the highest AUC. Both models,
    represented by the green and red curves, perform better than random guessing,
    as indicated by the straight off-diagonal line in blue.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.12 – Illustrating three different AUC curves](img/B18680_13_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.12 – Illustrating three different AUC curves
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing the previous exercise, we can now calculate the corresponding evaluation
    metrics. First, we use the trained logistic regression model to score all observations
    in the training set and obtain the corresponding probabilities using the `predict()`
    function and setting `type="response"`, as shown in the following code snippet.
    Note that we need to pass in a DataFrame with the corresponding feature names
    as input to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set a single cutoff threshold (`0.5`, in this case) to convert the
    predicted probabilities into the corresponding binary outcomes using the `ifelse()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With the binary outcomes and true target labels, we can obtain the confusion
    matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, the confusion matrix provides a breakdown of the correct and incorrect
    classifications from the model. Within the confusion matrix, the top-left cell
    means true negatives, the top-right cell means false positives, the bottom-left
    cell means false negatives, and the bottom-right cell means true positives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the confusion matrix, we can calculate the evaluation metrics as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Here, we extract the corresponding items of the confusion matrix and plug in
    the definitions of different evaluation metrics to complete the calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also calculate the AUC, starting with calculating the ROC curve using
    the `pROC` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Running the code generates *Figure 13**.13*, which suggests that the model is
    doing marginally better than random guessing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.13 – Visualizing the ROC curve](img/B18680_13_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.13 – Visualizing the ROC curve
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the AUC, we can call the `auc()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that when we have no preference for precision or recall, we can use the
    F1 score, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: F1 score =  2(Precision × Recall)  _______________  Precision + Recall
  prefs: []
  type: TYPE_NORMAL
- en: The next section discusses a challenging modeling situation when we have an
    imbalanced dataset to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with an imbalanced dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building a logistic regression model using a dataset whose target is a
    binary outcome, it could be the case that the target values are not equally distributed.
    This means that we would observe more non-events (y = 0) than events (y = 1),
    as is often the case in applications such as fraudulent transactions in banks,
    spam/phishing emails for corporate employees, identification of diseases such
    as cancer, and natural disasters such as earthquakes. In these situations, the
    classification performance may be dominated by the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: Such domination can result in misleadingly high accuracy scores, which correspond
    to poor predictive performance. To see this, suppose we are developing a default
    prediction model using a dataset that consists of 1,000 observations, where only
    10 (or 1%) of them are default cases. A naive model would simply predict every
    observation as non-default, resulting in a 99% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: When we encounter an imbalanced dataset, we are often more interested in the
    minority class, which represents the outcome to be detected in a classification
    problem. Since the signal on the minority class is relatively weak, we would need
    to rely on good modeling techniques to recognize a good pattern so that the signal
    can be correctly detected.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple techniques we can use to address the challenge of an imbalanced
    dataset. We will introduce a popular approach called data resampling, which requires
    oversampling and/or undersampling of the original dataset to make the overall
    distribution less imbalanced. Resampling includes oversampling the minority class,
    undersampling the majority class, or using a combination of them, as represented
    by **Synthetic Minority Oversampling TEchnique (SMOTE)**. However, such a remedy
    does not come without risk. Here, oversampling may lead to overfitting due to
    more samples being added to the original dataset, while undersampling may result
    in loss of information due to some majority observations being removed from the
    original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.14* illustrates the process of oversampling the minority class
    (the left panel) and undersampling the majority class (the right panel). Note
    that once the model is built based on the balanced dataset, we would still need
    to calibrate it on a new test set so that it performs well on a new real dataset
    as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.14 – Illustrating the process of oversampling the minority and
    undersampling the majority](img/B18680_13_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.14 – Illustrating the process of oversampling the minority and undersampling
    the majority
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through an exercise to understand how to perform undersampling or
    oversampling in a logistic regression setting.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 13.3 – performing undersampling and oversampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will create two artificial datasets based on undersampling
    and oversampling. We will then assess the performance of the resulting logistic
    regression model using the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the raw dataset into training (70%) and test (30%) sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we randomly sample a set of indexes used to select observations for the
    training set and allocate the rest to the test set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the class ratio in the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that the majority (class `0`) is more than twice the size of
    the minority (class `1`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Separate the training set into a majority set and a minority set based on the
    class label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will then use these two datasets to perform undersampling and oversampling.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Undersample the majority class and combine the undersampled majority class
    with the minority class. Check the resulting class ratio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The class ratio is balanced now. Let us perform oversampling for the minority
    class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Oversample the minority class and combine the oversampled minority class with
    the majority class. Check the resulting class ratio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit logistic regression models on both the undersampled and oversampled datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain the predicted probabilities on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply a threshold of `0.5` to convert the probabilities into binary classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the confusion matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result shows that both models deliver a similar performance using the undersampled
    or oversampled training dataset. Again, we would use another validation dataset,
    which can be taken from the original training set, to calibrate the model parameters
    further so that it performs better on the test set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It turns out that we can also add a lasso or ridge penalty in a logistic regression
    model, as discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Penalized logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the name suggests, a penalized logistic regression model includes an additional
    penalty term in the loss function of the usual logistic regression model. Recall
    that a standard logistic regression model seeks to minimize the negative log-likelihood
    function (or equivalently, maximize the log-likelihood function), defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Q(𝜷) =  1 _ N  ∑ i=1 N − [ y i logp i + (1 − y i)log(1 − p i)]
  prefs: []
  type: TYPE_NORMAL
- en: Here, p i =  1 _ 1 + e −(β 0+β 1x 1 (i)+β 2x 2 (i)+…+β px p (i)) is the predicted
    probability for input x (i), y i is the corresponding target label, and 𝜷 = {
    β 0, β 1, … , β p} are model parameters to be estimated. Note that we now express
    the loss as a function of the coefficient vector as it is directly determined
    by the set of parameters used in the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the penalty term aims to shrink the magnitude of the estimated coefficients,
    we would add it to the loss function so that the penalty terms will be relatively
    small (subject to a tuning hyperparameter, λ). For the case of ridge penalty,
    we would add up the squared coefficients, resulting in the following penalized
    negative log-likelihood function:'
  prefs: []
  type: TYPE_NORMAL
- en: Q ridge(𝜷) = Q(𝜷) + λ ||𝜷|| 2 2 = Q(𝜷) + λ∑ j=1 p β j 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Correspondingly, the penalized negative log-likelihood function using the lasso
    regularization term takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: Q lasso(𝜷) = Q(𝜷) + λ|𝜷| = Q(𝜷) + λ∑ j=1 p | β j|
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, the penalty term has the potential effect of shrinking the magnitude
    of the coefficients toward 0 relative to the original maximum likelihood estimates.
    This can help to prevent overfitting by controlling the complexity of the model
    estimation process. The tuning hyperparameter, λ, controls the amount of shrinkage.
    In particular, a larger λ adds more weight to the penalty term and thus leads
    to more shrinkage effect, while a smaller λ puts less weight on the overall magnitude
    of the estimated coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us illustrate the process of development of penalized logistic regression
    models. The task can be achieved using the `glmnet` package, which supports both
    lasso and ridge penalties. In the following code snippet, we use the first nine
    columns as predictors to model the binary outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set `alpha=1` to enable the lasso penalty. Setting `alpha=0` enables
    the ridge penalty, and setting alpha between `0` and `1` corresponds to the elastic
    net penalty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this procedure would evaluate a sequence of values for the λ hyperparameter,
    giving us an idea of the level of impact on the resulting coefficients based on
    the penalty. In particular, we can plot out the coefficient paths, as shown in
    the following, indicating the resulting coefficients for different values of λ:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Running the code generates *Figure 13**.15*, which suggests that more parameters
    shrink to 0 when λ gets big.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.15 – Visualizing the coefficient paths using lasso penalized logistic
    regression](img/B18680_13_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.15 – Visualizing the coefficient paths using lasso penalized logistic
    regression
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section discusses a more general setting: the multinomial logistic
    regression, a model class for multi-class classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Extending to multi-class classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many problems feature more than two classes. For example, the `AAA`, `AA`, `A`,
    and more like these. Corporate client accounts in a bank are categorized into
    good credit, past due, overdue, doubtful, or loss. Such settings require the multinomial
    logistic regression model, which is a generalization of the binomial logistic
    regression model in the multi-class classification context. Essentially, the target
    variable, y, can take more than two possible discrete outcomes and allows for
    more than two categorical values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that the target variable can take on three values, giving y ∈ {0,1,
    2}. Let us choose class `0` as the pivot value or the baseline. We will model
    the odds of the probabilities of the other categories (classes `1` and `2`) relative
    to this baseline. In other words, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 1) _ p(y = 0)  = e z 1
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 2) _ p(y = 0)  = e z 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the relative ratio of the predicted probabilities for each class
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'p(y = 2) : p(y = 1) : p(y = 0) = e z 2 : e z 1 : 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'We know the following:'
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 2) + p(y = 1) + p(y = 0) = 1
  prefs: []
  type: TYPE_NORMAL
- en: 'So we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 2) =  e z 2 _ e z 2 + e z 1 + 1
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 1) =  e z 1 _ e z 2 + e z 1 + 1
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 0) =  1 _ e z 2 + e z 1 + 1
  prefs: []
  type: TYPE_NORMAL
- en: Again, one of the main assumptions in a multinomial logistic regression model
    is that the log odds consist of a linear combination of the predictor variables.
    This is the same assumption as in binary logistic regression. The corresponding
    interpretation of the coefficients in a multinomial logistic regression will also
    change slightly. In particular, each coefficient now represents the change in
    the log odds of the corresponding category relative to the baseline category for
    a one-unit change in the corresponding predictor variable, while holding all other
    predictors constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rely on the `multinom()` function from the `nnet` package to create
    a multinomial logistic regression model. In the following code snippet, we use
    the previous `mtcars` dataset and convert the `gear` variable into a factor, which
    will serve as the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The frequency count shows a total of three categories in the variable. Next,
    we use `mpg`, `hp`, and `disp` to predict `gear` in a multinomial logistic regression
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The output message suggests that we have a total of eight variables in the model.
    This makes sense since we have four variables (`intercept`, `mpg`, `hp`, and `disp`)
    to model the difference between four-gear and three-gear cars in one submodel,
    and another four variables to model the difference between five-gear and three-gear
    cars in another submodel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us view the summary of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the summary includes two sets of coefficients for the two submodels
    (indexed by `4` and `5`, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let us make predictions using the multinomial logistic regression model
    and calculate the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The result suggests a decent classification performance with only two misclassifications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the world of logistic regression, its theoretical
    underpinnings, and its practical applications. We started by exploring the fundamental
    construct of logistic regression and its comparison with linear regression. We
    then introduced the concept of the sigmoid transformation, a crucial element in
    logistic regression, which ensures the output of our model is bounded between
    `0` and `1`. This section helped us better understand the advantages of logistic
    regression for binary classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we delved into the concept of log odds and odds ratio, two critical components
    of the logistic regression model. Understanding these allowed us to comprehend
    the real-world implications of the model’s predictions and to interpret its parameters
    effectively. The chapter then introduced the CEL, the cost function used in logistic
    regression. Specifically, we discussed how this loss function ensures our model
    learns to predict accurate probabilities for the binary classes.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to evaluating a logistic regression, we learned about various
    metrics, including accuracy, error rate, precision, recall, sensitivity, specificity,
    and AUC. This understanding will allow us to assess the performance of our logistic
    regression model accurately.
  prefs: []
  type: TYPE_NORMAL
- en: An important discussion revolved around the handling of imbalanced datasets,
    a common scenario in real-world data. We understood the effects of data imbalance
    on our model and learned about strategies, such as resampling techniques, to handle
    such situations effectively. Further, we discussed penalized logistic regression
    where we incorporate L1 (lasso) or L2 (ridge) regularization into our logistic
    regression model. This penalization technique helps us prevent overfitting by
    keeping the magnitude of the model weights small and creating simpler models when
    dealing with high-dimensional data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we touched upon multinomial logistic regression, an extension of logistic
    regression used for multi-class classification problems. This part provided insight
    into handling situations where the target variable consists of more than two classes.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we gained an extensive understanding of logistic
    regression, its implementation, and its nuances. This knowledge lays the groundwork
    for diving deeper into more complex classification methods and strategies.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover Bayesian statistics, another major branch
    of statistical modeling.
  prefs: []
  type: TYPE_NORMAL
