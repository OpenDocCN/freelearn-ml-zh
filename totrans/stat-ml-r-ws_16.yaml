- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Logistic Regression in R
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R中的逻辑回归
- en: In this chapter, we will introduce logistic regression, covering its theoretical
    construct, connection with linear regression, and practical implementation. As
    it is an important classification model that is widely used in areas where interpretability
    matters, such as credit risk modeling, we will focus on its modeling process in
    different contexts, along with extensions such as adding regularization to the
    loss function and predicting more than two classes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍逻辑回归，包括其理论结构、与线性回归的联系以及实际应用。由于它是一个在解释性重要的领域（如信用风险建模）中广泛使用的分类模型，我们将关注其在不同情境下的建模过程，以及添加正则化到损失函数和预测多于两个类别的扩展。
- en: By the end of this chapter, you will understand the fundamentals of the logistic
    regression model and its comparison with linear regression, including extended
    concepts such as the `sigmoid` function, odds ratio, and **cross-entropy loss**
    (**CEL**). You will also have grasped the commonly used evaluation metrics in
    the classification setting, as well as enhancements that deal with imbalanced
    datasets and multiple classes in the target variable.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将理解逻辑回归模型的基本原理及其与线性回归的比较，包括扩展概念，如`sigmoid`函数、优势比和**交叉熵损失**（**CEL**）。你还将掌握在分类设置中常用的评估指标，以及处理不平衡数据集和目标变量中多个类别的改进方法。
- en: 'In this chapter, we will cover the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下内容：
- en: Introducing logistic regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍逻辑回归
- en: Comparing logistic regression with linear regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较逻辑回归与线性回归
- en: More on log odds and odds ratio
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多关于对数优势比和优势比的内容
- en: Introducing the cross-entropy loss
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍交叉熵损失
- en: Evaluating a logistic regression model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估逻辑回归模型
- en: Dealing with an imbalanced dataset
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理不平衡数据集
- en: Penalized logistic regression
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惩罚逻辑回归
- en: Extending to multi-class classification
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展到多类分类
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'To run the code in this chapter, you will need to have the latest versions
    of the following packages:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的代码，你需要以下软件包的最新版本：
- en: '`caret` – 6.0.94'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`caret` – 6.0.94'
- en: '`tibble` – 3.2.1'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tibble` – 3.2.1'
- en: '`dplyr` – 1.0.10'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dplyr` – 1.0.10'
- en: '`pROC` – 1.18.2'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pROC` – 1.18.2'
- en: '`nnet` – 7.3.18'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nnet` – 7.3.18'
- en: '`glmnet` – 4.1.7'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glmnet` – 4.1.7'
- en: The versions mentioned along with the packages in the preceding list are the
    latest ones while I am writing this book.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提到的版本以及前述列表中的软件包都是我在写这本书时的最新版本。
- en: All the code and data for this chapter is available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_13/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_13/working.R).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码和数据均可在[https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_13/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_13/working.R)找到。
- en: Introducing logistic regression
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍逻辑回归
- en: Logistic regression is a binary classification model. It is still a linear model,
    but now the output is constrained to be a binary variable, taking the value of
    `0` or `1`, instead of modeling a continuous outcome as in the case of linear
    regression. In other words, we will observe and model the outcome y = 1 or y =
    0\. For example, in the case of credit risk modeling, y = 0 refers to a non-default
    loan application, while y = 1 indicates a default loan.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种二元分类模型。它仍然是一个线性模型，但现在输出被限制为二元变量，取值为`0`或`1`，而不是像线性回归那样模拟连续结果。换句话说，我们将观察和模拟结果y
    = 1或y = 0。例如，在信用风险建模的情况下，y = 0表示非违约贷款申请，而y = 1表示违约贷款。
- en: However, instead of directly predicting the binary outcome, the logistic regression
    model predicts the probability of y taking a specific value, such as P(y = 1).
    The probability of assuming the other category is P(y = 0) = 1 − P(y = 1), since
    the total probability should always sum to `1`. The final prediction would be
    the winner of the two, taking the value of `1` if P(y = 1) > P(y = 0), and `0`
    otherwise. In the credit risk example, P(y = 1) would be interpreted as the probability
    of a loan defaulting.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，逻辑回归模型不是直接预测二元结果，而是预测y取特定值的概率，例如P(y = 1)。假设其他类别的概率P(y = 0) = 1 − P(y = 1)，因为总概率应该始终加起来为`1`。最终的预测将是两个中的胜者，如果P(y
    = 1) > P(y = 0)，则取值为`1`，否则为`0`。在信用风险示例中，P(y = 1)将被解释为贷款违约的概率。
- en: In logistic regression, the term *logistic* is related to *logit*, which refers
    to the log odds. The odds are another way to describe the probability; instead
    of specifying the individual P(y = 1) and P(y = 0), it refers to the ratio of
    P(y = 1) to P(y = 0). Thus the log odds are calculated via logP(y = 1) _ P(y =
    0). Therefore, we can simply use the term *odds* to describe the probability of
    an event happening (y = 1) over the probability of it not occurring (y = 0).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，术语 *logistic* 与 *logit* 有关，它指的是对数几率。几率是描述概率的另一种方式；不是指定个体 P(y = 1) 和 P(y
    = 0)，而是指 P(y = 1) 与 P(y = 0) 的比率。因此，对数几率是通过 logP(y = 1) - P(y = 0) 计算的。因此，我们可以简单地使用术语
    *odds* 来描述事件发生的概率（y = 1）与事件不发生的概率（y = 0）之间的比率。
- en: First, let us look at how the logistic regression model transforms a continuous
    output (as in linear regression) into a probability score, which is a number bounded
    between `0` and `1`.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看逻辑回归模型如何将连续输出（如线性回归中的情况）转换为概率分数，这是一个介于 `0` 和 `1` 之间的数字。
- en: Understanding the sigmoid function
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 sigmoid 函数
- en: The `sigmoid` function is the key ingredient that maps any continuous number
    (from negative infinity to positive infinity) to a probability. Also known as
    the logistic function, the `sigmoid` function is characterized by an S-shaped
    curve, taking any real number as input and mapping it into a score between `0`
    and `1`, which happens to be the range of a valid probability score.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid` 函数是关键成分，它将任何连续数字（从负无穷大到正无穷大）映射到概率。也称为逻辑函数，`sigmoid` 函数的特点是 S 形曲线，它将任何实数作为输入，并将其映射到
    `0` 和 `1` 之间的分数，这恰好是有效概率分数的范围。'
- en: 'The standard `sigmoid` function takes the following form:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的 `sigmoid` 函数具有以下形式：
- en: f(x) =  1 _ 1 + e −x
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: f(x) = 1 / (1 + e^(-x))
- en: Note that this is a nonlinear function. That is, the input values will get disproportionate
    scaling when going through the transformation using this function. It is also
    a continuous function (thus differentiable) and monotone (f(x) will increase as
    x increases), thus enjoying high popularity as the go-to activation function to
    be used at the last layer of a typical neural network model for binary classification
    tasks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一个非线性函数。也就是说，当通过这个函数进行转换时，输入值将得到不成比例的缩放。它也是一个连续函数（因此可导）和单调函数（f(x) 将随着 x
    的增加而增加），因此作为二分类任务中典型神经网络模型最后一层的激活函数，它享有很高的流行度。
- en: 'Let us try to visualize this function. In the following code snippet, we use
    `seq(-10, 10, by = 0.1)` to create a sequence of equally spaced numbers from -10
    to 10, with a step size of `0.1`. For each number, we calculate the corresponding
    output using the `sigmoid` function. Here, we directly pass all the numbers of
    the function, which then calculates all the output in a parallel mode called vectorization.
    Here, vectorization refers to the process of applying an operation to an entire
    vector simultaneously instead of looping over each element one by one as in a
    `for` loop. Finally, we plot the function to show the characteristic S-shaped
    curve of the `sigmoid` function and add the gridlines using the `grid()` function:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试可视化这个函数。在下面的代码片段中，我们使用 `seq(-10, 10, by = 0.1)` 创建一个从 -10 到 10 的等间距数字序列，步长为
    `0.1`。对于每个数字，我们使用 `sigmoid` 函数计算相应的输出。在这里，我们直接传递函数的所有数字，然后以并行模式（称为向量化）计算所有输出。在这里，向量化是指同时将一个操作应用于整个向量，而不是像
    `for` 循环那样逐个元素循环。最后，我们绘制函数以显示 `sigmoid` 函数的特征 S 形曲线，并使用 `grid()` 函数添加网格线：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Running the preceding code generates the output shown in *Figure 13**.1*. The
    plot shows a different level of steepness across the whole domain, where the function
    is more sensitive in the middle region and becomes more saturated at the two extremes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码生成了 *图 13*.1 中所示的输出。该图显示了整个域内不同级别的陡峭程度，其中函数在中间区域更敏感，在两个极端处变得更加饱和。
- en: '![Figure 13.1 – Visualizing the sigmoid function](img/B18680_13_001.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.1 – sigmoid 函数的可视化](img/B18680_13_001.jpg)'
- en: Figure 13.1 – Visualizing the sigmoid function
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1 – sigmoid 函数的可视化
- en: Now that we understand the `sigmoid` function, let us look at the mathematical
    construct of the logistic regression model.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了 `sigmoid` 函数，让我们看看逻辑回归模型的数学结构。
- en: Grokking the logistic regression model
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解逻辑回归模型
- en: The logistic regression model is essentially a linear regression model generalized
    to the setting where the dependent outcome variable is binary. In other words,
    it is a linear regression model that models the log odds of the probability of
    an event.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归模型本质上是一种线性回归模型，它被推广到依赖结果变量为二元的设置中。换句话说，它是一个线性回归模型，用于建模事件发生概率的对数几率。
- en: 'To see this, let us first recall the following linear regression model, where
    we use a total of p features to model the target output variable, z:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，让我们首先回顾以下线性回归模型，其中我们使用总共 p 个特征来建模目标输出变量 z：
- en: z = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: z = β0 + β1 x1 + β2 x2 + … + βp xp
- en: Here, z is interpreted as the log odds, or logit, of the event of y = 1\. We
    are interested in estimating the parameters from β 0 to β p.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，z 被解释为事件 y = 1 的对数几率，或 logit。我们感兴趣的是从 β0 到 βp 的参数估计。
- en: 'Now, we know that the z variable is unbounded, meaning it can vary from negative
    infinity to positive infinity. We need a way to bound this output and convert
    it into a probability score valued between 0 and 1\. This is achieved via an additional
    transformation using the `sigmoid` function, which happens to satisfy all our
    needs. Mathematically, we have the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道 z 变量是无界的，这意味着它可以从负无穷大到正无穷大变化。我们需要一种方法来限制这个输出并将其转换为介于 0 和 1 之间的概率分数。这是通过使用
    `sigmoid` 函数的额外转换来实现的，它恰好满足我们的所有需求。从数学上讲，我们有以下公式：
- en: P(y = 1) =  1 _ 1 + e −z
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: P(y = 1) = 1 / (1 + e^(-z))
- en: 'Plugging in the definition of z gives the full logistic regression model:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 将 z 的定义代入，给出完整的逻辑回归模型：
- en: P(y = 1) =  1 ______________  1 + e −(β 0+β 1x 1+β 2x 2+…+β px p)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: P(y = 1) = 1 / (1 + e^(-β0 - β1x1 - β2x2 - … - βpxp))
- en: Here, P(y = 1) refers to the probability of having a success of y = 1 (this
    is a general statement), and correspondingly, P(y = 0) indicates the probability
    of having a failure.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，P(y = 1) 指的是 y = 1 成功的概率（这是一个一般性陈述），相应地，P(y = 0) 表示失败的概率。
- en: 'Note that we can equivalently express the model as the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以等效地将模型表达如下：
- en: log P(y = 1) _ P(y = 0)  = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: log P(y = 1) / P(y = 0) = β0 + β1 x1 + β2 x2 + … + βp xp
- en: Here, the term log P(y = 1) _ P(y = 0) stands for the log odds.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，术语 log P(y = 1) / P(y = 0) 表示对数几率。
- en: A key change here is the introduction of the `sigmoid` function. This makes
    the relationship between the predictors and the resulting probability no longer
    linear, but sigmoidal instead. To observe the subtlety here, we can look at the
    different regions of the `sigmoid` function across the domain. For example, when
    looking at the region around 0, a small change in the input would result in a
    relatively large change in the resulting probability output. However, the same
    change in the input will cause a very small change in the output when located
    on the two extreme sides of the function. Also, as the input becomes more extreme,
    either toward the negative or positive side, the resulting probability will gradually
    approach 0 or 1\. *Figure 13**.2* recaps the characteristics of the logistic regression
    model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里一个关键的变化是引入了 `sigmoid` 函数。这使得预测变量与结果概率之间的关系不再是线性的，而是 sigmoid 形式的。为了观察这里的微妙之处，我们可以查看
    `sigmoid` 函数在整个定义域中的不同区域。例如，当查看 0 附近的区域时，输入的微小变化会导致结果概率输出的相对较大变化。然而，当输入位于函数的两个极端一侧时，同样的输入变化将导致输出非常小的变化。此外，随着输入变得更加极端，无论是向负方向还是正方向，结果概率将逐渐接近
    0 或 1。*图 13.2* 总结了逻辑回归模型的特点。
- en: '![Figure 13.2 – Summarizing the logistic regression model](img/B18680_13_002.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.2 – 总结逻辑回归模型](img/B18680_13_002.jpg)'
- en: Figure 13.2 – Summarizing the logistic regression model
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 – 总结逻辑回归模型
- en: Note that a logistic regression model shares similar assumptions with the linear
    regression model. Specifically, it assumes that the observations are independent
    of each other, and the target outcome follows a Bernoulli distribution parameterized
    by p – that is, y ∼ Bernoulli(p). Given this, we do not assume a linear regression
    between the output variable and the input predicts; instead, we use the logistic
    link function to transform and introduce nonlinearity to the input variables.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，逻辑回归模型与线性回归模型具有相似的假设。具体来说，它假设观测值之间相互独立，并且目标结果遵循参数为 p 的伯努利分布，即 y ∼ Bernoulli(p)。鉴于这一点，我们并不假设输出变量和输入预测之间存在线性回归；相反，我们使用逻辑链接函数来转换并引入非线性到输入变量中。
- en: The following section further compares logistic regression with linear regression.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分进一步比较逻辑回归与线性回归。
- en: Comparing logistic regression with linear regression
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较逻辑回归与线性回归
- en: In this section, we will focus on a binary credit classification task using
    the *German Credit* dataset, which contains 1,000 observations and 20 columns.
    Each observation denotes a customer who had a loan application from the bank and
    is labeled as either good or bad in terms of credit risk. The dataset is available
    in the `caret` package in R.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于使用*German Credit*数据集的二进制信用分类任务，该数据集包含1,000个观测值和20列。每个观测值表示一位客户，该客户曾向银行申请贷款，并按信用风险被标记为好或坏。该数据集在R的`caret`包中可用。
- en: For our study, we will attempt to predict the target binary variable, `Class`,
    based on `Duration`, and compare the difference in the prediction outcome between
    linear regression and logistic regression. We specifically choose one predictor
    only so that we can visualize and compare the decision boundaries of the resultant
    model in a two-dimensional plot.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的研究，我们将尝试根据`Duration`预测目标二元变量`Class`，并比较线性回归和逻辑回归在预测结果上的差异。我们特意选择一个预测变量，以便在二维图中可视化和比较结果模型的决策边界。
- en: Exercise 13.1 – comparing linear regression with logistic regression
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习13.1 – 比较线性回归与逻辑回归
- en: 'In this exercise, we will demonstrate the advantage of using a logistic regression
    model in producing a probabilistic output compared to the unbounded output using
    a linear regression model:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将展示使用逻辑回归模型产生概率输出的优势，与使用线性回归模型产生的无界输出相比：
- en: 'Load the *German Credit* dataset from the `caret` package. Convert the target
    variable (`Class`) to numeric:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`caret`包中加载*German Credit*数据集。将目标变量（`Class`）转换为数值：
- en: '[PRE1]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we create a new target variable called `Class_num` to map the original
    `Class` variable to `1` if it takes on the value of `"Bad"`, and `0` otherwise.
    This is necessary as both linear regression and logistic regression models cannot
    accept a string-based variable as the target (or predictor).
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们创建一个新的目标变量`Class_num`，将原始的`Class`变量映射到`1`，如果它取值为`"Bad"`，否则为`0`。这是必要的，因为线性回归和逻辑回归模型都不能接受基于字符串的变量作为目标（或预测变量）。
- en: 'Build a linear regression model to regress `Class_num` against `Duration`:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个线性回归模型，将`Class_num`对`Duration`进行回归：
- en: '[PRE2]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we use the `lm()` function to build the linear regression model and `coefficients()`
    to extract the model coefficients, including the intercept and slope.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`lm()`函数构建线性回归模型，并使用`coefficients()`提取模型系数，包括截距和斜率。
- en: 'Visualize the prediction and the target:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化预测和目标：
- en: '[PRE3]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we plot the observed target variable as a scatter plot and use the `geom_abline()`
    function to plot the model as a straight line based on the estimated slope and
    intercept.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们将观察到的目标变量作为散点图绘制，并使用`geom_abline()`函数根据估计的斜率和截距绘制模型为直线。
- en: Running the preceding code generates *Figure 13**.3*.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行前面的代码生成*图13.3*。
- en: '![Figure 13.3 – Visualizing the linear regression model](img/B18680_13_003.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图13.3 – 可视化线性回归模型](img/B18680_13_003.jpg)'
- en: Figure 13.3 – Visualizing the linear regression model
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.3 – 可视化线性回归模型
- en: Since all target values are `0` or `1`, we can think of the predictions as probabilities
    valued between `0` and `1`. However, as we zoom out, the problem of having an
    unbounded probability would surface, as shown in the following steps.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有目标值都是`0`或`1`，我们可以将预测视为介于`0`和`1`之间的概率值。然而，当我们放大视图时，无界概率的问题就会显现出来，如下面的步骤所示。
- en: 'Re-plot the graph by zooming out to a wide domain of `(-30, 120)` for the *x*
    axis and `(-0.5, 1.5)` for the *y* axis:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将*x*轴的范围扩大到`(-30, 120)`和*y*轴的范围扩大到`(-0.5, 1.5)`来重新绘制图形：
- en: '[PRE4]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we enlarged the range of possible values for the *x* axis and *y* axis
    using the `xlim()` and `ylim()` functions.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`xlim()`和`ylim()`函数扩大了*x*轴和*y*轴可能值的范围。
- en: Running the preceding code generates the output shown in *Figure 13**.4*, which
    shows that the predicted values are outside the range of `[0,1]` when the value
    of `Duration` becomes more extreme, a situation called extrapolation beyond the
    observed range of values. This means that the predicted probabilities would be
    smaller than 0 or bigger than 1, which is obviously an invalid output. This calls
    for a generalized linear regression model called logistic regression, where the
    response will follow a logistic, S-shaped curve based on the transformation of
    the `sigmoid` function.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行前面的代码生成 *图 13.4* 中所示的输出，这表明当 `Duration` 的值变得极端时，预测值超出了 `[0,1]` 的范围，这种情况称为超出观察值范围的外推。这意味着预测概率将小于
    0 或大于 1，这显然是一个无效的输出。这需要一种称为逻辑回归的广义线性回归模型，其中响应将根据 `sigmoid` 函数的转换遵循逻辑、S 形曲线。
- en: '![Figure 13.4 – Visualizing the linear regression model with extended range](img/B18680_13_004.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.4 – 可视化扩展范围的线性回归模型](img/B18680_13_004.jpg)'
- en: Figure 13.4 – Visualizing the linear regression model with extended range
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 – 可视化扩展范围的线性回归模型
- en: 'Build a logistic regression model using the `glm()` function:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `glm()` 函数构建逻辑回归模型：
- en: '[PRE5]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The result shows the estimated intercept and slope, along with the residual
    deviance, a measure of goodness of fit for the logistic regression model.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示了逻辑回归模型的估计截距和斜率，以及残差偏差，这是衡量模型拟合优度的一个指标。
- en: 'Plot the estimated logistic curve on the previous plot:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在之前的图形上绘制估计的逻辑曲线：
- en: '[PRE6]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Running the preceding code will generate the output shown in *Figure 13**.5*,
    which suggests a slight deviation between the linear regression line and the logistic
    regression curve.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行前面的代码将生成 *图 13.5* 中所示的输出，这表明线性回归线和逻辑回归曲线之间存在轻微的偏差。
- en: '![Figure 13.5 – Visualizing the logistic regression curve](img/B18680_13_005.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.5 – 可视化逻辑回归曲线](img/B18680_13_005.jpg)'
- en: Figure 13.5 – Visualizing the logistic regression curve
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 – 可视化逻辑回归曲线
- en: Again, we can zoom out the figure and focus on the difference when going beyond
    the observed range of possible values in the dataset.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以放大图形并关注当超出数据集中可能值的观察范围时的差异。
- en: 'Plot the logistic curve within a wider range of values beyond the observed
    range:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更宽的值范围内绘制逻辑曲线，超出观察值范围：
- en: '[PRE7]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we first extract the coefficients for the logistic regression model, followed
    by generating a sequence of values for the input and the corresponding output
    using the `sigmoid` function transformation. Lastly, we plot the logistic curve
    together with the linear fit in the same plot with the observed data.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们首先提取逻辑回归模型的系数，然后使用 `sigmoid` 函数转换生成输入值的序列及其相应的输出。最后，我们在同一图形中与观察数据一起绘制逻辑曲线和线性拟合。
- en: Running the preceding code generates the output shown in *Figure 13**.6*, which
    shows that the logistic regression curve gets gradually saturated as the input
    value becomes more extreme. In addition, all values are now bounded in the range
    of `[0,1]`, making it a valid candidate for interpretation as probabilities instead
    of an unbounded value.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行前面的代码生成 *图 13.6* 中所示的输出，这表明随着输入值的极端化，逻辑回归曲线逐渐饱和。此外，所有值现在都被限制在 `[0,1]` 的范围内，这使得它成为将解释为概率而不是无界值的有效候选。
- en: '![Figure 13.6 – Visualizing the logistic regression model with extended range](img/B18680_13_006.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 13.6 – 可视化扩展范围的逻辑回归模型](img/B18680_13_006.jpg)'
- en: Figure 13.6 – Visualizing the logistic regression model with extended range
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 – 可视化扩展范围的逻辑回归模型
- en: The next section looks at how to make predictions using a logistic regression
    model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨如何使用逻辑回归模型进行预测。
- en: Making predictions using the logistic regression model
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用逻辑回归模型进行预测
- en: As discussed in the previous section, the direct predictions from a logistic
    regression model take the form of probabilities valued between `0` and `1`. To
    convert them into binary predictions, we could take the most probable prediction
    by rounding the probability using a threshold of `0.5`. For example, if the predicted
    probability is P(y = 1) = 0.8, the rounding operation will lead to a final binary
    prediction of y = 1\. On the other hand, if P(y = 1) = 0.3, rounding will result
    in y = 0.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，逻辑回归模型的直接预测形式是介于`0`和`1`之间的概率。要将它们转换为二元预测，我们可以通过使用`0.5`的阈值对概率进行四舍五入来实现。例如，如果预测概率是P(y
    = 1) = 0.8，四舍五入操作将导致最终的二元预测y = 1。另一方面，如果P(y = 1) = 0.3，四舍五入将导致y = 0。
- en: Let us go through the following exercise to understand how to perform predictions
    using the logistic regression model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下练习来了解如何使用逻辑回归模型进行预测。
- en: Exercise 13.2 – performing predictions using the logistic regression model
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习13.2 – 使用逻辑回归模型进行预测
- en: 'We have seen how to perform predictions using the explicit sigmoid transformation
    after extracting the slope and intercept of the logistic regression model. In
    this exercise, we will explore a more convenient approach using the `predict()`
    function:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何通过提取逻辑回归模型的斜率和截距来执行预测。在这个练习中，我们将探索使用`predict()`函数的更便捷的方法：
- en: 'Generate a sequence of `Duration` values ranging from `5` to `80` with a step
    size of `2` and predict the corresponding probabilities for the sequence using
    the `predict()` function based on the previous logistic regression model:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个从`5`到`80`的`Duration`值序列，步长为`2`，并使用之前的逻辑回归模型通过`predict()`函数预测该序列的对应概率：
- en: '[PRE8]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Here, we use the `seq()` function to create the equally spaced vector and store
    it in a `tibble` object called `pred_df`. We then use `predict()` to predict the
    corresponding probabilities by specifying `type="response"`.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，我们使用`seq()`函数创建等间隔向量，并将其存储在名为`pred_df`的`tibble`对象中。然后我们使用`predict()`通过指定`type="response"`来预测相应的概率。
- en: 'Visualize the predicted probabilities together with the raw data:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测概率与原始数据一起可视化：
- en: '[PRE9]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code will generate the following output:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码将生成以下输出：
- en: '![Figure 13.7 – Visualizing the predicted probabilities](img/B18680_13_007.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图13.7 – 可视化预测的概率](img/B18680_13_007.jpg)'
- en: Figure 13.7 – Visualizing the predicted probabilities
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7 – 可视化预测的概率
- en: 'Convert the probabilities to binary outcomes using the `round()` function:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`round()`函数将概率转换为二元结果：
- en: '[PRE10]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we round the predicted probabilities using a default threshold of `0.5`.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，我们使用默认阈值`0.5`对预测概率进行四舍五入。
- en: 'Add the binary outcomes as green points to the previous graph:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将二元结果作为绿色点添加到之前的图中：
- en: '[PRE11]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Running the preceding code generates the following output, which suggests that
    all predicted probabilities above `0.5` are converted to `1`, and those below
    `0.5` are converted to `0`.
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行上述代码将生成以下输出，表明所有预测概率高于`0.5`的都被转换为`1`，而低于`0.5`的都被转换为`0`。
- en: '![Figure 13.8 – Visualizing the predicted binary outcomes](img/B18680_13_008.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图13.8 – 可视化预测的二元结果](img/B18680_13_008.jpg)'
- en: Figure 13.8 – Visualizing the predicted binary outcomes
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8 – 可视化预测的二元结果
- en: The next section discusses the log odds further.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将进一步讨论对数赔率。
- en: More on log odds and odds ratio
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于对数赔率和赔率比率的更多内容
- en: 'Recall that the odds refer to the ratio of the probability of an event happening
    over its complement:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，赔率是指事件发生的概率与其补数的比率：
- en: odds =  probability of event happening   ________________________   probability
    of event not happening  =  p _ 1 − p  =  P(y = 1) _ P(y = 0)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: odds =  事件发生的概率   ________________________   事件不发生的概率  =  p _ 1 − p  =  P(y
    = 1) _ P(y = 0)
- en: 'Here, the probability is calculated as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，概率的计算如下：
- en: p = P(y = 1) =  1 _ 1 + e −z
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: p = P(y = 1) =  1 _ 1 + e −z
- en: 1 − p = 1 −  1 _ 1 + e −z  =  e −z _ 1 + e −z
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 1 − p = 1 −  1 _ 1 + e −z  =  e −z _ 1 + e −z
- en: 'Plugging in the definition of p and 1 − p gives us the following:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将p和1 − p的定义代入，我们得到以下结果：
- en: odds =  p _ 1 − p  = e z
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: odds =  p _ 1 − p  = e z
- en: 'Instead of directly working with the odds, we often use the log odds or logit.
    This term is typically modeled as a linear combination of predictors in a logistic
    regression model via the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常不直接处理赔率，而是使用对数赔率或logit。这个术语通常通过以下方式在逻辑回归模型中作为预测器的线性组合进行建模：
- en: log P(y = 1) _ P(y = 0)  = z = β 0 + β 1 x 1 + … + β p x p
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: log P(y = 1) _ P(y = 0)  = z = β 0 + β 1 x 1 + … + β p x p
- en: Here, we can interpret each coefficient β j as the expected change in the log
    odds for a one-unit increase in the jth predictor x j, while keeping all other
    predictors constant. This equation essentially says that the log odds of the target
    value y being `1` is linearly related to the input variables.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以将每个系数 β j 解释为在保持所有其他预测变量不变的情况下，jth 预测变量 x j 单位增加时对对数优势的预期变化。这个方程本质上表明，目标值
    y 为 `1` 的对数优势与输入变量呈线性关系。
- en: 'Now suppose x i is a binary input variable, making x i = 1 or 0\. We can calculate
    the odds of x i = 1 as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设 x i 是一个二元输入变量，使得 x i = 1 或 0。我们可以如下计算 x i = 1 的优势：
- en: p 1 _ 1 − p 1  = e z 1
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: p 1 _ 1 − p 1  = e z 1
- en: 'This measures the chance of an event for x i = 1, over the chance of a non-event.
    Similarly, we can calculate the odds of x i = 0 as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这衡量了 x i = 1 事件发生的概率与不发生事件的概率。同样，我们可以如下计算 x i = 0 的优势：
- en: p 0 _ 1 − p 0  = e z 0
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: p 0 _ 1 − p 0  = e z 0
- en: This measures the chance of an event for x i = 0 over the chance of a non-event.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这衡量了 x i = 0 事件发生的概率与不发生事件的概率。
- en: 'We can then calculate the odds ratio of x i, which is the ratio of the odds
    x i = 1 to the odds of x i = 0:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以计算 x i 的优势比，这是 x i = 1 的优势与 x i = 0 的优势之比：
- en: p 1 _ 1 − p 1 _  p 0 _ 1 − p 0  =  e β 0+β 1x 1+…β i*1+…+β px p ___________ e β 0+β 1x 1+…β i*0+…+β px p 
    = e β i
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: p 1 _ 1 − p 1 _  p 0 _ 1 − p 0  =  e β 0+β 1x 1+…β i*1+…+β px p ___________ e β 0+β 1x 1+…β i*0+…+β px p 
    = e β i
- en: Here, e β i measures the quantified impact of the binary input variable, x i,
    on the odds of the outcome y being `1`, while all other input variables remain
    unchanged. This gives us a way to measure the impact of any predictor in a logistic
    regression model, covering both categorical and numerical input variables.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，e β i 衡量了二元输入变量 x i 对结果 y 为 `1` 的优势的量化影响，同时保持所有其他输入变量不变。这为我们提供了一种衡量逻辑回归模型中任何预测变量影响的方法，包括分类和数值输入变量。
- en: For categorical input variables, we may use gender (`0` for male and `1` for
    female) to predict whether insurance is purchased (`1` for yes and `0` for no).
    We set the base categorical as `0` for male. If the estimated coefficient β gender
    = 0.2 for gender, its odds ratio is calculated as e 0.2 ≈ 1.22\. Therefore, the
    odds of female customers purchasing the insurance is 1.22 times the odds of their
    male counterparts purchasing the insurance, assuming all other variables remain
    unchanged.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类输入变量，我们可以使用性别（男性为 `0`，女性为 `1`）来预测是否购买保险（购买为 `1`，不购买为 `0`）。我们将男性的基准类别设置为
    `0`。如果性别估计系数 β gender = 0.2，其优势比计算为 e 0.2 ≈ 1.22。因此，女性客户购买保险的优势是男性客户购买保险优势的 1.22
    倍，假设所有其他变量保持不变。
- en: For numerical input variables, we may use age to predict whether insurance is
    purchased. There is no need to set the base category in this case. If the estimated
    coefficient β age = 0.3, the corresponding odds ratio is calculated as e 0.3 ≈
    1.35\. This means that the odds of a client purchasing is 1.35 times the odds
    of similar people who are one year younger, assuming *ceteris paribus*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值输入变量，我们可以使用年龄来预测是否购买保险。在这种情况下，无需设置基准类别。如果估计的系数 β age = 0.3，相应的优势比计算为 e 0.3
    ≈ 1.35。这意味着客户的购买概率是比一年年轻的人高 1.35 倍，假设其他条件相同（*ceteris paribus*）。
- en: 'Note that we can calculate the log odds using the predicted probabilities,
    as shown in the following code snippet:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们可以使用预测概率来计算对数优势，如下面的代码片段所示：
- en: '[PRE12]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The next section introduces more on the loss function of the logistic regression
    model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将介绍更多关于逻辑回归模型损失函数的内容。
- en: Introducing the cross-entropy loss
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入交叉熵损失
- en: 'The binary CEL, also called the **log loss**, is often used as the cost function
    in logistic regression. This is the loss that the logistic regression model will
    attempt to minimize by moving the parameters. This function takes the predicted
    probabilities and the corresponding targets as the input and outputs a scalar
    score, indicating the goodness of fit. For a single observation with a target
    of y i and predicted probability of p i, the loss is calculated as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 二元交叉熵损失，也称为 **log loss**，通常用作逻辑回归中的成本函数。这是逻辑回归模型将通过移动参数来尝试最小化的损失。这个函数将预测概率和相应的目标作为输入，并输出一个标量分数，表示拟合优度。对于具有目标
    y i 和预测概率 p i 的单个观察值，损失计算如下：
- en: Q i(y i, p i) = − [ y i logp i + (1 − y i)log(1 − p i)]
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Q i(y i, p i) = − [ y i logp i + (1 − y i)log(1 − p i)]
- en: 'Summing up all individual losses gives the total binary CEL:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有个别损失相加得到总二元交叉熵损失：
- en: Q(y, p) =  1 _ N  ∑ i N Q i =  1 _ N  ∑ i=1 N − [ y i logp i + (1 − y i)log(1
    − p i)]
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Q(y, p) =  1 _ N  ∑ i N Q i =  1 _ N  ∑ i=1 N − [ y i logp i + (1 − y i)log(1
    − p i)]
- en: The binary CEL function is a suitable choice for binary classification problems
    because it heavily penalizes confident but incorrect predictions. For example,
    as p i approaches *0* or *1*, the resulting CEL will go to infinity if the prediction
    is incorrect. This property thus encourages the learning process to output probabilities
    that are close to the true probabilities of the targets.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 二元 CEL 函数是二元分类问题的合适选择，因为它对自信但错误的预测进行重罚。例如，当 p i 接近 *0* 或 *1* 时，如果预测错误，则 resulting
    CEL 将趋于无穷大。因此，这一特性鼓励学习过程输出接近目标真实概率的概率。
- en: More generally, we use the CEL to model a classification problem with two or
    more classes in the target variable. For the i th observation, *x* i, the classification
    function would produce a probability output, denoted as p i,k = f( *x* i; *w*),
    to indicate the likelihood of belonging to the k th class. When we have a classification
    task with a total of C classes, the CEL for *x* i is defined as Q i(*w*) = − ∑ k=1 C  y i,k
    log(p i,k), which essentially sums across all classes. Again, y i,k = 1 if the
    target label for the i th observation belongs to the k th class, and y i,k = 0
    otherwise.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，我们使用 CEL 来模拟目标变量中有两个或更多类别的分类问题。对于第 i 个观察 *x* i，分类函数将产生一个概率输出，表示为 p i,k
    = f( *x* i; *w*)，以指示属于第 k 个类别的可能性。当我们有一个包含总共 C 个类别的分类任务时，*x* i 的 CEL 定义为 Q i(*w*)
    = − ∑ k=1 C  y i,k log(p i,k)，这实际上是对所有类别的求和。再次强调，如果第 i 个观察的目标标签属于第 k 个类别，则 y i,k
    = 1，否则 y i,k = 0。
- en: The summation means that the class-wise evaluation (i.e., the term y i,k log(p i,k))
    is performed for all classes and summed together to produce the total cost for
    the i th observation. For each observation, there are a total of C predictions
    corresponding to the respective probability of belonging to each class. The CEL
    thus aggregates the matrix of predicted probabilities by summing them into a single
    number. In addition, the result is negated to produce a positive number, since
    log(x) is negative if x is a probability between `0` and `1`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 求和意味着对每个类别执行类内评估（即，项 y i,k log(p i,k)），并将它们相加以产生第 i 个观察的总成本。对于每个观察，总共有 C 个预测与属于每个类别的相应概率相对应。因此，CEL
    通过将它们求和成一个单一的数字来聚合预测概率矩阵。此外，结果取反以产生一个正数，因为当 x 是介于 `0` 和 `1` 之间的概率时，log(x) 是负数。
- en: Note that the target label in the CEL calculation needs to be one-hot encoded.
    This means that a single categorical label is converted into an array of binary
    numbers that contains `1` for the class label and `0` otherwise. For example,
    for a digit image on number `8`, instead of passing the original class as the
    target output, the resulting one-hot-encoded target `[0, 0, 0, 0, 0, 0, 0, 1,
    0, 0]` would be used, where the 8th position is activated (i.e., hot) and the
    rest disabled. The target array would also have the same length as the probability
    output array, whose elements correspond to each of the classes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，CEL 计算中的目标标签需要是一维编码。这意味着单个分类标签被转换成一个包含 `1` 的二进制数字数组，表示类别标签，其他为 `0`。例如，对于一个数字
    `8` 的图像，而不是传递原始类别作为目标输出，将使用结果的一维编码目标 `[0, 0, 0, 0, 0, 0, 0, 1, 0, 0]`，其中第 8 位被激活（即，热），其余被禁用。目标数组也将与概率输出数组具有相同的长度，其元素对应于每个类别。
- en: 'Intuitively, we would hope for the predicted probability for the correct class
    to be close to `1` and for the incorrect class to be `0`. That is, the loss should
    increase as the predicted probabilities diverge from the actual class label. The
    CEL is designed to follow this intuition. Specifically, we can look at the following
    four scenarios for the i th observation:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，我们希望预测的正确类别的概率接近 `1`，而错误类别的概率为 `0`。也就是说，随着预测概率偏离实际类别标签，损失应该增加。CEL 设计是为了遵循这种直觉。具体来说，我们可以查看第
    i 个观察的以下四种情况：
- en: When the target label belongs to the k th class (i.e., y i,k = 1) and the predicted
    probability for the k th class is very strong (i.e., p i,k ≈ 1), the cost should
    be low.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当目标标签属于第 k 个类别（即，y i,k = 1）且第 k 个类别的预测概率非常强（即，p i,k ≈ 1）时，成本应该很低。
- en: When the target label belongs to the k th class (i.e., y i,k = 1) and the predicted
    probability for the k th class is very weak (i.e., p i,k ≈ 0), the cost should
    be high.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当目标标签属于第 k 个类别（即，y i,k = 1）且第 k 个类别的预测概率非常弱（即，p i,k ≈ 0）时，成本应该很高。
- en: When the target label does not belong to the k th class (i.e., y i,k = 0) and
    the predicted probability for the k th class is very strong (i.e., p i,k ≈ 1),
    the cost should be high.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the target label does not belong to the k th class (i.e., y i,k = 0) and
    the predicted probability for the k th class is very weak (i.e., p i,k ≈ 0), the
    cost should be low.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To calculate the CEL, we first calculate the weighted sum between the vector
    of binary labels and the vector of predicted probabilities across all classes
    for each observation. The results of all observations are added together, followed
    by a minus sign to reverse the cost to a positive number. The CEL is designed
    to match the intuition for the cost: the cost would be low when the prediction
    and the target closely match, and high otherwise. In other words, to calculate
    the total cost, we would simply sum individual costs, leading to Q(w) = − ∑ i=1 N  ∑ k=1 C  y i,k
    log(p i,k). *Figure 13**.9* summarizes the preceding discussion on the CEL.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.9 – Illustrating the CEL](img/B18680_13_009.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9 – Illustrating the CEL
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Note that the last two scenarios do not contribute at all to the loss calculation
    since the target value is equal to 0; any value multiplied by 0 becomes 0\. Also,
    observe that the predicted probabilities of a specific observation for these two
    classes need to add up to 1, making it sufficient to only focus on the predicted
    probability of one class (mostly class `1`).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: The next section introduces how to evaluate a logistic regression model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a logistic regression model
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are multiple metrics we can use to evaluate a logistic regression model.
    These are the metrics we use to determine the goodness of fit (over the test set),
    which needs to be differentiated from the CEL we use to train the model (over
    the training set).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list provides the commonly used metrics:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy rate**: This is the proportion of the number of correctly predicted
    observations made by the model out of the count of all observations. Since a correct
    prediction can be either a true positive or a true negative, the accuracy is calculated
    by summing up the true positives and true negatives and dividing the total number
    of observations.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error rate**: This is the proportion of incorrectly predicted observations
    made by the model over the total observations. An incorrect prediction can be
    a false positive or a false negative. It is calculated as *1 - accuracy rate*;
    that is, the error rate is the complement of the accuracy rate. In other words,
    it is calculated as *(false positives + false negatives) /* *total observations*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: Precision is the proportion of correct positive predictions
    among all positive predictions. This measure essentially tells us out of all predicted
    positive instances, how many of them are correct. Thus it indicates the model’s
    accuracy in predicting positive observations and is calculated as *true positives
    / (true positives + false positives)*. In the denominator, we note that among
    all positive instances, some are true positives and the rest are false positives.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: Recall refers to the proportion of actual positive instances that
    the model correctly predicts. Also called sensitivity or **true positive rate**
    (**TPR**), the recall measures the model’s ability to detect positive observations.
    It is calculated as *true positives / (true positives + false negatives)*, where
    the formula differs in the denominator compared with precision.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity**: Also called **true negative rate** (**TNR**), specificity
    measures the proportion of actual negative instances correctly predicted by the
    model. This is the opposite of sensitivity, which focuses on the model’s ability
    to capture the true positives. In other words, specificity measures the model’s
    ability to identify negative instances or non-events correctly. It is calculated
    as *true negatives / (true negatives +* *false positives)*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0` and `1`, indicating the degree of separability between two classes. A perfect
    model with 100% correct predictions has an AUC of `1`, while a model with completely
    wrong predictions has an AUC of `0`. A model that performs random guesses (choosing
    `0` or `1` with 50% probability) corresponds to an AUC of `0.5`, suggesting no
    class separation capacity.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we would exercise the principle of parsimony upon assessing two models
    with equally good evaluation metrics. The principle of parsimony says that if
    two competing models provide a similar level of fit to the data, the one with
    fewer input variables should be picked, thus preferring simplicity over complexity.
    The underlying assumption is that the most accurate model is not necessarily the
    best model.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.10* describes the process of laying out the confusion matrix that
    captures the prediction results in different scenarios, along with the details
    on calculating the aforementioned evaluation metrics.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.10 – Illustrating the confusion matrix and common evaluation metrics
    for binary classification tasks](img/B18680_13_010.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10 – Illustrating the confusion matrix and common evaluation metrics
    for binary classification tasks
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Note that we will only be able to calculate these metrics after selecting a
    threshold to cut off predicted probabilities. Specifically, an observation with
    a predicted probability greater than the cutoff threshold will be classified as
    positive, and negative otherwise.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Precision and recall usually have an inverse relationship with respect to the
    adjustment of the classification threshold. Reviewing both precision and recall
    is useful for cases where there is a huge imbalance in the target variable’s values.
    As precision and recall are two related but different metrics, which should we
    optimize for?
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: To answer this question, we need to assess the relative impact of making a false
    negative prediction. This is measured by the false negative rate, which is the
    opposite (or complement) of recall. As *Figure 13**.11* suggests, failing to spot
    a spam email is less risky than missing a fraudulent transaction or a positive
    cancer patient. We should aim to optimize for precision (so that the model’s predictions
    are more precise and targeted) for the first case and recall (so that we minimize
    the chance of missing a potentially positive case) for the second case.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.11 – Three cases with different impacts of having a false negative
    prediction](img/B18680_13_011.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Figure 13.11 – Three cases with different impacts of having a false negative
    prediction
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: As for the AUC, or area under the ROC curve, there is no need to select a specific
    threshold as it is calculated by evaluating a sequence of thresholds from `0`
    to `1`. The ROC curve plots the sensitivity on the *y* axis and *1 - specificity*
    on the *x* axis. This also corresponds to plotting TPR versus 1-TNR, or TPR versus
    FPR. As the classification threshold goes up, FPR goes down, leading to a leftward
    movement of the curve.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: A perfect binary classifier has an AUC score of `1`. This means that FPR, or
    *1 – specificity*, is `0`. That is, there is no false positive, and all negative
    cases are not predicted as positive. In addition, the sensitivity, or TPR, is
    `1`, meaning all positive cases are predicted as positive correctly.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.12* illustrates three different AUC curves. The topmost curve
    (in green) corresponds to a better model due to the highest AUC. Both models,
    represented by the green and red curves, perform better than random guessing,
    as indicated by the straight off-diagonal line in blue.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.12 – Illustrating three different AUC curves](img/B18680_13_012.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Figure 13.12 – Illustrating three different AUC curves
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing the previous exercise, we can now calculate the corresponding evaluation
    metrics. First, we use the trained logistic regression model to score all observations
    in the training set and obtain the corresponding probabilities using the `predict()`
    function and setting `type="response"`, as shown in the following code snippet.
    Note that we need to pass in a DataFrame with the corresponding feature names
    as input to the model:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we set a single cutoff threshold (`0.5`, in this case) to convert the
    predicted probabilities into the corresponding binary outcomes using the `ifelse()`
    function:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With the binary outcomes and true target labels, we can obtain the confusion
    matrix as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, the confusion matrix provides a breakdown of the correct and incorrect
    classifications from the model. Within the confusion matrix, the top-left cell
    means true negatives, the top-right cell means false positives, the bottom-left
    cell means false negatives, and the bottom-right cell means true positives.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the confusion matrix, we can calculate the evaluation metrics as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, we extract the corresponding items of the confusion matrix and plug in
    the definitions of different evaluation metrics to complete the calculations.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also calculate the AUC, starting with calculating the ROC curve using
    the `pROC` package:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Running the code generates *Figure 13**.13*, which suggests that the model is
    doing marginally better than random guessing.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.13 – Visualizing the ROC curve](img/B18680_13_013.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 13.13 – Visualizing the ROC curve
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the AUC, we can call the `auc()` function:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Note that when we have no preference for precision or recall, we can use the
    F1 score, defined as follows:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: F1 score =  2(Precision × Recall)  _______________  Precision + Recall
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: The next section discusses a challenging modeling situation when we have an
    imbalanced dataset to begin with.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with an imbalanced dataset
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building a logistic regression model using a dataset whose target is a
    binary outcome, it could be the case that the target values are not equally distributed.
    This means that we would observe more non-events (y = 0) than events (y = 1),
    as is often the case in applications such as fraudulent transactions in banks,
    spam/phishing emails for corporate employees, identification of diseases such
    as cancer, and natural disasters such as earthquakes. In these situations, the
    classification performance may be dominated by the majority class.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Such domination can result in misleadingly high accuracy scores, which correspond
    to poor predictive performance. To see this, suppose we are developing a default
    prediction model using a dataset that consists of 1,000 observations, where only
    10 (or 1%) of them are default cases. A naive model would simply predict every
    observation as non-default, resulting in a 99% accuracy.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: When we encounter an imbalanced dataset, we are often more interested in the
    minority class, which represents the outcome to be detected in a classification
    problem. Since the signal on the minority class is relatively weak, we would need
    to rely on good modeling techniques to recognize a good pattern so that the signal
    can be correctly detected.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple techniques we can use to address the challenge of an imbalanced
    dataset. We will introduce a popular approach called data resampling, which requires
    oversampling and/or undersampling of the original dataset to make the overall
    distribution less imbalanced. Resampling includes oversampling the minority class,
    undersampling the majority class, or using a combination of them, as represented
    by **Synthetic Minority Oversampling TEchnique (SMOTE)**. However, such a remedy
    does not come without risk. Here, oversampling may lead to overfitting due to
    more samples being added to the original dataset, while undersampling may result
    in loss of information due to some majority observations being removed from the
    original dataset.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 13**.14* illustrates the process of oversampling the minority class
    (the left panel) and undersampling the majority class (the right panel). Note
    that once the model is built based on the balanced dataset, we would still need
    to calibrate it on a new test set so that it performs well on a new real dataset
    as well.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.14 – Illustrating the process of oversampling the minority and
    undersampling the majority](img/B18680_13_014.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: Figure 13.14 – Illustrating the process of oversampling the minority and undersampling
    the majority
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Let us go through an exercise to understand how to perform undersampling or
    oversampling in a logistic regression setting.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 13.3 – performing undersampling and oversampling
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will create two artificial datasets based on undersampling
    and oversampling. We will then assess the performance of the resulting logistic
    regression model using the confusion matrix:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the raw dataset into training (70%) and test (30%) sets:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, we randomly sample a set of indexes used to select observations for the
    training set and allocate the rest to the test set.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the class ratio in the training set:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The result shows that the majority (class `0`) is more than twice the size of
    the minority (class `1`).
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Separate the training set into a majority set and a minority set based on the
    class label:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We will then use these two datasets to perform undersampling and oversampling.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Undersample the majority class and combine the undersampled majority class
    with the minority class. Check the resulting class ratio:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The class ratio is balanced now. Let us perform oversampling for the minority
    class.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Oversample the minority class and combine the oversampled minority class with
    the majority class. Check the resulting class ratio:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit logistic regression models on both the undersampled and oversampled datasets:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Obtain the predicted probabilities on the test set:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Apply a threshold of `0.5` to convert the probabilities into binary classes:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Calculate the confusion matrix:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The result shows that both models deliver a similar performance using the undersampled
    or oversampled training dataset. Again, we would use another validation dataset,
    which can be taken from the original training set, to calibrate the model parameters
    further so that it performs better on the test set.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It turns out that we can also add a lasso or ridge penalty in a logistic regression
    model, as discussed in the next section.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Penalized logistic regression
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the name suggests, a penalized logistic regression model includes an additional
    penalty term in the loss function of the usual logistic regression model. Recall
    that a standard logistic regression model seeks to minimize the negative log-likelihood
    function (or equivalently, maximize the log-likelihood function), defined as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Q(𝜷) =  1 _ N  ∑ i=1 N − [ y i logp i + (1 − y i)log(1 − p i)]
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Here, p i =  1 _ 1 + e −(β 0+β 1x 1 (i)+β 2x 2 (i)+…+β px p (i)) is the predicted
    probability for input x (i), y i is the corresponding target label, and 𝜷 = {
    β 0, β 1, … , β p} are model parameters to be estimated. Note that we now express
    the loss as a function of the coefficient vector as it is directly determined
    by the set of parameters used in the model.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the penalty term aims to shrink the magnitude of the estimated coefficients,
    we would add it to the loss function so that the penalty terms will be relatively
    small (subject to a tuning hyperparameter, λ). For the case of ridge penalty,
    we would add up the squared coefficients, resulting in the following penalized
    negative log-likelihood function:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Q ridge(𝜷) = Q(𝜷) + λ ||𝜷|| 2 2 = Q(𝜷) + λ∑ j=1 p β j 2
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Correspondingly, the penalized negative log-likelihood function using the lasso
    regularization term takes the following form:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Q lasso(𝜷) = Q(𝜷) + λ|𝜷| = Q(𝜷) + λ∑ j=1 p | β j|
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, the penalty term has the potential effect of shrinking the magnitude
    of the coefficients toward 0 relative to the original maximum likelihood estimates.
    This can help to prevent overfitting by controlling the complexity of the model
    estimation process. The tuning hyperparameter, λ, controls the amount of shrinkage.
    In particular, a larger λ adds more weight to the penalty term and thus leads
    to more shrinkage effect, while a smaller λ puts less weight on the overall magnitude
    of the estimated coefficients.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us illustrate the process of development of penalized logistic regression
    models. The task can be achieved using the `glmnet` package, which supports both
    lasso and ridge penalties. In the following code snippet, we use the first nine
    columns as predictors to model the binary outcome:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here, we set `alpha=1` to enable the lasso penalty. Setting `alpha=0` enables
    the ridge penalty, and setting alpha between `0` and `1` corresponds to the elastic
    net penalty.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this procedure would evaluate a sequence of values for the λ hyperparameter,
    giving us an idea of the level of impact on the resulting coefficients based on
    the penalty. In particular, we can plot out the coefficient paths, as shown in
    the following, indicating the resulting coefficients for different values of λ:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Running the code generates *Figure 13**.15*, which suggests that more parameters
    shrink to 0 when λ gets big.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.15 – Visualizing the coefficient paths using lasso penalized logistic
    regression](img/B18680_13_015.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: Figure 13.15 – Visualizing the coefficient paths using lasso penalized logistic
    regression
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'The next section discusses a more general setting: the multinomial logistic
    regression, a model class for multi-class classification.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Extending to multi-class classification
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many problems feature more than two classes. For example, the `AAA`, `AA`, `A`,
    and more like these. Corporate client accounts in a bank are categorized into
    good credit, past due, overdue, doubtful, or loss. Such settings require the multinomial
    logistic regression model, which is a generalization of the binomial logistic
    regression model in the multi-class classification context. Essentially, the target
    variable, y, can take more than two possible discrete outcomes and allows for
    more than two categorical values.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that the target variable can take on three values, giving y ∈ {0,1,
    2}. Let us choose class `0` as the pivot value or the baseline. We will model
    the odds of the probabilities of the other categories (classes `1` and `2`) relative
    to this baseline. In other words, we have the following:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 1) _ p(y = 0)  = e z 1
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 2) _ p(y = 0)  = e z 2
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the relative ratio of the predicted probabilities for each class
    is as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'p(y = 2) : p(y = 1) : p(y = 0) = e z 2 : e z 1 : 1'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'We know the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 2) + p(y = 1) + p(y = 0) = 1
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'So we have the following:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 2) =  e z 2 _ e z 2 + e z 1 + 1
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 1) =  e z 1 _ e z 2 + e z 1 + 1
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: p(y = 0) =  1 _ e z 2 + e z 1 + 1
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Again, one of the main assumptions in a multinomial logistic regression model
    is that the log odds consist of a linear combination of the predictor variables.
    This is the same assumption as in binary logistic regression. The corresponding
    interpretation of the coefficients in a multinomial logistic regression will also
    change slightly. In particular, each coefficient now represents the change in
    the log odds of the corresponding category relative to the baseline category for
    a one-unit change in the corresponding predictor variable, while holding all other
    predictors constant.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rely on the `multinom()` function from the `nnet` package to create
    a multinomial logistic regression model. In the following code snippet, we use
    the previous `mtcars` dataset and convert the `gear` variable into a factor, which
    will serve as the target variable:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The frequency count shows a total of three categories in the variable. Next,
    we use `mpg`, `hp`, and `disp` to predict `gear` in a multinomial logistic regression
    model:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The output message suggests that we have a total of eight variables in the model.
    This makes sense since we have four variables (`intercept`, `mpg`, `hp`, and `disp`)
    to model the difference between four-gear and three-gear cars in one submodel,
    and another four variables to model the difference between five-gear and three-gear
    cars in another submodel.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us view the summary of the model:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As expected, the summary includes two sets of coefficients for the two submodels
    (indexed by `4` and `5`, respectively).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let us make predictions using the multinomial logistic regression model
    and calculate the confusion matrix:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The result suggests a decent classification performance with only two misclassifications.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we delved into the world of logistic regression, its theoretical
    underpinnings, and its practical applications. We started by exploring the fundamental
    construct of logistic regression and its comparison with linear regression. We
    then introduced the concept of the sigmoid transformation, a crucial element in
    logistic regression, which ensures the output of our model is bounded between
    `0` and `1`. This section helped us better understand the advantages of logistic
    regression for binary classification tasks.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Next, we delved into the concept of log odds and odds ratio, two critical components
    of the logistic regression model. Understanding these allowed us to comprehend
    the real-world implications of the model’s predictions and to interpret its parameters
    effectively. The chapter then introduced the CEL, the cost function used in logistic
    regression. Specifically, we discussed how this loss function ensures our model
    learns to predict accurate probabilities for the binary classes.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to evaluating a logistic regression, we learned about various
    metrics, including accuracy, error rate, precision, recall, sensitivity, specificity,
    and AUC. This understanding will allow us to assess the performance of our logistic
    regression model accurately.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: An important discussion revolved around the handling of imbalanced datasets,
    a common scenario in real-world data. We understood the effects of data imbalance
    on our model and learned about strategies, such as resampling techniques, to handle
    such situations effectively. Further, we discussed penalized logistic regression
    where we incorporate L1 (lasso) or L2 (ridge) regularization into our logistic
    regression model. This penalization technique helps us prevent overfitting by
    keeping the magnitude of the model weights small and creating simpler models when
    dealing with high-dimensional data.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we touched upon multinomial logistic regression, an extension of logistic
    regression used for multi-class classification problems. This part provided insight
    into handling situations where the target variable consists of more than two classes.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we gained an extensive understanding of logistic
    regression, its implementation, and its nuances. This knowledge lays the groundwork
    for diving deeper into more complex classification methods and strategies.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover Bayesian statistics, another major branch
    of statistical modeling.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
