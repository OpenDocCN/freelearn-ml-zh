<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer278">
			<h1 id="_idParaDest-253"><em class="italic"><a id="_idTextAnchor252"/>Chapter 16</em>: Bringing Models into Production with MLOps</h1>
			<p>In the previous chapter, we looked into model interoperability using ONNX, hardware optimization using FPGAs, and the integration of trained models into other services and platforms. So far, you have learned how to implement each step in an end-to-end machine learning pipeline with data cleansing, preprocessing, labeling, experimentation, model training, optimization, and deployment. In this chapter, we will connect the bits and pieces from all the previous chapters to integrate and automate them in a build and release pipeline. We will reuse all these concepts to build a version-controlled, reproducible, automated ML training and deployment process as a <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI/CD</strong>) pipeline in Azure. In analogy to the <strong class="bold">DevOps</strong> methodology in software development, we will refer to this topic as <strong class="bold">MLOps</strong> in ML.</p>
			<p>First, we will take a look at how to produce reproducible builds, environments, and deployments for ML projects. We will cover version control for code, as well as the versioning/snapshotting of data and building artifacts.</p>
			<p>Next, we will learn how to automatically test our code and validate our code quality with a focus on ML projects. To do this, we will see how unit, integration, and end-to-end tests can be adapted for ensuring good quality of training data and ML models.</p>
			<p>Finally, you will build your own MLOps pipeline. First, you will learn how to set up Azure DevOps as your orchestration and coordination layer for MLOps, and then you will implement a build (CI) and release (CD) pipeline.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Ensuring reproducible builds and deployments</li>
				<li>Validating the code, data, and models</li>
				<li>Building an end-to-end MLOps pipeline</li>
			</ul>
			<h1 id="_idParaDest-254"><a id="_idTextAnchor253"/>Technical requirements</h1>
			<p>In this chapter, we will use the following Python libraries and versions to create MLOps pipelines in Azure DevOps:</p>
			<ul>
				<li><strong class="source-inline">azureml-core 1.34.0</strong></li>
				<li><strong class="source-inline">azureml-sdk</strong><strong class="source-inline"> 1.34.0</strong></li>
				<li><strong class="source-inline">pandas 1.3.3</strong></li>
				<li><strong class="source-inline">tensorflow 2.6.0</strong></li>
				<li><strong class="source-inline">pytest 7.1.1</strong></li>
				<li><strong class="source-inline">pytest-cov 3.0.0</strong></li>
				<li><strong class="source-inline">mock 4.0.3</strong></li>
				<li><strong class="source-inline">tox 3.24.5</strong></li>
			</ul>
			<p>Most of the scripts and pipelines discussed in this chapter need to be scheduled to execute in Azure DevOps.</p>
			<p>All code examples in this chapter can be found in the GitHub repository for this book: <a href="https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter16">https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter16</a>.</p>
			<h1 id="_idParaDest-255"><a id="_idTextAnchor254"/>Ensuring reproducible builds and deployments</h1>
			<p>DevOps has many <a id="_idIndexMarker1777"/>different meanings but is usually about enabling rapid and high-quality deployments when the source code changes. One way of achieving high-quality operational code is by guaranteeing reproducible and predictable builds. While it seems obvious that the compiled binary will look and behave similarly for application development with only a few minor configuration changes, the same is not true for the development of ML pipelines.</p>
			<p>ML engineers and data<a id="_idIndexMarker1778"/> scientists face many problems that make building reproducible deployments very difficult:</p>
			<ul>
				<li>The development process is often performed in notebooks and so it is not always linear.</li>
				<li>Refactoring notebook code often breaks older notebooks.</li>
				<li>There are mismatching library versions and drivers.</li>
				<li>Source data can be changed or modified.</li>
				<li>Non-deterministic<a id="_idIndexMarker1779"/> optimization techniques can lead to completely different outputs.</li>
			</ul>
			<p>We discussed interactive notebooks (such as Jupyter, Databricks, Zeppelin, and Azure notebooks) in the first few chapters of this book, and you have probably seen them in a lot of places when implementing ML models and data pipelines. While interactive notebooks have the great advantage of executing cells to validate blocks of models iteratively, they also often encourage a user to run cells in a non-linear order. The main benefit of using a notebook environment becomes a pain when trying to productionize or automate a pipeline.</p>
			<p>The second issue that is <a id="_idIndexMarker1780"/>common in ML is ensuring that the correct drivers, libraries, and runtimes are installed. While it is easy to run a small linear model based on scikit-learn in Python 2, it makes a big difference for deep learning models if the<a id="_idIndexMarker1781"/> deployed CUDA, cuDNN, libgpu, Open MPI, Horovod, TensorFlow, PyTorch, and similar libraries match the versions from development. Containerization via Docker or similar technologies helps to build reproducible environments, but it's not straightforward to use them throughout the experimentation, training, optimization, and deployment processes.</p>
			<p>Another challenge faced by data scientists is that often data changes over time. Either a new batch of data is added during development or data is cleaned, written back to the storage, and reused as input for other experiments. Data, due to its variability in format, scale, and quality, can be one of the biggest issues when producing reproducible models. Versioning data similar to version-controlling code is essential, not only for reproducible builds but also for auditing purposes.</p>
			<p>One more challenge that makes reproducible ML builds difficult is that they often contain an optimization step, as discussed in <a href="B17928_11_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 11</em></a>, <em class="italic">Hyperparameter Tuning and Automated Machine Learning</em>. While optimization is an essential step for ML (for example, for model selection, training, hyperparameter tuning, or stacking), it can add non-deterministic behavior to the training process. Let's find out how we can fight these problems step by step.</p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor255"/>Version-controlling your code</h2>
			<p>Version-controlling <a id="_idIndexMarker1782"/>source code is a best practice, not only for software development but also for data engineering, data science, and machine learning As an organization, you have the option to set up your own internal source code repository or use an <a id="_idIndexMarker1783"/>external service. <strong class="bold">GitHub</strong>, <strong class="bold">GitLab</strong>, <strong class="bold">Bitbucket</strong>, and <strong class="bold">Azure DevOps</strong> are popular services for managing source <a id="_idIndexMarker1784"/>control repositories. The benefit of<a id="_idIndexMarker1785"/> these services is that<a id="_idIndexMarker1786"/> some of them offer additional features, such as support for CI workers and <a id="_idIndexMarker1787"/>workflows. We will use the CI runner integration of Azure DevOps later in this chapter.</p>
			<p>Using version control for your code is more important than the version control system you use. Yes, <strong class="bold">Git</strong> works pretty well, but so does <strong class="bold">Mercurial</strong> and <strong class="bold">Subversion</strong> (<strong class="bold">SVN</strong>). For our example MLOps pipeline, we will use Git as it is the most widely used and supported. It's essential that you make yourself familiar with the basic workflows of the version control system that you choose. You should <a id="_idIndexMarker1788"/>be able to create commits and branches, submit <strong class="bold">pull requests</strong> (<strong class="bold">PRs</strong>), comment on and review requests, and merge changes.</p>
			<p>The power of version-controlling source code is to document changes. On each such change, we want to trigger an automatic pipeline that tests your changes, validates the code quality, and when successful and merged, trains your model and automatically deploys it to staging or production. Your commit and PR history will not only become a source of documenting changes but also triggering, running, and documenting whether these changes were tested and ready for production.</p>
			<p>In order to work effectively with version control, it is essential that you try to move business logic out of your interactive notebooks as soon as possible. Notebooks store the code and output of each cell in custom data formats â€“ for example, serialized to JSON files. This makes it very difficult to review changes in the serialized notebook. A good trade-off is to follow a hybrid approach, where you first test your code experiments in a notebook and gradually move the logic to a module that is imported into each file. Using auto-reload plugins, you can make sure that these modules get automatically reloaded whenever you change the logic, without needing to restart your kernel.</p>
			<p>Moving code from notebooks to modules will not only make your code reusable for all other experiments (no need to copy utility functions from notebook to notebook) but it will also make your commits much more readable. When multiple people change a few lines of code in a massive<a id="_idIndexMarker1789"/> JSON file (that's how your notebook environment stores the code and output of every cell), then the <a id="_idIndexMarker1790"/>changes made to the file will be almost impossible to review and merge. However, if those changes are made in a module (a separate file containing only executable code), then these changes will be a lot easier to read, review, reason about, and merge.</p>
			<p>Before we continue looking into the versioning of training data, this would be a good opportunity to brush up on your Git skills, create a (private) repository, and experiment with your version control features.</p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor256"/>Registering snapshots of your data</h2>
			<p>Your ML model is the<a id="_idIndexMarker1791"/> output of your training code and your training data. If we version-control the training source code to <a id="_idIndexMarker1792"/>create reproducible builds, we also need to version the training data. While it sounds reasonable to check small, text, non-binary, and non-compressed files into the version control system together with your source code, it doesn't sound reasonable for large binary or compressed data sources. In this section, we will discuss a solution on how to deal with the latter.</p>
			<p>Let's re-iterate the idea of reproducible builds: regardless of when the training is executed â€“ it could run today, or a year from now â€“ the output should be identical. This means that any modifications to the training data should create a new version of the dataset, and training should use a specific version of the dataset. We differentiate between operational transactional data and historical data. While the former is usually stateful and mutable, the latter is often immutable. Sometimes, we also see a mix of both, for example, mutable historical event data.</p>
			<p>When working with mutable data (for example, an operational database storing customer information), we need to create snapshots before pulling in the data for training. For ML, it's easier to use full snapshots than incremental snapshots, as each snapshot contains the complete dataset. While incremental snapshots are often created to save costs, full snapshots can also be stored cost-efficiently using column-compressed data formats and scalable blob storage systems (such as Azure Blob storage), even if you have multiple TBs of data.</p>
			<p>When dealing with historical or immutable data, we don't usually need to create full snapshots, since the data is partitionedâ€”that is, organized in directories where directories correspond to the values of the partition key. Historical data is often partitioned by processing date <a id="_idIndexMarker1793"/>or time, such as the time when the data ingestion was executed. Date or time partitions make it easier to point your training pipelines to a specific range of partitions instead of pointing to a set of files directly.</p>
			<p>There are multiple ways to take snapshots of your training data. However, when working with the Azure Machine Learning workspace, it is recommended to wrap your data in Azure Machine Learning datasets, as discussed in <a href="B17928_04_ePub.xhtml#_idTextAnchor071"><em class="italic">Chapter 4</em></a>, <em class="italic">Ingesting Data and Managing Datasets</em>. This makes it easy to take data snapshots or version your data. When processing <a id="_idIndexMarker1794"/> and modifying data in Azure Machine Learning, you should make a habit of incrementing the dataset's version. In addition, you should pass a specific version of the dataset when fetching the data in the training script.</p>
			<p>Whenever you pass parameters to your training scripts, it is helpful to parameterize the pipeline using deterministic placeholders. Parameters such as dates and timestamps should be created in the pipeline scheduling step rather than in the code itself. This ensures you can always re-run failed pipelines with historical parameters, and it will create the same outputs.</p>
			<p>So, make sure your input data is registered and versioned and your output data is registered and parameterized. This takes a bit of fiddling to set up properly but is worth it for the whole project life cycle.</p>
			<h2 id="_idParaDest-258"><a id="_idTextAnchor257"/>Tracking your model metadata and artifacts</h2>
			<p>Moving your code to<a id="_idIndexMarker1795"/> modules, checking<a id="_idIndexMarker1796"/> it into version control, and versioning your data will help to create reproducible models. If you are building an ML model for an enterprise, or you are building a model for your start-up, knowing which model version is deployed and with which dataset it was trained is essential. This is relevant for auditing, debugging, or resolving customers' inquiries about the predictions of your service.</p>
			<p>We have seen in the previous chapters that a few simple steps can enable you to track model artifacts and model versions in a model registry. Versioning the model artifacts is an essential step for continuous deployments. The model consists of artifacts, files that are generated while training, and metadata. Model assets contain the definition of the model architecture, parameters, and weights, whereas model metadata contains the dataset, commit hash, experiment and run IDs, and more of the training run.</p>
			<p>Another important consideration<a id="_idIndexMarker1797"/> is to specify and version-control the seed for your random number generators. During most training and optimization steps, algorithms will use pseudo-random numbers based on a random seed to shuffle data and parameter choices. So, in order to produce the same model after running your code multiple times, you need to ensure that you set a fixed random seed for every operation that uses randomized behaviors.</p>
			<p>Once you understand the <a id="_idIndexMarker1798"/>benefit of source code version control for your application code and versioning your datasets, you will understand that it makes a lot of sense for your trained models as well. However, instead of readable code, you now store the model artifacts (binaries that contain the model weights and architecture) and metadata for each model.</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor258"/>Scripting your environments and deployments</h2>
			<p>Automating every operation that you <a id="_idIndexMarker1799"/>perform during the training and deployment process will increase the initial time of development, testing, and deployment, but ultimately save you a ton of time when these steps have to be executed again. The benefit of cloud services, such as Azure Machine Learning and Azure DevOps, is that they provide you with all the necessary tools to automate every step of the development and deployment process.</p>
			<p>If you haven't already done so, you should start organizing your Python in virtual environments. Popular options are <strong class="source-inline">requirements</strong>, <strong class="source-inline">pyenv</strong>, <strong class="source-inline">Pipenv</strong>, or <strong class="source-inline">conda</strong> files that help you to track development and test dependencies. This helps you to specify dependencies as part of the virtual environment and not rely on global packages or the global state of the development machine.</p>
			<p>Azure DevOps and other CI runners will help you define dependencies because running integration tests will install all the defined dependencies automatically during the test. This is usually one of the first steps in a CI pipeline. Then, whenever you check in new code or tests to your version control system, the CI pipeline is executed and also tests the installation of your environment automatically. Therefore, it is good practice to add integration tests to all of your modules, so that you can never miss a package definition in your environment. If you miss declaring a dependency, the CI build will fail.</p>
			<p>Next, you also need to script, configure, and automate all your infrastructure. If you have followed the previous chapters in this book, you might have figured out by now why we did all the infrastructure automation and deployments through an authoring environment in Python. If you have scripted these steps previously, you can simply run and parameterize these scripts in your CI pipelines.</p>
			<p>If you run a CI pipeline<a id="_idIndexMarker1800"/> that generates a model, you most likely want to spin up a fresh Azure Machine Learning cluster for this job so you don't interfere with other releases, build pipelines, or experimentation. While this level of automation is very hard to achieve on on-premises infrastructures, you can do this easily in the cloud. Many services, such as YAML files in Azure Machine Learning, ARM templates in Azure, or Terraform from HashiCorp, provide full control over your infrastructure and configuration.</p>
			<p>The last part is to automate deployments within Azure Machine Learning. Performing deployments through code doesn't take much longer than through the UI but it gives you the benefit of a repeatable and reproducible deployment script. You will often be confronted to do the same operation in multiple ways; for example, deploying an ML model from Azure Machine Learning via the CLI, Python SDK, YAML, the Studio, or a plugin in Azure DevOps. It is recommended to pick whatever works for you, stick with one way of doing things, and perform all automation and deployments in the same way. Having said this, using Python as the scripting language for deployments and checking your deployment code in version control is a good and popular choice.</p>
			<p>The key to reproducible builds and CI pipelines is to automate the infrastructure and environment from the beginning. In the cloud, especially in Azure, this should be very easy as most tools and services can be automated through the SDK. The Azure Machine Learning team put a ton of work into the SDK so that you can automate each step â€“from ingestion to deployment â€“ from within Python. </p>
			<p>Next, let's take a look into the validation of code and assets to ensure the code and trained model work as expected.</p>
			<h1 id="_idParaDest-260"><a id="_idTextAnchor259"/>Validating the code, data, and models</h1>
			<p>When implementing a CI/CD pipeline, you need to make sure you have all the necessary tests in place<a id="_idIndexMarker1801"/> to deploy your newly created code with ease and confidence. Once you are running a CI or CI/CD pipeline, the power of automated tests will become immediately visible. It not <a id="_idIndexMarker1802"/>only helps you to detect failures in your code, but it also helps<a id="_idIndexMarker1803"/> to detect future issues in the whole ML process, including the environment setup, build dependencies, data requirements, model initialization, optimization, resource requirements, and deployment.</p>
			<p>When implementing a validation pipeline for our ML process, we can take inspiration from traditional software development principles (for example, unit testing, integration testing, and end-to-end testing). We can translate these techniques directly to steps during the ML process, such as input data, models, and the application code of the scoring service. Let's understand how we can adapt these testing techniques for ML projects.</p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor260"/>Testing data quality with unit tests</h2>
			<p>Unit tests are essential to writing <a id="_idIndexMarker1804"/>good-quality code. A unit test aims to test the smallest unit of code (a function) independently of all other code. Each test should only test one thing at a time and should run and finish quickly. Many application developers run unit tests either every time they change the code, or at least every time they submit a new commit to version control.</p>
			<p>Here is a simple example of a unit test written in Python using the <strong class="source-inline">unittest</strong> module provided by the standard library in Python 3:</p>
			<p class="source-code">import unittest</p>
			<p class="source-code">class TestStringMethods(unittest.TestCase):</p>
			<p class="source-code">Â Â def test_upper(self):</p>
			<p class="source-code">Â Â Â Â self.assertEqual('foo'.upper(), 'FOO')</p>
			<p>As you can see in the code snippet, we run a single function and test whether the outcome matches a predefined variable. We can add more tests as additional methods to the test class.</p>
			<p>In Python and many other languages, we differentiate between test frameworks and libraries that help us to author and organize tests, and libraries to execute tests and create reports. <strong class="source-inline">pytest</strong> and <strong class="source-inline">tox</strong> are great libraries to execute tests; <strong class="source-inline">unittest</strong> and <strong class="source-inline">mock</strong> help you to author and organize your tests in classes and mock out dependencies on other functions.</p>
			<p>When you write code for your ML model, you will also find units of code that can, and probably should, be unit tested on every commit. However, ML engineers, data engineers, and data scientists <a id="_idIndexMarker1805"/>now deal with another source of errors in their development cycle: the data. Therefore, it is a good idea to rethink what unit tests could mean in terms of data quality.</p>
			<p>Once you get the hang of it, you will quickly understand the power of using unit tests to measure data quality. You can interpret feature dimensions of your input data as a single testable unit and write tests to ensure each unit is fulfilling the defined requirements. This is especially important when new training data is collected over time and it is planned to retrain the model in the future. In such a case, we always want to ensure that the data is clean and matches our assumptions before we start the training process.</p>
			<p>Here are some examples of what your unit tests can test in the training data:</p>
			<ul>
				<li>Number of unique/distinct values</li>
				<li>Correlation of feature dimensions</li>
				<li>Skewness</li>
				<li>Minimum and maximum values</li>
				<li>Most common value</li>
				<li>Values containing zero or undefined values</li>
			</ul>
			<p>Let's put this into practice and write a unit test that ensures that the minimum value of a dataset is <strong class="source-inline">0</strong>. This simple test will ensure that your CI/CD pipeline will fail if your dataset contains unexpected values:</p>
			<p class="source-code">import unittest</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">class TestDataFrameStats(unittest.TestCase):</p>
			<p class="source-code">Â Â def setUp(self):</p>
			<p class="source-code">Â Â Â Â # initialize and load df</p>
			<p class="source-code">Â Â Â Â self.df = pd.DataFrame(data={'data': [0,1,2,3]})</p>
			<p class="source-code">Â Â def test_min(self):</p>
			<p class="source-code">Â Â Â Â self.assertEqual(self.df.min().values[0], 0)</p>
			<p>In the preceding code, we use <strong class="source-inline">unittest</strong> to organize the tests in multiple functions within the same class. Each <a id="_idIndexMarker1806"/>class corresponds to a specific data source, and in each class, we can test all feature dimensions. Once set up, we can install <strong class="source-inline">pytest</strong> and simply execute it from the command line to run the test.</p>
			<p>In Azure DevOps, we can set up <strong class="source-inline">pytest</strong> or <strong class="source-inline">tox</strong> as a simple step in our build pipeline. For a build pipeline step, we can simply add the following block to the <strong class="source-inline">azure-pipelines.yml</strong> file:</p>
			<p class="source-code">- displayName: 'Testing data quality'</p>
			<p class="source-code">Â Â script: |</p>
			<p class="source-code">Â Â Â Â pip install pytest pytest-cov</p>
			<p class="source-code">Â Â Â Â pytest tests --doctest-modules</p>
			<p>In the preceding code, we first installed <strong class="source-inline">pytest</strong> and <strong class="source-inline">pytest-cov</strong> to create a <strong class="source-inline">pytest</strong> coverage report. In the next line, we executed the tests, which will now use the dataset and compute all the statistical requirements. If the requirements are not met according to the tests, the tests will fail, and we will see these errors in the UI for this build. This adds protection to your ML pipeline, as you can now make sure no unforeseen problems with the training data make it into the release without you noticing.</p>
			<p>Unit testing is essential for software development, and so is unit testing for data. As with testing in general, it will take some initial effort to be implemented, which doesn't immediately turn into value. However, you will soon see that having these tests in place will give you good peace of mind when deploying new models faster, as it will catch errors with the training data at<a id="_idIndexMarker1807"/> build time and not when the model is already deployed.</p>
			<h2 id="_idParaDest-262"><a id="_idTextAnchor261"/>Integration testing for ML</h2>
			<p>In software development, integration testing <a id="_idIndexMarker1808"/>verifies individual so-called components often made up of multiple smaller units. You normally use a test driver to run the test suite and mock or stub other components in your tests that you don't want to test. In graphical applications, you could test a simple visual component while imitating the modules the component is interacting with. In the backend code, you test your business logic module while mocking all dependent persistence, configuration, and UI components.</p>
			<p>Integration tests, therefore, help you to detect critical errors when combining multiple units together, without the expense of scaffolding the whole application infrastructure. They are placed between unit testing and end-to-end testing and are typically run per commit, branch, or PR on the CI runtime.</p>
			<p>In ML, we can use the concept of integration testing to test the training process of an ML pipeline. This can help your training run to find potential bugs and errors during the build phase. Integration testing allows you to test whether your model, pre-trained weights, a piece of test data, and optimizer can yield a successful output. However, different algorithms require different integration tests to test whether something is wrong in the training process.</p>
			<p>When training a <strong class="bold">DNN</strong> model, you<a id="_idIndexMarker1809"/> can verify a lot of aspects of the model with integration tests. Here is a non-exhaustive list of steps to verify:</p>
			<ul>
				<li>Weights initialization</li>
				<li>Default loss</li>
				<li>Zero input</li>
				<li>Single batch fitting</li>
				<li>Default activations</li>
				<li>Default gradients</li>
			</ul>
			<p>Using a similar list, you can easily identify and catch cases where all activations are capped at the maximum value in a forward pass, or when all gradients are <strong class="source-inline">0</strong> during a backward pass. Theoretically, you can run any experiment, test, or check you would do manually before working with a fresh dataset and your model, continuously in your CI runtime. So, any time your model gets retrained or fine-tuned, these checks run automatically in the background.</p>
			<p>A more general assumption is that when training a regression model, the default mean should be close to the mean prediction value. When training a classifier, you could test the distribution of the output<a id="_idIndexMarker1810"/> classes. In both cases, you can detect issues due to modeling, data, or initialization error already, before starting the expensive training and optimization process.</p>
			<p>In terms of the runner and framework, you can choose the same libraries as used for unit testing because, in this case, integration testing differs only in the components that are tested and the way they are combined. Therefore, choosing <strong class="source-inline">unittest</strong>, <strong class="source-inline">mock</strong>, and <strong class="source-inline">pytest</strong> is a popular choice to scaffold your integration testing pipeline.</p>
			<p>Integration testing is essential for application development and for running end-to-end ML pipelines. It will save you a lot of time and lowers your operational costs, if you can detect and avoid such problems automatically.</p>
			<h2 id="_idParaDest-263"><a id="_idTextAnchor262"/>End-to-end testing using Azure Machine Learning</h2>
			<p>In end-to-end testing, we<a id="_idIndexMarker1811"/> want to verify all components involved in a request to a deployed and fully functional service. To do so, we need to deploy <a id="_idIndexMarker1812"/>the complete service all together. End-to-end testing is critical for catching errors that are triggered only when combining all the components together and running the service in a staging or testing environment without mocking any of the other components.</p>
			<p>In ML deployments, there are multiple steps where a lot of things can go very wrong if not tested properly. Let's discard the easy ones where we need to make sure that the environment is correctly installed and configured. A more critical piece of the deployment in Azure Machine Learning is the code for the application logic itself: the scoring file. There is no easy way to test the scoring file, the format of the request, and the output together without a proper end-to-end test.</p>
			<p>As you might imagine, end-to-end tests are usually quite expensive to build and operate. First, you need to write code and deploy applications to only test the code, which requires extra work, effort, and costs. However, this is the only way to truly test the scoring endpoint in a production-like end-to-end environment.</p>
			<p>The good thing is that by using<a id="_idIndexMarker1813"/> Azure Machine Learning deployments, end-to-end testing becomes so easy that it should be part of everyone's pipeline. If the model allows it, we could even do a no-code deployment where we don't specify the deployment target. If this is not <a id="_idIndexMarker1814"/>possible, we can specify an Azure Container Image as a compute target and deploy the model independently. This means taking the code from the previous <a id="_idIndexMarker1815"/>chapter, wrapping it in a Python script, and including it as a step in the build process.</p>
			<p>End-to-end testing is usually complicated and expensive. However, with Azure Machine Learning and automated deployments, a model deployment and sample request could just be part of the build pipeline.</p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor263"/>Continuous profiling of your model</h2>
			<p>Model profiling is an<a id="_idIndexMarker1816"/> important step during your experimentation and training phase. This will give you a good understanding of the resources your model will require when used as a scoring service. This is critical information for designing and choosing a properly sized inference environment.</p>
			<p>Whenever training and optimization processes run continuously, the model requirements and profile evolve over time. If you use optimization for model stacking or automated ML, your resulting models could grow bigger to fit the new data. So, it is good to keep an eye on your model requirements to account for deviations from your initial resource choices.</p>
			<p>Luckily, Azure Machine Learning provides a model profiling interface that you can feed with a model, scoring function, and test data. It will instantiate an inferencing environment for you, start the scoring service, run the test data through the service, and track the resource utilization. Let's bring all the pieces together and set up an end-to-end MLOps pipeline.</p>
			<h1 id="_idParaDest-265"><a id="_idTextAnchor264"/>Building an end-to-end MLOps pipeline</h1>
			<p>In this section, we want to <a id="_idIndexMarker1817"/>set up an end-to-end MLOps pipeline. All required training code should be checked into version control, and the datasets and model will be versioned as well. We want to trigger a CI pipeline to build the code and retrain the model when the code or training data changes. Through unit and integration tests we will ensure that the training and inferencing code works in isolation and that the data and model fulfill all requirements and don't deviate from our initial assumptions. Therefore, the CI pipeline will be responsible for automatic continuous code builds, training, and tests.</p>
			<p>Next, we will trigger the CD pipeline whenever a new model version is ready. This will deploy the model and inferencing configuration to a staging environment and run the end-to-end tests. After the tests have been completed successfully, we automatically want to deploy the model to production. Therefore, the CD pipeline will be responsible for the automatic deployment.</p>
			<p>The separation of the pipeline into CI and CD parts makes it easy to decouple the process of building assets from deploying assets. However, you can also combine both parts into a single CI/CD pipeline, and so build, train, optimize, and deploy it all with a single pipeline. It's up to you and your organization how to model the CI and CD components of your pipeline, and how to set up any triggers and (manual) approvals. You can choose between either deploying every commit to production or deploying a number of commits each day or week after manual approval.</p>
			<p>In this section, we will use Azure DevOps to author and execute the CI/CD pipelines and, therefore, to set up triggers, run<a id="_idIndexMarker1818"/> the build, training, and testing steps, and handle the deployment of the trained model. Azure DevOps has built-in functionalities to automate the end-to-end CI/CD process. In general, it lets you run pieces of <a id="_idIndexMarker1819"/>functionality, called tasks, grouped together in pipelines on a compute infrastructure that you define. You can either run pipelines that are triggered automatically through a new commit in your version control system or trigger them through a new revision of a build artifact or a button, for example, for semi-automated deployments. The former is<a id="_idIndexMarker1820"/> called a <strong class="bold">code pipeline</strong> and refers to CI, while the latter<a id="_idIndexMarker1821"/> is called a <strong class="bold">release pipeline</strong> and refers to CD.</p>
			<p>Let's start setting up an Azure DevOps project.</p>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor265"/>Setting up Azure DevOps</h2>
			<p>Azure DevOps will be the <a id="_idIndexMarker1822"/>container for authoring, configuring, triggering, and executing all our CI/CD pipelines. It provides useful abstractions to work with version-controlled resources, such as code repositories and a connection to Azure and the Azure Machine Learning workspace, and lets you collaboratively access runners, pipelines, and build artifacts.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout"><strong class="bold">Azure DevOps</strong> refers to the managed Azure DevOps Services accessible via <a href="https://dev.azure.com/">https://dev.azure.com/</a>. There also exists an <a id="_idIndexMarker1823"/>on-premises offering for similar<a id="_idIndexMarker1824"/> CI/CD integration capabilities called <strong class="bold">Azure DevOps Server</strong>, which was<a id="_idIndexMarker1825"/> formerly known as Visual Studio<strong class="bold"> Team Foundation Server</strong> (<strong class="bold">TFS</strong>).</p>
			<p>As a first step, we are going to set up the Azure DevOps workspace, so that we can author and execute Azure MLOps pipelines. Let's start by setting up the organization and projects.</p>
			<h3>Organization and projects</h3>
			<p>First, you need to set up your organization. An organization is a workspace to manage similar projects and <a id="_idIndexMarker1826"/>collaborate with a group of people. You can create an organization by either using your Microsoft account, GitHub account, or even connecting to <strong class="bold">Azure Active Directory</strong> (<strong class="bold">AAD</strong>). To create an organization, you need to log into Azure DevOps (<a href="https://dev.azure.com/">https://dev.azure.com/</a>), provide the slug<a id="_idIndexMarker1827"/> name for your organization, and select a region to host your organization's assets.</p>
			<p>The following figure shows the screen for creating a new Azure DevOps organization:</p>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/B17928_16_001.jpg" alt="Figure 16.1 â€“ Creating a new Azure DevOps organization " width="801" height="603"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.1 â€“ Creating a new Azure DevOps organization</p>
			<p>Next, you can set up projects in your organization; we will start with one project that will contain the configuration<a id="_idIndexMarker1828"/> and code to run your MLOps pipelines. A project is a place to keep all assets for a specific ML project logically grouped. You will be able to manage your code repositories, sprint boards, issues, PRs, build artifacts, test plans, and CI/CD pipelines within an Azure DevOps project.</p>
			<p>The following figure shows the process of creating a new Azure DevOps project. This will be the container for our pipelines, as well as testing and deployment configuration:</p>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<img src="image/B17928_16_002.jpg" alt="Figure 16.2 â€“ Creating a new Azure DevOps project " width="1017" height="530"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.2 â€“ Creating a new Azure DevOps project</p>
			<p>Once we have the<a id="_idIndexMarker1829"/> organization and project set up, we need to add the Azure Machine Learning capabilities to Azure DevOps by installing the appropriate Azure DevOps extension.</p>
			<h3>Azure Machine Learning extension</h3>
			<p>Next, it is recommended to install the<a id="_idIndexMarker1830"/> Azure Machine Learning extension for your Azure DevOps organization. This will tightly integrate your Azure Machine Learning workspace into Azure DevOps so that you can do the following things within Azure DevOps:</p>
			<ol>
				<li>Assign automatic permissions to access your Azure Machine Learning workspace resources automatically through Azure Resource Manager.</li>
				<li>Trigger release pipelines for new model revisions.</li>
				<li>Run Azure Machine Learning pipelines as tasks.</li>
				<li>Set pre-configured tasks for model deployment and model profiling.</li>
			</ol>
			<p>It's fair to say that all the preceding things can also be set up manually using custom credentials and the Azure ML Python SDK, but the tight integration makes it a lot easier to set up.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can install the Azure Machine Learning extension for Azure DevOps from <a href="https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml">https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml</a>.</p>
			<p>Next, we will <a id="_idIndexMarker1831"/>use the extension to set up the service connections and access permissions for your Azure and Azure Machine Learning workspace accounts.</p>
			<h3>Service connections</h3>
			<p>You might<a id="_idIndexMarker1832"/> remember from previous code examples that interacting with Azure and Azure Machine Learning resources requires the appropriate permissions, tenants, and subscriptions to be configured. Permissions to access these services and resources are <a id="_idIndexMarker1833"/>often defined through <strong class="bold">service principals</strong>. In Azure DevOps, we can set up permissions for our Azure DevOps pipelines to access Azure and Azure Machine Learning resources, create compute resources, and submit ML <a id="_idIndexMarker1834"/>experiments through <strong class="bold">service connections</strong>.</p>
			<p>In your Azure DevOps project, go to <strong class="bold">Settings</strong> | <strong class="bold">Service connections</strong> and configure a new Azure service connection with service principal authentication for your Azure Machine Learning workspace. The following figure shows how to set this up in Azure DevOps:</p>
			<div>
				<div id="_idContainer274" class="IMG---Figure">
					<img src="image/B17928_16_003.jpg" alt="Figure 16.3 â€“ Creating an Azure DevOps service connection " width="939" height="603"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.3 â€“ Creating an Azure DevOps service connection</p>
			<p>Similarly, you can also permit Azure DevOps pipelines to manage resources in an Azure resource group<a id="_idIndexMarker1835"/> programmatically. It is recommended that you create both permissions through service principals and note the name of both newly created connections.</p>
			<h3>Secrets</h3>
			<p>In the next step, we want to<a id="_idIndexMarker1836"/> store and manage all the variables and credentials outside of the actual CI/CD pipelines. We don't want to embed credentials or configuration parameters (such as subscription ID, workspace name, and tenant ID) into the pipeline, but pass them as parameters to the running pipeline.</p>
			<p>In Azure DevOps, you can achieve <a id="_idIndexMarker1837"/>this by using <strong class="bold">variable groups</strong> and <strong class="bold">secure files</strong>. You can <a id="_idIndexMarker1838"/>even connect a variable group to an Azure Key Vault instance to manage your secrets for you.</p>
			<p>It is recommended that you navigate to <strong class="bold">Pipelines</strong> | <strong class="bold">Library</strong> to set up a variable group that contains your subscription ID, tenant ID, names of your service connections, and so on as variables, so that they can be reused in pipelines. You can always come back later and add more variables if you need them. The following figure shows a sample variable group definition that can be included in your pipelines:</p>
			<div>
				<div id="_idContainer275" class="IMG---Figure">
					<img src="image/B17928_16_004.jpg" alt="Figure 16.4 â€“ Creating an Azure DevOps variable group " width="856" height="569"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.4 â€“ Creating an Azure DevOps variable group</p>
			<p>Next, we will set up a repository and write a code pipeline.</p>
			<h3>Agents and agent pools</h3>
			<p>Your CI and CD tasks will eventually <a id="_idIndexMarker1839"/>check out the project, build it, train the model, run the tests, and deploy it. To do all this (and more), you need a compute infrastructure to run the CI/CD jobs. In Azure DevOps, these compute<a id="_idIndexMarker1840"/> resources are called <strong class="bold">agents</strong>.</p>
			<p>Azure DevOps Services provides Microsoft-hosted agents, which will execute your pipeline jobs either in VMs or Docker images. Both compute resources are ephemeral and torn down after each pipeline job.</p>
			<p>When using Azure DevOps with public projects, Azure Pipelines is free and provides you with Microsoft-hosted agents for your CI/CD pipeline jobs. This allows you to run 10 parallel jobs for up to 6 hours each. For private projects, you are limited to one parallel job for up to 1 hour each with at most 30 hours per month.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">To prevent abuse, all free pipeline resources need to be requested for an organization via this form: <a href="https://aka.ms/azpipelines-parallelism-request">https://aka.ms/azpipelines-parallelism-request</a>.</p>
			<p>If more capacity is needed, we can either run self-hosted agents via Azure DevOps Server and/or Azure VM scale set agents or purchase additional Microsoft-hosted agents through Azure DevOps Services. For the purpose of this book, you should be able to start experimenting comfortably with the free capacity on private repositories.</p>
			<h2 id="_idParaDest-267"><a id="_idTextAnchor266"/>Continuous integration â€“ building code with pipelines</h2>
			<p>Now, we can start to set up an <a id="_idIndexMarker1841"/>automatic build, test, and training pipeline for our ML model using Azure DevOps pipelines. Conceptually, we will create or import a Git repository to Azure DevOps that serves as a container for our ML project and will contain the CI pipeline definitions. By convention, we will store the pipelines in the <strong class="source-inline">.pipeline/</strong> directory.</p>
			<p>The following figure shows how to set up or import a repository in Azure DevOps:</p>
			<div>
				<div id="_idContainer276" class="IMG---Figure">
					<img src="image/B17928_16_005.jpg" alt="Figure 16.5 â€“ Cloning or importing a repository " width="1285" height="789"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.5 â€“ Cloning or importing a repository</p>
			<p>Next, we open Visual Studio Code and start authoring our pipeline. Instead of constructing the CI pipeline from widgets and plugins, we will choose YAML to author the pipeline code. This is very similar to how GitHub CI or Jenkins workflows are written. </p>
			<p>A pipeline contains a<a id="_idIndexMarker1842"/> linear series of tasks to be executed to build, test, and train the ML model that can be triggered by a condition in the repository. In the Azure DevOps pipeline, tasks are organized in the following hierarchy:</p>
			<ul>
				<li>Stage A:<ul><li>Job 1:<ul><li>Step 1.1</li><li>Step 1.2</li></ul></li><li>Job 2:<ul><li>Step 2.1</li></ul></li></ul></li>
			</ul>
			<p>Therefore, a pipeline is made up<a id="_idIndexMarker1843"/> of stages, where each stage contains multiple jobs. Each job can contain multiple tasks called steps. Besides stages and jobs, the pipeline can contain the following sections:</p>
			<ul>
				<li>Pipeline definition:<ul><li><strong class="source-inline">name</strong>: The name of the pipeline</li></ul></li>
				<li>Pipeline triggers:<ul><li><strong class="source-inline">schedules</strong>: Scheduling-based pipeline trigger configuration</li><li><strong class="source-inline">trigger</strong>: Code-based pipeline trigger configuration</li><li><strong class="source-inline">pr</strong>: PR-based pipeline trigger configuration</li></ul></li>
				<li>Pipeline compute resources:<ul><li><strong class="source-inline">resources</strong>: Containers and repository configuration</li><li><strong class="source-inline">pool</strong>: Agent pool configuration for pipeline compute resources</li></ul></li>
				<li>Pipeline customization:<ul><li><strong class="source-inline">variables</strong>: Pipeline variables</li><li><strong class="source-inline">parameters</strong>: Pipeline parameters</li></ul></li>
				<li>Pipeline job definition:<ul><li><strong class="source-inline">stages</strong>: Grouping of pipeline jobs, can be skipped if the pipeline contains only a single stage</li><li><strong class="source-inline">jobs</strong>: Pipeline jobs to be executed</li></ul></li>
			</ul>
			<p>As you can see in the <a id="_idIndexMarker1844"/>preceding list, the Azure DevOps pipeline YAML schema allows you to customize pipeline triggers, compute resources, variables, and configurations, and lets you define the tasks to run in the pipeline. Azure DevOps pipelines also understand the concept of templating. You can use the <strong class="source-inline">template</strong> directive for stages, pipelines, jobs, steps, parameters, and variables to reference files from the template.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can find the documentation of the pipeline's YAML schema in the Microsoft documentation at <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/">https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/</a>.</p>
			<p>Let's use these step definitions and construct a simple pipeline to test the model code and start model training:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">ci-pipeline.yaml</p>
			<p class="source-code">trigger:</p>
			<p class="source-code">- main</p>
			<p class="source-code">pool: </p>
			<p class="source-code">Â Â vmImage: ubuntu-latest</p>
			<p class="source-code">stages:</p>
			<p class="source-code">- stage: CI</p>
			<p class="source-code">Â Â jobs:</p>
			<p class="source-code">Â Â - job: Build</p>
			<p class="source-code">Â Â Â Â steps:</p>
			<p class="source-code">Â Â Â Â - script: pytest tests --doctest-modules</p>
			<p class="source-code">- stage: Train</p>
			<p class="source-code">Â Â jobs:</p>
			<p class="source-code">Â Â - job: Train</p>
			<p class="source-code">Â Â Â Â steps:</p>
			<p class="source-code">Â Â Â Â - script: python train.py</p>
			<p>In the preceding<a id="_idIndexMarker1845"/> pipeline, we define the trigger to start the pipeline for new commits on the <strong class="source-inline">main</strong> branch. For execution, we run each job on the Microsoft-hosted free agent pool using an Ubuntu VM. Then, we group the tasks into two stages: <strong class="source-inline">CI</strong> and <strong class="source-inline">Train</strong>. The former will build and test the code and datasets, whereas the latter will train the ML model and create a new version of the model in the model registry.</p>
			<p>Now, we can add a commit to the repository and merge it to the <strong class="source-inline">main</strong> branch, and the CI pipeline will be triggered and train a new model version. You can use the preceding pipeline definition as a <a id="_idIndexMarker1846"/>starting point to add additional steps, tests, configurations, and triggers to fully customize your CI pipeline.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can find an up-to-date example of an MLOps pipeline in the Microsoft GitHub repository at <a href="https://github.com/microsoft/MLOpsPython">https://github.com/microsoft/MLOpsPython</a>.</p>
			<p class="callout">You can find more examples for MLOps starting points on the Azure MLOps repository <a href="https://github.com/Azure/mlops-v2">https://github.com/Azure/mlops-v2</a></p>
			<p>Next, we will take a look at a CD pipeline to deploy the trained model to production.</p>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor267"/>Continuous deployment â€“ deploying models with release pipelines</h2>
			<p>An additional benefit of tracking<a id="_idIndexMarker1847"/> model artifacts in a model registry (for example, in Azure Machine Learning) is that you can automatically trigger release pipelines in Azure DevOps when the artifacts change. Any artifact, such as a new ML model or version, can be configured to trigger a release in Azure DevOps. Therefore, code changes trigger CI build pipelines, and artifact changes trigger CD release pipelines. In this section, we will create a CD pipeline for our model and automatically roll the model out into staging and production.</p>
			<p>While the triggering mechanism for release pipelines is different from build pipelines, most of the concepts for pipeline execution are very similar. Release pipelines also have pipeline stages, whereas each stage can have multiple tasks. One additional feature of release pipelines, since they deal with the deployment of artifacts, is that each stage can have additional <strong class="bold">triggers</strong>, as well as <strong class="bold">pre-deployment</strong> and <strong class="bold">post-deployment conditions</strong>, such as <strong class="bold">manual approval</strong> and <strong class="bold">gates</strong>. </p>
			<p>Triggers will allow you to continue the pipeline execution during a specified schedule only. Manual approvals will halt the pipeline until it is approved by the defined user or user group, whereas gates will halt the pipeline for a predefined time before executing a programmatic check. Multiple stages, triggers, and pre- and post-deployment conditions are often combined to safely deploy artifacts to different environments.</p>
			<p>If you have the Azure Machine Learning plugin installed, you can select triggers and deployment tasks specifically for Azure Machine Learning, such as artifacts based on ML model versions and Azure Machine<a id="_idIndexMarker1848"/> Learning model deployment and profiling tasks. In this section, we will choose both the ML model artifact trigger and the ML model deployment task.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">You can find the available Azure DevOps tasks in the Microsoft documentation at <a href="https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/">https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/</a>.</p>
			<p>The following figure shows you an Azure DevOps release pipeline, where we select an ML model as an artifact for the release pipeline trigger. We configure the pipeline with two stages, a deployment to staging and a deployment to production. In addition, we add a manual approval as a post-deployment condition of the staging deployment:</p>
			<div>
				<div id="_idContainer277" class="IMG---Figure">
					<img src="image/B17928_16_006.jpg" alt="Figure 16.6 â€“ Defining an Azure DevOps Release Pipeline " width="946" height="435"/>
				</div>
			</div>
			<p class="figure-caption">Figure 16.6 â€“ Defining an Azure DevOps Release Pipeline</p>
			<p>By default, the release pipeline will require a user to create a release by pressing the <strong class="bold">Create release</strong> button in the top-right corner. This mode is intended to create releases only when an operator decides to trigger a deployment, and helps us avoid any automated deployments while configuring the release pipeline. However, once the operator is confident that the pipeline and release process are working as intended, we can enable automated <a id="_idIndexMarker1849"/>deployments by toggling the flash icon on the asset in the release pipeline. This will enable the CD trigger and, therefore, trigger a release and deployment whenever the asset has changed. As a final task in this chapter, you can go ahead and activate the CD trigger to fully automate your CD pipeline.</p>
			<h1 id="_idParaDest-269"><a id="_idTextAnchor268"/>Summary</h1>
			<p>In this chapter, we introduced MLOps, a DevOps-like workflow for developing, deploying, and operating ML services. DevOps stands for a quick and high-quality way of making changes to code and deploying these changes to production.</p>
			<p>We first learned that Azure DevOps gives us all the features to run powerful CI/CD pipelines. We can run either build pipelines, where steps are coded in YAML, or release pipelines, which are configured in the UI. Release pipelines can have manual or multiple automatic triggers (for example, a commit in the version control repository or if the artifact of a model registry was updated) and create an output artifact for release or deployment.</p>
			<p>Version-controlling your code is necessary, but it's not enough to run proper CI/CD pipelines. In order to create reproducible builds, we need to make sure that the dataset is also versioned and pseudo-random generators are seeded with a specified parameter. Environments and infrastructure should also be automated, and deployments can be done from the authoring environment.</p>
			<p>In order to keep the code quality high, you need to add tests to the ML pipeline. In application development, we differentiate between unit, integration, and end-to-end tests, where they test different parts of the code, either independently or together with other services. For data pipelines with changing or increasing data, unit tests should test the data quality as well as units of code in the application. Integration tests are great for loading a model or performing a forward or backward pass through a model independently from other components. With Azure Machine Learning, writing end-to-end tests becomes a real joy as they can be completely automated with very low effort and costs.</p>
			<p>Now, you have learned how to set up continuous pipelines that can retrain and optimize your models and then automatically build and redeploy the models to production. In the last chapter, we will look at what's next for you, your company, and your ML services in Azure.</p>
		</div>
	</div>
</div>
</body></html>