<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Apple Core ML</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we are going to briefly introduce the framework that we will be using throughout this book - Core ML. But before doing so, we will elaborate on what training and inference are, specifically how they differ; and then we'll look at the motivation for performing <strong>machine learning</strong> (<strong>ML</strong>) on the edge, that is, your iOS device.</p>
<p>We will be covering the following topics in the chapter:</p>
<ul>
<li>Highlighting the difference between training a model and using the model for inference </li>
<li>Motivation and opportunities for performing inference on the edge</li>
<li>Introducing Core ML and the general workflow </li>
<li>A brief introduction to some ML algorithms</li>
<li>Some considerations to keep in mind when developing ML-enabled applications </li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Difference between training and inference</h1>
                </header>
            
            <article>
                
<p>The difference between training and inference is similar to that of a student being taught something like algebra at school and then applying it in the real world. In school, the student is given numerous exercises; for each exercise, the student attempts the question and hands his/her answer over to the teacher, who provides feedback indicating whether it is correct or not. Initially, this feedback is likely to be skewed toward the student being wrong more often than right, but after many attempts, as the student starts building his/her understanding of the concepts, the feedback shifts towards mostly being right. At this point, the student is considered to have sufficiently learned algebra and is able to apply it to unseen problems in the real world, where he/she can be confident of the answer based on his/her exposure to the exercises provided during the lessons at school. </p>
<p>ML models are no different; the initial phase of building the model is through the process of <strong>training,</strong> where the model is provided with many examples. For each example, a <strong>loss function</strong> is used in place of the teacher to provide feedback, which, in turn, is used to make adjustments to the model to reduce the loss (the degree to which the model's answer was incorrect). This process of training can take many iterations and is typically compute intensive, but it offers opportunities for being parallelized (especially for neural networks); that is, a lot of the calculations can run in parallel with one another. For this reason, it's common to perform training in the cloud or some dedicated machines with enough memory and compute power. This process of training is illustrated in the following diagram:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/526df58d-2638-4bfc-85d4-da47fc6dc5c4.png"/></div>
<div class="packt_infobox">To better illustrate the compute power required, in the blog post <em>Cortana Intelligence and Machine Learning Blog</em>, Microsoft data scientist Miguel Fierro and others detail the infrastructure and time required for training on the ImageNet <span>dataset </span>(<span>1,000 classes with over 1.2 million photos</span>) using an 18-layer ResNet architecture. It took approximately three days to train over 30 epochs on an Azure N-series NC-24 virtual machine with 4 GPUs, 24 CPU cores, and 224 GB of memory. The full details are described here: <a href="https://blogs.technet.microsoft.com/machinelearning/2016/11/15/imagenet-deep-neural-network-training-using-microsoft-r-server-and-azure-gpu-vms/">https://blogs.technet.microsoft.com/machinelearning/2016/11/15/imagenet-deep-neural-network-training-using-microsoft-r-server-and-azure-gpu-vms/</a>.</div>
<p>After the training is complete, the model is now ready for the real world; like our student, we can now deploy and use our model to solve unseen problems. This is known as <strong>inference</strong>. Unlike training, inference only requires a single pass through the model using its gained understanding from training, that is, weights and coefficients. Additionally, there are some sections in our model that are no longer needed, so there is a degree of pruning (the reduction of less important aspects that do not affect accuracy) that can be performed to further optimize the model:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/6821673b-b328-478f-bea7-4299b8635afe.png"/></div>
<p>Because of these conditions, a single pass, and pruning, we can afford to perform inference on less performant machines, like our smartphone. But why would you want to do this? What are the advantages of performing inference on the edge? This is the topic of the next section. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inference on the edge</h1>
                </header>
            
            <article>
                
<p>For those unfamiliar with the term <strong>edge computing</strong>, it simply refers to computation performed at the end, or edge, of a network as opposed to sending it to a central server for computation. Some examples of edge devices include cars, robots, <strong>Internet of Things</strong> (<strong>IoT</strong>), and, of course, smartphones. </p>
<p>The motivation for performing computation at the edge, where the data resides, is that sending data across the network is expensive and time-consuming; this incurred latency and cost restrict us with what experiences we can deliver to the user. Removing these barriers opens up new applications that would otherwise not be possible. Another benefit of performing inference at the edge is data privacy; removing the need of having to transmit personal data across the network reduces the opportunities that a malicious user has for obtaining it.</p>
<p><span>Luckily, technology advances at an astonishing rate and improvements in hardware and software have now made it feasible to per</span>form inference at the edge. </p>
<div class="packt_infobox">As this book's focus is on applied ML on iOS; <strong>detailed</strong> model architectures and training have been intentionally omitted as training currently requires significant computational power that is still out of reach of most of today's edge devices - although this is likely to change in the near future as edge devices become increasingly powerful, with the most likely next advancement being around tuning and personalizing models using personal data that resides on the device.</div>
<p>Some common use cases for ML on the device include:</p>
<ul>
<li><strong>Speech recognition</strong>: It's currently common to perform wake (or hot) word detection locally rather than continuously streaming data across the network. For example, <strong>Hey Siri</strong> is most likely performed locally on the device, and once detected, it streams the utterance to a server for further processing.</li>
<li><strong>Image recognition</strong>: It can be useful for the device to be able to understand what it is seeing in order to assist the user in taking a photo, such as applying the appropriate filters, adding captions to the photos to make them easier to find and grouping similar images together. These enhancements may not be significant enough to justify opening a connection to a remote server, but because these can be performed locally, we can use them without worrying about cost, latency, or privacy issues. </li>
<li><strong>Object localization</strong>: Sometimes, it is useful to know <span>not only </span>what is present in view, but also where it is in the view. An example of this can be seen in <strong>augmented reality</strong> (<strong>AR</strong>) apps, where information is overlaid onto the scene. Having these experiences responsive is critical for their success, and therefore there is a need for extremely low latency in performing inference.</li>
<li><strong>Optical character recognition</strong>: One of the first commercial applications of neural networks is still just as useful as it was when it was used in American post offices in 1989. Being able to read allows for applications such as digitizing a physical copy or performing computations on it; examples include language translation or solving a Sudoku puzzle.</li>
<li><strong>Translation</strong>: Translating from one language to another quickly and accurately, even if you don't have a network connection, is an important use case and complements many of the visual-based scenarios we have discussed so far, such as AR and optical character recognition.</li>
<li><strong>Gesture recognition</strong>: Gesture recognition provides us with a rich interaction mode, allowing quick shortcuts and intuitive user interactions that can improve and enhance user experience.</li>
<li><strong>Text prediction</strong>: Being able to predict the next word the user is going to type, or even predicting the user's response, has turned something fairly cumbersome and painful to use (the smartphone soft keyboard) into something that is just as quick or even quicker than its counterpart (the conventional keyboard). Being able to perform this prediction on the device increases your ability to protect the user's privacy and offer a responsive solution. This is not feasible if the request has to be routed to a remote server. </li>
<li><strong>Text classification</strong>: This covers everything from sentiment analysis to topic discovery and facilitates many useful applications, such as providing means to recommend relevant content to the user or eliminate duplicates.</li>
</ul>
<p>These examples of use cases and applications hopefully show why we may want to perform inference on the edge; it means you can offer a higher level of interactivity than what could be possible with performing inference off the device. It allows you to deliver an experience <span>even </span>if the device has poor network <span>connectivity</span> or no network connectivity. And finally, it's scalable—an increase in demand doesn't directly correlate to the load on your server. </p>
<p>So far, we have introduced inference and the importance of being able to perform it on the edge. In the next section, we will introduce the framework that facilitates this on iOS devices: Core ML.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief introduction to Core ML</h1>
                </header>
            
            <article>
                
<p><span>With the release of iOS 11 and Core ML, performing inference is just a matter of a few lines of code. </span>Prior to iOS 11, inference was possible, but it required some work to take a pre-trained model and port it across using an existing framework such as <strong>Accelerate</strong> or <strong>metal performance shaders</strong> (<strong>MPSes</strong>). <span><strong>Accelerate</strong> and MPSes are still used under the hood by Core ML, but Core ML takes care of deciding which underlying framework your model should use (<strong>Accelerate</strong> using the CPU for memory-heavy tasks and MPSes using the GPU for compute-heavy tasks). It also takes care of abstracting a lot of the details away; this layer of abstraction is shown in the following diagram: </span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fbdf956c-99c2-43fb-a357-834cf0ef66d7.png"/></div>
<p>There are additional layers too; iOS 11 has introduced and extended domain-specific layers that further abstract a lot of the common tasks you may use when working with image and text data, such as face detection, object tracking, language translation, and <strong>named entity recognition</strong> (<strong>NER</strong>). These domain-specific layers are encapsulated in the <strong>Vision</strong> and <strong>natural language processing</strong> (<strong>NLP</strong>) frameworks; we won't be going into any details of these frameworks here, but you will get a chance to use them in later chapters:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/754528fc-4c89-42c7-8aa1-f3055e811d49.png" style="width:36.00em;height:14.83em;"/> </div>
<p>It's worth noting that these layers are not mutually exclusive and it is common to find yourself using them together, especially the domain-specific frameworks that provide useful preprocessing methods we can use to prepare our data before sending to a Core ML model. </p>
<p>So what exactly is Core ML? You can think of Core ML as a suite of tools used to facilitate the process of bringing ML models to iOS and wrapping them in a standard interface so that you can easily access and make use of them in your code. Let's now take a closer look at the typical workflow when working with Core ML. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Workflow </h1>
                </header>
            
            <article>
                
<p>As described previously, the two main tasks of a ML workflow consist of <strong>training</strong> and <strong>inference</strong>. Training involves obtaining and preparing the data, defining the model, and then the real training. Once your model has achieved satisfactory results during training and is able to perform adequate predictions (including on data it hasn't seen before), your model can then be deployed and used for inference using data outside of the training set. Core ML provides a suite of tools to facilitate getting a trained model into iOS, one being the Python packaged released called <strong>Core ML Tools</strong>; it is used to take a model (consisting of the architecture and weights) from one of the many popular packages and exporting a <kbd>.mlmodel</kbd> file, which can then be imported into your Xcode project. </p>
<p><span>Once imported, Xcode will generate an interface for the model, making it easily accessible via code you are familiar with. Finally, when you build your app, the model is further optimized and packaged up within your application. A summary of the process of generating the model is shown in the following </span>diagram<span>: </span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/870eb15e-d50e-45c2-a61c-ea386dd88d36.png" style="width:43.75em;height:17.08em;"/></div>
<p>The previous diagram illustrates the process of creating the <kbd>.mlmodel;</kbd>, either using an existing model from one of the supported frameworks, or by training it from scratch. Core ML Tools supports most of the frameworks, either internal or as third party plug-ins, including  Keras, turi, Caffe, scikit-learn, LibSVN, and XGBoost <span>frameworks. </span>Apple has also made this package open source and modular for easy adaption for other frameworks or by yourself. The process of importing the model is illustrated in this diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/39178104-5b20-473a-8327-dad08a10a23f.png" style="width:27.67em;height:19.50em;"/></div>
<p>In addition; there are frameworks with tighter integration with Core ML that handle generating the Core ML model such as <strong>Turi Create</strong>, <strong>IBM Watson Services for Core ML</strong>, and <span><strong>Create ML</strong>. </span></p>
<div class="packt_infobox">We will be introducing Create ML in chapter 10; for those interesting in learning more about Turi Create and IBM Watson Services for Core ML then please refer to the official webpages via the following links: <br/>
Turi Create; <a href="https://github.com/apple/turicreate">https://github.com/apple/turicreate</a> <br/>
IBM Watson Services for Core ML; <a href="https://developer.apple.com/ibm/">https://developer.apple.com/ibm/</a></div>
<p>Once the model is imported, as mentioned previously, Xcode generates an interface that wraps the <strong>model</strong>, model <strong>inputs</strong>, and <strong>outputs</strong>. You will get acquainted with these throughout the rest of this book, so we won't go into any further details here. </p>
<p>In the previous diagrams we have seen the workflow of training and importing a <strong>model</strong> - let's now delve into the details of what this model is and what Core ML currently supports. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning algorithms </h1>
                </header>
            
            <article>
                
<p>In <a href="7d4f641f-7137-4a8a-ae6e-2bb0e2a6db5c.xhtml" target="_blank">Chapter 1</a>, <em>Introduction to Machine Learning</em>, we saw many different types of learning algorithms and learned that ML is really a process of automatically discovering rules given a set of examples. The main components required for this process, specifically for supervised learning, include: </p>
<ul>
<li><strong>Input data points</strong>: For image classification, we would require images of the domain we want to classify, for example, animals. </li>
<li><strong>The expected outputs for these inputs</strong>: Continuing from our previous example of image classification of animals, the expected outputs could be labels associated with each of the images, for example, cat, dog, and many more. </li>
<li><strong>A ML algorithm</strong>: This is the algorithm used to automatically learn how to transform the input data points into a meaningful output. These derived sets of rules are what we call the model, derived through a process of learning called <strong>training</strong>. </li>
</ul>
<p>Let's make these concepts more concrete by working through a simple example. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Auto insurance in Sweden</h1>
                </header>
            
            <article>
                
<p>If you haven't done so already, navigate to the repository at <a href="https://github.com/joshnewnham/MachineLearningWithCoreML">https://github.com/joshnewnham/MachineLearningWithCoreML</a> and download the latest code. Once downloaded, navigate to the directory <kbd>Chapter2/Start/</kbd> and open the playground <kbd>LinearRegression.playground</kbd>.</p>
<p>We will be creating a model that will predict the total payments for all claims (y) given the number of claims (x); the dataset we will be working with is auto insurance claims in Sweden. It consists of 2 columns and 64 rows, the first column containing the number of claims, and the second containing the total payments for all claims. Here is an extract from the dataset:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<thead>
<tr style="background-color: #e6e6e6">
<td>
<div class="CDPAlignLeft CDPAlign"><strong>Number of claims</strong></div>
</td>
<td><strong>Total payments for all claims in thousands of Swedish Kronor</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td>108</td>
<td>329.5</td>
</tr>
<tr>
<td>19</td>
<td>46.2</td>
</tr>
<tr>
<td>13</td>
<td>15.7</td>
</tr>
<tr>
<td>124</td>
<td>422.2</td>
</tr>
<tr>
<td>...</td>
<td>...</td>
</tr>
</tbody>
</table>
<div class="packt_infobox">For more details, visit the source website: <a href="http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr06.html">http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr06.html</a>.</div>
<p>In the playground script, you will see that we are creating a view of the type <kbd>ScatterPlotView</kbd> and assigning it to the playground's live view. We will use this view to visualize the data and the predictions from our model:</p>
<pre>let view = ScatterPlotView(frame: CGRect(x: 20, y: 20, width: 300, height: 300))<br/><br/>PlaygroundPage.current.liveView = view</pre>
<p>By using this view, we can plot an array of data points using the <kbd>view.scatter(dataPoints:)</kbd> method and draw a line using the <kbd>view.line(pointA:,pointB)</kbd> method. Let's load the raw data and visualize it:</p>
<pre>let csvData = parseCSV(contents:loadCSV(file:"SwedishAutoInsurance"))<br/><br/>let dataPoints = extractDataPoints(data: csvData, xKey: "claims", yKey: "payments")<br/><br/>view.scatter(dataPoints)</pre>
<p>In the previous code snippet, we first load the data into the <kbd>csvData</kbd> variable and then cast it into a strongly typed array of <kbd>DataPoint</kbd> (a strongly typed data object, which our view is expecting). Once loaded, we pass our data to the view via the <kbd>scatter</kbd> method, which renders the following output:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/ac2f3a43-c656-4fdc-ba2b-e51ca485bafc.png" style="width:37.50em;height:37.58em;"/> </div>
<p>Each dot represents a single datapoint plotted against the number of claims (<em>x</em> axis) and total payments for all claims (<em>y</em> axis). From this visualization, we can infer some linear relationship between the <strong>number of claims</strong> and <strong>total payments for all claims</strong>; that is, an increase in <strong>number of claims</strong> increases the <strong>total payments for all claims</strong>. Using this intuition, we will attempt to model the data according to a linear model, one that, when given the <strong>number of claims</strong>, is able to predict the <strong>total payments for all claims</strong>. What we are describing here is a type of algorithm known as <strong>simple linear regression</strong>; in essence, this is just finding a straight line that best fits our data. It can be described with the function <em>y = w * x + b,</em> where <em>y</em> is the <strong>total payments for all claims</strong>, <em>x</em> is the <strong>number of claims</strong>, <em>w</em> is the relationship between <em>y</em> and <em>x,</em> and <em>b</em> is the intercept.</p>
<div class="packt_infobox">Linear regression is a type of regression model that maps a linear function from a set of continuous inputs to a continuous output. For example, you may want to model and predict <strong>house prices</strong>; here, your inputs may be the <strong>number of</strong> <strong>bedrooms</strong> and the <strong>number of bathrooms</strong>. Using these two features, you'd want to find a function that can predict the house price, one that assumes there is a linear correlation. </div>
<p>Simple enough! Our next problem is finding this line that best fits our data. For this, we are going to use an approach called <strong>gradient descent</strong>;<strong> </strong>there are plenty of books that go into the theoretical and technical details of gradient descent, so here we will just present some intuition behind it and leave it to you, the curious reader, to study the details. </p>
<div class="packt_infobox">Gradient descent is a set of algorithms that minimize a function; in our case, they minimize the loss of our output with respect to the actual output. They achieve this by starting with an initial set of parameters (weights or coefficients) and iteratively adjusting these to minimize the calculated loss. The direction and magnitude of these adjustments are determined by how far off the predicted value is compared to the expected error and the parameters' contribution.</div>
<p>You can think of gradient descent as a search for some minimum point; what determines this minimum point is something called a <strong>loss function</strong>. For us, it will be the absolute error between our prediction and actual number of claims. The algorithm is steered by calculating the relative contribution of each of our variables (here it is <em>w</em><strong> </strong>and <em>b</em>). Let's see how this looks in code by working through the <kbd>train</kbd> method:</p>
<pre style="padding-left: 30px">func train(<br/>    x:[CGFloat],<br/>    y:[CGFloat],<br/>    b:CGFloat=0.0,<br/>    w:CGFloat=0.0,<br/>    learningRate:CGFloat=0.00001,<br/>    epochs:Int=100,<br/>    trainingCallback: ((Int, Int, CGFloat, CGFloat) -&gt; Void)? = nil) -&gt; (b:CGFloat, w:CGFloat){<br/>    <br/>    var B = b // bias<br/>    var W = w // weight<br/>    <br/>    let N = CGFloat(x.count) // number of data points<br/>    <br/>    for epoch in 0...epochs{<br/>        <strong>// TODO: create variable to store this epoch's gradient for b and w</strong><br/>        for i in 0..&lt;x.count{<br/>            <strong>// TODO:</strong><strong> make a prediction (using the linear equation y = b + x * w</strong><br/>            <strong>// TODO: calculate the absolute error (prediction - actual value)</strong><br/>            <strong>// TODO: calculate the gradient with respect to the error and b (); adding it to the epochs bias gradient</strong><br/>            <strong>// TODO: calculate the gradient with respect to the error and w (); adding it to the epochs weight gradient</strong><br/>        }<br/>        <strong>// TODO: update the bias (B) using the learningRate</strong><br/>        <strong>// TODO: update the weight (W) using the learningRate</strong><br/>        if let trainingCallback = trainingCallback{<br/>            trainingCallback(epoch, epochs, W, B)<br/>        }<br/>    }<br/>    <br/>    return (b:B, w:W)<br/>}   </pre>
<p class="mce-root">Our <kbd>train</kbd> method takes in these arguments: </p>
<ul>
<li><kbd>x</kbd>: <span>An a</span>rray of <kbd>DataPoint</kbd> containing the number of claims </li>
<li><kbd>y</kbd>: An array of <kbd>DataPoint</kbd> containing the total number of payments </li>
<li><kbd>b</kbd>: This is a random value used in our linear function to start our search </li>
<li><kbd>w</kbd>: Another random value used in our linear function to start our search </li>
<li><kbd>learningRate</kbd>: How quickly we adjust the weights </li>
<li><kbd>epochs</kbd>: The number of times we iterate, that is, make a prediction, and adjust our coefficients based on the difference between the prediction and expected value</li>
<li><kbd>trainingCallback</kbd>: This function is called after each epoch to report the progress</li>
</ul>
<p>We next create some variables that will be used throughout training and begin our search (<kbd>for epoch in 0...epochs</kbd>). Let's step through each <kbd>TODO</kbd> and replace them with their respective code.</p>
<p>First, we start by creating two variables to hold the gradients for our variables <kbd>b</kbd> and <kbd>w</kbd> (these are the adjustments we need to make to their respective coefficients to minimize the loss, also known as <strong>absolute error</strong>):</p>
<pre>// TODO: create variable to store this epoch's gradient for b and w<br/><strong>var bGradient : CGFloat = 0.0</strong><br/><strong>var wGradient : CGFloat = 0.0</strong></pre>
<p>Next, we iterate over each data point, and for each data point, make a prediction and calculate the absolute error:</p>
<pre>// TODO: make a prediction (using the linear equation y = b + x * w<br/><strong>let yHat = W * x[i] + B</strong><br/>// TODO: calculate the absolute error (prediction - actual value)<br/><strong>let error = y[i] - yHat</strong></pre>
<p>Now calculate the partial derivative with respect to the error. Think of this as a way to steer the search in the right direction, that is, calculating this gives us the <strong>direction</strong> and <strong>magnitude</strong> that we need to change <kbd>b</kbd> and <kbd>w</kbd> to minimize our error:</p>
<div class="packt_infobox">Note that this is done after iterating through all data points; that is, it is influenced by all data points. Alternatives are to perform this update per data point or over a subset, known as a <strong>batch</strong>. </div>
<pre>// TODO: calculate the gradient with respect to the error and b (); adding it to the epochs bias gradient<br/><strong>B = B - (learningRate * bGradient)</strong><br/>            <br/>// TODO: calculate the gradient with respect to the error and w (); adding it to the epochs weight gradient <br/><strong>W = W - (learningRate * wGradient)</strong></pre>
<p>After iterating over each data point, we adjust the coefficients <kbd>B</kbd> and <kbd>W</kbd> using their accumulated gradients.</p>
<p>After each epoch, <kbd>trainingCallback</kbd> is called to draw a line using the current model's coefficients (its current best fit line that fits the data); the progress of this is shown in the following <span>diagram</span>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/92f56310-c232-481b-947c-6072e198f4f7.png"/></div>
<p>Admittedly, this is difficult to interpret without a key! But the pattern will hopefully be obvious; with each iteration, our line better fits the data. After 100 epochs, we end up with this model:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/0ef20d7f-0f58-4561-b958-182ea6f17a50.png" style="width:44.00em;height:44.00em;"/></div>
<p>The function describing this line is <kbd>y = 0.733505317339142 + 3.4474988368438 * x</kbd>. Using this model, we can predict the <strong>total payments for all claims</strong> given the <strong>number of claims</strong> (by simply substituting <em>x</em> with the <strong>number of claims</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supported learning algorithms</h1>
                </header>
            
            <article>
                
<p>In the previous example, we used <strong>l</strong><span><strong>inear regression</strong></span> (algorithm) to build a model that predicts the total payments for all claims (<span>output</span>) given the number of claims (<span>input</span>). This is one of many algorithms available for ML; a few of them are plotted in the following diagram, grouped into <strong>unsupervised</strong> or <strong>supervised</strong>, and <strong>continuous</strong> or <strong>categorical</strong>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2a171496-cdec-4b72-a2d2-a09dbbdc50d3.png" style="width:22.17em;height:20.75em;"/></div>
<p>The process of creating Core ML models involves translating the model from the source framework into something that can be run on iOS. The following <span>diagram</span> shows which learning algorithms Core ML currently supports:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/eb816183-f6b1-4030-ac42-8bd66a56f8d0.png" style="width:36.08em;height:15.33em;"/></div>
<p>The supported algorithms and neural networks should be versatile enough for most ML tasks but given how fast this field is moving, it's inevitable that you will encounter one that is not supported. Apple have anticipated this and provides two protocols for extending the framework; <kbd>MLCustomLayer</kbd> can be used to create custom layers (which we cover in later chapters) and <kbd>MLCustomModel</kbd> for creating custom models. </p>
<p>This has hopefully given you some idea of where Core ML fits in the general ML workflow and why Apple has made the design decisions it has. We will finish this chapter by looking at a few high-level considerations when dealing with ML on an iOS device, or, more generally, the edge before wrapping up.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Considerations </h1>
                </header>
            
            <article>
                
<p>When performing ML on the edge, you lose some of the luxuries you tend to have when running on a more powerful device (albeit this is shifting all the time). Here is a list of considerations to keep in mind:</p>
<ul>
<li><strong>Model size</strong>: Previously, we walked through building a simple linear regression model. The model itself consists of two floats (bias and weight coefficients), which of course are negligible in terms of memory requirements. But, as you dive into the world of deep learning, it's common to find models hundreds of megabytes in size. For example, the VGG16 model is a 16-layer conventional neural network architecture trained on the ImageNet dataset used for image classification, available on Apple's site. It is just over 500 megabytes. Currently, Apple allows apps 2 gigabytes in size, but asking your user to download such a large file may well put them off. </li>
<li><strong>Memory</strong><span><span>: It's not just the executable size that you need to be mindful of, but also the amount of working memory available. It's common for desktop machines to have memory in the range of 16-32 gigabytes, but the memory for the latest iPhone (iPhone 8) is just 2 gigabytes—impressive for a mobile device, but quite a difference from its counterpart. This constraint is likely to dictate what model you choose, more so than how much memory it takes on disk. It is also worth mentioning that it's not just the model weights you'll need to load into the memory; you will also need to load in any label data and, of course, the input data you are performing inference on.</span></span></li>
<li><strong>Speed</strong>: This, of course, is correlated to the model size (in normal circumstances) and relevant to your specific use case. Just keep in mind that performing inference is only one part of the workflow. You have pre-processing and post-processing tasks that also need to be taken into account, such as loading and pre-processing the input data. In some cases, you may have to trade off accuracy with performance and size.</li>
<li><strong>Supported algorithms and data types</strong>: In the previous section, we presented the current algorithms that Core ML supports. Along with these, Core ML supports a subset of data types, summarized in the following table for convenience:</li>
</ul>
<table style="border-collapse: collapse;width: 100%" border="1">
<thead>
<tr style="background-color: #e6e6e6">
<td><strong>Input type</strong></td>
<td><strong>Data type</strong></td>
</tr>
</thead>
<tbody>
<tr>
<td>Numeric</td>
<td><kbd>Double</kbd>, <kbd>Int64</kbd></td>
</tr>
<tr>
<td>Categories</td>
<td><kbd>String</kbd>, <kbd>Int64</kbd></td>
</tr>
<tr>
<td>Images</td>
<td><kbd>CVPixelBuffer</kbd></td>
</tr>
<tr>
<td>Arrays</td>
<td><kbd>MLMultiArray</kbd></td>
</tr>
<tr>
<td>Dictionaries </td>
<td><kbd>[String : Double]</kbd>, <kbd>[Int64, Double]</kbd></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Here, we have presented just a few of the considerations at a high level when performing ML on a mobile device. The specifics will be dependent on your use case and models available, but it's worth keeping these in the back of your mind and reminding yourself that these, albeit very powerful devices, are still mobile devices. They run on a battery and are therefore subject to the typical considerations and optimizations normally required for a mobile project. These considerations are even more applicable to those who plan to create their own model, which should be most of you if you plan to take advantage of ML.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed the difference between training and inference, along with the typical ML workflow and where Core ML fits in. We also saw how Core ML is not just a single framework, but rather a suite of tools that facilitate getting pretrained models into the iOS platform and making them available to your application via a familiar and simple interface. Thus, it <span>democratizes ML and puts it into the hands of many iOS app developers.</span></p>
<p><span>It has been suggested that the explosion in diverse apps contributed to the success of the adoption of smartphones; if this is true, then prepare yourself for the next explosion of AI-enhanced apps. And take comfort knowing that you are in the perfect place to begin and lead this journey, where we will explore many concepts and examples related to computer vision using Core ML, including these:</span></p>
<ul>
<li>Recognizing objects through the video feed of your camera </li>
<li>Leveraging object detection to build intelligent image search, allowing you to search for images with specific objects and their position relative to one another </li>
<li>Recognizing facial expressions and inferring the emotional state of a person </li>
<li><span>Recognizing</span> hand-drawn sketches using convolutional neural networks and then with recurrent neural networks</li>
<li>Learning the secrets behind Prisma's style transfer and implementing your own version </li>
<li>Finally, using image segmentation to create the action shot effect</li>
</ul>
<p><span>There is plenty to get through, so let's get started!</span></p>


            </article>

            
        </section>
    </body></html>