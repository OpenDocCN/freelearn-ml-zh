<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer245">
<h1 id="_idParaDest-161"><em class="italic"><a id="_idTextAnchor162"/>Chapter 14</em>: Naïve Bayes Classification</h1>
<p>In this chapter, we will examine situations where naïve Bayes might be a more efficient classifier than the ones we have examined so far. Naïve Bayes is a very intuitive and easy-to-implement classifier. Assuming our features are independent, we may even get improved performance over logistic regession, particularly if we are not using regularization with the latter.</p>
<p>In this chapter, we will discuss the fundamental assumptions of naïve Bayes and how the algorithm is used to tackle some of the modeling challenges we have already explored, as well as some new ones, such as text classification. We will consider when naïve Bayes is a good option and when it is not. We will also examine the interpretation of naïve Bayes models. </p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Key concepts</li>
<li>Naïve Bayes classification models</li>
<li>Naïve Bayes for text classification</li>
</ul>
<h1 id="_idParaDest-162"><a id="_idTextAnchor163"/>Technical requirements</h1>
<p>We will ma<a id="_idTextAnchor164"/>inly stick to the pandas, NumPy, and scikit-learn libraries in this chapter. The only exception is the imbalanced-learn library, which can be installed with <strong class="source-inline">pip install imbalanced-learn</strong>. All the code in this chapter was tested with scikit-learn versions 0.24.2 and 1.0.2.</p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor165"/>Key concepts</h1>
<p>The<a id="_idIndexMarker1069"/> naïve Bayes classifier uses Bayes’ theorem to predict class membership. Bayes’ theorem describes the relationship between the probability of an event and the probability of an event given new, relevant data. The probability of an event given new <a id="_idIndexMarker1070"/>data is called the <strong class="bold">posterior probability</strong>. The probability of an event occurring before the new data is appropriately referred<a id="_idIndexMarker1071"/> to as the <strong class="bold">prior probability</strong>. </p>
<p>Bayes’ theorem gives us the following equation: </p>
<div>
<div class="IMG---Figure" id="_idContainer232">
<img alt="" height="107" src="image/B17978_14_001.jpg" width="1435"/>
</div>
</div>
<p>The <a id="_idIndexMarker1072"/>posterior probability (the probability of an event given new data) is equal to the probability of the data given the event, times the prior probability of the event, divided by the probability of the new data.  </p>
<p>Somewhat less colloquially, this is typically written as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer233">
<img alt="" height="117" src="image/B17978_14_002.jpg" width="471"/>
</div>
</div>
<p>Here, <em class="italic">A</em> is an event, such as class membership, and <em class="italic">B</em> is new information. When applied to classification, we get the following equation:</p>
<div>
<div class="IMG---Figure" id="_idContainer234">
<img alt="" height="129" src="image/B17978_14_003.jpg" width="674"/>
</div>
</div>
<p>Here, <img alt="" height="54" src="image/B17978_14_004.png" width="288"/> is the probability of class membership for an instance given the features for the instance, and <img alt="" height="52" src="image/B17978_14_005.png" width="288"/> is the probability of features given class membership. <em class="italic">P(y)</em> is the probability of class membership, while <img alt="" height="51" src="image/B17978_14_006.png" width="227"/> is the probability of feature values. Therefore, the posterior probability, <img alt="" height="53" src="image/B17978_14_007.png" width="288"/>, is equal to the probability of the feature values given class membership, times the probability of class membership, divided by the probability of the feature values. </p>
<p>The assumption here is that the features are independent of one another. This is what gives this method the <em class="italic">naïve</em> adjective. As a practical matter, though, feature independence is not necessary to get good results with naïve Bayes.</p>
<p>Naïve Bayes can work with numeric or categorical features. We typically use Gaussian naïve Bayes when we have mainly numeric features. As its name implies, Gaussian naïve Bayes assumes that the conditional probability of feature values, <img alt="" height="53" src="image/B17978_14_008.png" width="288"/>, follows a normal distribution. <img alt="" height="52" src="image/B17978_14_009.png" width="288"/> can then be calculated relatively straightforwardly using the standard deviations and means of features within each class.</p>
<p>When our <a id="_idIndexMarker1073"/>features are discrete or are counts, we can use multinomial naïve Bayes instead. More generally, it works well when the conditional probability of feature values follows a multinomial distribution. A common application of multinomial naïve Bayes is with text classification using the <strong class="bold">bag-of-words</strong> approach. With bag-of-words, the<a id="_idIndexMarker1074"/> features are the counts of each word in a document. We can apply Bayes’ theorem to estimate the probability of class membership:</p>
<div>
<div class="IMG---Figure" id="_idContainer241">
<img alt="" height="142" src="image/B17978_14_010.jpg" width="648"/>
</div>
</div>
<p>Here, <img alt="" height="47" src="image/B17978_14_011.png" width="175"/> is the probability of membership in the <em class="italic">k</em> class, given a vector of word counts, <em class="italic">W</em>. We will put this to good use in the last section of this chapter.</p>
<p>There is a fairly wide range of text classification tasks for which naïve Bayes is applicable. It is used in sentiment analysis, spam detection, and news story categorization, to name just a few examples.</p>
<p>Naïve Bayes is an efficient algorithm for both training and prediction and often performs very well. It is quite scalable, working well with large numbers of instances and features. It is also very easy to interpret. The algorithm does best when model complexity is not necessary for<a id="_idIndexMarker1075"/> good predictions. Even when naïve Bayes is not likely to be the approach that will yield the least bias, it is often useful for diagnostic purposes, or to check the results from a different algorithm.</p>
<p>The NBA data we worked with in the previous chapter might be a good candidate for modeling with naïve Bayes. We will explore that in the next section. </p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor166"/>Naïve Bayes classification models</h1>
<p>One of<a id="_idIndexMarker1076"/> the attractions of naïve Bayes is that you can get decent results quickly, even when you have lots of data. Both fitting and predicting are fairly easy on system resources. Another advantage is that relatively complex relationships can be captured without having to transform the feature space or doing much hyperparameter tuning. We can demonstrate this with the NBA data we worked with in the previous chapter.</p>
<p>We will work with data <a id="_idIndexMarker1077"/>on <strong class="bold">National Basketball Association</strong> (<strong class="bold">NBA</strong>) games in this section. The dataset contains statistics from each NBA game from the 2017/2018 season through the 2020/2021 season. This includes the home team; whether the home team won; the visiting team; shooting percentages for visiting and home teams; turnovers, rebounds, and assists by both teams; and several other measures.</p>
<p class="callout-heading">Note</p>
<p class="callout">The NBA game data <a id="_idIndexMarker1078"/>can be downloaded by the public at <a href="https://www.kaggle.com/datasets/wyattowalsh/basketball">https://www.kaggle.com/datasets/wyattowalsh/basketball</a>. This dataset contains game data starting with the 1946/1947 NBA season. It uses <strong class="source-inline">nba_api</strong> to pull stats from <strong class="source-inline">nba.com</strong>. That API is available at <a href="https://github.com/swar/nba_api">https://github.com/swar/nba_api</a>.</p>
<p>Let’s build a <a id="_idIndexMarker1079"/>classification model using naïve Bayes:</p>
<ol>
<li>We will load the same libraries we have been using in the last few chapters:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import OneHotEncoder, StandardScaler</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.feature_selection import RFE</p><p class="source-code">from sklearn.naive_bayes import GaussianNB</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.model_selection import cross_validate, \</p><p class="source-code">  RandomizedSearchCV, RepeatedStratifiedKFold</p><p class="source-code">import sklearn.metrics as skmet</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + "/helperfunctions")</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>Next, we <a id="_idIndexMarker1080"/>will load the NBA games data. We need to do a little data cleaning here. A handful of observations have missing values for whether the home team won or not, <strong class="source-inline">WL_HOME</strong>. We will remove those as that will be our target. We will also convert <strong class="source-inline">WL_HOME</strong> into an integer. Notice that there is not so much class imbalance that we need to take aggressive steps to deal with it:<p class="source-code">nbagames = pd.read_csv("data/nbagames2017plus.csv", parse_dates=['GAME_DATE'])</p><p class="source-code">nbagames = nbagames.loc[nbagames.WL_HOME.isin(['W','L'])]</p><p class="source-code">nbagames.shape</p><p class="source-code"><strong class="bold">(4568, 149)</strong></p><p class="source-code">nbagames['WL_HOME'] = \</p><p class="source-code">  np.where(nbagames.WL_HOME=='L',0,1).astype('int')</p><p class="source-code">nbagames.WL_HOME.value_counts(dropna=False)</p><p class="source-code"><strong class="bold">1    2586</strong></p><p class="source-code"><strong class="bold">0    1982</strong></p><p class="source-code"><strong class="bold">Name: WL_HOME, dtype: int64</strong></p></li>
<li>Now, let’s <a id="_idIndexMarker1081"/>create training and testing DataFrames, organizing them by numeric and categorical features. We should also generate some descriptive statistics. Since we did that in the previous chapter, we will not repeat that here; however, it might be helpful to go back and take a look at those numbers to get ready for the modeling stage:<p class="source-code">num_cols = ['FG_PCT_HOME','FTA_HOME','FG3_PCT_HOME',</p><p class="source-code">  'FTM_HOME','FT_PCT_HOME','OREB_HOME','DREB_HOME',</p><p class="source-code">  'REB_HOME','AST_HOME','STL_HOME','BLK_HOME',</p><p class="source-code">  'TOV_HOME', 'FG_PCT_AWAY','FTA_AWAY','FG3_PCT_AWAY',</p><p class="source-code">  'FT_PCT_AWAY','OREB_AWAY','DREB_AWAY','REB_AWAY',</p><p class="source-code">  'AST_AWAY','STL_AWAY','BLK_AWAY','TOV_AWAY']</p><p class="source-code">cat_cols = ['TEAM_ABBREVIATION_HOME','SEASON']</p><p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nbagames[num_cols + cat_cols],\</p><p class="source-code">  nbagames[['WL_HOME']], test_size=0.2,random_state=0)</p></li>
<li>Now, we need to set up the column transformations. We will deal with some outliers for the numeric features, assigning those values and any missing values to the median. Then, we will use the standard scaler. We will set up one-hot encoding for the categorical features:<p class="source-code">ohe = OneHotEncoder(drop='first', sparse=False)</p><p class="source-code">cattrans = make_pipeline(ohe)</p><p class="source-code">standtrans = make_pipeline(OutlierTrans(2),</p><p class="source-code">  SimpleImputer(strategy="median"), StandardScaler())</p><p class="source-code">coltrans = ColumnTransformer(</p><p class="source-code">  transformers=[</p><p class="source-code">    ("cat", cattrans, cat_cols),</p><p class="source-code">    ("stand", standtrans, num_cols)</p><p class="source-code">  ]</p><p class="source-code">)</p></li>
<li>We are <a id="_idIndexMarker1082"/>now ready to run a naïve Bayes classifier. We will add a Gaussian naïve Bayes instance to a pipeline that will be run after the column transformation and some recursive feature elimination:<p class="source-code">nb = GaussianNB()</p><p class="source-code">rfe = RFE(estimator=LogisticRegression(),</p><p class="source-code">  n_features_to_select=15)</p><p class="source-code">pipe1 = make_pipeline(coltrans, rfe, nb)</p></li>
<li>Let’s evaluate this model with K-fold cross-validation. We get okay scores, though not as good as those with support vector classification in the previous chapter:<p class="source-code">kf = RepeatedStratifiedKFold(n_splits=7,n_repeats=10,\</p><p class="source-code">  random_state=0)</p><p class="source-code">scores = cross_validate(pipe1, X_train, \</p><p class="source-code">  y_train.values.ravel(), \</p><p class="source-code">  scoring=['accuracy','precision','recall','f1'], \</p><p class="source-code">  cv=kf, n_jobs=-1)</p><p class="source-code">print("accuracy: %.2f, precision: %.2f, </p><p class="source-code">  sensitivity: %.2f, f1: %.2f"  %</p><p class="source-code">  (np.mean(scores['test_accuracy']),\</p><p class="source-code">   np.mean(scores['test_precision']),\</p><p class="source-code">   np.mean(scores['test_recall']),\</p><p class="source-code">   np.mean(scores['test_f1'])))</p><p class="source-code"><strong class="bold">accuracy: 0.81, precision: 0.84, sensitivity: 0.83, f1: 0.83</strong></p></li>
<li>With <a id="_idIndexMarker1083"/>Gaussian naïve Bayes, there is only one hyperparameter we must worry about tuning. We can determine how much smoothing to use with the <strong class="source-inline">var_smoothing</strong> hyperparameter. We can do a randomized grid search to figure out the best value.</li>
</ol>
<p>The <strong class="source-inline">var_smoothing</strong> hyperparameter determines how much is added to variances, which will cause models to be less dependent on instances close to mean values:</p>
<p class="source-code">nb_params = {</p>
<p class="source-code">    'gaussiannb__var_smoothing': np.logspace(0,-9, num=100)</p>
<p class="source-code">}</p>
<p class="source-code">rs = RandomizedSearchCV(pipe1, nb_params, cv=kf, \</p>
<p class="source-code">  scoring='accuracy')</p>
<p class="source-code">rs.fit(X_train, y_train.values.ravel())</p>
<ol>
<li value="8">We <a id="_idIndexMarker1084"/>get somewhat better accuracy:<p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{'gaussiannb__var_smoothing': 0.657933224657568}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">0.8608648056923919</strong></p></li>
<li>We should also take a look at the results from the different iterations. As we can see, larger smoothing values do better:<p class="source-code">results = \</p><p class="source-code">  pd.DataFrame(rs.cv_results_['mean_test_score'], \</p><p class="source-code">    columns=['meanscore']).\</p><p class="source-code">  join(pd.DataFrame(rs.cv_results_['params'])).\</p><p class="source-code">  sort_values(['meanscore'], ascending=False)</p><p class="source-code">results</p><p class="source-code"><strong class="bold">         meanscore     gaussiannb__var_smoothing</strong></p><p class="source-code"><strong class="bold">2        0.86086        0.65793</strong></p><p class="source-code"><strong class="bold">1        0.85118        0.03511</strong></p><p class="source-code"><strong class="bold">9        0.81341        0.00152</strong></p><p class="source-code"><strong class="bold">5        0.81212        0.00043</strong></p><p class="source-code"><strong class="bold">7        0.81180        0.00019</strong></p><p class="source-code"><strong class="bold">8        0.81169        0.00002</strong></p><p class="source-code"><strong class="bold">3        0.81152        0.00000</strong></p><p class="source-code"><strong class="bold">6        0.81152        0.00000</strong></p><p class="source-code"><strong class="bold">0        0.81149        0.00000</strong></p><p class="source-code"><strong class="bold">4        0.81149        0.00000</strong></p></li>
<li> We can <a id="_idIndexMarker1085"/>also look at the average fit and score times for each iteration:<p class="source-code">print("fit time: %.3f, score time: %.3f"  %</p><p class="source-code">  (np.mean(rs.cv_results_['mean_fit_time']),\</p><p class="source-code">  np.mean(rs.cv_results_['mean_score_time'])))</p><p class="source-code"><strong class="bold">fit time: 0.660, score time: 0.029</strong></p></li>
<li>Let’s look at the predictions for the best model. In addition to improving accuracy, there is an improvement in sensitivity, from <strong class="source-inline">0.83</strong> to <strong class="source-inline">0.92</strong>:<p class="source-code">pred = rs.predict(X_test)</p><p class="source-code">print("accuracy: %.2f, sensitivity: %.2f, \</p><p class="source-code">  specificity: %.2f, precision: %.2f"  %</p><p class="source-code">  (skmet.accuracy_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred, \</p><p class="source-code">     pos_label=0),</p><p class="source-code">  skmet.precision_score(y_test.values.ravel(), pred)))</p><p class="source-code"><strong class="bold">accuracy: 0.86, sensitivity: 0.92, specificity: 0.79, precision: 0.83</strong></p></li>
<li>It is a good idea to also look at a confusion matrix to get a better sense of how the model does:<p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = skmet.ConfusionMatrixDisplay(</p><p class="source-code">  confusion_matrix=cm, display_labels=['Loss', 'Won'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(title='Home Team Win Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This <a id="_idIndexMarker1086"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer243">
<img alt="Figure 14.1 – Confusion matrix for home team wins based on the Gaussian naïve Bayes model " height="444" src="image/B17978_14_0011.jpg" width="523"/>
</div>
</div>
<p class="figure-caption">Figure 14.1 – Confusion matrix for home team wins based on the Gaussian naïve Bayes model</p>
<p>This is not bad, though still not as good as our support vector model in the previous chapter. In particular, we would like to do a little better at predicting losses. This is also reflected in the relatively low specificity score of <strong class="bold">0.79</strong> that we saw in the previous step. Recall that <a id="_idIndexMarker1087"/>specificity is the rate at which we correctly predict negative values out of the actual negatives.</p>
<p>On the other hand, the fitting and scoring ran quite swiftly. We also did not need to do much hyperparameter tuning. Naïve Bayes can often be a good place to start when modeling either a binary or multiclass target.</p>
<p>Naïve Bayes has turned out to be an even more popular option for text classification. We will use it for that purpose in the next section.</p>
<h1 id="_idParaDest-165"><a id="_idTextAnchor167"/>Naïve Bayes for text classification</h1>
<p>It is perhaps<a id="_idIndexMarker1088"/> surprising that an algorithm based on calculating conditional probabilities could be useful for text classification. But this follows fairly straightforwardly with a key simplifying assumption. Let’s assume that our documents can be well represented by the counts of each word in the document, without regard for word order or grammar. This is known as a bag-of-words. The relationship that a bag-of-words has to a categorical target – say, spam/not spam or positive/negative – can be modeled successfully with multinomial naïve Bayes.  </p>
<p>We will work with text message data in this section. The dataset we will use contains labels for spam and not spam messages.</p>
<p class="callout-heading">Note</p>
<p class="callout">This dataset on text messages can be downloaded by the public at <a href="https://www.kaggle.com/datasets/team-ai/spam-text-message-classification">https://www.kaggle.com/datasets/team-ai/spam-text-message-classification</a>. It contains two columns: the text message and the spam or not spam (ham) label.</p>
<p>Let’s do some text classification with naïve Bayes:</p>
<ol>
<li value="1">We will <a id="_idIndexMarker1089"/>need a couple of modules that we have not used so far in this book. We will import <strong class="source-inline">MultinomialNB</strong>, which we will need to construct a multinomial naïve Bayes model. We will also need <strong class="source-inline">CountVectorizer</strong> to create a bag-of-words. We will import<a id="_idIndexMarker1090"/> the <strong class="source-inline">SMOTE</strong> module to handle class imbalance. Note that we will use an <em class="italic">imbalanced-learn</em> pipeline<a id="_idIndexMarker1091"/> rather than a <em class="italic">scikit-learn</em> one. This is because we will be using <strong class="source-inline">SMOTE</strong> in our pipeline:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from imblearn.pipeline import make_pipeline</p><p class="source-code">from imblearn.over_sampling import SMOTE</p><p class="source-code">from sklearn.naive_bayes import MultinomialNB</p><p class="source-code">from sklearn.feature_extraction.text import CountVectorizer</p><p class="source-code">import sklearn.metrics as skmet</p><p class="callout-heading">Note</p><p class="callout">We are using <strong class="source-inline">SMOTE</strong> in this section to do oversampling; that is, we will be duplicating instances in underrepresented classes. Oversampling can be a good option when we are concerned that our model is doing a poor job of capturing variation in a class because we have too few instances of that class, relative to one or more other classes. Oversampling duplicates instances of that class.</p></li>
<li>Next, we will load the text message dataset. We will convert our target into an integer variable and confirm that it worked as expected. Note the significant class imbalance. Let’s look at the first few rows to get a better feel for the data:<p class="source-code">spamtext = pd.read_csv("data/spamtext.csv")</p><p class="source-code">spamtext['spam'] = np.where(spamtext.category=='spam',1,0)</p><p class="source-code">spamtext.groupby(['spam','category']).size()</p><p class="source-code"><strong class="bold">spam  category</strong></p><p class="source-code"><strong class="bold">0     ham         4824</strong></p><p class="source-code"><strong class="bold">1     spam         747</strong></p><p class="source-code"><strong class="bold">dtype: int64</strong></p><p class="source-code">spamtext.head()</p><p class="source-code"> <strong class="bold">category  message                                spam</strong></p><p class="source-code"><strong class="bold">0  ham     Go until jurong point, crazy..         0</strong></p><p class="source-code"><strong class="bold">1  ham     Ok lar... Joking wif u oni...          0</strong></p><p class="source-code"><strong class="bold">2  spam    Free entry in 2 a wkly comp to win...  1</strong></p><p class="source-code"><strong class="bold">3  ham     U dun say so early hor... U c already..0</strong></p><p class="source-code"><strong class="bold">4  ham     Nah I don't think he goes to usf, ..   0</strong></p></li>
<li>Now, we<a id="_idIndexMarker1092"/> create training <a id="_idIndexMarker1093"/>and testing DataFrames. We will use the <strong class="source-inline">stratify</strong> parameter to ensure equal distributions of target values in the training and testing data. </li>
</ol>
<p>We will also instantiate a <strong class="source-inline">CountVectorizer</strong> object to create our bag-of-words later. We indicate that we want some words to be ignored because they do not provide useful information. We could have created a stop word list, but here, we will take advantage of scikit-learn’s list of stop words in English:</p>
<p class="source-code">X_train, X_test, y_train, y_test =  \</p>
<p class="source-code">  train_test_split(spamtext[['message']],\</p>
<p class="source-code">  spamtext[['spam']], test_size=0.2,\</p>
<p class="source-code">  stratify=spamtext[['spam']], random_state=0)</p>
<p class="source-code">countvectorizer = CountVectorizer(analyzer='word', \</p>
<p class="source-code">  stop_words='english')</p>
<ol>
<li value="4">Let’s look at how the vectorizer works with a couple of observations from our data. To make it easier to view, we will only pull from messages that contain fewer than 50 characters.</li>
</ol>
<p>Using <a id="_idIndexMarker1094"/>the vectorizer, we get counts<a id="_idIndexMarker1095"/> for all non stop words used for each observation. For example, <strong class="source-inline">like</strong> is used once in the first message and not at all in the second. This gives <strong class="source-inline">like</strong> a value of <strong class="source-inline">1</strong> for the first observation in the transformed data and a value of <strong class="source-inline">0</strong> for the second observation.</p>
<p>We won’t use anything from this step in our model. We are only doing this for illustrative purposes:</p>
<p class="source-code">smallsample = \</p>
<p class="source-code">  X_train.loc[X_train.message.str.len()&lt;50].\</p>
<p class="source-code">    sample(2, random_state=35)</p>
<p class="source-code">smallsample</p>
<p class="source-code"><strong class="bold">                                        message</strong></p>
<p class="source-code"><strong class="bold">2079                I can take you at like noon</strong></p>
<p class="source-code"><strong class="bold">5393  I dont know exactly could you ask chechi.</strong></p>
<p class="source-code">ourvec = \</p>
<p class="source-code">    pd.DataFrame(countvectorizer.\</p>
<p class="source-code">    fit_transform(smallsample.values.ravel()).\</p>
<p class="source-code">    toarray(),\</p>
<p class="source-code">    columns=countvectorizer.get_feature_names())</p>
<p class="source-code">ourvec</p>
<p class="source-code"><strong class="bold">    ask   chechi  dont   exactly  know  like  noon</strong></p>
<p class="source-code"><strong class="bold">0    0    0       0      0        0     1     1</strong></p>
<p class="source-code"><strong class="bold">1    1    1       1      1        1     0     0</strong></p>
<ol>
<li value="5">Now, let’s<a id="_idIndexMarker1096"/> instantiate a <strong class="source-inline">MultinomialNB</strong> object <a id="_idIndexMarker1097"/>and add it to a pipeline. We will add oversampling using <strong class="source-inline">SMOTE</strong> to handle the class imbalance:<p class="source-code">nb = MultinomialNB()</p><p class="source-code">smote = SMOTE(random_state=0)</p><p class="source-code">pipe1 = make_pipeline(countvectorizer, smote, nb)</p><p class="source-code">pipe1.fit(X_train.values.ravel(), </p><p class="source-code">  y_train.values.ravel())</p></li>
<li>Now, let’s look at some predictions. We get an impressive <strong class="bold">0.97</strong> accuracy rate and equally good specificity. This excellent specificity suggests that we do not have many false positives. The somewhat lower sensitivity indicates that we are not catching some of the positives (the spam messages), though we are still doing quite well:<p class="source-code">pred = pipe1.predict(X_test.values.ravel())</p><p class="source-code">print("accuracy: %.2f, sensitivity: %.2f, specificity: %.2f, precision: %.2f"  %</p><p class="source-code">  (skmet.accuracy_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred),</p><p class="source-code">  skmet.recall_score(y_test.values.ravel(), pred, pos_label=0),</p><p class="source-code">  skmet.precision_score(y_test.values.ravel(), pred)))</p><p class="source-code"><strong class="bold">accuracy: 0.97, sensitivity: 0.87, specificity: 0.98, precision: 0.87</strong></p></li>
<li>It is helpful<a id="_idIndexMarker1098"/> to visualize our model’s <a id="_idIndexMarker1099"/>performance with a confusion matrix:<p class="source-code">cm = skmet.confusion_matrix(y_test, pred)</p><p class="source-code">cmplot = skmet.ConfusionMatrixDisplay(</p><p class="source-code">  confusion_matrix=cm, \</p><p class="source-code">  display_labels=['Not Spam', 'Spam'])</p><p class="source-code">cmplot.plot()</p><p class="source-code">cmplot.ax_.set(</p><p class="source-code">  title='Spam Prediction Confusion Matrix', </p><p class="source-code">  xlabel='Predicted Value', ylabel='Actual Value')</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer244">
<img alt="Figure 14.2 – Spam prediction using multinomial naïve Bayes " height="439" src="image/B17978_14_0021.jpg" width="564"/>
</div>
</div>
<p class="figure-caption">Figure 14.2 – Spam prediction using multinomial naïve Bayes</p>
<p>Naïve Bayes <a id="_idIndexMarker1100"/>can yield excellent results <a id="_idIndexMarker1101"/>when constructing a text classification model. The metrics are often quite good and quite efficient. This was a very straightforward binary classification problem. However, naïve Bayes can also be effective with multiclass text classification problems. The algorithm can be applied in pretty much the same way as we did here with multiclass targets.</p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor168"/>Summary</h1>
<p>Naïve Bayes is a great algorithm to add to our regular toolkit for solving classification problems. It is not often the approach that will produce predictions with the least bias. However, the flip side is also true. There is less risk of overfitting, particularly when working with continuous features. It is also quite efficient, scaling well to a large number of observations and a large feature space.</p>
<p>The next two chapters of this book will explore unsupervised learning algorithms – those where we do not have a target to predict. In the next chapter, we will examine principal component analysis, and then K-means clustering in the chapter after that.</p>
</div>
</div>
</body></html>