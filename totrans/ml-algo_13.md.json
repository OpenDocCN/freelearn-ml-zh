["```py\nfrom nltk.corpus import brown\n\n>>> sentences = brown.sents(categories=['news'])[0:500]\n>>> corpus = []\n\n>>> for s in sentences:\n>>>   corpus.append(' '.join(s))\n```", "```py\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n>>> vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', norm='l2', sublinear_tf=True)\n>>> Xc = vectorizer.fit_transform(corpus).todense()\n```", "```py\nfrom scipy.linalg import svd\n\n>>> U, s, V = svd(Xc, full_matrices=False)\n```", "```py\nimport numpy as np\n\n>>> rank = 2\n\n>>> Uk = U[:, 0:rank]\n>>> sk = np.diag(s)[0:rank, 0:rank]\n>>> Vk = V[0:rank, :]\n```", "```py\n>>> Mtwks = np.argsort(Vk, axis=1)[::-1]\n\n>>> for t in range(rank):\n>>>   print('\\nTopic ' + str(t))\n>>>     for i in range(10):\n>>>        print(vectorizer.get_feature_names()[Mtwks[t, i]])\n\nTopic 0\nsaid\nmr\ncity\nhawksley\npresident\nyear\ntime\ncouncil\nelection\nfederal\n\nTopic 1\nplainfield\nwasn\ncopy\nreleased\nabsence\nafrica\nclash\nexacerbated\nfacing\ndifficulties\n```", "```py\n>>> Mtwks = np.argsort(np.abs(Vk), axis=1)[::-1]\n```", "```py\n>>> print(corpus[0])\nThe Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n\n>>> Mdtk = Uk.dot(sk)\n\n>>> print('d0 = %.2f*t1 + %.2f*t2' % (Mdtk[0][0], Mdtk[0][1]))\nd0 = 0.15*t1 + -0.12*t2\n```", "```py\nsentences = brown.sents(categories=['news', 'fiction'])\ncorpus = []\n\nfor s in sentences:\n corpus.append(' '.join(s))\n```", "```py\nfrom sklearn.decomposition import TruncatedSVD\n\n>>> tsvd = TruncatedSVD(n_components=rank)\n>>> Xt = tsvd.fit_transform(Xc)\n```", "```py\n>>> Mtws = np.argsort(tsvd.components_, axis=1)[::-1]\n\n>>> for t in range(rank):\n>>>    print('\\nTopic ' + str(t))\n>>>       for i in range(10):\n>>>          print(vectorizer.get_feature_names()[Mwts[t, i]])\n\nTopic 0\nsaid\nrector\nhans\naloud\nliston\nnonsense\nleave\nwhiskey\nchicken\nfat\n\nTopic 1\nbong\nvarnessa\nschoolboy\nkaboom\nkeeeerist\naggravated\njealous\nhides\nmayonnaise\nfowl\n```", "```py\nimport numpy as np\n\nnp.random.seed(1234)\n```", "```py\n>>> sentences_1 = brown.sents(categories=['editorial'])[0:10]\n>>> sentences_2 = brown.sents(categories=['fiction'])[0:10]\n>>> corpus = []\n\n>>> for s in sentences_1 + sentences_2:\n>>>    corpus.append(' '.join(s))\n```", "```py\nimport numpy as np\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n>>> cv = CountVectorizer(strip_accents='unicode', stop_words='english')\n>>> Xc = np.array(cv.fit_transform(corpus).todense())\n```", "```py\n>>> rank = 2\n>>> alpha_1 = 1000.0\n>>> alpha_2 = 10.0\n\n>>> Ptd = np.random.uniform(0.0, 1.0, size=(len(corpus), rank))\n>>> Pwt = np.random.uniform(0.0, 1.0, size=(rank, len(cv.vocabulary_)))\n>>> Ptdw = np.zeros(shape=(len(cv.vocabulary_), len(corpus), rank))\n\n>>> for d in range(len(corpus)):\n>>>    nf = np.sum(Ptd[d, :])\n>>>    for t in range(rank):\n>>>       Ptd[d, t] /= nf\n\n>>> for t in range(rank):\n>>>    nf = np.sum(Pwt[t, :])\n>>>    for w in range(len(cv.vocabulary_)):\n>>>       Pwt[t, w] /= nf\n```", "```py\n>>> def log_likelihood():\n>>>    value = 0.0\n>>> \n>>>    for d in range(len(corpus)):\n>>>       for w in range(len(cv.vocabulary_)):\n>>>          real_topic_value = 0.0\n>>>\n>>>          for t in range(rank):\n>>>             real_topic_value += Ptd[d, t] * Pwt[t, w]\n>>>\n>>>          if real_topic_value > 0.0:\n>>>             value += Xc[d, w] * np.log(real_topic_value)\n>>> \n>>>    return value\n```", "```py\n>>> def expectation():\n>>>    global Ptd, Pwt, Ptdw\n>>>\n>>>    for d in range(len(corpus)):\n>>>       for w in range(len(cv.vocabulary_)):\n>>>          nf = 0.0\n>>> \n>>>          for t in range(rank):\n>>>             Ptdw[w, d, t] = Ptd[d, t] * Pwt[t, w]\n>>>             nf += Ptdw[w, d, t]\n>>> \n>>>          Ptdw[w, d, :] = (Ptdw[w, d, :] / nf) if nf != 0.0 else 0.0\n```", "```py\n>>> def maximization():\n>>>    global Ptd, Pwt, Ptdw\n>>>\n>>>    for t in range(rank):\n>>>       nf = 0.0\n>>> \n>>>       for d in range(len(corpus)):\n>>>          ps = 0.0\n>>> \n>>>          for w in range(len(cv.vocabulary_)):\n>>>             ps += Xc[d, w] * Ptdw[w, d, t]\n>>> \n>>>          Pwt[t, w] = ps\n>>>          nf += Pwt[t, w]\n>>>\n>>>       Pwt[:, w] /= nf if nf != 0.0 else alpha_1\n>>>\n>>>    for d in range(len(corpus)):\n>>>       for t in range(rank):\n>>>          ps = 0.0\n>>>          nf = 0.0\n>>>\n>>>          for w in range(len(cv.vocabulary_)):\n>>>             ps += Xc[d, w] * Ptdw[w, d, t]\n>>>             nf += Xc[d, w]\n>>> \n>>>          Ptd[d, t] = ps / (nf if nf != 0.0 else alpha_2)\n```", "```py\n>>> print('Initial Log-Likelihood: %f' % log_likelihood())\n\n>>> for i in range(50):\n>>>    expectation()\n>>>    maximization()\n>>>    print('Step %d - Log-Likelihood: %f' % (i, log_likelihood()))\n\nInitial Log-Likelihood: -1242.878549\nStep 0 - Log-Likelihood: -1240.160748\nStep 1 - Log-Likelihood: -1237.584194\nStep 2 - Log-Likelihood: -1236.009227\nStep 3 - Log-Likelihood: -1234.993974\nStep 4 - Log-Likelihood: -1234.318545\nStep 5 - Log-Likelihood: -1233.864516\nStep 6 - Log-Likelihood: -1233.559474\nStep 7 - Log-Likelihood: -1233.355097\nStep 8 - Log-Likelihood: -1233.218306\nStep 9 - Log-Likelihood: -1233.126583\nStep 10 - Log-Likelihood: -1233.064804\nStep 11 - Log-Likelihood: -1233.022915\nStep 12 - Log-Likelihood: -1232.994274\nStep 13 - Log-Likelihood: -1232.974501\nStep 14 - Log-Likelihood: -1232.960704\nStep 15 - Log-Likelihood: -1232.950965\n...\n```", "```py\n>>> Pwts = np.argsort(Pwt, axis=1)[::-1]\n\n>>> for t in range(rank):\n>>>    print('\\nTopic ' + str(t))\n>>>       for i in range(5):\n>>>          print(cv.get_feature_names()[Pwts[t, i]])\n\nTopic 0\nyears\nquestions\nsouth\nreform\nsocial\n\nTopic 1\nconvened\nmaintenance\npenal\nyear\nlegislators\n```", "```py\n>>> sentences_1 = brown.sents(categories=['reviews'])[0:1000]\n>>> sentences_2 = brown.sents(categories=['government'])[0:1000]\n>>> sentences_3 = brown.sents(categories=['fiction'])[0:1000]\n>>> sentences_4 = brown.sents(categories=['news'])[0:1000]\n>>> corpus = []\n\n>>> for s in sentences_1 + sentences_2 + sentences_3 + sentences_4:\n>>>    corpus.append(' '.join(s))\n```", "```py\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n>>> cv = CountVectorizer(strip_accents='unicode', stop_words='english', analyzer='word', token_pattern='[a-z]+')\n>>> Xc = cv.fit_transform(corpus)\n\n>>> lda = LatentDirichletAllocation(n_topics=8, learning_method='online', max_iter=25)\n>>> Xl = lda.fit_transform(Xc)\n```", "```py\n>>> Mwts_lda = np.argsort(lda.components_, axis=1)[::-1]\n\n>>> for t in range(8):\n>>>    print('\\nTopic ' + str(t))\n>>>       for i in range(5):\n>>>          print(cv.get_feature_names()[Mwts_lda[t, i]])\n\nTopic 0\ncode\ncadenza\nunlocks\nophthalmic\nquo\n\nTopic 1\ncountless\nharnick\nleni\naddle\nchivalry\n\nTopic 2\nevasive\nerrant\ntum\nrum\norations\n\nTopic 3\ngrigory\ntum\nabsurdity\ntarantara\nsuitably\n\nTopic 4\nseventeenth\nconant\nchivalrous\njanitsch\nknight\n\nTopic 5\nhypocrites\nerrantry\nadventures\nknight\nerrant\n\nTopic 6\ncounter\nrogues\ntum\nlassus\nwars\n\nTopic 7\npitch\ncards\ncynicism\nsilences\nshrewd\n```", "```py\n>>> print(corpus[0])\nIt is not news that Nathan Milstein is a wizard of the violin .\n\n>>> print(corpus[2500])\nThe children had nowhere to go and no place to play , not even sidewalks .\n```", "```py\n>>> print(Xl[0])\n[ 0.85412134 0.02083335 0.02083335 0.02083335 0.02083335 0.02083677\n 0.02087515 0.02083335]\n\n>>> print(Xl[2500])\n[ 0.22499749 0.02500001 0.22500135 0.02500221 0.025 0.02500219\n 0.02500001 0.42499674]\n```", "```py\n>>> test_doc = corpus[0] + ' ' + corpus[2500]\n>>> y_test = lda.transform(cv.transform([test_doc]))\n\n>>> print(y_test)\n[[ 0.61242771 0.01250001 0.11251451 0.0125011 0.01250001 0.01250278\n 0.01251778 0.21253611]]\n```", "```py\n>>> dataset = 'dataset.csv'\n\n>>> corpus = []\n>>> labels = []\n\n>>> with open(dataset, 'r', encoding='utf-8') as df:\n>>>    for i, line in enumerate(df):\n>>>    if i == 0:\n>>>       continue\n>>> \n>>>    parts = line.strip().split(',')\n>>>    labels.append(float(parts[1].strip()))\n>>>    corpus.append(parts[3].strip())\n```", "```py\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.lancaster import LancasterStemmer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\n>>> rt = RegexpTokenizer('[a-zA-Z0-9\\.]+')\n>>> ls = LancasterStemmer()\n>>> sw = set(stopwords.words('english'))\n\n>>> def tokenizer(sentence):\n>>>    tokens = rt.tokenize(sentence)\n>>>    return [ls.stem(t.lower()) for t in tokens if t not in sw]\n\n>>> tfv = TfidfVectorizer(tokenizer=tokenizer, sublinear_tf=True, ngram_range=(1, 2), norm='l2')\n>>> X = tfv.fit_transform(corpus[0:100000])\n>>> Y = np.array(labels[0:100000])\n\n>>> X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport multiprocessing\n\n>>> rf = RandomForestClassifier(n_estimators=20, n_jobs=multiprocessing.cpu_count())\n>>> rf.fit(X_train, Y_train)\n```", "```py\nfrom sklearn.metrics import precision_score, recall_score\n\n>>> print('Precision: %.3f' % precision_score(Y_test, rf.predict(X_test)))\nPrecision: 0.720\n\n>>> print('Recall: %.3f' % recall_score(Y_test, rf.predict(X_test)))\nRecall: 0.784 \n```", "```py\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n>>> text = 'This is a very interesting and quite powerful sentiment analyzer'\n\n>>> vader = SentimentIntensityAnalyzer()\n>>> print(vader.polarity_scores(text))\n{'neg': 0.0, 'neu': 0.535, 'pos': 0.465, 'compound': 0.7258} \n```"]