- en: Neural Networks on Mobile
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 2](1b52495b-c6cb-4197-8fcd-a1e764c1f1c2.xhtml), *Supervised and
    Unsupervised Learning Algorithms,* when we introduced you to TensorFlow, its components,
    and how it works, we talked briefly about **convolutional neural networks** (**CNNs**) and
    how they work. In this chapter, we will delve into the basic concepts of neural
    networks. We will explore the similarities and variations between machine learning
    and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: We will also go through some of the challenges of executing deep learning algorithms
    on mobile devices. We will briefly go through the various deep learning and neural
    network SDKs available for mobile applications that can be run on mobile devices
    directly. Toward the end of this chapter, we will create an interesting assignment
    that will utilize both TensorFlow and Core ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a TensorFlow image recognition model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converting the TensorFlow model into a Core ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an iOS mobile application that utilizes the Core ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a handwritten digit recognition solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we are going to implement all of the major topics we have gone
    through in this book. Before proceeding, make sure you have gone through all the
    previous chapters in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network is a system of hardware and/or software that is modeled on
    the operation of neurons in the human brain. The design behind neural networks
    is inspired by the human brain and its functionality. Let's understand the design
    of the human brain. The neuron is the basic working unit of the brain. It's a
    specialized cell that can transmit information to other nerve cells. The brain
    is made up of approximately 100,000,000,000 neurons. A neuron's main function
    is to process and transmit information.
  prefs: []
  type: TYPE_NORMAL
- en: Communication steps of  a neuron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neuron communication follows a four-step path:'
  prefs: []
  type: TYPE_NORMAL
- en: A neuron receives information from the external environment or from other neurons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neuron integrates, or processes, the information from all of its input and
    determines whether to send an output signal. This integration takes place both
    in time (the duration of the input and the time between input) and space (across
    the surface of the neuron).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neuron propagates the signal along its length at a high speed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neuron converts this electrical signal to a chemical one and transmits it
    to another neuron or to an effect such as a muscle or gland.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get a better understanding of how neurons—the basic building blocks of the
    human brain—work, check out [http://www.biologyreference.com/Mo-Nu/Neuron.html#ixzz5ZD78t97u](http://www.biologyreference.com/Mo-Nu/Neuron.html#ixzz5ZD78t97u).
  prefs: []
  type: TYPE_NORMAL
- en: Now, coming to the neurons' artificial neural networks, the function of these
    neurons is to take in some input and fire an output.
  prefs: []
  type: TYPE_NORMAL
- en: The activation function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To express this categorically, the neuron is a placeholder function that takes
    in inputs, processes them by applying the function on the input, and produces
    the output. Any simple function can be put in the defined placeholder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78af82c6-15f8-4bd3-b35b-b92752f37f05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The function that''s used in a neuron is generally called an activation function. In
    the human body, there are three types of neurons: sensory neurons, motor neurons,
    and interneurons. In the artificial world, the activation function would probably
    create the different capability and functionality of the neuron.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few commonly used activation functions:'
  prefs: []
  type: TYPE_NORMAL
- en: step
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tanh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU-Rectified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear Unit (used mostly in deep learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is outside the scope of this book to delve into the details of each function.
    However, it will be good for you to understand these functions and their intricacies
    if you want to study neural networks further.
  prefs: []
  type: TYPE_NORMAL
- en: Arrangement of neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's look at the arrangement of neurons in the human body. A typical neuron
    has several dendrites, normally arranged in an extremely branched fashion, in
    order to establish contact with many other neurons. Neurons in the human body
    are also arranged in layers. The number of these layers varies across different
    parts of the body and brain, but normally is ranges from three to six layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the artificial world, these neurons are also arranged as layers. The following
    diagram will help you understand the organization of neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9de5a4d1-7ae1-4dca-9f82-aed22bd28c18.png)'
  prefs: []
  type: TYPE_IMG
- en: The leftmost layer of the network is called the **input layer**, and the rightmost
    layer is called the **output layer**. The middle layer of neurons is called the **hidden
    layer** because its values are not observed in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: In this sample neural network, there are three inputs, three hidden units, and
    one output unit. Any neural network will have at least one input and one output
    layer. The number of hidden layers can vary.
  prefs: []
  type: TYPE_NORMAL
- en: The activation function used in each hidden layer can be different for the same
    network. This means that the activation function for hidden layer 1 and the b
    activation function for hidden layer 2 of the same network.
  prefs: []
  type: TYPE_NORMAL
- en: Types of neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Neural networks vary based on the number of hidden layers and the activation
    functions used in each layer. Here are some of the common types of neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deep neural networks**: Networks with more than one hidden layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CNN**: Commonly used in computer-vision-related learning problems. The CNN
    hidden layer uses convolution functions as the activation function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent neural networks**: Commonly used in problems related to natural
    language processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Current projects/research in the field of improving neural networks in mobile
    devices include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileNet V2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MNasNet—implementing reinforcement learning in mobile devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image recognition solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine you go to a restaurant with your friends. Assume you are a fitness freak
    and though you have come to the party to enjoy the buffet, as a fitness freak,
    you are calorie conscious and don't want to go overboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, imagine you have a mobile application that comes to your rescue: it takes
    a picture of the dish, identifies its ingredients, and calculates the caloric
    value of the food! You could take a picture of every dish and calculate its caloric
    value and can then decide whether to put it on your plate. Further, this app keeps
    on learning the different dishes that you take pictures of and continues to learn
    and master itself in this trade so that it can take very good care of your health.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I can see the sparkle in your eyes. Yes, this is the mobile application we
    want to try in this chapter. We also want to utilize both TensorFlow and Core
    ML to accomplish this activity. We will be performing the following steps to create
    the application that we just discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the TensorFlow image recognition model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert it into a `.ml` model file
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an iOS/SWIFT app to use that model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We will go through each of these steps in detail in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a TensorFlow image recognition model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is an open source software library for data flow programming across
    a range of tasks. It is a symbolic math library and is also used for machine learning
    applications, such as neural networks. It is used for both research and production
    at Google, often replacing its closed source predecessor, DistBelief. TensorFlow
    was developed by the Google Brain team for internal Google use. It was released
    under the Apache 2.0 open source license on November 9, 2015.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow is cross-platform. It runs on nearly everything: GPUs and CPUs–including
    mobile and embedded platforms–and even **tensor processing units** (**TPUs**),
    which are specialized hardware for performing tensor math.'
  prefs: []
  type: TYPE_NORMAL
- en: What does TensorFlow do?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To keep it simple, let''s assume you want two numbers. Now, if you want to
    write a program in a regular programming language, such as Python, you would use
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*a = 1*'
  prefs: []
  type: TYPE_NORMAL
- en: '*b = 2*'
  prefs: []
  type: TYPE_NORMAL
- en: '*print(a+b)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the program, you will see the output as *3*, and then you''ll see
    the same implementation on `tensorflow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let me explain the preceding code. First, we are creating a constant with node
    name `x`, adding `5` to it, and storing it in another variable/node `y`. If you
    can see the output of the console of y at this point, you will find the definition
    of the node, but not the value of 40.
  prefs: []
  type: TYPE_NORMAL
- en: Here, you are defining the nodes of the graph and its corresponding operations.
    You can make use of the graph once you initialize the variables and create and
    get a session/instance of the graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram will help you understand this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30384711-7433-40c5-823d-61651eae7f2e.png)'
  prefs: []
  type: TYPE_IMG
- en: In TensorFlow, all of the constants, placeholders, and variables we will use
    to create the definition and the linkage between nodes will create one graph,
    which is just like your class concept in object-oriented programming. Think of
    the graph as a class and the nodes as data members, `tf.globalvariableinitilizer()`
    as calling the static method to initialize the constants and variable, and `session.run()`
    as calling the constructor of a class.
  prefs: []
  type: TYPE_NORMAL
- en: Retraining the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To create an image classifier, we need to go through many things and do a lot
    of coding. To keep it simple, we will be showing you how to create it using the
    Google Code Lab provided code. The following content was taken from Google's Code
    Lab tutorial.
  prefs: []
  type: TYPE_NORMAL
- en: This was made using CNNs. Explaining all of this is outside the scope of this
    book. We briefly explored CNN in the introduction of this chapter. However that
    is very less, compared to what is an ocean. For more information, interested readers
    can check out [https://colah.github.io/posts/2014-07-Conv-Nets-Modular/](https://colah.github.io/posts/2014-07-Conv-Nets-Modular/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how easily we can create an image classifier in `tensorflow`. To
    get started, we need to install anaconda and then run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you run the preceding command, you will get the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3c99d03-b167-4c91-8900-b847e37cbf0c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Type `y` to proceed. Once the command has successfully executed, you will see
    the following screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8cd762ea-10d1-40bb-82c2-41f442366a7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Type `activate` project. Once the project has been activated, you will see
    the prompt, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, type the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following command to verify the installed packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It has to produce the following result. If you don''t see some of these packages
    in your machine, reinstall them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e896abf-5d1e-4f8b-b96e-020738b91779.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we have successfully installed `tensorflow` and its dependencies. Let's
    get the code from Google Code Labs that will do the classification. For this,
    make sure you have installed Git on your machine. There are several ways to install
    it, but the simplest way is through `npm`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check that Git is properly installed, type `git` in the opened command prompt.
    You will see all the options available for that command. If it is prompting as
    `invalid command`, please try to install it correctly. Now, let''s execute the
    command to clone the repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you are done, go to `tensorflow-for-poets-2` using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The following folder contains all of the that are scripts required to train
    a model for image recognition. If you check the `tf_file` folder, it will be empty.
    Here, we will be using this folder to keep the training images and train the model
    using the scripts in the scripts folder.
  prefs: []
  type: TYPE_NORMAL
- en: To input the images, you need to first download the images. For our sample,
    we are using food images with four class labels. You can download it from our
    Git repository, `project/food_photos`, and then paste that folder into `tf_files`.
    If you are unable to execute this command, open the folder in Internet Explorer,
    and then download the in `tensorflow-for-poets-2/tf_files` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the files into flat files, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6af75a10-bc53-4b36-9597-133b76e45a2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we are going to retrain the model using the following script. Execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous Python script is used to retrain a model that has many arguments,
    but we will use and discuss only a few important arguments, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bottleneck_dir`: This will save these files to the bottlenecks/ directory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`how_many_training_steps`: This will a number below 4,000\. A higher number
    will give your model greater accuracy, but takes too much time to build, and the
    model file will be too big.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model_dir`:This tells us where to save the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summaries_dir`: Contains the training summaries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_graph`:Where to save the output graph. This is the resultant model
    that we will use in mobiles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`output_labels`:This is the file that holds the class labels. Usually, the
    class label for an image is the folder name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`architecture`:This tells us which architecture to use. Here, we are using
    the mobilenet model with a 0.50 relative size of the model and a 244 image size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image_dir`:Inputs the images directory, in this case, `food_photos`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Executing the previous command will give you the following as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8238586a-3f12-49b0-8fc9-2eac78dba889.png)'
  prefs: []
  type: TYPE_IMG
- en: About bottlenecks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will try to understand how the retraining process works. The ImageNet
    models we are using are made up of many layers stacked on top of each other. These
    layers are pre-trained and already have sufficient information that will help
    in image classification. All we are trying to do is train the very last layer, `final_training_ops` , when
    all the previous layers retrain their already trained state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot is taken from TensorBoard. You can open TensorBoard
    in your browser to get a better look at it. You will find it in the Graphs tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61b41369-49ff-4f91-9bd5-e310aaf4f503.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, the softmax node on the left-hand side is the output
    layer of the original model. All the nodes to the right of softmax were added
    by the retraining script.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this will only work after the retrain script finishes generating the
    **bottleneck** files.
  prefs: []
  type: TYPE_NORMAL
- en: Bottleneck is the term used to refer to the layer just before the final output
    layer that does the classification. Bottleneck does not imply its conventional
    meaning of something that slows down the whole process. We use the term bottleneck
    because, near the output, the representation is much more compact than in the
    main body of the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every image is reused multiple times during training. Calculating the layers
    behind the bottleneck for each image takes a significant amount of time. Since
    these lower layers of the network are not being modified, their output can be
    cached and reused. Now, you have the TensorFlow retrained model in your hand.
    Let''s test the model that we just trained using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Executing the previous code block will give you the class that the food image
    belongs to. Now, let''s go to the next task: converting the `tensorflow` model
    into the Core ML format.'
  prefs: []
  type: TYPE_NORMAL
- en: Converting the TensorFlow model into the Core ML model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TensorFlow team has developed a package that is used to convert the models
    created in TensorFlow into Core ML, which in through is used in iOS apps. To use
    this, you must have macOS with Python 3.6 and TensorFlow installed. Using this,
    we can convert the TensorFlow model file (`.pb`) into the Core ML format (`.mlmodel`).
    First, you need to execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this is installed, write the following code in your Python file, name
    it `inspect.py`, and save it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will take the model file as an input argument, and save
    all the operations and input/output node names with a description in a text file
    that we supply as input. To run this, enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In this command, you are executing the `inspect.py` code you saved before. This
    will also input the graph file obtained from the previous section and, path of
    a text file where you want to save the summaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you execute this command, `summeries.txt` will be created with all the
    summaries, as shown here. These will be added into that file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b83a234-07e4-42b9-8eb8-5e64f215a9fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this file, you can see all the operations, input and output names, and their
    shapes; you can also see the overall operators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba72d5a4-2aab-4881-9f57-f3df430b5dd2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Toward the end of the file, you will find the definition of the end node; in
    our case, it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e6d02c2-44c4-4846-bf2b-7c3c39f4e891.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can see that the end node operation type is `Softmax`, and the output
    that it will produce will be stored in the `final_result:0` name. Now, check out
    the following code block, which is used to generate a corresponding Core ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c1fdba4-e5f6-40ec-a2fe-db4c29da5825.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand the previous code block in detail. You must have noticed
    that we imported the `tfcoreml` package in the first line, and then used its **convert**
    function. The following are its arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Tf_model_path`: The (`.pb`) file path that you generated in the previous section, *Converting
    the TensorFlow model into the Core ML model*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Mlmodel_path`: The output model file path where you want to generate the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Output_feature_names`: In this, we will get the output variable name that
    you obtained from the previous text file that was generated by our model-inspection
    code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Image_input_names`: Name you want to give for the image input. In Core ML/iOS,
    this will be the image buffer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Class_labels`: This is the file you will get in the training step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you run the preceding code, you will see the generated `converted``.mlmodel`
    file in your directory. You can import this into your Xcode project and make use
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the iOS mobile application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to create an app to make use of the image recognition
    model that we've created to predict images using your iOS mobile camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, you need a Mac PC running Xcode version 9+. Download the source code
    (x-code project) from the Git repository and navigate to the project folder. Open
    the `recognition.xcodeproj` image in Xcode. The following screenshot shows the
    folder structure of the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d539950-49bf-4128-9e77-b64d2d50d73d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The main file we are going to view is `con``troller.swift`. It contains the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the outlets for the image-view control and title-label control in
    the main storyboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the instance of the model that was generated when we added the `core-ml`
    file we created in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We hardcoded the contents to display in the title label for the corresponding
    class label we trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the images we have added to the project; they''ll serve as input
    for our prediction app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As we trained our model with 224 px images, we are also resizing the images
    of the input and converting it into an image buffer, which we want to give to
    the prediction method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we are inputting the image and getting the prediction results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, depending on the class label, we are displaying the
    content to the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This completes the application''s creation. Now, we will execute the application
    to find the following images as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10423fe5-1a92-4003-a7fd-b82cdca9a808.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Click on Next to find the our next image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ac3a3ed-ef50-4d24-88d0-4cb66649060e.png)'
  prefs: []
  type: TYPE_IMG
- en: Handwritten digit recognition solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, we created an application that helped us get insights into the implementation
    of a neural network image recognition program using the TensorFlow model for mobile
    devices. Now, we will create another application that uses the concept of a neural
    network and Keras for an image recognition program of handwritten digits. In this
    section, we will create an application for a handwritten digit recognition solution
    on mobile devices using Keras. Then, we will convert this Keras model into a Core
    ML model and use it to build an iOS mobile application. Let's start by introducing
    you to Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is a high-level neural network API, written in Python and capable of running
    on top of TensorFlow, CNTK, or Theano. It was developed with the aim of enabling
    fast experimentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the key uses of Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: Allows for easy and fast prototyping (through user-friendliness, modularity,
    and extensibility)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supports both convolutional networks and recurrent networks, as well as a combination
    of the two
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs seamlessly on CPU and GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keras was designed on the following principles:'
  prefs: []
  type: TYPE_NORMAL
- en: User-friendliness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modularity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy extensibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compatibility with Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn more about Keras, check out [https://keras.io/](https://keras.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Installing Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we already discussed, Keras doesn't have its own backend system. As it is
    running on top of TensorFlow, CNTK, or Theano, we need to install one of these—personally,
    we recommend TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to install the `h5py` library, with the help of the `pip` package manager,
    in order to save the Keras models to disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding commands will install the basic required libraries for the model,
    which we are going to create now.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to see a practical implementation of a neural
    network. We will define the problem statement, then we will understand the dataset
    we are going to use to solve the problem, whereupon we will create the model in
    Keras to solve the problem. Once the model is created in Keras, we will convert
    it into a model that's compatible with Core ML. This Core ML model will be imported
    into an iOS application, and a program will be written to use this model and interpret
    the handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the problem statement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are going to tackle the problem of recognizing handwritten digits through
    a machine learning model that we'll implement in an iOS mobile application. The
    first step is to have the database of handwritten digits that can be used for
    model training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: The MNIST digits dataset ([http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/))
    provides a database of handwritten digits, and has a training set of 60,000 examples
    and a test set of 10,000 examples. It is a subset of a larger set that's available
    from MNIST. The digits have been size-normalized and centered in a fixed-size
    image. It is a good database for people who want to learn techniques and pattern
    recognition methods on real-world data while exerting minimal effort on preprocessing
    and formatting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before solving this problem, we will spend some time understanding the problem
    to see where the neural network can help. We can split the problem of recognizing
    handwritten digits into two sub-problems. Suppose we are given a handwritten number,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11dd29c2-990c-4b00-a60e-24afae4c2880.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we need to break an image containing many digits into a sequence of
    separate images, each containing a single digit. For example, we''d like to break
    this image into seven separate images, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae98dc4e-966b-481c-a6ee-c709aff7d024.png)'
  prefs: []
  type: TYPE_IMG
- en: For humans, the digits can be easily separated, but it is very challenging for
    machines to do this simple task. Once the digits are separated, the program needs
    to classify each individual digit. So, for instance, we'd like our program to
    recognize that the first digit is a **5**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are now trying to focus on the second part of the problem: to recognize
    the individual digits and classify them. We are going to use a neural network
    to solve the problem of recognizing individual, handwritten digits.'
  prefs: []
  type: TYPE_NORMAL
- en: We can solve this problem using a 3-layer neural network, with the output layers
    having 10 neurons. The input layer and the hidden layers are where the processing
    happens. in the output layer, based on the neuron that fires, we can easily infer
    the digit that was recognized. Neurons 0 to 9 each identify one digit.
  prefs: []
  type: TYPE_NORMAL
- en: Problem solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The problem solution consists of the following key steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training and fitting the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting the trained Keras model into a Core ML model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Writing the iOS mobile application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's go through the steps one by one and see what we need to do in each
    of these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first activity is the data preparation. To start, let''s import all the
    required libraries. As we discussed earlier, we are going to use the MNIST database
    for the dataset of handwritten digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`mnist` is the dataset that contains the handwritten digits database, so we
    need to import that, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code imports the `Sequential` model type from Keras. This is
    simply a linear stack of neural network layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to import the core layers from Keras. These are the layers that
    are used in almost any neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the CNN layers from Keras. These are the convolutional layers that will
    help us efficiently train on image data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Import Utils. This will help us do data transformation later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '`coremltools` will help us convert the Keras model into the Core ML model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the pre-shuffled MNIST data into train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: If you run the preceding code, it will show the shape of X, Y, and also the
    first record of X.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have 60,000 samples in our training set, and the images are 28 x 28
    pixels each. We can confirm this by plotting the first sample in `matplotlib`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60504c64-3e01-40f5-8435-9597e1956068.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This statement will use the `matplotlib` library to plot the first record of `x_train`,
    which will give the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a458001-b1a7-4714-8b7c-a5d60ec0a224.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following lines will print the `y_train` shape and the first 10 elements
    in `y_train`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will find the input shape of the image. The MNIST image
    data values are of the `uint8` type, in the *[0, 255]* range, but Keras needs
    values of the `float32` type in the *[0, 1]* range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the following code, we are converting the datatype to be compatible with
    the datatype that is defined in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have a one-dimensional of 60,000 elements in `y`. Let''s convert it
    into a 60,000 x 10 array, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, `y_train` will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba98ae88-78d7-428e-90ae-8f42166b6a71.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding array, we can find that for the presence of digits, the corresponding
    position will be filled with 1—all others will be filled with 0\. For the first
    record, we can understand that the predicted digit is 5, because the 6th position
    (starting from 0) was filled with 1.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data preparation is complete, we need to define the model's architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the model's architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the data preparation is completed, the next step is to define the model
    and create it, so let''s create the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line will create a sequential model that will process the layers
    in the sequential way they are arranged. There are two ways to build Keras models, sequential and functional:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The sequential API**: This allows us to create models layer-by-layer. Through
    this, we cannot create models that share layers or have multiple input or output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The functional ****API**: This allows us to create models that are more than
    and can have complex connection layers—you can literally connect from any layer
    to any other layer:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The input shape parameter should be the shape of `1` sample. In this case, it's
    the same `(1, 28, 28)`, which corresponds to the (depth, width, height) of each
    digit image.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what do the other parameters represent? They correspond to the number of
    convolutional filters to use, the number of rows in each convolution kernel, and
    the number of columns in each convolution kernel, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '`MaxPooling2D` is a way to reduce the number of parameters in our model by
    sliding a 2 x 2 pooling filter across the previous layer and taking the max of
    the 4 values in the 2 x 2 filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a method for regularizing our model in order to prevent overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you run the preceding lines of code, the model architecture''s names of
    the layers will be printed in the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2d7f91a-f566-4497-bec5-356f3064bb0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Compiling and fitting the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step is to compile and train the model. We put the model through the
    training phase with a series of iterations. Epochs determine the number of iterations
    to be done on a model in the training phase. The weights will be passed to the
    layers defined in the model. A good number of Epochs will give greater accuracy
    and minimum loss. Here, we are using 10 Epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras has a callback mechanism that will be called during each training iteration
    of the model, that is, at the end of each Epoch. In the callback method, we save
    the computed weights of that Epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, compile the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The `categorical_crossentropy` loss function measures the distance between the
    probability distribution calculated by the CNN, and the true distribution of the
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'An `optimizer` is the stochastic gradient descent algorithm that tries to minimize
    the loss function by following the gradient at just the right speed. `accuracy` the
    fraction of the images that were correctly classified—this is the most common
    metric monitored during training and testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, fit the model using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Once the program finishes executing, you will find files in your running directory
    with the `best_model.01-0.15.h5` name. This states `best_model.{epoch number}-{loss
    value}.h5`.
  prefs: []
  type: TYPE_NORMAL
- en: This the Keras model that was created and trained for the given dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Converting the Keras model into the Core ML model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the Keras model has been created, the next step is to convert the
    Keras model into the Core ML model. For the first argument, use the filename of
    the newest `.h5` file in the notebook folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Once you successfully run the code, you will find the `minsit_classifer.mlmodel` file
    created in your directory. We are going to use this to create an iOS mobile application
    to detect the digits.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the iOS mobile application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, we are going to create the iOS app. You can download the code from our
    Packt GitHub repository in the `ImageClassificationwithVisionandCoreML` folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the project in Xcode9+; the project structure will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e59ce2a-bd3d-4f4d-b86f-ce982b01de99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you open `main.storyboard` in your designer, you will see the following
    UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bade5a63-117d-4e93-ad49-65b8b28b932f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Most code is common iOS code. Check out the following piece of code, which
    is of specific interest to us, and includes the handwritten digit prediction code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'It contains two buttons at the bottom: one to pick an image from mobile and
    another option to take a snapshot. Please note that the camera will not work if
    you are running this in simulators.'
  prefs: []
  type: TYPE_NORMAL
- en: You can build and run the app in a simulator. Once the app successfully opens
    in a simulator, drag the image of the handwritten digit 6 into the folder example
    image into the simulator–this will save the file in the simulator's memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Return to the app and select the dragged image that was saved in the device''s
    memory. It will show the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/add4696c-39dc-4307-9f16-b797634bef23.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the concept of neural networks and their use in
    the field of mobile machine learning. We created an application to recognize images
    using TensorFlow and Core ML in iOS and Xcode. We also explored the Keras deep
    learning framework. We tried to solve the handwritten digit recognition problem
    using a neural network in Keras. We built the Keras machine learning model to
    solve this problem. Then, we converted this model into a Core ML model using Core
    ML conversion tools. We used this Core ML model in an iOS mobile application to
    perform the handwritten digit recognition.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to use the Google Cloud Vision label
    detection technique in Android.
  prefs: []
  type: TYPE_NORMAL
