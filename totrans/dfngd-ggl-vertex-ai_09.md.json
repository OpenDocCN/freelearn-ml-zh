["```py\nimport numpy as np\nimport glob\nimport matplotlib.pyplot as plt\nimport os\nimport google.cloud.aiplatform as aiplatform\nfrom google.cloud.aiplatform import hyperparameter_tuning as hpt\nfrom datetime import datetime\nTIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n%matplotlib inline\n```", "```py\nPROJECT_ID='************'\nREGION='us-west2'\nSERVICE_ACCOUNT='417xxxxxxxxx7-compute@developer.gserviceaccount.com'\nBUCKET_URI='gs://my-training-artifacts'\n```", "```py\naiplatform.init(project=PROJECT_ID, location=REGION, \\\n    staging_bucket=BUCKET_URI)\n```", "```py\ndef get_args():\n    '''Parses args. Must include all hyperparameters you want to tune.'''\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n      '--epochs',\n      required=True,\n      type=int,\n      help='training epochs')\n    parser.add_argument(\n      '--steps_per_epoch',\n      required=True,\n      type=int,\n      help='steps_per_epoch')\n```", "```py\n    parser.add_argument(\n      '--learning_rate',\n      required=True,\n      type=float,\n      help='learning rate')\n    parser.add_argument(\n      '--batch_size',\n      required=True,\n      type=int,\n      help='training batch size')\n    parser.add_argument(\n      '--loss',\n      required=True,\n      type=str,\n      help='loss function')\n    args = parser.parse_args()\n    return args\n```", "```py\ndef make_datasets_unbatched():\n    # Load train, validation and test sets\n    dest = 'gs://data-bucket-417812395597/'\n    train_x = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'train_x', \\\n            binary_mode=True)\n    ))\n    train_y = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'train_y', \\\n            binary_mode=True)\n    ))\n```", "```py\n    val_x = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'val_x', \\\n            binary_mode=True)\n    ))\n    val_y = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'val_y', \\\n            binary_mode=True)\n    ))\n    test_x = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'test_x', \\\n            binary_mode=True)\n    ))\n    test_y = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'test_y', \\\n            binary_mode=True)\n    ))\n    return train_x, train_y, val_x, val_y, test_x, test_y\n```", "```py\ndef tf_model():\n    black_n_white_input = tensorflow.keras.layers.Input(shape=(80, 80, 1))\n    enc = black_n_white_input\n```", "```py\n    #Encoder part\n    enc = tensorflow.keras.layers.Conv2D(\n        32, kernel_size=3, strides=2, padding='same'\n    )(enc)\n    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n    enc = tensorflow.keras.layers.Conv2D(\n        64, kernel_size=3, strides=2, padding='same'\n    )(enc)\n    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n```", "```py\n    enc = tensorflow.keras.layers.Conv2D(\n        128, kernel_size=3, strides=2, padding='same'\n    )(enc)\n    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n    enc = tensorflow.keras.layers.Conv2D(\n        256, kernel_size=1, strides=2, padding='same'\n    )(enc)\n    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n    enc = tensorflow.keras.layers.Dropout(0.5)(enc)\n```", "```py\n    #Decoder part\n    dec = enc\n    dec = tensorflow.keras.layers.Conv2DTranspose(\n        256, kernel_size=3, strides=2, padding='same'\n    )(dec)\n    dec = tensorflow.keras.layers.Activation('relu')(dec)\n    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n    dec = tensorflow.keras.layers.Conv2DTranspose(\n        128, kernel_size=3, strides=2, padding='same'\n    )(dec)\n    dec = tensorflow.keras.layers.Activation('relu')(dec)\n    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n```", "```py\n    dec = tensorflow.keras.layers.Conv2DTranspose(\n        64, kernel_size=3, strides=2, padding='same'\n    )(dec)\n    dec = tensorflow.keras.layers.Activation('relu')(dec)\n    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n    dec = tensorflow.keras.layers.Conv2DTranspose(\n        32, kernel_size=3, strides=2, padding='same'\n    )(dec)\n    dec = tensorflow.keras.layers.Activation('relu')(dec)\n    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n    dec = tensorflow.keras.layers.Conv2D(\n        3, kernel_size=3, padding='same'\n    )(dec)\n```", "```py\n    color_image = tensorflow.keras.layers.Activation('tanh')(dec)\n    return black_n_white_input, color_image\n```", "```py\n# Build the and compile TF model\ndef build_and_compile_tf_model(loss_fn, learning_rate):\n    black_n_white_input, color_image = tf_model()\n    model = tensorflow.keras.models.Model(\n        inputs=black_n_white_input,\n        outputs=color_image\n    )\n    _optimizer = tensorflow.keras.optimizers.Adam(\n        learning_rate=learning_rate,\n        beta_1=0.5\n    )\n    model.compile(\n        loss=loss_fn,\n        optimizer=_optimizer\n    )\n    return model\n```", "```py\ndef main():\n    args = get_args()\n```", "```py\n    NUM_WORKERS = strategy.num_replicas_in_sync\n    # Global batch size should be scaled as per the number     # of workers used in training.    GLOBAL_BATCH_SIZE = args.batch_size * NUM_WORKERS\n    MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n    train_x, train_y, val_x, val_y, _, _ = \\\n        make_datasets_unbatched()\n```", "```py\n    with strategy.scope():\n        # Creation of dataset, and model building/compiling need to be within\n        # `strategy.scope()`.\n        model = build_and_compile_tf_model(args.loss, \\\n            args.learning_rate)\n    history = model.fit(\n        train_x,\n        train_y,\n        batch_size=GLOBAL_BATCH_SIZE,\n        epochs=args.epochs,\n        steps_per_epoch=args.steps_per_epoch,\n        validation_data=(val_x, val_y),\n    )\n    model.save(MODEL_DIR)\n```", "```py\n    # DEFINE HPT METRIC\n    hp_metric = history.history['val_loss'][-1]\n    hpt = hypertune.HyperTune()\n    hpt.report_hyperparameter_tuning_metric(\n      hyperparameter_metric_tag='val_loss',\n      metric_value=hp_metric,\n      global_step=args.epochs)\n```", "```py\n%%writefile task.py\n# Single, Mirror and Multi-Machine Distributed Training\n```", "```py\nimport tensorflow as tf\nimport tensorflow\nfrom tensorflow.python.client import device_lib\nimport argparse\nimport os\nimport sys\nfrom io import BytesIO\nimport numpy as np\nfrom tensorflow.python.lib.io import file_io\nimport hypertune\n```", "```py\ndef get_args():\n    '''Parses args. Must include all hyperparameters you want to tune.'''\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n      '--epochs',\n      required=True,\n      type=int,\n      help='training epochs')\n    parser.add_argument(\n      '--steps_per_epoch',\n      required=True,\n      type=int,\n      help='steps_per_epoch')\n```", "```py\n    parser.add_argument(\n      '--learning_rate',\n      required=True,\n      type=float,\n      help='learning rate')\n    parser.add_argument(\n      '--batch_size',\n      required=True,\n      type=int,\n      help='training batch size')\n    parser.add_argument(\n      '--loss',\n      required=True,\n      type=str,\n      help='loss function')\n    args = parser.parse_args()\n    return args\n```", "```py\nprint('Python Version = {}'.format(sys.version))\nprint('TensorFlow Version = {}'.format(tf.__version__))\nprint('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', \\\n    'Not found')))\nprint('DEVICES', device_lib.list_local_devices())\n```", "```py\nDISTRIBUTE='single'\nif DISTRIBUTE == 'single':\n    if tf.test.is_gpu_available():\n        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n    else:\n        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n# Single Machine, multiple compute device\nelif DISTRIBUTE == 'mirror':\n    strategy = tf.distribute.MirroredStrategy()\n# Multiple Machine, multiple compute device\nelif DISTRIBUTE == 'multi':\n    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n```", "```py\n# Preparing dataset\nBUFFER_SIZE = 10000\ndef make_datasets_unbatched():\n    # Load train, validation and test sets\n    dest = 'gs://data-bucket-417812395597/'\n    train_x = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'train_x', \\\n            binary_mode=True)\n    ))\n    train_y = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'train_y', \\\n            binary_mode=True)\n    ))\n```", "```py\n    val_x = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'val_x', \\\n            binary_mode=True)\n    ))\n    val_y = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'val_y', \\\n            binary_mode=True)\n    ))\n    test_x = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'test_x', \\\n            binary_mode=True)\n    ))\n    test_y = np.load(BytesIO(\n        file_io.read_file_to_string(dest+'test_y', \\\n            binary_mode=True)\n    ))\n    return train_x, train_y, val_x, val_y, test_x, test_y\n```", "```py\ndef tf_model():\n    black_n_white_input = tensorflow.keras.layers.Input(shape=(80, 80, 1))\n    enc = black_n_white_input\n```", "```py\n    #Encoder part\n    enc = tensorflow.keras.layers.Conv2D(\n        32, kernel_size=3, strides=2, padding='same'\n    )(enc)\n    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n    enc = tensorflow.keras.layers.Conv2D(\n        64, kernel_size=3, strides=2, padding='same'\n    )(enc)\n    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n```", "```py\n    enc = tensorflow.keras.layers.Conv2D(\n        128, kernel_size=3, strides=2, padding='same'\n    )(enc)\n    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n    enc = tensorflow.keras.layers.Conv2D(\n        256, kernel_size=1, strides=2, padding='same'\n    )(enc)\n    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n    enc = tensorflow.keras.layers.Dropout(0.5)(enc)\n```", "```py\n     #Decoder part\n    dec = enc\n    dec = tensorflow.keras.layers.Conv2DTranspose(\n        256, kernel_size=3, strides=2, padding='same'\n    )(dec)\n    dec = tensorflow.keras.layers.Activation('relu')(dec)\n    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n    dec = tensorflow.keras.layers.Conv2DTranspose(\n        128, kernel_size=3, strides=2, padding='same'\n    )(dec)\n    dec = tensorflow.keras.layers.Activation('relu')(dec)\n    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n```", "```py\n    dec = tensorflow.keras.layers.Conv2DTranspose(\n        64, kernel_size=3, strides=2, padding='same'\n    )(dec)\n    dec = tensorflow.keras.layers.Activation('relu')(dec)\n    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n    dec = tensorflow.keras.layers.Conv2DTranspose(\n        32, kernel_size=3, strides=2, padding='same'\n    )(dec)\n    dec = tensorflow.keras.layers.Activation('relu')(dec)\n    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n    dec = tensorflow.keras.layers.Conv2D(\n        3, kernel_size=3, padding='same'\n    )(dec)\n```", "```py\n    color_image = tensorflow.keras.layers.Activation('tanh')(dec)\n    return black_n_white_input, color_image\n```", "```py\n`# Build the and compile TF model\ndef build_and_compile_tf_model(loss_fn, learning_rate):\n    black_n_white_input, color_image = tf_model()\n    model = tensorflow.keras.models.Model(\n        inputs=black_n_white_input,\n        outputs=color_image\n    )\n    _optimizer = tensorflow.keras.optimizers.Adam(\n        learning_rate=learning_rate,\n        beta_1=0.5\n    )\n    model.compile(\n        loss=loss_fn,\n        optimizer=_optimizer\n    )\n    return model\n```", "```py\ndef main():\n    args = get_args()\n    NUM_WORKERS = strategy.num_replicas_in_sync\n    # Here the batch size scales up by number of workers since\n    # `tf.data.Dataset.batch` expects the global batch size.\n    GLOBAL_BATCH_SIZE = args.batch_size * NUM_WORKERS\n    MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n```", "```py\n    train_x, train_y, val_x, val_y, _, _ = \\\n        make_datasets_unbatched()\n    with strategy.scope():\n        # Creation of dataset, and model building/compiling need to be within\n        # `strategy.scope()`.\n        model = build_and_compile_tf_model(args.loss, \\\n            args.learning_rate)\n    history = model.fit(\n        train_x,\n        train_y,\n        batch_size=GLOBAL_BATCH_SIZE,\n        epochs=args.epochs,\n        steps_per_epoch=args.steps_per_epoch,\n        validation_data=(val_x, val_y),\n    )\n    model.save(MODEL_DIR)\n```", "```py\n    # DEFINE HPT METRIC\n    hp_metric = history.history['val_loss'][-1]\n    hpt = hypertune.HyperTune()\n    hpt.report_hyperparameter_tuning_metric(\n      hyperparameter_metric_tag='val_loss',\n      metric_value=hp_metric,\n      global_step=args.epochs)\nif __name__ == \"__main__\":\n    main()\n```", "```py\nBUCKET_URI = \"gs://hpt-staging\"  # @param {type:\"string\"}\nif BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}\nGCS_OUTPUT_BUCKET = BUCKET_URI + \"/output/\"\n```", "```py\n%%writefile Dockerfile\nFROM gcr.io/deeplearning-platform-release/tf2-gpu.2-8\nWORKDIR /\n# Installs hypertune library\nRUN pip install cloudml-hypertune\n# Copies the trainer code to the Docker image.\nCOPY task.py .\n# Sets up the entry point to invoke the trainer.\nENTRYPOINT [\"python\", \"-m\", \"task\"]\n```", "```py\nPROJECT_NAME=\"*******-project\"\nIMAGE_URI = (\n    f\"gcr.io/{PROJECT_NAME}/example-tf-hptune:latest\"\n)\n! docker build ./ -t $IMAGE_URI\n! docker push $IMAGE_URI\n```", "```py\n# The spec of the worker pools including machine type and Docker image\n# Be sure to replace PROJECT_ID in the `image_uri` with your project.\nworker_pool_specs = [\n    {\n        \"machine_spec\": {\n            \"machine_type\": \"n1-standard-8\",\n            \"accelerator_type\": None,\n            \"accelerator_count\": 0,\n        },\n        \"replica_count\": 1,\n        \"container_spec\": {\n            \"image_uri\": f\"gcr.io/{PROJECT_NAME}/example-tf-hptune:latest\"\n        },\n    }\n]\n```", "```py\n# Dictionary representing parameters to optimize.\n# The dictionary key is the parameter_id, which is passed into your training\n# job as a command line argument,\n# And the dictionary value is the parameter specification of the metric.\nparameter_spec = {\n    \"learning_rate\": hpt.DoubleParameterSpec(min=0.0001, \\\n        max=0.001, scale=\"log\"),\n    \"epochs\": hpt.DiscreteParameterSpec(values=[10, 20, \\\n        30], scale=None),\n    \"steps_per_epoch\": hpt.IntegerParameterSpec(min=100, \\\n        max=300, scale=\"linear\"),\n    \"batch_size\": hpt.DiscreteParameterSpec(values=[16,32,\\\n        64], scale=None),\n    \"loss\": hpt.CategoricalParameterSpec([\"mse\"]), # we can add other loss values\n}\n```", "```py\nmetric_spec = {\"val_loss\": \"minimize\"}\n```", "```py\nmy_custom_job = aiplatform.CustomJob(\n    display_name=\"example-tf-hpt-job\",\n    worker_pool_specs=worker_pool_specs,\n    staging_bucket=GCS_OUTPUT_BUCKET,\n)\n```", "```py\nhp_job = aiplatform.HyperparameterTuningJob(\n    display_name=\"example-tf-hpt-job\",\n    custom_job=my_custom_job,\n    metric_spec=metric_spec,\n    parameter_spec=parameter_spec,\n    max_trial_count=5,\n    parallel_trial_count=3,\n)\n```", "```py\nhp_job.run()\n```"]