- en: Evaluating Kernel Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In machine learning, pattern finding is an area that is being explored to the
    hilt. There are many methods and algorithms that can drive this kind of work and
    analysis. However, in this chapter, we will try to focus on how kernels are making
    a significant difference to the whole machine learning outlook. The application
    of kernel learning doesn''t have any boundaries: starting from a simple regression
    problem to a computer vision classification, it has made its presence felt everywhere.
    **Support vector machine** (**SVM**) is one of those algorithms that happens to
    make use of kernel learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be focusing on the following concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Concepts of vectors, linear separability, and hyperplanes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel tricks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before moving on to the core topic, we would like to build a foundation for
    getting there. Hence, this segment of the chapter is very important. It might
    look familiar to you and many of you will be cognizant about this. However, going
    through this channel will set the flow.
  prefs: []
  type: TYPE_NORMAL
- en: 'A vector is an object that has both a direction and magnitude. It is represented
    by an arrow and with a coordinate (*x*, *y*) in space, as shown in the following
    plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69d91204-36be-4e74-87ee-6f080307281d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the preceding diagram, the vector OA has the coordinates *(4,3)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Vector OA= (4,3)*'
  prefs: []
  type: TYPE_NORMAL
- en: However, it is not sufficient to define a vector just by coordinates—we also
    need a direction. That means the direction from the *x* axis.
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude of the vector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The magnitude of the vector is also called the **norm**. It is represented
    by *||OA||*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b44d86d0-04f1-4c5d-abdb-4d64361bb275.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To find out magnitude of this vector, we can follow the Pythagorean theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '*OA^(2 )= OB² + AB²*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 4^(2 )+ 3² *'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 16 + 9*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 25*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*OA = √25 = 5*'
  prefs: []
  type: TYPE_NORMAL
- en: '*||OA||= 5*'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if there is a vector *x = (x[1,]x[2],....,x[n])*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*||x||= x[1]^(2 )+ x[2]²+........+x[n]²*'
  prefs: []
  type: TYPE_NORMAL
- en: 'And direction of this vector as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/416b0c98-2520-42ea-9f6f-72e6aff74934.png)'
  prefs: []
  type: TYPE_IMG
- en: Dot product
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dot product of two vectors returns a number that happens to be scalar. It
    is a representation of how two vectors are associated with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometrically, the dot product of two vectors *x* and *y* would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x . y= ||x|| ||y|| cosθ*'
  prefs: []
  type: TYPE_NORMAL
- en: '*θ* is the angle between the vector *x* and *y*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, algebraically, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92a6025d-116f-4815-a073-b003c343e241.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Geometrically, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*θ=β-α*'
  prefs: []
  type: TYPE_NORMAL
- en: '*cosθ=cos(β-α)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*cosθ = cosβ cosα + sinβ sinα*'
  prefs: []
  type: TYPE_NORMAL
- en: '*cosθ = (x[1]/||x||) (y[1]/||y||) + (x2/||x||) (y2/||y||)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*||x||||y|| cosθ= x[1] y[1 ]+ x[2]y[2]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*x . y = x[1] y[1 ]+ x[2]y[2]*'
  prefs: []
  type: TYPE_NORMAL
- en: Linear separability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear separability implies that if there are two classes then there will be
    a point, line, plane, or hyperplane that splits the input features in such a way
    that all points of one class are in one-half space and the second class is in
    the other half-space.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is a case of selling a house based on area and price. We
    have got a number of data points for that along with the class, which is house
    **Sold**/**Not Sold**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c608dc6d-58f1-4548-b5d7-f3ba22fe1709.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding figure, all the **N**, are the class (event) of **Not Sold**,
    which has been derived based on the **Price** and **Area** of the house and all
    the instances of **S** represent the class of the house getting sold. The number
    of **N** and **S** represent the data points on which the class has been determined.
  prefs: []
  type: TYPE_NORMAL
- en: In the first diagram, **N** and **S** are quite close and happen to be more
    random, hence, it's difficult to have linear separability achieved as no matter
    how you try to separate two classes, at least one of them would be in the misclassified
    region. It implies that there won't be a correct possible line to separate the
    two. But the second diagram depicts datasets that can easily be separated based
    on given conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Separation methodology changes based on the number of dimensions. If there
    is just one dimensional situation, we can have a point separating classes. Adding
    more dimensions will require a different arrangement to split the class. Once
    we have got a 2D situation, a line (as seen previously) will be required to separate
    it. Similarly, more than 2D will need a plane (a set of points) in order to separate
    the classes, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffd8af10-e582-402b-8887-197f5abb2b79.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Separation method:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number of dimensions** | **Separation method** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Point |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Line |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Plane |'
  prefs: []
  type: TYPE_TB
- en: What if we have more than 3D? What do we do? What's the solution? Any guesses?
  prefs: []
  type: TYPE_NORMAL
- en: Hyperplanes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many of you will have guessed it right. We use hyperplanes when it comes to
    more than 3D. We will define it using a bit of mathematics.
  prefs: []
  type: TYPE_NORMAL
- en: A linear equation looks like this: *y = ax + b* has got two variables, *x* and
    *y*, and a *y*-intercept, which is *b*. If we rename *y* as *x[2]* and *x* as
    *x[1]*, the equation comes out as *x[2]=ax[1] + b *which implies *ax[1] - x[2]
    + b=0*. If we define 2D vectors as *x= (x[1],x[2])* and *w=(a,-1)* and if we make
    use of the dot product, then the equation becomes *w.x + b = 0.*
  prefs: []
  type: TYPE_NORMAL
- en: Remember, *x.y = x[1]y[1] + x[2]y[2].*
  prefs: []
  type: TYPE_NORMAL
- en: So, a hyperplane is a set of points that satisfies the preceding equation. But
    how do we classify with the help of hyperplane?
  prefs: []
  type: TYPE_NORMAL
- en: 'We define a hypothesis function *h*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*h(x[i]) = +1 if w.x[i] + b ≥ 0*'
  prefs: []
  type: TYPE_NORMAL
- en: '*-1 if w.x[i] + b < 0*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This could be equivalent to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*h(x[i])= sign(w.x[i] + b) *'
  prefs: []
  type: TYPE_NORMAL
- en: 'It could also be equivalent to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*sign(w.x[i]) if (x[0]=1 and w[0]=b)*'
  prefs: []
  type: TYPE_NORMAL
- en: What it means is that it will use the position of *x* with respect to the hyperplane
    to predict a value for *y*. A data point on one side of the hyperplane gets a
    classification and a data point on other side of hyperplane gets another class.
  prefs: []
  type: TYPE_NORMAL
- en: Because it uses the equation of a hyperplane that happens to be the linear combination
    of the values, it is called a **linear classifier**. The shape of hyperplane is
    by *w* as it has elements as b and a responsible for the shape.
  prefs: []
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we are ready to understand SVMs. SVM is an algorithm that enables us to
    make use of it for both classification and regression. Given a set of examples,
    it builds a model to assign a group of observations into one category and others
    into a second category. It is a non-probabilistic linear classifier. Training
    data being linearly separable is the key here. All the observations or training
    data are a representation of vectors that are mapped into a space and SVM tries
    to classify them by using a margin that has to be as wide as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d4cb88e-65ce-4483-81ea-5b640aa0c860.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's say there are two classes **A** and **B** as in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'And from the preceding section, we have learned the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*g(x) = w. x + b*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w*: Weight vector that decides the orientation of the hyperplane'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b*: Bias term that decides the position of the hyperplane in n-dimensional
    space by biasing it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The preceding equation is also called a **linear discriminant function**. If
    there is a vector *x[1]* that lies on the positive side of the hyperplane, the
    equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*g(x[1])= w.x[1] +b >0 *'
  prefs: []
  type: TYPE_NORMAL
- en: 'The equation will become the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*g(x[1])<0*'
  prefs: []
  type: TYPE_NORMAL
- en: If *x[1]* lies on the positive side of the hyperplane.
  prefs: []
  type: TYPE_NORMAL
- en: What if *g(x[1])=0*? Can you guess where *x[1]* would be? Well, yes, it would
    be on the hyperplane, since our goal is to find out the class of the vector.
  prefs: []
  type: TYPE_NORMAL
- en: So, if *g(x[1])>0 => x[1]* belongs to **Class A**, *g(x[1])<0 => x[1]* belongs
    to **Class B**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, it''s evident that we can find out the classification by using the previous
    equation. But can you see the issue in it? Let''s say the boundary line is like
    the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9adf98c4-d994-4397-89f9-a022df614a5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Even in the preceding scenario, we are able to classify those feature vectors
    here. But is it desirable? What can be seen here is that the boundary line or
    the classifier is close to the **Class B**. It implies that it brings in a large
    bias in the favor of **Class A** but penalizes **Class B**. As a result of that,
    due to any disturbances in the vectors close to the boundary, they might cross
    over and become part of **Class A**, which might not be correct. Hence, our goal
    is to find an optimal classifier that has got the widest margin, like what is
    shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d4cb88e-65ce-4483-81ea-5b640aa0c860.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Through SVM, we are attempting to create a boundary or hyperplane such that
    the distance from each of the feature vectors to the boundary is maximized so
    that any slight noise or disturbance won''t cause the change in classification.
    So, in this scenario, if we try to bring in certain *y[i]* which happens to be
    the class belonging to *xi,* we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y[i]= ± 1*'
  prefs: []
  type: TYPE_NORMAL
- en: '*y[i] (w.x[i] + b)* will always be greater than 0. *y[i](w.x[i] + b) >0* because
    when *x[i ]∈ class A*, *w.x[i] +b>0* then *y[i]>0,* so the whole term will be
    positive. Also, if *x[i ]∈ class B*, *w.x[i] + b<0* then *y[i]<0*, and it will
    make the term positive.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now if we have to redesign it, we say the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w.x[i] + b> γ* where *γ* is the measure of the distance of hyperplane from
    *xi*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'And if there is a hyperplane *w.x + b = 0*, then the distance of point *x*
    from the preceding hyperplane is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '* w.x + b/ ||w||*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, as mentioned previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w.x + b/ ||w|| ≥ γ*'
  prefs: []
  type: TYPE_NORMAL
- en: '*w.x + b ≥ γ.||w||*'
  prefs: []
  type: TYPE_NORMAL
- en: 'On performing proper scaling, we can say the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w.x + b ≥ 1 (since γ.||w|| = 1)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'It implies that if there is a classification to be arrived at based on the
    previous result, it follows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*w.x + b ≥ 1 if x ∈ class A and*'
  prefs: []
  type: TYPE_NORMAL
- en: '*w.x + b ≤ -1 if x ∈ class B*'
  prefs: []
  type: TYPE_NORMAL
- en: 'And now, again, if we bring in a class belonging to *y[i]* here, the equation
    becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*yi (w.xi + b) ≥ 1*'
  prefs: []
  type: TYPE_NORMAL
- en: But, if *y[i] (w.x[i] + b) =** 1*, *x[i]* is a support vector. Next, we will
    learn what a support vector is.
  prefs: []
  type: TYPE_NORMAL
- en: Support vector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We draw two boundary lines passing through feature vectors of one class closest
    to the feature vectors of another class. The center line of these boundary lines
    is the hyperplane we have been talking about. For example, for **Class B**, a
    boundary line is passing through **p** and **q** along the way and another boundary
    line through **r** and **s** because **p** and **q** are the closest to the feature
    vectors of **Class B** and so are **r** and **s**. These are called **support
    vectors**. We will understand now why these are called **support vectors**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/480ad417-ed82-49ab-ae3f-fa33bcc91538.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's say that if we try to remove one of the feature vectors that is not so
    close to the boundary line, we will not have an impact on the position or orientation
    of the hyperplane because the hyperplane's position is decided by boundary lines
    crossing through vectors **p**, **q**, **r**, and **s**. And, since these are
    the points holding (supporting) the hyperplane together, they have been named
    support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: So, this equation *y[i] (w.x[i] + b) =** 1* holds true when *x[i]* is **p**, **q**, **r**,
    or **s**.
  prefs: []
  type: TYPE_NORMAL
- en: We will go back to the equation *w.x + b/ ||w|| ≥ γ*; here, we are trying to
    maximize *γ*, and in order to do so either we need to maximize b or minimize *||w||*.
  prefs: []
  type: TYPE_NORMAL
- en: Or we can say we have to minimize *w.w*. If we convert that into a function, *Φ(w)
    = w.w* has to be minimized. *Φ(w) =1/2( w.w)* (here 1/2 has been added for mathematical
    convenience).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the objective function of SVM becomes *Φ(w) =1/2( w.w)*, which has to be
    minimized subject to constraints, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y[i] (w.x[i] + b) = 1*'
  prefs: []
  type: TYPE_NORMAL
- en: Since it is a constrained optimization problem, it can be converted into an
    unconstrained optimization problem using the Lagrangian multiplier.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, *L(w,b)= 1/2(w.w) - ∑ αi [yi(w.xi+b) - 1]* where αi is the Lagrangian
    multiplier, *L(w,b)= 1/2(w.w) - ∑ α[i] y[i] (w.x[i]) -∑ α[i] y[i] b + ∑ α[i]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s find out *w* and *b* by using maxima and minima calculus:'
  prefs: []
  type: TYPE_NORMAL
- en: '*δL/δb = 0 *'
  prefs: []
  type: TYPE_NORMAL
- en: 'It results in* ∑ α[i] y[i]=0, **δL/**δw = 0* would result in *∑ αi yi xi =
    w*.Now, putting these results back into the Lagrangian function yields the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*L= ∑ α[i] - 1/2 ∑ α[i] α[j] y[i] y[j] (x[j].x[i])*'
  prefs: []
  type: TYPE_NORMAL
- en: 'It means that if the value of *α[i]* is very high then the corresponding *x*.There
    will be a lot of influence on the position of the hyperplane. Hence, for classification
    and for unknown feature vector *z*, the required equation would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*D(z) = Sign( ∑ αi xi yi z + b)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If *D(z) >0*, then *z* would belong to class A and if *D(z)<0*, *z ∈ class
    B*. Let''s try to perform a case study in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44b032dc-d35e-4103-8986-55a7e8d79b98.png)'
  prefs: []
  type: TYPE_IMG
- en: Kernel trick
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already seen that SVM works smoothly when it comes to having linear
    separable data. Just have a look at the following figure; it depicts that vectors
    are not linearly separable, but the noticeable part is that it is not being separable
    in 2D space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c938db90-3030-49a0-bcbb-76090683fa47.png)'
  prefs: []
  type: TYPE_IMG
- en: With a few adjustments, we can still make use of SVM here.
  prefs: []
  type: TYPE_NORMAL
- en: Transformation of a two-dimensional vector into a 3D vector or any other higher
    dimensional vector can set things right for us. The next step would be to train
    the SVM using a higher dimensional vector. But the question arises of how high
    in dimension we should go to transform the vector. What this means is if the transformation
    has to be a two-dimensional vector, or 3D or 4D or more. It actually depends on
    the which brings separability into the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A non-separable dataset like the one used previously is always a tough thing
    to deal with, however, there are ways to deal with it. One way is to set the vectors
    into higher dimensions through transformation. But, can we really do it when we
    have millions of data or vector in reckoning? It will take lots of computation
    and, also, time. That's where kernel to saves our day.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have seen the following equation. In this, only the dot product of the training
    examples are responsible for making the model learn. Let''s try to do a small
    exercise here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81286c92-7e64-42d9-931f-29a4232387d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s take two vectors here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now, build a transformation function that will help in transforming these 2D
    vectors into 3D.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function to be used in order to transform is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*t(x1,x2)= (x1²,x1 x2 √2,x2²)*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'But can''t we do this without transforming the values. Kernel can help us in
    doing it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s the time to use this `kernel` now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Isn't it quite thrilling to see such an amazing result that is the same as before,
    without using transformation? So, kernel is a function that leads to the dot-product-like
    result in another space.
  prefs: []
  type: TYPE_NORMAL
- en: Back to Kernel trick
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, now we have got a fair understanding of kernel and its importance. And,
    as discussed in the last section, the `kernel` function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*K(x[i],x[j])= x[i ]. x[j]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now the margin problem becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81b41737-cd77-4e76-8316-b22acd9dc135.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is subject to *0 ≤ α[i] **≤ C*, for any *i = 1, ..., m*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46d3bf1e-49a9-4c32-8561-e063baab823b.png)'
  prefs: []
  type: TYPE_IMG
- en: Applying the kernel trick simply means replacing the dot product of two examples
    with a `kernel` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now even the hypothesis function will change as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1477ee3-964a-4d74-b35a-460418052ad5.png)'
  prefs: []
  type: TYPE_IMG
- en: This function will be able to decide on and classify the categories. Also, since
    *S* denotes the set of support vectors, it implies that we need to compute the
    `kernel` function only on support vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We're going to explain the types of in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Linear kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s say there are two vectors, *x*[*1* ]and *x[2]*, so the linear kernel
    can be defined by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*K(x[1,] x[2])= x[1 . ]x[2]*'
  prefs: []
  type: TYPE_NORMAL
- en: Polynomial kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If there are two vectors, *x**[1]*and *x**[2]*, the linear kernel can be defined
    by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*K(x[1,] x[2])= (x[1 . ]x[2 ]+ c)^d*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*c*: Constant'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*d*: Degree of polynomial:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use the same `x1` and `x2` as used previously, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If we increase the degree of polynomial, we will try to get influenced by other
    vectors as the decision boundary becomes too complex and it will result in overfitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14b37f8d-a6d2-4f1e-8d95-e64b604952e2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/4b52d85d-9734-4bf2-9432-c3ccccadc31c.png)'
  prefs: []
  type: TYPE_IMG
- en: Polynomial kernel using degree as 6.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The polynomial kernel has given us a good boundary line. But can we work with
    polynomial kernels all the time? Not in the following scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5af809b-ac44-4604-9bfa-888ecf1ea487.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The solution is a radial basis function or Gaussian kernel. It''s nothing but
    the similarity function of the vectors to translate them into a high dimensional
    space or infinite dimensional space. Its value depends on the distance from the
    Gaussian kernel function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*K(x,x^'') = exp(-γ ||x-x''||²)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without loss of generality, let ![](img/610290f7-d9ab-43ca-8211-b8623507ae1f.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a59facc-78d6-4edb-bbb9-2d3bd597f660.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/e89bba32-a989-40f4-8517-c3dce9f75b45.png)'
  prefs: []
  type: TYPE_IMG
- en: With the help of this RBF as a similarity function, all the feature vectors
    get calculated.
  prefs: []
  type: TYPE_NORMAL
- en: SVM example and parameter optimization through grid search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we are taking a breast cancer dataset wherein we have classified according
    to whether the cancer is benign/malignant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is for importing all the required libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s load the breast cancer dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following allows us to check the details of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This if for splitting the dataset into train and test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This is for setting the model with the linear kernel and finding out the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the accuracy output as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Setting the model with the Gaussian/RBF kernel and accuracy is done like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s quite apparent that the model is overfitted. So, we will go for normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is for setting up the model again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following shows the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the overfitting issue cannot be seen any more. Let''s move on to having
    an optimal result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With the help of grid search, we get the optimal combination for `gamma`, `kernel`,
    and `C` as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0614335-6cba-4c29-9033-97af3f63bb68.png)'
  prefs: []
  type: TYPE_IMG
- en: With the help of this, we can see and find out which combination of parameters
    is giving us the better result.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the best combination turns out to be a linear kernel with a `C` value
    of `1`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to vectors, magnitude of vector, and the
    dot product. We learned about SVMs that can be used for both classification and
    regression. We studied support vectors and kernels and the different types of
    kernels. Lastly, we studied the SVM example and parameter optimization through
    grid search.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about performance in ensemble learning.
  prefs: []
  type: TYPE_NORMAL
