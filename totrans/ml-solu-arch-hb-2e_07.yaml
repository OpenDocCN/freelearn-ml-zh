- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Open-Source ML Platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we covered how Kubernetes can be used as the foundational
    infrastructure for running ML tasks, such as running model training jobs or building
    data science environments such as **Jupyter Notebook** servers. However, to perform
    these tasks at scale and more efficiently for large organizations, you will need
    to build ML platforms with the capabilities to support the full data science lifecycle.
    These capabilities include scalable data science environments, model training
    services, model registries, and model deployment capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the core components of an ML platform and explore
    additional open-source technologies that can be used for building ML platforms.
    We will begin with technologies designed for building a data science environment
    capable of supporting a large number of users for experimentation. Subsequently,
    we will delve into various technologies for model training, model registries,
    model deployment, and ML pipeline automation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics are covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Core components of an ML platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open-source technologies for building ML platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core components of an ML platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An ML platform is a complex system encompassing multiple environments for running
    distinct tasks and orchestrating complex workflow processes. Furthermore, an ML
    platform needs to cater to a multitude of roles, including data scientists, ML
    engineers, infrastructure engineers, operations teams, and security and compliance
    stakeholders. To construct an ML platform, several components come into play.
  prefs: []
  type: TYPE_NORMAL
- en: 'These components include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data science environment**: The data science environment provides data analysis
    and ML tools, such as Jupyter notebooks, data sources and storage, code repositories,
    and ML frameworks. Data scientists and ML engineers use the data science environment
    to perform data analysis, run data science experiments, and build and tune models.
    The data science environment also provides collaboration capabilities, allowing
    data scientists to share and collaborate on code, data, experiments, and models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model training environment**: The model training environment provides a separate
    infrastructure tailored to meet specific model training requirements. While data
    scientists and ML engineers can execute small-scale model training tasks directly
    within their local Jupyter environment, they need a separate dedicated infrastructure
    for large-scale model training. By utilizing a dedicated training infrastructure,
    organizations can exercise greater control over model training process management
    and model lineage management processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model registry**: Trained models need to be tracked and managed within a
    model registry. The model registry serves as a centralized repository for inventorying
    and managing models, ensuring effective lineage management, version control, model
    discovery, and comprehensive lifecycle management. This becomes particularly significant
    when dealing with a large number of models. Data scientists can register models
    directly in the registry as they perform experiments in their data science environment.
    Additionally, models can be registered as part of automated ML model pipeline
    executions, enabling streamlined and automated integration of models into the
    registry.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model serving environment**: To serve predictions from trained ML models
    to client applications, it is necessary to host the models within a dedicated
    model serving infrastructure that operates behind an API endpoint in real time.
    This infrastructure should also provide support for batch transform capabilities,
    allowing predictions to be processed in large batches. Several types of model
    serving frameworks are available to fulfill these requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML pipeline development:** To effectively manage the various ML components
    and stages in the lifecycle, it is crucial to incorporate capabilities that enable
    pipeline development to orchestrate ML training and prediction workflows. These
    pipelines play an important role in coordinating different stages, such as data
    preparation, model training, and evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model monitoring**: Robust model monitoring is crucial for maintaining the
    high performance of ML models in production. Continuous monitoring tracks metrics
    like prediction accuracy, data drift, latency, errors, and anomalies over time.
    Monitoring enables platform operators to detect production model degradation before
    it impacts users. When monitored metrics cross defined thresholds, alerts trigger
    investigative workflows and mitigation where needed. Effective monitoring also
    provides performance dashboards and visibility into all deployed models. This
    facilitates continuous improvement of models and allows replacing underperforming
    models proactively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML feature management**: Managing features is a key capability in the ML
    lifecycle. Feature management entails the ongoing curation, monitoring, and sharing
    of ML features to accelerate model development. This includes tools for discovery,
    lineage tracking, and governance of feature data. Centralized feature stores democratize
    access to high-quality features by teams across an organization. They provide
    a single source of truth, eliminating duplication of feature engineering efforts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous integration** (**CI**)/**continuous deployment** (**CD**) **and
    workflow automation**: Finally, to streamline the data processing, model training,
    and model deployment processes on an ML platform, it is crucial to establish CI/CD
    practices, along with workflow automation capabilities. These practices and tools
    significantly contribute to increasing the velocity, consistency, reproducibility,
    and observability of ML deployments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these core components, there are several other platform architecture
    factors to consider when building an end-to-end ML platform. These factors include
    security and authentication, version control and reproducibility, and data management
    and governance. By integrating these additional architectural factors into the
    ML platform, organizations can enhance security, gain visibility into system operations,
    and enforce governance policies. In the following sections, we will explore various
    open-source technologies that can be used to build an end-to-end ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Open-source technologies for building ML platforms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing ML tasks individually by deploying standalone ML containers in a Kubernetes
    cluster can become challenging when dealing with a large number of users and workloads.
    To address this complexity and enable efficient scaling, many open-source technologies
    have emerged as viable solutions. These technologies, including Kubeflow, MLflow,
    Seldon Core, GitHub, Feast, and Airflow, provide comprehensive support for building
    data science environments, model training services, model inference services,
    and ML workflow automation.
  prefs: []
  type: TYPE_NORMAL
- en: Before delving into the technical details, let’s first explore why numerous
    organizations opt for open-source technologies to construct their ML platforms.
    For many, the appeal lies in the ability to tailor the platform to specific organizational
    needs and workflows, with open standards and interoperable components preventing
    vendor lock-in and allowing the flexibility to adopt new technologies over time.
    Leveraging popular open-source ML projects also taps into a rich talent pool,
    as many practitioners are already proficient with these technologies. Additionally,
    open-source allows complete control over the platform roadmap to internal teams,
    reducing dependence on a vendor’s priorities. When executed efficiently, an open-source
    stack can lead to cost savings for organizations, as there are no licensing costs
    associated with the software.
  prefs: []
  type: TYPE_NORMAL
- en: Building an ML platform using open-source technology comes with notable advantages.
    However, it’s also crucial to consider the potential drawbacks. Challenges may
    arise from integration complexities, a lack of comprehensive support, security
    vulnerabilities, and potential limitations in features compared to commercial
    solutions. Additionally, the resource-intensive nature of maintaining an open-source
    platform, coupled with a potential learning curve for the team, could impact efficiency
    and total cost of ownership. Concerns about documentation quality, the absence
    of standardization, and the responsibility for updates and maintenance further
    underscore the need for careful consideration. You must weigh these factors against
    the benefits, taking into account their specific requirements, resources, and
    expertise before opting for an open-source approach.
  prefs: []
  type: TYPE_NORMAL
- en: With these considerations in mind, let’s explore the design of core ML platform
    components using open-source technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a data science environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubeflow is an open-source ML platform built on top of Kubernetes. It offers
    a set of tools and frameworks specifically designed to simplify the deployment,
    orchestration, and management of ML workloads. Kubeflow provides features like
    Jupyter notebooks for interactive data exploration and experimentation, distributed
    training capabilities, and model serving infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Core capabilities of Kubeflow include:'
  prefs: []
  type: TYPE_NORMAL
- en: A central UI dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Jupyter Notebook server for code authoring and model building
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kubeflow pipeline for ML pipeline orchestration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KFServing** for model serving'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training operators for model training support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following figure illustrates how Kubeflow can provide the various components
    needed for a data science environment. Specifically, we will delve into its support
    for Jupyter Notebook servers as it is the main building block for a data science
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – A Kubeflow-based data science environment ](img/B20836_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: A Kubeflow-based data science environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubeflow provides a multi-tenant Jupyter Notebook server environment with built-in
    authentication and authorization support. Let’s discuss each of these core components
    in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Jupyter Notebook**: As a data scientist, you can take advantage of the Kubeflow
    Jupyter Notebook server, which offers a platform to author and run your **Python**
    code to explore data and build models inside the Jupyter notebook. With Kubeflow,
    you can spawn multiple notebook servers, each server associated with a single
    Kubernetes namespace that corresponds to a team, project, or individual user.
    Each notebook server runs a container inside a **Kubernetes** **Pod**. By default,
    a Kubeflow notebook server provides a list of notebook container images hosted
    in public container image repositories to choose from. Alternatively, you can
    create custom notebook container images to tailor to your specific requirements.
    To ensure standards and consistency, Kubeflow administrators can provide a list
    of standard images for users to use. When creating a notebook server, you select
    the namespace to run the notebook server in. Additionally, you specify the **Universal
    Resource Identifier** (**URI**) of the container image for the notebook server.
    You also have the flexibility to specify the resource requirements, such as the
    number of CPUs/GPUs and memory size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authentication and authorization**: You access the notebook server through
    the Kubeflow UI dashboard, which provides an authentication service through the
    Dex **OpenID Connect** (**OIDC**) provider. Dex is an identity service that uses
    OIDC to provide authentication for other applications. Dex can federate with other
    authentication services such as the **Active Directory** service. Each notebook
    is associated with a default Kubernetes service account (`default-editor`) that
    can be used for entitlement purposes (such as granting the notebook permission
    to access various resources in the Kubernetes cluster). Kubeflow uses Istio **role-based
    access control** (**RBAC**) to control in-cluster traffic. The following **YAML**
    file grants the `default-editor` service account (which is associated with the
    Kubeflow notebook) access to the Kubeflow pipeline service by attaching the `ml-pipeline-services`
    service role to it:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Multi-tenancy**: Kubeflow offers the capability for multiple users to access
    a shared Kubeflow environment while ensuring resource isolation. This is achieved
    by creating individual namespaces for each user and leveraging Kubernetes RBAC
    and Istio RBAC to manage access control for these namespaces and their associated
    resources. For collaborative work within teams, the owner of a namespace has the
    ability to grant access to other users directly from the Kubeflow dashboard UI.
    Using the `Manage Contributor` function, the namespace owner can specify which
    users are granted access to the namespace and its resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the preceding core components, Kubeflow provides a mechanism
    for onboarding users to access different Kubeflow resources. To onboard a new
    Kubeflow user, you create a new user profile, which automatically generates a
    new namespace for the profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following YAML file, once applied using `kubectl`, creates a new user profile
    called `test-user` with an email of `test-user@kubeflow.org`, and it also creates
    a new namespace called `test-user`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can run the `kubectl get profiles` and `kubectl get namespaces` commands
    to verify that the profile and namespaces have been created.
  prefs: []
  type: TYPE_NORMAL
- en: After a user is created and added to the Kubeflow Dex authentication service,
    the new user can log in to the Kubeflow dashboard and access the Kubeflow resources
    (such as a Jupyter Notebook server) under the newly created namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing Kubeflow for a data science environment poses several key challenges
    that one must be aware of before committing to its implementation. Installing
    Kubeflow on top of Kubernetes, whether on-premises or in the cloud on platforms
    like AWS, can be complicated, often requiring substantial configuration and debugging.
    The many moving parts make installation non-trivial. Kubeflow consists of many
    loosely coupled components, each with its own version. Orchestrating these diverse
    components across different versions to work together seamlessly as an integrated
    platform can present difficulties. There is a lack of documentation on Kubeflow.
    Kubeflow documentation often points to older component versions. The out-of-date
    documentation makes adoption more difficult as new Kubeflow users battle mismatches
    between docs and platform versions. Despite these limitations, Kubeflow is a highly
    recommended technology for building ML platforms due to its support for end-to-end
    pipelines, rich support for different ML frameworks, and portability.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have reviewed how Kubeflow can be used to provide a multi-tenant
    Jupyter Notebook environment for experimentation and model building. Next, let’s
    see how to build a model training environment.
  prefs: []
  type: TYPE_NORMAL
- en: Building a model training environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed earlier, within an ML platform, it is common to provide a dedicated
    model training service and infrastructure to support large-scale and automated
    model training in an ML pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: This dedicated training service should be easily accessible from different components
    within the platform, such as the experimentation environment (such as a Jupyter
    notebook) as well as the ML automation pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a Kubernetes-based environment, there are two main approaches for model
    training you can choose from depending on your training needs:'
  prefs: []
  type: TYPE_NORMAL
- en: Model training using **Kubernetes** **Jobs**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training using **Kubeflow** **training operators**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at each one of these approaches in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model training using Kubernetes Jobs**: As we discussed in *Chapter 6*, *Kubernetes
    Container Orchestration Infrastructure Management*, a Kubernetes Job creates one
    or more containers and runs them through to completion. This pattern is well suited
    for running certain types of ML model training jobs, as an ML job runs a training
    loop to completion and does not run forever. For example, you can package a container
    with a Python training script and all the dependencies that train a model and
    use the Kubernetes Job to load the container and kick off the training script.
    When the script completes and exits, the Kubernetes Job also ends. The following
    sample YAML file kicks off a model training job if submitted with the `kubectl
    apply` command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To query the status of the job and see the detailed training logs, you can run
    the `kubectl get jobs` command and the `kubectl logs <pod name>` command, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model training using Kubeflow training operators**: A Kubernetes Job can
    launch a model training container and run a training script inside the container
    to completion. Since the controller for a Kubernetes Job does not have application-specific
    knowledge about the training job, it can only handle generic Pod deployment and
    management for the running jobs, such as running the container in a Pod, monitoring
    the Pod, and handling generic Pod failure. However, some model training jobs,
    such as distributed training jobs in a cluster, require the special deployment,
    monitoring, and maintenance of stateful communications among various Pods. This
    is where the Kubernetes training operator pattern can be applied.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubeflow offers a list of pre-built training operators (such as the **TensorFlow**,
    **PyTorch**, and **XGBoost** operators) for complex model training jobs. Each
    Kubeflow training operator has a **custom resource** (**CR**) (for example, `TFJob
    CR` for TensorFlow jobs) that defines the training job’s specific configurations,
    such as the type of Pod in the training job (for example, `master`, `worker`,
    or `parameter server`), or runs policies on how to clean up resources and for
    how long a job should run. The controller for the CR is responsible for configuring
    the training environment, monitoring the training job’s specific status, and maintaining
    the desired training job’s specific state. For instance, the controller can set
    environment variables to make the training cluster specifications (for example,
    types of Pods and indices) available to the training code running inside the containers.
    Additionally, the controller can inspect the exit code of a training process and
    fail the training job if the exit code indicates a permanent failure. The following
    YAML file sample template represents a specification for running training jobs
    using the TensorFlow operator (`tf-operator`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this example template, the specification will create one copy of the parameter
    servers (which aggregate model parameters across different containers) and two
    copies of the workers (which run model training loops and communicate with the
    parameter servers). The operator will process the `TFJob` object according to
    the specification, keep the `TFJob` object stored in the system with the actual
    running services and Pods, and replace the actual state with the desired state.
    You can submit the training job using `kubectl apply -f <TFJob specs template>`
    and can get the status of the `TFJob` with the `kubectl get tfjob` command.
  prefs: []
  type: TYPE_NORMAL
- en: As a data scientist, you can submit Kubernetes training jobs or Kubeflow training
    jobs using the `kubectl` utility, or from your Jupyter Notebook environment using
    the **Python SDK**. For example, the `TFJob` object has a Python SDK called `kubernet.tfjob`,
    and Kubernetes has a client SDK called `kubernetes.client` for interacting with
    the Kubernetes and Kubeflow environments from your Python code. You can also invoke
    training jobs using the `Kubeflow Pipeline` component, which we will cover later,
    in the *Kubeflow pipeline* section.
  prefs: []
  type: TYPE_NORMAL
- en: Using Kubernetes Jobs for ML training requires the installation and configuration
    of the necessary ML software components for training different models using different
    frameworks. You will also need to build logging and monitoring capabilities to
    monitor the training progress.
  prefs: []
  type: TYPE_NORMAL
- en: The adoption of Kubeflow training operators also presents its own set of challenges.
    Several operators, including the MPI training operator, are still in the maturing
    phase and are not yet suitable for production adoption. While operators provide
    certain logs and metrics, obtaining a comprehensive view of extensive training
    runs across Pods remains challenging and necessitates the integration of multiple
    dashboards. The existence of separate operators for various ML frameworks with
    fragmented capabilities and statuses complicates achieving a unified experience.
  prefs: []
  type: TYPE_NORMAL
- en: The learning curve for running training operators can be high, as it involves
    understanding many components, such as the development of training YAML files,
    distributed training job configuration, and training job monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Registering models with a model registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **model registry** is an important component in model management and governance,
    and it is a key link between the model training stage and the model deployment
    stage, as models need to be properly stored in a managed repository for a governed
    model deployment. There are several open-source options for implementing a model
    registry in an ML platform. In this section, we will explore MLflow Model Registry
    for model management.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow Model Registry is one of the leading model registry solutions with strong
    support in model artifacts lifecycle management, versioning support, and a range
    of deployment targets like Docker, Amazon SageMaker, and Azure ML. It is designed
    for managing all stages of the ML lifecycle, including experiment management,
    model management, reproducibility, and model deployment. It has the following
    four main components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Experiment tracking**: During model development, data scientists run many
    training jobs as experiments using different datasets, algorithms, and configurations
    to find the best working model. Tracking the inputs and outputs of these experiments
    is critical to ensure efficient progress. Experiment tracking logs the parameters,
    code versions, metrics, and artifacts when running your ML code and for later
    visualizing of the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML projects**: Projects package data science code in a format to reproduce
    runs on any platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models:** Models provide a standard unit for packaging and deploying ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Registry:** Model Registry stores, annotates, discovers, and manages
    models in a central repository.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Model Registry component of MLflow provides a central model repository for
    saved models. It captures model details such as model lineage, model version,
    annotation, and description, and also captures model stage transitions from staging
    to production (so the status of the model state is clearly described).
  prefs: []
  type: TYPE_NORMAL
- en: To use MLflow Model Registry in a team environment, you need to set up an MLflow
    tracking server with a database as a backend and storage for the model artifacts.
    MLflow provides a UI and an API to interact with its core functionality, including
    Model Registry. Once the model is registered in Model Registry, you can add, modify,
    update, transition, or delete the model through the UI or the API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows an architecture setup for an MLflow tracking server
    and its associated Model Registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – The MLflow tracking server and model registry ](img/B20836_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: The MLflow tracking server and Model Registry'
  prefs: []
  type: TYPE_NORMAL
- en: MLflow supports basic HTTP authentication to enable access control over experiments
    and registered models. The MLflow tracking server also supports basic authentication.
    However, these security capabilities might not be sufficient for enterprise requirements
    such as user group management and integration with third-party authentication
    providers. Organizations often need to implement separate security and authentication
    controls to manage access to resources.
  prefs: []
  type: TYPE_NORMAL
- en: Serving models using model serving services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once a model has been trained and saved, utilizing it to generate predictions
    is a matter of loading the saved model into an ML package and invoking the appropriate
    model prediction function provided by the package. However, for large-scale and
    complex model serving requirements, you will need to consider implementing a dedicated
    model serving infrastructure to meet those needs.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we will explore a variety of open-source model serving
    frameworks that can assist in addressing such needs.
  prefs: []
  type: TYPE_NORMAL
- en: The Gunicorn and Flask inference engine
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Gunicorn** and **Flask** are often used for building custom model serving
    web frameworks. The following figure shows a typical architecture that uses Flask,
    Gunicorn, and Nginx as the building blocks for a model serving service.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – A model serving architecture using Flask and Gunicorn ](img/B20836_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: A model serving architecture using Flask and Gunicorn'
  prefs: []
  type: TYPE_NORMAL
- en: Flask is a Python-based micro web framework for building web apps quickly. It
    is lightweight and has almost no dependencies on external libraries. With Flask,
    you can define different invocation routes and associate handler functions to
    handle different web calls (such as health check calls and model invocation calls).
    To handle model prediction requests, the Flask app would load the model into memory
    and call the `predict` function on the model to generate the prediction. Flask
    comes with a built-in web server, but it does not scale well as it can only support
    one request at a time.
  prefs: []
  type: TYPE_NORMAL
- en: This is where Gunicorn can help address the scalability gap. Gunicorn is a web
    server for hosting web apps, including Flask apps. It can handle multiple requests
    in parallel and distribute the traffic to the hosted web apps efficiently. When
    it receives a web request, it will invoke the hosted Flask app to handle the request,
    such as invoking the function to generate the model prediction.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to serving prediction requests as web requests, an enterprise inference
    engine also needs to handle secure web traffic (such as SSL/TLS traffic), as well
    as load balancing when there are multiple web servers. This is where Nginx can
    play an important role. Nginx can serve as a load balancer for multiple web servers
    and can handle termination for SSL/TLS traffic more efficiently, so web servers
    do not have to handle it.
  prefs: []
  type: TYPE_NORMAL
- en: A Flask/Gunicorn-based model serving architecture can be a good option for hosting
    simple model serving patterns. But for more complicated patterns such as serving
    different versions of models, A/B testing (showing two variants of a model to
    different user groups and comparing their responses), or large model serving,
    this architecture will have limitations. The Flask/Gunicorn architecture pattern
    also requires custom code (such as the Flask app) to work, as it does not provide
    built-in support for the different ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore some purpose-built model serving frameworks and see how
    they are different from the custom Flask-based inference engine.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow Serving framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**TensorFlow Serving** is a production-grade, open-source model serving framework,
    and provides out-of-the-box support for serving TensorFlow models behind a RESTFul
    endpoint. It manages the model lifecycle for model serving and provides access
    to versioned and multiple models behind a single endpoint. There is also built-in
    support for canary deployments. A **canary deployment** allows you to deploy a
    model to support a subset of traffic. In addition to the real-time inference support,
    there is also a batch scheduler feature that can batch multiple prediction requests
    and perform a single joint execution. With TensorFlow Serving, there is no need
    to write custom code to serve the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the architecture of TensorFlow Serving:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – TensorFlow Serving architecture ](img/B20836_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: TensorFlow Serving architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss each of the architecture components in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The *API handler* provides APIs for TensorFlow Serving. It comes with a built-in,
    lightweight HTTP server to serve RESTful-based API requests. It also supports
    **gRPC** (a **remote procedure call** protocol) traffic. gRPC is a more efficient
    and fast networking protocol; however, it is more complicated to use than the
    REST protocol. TensorFlow Serving has a concept called a *servable*, which refers
    to the actual objects that handle a task, such as model inferences or lookup tables.
    For example, a trained model is represented as a *servable*, and it can contain
    one or more algorithms and lookup tables or embedding tables. The API handler
    uses the servable to fulfill client requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *model manager* manages the lifecycle of servables, including loading the
    servables, serving the servables, and unloading the servables. When a servable
    is needed to perform a task, the model manager provides the client with a handler
    to access the servable instances. The model manager can manage multiple versions
    of a servable, allowing gradual rollout of different versions of a model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *model loader* is responsible for loading models from different sources,
    such as **Amazon** **S3**. When a new model is loaded, the model loader notifies
    the model manager about the availability of the new model, and the model manager
    will decide what the next step should be, such as unloading the previous version
    and loading the new version.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow Serving can be extended to support non-TensorFlow models. For example,
    models trained in other frameworks can be converted to the **ONNX** format and
    served using TensorFlow Serving. ONNX is a common format for representing models
    to support interoperability across different ML frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: The TorchServe serving framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**TorchServe** is an open-source framework for serving trained **PyTorch**
    models. Similar to TensorFlow Serving, TorchServe provides a REST API for serving
    models with its built-in web server. With core features such as multi-model serving,
    model versioning, server-side request batching, and built-in monitoring, TorchServe
    can serve production workloads at scale. There is also no need to write custom
    code to host PyTorch models with TorchServe. In addition, TorchServe comes with
    a built-in web server for hosting the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates the architecture components of the TorchServe
    framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – TorchServe architecture ](img/B20836_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: TorchServe architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The *inference API* is responsible for handling prediction requests from client
    applications using loaded PyTorch models. It supports the REST protocol and provides
    a prediction API, as well as other supporting APIs such as health check and model
    explanation APIs. The inference API can handle prediction requests for multiple
    models.
  prefs: []
  type: TYPE_NORMAL
- en: The model artifacts are packaged into a single archive file and stored in a
    model store within the TorchServe environment. You use a **command-line interface**
    (**CLI**) command called `torch-mode-archive` to package the model.
  prefs: []
  type: TYPE_NORMAL
- en: The TorchServe backend loads the archived models from the model store into different
    worker processes. These worker processes interact with the inference API to process
    requests and send back responses.
  prefs: []
  type: TYPE_NORMAL
- en: The management API is responsible for handling management tasks such as registering
    and unregistering PyTorch models, checking the model status, and scaling the worker
    process. The management API is normally used by system administrators.
  prefs: []
  type: TYPE_NORMAL
- en: TorchServe also provides built-in support for logging and metrics. The logging
    component logs both access logs and processing logs. The TorchServe metrics collect
    a list of system metrics, such as CPU/GPU utilization and custom model metrics.
  prefs: []
  type: TYPE_NORMAL
- en: KFServing framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TensorFlow Serving and TorchServe are standalone model serving frameworks for
    a specific deep learning framework. In contrast, **KFServing** is a general-purpose,
    multi-framework, model serving framework that supports different ML models. KFServing
    uses standalone model serving frameworks such as TensorFlow Serving and TorchServe
    as the backend model servers. It is part of the Kubeflow project and provides
    pluggable architecture for different model formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – KFServing components ](img/B20836_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: KFServing components'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a general-purpose, multi-framework model serving solution, KFServing provides
    several out-of-the-box model servers (also known as predictors) for different
    model types, including TensorFlow, **PyTorch** **XGBoost**, **scikit-learn**,
    and ONNX. With KFServing, you can serve models using both REST and gRPC protocols.
    To deploy a supported model type, you simply need to define a YAML specification
    that points to the model artifact in a data store. Furthermore, you can build
    your own custom containers to serve models in KFServing. The container needs to
    provide a model serving implementation as well as a web server. The following
    code shows a sample YAML specification to deploy a `tensorflow` model using KFServing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: KFServing has a transformer component that allows the custom processing of the
    input payload before it is sent to the predictors, and also allows transforming
    the response from the predictor before it is sent back to the calling client.
    Sometimes, you need to provide an explanation for the model prediction, such as
    which features have a stronger influence on the prediction, which we will cover
    in more detail in a later chapter.
  prefs: []
  type: TYPE_NORMAL
- en: KFServing is designed for production deployment and provides a range of production
    deployment capabilities. Its auto-scaling feature allows the model server to scale
    up/down based on the amount of request traffic. With KFServing, you can deploy
    both the default model serving endpoint and the canary endpoint, split the traffic
    between the two, and specify model revisions behind the endpoint. For operational
    support, KFServing also has built-in functionality for monitoring (for example,
    monitoring request data and request latency).
  prefs: []
  type: TYPE_NORMAL
- en: Seldon Core
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Seldon Core** is another multi-framework model serving framework for deploying
    models on Kubernetes. Compared to KFServing, Seldon Core provides richer model
    serving features, for example, model serving inference graphs for use cases such
    as A/B testing and model ensembles. The following figure shows the core components
    of the Seldon Core framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – The Seldon Core model serving framework architecture ](img/B20836_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: The Seldon Core model serving framework architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Seldon Core provides packaged model servers for some of the common ML libraries,
    including the `SKLearn` server for scikit-learn models, the XGBoost server for
    XGBoost models, TensorFlow Serving for TensorFlow models, and MLflow server-based
    model serving. You can also build your own custom serving container for specific
    model serving needs and host it using Seldon Core.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following template shows how to deploy a model using the SKLearn server
    using Seldon Core. You simply need to change the `modelUri` path to point to a
    saved model on a cloud object storage provider such as **Google** **Cloud Storage**,
    **Amazon S3 storage**, or **Azure** **Blob storage**. To test with an example,
    you can change the following `modelUri` value to an example provided by Seldon
    Core – `gs://seldon-models/sklearn/iris`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Seldon Core also supports an advanced workflow, known as an inference graph,
    for serving models. The *inference graph* feature allows you to have a graph with
    different models and other components in a single inference pipeline. An inference
    graph can consist of several components:'
  prefs: []
  type: TYPE_NORMAL
- en: One or more ML models for the different prediction tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic routing management for different usage patterns, such as traffic splitting
    to different models for A/B testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A component for combining results from multiple models, such as a model ensemble
    component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Components for transforming the input requests (such as performing feature engineering)
    or output responses (for example, returning an array format as a **JSON** format)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To build inference graph specifications in YAML, you need the following key
    components in the `seldondeployment` YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: A list of predictors, with each predictor having its own `componentSpecs` section
    that specifies details such as container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A graph that describes how the components are linked together for each `componentSpecs`
    section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following sample template shows the inference graph for a custom canary
    deployment to split the traffic into two different versions of a model – one with
    75% of the traffic and another one with 25% of the traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Once a deployment manifest is applied, the Seldon Core operator is responsible
    for creating all the resources needed to serve an ML model. Specifically, the
    operator will create resources defined in the manifest, add orchestrators to the
    Pods to manage the orchestration of the inference graph, and configure the traffic
    using ingress gateways such as Istio.
  prefs: []
  type: TYPE_NORMAL
- en: Triton Inference Server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Triton Inference Server is open-source software designed to streamline the
    process of AI inferencing. It offers a versatile solution for deploying AI models
    from various deep learning and ML frameworks, including TensorRT, TensorFlow,
    PyTorch, ONNX, OpenVINO, Python, and more. Triton is compatible with a wide range
    of devices, supporting inference across cloud environments, data centers, edge
    devices, and embedded systems. Compared to Seldon Core, Triton Inference Server
    is more focused on performance. It is designed to be highly scalable and efficient,
    making it a good choice for high-traffic applications. The following figure depicts
    the core components of Triton Inference Server:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, number  Description automatically
    generated](img/B20836_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Triton Inference Server architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The Triton Inference Server architecture encompasses several components that
    work together to enable efficient and scalable inferencing. At its core is the
    backend, which represents specific deep learning or ML frameworks supported by
    Triton. Each backend handles the loading and execution of models trained with
    its corresponding framework. Triton Inference Server acts as the central hub,
    receiving and managing inference requests. It communicates with clients, such
    as web applications or services, and orchestrates the inferencing process. The
    model repository serves as the storage location for trained models. It contains
    serialized versions of models compatible with the supported backends. When requested
    by clients, the server accesses and loads the models into memory for inferencing.
  prefs: []
  type: TYPE_NORMAL
- en: Triton supports multiple inference protocols, including HTTP/REST and gRPC,
    allowing clients to communicate with the server and make inference requests. Clients
    can specify input data and desired output formats using these protocols. To monitor
    and optimize performance, Triton provides metrics and monitoring capabilities.
    These metrics include GPU utilization, server throughput, latency, and other relevant
    statistics. Monitoring these metrics helps administrators optimize resource utilization
    and identify potential bottlenecks.
  prefs: []
  type: TYPE_NORMAL
- en: Triton also offers dynamic batching capability. This feature allows for efficient
    processing of multiple inference requests by grouping them together into batches.
    This batching mechanism optimizes resource utilization and improves overall inferencing
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the architecture of Triton Inference Server is designed to facilitate
    the efficient deployment and execution of AI models across diverse frameworks
    and hardware platforms. It offers flexibility, scalability, and extensibility,
    enabling organizations to leverage their preferred frameworks while ensuring high-performance
    inferencing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring models in production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Model performance can deteriorate over time due to various factors such as changing
    data patterns, shifts in user behavior, or unforeseen scenarios. To ensure the
    ongoing effectiveness of deployed ML models, continuous monitoring of their performance
    and behavior in production is essential.
  prefs: []
  type: TYPE_NORMAL
- en: 'Model monitoring involves actively tracking and analyzing the performance of
    deployed ML models. This process includes collecting data on different metrics
    and indicators, comparing them to predefined thresholds or baselines, and identifying
    anomalies or deviations from expected behavior. Two critical aspects of model
    monitoring are data drift and model drift:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data drift**: Data drift refers to the scenarios where the statistical properties
    of incoming data change over time. This can create a disconnect between the data
    used to train the model and the data it encounters in the production environment.
    Data drift significantly impacts the performance and reliability of ML models,
    as they may struggle to adapt to new and evolving patterns in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model drift**: Model drift refers to the degradation of an ML model’s performance
    over time due to changes in underlying patterns or relationships in the data.
    When the assumptions made during model training no longer hold true in the production
    environment, model drift occurs. It can lead to decreased accuracy, increased
    errors, and suboptimal decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To support model monitoring efforts, there are several open-source and commercial
    products available in the market. These tools provide capabilities for monitoring
    model performance, detecting data drift, identifying model drift, and generating
    insights to help organizations take necessary corrective actions. Some popular
    examples include Evidently AI, Arize AI, Seldon Core, Fiddler, and Author AI.
  prefs: []
  type: TYPE_NORMAL
- en: Managing ML features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As organizations increasingly adopt ML solutions, they recognize the need to
    standardize and share commonly used data and code throughout the ML lifecycle.
    One crucial element that organizations seek to manage centrally is ML features,
    which are commonly used data attributes that serve as inputs to ML models. To
    enable standardization and reuse of these features, organizations often turn to
    a feature store.
  prefs: []
  type: TYPE_NORMAL
- en: A feature store acts as a centralized repository for storing and managing ML
    features. It provides a dedicated platform for organizing, validating, and sharing
    features across different ML projects and teams within an organization. By consolidating
    features in a single location, the feature store promotes consistency and facilitates
    collaboration among data scientists and ML practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of a feature store has gained significant attention in the ML community
    due to its numerous benefits. Firstly, it enhances productivity by eliminating
    the need to recreate and engineer features for each ML project. Instead, data
    scientists can readily access precomputed and validated features from the store,
    saving time and effort. Additionally, a feature store improves model performance
    by ensuring the consistency and quality of features used in ML models. By centralizing
    feature management, organizations can enforce data governance practices, perform
    feature validation, and monitor feature quality, leading to more reliable and
    accurate ML models.
  prefs: []
  type: TYPE_NORMAL
- en: Several open-source feature store frameworks are available in the market, such
    as Feast and Hopsworks Feature Store, offering organizations flexible options
    for managing their ML features. Let’s take a closer look at Feast as an example
    to get a deep understanding of how a feature store works.
  prefs: []
  type: TYPE_NORMAL
- en: Feast is an open-source feature store that enables organizations to manage,
    discover, and serve features for ML applications. Developed by Tecton, Feast is
    designed to handle large-scale, real-time feature data. It supports feature ingestion
    from various sources, including batch pipelines and streaming systems like Apache
    Kafka. Feast integrates well with popular ML frameworks such as TensorFlow and
    PyTorch, allowing seamless integration into ML workflows. With features like feature
    versioning, data validation, and online and offline serving capabilities, Feast
    provides a comprehensive solution for feature management.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B20836_07_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Feast feature store'
  prefs: []
  type: TYPE_NORMAL
- en: At the core of the Feast architecture is the online and offline feature storage,
    which serves as the centralized storage for feature data. The feature repository
    stores feature data in a distributed storage system, allowing for scalable and
    efficient storage and retrieval of feature values.
  prefs: []
  type: TYPE_NORMAL
- en: Feast employs a decoupled architecture, where the ingestion of feature data
    and the serving of features are separated. The data ingestion component is responsible
    for extracting feature data from various sources, such as data warehouses, databases,
    and streaming platforms. It then transforms and loads the feature data into the
    feature storage, ensuring data quality and consistency.
  prefs: []
  type: TYPE_NORMAL
- en: The feature serving component is responsible for providing low-latency access
    to feature data for ML models during training and inference. The feature serving
    component also supports online and offline serving modes, allowing for real-time
    and batch feature serving.
  prefs: []
  type: TYPE_NORMAL
- en: To enable efficient data discovery, Feast employs a feature registry. The feature
    registry allows for the fast lookup and retrieval of feature values based on different
    feature combinations and time ranges.
  prefs: []
  type: TYPE_NORMAL
- en: Feast also integrates with popular ML frameworks, such as TensorFlow and PyTorch,
    through its SDKs and client libraries. These integrations enable seamless integration
    of Feast into ML pipelines and workflows, making it easy for data scientists and
    ML engineers to access and utilize feature data in their models.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the Feast feature store architecture provides a robust and scalable
    solution for managing and serving ML features. By centralizing feature data management,
    Feast enables organizations to enhance productivity, improve model performance,
    and promote collaboration in ML development.
  prefs: []
  type: TYPE_NORMAL
- en: Automating ML pipeline workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To automate the core ML platform components we have discussed so far, we need
    to build pipelines that can orchestrate different steps using these components.
    Automation brings efficiency, productivity, and consistency while enabling reproducibility
    and minimizing human errors. There are several open-source technologies available
    to automate ML workflows, with Apache Airflow and Kubeflow Pipelines being prominent
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Airflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Apache** **Airflow** is an open-source software package for programmatically
    authoring, scheduling, and monitoring multi-step workflows. It is a general-purpose
    workflow orchestration tool that can be leveraged to define workflows for a wide
    range of tasks, including ML tasks. First, let’s explore some core Airflow concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Directed Acyclic Graph** (**DAG**): A DAG defines independent tasks that
    are executed independently in a pipeline. The sequences of the execution can be
    visualized like a graph.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tasks**: Tasks are basic units of execution in Airflow. Tasks have dependencies
    between them during executions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operators**: Operators are DAG components that describe a single task in
    the pipeline. An operator implements the task execution logic. Airflow provides
    a list of operators for common tasks, such as a Python operator for running Python
    code, or an Amazon S3 operator to interact with the S3 service. Tasks are created
    when operators are instantiated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduling**: A DAG can run on demand or on a predetermined schedule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensors**: Sensors are a special type of operator that are designed to wait
    for something to occur. They can then help trigger a downstream task to happen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Airflow can run on a single machine or in a cluster. Additionally, it can be
    deployed on the Kubernetes infrastructure. The following figure shows a multi-node
    Airflow deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Apache Airflow architecture ](img/B20836_07_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Apache Airflow architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The *master node* mainly runs the *web server* and *scheduler*. The scheduler
    is responsible for scheduling the execution of the DAGs. It sends tasks to a queue,
    and the worker nodes retrieve the tasks from the queue and run them. The metadata
    store is used to store the metadata of the Airflow cluster and processes, such
    as task instance details or user data.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can author the Airflow DAGs using Python. The following sample code shows
    how to author a basic Airflow DAG in Python with two Bash operators in a sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Airflow can connect to many different sources and has built-in operators for
    many external services, such as **Amazon** **EMR** and **Amazon** **SageMaker**.
    It has been widely adopted by many organizations to run large-scale workflow orchestration
    jobs in production, such as coordinating ETL jobs and ML data processing jobs.
    AWS even has a managed Airflow offering to help reduce the operational overhead
    of running Airflow infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow also comes with some limitations. Airflow does not offer a UI designer
    for DAG development, which can be a challenge for users without Python programming
    skills to design workflows. The lack of versioning control with DAG pipelines
    also poses some challenges with managing and understanding the evolving variations
    of pipelines. Operating Airflow on Kubernetes can be complex, which is why many
    organizations opt for managed offerings. Despite these limitations, however, Airflow
    has emerged as a highly popular workflow orchestration tool due to its enterprise-ready
    capability, strong community support, and rich ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow Pipelines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Kubeflow Pipelines** is a Kubeflow component, and it is purpose-built for
    authoring and orchestrating end-to-end ML workflows on Kubernetes. First, let’s
    review some core concepts of Kubeflow Pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipeline**: A pipeline describes an ML workflow, all the components in the
    workflow, and how the components are related to each other in the pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline components**: A pipeline component performs a task in the pipeline.
    An example of a pipeline component could be a data processing component or a model
    training component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experiment**: An experiment organizes different trial runs (model training)
    for an ML project so you can easily inspect and compare the different runs and
    their results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Step**: The execution of one component in a pipeline is called a step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Run trigger**: You use a run trigger to kick off the execution of a pipeline.
    A run trigger can be a periodic trigger (for example, to run every 2 hours), or
    a scheduled trigger (for example, run at a specific date and time).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output artifacts**: Output artifacts are the outputs from the pipeline components.
    Examples of output artifacts could be model training metrics or visualizations
    of datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubeflow Pipelines is installed as part of the Kubeflow installation. It comes
    with its own UI, which is part of the overall Kubeflow dashboard UI. The Pipelines
    service manages the pipelines and their run status and stores them in a metadata
    database. There is an orchestration and workflow controller that manages the actual
    execution of the pipelines and the components. The following figure illustrates
    the core architecture components in a Kubeflow pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Kubeflow Pipelines architecture ](img/B20836_07_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Kubeflow Pipelines architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'You author the pipeline using the Pipeline SDK in Python. To create and run
    a pipeline, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a pipeline definition using the Kubeflow SDK. The pipeline definition
    specifies a list of components and how they are joined together in a graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compile the definition into a static YAML specification to be executed by the
    Kubeflow Pipelines service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the specification with the Kubeflow Pipelines service and call the
    pipeline to run from the static definition.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Kubeflow Pipelines service calls the API server to create resources to run
    the pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Orchestration controllers execute various containers to complete the pipeline
    run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is important to note that running pipelines using Kubeflow Pipelines requires
    a high degree of competency with Kubernetes, which could be challenging for people
    without deep Kubernetes skills. Building the workflow in Python can be a complex
    task that involves writing a Dockerfile or YAML file for each component, and Python
    scripts for the workflow and execution. Kubeflow Pipelines mainly works within
    the Kubeflow environment, with very limited integration with external tools and
    services. It also lacks native pipeline versioning capability. Despite these challenges,
    Kubeflow Pipelines is still widely adopted due to its support for end-to-end ML
    management, workflow visualization, portability, and reproducibility.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored various open-source tools for building ML platforms,
    let’s delve into end-to-end architecture using these open-source frameworks and
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Designing an end-to-end ML platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After discussing several open-source technologies individually, let’s now delve
    into their integration and see how these components come together. The architecture
    patterns and technology stack selection may vary based on specific needs and requirements.
    The following diagram presents the conceptual building blocks of an ML platform
    architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, design  Description automatically
    generated](img/B20836_07_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: ML platform architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s delve into different strategies to implement this architecture concept
    with different combinations of open-source technologies.
  prefs: []
  type: TYPE_NORMAL
- en: ML platform-based strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When designing an ML platform using open-source technologies, one effective
    strategy is to utilize an ML platform framework as a base platform and then integrate
    additional open-source components to address specific requirements. One such ML
    platform framework is Kubeflow, which provides a robust foundation with its built-in
    building blocks for an ML platform. By leveraging Kubeflow, you can benefit from
    its core components while extending the platform’s capabilities through the integration
    of complementary open-source tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'This strategy allows for flexibility and customization by seamlessly integrating
    a range of open-source ML components into the platform. You would choose this
    approach if the base ML platform framework meets most of your requirements, or
    if you can work within the limitations of the framework. The following table outlines
    key ML platform components and their corresponding open-source frameworks and
    tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **ML Platform Component** | **Open-Source Framework** |'
  prefs: []
  type: TYPE_TB
- en: '| Code repository | GitHub |'
  prefs: []
  type: TYPE_TB
- en: '| Experimentation and model development | Kubeflow Jupyter Notebook |'
  prefs: []
  type: TYPE_TB
- en: '| Experiment tracking | MLflow experiment tracker |'
  prefs: []
  type: TYPE_TB
- en: '| Feature store | Feast feature store |'
  prefs: []
  type: TYPE_TB
- en: '| Data annotation | Computer Vision Annotation Tool (CVAT) |'
  prefs: []
  type: TYPE_TB
- en: '| Training | Kubeflow training operators |'
  prefs: []
  type: TYPE_TB
- en: '| Data and model testing | Deepchecks |'
  prefs: []
  type: TYPE_TB
- en: '| Model repository | MLflow Model Repository |'
  prefs: []
  type: TYPE_TB
- en: '| ML pipeline development | Kubeflow Pipelines |'
  prefs: []
  type: TYPE_TB
- en: '| Model inference | Kubeflow KFServing (Seldon Core, TFServing, Triton) |'
  prefs: []
  type: TYPE_TB
- en: '| Docker image repository | Docker Hub |'
  prefs: []
  type: TYPE_TB
- en: '| CI/CD | GitHub Actions |'
  prefs: []
  type: TYPE_TB
- en: '| Drift monitoring | Deepchecks |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7.1: ML platform components and their corresponding open-source frameworks'
  prefs: []
  type: TYPE_NORMAL
- en: 'By incorporating these frameworks and tools into the architectural conceptual
    diagram, we can visualize the resulting diagram as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, design  Description automatically
    generated](img/B20836_07_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: Kubeflow-based ML platform'
  prefs: []
  type: TYPE_NORMAL
- en: Using this architecture, data scientists utilize Kubeflow Jupyter Notebook for
    conducting experiments and building models. Experiment runs and relevant details,
    such as data statistics, hyperparameters, and model metrics, are tracked and saved
    in the MLflow experiment tracking component.
  prefs: []
  type: TYPE_NORMAL
- en: Common ML features are stored in the Feast feature store. When there is a need
    for data annotation, data annotators can employ open-source data annotation tools
    like the **Computer Vision Annotation Tool** (**CVAT**) and Label Studio to label
    data for model training. Data scientists can utilize features from the feature
    store and the labeled dataset as part of their experimentation and model building.
  prefs: []
  type: TYPE_NORMAL
- en: GitHub serves as the code repository for data scientists. They save all source
    code, including training scripts, algorithm code, and data transformation scripts,
    in the code repository. Model training and inference Docker images are stored
    in Docker Hub. A Docker image build process can be deployed to create new Docker
    images for training and inference purposes.
  prefs: []
  type: TYPE_NORMAL
- en: For formal model training, the training script is pulled from the GitHub repository,
    and the training Docker image is pulled from Docker Hub into the Kubeflow training
    operator to initiate the model training, along with the training dataset. Deepchecks
    can be utilized to perform data validation and model performance checks. Once
    the model is trained, the model artifacts, along with any metadata such as model
    metrics and evaluation graphs, are stored in MLflow Model Registry.
  prefs: []
  type: TYPE_NORMAL
- en: When it is time to deploy the model, models are fetched from MLflow Model Registry
    and loaded into KFServing for model inference, along with the model inference
    Docker image and inference script. KFServing offers the flexibility to choose
    different inference servers, including Seldon Core, TFServing, and Triton.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction logs can be sent to the model monitoring component for detecting
    data drift and model drift. Open-source software tools like Evidently AI can be
    employed for data drift and model drift detection.
  prefs: []
  type: TYPE_NORMAL
- en: To orchestrate various tasks such as data processing, feature engineering, model
    training, and model validation, Kubeflow Pipelines can be developed. For **CI/CD**
    (**Continuous Integration/Continuous Deployment**), GitHub Actions can be used
    as a triggering mechanism to initiate different pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this approach allows you to combine the benefits of a base ML platform
    framework with the flexibility provided by a wide range of open-source components.
  prefs: []
  type: TYPE_NORMAL
- en: ML component-based strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative approach is to build the ML platform using individual components
    rather than relying on a base ML platform framework. This strategy offers the
    advantage of selecting the best-in-class components for each aspect of the platform,
    allowing organizations to adhere to their existing open-source standards for core
    components like pipeline development or notebook IDEs. The following architectural
    pattern illustrates this approach.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, diagram, design  Description automatically
    generated](img/B20836_07_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: Component-based architecture'
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture pattern, alternative technologies and tools are utilized
    for pipeline development, notebook environments, and training infrastructure management.
    Additionally, Kubernetes serves as the infrastructure management framework. This
    approach allows organizations to leverage specific technologies and tools that
    align with their unique requirements and preferences.
  prefs: []
  type: TYPE_NORMAL
- en: One notable aspect is the use of Airflow as the standard orchestration and pipeline
    tool across various technical disciplines, including ML and data management. Airflow’s
    widespread adoption within organizations enables it to serve as a unifying pipeline
    management tool, facilitating seamless integration between different components
    and workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, this architecture pattern emphasizes the building of custom training
    and inference infrastructure on top of Kubernetes. By leveraging Kubernetes, organizations
    gain the flexibility to create customized training and inference environments
    tailored to their specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the availability of free open-source tools that meet ML platform
    requirements, it is important to consider the integration of commercial components
    into the open-source architecture. These commercial offerings can enhance specific
    aspects of the ML platform and provide additional capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, when deploying this architecture pattern on AWS, it is advisable
    to explore the use of Amazon Elastic Container Registry (ECR) as the Docker image
    repository. Amazon ECR provides a managed and secure solution for storing and
    managing container images, integrating seamlessly with other AWS services.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to monitoring, there are commercial products like Fiddler and
    Author AI that can offer advanced features and insights. These tools can enhance
    the monitoring capabilities of the ML platform, providing in-depth analysis, model
    explainability, and visualization of model behavior and performance.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the advantages of this architecture pattern include the ability to
    choose alternative technologies and tools for different aspects of the ML platform,
    leveraging Airflow as a unifying pipeline management tool, and building custom
    training and inference infrastructure on Kubernetes. These choices enable organizations
    to create a tailored and optimized ML platform that aligns precisely with their
    requirements and allows for highly customized training and inference processes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have gained an understanding of the core architecture components
    of a typical ML platform and their capabilities. We have explored various open-source
    technologies such as Kubeflow, MLflow, TensorFlow Serving, Seldon Core, Triton
    Inference Server, Apache Airflow, and Kubeflow Pipelines. Additionally, we have
    discussed different strategies for approaching the design of an ML platform using
    open-source frameworks and tools.
  prefs: []
  type: TYPE_NORMAL
- en: While these open-source technologies offer powerful features for building sophisticated
    ML platforms, it is important to acknowledge that constructing and maintaining
    such environments requires substantial engineering effort and expertise, especially
    when dealing with large-scale ML platforms.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve into fully managed, purpose-built ML solutions
    that are specifically designed to facilitate the development and operation of
    ML environments. These managed solutions aim to simplify the complexities of building
    and managing ML platforms, providing preconfigured and scalable infrastructure,
    as well as additional features tailored for ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Leave a review!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Enjoying this book? Help readers like you by leaving an Amazon review. Scan
    the QR code below to get a free eBook of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Review_Copy.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Limited Offer*'
  prefs: []
  type: TYPE_NORMAL
