<html><head></head><body>
		<div id="_idContainer091">
			<h1 id="_idParaDest-101"><em class="italic"><a id="_idTextAnchor119"/><a id="_idTextAnchor120"/>Chapter 7</em>: Data and Feature Management</h1>
			<p>In this chapter, we will add a feature management data layer to the machine learning platform being built. We will leverage the features of the MLflow Projects module to structure our data pipeline.</p>
			<p>Specifically, we will look at the following sections in this chapter:</p>
			<ul>
				<li>Structuring your data pipeline project</li>
				<li>Acquiring stock data</li>
				<li>Checking data quality</li>
				<li>Managing features</li>
			</ul>
			<p>In this chapter, we will acquire relevant data to provide datasets for training. Our primary resource will be the Yahoo Finance Data for BTC dataset. Alongside that data, we will acquire the following extra datasets. </p>
			<p>Leveraging our productionization architecture introduced in <a href="B16783_06_Final_SB_epub.xhtml#_idTextAnchor106"><em class="italic">Chapter 6</em></a>,<em class="italic"> Introducing ML Systems Architecture</em>, represented in <em class="italic">Figure 7.1</em>, the feature and data component is responsible for acquiring data from sources and making the data available in a format consumable by the different components of the platform:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/image0013.jpg" alt="Figure 7.1 – High-level architecture with a data layer reference&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – High-level architecture with a data layer reference</p>
			<p>Let's delve into this chapter and see how we will structure and populate the data layer with relevant data to be used for training models and generating features.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor121"/>Technical requirements</h1>
			<p>For this chapter, you will need the following prerequisites:</p>
			<ul>
				<li>The latest version of Docker installed on your machine. If you don't already have it installed, please follow the instructions at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
				<li>The latest version of docker-compose installed. Please follow the instructions at <a href="https://docs.docker.com/compose/install/">https://docs.docker.com/compose/install/</a>.</li>
				<li>Access to Git on the command line and installed as described at <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>.</li>
				<li>Access to a Bash terminal (Linux or Windows).</li>
				<li>Access to a browser.</li>
				<li>Python 3.5+ installed.</li>
				<li>The latest version of your machine learning installed locally as described in <a href="B16783_03_Final_SB_epub.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a>, <em class="italic">Your Data Science Workbench</em>.</li>
			</ul>
			<p>In the next section, we will describe the structure of our data pipeline, the data sources, and the different steps that we will execute to implement our practical example leveraging MLflow project features to package the project.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Copying and pasting directly from the code snippets might cause issues with your editor. Please refer to the GitHub repository of the chapter available at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter07</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor122"/>Structuring your data pipeline project</h1>
			<p>At a high level, our data pipeline will run weekly, collecting data for the preceding 7 days and storing it <a id="_idIndexMarker239"/>in a way that can be run by machine learning jobs to generate models upstream. We will structure our data folders into three types of data:</p>
			<ul>
				<li><strong class="bold">Raw data</strong>: A dataset <a id="_idIndexMarker240"/>generated by retrieving data from the Yahoo Finance API for the last 90 days. We will store the data in CSV format – the same format that it was received in from the API. We will log the run in MLflow and extract the number of rows collected.</li>
				<li><strong class="bold">Staged data</strong>: Over the <a id="_idIndexMarker241"/>raw data, we will run quality checks, schema verification, and confirm that the data can be used in production. This information about data quality will be logged in MLflow Tracking.</li>
				<li><strong class="bold">Training data</strong>: The training data is the final product of the data pipeline. It must be executed <a id="_idIndexMarker242"/>over data that is deemed as clean and suitable to execute models. The data contains the data processed into features that can be consumed directly for the training process.</li>
			</ul>
			<p>This folder structure will be implemented initially on the filesystem and will be transposed to the relevant environment (examples: AWS S3, Kubernetes PersistentVolume, and so on) during deployment.</p>
			<p>In order to execute our data pipeline project, we will use the <strong class="bold">MLflow Project</strong> module to package the <a id="_idIndexMarker243"/>data pipeline in an execution environment-independent format. We will use the Docker format to package the <strong class="bold">MLflow Project</strong>. The Docker <a id="_idIndexMarker244"/>format provides us with different options to deploy our project in the cloud or on-premises depending on the available infrastructure to deploy our project:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/image0024.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – High-level architecture with a data layer reference</p>
			<p>Our workflow will execute the following steps, as illustrated in <em class="italic">Figure 7.2</em>:</p>
			<ol>
				<li><strong class="bold">Acquire Data</strong>: This is responsible for retrieving the data from the API and saving the data as a CSV file in the <strong class="source-inline">data/raw/data.csv folder</strong>.</li>
				<li><strong class="bold">Check and Verify Data</strong>: This is responsible for checking whether the data provided meets the quality requirements of the data pipeline, and if it does, it will report different metrics and write data in the <strong class="source-inline">data/staged/data.csv file</strong>.</li>
				<li><strong class="bold">Generate Feature Sets</strong>: Based on the staged data, this will transform the data into a format consumable by the machine learning code and produce a new training dataset at the <strong class="source-inline">data/training/data.csv location</strong>.</li>
			</ol>
			<p>With these three distinct phases, we ensure the reproducibility of the training data generation process, visibility, and a clear separation of the different steps of the process.</p>
			<p>We will start <a id="_idIndexMarker245"/>by organizing our MLflow project into steps and creating placeholders for each of the components of the pipeline:</p>
			<ol>
				<li value="1">Create a new folder on your local machine with the name <strong class="source-inline">psytock-data-features</strong>.</li>
				<li>Add the <strong class="source-inline">MLProject file</strong>:<p class="source-code">name: pystock_data_features</p><p class="source-code">conda:</p><p class="source-code">  file: conda.yaml</p><p class="source-code">entry_points:</p><p class="source-code">  data_acquisition:</p><p class="source-code">    command: "python data_acquisition.py"</p><p class="source-code">  clean_validate_data:</p><p class="source-code">    command: "python clean_validate_data.py "</p><p class="source-code">  feature_set_generation:</p><p class="source-code">    command: "python feature_set_generation.py"</p><p class="source-code">  main:</p><p class="source-code">    command: "python main.py"</p></li>
				<li>Add the following <strong class="source-inline">conda.yaml</strong> file: <p class="source-code">    name: pystock-data-features</p><p class="source-code">channels:</p><p class="source-code">  - defaults</p><p class="source-code">dependencies:</p><p class="source-code">  - python=3.8</p><p class="source-code">  - numpy</p><p class="source-code">  - scipy</p><p class="source-code">  - pandas</p><p class="source-code">  - cloudpickle</p><p class="source-code">  - pip:</p><p class="source-code">    - git+git://github.com/mlflow/mlflow</p><p class="source-code">    - pandas_datareader</p><p class="source-code">    - great-expectations==0.13.15</p><p class="source-code">    </p></li>
				<li>You can <a id="_idIndexMarker246"/>now add a sample <strong class="source-inline">main.py</strong> file to the folder to ensure that the basic structure of the project is working: <p class="source-code">import mlflow</p><p class="source-code">import click</p><p class="source-code">def _run(entrypoint, parameters={}, source_version=None, use_cache=True):</p><p class="source-code">    #existing_run = _already_ran(entrypoint, parameters, source_version)</p><p class="source-code">    #if use_cache and existing_run:</p><p class="source-code">    #    print("Found existing run for entrypoint=%s and parameters=%s" % (entrypoint, parameters))</p><p class="source-code">     #   return existing_run</p><p class="source-code">    print("Launching new run for entrypoint=%s and parameters=%s" % (entrypoint, parameters))</p><p class="source-code">    submitted_run = mlflow.run(".", entrypoint, parameters=parameters)</p><p class="source-code">    return submitted_run</p><p class="source-code">@click.command()</p><p class="source-code">def workflow():</p><p class="source-code">    with mlflow.start_run(run_name ="pystock-data-pipeline") as active_run:</p><p class="source-code">        mlflow.set_tag("mlflow.runName", "pystock-data-pipeline")</p><p class="source-code">        _run("load_raw_data")</p><p class="source-code">        _run("clean_validate_data")</p><p class="source-code">        _run("feature_set_generation")</p><p class="source-code">        </p><p class="source-code">        </p><p class="source-code">if __name__=="__main__":</p><p class="source-code">    workflow()</p></li>
				<li>Test the <a id="_idIndexMarker247"/>basic structure by running the following command:<p class="source-code">mlflow run .</p><p>This command will build your project based on the environment created by your <strong class="source-inline">conda.yaml</strong> file and run the basic project that you just created. It should error out as we need to add the missing files. The <em class="italic">file not found</em> error will look like the following :</p><p class="source-code">python: can't open file 'check_verify_data.py': [Errno 2] No such file or directory</p></li>
			</ol>
			<p>At this stage, we have <a id="_idIndexMarker248"/>the basic blocks of the MLflow project of the data pipeline that we will be building in this chapter. We will next fill in the Python script to acquire the data in the next section.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor123"/>Acquiring stock data</h1>
			<p>Our script to acquire the data will be based on the <strong class="source-inline">pandas-datareader Python package</strong>. It provides a <a id="_idIndexMarker249"/>simple abstraction to remote financial APIs we can leverage in the future in the pipeline. The abstraction is very simple. Given a data source such as Yahoo Finance, you provide the stock ticker/pair and date range, and the data is provided in a DataFrame.</p>
			<p>We will now create the <strong class="source-inline">load_raw_data.py file</strong>, which will be responsible for loading the data and saving it in the <strong class="source-inline">raw</strong> folder. You can look at the contents of the file in the repository at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter07/psystock-data-features-main/load_raw_data.py. Execute the following steps to implement the file:</p>
			<ol>
				<li value="1">We will start by importing the relevant packages:<p class="source-code">import mlflow</p><p class="source-code">from datetime import date</p><p class="source-code">from dateutil.relativedelta import relativedelta</p><p class="source-code">import pprint</p><p class="source-code">import pandas</p><p class="source-code">import pandas_datareader.data as web</p></li>
				<li>Next, you should add a function to retrieve the data: <p class="source-code">if __name__ == "__main__":</p><p class="source-code">    </p><p class="source-code">    with mlflow.start_run(run_name="load_raw_data") as </p><p class="source-code">run:</p><p class="source-code">        mlflow.set_tag("mlflow.runName", "load_raw_data")</p><p class="source-code">        end = date.today()</p><p class="source-code">        start = end + relativedelta(months=-3)</p><p class="source-code">        </p><p class="source-code">        df = web.DataReader("BTC-USD", 'yahoo', start, end)</p><p class="source-code">        df.to_csv("./data/raw/data.csv") </p><p class="source-code">   </p></li>
			</ol>
			<p>Now that we've <a id="_idIndexMarker250"/>acquired the data, we need to apply the best practices we will address in the next section – an approach to check the data quality of the data acquired.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor124"/>Checking data quality </h1>
			<p>Checking data quality as part of your machine learning system is extremely critical to ensure the integrity <a id="_idIndexMarker251"/>and correctness of your model training and inference. Principles of software testing and quality should be borrowed and used on the data layer of machine learning platforms.</p>
			<p>From a data quality perspective, in a dataset there are a couple of critical dimensions with which to assess and profile our data, namely:</p>
			<ul>
				<li><strong class="bold">Schema compliance</strong>: Ensuring <a id="_idIndexMarker252"/>the data is from the expected types; making sure that numeric values don't contain any other types of data</li>
				<li><strong class="bold">Valid data</strong>: Assessing <a id="_idIndexMarker253"/>from a data perspective whether the data is valid from a business perspective</li>
				<li><strong class="bold">Missing data</strong>: Assessing <a id="_idIndexMarker254"/>whether all the data needed to run analytics and algorithms is available</li>
			</ul>
			<p>For data validation, we will use the <em class="italic">Great Expectations</em> Python package (available at <a href="https://github.com/great-expectations/great_expectations">https://github.com/great-expectations/great_expectations</a>). It allows making assertions on data with many data-compatible packages, such as pandas, Spark, and cloud environments. It provides a DSL in JSON with which to declare the rules that we want our data to be compliant with.</p>
			<p>For our current project, we want the following rules/constraints to be verifiable:</p>
			<ul>
				<li>Date values should be valid dates and cannot be missing.</li>
				<li>Check numeric and long values are correctly typed.</li>
				<li>All columns are present in the dataset.</li>
			</ul>
			<p>We will now create the <strong class="source-inline">check_verify_data.py file</strong>, which will be responsible for loading the data and <a id="_idIndexMarker255"/>saving it in the <strong class="source-inline">staging</strong> folder where all the data is valid and ready to be used for ML training. You can look at the contents of the file in the repository at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter07/psystock-data-features-main/check_verify_data.py.</p>
			<ol>
				<li value="1">In order to convert the preceding rules so they can be relied on by our system, we will need to import the following dependencies:<p class="source-code">import mlflow</p><p class="source-code">from datetime import date</p><p class="source-code">from dateutil.relativedelta import relativedelta</p><p class="source-code">import pprint</p><p class="source-code">import pandas_datareader</p><p class="source-code">import pandas</p><p class="source-code">from pandas_profiling import ProfileReport</p><p class="source-code">import great_expectations as ge</p><p class="source-code">from great_expectations.profile.basic_dataset_profiler import BasicDatasetProfiler</p></li>
				<li>Next, we <a id="_idIndexMarker256"/>will implement the script:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    with mlflow.start_run(run_name="check_verify_data") as run:</p><p class="source-code">        mlflow.set_tag("mlflow.runName", "check_verify_data")</p><p class="source-code">        df = pandas.read_csv("./data/raw/data.csv")</p><p class="source-code">        describe_to_dict=df.describe().to_dict()</p><p class="source-code">        mlflow.log_dict(describe_to_dict,"describe_data.json")</p><p class="source-code">        pd_df_ge = ge.from_pandas(df)</p><p class="source-code">        assert pd_df_ge.expect_column_values_to_match_</p><p class="source-code">strftime_format("Date", "%Y-%m-%d").success == True</p><p class="source-code">        assert pd_df_ge.expect_column_values_to_be_of_</p><p class="source-code">type("High", "float").success == True</p><p class="source-code">        assert pd_df_ge.expect_column_values_to_be_of_type("Low", "float").success == True</p><p class="source-code">        assert pd_df_ge.expect_column_values_to_be_of_type("Open", "float").success == True</p><p class="source-code">        assert pd_df_ge.expect_column_values_to_be_of_type("Close", "float").success == True</p><p class="source-code">        assert pd_df_ge.expect_column_values_to_be_of_type("Volume", "long").success == True</p><p class="source-code">        assert pd_df_ge.expect_column_values_to_be_of_type("Adj Close", "float").success == True</p></li>
				<li>Now we can progress to do a little bit of cleaning:<p class="source-code">        #we can do some basic cleaning by dropping the null values</p><p class="source-code">        df.dropna(inplace=True)</p><p class="source-code">        #if data_passes_quality_can_go_to_features:</p><p class="source-code">        df.to_csv("data/staging/data.csv")</p></li>
			</ol>
			<p>Having verified <a id="_idIndexMarker257"/>the quality of the data and staging to be used, it can now be utilized for feature generation with a high degree of confidence. </p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor125"/>Generating a feature set and training data</h1>
			<p>We will refactor a bit of the code previously developed in our local environment to generate features for training to add to our MLflow project the data pipelineof our MLflow project .</p>
			<p>We will now <a id="_idIndexMarker258"/>create the <strong class="source-inline">feature_set_generation.py file</strong>, which will be responsible <a id="_idIndexMarker259"/>for generating our features and saving them in the <strong class="source-inline">training</strong> folder where all the data is valid and ready to be used for ML training. You can look at the contents in the file in the repository https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter07/psystock-data-features-main/feature_set_generation.py:</p>
			<ol>
				<li value="1">We need to import the following dependencies:<p class="source-code">import mlflow</p><p class="source-code">from datetime import date</p><p class="source-code">from dateutil.relativedelta import relativedelta</p><p class="source-code">import pprint</p><p class="source-code">import pandas as pd</p><p class="source-code">import pandas_datareader</p><p class="source-code">import pandas_datareader.data as web</p><p class="source-code">import numpy as np</p></li>
				<li>Before delving into the main component of the code, we'll now proceed to implement a critical function to generate the features by basically transforming the difference with each <em class="italic">n</em> preceding day in a feature that we will use to predict the next day, very similar <a id="_idIndexMarker260"/>to the approach that we used in previous <a id="_idIndexMarker261"/>chapters of the book for our running use case:<p class="source-code">def rolling_window(a, window):</p><p class="source-code">    """</p><p class="source-code">        Takes np.array 'a' and size 'window' as parameters</p><p class="source-code">        Outputs an np.array with all the ordered sequences of values of 'a' of size 'window'</p><p class="source-code">        e.g. Input: ( np.array([1, 2, 3, 4, 5, 6]), 4 )</p><p class="source-code">             Output: </p><p class="source-code">                     array([[1, 2, 3, 4],</p><p class="source-code">                           [2, 3, 4, 5],</p><p class="source-code">                           [3, 4, 5, 6]])</p><p class="source-code">    """</p><p class="source-code">    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)</p><p class="source-code">    strides = a.strides + (a.strides[-1],)</p><p class="source-code">    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)</p></li>
				<li>Next, we'll proceed to read the staged file that is deemed as clean and ready to be used by upstream processes:<p class="source-code">    with mlflow.start_run() as run:</p><p class="source-code">        mlflow.set_tag("mlflow.runName", "feature_set_</p><p class="source-code">generation")</p><p class="source-code">        btc_df = pd.read_csv("data/staging/data.csv")</p><p class="source-code">        btc_df['delta_pct'] = (btc_df['Close'] - btc_df['Open'])/btc_df['Open']</p><p class="source-code">        btc_df['going_up'] = btc_df['delta_pct'].apply(lambda d: 1 if d&gt;0.00001 else 0).to_numpy()</p><p class="source-code">        element=btc_df['going_up'].to_numpy()</p><p class="source-code">        WINDOW_SIZE=15</p><p class="source-code">        training_data = rolling_window(element, WINDOW_SIZE)</p><p class="source-code">        pd.DataFrame(training_data).to_csv("data/training/da<a id="_idTextAnchor126"/>ta.csv", index=False)</p></li>
			</ol>
			<p>We generate <a id="_idIndexMarker262"/>the feature set and features. We are now able to run all of the <a id="_idIndexMarker263"/>end-to-end pipeline from data acquisition to feature generation.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor127"/>Running your end-to-end pipeline</h2>
			<p>In this section, we will run <a id="_idIndexMarker264"/>the complete example, which you can retrieve from the following address for the book's GitHub repository in the folder at <strong class="source-inline">/Chapter07/psytock-data-features-main</strong>. <em class="italic">Figure 7.3</em> illustrates the complete folder structure of the project that you can inspect in GitHub and compare with your local version:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/image0034.jpg" alt="Figure 7.3 – Folder structure&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Folder structure</p>
			<p>To run the pipeline <a id="_idIndexMarker265"/>end to end, you should execute the following command in the directory with the code:</p>
			<p class="source-code">mlflow run . --experiment-name=psystock_data_pipelines</p>
			<p>It will basically execute the end-to-end pipeline and you can inspect it directly in the MLflow UI, running each of the steps of the pipeline in order:</p>
			<p class="source-code">mlflow ui</p>
			<p>You can run and explore the tracking information in MLflow at <a href="http://localhost:5000">http://localhost:5000</a>.</p>
			<p>In <em class="italic">Figure 7.4</em>, you can see the different runs of the main project and subprojects of the stages of the pipeline in a nested workflow format that you can browse to inspect the details:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/image0044.jpg" alt="Figure 7.4 – High-level architecture with a data layer reference&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – High-level architecture with a data layer reference</p>
			<p>In <em class="italic">Figure 7.5</em>, you can see <a id="_idIndexMarker266"/>the reference to the <strong class="source-inline">load_raw_data</strong> phase of the data pipeline and check when it was started and stopped and the parameters used:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/image0054.jpg" alt="Figure 7.5 – High-level architecture with a data layer reference&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – High-level architecture with a data layer reference</p>
			<p>In <em class="italic">Figure.7.6</em>, you can see the reference to the <strong class="source-inline">check_verify_data</strong> phase of the data pipeline where we logged some basic statistical information of the dataset obtained:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/image0063.jpg" alt="Figure 7.6 – High-level architecture with a data layer reference&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – High-level architecture with a data layer reference</p>
			<p>If any data <a id="_idIndexMarker267"/>quality issues are detected, the workflow will fail with a clear indication of which section failed, as represented in <em class="italic">Figure 7.7</em>:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/image0073.jpg" alt="Figure 7.7 – Checking errors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Checking errors</p>
			<p>With this section, we have concluded the description of the process of data management and feature generation in a data pipeline implemented with the <strong class="source-inline">MLProjects</strong> module in MLflow. We will now look at how to manage the data in a feature store.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor128"/>Using a feature store</h1>
			<p>A feature store is a <a id="_idIndexMarker268"/>software layer on top of your data to abstract all the <a id="_idIndexMarker269"/>production and management processes for data by providing inference systems with an interface to retrieve a feature set that can be used for inference or training. </p>
			<p>In this section, we will illustrate the concept of a feature store by using Feast (a feature store), an operational data system for managing and serving machine learning features to models in production: </p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/image0083.jpg" alt="Figure 7.8 – Feast Architecture (retrieved from https://docs.feast.dev/)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – Feast Architecture (retrieved from https://docs.feast.dev/)</p>
			<p>In order to understand how Feast works and how it can fit into your data layer component (code available at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter07/psystock_feature_store, execute the following steps:</p>
			<ol>
				<li value="1">Install <strong class="source-inline">feast</strong>:<p class="source-code">pip install feast==0.10</p></li>
				<li>Initialize a feature repository:<p class="source-code">feast init</p></li>
				<li>Create your feature definitions by replacing the <strong class="source-inline">yaml</strong> file generated automatically:<p class="source-code">project: psystock_feature_store</p><p class="source-code">registry: data/registry.db</p><p class="source-code">provider: local</p><p class="source-code">online_store:</p><p class="source-code">    path: data/online_store.db</p></li>
				<li>We will now proceed to import dependencies of the feature definition:<p class="source-code">from google.protobuf.duration_pb2 import Duration</p><p class="source-code">from feast import Entity, Feature, FeatureView, ValueType</p><p class="source-code">from feast.data_source import FileSource</p></li>
				<li>We can <a id="_idIndexMarker270"/>now load the feature files:<p class="source-code">token_features = FileSource(</p><p class="source-code">    path="/data/features.csv",</p><p class="source-code">    event_timestamp_column="create_date",</p><p class="source-code">    created_timestamp_column="event_date",</p><p class="source-code">)</p><p class="source-code">token= Entity(name="token", value_type=ValueType.STRING, description="token id",)</p></li>
				<li>We can now add a feature view: <p class="source-code">hourly_view_features_token = FeatureView(</p><p class="source-code">    name="token_hourly_features",</p><p class="source-code">    entities=["token"],</p><p class="source-code">    ttl=Duration(seconds=3600 * 1),</p><p class="source-code">    features=[</p><p class="source-code">        Feature(name="prev_10days", dtype=ValueType.INT64),</p><p class="source-code">        Feature(name="prev_11days", dtype=ValueType.INT64),</p><p class="source-code">        Feature(name="prev_12days", dtype=ValueType.INT64),</p><p class="source-code">        Feature(name="prev_13days", dtype=ValueType.INT64)</p><p class="source-code">    ],</p><p class="source-code">    online=True,</p><p class="source-code">    input=token_features,</p><p class="source-code">    tags={},</p><p class="source-code">)</p></li>
				<li>To deploy the feature store with the configurations added so far, we need to run the following command:<p class="source-code">feast apply</p><p>At this stage, the feature store is deployed in your environment (locally in this case) and the feature store is available to be used from your MLflow job.</p></li>
				<li>We can now <a id="_idIndexMarker271"/>do feature retrieval, now that all the features are stored in a feature store:<p class="source-code">import pandas as pd</p><p class="source-code">from datetime import datetime</p><p class="source-code">from feast import FeatureStore</p><p class="source-code"># entity_df generally comes from upstream systems</p><p class="source-code">event_data_point = pd.DataFrame.from_dict({</p><p class="source-code">    "token": ["btc","btc"],</p><p class="source-code">    "event_date": [</p><p class="source-code">        datetime(2021, 4, 12, 10, 59, 42),</p><p class="source-code">        datetime(2021, 4, 12, 8,  12, 10),</p><p class="source-code">    ]</p><p class="source-code">})</p><p class="source-code">store = FeatureStore(repo_path=".")</p><p class="source-code">feature_loading_df = store.get_historical_features(</p><p class="source-code">    entity_df=entity_df, </p><p class="source-code">    feature_refs = [</p><p class="source-code">        'token_hourly_features:prev_3days',</p><p class="source-code">        'token_hourly_features:prev_4days',</p><p class="source-code">        'token_hourly_features:prev_5days'        </p><p class="source-code">    ],</p><p class="source-code">).to_df()</p></li>
			</ol>
			<p>You can now <a id="_idIndexMarker272"/>integrate your feature store repository into your MLflow workloads. </p>
			<p>With this section, we have concluded the description of the process of data management and feature generation in a data pipeline implemented with the <strong class="source-inline">MLProjects module</strong> in MLflow. We are now ready to deal with production environment deployments in subsequent chapters. </p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor129"/>Summary</h1>
			<p>In this chapter, we covered MLflow and its integration with the feature management data layer of our reference architecture. We leveraged the features of the MLflow Projects module to structure our data pipeline.</p>
			<p>The important layer of data and feature management was introduced, and the need for feature generation was made clear, as were the concepts of data quality, validation, and data preparation.</p>
			<p>We applied the different stages of producing a data pipeline to our own project. We then formalized data acquisition and quality checks. In the last section, we introduced the concept of a feature store and how to create and use one.</p>
			<p>In the next chapters and following section of the book, we will focus on applying the data pipeline and features to the process of training and deploying the data pipeline in production.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor130"/>Further reading</h1>
			<p>In order to further your knowledge, you can consult the documentation at the following link:</p>
			<p><a href="https://github.com/mlflow/mlflow/blob/master/examples/multistep_workflow/MLproject">https://github.com/mlflow/mlflow/blob/master/examples/multistep_workflow/MLproject</a></p>
		</div>
	</body></html>