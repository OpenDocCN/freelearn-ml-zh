<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Classification Models"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Classification Models</h1></div></div></div><p>Classification is another kind of supervised machine learning. In this chapter, before getting into the details of building a classification model using ML Studio, you will start with gaining the basic knowledge about a classification algorithm and how a model is evaluated. Then, you will build models with different datasets using different algorithms.</p><div class="section" title="Understanding classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec41"/>Understanding classification</h1></div></div></div><p>Consider you are <a id="id237" class="indexterm"/>given the following hypothetical dataset containing data of patients: the size of the tumor in their body, their age, and a class that justifies whether they are affected by cancer or not, 1 being positive (affected by cancer) and 0 being negative (not affected by cancer):</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Age</p>
</th><th style="text-align: left" valign="bottom">
<p>Tumor size</p>
</th><th style="text-align: left" valign="bottom">
<p>Class</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>135</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>37</p>
</td><td style="text-align: left" valign="top">
<p>121</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>18</p>
</td><td style="text-align: left" valign="top">
<p>156</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>55</p>
</td><td style="text-align: left" valign="top">
<p>162</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>67</p>
</td><td style="text-align: left" valign="top">
<p>107</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>73</p>
</td><td style="text-align: left" valign="top">
<p>157</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>36</p>
</td><td style="text-align: left" valign="top">
<p>123</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>42</p>
</td><td style="text-align: left" valign="top">
<p>189</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>29</p>
</td><td style="text-align: left" valign="top">
<p>148</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr></tbody></table></div><p>Here, the patients are classified as cancer-affected or not. A new patient comes in at the age 17 and is diagnosed of having a tumor the of size 149. Now, you need to predict the classification of this new patient based on the previous data. That's classification for you as you need to predict the class of the dependent variable; here it is <span class="strong"><strong>0</strong></span> or <span class="strong"><strong>1</strong></span>—you may also think of it as true or false.</p><p>For a regression problem, you predict a number, for example, the housing price or a numerical value. In a classification problem, you predict a categorical value, though it may be represented with a number, such as <span class="strong"><strong>0</strong></span> or <span class="strong"><strong>1</strong></span>.</p><p>You should not be confused between a regression and classification problem. Consider a case where you need to predict the <a id="id238" class="indexterm"/>housing price not as a number, but as categories, such as greater than 100K or less than 100K. In this case, though you are predicting the housing price, you are indeed predicting a class or category for the housing price and hence, it's a classification problem.</p><p>You build a classification model by training an algorithm with the given training data. In the training dataset, the class or target variable is already known.</p><div class="section" title="Evaluation metrics"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec46"/>Evaluation metrics</h2></div></div></div><p>Suppose that you have built a <a id="id239" class="indexterm"/>model and trained a classification <a id="id240" class="indexterm"/>algorithm with the dataset in Table 7.1 as the training data. Now, you are using the following table as your test data. As you can see, the last column has the predicted class.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Age</p>
</th><th style="text-align: left" valign="bottom">
<p>Tumor size</p>
</th><th style="text-align: left" valign="bottom">
<p>Actual class</p>
</th><th style="text-align: left" valign="bottom">
<p>Predicted class</p>
</th><th style="text-align: left" valign="bottom"> </th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>32</p>
</td><td style="text-align: left" valign="top">
<p>135</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>TN</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>47</p>
</td><td style="text-align: left" valign="top">
<p>121</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>FP</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>28</p>
</td><td style="text-align: left" valign="top">
<p>156</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>FN</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>45</p>
</td><td style="text-align: left" valign="top">
<p>162</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>TP</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>77</p>
</td><td style="text-align: left" valign="top">
<p>107</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>FP</p>
</td></tr></tbody></table></div><div class="section" title="True positive"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec27"/>True positive</h3></div></div></div><p>This is the number of <a id="id241" class="indexterm"/>times an actual class was positive and was predicted as positive. For example, the patient is actually affected by cancer and the model is also predicted positive.</p><p>In our preceding example, there is one instance where the <span class="strong"><strong>Actual Class</strong></span> = <span class="strong"><strong>1</strong></span> and <span class="strong"><strong>Predicted Class</strong></span> = <span class="strong"><strong>1</strong></span>. So here, <span class="strong"><strong>TP</strong></span> = <span class="strong"><strong>1</strong></span>.</p></div><div class="section" title="False positive"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec28"/>False positive</h3></div></div></div><p>This is the number of <a id="id242" class="indexterm"/>times an actual class was negative and was predicted as positive. For example, the patient is actually <span class="emphasis"><em>not</em></span> affected by cancer but the model is predicted as positive.</p><p>In our preceding example, there are two instances where the <span class="strong"><strong>Actual Class</strong></span> = <span class="strong"><strong>0</strong></span> and <span class="strong"><strong>Predicted Class</strong></span> = <span class="strong"><strong>1</strong></span>. So here, <span class="strong"><strong>FP</strong></span> = <span class="strong"><strong>2</strong></span>.</p></div><div class="section" title="True negative"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec29"/>True negative</h3></div></div></div><p>This is the number of times an <a id="id243" class="indexterm"/>actual class was negative and it was predicted as negative. For example, the patient is actually NOT affected by cancer and the model also predicted it as negative.</p><p>In our preceding example, there was one instance where the <span class="strong"><strong>Actual Class</strong></span> = <span class="strong"><strong>0</strong></span> and <span class="strong"><strong>Predicted Class</strong></span> = <span class="strong"><strong>0</strong></span>. So here, <span class="strong"><strong>TN</strong></span> = <span class="strong"><strong>1</strong></span>.</p></div><div class="section" title="False negative"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec30"/>False negative</h3></div></div></div><p>This is the number of times an <a id="id244" class="indexterm"/>actual class was positive but was predicted as negative. For example, the patient is actually affected by cancer but the model predicted it as negative.</p><p>In our preceding example, there was one instance where the <span class="strong"><strong>Actual Class</strong></span> = <span class="strong"><strong>1</strong></span> and <span class="strong"><strong>Predicted Class</strong></span> = <span class="strong"><strong>0</strong></span>. So here, <span class="strong"><strong>FN</strong></span> = <span class="strong"><strong>1</strong></span>.</p><p>The following table shows <span class="strong"><strong>TP</strong></span>, <span class="strong"><strong>TN</strong></span>, <span class="strong"><strong>FP</strong></span>, and <span class="strong"><strong>FN</strong></span> in a matrix:</p><div class="mediaobject"><img src="graphics/0792EN_07_27.jpg" alt="False negative"/></div></div><div class="section" title="Accuracy"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec31"/>Accuracy</h3></div></div></div><p>It is the proportion of a <a id="id245" class="indexterm"/>true prediction to the total number of predictions. While true prediction is <span class="emphasis"><em>TP + TN</em></span>, the total number of predictions are of the size of a test dataset, which is also <span class="emphasis"><em>TP + TN + FP + FN</em></span>. So, accuracy can be represented in a formula as follows:</p><p>Accuracy = (TP + TN) / (TP + TN + FP + FN)</p><p>So in our example, <span class="emphasis"><em>Accuracy = (1 + 1) / (1 + 1 + 2 + 1)  = 2/5 = .4</em></span>.</p><p>Accuracy can also be represented as a percentage of the prediction that was accurate. So, in our example, accuracy is 40 percent.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>Note that the preceding figures are just for illustration of how the calculation is done. In practice, when you build a model, it should have an accuracy of more than 50 percent; otherwise, the model is no good because even a random trial will have 50 percent accuracy.</p></div></div></div><div class="section" title="Precision"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec32"/>Precision</h3></div></div></div><p>The positive predictive <a id="id246" class="indexterm"/>value or precision is the proportion of positive cases that the model has correctly identified. Precision can be represented in the formula form, as follows:</p><p>Precision = TP / (TP + FP)</p><p>So in our example, <span class="emphasis"><em>Precision = 1 / ( 1+1) = 1/2 = .5</em></span>.</p></div><div class="section" title="Recall"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec33"/>Recall</h3></div></div></div><p>Sensitivity or recall is the <a id="id247" class="indexterm"/>proportion of actual positive cases that are correctly identified. The formula for recall is:</p><p>
<span class="emphasis"><em>Recall = TP / (TP + FN)</em></span>
</p><p>So in our example, <span class="emphasis"><em>Recall = 1 / (1 +2) = 1/3 = .33</em></span>.</p></div><div class="section" title="The F1 score"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec34"/>The F1 score</h3></div></div></div><p>The F1 Score can be <a id="id248" class="indexterm"/>defined as a formula, as <a id="id249" class="indexterm"/>follows:</p><p>
<span class="emphasis"><em>F1 = 2TP / (2TP + FP + FN)</em></span>
</p><p>The F1 Score can also be defined in terms of precision (P) and recall (R), as follows:</p><p>
<span class="emphasis"><em>F1 = 2PR/(P+R)</em></span>
</p><p>So in our example, <span class="emphasis"><em>F1 =  (1 * 2) /   {(1 * 2) + 2 + 1 }  = 2/ (2 + 2 +1) = 2/5 =.4</em></span>.</p></div><div class="section" title="Threshold"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec35"/>Threshold</h3></div></div></div><p>Threshold is the value <a id="id250" class="indexterm"/>above which the threshold belongs to the <a id="id251" class="indexterm"/>first class and all the other values belong to the second class. For example, if the threshold is <span class="strong"><strong>0.5</strong></span>, then any patient who has scored more than or equal to <span class="strong"><strong>0.5</strong></span> is identified as sick; otherwise, the patient is identified as healthy. You can think of threshold as probability. To illustrate, if there is a probability of 80 percent or <span class="strong"><strong>.8</strong></span> percent that it may rain today, then you may predict that rain for today is true. Similarly, if it is less than <span class="strong"><strong>.8</strong></span>, then you can predict that it won't rain. So your prediction would depend on the threshold here.</p></div><div class="section" title="Understanding ROC and AUC"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec36"/>Understanding ROC and AUC</h3></div></div></div><p>The <span class="strong"><strong>receiver </strong></span><a id="id252" class="indexterm"/>
<span class="strong"><strong>operating characteristics</strong></span> (<span class="strong"><strong>ROC</strong></span>) graph is a two-dimensional graph in which the true positive rate (TP) is plotted on the <span class="emphasis"><em>y</em></span> axis and the false positive rate (FP) is plotted on the x <a id="id253" class="indexterm"/>axis. An ROC graph depicts the relative tradeoffs between benefits (true positives) and costs (false positives).</p><p>The <span class="strong"><strong>Area Under the Curve</strong></span> (<span class="strong"><strong>AUC</strong></span>) is a portion of the area under the ROC curve of the unit square; its value <a id="id254" class="indexterm"/>will always be <a id="id255" class="indexterm"/>between <span class="emphasis"><em>0</em></span> and <span class="emphasis"><em>1</em></span>, where <span class="emphasis"><em>1</em></span> is the best case or everything is predicted correctly. However, because random guessing produces the diagonal line between <span class="emphasis"><em>(0, 0)</em></span> and <span class="emphasis"><em>(1, 1)</em></span>, which has an area of <span class="strong"><strong>0.5</strong></span>, no realistic classifier should have an AUC less than <span class="strong"><strong>0.5</strong></span>. AUC is often used as a measure of quality of a classification model.</p><p>In the following diagram, the blue curve shows the ROC while the area painted in red shows the AUC. The yellow painted diagonal line represents the random guessing:</p><div class="mediaobject"><img src="graphics/0792EN_07_23.jpg" alt="Understanding ROC and AUC"/></div></div><div class="section" title="Motivation for the matrix to consider"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec37"/>Motivation for the matrix to consider</h3></div></div></div><p>While choosing an <a id="id256" class="indexterm"/>algorithm for your model, you will have to rely on the preceding metrics that are defined. Often, one metric may not be sufficient to take a decision. To start with, you may look at accuracy, but at times it might be deceptive. Consider a case where you are making a prediction for a rare disease where in reality, 99 percent negative cases and 1 percent of positive cases appear. If your classification model predicts all the cases as true negatives, then the accuracy is still 99 percent. In this case, the F1 score might be useful as it would give you a clear picture. AUC might also be useful for this.</p><p>Consider another scenario. Let's stick to our disease prediction example. Suppose you are predicting whether a patient has cancer or not. If you predict a false case (where the patient is NOT affected by the disease) as true, it's a false positive case. In the practical scenario, after such a prediction, the patient will have further medical tests to manually declare as not affected by cancer. However, if you have a predicted true case (where the patient is actually affected by the disease) as false, then it's a false negative case. In practical scenarios, after such a prediction, the patient is left free and allowed to go home without medication. This might be dangerous as the patient might lose life. You may never like to make such a prediction. In such a scenario, as in this story, you may reduce the threshold value to reduce the chance of releasing any true positive cases. Hence, it would result in higher recall and lower precision.</p><p>In the scenario opposite to the preceding, say you have a classification model to predict the fraud for an online transaction. Here, predicting a case as fraud, (which is actually not—a case of false positive) may <a id="id257" class="indexterm"/>result in poor customer satisfaction. So in this scenario, you may increase the threshold value and hence it would result in higher precision and lower recall.</p><p>As you may find from the preceding definition, the F1 score is a balanced approach for measurement, which involves both precision and recall.</p><p>When you are not too worried about precision and recall or you are not so sure about them, you can just follow the value of AUC (the higher the better). Many find AUC the best way to measure the performance of a classification model. AUC also provides a graphical representation. However, it is always a good idea to take a note of more than one metric.</p></div></div></div></div>
<div class="section" title="Training, scoring, and evaluating modules"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec42"/>Training, scoring, and evaluating modules</h1></div></div></div><p>As with regression <a id="id258" class="indexterm"/>problems, which you saw in the previous chapter, with <a id="id259" class="indexterm"/>classification problems, you can start with an algorithm and train it <a id="id260" class="indexterm"/>with data. You can then score ideally with the test data and evaluate the performance of the model.</p><p>Navigate to the <span class="strong"><strong>Train</strong></span> | <span class="strong"><strong>Score</strong></span> | <span class="strong"><strong>Evaluate</strong></span> option on the screen.</p><p>The <span class="strong"><strong>Train</strong></span>, <span class="strong"><strong>Score</strong></span>, and <span class="strong"><strong>Evaluate</strong></span> modules are the same as you used for regression. The <span class="strong"><strong>Train</strong></span> module requires the name of the target (class) variable. The <span class="strong"><strong>Evaluate</strong></span> module generates evaluation metrics for classification.</p><p>If you want to tune parameters of an algorithm by parameter sweeping, you can use the same <span class="strong"><strong>Sweep Parameters</strong></span> module.</p></div>
<div class="section" title="Classifying diabetes or not"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec43"/>Classifying diabetes or not</h1></div></div></div><p>The <span class="strong"><strong>Pima Indians Diabetes Binary Classification dataset</strong></span> module is present as a sample dataset in ML <a id="id261" class="indexterm"/>Studio. It contains all of the data of female patients of the same age belonging to Pima Indian heritage. The data includes medical data, such as glucose and insulin levels, as well as lifestyle factors of the patients. The columns in the dataset are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Number of times pregnant</li><li class="listitem" style="list-style-type: disc">Plasma glucose concentration of 2 hours in an oral glucose tolerance test</li><li class="listitem" style="list-style-type: disc">Diastolic blood pressure (mm Hg)</li><li class="listitem" style="list-style-type: disc">Triceps skin fold thickness (mm)</li><li class="listitem" style="list-style-type: disc">2-hour serum insulin (mu U/ml)</li><li class="listitem" style="list-style-type: disc">Body mass index (weight in kg/(height in m)^2)</li><li class="listitem" style="list-style-type: disc">Diabetes pedigree function</li><li class="listitem" style="list-style-type: disc">Age (years)</li><li class="listitem" style="list-style-type: disc">Class variable (0 or 1)</li></ul></div><p>The last column is the target variable or class variable that takes the value 0 or 1, where 1 is positive or affected by diabetes and 0 means that the patient is not affected.</p><p>You have to build models that could predict whether a patient has diabetes or tests positive or not.</p><div class="section" title="Two-class bayes point machine"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec47"/>Two-class bayes point machine</h2></div></div></div><p>Two-class <a id="id262" class="indexterm"/>Bayes Point Machine is a simple-to-train yet powerful linear classifier. We will build our first classification model using it.</p><p>Start a new experiment. On the left-hand side module palette on the screen, expand the <span class="strong"><strong>Saved Datasets</strong></span> option, scroll down, and drag the <span class="strong"><strong>Pima Indians Diabetes Binary Classification dataset</strong></span> module to the canvas. Alternatively, you could just type <code class="literal">pima</code> in the search box to locate the module and then drag it.</p><p>Right-click on its output port and click on the <span class="strong"><strong>Visualize</strong></span> option to explore the dataset. You can note that it now has 768 rows and 9 columns.</p><p>You have to split this dataset into two to prepare your train and test dataset. So, drag the <span class="strong"><strong>Split</strong></span> module to the canvas and connect the output of the dataset module to the input of the <span class="strong"><strong>Split</strong></span> module. Set <span class="strong"><strong>0.8</strong></span> as the parameter; the <span class="strong"><strong>Fraction of rows</strong></span> option is the first output dataset that splits itself in the ratio of 80:20 to get your train and test dataset, respectively.</p><p>Drag the <span class="strong"><strong>Two-Class Bayes Point Machine</strong></span> module, which you can find by navigating to <span class="strong"><strong>Machine Learning</strong></span> | <span class="strong"><strong>Initialize Model</strong></span> | <span class="strong"><strong>Classification</strong></span> on the left-hand side module's palette to the canvas.</p><p>This module has three parameter values to set. The <span class="strong"><strong>Number of training iterations</strong></span> module is the value that decides the number of times the algorithm iterates over the dataset. The default value <span class="strong"><strong>30</strong></span> is <a id="id263" class="indexterm"/>sufficient most of the time. The <span class="strong"><strong>Include bias</strong></span> checkbox if ticked or set to true, adds a constant feature or bias to each instance in training and prediction. The default value is true and it is required to be true most of the time. The last parameter, <span class="strong"><strong>Allow unknown values in categorical features</strong></span>, if ticked or set to true, creates an additional level for each categorical column. Any levels in the test dataset not available in the training dataset are mapped to this additional level. Unless you are doing the required data preprocessing, it is suggested that you tick this or leave it at the default value.</p><div class="mediaobject"><img src="graphics/0792EN_07_26.jpg" alt="Two-class bayes point machine"/></div><p>Drag the <span class="strong"><strong>Train Model</strong></span> module to the canvas and connect the output port of the <span class="strong"><strong>Two-Class Bayes Point Machine</strong></span> module to the first input port of the <span class="strong"><strong>Train Model</strong></span> module. Connect the first output port of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Train Model</strong></span> module. In the properties pane for the <span class="strong"><strong>Train Model</strong></span> module, click on the <span class="strong"><strong>Launch column selector</strong></span> button and when the pop-up appears, set <span class="strong"><strong>Class variable (0 or 1)</strong></span> as the column's target variable, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_13.jpg" alt="Two-class bayes point machine"/></div><p>Next, drag the <span class="strong"><strong>Score Model</strong></span> and <span class="strong"><strong>Evaluate Model</strong></span> modules to the canvas. Connect the output of the <a id="id264" class="indexterm"/>
<span class="strong"><strong>Train Model</strong></span> module to the first input of the <span class="strong"><strong>Score Model</strong></span> module and the second output of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Score Model</strong></span> module. Then, connect the output of the <span class="strong"><strong>Score Model</strong></span> module to the first input of the <span class="strong"><strong>Evaluate Model</strong></span> module. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_14.jpg" alt="Two-class bayes point machine"/></div><p>Click on <span class="strong"><strong>RUN</strong></span> and <a id="id265" class="indexterm"/>run the experiment. When it finishes (after all the modules gets a green tick mark), right-click on the output of the <span class="strong"><strong>Evaluate Model</strong></span> module and click on the <span class="strong"><strong>Visualize</strong></span> option to view the <span class="strong"><strong>Evaluation Results</strong></span>, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_15.jpg" alt="Two-class bayes point machine"/></div><p>By default, the graph shows the ROC curve. The more area it covers, the better the model performs. This is represented by the matric AUC. <a id="id266" class="indexterm"/>AUC, as you can find here, is <span class="strong"><strong>0.788</strong></span>.</p><p>Note the <span class="strong"><strong>Threshold</strong></span> scrollbar, which is set to <span class="strong"><strong>0.51</strong></span> at the moment, which is <span class="strong"><strong>0.5</strong></span> by default. You can increase or decrease it by dragging it to the left or right. As you change the value of threshold, all the other metrics apart from AUC get changed. The reason is obvious because when there are changes to the value of true positive and true negative, the rest of the values change. At the current value, the <span class="strong"><strong>Threshold</strong></span> (<span class="strong"><strong>0.51</strong></span>) <span class="strong"><strong>Accuracy</strong></span> option is set at <span class="strong"><strong>77.9</strong></span> percent.</p><p>You can also view the graph for precision/recall and lift by clicking on the respective tab at the top-left corner of the screen.</p></div><div class="section" title="Two-class neural network with parameter sweeping"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec48"/>Two-class neural network with parameter sweeping</h2></div></div></div><p>We will use the <a id="id267" class="indexterm"/>same diabetes dataset <a id="id268" class="indexterm"/>that we used to build the model using the neural network and to tune the parameter by parameter sweeping.</p><p>Create a new experiment. Drag and connect the same dataset to the <span class="strong"><strong>Split</strong></span> module, as you did in the previous section. Set <span class="strong"><strong>0.8</strong></span> as the parameter; <span class="strong"><strong>Fraction of rows</strong></span> in the first output dataset is split into 80-20 to get your train and test dataset, respectively.</p><p>Type <code class="literal">Sweep</code> in the search box in the modules palette to the left of the screen and when the <span class="strong"><strong>Sweep Parameters</strong></span> module appears, drag it to the canvas. Then, join the first output of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Sweep Parameters</strong></span> module and join the second output of the <span class="strong"><strong>Split</strong></span> module to the third input of the <span class="strong"><strong>Sweep Parameters</strong></span> module. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_16.jpg" alt="Two-class neural network with parameter sweeping"/></div><p>Now, you need to set the column of the dataset that is your target or label or class column for which you will train a model to make a prediction. In this case, <span class="strong"><strong>Class variable (0 or 1)</strong></span> is the target variable or class for which you are going to make a prediction. Also, set the sweeping mode to <span class="strong"><strong>Entire grid</strong></span> and <span class="strong"><strong>Metric for measure the performance for Classification</strong></span> to <span class="strong"><strong>Accuracy</strong></span>. Ignore the other parameter as this is a classification problem.</p><p>Type <code class="literal">Two-Class Neural Network</code> in search box at the top of the modules palette to the left and drag the <span class="strong"><strong>Two-Class Neural Network</strong></span> module to the canvas. Connect it to the first input of the <span class="strong"><strong>Sweep Parameters</strong></span> module. As usual, drag the <span class="strong"><strong>Score Model</strong></span> and <span class="strong"><strong>Evaluate Model</strong></span> modules to the canvas and make the necessary connections.</p><p>Connect the second output port of the <span class="strong"><strong>Sweep Parameters</strong></span> module to the first input port of the <span class="strong"><strong>Score Model</strong></span> module and connect the second output of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Score Model</strong></span> module. Then, connect the output of the <span class="strong"><strong>Score Model</strong></span> module to the first input of the <span class="strong"><strong>Evaluate Model</strong></span> module.</p><p>Run the <a id="id269" class="indexterm"/>experiment. Let's take a look <a id="id270" class="indexterm"/>at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_17.jpg" alt="Two-class neural network with parameter sweeping"/></div><p>As the experiment finishes running, visualize the output of the <span class="strong"><strong>Evaluate Model</strong></span> module to measure the performance of the model. Note the AUC and accuracy metrics.</p><p>While using parameter sweeping to find the best parameters for the model, it is a good practice to use a separate dataset to score and evaluate the prediction than what is used for training and parameter tuning. To illustrate the point, you can split your dataset into 60 percent and 40 percent. Then, use another split module to split the 40 percent (the second dataset) into 50 percent each. So now, you have three datasets containing 60 percent, 20 percent, and 20 percent of your original dataset. Then, use the first 60 percent and 20 percent for the <span class="strong"><strong>Sweep Parameters</strong></span> module and the rest 20 percent for scoring and evaluation. Let's <a id="id271" class="indexterm"/>take a look at the <a id="id272" class="indexterm"/>following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_25.jpg" alt="Two-class neural network with parameter sweeping"/></div></div></div>
<div class="section" title="Predicting adult income with decision-tree-based models"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec44"/>Predicting adult income with decision-tree-based models</h1></div></div></div><p>ML Studio <a id="id273" class="indexterm"/>comes with three <a id="id274" class="indexterm"/>decision-tree-based algorithms for two-class classification: the <span class="strong"><strong>Two-Class Decision Forest</strong></span>, <span class="strong"><strong>Two-Class Boosted Decision Tree</strong></span>, and <span class="strong"><strong>Two-Class Decision Jungle</strong></span> modules. These are known as ensemble models where more than one decision trees are assembled to obtain better predictive performance. Though all the three are based on decision trees, their underlying algorithms differ.</p><p>We will first build a model with the <span class="strong"><strong>Two-Class Decision Forest</strong></span> module and then compare it with the <span class="strong"><strong>Two-Class Boosted Decision Tree</strong></span> module for the <span class="strong"><strong>Adult Census Income Binary Classification dataset</strong></span> module, which is one of the sample datasets available in ML Studio. The dataset is a subset of the 1994 US census database and contains the demographic information of working adults over the 16 years age limit. Each instance or example in the dataset has a label or class variable that states whether a person earns 50K a year or not.</p><p>Create an <a id="id275" class="indexterm"/>experiment and drag the <a id="id276" class="indexterm"/>dataset from the <span class="strong"><strong>Saved Datasets</strong></span> group in the module palette. Right-click on the output port and click on <span class="strong"><strong>Visualize</strong></span> to explore the dataset. When you click on the different columns, you can find that these columns contain a large number of missing values: <span class="strong"><strong>workclass</strong></span>, <span class="strong"><strong>occupation</strong></span>, and <span class="strong"><strong>native-country</strong></span>. Other columns don't have missing values. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_18.jpg" alt="Predicting adult income with decision-tree-based models"/></div><p>Though it would still work, if you still build models with missing values, we would get rid of these columns in our models. Missing values may impact the predicted result.</p><p>In the search box, type <code class="literal">Project</code> and drag the <span class="strong"><strong>Project Columns</strong></span> module to the canvas. Connect the dataset module to this module. On the properties pane, click on the <span class="strong"><strong>Launch Column Selector</strong></span> module, so that the pop-up columns selector comes up. As you can see in the following screenshot, begin with all the columns and exclude the columns with the missing values: <span class="strong"><strong>workclass</strong></span>, <span class="strong"><strong>occupation</strong></span>, and <span class="strong"><strong>native-country</strong></span>:</p><div class="mediaobject"><img src="graphics/0792EN_07_19.jpg" alt="Predicting adult income with decision-tree-based models"/></div><p>Expand the <span class="strong"><strong>Data Transformation</strong></span> group and then expand the <span class="strong"><strong>Sample and Split</strong></span> option in the modules <a id="id277" class="indexterm"/>palette and drag the <span class="strong"><strong>Split</strong></span> module to the canvas. Set the <span class="strong"><strong>Fraction of rows</strong></span> parameter in the first output dataset to <span class="strong"><strong>0.8</strong></span> and leave the others at their default values. You are splitting the dataset so that 80 percent of the data will be used to train and rest 20 percent will be used for test.</p><p>Likewise, now drag the <span class="strong"><strong>Two-Class Decision Forest</strong></span> module to the canvas. Type <code class="literal">Decision Forest</code> in the search box in the modules palette to the left and when the module <a id="id278" class="indexterm"/>appears, drag it to the canvas. Set the <span class="strong"><strong>Resampling method</strong></span> property to <span class="strong"><strong>Bagging</strong></span> and leave the rest of the parameters at their default values. Leave the module with the default values for the properties.</p><p>Drag a <span class="strong"><strong>Train Model</strong></span> module to the canvas and connect the output port of the <span class="strong"><strong>Two-Class Decision Forest</strong></span> module to the first input port of the <span class="strong"><strong>Train Model</strong></span> module. Connect the first output port of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Train Model</strong></span> module. In the properties pane for the <span class="strong"><strong>Train</strong></span> module, click on the <span class="strong"><strong>column selector</strong></span> option and set <span class="strong"><strong>income</strong></span> as the column's target variable.</p><p>Next, drag the <span class="strong"><strong>Score Model</strong></span> and <span class="strong"><strong>Evaluate Model</strong></span> modules to the canvas. Connect the output of the <span class="strong"><strong>Train Model</strong></span> module to the first input of the <span class="strong"><strong>Score Model</strong></span> module and connect the second output of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Score Model</strong></span> module. Then, connect the output of the <span class="strong"><strong>Score Model</strong></span> module to the first input of the <span class="strong"><strong>Evaluate Model</strong></span> module.</p><p>Run the experiment and after its successful execution, visualize the evaluation result. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_20.jpg" alt="Predicting adult income with decision-tree-based models"/></div><p>As with any experiment, you can now compare your model with another algorithm. You built a model using the <span class="strong"><strong>Two-Class Decision Forest</strong></span> module. Now, use another algorithm, such as the <span class="strong"><strong>Two-Class Boosted Decision Tree</strong></span> module and evaluate it.</p><p>To do so, start <a id="id279" class="indexterm"/>with the experiment and <a id="id280" class="indexterm"/>select the <span class="strong"><strong>Train</strong></span> and <span class="strong"><strong>Score</strong></span> modules by pressing <span class="emphasis"><em>ctrl</em></span> on your keyboard and clicking on both the modules. Then, copy the selected modules and paste them on the canvas by right-clicking on them and pasting them or just by pressing <span class="emphasis"><em>ctrl</em></span> + <span class="emphasis"><em>v</em></span> on your keyboard. It supports copy paste much like any other MS product, for example, MS Word.</p><p>Now, click anywhere on the canvas to unselect the pasted modules and rearrange them so that no module is placed on another and all are readable. Remove the connection between the <span class="strong"><strong>Two-Class Decision Forest</strong></span> and the <span class="strong"><strong>Train Model</strong></span> modules by selecting them and pressing <span class="emphasis"><em>Delete</em></span>. Drag the <span class="strong"><strong>Two-Class Boosted Decision Tree</strong></span> module from the left-hand side palette, to the canvas and connect the output of the module to the <span class="strong"><strong>Train Model</strong></span> module. Leave it at the default property values. Connect the output of the <span class="strong"><strong>Score Model</strong></span> <a id="id281" class="indexterm"/>module to the <a id="id282" class="indexterm"/>second input of the <span class="strong"><strong>Evaluate Model</strong></span> module and run the experiment. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_21.jpg" alt="Predicting adult income with decision-tree-based models"/></div><p>After the successful run, right-click on the output port of the <span class="strong"><strong>Evaluate Model</strong></span> module and click on <span class="strong"><strong>Visualize</strong></span> to find the evaluation result of the two models on a single canvas.</p><div class="mediaobject"><img src="graphics/0792EN_07_22.jpg" alt="Predicting adult income with decision-tree-based models"/></div><p>As you can see <a id="id283" class="indexterm"/>in the preceding graph, with the current settings, the model with the <span class="strong"><strong>Two-Class Boosted Decision Tree</strong></span> <a id="id284" class="indexterm"/>module has higher values than the other when you see the AUC and accuracy figures.</p><p>So, we know that it is performing better than the other one.</p></div>
<div class="section" title="Do it yourself &#x2013; comparing models to choose the best"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec45"/>Do it yourself – comparing models to choose the best</h1></div></div></div><p>You have already <a id="id285" class="indexterm"/>tried two algorithms for the <span class="strong"><strong>Adult Census Income Binary Classification dataset</strong></span> module. Now, try another two modules to choose the best one for your final model: the <span class="strong"><strong>Two-Class Boosted Decision Tree</strong></span> and the <span class="strong"><strong>Two-Class Neural Network</strong></span> modules. Try out different parameters; use the <span class="strong"><strong>Sweep Parameters</strong></span> module to optimize the parameters for the algorithms. The following screenshot is just for your reference—your experiment might differ. You may also try this with other available algorithms, for example, the <span class="strong"><strong>Two-Class Averaged Perceptron</strong></span> or the <span class="strong"><strong>Two-Class Logistic </strong></span><a id="id286" class="indexterm"/>
<span class="strong"><strong>Regression</strong></span> modules to find the best model. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_11.jpg" alt="Do it yourself – comparing models to choose the best"/></div></div>
<div class="section" title="Multiclass classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec46"/>Multiclass classification</h1></div></div></div><p>The classification you have <a id="id287" class="indexterm"/>seen and experienced so far is a two-class classification where the target variable can be of two classes. In multiclass classification, you classify in more than two classes, for example continuing on our hypothetical tumor problem, for a given tumor size and age of a patient, you might predict one of these three classes as the possibility of a patient being affected with cancer: High, Medium, and Low. In theory, a target variable can have any number of classes.</p><div class="section" title="Evaluation metrics – multiclass classification"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec49"/>Evaluation metrics – multiclass classification</h2></div></div></div><p>ML Studio lets you <a id="id288" class="indexterm"/>evaluate your model with an accuracy that is calculated as a ratio of the number of correct predictions versus the incorrect ones. Consider the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Age</p>
</th><th style="text-align: left" valign="bottom">
<p>Tumor size</p>
</th><th style="text-align: left" valign="bottom">
<p>Actual class</p>
</th><th style="text-align: left" valign="bottom">
<p>Predicted class</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>32</p>
</td><td style="text-align: left" valign="top">
<p>135</p>
</td><td style="text-align: left" valign="top">
<p>Low</p>
</td><td style="text-align: left" valign="top">
<p>Medium</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>47</p>
</td><td style="text-align: left" valign="top">
<p>121</p>
</td><td style="text-align: left" valign="top">
<p>Medium</p>
</td><td style="text-align: left" valign="top">
<p>Medium</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>28</p>
</td><td style="text-align: left" valign="top">
<p>156</p>
</td><td style="text-align: left" valign="top">
<p>Medium</p>
</td><td style="text-align: left" valign="top">
<p>High</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>45</p>
</td><td style="text-align: left" valign="top">
<p>162</p>
</td><td style="text-align: left" valign="top">
<p>High</p>
</td><td style="text-align: left" valign="top">
<p>High</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>77</p>
</td><td style="text-align: left" valign="top">
<p>107</p>
</td><td style="text-align: left" valign="top">
<p>Medium</p>
</td><td style="text-align: left" valign="top">
<p>Medium</p>
</td></tr></tbody></table></div><p>The following can be the evaluation metrics where in the columns, the text is marked in bold and have background colors according to the accuracy per class. For example, there were three actual classes as Medium, but only two were correctly predicted, so <span class="emphasis"><em>accuracy = 2/ 3 = 66.6 %</em></span>. It also shows that 33.3 percent of the Medium class was inaccurately predicted as High. This is also <a id="id289" class="indexterm"/>known as the confusion matrix. Let's take a look at the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Predicted Actual</p>
</th><th style="text-align: left" valign="bottom">
<p>Low</p>
</th><th style="text-align: left" valign="bottom">
<p>Medium</p>
</th><th style="text-align: left" valign="bottom">
<p>High</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Low</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>0 (0 percent)</p>
</td><td style="text-align: left" valign="top">
<p>1 (100 percent)</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Medium</strong></span>
</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>2 (66.6 percent)</p>
</td><td style="text-align: left" valign="top">
<p>1(33.3 percent)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>High</strong></span>
</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top">
<p>1 (100 percent)</p>
</td></tr></tbody></table></div></div></div>
<div class="section" title="Multiclass classification with the Iris dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec47"/>Multiclass classification with the Iris dataset</h1></div></div></div><p>The <span class="strong"><strong>Iris</strong></span> dataset is one <a id="id290" class="indexterm"/>of the classic and simple <a id="id291" class="indexterm"/>datasets. It contains the observations <a id="id292" class="indexterm"/>about the Iris plant. Each instance has four features: the sepal length, sepal width, petal length, and petal width. All the measurements are in centimeters. The dataset contains three classes for the target variable, where <a id="id293" class="indexterm"/>each class refers to a type of Iris plant: <span class="strong"><strong>Iris Setosa</strong></span>, <span class="strong"><strong>Iris Versicolour</strong></span>, and <span class="strong"><strong>Iris Virginica</strong></span>.</p><p>You can find <a id="id294" class="indexterm"/>more information on this dataset at <a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Iris">http://archive.ics.uci.edu/ml/datasets/Iris</a>.</p><p>As this dataset is not present as a sample dataset in ML Studio, you need to import it to ML Studio using a reader module before building any model on it. Note that the <span class="strong"><strong>Iris dataset</strong></span> present in the <a id="id295" class="indexterm"/>
<span class="strong"><strong>Saved Dataset</strong></span> section is the <a id="id296" class="indexterm"/>subset of the original dataset and is only present for two classes.</p><div class="section" title="Multiclass decision forest"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec50"/>Multiclass decision forest</h2></div></div></div><p>Decision forest <a id="id297" class="indexterm"/>is also available for multiclass classification. We will first use this with parameter sweep to train the model.</p><p>Follow the given steps to import the Iris dataset:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Go to ML Studio. Click on the <span class="strong"><strong>+NEW</strong></span> button and choose <span class="strong"><strong>Blank Experiment</strong></span>.</li><li class="listitem">From the modules palette, find the <span class="strong"><strong>Reader</strong></span> module under the <span class="strong"><strong>Data Input and Output</strong></span> group and drag it to the experiment canvas.</li><li class="listitem">The module properties pane is displayed after this. Choose the data source as <span class="strong"><strong>HTTP</strong></span>.</li><li class="listitem">Specify a complete URL: <a class="ulink" href="http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data">http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data</a>.</li><li class="listitem">Specify the data <a id="id298" class="indexterm"/>format as <span class="strong"><strong>CSV</strong></span>.</li><li class="listitem">Don't tick the checkbox for the header row, as the dataset does not contain any header. You might end up with something as follows:<div class="mediaobject"><img src="graphics/0792EN_07_01.jpg" alt="Multiclass decision forest"/></div></li></ol></div><p>Run the experiment and when you see the green tick mark on the <span class="strong"><strong>Reader</strong></span> module, right-click on the output port and click on <span class="strong"><strong>Visualize</strong></span>. Clicking on any column, you can notice that ML Studio shows a missing value.</p><p>Use the <span class="strong"><strong>Clean Missing Data</strong></span> module to remove the row containing the missing value. Drag the module that can be found under the <span class="strong"><strong>Data Transformation</strong></span> group and then under <span class="strong"><strong>Manipulation</strong></span> in the modules palette to the canvas. Connect the output port of the <span class="strong"><strong>Reader</strong></span> module to the input port of this module. On the properties pane, choose <span class="strong"><strong>Remove entire row</strong></span> for the property for <span class="strong"><strong>Cleaning mode</strong></span>, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_02.jpg" alt="Multiclass decision forest"/></div><p>Expand the <span class="strong"><strong>Data Transformation</strong></span> group and then expand the <span class="strong"><strong>Sample and Split</strong></span> option in the modules palette and drag the <span class="strong"><strong>Split</strong></span> module to the canvas. Set the <span class="strong"><strong>Fraction of rows</strong></span> parameter in the first output dataset to <span class="strong"><strong>0.7</strong></span> and leave the others at their default values. You are <a id="id299" class="indexterm"/>splitting the dataset so that 70 percent of the data will be used to train and the other 30 percent will be used for test.</p><p>Likewise, now drag the <span class="strong"><strong>Multiclass Decision Forest</strong></span> module to canvas. To do so, type <code class="literal">Decision Forest</code> in the search box in the modules palette to the left and when the module appears, drag it to the canvas. Set the <span class="strong"><strong>Resampling method</strong></span> property to <span class="strong"><strong>Bagging</strong></span> and leave the rest of the properties at their default values. Leave the module with the default values for the properties.</p><p>Drag a <span class="strong"><strong>Train Model</strong></span> module to the canvas and connect the output port of the <span class="strong"><strong>Multiclass Decision Forest</strong></span> module to the first input port of the <span class="strong"><strong>Train Model</strong></span> module. Connect the first output port of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Train Medel</strong></span> module. In the properties pane for the <span class="strong"><strong>Train Model</strong></span> module, click on the column selector and set <span class="strong"><strong>Col5</strong></span> as the column's target variable.</p><p>Next, drag the <span class="strong"><strong>Score Model</strong></span> and <span class="strong"><strong>Evaluate Model</strong></span> modules to the canvas. Connect the output of the <span class="strong"><strong>Train Model</strong></span> module to the first input of the <span class="strong"><strong>Score Model</strong></span> module and connect the second output of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Score Model</strong></span> module. Then, connect the output of the <span class="strong"><strong>Score Model</strong></span> module to the first input of the <span class="strong"><strong>Evaluate </strong></span><a id="id300" class="indexterm"/>
<span class="strong"><strong>Model</strong></span> module. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_04.jpg" alt="Multiclass decision forest"/></div><p>Now, run the experiment and after its completion, visualize the output of the <span class="strong"><strong>Evaluate Model</strong></span> module to know the performance of the model.</p><div class="mediaobject"><img src="graphics/0792EN_07_05.jpg" alt="Multiclass decision forest"/></div><p>As you can <a id="id301" class="indexterm"/>see in the preceding graph, the <span class="strong"><strong>Iris Versicolour</strong></span> class has <span class="strong"><strong>92.3%</strong></span> accuracy, while others have <span class="strong"><strong>100%</strong></span>. Also, <span class="strong"><strong>7.7%</strong></span> of the time the <span class="strong"><strong>Iris Versicolour</strong></span> class has been misclassified as <span class="strong"><strong>Iris Virginica</strong></span>.</p><p>Note that you have not done any optimization by tuning the parameters. You can try out different values for the parameters and evaluate the performance or simply use the <span class="strong"><strong>Sweep Parameters</strong></span> module to get the best parameters.</p></div><div class="section" title="Comparing models – multiclass decision forest and logistic regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec51"/>Comparing models – multiclass decision forest and logistic regression</h2></div></div></div><p>As with any <a id="id302" class="indexterm"/>experiment, you can now compare your model with another algorithm. You built a model using multiclass decision forest. Now, use another algorithm, such as multiclass logistic regression to evaluate the prediction.</p><p>To do so, start with the experiment and select the <span class="strong"><strong>Train</strong></span> and <span class="strong"><strong>Score</strong></span> modules by pressing <span class="emphasis"><em>ctrl</em></span> on your keyboard and click on both the modules. Then, copy the selected modules and paste them on the canvas by right-clicking on it and pasting them or just by pressing <span class="emphasis"><em>ctrl</em></span> + <span class="emphasis"><em>v</em></span> on your keyboard. Now, click anywhere on the canvas to unselect the pasted modules and rearrange them so that no module is placed on another and all are readable. Let's <a id="id303" class="indexterm"/>take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_06.jpg" alt="Comparing models – multiclass decision forest and logistic regression"/></div><p>Now, remove the connection between the <span class="strong"><strong>Multiclass Decision Forest</strong></span> and <span class="strong"><strong>Train Model</strong></span> modules by selecting the connection and pressing <span class="emphasis"><em>Delete</em></span>. Note the connection in the preceding screenshot. Drag the <span class="strong"><strong>Multiclass Logistic Regression</strong></span> module from the left-hand side palette to the canvas and connect the output of the module to the <span class="strong"><strong>Train Model</strong></span> module. Leave the properties of the <span class="strong"><strong>Multiclass Logistic Regression</strong></span> module at their default values. Connect the output of the <span class="strong"><strong>Score Model</strong></span> module to the second input of the <span class="strong"><strong>Evaluate Model</strong></span> module. Let's take a look at the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_07.jpg" alt="Comparing models – multiclass decision forest and logistic regression"/></div><p>You can run the model to find out how the new model is performing and then you can compare the <a id="id304" class="indexterm"/>evaluation metrics. After the experiment finishes running, visualize the output of the <span class="strong"><strong>Evaluate Model</strong></span> module to know the performance of the model.</p><div class="mediaobject"><img src="graphics/0792EN_07_08.jpg" alt="Comparing models – multiclass decision forest and logistic regression"/></div><p>As you can <a id="id305" class="indexterm"/>note, for the model with the logistic regression, you are getting <span class="strong"><strong>100%</strong></span> accuracy for all the classes. Given such a scenario, you know which model to pick up.</p></div></div>
<div class="section" title="Multiclass classification with the Wine dataset"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec48"/>Multiclass classification with the Wine dataset</h1></div></div></div><p>The <span class="strong"><strong>Wine</strong></span> dataset <a id="id306" class="indexterm"/>is another classic and simple dataset hosted in the UCI machine learning repository. It contains chemical analysis of the <a id="id307" class="indexterm"/>content of wines grown <a id="id308" class="indexterm"/>in the same region in Italy, but derived from three different cultivars. It is used to determine models for classification problems by predicting the source (cultivar) of wine as class or target variable. The dataset has the following 13 features (dependent variables), which are all numeric:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Alcohol</li><li class="listitem" style="list-style-type: disc">Malic acid</li><li class="listitem" style="list-style-type: disc">Ash</li><li class="listitem" style="list-style-type: disc">Alcalinity of ash</li><li class="listitem" style="list-style-type: disc">Magnesium</li><li class="listitem" style="list-style-type: disc">Total phenols</li><li class="listitem" style="list-style-type: disc">Flavanoids</li><li class="listitem" style="list-style-type: disc">Nonflavanoid phenols</li><li class="listitem" style="list-style-type: disc">Proanthocyanins</li><li class="listitem" style="list-style-type: disc">Color intensity</li><li class="listitem" style="list-style-type: disc">Hue</li><li class="listitem" style="list-style-type: disc">OD280/OD315 of diluted wines</li><li class="listitem" style="list-style-type: disc">Proline</li></ul></div><p>The examples or <a id="id309" class="indexterm"/>instances are classified into <a id="id310" class="indexterm"/>three classes: 1, 2 and 3.</p><p>You can <a id="id311" class="indexterm"/>find <a id="id312" class="indexterm"/>more about the dataset at <a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Wine">http://archive.ics.uci.edu/ml/datasets/Wine</a>.</p><div class="section" title="Multiclass neural network with parameter sweep"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec52"/>Multiclass neural network with parameter sweep</h2></div></div></div><p>We will <a id="id313" class="indexterm"/>build a model with multiclass neural network and optimize the parameters <a id="id314" class="indexterm"/>with the <span class="strong"><strong>Sweep Parameter</strong></span> module.</p><p>As you did the last time, use the <a id="id315" class="indexterm"/>
<span class="strong"><strong>Reader</strong></span> module to import the dataset from <a class="ulink" href="http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data">http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data</a>.</p><p>It is in the CSV format and has no header row, as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/0792EN_07_10.jpg" alt="Multiclass neural network with parameter sweep"/></div><p>Use the <span class="strong"><strong>Split</strong></span> module and split it into the ratio of 70:30 for a train and test dataset, respectively.</p><p>Type <code class="literal">Sweep</code> in the search box in the modules palette to the left and when the <span class="strong"><strong>Sweep Parameters</strong></span> module appears, drag it to the canvas. Then, join the first output of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Sweep Parameters</strong></span> module and join the second output of the <span class="strong"><strong>Split</strong></span> module to the third input of the <span class="strong"><strong>Sweep Parameters</strong></span> module.</p><p>Now, you need to set the column of the dataset that is your target, label, or class column for which you will train a model to make a prediction. In this case, <span class="strong"><strong>Col1</strong></span> is the target variable or class for which you are going to make a prediction. Also, set the sweeping mode to <span class="strong"><strong>Entire grid</strong></span> for metric to measure the performance of the <span class="strong"><strong>Classification to Accuracy</strong></span> option.</p><p>Also, get the <a id="id316" class="indexterm"/>
<span class="strong"><strong>Multiclass Neural Network</strong></span> module and connect it to the first input <a id="id317" class="indexterm"/>of the <span class="strong"><strong>Sweep Parameters</strong></span> module. As usual, drag the <span class="strong"><strong>Score Model</strong></span> and <span class="strong"><strong>Evaluate Model</strong></span> modules to the canvas. Connect the second output port of the <span class="strong"><strong>Sweep Parameters</strong></span> module to the first input port of the <span class="strong"><strong>Score Model</strong></span> module and connect the second output of the <span class="strong"><strong>Split</strong></span> module to the second input of the <span class="strong"><strong>Score Model</strong></span> module. Then, connect the output of the <span class="strong"><strong>Score Model</strong></span> module to the first input of the <span class="strong"><strong>Evaluate Model</strong></span> module.</p><p>Run the experiment.</p><div class="mediaobject"><img src="graphics/0792EN_07_09.jpg" alt="Multiclass neural network with parameter sweep"/></div><p>As the <a id="id318" class="indexterm"/>experiment finishes running, visualize the output of the <span class="strong"><strong>Evaluate Model</strong></span> module <a id="id319" class="indexterm"/>to know the performance of the model.</p></div><div class="section" title="Do it yourself – multiclass decision jungle"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec53"/>Do it yourself – multiclass decision jungle</h2></div></div></div><p>Use the same Wine dataset and build a model using the <span class="strong"><strong>Multiclass Decision Jungle</strong></span> module. You can use the Sweep Parameters module to optimize the parameters of the algorithms. After you run the experiment, check out the evaluation metrics. Do you find any improvement in the performance than the previous model you built with neural network or any other available algorithms?</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec49"/>Summary</h1></div></div></div><p>You started the chapter with understanding predictive analysis with classification and explored the concepts of training, testing, and validating a classification model. You then proceeded to carry on building experiments with different two-class and multiclass classification models, such as logistic regression, decision forest, neural network, and boosted decision trees inside ML Studio. You learned how to score and evaluate a model after training. You also learned how to optimize different parameters for a learning algorithm by the module, Sweep Parameters.</p><p>After exploring the two-class classification, you understood multiclass classification and learnt how to evaluate a model for the same. You then built a couple of models for multiclass classification using different available algorithms.</p><p>In the next chapter, you will explore the process of building a model using clustering, an unsupervised learning algorithm.</p></div></body></html>