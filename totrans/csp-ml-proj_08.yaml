- en: Handwritten Digit Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have looked at how to build recommendation models using multi-class classification
    models. In this chapter, we are going to expand our knowledge and experience of
    building multi-class classification models with an image dataset. Image recognition
    is a well-known **machine learning** (**ML**) problem and is one of the topics
    that are actively being researched. One image recognition problem that has high
    applicability to our lives is recognizing handwritten letters and digits. A good
    example of the application of a handwritten image recognition system is the address
    recognition system that is used at post offices. Using such a technology, post
    offices can now automatically and more quickly identify addresses that are written
    by hand, and expedite and improve overall mailing services.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to build machine learning models for handwritten
    digit recognition. We are going to start with a dataset that contains grayscale
    pixel-by-pixel information about over 40,000 handwritten digit images. We will
    look at the distributions of the values in each pixel and discuss how sparse this
    grayscale image dataset is. Then, we are going to discuss when and how to apply
    dimensionality reduction techniques, more specifically **Principal Component Analysis**
    (**PCA**), and how we can benefit from this technique for our image recognition
    project. We will be exploring different learning algorithms, such as logistic
    regression and Naive Bayes, and will also cover how to build an **Artificial Neural
    Network** (**ANN**), which forms the backbone of deep learning technologies, using
    the Accord.NET framework. Then, we will compare the prediction performances of
    these ML models by looking at various evaluation metrics, and discuss which model
    performed the best for the handwritten digit recognition project.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Problem definition for the handwritten digit recognition project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis for an image dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering and dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML models for handwritten digit recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating multi-class classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image recognition technology can be applied to, and can be found easily in,
    our daily lives. At post offices, image recognition systems are used to programmatically
    understand addresses that are written by hand. Social network services, such as
    Facebook, use image recognition technology for automatic people tag suggestions,
    for instance, when you want to tag people in your photos. Also, as briefly mentioned
    in the very first chapter of this book, Microsoft's Kinect uses image recognition
    technology for its motion-sensing games. Of these real-life applications, we are
    going to experiment with building a handwritten digit recognition system. As you
    can imagine, such digit image recognition models and systems can be used for automated
    handwritten address recognition at post offices. Before we had this ability to
    teach machines to identify and understand handwritten digits, people had to go
    through and look at each letter to find out the destination and the origin of
    individual letters. However, now that we can train machines to understand handwritten
    addresses, mailing processes have become much easier and faster.
  prefs: []
  type: TYPE_NORMAL
- en: In order to build a handwritten digit recognition model, we are going to use
    the **MNIST** dataset, which has over 60,000 handwritten digit images. The **MNIST**
    dataset contains 28 x 28 images that are in grayscale. You can find more information
    at this link: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
    For this project, we will be using a cleaned and processed MNIST dataset that
    can be found at this link: [https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data).
    With this data, we will first look at how the digits are distributed across the
    dataset, and how sparse the feature set is. Then, we are going to use PCA for
    dimensionality reduction and to visualize the differences in the distributions
    of features among different classes. With this PCA-transformed data, we are going
    to train a few ML models to compare their prediction performances. On top of logistic
    regression and Naive Bayes classification algorithms, we are going to experiment
    with the ANN, as it is known to work well for image datasets. We will look at
    accuracy, precision versus recall, and **area under the curve** (**AUC**), to
    compare the prediction performances among different machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize our problem definition for the handwritten digit recognition project:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the problem? We need a handwritten digit recognition model that can
    classify each handwritten image into a corresponding digit class, so that it can
    be used for applications such as the address recognition system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is it a problem? Without such a model, it takes an enormous amount of human
    labor to identify and organize letters by addresses. If we have a technology that
    can recognize handwritten digits that are written on letters, it can significantly
    reduce the amount of human labor required for the same task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are some of the approaches to solving this problem? We are going to use
    publicly available data that contains numerous examples of handwritten digit images.
    With this data, we are going to build machine learning models that can classify
    each image into one of 10 digits.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the success criteria? We want a machine learning model that accurately
    classifies each image with the corresponding digit. Since this model will eventually
    be used for address recognition, we want high precision rates, even if we have
    to sacrifice recall rates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis for the image dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start looking into this image dataset. As mentioned in the previous section,
    we will be using the data from this link: [https://www.kaggle.com/c/digit-recognizer/data](https://www.kaggle.com/c/digit-recognizer/data).
    You can download the `train.csv` data from the link and store it in a place from
    where you can load it into your C# environment.
  prefs: []
  type: TYPE_NORMAL
- en: Target variable distribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing we are going to look at is the distribution of the target variables.
    Our target variable is encoded in the `label` column, which can take values between
    0 and 9, and represents the digit that the image belongs to. The following code
    snippet shows how we aggregated the data by the target variable and counted the
    number of examples for each digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As in other chapters, we used the `AggregateRowsBy` method in Deedle''s data
    frame to aggregate data by the target variable, `label`, count the number of records
    in each label, and sort by the counts. Similar to previous chapters, we are using
    the `DataBarBox` class to display a bar plot of target variable distributions
    in the dataset. The following is the bar plot that you will see when you run this
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00119.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the console output, you will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00120.gif)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from the bar plot and this console output, the digit `1`, occurs
    the most in the dataset, and the digit `5`, occurs the least. However, there is
    no one class that takes the majority of the examples in the dataset, and the target
    variables are pretty well balanced and spread across different classes.
  prefs: []
  type: TYPE_NORMAL
- en: Handwritten digit images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start looking into the feature set, let''s first look at actual images
    of handwritten digits. In each record of our dataset, we have the grayscale values
    for 784 pixels for each of the 28 x 28 images. In order to build an image from
    this flattened dataset, we need to first convert each array of 784-pixel values
    into a two-dimensional array. The following code shows the helper function we
    wrote to create an image from a flattened array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from this code, it first initializes a two-dimensional integer
    array, `pixelData`, which is going to store the pixel data. Since we know each
    image is a 28 x 28 image, we are going to take the first 28 pixels in the flattened
    data as the first row in the two-dimensional integer array, the second set of
    28 pixels as the second row, and so forth. Inside the `for` loop, we are converting
    the value of each pixel into a **Blue-Green-Red-Alpha** (**BGRA**) byte array,
    named `bgra`. As we know the images are in grayscale, we can use the same value
    for blue, green, and red components. Once we have converted the flattened pixel
    data into a 28 x 28 two-dimensional integer array, we can now build images of
    the handwritten digit images. We are using the `Bitmap` class to reconstruct these
    handwritten digit images. The following code shows how we used this helper function
    to build images for each digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you will see the following images being stored on your
    local drive:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00121.gif)'
  prefs: []
  type: TYPE_IMG
- en: You can use the same code to generate more images, which will help you better
    understand what raw images of handwritten digits look like.
  prefs: []
  type: TYPE_NORMAL
- en: Image features - pixels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's now look at image features. In our dataset, we have integer values for
    each pixel in each image that represent a grayscale value. It will be helpful
    to understand the ranges of values each pixel can take, and whether we can find
    any noticeable differences in the distributions of that pixel data among different
    handwritten digit classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first take a look at individual distributions of pixel data. The following
    code snippet shows how you can calculate the quartiles for each pixel in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the case previous chapters, we used the `Quantiles` method in `Accord.Statistics.Measures`
    to get the quartiles for each pixel. As you might recall from previous chapters,
    quartiles are the values that separate the data into four sections. In other words,
    the first quartile (`Q1`) represents the 25% percentile that is the middle point
    between the minimum value and the median value. The second quartile (`Q2`) represents
    the median value, and the third quartile (`Q3`) represents the 75% percentile
    that is the middle point between the median and the maximum. In this code example,
    we are only computing quartiles for the first 20 pixels that have values other
    than 0, as you can see in lines 4-7, and in line 11\. When you run this code,
    you will get an output that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00122.gif)'
  prefs: []
  type: TYPE_IMG
- en: Here, we are only showing the first five distributions. As you can see from
    this output, the majority of the pixel values are 0\. If you look at the images
    that we reconstructed in the previous section, the majority of the pixels in the
    image are black and only a subset of the pixels are used to show digits. These
    pixels in black are encoded as `0` in our pixel data, and thus it is expected
    that many pixels have 0 values for the corresponding image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build some scatter plots, so that we can understand this data better
    visually. The following code builds scatter plots of distributions of the first
    20 non-zero pixel features for each handwritten digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If you look closely at this code, we first build a `featureColumns` string array
    from the `featureCols`, `List` object. The `List` object, `featureCols`, is a
    list of the first 20 pixels that have values other than 0, and this was built
    from the previous code when we were computing quartiles. We are using the same
    helper function, `BuildXYPairs`, that we used in the previous chapter to transform
    the data frame into an array of x-y pairs, where the `x` values are the indexes
    of each pixel and the `y` values are the actual pixel value. Using this helper
    function, we use the `ScatterplotBox` class to display a scatter plot that shows
    the pixel distribution for each of the 20 sample pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a scatter plot for the 0 digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The majority of the first 20 pixels have 0 values for all the images in the
    0 digit class. Of those 20 pixels that we show in this scatter plot, there are
    only three pixels that have values other than 0\. Let's look at the distributions
    of these pixels for a different digit class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following scatter plot is for the 1 digit class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00124.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Similar to the case of the 0 digit class, of those 20 pixels that we show here,
    the majority have 0 values and only three pixels have values other than 0\. Compared
    to the previous scatter plot for of the 0 digit class, the distributions of the
    pixel data are slightly different for the 1 digit class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is for the 2 digit class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00125.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This scatter plot shows quite different distributions for the 20 pixels that
    we show here. The majority of those 20 pixels have values ranging between 0 and
    255, and only a few have 0 values for all the images. This kind of difference
    in the distributions of the feature set will help our ML models learn how to correctly
    classify handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we are going to look at one more scatter plot, where we will see how
    the target variables are distributed across two different pixels. We used the
    following code to generate a sample two-dimensional scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For illustration purposes, we chose the fifteenth and sixteenth indexed features,
    which turn out to be `pixel43` and `pixel44`. When you run this code, you will
    see the following scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00126.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see some distinctions among different classes, but since the majority
    of the pixel values for both `pixel43` and `pixel44` are 0, it is quite difficult
    to draw a clear distinction among different target classes by looking at this
    scatter plot. In the next section, we are going to look at how to use PCA and
    its principal components to create another version of this scatter plot that can
    help us identify a clearer distinction among different target classes when we
    visualize the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code for this data analysis step can be found at this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/DataAnalyzer.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/DataAnalyzer.cs).'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering and dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have looked at the distributions of the target variables and pixel
    data. In this section, we are going to start discussing building train and test
    sets for our ML modeling step, and then how we can use PCAfor dimensionality reduction
    and to visualize data using the principal components.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the sample set into train versus test sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first task we are going to do in this step is to randomly split our dataset
    into train and test sets. Let''s first look at the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the preceding code, we are taking roughly about 70% of our
    data for training, and the rest for testing. Here, we are using the `Random` class
    to generate random numbers to split the sample set into train and test sets using
    the indexes of the records. Once we have built train and test sets, we are removing
    columns or pixels that have 0 values for all the images (line 12). This is because
    if a feature doesn't vary among different target classes, it doesn't have any
    information about those target classes for ML models to learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have train and test sets, let''s check on the distributions of
    target classes in both train and test sets. The following code can be used for
    the aggregation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'When you run this code, you will see the following plot for the target variable
    distribution in the train set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And, the following is what we see for the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00128.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: These distributions look similar to what we saw in the data analysis step, when
    we analyzed the target variable distribution in the overall dataset. Let's now
    start discussing how we can apply PCA to our train set.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction by PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw that many of our feature or pixel values are 0, when we were analyzing
    our data. In such cases, applying PCA can be helpful for reducing the dimensions
    of the data, while minimizing the loss of information from the reduced dimensions.
    Simply put, PCA is used to explain a dataset and its structure through linear
    combinations of the original features. So, each principal component is a linear
    combination of the features. Let's start looking at how we can run PCA in C#,
    using the Accord.NET framework.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how you can initialize and train a PCA:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Once a `PrincipalComponentAnalysis` is trained with the data, it contains all
    the information about the linear combinations for each principal component and
    can be applied to transform other data. We used `PrincipalComponentMethod.Standardize`
    to standardize our data before applying PCA. This is because PCA is sensitive
    to the scale of each feature. So, we want to standardize our dataset before applying
    PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to PCA-transform other data, you can use the `Transform` method, as
    shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have learned how we can apply PCA to our dataset, let''s look at
    the first two principal components and see if we can find any noticeable patterns
    in the target variable distributions. The following code shows how we can build
    a scatter plot of the first two components with target classes color-coded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you run this code, you will see the following scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00129.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: When you compare this chart with the one between `pixel43` and `pixel44` that
    we looked at during the data analysis step, this looks quite different. From this
    scatter plot of the first two principal components, we can see that the target
    classes are more discernible. Although it is not perfectly separable from these
    two components, we can see that if we combine more components into our analysis
    and modeling, it will get easier to separate one target class from another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important aspect of PCA that we should look at is the amount of variance
    explained by each principal component. Let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can retrieve the cumulative proportion of the variance in our data explained
    by each PCA component by using the `CumulativeProportion` property. In order to
    get the individual proportion explained by each PCA component, you can use the
    `Proportion` property of each PCA component. Then, we will use the `DataSeriesBox`
    class to plot a line chart to display the cumulative proportions of the variance
    explained by each component.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run this code, it will produce the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00130.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from this plot, about 90% of the variance in the dataset can
    be explained by the first 200 components. With 600 components, we can explain
    almost 100% of the variance in our dataset. Compared to the total of 784 pixels
    we had as our features in the raw dataset, this is a big reduction in the dimension
    of our data. Depending on how much variance you want to capture for your ML models,
    you can use this chart to decide the number of components that is most suitable
    for your modeling process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to export the train and test sets, so that we can use them
    for the following model building step. You can use the following code to export
    the PCA-transformed train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The full code for this feature engineering and dimensionality reduction step
    can be found at this link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/FeatureEngineering.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/FeatureEngineering.cs).
  prefs: []
  type: TYPE_NORMAL
- en: ML models for handwritten digit recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have everything ready for building ML models, let's start building
    those models. In this section, we will cover how to sub-select the features based
    on the PCA results and then discuss how we can build logistic regression and Naive
    Bayes classifiers for the handwritten digit recognition model. We are going to
    introduce a new learning model, the neural network, and explain how to build one
    for this project, using the Accord.NET framework.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first step in building a ML model for handwritten digit recognition is
    to load the data that we built from the previous section. You can use the following
    code to load the train and test sets that we created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For our experimentation with different models in this chapter, we will be using
    the principal components that cumulatively explain about 70% of the variance in
    our dataset. Take a look at the following code to see how we filtered for the
    components of our interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the first line of this code, we are taking the first 91 components
    (up to the ninetieth index) as the features for our models. If you recall from
    the previous step or look at the plot for the cumulative variance proportion explained
    by the components, you will see that the first 91 components capture about 70%
    of the variance in our dataset. Then, we create a two-dimensional array of doubles
    that we will use for training and testing our ML models. The following code shows
    the helper function, `BuildJaggedArray`, that we wrote to convert a data frame
    into a two-dimensional array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Logistic regression classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first learning algorithm we are going to experiment with for handwritten
    digit recognition is logistic regression. We wrote a method, named `BuildLogitModel`,
    which takes in the inputs and outputs to the model, trains a logistic regression
    classifier, and then evaluates the performance. The following code shows how this
    method is written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Similar to the previous chapter, we are using the `MultinomialLogisticLearning`
    class to train a logistic regression classifier. Once this model is trained, we
    start evaluating by various evaluation metrics, which we will discuss in more
    detail in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The second model we are going to experiment with is a Naive Bayes classifier.
    Similar to the previous case involving the logistic regression classifier, we
    wrote a helper function, `BuildNBModel`, that takes in the inputs and outputs,
    trains a Naive Bayes classifier, and then evaluates the trained model. The code
    looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you might recall from the previous chapter, we are using the `NaiveBayesLearning`
    class to train a Naive Bayes classifier. We are using `NormalDistribution`, as
    all the features for our ML models are the principal components from the previous
    PCA step, and the values of these components are continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: Neural network classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last learning algorithm that we are going to experiment with is the ANN.
    As you might know already, the neural network model is the backbone of all of
    the deep learning technologies. The neural network model is known to perform well
    for image datasets, so we will compare the performance of this model against the
    other models to see how much performance gain we get by using the neural network
    over the other classification models. In order to build neural network models
    in C# using the Accord.NET framework, you will need to install the `Accord.Neuro`
    package first. You can install the `Accord.Neuro` package by using the following
    command in the **NuGet Package Manager Console**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now take a look at how we can build a neural network model in C#, using
    the Accord.NET framework. The code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a closer look at this code. We first transform the training labels
    from a one-dimensional array into a two-dimensional array, where the columns are
    the target classes and the values are 1 if the given record belongs to the given
    target class, and 0 if it does not. We are using the `Accord.Math.Jagged.OneHot`
    method to perform one-hot encoding for the training labels. Then, we build a neural
    network by using the `ActivationNetwork` class. The `ActivationNetwork` class
    takes three parameters: the activation function, the input count, and the information
    about the layers. For the activation function, we are using a sigmoid function, `BipolarSigmoidFunction`.
    The input count is straightforward, as it is the number of features that we are
    going to use to train this model, which is 91\. For this model, we only used one
    hidden layer with 20 neurons. For a deeper neural network, you can use more than
    one hidden layer and can also experiment with different numbers of neurons in
    each hidden layer. Lastly, the last parameter of the `ActivationNetwork` constructor
    represents the output count. Since the target variable is the digit class, it
    can take values between 0 and 9, and thus the number of output neurons we need
    is 10. Once this network is built, we can use the `LevenbergMarquardtLearning`
    learning algorithm to train the network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have set up the network and the learning algorithm, we can actually
    start training a neural network model. As you might know already, a neural network
    model needs to be run through the dataset multiple times (epochs) during its learning
    phase for better predictability. You can use the `RunEpoch` method to train and
    update the neural network model in each epoch. To save some time, we are only
    running 10 epochs to train our neural network model. However, we recommend you
    try increasing this value, as it can improve the performance of your neural network
    model. The following shows how the error measure decreases as we train and update
    the neural network model in each epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00131.gif)'
  prefs: []
  type: TYPE_IMG
- en: As you can see from this output, the error measure decreases significantly in
    each epoch. One thing to note here is that the amount of reduction in the error
    measure decreases in each additional epoch. When you are building a neural network
    model with large numbers of epochs, you can monitor the amount of gain in each
    run and decide to stop when there is no more significant performance gain.
  prefs: []
  type: TYPE_NORMAL
- en: The full code that we used for the model building step can be found at this
    link: [https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/Modeling.cs](https://github.com/yoonhwang/c-sharp-machine-learning/blob/master/ch.8/Modeling.cs).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating multi-class classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to evaluate the three models that we built in
    the previous section. We are going to revisit the validation metrics that we used
    previously for the classification models, and compare the performances of each
    model against the others.
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s look at confusion matrices. The following code shows how you
    can build a confusion matrix with the predicted output and the actual output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This method is similar to the one we wrote in the previous chapter, except that
    it is returning a two-dimensional array, instead of a string array. We are going
    to use this two-dimensional array output for calculating precision and recall
    rates in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix for the logistic regression classifier looks like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00132.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'For the Naive Bayes classifier, you will get a confusion matrix that looks
    similar to the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00133.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, for the neural network model, the confusion matrix looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00134.gif)'
  prefs: []
  type: TYPE_IMG
- en: From these confusion matrices, the neural network model outperforms the other
    two models, and the logistic regression model seems to come in second.
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and precision/recall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The second metric that we are going to look at is the accuracy measure. We
    are use `ZeroOneLoss` to compute the loss, and then subtract it from `1` to get
    the accuracy number. The code to compute the accuracy measure is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The third and fourth metrics that we are going to look at are the precision
    and recall rates. Unlike before, we have 10 classes for the target prediction.
    So, we are going to have to calculate precision and recall rates separately for
    each of the target classes. The code looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this code, the input to this `PrintPrecisionRecall` method
    is the confusion matrix that we built from the previous section. In this method,
    it iterates through each of the target classes and computes the precision and
    recall rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output that we get when we compute accuracy, precision,
    and recall for the logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00135.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'For the Naive Bayes model, we get the following results for the metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00136.gif)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, for the neural network model, the performance results look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00137.gif)'
  prefs: []
  type: TYPE_IMG
- en: As you might notice from these results, the neural network model outperformed
    the other two models. Both the overall accuracy and the precision/recall rates
    are the highest for the neural network model, when compared to the logistic regression
    and Naive Bayes models. The logistic regression model seems to come in as the
    second best model among the three that we built.
  prefs: []
  type: TYPE_NORMAL
- en: One versus Rest AUC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last evaluation measure that we are going to look at is the **Receiver
    Operating Characteristic** (**ROC**) curve and the AUC. One thing we need to do
    differently in this chapter, when we are building a ROC curve and an AUC, is that
    we need to build one for each of the target classes. Let''s take a look at the
    code first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from this `DrawROCCurve` method that we wrote, we iterate through
    each target class in a `for` loop, and reformat the predicted and actual labels
    by encoding `1` if each label matches with the target class, and 0 if it does
    not. After we have done this encoding, we can then use the `ReceiverOperatingCharacteristic`
    class to compute the AUC and build the ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the ROC curve for the logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00138.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'For the Naive Bayes model, the ROC curve looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00139.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Lastly, the ROC curve for the neural network model looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00140.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As expected from the previous metrics that we have looked at, the results look
    the best for the neural network model, and the logistic regression model comes
    in as second-best. For the Naive Bayes model, there are some digits that it didn't
    compute well. For example, the Naive Bayes model struggles to classify the digits
    6 and 7 well. However, the AUC numbers for all of the target classes are close
    to 1 for the neural network, which suggests that the model is trained well to
    identify digits for handwritten images.
  prefs: []
  type: TYPE_NORMAL
- en: From looking at the confusion matrix, the accuracy, precision and recall rates,
    and the ROC curves, we can conclude that the neural network model works the best
    among the three classifiers that we trained in this chapter. This reaffirms the
    fact that neural networks work well on image datasets and image recognition problems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we built our first image recognition model that can identify
    handwritten digits in grayscale images. We started this chapter by discussing
    how this type of model can be widely used in real-life applications, and how we
    are planning to build a handwritten digit recognition model. Then, we started
    looking into the dataset. We first looked at the distributions of target classes
    to see if the sample set is a well-balanced set. When we were analyzing the pixel
    data, we noticed that the majority of the pixel values were 0, and we could intuitively
    make sense of it by reconstructing the images from the pixel data. During the
    feature engineering step, we discussed how we can use PCA for dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: With these PCA-transformed features, we then started building various machine
    learning models. On top of the logistic regression and Naive Bayes models that
    we are already familiar with, we introduced a new ML model, neural network. We
    learned how to initialize the `ActivationNetwork` model with `BipolarSigmoidFunction`
    as an activation function. We then started training the neural network with the
    `LevenbergMarquardtLearning` learning algorithm over 10 epochs. We saw how error
    measures decrease in each additional epoch, and discussed how the amount of gain
    in the error rate is in diminishing returns for additional epochs. In the model
    evaluation step, we combined multiple validation metrics for classification models.
    For the machine learning models we built in this chapter, we looked at the confusion
    matrix, prediction accuracy, precision and recall rates, and the ROC curves and
    AUC. We noticed how the neural network model outperformed the other two models,
    which reaffirmed that neural network models work well for image data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to switch gears and start building models
    for anomaly detection. We are going to work on a cyber attack detection project
    using PCA. With the Network Intrusion dataset, we will discuss how to use PCA
    to detect cyber attacks, and run multiple experiments to find the optimal threshold
    at which to notify us about potential cyber attacks.
  prefs: []
  type: TYPE_NORMAL
