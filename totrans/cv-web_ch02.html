<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" style="font-size:1.200rem;">
<head><title>Chapter&#160;2.&#160;Turn Your Browser into Photoshop</title>
<link rel="stylesheet" href="../Styles/style0001.css" type="text/css"/>
<meta name="generator" content="DocBook XSL Stylesheets V1.75.2"/>
</head>
<body id="page">
<div class="chapter" title="Chapter&#160;2.&#160;Turn Your Browser into Photoshop" id="aid-I3QM1"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"></a>Chapter&#160;2.&#160;Turn Your Browser into Photoshop</h1>
</div>
</div>
</div>
<p>It is likely that you have used Photoshop or at least heard about it. With a few clicks, you can easily modify an image, enhance it, or do some sort of preprocessing. Actually, it is not that hard to do using JavaScript. For most of the functions, you need only a couple lines of code. This chapter is mostly about filters and image segmentation. Here, we will discuss many popular techniques and their applications. Moreover, we will introduce a new JavaScript library—tracking.js (<a class="ulink" href="http://trackingjs.com">http://trackingjs.com</a>). It is mostly used for object tracking applications, but there are many utilties, which are relevant to the topic. It is interesting to know how to use both JSFeat, which we introduced in the first chapter, and tracking.js libraries together. We will see how to do this. Besides, we will compare their advantages in terms of image filtering. We will start from the installation of the new library and then follow the filter examples from the easiest to the most exciting ones.</p>
<p>We will cover the following topics in this chapter:</p>
<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Introducing the tracking.js library</li>
<li class="listitem">What is filtering and how to use it</li>
<li class="listitem">Basic edge detection</li>
<li class="listitem">Advanced image processing</li>
</ul>
</div>
<div class="section" title="Introducing the tracking.js library"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"></a>Introducing the tracking.js library</h1>
</div>
</div>
</div>
<p>Let me give you a quick review of the tracking.js library. It is a great library that helps you with object<a id="id41" class="indexterm"></a> detection, tracking, and image filtering. You can download it from <a class="ulink" href="http://trackingjs.com">http://trackingjs.com</a>. In this section, we will focus on the the installation of the<a id="id42" class="indexterm"></a> library and how both JSFeat and tracking.js libraries can be used together.</p>
<div class="section" title="Installation and image loading"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec11"></a>Installation and image loading</h2>
</div>
</div>
</div>
<p>Actually, the<a id="id43" class="indexterm"></a> installation of a JavaScript library is straightforward. You just need to add a script file to your <code class="literal">&lt;head&gt;</code> tag:</p>
<div class="informalexample"><pre class="programlisting">&lt;script src="js/tracking.js"&gt;&lt;/script&gt;</pre>
</div>
<p>The image loading<a id="id44" class="indexterm"></a> is done using the context, just like we did in the previous chapter:</p>
<div class="informalexample"><pre class="programlisting">var imageData = context.getImageData(0, 0, cols, rows);</pre>
</div>
<p>In contrast to the JSFeat library, tracking.js works with arrays and it does not create a new object for images (as you remember, it is the <code class="literal">matrix_t</code> function for JSFeat). In that case, how do we apply a simple operation? Here is an example of how to convert a colored image to grayscale:</p>
<div class="informalexample"><pre class="programlisting">var gray = tracking.Image.grayscale(imageData.data, cols, rows, true);</pre>
</div>
<p>The last parameter indicates whether you need to return the array in the RGBA format (<code class="literal">true</code>) or just in one channel grayscale (<code class="literal">false</code>). In that case, we receive a <code class="literal">Uint8ClampedArray</code>, which we can easily convert to the <code class="literal">ImageData</code> constructor and put it to the canvas context:</p>
<div class="informalexample"><pre class="programlisting">context.putImageData(new ImageData(gray, cols, rows), 0, 0);</pre>
</div>
<p>Simple, isn't it? The only thing we should mention is that for most operations, tracking.js returns <code class="literal">Float32Array</code>. Generally, you can cast it to the unsigned byte array without losing any information.</p>
</div>
<div class="section" title="Conversion between JSFeat and tracking.js image formats"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec12"></a>Conversion between JSFeat and tracking.js image formats</h2>
</div>
</div>
</div>
<p>In some cases JSFeat and tracking.js libraries complement each other. To benefit from using <a id="id45" class="indexterm"></a>them together, you will probably<a id="id46" class="indexterm"></a> need to convert a JSFeat matrix to an array and vice versa. The critical difference is that, even for a grayscale data, tracking.js sometimes uses four array elements: R, G, B, A.</p>
<p>To use a matrix as an array, we just need to get <code class="literal">mat.data</code> from a matrix. In the following code, we load matrix from the <code class="literal">ImageData</code> constructor and pass <code class="literal">mat.data</code> to the tracking.js grayscale function:</p>
<div class="informalexample"><pre class="programlisting">var dataBuffer = new jsfeat.data_t(cols * rows, imageData.data.buffer);
var mat = new jsfeat.matrix_t(cols, rows, jsfeat.U8C4_t, dataBuffer);
var gray = tracking.Image.grayscale(mat.data, cols, rows, true);</pre>
</div>
<p>Since the <code class="literal">mat</code> variable consists of four channels, we do not need to convert it to a different format. But what if we want to use the gray variable as a matrix?</p>
<div class="informalexample"><pre class="programlisting">var buf = new Array(gray.length / 4);
for (var i = 0, j = 0; i &lt; gray.length; i += 4, ++j) {
    buf[j] = gray[i];
}
var matGray = new jsfeat.matrix_t(cols, rows, jsfeat.U8C1_t,
        new jsfeat.data_t(cols * rows, buf));</pre>
</div>
<p>In that <a id="id47" class="indexterm"></a>case, we skip G, B, A elements <a id="id48" class="indexterm"></a>and add only R elements to the buffer. With that buffer, we then populate the one channel matrix.</p>
<p>As you can see, the conversion process is simple, just keep in mind that tracking.js usually uses 4-channel data.</p>
</div>
</div>
</div>


<div class="section" title="What is filtering and how to use it?"><div class="titlepage" id="aid-J2B82"><div><div><h1 class="title"><a id="ch02lvl1sec18"></a>What is filtering and how to use it?</h1>
</div>
</div>
</div>
<p>Image filtering is<a id="id49" class="indexterm"></a> always a powerful tool to use in your Computer Vision applications. It allows you to apply many exciting effects on your photos, such as image correction, noise reduction, embossing, and many more. Image filtering is actually a huge subpart of an image processing area. In this section, we will discuss the concepts of image filtering and talk about a basic operation—convolution, which is widely used in all Computer Vision applications. Furthermore, we will see how different effects, such as blurring, are achieved.</p>
<div class="section" title="Image convolution"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec13"></a>Image convolution</h2>
</div>
</div>
</div>
<p>The core of <a id="id50" class="indexterm"></a>most filtering operations is image convolution. With its understanding you will have the power to make your own image filters.</p>
<p>The image convolution idea is that you want to apply to each pixel of the original image a transformation which is based on neighboring pixels. For this, you have a kernel—a simple 2D matrix, this is our transformation matrix. For each pixel of the original image, we take the sum of products, each product is just a new value of a resulting image. To compute it, each element of the kernel should be multiplied with the corresponding image pixel, where the center of the kernel must be multiplied with the current pixel of an image. The whole process is called convolution.</p>
<p>To see a practical example of convolution, we should move to one of the most popular filters, it uses the <span class="strong"><strong>Gaussian kernel</strong></span>. The filter itself is called a Gaussian filter (or Gaussian blur), and it is <a id="id51" class="indexterm"></a>used for image smoothing, noise removing, and for edge detection. Most of the edge detection algorithms are sensitive to noise, using the Gaussian filter before the edge detection helps to remove unnecessary noise.</p>
<p>In the following figure, we present an example of convolution using the Gaussian kernel:</p>
<div class="mediaobject"><img src="../Images/image00101.jpeg" alt="Image convolution"/><div class="caption"><p>From left to right: the Gaussian kernel, original matrix, and result matrix.</p>
</div>
</div>
<p style="clear:both; height: 1em;"> </p>
<p>To compute <a id="id52" class="indexterm"></a>a value in the <code class="literal">(2, 2)</code> position of the result matrix, we do the convolution:</p>
<p>
<span class="emphasis"><em>(1*0 + 2*1 + 1*3 + 2*2 + 4*1 + 2*2 + 1*4 + 2*3 + 1*5) / 16 = 2</em></span>
</p>
<p>See how the neighbors of the original matrix affect the result? Simply, a kernel matrix represents weights for the transformation process.</p>
<p>The 2D convolution requires four loops to compute so, in that case, it is better not to use big kernels; otherwise, our filtering process will be too slow. Usually, in image processing, the kernels from <span class="emphasis"><em>3x3</em></span> to <span class="emphasis"><em>7x7</em></span> are used and, as we already mentioned, the kernel should have a center and the dimensions should be odd. There are methods to improve the performance of the convolution operation, and we will analyze one of them in the next section.</p>
</div>
<div class="section" title="The Gaussian filter and separate convolution"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec14"></a>The Gaussian filter and separate convolution</h2>
</div>
</div>
</div>
<p>Normally, we<a id="id53" class="indexterm"></a> do not want to use heavy processing methods, such as<a id="id54" class="indexterm"></a> applying a 2D kernel on an image. To speed <a id="id55" class="indexterm"></a>up the computation, we can use a different approach. For <a id="id56" class="indexterm"></a>most of the Computer Vision applications, we need only some sort of blurring and edge detection methods. In that case, the 2D kernels which are used there may be presented as two separate 1D kernels. This type of operation states that you can get the same result by applying two separate filters for rows and columns. The process is called separate convolution. Here is an example of the Gaussian kernel:</p>
<div class="informalexample"><pre class="programlisting">[0.25]                       [0.0625, 0.125,  0.0625]
[0.5 ] X [0.25, 0.5, 0.25] = [0.125,  0.0625, 0.125 ]
[0.25]                       [0.0625, 0.125,  0.25  ]</pre>
</div>
<p>The 2D matrix is separable if it can be presented as the outer product of two vectors.</p>
<p>Enough of the theory! We did not even see the Gaussian filter in work. Moreover, it is a good point to combine both JSFeat and tracking.js libraries.</p>
<p>To get a Gaussian kernel, we will use JSFeat:</p>
<div class="informalexample"><pre class="programlisting">var kernelSize = 3, sigma = 0, kernelArray = [], dataType = jsfeat.F32_t;
jsfeat.math.get_gaussian_kernel(kernelSize, sigma, kernelArray, dataType);</pre>
</div>
<p>You can get <a id="id57" class="indexterm"></a>different sizes of a Gaussian kernel. The larger the size, the<a id="id58" class="indexterm"></a> more blurry the image will be. Previously, we <a id="id59" class="indexterm"></a>saw kernels only for the size of 3 elements. Next, sigma specifies how <a id="id60" class="indexterm"></a>wide your blur will be. If you set it to 0, then the function calculates the optimal value for the given kernel size itself. The result is written to the <code class="literal">kernelArray</code> variable and, of course, the data type is float, since we are working with floating point operations. After executing the function, <code class="literal">kernelArray</code> will contain the following:</p>
<div class="informalexample"><pre class="programlisting">[0.25, 0.5, 0.25]</pre>
</div>
<p>To get a full 2D kernel, we can use the <code class="literal">multiply</code> function, which we saw in the previous chapter:</p>
<div class="informalexample"><pre class="programlisting">var gaussianKernel = new jsfeat.data_t(kernelArray.length, kernelArray);
var A = new jsfeat.matrix_t(1, kernelSize, jsfeat.F32C1_t, gaussianKernel),
        B = new jsfeat.matrix_t(kernelSize, 1, jsfeat.F32C1_t, gaussianKernel),
        C = new jsfeat.matrix_t(kernelSize, kernelSize, jsfeat.F32C1_t);
jsfeat.matmath.multiply(C, A, B);</pre>
</div>
<p>You can print <code class="literal">C</code> to see that it represents the Gaussian kernel.</p>
<p>To use 1D kernels, we need to apply filters one by one to the original image. Unfortunately, JSFeat library does not provide you with such functionality. But tracking.js does! We will do this as follows:</p>
<div class="informalexample"><pre class="programlisting">var buf = tracking.Image.horizontalConvolve(gray, cols, rows, kernelArray, true);
buf = tracking.Image.verticalConvolve(buf, cols, rows, kernelArray, true);</pre>
</div>
<p>Remember that you need to set the last parameter to <code class="literal">true</code> if you want to return an RGBA array. Using the preceding code, we receive a blurred image, but what if we apply each filter separately? To see a clearer result, we need to choose a larger kernel size:</p>
<div class="mediaobject"><img src="../Images/image00102.jpeg" alt="The Gaussian filter and separate convolution"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>From left to<a id="id61" class="indexterm"></a> right, we see a gray image and then we see a horizontal filter<a id="id62" class="indexterm"></a> applied to it followed by a vertical filter for the the same<a id="id63" class="indexterm"></a> gray image. Finally, if we apply both filters to the<a id="id64" class="indexterm"></a> original image, as we did in the code, we will receive a blurred image like the last one. Eventually, the separate convolution works! It is really great to use that when you can present a 2D kernel as two 1D kernels.</p>
<p>It is worth mentioning that you can apply a Gaussian filter without using separate filters. For JSFeat library, see the following code:</p>
<div class="informalexample"><pre class="programlisting">jsfeat.imgproc.gaussian_blur(matGray, matBlurred, kernelSize);</pre>
</div>
<p>For the tracking.js, see the following code:</p>
<div class="informalexample"><pre class="programlisting">var blurred = tracking.Image.blur(gray, cols, rows, kernelSize);</pre>
</div>
<p>For the last one, the result returns <code class="literal">Float32Array</code>. So if you want to display it properly, you need to convert it to the <code class="literal">Uint8ClampedArray</code> type. In addition, this is the first function you see which returns RGBA values, you cannot return only one channel array here.</p>
<p>Here are the examples with different kernel sizes:</p>
<div class="mediaobject"><img src="../Images/image00103.jpeg" alt="The Gaussian filter and separate convolution"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>As you can <a id="id65" class="indexterm"></a>see, the bigger the kernel size we take, the less the<a id="id66" class="indexterm"></a> information we receive from an image and the<a id="id67" class="indexterm"></a> blurrier it becomes.</p>
<p>The Gaussian filter is <a id="id68" class="indexterm"></a>very useful when you need to reduce the image noise and reduce its details. Besides, it is commonly used to reduce the size of an image to get a better image approximation for a small size.</p>
</div>
<div class="section" title="The box blur"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec15"></a>The box blur</h2>
</div>
</div>
</div>
<p>There is a <a id="id69" class="indexterm"></a>different blur method that needs to be discussed. Sometimes, you just<a id="id70" class="indexterm"></a> need a rough approximation for the Gaussian Blur operation, a filter which is faster than Gaussian. Furthermore, you can sacrifice some blur quality. In that case, you can use the box blur filter.</p>
<p>This is just a regular mean operation. It has a simple kernel for a diameter of 3 elements:</p>
<div class="informalexample"><pre class="programlisting">[1/9, 1/9, 1/9]
[1/9, 1/9, 1/9]
[1/9, 1/9, 1/9]</pre>
</div>
<p>So, for a diameter = <span class="emphasis"><em>d</em></span> it will be like the following:</p>
<div class="informalexample"><pre class="programlisting">[1/n, 1/n, ..., 1/n]
[1/n, 1/n, ..., 1/n]
[..., ..., ..., ...]
[1/n, 1/n  ..., 1/n]</pre>
</div>
<p>Where, <span class="emphasis"><em>n = d * d</em></span> is the number of elements in a kernel.</p>
<p>It is simpler than Gaussian blur, but produces worse results. It is usually used as an approximation of a Gaussian blur. With the JSFeat library, it can be applied as follows:</p>
<div class="informalexample"><pre class="programlisting">jsfeat.imgproc.box_blur_gray(matGray, blurred, kernelRadius);</pre>
</div>
<p>Here, we use a <code class="literal">kernelRadius</code> parameter instead of a diameter (matrix size). The same result can be achieved if you use the following:</p>
<div class="informalexample"><pre class="programlisting">[1/3, 1/3, 1/3]</pre>
</div>
<p>Separate filter <a id="id71" class="indexterm"></a>vector with tracking.js library. Actually, a close result to the <a id="id72" class="indexterm"></a>Gaussian filter can be achieved if you apply box blur three times with a three times smaller kernel. For example, if the Gaussian kernel size is equal to 33 values, then the kernel size for the box blur should be 11 (or with radius = 5):</p>
<div class="mediaobject"><img src="../Images/image00104.jpeg" alt="The box blur"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>The first image has the Gaussian filter applied, the next has the box blur, which was applied thrice, and the last has the box blur with the same kernel size as the Gaussian kernel. Can you tell the difference between the first two images? It is really impossible to point it out. In addition, we see that the box blur for the last image removed more details and it produced an even worse result. Use the box blur filter only when you need to speed up the computation. But why is it so fast? There is magic in computing an integral image.</p>
</div>
<div class="section" title="The integral image"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec16"></a>The integral image</h2>
</div>
</div>
</div>
<p>Integrals are<a id="id73" class="indexterm"></a> really useful when you need to compute image parameters <a id="id74" class="indexterm"></a>quickly. For example, you can compute a filtered image for the box blur filter using the same amount of processing time for any kernel size. Isn't this amazing? Furthermore, it is also used for object detection.</p>
<p>Computing the integral image is just a simple algorithm that generates sums of values in rectangular subsets of a matrix. For the JSFeat library, it can be computed like this:</p>
<div class="informalexample"><pre class="programlisting">var matCopy1 = new jsfeat.matrix_t(matGray.cols + 1, matGray.rows + 1, jsfeat.F32C1_t);
var matCopy2 = new jsfeat.matrix_t(matGray.cols + 1, matGray.rows + 1, jsfeat.F32C1_t);
var matCopy3 = new jsfeat.matrix_t(matGray.cols + 1, matGray.rows + 1, jsfeat.F32C1_t);

jsfeat.imgproc.compute_integral_image(matGray, matCopy1.data, matCopy2.data, matCopy3.data);</pre>
</div>
<p>The first matrix <a id="id75" class="indexterm"></a>will contain the integral image or regular sums of image <a id="id76" class="indexterm"></a>subsets, the next matrix will contain squares of those sums, and the last will contain the tilted integral image. The dimensions of all input matrices should be 1 pixel larger than the original. To display the result, we need to normalize it, for example, the bottom-right element will contain the sum of all pixel values in a matrix. We cannot display the result matrix because the largest possible pixel value of an image is 255. We need to divide each pixel by the maximum value in a matrix and multiply it by 255. Here is the result:</p>
<div class="mediaobject"><img src="../Images/image00105.jpeg" alt="The integral image"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>We got what we expected—the maximum value is situated in the bottom-right corner for the first two matrices and the tilted result is presented in the last.</p>
<p>You can apply a box blur using integrals by yourself! Here is a simple explanation of how to do this:</p>
<div class="mediaobject"><img src="../Images/image00106.jpeg" alt="The integral image"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>We need to <a id="id77" class="indexterm"></a>compute the sum in the ABCD rectangle. From the<a id="id78" class="indexterm"></a> integral image, we know which sums are stored in <span class="emphasis"><em>A</em></span>, <span class="emphasis"><em>B</em></span>, <span class="emphasis"><em>C</em></span>, and <span class="emphasis"><em>D</em></span> positions. The sum of the rectangle can be computed using the following formula:</p>
<p>
<span class="emphasis"><em>S = value in C – value in B – value in D + value in A</em></span>
</p>
<p>This can be applied to any size of the box blur filter, and this is why it is that fast. First, we need to compute an integral image, which is done in one loop over all pixel values and then we just calculate <span class="emphasis"><em>S</em></span> for each pixel.</p>
</div>
</div>


<div class="section" title="Basic edge detection" id="aid-K0RQ1"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec19"></a>Basic edge detection</h1>
</div>
</div>
</div>
<p>For most Computer Vision applications, you process an image but you do not actually need to get all the information from it. For example, sometimes you just need to get the shape information to find an appropriate object. There is a huge topic in the field of image processing<a id="id79" class="indexterm"></a> called <span class="strong"><strong>edge detection</strong></span>. Methods related to that topic, search for points where pixel brightness changes dramatically. The extracted information aims to capture changes in the properties of an image. To understand the concept better and to see how the basic edge information can be extracted from an image, we will discuss different edge filters (or operators) starting with the Sobel filter.</p>
<div class="section" title="The Sobel filter"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec17"></a>The Sobel filter</h2>
</div>
</div>
</div>
<p>The Sobel operator or Sobel filter is common and widely used. It helps to detect edges and transitions in<a id="id80" class="indexterm"></a> images. The Sobel operator uses two kernels during the<a id="id81" class="indexterm"></a> processing—one for horizontal changes in brightness and another for vertical changes. Its kernel values are focused not on the current pixel, but on it neighboring pixels.</p>
<p>Typical Sobel kernels look like this:</p>
<div class="informalexample"><pre class="programlisting">[-1, 0, 1]   [1]                [-1, -2, -1]   [-1]
[-2, 0, 2] = [2] X [-1, 0, 1]   [ 0,  0,  0] = [ 0] X [1, 2, 1]
[-1, 0, 1]   [1]                [ 1,  2,  1]   [ 1]</pre>
</div>
<p>As you see, the kernels can be decomposed of two separate filters, which is good in terms of processing time. You can run this filter in different ways in the libraries. Since tracking.js provides more functionality with separable filters, let's see some of its examples:</p>
<div class="informalexample"><pre class="programlisting">var sobelSignVector = [-1, 0, 1];
var sobelScaleVector = [1, 2, 1];
var horizontal = tracking.Image.separableConvolve(gray, cols, rows, sobelScaleVector, sobelSignVector, true);
var vertical = tracking.Image.separableConvolve(gray, cols, rows, sobelSignVector, sobelScaleVector, true);</pre>
</div>
<p>This is another way of applying separable filters in tracking.js library. We use the <code class="literal">separableConvolve</code> function, whereas for the fourth and fifth parameters, it uses Sobel vectors.</p>
<p>The results are usually called derivatives, since they measure the change in values. We can compute these derivatives in JSFeat as follows:</p>
<div class="informalexample"><pre class="programlisting">jsfeat.imgproc.sobel_derivatives(matGray, imgGxGy);</pre>
</div>
<p>Where <code class="literal">imgGxGy</code> returns a 2-channel matrix, the first channel represents horizontal derivatives and the second represents vertical derivatives.</p>
<p>To get the result of a Sobel filter, we need to combine those two results; this is done using the following equation:</p>
<div class="informalexample"><pre class="programlisting">var value = Math.sqrt(h * h + v * v);</pre>
</div>
<p>Here, the value variable is the value of each pixel and it is computed using pixel values from horizontal and vertical derivatives.</p>
<p>To run the Sobel operator on an image directly you may prefer to use the following tracking.js function:</p>
<div class="informalexample"><pre class="programlisting">var sobelImg = tracking.Image.sobel(gray, cols, rows);</pre>
</div>
<p>You need to remember that it returns an RGBA array and you need to normalize it, since it contains values larger than 255.</p>
<p>The final result will look like this:</p>
<div class="mediaobject"><img src="../Images/image00107.jpeg" alt="The Sobel filter"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>From left to<a id="id82" class="indexterm"></a> right, we see the horizontal derivative, followed by the <a id="id83" class="indexterm"></a>vertical derivative, and finally, the result after applying the Sobel filter. As you can see, the edges of the image have a good visualization. To get edges, not just changes in image pixels, we need to go a bit deeper. But let's discuss several other useful operators that you may want to use in edge detection.</p>
</div>
<div class="section" title="Other operators"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec18"></a>Other operators</h2>
</div>
</div>
</div>
<p>You need to<a id="id84" class="indexterm"></a> keep in mind that in Computer Vision, there is usually no perfect way for doing things. There are several operators that need to be mentioned; in some cases, they can produce better results than the Sobel filter. For example, the Prewitt operator:</p>
<div class="informalexample"><pre class="programlisting">[-1, 0, 1]   [1]                 [-1, -1, -1]   [-1]
[-1, 0, 1] = [1] X [-1, 0, 1]   [ 0,  0,  0] = [ 0] X [1, 1, 1]
[-1, 0, 1]   [1]                 [ 1,  1,  1]   [ 1]</pre>
</div>
<p>Sometimes, it is a good point to start from, but it averages the result value too much, remember the box blur filter? Compare it with the Gaussian Blur, where the center of a kernel has more weight. If we want to do that, the Sobel filter is preferred. However, sometimes you need to save just a bit more information for the center. And if you need that, you can use the Scharr filter:</p>
<div class="informalexample"><pre class="programlisting">[ -3, 0,  3]   [ 3]                [-3, -10, -3]   [-1]
[-10, 0, 10] = [10] X [-1, 0, 1]   [ 0,   0,  0] = [ 0] X [3, 10, 3]
[ -3, 0,  3]   [ 3]                [ 3,  10,  3]   [ 1]</pre>
</div>
<p>See, the centre<a id="id85" class="indexterm"></a> has more weight now. Actually, it is difficult to see the difference between Prewitt, Sobel and Sharr operators, which is why we don't have visual examples here. It is better to perform some experiments and check which filter you need exactly.</p>
</div>
</div>


<div class="section" title="Advanced image processing" id="aid-KVCC1"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec20"></a>Advanced image processing</h1>
</div>
</div>
</div>
<p>We talked about filters a lot, but they usually require only some sort of a matrix kernel and that is it. If you <a id="id86" class="indexterm"></a>think that there should be more cool stuff in image filtering, you are totally right! First, we will see how to apply edge detection and how it works. In the final part, we will review the histogram equalization algorithm, which you probably use a lot if you have Photoshop.</p>
<div class="section" title="The Canny edge detector"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec19"></a>The Canny edge detector</h2>
</div>
</div>
</div>
<p>Let's be <a id="id87" class="indexterm"></a>curious; what if we threshold an image <a id="id88" class="indexterm"></a>after the Sobel filter? Thresholding is done by iterating over all pixels of a grayscale image and checking whether the value exceeds the threshold value:</p>
<div class="informalexample"><pre class="programlisting">for (var i = 0; i &lt; arr.length; i++)
    arr[i] = arr[i] &gt; threshold ? 255 : 0;</pre>
</div>
<p>This is what the threshold looks like. Just set the value to <code class="literal">255</code> if it is higher than the threshold and to <code class="literal">0</code> when it is not.</p>
<p>Here are several examples of different thresholds, each image having a higher threshold value than the previous:</p>
<div class="mediaobject"><img src="../Images/image00108.jpeg" alt="The Canny edge detector"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>See? The higher the threshold we set, the fewer the edges we get. This is the first step of the Canny edge detection algorithm.</p>
<p>When you <a id="id89" class="indexterm"></a>need to process only the most important information from images (it is usually shape information), and you need to remove <a id="id90" class="indexterm"></a>unnecessary data without losing the important structural properties of an image, it is really smart to use an edge detector. Nowadays, the Canny edge detector is used most commonly.</p>
<p>You can run the whole algorithm using the JSFeat library:</p>
<div class="informalexample"><pre class="programlisting">var canny = new jsfeat.matrix_t(cols, rows, jsfeat.U8C1_t);
jsfeat.imgproc.gaussian_blur(matGray, canny, kernelSize);
jsfeat.imgproc.canny(canny, canny, lowThresh, highThresh);</pre>
</div>
<p>Before the start of a Canny algorithm we usually apply the Gaussian Blur to reduce the noise. The larger you choose the kernel size, the fewer edges and less noise you get. Lower and higher thresholds are usually chosen empirically.</p>
<p>Under the lower threshold, all pixels (weak pixels) are removed (or suppressed) by the algorithm, as we did while playing with the Sobel filter thresholding. Pixels with a value larger than the higher threshold are marked as strong pixels. At the last stage, in addition to weak pixels, the algorithm suppress all pixels that are not connected to those strong pixels. As a rule of thumb, the smaller your lower threshold is, the more noise you get; the larger your higher threshold is, the fewer the object edges you receive. This is shown in the following figure:</p>
<div class="mediaobject"><img src="../Images/image00109.jpeg" alt="The Canny edge detector"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>The algorithm<a id="id91" class="indexterm"></a> detects object boundaries or<a id="id92" class="indexterm"></a> edges. For the first image, we picked 50 and 300 as the lower and higher thresholds, respectively, and we did not use the Gaussian Blur. For the second image, we applied the Gaussian filter. As a result, many noise edges were removed. If we increase the lower threshold to 100, then we will get the result from the third image. In that case, much of the noise data from the ground is removed. After increasing the higher threshold, we get fewer object edges, which can be seen in the fourth image. You can play with parameters; just remember that when you increase any of the thresholds, you receive less information.</p>
<p>The Canny filter returns only 0 for background and 255 for edges. The thickness of the edges is 1 pixel, which is really important when you need to find an object. The Canny edge detector is included in many Computer Vision frameworks and its application is very wide. It is adaptive to various environment conditions and it is very robust.</p>
</div>
<div class="section" title="Histogram equalization"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec20"></a>Histogram equalization</h2>
</div>
</div>
</div>
<p>Sometimes, you<a id="id93" class="indexterm"></a> may want to improve<a id="id94" class="indexterm"></a> the contrast of an image. It helps to see the details better when the important data is represented by close contrast values. The help comes from methods that operate with image histograms. An image histogram presents the number of pixels for each tonal value. Suppose you have an array, as follows:</p>
<div class="informalexample"><pre class="programlisting">var arr = [30, 100, 10, 30, 30, 100, 50, 255];</pre>
</div>
<p>You may want to redistribute values in case they have a better spreading of their intensity values. Let's use the histogram equalization method which is provided by JSFeat:</p>
<div class="informalexample"><pre class="programlisting">var matGray = new jsfeat.matrix_t(arr.length, 1, jsfeat.U8C1_t,
        new jsfeat.data_t(arr.length, arr));
var equalized = new jsfeat.matrix_t(arr.length, 1, jsfeat.U8C1_t);
jsfeat.imgproc.equalize_histogram(matGray, equalized);</pre>
</div>
<p>Histogram equalization is just a <a id="id95" class="indexterm"></a>usual function. The<a id="id96" class="indexterm"></a> first parameter indicates the input matrix, the second indicates the output equalized matrix. With our array, we receive as a result of equalization <code class="literal">equalized.data</code>:</p>
<div class="informalexample"><pre class="programlisting">[128, 223, 32, 128, 128, 223, 159, 255]</pre>
</div>
<p>The histogram for the original (left) and equalized (right) arrays will look like this:</p>
<div class="informalexample"><pre class="programlisting"> 10: 1      32: 1
 30: 3     128: 3
 50: 1     159: 1
100: 2     223: 2
255: 1     255: 1</pre>
</div>
<p>The equalize histogram function maps old values to the new ones, performing a better spread over the whole range of values, 0-255. In the preceding example, most of the values were situated in the first part of the range, after the redistribution, the difference between values was increased. Visually, it helps to distinguish separate image objects.</p>
<p>Here is how it looks with an image:</p>
<div class="mediaobject"><img src="../Images/image00110.jpeg" alt="Histogram equalization"/><div class="caption"><p>Left – the original grayscale image, and right – the image after equalization.</p>
</div>
</div>
<p style="clear:both; height: 1em;"> </p>
<p>As you can see, the contrast is much better now and the image itself looks more impressive. The grass and plants got much darker and the constructions are brighter. The histograms for the input and output images are as follows:</p>
<div class="mediaobject"><img src="../Images/image00111.jpeg" alt="Histogram equalization"/></div>
<p style="clear:both; height: 1em;"> </p>
<p>As a<a id="id97" class="indexterm"></a> consequence of better spreading, histogram equalization makes the histogram a bit more flat, so the histogram <a id="id98" class="indexterm"></a>values do not have such a clear center.</p>
<p>Histogram equalization can be used not only for a better image view, but also for extracting better image information. It is usually useful when an image background and foreground do not have high contrast. The biggest drawback that you should know is that this function may increase image noise. Anyway, histogram equalization is really useful, for example, in medical imaging and photo correction.</p>
</div>
</div>


<div class="section" title="Summary" id="aid-LTSU1"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec21"></a>Summary</h1>
</div>
</div>
</div>
<p>In this chapter, you first learned how to install tracking.js and how to use it with JSFeat. Now you know how to create your own image filters using the image convolution operation. Moreover, with separable convolutions, you can create much faster implementations of regular filters. When you need to reduce the noise, you will commonly use the Gaussian filter or the box blur filter when you need a faster algorithm. Edge detection? No problem, you can implement it and use it in your applications for both cases, when you need only the edges or the whole information about a change in image brightness. Last but not least, you now know how to improve the image contrast using histogram equalization. Look at how much we have covered in such a small chapter! There are many more topics on image processing and filtering, we just discussed a small portion of it. Eventually, we will be ready to use this knowledge in object detection.</p>
<p>In the next chapter, you will learn how to detect various objects using different tracking techniques, such as color detection and feature estimation. In addition, we will be able to create our own tracker. See you there!</p>
</div>
</body>
</html>