["```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nscala> import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg.Vectors\nWha\nscala> \n\nscala> val points = Array(\n |    LabeledPoint(0.0, Vectors.sparse(3, Array(1), Array(1.0))),\n |    LabeledPoint(1.0, Vectors.dense(0.0, 2.0, 0.0)),\n |    LabeledPoint(2.0, Vectors.sparse(3, Array((1, 3.0)))),\n |    LabeledPoint.parse(\"(3.0,[0.0,4.0,0.0])\"));\npts: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,(3,[1],[1.0])), (1.0,[0.0,2.0,0.0]), (2.0,(3,[1],[3.0])), (3.0,[0.0,4.0,0.0]))\nscala> \n\nscala> val rdd = sc.parallelize(points)\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = ParallelCollectionRDD[0] at parallelize at <console>:25\n\nscala> \n\nscala> val df = rdd.repartition(1).toDF\ndf: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nscala> df.write.parquet(\"points\")\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ wget -O - http://archive.cloudera.com/cdh5/cdh/5/parquet-1.5.0-cdh5.5.0.tar.gz | tar xzvf -\n\nakozlov@Alexanders-MacBook-Pro$ cd parquet-1.5.0-cdh5.5.0/parquet-tools\n\nakozlov@Alexanders-MacBook-Pro$ tar xvf xvf parquet-1.5.0-cdh5.5.0/parquet-tools/target/parquet-tools-1.5.0-cdh5.5.0-bin.tar.gz\n\nakozlov@Alexanders-MacBook-Pro$ cd parquet-tools-1.5.0-cdh5.5.0\n\nakozlov@Alexanders-MacBook-Pro $ ./parquet-schema ~/points/*.parquet \nmessage spark_schema {\n optional double label;\n optional group features {\n required int32 type (INT_8);\n optional int32 size;\n optional group indices (LIST) {\n repeated group list {\n required int32 element;\n }\n }\n optional group values (LIST) {\n repeated group list {\n required double element;\n }\n }\n }\n}\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro $ ./parquet-dump ~/points/*.parquet \nrow group 0 \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\nlabel:       DOUBLE GZIP DO:0 FPO:4 SZ:78/79/1.01 VC:4 ENC:BIT_PACKED,PLAIN,RLE\nfeatures: \n.type:       INT32 GZIP DO:0 FPO:82 SZ:101/63/0.62 VC:4 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE\n.size:       INT32 GZIP DO:0 FPO:183 SZ:97/59/0.61 VC:4 ENC:BIT_PACKED,PLAIN_DICTIONARY,RLE\n.indices: \n..list: \n...element:  INT32 GZIP DO:0 FPO:280 SZ:100/65/0.65 VC:4 ENC:PLAIN_DICTIONARY,RLE\n.values: \n..list: \n...element:  DOUBLE GZIP DO:0 FPO:380 SZ:125/111/0.89 VC:8 ENC:PLAIN_DICTIONARY,RLE\n\n label TV=4 RL=0 DL=1\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:BIT_PACKED VLE:PLAIN SZ:38 VC:4\n\n features.type TV=4 RL=0 DL=1 DS:                 2 DE:PLAIN_DICTIONARY\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:9 VC:4\n\n features.size TV=4 RL=0 DL=2 DS:                 1 DE:PLAIN_DICTIONARY\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:BIT_PACKED VLE:PLAIN_DICTIONARY SZ:9 VC:4\n\n features.indices.list.element TV=4 RL=1 DL=3 DS: 1 DE:PLAIN_DICTIONARY\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:RLE VLE:PLAIN_DICTIONARY SZ:15 VC:4\n\n features.values.list.element TV=8 RL=1 DL=3 DS:  5 DE:PLAIN_DICTIONARY\n ------------------------------------------------------------------------------------------------------------------------------------------------------------------\n page 0:                                           DLE:RLE RLE:RLE VLE:PLAIN_DICTIONARY SZ:17 VC:8\n\nDOUBLE label \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:1 V:0.0\nvalue 2: R:0 D:1 V:1.0\nvalue 3: R:0 D:1 V:2.0\nvalue 4: R:0 D:1 V:3.0\n\nINT32 features.type \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:1 V:0\nvalue 2: R:0 D:1 V:1\nvalue 3: R:0 D:1 V:0\nvalue 4: R:0 D:1 V:1\n\nINT32 features.size \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:2 V:3\nvalue 2: R:0 D:1 V:<null>\nvalue 3: R:0 D:2 V:3\nvalue 4: R:0 D:1 V:<null>\n\nINT32 features.indices.list.element \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 4 *** \nvalue 1: R:0 D:3 V:1\nvalue 2: R:0 D:1 V:<null>\nvalue 3: R:0 D:3 V:1\nvalue 4: R:0 D:1 V:<null>\n\nDOUBLE features.values.list.element \n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n*** row group 1 of 1, values 1 to 8 *** \nvalue 1: R:0 D:3 V:1.0\nvalue 2: R:0 D:3 V:0.0\nvalue 3: R:1 D:3 V:2.0\nvalue 4: R:1 D:3 V:0.0\nvalue 5: R:0 D:3 V:3.0\nvalue 6: R:0 D:3 V:0.0\nvalue 7: R:1 D:3 V:4.0\nvalue 8: R:1 D:3 V:0.0\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> val df = sqlContext.read.parquet(\"points\")\ndf: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nscala> val df = sqlContext.read.parquet(\"points\").collect\ndf: Array[org.apache.spark.sql.Row] = Array([0.0,(3,[1],[1.0])], [1.0,[0.0,2.0,0.0]], [2.0,(3,[1],[3.0])], [3.0,[0.0,4.0,0.0]])\n\nscala> val rdd = df.map(x => LabeledPoint(x(0).asInstanceOf[scala.Double], x(1).asInstanceOf[org.apache.spark.mllib.linalg.Vector]))\nrdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[16] at map at <console>:25\n\nscala> rdd.collect\nres12: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,(3,[1],[1.0])), (1.0,[0.0,2.0,0.0]), (2.0,(3,[1],[3.0])), (3.0,[0.0,4.0,0.0]))\n\nscala> rdd.filter(_.features(1) <= 2).collect\nres13: Array[org.apache.spark.mllib.regression.LabeledPoint] = Array((0.0,(3,[1],[1.0])), (1.0,[0.0,2.0,0.0]))\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> case class Person(id: String, visits: Array[String]) { override def toString: String = { val vsts = visits.mkString(\",\"); s\"($id -> $vsts)\" } }\ndefined class Person\n\nscala> val p1 = Person(\"Phil\", Array(\"http://www.google.com\", \"http://www.facebook.com\", \"http://www.linkedin.com\", \"http://www.homedepot.com\"))\np1: Person = (Phil -> http://www.google.com,http://www.facebook.com,http://www.linkedin.com,http://www.homedepot.com)\n\nscala> val p2 = Person(\"Emily\", Array(\"http://www.victoriassecret.com\", \"http://www.pacsun.com\", \"http://www.abercrombie.com/shop/us\", \"http://www.orvis.com\"))\np2: Person = (Emily -> http://www.victoriassecret.com,http://www.pacsun.com,http://www.abercrombie.com/shop/us,http://www.orvis.com)\n\nscala> sc.parallelize(Array(p1,p2)).repartition(1).toDF.write.parquet(\"history\")\n\nscala> import scala.collection.mutable.WrappedArray\nimport scala.collection.mutable.WrappedArray\n\nscala> val df = sqlContext.read.parquet(\"history\")\ndf: org.apache.spark.sql.DataFrame = [id: string, visits: array<string>]\n\nscala> val rdd = df.map(x => Person(x(0).asInstanceOf[String], x(1).asInstanceOf[WrappedArray[String]].toArray[String]))\nrdd: org.apache.spark.rdd.RDD[Person] = MapPartitionsRDD[27] at map at <console>:28\n\nscala> rdd.collect\nres9: Array[Person] = Array((Phil -> http://www.google.com,http://www.facebook.com,http://www.linkedin.com,http://www.homedepot.com), (Emily -> http://www.victoriassecret.com,http://www.pacsun.com,http://www.abercrombie.com/shop/us,http://www.orvis.com))\n\n```", "```py\nscala> :paste\n// Entering paste mode (ctrl-D to finish)\n\nimport com.esotericsoftware.kryo.Kryo\nimport org.apache.spark.serializer.{KryoSerializer, KryoRegistrator}\n\nclass MyKryoRegistrator extends KryoRegistrator {\n override def registerClasses(kryo: Kryo) {\n kryo.register(classOf[Person])\n }\n}\n\nobject MyKryoRegistrator {\n def register(conf: org.apache.spark.SparkConf) {\n conf.set(\"spark.serializer\", classOf[KryoSerializer].getName)\n conf.set(\"spark.kryo.registrator\", classOf[MyKryoRegistrator].getName)\n }\n}\n^D\n\n// Exiting paste mode, now interpreting.\n\nimport com.esotericsoftware.kryo.Kryo\nimport org.apache.spark.serializer.{KryoSerializer, KryoRegistrator}\ndefined class MyKryoRegistrator\ndefined module MyKryoRegistrator\n\nscala>\n\n```", "```py\nrow_format\n  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]\n    [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]\n    [NULL DEFINED AS char]\n```", "```py\n$ cat data\n0^A1^B1^D1.0$\n2^A1^B1^D3.0$\n1^A0^B0.0^C2.0^C0.0$\n3^A0^B0.0^C4.0^C0.0$\n\n```", "```py\n$ tar xf hive-1.1.0-cdh5.5.0.tar.gz \n$ cd hive-1.1.0-cdh5.5.0\n$ bin/hive\n…\nhive> CREATE TABLE LABELED_POINT ( LABEL INT, VECTOR UNIONTYPE<ARRAY<DOUBLE>, MAP<INT,DOUBLE>> ) STORED AS TEXTFILE;\nOK\nTime taken: 0.453 seconds\nhive> LOAD DATA LOCAL INPATH './data' OVERWRITE INTO TABLE LABELED_POINT;\nLoading data to table alexdb.labeled_point\nTable labeled_point stats: [numFiles=1, numRows=0, totalSize=52, rawDataSize=0]\nOK\nTime taken: 0.808 seconds\nhive> select * from labeled_point;\nOK\n0  {1:{1:1.0}}\n2  {1:{1:3.0}}\n1  {0:[0.0,2.0,0.0]}\n3  {0:[0.0,4.0,0.0]}\nTime taken: 0.569 seconds, Fetched: 4 row(s)\nhive>\n\n```", "```py\n(id, timestamp, path)\n```", "```py\nSELECT id, timestamp, path \n  ANALYTIC_FUNCTION(path) OVER (PARTITION BY id ORDER BY timestamp) AS agg\nFROM log_table;\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> :paste\n// Entering paste mode (ctrl-D to finish)\n\nimport java.io._\n\n// a basic page view structure\n@SerialVersionUID(123L)\ncase class PageView(ts: String, path: String) extends Serializable with Ordered[PageView] {\n override def toString: String = {\n s\"($ts :$path)\"\n }\n def compare(other: PageView) = ts compare other.ts\n}\n\n// represent a session\n@SerialVersionUID(456L)\ncase class Session[A  <: PageView](id: String, visits: Seq[A]) extends Serializable {\n override def toString: String = {\n val vsts = visits.mkString(\"[\", \",\", \"]\")\n s\"($id -> $vsts)\"\n }\n}^D\n// Exiting paste mode, now interpreting.\n\nimport java.io._\ndefined class PageView\ndefined class Session\n\n```", "```py\nscala> val rdd = sc.textFile(\"log.csv\").map(x => { val z = x.split(\",\",3); (z(1), new PageView(z(0), z(2))) } ).groupByKey.map( x => { new Session(x._1, x._2.toSeq.sorted) } ).persist\nrdd: org.apache.spark.rdd.RDD[Session] = MapPartitionsRDD[14] at map at <console>:31\n\nscala> rdd.take(3).foreach(println)\n(189.248.74.238 -> [(2015-08-23 23:09:16 :mycompanycom>homepage),(2015-08-23 23:11:00 :mycompanycom>homepage),(2015-08-23 23:11:02 :mycompanycom>running:slp),(2015-08-23 23:12:01 :mycompanycom>running:slp),(2015-08-23 23:12:03 :mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp),(2015-08-23 23:12:08 :mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp),(2015-08-23 23:12:08 :mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp),(2015-08-23 23:12:42 :mycompanycom>running:slp),(2015-08-23 23:13:25 :mycompanycom>homepage),(2015-08-23 23:14:00 :mycompanycom>homepage),(2015-08-23 23:14:06 :mycompanycom:mobile>mycompany photoid>landing),(2015-08-23 23:14:56 :mycompanycom>men>shoes:segmentedgrid),(2015-08-23 23:15:10 :mycompanycom>homepage)])\n(82.166.130.148 -> [(2015-08-23 23:14:27 :mycompanycom>homepage)])\n(88.234.248.111 -> [(2015-08-23 22:36:10 :mycompanycom>plus>home),(2015-08-23 22:36:20 :mycompanycom>plus>home),(2015-08-23 22:36:28 :mycompanycom>plus>home),(2015-08-23 22:36:30 :mycompanycom>plus>onepluspdp>sport band),(2015-08-23 22:36:52 :mycompanycom>onsite search>results found),(2015-08-23 22:37:19 :mycompanycom>plus>onepluspdp>sport band),(2015-08-23 22:37:21 :mycompanycom>plus>home),(2015-08-23 22:37:39 :mycompanycom>plus>home),(2015-08-23 22:37:43 :mycompanycom>plus>home),(2015-08-23 22:37:46 :mycompanycom>plus>onepluspdp>sport watch),(2015-08-23 22:37:50 :mycompanycom>gear>mycompany+ sportwatch:standardgrid),(2015-08-23 22:38:14 :mycompanycom>homepage),(2015-08-23 22:38:35 :mycompanycom>homepage),(2015-08-23 22:38:37 :mycompanycom>plus>products landing),(2015-08-23 22:39:01 :mycompanycom>homepage),(2015-08-23 22:39:24 :mycompanycom>homepage),(2015-08-23 22:39:26 :mycompanycom>plus>whatismycompanyfuel)])\n\n```", "```py\nscala> import java.time.ZoneOffset\nimport java.time.ZoneOffset\n\nscala> import java.time.LocalDateTime\nimport java.time.LocalDateTime\n\nscala> import java.time.format.DateTimeFormatter\nimport java.time.format.DateTimeFormatter\n\nscala> \nscala> def toEpochSeconds(str: String) : Long = { LocalDateTime.parse(str, DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")).toEpochSecond(ZoneOffset.UTC) }\ntoEpochSeconds: (str: String)Long\n\nscala> val checkoutPattern = \".*>checkout.*\".r.pattern\ncheckoutPattern: java.util.regex.Pattern = .*>checkout.*\n\nscala> val lengths = rdd.map(x => { val pths = x.visits.map(y => y.path); val pchs = pths.indexWhere(checkoutPattern.matcher(_).matches); (x.id, x.visits.map(y => y.ts).min, x.visits.map(y => y.ts).max, x.visits.lastIndexWhere(_ match { case PageView(ts, \"mycompanycom>homepage\") => true; case _ => false }, pchs), pchs, x.visits) } ).filter(_._4>0).filter(t => t._5>t._4).map(t => (t._5 - t._4, toEpochSeconds(t._6(t._5).ts) - toEpochSeconds(t._6(t._4).ts)))\n\nscala> lengths.toDF(\"cnt\", \"sec\").agg(avg($\"cnt\"),min($\"cnt\"),max($\"cnt\"),avg($\"sec\"),min($\"sec\"),max($\"sec\")).show\n+-----------------+--------+--------+------------------+--------+--------+\n\n|         avg(cnt)|min(cnt)|max(cnt)|          avg(sec)|min(sec)|max(sec)|\n+-----------------+--------+--------+------------------+--------+--------+\n|19.77570093457944|       1|     121|366.06542056074767|      15|    2635|\n+-----------------+--------+--------+------------------+--------+--------+\n\nscala> lengths.map(x => (x._1,1)).reduceByKey(_+_).sortByKey().collect\nres18: Array[(Int, Int)] = Array((1,1), (2,8), (3,2), (5,6), (6,7), (7,9), (8,10), (9,4), (10,6), (11,4), (12,4), (13,2), (14,3), (15,2), (17,4), (18,6), (19,1), (20,1), (21,1), (22,2), (26,1), (27,1), (30,2), (31,2), (35,1), (38,1), (39,2), (41,1), (43,2), (47,1), (48,1), (49,1), (65,1), (66,1), (73,1), (87,1), (91,1), (103,1), (109,1), (121,1))\n\n```", "```py\ndef splitSession(session: Session[PageView]) : Seq[Session[PageView]] = { … }\n```", "```py\nval newRdd = rdd.flatMap(splitSession)\n```", "```py\nscala> trait Epoch {\n |   this: PageView =>\n |   def epoch() : Long = { LocalDateTime.parse(ts, DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\")).toEpochSecond(ZoneOffset.UTC) }\n | }\ndefined trait Epoch\n\n```", "```py\nscala> val rddEpoch = rdd.map(x => new Session(x.id, x.visits.map(x => new PageView(x.ts, x.path) with Epoch)))\nrddEpoch: org.apache.spark.rdd.RDD[Session[PageView with Epoch]] = MapPartitionsRDD[20] at map at <console>:31\n\n```", "```py\nscala> rddEpoch.map(x => (x.id, x.visits.zip(x.visits.tail).map(x => (x._2.path, x._2.epoch - x._1.epoch)).mkString(\"[\", \",\", \"]\"))).take(3).foreach(println)\n(189.248.74.238,[(mycompanycom>homepage,104),(mycompanycom>running:slp,2),(mycompanycom>running:slp,59),(mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp,2),(mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp,5),(mycompanycom>running>stories>2013>04>themycompanyfreestore:cdp,0),(mycompanycom>running:slp,34),(mycompanycom>homepage,43),(mycompanycom>homepage,35),(mycompanycom:mobile>mycompany photoid>landing,6),(mycompanycom>men>shoes:segmentedgrid,50),(mycompanycom>homepage,14)])\n(82.166.130.148,[])\n(88.234.248.111,[(mycompanycom>plus>home,10),(mycompanycom>plus>home,8),(mycompanycom>plus>onepluspdp>sport band,2),(mycompanycom>onsite search>results found,22),(mycompanycom>plus>onepluspdp>sport band,27),(mycompanycom>plus>home,2),(mycompanycom>plus>home,18),(mycompanycom>plus>home,4),(mycompanycom>plus>onepluspdp>sport watch,3),(mycompanycom>gear>mycompany+ sportwatch:standardgrid,4),(mycompanycom>homepage,24),(mycompanycom>homepage,21),(mycompanycom>plus>products landing,2),(mycompanycom>homepage,24),(mycompanycom>homepage,23),(mycompanycom>plus>whatismycompanyfuel,2)])\n\n```", "```py\nscala> def findAllMatchedSessions(h: Seq[Session[PageView]], s: Session[PageView]) : Seq[Session[PageView]] = {\n |     def matchSessions(h: Seq[Session[PageView]], id: String, p: Seq[PageView]) : Seq[Session[PageView]] = {\n |       p match {\n |         case Nil => Nil\n |         case PageView(ts1, \"mycompanycom>homepage\") :: PageView(ts2, \"mycompanycom>plus>products landing\") :: tail =>\n |           matchSessions(h, id, tail).+:(new Session(id, p))\n |         case _ => matchSessions(h, id, p.tail)\n |       }\n |     }\n |    matchSessions(h, s.id, s.visits)\n | }\nfindAllSessions: (h: Seq[Session[PageView]], s: Session[PageView])Seq[Session[PageView]]\n\n```", "```py\nscala> rdd.flatMap(x => findAllMatchedSessions(Nil, x)).take(10).foreach(println)\n(88.234.248.111 -> [(2015-08-23 22:38:35 :mycompanycom>homepage),(2015-08-23 22:38:37 :mycompanycom>plus>products landing),(2015-08-23 22:39:01 :mycompanycom>homepage),(2015-08-23 22:39:24 :mycompanycom>homepage),(2015-08-23 22:39:26 :mycompanycom>plus>whatismycompanyfuel)])\n(148.246.218.251 -> [(2015-08-23 22:52:09 :mycompanycom>homepage),(2015-08-23 22:52:16 :mycompanycom>plus>products landing),(2015-08-23 22:52:23 :mycompanycom>homepage),(2015-08-23 22:52:32 :mycompanycom>homepage),(2015-08-23 22:52:39 :mycompanycom>running:slp)])\n(86.30.116.229 -> [(2015-08-23 23:15:00 :mycompanycom>homepage),(2015-08-23 23:15:02 :mycompanycom>plus>products landing),(2015-08-23 23:15:12 :mycompanycom>plus>products landing),(2015-08-23 23:15:18 :mycompanycom>language tunnel>load),(2015-08-23 23:15:23 :mycompanycom>language tunnel>geo selected),(2015-08-23 23:15:24 :mycompanycom>homepage),(2015-08-23 23:15:27 :mycompanycom>homepage),(2015-08-23 23:15:30 :mycompanycom>basketball:slp),(2015-08-23 23:15:38 :mycompanycom>basketball>lebron-10:cdp),(2015-08-23 23:15:50 :mycompanycom>basketball>lebron-10:cdp),(2015-08-23 23:16:05 :mycompanycom>homepage),(2015-08-23 23:16:09 :mycompanycom>homepage),(2015-08-23 23:16:11 :mycompanycom>basketball:slp),(2015-08-23 23:16:29 :mycompanycom>onsite search>results found),(2015-08-23 23:16:39 :mycompanycom>onsite search>no results)])\n(204.237.0.130 -> [(2015-08-23 23:26:23 :mycompanycom>homepage),(2015-08-23 23:26:27 :mycompanycom>plus>products landing),(2015-08-23 23:26:35 :mycompanycom>plus>fuelband activity>summary>wk)])\n(97.82.221.34 -> [(2015-08-23 22:36:24 :mycompanycom>homepage),(2015-08-23 22:36:32 :mycompanycom>plus>products landing),(2015-08-23 22:37:09 :mycompanycom>plus>plus activity>summary>wk),(2015-08-23 22:37:39 :mycompanycom>plus>products landing),(2015-08-23 22:44:17 :mycompanycom>plus>home),(2015-08-23 22:44:33 :mycompanycom>plus>home),(2015-08-23 22:44:34 :mycompanycom>plus>home),(2015-08-23 22:44:36 :mycompanycom>plus>home),(2015-08-23 22:44:43 :mycompanycom>plus>home)])\n(24.230.204.72 -> [(2015-08-23 22:49:58 :mycompanycom>homepage),(2015-08-23 22:50:00 :mycompanycom>plus>products landing),(2015-08-23 22:50:30 :mycompanycom>homepage),(2015-08-23 22:50:38 :mycompanycom>homepage),(2015-08-23 22:50:41 :mycompanycom>training:cdp),(2015-08-23 22:51:56 :mycompanycom>training:cdp),(2015-08-23 22:51:59 :mycompanycom>store locator>start),(2015-08-23 22:52:28 :mycompanycom>store locator>landing)])\n(62.248.72.18 -> [(2015-08-23 23:14:27 :mycompanycom>homepage),(2015-08-23 23:14:30 :mycompanycom>plus>products landing),(2015-08-23 23:14:33 :mycompanycom>plus>products landing),(2015-08-23 23:14:40 :mycompanycom>plus>products landing),(2015-08-23 23:14:47 :mycompanycom>store homepage),(2015-08-23 23:14:50 :mycompanycom>store homepage),(2015-08-23 23:14:55 :mycompanycom>men:clp),(2015-08-23 23:15:08 :mycompanycom>men:clp),(2015-08-23 23:15:15 :mycompanycom>men:clp),(2015-08-23 23:15:16 :mycompanycom>men:clp),(2015-08-23 23:15:24 :mycompanycom>men>sportswear:standardgrid),(2015-08-23 23:15:41 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:45 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:45 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:49 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:50 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:15:56 :mycompanycom>men>sportswear:standardgrid),(2015-08-23 23:18:41 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:18:42 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:18:53 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:18:55 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:18:57 :mycompanycom>pdp>mycompany bruin low men's shoe),(2015-08-23 23:19:04 :mycompanycom>men>sportswear:standardgrid),(2015-08-23 23:20:12 :mycompanycom>men>sportswear>silver:standardgrid),(2015-08-23 23:28:20 :mycompanycom>onsite search>no results),(2015-08-23 23:28:33 :mycompanycom>onsite search>no results),(2015-08-23 23:28:36 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:40 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:41 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:43 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:28:43 :mycompanycom>pdp>mycompany blazer low premium vintage suede men's shoe),(2015-08-23 23:29:00 :mycompanycom>pdp:mycompanyid>mycompany blazer low id shoe)])\n(46.5.127.21 -> [(2015-08-23 22:58:00 :mycompanycom>homepage),(2015-08-23 22:58:01 :mycompanycom>plus>products landing)])\n(200.45.228.1 -> [(2015-08-23 23:07:33 :mycompanycom>homepage),(2015-08-23 23:07:39 :mycompanycom>plus>products landing),(2015-08-23 23:07:42 :mycompanycom>plus>products landing),(2015-08-23 23:07:45 :mycompanycom>language tunnel>load),(2015-08-23 23:07:59 :mycompanycom>homepage),(2015-08-23 23:08:15 :mycompanycom>homepage),(2015-08-23 23:08:26 :mycompanycom>onsite search>results found),(2015-08-23 23:08:43 :mycompanycom>onsite search>no results),(2015-08-23 23:08:49 :mycompanycom>onsite search>results found),(2015-08-23 23:08:53 :mycompanycom>language tunnel>load),(2015-08-23 23:08:55 :mycompanycom>plus>products landing),(2015-08-23 23:09:04 :mycompanycom>homepage),(2015-08-23 23:11:34 :mycompanycom>running:slp)])\n(37.78.203.213 -> [(2015-08-23 23:18:10 :mycompanycom>homepage),(2015-08-23 23:18:12 :mycompanycom>plus>products landing),(2015-08-23 23:18:14 :mycompanycom>plus>products landing),(2015-08-23 23:18:22 :mycompanycom>plus>products landing),(2015-08-23 23:18:25 :mycompanycom>store homepage),(2015-08-23 23:18:31 :mycompanycom>store homepage),(2015-08-23 23:18:34 :mycompanycom>men:clp),(2015-08-23 23:18:50 :mycompanycom>store homepage),(2015-08-23 23:18:51 :mycompanycom>footwear:segmentedgrid),(2015-08-23 23:19:12 :mycompanycom>men>footwear:segmentedgrid),(2015-08-23 23:19:12 :mycompanycom>men>footwear:segmentedgrid),(2015-08-23 23:19:26 :mycompanycom>men>footwear>new releases:standardgrid),(2015-08-23 23:19:26 :mycompanycom>men>footwear>new releases:standardgrid),(2015-08-23 23:19:35 :mycompanycom>pdp>mycompany cheyenne 2015 men's shoe),(2015-08-23 23:19:40 :mycompanycom>men>footwear>new releases:standardgrid)])\n\n```", "```py\ndef randomeProjecton(data: NestedStructure) : Vector = { … }\n\n```"]