- en: Chapter 11. Topic Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Topic modeling is a relatively recent and exciting area that originated from
    the fields of natural language processing and information retrieval, but has seen
    applications in a number of other domains as well. Many problems in classification,
    such as sentiment analysis, involve assigning a single class to a particular observation.
    In topic modeling, the key idea is that we can assign a mixture of different classes
    to an observation. As this field takes its inspiration from information retrieval,
    we often think of our observations as documents and our output classes as topics.
    In many applications, this is actually the case and so we will focus on the domain
    of text documents and their topics, this being a very natural way to learn about
    this important model. In particular, we'll focus on a technique known as **Latent
    Dirichlet Allocation** (**LDA**), which is the most prominently used method for
    topic modeling.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of topic modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](part0076_split_000.html#28FAO2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 10. Probabilistic Graphical Models"), *Probabilistic Graphical Models*,
    we saw how we can use a bag of words as a feature of a Naïve Bayes model in order
    to perform sentiment analysis. There, the specific predictive task involved determining
    whether a particular movie review was expressing a positive sentiment or a negative
    sentiment. We explicitly assumed that the movie review was exclusively expressing
    only one possible sentiment. Each of the words used as features (such as *bad*,
    *good*, *fun*, and so on) had a different likelihood of appearing in a review
    under each sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the model's decision, we basically computed the likelihood of all
    the words in a particular review under one class, and compared this to the likelihood
    of all the words having been generated by the other class. We adjusted these likelihoods
    using the prior probability of each class, so that, when we know that one class
    is more popular in the training data, we expect to find it more frequently represented
    on unseen data in the future. There was no opportunity for a movie review to be
    partially positive, so that some of the words came from the positive class, and
    partially negative, so that the rest of the words occurred in the negative class.
  prefs: []
  type: TYPE_NORMAL
- en: The core premise behind **topic models** is that in our problem we have a set
    of features and a set of hidden or latent variables that generate these features.
    Crucially, each observation in our data contains features that have been generated
    from a mixture or? a subset of these hidden variables. For example, an essay,
    website, or news article might have a central topic or theme such as politics,
    but might also include one or more elements from other themes as well, such as
    human rights, history, or economics.
  prefs: []
  type: TYPE_NORMAL
- en: In the image domain, we might be interested in identifying a particular object
    in a scene from a set of visual features such as shadows and surfaces. These,
    in turn, might be the product of a mixture of different objects. Our task in topic
    modeling is to observe the words inside a document, or the pixels and visual features
    of an image, and from these determine the underlying mix of topics and objects
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling on text data can be used in a number of different ways. One possible
    application is to group together similar documents, either based on their most
    predominant topic or based on their topical mix. Thus, it can be viewed as a form
    of clustering. By studying the topic composition, the most frequent words, as
    well as the relative sizes of the clusters we obtain, we are able to summarize
    information about a particular collection of documents.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the most frequent words and topics of a cluster to describe a cluster
    directly, and in turn this might be useful for automatically generating tags,
    for example to improve the search capabilities of an information retrieval service
    for our documents. Yet another example might be to automatically recommending
    Twitter hashtags once we have built a topic model for a database of tweets.
  prefs: []
  type: TYPE_NORMAL
- en: When we describe documents such as websites using a bag of words approach, each
    document is essentially a vector indexed by the words in our dictionary. The elements
    of the vector are either counts of the various words or binary variables capturing
    whether a word was present in the document. Either way, this representation is
    a good method of encoding text into a numerical format, but the result is a sparse
    vector in a high-dimensional space as the word dictionary is typically large.
    Under a topic model, each document is represented by a mixture of topics. As this
    number tends to be much smaller than the dictionary size, topic modeling can also
    function as a form of dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, topic modeling can also be viewed as a predictive task for classification.
    If we have a collection of documents labeled with a predominant theme label, we
    can perform topic modeling on this collection. If the predominant topic clustering
    we obtain from this method coincides with our labeled categories, we can use the
    model to predict a topical mixture for an unknown document and classify it according
    to the most dominant topic. We'll see an example of this later on in this chapter.
    We will now introduce the most well-known technique for performing topic modeling,
    Latent Dirichlet Allocation.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Latent Dirichlet Allocation** (**LDA**) is the prototypical method of performing
    topic modeling. Rather unfortunately, the acronym LDA is also used for another
    method in machine learning. This latter method is completely different from LDA
    and is commonly used as a way to perform dimensionality reduction and classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Although LDA involves a substantial amount of mathematics, it is worth exploring
    some of its technical details in order to understand how the model works and the
    assumptions that it uses. First and foremost, we should learn about the **Dirichlet
    distribution**, which lends its name to LDA.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An excellent reference for a fuller treatment of Topic Models with LDA is the
    *Topic Models* chapter in *Text Mining: Classification, Clustering, and Applications*,
    edited by *A. Srivastava* and *M. Sahami* and published by *Chapman & Hall*, 2009.'
  prefs: []
  type: TYPE_NORMAL
- en: The Dirichlet distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose we have a classification problem with *K* classes and the probability
    of each class is fixed. Given a vector of length *K* containing counts of the
    occurrence of each class, we can estimate the probabilities of each class by just
    dividing each entry in the vector by the sum of all the counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now suppose we would like to predict the number of times each class will occur
    over a series of *N* trials. If we have two classes, we can model this with a
    binomial distribution, as we would normally do in a coin flip experiment. For
    *K* classes, the binomial distribution generalizes to the **multinomial distribution**,
    where the probability of each class, *pi*, is fixed and the sum of all instances
    of *pi* equals one. Now, suppose that we wanted to model the random selection
    of a particular multinomial distribution with *K* categories. The Dirichlet distribution
    achieves just that. Here is its form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Dirichlet distribution](img/00183.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: This equation seems complex, but if we break it down to its constituent parts
    and label the symbols used, we will then be able to understand it better. To begin
    with, the ![The Dirichlet distribution](img/00184.jpeg) term is a vector with
    *K* components, *x[k]*, representing a particular multinomial distribution. The
    vector ![The Dirichlet distribution](img/00185.jpeg) is also a *K* component vector
    containing the *K* parameters, *α[k]*, of the Dirichlet distribution. Thus, we
    are computing the probability of selecting a particular multinomial distribution
    given a particular parameter combination. Notice that we provide the Dirichlet
    distribution with a parameter vector whose length is the same as the number of
    classes of the multinomial distribution that it will return.
  prefs: []
  type: TYPE_NORMAL
- en: 'The fraction before the large product on the right-hand side of the equation
    is a normalizing constant, which depends only on the values of the Dirichlet parameters
    and is expressed in terms of the **gamma function**. For completeness, the gamma
    function, a generalization of the factorial function, is given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Dirichlet distribution](img/00186.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Lastly, in the final product, we see that every parameter, *α[k]*, is paired
    with the corresponding component of the multinomial distribution, *x[k]*, in forming
    the terms of the product. The important point to remember about this distribution
    is that, by modifying the *α[k]* parameters, we are modifying the probabilities
    of the different multinomial distributions that we can draw.
  prefs: []
  type: TYPE_NORMAL
- en: We are especially interested in the total sum of the *α[k]* parameters as well
    as the relative proportions among them. A large total for the *α[k]* parameters
    tends to produce a smoother distribution involving a mix of many topics and this
    distribution is more likely to follow the pattern of alpha parameters in their
    relative proportions.
  prefs: []
  type: TYPE_NORMAL
- en: A special case of the Dirichlet distribution is the **Symmetrical Dirichlet
    distribution**, in which all the *α[k]* parameters have an identical value. When
    the *α[k]* parameters are identical and large in value, we are likely to sample
    a multinomial distribution that is close to being uniform. Thus, the symmetrical
    Dirichlet distribution is used when we have no information about a preference
    over a particular topic distribution and we consider all topics to be equally
    likely.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, suppose we had a skewed vector of *α[k]* parameters with large absolute
    values. For example, we might have a vector in which one of the *α[k]* parameters
    was much higher than the others, indicating a preference for selecting one of
    the topics. If we used this as an input to the Dirchlet distribution, we would
    likely sample a multinomial distribution in which the aforementioned topic was
    probable.
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast, if the *α[k]* parameters add up to a small number, this usually
    results in a peaky distribution, in which only one or two of the topics are likely
    and the rest are unlikely. Consequently, if we wanted to model the process of
    drawing a multinomial with only a few topics selected, we would use low values
    for the *α[k]* parameters, whereas, if we wanted a good mix, we would use larger
    values. The following two plots will help visualize this behavior. The first plot
    is for a symmetric Dirichlet distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Dirichlet distribution](img/00187.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In this plot, each column contains four random samples of a multinomial distribution
    generated using a symmetric Dirichlet distribution for five topics. In the first
    column, all the *α[k]* parameters are set to 0.1\. Note that the distributions
    are very peaky and, because all the *α[k]* parameters are equally likely, there
    is no preference for which topic will tend to be chosen as the highest peak.
  prefs: []
  type: TYPE_NORMAL
- en: In the middle column, the *α[k]* parameters are set to 1 and, as the sum of
    the parameters is now larger, we see a greater mix of topics, even if the distribution
    is still skewed. When we set the *α[k]* parameters to the value of 10 in the third
    column, we see that the samples are now much closer to a uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many scenarios, we use the Dirichlet distribution as a **prior distribution**;
    that is, a distribution that describes our prior beliefs about the multinomial
    distribution that we are trying to sample. When the sum of the *α[k]* parameters
    is high, we tend to think of our prior as having very strong beliefs. In the next
    plot, we will skew the distribution of our *α[k]* parameters to favor the first
    topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Dirichlet distribution](img/00188.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: In the first column, the average value of the *α[k]* parameters is 0.1, but
    we adjusted their distribution so that *α[1]*, which corresponds to the first
    topic, now has a value four times as high as the others. We see that this has
    increased the probability of the first topic featuring prominently in the sampled
    multinomial distribution, but it is not guaranteed to be the distribution's mode.
  prefs: []
  type: TYPE_NORMAL
- en: In the middle column, where the average of the *α[k]* parameters is now 1 but
    with the same skew, Topic 1 is the mode of distribution in all the samples. Additionally,
    there is still a high variance in how the other topics will be selected. In the
    third column, we have a smoother distribution that simultaneously mixes all five
    topics but enforces the preference for the first topic.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an idea of how this distribution works, we will see in the
    next section how it is used to build topic models with LDA.
  prefs: []
  type: TYPE_NORMAL
- en: The generative process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We delved into the Dirichlet distribution with significant detail because it
    is at the heart of how topic modeling with LDA operates. Armed with this understanding,
    we'll now describe the **generative process** behind LDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The generative process is aptly named as it describes how the LDA model assumes
    that documents, topics, and words are generated in our data. This process is essentially
    an illustration of the model''s assumptions. The optimization procedures that
    are used in order to fit an LDA model to data essentially estimate the parameters
    of the generative process. We''ll now see how this process works:'
  prefs: []
  type: TYPE_NORMAL
- en: For each of our K topics, draw a multinomial distribution, *φ[k]*, over the
    words in our vocabulary using a Dirichlet distribution parameterized by a vector,
    α. This vector has length *V*, the size of our vocabulary. Even though we sample
    from the same Dirichlet distribution each time, we've seen that the sampled multinomial
    distributions will likely differ from each other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For every document, *d*, that we want to generate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the mix of topics for this document by drawing a multinomial distribution,
    *θ[k]*, from a Dirichlet distribution, this time parameterized by a vector *β*
    of length *K*, the number of topics. Each document will thus have a different
    mix of topics.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For every word, *w*, in the document that we want to generate:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the multinomial topic distribution for this document, *θ[k]*, to draw a
    topic with which this word will be associated
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Use that particular topic's distribution, *φ[k]*, to pick the actual word
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that we use two differently parameterized Dirichlet distributions in our
    generative process, one for drawing a multinomial distribution of topics and another
    for drawing a multinomial distribution of words. Although the model is simple,
    it does capture certain intuitions about documents and topics. In particular,
    it captures the notion that documents about different topics will, in general,
    contain different words in them and in different proportions. Additionally, a
    particular word can be associated with more than one topic, but for some topics
    it will appear at a higher frequency than others. Documents may have a central
    topic, but they may also discuss other topics as well, and therefore we can think
    of a document as dealing with a mixture of topics. A topic that is more important
    in a document will be so because a greater percentage of the words in the document
    deal with that topic.
  prefs: []
  type: TYPE_NORMAL
- en: Dirichlet distributions can be smooth or skewed, and the mixture of components
    can be controlled via the *α[k]* parameters. Consequently, by tuning the Dirichlet
    distributions appropriately, this process can produce documents with a single
    theme as well as documents covering many topics.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, it is important to bear in mind the limitations of the model
    through some of the simplifying assumptions that it makes. The model completely
    ignores the word order inside a document, and the generative process is memoryless
    in that, when it generates the *n^(th)* word of a document, it does not take into
    account the existing *n-1* words that were previously drawn for that document.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, LDA does not attempt to model any relationships between the topics
    that are drawn for a document so that we do not try to organize topics that are
    more likely to co-occur, such as weather and travel or biology and chemistry.
    This is a significant limitation of the LDA model, for which there are proposed
    solutions. For example, one variant of LDA, known as the **Correlated Topic Model**
    (**CTM**), follows the same generative process as LDA but uses a different distribution
    that allows one to also model the correlations between the topics. In our experimental
    section, we will also see an implementation of the CTM model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The correlated topic model was presented in *A Correlated Topic Model of Science*
    by *D. M. Blei* and *J. D. Lafferty*, published by the *Annals of Applied Statistics*
    in 2007.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting an LDA model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fitting an LDA model to a corpus of documents essentially involves computationally
    estimating the multinomial topic and word distributions, *φ[k]* and *θ[d]*, that
    would most likely be able to generate the data, assuming the LDA generative process.
    These variables are hidden or latent, which explains why the method is known as
    LDA.
  prefs: []
  type: TYPE_NORMAL
- en: A number of optimization procedures have been proposed to solve this problem,
    but the mathematical details are beyond the scope of this book. We will mention
    two of these, which we will encounter in the next section. The first method is
    known as **Variational Expectation Maximization** (**VEM**) and is a variant of
    the well-known **Expectation Maximization** (**EM**) algorithm. The second is
    known as Gibbs sampling, and is a method based on **Markov Chain Monte Carlo**
    (**MCMC**).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a tutorial on the EM algorithm and VEM, we recommend *The Variational Approximation
    for Bayesian Inference* by *Dimitris G. Tzikas* and others in the November 2008
    issue of the *IEEE Signal Processing Magazine*. For Gibbs sampling, there is a
    1992 article in *The American Statistician* by *George Casella*, entitled *Explaining
    the Gibbs Sampler*. Both are readable, but quite technical. A more thorough tutorial
    on Gibbs sampling is *Gibbs Sampling for the Uninitiated* by *Philip Resnik* and
    *Eric Hardisty*. This last reference is less mathematically demanding and can
    be found online at [http://www.cs.umd.edu/~hardisty/papers/gsfu.pdf](http://www.cs.umd.edu/~hardisty/papers/gsfu.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: Modeling the topics of online news stories
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To see how topic models perform on real data, we will look at two datasets containing
    articles originating from BBC News during the period of 2004-2005\. The first
    dataset, which we will refer to as the *BBC dataset*, contains 2,225 articles
    that have been grouped into five topics. These are *business*, *entertainment*,
    *politics*, *sports*, and *technology*.
  prefs: []
  type: TYPE_NORMAL
- en: The second dataset, which we will call the *BBCSports dataset*, contains 737
    articles only on sports. These are also grouped into five categories according
    to the type of sport being described. The five sports in question are *athletics*,
    *cricket*, *football*, *rugby*, and *tennis*. Our objective will be to see if
    we can build topic models for each of these two datasets that will group together
    articles from the same major topic.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both BBC datasets were presented in a paper by *D. Greene* and *P. Cunningham*,
    entitled *Producing Accurate Interpretable Clusters from High-Dimensional Data*
    and published in the proceedings of the 9th European Conference on *Principles
    and Practice of Knowledge Discovery in Databases (PKDD'05)* in October 2005.
  prefs: []
  type: TYPE_NORMAL
- en: The two datasets can be found at [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).
    When downloaded, each dataset is a folder containing a few different files. We
    will use the variables `bbc_folder` and `bbcsports_folder` to store the paths
    of these folders on our computer.
  prefs: []
  type: TYPE_NORMAL
- en: Each folder contains three important files. The file with the extension `.mtx`
    is essentially a file containing a term document matrix in sparse matrix form.
    Concretely, the rows of the matrix are terms that can be found in the articles
    and the columns are the articles themselves. An entry *M[i,j]* in this matrix
    contains the number of times the term corresponding to row *i* was found in the
    document corresponding to column *j*. A term document matrix is thus a transposed
    document term matrix, which we encountered in [Chapter 8](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 8. Dimensionality Reduction"), *Probabilistic Graphical Models*. The
    specific format used to store the matrix in the file is a format known as the
    **Matrix Market format**, where each line corresponds to a nonempty cell in the
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, when working with text such as news articles, we need to perform
    some preprocessing steps, such as stop-word removal, just as we did when using
    the `tm` package in our example on sentiment analysis in [Chapter 8](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 8. Dimensionality Reduction"), *Probabilistic Graphical Models*. Fortunately
    for us, the articles in these datasets have already been processed so that they
    have been stemmed; stop words have been removed, as have any terms that appear
    fewer than three times.
  prefs: []
  type: TYPE_NORMAL
- en: In order to interpret the term document matrix, the files with the extension
    `.terms` contain the actual terms, one per line, which are the row names of the
    term document matrix. Similarly, the document names that are the column names
    in the term document matrix are stored in the files with the extension `.docs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create variables for the paths to the three files that we need for
    each dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to load data into R from a file in Market Matrix format, we can use
    the `readMM()` function from the `Matrix` R package. This function loads the data
    and stores it into a sparse matrix object. We can convert this into a term document
    matrix that the `tm` package can interpret, using the `as.TermDocumentMatrix()`
    function in the `tm` package. Aside from the matrix object that is the first argument
    to that function, we also need to specify the `weighting` parameter. This parameter
    describes what the numbers in the original matrix correspond to. In our case,
    we have raw term frequencies, so we specify the value `weightTf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load the terms and document identifiers from the two remaining files
    and use these to create appropriate row and column names respectively for the
    term document matrices. We can use the standard `scan()` function to read files
    with a single entry per line and load the entries into vectors. Once we have a
    term vector and a document identifier vector, we will use these to update the
    row and column names for the term document matrix. Finally, we''ll transpose this
    matrix into a document term matrix, as this is the format we will need for subsequent
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the document term matrices for our two datasets ready. We can see
    that there are roughly twice as many terms for the BBC dataset as there are for
    the BBCSports dataset, and the latter also has about a third of the number of
    documents, so it is a much smaller dataset. Before we build our topic models,
    we must also create the vectors containing the original topic classification of
    the articles. If we examine the document IDs, we can see that the format of each
    document identifier is `<topic>.<counter>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a vector with the correct topic assignments, we simply need to strip
    out the last four characters of each entry. If we then convert the result into
    a factor, we can see how many documents we have per topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This shows that the BBC dataset is fairly even in the distribution of its topics.
    In the BBCSports data, however, we see that there are roughly twice as many articles
    on football than the other four sports.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each of our two datasets, we will now build some topic models using the
    package `topicmodels`. This is a very useful package as it allows us to use data
    structures created with the `tm` package to perform topic modeling. For each dataset,
    we will build the following four different topic models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LDA_VEM`: This is an LDA model trained with the **Variational Expectation
    Maximization** (**VEM**) method. This method automatically estimates the `α`Dirichlet
    parameter vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LDA_VEM_α`: This is an LDA model trained with VEM, but the difference here
    is that the `α`Dirichlet parameter vector is not estimated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LDA_GIB`: This is an LDA model trained with Gibbs sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CTM_VEM`: This is an implementation of the **Correlated Topic Model** (**CTM**)
    model trained with VEM. Currently, the `topicmodels` package does not support
    training this method with Gibbs sampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To train an LDA model, the `topicmodels` package provides us with the `LDA()`
    function. We will use four key parameters for this function. The first of these
    specifies the document term matrix for which we want to build an LDA model. The
    second of these, `k`, specifies the target number of topics we want to have in
    our model. The third parameter, `method`, allows us to select which training algorithm
    to use. This is set to `VEM` by default, so we only need to specify this for our
    `LDA_GIB` model , which uses Gibbs sampling.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, there is a `control` parameter, which takes in a list of parameters
    that affect the fitting process. As there is an inherent random component involved
    in the training of topic models, we can specify a `seed` parameter in this list
    in order to make the results reproducible. Additionally, this is where we can
    specify whether we want to estimate the *α*Dirichlet parameter. This is also where
    we can include parameters for the Gibbs sampling procedure, such as the number
    of omitted Gibbs iterations at the start of the training procedure (`burnin`),
    the number of omitted in-between iterations (`thin`), and the total number of
    Gibbs iterations (`iter`). To train a CTM model, the `topicmodels` package provides
    us with the `CTM()` function, which has a similar syntax to the `LDA()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this knowledge, we''ll define a function that creates a list of four
    trained models given a particular document term matrix, the number of topics required,
    and the seed. For this function, we have used some standard values for the aforementioned
    training parameters with which the reader is encouraged to experiment, ideally
    after investigating the references provided for the two optimization methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll now use this function to train a list of models for the two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To get a sense of how the topic models have performed, let's first see whether
    the five topics learned by each model correspond to the five topics to which the
    articles were originally assigned. Given one of these trained models, we can use
    the `topics()` function to get a vector of the most likely topic chosen for each
    document.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function actually takes a second parameter, *k*, which is by default set
    to `1` and returns the top *k* topics predicted by the model. We only want one
    topic per model in this particular instance. Having found the most likely topic,
    we can then tabulate the predicted topics against the vector of labeled topics.
    These are the results for the `LDA_VEM` model for the BBC dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Looking at this table, we can see that topic 5 corresponds almost exclusively
    to the *sports* category. Similarly, topics 4 and 3 seem to match the *politics*
    and *business* categories respectively. Unfortunately, models 1 and 2 both contain
    a mixture of *entertainment* and *technology* articles, and as a result this model
    hasn't really succeeded in distinguishing between the categories that we want.
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be clear that, in an ideal situation, each model topic should match
    to one gold topic (we often use the adjective *gold* to refer to the correct or
    labeled value of a particular variable. This is derived from the expression *gold
    standard*, which refers to a widely accepted standard). We can repeat this process
    on the `LDA_GIB` model, where the story is different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Intuitively, we feel that this topic model is a better match to our original
    topics than the first, as evidenced by the fact that each model topic selects
    articles from primarily one gold topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'A rough way to estimate the quality of the match between a topic model and
    our target vector of topics is to say that the largest value in every row corresponds
    to the gold topic assigned to the model topic represented by that row. Then, the
    total accuracy is the ratio of these maximum row values over the total number
    of documents. In the preceding example, for the `LDA_GIB` model, this number would
    be *(471+506+371+399+364)/2225 = 2111/2225= 94.9 %*. The following function computes
    this value given a model and a vector of gold topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this notion of accuracy, let''s see which model performs better in our
    two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For the BBC dataset, we see that the `LDA_GIB` model significantly outperforms
    the others and the `CTM_VEM` model is significantly worse than the LDA models.
    For the BBCSports dataset, all the models perform roughly the same, but the `LDA_VEM`
    model is slightly better.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way to assess the quality of a model fit is by computing the log likelihood
    of the data given the model, remembering that the larger this value, the better
    the fit. We can do this with the `logLik()` function in the `topicmodels` package,
    which suggests that the best model is the LDA model trained with Gibbs sampling
    in both cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Model stability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It turns out that the random component of the optimization procedures involved
    in fitting these models often has a significant impact on the model that is trained.
    Put differently, we may find that if we use different random number seeds, the
    results may sometimes change appreciably.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, we would like our model to be **stable**, which is to say that we
    would like the effect of the initial conditions of the optimization procedure,
    which are determined by a random number seed, to be minimal. It is a good idea
    to investigate the effect of different seeds on our four models by training them
    on multiple seeds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we used a sequence of five consecutive seeds and trained our models on
    both datasets five times. Having done this, we can investigate the accuracy of
    our models for the various seeds. If the accuracy of a method does not vary by
    a large degree across the seeds, we can infer that the method is quite stable
    and produces similar topic models (although, in this case, we are only considering
    the most prominent topic per document).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: On both datasets, we can clearly see that Gibbs sampling results in a more stable
    model and, in the case of the BBC dataset, it is also the clear winner in terms
    of accuracy. Gibbs sampling generally tends to produce more accurate models but,
    even though it was not readily apparent on these datasets, it can become significantly
    slower than VEM methods once the dataset becomes large.
  prefs: []
  type: TYPE_NORMAL
- en: The two LDA models trained with variational methods exhibit scores that vary
    within a roughly 10 % range on both datasets. On both datasets, we see that `LDA_VEM`
    is consistently better than `LDA_VEM_a` by a small amount. This method also produces,
    on average, better accuracy among all models in the BBCSports dataset. The CTM
    model is the least stable of all the models, exhibiting a high degree of variance
    on both datasets. Interestingly, though, the highest performance of the CTM model
    across the five iterations performs marginally worse than the best accuracy possible
    using the other methods.
  prefs: []
  type: TYPE_NORMAL
- en: If we see that our model is not very stable across a few seeded iterations,
    we can specify the `nstart` parameter during training, which specifies the number
    of random restarts that are used during the optimization procedure. To see how
    this works in practice, we have created a modified `compute_model_list()` function
    that we named `compute_model_list_r()`, which takes in an extra parameter, `nstart`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other difference is that the `seed` parameter now needs a vector of seeds
    with as many entries as the number of random restarts. To deal with this, we will
    simply create a suitably sized range of seeds starting from the one provided.
    Here is our new function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use this function to create a new model list. Note that using random
    restarts means we are increasing the amount of time needed to train, so these
    next few commands will take some time to complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note that, even after using only five random restarts, the accuracy of the models
    has improved. More importantly, we now see that using random restarts has overcome
    the fluctuations that the CTM model experiences, and as a result it is now performing
    almost as well as the best model in each dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the number of topics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this predictive task, the number of different topics was known beforehand.
    This turned out to be very important because it is provided as an input to the
    functions that trained our models. The number of topics might not be known when
    we are using topic modeling as a form of exploratory analysis where our goal is
    simply to cluster documents together based on the similarity of their topics.
  prefs: []
  type: TYPE_NORMAL
- en: This is a challenging question and bears some similarity to the general problem
    of selecting the number of clusters when we perform clustering. One proposed solution
    to this problem is to perform cross-validation over a range of different numbers
    of topics. This approach will not scale well at all when the dataset is large,
    especially since training a single topic model is already quite computationally
    intensive when we factor, issues such as random restarts.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A paper that discusses a number of different approaches for estimating the number
    of topics in topic models is *Reconceptualizing the classification of PNAS articles*
    by *Edoardo M. Airoldi* and others. This appears in the *Proceedings of the National
    Academy of Sciences*, volume 107, 2010.
  prefs: []
  type: TYPE_NORMAL
- en: Topic distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We saw in the description of the generative process that we use a Dirichlet
    distribution to sample a multinomial distribution of topics. In the `LDA_VEM`
    model, the *αk* parameter vector is estimated. Note that, in all cases, a symmetric
    distribution is used in this implementation so that we are only estimating the
    value of *α*, which is the value that all the *α[k]* parameters take on. For LDA
    models, we can investigate which value of this parameter is used with and without
    estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, when we estimate the value of *α*, we obtain a much lower value
    of *α* than we use by default, indicating that, for both datasets, the topic distribution
    is thought to be peaky. We can use the `posterior()` function in order to view
    the distribution of topics for each model. For example, for the `LDA_VEM` model
    on the BBC dataset, we find the following distributions of topics for the first
    few articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following plot is a histogram of the posterior probability of the most
    likely topic predicted by our four models. The `LDA_VEM` model assumes a very
    peaky distribution, whereas the other models have a wider spread. The `CTM_VEM`
    model also has a peak at very high probabilities but, unlike `LDA_VEM`, the probability
    mass is spread over a wide range of values. We can see that the minimum probability
    for the most likely topic is 0.2 because we have five topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Topic distributions](img/00189.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another approach to estimating the smoothness of the topic distributions is
    to compute the *model entropy*. We will define this as the average entropy of
    all the topic distributions across the different documents. Smooth distributions
    will exhibit higher entropy than peaky distributions. To compute the entropy of
    our model, we will define two functions. The function `compute_entropy()` computes
    the entropy of a particular topic distribution of a document, and the `compute_model_mean_entropy()`
    function computes the average entropy across all the different documents in the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Using these functions, we can compute the average model entropies for the models
    trained on our two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: These results are consistent with what the preceding plots show, which is that
    the `LDA_VEM` model, which is the peakiest, has a much lower entropy than the
    other models.
  prefs: []
  type: TYPE_NORMAL
- en: Word distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just as with the previous section, where we looked at the distribution of topics
    across different documents, we are often also interested in understanding the
    most important terms that are frequent in documents that are assigned to the same
    topic. We can see the *k* most frequent terms of the topics of a model using the
    function `terms()`. This takes in a model and a number specifying the number of
    most frequent terms that we want retrieved. Let''s see the 10 most important words
    per topic in the `LDA_GIB` model of the BBC dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, given this list of word stems, one could easily guess which of
    the five topic labels we should assign to each topic. A very handy way to visualize
    frequent terms in a collection of documents is through a **word cloud**. The R
    package `wordcloud` is useful for creating these. The function `wordcloud()` allows
    us to specify a vector of terms followed by a vector of their frequencies, and
    this information is then used for plotting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, we will have to do some manipulation on the document term matrices
    in order to get the word frequencies by topic so that we can feed them into this
    function. To that end, we''ve created our own function `plot_wordcloud()`, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Our function takes in a model, a document term matrix, an index of a topic,
    and the number of most frequent terms that we want to display in the word cloud.
    We begin by first computing the most frequent terms for the model by topic, as
    we did earlier. We also compute the most probable topic assignments. Next, we
    subset the document term matrix so that we obtain only the cells involving the
    terms we are interested in and the documents corresponding to the topic with the
    index that we passed in as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'From this reduced document term matrix, we sum over the columns to compute
    the frequencies of the most frequent terms, and finally we plot the word cloud.
    We''ve used this function to plot word clouds for the topics in the BBC dataset
    using the `LDA_GIB` model and 25 words per topic. This is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Word distributions](img/00190.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: LDA extensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Topic models are an active area of research, and as a result several extensions
    for the LDA model have been proposed. We will briefly mention two of these. The
    first is the **supervised LDA** model, an implementation of which can be found
    in the `lda` R package. This is a more direct way to model a response variable
    with the standard LDA method and would be a good next step for investigating the
    application discussed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: A second interesting extension is the **author-topic model**. This is designed
    to add an extra step in the generative process to account for authorship information
    and is a good model to use when building models that summarize or predict the
    writing habits and topics of authors.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The standard reference for supervised LDA is the paper *Supervised Topic Models*
    by *David M. Blei* and *Jon D. McAuliffe*. This was published in 2007 in the journal
    *Neural Information Processing Systems*. For the author-topic model, consult the
    paper entitled *The Author-Topic Model for Authors and Documents* by *Michal Rosen-Zvi*
    and others. This appears in the proceedings of the *20th conference on Uncertainty
    In Artificial Intelligence*.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling tweet topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning and natural language processing, a *topic model* is a type
    of statistical model used to discover the abstract topics that occur in a collection
    of documents. A good example or use case to illustrate this concept is *Twitter*.
    Suppose we could analyze an individual's (or an organization's) tweets to discover
    any overriding trend. Let's look at a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a Twitter account, you can perform this exercise pretty easily (you
    can then apply the same process to an archive of tweets you want to focus on and/or
    model). First, we need to create a tweet archive file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under **Settings**, you can submit a request to receive your tweets in an archive
    file. Once it''s ready, you''ll get an email with a link to download it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling tweet topics](img/00191.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'And then save your file locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling tweet topics](img/00192.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have a data source to work with, we can move the tweets into a
    list *object* (we''ll call it *x*) and then convert that into an R data frame
    object (df1):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling tweet topics](img/00193.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The tweets were first converted to a *data frame* before using the R `tm` package
    to convert them to a *corpus* or Corpus collection (of text documents) object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling tweet topics](img/00194.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we convert the Corpus to a *Document-Term Matrix* object with the following
    code. This creates a *mathematical matrix* that describes the *frequency of terms*
    that occur in a collection of documents, in this case, our collection of tweets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling tweet topics](img/00195.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Word clouding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After building a document-term matrix (shown earlier), we can more easily show
    the importance of the words found within our tweets with a *word cloud* (also
    known as a tag cloud). We can do this using the R package `wordcloud`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Word clouding](img/00196.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, let''s generate the word cloud visual:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Word clouding](img/00197.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Seems like there may be a theme involved here! The word cloud shows us that
    the words **south** and **carolinas** are the most important words.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter was devoted to learning about topic models; after sentiment analysis
    on movie reviews, this was our second foray into working with real-life text data.
    This time, our predictive task was classifying the topics of news articles on
    the web. The primary technique for topic modeling on which we focused was LDA.
    This derives its name from the fact that it assumes that the topic and word distributions
    that can be found inside a document arise from hidden multinomial distributions
    that are sampled from Dirichlet priors. We saw that the generative process of
    sampling words and topics from these multinomial distributions mirrors many of
    the natural intuitions that we have about this domain; however, it signally fails
    to account for correlations between the various topics that can co-occur inside
    a document.
  prefs: []
  type: TYPE_NORMAL
- en: In our experiments with LDA, we saw that there is more than one way to fit an
    LDA model, and in particular we saw that the method known as Gibbs sampling tends
    to be more accurate, even if it often is more computationally expensive. In terms
    of performance, we saw that, when the topics in question are quite distinct from
    each other, such as the topics in the BBC dataset, we got very high accuracy in
    our topic prediction.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, however, when we classified documents with topics that are
    more similar to each other, such as the different sports documents in the BBCSports
    dataset, we saw that this posed more of a challenge and our results were not quite
    as high. In our case, another factor that probably played a role is that both
    the documents and the available features were much fewer in number than in the
    BBCSports dataset. Currently, an increasing number of variations on LDA are being
    researched and developed in order to deal with limitations in both performance
    and training speed.
  prefs: []
  type: TYPE_NORMAL
- en: As an interesting exercise, we also downloaded an archive of tweets and used
    R commands to create a **document-term matrix** object, which we then used as
    an input for creating a word cloud object that visualized the words found within
    the tweets.
  prefs: []
  type: TYPE_NORMAL
- en: Topic models can be viewed as a form of clustering, and this was our first glimpse
    in to this area. In the next chapter on recommendation systems, we will delve
    more deeply into the field of clustering in order to understand the way in which
    websites such as Amazon are able to make product recommendations by predicting
    which products a shopper is most likely to be interested in based on their previous
    shopping history and the shopping habits of similar shoppers.
  prefs: []
  type: TYPE_NORMAL
