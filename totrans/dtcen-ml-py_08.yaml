- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Techniques for Identifying and Removing Bias
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the realm of data-centric machine learning, the pursuit of unbiased and
    fair models is paramount. The consequences of biased algorithms can range from
    poor performance to ethically questionable decisions. It is important to recognize
    that bias can manifest at two key stages of the machine learning pipeline: data
    and model. While model-centric approaches have garnered significant attention
    in recent years, this chapter sheds light on the equally crucial data-centric
    strategies that are often overlooked.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the intricacies of bias in machine learning,
    emphasizing why data-centricity is a fundamental aspect of bias mitigation. We
    will explore real-world examples from finance, human resources, and healthcare,
    where the failure to address bias has had or could have far-reaching implications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The bias conundrum
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of bias
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data-centric imperative
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bias conundrum
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bias in machine learning is not a novel concern. It is deeply rooted in the
    data we collect and the algorithms we design. Bias can arise from historical disparities,
    societal prejudices, and even the human decisions made during data collection
    and annotation. Ignoring bias, or addressing it solely through model-centric techniques,
    can lead to detrimental outcomes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following scenarios, which illustrate the multifaceted nature
    of bias:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias in finance**: In the financial sector, machine learning models play
    a pivotal role in credit scoring, fraud detection, and investment recommendations.
    However, if historical lending practices favor certain demographic groups over
    others, these biases can seep into the data used to train models. As a result,
    marginalized communities may face unfair lending practices, perpetuating socioeconomic
    inequalities.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias in human resources**: The use of AI in human resources has gained momentum
    for recruitment, employee performance assessment, and even salary negotiations.
    If job postings or historical hiring data are biased toward specific genders,
    ethnicities, or backgrounds, the AI systems can inadvertently perpetuate discrimination,
    leading to a lack of diversity and inclusion in the workplace.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias in healthcare**: In healthcare, diagnostic algorithms are relied upon
    for disease detection and treatment recommendations. If training data predominantly
    represents certain demographics, individuals from underrepresented groups may
    receive suboptimal care or face delayed diagnoses. The implications can be life-altering,
    underscoring the need for equitable healthcare AI.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have covered areas where bias can arise, in the next section, we
    will cover the types of bias prevalent in machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Types of bias
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, there are generally five categories of bias that warrant
    attention. Although the list provided isn't exhaustive, these categories represent
    the most prevalent types of bias, each of which can be further subdivided.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Easy to identify bias
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some types of bias can be easy to identify using active monitoring and by conducting
    analysis. These include the following.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Reporting bias
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This type of bias occurs when the data producers, data annotators, or data capturers
    miss out on important elements, which results in data not being representative
    of the real world. For instance, a healthcare business might be interested in
    patients’ sentiments toward a health program; however, the data annotators may
    decide to focus on negative and positive sentiments, and sentiments that were
    neutral may be underrepresented. A model trained on such data will be good at
    identifying positive and negative sentiments but may fail to accurately predict
    neutral sentiments. This type of bias can be identified with active monitoring,
    where predictions on live data show drift from predictions on training data. To
    reduce reporting bias, it is important to articulate data points needed for the
    problem at the beginning phase of ML system design. It is also important to ensure
    that data used for training represents real-life data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Automation bias
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This type of bias occurs due to relying on automated ways of data collection
    and assuming data capture is not error-prone. As AI is becoming better, reliance
    on humans has significantly reduced, and hence it is often assumed that if an
    automated system is put in place, then it will magically solve all problems. Using
    active monitoring can help identify this type of bias, where the model accuracy
    is highly poor on real-life data. Another way of identifying this is by using
    humans to annotate labels and measure human performance versus algorithmic performance.
    As covered in [*Chapter 6*](B19297_06.xhtml#_idTextAnchor089)*, Techniques for
    Programmatic Labeling in Machine Learning* systems fail and can lead to missing
    data or inaccurate data. AI is as good as the data it was trained on. One of the
    key principles of data-centricity is to keep humans in the loop; hence, when building
    automated systems, we should ensure the data generated represents real-world scenarios
    and data is diverse rather than overrepresented or underrepresented.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Selection bias
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This type of bias occurs when data selected for training the model is not representative
    of real-life data. This bias can take multiple forms:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '**Coverage bias**: This bias can occur when data is not collected in a representative
    manner. This can happen when the business and practitioners are focused on outcomes,
    and ignore data points that do not contribute to the outcome. In healthcare, insurance
    companies may want to predict hospital admissions; however, data on people churning
    on insurance companies and using competitive insurance products, or data on people
    not claiming benefits to go into the hospital may not be readily available and,
    as a result, these groups of people may not be represented well in the training
    data.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖偏差**：这种偏差可能发生在数据不是以代表性的方式收集时。这可能发生在企业和从业者专注于结果，而忽略了那些对结果没有贡献的数据点。在医疗保健领域，保险公司可能想要预测医院的入院人数；然而，关于那些在保险公司频繁更换并使用竞争性保险产品的人，或者那些没有申请福利而进入医院的人的数据可能并不容易获得，因此，这些人群在训练数据中可能没有得到很好的代表。'
- en: '**Participation bias**: This bias can occur due to participants opting out
    of data collection processes, leading to one group being overrepresented over
    another group. For example, a model is trained to predict churn using survey data,
    where 80% of people who have moved to a new competitor are unlikely to respond
    to the survey, and their data is highly underrepresented in the sample.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参与偏差**：这种偏差可能由于参与者选择退出数据收集过程，导致某一群体在另一群体中过度代表。例如，一个模型被训练用来根据调查数据预测流失率，其中80%已经转移到新竞争对手的人不太可能回应调查，他们的数据在样本中高度代表性不足。'
- en: '**Sampling bias**: This bias can occur when data collectors do not use proper
    randomization methods in data collection processes. For example, a model is trained
    to predict health scores based on survey data; instead of targeting the population
    at random, the surveyors chose 80% of people who are highly engaged with their
    health and are more likely to respond, compared to the rest of the responders.
    In the health industry, people who are more engaged with their health are likely
    to have a better health score than people who are less engaged, thus leading to
    a biased model toward healthy people.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽样偏差**：这种偏差可能发生在数据收集者没有在数据收集过程中使用适当的随机化方法时。例如，一个模型被训练用来根据调查数据预测健康分数；调查者没有随机地针对人口，而是选择了80%高度关注健康并且更有可能回应的人，与那些不太可能回应的其他受访者相比。在健康产业中，那些更关注健康的人可能比那些不太关注健康的人有更好的健康分数，这可能导致模型偏向于健康人群。'
- en: Selection biases are difficult to identify; however, if drift is noted frequently
    in the data and highly frequent retraining is done to ensure the model quality
    does not degrade, then it will be a good time to investigate and check whether
    the data captured represents real-life data. Two types of analysis in regression
    modeling can help to identify this bias. One is conducting bivariate analysis,
    where a sensitive variable can be represented on the *x* axis and the target variable
    can be put on the *y* axis. If there is a strong association between the two variables,
    then it is important to evaluate the difference in the association metric at training
    time and post-scoring time. If the difference is significant, it is quite possible
    that the data used for training is not representative of real life. The second
    technique is to use multivariate analysis by comparing the possible outcomes when
    data is not fully represented and when data is fully represented. This can be
    done by separating the subgroups into data points that were included and the ones
    that were excluded at training time. We can run a multi-regression model by creating
    an independent variable group by labeling group 1 for data included and group
    2 for data not included. We will add this new variable as a feature to the model
    training and then compare whether there is a significant difference in outcome
    between groups 1 and 2\. If there is a difference, then the data collection was
    biased.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 选择偏差难以识别；然而，如果数据中频繁出现漂移，并且频繁重新训练以确保模型质量不下降，那么就是调查和检查所捕获的数据是否代表真实世界数据的好时机。回归模型中的两种分析方法可以帮助识别这种偏差。一种是进行双变量分析，其中敏感变量可以表示在*x*轴上，目标变量可以放在*y*轴上。如果两个变量之间存在强烈的关联，那么在训练时间和评分后时间评估关联指标差异是很重要的。如果差异显著，那么用于训练的数据很可能不代表真实生活。第二种技术是通过比较数据未完全代表和完全代表时的可能结果来进行多元分析。这可以通过将子群体分为训练时包含的数据点和排除的数据点来完成。我们可以通过创建一个独立变量组来运行多回归模型，将组1标记为包含的数据，组2标记为未包含的数据。然后我们将这个新变量作为特征添加到模型训练中，并比较组1和组2之间是否存在显著的差异。如果存在差异，那么数据收集存在偏差。
- en: In classification examples, we can use false positive rates and/or false negative
    rates across sensitive subgroups to see whether these are vastly different. If
    they are, data is likely to be biased toward one or a couple of subgroups. Another
    metric that can be used to check whether bias persists is demographic parity,
    which is a probability comparison of the likelihood of selection from one subgroup
    over another. If the ratio of probabilities between the higher selection subgroup
    and the lower selection subgroup is below 0.8, it is quite likely data is biased
    and does not have enough representative samples. It is recommended to check multiple
    metrics to understand bias in the data and the algorithm.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类示例中，我们可以通过查看敏感子群体中的假阳性率和/或假阴性率来观察这些值是否差异很大。如果差异很大，数据很可能偏向于一个或几个子群体。另一个可以用来检查偏差是否持续存在的指标是人口统计学上的平等性，它是对从一个子群体到另一个子群体选择可能性的概率比较。如果高选择子群体与低选择子群体之间的概率比率低于0.8，那么数据很可能存在偏差，并且代表性样本不足。建议检查多个指标以了解数据及算法中的偏差。
- en: To treat such biases, it is recommended, when collecting data, to use techniques
    such as stratified sampling to ensure that different groups are represented proportionally
    in the dataset. Now that we have covered types of bias that are easy to identify,
    in the next section, we will cover some types of bias that are difficult to identify.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这类偏差，建议在收集数据时采用分层抽样等技巧，以确保数据集中不同群体按比例代表。现在我们已经介绍了易于识别的偏差类型，在下一节中，我们将讨论一些难以识别的偏差类型。
- en: Difficult to identify bias
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 难以识别的偏差
- en: Some types of bias can be challenging because they are biases that individuals
    may not be consciously aware of. These biases often operate at a subconscious
    level and can influence perceptions, attitudes, and behaviors. In order to capture
    these, organizations and individuals need processes and training to ensure these
    biases are not present in the workspace. Once it has been identified that there
    was bias in the data collection process or data labeling process, then sensitive
    labels can be defined to measure and check whether the model is free from bias
    or whether there is an acceptable level of bias in the model. Some of these biases
    are described next.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Group attribution bias
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This type of bias occurs when attribution is done for the entire data based
    on some data points. This usually occurs when the data creators have preconceived
    biases about the types of attributes present in the data. This type of bias can
    take two forms:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '**In-group bias**: This is a preconceived bias where associated data points
    resonate with the data creator, hence those data points get a favorable outcome
    – for example, if a data engineering manager is designing a resume selection algorithm
    where they believe someone doing a Udacity nanodegree is qualified for the role.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Out-group homogeneity bias**: This is a preconceived bias where data points
    do not resonate with the data creator, hence those data points get a negative
    outcome – for example, if a data engineering manager is designing a resume selection
    algorithm where they believe someone not doing a Udacity nanodegree is not qualified
    for the role.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on to another type of bias that is difficult to identify.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Implicit bias
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This type of bias occurs when data creators make assumptions about the data
    based on their own mental models and personal experiences. For example, a sentiment
    analysis model trained on airline food service review data is likely to associate
    the word “okay” with neutral sentiment. However, some regions of the world use
    the word “okay” to signify a positive sentiment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Bias in machine learning can take many forms; hence, we categorize these biases
    into two main types, **easy to identify** biases and **difficult to identify**
    biases. Practitioners are known to take a model-centric approach to treat these
    biases, where modifying the algorithm or using bias-friendly algorithms has been
    considered acceptable practice. In the next section, we will take an alternative
    view to the model-centric approach: the data-centric approach.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: The data-centric imperative
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Addressing bias in machine learning necessitates a holistic approach, with data-centric
    strategies complementing model-centric techniques. Data-centricity involves taking
    proactive steps to curate, clean, and enhance the dataset itself, thus minimizing
    the bias that models can inherit. By embracing data-centric practices, organizations
    can foster fairness, accountability, and ethical AI.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this chapter, we will explore a spectrum of data-centric
    strategies that empower machine learning practitioners to reduce bias. These include
    data resampling, augmentation, cleansing, feature selection, and more. Real-world
    examples will illustrate the tangible impact of these strategies in the domains
    of finance, human resources, and healthcare.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: If data is fairly and accurately captured or created, then it is quite likely
    algorithms will be mostly free from bias. However, the techniques we will cover
    in this chapter are post-data creation, where ML practitioners have to work with
    provided data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will discuss some data-centric strategies for
    reducing bias in machine learning without changing the algorithm. These can be
    referred to as data debiasing techniques.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Sampling methods
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sampling methods such as undersampling and oversampling address class imbalances.
    Undersampling reduces majority class instances, whereas oversampling augments
    minority class examples. Integrating both mitigates overfitting and information
    loss, balancing class representation effectively. These methods can be combined
    with outlier treatment and Shapley values to further sample the data where harder-to-classify
    or harder-to-estimate data points can be removed or introduced to enhance the
    fairness metrics. These techniques are covered next.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In undersampling, we remove random or strategic subsets of overrepresented data
    points to balance class distributions – deleting data points from overrepresented
    classes where examples are difficult to classify or at random is a commonly used
    technique. We can also use outlier removal for regression tasks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In oversampling, we add random or strategic subsets of underrepresented data
    points to provide more examples to the algorithm. We can duplicate or generate
    synthetic data points for underrepresented classes to balance class distributions.
    We can use techniques such as the **Synthetic Minority Oversampling Technique**
    (**SMOTE**) and random oversampling for classification tasks. Alternatively, we
    can utilize outlier or edge case addition/removal for regression tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Combination of undersampling and oversampling
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These cover techniques such as `SMOTEENN` or `SMOTETomek`, where `SMOTE` is
    utilized to oversample the minority class. Techniques such as **Edited Nearest
    Neighbors** (**ENN**) or Tomek Links are used to remove the examples that are
    difficult to classify or agree on using nearest neighbors, as these points are
    close to the boundary and there is no clear separation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection for oversampling and undersampling the data
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This covers using an anomaly detection technique to identify data points that
    are edge cases, and then these points can be reintroduced multiple times or removed
    so the model can get a better signal or become more generalized.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Use of Shapley values for oversampling and undersampling data
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This covers using Shapley values to oversample or undersample data. Shapley
    values quantify feature importance by assessing each feature’s contribution to
    a model’s prediction. High Shapley values highlight influential features. Removing
    instances with high Shapley values but wrong predictions might enhance model accuracy
    by reducing outliers. Oversampling instances with high Shapley values and correct
    predictions can reinforce the model’s understanding of crucial patterns, potentially
    improving performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Other data-centric techniques
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides sampling methods, there are other data-centric techniques that can be
    used to reduce bias, some of which have been covered in previous chapters, and
    some we will utilize in the case study. The three main ones are described next.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Data cleansing
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This includes removing missing data, where the inclusion of missing data can
    lead to unfair outcomes. These techniques were covered in [*Chapter 5*](B19297_05.xhtml#_idTextAnchor070),
    *Techniques for Data Cleaning*, where missing data was classified as “missing
    not at random.”
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This includes selecting specific features or eliminating features that will
    reduce bias. This may mean identifying a variable that is highly associated with
    a sensitive variable and outcome label, and removing such indirect variables or
    removing sensitive variables.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature engineering offers potent tools to mitigate model bias. Techniques such
    as re-encoding sensitive attributes, creating interaction terms, or introducing
    proxy variables enable models to learn without direct access to sensitive information.
    Feature selection and dimensionality reduction methods trim irrelevant or redundant
    features, fostering fairer and more robust models. Additionally, generating synthetic
    features or utilizing domain-specific knowledge helps improve models with a better
    understanding of data, aiding in fairer decision-making while improving overall
    model performance and reducing bias. We will create a synthetic variable, “Interest,”
    in the example to show how the model is biased toward one subgroup over another.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered data-centric methods, in the next section, we will
    describe the problem statement, and walk through examples of how we can identify
    and reduce bias in real life.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The challenge at hand centers on uncovering and addressing potential bias within
    a dataset pertaining to credit card defaults in Taiwan. Acquired from the UC Irvine
    Machine Learning Repository ([https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients)),
    this dataset comprises information from 30,000 credit card clients over a six-month
    span, including demographic factors such as gender, marital status, and education.
    The key concern is whether these demographic features introduce bias into a decision
    tree classifier trained on all available features, with a specific focus on gender-related
    bias. The overarching objective of this example is to not only identify but also
    mitigate any biased outcomes through the application of data-centric techniques.
    By reevaluating the algorithm’s performance using fairness metrics, the example
    aims to shed light on the real-world implications of bias in financial decision-making,
    particularly how these biases can impact individuals based on gender and other
    demographic factors, potentially leading to unequal treatment in credit assessments
    and financial opportunities. Addressing and rectifying such biases is crucial
    for promoting fairness and equity in financial systems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use two key metrics to check the fairness of the algorithm:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '**Equalized odds difference**: This metric compares the false negative rate
    and false positive rate across the sensitive variables, then takes the maximum
    difference between the false negative rate and false positive rate. For instance,
    on the test set, the false positive rate among men and women is 0.3 and 0.2 (difference
    of 0.1), whereas the false negative rate among men and women is 0.15 and 0.12
    (difference of 0.03). Since the difference is larger on the false positive rate,
    the equalized odds will be 0.1.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Demographic parity ratio**: This metric measures whether the predictions
    made by a model are independent of a sensitive variable, such as race, gender,
    or age. Given this is a ratio, it measures the ratio of a lower selection rate
    to that of a higher selection rate. A ratio of 1 means that demographic parity
    is achieved, whereas below 0.8 usually means that the algorithm is highly biased
    toward one group of individuals over the others.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a description of the features in the dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '`LIMIT_BAL`: Amount of the given credit in NT dollars, including both the individual
    consumer credit and their family (supplementary) credit.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sex`: Gender (1 = male; 2 = female).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Education X3`: Education (1 = graduate school; 2 = university; 3 = high school;
    4 = others)'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Marriage X4`: Marital status (1 = married; 2 = single; 3 = others)'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age X5`: Age of the person in years'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PAY_0- PAY_5; X6 - X11: History of past payments, which includes the past monthly
    payment records (from April to September 2005), where PAY_0X6 = the repayment
    status in September, PAY_2; X7 = the repayment status in August 2005; ... PAY_6;
    X11 = the repayment status in April 2005\. The measurement scale for the repayment
    status is -1 = amount paid duly; 1 = payment delay for 1 month; 2 = payment delay
    for 2 months; ... ; 8 = payment delay for 8 months; 9 = payment delay for 9 months,
    and so on.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BILL_AMT1 . BILL_AMT6; X12-X17: Bill statement amount (in Taiwan dollars).
    BILL_AMT1;X12 means the amount on the credit card statement as of September 2005,
    while BILL_AMT6;X17 means the amount on the credit card statement as of April
    2005.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PAY_AMT1-PAY_AMT6; X18-X23: Amount of payments made based on the previous month''s
    bill statement. PAY_AMT1;X18 means amount paid in September 2005, while PAY_AMT6;X23
    means amount paid on April 2005.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default payment next month`: Whether a person defaulted on the next month’s
    payment (Yes = 1, No = 0), in 2005'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To import the dataset, you need to install `pandas`. We will also use the `os`
    library to navigate the path and store the dataset. This library is native to
    Python. We will call the `loan_dataset.csv` file and save it in the same directory,
    from where we will run this example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The file takes a couple of seconds to a minute based on internet speed, so
    when we run this example for the first time, the file will be stored locally.
    However, on the subsequent runs, with the help of the `os` library, we will check
    that the file exists, else download it. We will rename two variables: `PAY_0`
    to `PAY_1`, and also `default payment next month` to `default`. We don’t believe
    the `ID` column will be useful for machine learning, hence we will drop it:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we load the file from the local directory into a DataFrame called `dataset`.
    There are 30,000 rows and 24 columns including the target variable:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we run the `dataset.info()` method to check whether there are any missing
    values or wrongly encoded columns:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Output of the dataset.info() method](img/B19297_08_1.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Output of the dataset.info() method
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t have any missing data; however, three categorical columns (`SEX`,
    `EDUCATION`, and `MARRIAGE`) have integer data types, which we may have to convert
    to strings. Since values in `SEX` might be ordinal, we will first remap them to
    `1` and `0`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we rerun `dataset.info()`, we will see that the data type for the three
    columns is now `category`; we can now one-hot encode them. We exclude `SEX` from
    one-hot encoding since a person is either a male or female (in this dataset) and
    that information can be captured in one column. We will also extract `SEX` and
    store it in another variable, `A`, and separate the target variable and independent
    features. Next, we create a mapping for values in the `SEX` feature to be used
    for analysis and visualization, to help interpret the results, so `1` will be
    mapped to `male` values and `0` will be mapped to `female` values. We store this
    mapping in the `A_str` variable:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, let’s load all the required libraries.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Loading the libraries
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run the example, you will need the following additional libraries:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn` (scikit-learn) for data preprocessing and fitting the models'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` to calculate some metrics and do some data wrangling'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imblearn` for over and undersampling'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fairlearn` to calculate bias and fairness scores'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shap` to visualize the interpretations of the model'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We load all the libraries at the start:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we split the dataset into `train` and `test`, using `train_test_split`,
    and assign 20% of the data to test. We also split `A_str` into `A_train` and `A_test`,
    so we can calculate fairness scores on test data:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we create the decision tree classifier pipeline and train the algorithm
    with the sensitive features:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we calculate the ROC score and extract the predictions. We also visualize
    the confusion matrix:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code generates the following confusion matrix:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Output confusion matrix](img/B19297_08_2.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Output confusion matrix
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: In order to check whether the algorithm is fair or not, we will first calculate
    false positive and false negative rates, and then compare those across male and
    female cohorts on the test dataset to see whether there are big differences between
    the two cohorts.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we have created two functions to calculate a false
    positive rate and a false negative rate. We have further created a dictionary
    of fairness metrics, in which we use the false positive rate and false negative
    rate, alongside a balanced accuracy metric from scikit-learn. We have then created
    a list of fairness metrics and stored them in a variable for easy access:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have also created a function to report the differences between male and
    female cohorts on the fairness metrics. We first create a DataFrame called `metricframe`
    using the convenience function from `fairlearn` called `MetricFrame`. It takes
    in true labels, predictions, and sensitive feature values, along with a dictionary
    of metrics to report on. We then leverage the `.by_group` property to report on
    fairness metrics for each cohort. Within the function, we also report on `equalised_odds_difference`
    and `demographic_parity_ratio` from the `fairlearn` library to understand the
    overall fairness of the model:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We now run the function and calculate the fairness scores. It is evident that
    the model is quite similar in male and female cohorts since false positive rates
    and false negative rates are similar among the cohorts. Since the difference in
    the false positive rate is larger than the false negative rate, the equalized
    odds difference is the same as the difference between the false positive rate
    of the two groups. We can also see that the demographic parity ratio is above
    0.8, which means that both cohorts are quite likely to get selected for a positive
    outcome:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will display the following output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Fairness scores](img/B19297_08_3.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Fairness scores
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate bias in the dataset, we may need to generate a synthetic variable
    that correlates with a real-world scenario where, based on history, a cohort is
    treated more unfairly. First, we compare the default rate across males and females
    in the training dataset. We then add the synthetic noise:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Given that the male default is higher than the female, we can replicate a biased
    scenario where applicants with lower default rates will have lower interest rates,
    but applicants with higher default rates will have higher interest rates imposed
    by the bank. Let’s assume the bank managers believe males are more likely to default,
    and instead of generalizing the scenario, the bank decides to charge higher interest
    rates to males.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: To mimic this scenario, we will introduce a new feature, `Interest_rate`, following
    a Gaussian distribution. The mean will be 0 where someone hasn’t defaulted, but
    will be 2 times 1 where someone has defaulted. We also set the standard deviation
    to 2 for males and 1 for females.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the synthetic Gaussian distribution, we use the `numpy.random.normal`
    method, with a seed of `42` for reproducibility:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now that we have added the noise, we retrain the algorithm with the interest
    variable and recalculate the fairness metrics. We first split the data, then retrain
    and recalculate the fairness metrics. We resplit the data into `train` and `test`,
    as shown previously, and retrain the algorithm. Once retrained, we calculate the
    impact.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see, in the following code, that by adding the synthetic interest variable,
    we have improved the ROC metric:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'It is clear from the following output that we now have a more biased algorithm,
    based on equalized odds. The false negative rate is quite high in males, which
    means that more males who are unlikely to pay back to the bank are likely to be
    given a loan, and if this model was productionized, this could result in unfair
    outcomes:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will print the following information:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Fairness scores](img/B19297_08_4.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Fairness scores
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the bias, we will apply the first data-centric debiasing technique
    under the feature selection by removing the sensitive variable from the algorithm.
    This can be done by retraining the algorithm without the `SEX` variable.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the dataset is biased toward one gender due to a higher variation
    in interest rates, it is recommended in the real world that data engineers and
    data scientists work with domain experts and data producers to reduce this bias
    in the dataset. For instance, instead of using `SEX` to determine the interest
    rate, other features could be used, such as payment history, credit history, and
    income. In the training step, we can drop the `SEX` variable:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'From the following output, we can see that by removing the `SEX` variable,
    the ROC score has dropped from 0.846 to 0.839:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Looking at the following fairness metrics, it is obvious that when the outcome
    is biased based on the cohort of data, removing the variable from the training
    can debias the algorithm. The false negative rate in `male` has decreased, whereas,
    in `female`, it has increased; however, the algorithm is more fair compared to
    when the `SEX` variable was used. The equalized odds have dropped from 0.18 to
    0.07, but the demographic parity ratio has reduced, which means one group has
    more chance of getting a loan than the other:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Fairness metrics](img/B19297_08_5.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Fairness metrics
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will show you how to apply undersampling techniques to ensure the outcome
    variable is balanced.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: AllKNN undersampling method
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start with the AllKNN algorithm from the `imblearn` package, and then
    try the instant hardness algorithm. Since the algorithms use KNN under the hood,
    which is a distance-based measure, we need to ensure that we scale the features
    using the scikit-learn `StandardScaler` method. We will first scale the variables,
    then run the sampling algorithm, and then train the decision tree. We will run
    the algorithm with 5 k cross-validation, and ensure the function returns the model
    trained. Cross-validation will be scored on `roc_auc` and balanced accuracy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: We will first try an undersampling technique, `AllKNN`, from `imblearn`. This
    algorithm does not aim at balancing majority and minority classes; however, it
    removes instances that are harder to classify from the majority class. It does
    that iteratively where, first, the model is trained on the entire dataset. Then,
    in the prediction step of the majority class, if a disagreement occurs between
    any of the neighbors about the predicted outcome, the data point is removed from
    the majority class. In the first iteration, a 1-KNN model is trained and some
    samples are removed, and then in the next iteration, a 2-KNN model is trained,
    and in the following iteration, a 3-KNN model is trained. Usually, the algorithm
    (by default) will end at the 3-KNN iteration; however, the practitioner can choose
    more iterations, and the algorithm will not stop until the number of samples between
    the majority and minority class becomes the same or a maximum number of iterations
    is reached – whichever happens earlier.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first define the scaler and sampler method:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we create a pipeline object and pass the scaler, sampler, and estimator
    to the pipeline:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then we pass the training data and run the cross-validation. We set the cross-validation
    method to return the estimator (pipeline) by setting `return_estimator=True`,
    so that we can use it to make predictions on the test data:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we print the mean and standard deviation of ROC and balanced accuracy
    from the cross-validation step, returned from prediction results in each step,
    where at each step, four folds were used on training and the prediction was made
    on the fifth fold:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can see that by removing hard examples using the undersampling technique,
    `roc_auc` on `test` data bumped from 0.839 in the previous step to 0.85:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we calculate the fairness metrics. Although the false negative rate has
    decreased for both males and females, the false positive rate has increased, and
    the equalized odds difference has also increased from the previous step. This
    might be because cases with male samples that were difficult to classify have
    been removed:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Figure 8.6 – Fairness metrics](img/B19297_08_6.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Fairness metrics
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore the impact on the fairness metrics by introducing hard cases.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Instance hardness undersampling method
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, the instance hardness method focuses on samples that are
    harder to classify, which are usually at the boundary or overlap with other classes.
    Usually, this depends on the algorithm used (as some algorithms are better at
    some hard cases than others) and the level of overlap between the classes. For
    such samples, the learning algorithm will usually show the low probability prediction
    on the hard cases, which means the lower the probability, the higher the instance
    hardness. Under the hood, the method has the capability to retain the right number
    of samples, based on the class imbalance.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, we will define the algorithm, and the algorithm will be passed
    on to the instance hardness step. We will then define the instance hardness undersampling
    method, with three-fold cross-validation.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the decision tree estimator. Finally, we combine the steps
    in the pipeline with scaling the dataset, then undersampling the data, and finally,
    training the model. When the pipeline is defined, we run the cross-validation
    similar to the previous pipeline:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Both `AllKNN` and `InstanceHardness` returned similar cross-validation results:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The ROC slightly bumped from 0.85 to 0.854 on the `test` data when using the
    instance hardness method:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The fairness metrics are quite similar to the previous undersampling technique,
    and probably due to similar reasons, where, by removing difficult cases, the model
    is unable to deal with predicting difficult `male` cases. However, in both undersampling
    methods, the equalized odds have increased, compared to the feature selection
    step. Also, the demographic parity ratio is still under 0.8, which means one subclass
    of gender is more likely to be selected over another when predicting `default`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '![Figure 8.7 – Fairness metrics](img/B19297_08_7.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Fairness metrics
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at oversampling methods.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling methods
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way of improving model performance and fairness metrics is by introducing
    additional examples. The next two oversampling techniques, `SMOTE` and `ADASYN`,
    were introduced in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111), *Using Synthetic
    Data in Data-Centric Machine Learning*, hence we will not cover the details behind
    the algorithm. We will use these techniques in the context of improving fairness
    metrics by adding additional examples, in the hope that the model is able to learn
    better with additional data points.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: For each of the methods, we will first scale the dataset, add additional minority
    class examples, and then train the model. We will print the cross-validation scores
    and the `test` ROC score, as well as fairness metrics.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given that we used this algorithm in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111),
    *Using Synthetic Data in Data-Centric Machine Learning*, we will dive straight
    into the code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The validation metrics and the `test` ROC score show poorer results compared
    to the undersampling methods covered previously. In the next step, we explore
    the fairness metrics:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Figure 8.8 – Fairness metrics](img/B19297_08_8.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Fairness metrics
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: The fairness metrics are better in comparison with the undersampling methods
    – that is, the difference between false positive and false negative rates is reduced
    between men and women and, based on the demographic parity ratio, the model is
    more likely to select both types of gender applicants for loan default. In the
    next section, we will use the `ADASYN` algorithm and compare it with `SMOTE` and
    other undersampling methods.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: ADASYN
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly to the `SMOTE` method, we covered `ADASYN` in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111),
    *Using Synthetic Data in Data-Centric Machine Learning*, hence we will dive straight
    into the code, in which we oversample the minority class:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The validation metrics and `test` ROC score are slightly below the `SMOTE`
    results and undersampling methods. Now, let’s review the fairness metrics:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![Figure 8.9 – Fairness metrics](img/B19297_08_9.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Fairness metrics
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: The equalized odds are slightly higher for `ADASYN`, whereas demographic parity
    is slightly better when compared to `SMOTE`, and both oversampling techniques
    guarantee higher fairness over undersampling methods, but slightly poorer ROC
    performance.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: We have now seen that, despite balancing the classes, model fairness is compromised,
    and it is mostly `male` examples where the model is making more errors. So, in
    the next section, we will randomly introduce some additional `male` examples where
    the model is misclassifying positive cases.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling plus misclassified examples at random
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will first balance the dataset using `ADASYN` and avoid undersampling techniques
    since we want to retain hard cases that are difficult to classify. We then train
    the model and identify `male` cases that the model believes should be positive
    but wrongly classifies as negative. We then randomly select 10% of these cases,
    add them back to the training dataset, and retrain the model with the same algorithm.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: At the end, we review the model metrics and fairness metrics on the `test` data.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'We utilize oversampling and reintroduce misclassified data points at random.
    Let’s run the pipeline with the `ADASYN` oversampling method:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we identify the examples from the training dataset where the model is
    making errors on the male population – that is, examples where the model predicts
    false negatives. We first subset the data associated with males and then run predictions
    over this data:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then we subset this data where the `true` label is `1` but model predictions
    are `0`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We randomly select 10% of the values and add them to the `X_train` dataset.
    We leverage the `.sample` method, and this random selection is done with replacement:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Next, we add this 10% to `X_train` and `y_train` and create a new dataset:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Then, we train the algorithm on this new dataset and print out the validation
    metrics and the `test` ROC score:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Compared to the oversampling section, the validation metrics are quite similar,
    as is the `test` ROC score. Next, we review the fairness metrics to check whether
    they have improved:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![Figure 8.10 – Fairness metrics](img/B19297_08_10.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Fairness metrics
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: By adding some false negative `male` examples, we can see that the equalized
    odds have improved slightly to 0.098 and the demographic ratio has also improved,
    increasing to 0.85\. We believe that even better results can be achieved if we
    add false positive examples and false negative examples and combine these with
    undersampling and oversampling techniques.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate this, we will iterate over four undersampling techniques (`AllKNN`,
    `RepeatedEditedNearestNeighbours`, `InstanceHardnessThreshold`, and `Tomek`),
    two oversampling techniques (`SMOTE` and `ADASYN`), and two combinations of over
    and undersampling techniques (`SMOTEENN` and `SMOTETomek`). How these algorithms
    work is outside the scope of this example. Instead, the goal is to demonstrate
    how these data techniques can lead to better selection and a generalized model
    with slightly poorer performance, but higher fairness.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: We will now develop a mechanism where we first train the algorithm and then
    add false positive examples and false negative examples. Once the examples are
    added, we run the pipeline by sampling the dataset, using the previous algorithms.
    We'll record fairness outcomes and the ROC score to find the technique that best
    fosters a balance between fairness and performance in our algorithm.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create a dictionary with a configuration of each of the aforementioned
    sampling techniques so we can iterate over it. We can call this AutoML for the
    sampling technique:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we create two functions that take the training dataset, model, column,
    and its subset value to help create random samples. The following function will
    sample false positives:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'And this function samples false negatives. By default, both methods will add
    10% random examples with replacement:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we create a function that calculates test metrics post-data improvements.
    The function takes the test data and estimator and returns model metrics and fairness
    metrics:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Next, we create a pipeline that will sample the dataset and then create random
    false positive `male` and false negative `male` examples. We then combine these
    into the training data, one at a time, and retrain the same algorithm. We then
    calculate the metrics and store them in a list called `results` with columns.
    Each iteration adds false negative and false positive examples with model performance
    and fairness metrics. We then use this list to compare the results across algorithms.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for creating the pipeline is pretty lengthy. Please refer to GitHub
    for the full code: [https://github.com/PacktPublishing/Data-Centric-Machine-Learning-with-Python/tree/main/Chapter%208%20-%20Techniques%20for%20identifying%20and%20removing%20bias](https://github.com/PacktPublishing/Data-Centric-Machine-Learning-with-Python/tree/main/Chapter%208%20-%20Techniques%20for%20identifying%20and%20removing%20bias)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a DataFrame called `df` and add all the `test` metrics so we
    can compare which method reaped the best model performance and fairness metrics:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let’s sort the DataFrame based on equalized odds:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We can see that when the dataset was sampled with Tomek Links, where difficult
    cases were removed from the boundary and combined with additional false positive
    `male` training samples, this resulted in the best equalized odds of 0.075; however,
    a demographic parity of 0.8 was not achieved. When the SMOTETomek technique was
    used in combination with false negative `male` examples, the model achieved a
    0.088 equalized odds ratio, which was the best among the sampling methods, and
    the model also achieved a high demographic parity ratio.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Resulting output dataset](img/B19297_08_11.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Resulting output dataset
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling with anomalies
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous steps, we learned that by adding poorly classified examples
    to the training dataset, we were able to improve model fairness. In the next step,
    instead of choosing samples at random, we will utilize an algorithm that identifies
    anomalies and then we add these anomalies to the training dataset as an oversampling
    mechanism.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a pipeline to oversample the minority class:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we extract the oversampled data. There is no reason why undersampling
    or no sampling could not have been chosen. Once the oversampled data is extracted,
    we then scale it back to the original feature space:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, we train the isolation forest to identify 10% of anomalies. To do that,
    we set the contamination to `0.1`. We then fit the model on resampled data, and
    run prediction on this data. We store the results in a column called `IF_anomaly`
    and add it to the resampled dataset. We then extract these anomalies, as isolation
    forest labels with a value of `-1`:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, we add these additional data points to the original dataset and train
    the decision tree model. Once the model is fitted, we calculate the ROC score
    on the `test` data. We can see that this is 0.82:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we calculate the fairness metrics. Based on the following results, we
    can say that the model trained in the previous section produced better fairness
    and demographic parity ratio scores:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Fairness metrics](img/B19297_08_12.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Fairness metrics
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have utilized various examples of undersampling and oversampling
    data, including reintroducing random misclassified examples and anomalies, in
    the next section, we will utilize an advanced technique, where we will be more
    selective with which examples to add and which examples to remove, to further
    reduce bias in the algorithm.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Shapley values to detect bias, oversample, and undersample data
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will utilize Shapley values to identify examples where the
    model struggles to make the correct prediction. We will use the impact score to
    either add, eliminate, or use a combination of both to improve the fairness metrics.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '**SHAP** (which stands for **Shapley Additive exPlanations**) is a model-agnostic
    approach in machine learning that is built on the principles of game theory. It
    helps study the importance of the feature and the feature interaction on the final
    outcome by assigning it a score, similar to how it would be done in a game where
    each player’s contribution at a given time is calculated in the output of the
    score.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Shapley values can help provide global importance (the overall impact of the
    feature on all the predictions), but also local importance (the impact of each
    feature on a single outcome). It can also help understand the direction of impact
    – that is, whether a feature has a positive impact or a negative impact.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Hence, there are a lot of use cases for Shapley values in machine learning,
    such as bias detection, local and global model debugging, model auditing, and
    model interpretability.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'We use Shapley values in this section to understand the model and feature impacts
    on the outcomes. We leverage the impacts of these features and identify where
    the model is likely to make the most mistakes. We then apply two techniques: one
    to remove these rows from the data and the other to oversample the data with these
    rows.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import SHAP and then train the decision tree model on the oversampled
    dataset. At the end of the step, we have a model and oversampled `X` and `y` samples.
    We include the `SEX` variable in the training data to see whether Shapley values
    can help us detect bias. First, we need to resplit the data into `train` and `test`
    sets, as done in the previous sections:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next, we define the SHAP tree explainer, by providing the decision tree model
    and then extract the Shapley values for the `train` set using the `.``shap_values`
    method:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Let’s extract the first row of Shapley values, for class 0\. The array contains
    the contribution of each feature value to decide the final output. Positive values
    mean the corresponding features have a positive impact on predicting the output
    as class 0, while negative values negatively contribute toward predicting class
    0:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This will print out the following array:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Resulting output array](img/B19297_08_13.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Resulting output array
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we generate a summary plot for class label 0:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This will generate the following plot:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – SHAP values](img/B19297_08_14.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – SHAP values
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: The red dots represent the high value of a feature while the blue dots represent
    the low value of the corresponding feature. The *x* axis denotes the Shapley value,
    where the positive value means the data point has a positive impact in predicting
    class 0, whereas the negative value means the data point for the corresponding
    feature negatively affects the prediction for class 0\. If we look at *Figure
    8**.13,* it is quite evident that high interest rates and male customers negatively
    affect the prediction of class 0\. Shapley values do indicate a model bias toward
    male customers.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we generate a summary plot for class label 1:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This will generate the following summary plot:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – SHAP summary plot](img/B19297_08_15.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – SHAP summary plot
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: In comparison with the summary plot for class 0, high interest rates and male
    customers positively impact defaulting on the loan – that is, if you are a male
    and previously had a higher interest rate, you are likely to default on the loan.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'We previously learned that by removing the `SEX` feature from model training,
    the model becomes fairer, and Shapley values are clearly indicated using summary
    plots. Now, we extract the Shapley values by training the new model without the
    `SEX` feature. We then score the training data to first identify all the rows
    corresponding to false negatives and false positives. We then calculate the sum
    of Shapley values for each row where the model made errors, and then hold out
    the ones with the lowest impact. We run two experiments: first, we undersample
    the training dataset and calculate fairness metrics, and second, we oversample
    the training dataset to give a better signal to the model and recalculate fairness
    metrics.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s train the model without the `SEX` feature:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, we extract Shapley values:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We score the training data, calculate predictions, and store these in `Y_pred`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then we will check the sum of the Shapley value for class 0 and class 1 at
    index 0, and print the corresponding prediction and `true` value:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The model predicted `0`. Next, we extract the Shapley values where the model
    made a mistake. For that, we use the list comprehension with zip functionality.
    The first value of the array will be the index location of the data point so we
    know which Shapley value is associated with which row. The next values are in
    order of prediction, the `true` value, the sum of Shapley values for the row for
    class 0, and the sum of Shapley values for class 1\. Once we have extracted those,
    we create a DataFrame and store the values in `df`, and we sample through the
    DataFrame to see five values. We use a random seed for reproducibility:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'This generate the following output:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – DataFrame displaying the Shapley values that made a mistake](img/B19297_08_16.jpg)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – DataFrame displaying the Shapley values that made a mistake
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: For index `7915`, the Shapley values are close, meaning feature contributions
    to the model prediction are closer to `0` for each class, whereas for index `4255`,
    the Shapley values are far apart from `0` and features are discriminatory in predicting
    each class.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Given that we can extract the SHAP impact of features for each class, we want
    to know the rows where the Shapley value impact is highest so we can eliminate
    such data points from training; where the impact is low and quite close to the
    boundary, we can oversample the data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the force plots for index `4255`, for the expected class `0`, the
    model is likely to predict `1`, given that `f(x)` is quite low, and the model
    wrongly predicts `1`, whereas the force plot for the expected class `1` shows
    an `f(x)` value of `0.7`. Such data points can be eliminated from the dataset:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'This will generate the following plot:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Force plot for class 0](img/B19297_08_17.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Force plot for class 0
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the force plot for class `1`:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'This will display the following output:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Force plot for class 1](img/B19297_08_18.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – Force plot for class 1
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we calculate the Shapley impact of row index `4255` since it’s a false
    positive prediction. The row index is at the `422` location in the DataFrame.
    We take the absolute value of the Shapley impact and, where the Shapley impact
    is highest and the prediction is wrong, those values can be eliminated to improve
    model performance:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, we create a function that calculates the Shapley impact. We are interested
    in those rows where a single feature has a minimum of `0.2` Shapley impact. First,
    we get the absolute impact of each feature in an array, and then we extract the
    maximum value. If the maximum value is greater than `0.2`, we proceed with that
    row. Next, we check where the prediction doesn’t match the actual value, and for
    such rows, we extract the SHAP impact:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We create a holdout dataset, where `X_train` will be further divided into training
    and validation datasets. We leverage 80% for training and 20% for validation:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Then we resample the dataset using `SMOTETomek`, which was the best sampling
    method for fairness and performance, and performed by adding difficult examples
    back to the dataset. Once the dataset is resampled, we train the standard decision
    tree as in the previous steps, and calculate the ROC score on the holdout validation
    dataset:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We calculate the fairness and performance metrics on the testing dataset. The
    equalized odds are high but the demographic parity ratio is within the accepted
    range:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Next, we calculate the fairness metrics:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'This will print out the following metrics:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Fairness metrics](img/B19297_08_19.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Fairness metrics
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'We extract the Shapley values using the SHAP explainer. We have ensured that
    the `SEX` feature is removed:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The fairness metrics demonstrate that the gap between false negative rates is
    higher between the subclass of men and women, hence we focus on reducing false
    negative cases for males using Shapley values.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we extract Shapley values where the model predicts class
    `0,` but the `true` value is `1`. Hence, we are interested in Shapley values for
    class `1`, as a high SHAP impact for class `1` where the model made an error could
    be a data point that the model is unable to make a correct prediction on:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We are also interested in those data points where both Shapley values and model
    prediction agree with the actual values. Hence, we focus on those data points
    where the model rightly predicts class `0` for `male` data points. Once we have
    extracted those, we focus on high-impact Shapley values for class `0` so we can
    oversample the dataset with those, such that the model can get a better signal
    for such data points:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Next, we sort the false negative Shapley values so we can extract the high-impact
    data points:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Similar to the preceding, we are interested in true negative high-impact Shapley
    values, so we sort the list according to high-impact Shapley values:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now that we have extracted and sorted the Shapley values for false negative
    and true negative `male` data points, we pick the top 100 data points to eliminate
    from the false negative list and pick the top 100 data points from the true negative
    list to add back to the training data. The top 100 data points from the true negative
    list will be shuffled and only 50 data points from there will be added at random
    with a replacement strategy. We encourage practitioners to try another ratio for
    shuffling. Once the data points are identified for elimination and reintroduction
    to the final training set, we update the training data. These are named `X_train_final`
    and `y_train_final`:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next, we train the updated training data and calculate fairness metrics and
    performance metrics on the `test` data. It is evident that the gap between false
    negative rates has reduced, the equalized odds ratio has improved to 0.082, and
    the ROC score has slightly improved from the previous step, from 0.825 to 0.826:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We recalculate the fairness metrics:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output is as follows:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Fairness metrics](img/B19297_08_20.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – Fairness metrics
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have determined that by using Shapley values we can identify data
    points that are difficult to classify and easy to classify, we can build an automatic
    mechanism to iterate over the data points such that we can reach a better fairness
    score than previously.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a range of percentages to iterate so we can leverage and sort
    through the top data points as a percentage of top data points to eliminate and
    reintroduce. We will leverage the NumPy `linspace` method to create a list of
    percentage values to iterate. We choose 10 values from `0.05` through `0.5` (5
    to 50 percent). We call this list `perc_points_to_eliminate`:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: We iterate through these percentages and repeat the preceding step where we
    eliminated some values and reintroduced some values. However, this time, instead
    of 100, we use percentages to remove the top percent of data points or introduce
    the top percent of data points.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: We also create an empty list of data so, for each iteration, we capture the
    percentage of data points eliminated or reintroduced, false negative and false
    positive rates on test data, equalized odds ratio, and demographic parity ratio.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have iterated over all the values – 10*10 iterations, we store them
    in a DataFrame to see how many data points need to be removed and how many added
    to lead to the best fairness metrics:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Next, we create a DataFrame called `df_shapley` for the metadata for each iteration
    and sort it by equalized odds ratio:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'This will output the following DataFrame:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure  8.21 – The df_shapley DataFrame after sorting](img/B19297_08_21.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – The df_shapley DataFrame after sorting
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: It’s evident that when the top 25% of false negative data points are removed
    and 30% of the top true negative data points are reintroduced, the model can achieve
    an equalized odds ratio of 0.074 with an optimum demographic parity ratio score
    of 0.85.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we extract the top percentages and train the final model:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'We train the model and calculate the fairness and model metrics. We can see
    that the false negative rate for `female` has increased but the gap between `male`
    and `female` has reduced, and the false positive rate for `male` has reduced.
    The ROC score achieved is 0.82, but the model is much fairer based on two fairness
    metrics:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Finally, we calculate the fairness metrics.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'This will print out the following:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.22 – Fairness metrics](img/B19297_08_22.jpg)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 – Fairness metrics
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored different data-centric techniques for reducing bias
    by improving data quality, we encourage you to experiment with the previous techniques
    and try a combination of these. Once you have exhausted these data-centric approaches,
    we encourage you use some model-centric approaches, such as utilizing algorithms
    that are fairness aware and trying ensembling methods, AutoML, or iterating through
    your own list of algorithms.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided an extensive exploration of the pervasive challenge of
    bias in machine learning. It started by explaining various forms of bias inherent
    in machine learning models and examined their impact on different industries.
    The emphasis was on recognizing, monitoring, and mitigating bias, underscoring
    the importance of collecting data with minimal selection and sampling bias.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: The central theme advocated a data-centric imperative over a model-centric one
    in addressing bias. Techniques such as oversampling, undersampling, feature selection
    enhancement, and anomaly detection were explored for bias rectification. Shapley
    values play a crucial role in bias identification, emphasizing the removal of
    examples with misaligned high Shapley values and the reintroduction of data points
    with replacement to improve ratios. Stratification of misclassified examples based
    on sensitive variables such as `SEX` was outlined for targeted bias correction.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: The chapter concluded by highlighting the significance of refining and balancing
    datasets concerning sensitive variables as a foundational step. It suggested progressing
    toward model-centric approaches, such as ensembling and fairness algorithms, once
    the dataset itself has been improved. These subsequent model-centric strategies
    aim to enhance both performance and fairness metrics, establishing a foundation
    for more generalized and equitable AI models.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: This comprehensive approach strives to create a balanced dataset as a precursor
    to applying model-centric techniques, promoting performance and fairness in AI
    systems.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
