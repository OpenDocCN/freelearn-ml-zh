- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Techniques for Identifying and Removing Bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the realm of data-centric machine learning, the pursuit of unbiased and
    fair models is paramount. The consequences of biased algorithms can range from
    poor performance to ethically questionable decisions. It is important to recognize
    that bias can manifest at two key stages of the machine learning pipeline: data
    and model. While model-centric approaches have garnered significant attention
    in recent years, this chapter sheds light on the equally crucial data-centric
    strategies that are often overlooked.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore the intricacies of bias in machine learning,
    emphasizing why data-centricity is a fundamental aspect of bias mitigation. We
    will explore real-world examples from finance, human resources, and healthcare,
    where the failure to address bias has had or could have far-reaching implications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The bias conundrum
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data-centric imperative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bias conundrum
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bias in machine learning is not a novel concern. It is deeply rooted in the
    data we collect and the algorithms we design. Bias can arise from historical disparities,
    societal prejudices, and even the human decisions made during data collection
    and annotation. Ignoring bias, or addressing it solely through model-centric techniques,
    can lead to detrimental outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following scenarios, which illustrate the multifaceted nature
    of bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias in finance**: In the financial sector, machine learning models play
    a pivotal role in credit scoring, fraud detection, and investment recommendations.
    However, if historical lending practices favor certain demographic groups over
    others, these biases can seep into the data used to train models. As a result,
    marginalized communities may face unfair lending practices, perpetuating socioeconomic
    inequalities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias in human resources**: The use of AI in human resources has gained momentum
    for recruitment, employee performance assessment, and even salary negotiations.
    If job postings or historical hiring data are biased toward specific genders,
    ethnicities, or backgrounds, the AI systems can inadvertently perpetuate discrimination,
    leading to a lack of diversity and inclusion in the workplace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias in healthcare**: In healthcare, diagnostic algorithms are relied upon
    for disease detection and treatment recommendations. If training data predominantly
    represents certain demographics, individuals from underrepresented groups may
    receive suboptimal care or face delayed diagnoses. The implications can be life-altering,
    underscoring the need for equitable healthcare AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have covered areas where bias can arise, in the next section, we
    will cover the types of bias prevalent in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Types of bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, there are generally five categories of bias that warrant
    attention. Although the list provided isn't exhaustive, these categories represent
    the most prevalent types of bias, each of which can be further subdivided.
  prefs: []
  type: TYPE_NORMAL
- en: Easy to identify bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some types of bias can be easy to identify using active monitoring and by conducting
    analysis. These include the following.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This type of bias occurs when the data producers, data annotators, or data capturers
    miss out on important elements, which results in data not being representative
    of the real world. For instance, a healthcare business might be interested in
    patients’ sentiments toward a health program; however, the data annotators may
    decide to focus on negative and positive sentiments, and sentiments that were
    neutral may be underrepresented. A model trained on such data will be good at
    identifying positive and negative sentiments but may fail to accurately predict
    neutral sentiments. This type of bias can be identified with active monitoring,
    where predictions on live data show drift from predictions on training data. To
    reduce reporting bias, it is important to articulate data points needed for the
    problem at the beginning phase of ML system design. It is also important to ensure
    that data used for training represents real-life data.
  prefs: []
  type: TYPE_NORMAL
- en: Automation bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This type of bias occurs due to relying on automated ways of data collection
    and assuming data capture is not error-prone. As AI is becoming better, reliance
    on humans has significantly reduced, and hence it is often assumed that if an
    automated system is put in place, then it will magically solve all problems. Using
    active monitoring can help identify this type of bias, where the model accuracy
    is highly poor on real-life data. Another way of identifying this is by using
    humans to annotate labels and measure human performance versus algorithmic performance.
    As covered in [*Chapter 6*](B19297_06.xhtml#_idTextAnchor089)*, Techniques for
    Programmatic Labeling in Machine Learning* systems fail and can lead to missing
    data or inaccurate data. AI is as good as the data it was trained on. One of the
    key principles of data-centricity is to keep humans in the loop; hence, when building
    automated systems, we should ensure the data generated represents real-world scenarios
    and data is diverse rather than overrepresented or underrepresented.
  prefs: []
  type: TYPE_NORMAL
- en: Selection bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This type of bias occurs when data selected for training the model is not representative
    of real-life data. This bias can take multiple forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coverage bias**: This bias can occur when data is not collected in a representative
    manner. This can happen when the business and practitioners are focused on outcomes,
    and ignore data points that do not contribute to the outcome. In healthcare, insurance
    companies may want to predict hospital admissions; however, data on people churning
    on insurance companies and using competitive insurance products, or data on people
    not claiming benefits to go into the hospital may not be readily available and,
    as a result, these groups of people may not be represented well in the training
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Participation bias**: This bias can occur due to participants opting out
    of data collection processes, leading to one group being overrepresented over
    another group. For example, a model is trained to predict churn using survey data,
    where 80% of people who have moved to a new competitor are unlikely to respond
    to the survey, and their data is highly underrepresented in the sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling bias**: This bias can occur when data collectors do not use proper
    randomization methods in data collection processes. For example, a model is trained
    to predict health scores based on survey data; instead of targeting the population
    at random, the surveyors chose 80% of people who are highly engaged with their
    health and are more likely to respond, compared to the rest of the responders.
    In the health industry, people who are more engaged with their health are likely
    to have a better health score than people who are less engaged, thus leading to
    a biased model toward healthy people.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selection biases are difficult to identify; however, if drift is noted frequently
    in the data and highly frequent retraining is done to ensure the model quality
    does not degrade, then it will be a good time to investigate and check whether
    the data captured represents real-life data. Two types of analysis in regression
    modeling can help to identify this bias. One is conducting bivariate analysis,
    where a sensitive variable can be represented on the *x* axis and the target variable
    can be put on the *y* axis. If there is a strong association between the two variables,
    then it is important to evaluate the difference in the association metric at training
    time and post-scoring time. If the difference is significant, it is quite possible
    that the data used for training is not representative of real life. The second
    technique is to use multivariate analysis by comparing the possible outcomes when
    data is not fully represented and when data is fully represented. This can be
    done by separating the subgroups into data points that were included and the ones
    that were excluded at training time. We can run a multi-regression model by creating
    an independent variable group by labeling group 1 for data included and group
    2 for data not included. We will add this new variable as a feature to the model
    training and then compare whether there is a significant difference in outcome
    between groups 1 and 2\. If there is a difference, then the data collection was
    biased.
  prefs: []
  type: TYPE_NORMAL
- en: In classification examples, we can use false positive rates and/or false negative
    rates across sensitive subgroups to see whether these are vastly different. If
    they are, data is likely to be biased toward one or a couple of subgroups. Another
    metric that can be used to check whether bias persists is demographic parity,
    which is a probability comparison of the likelihood of selection from one subgroup
    over another. If the ratio of probabilities between the higher selection subgroup
    and the lower selection subgroup is below 0.8, it is quite likely data is biased
    and does not have enough representative samples. It is recommended to check multiple
    metrics to understand bias in the data and the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: To treat such biases, it is recommended, when collecting data, to use techniques
    such as stratified sampling to ensure that different groups are represented proportionally
    in the dataset. Now that we have covered types of bias that are easy to identify,
    in the next section, we will cover some types of bias that are difficult to identify.
  prefs: []
  type: TYPE_NORMAL
- en: Difficult to identify bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some types of bias can be challenging because they are biases that individuals
    may not be consciously aware of. These biases often operate at a subconscious
    level and can influence perceptions, attitudes, and behaviors. In order to capture
    these, organizations and individuals need processes and training to ensure these
    biases are not present in the workspace. Once it has been identified that there
    was bias in the data collection process or data labeling process, then sensitive
    labels can be defined to measure and check whether the model is free from bias
    or whether there is an acceptable level of bias in the model. Some of these biases
    are described next.
  prefs: []
  type: TYPE_NORMAL
- en: Group attribution bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This type of bias occurs when attribution is done for the entire data based
    on some data points. This usually occurs when the data creators have preconceived
    biases about the types of attributes present in the data. This type of bias can
    take two forms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**In-group bias**: This is a preconceived bias where associated data points
    resonate with the data creator, hence those data points get a favorable outcome
    – for example, if a data engineering manager is designing a resume selection algorithm
    where they believe someone doing a Udacity nanodegree is qualified for the role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Out-group homogeneity bias**: This is a preconceived bias where data points
    do not resonate with the data creator, hence those data points get a negative
    outcome – for example, if a data engineering manager is designing a resume selection
    algorithm where they believe someone not doing a Udacity nanodegree is not qualified
    for the role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on to another type of bias that is difficult to identify.
  prefs: []
  type: TYPE_NORMAL
- en: Implicit bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This type of bias occurs when data creators make assumptions about the data
    based on their own mental models and personal experiences. For example, a sentiment
    analysis model trained on airline food service review data is likely to associate
    the word “okay” with neutral sentiment. However, some regions of the world use
    the word “okay” to signify a positive sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bias in machine learning can take many forms; hence, we categorize these biases
    into two main types, **easy to identify** biases and **difficult to identify**
    biases. Practitioners are known to take a model-centric approach to treat these
    biases, where modifying the algorithm or using bias-friendly algorithms has been
    considered acceptable practice. In the next section, we will take an alternative
    view to the model-centric approach: the data-centric approach.'
  prefs: []
  type: TYPE_NORMAL
- en: The data-centric imperative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Addressing bias in machine learning necessitates a holistic approach, with data-centric
    strategies complementing model-centric techniques. Data-centricity involves taking
    proactive steps to curate, clean, and enhance the dataset itself, thus minimizing
    the bias that models can inherit. By embracing data-centric practices, organizations
    can foster fairness, accountability, and ethical AI.
  prefs: []
  type: TYPE_NORMAL
- en: In the remainder of this chapter, we will explore a spectrum of data-centric
    strategies that empower machine learning practitioners to reduce bias. These include
    data resampling, augmentation, cleansing, feature selection, and more. Real-world
    examples will illustrate the tangible impact of these strategies in the domains
    of finance, human resources, and healthcare.
  prefs: []
  type: TYPE_NORMAL
- en: If data is fairly and accurately captured or created, then it is quite likely
    algorithms will be mostly free from bias. However, the techniques we will cover
    in this chapter are post-data creation, where ML practitioners have to work with
    provided data.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will discuss some data-centric strategies for
    reducing bias in machine learning without changing the algorithm. These can be
    referred to as data debiasing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sampling methods such as undersampling and oversampling address class imbalances.
    Undersampling reduces majority class instances, whereas oversampling augments
    minority class examples. Integrating both mitigates overfitting and information
    loss, balancing class representation effectively. These methods can be combined
    with outlier treatment and Shapley values to further sample the data where harder-to-classify
    or harder-to-estimate data points can be removed or introduced to enhance the
    fairness metrics. These techniques are covered next.
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In undersampling, we remove random or strategic subsets of overrepresented data
    points to balance class distributions – deleting data points from overrepresented
    classes where examples are difficult to classify or at random is a commonly used
    technique. We can also use outlier removal for regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In oversampling, we add random or strategic subsets of underrepresented data
    points to provide more examples to the algorithm. We can duplicate or generate
    synthetic data points for underrepresented classes to balance class distributions.
    We can use techniques such as the **Synthetic Minority Oversampling Technique**
    (**SMOTE**) and random oversampling for classification tasks. Alternatively, we
    can utilize outlier or edge case addition/removal for regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Combination of undersampling and oversampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These cover techniques such as `SMOTEENN` or `SMOTETomek`, where `SMOTE` is
    utilized to oversample the minority class. Techniques such as **Edited Nearest
    Neighbors** (**ENN**) or Tomek Links are used to remove the examples that are
    difficult to classify or agree on using nearest neighbors, as these points are
    close to the boundary and there is no clear separation.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection for oversampling and undersampling the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This covers using an anomaly detection technique to identify data points that
    are edge cases, and then these points can be reintroduced multiple times or removed
    so the model can get a better signal or become more generalized.
  prefs: []
  type: TYPE_NORMAL
- en: Use of Shapley values for oversampling and undersampling data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This covers using Shapley values to oversample or undersample data. Shapley
    values quantify feature importance by assessing each feature’s contribution to
    a model’s prediction. High Shapley values highlight influential features. Removing
    instances with high Shapley values but wrong predictions might enhance model accuracy
    by reducing outliers. Oversampling instances with high Shapley values and correct
    predictions can reinforce the model’s understanding of crucial patterns, potentially
    improving performance.
  prefs: []
  type: TYPE_NORMAL
- en: Other data-centric techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides sampling methods, there are other data-centric techniques that can be
    used to reduce bias, some of which have been covered in previous chapters, and
    some we will utilize in the case study. The three main ones are described next.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleansing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This includes removing missing data, where the inclusion of missing data can
    lead to unfair outcomes. These techniques were covered in [*Chapter 5*](B19297_05.xhtml#_idTextAnchor070),
    *Techniques for Data Cleaning*, where missing data was classified as “missing
    not at random.”
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This includes selecting specific features or eliminating features that will
    reduce bias. This may mean identifying a variable that is highly associated with
    a sensitive variable and outcome label, and removing such indirect variables or
    removing sensitive variables.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Feature engineering offers potent tools to mitigate model bias. Techniques such
    as re-encoding sensitive attributes, creating interaction terms, or introducing
    proxy variables enable models to learn without direct access to sensitive information.
    Feature selection and dimensionality reduction methods trim irrelevant or redundant
    features, fostering fairer and more robust models. Additionally, generating synthetic
    features or utilizing domain-specific knowledge helps improve models with a better
    understanding of data, aiding in fairer decision-making while improving overall
    model performance and reducing bias. We will create a synthetic variable, “Interest,”
    in the example to show how the model is biased toward one subgroup over another.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered data-centric methods, in the next section, we will
    describe the problem statement, and walk through examples of how we can identify
    and reduce bias in real life.
  prefs: []
  type: TYPE_NORMAL
- en: Case study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The challenge at hand centers on uncovering and addressing potential bias within
    a dataset pertaining to credit card defaults in Taiwan. Acquired from the UC Irvine
    Machine Learning Repository ([https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients)),
    this dataset comprises information from 30,000 credit card clients over a six-month
    span, including demographic factors such as gender, marital status, and education.
    The key concern is whether these demographic features introduce bias into a decision
    tree classifier trained on all available features, with a specific focus on gender-related
    bias. The overarching objective of this example is to not only identify but also
    mitigate any biased outcomes through the application of data-centric techniques.
    By reevaluating the algorithm’s performance using fairness metrics, the example
    aims to shed light on the real-world implications of bias in financial decision-making,
    particularly how these biases can impact individuals based on gender and other
    demographic factors, potentially leading to unequal treatment in credit assessments
    and financial opportunities. Addressing and rectifying such biases is crucial
    for promoting fairness and equity in financial systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use two key metrics to check the fairness of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Equalized odds difference**: This metric compares the false negative rate
    and false positive rate across the sensitive variables, then takes the maximum
    difference between the false negative rate and false positive rate. For instance,
    on the test set, the false positive rate among men and women is 0.3 and 0.2 (difference
    of 0.1), whereas the false negative rate among men and women is 0.15 and 0.12
    (difference of 0.03). Since the difference is larger on the false positive rate,
    the equalized odds will be 0.1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Demographic parity ratio**: This metric measures whether the predictions
    made by a model are independent of a sensitive variable, such as race, gender,
    or age. Given this is a ratio, it measures the ratio of a lower selection rate
    to that of a higher selection rate. A ratio of 1 means that demographic parity
    is achieved, whereas below 0.8 usually means that the algorithm is highly biased
    toward one group of individuals over the others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a description of the features in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LIMIT_BAL`: Amount of the given credit in NT dollars, including both the individual
    consumer credit and their family (supplementary) credit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Sex`: Gender (1 = male; 2 = female).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Education X3`: Education (1 = graduate school; 2 = university; 3 = high school;
    4 = others)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Marriage X4`: Marital status (1 = married; 2 = single; 3 = others)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Age X5`: Age of the person in years'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PAY_0- PAY_5; X6 - X11: History of past payments, which includes the past monthly
    payment records (from April to September 2005), where PAY_0X6 = the repayment
    status in September, PAY_2; X7 = the repayment status in August 2005; ... PAY_6;
    X11 = the repayment status in April 2005\. The measurement scale for the repayment
    status is -1 = amount paid duly; 1 = payment delay for 1 month; 2 = payment delay
    for 2 months; ... ; 8 = payment delay for 8 months; 9 = payment delay for 9 months,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BILL_AMT1 . BILL_AMT6; X12-X17: Bill statement amount (in Taiwan dollars).
    BILL_AMT1;X12 means the amount on the credit card statement as of September 2005,
    while BILL_AMT6;X17 means the amount on the credit card statement as of April
    2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PAY_AMT1-PAY_AMT6; X18-X23: Amount of payments made based on the previous month''s
    bill statement. PAY_AMT1;X18 means amount paid in September 2005, while PAY_AMT6;X23
    means amount paid on April 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`default payment next month`: Whether a person defaulted on the next month’s
    payment (Yes = 1, No = 0), in 2005'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To import the dataset, you need to install `pandas`. We will also use the `os`
    library to navigate the path and store the dataset. This library is native to
    Python. We will call the `loan_dataset.csv` file and save it in the same directory,
    from where we will run this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The file takes a couple of seconds to a minute based on internet speed, so
    when we run this example for the first time, the file will be stored locally.
    However, on the subsequent runs, with the help of the `os` library, we will check
    that the file exists, else download it. We will rename two variables: `PAY_0`
    to `PAY_1`, and also `default payment next month` to `default`. We don’t believe
    the `ID` column will be useful for machine learning, hence we will drop it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we load the file from the local directory into a DataFrame called `dataset`.
    There are 30,000 rows and 24 columns including the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we run the `dataset.info()` method to check whether there are any missing
    values or wrongly encoded columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Output of the dataset.info() method](img/B19297_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Output of the dataset.info() method
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t have any missing data; however, three categorical columns (`SEX`,
    `EDUCATION`, and `MARRIAGE`) have integer data types, which we may have to convert
    to strings. Since values in `SEX` might be ordinal, we will first remap them to
    `1` and `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If we rerun `dataset.info()`, we will see that the data type for the three
    columns is now `category`; we can now one-hot encode them. We exclude `SEX` from
    one-hot encoding since a person is either a male or female (in this dataset) and
    that information can be captured in one column. We will also extract `SEX` and
    store it in another variable, `A`, and separate the target variable and independent
    features. Next, we create a mapping for values in the `SEX` feature to be used
    for analysis and visualization, to help interpret the results, so `1` will be
    mapped to `male` values and `0` will be mapped to `female` values. We store this
    mapping in the `A_str` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s load all the required libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run the example, you will need the following additional libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn` (scikit-learn) for data preprocessing and fitting the models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` to calculate some metrics and do some data wrangling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imblearn` for over and undersampling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fairlearn` to calculate bias and fairness scores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shap` to visualize the interpretations of the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We load all the libraries at the start:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we split the dataset into `train` and `test`, using `train_test_split`,
    and assign 20% of the data to test. We also split `A_str` into `A_train` and `A_test`,
    so we can calculate fairness scores on test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create the decision tree classifier pipeline and train the algorithm
    with the sensitive features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we calculate the ROC score and extract the predictions. We also visualize
    the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The code generates the following confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Output confusion matrix](img/B19297_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Output confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: In order to check whether the algorithm is fair or not, we will first calculate
    false positive and false negative rates, and then compare those across male and
    female cohorts on the test dataset to see whether there are big differences between
    the two cohorts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we have created two functions to calculate a false
    positive rate and a false negative rate. We have further created a dictionary
    of fairness metrics, in which we use the false positive rate and false negative
    rate, alongside a balanced accuracy metric from scikit-learn. We have then created
    a list of fairness metrics and stored them in a variable for easy access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have also created a function to report the differences between male and
    female cohorts on the fairness metrics. We first create a DataFrame called `metricframe`
    using the convenience function from `fairlearn` called `MetricFrame`. It takes
    in true labels, predictions, and sensitive feature values, along with a dictionary
    of metrics to report on. We then leverage the `.by_group` property to report on
    fairness metrics for each cohort. Within the function, we also report on `equalised_odds_difference`
    and `demographic_parity_ratio` from the `fairlearn` library to understand the
    overall fairness of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We now run the function and calculate the fairness scores. It is evident that
    the model is quite similar in male and female cohorts since false positive rates
    and false negative rates are similar among the cohorts. Since the difference in
    the false positive rate is larger than the false negative rate, the equalized
    odds difference is the same as the difference between the false positive rate
    of the two groups. We can also see that the demographic parity ratio is above
    0.8, which means that both cohorts are quite likely to get selected for a positive
    outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Fairness scores](img/B19297_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Fairness scores
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate bias in the dataset, we may need to generate a synthetic variable
    that correlates with a real-world scenario where, based on history, a cohort is
    treated more unfairly. First, we compare the default rate across males and females
    in the training dataset. We then add the synthetic noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Given that the male default is higher than the female, we can replicate a biased
    scenario where applicants with lower default rates will have lower interest rates,
    but applicants with higher default rates will have higher interest rates imposed
    by the bank. Let’s assume the bank managers believe males are more likely to default,
    and instead of generalizing the scenario, the bank decides to charge higher interest
    rates to males.
  prefs: []
  type: TYPE_NORMAL
- en: To mimic this scenario, we will introduce a new feature, `Interest_rate`, following
    a Gaussian distribution. The mean will be 0 where someone hasn’t defaulted, but
    will be 2 times 1 where someone has defaulted. We also set the standard deviation
    to 2 for males and 1 for females.
  prefs: []
  type: TYPE_NORMAL
- en: 'To generate the synthetic Gaussian distribution, we use the `numpy.random.normal`
    method, with a seed of `42` for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have added the noise, we retrain the algorithm with the interest
    variable and recalculate the fairness metrics. We first split the data, then retrain
    and recalculate the fairness metrics. We resplit the data into `train` and `test`,
    as shown previously, and retrain the algorithm. Once retrained, we calculate the
    impact.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see, in the following code, that by adding the synthetic interest variable,
    we have improved the ROC metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It is clear from the following output that we now have a more biased algorithm,
    based on equalized odds. The false negative rate is quite high in males, which
    means that more males who are unlikely to pay back to the bank are likely to be
    given a loan, and if this model was productionized, this could result in unfair
    outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Fairness scores](img/B19297_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Fairness scores
  prefs: []
  type: TYPE_NORMAL
- en: To reduce the bias, we will apply the first data-centric debiasing technique
    under the feature selection by removing the sensitive variable from the algorithm.
    This can be done by retraining the algorithm without the `SEX` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that the dataset is biased toward one gender due to a higher variation
    in interest rates, it is recommended in the real world that data engineers and
    data scientists work with domain experts and data producers to reduce this bias
    in the dataset. For instance, instead of using `SEX` to determine the interest
    rate, other features could be used, such as payment history, credit history, and
    income. In the training step, we can drop the `SEX` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'From the following output, we can see that by removing the `SEX` variable,
    the ROC score has dropped from 0.846 to 0.839:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Looking at the following fairness metrics, it is obvious that when the outcome
    is biased based on the cohort of data, removing the variable from the training
    can debias the algorithm. The false negative rate in `male` has decreased, whereas,
    in `female`, it has increased; however, the algorithm is more fair compared to
    when the `SEX` variable was used. The equalized odds have dropped from 0.18 to
    0.07, but the demographic parity ratio has reduced, which means one group has
    more chance of getting a loan than the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Fairness metrics](img/B19297_08_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will show you how to apply undersampling techniques to ensure the outcome
    variable is balanced.
  prefs: []
  type: TYPE_NORMAL
- en: AllKNN undersampling method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will start with the AllKNN algorithm from the `imblearn` package, and then
    try the instant hardness algorithm. Since the algorithms use KNN under the hood,
    which is a distance-based measure, we need to ensure that we scale the features
    using the scikit-learn `StandardScaler` method. We will first scale the variables,
    then run the sampling algorithm, and then train the decision tree. We will run
    the algorithm with 5 k cross-validation, and ensure the function returns the model
    trained. Cross-validation will be scored on `roc_auc` and balanced accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: We will first try an undersampling technique, `AllKNN`, from `imblearn`. This
    algorithm does not aim at balancing majority and minority classes; however, it
    removes instances that are harder to classify from the majority class. It does
    that iteratively where, first, the model is trained on the entire dataset. Then,
    in the prediction step of the majority class, if a disagreement occurs between
    any of the neighbors about the predicted outcome, the data point is removed from
    the majority class. In the first iteration, a 1-KNN model is trained and some
    samples are removed, and then in the next iteration, a 2-KNN model is trained,
    and in the following iteration, a 3-KNN model is trained. Usually, the algorithm
    (by default) will end at the 3-KNN iteration; however, the practitioner can choose
    more iterations, and the algorithm will not stop until the number of samples between
    the majority and minority class becomes the same or a maximum number of iterations
    is reached – whichever happens earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first define the scaler and sampler method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a pipeline object and pass the scaler, sampler, and estimator
    to the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we pass the training data and run the cross-validation. We set the cross-validation
    method to return the estimator (pipeline) by setting `return_estimator=True`,
    so that we can use it to make predictions on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we print the mean and standard deviation of ROC and balanced accuracy
    from the cross-validation step, returned from prediction results in each step,
    where at each step, four folds were used on training and the prediction was made
    on the fifth fold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that by removing hard examples using the undersampling technique,
    `roc_auc` on `test` data bumped from 0.839 in the previous step to 0.85:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we calculate the fairness metrics. Although the false negative rate has
    decreased for both males and females, the false positive rate has increased, and
    the equalized odds difference has also increased from the previous step. This
    might be because cases with male samples that were difficult to classify have
    been removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 8.6 – Fairness metrics](img/B19297_08_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore the impact on the fairness metrics by introducing hard cases.
  prefs: []
  type: TYPE_NORMAL
- en: Instance hardness undersampling method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, the instance hardness method focuses on samples that are
    harder to classify, which are usually at the boundary or overlap with other classes.
    Usually, this depends on the algorithm used (as some algorithms are better at
    some hard cases than others) and the level of overlap between the classes. For
    such samples, the learning algorithm will usually show the low probability prediction
    on the hard cases, which means the lower the probability, the higher the instance
    hardness. Under the hood, the method has the capability to retain the right number
    of samples, based on the class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, we will define the algorithm, and the algorithm will be passed
    on to the instance hardness step. We will then define the instance hardness undersampling
    method, with three-fold cross-validation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the decision tree estimator. Finally, we combine the steps
    in the pipeline with scaling the dataset, then undersampling the data, and finally,
    training the model. When the pipeline is defined, we run the cross-validation
    similar to the previous pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Both `AllKNN` and `InstanceHardness` returned similar cross-validation results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The ROC slightly bumped from 0.85 to 0.854 on the `test` data when using the
    instance hardness method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The fairness metrics are quite similar to the previous undersampling technique,
    and probably due to similar reasons, where, by removing difficult cases, the model
    is unable to deal with predicting difficult `male` cases. However, in both undersampling
    methods, the equalized odds have increased, compared to the feature selection
    step. Also, the demographic parity ratio is still under 0.8, which means one subclass
    of gender is more likely to be selected over another when predicting `default`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 8.7 – Fairness metrics](img/B19297_08_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at oversampling methods.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another way of improving model performance and fairness metrics is by introducing
    additional examples. The next two oversampling techniques, `SMOTE` and `ADASYN`,
    were introduced in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111), *Using Synthetic
    Data in Data-Centric Machine Learning*, hence we will not cover the details behind
    the algorithm. We will use these techniques in the context of improving fairness
    metrics by adding additional examples, in the hope that the model is able to learn
    better with additional data points.
  prefs: []
  type: TYPE_NORMAL
- en: For each of the methods, we will first scale the dataset, add additional minority
    class examples, and then train the model. We will print the cross-validation scores
    and the `test` ROC score, as well as fairness metrics.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given that we used this algorithm in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111),
    *Using Synthetic Data in Data-Centric Machine Learning*, we will dive straight
    into the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The validation metrics and the `test` ROC score show poorer results compared
    to the undersampling methods covered previously. In the next step, we explore
    the fairness metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 8.8 – Fairness metrics](img/B19297_08_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: The fairness metrics are better in comparison with the undersampling methods
    – that is, the difference between false positive and false negative rates is reduced
    between men and women and, based on the demographic parity ratio, the model is
    more likely to select both types of gender applicants for loan default. In the
    next section, we will use the `ADASYN` algorithm and compare it with `SMOTE` and
    other undersampling methods.
  prefs: []
  type: TYPE_NORMAL
- en: ADASYN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Similarly to the `SMOTE` method, we covered `ADASYN` in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111),
    *Using Synthetic Data in Data-Centric Machine Learning*, hence we will dive straight
    into the code, in which we oversample the minority class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The validation metrics and `test` ROC score are slightly below the `SMOTE`
    results and undersampling methods. Now, let’s review the fairness metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 8.9 – Fairness metrics](img/B19297_08_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: The equalized odds are slightly higher for `ADASYN`, whereas demographic parity
    is slightly better when compared to `SMOTE`, and both oversampling techniques
    guarantee higher fairness over undersampling methods, but slightly poorer ROC
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: We have now seen that, despite balancing the classes, model fairness is compromised,
    and it is mostly `male` examples where the model is making more errors. So, in
    the next section, we will randomly introduce some additional `male` examples where
    the model is misclassifying positive cases.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling plus misclassified examples at random
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will first balance the dataset using `ADASYN` and avoid undersampling techniques
    since we want to retain hard cases that are difficult to classify. We then train
    the model and identify `male` cases that the model believes should be positive
    but wrongly classifies as negative. We then randomly select 10% of these cases,
    add them back to the training dataset, and retrain the model with the same algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: At the end, we review the model metrics and fairness metrics on the `test` data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We utilize oversampling and reintroduce misclassified data points at random.
    Let’s run the pipeline with the `ADASYN` oversampling method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we identify the examples from the training dataset where the model is
    making errors on the male population – that is, examples where the model predicts
    false negatives. We first subset the data associated with males and then run predictions
    over this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we subset this data where the `true` label is `1` but model predictions
    are `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We randomly select 10% of the values and add them to the `X_train` dataset.
    We leverage the `.sample` method, and this random selection is done with replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add this 10% to `X_train` and `y_train` and create a new dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we train the algorithm on this new dataset and print out the validation
    metrics and the `test` ROC score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Compared to the oversampling section, the validation metrics are quite similar,
    as is the `test` ROC score. Next, we review the fairness metrics to check whether
    they have improved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 8.10 – Fairness metrics](img/B19297_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: By adding some false negative `male` examples, we can see that the equalized
    odds have improved slightly to 0.098 and the demographic ratio has also improved,
    increasing to 0.85\. We believe that even better results can be achieved if we
    add false positive examples and false negative examples and combine these with
    undersampling and oversampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate this, we will iterate over four undersampling techniques (`AllKNN`,
    `RepeatedEditedNearestNeighbours`, `InstanceHardnessThreshold`, and `Tomek`),
    two oversampling techniques (`SMOTE` and `ADASYN`), and two combinations of over
    and undersampling techniques (`SMOTEENN` and `SMOTETomek`). How these algorithms
    work is outside the scope of this example. Instead, the goal is to demonstrate
    how these data techniques can lead to better selection and a generalized model
    with slightly poorer performance, but higher fairness.
  prefs: []
  type: TYPE_NORMAL
- en: We will now develop a mechanism where we first train the algorithm and then
    add false positive examples and false negative examples. Once the examples are
    added, we run the pipeline by sampling the dataset, using the previous algorithms.
    We'll record fairness outcomes and the ROC score to find the technique that best
    fosters a balance between fairness and performance in our algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first create a dictionary with a configuration of each of the aforementioned
    sampling techniques so we can iterate over it. We can call this AutoML for the
    sampling technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create two functions that take the training dataset, model, column,
    and its subset value to help create random samples. The following function will
    sample false positives:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'And this function samples false negatives. By default, both methods will add
    10% random examples with replacement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a function that calculates test metrics post-data improvements.
    The function takes the test data and estimator and returns model metrics and fairness
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Next, we create a pipeline that will sample the dataset and then create random
    false positive `male` and false negative `male` examples. We then combine these
    into the training data, one at a time, and retrain the same algorithm. We then
    calculate the metrics and store them in a list called `results` with columns.
    Each iteration adds false negative and false positive examples with model performance
    and fairness metrics. We then use this list to compare the results across algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for creating the pipeline is pretty lengthy. Please refer to GitHub
    for the full code: [https://github.com/PacktPublishing/Data-Centric-Machine-Learning-with-Python/tree/main/Chapter%208%20-%20Techniques%20for%20identifying%20and%20removing%20bias](https://github.com/PacktPublishing/Data-Centric-Machine-Learning-with-Python/tree/main/Chapter%208%20-%20Techniques%20for%20identifying%20and%20removing%20bias)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a DataFrame called `df` and add all the `test` metrics so we
    can compare which method reaped the best model performance and fairness metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s sort the DataFrame based on equalized odds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We can see that when the dataset was sampled with Tomek Links, where difficult
    cases were removed from the boundary and combined with additional false positive
    `male` training samples, this resulted in the best equalized odds of 0.075; however,
    a demographic parity of 0.8 was not achieved. When the SMOTETomek technique was
    used in combination with false negative `male` examples, the model achieved a
    0.088 equalized odds ratio, which was the best among the sampling methods, and
    the model also achieved a high demographic parity ratio.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Resulting output dataset](img/B19297_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Resulting output dataset
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling with anomalies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous steps, we learned that by adding poorly classified examples
    to the training dataset, we were able to improve model fairness. In the next step,
    instead of choosing samples at random, we will utilize an algorithm that identifies
    anomalies and then we add these anomalies to the training dataset as an oversampling
    mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a pipeline to oversample the minority class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract the oversampled data. There is no reason why undersampling
    or no sampling could not have been chosen. Once the oversampled data is extracted,
    we then scale it back to the original feature space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we train the isolation forest to identify 10% of anomalies. To do that,
    we set the contamination to `0.1`. We then fit the model on resampled data, and
    run prediction on this data. We store the results in a column called `IF_anomaly`
    and add it to the resampled dataset. We then extract these anomalies, as isolation
    forest labels with a value of `-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add these additional data points to the original dataset and train
    the decision tree model. Once the model is fitted, we calculate the ROC score
    on the `test` data. We can see that this is 0.82:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we calculate the fairness metrics. Based on the following results, we
    can say that the model trained in the previous section produced better fairness
    and demographic parity ratio scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Fairness metrics](img/B19297_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have utilized various examples of undersampling and oversampling
    data, including reintroducing random misclassified examples and anomalies, in
    the next section, we will utilize an advanced technique, where we will be more
    selective with which examples to add and which examples to remove, to further
    reduce bias in the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Shapley values to detect bias, oversample, and undersample data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will utilize Shapley values to identify examples where the
    model struggles to make the correct prediction. We will use the impact score to
    either add, eliminate, or use a combination of both to improve the fairness metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '**SHAP** (which stands for **Shapley Additive exPlanations**) is a model-agnostic
    approach in machine learning that is built on the principles of game theory. It
    helps study the importance of the feature and the feature interaction on the final
    outcome by assigning it a score, similar to how it would be done in a game where
    each player’s contribution at a given time is calculated in the output of the
    score.'
  prefs: []
  type: TYPE_NORMAL
- en: Shapley values can help provide global importance (the overall impact of the
    feature on all the predictions), but also local importance (the impact of each
    feature on a single outcome). It can also help understand the direction of impact
    – that is, whether a feature has a positive impact or a negative impact.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, there are a lot of use cases for Shapley values in machine learning,
    such as bias detection, local and global model debugging, model auditing, and
    model interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use Shapley values in this section to understand the model and feature impacts
    on the outcomes. We leverage the impacts of these features and identify where
    the model is likely to make the most mistakes. We then apply two techniques: one
    to remove these rows from the data and the other to oversample the data with these
    rows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import SHAP and then train the decision tree model on the oversampled
    dataset. At the end of the step, we have a model and oversampled `X` and `y` samples.
    We include the `SEX` variable in the training data to see whether Shapley values
    can help us detect bias. First, we need to resplit the data into `train` and `test`
    sets, as done in the previous sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the SHAP tree explainer, by providing the decision tree model
    and then extract the Shapley values for the `train` set using the `.``shap_values`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s extract the first row of Shapley values, for class 0\. The array contains
    the contribution of each feature value to decide the final output. Positive values
    mean the corresponding features have a positive impact on predicting the output
    as class 0, while negative values negatively contribute toward predicting class
    0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out the following array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Resulting output array](img/B19297_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Resulting output array
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we generate a summary plot for class label 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – SHAP values](img/B19297_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – SHAP values
  prefs: []
  type: TYPE_NORMAL
- en: The red dots represent the high value of a feature while the blue dots represent
    the low value of the corresponding feature. The *x* axis denotes the Shapley value,
    where the positive value means the data point has a positive impact in predicting
    class 0, whereas the negative value means the data point for the corresponding
    feature negatively affects the prediction for class 0\. If we look at *Figure
    8**.13,* it is quite evident that high interest rates and male customers negatively
    affect the prediction of class 0\. Shapley values do indicate a model bias toward
    male customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we generate a summary plot for class label 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following summary plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – SHAP summary plot](img/B19297_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – SHAP summary plot
  prefs: []
  type: TYPE_NORMAL
- en: In comparison with the summary plot for class 0, high interest rates and male
    customers positively impact defaulting on the loan – that is, if you are a male
    and previously had a higher interest rate, you are likely to default on the loan.
  prefs: []
  type: TYPE_NORMAL
- en: 'We previously learned that by removing the `SEX` feature from model training,
    the model becomes fairer, and Shapley values are clearly indicated using summary
    plots. Now, we extract the Shapley values by training the new model without the
    `SEX` feature. We then score the training data to first identify all the rows
    corresponding to false negatives and false positives. We then calculate the sum
    of Shapley values for each row where the model made errors, and then hold out
    the ones with the lowest impact. We run two experiments: first, we undersample
    the training dataset and calculate fairness metrics, and second, we oversample
    the training dataset to give a better signal to the model and recalculate fairness
    metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s train the model without the `SEX` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract Shapley values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'We score the training data, calculate predictions, and store these in `Y_pred`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we will check the sum of the Shapley value for class 0 and class 1 at
    index 0, and print the corresponding prediction and `true` value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The model predicted `0`. Next, we extract the Shapley values where the model
    made a mistake. For that, we use the list comprehension with zip functionality.
    The first value of the array will be the index location of the data point so we
    know which Shapley value is associated with which row. The next values are in
    order of prediction, the `true` value, the sum of Shapley values for the row for
    class 0, and the sum of Shapley values for class 1\. Once we have extracted those,
    we create a DataFrame and store the values in `df`, and we sample through the
    DataFrame to see five values. We use a random seed for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'This generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – DataFrame displaying the Shapley values that made a mistake](img/B19297_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – DataFrame displaying the Shapley values that made a mistake
  prefs: []
  type: TYPE_NORMAL
- en: For index `7915`, the Shapley values are close, meaning feature contributions
    to the model prediction are closer to `0` for each class, whereas for index `4255`,
    the Shapley values are far apart from `0` and features are discriminatory in predicting
    each class.
  prefs: []
  type: TYPE_NORMAL
- en: Given that we can extract the SHAP impact of features for each class, we want
    to know the rows where the Shapley value impact is highest so we can eliminate
    such data points from training; where the impact is low and quite close to the
    boundary, we can oversample the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the force plots for index `4255`, for the expected class `0`, the
    model is likely to predict `1`, given that `f(x)` is quite low, and the model
    wrongly predicts `1`, whereas the force plot for the expected class `1` shows
    an `f(x)` value of `0.7`. Such data points can be eliminated from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Force plot for class 0](img/B19297_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Force plot for class 0
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the force plot for class `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Force plot for class 1](img/B19297_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – Force plot for class 1
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we calculate the Shapley impact of row index `4255` since it’s a false
    positive prediction. The row index is at the `422` location in the DataFrame.
    We take the absolute value of the Shapley impact and, where the Shapley impact
    is highest and the prediction is wrong, those values can be eliminated to improve
    model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a function that calculates the Shapley impact. We are interested
    in those rows where a single feature has a minimum of `0.2` Shapley impact. First,
    we get the absolute impact of each feature in an array, and then we extract the
    maximum value. If the maximum value is greater than `0.2`, we proceed with that
    row. Next, we check where the prediction doesn’t match the actual value, and for
    such rows, we extract the SHAP impact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a holdout dataset, where `X_train` will be further divided into training
    and validation datasets. We leverage 80% for training and 20% for validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we resample the dataset using `SMOTETomek`, which was the best sampling
    method for fairness and performance, and performed by adding difficult examples
    back to the dataset. Once the dataset is resampled, we train the standard decision
    tree as in the previous steps, and calculate the ROC score on the holdout validation
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate the fairness and performance metrics on the testing dataset. The
    equalized odds are high but the demographic parity ratio is within the accepted
    range:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we calculate the fairness metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Fairness metrics](img/B19297_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: 'We extract the Shapley values using the SHAP explainer. We have ensured that
    the `SEX` feature is removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The fairness metrics demonstrate that the gap between false negative rates is
    higher between the subclass of men and women, hence we focus on reducing false
    negative cases for males using Shapley values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we extract Shapley values where the model predicts class
    `0,` but the `true` value is `1`. Hence, we are interested in Shapley values for
    class `1`, as a high SHAP impact for class `1` where the model made an error could
    be a data point that the model is unable to make a correct prediction on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'We are also interested in those data points where both Shapley values and model
    prediction agree with the actual values. Hence, we focus on those data points
    where the model rightly predicts class `0` for `male` data points. Once we have
    extracted those, we focus on high-impact Shapley values for class `0` so we can
    oversample the dataset with those, such that the model can get a better signal
    for such data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we sort the false negative Shapley values so we can extract the high-impact
    data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the preceding, we are interested in true negative high-impact Shapley
    values, so we sort the list according to high-impact Shapley values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have extracted and sorted the Shapley values for false negative
    and true negative `male` data points, we pick the top 100 data points to eliminate
    from the false negative list and pick the top 100 data points from the true negative
    list to add back to the training data. The top 100 data points from the true negative
    list will be shuffled and only 50 data points from there will be added at random
    with a replacement strategy. We encourage practitioners to try another ratio for
    shuffling. Once the data points are identified for elimination and reintroduction
    to the final training set, we update the training data. These are named `X_train_final`
    and `y_train_final`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we train the updated training data and calculate fairness metrics and
    performance metrics on the `test` data. It is evident that the gap between false
    negative rates has reduced, the equalized odds ratio has improved to 0.082, and
    the ROC score has slightly improved from the previous step, from 0.825 to 0.826:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We recalculate the fairness metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Fairness metrics](img/B19297_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have determined that by using Shapley values we can identify data
    points that are difficult to classify and easy to classify, we can build an automatic
    mechanism to iterate over the data points such that we can reach a better fairness
    score than previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a range of percentages to iterate so we can leverage and sort
    through the top data points as a percentage of top data points to eliminate and
    reintroduce. We will leverage the NumPy `linspace` method to create a list of
    percentage values to iterate. We choose 10 values from `0.05` through `0.5` (5
    to 50 percent). We call this list `perc_points_to_eliminate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: We iterate through these percentages and repeat the preceding step where we
    eliminated some values and reintroduced some values. However, this time, instead
    of 100, we use percentages to remove the top percent of data points or introduce
    the top percent of data points.
  prefs: []
  type: TYPE_NORMAL
- en: We also create an empty list of data so, for each iteration, we capture the
    percentage of data points eliminated or reintroduced, false negative and false
    positive rates on test data, equalized odds ratio, and demographic parity ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have iterated over all the values – 10*10 iterations, we store them
    in a DataFrame to see how many data points need to be removed and how many added
    to lead to the best fairness metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a DataFrame called `df_shapley` for the metadata for each iteration
    and sort it by equalized odds ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure  8.21 – The df_shapley DataFrame after sorting](img/B19297_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – The df_shapley DataFrame after sorting
  prefs: []
  type: TYPE_NORMAL
- en: It’s evident that when the top 25% of false negative data points are removed
    and 30% of the top true negative data points are reintroduced, the model can achieve
    an equalized odds ratio of 0.074 with an optimum demographic parity ratio score
    of 0.85.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we extract the top percentages and train the final model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We train the model and calculate the fairness and model metrics. We can see
    that the false negative rate for `female` has increased but the gap between `male`
    and `female` has reduced, and the false positive rate for `male` has reduced.
    The ROC score achieved is 0.82, but the model is much fairer based on two fairness
    metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we calculate the fairness metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.22 – Fairness metrics](img/B19297_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 – Fairness metrics
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored different data-centric techniques for reducing bias
    by improving data quality, we encourage you to experiment with the previous techniques
    and try a combination of these. Once you have exhausted these data-centric approaches,
    we encourage you use some model-centric approaches, such as utilizing algorithms
    that are fairness aware and trying ensembling methods, AutoML, or iterating through
    your own list of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided an extensive exploration of the pervasive challenge of
    bias in machine learning. It started by explaining various forms of bias inherent
    in machine learning models and examined their impact on different industries.
    The emphasis was on recognizing, monitoring, and mitigating bias, underscoring
    the importance of collecting data with minimal selection and sampling bias.
  prefs: []
  type: TYPE_NORMAL
- en: The central theme advocated a data-centric imperative over a model-centric one
    in addressing bias. Techniques such as oversampling, undersampling, feature selection
    enhancement, and anomaly detection were explored for bias rectification. Shapley
    values play a crucial role in bias identification, emphasizing the removal of
    examples with misaligned high Shapley values and the reintroduction of data points
    with replacement to improve ratios. Stratification of misclassified examples based
    on sensitive variables such as `SEX` was outlined for targeted bias correction.
  prefs: []
  type: TYPE_NORMAL
- en: The chapter concluded by highlighting the significance of refining and balancing
    datasets concerning sensitive variables as a foundational step. It suggested progressing
    toward model-centric approaches, such as ensembling and fairness algorithms, once
    the dataset itself has been improved. These subsequent model-centric strategies
    aim to enhance both performance and fairness metrics, establishing a foundation
    for more generalized and equitable AI models.
  prefs: []
  type: TYPE_NORMAL
- en: This comprehensive approach strives to create a balanced dataset as a precursor
    to applying model-centric techniques, promoting performance and fairness in AI
    systems.
  prefs: []
  type: TYPE_NORMAL
