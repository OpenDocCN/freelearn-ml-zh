- en: Learning Object Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 5](b788527c-5892-4547-8add-0864ccbd3f95.xhtml), *Automated Optical
    Inspection, Object Segmentation, and Detection*, we introduced the basic concepts
    of object segmentation and detection. This refers to isolating the objects that
    appear in an image for future processing and analysis. This chapter explains how
    to classify each of these isolated objects. To allow us to classify each object,
    we have to train our system to be capable of learning the required parameters
    so that it decide which specific label will be assigned to the detected object
    (depending on the different categories taken into account during the training
    phase).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces the basics concepts of machine learning to classify
    images with different labels. To do this, we are going to create a basic application
    based on the segmentation algorithm of [Chapter 5](b788527c-5892-4547-8add-0864ccbd3f95.xhtml),
    *Automated Optical Inspection, Object Segmentation, and Detection*. This segmentation
    algorithm extracts parts of images that contain unknown objects. For each detected object,
    we are going to extract different features that are going to be classified using
    a machine learning algorithm. Finally, we are going to show the obtained results
    using our user interface, together with the labels of each object detected in
    the input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter involves different topics and algorithms, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to machine learning concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common machine learning algorithms and processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: support vector machines (SVM)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires familiarity with the basic C++ programming language. All
    of the code that's used in this chapter can be downloaded from the following GitHub
    link: [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_06](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_06). This
    code can be executed on any operating system, though it is only tested on Ubuntu.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://bit.ly/2KGD4CO](http://bit.ly/2KGD4CO)'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing machine learning concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a concept that was defined by *Arthur Samuel* in 1959 as
    a field of study that gives computers the ability to learn without being explicitly
    programmed. *Tom M. Mitchel* provided a more formal definition for machine learning,
    in which he links the concept of samples with experience data, labels, and a performance
    measurement of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **machine learning** definition by *Arthur Samuel* is referenced in *Some
    Studies in Machine Learning Using the Game of Checkers* in *IBM Journal of Research
    and Development* (*Volume*: *3*, *Issue*: *3*), *p*. *210*. It was also referenced
    in *The New Yorker* and *Office Management* in the same year.'
  prefs: []
  type: TYPE_NORMAL
- en: The more formal definition from *Tom M. Mitchel* is referenced in *Machine Learning
    Book, McGray Hill 1997:* ([http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html](http://www.cs.cmu.edu/afs/cs.cmu.edu/user/mitchell/ftp/mlbook.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning involves pattern recognition and learning theory in artificial
    intelligence, and is related with computational statistics. It is used in hundreds
    of applications, such as **optical character recognition** (**OCR**), spam filtering,
    search engines, and thousands of computer vision applications, such as the example
    that we will develop in this chapter, where a machine learning algorithm tries
    to classify objects that appear in the input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on how machine learning algorithms learn from the input data, we
    can divide them into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning**: The computer learns from a set of labeled data. The
    goal here is to learn the parameters of the model and rules that allow computers
    to map the relationship between data and output label results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised learning**: No labels are given and the computer tries to discover
    the input structure of the given data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reinforcement learning**: The computer interacts with a dynamic environment,
    reaching their goal and learning from their mistakes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Depending on the results we wish to gain from our machine learning algorithm,
    we can categorize the results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification**: The space of the inputs can be divided into **N** classes,
    and the prediction results for a given sample are one of these training classes.
    This is one of the most used categories. A typical example can be email spam filtering,
    where there are only two classes: spam and non-spam. Alternatively, we can use
    OCR, where only N characters are available and each character is one class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: The output is a continuous value instead of a discrete value
    like a classification result. One example of regression could be the prediction
    of a house price given the house''s size, number of years since it was built,
    and location.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Clustering**: The input is to be divided into N groups, which is typically
    done using unsupervised training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Density estimation**: Finds the (probability) distribution of inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our example, we are going to use a supervised learning and classification
    algorithm where a training dataset with labels is used to train the model and
    the result of the model's prediction is one of the possible labels. In machine
    learning, there are several approaches and methods for this. Some of the more
    popular ones include the following: **support vector machines** (**SVM**), **artificial
    neural networks** (**ANN**), clustering, k-nearest neighbors, decision trees,
    and deep learning. Almost all of these methods and approaches are supported, implemented,
    and well documented in OpenCV. In this chapter, we are going to explain support
    vector machines.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenCV implements eight of these machine learning algorithms. All of them are
    inherited from the `StatModel` class:'
  prefs: []
  type: TYPE_NORMAL
- en: Artificial neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expectation maximization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-nearest neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal Bayes classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stochastic gradient descent SVMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Version 3 supports deep learning at a basic level, but version 4 is stable and
    more supported. We will delve into deep learning in detail in further chapters.
  prefs: []
  type: TYPE_NORMAL
- en: To get more information about each algorithm, read the OpenCV document page
    for machine learning at [http://docs.opencv.org/trunk/dc/dd6/ml_intro.html](http://docs.opencv.org/trunk/dc/dd6/ml_intro.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the machine learning class hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/969f81a8-ad37-401b-9b90-f3c4dab87b48.png)'
  prefs: []
  type: TYPE_IMG
- en: The `StatModel` class is the base class for all machine learning algorithms.
    This provides the prediction and all the read and write functions that are very
    important for saving and reading our machine learning parameters and training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, the most time-consuming and computing resource-consuming
    part is the training method. Training can take from seconds to weeks or months
    for large datasets and complex machine learning structures. For example, in deep
    learning, big neural network structures with more than 100,000 image datasets
    can take a long time to train. With deep learning algorithms, it is common to
    use parallel hardware processing such as GPUs with CUDA technology to decrease
    the computing time during training, or most new chip devices such as Intel Movidius.
    This means that we cannot train our algorithm each time we run our application,
    and therefore it's recommended to save our trained model with all of the parameters
    that have been learned. In future executions, we only have to load/read from our
    saved model without training, except if we need to update our model with more
    sample data.
  prefs: []
  type: TYPE_NORMAL
- en: '`StatModel` is the base class of all machine learning classes, such as SVM
    or ANN, except deep learning methods. `StatModel` is basically a virtual class
    that defines the two most important functions—`train` and `predict`. The `train`
    method is the main method that''s responsible for learning model parameters using
    a training dataset. This has the following three possible calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The train function has the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`TrainData`: Training data that can be loaded or created from the `TrainData`
    class. This class is new in OpenCV 3 and helps developers create training data
    and abstract from the machine learning algorithm. This is done because different
    algorithms require different types of structures of arrays for training and prediction,
    such as the ANN algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`samples`: An array of training array samples such as training data in the
    format required by the machine learning algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`layout`: `ROW_SAMPLE` (training samples are the matrix rows) or `COL_SAMPLE`
    (training samples are the matrix columns).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`responses`: Vector of responses associated with the sample data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags`: Optional flags defined by each method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last train method creates and trains a model of the `_TP` class type. The
    only classes accepted are the classes that implement a static create method with
    no parameters or with all default parameter values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `predict` method is much simpler and has only one possible call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The predict function has the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`samples`: The input samples to predict results from the model can consist
    of any amount of data, whether single or multiple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`results`: The results of each input row sample (computed by the algorithm
    from the previously trained model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flags`: These optional flags are model-dependent. Some models, such as Boost,
    are recognized by the SVM `StatModel::RAW_OUTPUT` flag, which makes the method
    return the raw results (the sum), and not the class label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `StatModel` class provides an interface for other very useful methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`isTrained()` returns true if the model is trained'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '`isClassifier()` returns true if the model is a classifier, or false in the
    case of regression'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`getVarCount()` returns the number of variables in training samples'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save(const string& filename)` saves the model in the filename'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Ptr<_Tp> load(const string& filename)` loads the `<indexentry content="StatModel
    class:Ptr load(const string& filename)">` model from a filename, for example—`Ptr<SVM>
    svm = StatModel::load<SVM>("my_svm_model.xml")`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`calcError(const Ptr<TrainData>& data, bool test, OutputArray resp)` calculates
    the error from test data, where the data is the training data. If the test parameter
    is true, the method calculates the error from a test subset of data; if its false,
    the method calculates the error from all training data. `resp` is the optional
    output result.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we are going to introduce how a basic application that uses machine learning
    in a computer vision application is constructed.
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision and the machine learning workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Computer vision applications with machine learning have a common basic structure.
    This structure is divided into different steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-process**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Segmentation**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature extraction**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Classification result**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**P****ost-process**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These are common in almost all computer vision applications, while others are
    omitted. In the following diagram, you can see the different steps that are involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/521ce802-ce25-4153-bd62-596f5840e17a.png)'
  prefs: []
  type: TYPE_IMG
- en: Almost all computer vision applications start with a **Pre-process** applied
    to the input image, which consists of the removal of light and noise, filtering,
    blurring, and so on. After applying all pre-processing required to the input image,
    the second step is **Segmentation**. In this step, we have to extract the regions
    of interest in the image and isolate each one as a unique object of interest.
    For example, in a face detection system, we have to separate the faces from the
    rest of the parts in the scene. After detecting the objects inside the image,
    we continue to the next step. Here, we have to extract the features of each one;
    the features are normally a vector of characteristics of objects. A characteristic
    describes our objects and can be the area of an object, contour, texture pattern,
    pixels, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have the descriptor, also known as a feature vector or feature set,
    of our object. Descriptors are the features that describe an object, and we use
    these to train or predict a model. To do this, we have to create a large dataset
    of features where thousands of images are pre-processed. We then use the extracted
    features (image/object characteristics) such as area, size, and aspect ration,
    in the **Train** model function we choose. In the following diagram, we can see
    how a dataset is fed into a **Machine Learning Algorithm** to train and **generate**
    a **Model**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff9a0fe6-976b-48b7-aaf7-bed6afbe594a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we **Train** with a dataset, the **Model** learns all the parameters required
    to be able to predict when a new vector of features with an unknown label is given
    as input to our algorithm. In the following diagram, we can see how an unknown
    vector of features is used to **Predict** using the generated **Model**, thus
    returning the **Classification result** or regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddc7b6bc-9708-454e-9738-f97959badc3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After predicting the result, the post-processing of output data is sometimes
    required, for example, merging multiple classifications to decrease the prediction
    error or merging multiple labels. A sample case in Optical Character recognition
    is where the **Classification result** is according to each predicted character,
    and by combining the results of character recognition, we construct a word. This
    means that we can create a post-processing method to correct errors in detected
    words. With this small introduction to machine learning for computer vision, we
    are going to implement our own application that uses machine learning to classify
    objects in a slide tape. We are going to use support vector machines as our classification
    method and explain how to use them. The other machine learning algorithms are
    used in a very similar way. The OpenCV documentation has detailed information
    about all of the machine learning algorithms at the following link: [https://docs.opencv.org/master/dd/ded/group__ml.html](https://docs.opencv.org/master/dd/ded/group__ml.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic object inspection classification example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 5](b788527c-5892-4547-8add-0864ccbd3f95.xhtml), *Automated Optical
    Inspection, Object Segmentation, and Detection*, we looked at an example of automatic
    object inspection segmentation where a carrier tape contained three different
    types of object: nuts, screws, and rings. With computer vision, we will be able
    to recognize each one of these so that we can send notifications to a robot or
    put each one in a different box. The following is a basic diagram of the carrier
    tape:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e9e15e5-03f9-44fa-a97e-16d176e4c291.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In [Chapter 5](b788527c-5892-4547-8add-0864ccbd3f95.xhtml), *Automated Optical
    Inspection*, *Object Segmentation*, *and Detection*, we pre-processed the input
    images and extracted the regions of interest, isolating each object using different
    techniques. Now, we are going to apply all the concepts we explained in the previous
    sections in this example to extract features and classify each object, allowing
    the robot to put each one in a different box. In our application, we are only
    going to show the labels of each image, but we could send the positions in the
    image and the label to other devices, such as a robot. At this point, our goal
    is to give an input image with different objects, allowing the computer to detect
    the objects and show the objects'' names over each image, as demonstrated in the
    following images. However, to learn the steps of the whole process, we are going
    to train our system by creating a plot to show the feature distribution that we
    are going to use, and visualize it with different colors. We will also show the
    pre-processed input image, and the output classification result obtained. The
    final result looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58162061-407f-44aa-98c5-941e9a0a5ee7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are going to follow these steps for our example application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each input image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Segment the image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For each object in an image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Add the features to the training feature vector with a corresponding label (nut,
    screw, ring)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an SVM model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train our SVM model with the training feature vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preprocess the input image to classify each segmented object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Segment the input image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each object detected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the features
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict it with the SVM
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: model
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Paint the result in the output image
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: For pre-processing and segmentation, we are going to use the code found in [Chapter
    5](b788527c-5892-4547-8add-0864ccbd3f95.xhtml), *Automated Optical Inspection*,
    *Object Segmentation*, *and Detection.* We are then going to explain how to extract
    the features and create the vectors required to **train** and **predict** our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next thing we need to do is extract the features for each object. To understand
    the feature vector concept, we are going to extract very simple features in our
    example, as this is enough to get good results. In other solutions, we can get
    more complex features such as texture descriptors, contour descriptors, and so
    on. In our example, we only have nuts, rings, and screws in different positions
    and orientations in the image. The same object can be in any position of image
    and orientation, for example, the screw or the nut. We can see different orientations in
    the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79ebe946-48c9-4941-8750-df622f1880ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are going to explore some features or characteristics that could improve
    the accuracy of our machine learning algorithm. These possible characteristics
    of our different objects (nuts, screws, and rings) are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The area of the object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aspect ratio, that is, the width divided by the height of the bounding rectangle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of holes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of contour sides
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These characteristics can describe our objects very well, and if we use all
    of them, the classification error will be very small. However, in our implemented
    example, we are only going to use the first two characteristics, area and aspect
    ratio, for learning purposes, because we can plot these characteristics in a 2D
    graphic and show that these values correctly describe our objects. We can also
    show that we can visually differentiate between one kind of object and another
    in the graphic plot. To extract these features, we are going to use the black/white
    ROI image as input, where only one object appears in white with a black background.
    This input is the segmentation result of [Chapter 5](b788527c-5892-4547-8add-0864ccbd3f95.xhtml),
    *Automated Optical Inspection*, *Object Segmentation*, *and Detection*. We are
    going to use the `findCountours` algorithm for segmenting objects and create the
    `ExtractFeatures` function for this purpose, as we can see in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let's explain the code that we use to extract features. We are going to create
    a function that has one image as input and return two vectors of the left and
    top position for each object detected in the image as a parameter. This data will
    be used for drawing the corresponding label over each object. The output of a
    function is a vector of vectors of floats. In other words, it is a matrix where
    each row contains the features of each object that's detected.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to create the output vector variable and the contours variable
    that are going to be used in our find contours algorithm segmentation. We also
    have to create a copy of our input image, because the `findCoutours` OpenCV functions
    modify the input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the `findContours` function to retrieve each object in an image.
    If we don''t detect any contour, we return an empty output matrix, as we can see
    in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If objects are detected, for each contour we are going to draw the object in
    white on a black image (zero values). This will be done using `1` values, like
    a mask image. The following piece of code generates the mask image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s important to use the value of `1` to draw inside the shape because we
    can calculate the area by summing all of the values inside the contour, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This area is our first feature. We are going to use this value as a filter
    to remove all possible small objects that we have to avoid. All objects with an
    area less than the minimum threshold area that we considered will be discarded.
    After passing the filter, we create the second feature and the aspect ratio of
    the object. This refers to the maximum of the width or height, divided by the
    minimum of the width or height. This feature can tell the difference between the
    screw and other objects easily. The following code describes how to calculate
    the aspect ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have the features, we only have to add them to the output vector. To
    do this, we will create a row vector of floats and add the values, followed by
    adding this row to the output vector, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If the left and top parameters are passed, then add the top-left values to
    output the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we are going to show the detected objects in a window for user feedback.
    When we finish processing all of the objects in the image, we are going to return
    the output feature vector, as described in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have extracted the features of each input image, we can continue
    with the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Training an SVM model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now going to use supervised learning and then obtain a set of images
    for each object and its corresponding label. There is no minimum number of images
    in the dataset; if we provide more images for the training process, we will get
    a better classification model (in most cases). However, for simple classifiers,
    it could be enough to train simple models. To do this, we created three folders
    (`screw`, `nut`, and `ring`), where all of the images of each type are placed
    together. For each image in the folder, we have to extract the features, add them
    to the `train` feature matrix and, at the same time, create a new vector with
    the labels for each row corresponding to each training matrix. To evaluate our
    system, we will split each folder into a number of images according to testing
    and training. We will leave around 20 images for testing and the others for training.
    We are then going to create two vectors of labels and two matrices for training
    and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go inside of our code. First, we have to create our model. We are going
    to declare the model out of all functions to be able to gain access to it as a
    global variable. OpenCV uses the `Ptr` template class for pointer management:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'After declaring the pointer to the new SVM model, we are going to create it
    and train it. We created the `trainAndTest` function for this purpose. The complete
    function code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s explain the code. First of all, we have to create the required
    variables to store the training and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned previously, we have to read all of the images from each folder,
    extract the features, and save them in our training and testing data. To do this,
    we are going to use the `readFolderAndExtractFeatures` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `readFolderAndExtractFeatures` function uses the `VideoCapture` OpenCV
    function to read all of the images in a folder, including videos and camera frames.
    For each image that''s read, we extract the features and add them to the corresponding
    output vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After filling all of the vectors with features and labels, we have to convert
    from vectors to an OpenCV `Mat` format so that we can send it to the training
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to create and train our machine learning model. As we stated
    previously, we are going to use the support vector machine for this. First, we
    are going to set up the basic model parameters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now going to define the SVM type and kernel to use, as well as the criteria
    to stop the learning process. In our case, we are going to use a number of maximum
    iterations, stopping at 100 iterations. For more information about each parameter
    and what it does, check the OpenCV documentation at the following link:[ https://docs.opencv.org/master/d1/d2d/classcv_1_1ml_1_1SVM.html](https://docs.opencv.org/master/d1/d2d/classcv_1_1ml_1_1SVM.html).
    After creating the setup parameters, we are going to create the model by calling
    the `train` method and using `trainingDataMat` and response matrices as a `TrainData`
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the test vector (setting the `num_for_test` variable to greater than
    `0`) to obtain an approximation error of our model. To get the error estimation,
    we are going to predict all test vector features to obtain the SVM prediction
    results and compare these results to the original labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We use the `predict` function by using the `testDataMat` features and a new
    `Mat` for prediction results. The `predict` function makes it possible to make
    multiple predictions at the same time, giving a matrix as the result instead of
    only one row or vector. After prediction, we only have to compute the differences
    of `testPredict` with our `testResponses` (the original labels). If there are
    differences, we only have to count how many there are and divide this by the total
    number of tests in order to calculate the error.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the new `TrainData` class to generate the feature vectors, samples,
    and split our train data between test and train vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we are going to show the training data in a 2D plot, where the *y*-axis
    is the aspect ratio feature and the *x*-axis is the area of objects. Each point
    has different colors and shapes (cross, square, and circle) that show each different
    kind of object, and we can clearly see the groups of objects in the following
    image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0d5aeff-d753-4785-a611-f29df5d40c3f.png)'
  prefs: []
  type: TYPE_IMG
- en: We are now very close to finishing our application sample. At this point, we
    have trained the SVM model; we can now use it for classification to detect the
    type of a new incoming and unknown feature vector. The next step is to predict
    an input image with unknown objects.
  prefs: []
  type: TYPE_NORMAL
- en: Input image prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to explain the main function, which loads the input image
    and predicts the objects that appear inside it. We are going to use something
    like the following picture as the input image. Here, multiple different objects appear in
    the image. We did not have the labels or names of these, but the computer must
    be able to identify them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fda7040-c34d-4631-a829-c029286088f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As with all training images, we have to load and pre-process the input image,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we load and convert the image into gray color values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we apply the pre-processing tasks (as we learned in [Chapter 5](b788527c-5892-4547-8add-0864ccbd3f95.xhtml), *Automated
    Optical Inspect*ion, *Object Segmentation*, *and Detection)* using the `preprocessImage`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are going to extract the feature of vectors for all objects that appear
    in the image and the top-left positions of each one by using the `ExtractFeatures`
    that we previously described:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We store each object we detect as a feature row and then convert each row as
    a `Mat` of one row and two features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we can predict the single object using the `predict` function of
    our `StatModel` SVM. The float result of the prediction is the label of the object
    detected. Then, to finish the application, we have to draw the label of each object
    that''s detected and classified over the output image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We are going to use a `stringstream` to store the text and a `Scalar` to store
    the color for each different label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We are also going to draw the label text over each object using its detected
    position in the `ExtractFeatures` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we are going to draw our results in the output window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The final result of our application shows a window tiled with four screens.
    Here, the top-left image is the input training image, the top-right is the plot
    training image, the bottom left is the input image to analyze pre-processed images,
    and the bottom-right is the final result of the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/162298b6-6d4e-42e0-8019-c7467243b7fc.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we learned about the basics of machine learning and applied
    them to a small sample application. This allowed us to understand the basic techniques
    that we can use to create our own machine learning application. Machine learning
    is complex and involves different techniques for each use case (supervised learning,
    unsupervised, clustering, and so on). We also learned how to create the most typical
    machine learning application, the supervised learning application, with SVM. The
    most important concepts in supervised machine learning are as follows: you must
    have an appropriate number of samples or a dataset, you must accurately choose
    the features that describe our objects (for more information on image features,
    go to [Chapter 8](58a72603-be5a-465f-aa7b-fc8ab1aae596.xhtml), *Video Surveillance*,
    *Background Modeling*, *and Morphological Operations*)*,* and you must choose
    a model that gives the best predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: If we don't get the correct predictions, we have to check each one of these
    concepts to find the issue.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to introduce background subtraction methods,
    which are very useful for video surveillance applications where the background
    doesn't give us any interesting information and must be discarded so that we can
    segment the image to detect and analyze the image objects.
  prefs: []
  type: TYPE_NORMAL
