["```py\nfrom sklearn import linear_model, datasets, preprocessing\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.neural_network import BernoulliRBM\nfrom pandas_ml import ConfusionMatrix\nimport numpy as np\nimport pandas as pd\n```", "```py\nBC = datasets.load_breast_cancer()\n```", "```py\nprint(BC.data.shape)\nprint(BC.target.shape)\n```", "```py\n(569, 30)\n(569,)\n```", "```py\nX = BC.data\nY = BC.target\n```", "```py\nXdata=pd.DataFrame(X)\nprint(Xdata.describe())\n```", "```py\n               0           1           2            3           4   \\\ncount  569.000000  569.000000  569.000000   569.000000  569.000000\nmean    14.127292   19.289649   91.969033   654.889104    0.096360\nstd      3.524049    4.301036   24.298981   351.914129    0.014064\nmin      6.981000    9.710000   43.790000   143.500000    0.052630\n25%     11.700000   16.170000   75.170000   420.300000    0.086370\n50%     13.370000   18.840000   86.240000   551.100000    0.095870\n75%     15.780000   21.800000  104.100000   782.700000    0.105300\nmax     28.110000   39.280000  188.500000  2501.000000    0.163400\n```", "```py\nX = (X - np.min(X, 0)) / (np.max(X, 0) - np.min(X, 0))\n```", "```py\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n```", "```py\nRbmModel = BernoulliRBM(random_state=0, verbose=True)\n```", "```py\nFitRbmModel = RbmModel.fit_transform(X_train, Y_train)\n```", "```py\nLogModel = linear_model.LogisticRegression()\n```", "```py\nLogModel.coef_ = FitRbmModel\n```", "```py\nClassifier = Pipeline(steps=[('RbmModel', RbmModel), ('LogModel', LogModel)])\n```", "```py\nLogModel.fit(X_train, Y_train)\nClassifier.fit(X_train, Y_train)\n```", "```py\nprint (\"The RBM model:\")\nprint (\"Predict: \", Classifier.predict(X_test))\nprint (\"Real:    \", Y_test)\n```", "```py\nCM = ConfusionMatrix(Y_test, Classifier.predict(X_test))\nCM.print_stats()\n```", "```py\npopulation: 114\nP: 72\nN: 42\nPositiveTest: 87\nNegativeTest: 27\nTP: 71\nTN: 26\nFP: 16\nFN: 1\nTPR: 0.9861111111111112\nTNR: 0.6190476190476191\nPPV: 0.8160919540229885\nNPV: 0.9629629629629629\nFPR: 0.38095238095238093\nFDR: 0.1839080459770115\nFNR: 0.013888888888888888\nACC: 0.8508771929824561\nF1_score: 0.8930817610062893\nMCC: 0.6866235389841608\ninformedness: 0.6051587301587302\nmarkedness: 0.7790549169859515\nprevalence: 0.631578947368421\nLRP: 2.588541666666667\nLRN: 0.022435897435897433\nDOR: 115.37500000000003\nFOR: 0.037037037037037035\n```", "```py\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n```", "```py\nfrom keras.datasets import mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n```", "```py\nx_train = x_train.astype('float32') / 255\nx_test = x_test.astype('float32') / 255\n```", "```py\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n```", "```py\nprint (x_train.shape)\nprint (x_test.shape)\n```", "```py\n(60000, 28, 28)\n(10000, 28, 28)\n(60000, 784)\n(10000, 784)\n```", "```py\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nmodel = Sequential([\n    Dense(32, input_shape=(784,)),\n    Activation('relu'),\n    Dense(10),\n    Activation('softmax'),\n])\n```", "```py\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))\n```", "```py\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\ninputs = Input(shape=(784,))\nx = Dense(64, activation='relu')(inputs)\nx = Dense(64, activation='relu')(x)\npredictions = Dense(10, activation='softmax')(x)\nmodel = Model(inputs=inputs, outputs=predictions)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(data, labels) \n```", "```py\nInputModel = Input(shape=(784,))\n```", "```py\nEncodedLayer = Dense(32, activation='relu')(InputModel)\n```", "```py\nDecodedLayer = Dense(784, activation='sigmoid')(EncodedLayer)\n```", "```py\nAutoencoderModel = Model(InputModel, DecodedLayer)\n```", "```py\nAutoencoderModel.compile(optimizer='adadelta', loss='binary_crossentropy')\n```", "```py\nhistory = AutoencoderModel.fit(x_train, x_train,\n                batch_size=256,\n                epochs=100,\n                shuffle=True,\n                validation_data=(x_test, x_test))\n```", "```py\nDecodedDigits = AutoencoderModel.predict(x_test)\n```", "```py\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Autoencoder Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n```", "```py\nn=5\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(DecodedDigits[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n```"]