<html><head></head><body>
		<div><h1 id="_idParaDest-21" class="chapter-number"><a id="_idTextAnchor020"/><st c="0">1</st></h1>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/><a id="_idTextAnchor022"/><st c="2">Imputing Missing Data</st></h1>
			<p><st c="23">Missing data—meaning the absence</st><a id="_idIndexMarker000"/><st c="56"> of values for certain observations—is an unavoidable problem in most data sources. </st><st c="140">Some machine learning model implementations can handle missing data out of the box. </st><st c="224">To train other models, we must remove observations with missing data or transform them into </st><st c="316">permitted values.</st></p>
			<p><st c="333">The act of replacing missing data</st><a id="_idIndexMarker001"/><st c="367"> with their statistical estimates is called </st><strong class="bold"><st c="411">imputation</st></strong><st c="421">. The goa</st><a id="_idTextAnchor023"/><st c="430">l of any imputation technique is to produce a complete dataset. </st><st c="495">There are multiple imputation methods. </st><st c="534">We select which one to use, depending on whether the data is missing at random, the proportion of missing values, and the machine learning model we intend to use. </st><st c="697">In this chapter, we will discuss several </st><st c="738">imputation methods.</st></p>
			<p><st c="757">This chapter will cover the </st><st c="786">following recipes:</st></p>
			<ul>
				<li><st c="804">Removing observations with </st><st c="832">missing data</st></li>
				<li><st c="844">Performing mean or </st><st c="864">median imputation</st></li>
				<li><st c="881">Imputing </st><st c="891">categorical variables</st></li>
				<li><st c="912">Replacing missing values with an </st><st c="946">arbitrary number</st></li>
				<li><st c="962">Finding extreme values </st><st c="986">for imputation</st></li>
				<li><st c="1000">Marking </st><st c="1009">imputed values</st></li>
				<li><st c="1023">Implementing forward and </st><st c="1049">backward fill</st></li>
				<li><st c="1062">Carrying </st><st c="1072">out interpolation</st></li>
				<li><st c="1089">Performing multivariate imputation by </st><st c="1128">chained equations</st></li>
				<li><st c="1145">Estimating missing data with </st><st c="1175">nearest neighbo</st><a id="_idTextAnchor024"/><st c="1190">r</st><a id="_idTextAnchor025"/><a id="_idTextAnchor026"/><st c="1192">s</st></li>
			</ul>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor027"/><st c="1193">Technical requirements</st></h1>
			<p><st c="1215">In this chapter, we will use the Python libraries Matplotlib, pandas, NumPy, scikit-learn, and Feature-engine. </st><st c="1327">If you need to install Python, the free Anaconda Python distribution (</st><a href="https://www.anaconda.com/"><st c="1397">https://www.anaconda.com/</st></a><st c="1423">) includes most numerical </st><st c="1450">computing libraries.</st></p>
			<p><code><st c="1470">feature-engine</st></code><st c="1485"> can be installed with </st><code><st c="1508">pip</st></code> <st c="1511">as follows:</st></p>
			<pre class="console"><st c="1523">
pip install feature-engine</st></pre>			<p><st c="1550">If you use Anaconda, you can install </st><code><st c="1588">feature-engine</st></code> <st c="1602">with </st><code><st c="1608">conda</st></code><st c="1613">:</st></p>
			<pre class="console"><st c="1615">
conda install -c conda-forge feature_engine</st></pre>			<p class="callout-heading"><st c="1659">Note</st></p>
			<p class="callout"><st c="1664">The recipes from this chapter were created using the latest versions of the Python libraries at the time of publishing. </st><st c="1785">You can check the versions in the </st><code><st c="1819">requirements.txt</st></code><st c="1835"> file in the accompanying GitHub repository, </st><st c="1880">at </st><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/requirements.txt"><st c="1883">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/requirements.txt</st></a><st c="1994">.</st></p>
			<p><st c="1995">We will use the </st><strong class="bold"><st c="2012">Credit Approval</st></strong><st c="2027"> dataset from the </st><em class="italic"><st c="2045">UCI Machine Learning Repository</st></em><st c="2076"> (</st><a href="https://archive.ics.uci.edu/"><st c="2078">https://archive.ics.uci.edu/</st></a><st c="2106">), licensed under the CC BY 4.0 creative commons attribution: </st><a href="https://creativecommons.org/licenses/by/4.0/legalcode"><st c="2169">https://creativecommons.org/licenses/by/4.0/legalcode</st></a><st c="2222">. You’ll find the dataset at this </st><st c="2256">link: </st><a href="http://archive.ics.uci.edu/dataset/27/credit+approval"><st c="2262">http://archive.ics.uci.edu/dataset/27/credit+approval</st></a><st c="2315">.</st></p>
			<p><st c="2316">I downloaded and modified the data as shown in this </st><st c="2369">notebook: </st><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/credit-approval-dataset.ipynb"><st c="2379">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/credit-approval-dataset.ipynb</st></a></p>
			<p><st c="2532">We will also use the </st><strong class="bold"><st c="2554">air passenger</st></strong><st c="2567"> dataset located in Facebook’s Prophet GitHub repository (</st><a href="https://github.com/facebook/prophet/blob/main/examples/example_air_passengers.csv"><st c="2625">https://github.com/facebook/prophet/blob/main/examples/example_air_passengers.csv</st></a><st c="2707">), licensed under the MIT </st><st c="2734">license: </st><a href="https://github.com/facebook/prophet/blob/main/LICENSE"><st c="2743">https://github.com/facebook/prophet/blob/main/LICENSE</st></a></p>
			<p><st c="2796">I modified the data as shown in this </st><st c="2834">notebook: </st><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/air-passengers-dataset.ipynb"><st c="2844">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/air-passengers-dataset.ipynb</st></a></p>
			<p><st c="2996">You’ll find a copy of the modified data sets in the accompanying GitHub </st><st c="3069">repository: </st><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/"><st c="3081">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputatio</st><st c="3203">n</st><st c="3205">/</st></a></p>
			<h1 id="_idParaDest-24"><a id="_idTextAnchor032"/><st c="3206">Removing observations with missing data</st></h1>
			<p><strong class="bold"><st c="3245">Complete Case Analysis</st></strong><st c="3268"> (</st><strong class="bold"><st c="3270">CCA</st></strong><st c="3273">), als</st><a id="_idTextAnchor033"/><st c="3280">o called list-wis</st><a id="_idTextAnchor034"/><st c="3298">e deletion of cases, consists</st><a id="_idIndexMarker002"/><st c="3328"> of discarding</st><a id="_idIndexMarker003"/><st c="3342"> observations</st><a id="_idIndexMarker004"/><st c="3355"> with missing data. </st><st c="3375">CCA can be applied to both categorical</st><a id="_idIndexMarker005"/><st c="3413"> and numerical variables. </st><st c="3439">With CCA, we pre</st><a id="_idTextAnchor035"/><st c="3455">serve the distribution o</st><a id="_idTextAnchor036"/><st c="3480">f the variables after the imputation, provided the data is missing at random and only in a small proportion of observations. </st><st c="3606">However, if data is missing across many variables, CCA may lead to the removal of a large portion of </st><st c="3707">the dataset.</st></p>
			<p class="callout-heading"><st c="3719">Note</st></p>
			<p class="callout"><st c="3724">Use CCA only when a small number of observations are missing and you have good reasons to believe that they are not important to </st><st c="3854">your m</st><a id="_idTextAnchor037"/><a id="_idTextAnchor038"/><st c="3860">odel.</st></p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor039"/><st c="3866">How to do it...</st></h2>
			<p><st c="3882">Let’s begin by making some imports and loading </st><st c="3930">the dataset:</st></p>
			<ol>
				<li><st c="3942">Let’s import </st><code><st c="3956">pandas</st></code><st c="3962">, </st><code><st c="3964">matplotlib</st></code><st c="3974">, and the train/test split function </st><st c="4010">from scikit-learn:</st><pre class="source-code"><st c="4028">
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split</st></pre></li>				<li><st c="4133">Let’s load and display the dataset described in the </st><em class="italic"><st c="4186">Technical </st></em><em class="italic"><st c="4196">requirements</st></em><st c="4208"> section:</st><pre class="source-code"><st c="4217">
data = pd.read_csv("credit_approval_uci.csv")
data.head()</st></pre><p class="list-inset"><st c="4275">In the following image, we see the first 5 rows </st><st c="4324">of data:</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_01.jpg" alt="Figure 1.1 – First 5 rows of the dataset"/><st c="4332"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="4633">Figure 1.1 – First 5 rows of the dataset</st></p>
			<ol>
				<li value="3"><st c="4673">Let’s proceed as we normally</st><a id="_idIndexMarker006"/><st c="4702"> would if we were preparing</st><a id="_idIndexMarker007"/><st c="4729"> the data to train machine learning models; by splitting the data into a training and a </st><st c="4817">test set:</st><pre class="source-code"><st c="4826">
X_train, X_test, y_train, y_test = train_test_split(
    data.drop("target", axis=1),
    data["target"],
    test_size=0.30,
    random_state=42,
)</st></pre></li>				<li><st c="4959">Let’s now make a bar plot with the proportion of missing data per variable in the training and </st><st c="5055">te</st><a id="_idTextAnchor040"/><st c="5057">st sets:</st><pre class="source-code"><st c="5066">
fig, axes = plt.subplots(
    2, 1, figsize=(15, 10), squeeze=False)
X_train.isnull().mean().plot(
    kind='bar', color='grey', ax=axes[0, 0], title="train")
X_test.isnull().mean().plot(
    kind='bar', color='black', ax=axes[1, 0], title="test")
axes[0, 0].set_ylabel('Fraction of NAN')
axes[1, 0].set_ylabel('Fraction of NAN')
plt.show()</st></pre><p class="list-inset"><st c="5395">The previous code block returns</st><a id="_idIndexMarker008"/><st c="5427"> the following bar plots</st><a id="_idIndexMarker009"/><st c="5451"> with the fraction of missing data per variable in the training (top) and test </st><st c="5530">sets (b</st><a id="_idTextAnchor041"/><st c="5537">ottom):</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_02.jpg" alt="Figure 1.2 – Proportion of missing data per variable"/><st c="5545"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="5717">Figure 1.2 – Proportion of missing data per variable</st></p>
			<ol>
				<li value="5"><st c="5769">Now</st><a id="_idTextAnchor042"/><st c="5773">, we’ll remove obser</st><a id="_idTextAnchor043"/><st c="5793">vations if they have missing values in </st><st c="5833">any variable:</st><pre class="source-code"><st c="5846">
train_cca = X_train.dropna()
test_cca = X_test.dropna()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="5902">Note</st></p>
			<p class="callout"><st c="5907">pandas’ </st><code><st c="5916">dropna()</st></code><st c="5924">drops observations with any missing value by default. </st><st c="5979">We can remove observations with missing data in a subset of variables like this: </st><code><st c="6060">data.dropna(subset=["A3", "A4"])</st></code><st c="6092">.</st></p>
			<ol>
				<li value="6"><st c="6093">Let’s print and compare the size of the original and complete </st><st c="6156">case datasets:</st><pre class="source-code"><st c="6170">
print(f"Total observations: {len(X_train)}")
print(f"Observations without NAN: {len(train_cca)}")</st></pre><p class="list-inset"><st c="6268">We removed more than 200 observations with missing data from the training set, as shown in the </st><st c="6364">following output:</st></p><pre class="source-code"><strong class="bold"><st c="6381">Total observations: 483</st></strong>
<strong class="bold"><st c="6405">Observations without NAN: 264</st></strong></pre></li>				<li><st c="6435">After removing</st><a id="_idIndexMarker010"/><st c="6450"> observations from the training</st><a id="_idIndexMarker011"/><st c="6481"> and test sets, we need to align the </st><st c="6518">target variables:</st><pre class="source-code"><st c="6535">
y_train_cca = y_train.loc[train_cca.index]
y_test_cca = y_test.loc[test_cca.index]</st></pre><p class="list-inset"><st c="6618">Now, the datasets and target variables contain the rows without </st><st c="6683">missing data.</st></p></li>				<li><st c="6696">To drop observations with missing data utilizing </st><code><st c="6746">feature-engine</st></code><st c="6760">, let’s import the </st><st c="6779">required transformer:</st><pre class="source-code"><st c="6800">
from feature_engine.imputation import DropMissingData</st></pre></li>				<li><st c="6854">Let</st><a id="_idTextAnchor044"/><st c="6858">’s set up the imputer to auto</st><a id="_idTextAnchor045"/><st c="6888">matically find the variables with </st><st c="6923">missing data:</st><pre class="source-code"><st c="6936">
cca = DropMissingData(variables=None, missing_only=True)</st></pre></li>				<li><st c="6993">Let’s fit the transformer so that it finds the variables with </st><st c="7056">missing data:</st><pre class="source-code"><st c="7069">
cca.fit(X_train)</st></pre></li>				<li><st c="7086">Let’s inspect the variables with NAN that the </st><st c="7133">transformer found:</st><pre class="source-code"><st c="7151">
cca.variables_</st></pre><p class="list-inset"><st c="7166">The previous command returns the names of the variables with </st><st c="7228">missing data:</st></p><pre class="source-code"><strong class="bold"><st c="7241">['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A14']</st></strong></pre></li>				<li><st c="7310">Let’s remove the rows with missing data in the training and </st><st c="7371">test sets:</st><pre class="source-code"><st c="7381">
train_cca = cca.transform(X_train)
test_cca = cca.transform(X_test)</st></pre><p class="list-inset"><st c="7449">Use </st><code><st c="7454">train_cca.isnull().sum()</st></code><st c="7478"> to corroborate</st><a id="_idIndexMarker012"/><st c="7493"> the absence of missing data in the complete</st><a id="_idIndexMarker013"/> <st c="7537">case dataset.</st></p></li>				<li><code><st c="7551">DropMissingData</st></code><st c="7567"> can automatically adjust the target after removing missing data from the </st><st c="7641">training set:</st><pre class="source-code"><st c="7654">
train_c, y_train_c = cca.transform_x_y( X_train, y_train)
test_c, y_test_c = cca.transform_x_y(X_test, y_test)</st></pre></li>			</ol>
			<p><st c="7765">The previous code removed rows with </st><code><st c="7802">nan</st></code><st c="7805"> from the training and test sets and then re-aligned the </st><st c="7862">target variables.</st></p>
			<p class="callout-heading"><st c="7879">Note</st></p>
			<p class="callout"><st c="7884">To remove observations with missing data in a subset of variables, use </st><code><st c="7956">DropMissingData(variables=['A3', 'A4'])</st></code><st c="7995">. To remove rows with </st><code><st c="8017">nan</st></code><st c="8020"> in at least 5% of the variables, </st><st c="8054">use </st><code><st c="8058">DropMissingData(thres</st><a id="_idTextAnchor046"/><a id="_idTextAnchor047"/><st c="8079">hold=0.95)</st></code><st c="8090">.</st></p>
			<h2 id="_idParaDest-26"><st c="8091">How it works</st><a id="_idTextAnchor048"/><st c="8104">...</st></h2>
			<p><st c="8107">In this recipe,</st><a id="_idTextAnchor049"/><st c="8123"> we plotted</st><a id="_idIndexMarker014"/><st c="8134"> the proportion of missing data</st><a id="_idIndexMarker015"/><st c="8165"> in each variable and then removed all observations with </st><st c="8222">missing values.</st></p>
			<p><st c="8237">We used </st><code><st c="8246">pandas</st></code> <code><st c="8252">isnull()</st></code><st c="8261"> and </st><code><st c="8266">mean()</st></code><st c="8272"> methods to determine the proportion of missing observations in each variable. </st><st c="8351">The </st><code><st c="8355">isnull()</st></code><st c="8363"> method created a Boolean vector per variable with </st><code><st c="8414">True</st></code><st c="8418"> and </st><code><st c="8423">False</st></code><st c="8428"> values indicating whether a value was missing. </st><st c="8476">The </st><code><st c="8480">mean()</st></code><st c="8486"> method took the average of these values and returned the proportion of </st><st c="8558">missing data.</st></p>
			<p><st c="8571">We used </st><code><st c="8580">pandas</st></code> <code><st c="8586">plot.bar()</st></code><st c="8597"> to create a bar plot of the fraction of missing data per variable. </st><st c="8665">In </st><em class="italic"><st c="8668">Figure 1</st></em><em class="italic"><st c="8676">.2</st></em><st c="8678">, we saw the fraction of </st><code><st c="8703">nan</st></code><st c="8706"> per variable in the training and </st><st c="8740">test sets.</st></p>
			<p><st c="8750">To remove observations with missing values in </st><em class="italic"><st c="8797">any</st></em><st c="8800"> variable, we used pandas’ </st><code><st c="8827">dropna()</st></code><st c="8835">, thereby obtaining a complete </st><st c="8866">case dataset.</st></p>
			<p><st c="8879">Finally, we removed missing data using Feature-engine’s </st><code><st c="8936">DropMissingData()</st></code><st c="8953">. This imputer automatically identified and stored the variables with missing data from the train set when we called the </st><code><st c="9074">fit()</st></code><st c="9079"> method. </st><st c="9088">With the </st><code><st c="9097">transform()</st></code><st c="9108"> method, the imputer removed observations with</st><a id="_idTextAnchor050"/> <code><st c="9154">nan</st></code><st c="9158"> in those variables. </st><st c="9179">With </st><code><st c="9184">transform_x_y()</st></code><st c="9199">, the imputer removed rows with </st><code><st c="9231">nan</st></code><st c="9234"> from the data sets and then realigned the </st><st c="9277">target variable.</st></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor051"/><st c="9293">See also</st></h2>
			<p><st c="9302">If you want to use </st><code><st c="9322">DropMissingData()</st></code><st c="9339"> within a pipeline together with other Feature-engine or scikit-learn transformers, check</st><a id="_idIndexMarker016"/><st c="9428"> out Feature-engine’s </st><code><st c="9450">Pipeline</st></code><st c="9458">: </st><a href="https://Feature-engine.trainindata.com/en/latest/user_guide/pipeline/Pipeline.html"><st c="9461">https://Feature-engine.trainindata.com/en/latest/user_guide/pipeline/Pipeline.html</st></a><st c="9543">. This pipeline can align the target with the training  and test sets after </st><a id="_idTextAnchor052"/><a id="_idTextAnchor053"/><st c="9618">removing rows.</st></p>
			<h1 id="_idParaDest-28"><st c="9632">Performing mean or median impu</st><a id="_idTextAnchor054"/><st c="9663">tation</st></h1>
			<p><st c="9670">Mean or median imputation</st><a id="_idIndexMarker017"/><st c="9696"> consists of replaci</st><a id="_idTextAnchor055"/><st c="9716">ng missing data</st><a id="_idIndexMarker018"/><st c="9732"> with the variable’s mean or median value. </st><st c="9775">To avoid data leakage, we determine the mean or median using the train set, and then use these values to impute the train and test sets, and all </st><st c="9920">future data.</st></p>
			<p><st c="9932">Scikit-learn and Feature-engine learn the mean or median from the train set and store these parameters for future use out of </st><st c="10058">the box.</st></p>
			<p><st c="10066">In this recipe, we will perform mean and median imputation using </st><code><st c="10132">pandas</st></code><st c="10138">, </st><code><st c="10140">scikit</st></code><st c="10146">-</st><code><st c="10148">learn</st></code><st c="10153">, </st><st c="10155">and </st><code><st c="10159">feature-engine</st></code><st c="10173">.</st></p>
			<p class="callout-heading"><st c="10174">Note</st></p>
			<p class="callout"><st c="10179">Use mean imputation if variables are normally distributed and median imputation otherwise. </st><st c="10271">Mean and median imputation may distort the variable distribution if there is a high percentage </st><a id="_idTextAnchor056"/><a id="_idTextAnchor057"/><st c="10366">of </st><st c="10369">missing data.</st></p>
			<h2 id="_idParaDest-29"><st c="10382">How t</st><a id="_idTextAnchor058"/><st c="10388">o do it...</st></h2>
			<p><st c="10399">Let’s begin </st><st c="10412">this reci</st><a id="_idTextAnchor059"/><st c="10421">pe:</st></p>
			<ol>
				<li><st c="10425">First, we’ll import </st><code><st c="10446">pandas</st></code><st c="10452"> and the required functions and classes from</st><code><st c="10496"> scikit-learn</st></code> <st c="10509">and </st><code><st c="10514">feature-engine</st></code><st c="10528">:</st><pre class="source-code"><st c="10530">
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from feature_engine.imputation import MeanMedianImputer</st></pre></li>				<li><st c="10746">Let’s load the dataset that we prepared in the </st><em class="italic"><st c="10794">Technical </st></em><em class="italic"><st c="10804">requirements</st></em><st c="10816"> section:</st><pre class="source-code"><st c="10825">
data = pd.read_csv("credit_approval_uci.csv")</st></pre></li>				<li><st c="10871">Let’s split the data into train and test sets with their </st><st c="10929">respective targets:</st><pre class="source-code"><st c="10948">
X_train, X_test, y_train, y_test = train_test_split(
    data.drop("target", axis=1),
    data["target"],
    test_size=0.3,
    random_</st><a id="_idTextAnchor060"/><st c="11069">state=0,
)</st></pre></li>				<li><st c="11080">Let’s make a list with</st><a id="_idIndexMarker019"/><st c="11103"> the numerical variables by excluding</st><a id="_idIndexMarker020"/><st c="11140"> variables of </st><st c="11154">type object:</st><pre class="source-code"><st c="11166">
numeric_vars = X_train.select_dtypes(
    exclude="O").columns.to_list()</st></pre><p class="list-inset"><st c="11235">If you execute </st><code><st c="11251">numeric_vars</st></code><st c="11263">, you will see the names of the numerical variables: </st><code><st c="11316">['A2', 'A3', 'A8', 'A11', '</st></code><code><st c="11343">A14', 'A15']</st></code><st c="11356">.</st></p></li>				<li><st c="11357">Let’s capture the variables’ median values in </st><st c="11404">a dictionary:</st><pre class="source-code"><st c="11417">
median_values = X_train[
    numeric_vars].median().to_dict()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="11475">Tip</st></p>
			<p class="callout"><st c="11479">Note how we calculate the median using the train set. </st><st c="11534">We will use these values to replace missing data in the train and test sets. </st><st c="11611">To calculate the mean, use pandas </st><code><st c="11645">mean()</st></code><st c="11651"> instead </st><st c="11660">of </st><code><st c="11663">median()</st></code><st c="11671">.</st></p>
			<p class="list-inset"><st c="11672">If you execute </st><code><st c="11688">median_values</st></code><st c="11701">, you will see a dictionary with the median value per variable: </st><code><st c="11765">{'A2': 28.835, 'A3': 2.75, 'A8': 1.0, 'A11': 0.0, 'A14': 160.0, '</st></code><code><st c="11830">A15': 6.0}.</st></code></p>
			<ol>
				<li value="6"><st c="11842">Let’s replace missing data with </st><st c="11875">the median:</st><pre class="source-code"><st c="11886">
X_train_t = X_train.fillna(value=median_values)
X_test_t = X_test.fillna(value=median_values)</st></pre><p class="list-inset"><st c="11980">If you execute </st><code><st c="11996">X_train_t[numeric_vars].isnull().sum()</st></code><st c="12034"> after the imputation, the number of missing values in the numerical variables should </st><st c="12120">be </st><code><st c="12123">0</st></code><st c="12124">.</st></p></li>			</ol>
			<p class="callout-heading"><st c="12125">Note</st></p>
			<p class="callout"><code><st c="12130">pandas</st></code> <code><st c="12137">fillna()</st></code><st c="12146"> returns a new dataset with imputed values by default. </st><st c="12201">To replace missing data in the original DataFrame, set the </st><code><st c="12260">inplace</st></code><st c="12267"> parameter to </st><code><st c="12281">True</st></code><st c="12285">: </st><code><st c="12288">X_train.fillna(value=median_values, inplace</st><a id="_idTextAnchor061"/><st c="12331">=True)</st></code><st c="12338">.</st></p>
			<p class="list-inset"><st c="12339">Now, let’s impute missing valu</st><a id="_idTextAnchor062"/><st c="12370">es with the median </st><st c="12390">using </st><code><st c="12396">scikit-learn</st></code><st c="12408">.</st></p>
			<ol>
				<li value="7"><st c="12409">Let’s set up the imputer</st><a id="_idIndexMarker021"/><st c="12434"> to replace missing data with</st><a id="_idIndexMarker022"/> <st c="12463">the median:</st><pre class="source-code"><st c="12475">
imputer = SimpleImputer(strategy="median")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="12518">Note</st></p>
			<p class="callout"><st c="12523">To perform mean imputation, set </st><code><st c="12556">SimpleImputer()</st></code><st c="12571"> as follows: </st><code><st c="12584">imputer =</st></code> <code><st c="12593">SimpleImputer(strategy = "</st></code><code><st c="12620">mean")</st></code><st c="12627">.</st></p>
			<ol>
				<li value="8"><st c="12628">We restrict the imputation to the numerical variables by </st><st c="12686">using </st><code><st c="12692">ColumnTransformer()</st></code><st c="12711">:</st><pre class="source-code"><st c="12713">
ct = ColumnTransformer(
    [("imputer", imputer, numeric_vars)],
    remainder="passthrough",
    force_int_remainder_cols=False,
).set_output(transform="pandas")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="12865">Note</st></p>
			<p class="callout"><st c="12870">Scikit-learn can return </st><code><st c="12895">numpy</st></code><st c="12900"> arrays, </st><code><st c="12909">pandas</st></code><st c="12915"> DataFrames, or </st><code><st c="12931">polar</st></code><st c="12936"> frames, depending on how we set out the transform output. </st><st c="12995">By default, it returns </st><code><st c="13018">numpy</st></code><st c="13023"> arrays.</st></p>
			<ol>
				<li value="9"><st c="13031">Let’s fit the imputer</st><a id="_idIndexMarker023"/><st c="13053"> to the train set so that it learns the </st><st c="13093">median</st><a id="_idIndexMarker024"/><st c="13099"> values:</st><pre class="source-code"><st c="13107">
ct.fit(X_tr</st><a id="_idTextAnchor063"/><st c="13119">ain)</st></pre></li>				<li><st c="13124">Let’s check out the learned </st><st c="13153">median values:</st><pre class="source-code"><st c="13167">
ct.named_transformers_.i</st><a id="_idTextAnchor064"/><st c="13192">mputer.statistics_</st></pre><p class="list-inset"><st c="13211">The previous command returns the median values </st><st c="13259">per variable:</st></p><pre class="source-code"><st c="13272">array([ 28.835,   2.75,   1.,   0., 160.,   6.])</st></pre></li>				<li><st c="13313">Let’s replace missing values with </st><st c="13348">the median:</st><pre class="source-code"><st c="13359">
X_train_t = ct.transform(X_train)
X_test_t = ct.transform(X_test)</st></pre></li>				<li><st c="13425">Let’s display the resulting </st><st c="13454">training set:</st><pre class="source-code"><st c="13467">
print(X_train_t.head())</st></pre><p class="list-inset"><st c="13491">We see the resulting DataFrame in the </st><st c="13530">following image:</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_03.jpg" alt="Figure 1.3 – Training set after the imputation. The imputed variables are marked by the imputer prefix; the untransformed variables show the prefix remainder"/><st c="13546"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="14090">Figure 1.3 – Training set after the imputation. </st><st c="14138">The imputed variables are marked by the imputer prefix; the untransformed variables show the prefix remainder</st></p>
			<p class="list-inset"><st c="14247">Finally, let’s perform median imputation </st><st c="14289">using </st><code><st c="14295">feature-engine</st></code><st c="14309">.</st></p>
			<ol>
				<li value="13"><st c="14310">Let’s set up the imputer</st><a id="_idIndexMarker025"/><st c="14335"> to replace missing data in numerical variables</st><a id="_idIndexMarker026"/><st c="14382"> with </st><st c="14388">the median:</st><pre class="source-code"><st c="14399">
imputer = MeanMedianImputer(
    imputation_method="median",
    variables=numeric_vars,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="14482">Note</st></p>
			<p class="callout"><st c="14487">To perform mean imputation, change </st><code><st c="14523">imputation_method</st></code><st c="14540"> to </st><code><st c="14544">"mean"</st></code><st c="14550">. By default </st><code><st c="14563">MeanMedianImputer()</st></code><st c="14582"> will impute all numerical variables in the DataFrame, ignoring categorical variables. </st><st c="14669">Use the </st><code><st c="14677">variables</st></code><st c="14686"> argument to restrict the imputation to a subset of </st><st c="14738">numer</st><a id="_idTextAnchor065"/><st c="14743">ical variables.</st></p>
			<ol>
				<li value="14"><st c="14759">Fit the imputer so th</st><a id="_idTextAnchor066"/><st c="14781">at it learns the </st><st c="14799">median values:</st><pre class="source-code"><st c="14813">
imputer.fit(X_train)</st></pre></li>				<li><st c="14834">Inspect the </st><st c="14847">learned medians:</st><pre class="source-code"><st c="14863">
imputer.imputer_dict_</st></pre><p class="list-inset"><st c="14885">The previous command returns the median values in </st><st c="14936">a dictionary:</st></p><pre class="source-code"><st c="14949">{</st><strong class="bold"><st c="14951">'A2': 28.835, 'A3': 2.75, 'A8': 1.0, 'A11': 0.0, 'A14': 160.0, 'A15': 6.0}</st></strong></pre></li>				<li><st c="15025">Finally, let’s replace the missing values with </st><st c="15073">the median:</st><pre class="source-code"><st c="15084">
X_train = imputer.transform(X_train)
X_test = imputer.transform(X_test)</st></pre></li>			</ol>
			<p><st c="15156">Feature-engine’s </st><code><st c="15174">MeanMedianImputer()</st></code><st c="15193"> returns a </st><code><st c="15204">DataFrame</st></code><st c="15213">. You can check that the imputed variables do not contain missing values </st><st c="15286">using </st><code><st c="15292">X_train[numeric</st><a id="_idTextAnchor067"/><a id="_idTextAnchor068"/><st c="15307">_vars].isnull().mean()</st></code><st c="15330">.</st></p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor069"/><st c="15331">How it works...</st></h2>
			<p><st c="15347">In this recipe, we replaced</st><a id="_idIndexMarker027"/><st c="15375"> missing data with the variable’s median values</st><a id="_idIndexMarker028"/><st c="15422"> using </st><code><st c="15429">pandas</st></code><st c="15435">, </st><code><st c="15437">scikit-learn</st></code><st c="15449">, </st><st c="15451">and </st><code><st c="15455">feature-engine</st></code><st c="15469">.</st></p>
			<p><st c="15470">We divided the dataset into train and test sets using scikit-learn’s </st><code><st c="15540">train_test_split()</st></code><st c="15558"> function. </st><st c="15569">The function takes the predictor variables, the target, the fraction of observations to retain in the test set, and a </st><code><st c="15687">random_state</st></code><st c="15699"> value for reproducibility, as arguments. </st><st c="15741">It returned a train set with 70% of the original observations and a test set with 30% of the original observations. </st><st c="15857">The 70:30 split was done </st><st c="15882">at random.</st></p>
			<p><st c="15892">To impute missing data with pandas, in </st><em class="italic"><st c="15932">step 5</st></em><st c="15938">, we created a dictionary with the numerical variable names as keys and their medians as values. </st><st c="16035">The median values were learned from the training set to avoid data leakage. </st><st c="16111">To replace missing data, we applied </st><code><st c="16147">pandas</st></code><st c="16153">’ </st><code><st c="16156">fillna()</st></code><st c="16164"> to train and test sets, passing the dictionary with the median values per variable as </st><st c="16251">a parameter.</st></p>
			<p><st c="16263">To replace the missing values with the median using </st><code><st c="16316">scikit-learn</st></code><st c="16328">, we used </st><code><st c="16338">SimpleImputer()</st></code><st c="16353"> with the </st><code><st c="16363">strategy</st></code><st c="16371"> set to </st><code><st c="16379">"median"</st></code><st c="16387">. To restrict the imputation to numerical variables, we used </st><code><st c="16448">ColumnTransformer()</st></code><st c="16467">. With the </st><code><st c="16478">remainder</st></code><st c="16487"> argument set to </st><code><st c="16504">passthrough</st></code><st c="16515">, we made </st><code><st c="16525">ColumnTransformer()</st></code><st c="16544"> return </st><em class="italic"><st c="16552">all the variables</st></em><st c="16569"> seen in the training set in the transformed output; the imputed ones followed by those that were </st><st c="16667">not transformed.</st></p>
			<p class="callout-heading"><st c="16683">Note</st></p>
			<p class="callout"><code><st c="16688">ColumnTransformer()</st></code><st c="16708"> changes the names of the variables in the output. </st><st c="16759">The transformed variables show the prefix </st><code><st c="16801">imputer</st></code><st c="16808"> and the unchanged variables show the </st><st c="16846">prefix </st><code><st c="16853">remainder</st></code><st c="16862">.</st></p>
			<p><st c="16863">In </st><em class="italic"><st c="16867">step 8</st></em><st c="16873">, we set the output of the column transformer to </st><code><st c="16922">pandas</st></code><st c="16928"> to obtain a DataFrame as a result. </st><st c="16964">By default, </st><code><st c="16976">ColumnTransformer()</st></code><st c="16995"> returns </st><code><st c="17004">numpy</st></code><st c="17009"> arrays.</st></p>
			<p class="callout-heading"><st c="17017">Note</st></p>
			<p class="callout"><st c="17022">From version 1.4.0, </st><code><st c="17043">scikit-learn</st></code><st c="17055"> transformers can return </st><code><st c="17080">numpy</st></code><st c="17085"> arrays, </st><code><st c="17094">pandas</st></code><st c="17100"> DataFrames, or </st><code><st c="17116">polar</st></code><st c="17121"> frames as a result of the </st><code><st c="17148">transform()</st></code><st c="17159"> method.</st></p>
			<p><st c="17167">With </st><code><st c="17173">fit()</st></code><st c="17178">, </st><code><st c="17180">SimpleImputer()</st></code><st c="17195"> learned the median of each numerical variable in the train set and stored them in its </st><code><st c="17282">statistics_</st></code><st c="17293"> attribute. </st><st c="17305">With </st><code><st c="17310">transform()</st></code><st c="17321">, it replaced the missing values with </st><st c="17359">the medians.</st></p>
			<p><st c="17371">To replace missing values with the median using Feature-engine, we used the </st><code><st c="17448">MeanMedianImputer()</st></code><st c="17467"> with the </st><code><st c="17477">imputation_method</st></code><st c="17494"> set to </st><code><st c="17502">median</st></code><st c="17508">. To restrict the imputation to a subset of variables, we passed the variable names in a list to the </st><code><st c="17609">variables</st></code><st c="17618"> parameter. </st><st c="17630">With </st><code><st c="17635">fit()</st></code><st c="17640">, the transformer learned and stored the median values per</st><a id="_idTextAnchor070"/><st c="17698"> variable</st><a id="_idIndexMarker029"/><st c="17707"> in a dictionary</st><a id="_idIndexMarker030"/><st c="17723"> in its </st><code><st c="17731">imputer_dict_</st></code><st c="17744"> attribute. </st><st c="17756">With </st><code><st c="17761">transform()</st></code><st c="17772">, it replaced the missing values, re</st><a id="_idTextAnchor071"/><a id="_idTextAnchor072"/><st c="17808">turning a </st><st c="17819">pandas DataFrame.</st></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor073"/><st c="17836">Imputing categorical variables</st></h1>
			<p><st c="17867">We typically impute categorical variables</st><a id="_idIndexMarker031"/><st c="17909"> with the most frequent category, or with a specific string. </st><st c="17970">To avoid data leakage, we find the frequent categories from the train set. </st><st c="18045">Then, we use these values to impute the train, test, and future datasets. </st><code><st c="18119">scikit-learn</st></code><st c="18131"> and </st><code><st c="18136">feature-engine</st></code><st c="18150"> find and store the frequent categories for the imputation, out of </st><st c="18217">the box.</st></p>
			<p><st c="18225">In this recipe, we will replace missing data in categorical variables with the most frequent category, </st><a id="_idTextAnchor074"/><a id="_idTextAnchor075"/><st c="18329">or with an </st><st c="18340">arbitrary string.</st></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor076"/><st c="18357">How to do it...</st></h2>
			<p><st c="18373">To begin, let’s make a few imports and prepare </st><st c="18421">the data:</st></p>
			<ol>
				<li><st c="18430">Let’s import </st><code><st c="18444">pandas</st></code><st c="18450"> and the required functions and classes from </st><code><st c="18495">scikit-learn</st></code> <st c="18507">and </st><code><st c="18512">feature-engine</st></code><st c="18526">:</st><pre class="source-code"><st c="18528">
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from feature_engine.imputation import CategoricalImputer</st></pre></li>				<li><st c="18745">Let’s load the dataset that we prepared in the </st><em class="italic"><st c="18793">Technical </st></em><em class="italic"><st c="18803">requirements</st></em><st c="18815"> section:</st><pre class="source-code"><st c="18824">
data = pd.read_csv("credit_ap</st><a id="_idTextAnchor077"/><st c="18854">proval_uci.csv")</st></pre></li>				<li><st c="18871">Let’s split the data into train and test sets and their </st><st c="18928">respective targets:</st><pre class="source-code"><st c="18947">
X_train, X_test, y_train, y_test = train_test_split(
    data.drop("target", axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="19078">Let’s capture the categorical variables in </st><st c="19122">a list:</st><pre class="source-code"><st c="19129">
categorical_vars = X_train.select_dtypes(
    include="O").columns.to_list()</st></pre></li>				<li><st c="19202">Let’s store the variables’ most frequent categories in </st><st c="19258">a dictionary:</st><pre class="source-code"><st c="19271">
frequent_values = X_train[
    categorical_vars].mode().iloc[0].to_dict()</st></pre></li>				<li><st c="19341">Let’s replace missing values</st><a id="_idIndexMarker032"/><st c="19370"> with the </st><st c="19380">frequent categories:</st><pre class="source-code"><st c="19400">
X_train_t = X_train.fillna(value=frequent_values)
X_test_t = X_test.fillna(value=frequent_values)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="19498">Note</st></p>
			<p class="callout"><code><st c="19503">fillna()</st></code><st c="19512"> returns a new DataFrame with the imputed values by default. </st><st c="19573">We can replace missing data in the original DataFrame by executing </st><code><st c="19640">X_train.fillna(value=frequent_valu</st><a id="_idTextAnchor078"/><st c="19674">es, inplace=True)</st></code><st c="19692">.</st></p>
			<ol>
				<li value="7"><st c="19693">To replace missing data with a specific string, let’s create an imputation dictionary with the categorical variable names as the keys and an arbitrary string as </st><st c="19855">the values:</st><pre class="source-code"><st c="19866">
imputation_dict = {var:
     "no_data" for var in categorical_vars}</st></pre><p class="list-inset"><st c="19929">Now, we can use this dictionary and the code in </st><em class="italic"><st c="19978">step 6</st></em><st c="19984"> to replace </st><st c="19996">missing data.</st></p></li>			</ol>
			<p class="callout-heading"><st c="20009">Note</st></p>
			<p class="callout"><st c="20014">With </st><code><st c="20020">pandas</st></code> <code><st c="20026">value_counts()</st></code><st c="20041"> we can see the string added by the imputation. </st><st c="20089">Try executing, for </st><st c="20108">example, </st><code><st c="20117">X_train["A1"].value_counts()</st></code><st c="20145">.</st></p>
			<p class="list-inset"><st c="20146">Now, let’s impute missing values with the most frequent category </st><st c="20212">using </st><code><st c="20218">scikit-learn</st></code><st c="20230">.</st></p>
			<ol>
				<li value="8"><st c="20231">Let’s set up the imputer to find the most frequent category </st><st c="20292">per variable:</st><pre class="source-code"><st c="20305">
imputer = SimpleImputer(strategy='most_frequent')</st></pre></li>			</ol>
			<p class="callout-heading"><st c="20355">Note</st></p>
			<p class="callout"><code><st c="20360">SimpleImputer()</st></code><st c="20376"> will learn the mode for numerical and categorical variables alike. </st><st c="20444">But in practice, mode imputation is done for categorical </st><st c="20501">variables only.</st></p>
			<ol>
				<li value="9"><st c="20516">Let’s restrict the imputation </st><a id="_idIndexMarker033"/><st c="20547">to the </st><st c="20554">categorical variables:</st><pre class="source-code"><st c="20576">
ct = ColumnTransformer(
    [("imputer",imputer, categorical_vars)],
    remainder="passthrough"
    ).set_output(transform="pandas")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="20698">Note</st></p>
			<p class="callout"><st c="20703">To impute missing data with a string instead of the most frequent category, set </st><code><st c="20784">SimpleImputer()</st></code><st c="20799"> as follows: </st><code><st c="20812">imputer = </st></code><code><st c="20822">SimpleImputer(strategy="constant", </st><a id="_idTextAnchor079"/><st c="20857">fill_value="missing")</st></code><st c="20878">.</st></p>
			<ol>
				<li value="10"><st c="20879">Fit the imputer to the train set so that it learns the most </st><st c="20940">frequent values:</st><pre class="source-code"><st c="20956">
ct.fit(X_train)</st></pre></li>				<li><st c="20972">Let’s take a look at the most frequent values learned by </st><st c="21030">the imputer:</st><pre class="source-code"><st c="21042">
ct.named_transformers_.imputer.statistics_</st></pre><p class="list-inset"><st c="21085">The previous command returns the most frequent values </st><st c="21140">per variable:</st></p><pre class="source-code"><strong class="bold"><st c="21153">array(['b', 'u', 'g', 'c', 'v', 't', 'f', 'f', 'g'], dtype=object)</st></strong></pre></li>				<li><st c="21220">Finally, let’s replace missing values with the </st><st c="21268">frequent categories:</st><pre class="source-code"><st c="21288">
X_train_t = ct.transform(X_train)
X_test_t = ct.transform(X_test)</st></pre><p class="list-inset"><st c="21354">Make sure to inspect the resulting DataFrames by </st><st c="21404">executing </st><code><st c="21414">X_train_t.head()</st></code><st c="21430">.</st></p></li>			</ol>
			<p class="callout-heading"><st c="21431">Note</st></p>
			<p class="callout"><st c="21436">The </st><code><st c="21441">ColumnTransformer()</st></code><st c="21460"> changes the names of the variables. </st><st c="21497">The imputed variables show the prefix </st><code><st c="21535">imputer</st></code><st c="21542"> and the untransformed variables the </st><st c="21579">prefix </st><code><st c="21586">remainder</st></code><st c="21595">.</st></p>
			<p class="list-inset"><st c="21596">Finally, let’s impute missing value</st><a id="_idTextAnchor080"/><st c="21632">s </st><st c="21635">using </st><code><st c="21641">feature-engine</st></code><st c="21655">.</st></p>
			<ol>
				<li value="13"><st c="21656">Let’s set up the imputer</st><a id="_idIndexMarker034"/><st c="21681"> to replace the missing data in categorical variables with their most </st><st c="21751">frequent value:</st><pre class="source-code"><st c="21766">
imputer = CategoricalImputer(
    imputation_method="frequent",
    variables=categorical_vars,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="21856">Note</st></p>
			<p class="callout"><st c="21861">With the </st><code><st c="21871">variables</st></code><st c="21880"> parameter set to </st><code><st c="21898">None</st></code><st c="21902">, </st><code><st c="21904">CategoricalImputer()</st></code><st c="21924"> will automatically impute all categorical variables found in the train set. </st><st c="22001">Use this parameter to restrict the imputation to a subset of categorical variables, as shown in </st><em class="italic"><st c="22097">step 13</st></em><st c="22104">.</st></p>
			<ol>
				<li value="14"><st c="22105">Fit the imputer to the train set so that it learns the most </st><st c="22166">frequent categories:</st><pre class="source-code"><st c="22186">
imputer.fit(X_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="22207">Note</st></p>
			<p class="callout"><st c="22212">To impute categorical variables with a specific string, set </st><code><st c="22273">imputation_method</st></code><st c="22290"> to </st><code><st c="22294">missing</st></code><st c="22301"> and </st><code><st c="22306">fill_value</st></code><st c="22316"> to the </st><st c="22324">desired string.</st></p>
			<ol>
				<li value="15"><st c="22339">Let’s check out the </st><st c="22360">learned categories:</st><pre class="source-code"><st c="22379">
imputer.imputer_dict_</st></pre><p class="list-inset"><st c="22401">We can see the dictionary</st><a id="_idIndexMarker035"/><st c="22427"> with the most frequent values in the </st><st c="22465">following output:</st></p><pre class="source-code"><strong class="bold"><st c="22482">{'A1': 'b',</st></strong>
<strong class="bold"><st c="22494"> 'A4': 'u',</st></strong>
<strong class="bold"><st c="22505"> 'A5': 'g',</st></strong>
<strong class="bold"><st c="22516"> 'A6': 'c',</st></strong>
<strong class="bold"><st c="22527"> 'A7': 'v',</st></strong>
<strong class="bold"><st c="22538"> 'A9': 't',</st></strong>
<strong class="bold"><st c="22549"> 'A10': 'f',</st></strong>
<strong class="bold"><st c="22561"> 'A12': 'f',</st></strong>
<strong class="bold"><st c="22573"> 'A13': 'g'}</st></strong></pre></li>				<li><st c="22585">Finally, let’s replace the missing values with </st><st c="22633">frequent categories:</st><pre class="source-code"><st c="22653">
X_train_t = imputer.transform(X_train)
X_test_t = imputer.transform(X_test)</st></pre><p class="list-inset"><st c="22729">If you want to impute numerical variables with a string or the most frequent value using </st><code><st c="22819">CategoricalImputer()</st></code><st c="22839">, set the </st><code><st c="22849">ignore_format</st></code><st c="22862"> parameter </st><st c="22873">to </st><code><st c="22876">True</st></code><st c="22880">.</st></p></li>			</ol>
			<p><code><st c="22881">CategoricalImputer()</st></code><st c="22902"> returns</st><a id="_idTextAnchor081"/><a id="_idTextAnchor082"/><st c="22910"> a pandas DataFrame as </st><st c="22933">a result.</st></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor083"/><st c="22942">How it works...</st></h2>
			<p><st c="22958">In this recipe, we replaced missing values in categorical </st><a id="_idTextAnchor084"/><st c="23017">variables with the most frequent categories or an arbitrary string. </st><st c="23085">We used </st><code><st c="23093">pandas</st></code><st c="23099">, </st><code><st c="23101">scikit-learn</st></code><st c="23113">, </st><st c="23115">and </st><code><st c="23119">feature-engine</st></code><st c="23133">.</st></p>
			<p><st c="23134">In </st><em class="italic"><st c="23138">step 5</st></em><st c="23144">, we created a dictionary </st><a id="_idIndexMarker036"/><st c="23170">with the variable names as keys and the frequent categories as values. </st><st c="23241">To capture the frequent categories, we used pandas </st><code><st c="23292">mode()</st></code><st c="23298">, and to return a dictionary, we used pandas </st><code><st c="23343">to_dict()</st></code><st c="23352">. To replace the missing data, we used </st><code><st c="23391">pandas</st></code> <code><st c="23397">fillna()</st></code><st c="23406">, passing the dictionary with the variables and their frequent categories as parameters. </st><st c="23495">There can be more than one mode in a variable, that’s why we made sure to capture only one of those valu</st><a id="_idTextAnchor085"/><st c="23599">es by </st><st c="23606">using </st><code><st c="23612">.iloc[0]</st></code><st c="23620">.</st></p>
			<p><st c="23621">To replace the missing values using </st><code><st c="23658">scikit-learn</st></code><st c="23670">, we used </st><code><st c="23680">SimpleImputer()</st></code><st c="23695"> with the </st><code><st c="23705">strategy</st></code><st c="23713"> set to </st><code><st c="23721">most_frequent</st></code><st c="23734">. To restrict the imputation to categorical variables, we used </st><code><st c="23797">ColumnTransformer()</st></code><st c="23816">. With </st><code><st c="23823">remainder</st></code><st c="23832"> set to </st><code><st c="23840">passthrough</st></code><st c="23851">, we made </st><code><st c="23861">ColumnTransformer()</st></code><st c="23880"> return all the variables present in the training set as a result of the </st><code><st c="23953">transform()</st></code> <st c="23964">method .</st></p>
			<p class="callout-heading"><st c="23973">Note</st></p>
			<p class="callout"><code><st c="23978">ColumnTransformer()</st></code><st c="23998"> changes the names of the variables in the output. </st><st c="24049">The transformed variables show the prefix </st><code><st c="24091">imputer</st></code><st c="24098"> and the unchanged variables show the </st><st c="24136">prefix </st><code><st c="24143">remainder</st></code><st c="24152">.</st></p>
			<p><st c="24153">With </st><code><st c="24159">fit()</st></code><st c="24164">, </st><code><st c="24166">SimpleImputer()</st></code><st c="24181"> learned the variables’ most frequent categories and stored them in its </st><code><st c="24253">statistics_</st></code><st c="24264"> attribute. </st><st c="24276">With </st><code><st c="24281">transform()</st></code><st c="24292">, it replaced the missing data with the </st><st c="24332">learned parameters.</st></p>
			<p><code><st c="24351">SimpleImputer()</st></code><st c="24367"> and </st><code><st c="24372">ColumnTransformer()</st></code><st c="24391"> return NumPy arrays by default. </st><st c="24424">We can change this behavior with the </st><code><st c="24461">set_output()</st></code><st c="24473"> parameter.</st></p>
			<p><st c="24484">To replace missing values with </st><code><st c="24516">feature-engine</st></code><st c="24530">, we used the </st><code><st c="24544">CategoricalImputer()</st></code><st c="24564"> with </st><code><st c="24570">imputation_method</st></code><st c="24587"> set to </st><code><st c="24595">frequent</st></code><st c="24603">. With </st><code><st c="24610">fit()</st></code><st c="24615">, the transformer learned and stored the most frequent categories in a dictionary in its </st><code><st c="24704">imputer_dict_</st></code><st c="24717"> attribute. </st><st c="24729">With </st><code><st c="24734">transform()</st></code><st c="24745">, it replaced the missing values with the </st><st c="24787">learned parameters.</st></p>
			<p><st c="24806">Unlike </st><code><st c="24814">SimpleImputer()</st></code><st c="24829">, </st><code><st c="24831">CategoricalImputer()</st></code><st c="24851"> will only impute categorical variables, unless specificall</st><a id="_idTextAnchor086"/><st c="24910">y told not to do so by setting the </st><code><st c="24946">ignore_format</st></code><st c="24959"> parameter to </st><code><st c="24973">True</st></code><st c="24977">. In addition, with </st><code><st c="24997">feature-engine</st></code><st c="25011"> transformers we can restrict the transformations to a subset of variables through the transformer itself. </st><st c="25118">For </st><code><st c="25122">scikit-learn</st></code><st c="25134"> transformers, we need the additional </st><code><st c="25172">ColumnTransformer()</st></code><st c="25191"> class to apply the transformat</st><a id="_idTextAnchor087"/><a id="_idTextAnchor088"/><st c="25222">ion</st><a id="_idIndexMarker037"/><st c="25226"> to a subset of </st><st c="25242">the variables.</st></p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor089"/><st c="25256">Replacing missing values with an arbitrary number</st></h1>
			<p><st c="25306">We can replace missing data</st><a id="_idTextAnchor090"/><a id="_idIndexMarker038"/><st c="25334"> with an arbitrary value. </st><st c="25360">Commonly</st><a id="_idIndexMarker039"/><st c="25368"> used values are </st><code><st c="25385">999</st></code><st c="25388">, </st><code><st c="25390">9999</st></code><st c="25394">, or </st><code><st c="25399">-1</st></code><st c="25401"> for positive distributions. </st><st c="25430">This method is used for numerical variables. </st><st c="25475">For categorical variables, the equivalent metho</st><a id="_idTextAnchor091"/><st c="25522">d is to replace missing data with an arbitrary string, as described in the </st><em class="italic"><st c="25598">Imputing categorical </st></em><em class="italic"><st c="25619">variables</st></em><st c="25628"> recipe.</st></p>
			<p><st c="25636">When replacing missing values with arbitrary numbers, we need to be careful not to select a value close to the mean, the median, or any other common value of </st><st c="25795">the distribution.</st></p>
			<p class="callout-heading"><st c="25812">Note</st></p>
			<p class="callout"><st c="25817">We’d use arbitrary number imputation when data is not missing at random, use non-linear models, or when the percentage of missing data is high. </st><st c="25962">This imputation technique distorts the original </st><st c="26010">variable distribution.</st></p>
			<p><st c="26032">In this recipe, we will impute missing data with arbitrary numbers using </st><code><st c="26106">panda</st><a id="_idTextAnchor092"/><a id="_idTextAnchor093"/><st c="26111">s</st></code><st c="26113">, </st><code><st c="26115">scikit-learn</st></code><st c="26127">, </st><st c="26129">and </st><code><st c="26133">feature-engine</st></code><st c="26147">.</st></p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor094"/><st c="26148">How to do it...</st></h2>
			<p><st c="26164">Let’s begin by importing the necessary tools and loading </st><st c="26222">the data:</st></p>
			<ol>
				<li><st c="26231">Import </st><code><st c="26239">pandas</st></code><st c="26245"> and the required functions </st><st c="26273">and classes:</st><pre class="source-code"><st c="26285">
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from feature_engine.imputation import ArbitraryNumberImputer</st></pre></li>				<li><st c="26460">Let’s load the dataset described in the </st><em class="italic"><st c="26501">Technical </st></em><em class="italic"><st c="26511">requirements</st></em><st c="26523"> section:</st><pre class="source-code"><st c="26532">
data = pd.read_csv("credit_approval_uci.csv")</st></pre></li>				<li><st c="26578">Let’s separate</st><a id="_idIndexMarker040"/><st c="26593"> the data into train and </st><st c="26618">test</st><a id="_idIndexMarker041"/><st c="26622"> sets:</st><pre class="source-code"><st c="26628">
X_train, X_test, y_train, y_test = train_test_split(
    data.drop("target", axis=1),
    data["target"],
    test_s</st><a id="_idTextAnchor095"/><st c="26733">ize=0.3,
    random_stat</st><a id="_idTextAnchor096"/><st c="26754">e=0,
)</st></pre><p class="list-inset"><st c="26761">We will select arbitrary values greater than the maximum value of </st><st c="26828">the distribution.</st></p></li>				<li><st c="26845">Let’s find the maximum value of four </st><st c="26883">numerical variables:</st><pre class="source-code"><st c="26903">
X_train[['A2','A3', 'A8', 'A11']].max()</st></pre><p class="list-inset"><st c="26943">The previous</st><a id="_idTextAnchor097"/><st c="26956"> command returns the </st><st c="26977">following output:</st></p><pre class="source-code"><strong class="bold"><st c="26994">A2     76.750</st></strong>
<strong class="bold"><st c="27004">A3     26.335</st></strong>
<strong class="bold"><st c="27014">A8     28.500</st></strong>
<strong class="bold"><st c="27024">A11    67.000</st></strong>
<code><st c="27061">99</st></code><st c="27063"> for the imputation because it is bigger than the maximum values of the numerical variables in </st><em class="italic"><st c="27158">step 4</st></em><st c="27164">.</st></p></li>				<li><st c="27165">Let’s make a copy of the </st><st c="27191">original DataFrames:</st><pre class="source-code"><st c="27211">
X_train_t = X_train.copy()
X_test_t = X_test.copy()</st></pre></li>				<li><st c="27263">Now, we replace the missing values </st><st c="27299">with </st><code><st c="27304">99</st></code><st c="27306">:</st><pre class="source-code"><st c="27308">
X_train_t[["A2", "A3", "A8", "A11"]] = X_train_t[[
    "A2", "A3", "A8", "A11"]].fillna(99)
X_test_t[["A2", "A3", "A8", "A11"]] = X_test_t[[
    "A2", "A3", "A8", "A11"]].fillna(99)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="27482">Note</st></p>
			<p class="callout"><st c="27487">To impute different variables with different values using </st><code><st c="27546">pandas</st></code> <code><st c="27552">fillna()</st></code><st c="27561">, use a dictionary like this: </st><code><st c="27591">imputation_dict = {"A2": -1, "A3": -</st><a id="_idTextAnchor098"/><st c="27627">1, "A8": 999, "</st><a id="_idTextAnchor099"/></code><code><st c="27643">A11": 9999}</st></code><st c="27655">.</st></p>
			<p><st c="27656">Now, we’ll impute missing</st><a id="_idIndexMarker042"/><st c="27682"> values with an arbitrary number</st><a id="_idIndexMarker043"/> <st c="27714">using </st><code><st c="27721">scikit-learn</st></code><st c="27733">.</st></p>
			<ol>
				<li><st c="27734">Let’s set up </st><code><st c="27748">imputer</st></code><st c="27755"> to replace missing values </st><st c="27782">with </st><code><st c="27787">99</st></code><st c="27789">:</st><pre class="source-code"><st c="27791">
imputer = SimpleImputer(strategy='constant', fill_value=99)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="27851">Note</st></p>
			<p class="callout"><st c="27856">If your dataset contains categorical variables, </st><code><st c="27905">SimpleImputer()</st></code><st c="27920"> will add </st><code><st c="27930">99</st></code><st c="27932"> to those variables as well if any values </st><st c="27974">are missing.</st></p>
			<ol>
				<li value="2"><st c="27986">Let’s fit </st><code><st c="27997">imputer</st></code><st c="28004"> to a slice of the train set containing the variables </st><st c="28058">to impute:</st><pre class="source-code"><st c="28068">
vars = ["A2", "A3", "A8", "A11"]
imputer.fit(X_train[vars])</st></pre></li>				<li><st c="28128">Replace the missing values with </st><code><st c="28161">99</st></code><st c="28163"> in the </st><st c="28171">desired variables:</st><pre class="source-code"><st c="28189">
X_train_t[vars] = imputer.transform(X_train[vars])
X_test_t[vars] = imputer.transform(X_test[vars])</st></pre><p class="list-inset"><st c="28289">Go ahead and check the lack of missing values by executing </st><code><st c="28349">X_test_t[["A2", "A3", "</st></code><code><st c="28372">A8", "A11"]].isnull().sum()</st></code><st c="28400">.</st></p><p class="list-inset"><st c="28401">To finish, let’s impute mis</st><a id="_idTextAnchor100"/><st c="28429">sing values </st><st c="28442">using </st><code><st c="28448">f</st><a id="_idTextAnchor101"/><st c="28449">eature-engine</st></code><st c="28462">.</st></p></li>				<li><st c="28463">Let’s set up the </st><code><st c="28481">imputer</st></code><st c="28488"> to replace</st><a id="_idIndexMarker044"/><st c="28499"> missing values</st><a id="_idIndexMarker045"/><st c="28514"> with </st><code><st c="28520">99</st></code><st c="28522"> in 4 </st><st c="28528">specific variables:</st><pre class="source-code"><st c="28547">
imputer = ArbitraryNumberImputer(
    arbitrary_number=99,
    variables=["A2", "A3", "A8", "A11"],
)</st></pre></li>			</ol>
			<p class="callout-heading"><code><st c="28641">Note</st></code></p>
			<p class="callout"><code><st c="28646">ArbitraryNumberImputer()</st></code><st c="28671"> will automatically select all numerical variables in the train set for imputation if we set the </st><code><st c="28768">variables</st></code><st c="28777"> parameter </st><st c="28788">to </st><code><st c="28791">None</st></code><st c="28795">.</st></p>
			<ol>
				<li value="5"><st c="28796">Finally, let’s replace the missing values </st><st c="28839">with </st><code><st c="28844">99</st></code><st c="28846">:</st><pre class="source-code"><st c="28848">
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="28924">Note</st></p>
			<p class="callout"><st c="28929">To impute different variables with different numbers, set up </st><code><st c="28991">ArbitraryNumberImputer()</st></code><st c="29015"> as follows: </st><code><st c="29028">ArbitraryNumberImputer(imputater_dict = {"A2": -1, "A3": -1, "A8": 999, "</st></code><code><st c="29101">A11": 9999})</st></code><st c="29114">.</st></p>
			<p><st c="29115">We have now replaced missing data with arbitrary numbers us</st><a id="_idTextAnchor102"/><a id="_idTextAnchor103"/><st c="29175">ing three different </st><st c="29196">open-source libraries.</st></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor104"/><st c="29218">How it works...</st></h2>
			<p><st c="29234">In this recipe, we replaced</st><a id="_idIndexMarker046"/><st c="29262"> missing values in numerical variables</st><a id="_idIndexMarker047"/><st c="29300"> with an arbitrary number using </st><code><st c="29332">pandas</st></code><st c="29338">, </st><code><st c="29340">sc</st><a id="_idTextAnchor105"/><st c="29342">ikit-learn</st></code><st c="29353">, </st><st c="29355">and </st><code><st c="29359">featur</st><a id="_idTextAnchor106"/><st c="29365">e-engine</st></code><st c="29374">.</st></p>
			<p><st c="29375">To determine which arbitrary value to use, we inspected the maximum values of four numerical variables using pandas’ </st><code><st c="29493">max()</st></code><st c="29498">. We chose </st><code><st c="29509">99</st></code><st c="29511"> because it was greater than the maximum values of the selected variables. </st><st c="29586">In </st><em class="italic"><st c="29589">step 5</st></em><st c="29595">, we used </st><code><st c="29605">pandas</st></code> <code><st c="29611">fillna()</st></code><st c="29620"> to replace the </st><st c="29636">missing data.</st></p>
			<p><st c="29649">To replace missing values using </st><code><st c="29682">scikit-learn</st></code><st c="29694">, we utilized </st><code><st c="29708">SimpleImputer()</st></code><st c="29723">, with the </st><code><st c="29734">strategy</st></code><st c="29742"> set to </st><code><st c="29750">constant</st></code><st c="29758">, and specified </st><code><st c="29774">99</st></code><st c="29776"> in the </st><code><st c="29784">fill_value</st></code><st c="29794"> argument. </st><st c="29805">Next, we fitted the imputer to a slice of the train set with the numerical variables to impute. </st><st c="29901">Finally, we replaced missing values </st><st c="29937">using </st><code><st c="29943">transform()</st></code><st c="29954">.</st></p>
			<p><st c="29955">To replace missing values with </st><code><st c="29987">feature-engine</st></code><st c="30001"> we used </st><code><st c="30010">ArbitraryValueImputer()</st></code><st c="30033">, specifying the value </st><code><st c="30056">99</st></code><st c="30058"> and the variables to impute as parameters. </st><st c="30102">Next, we applied the </st><code><st c="30123">fit_transform()</st></code><st c="30138"> method to replace missing data in the train set and the </st><code><st c="30195">transform()</st></code><st c="30206"> met</st><a id="_idTextAnchor107"/><a id="_idTextAnchor108"/><st c="30210">hod to replace missing data in the </st><st c="30246">test set.</st></p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor109"/><st c="30255">Finding extreme values for imputation</st></h1>
			<p><st c="30293">Replacing missing values</st><a id="_idIndexMarker048"/><st c="30318"> with a value at the end of the variable</st><a id="_idIndexMarker049"/><st c="30358"> distribution (extreme values) is like replacing them with an arbitra</st><a id="_idTextAnchor110"/><st c="30427">ry value, but instead of setting the arbitrary values manually, the values are automatically selected from the end of the </st><st c="30550">variable distribution.</st></p>
			<p><st c="30572">We can replace missing data with a value that is greater or smaller than most values in the variable. </st><st c="30675">To select a value that is greater, we can use the mean plus a factor of the standard deviation. </st><st c="30771">Alternatively, we can set it to the 75th quantile + IQR × 1.5. </st><strong class="bold"><st c="30834">IQR</st></strong><st c="30837"> stands for </st><strong class="bold"><st c="30849">inter-quartile range</st></strong><st c="30869"> and is the difference between</st><a id="_idIndexMarker050"/><st c="30899"> the 75th and 25th quantile. </st><st c="30928">To replace missing data with values that are smaller than the remaining values, we can use the mean minus a factor of the standard deviation, or the 25th quantile – IQR × </st><st c="31099">1.5.</st></p>
			<p class="callout-heading"><st c="31103">Note</st></p>
			<p class="callout"><st c="31108">End-of-tail imputation may distort the distribution of the original variables, so it may not be suitable for </st><st c="31218">linear models.</st></p>
			<p><st c="31232">In this recipe, we will implement end-of-tail or extreme va</st><a id="_idTextAnchor111"/><a id="_idTextAnchor112"/><st c="31292">lue imputation using </st><code><st c="31314">pandas</st></code> <st c="31320">and </st><code><st c="31325">feature-engine</st></code><st c="31339">.</st></p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor113"/><st c="31340">How to do it...</st></h2>
			<p><st c="31356">To begin this</st><a id="_idIndexMarker051"/><st c="31370"> recipe, let’s import the necessary tools</st><a id="_idIndexMarker052"/><st c="31411"> and load </st><st c="31421">the data:</st></p>
			<ol>
				<li><st c="31430">Let’s import </st><code><st c="31444">pandas</st></code><st c="31450"> and the required function </st><st c="31477">and class:</st><pre class="source-code"><st c="31487">
import pandas as pd
from sklearn.model_selection import train_test_split
from feature_engine.imputation import EndTailImputer</st></pre></li>				<li><st c="31613">Let’s load the dataset we described in the </st><em class="italic"><st c="31657">Technical </st></em><em class="italic"><st c="31667">requirements</st></em><st c="31679"> section:</st><pre class="source-code"><st c="31688">
data = pd.read_csv("credit_approval_uci.csv")</st></pre></li>				<li><st c="31734">Let’s capture the numerical variables in a list, excluding </st><st c="31794">the target:</st><pre class="source-code"><st c="31805">
numeric_vars = [
    var for var in data.select_dtypes(
        exclude="O").columns.to_list()
    if var !="target"
]</st></pre></li>				<li><st c="31908">Let’s split the data into train and test sets, keeping only the </st><st c="31973">numerical variables:</st><pre class="source-code"><st c="31993">
X_train, X_test, y_train, y_test = train_test_split(
    data[numeric_vars],
    data["target"],
    test_si</st><a id="_idTextAnchor114"/><st c="32090">ze=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="32116">We’ll now determine </st><st c="32137">the IQR:</st><pre class="source-code"><st c="32145">
IQR = X_train.quantile(0.75) - X_train.quantile(0.25)</st></pre><p class="list-inset"><st c="32199">We can visualize</st><a id="_idIndexMarker053"/><st c="32216"> the IQR values</st><a id="_idIndexMarker054"/><st c="32231"> by executing </st><code><st c="32245">IQR</st></code> <st c="32248">or </st><code><st c="32252">print(IQR)</st></code><st c="32262">:</st></p><pre class="source-code"><strong class="bold"><st c="32264">A2      16.4200</st></strong>
<strong class="bold"><st c="32274">A3       </st></strong><strong class="bold"><st c="32278">6.5825</st></strong>
<strong class="bold"><st c="32284">A8       2.8350</st></strong>
<strong class="bold"><st c="32294">A11      3.0000</st></strong>
<strong class="bold"><st c="32305">A14    212.0000</st></strong>
<strong class="bold"><st c="32318">A15    450.0000</st></strong>
<strong class="bold"><st c="32331">dtype: float64</st></strong></pre></li>				<li><st c="32346">Let’s create a dictionary with the variable names and the </st><st c="32405">imputation values:</st><pre class="source-code"><st c="32423">
imputation_dict = (
    X_train.quantile(0.75) + 1.5 * IQR).to_dict()</st></pre></li>			</ol>
			<p class="callout-heading"><st c="32489">Note</st></p>
			<p class="callout"><st c="32494">If we use the inter-quartile range proximity rule, we determine the imputation values by adding 1.5 times the IQR to the 75th quantile. </st><st c="32631">If variables are normally distributed, we can calculate the imputation values as the mean plus a factor of the standard deviation, </st><code><st c="32762">imputation_dict = (X_train.me</st><a id="_idTextAnchor115"/><st c="32791">an() + 3 * </st></code><code><st c="32803">X_train.std()).to_dict()</st></code><st c="32827">.</st></p>
			<ol>
				<li value="7"><st c="32828">Finally, let’s replace the </st><st c="32856">missing data:</st><pre class="source-code"><st c="32869">
X_train_t = X_train.fillna(value=imputation_dict)
X_test_t = X_test.fillna(value=imputation_dict)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="32967">Note</st></p>
			<p class="callout"><st c="32972">We can also replace missing data with values at the left tail of the distribution using </st><code><st c="33061">value = X_train[var].quantile(0.25) - 1.5 * IQR</st></code><st c="33108"> or </st><code><st c="33112">value = X_train[var].mean() – 3 * </st></code><code><st c="33146">X_train[var].std()</st></code><st c="33164">.</st></p>
			<p><st c="33165">To finish, let’s impute</st><a id="_idIndexMarker055"/><st c="33189"> missing values</st><a id="_idIndexMarker056"/> <st c="33204">using </st><code><st c="33211">feature-engine</st></code><st c="33225">.</st></p>
			<ol>
				<li><st c="33226">Let’s set up </st><code><st c="33240">imputer</st></code><st c="33247"> to estimate a value at the right of the distribution using the IQR </st><st c="33315">proximity rule:</st><pre class="source-code"><st c="33330">
imputer = EndTailImputer(
    imputation_method="iqr",
    tail="right",
    fold=3,
    variables=None,
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="33421">Note</st></p>
			<p class="callout"><st c="33426">To use the mean and standard deviation to calculate the replacement values, set </st><code><st c="33507">imputation_method="Gaussian"</st></code><st c="33535">. Use </st><code><st c="33541">left</st></code><st c="33545"> or </st><code><st c="33549">right</st></code><st c="33554"> in the </st><code><st c="33562">tail</st></code><st c="33566"> argument to specify the side of the distribution to consider when finding values for </st><st c="33652">the imputation.</st></p>
			<ol>
				<li value="2"><st c="33667">Let’s fit </st><code><st c="33678">EndTailImputer()</st></code><st c="33694"> to the train set so that it learns the </st><a id="_idTextAnchor116"/><st c="33734">values for </st><st c="33745">the imputation:</st><pre class="source-code"><st c="33760">
imputer.fit(X_train)</st></pre></li>				<li><st c="33781">Let’s inspect the </st><st c="33800">learned values:</st><pre class="source-code"><st c="33815">
imputer.imputer_dict_</st></pre><p class="list-inset"><st c="33837">The previous command</st><a id="_idIndexMarker057"/><st c="33858"> returns a dictionary with the values</st><a id="_idIndexMarker058"/><st c="33895"> to use to impute </st><st c="33913">each variable:</st></p><pre class="source-code"><strong class="bold"><st c="33927">{'A2': 88.18,</st></strong>
<strong class="bold"><st c="33941"> 'A3': 27.31,</st></strong>
<strong class="bold"><st c="33954"> 'A8': 11.504999999999999,</st></strong>
<strong class="bold"><st c="33980"> 'A11': 12.0,</st></strong>
<strong class="bold"><st c="33993"> 'A14': 908.0,</st></strong>
<strong class="bold"><st c="34007"> 'A15': 1800.0}</st></strong></pre></li>				<li><st c="34022">Finally, let’s replace the </st><st c="34050">missing values:</st><pre class="source-code"><st c="34065">
X_train = imputer.transform(X_train)
X_test = imputer.transform(X_test)</st></pre></li>			</ol>
			<p><st c="34137">Remember that you can corroborate that the missing values were replaced by using </st><code><st c="34219">X_train[[</st><a id="_idTextAnchor117"/><a id="_idTextAnchor118"/><st c="34228">'A2','A3', 'A8', 'A11', '</st></code><code><st c="34254">A14', 'A15']].isnull().mean()</st></code><st c="34284">.</st></p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor119"/><st c="34285">How it works...</st></h2>
			<p><st c="34301">In this recipe, we replaced missing values in numerical variables with a number at the end </st><a id="_idTextAnchor120"/><st c="34393">of the distribution using </st><code><st c="34419">pandas</st></code> <st c="34425">and </st><code><st c="34430">feature-engine</st></code><st c="34444">.</st></p>
			<p><st c="34445">We determined the imputation values according to the formulas described in the introduction to this recipe. </st><st c="34554">We used pandas </st><code><st c="34569">quantile()</st></code><st c="34579"> to find specific quantile values, or </st><code><st c="34617">pandas</st></code> <code><st c="34623">mean()</st></code><st c="34630"> and </st><code><st c="34635">std()</st></code><st c="34640"> for the mean and standard deviation. </st><st c="34678">With pandas </st><code><st c="34690">fillna()</st></code><st c="34698"> we replaced the </st><st c="34715">missing values.</st></p>
			<p><st c="34730">To replace missing values with </st><code><st c="34762">EndTailImputer()</st></code><st c="34778"> from </st><code><st c="34784">feature-engine</st></code><st c="34798">, we set </st><code><st c="34807">distribution</st></code><st c="34819"> to </st><code><st c="34823">iqr</st></code><st c="34826"> to calculate the values based on the IQR proximity rule. </st><st c="34884">With </st><code><st c="34889">tail</st></code><st c="34893"> set to </st><code><st c="34901">right</st></code><st c="34906"> the transformer found the imputation values from the right of the distribution. </st><st c="34987">With </st><code><st c="34992">fit()</st></code><st c="34997">, the imputer learned and stored the values for the imputation in a dictionary in the </st><code><st c="35083">imputer_dict_</st></code><st c="35096"> attribute. </st><st c="35108">With </st><a id="_idTextAnchor121"/><code><st c="35113">transform()</st><a id="_idTextAnchor122"/><a id="_idTextAnchor123"/></code><st c="35124">, we replaced</st><a id="_idIndexMarker059"/><st c="35137"> the missing</st><a id="_idIndexMarker060"/><st c="35149"> values, </st><st c="35158">returning DataFrames.</st></p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor124"/><st c="35179">Marking imputed values</st></h1>
			<p><st c="35202">In the previous recipes, we focused </st><a id="_idIndexMarker061"/><st c="35239">on replacing missing data with estimates of their values. </st><st c="35297">In addition, we can add missing indicators to </st><em class="italic"><st c="35343">mark</st></em><st c="35347"> observations where values </st><st c="35374">were missing.</st></p>
			<p><st c="35387">A missing indicator is a binary var</st><a id="_idTextAnchor125"/><st c="35423">iable that takes the value </st><code><st c="35451">1</st></code><st c="35452"> or </st><code><st c="35456">True</st></code><st c="35460"> to indicate whether a value was missing, and </st><code><st c="35506">0</st></code><st c="35507"> or </st><code><st c="35511">False</st></code><st c="35516"> otherwise. </st><st c="35528">It is common practice to replace missing observations with the mean, median, or most frequent category while simultaneously marking those missing observations with missing indicators. </st><st c="35712">In this recipe, we will learn how to add missing </st><a id="_idTextAnchor126"/><a id="_idTextAnchor127"/><st c="35761">indicators using </st><code><st c="35778">pandas</st></code><st c="35784">, </st><code><st c="35786">scikit-learn</st></code><st c="35798">, </st><st c="35800">and </st><code><st c="35804">feature-engine</st></code><st c="35818">.</st></p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor128"/><st c="35819">How to do it...</st></h2>
			<p><st c="35835">Let’s begin by making some imports and loading </st><st c="35883">the data:</st></p>
			<ol>
				<li><st c="35892">Let’s import the required libraries, functions, </st><st c="35941">and classes:</st><pre class="source-code"><st c="35953">
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from feature_engine.imputation import(
    AddMissingIndicator,
</st><a id="_idTextAnchor129"/><st c="36231">    CategoricalImputer,
    MeanMedianImputer
)</st></pre></li>				<li><st c="36270">Let’s load and split the dataset</st><a id="_idIndexMarker062"/><st c="36303"> described in the </st><em class="italic"><st c="36321">Technical </st></em><em class="italic"><st c="36331">requirements</st></em><st c="36343"> section:</st><pre class="source-code"><st c="36352">
data = pd.read_csv("credit_approval_uci.csv")
X_train, X_test, y_train, y_test = train_test_split(
    data.drop("target", axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="36529">Let’s capture the variable names in </st><st c="36566">a list:</st><pre class="source-code"><st c="36573">
varnames = ["A1", "A3", "A4", "A5", "A6", "A7", "A8"]</st></pre></li>				<li><st c="36627">Let’s create names for the missing indicators and store them in </st><st c="36692">a list:</st><pre class="source-code"><st c="36699">
indicators = [f"{var}_na" for var in varnames]</st></pre><p class="list-inset"><st c="36746">If we execute </st><code><st c="36761">indicators</st></code><st c="36771">, we will see the names we will use for the new variables: </st><code><st c="36830">['A1_na', 'A3_na', 'A4_na', 'A5_na', 'A6_na', '</st></code><code><st c="36877">A7_na', 'A8_na']</st></code><st c="36894">.</st></p></li>				<li><st c="36895">Let’s make a copy of the </st><st c="36921">original DataFrames:</st><pre class="source-code"><st c="36941">
X_trai</st><a id="_idTextAnchor130"/><st c="36948">n_t = X_train.copy()
X_test_t = X_test.copy()</st></pre></li>				<li><st c="36994">Let’s add the </st><st c="37009">missing indicators:</st><pre class="source-code"><st c="37028">
X_train_t[indicators] =X_train[
    varnames].isna().astype(int)
X_test_t[indicators] = X_test[
    varnames].isna().astype(int)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="37149">Note</st></p>
			<p class="callout"><st c="37154">If you want the indicators to have </st><code><st c="37190">True</st></code><st c="37194"> and </st><code><st c="37199">False</st></code><st c="37204"> as values instead of </st><code><st c="37226">0</st></code><st c="37227"> and </st><code><st c="37232">1</st></code><st c="37233">, remove </st><code><st c="37242">astype(int)</st></code><st c="37253"> in </st><em class="italic"><st c="37257">step 6</st></em><st c="37263">.</st></p>
			<ol>
				<li value="7"><st c="37264">Let’s inspect the </st><st c="37283">resulting</st><a id="_idIndexMarker063"/><st c="37292"> DataFrame:</st><pre class="source-code"><st c="37303">
X_train_t.head()</st></pre><p class="list-inset"><st c="37320">We can see the newly added vari</st><a id="_idTextAnchor131"/><st c="37352">ables at the right of the DataFrame in the </st><st c="37396">following image:</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_04.jpg" alt="Figure 1.4 – DataFrame with the missing indicators"/><st c="37412"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="37815">Figure 1.4 – DataFrame with the missing indicators</st></p>
			<p class="list-inset"><st c="37865">Now, let’s add missing indicators using </st><st c="37906">Feature-engine instead.</st></p>
			<ol>
				<li value="8"><st c="37929">Set up the imputer to add binary indicators to every variable with </st><st c="37997">missing data:</st><pre class="source-code"><st c="38010">
imputer = AddMissingInd</st><a id="_idTextAnchor132"/><st c="38034">icator(
    variables=None, missing_only=True
    )</st></pre></li>				<li><st c="38078">Fit the imputer to the train set so that it finds the variables with </st><st c="38148">missing data:</st><pre class="source-code"><st c="38161">
imputer.fit(X_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="38182">Note</st></p>
			<p class="callout"><st c="38187">If we execute </st><code><st c="38202">imputer.variables_</st></code><st c="38220">, we will find the variables for which missing indicators will </st><st c="38283">be added.</st></p>
			<ol>
				<li value="10"><st c="38292">Finally, let’s add the </st><st c="38316">missing indicators:</st><pre class="source-code"><st c="38335">
X_train_t = imputer.transform(X_train)
X_test_t = imputer.transform(X_test)</st></pre><p class="list-inset"><st c="38411">So far, we just added missing indicators. </st><st c="38454">But we still have the missing data in our variables. </st><st c="38507">We need to replace them with numbers. </st><st c="38545">In the rest of this recipe, we will combine the use of missing indicators with mean and </st><st c="38633">mode imputation.</st></p></li>				<li><st c="38649">Let’s create a pipeline</st><a id="_idIndexMarker064"/><st c="38673"> to add missing indicators to categorical and numerical variables, then impute categorical variables with the most frequent category, and numerical variables with </st><st c="38836">the mean:</st><pre class="source-code"><st c="38845">
pipe = Pipeline([
    ("indicators",
        AddMissingIndicator(missing_only=True)),
    ("categorical", CategoricalImputer(
        imputation_method="frequent")),
    ("numerical", MeanMedianImputer()),
])</st></pre></li>			</ol>
			<p class="callout-heading"><st c="39026">Note</st></p>
			<p class="callout"><code><st c="39031">feature-engine</st></code><st c="39046"> imputers automatically identify numerical or categorical variables. </st><st c="39115">So there is no need to slice the data or pass the variabl</st><a id="_idTextAnchor133"/><st c="39172">e names as arguments to the transformers in </st><st c="39217">this case.</st></p>
			<ol>
				<li value="12"><st c="39227">Let’s add the indicators and impute </st><st c="39264">missing values:</st><pre class="source-code"><st c="39279">
X_train_t = pipe.fit_transform(X_train)
X_test_t = pipe.transform(X_test)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="39353">Note</st></p>
			<p class="callout"><st c="39358">Use </st><code><st c="39363">X_train_t.isnull().sum()</st></code><st c="39387"> to corroborate that there is no data missing. </st><st c="39434">Execute </st><code><st c="39442">X_train_t.head()</st></code><st c="39458"> to get a view of the </st><st c="39480">resulting datafame.</st></p>
			<p class="list-inset"><st c="39499">Finally, let’s add missing indicators and simultaneously impute numerical and categorical variables with the mean and most frequent categories respectively, </st><st c="39657">utilizing scikit-learn.</st></p>
			<ol>
				<li value="13"><st c="39680">Let’s make a list with the names</st><a id="_idIndexMarker065"/><st c="39713"> of the numerical and </st><st c="39735">categorical variables:</st><pre class="source-code"><st c="39757">
numvars = X_train.select_dtypes(
    exclude="O").columns.to_list()
catvars = X_train.select_dtypes(
    include="O").columns.to_list()</st></pre></li>				<li><st c="39885">Let’s set up a pipeline to perform mean and frequent category imputation while marking the </st><st c="39977">missing data:</st><pre class="source-code"><st c="39990">
pipe = ColumnTransformer([
    ("num_imputer", SimpleImputer(
        strategy="mean",
        add_indicator=True),
    numvars),
    ("cat_imputer", SimpleImputer(
        strategy="most_frequent",
        add_indicator=True),
    catvars),
]).set_output(transform="pandas")</st></pre></li>				<li><st c="40218">Now, let’s carry out </st><st c="40240">the imputation:</st><pre class="source-code"><st c="40255">
X_train_t = pipe.fit_transform(X_train)
X_test_t = pipe.transform(X_test)</st></pre></li>			</ol>
			<p><st c="40329">Make sure to explore</st><a id="_idTextAnchor134"/><a id="_idTextAnchor135"/> <code><st c="40350">X_train_t.head()</st></code><st c="40367"> to get familiar with the </st><st c="40393">pipeline’s output.</st></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor136"/><st c="40411">How it works...</st></h2>
			<p><st c="40427">To add missing</st><a id="_idTextAnchor137"/><st c="40442"> indicators</st><a id="_idIndexMarker066"/><st c="40453"> using pandas, we used </st><code><st c="40476">isna()</st></code><st c="40482">, which created a new vector assigning the value of </st><code><st c="40534">True</st></code><st c="40538"> if there was a missing value or </st><code><st c="40571">False</st></code><st c="40576"> otherwise. </st><st c="40588">We used </st><code><st c="40596">astype(int)</st></code><st c="40607"> to convert the Boolean vectors into binary vectors with values </st><code><st c="40671">1</st></code> <st c="40672">and </st><code><st c="40676">0</st></code><st c="40677">.</st></p>
			<p><st c="40678">To add a missing indicator with </st><code><st c="40711">feature-engine</st></code><st c="40725">, we used </st><code><st c="40735">AddMissingIndicator()</st></code><st c="40756">. With </st><code><st c="40763">fit()</st></code><st c="40768"> the transformer found the variables with missing data. </st><st c="40824">With </st><code><st c="40829">transform()</st></code><st c="40840"> it appended the missing indicators to the right of the train and </st><st c="40906">test sets.</st></p>
			<p><st c="40916">To sequentially add missing indicators and then replace the </st><code><st c="40977">nan</st></code><st c="40980"> values with the most frequent category or the mean, we lined up Feature-engine’s </st><code><st c="41062">AddMissingIndicator()</st></code><st c="41083">, </st><code><st c="41085">CategoricalImputer()</st></code><st c="41105">, and </st><code><st c="41111">MeanMedianImputer()</st></code><st c="41130"> within a </st><code><st c="41140">pipeline</st></code><st c="41148">. The </st><code><st c="41154">fit()</st></code><st c="41159"> method from the </st><code><st c="41176">pipeline</st></code><st c="41184"> made the transformers find the variables with </st><code><st c="41231">nan</st></code><st c="41234"> and calculate the mean of the numerical variables and the mode of the categorical variables. </st><st c="41328">The </st><code><st c="41332">transform()</st></code><st c="41343"> method from the </st><code><st c="41360">pipeline</st></code><st c="41368"> made the transformers add the missing indicators and then replace the missing values with </st><st c="41459">their estimates.</st></p>
			<p class="callout-heading"><st c="41475">Note</st></p>
			<p class="callout"><st c="41480">Feature-engine transformations return DataFrames respecting the original names and order of the variables. </st><st c="41588">Scikit-learn’s </st><code><st c="41603">ColumnTransformer()</st></code><st c="41622">, on the other hand, changes the variable’s names and order in the </st><st c="41689">resulting data.</st></p>
			<p><st c="41704">Finally, we added missing indicators and replaced missing data with the mean and most frequent category using </st><code><st c="41815">scikit-learn</st></code><st c="41827">. We lined up two instances of </st><code><st c="41858">SimpleImputer()</st></code><st c="41873">, the first to impute data with the mean and the second to impute data with the most frequent category. </st><st c="41977">In both cases, we set the </st><code><st c="42003">add_indicator</st></code><st c="42016"> parameter to </st><code><st c="42030">True</st></code><st c="42034"> to add the missing indicators. </st><st c="42066">We wrapped </st><code><st c="42077">SimpleImputer()</st></code><st c="42092"> with </st><code><st c="42098">ColumnTransformer()</st></code><st c="42117"> to specifically modify numerical or categorical variables. </st><st c="42177">Then we used the </st><code><st c="42194">fit()</st></code><st c="42199"> and </st><code><st c="42204">transform()</st></code><st c="42215"> methods from the </st><code><st c="42233">pipeline</st></code><st c="42241"> to train the transformers and then add the indicators and replace the </st><st c="42312">missing data.</st></p>
			<p><st c="42325">When returning DataFrames, </st><code><st c="42353">ColumnTransformer()</st></code><st c="42372"> changes the names of the variables and their order. </st><st c="42425">Take a look at the result from </st><em class="italic"><st c="42456">step 15</st></em><st c="42463"> by executing </st><code><st c="42477">X_train_t.head()</st></code><st c="42493">. You’ll see that the name given to each step of the pipeline is added as a prefix to the variables to flag which variable was modified with each transformer. </st><st c="42652">Then, </st><code><st c="42658">num_imputer__A2</st></code><st c="42673"> was returned by the first step of the pipeline, while </st><code><st c="42728">cat_imputer__A12</st></code><st c="42744"> was returned</st><a id="_idIndexMarker067"/><st c="42757"> by the second step of </st><st c="42780">the pipeline.</st></p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor138"/><st c="42793">There’s more…</st></h2>
			<p><st c="42807">Scikit-learn has</st><a id="_idIndexMarker068"/><st c="42824"> the </st><code><st c="42829">MissingIndicator()</st></code><st c="42847"> transformer that just adds missing indicators. </st><st c="42895">Check it out in the documentation: </st><a href="https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html"><st c="42930">https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html</st></a><st c="43016"> and find an example in the accompanying GitHub repository </st><st c="43075">at </st><a href="https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01-missing-data-imputation/Recipe-06-Marking-imputed-values.ipynb"><st c="43078">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch01</st><st c="43177">-missing-data-imputation/Recipe-06-Marking-imputed-values.ipynb</st></a><st c="43241">.</st></p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor141"/><st c="43242">Implementing forward and backward fill</st></h1>
			<p><st c="43281">Time series data also show missing values. </st><st c="43325">To impute</st><a id="_idIndexMarker069"/><st c="43334"> missing data in time series, we use specific methods. </st><st c="43389">Forward fill imputation involves filling missing values in a dataset with the most recent non-missing value that precedes it in the data sequence. </st><st c="43536">In other words, we carry forward the last seen value to the next valid value. </st><st c="43614">Backward fill imputation involves filling missing values with the next non-missing value that follows it in the data sequence. </st><st c="43741">In other words, we carry the last valid value backward to its preceding </st><st c="43813">valid value.</st></p>
			<p><st c="43825">In this recipe, we will replace missing data in a time series with forward and </st><st c="43905">backward fills.</st></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor142"/><st c="43920">How to do it...</st></h2>
			<p><st c="43936">Let’s begin by importing the required</st><a id="_idIndexMarker070"/><st c="43974"> libraries and time </st><st c="43994">series dataset:</st></p>
			<ol>
				<li><st c="44009">Let’s import </st><code><st c="44023">pandas</st></code> <st c="44029">and </st><code><st c="44034">matplotlib</st></code><st c="44044">:</st><pre class="source-code"><st c="44046">
import matplotlib.pyplot as plt
import pandas as pd</st></pre></li>				<li><st c="44098">Let’s load the air passengers dataset that we described in the </st><em class="italic"><st c="44162">Technical requirements</st></em><st c="44184"> section and display the first five rows of the </st><st c="44232">time series:</st><pre class="source-code"><st c="44244">
df = pd.read_csv(
    "air_passengers.csv",
    parse_dates=["ds"],
    index_col=["ds"],
)
print(df.head())</st></pre><p class="list-inset"><st c="44341">We see the time series in the </st><st c="44372">following output:</st></p><pre class="source-code"><strong class="bold"><st c="44389">                y</st></strong>
<strong class="bold"><st c="44391">ds</st></strong>
<strong class="bold"><st c="44393">1949-01-01  112.0</st></strong>
<strong class="bold"><st c="44410">1949-02-01  118.0</st></strong>
<strong class="bold"><st c="44427">1949-03-01</st></strong><strong class="bold"><st c="44438">  132.0</st></strong>
<strong class="bold"><st c="44444">1949-04-01  129.0</st></strong>
<strong class="bold"><st c="44461">1949-05-01  121.0</st></strong></pre></li>			</ol>
			<p class="callout-heading"><st c="44478">Note</st></p>
			<p class="callout"><st c="44483">You can determine the percentage of missing data by </st><st c="44536">executing </st><code><st c="44546">df.isnull().mean()</st></code><st c="44564">.</st></p>
			<ol>
				<li value="3"><st c="44565">Let’s plot the time series to spot any obvious </st><st c="44613">data gaps:</st><pre class="source-code"><st c="44623">
ax = df.plot(marker=".", figsize=[10, 5], legend=None)
ax.set_title("Air passengers")
ax.set_ylabel("Number of passengers")
ax.set_xlabel("Time")</st></pre><p class="list-inset"><st c="44769">The previous code returns</st><a id="_idIndexMarker071"/><st c="44795"> the following plot, where we see intervals of time where data </st><st c="44858">is missing:</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_05.jpg" alt="Figure 1.5 – Time series data showing missing values"/><st c="44869"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="44967">Figure 1.5 – Time series data showing missing values</st></p>
			<ol>
				<li value="4"><st c="45019">Let’s impute missing data by carrying the last observed value in any interval to the next </st><st c="45110">valid value:</st><pre class="source-code"><st c="45122">
df_imputed = df.ffill()</st></pre><p class="list-inset"><st c="45146">You can verify the absence of missing data by </st><st c="45193">executing </st><code><st c="45203">df_imputed.isnull().sum()</st></code><st c="45228">.</st></p></li>				<li><st c="45229">Let’s now plot the complete dataset and overlay as a dotted line the values used for </st><st c="45315">the imputation:</st><pre class="source-code"><st c="45330">
ax = df_imputed.plot(
    linestyle="-", marker=".", figsize=[10, 5])
df_imputed[df.isnull()].plot(
    ax=ax, legend=None, marker=".", color="r")
ax.set_title("Air passengers")
ax.set_ylabel("Number of passengers")
ax.set_xlabel("Time")</st></pre><p class="list-inset"><st c="45560">The previous code returns the following plot, where we see the values used to replace </st><code><st c="45647">nan</st></code><st c="45650"> as dotted lines</st><a id="_idIndexMarker072"/><st c="45666"> overlaid in between the continuous time </st><st c="45707">series lines:</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_06.jpg" alt="Figure 1.6 – Time series data where missing values were replaced by the last seen observations (dotted line)"/><st c="45720"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="45819">Figure 1.6 – Time series data where missing values were replaced by the last seen observations (dotted line)</st></p>
			<ol>
				<li value="6"><st c="45927">Alternatively, we can impute missing data using </st><st c="45976">backward fill:</st><pre class="source-code"><st c="45990">
df_imputed = df.bfill()</st></pre><p class="list-inset"><st c="46014">If we plot the imputed dataset and overlay the imputation values as we did in </st><em class="italic"><st c="46093">step 5</st></em><st c="46099">, we’ll see the </st><st c="46115">following plot:</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_07.jpg" alt="Figure 1.7 – Time series data where missing values were replaced by backward fill (dotted line)"/><st c="46130"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="46225">Figure 1.7 – Time series data where missing values were replaced by backward fill (dotted line)</st></p>
			<p class="callout-heading"><st c="46320">Note</st></p>
			<p class="callout"><st c="46325">The heights of the values used in the imputation are different in </st><em class="italic"><st c="46392">Figures 1.6 and 1.7</st></em><st c="46411">. In </st><em class="italic"><st c="46416">Figure 1</st></em><em class="italic"><st c="46424">.6</st></em><st c="46426">, we carry the last value forward, hence the height is lower. </st><st c="46488">In </st><em class="italic"><st c="46491">Figure 1</st></em><em class="italic"><st c="46499">.7</st></em><st c="46501">, we carry the next value backward, hence the height </st><st c="46554">is higher.</st></p>
			<p><st c="46564">We’ve now obtained complete</st><a id="_idIndexMarker073"/><st c="46592"> datasets that we can use for time series analysis </st><st c="46643">and modeling.</st></p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor143"/><st c="46656">How it works...</st></h2>
			<p><code><st c="46672">pandas</st></code> <code><st c="46679">ffill()</st></code><st c="46687"> takes the last seen value in any temporal gap in a time series and propagates it forward to the next observed value. </st><st c="46805">Hence, in </st><em class="italic"><st c="46815">Figure 1</st></em><em class="italic"><st c="46823">.6</st></em><st c="46825"> we see the dotted overlay corresponding to the imputation values at the height of the last </st><st c="46917">seen observation.</st></p>
			<p><code><st c="46934">pandas</st></code> <code><st c="46941">bfill()</st></code><st c="46949"> takes the next valid value in any temporal gap in a time series and propagates it backward to the previously observed value. </st><st c="47075">Hence, in </st><em class="italic"><st c="47085">Figure 1</st></em><em class="italic"><st c="47093">.7</st></em><st c="47095"> we see the dotted overlay corresponding to the imputation values at the height of the next observation in </st><st c="47202">the gap.</st></p>
			<p><st c="47210">By default, </st><code><st c="47223">ffill()</st></code><st c="47230"> and </st><code><st c="47235">bfill()</st></code><st c="47242"> will impute all values between valid observations. </st><st c="47294">We can restrict the imputation to a maximum number of data points in any interval by setting a limit, using the </st><code><st c="47406">limit</st></code><st c="47411"> parameter in both methods. </st><st c="47439">For example, </st><code><st c="47452">ffill(limit=10)</st></code><st c="47467"> will only replace</st><a id="_idIndexMarker074"/><st c="47485"> the first 10 data points in </st><st c="47514">any gap.</st></p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor144"/><st c="47522">Carrying out interpolation</st></h1>
			<p><st c="47549">We can impute missing</st><a id="_idIndexMarker075"/><st c="47571"> data in time series by using interpolation between two non-missing data points. </st><st c="47652">Interpolation is the estimation of one or more values in a range by means of a function. </st><st c="47741">In linear interpolation, we fit a linear function between the last observed value and the next valid point. </st><st c="47849">In spline interpolation, we fit a low-degree polynomial between the last and next observed values. </st><st c="47948">The idea of using interpolation is to obtain better estimates of the </st><st c="48017">missing data.</st></p>
			<p><st c="48030">In this recipe, we’ll carry out linear and spline interpolation in a </st><st c="48100">time series.</st></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor145"/><st c="48112">How to do it...</st></h2>
			<p><st c="48128">Let’s begin by importing the required libraries and time </st><st c="48186">series dataset.</st></p>
			<ol>
				<li><st c="48201">Let’s import </st><code><st c="48215">pandas</st></code> <st c="48221">and </st><code><st c="48226">matplotlib</st></code><st c="48236">:</st><pre class="source-code"><st c="48238">
import matplotlib.pyplot as plt
import pandas as pd</st></pre></li>				<li><st c="48290">Let’s load the time series data described in the </st><em class="italic"><st c="48340">Technical </st></em><em class="italic"><st c="48350">requirements</st></em><st c="48362"> section:</st><pre class="source-code"><st c="48371">
df = pd.read_csv(
    "air_passengers.csv",
    parse_dates=["ds"],
    index_col=["ds"],
)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="48451">Note</st></p>
			<p class="callout"><st c="48456">You can plot the time series to find data gaps as we did in </st><em class="italic"><st c="48517">step 3</st></em><st c="48523"> of the </st><em class="italic"><st c="48531">Implementing forward and backward </st></em><em class="italic"><st c="48565">fill</st></em><st c="48569"> recipe.</st></p>
			<ol>
				<li value="3"><st c="48577">Let’s impute the missing</st><a id="_idIndexMarker076"/><st c="48602"> data by </st><st c="48611">linear interpolation:</st><pre class="source-code"><st c="48632">
df_imputed = df.interpolate(method="linear")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="48677">Note</st></p>
			<p class="callout"><st c="48682">If the time intervals between rows are not uniform then the </st><code><st c="48743">method</st></code><st c="48749"> should be set to </st><code><st c="48767">time</st></code><st c="48771"> to achieve a </st><st c="48785">linear fit.</st></p>
			<p class="list-inset"><st c="48796">You can verify the absence of missing data by </st><st c="48843">executing </st><code><st c="48853">df_imputed.isnull().sum()</st></code><st c="48878">.</st></p>
			<ol>
				<li value="4"><st c="48879">Let’s now plot the complete dataset and overlay as a dotted line the values used for </st><st c="48965">the imputation:</st><pre class="source-code"><st c="48980">
ax = df_imputed.plot(
    linestyle="-", marker=".", figsize=[10, 5])
df_imputed[df.isnull()].plot(
    ax=ax, legend=None, marker=".", color="r")
ax.set_title("Air passengers")
ax.set_ylabel("Number of passengers")
ax.set_xlabel("Time")</st></pre><p class="list-inset"><st c="49210">The previous code returns the following plot, where we see the values used to replace </st><code><st c="49297">nan</st></code><st c="49300"> as dotted lines in between the continuous line of the </st><st c="49355">time series:</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_08.jpg" alt="Figure 1.8 – Time series data where missing values were replaced by linear interpolation between the last and next valid data points (dotted line)"/><st c="49367"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="49462">Figure 1.8 – Time series data where missing values were replaced by linear interpolation between the last and next valid data points (dotted line)</st></p>
			<ol>
				<li value="5"><st c="49608">Alternatively, we can impute</st><a id="_idIndexMarker077"/><st c="49637"> missing data by doing spline interpolation. </st><st c="49682">We’ll use a polynomial of the </st><st c="49712">second degree:</st><pre class="source-code"><st c="49726">
df_imputed = df.interpolate(method="spline", order=2)</st></pre><p class="list-inset"><st c="49780">If we plot the imputed dataset and overlay the imputation values as we did in </st><em class="italic"><st c="49859">step 4</st></em><st c="49865">, we’ll see the </st><st c="49881">following plot:</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_09.jpg" alt="Figure 1.9 – Time series data where missing values were replaced by fitting a second-degree polynomial between the last and next valid data points (dotted line)"/><st c="49896"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="49997">Figure 1.9 – Time series data where missing values were replaced by fitting a second-degree polynomial between the last and next valid data points (dotted line)</st></p>
			<p class="callout-heading"><st c="50157">Note</st></p>
			<p class="callout"><st c="50162">Change the degree of the polynomial used in the interpolation to see how the replacement </st><st c="50252">values vary.</st></p>
			<p><st c="50264">We’ve now obtained complete</st><a id="_idIndexMarker078"/><st c="50292"> datasets that we can use for analysis </st><st c="50331">and modeling.</st></p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor146"/><st c="50344">How it works...</st></h2>
			<p><code><st c="50360">pandas</st></code> <code><st c="50367">interpolate()</st></code><st c="50381"> fills missing values in a range by using an interpolation method. </st><st c="50448">When we set the </st><code><st c="50464">method</st></code><st c="50470"> to </st><code><st c="50474">linear</st></code><st c="50480">, </st><code><st c="50482">interpolate()</st></code><st c="50495"> treats all data points as equidistant and fits a line between the last and next valid points in an interval with </st><st c="50609">missing data.</st></p>
			<p class="callout-heading"><st c="50622">Note</st></p>
			<p class="callout"><st c="50627">If you want to perform linear interpolation, but your data points are not equally distanced, set </st><code><st c="50725">method</st></code> <st c="50731">to </st><code><st c="50735">time</st></code><st c="50739">.</st></p>
			<p><st c="50740">We then performed spline interpolation with a second-degree polynomial by setting </st><code><st c="50823">method</st></code><st c="50829"> to </st><code><st c="50833">spline</st></code><st c="50839"> and </st><code><st c="50844">order</st></code> <st c="50849">to </st><code><st c="50853">2</st></code><st c="50854">.</st></p>
			<p><code><st c="50855">pandas</st></code> <code><st c="50862">interpolate()</st></code><st c="50876"> uses </st><code><st c="50882">scipy.interpolate.interp1d</st></code><st c="50908"> and </st><code><st c="50913">scipy.interpolate.UnivariateSpline</st></code><st c="50947"> under the hood, and can therefore implement other interpolation</st><a id="_idIndexMarker079"/><st c="51011"> methods. </st><st c="51021">Check out pandas</st><a id="_idIndexMarker080"/><st c="51037"> documentation for more details </st><st c="51069">at </st><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html"><st c="51072">https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html</st></a><st c="51164">.</st></p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor147"/><st c="51165">See also</st></h2>
			<p><st c="51174">While interpolation aims to get better estimates of the missing data compared to forward and backward fill, these estimates may still not be accurate if the times series show strong trend and seasonality. </st><st c="51380">To obtain better estimates of the missing data in these types of time series, check out time series decomposition</st><a id="_idIndexMarker081"/><st c="51493"> followed by interpolation in the </st><em class="italic"><st c="51527">Feature Engineering for Time Series Course</st></em> <st c="51569">at </st><a href="https://www.trainindata.com/p/feature-engineering-for-forecasting"><st c="51573">https://www.trainindata.com/p/feature-engineering-for-forecasting</st></a><st c="51638">.</st></p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor148"/><st c="51639">Performing multivariate imputation by chained equations</st></h1>
			<p><st c="51695">Multivariate imputat</st><a id="_idTextAnchor149"/><st c="51716">ion</st><a id="_idIndexMarker082"/><st c="51720"> methods, as opposed</st><a id="_idIndexMarker083"/><st c="51740"> to univariate imputation, use multiple variables to estimate the missing values. </st><strong class="bold"><st c="51822">Multivariate Imputation by Chained Equations</st></strong><st c="51866"> (</st><strong class="bold"><st c="51868">MICE</st></strong><st c="51872">) models each variable with missing</st><a id="_idIndexMarker084"/><st c="51908"> values as a function of the remaining variables in the dataset. </st><st c="51973">The output of that function is used to replace </st><st c="52020">missing data.</st></p>
			<p><st c="52033">MICE involves the </st><st c="52052">following steps:</st></p>
			<ol>
				<li><st c="52068">First, it performs a simple univariate imputation to every variable with missing data. </st><st c="52156">For example, </st><st c="52169">median imputation.</st></li>
				<li><st c="52187">Next, it selects one specific variable, say, </st><code><st c="52233">var_1</st></code><st c="52238">, and sets the missing values back </st><st c="52273">to missing.</st></li>
				<li><st c="52284">It trains a model to predict </st><code><st c="52314">var_1</st></code><st c="52319"> using the other variables as </st><st c="52349">input features.</st></li>
				<li><st c="52364">Finally, it replaces the missing values of </st><code><st c="52408">var_1</st></code><st c="52413"> with the output of </st><st c="52433">the model.</st></li>
			</ol>
			<p><st c="52443">MICE repeats </st><em class="italic"><st c="52457">steps 2</st></em><st c="52464"> to </st><em class="italic"><st c="52468">4</st></em><st c="52469"> for each of the </st><st c="52486">remaining variables.</st></p>
			<p><st c="52506">An imputation cycle</st><a id="_idIndexMarker085"/><st c="52526"> concludes once all the variables</st><a id="_idIndexMarker086"/><st c="52559"> have been modeled. </st><st c="52579">MICE carries out multiple imputation cycles, typically 10. </st><st c="52638">That is, we repeat </st><em class="italic"><st c="52657">steps 2</st></em><st c="52664"> to </st><em class="italic"><st c="52668">4</st></em><st c="52669"> for each variable 10 times. </st><st c="52698">The idea is th</st><a id="_idTextAnchor150"/><st c="52712">at by the end of the cycles, we should have found the best possible estimates of the missing data for </st><st c="52815">each variable.</st></p>
			<p class="callout-heading"><st c="52829">Note</st></p>
			<p class="callout"><st c="52834">Multivariate imputation</st><a id="_idIndexMarker087"/><st c="52858"> can be a useful alternative to univariate imputation in situations where we don’t want to distort the variable distributions. </st><st c="52985">Multivariate imputation is also useful when we are interested in having good estimates o</st><a id="_idTextAnchor151"/><a id="_idTextAnchor152"/><st c="53073">f the </st><st c="53080">missing data.</st></p>
			<p><st c="53093">In th</st><a id="_idTextAnchor153"/><st c="53099">is recipe, we will implement MICE </st><st c="53134">using scikit-learn.</st></p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor154"/><st c="53153">How to do it...</st></h2>
			<p><st c="53169">To begin the recipe, let’s import the required libraries and load </st><st c="53236">the data:</st></p>
			<ol>
				<li><st c="53245">Let’s import the required Python libraries, classes, </st><st c="53299">and functions:</st><pre class="source-code"><st c="53313">
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import BayesianRidge
from sklearn.experimental import (
    enable_iterative_imputer
)
from sklearn.impute import (
    IterativeImputer,
    SimpleImputer
)</st></pre></li>				<li><st c="53590">Let’s load some numerical</st><a id="_idIndexMarker088"/><st c="53616"> variables from the dataset</st><a id="_idIndexMarker089"/><st c="53643"> described in the </st><em class="italic"><st c="53661">Technical </st></em><em class="italic"><st c="53671">requirements</st></em><st c="53683"> section:</st><pre class="source-code"><st c="53692">
variables = [
    "A2", "A3", "A8", "A11", "A14", "A15", "target"]
data = pd.read_csv(
    "credit_approval_uci.csv",
    usecols=variables)</st></pre></li>				<li><st c="53821">Let’s divide the data into train and </st><st c="53859">test sets:</st><pre class="source-code"><st c="53869">
X_train, X_test, y_train, y_test = train_test_split(
    data.drop("targ</st><a id="_idTextAnchor155"/><st c="53938">et", axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="54001">Let’s create a MICE imputer using Bayes regression, specifying the number of iteration cycles and setting </st><code><st c="54108">random_state</st></code> <st c="54120">for reproducibility:</st><pre class="source-code"><st c="54141">
imputer = IterativeImputer(
    estimator= BayesianRidge(),
    max_iter=10,
    random_state=0,
).set_output(transform="pandas")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="54259">Note</st></p>
			<p class="callout"><code><st c="54264">IterativeImputer()</st></code><st c="54283"> contains other useful arguments. </st><st c="54317">For example, we can specify the first imputation strategy using the </st><code><st c="54385">initial_strategy</st></code><st c="54401"> parameter. </st><st c="54413">We can choose from the mean, median, mode, or arbitrary imputation. </st><st c="54481">We can also specify how we want to cycle over the variables, either randomly or from the one with the fewest missing values to the one with </st><st c="54621">the most.</st></p>
			<ol>
				<li value="5"><st c="54630">Let’s fit </st><code><st c="54641">IterativeImputer()</st></code><st c="54659"> so that it trains</st><a id="_idIndexMarker090"/><st c="54677"> the estimators</st><a id="_idIndexMarker091"/><st c="54692"> to predict the missing values in </st><st c="54726">each variable:</st><pre class="source-code"><st c="54740">
imputer.fit(X_train)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="54761">Note</st></p>
			<p class="callout"><st c="54766">We can use any regression model to estimate the missing data </st><st c="54828">with </st><code><st c="54833">IterativeImputer()</st></code><st c="54851">.</st></p>
			<ol>
				<li value="6"><st c="54852">Finally, let’s fill in the missing values in both the train and </st><st c="54917">test sets:</st><pre class="source-code"><st c="54927">
X_train_t = imputer.transform(X_train)
X_test_t = imputer.transform(X_test)</st></pre></li>			</ol>
			<p class="callout-heading"><st c="55003">Note</st></p>
			<p class="callout"><st c="55008">To corroborate the lack of missing data, we can </st><st c="55057">execute </st><code><st c="55065">X_train_t.isnull().sum()</st></code><st c="55089">.</st></p>
			<p class="list-inset"><st c="55090">To wrap up the recipe, let’s impute the variables with a simple univariate imputation method and compare the effect on the </st><st c="55214">variables’ distribution.</st></p>
			<ol>
				<li value="7"><st c="55238">Let’s set up scikit-learn’s </st><code><st c="55267">SimpleImputer()</st></code><st c="55282"> to perform mean imputation, and then transform </st><st c="55330">the datasets:</st><pre class="source-code"><st c="55343">
imputer_simple = SimpleImputer(
    strategy="mean").set_output(transform="pandas")
X_train_s =</st><a id="_idTextAnchor156"/><a id="_idTextAnchor157"/><st c="55435"> imputer_simple.fit_transform(X_train)
X_test_s = imputer_simple.transform(X_test)</st></pre></li>				<li><st c="55517">Let’s now make a histogram</st><a id="_idIndexMarker092"/><st c="55544"> of the </st><code><st c="55552">A3</st></code><st c="55554"> variable</st><a id="_idIndexMarker093"/><st c="55563"> after MICE imputation, followed by a histogram of the same variable after </st><st c="55638">mean imputation:</st><pre class="source-code"><st c="55654">
fig, axes = plt.subplots(
    2, 1, figsize=(10, 10), squeeze=False)
X_test_t["A3"].hist(
    bins=50, ax=axes[0, 0], color="blue")
X_test_s["A3"].hist(
    bins=50, ax=axes[1, 0], color="green")
axes[0, 0].set_ylabel('Number of observations')
axes[1, 0].set_ylabel('Number of observations')
axes[0, 0].set_xlabel('A3')
axes[1, 0].set_xlabel('A3')
axes[0, 0].set_title('MICE')
axes[1, 0].set_title('Mean imputation')
plt.show()</st></pre><p class="list-inset"><st c="56070">In the following plot, we see that mean imputation distorts the variable distribution, with more observations toward the </st><st c="56192">mean value:</st></p></li>			</ol>
			<div><div><img src="img/B22396_01_10.jpg" alt="Figure 1.10 –  Histogram of variable A3 after mice imputation (top) or mean imputation (bottom), showing the distortion in the variable distribution caused by the latter"/><st c="56203"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="56285">Figure 1.10 –  Histogram of variable A3 after mice imputation (top) or mean imputation (bottom), showing the distortion in the variable distribution caused by the latter</st></p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor158"/><st c="56453">How it works...</st></h2>
			<p><st c="56469">In this recipe, we performed</st><a id="_idIndexMarker094"/><st c="56498"> multivariate imputation</st><a id="_idIndexMarker095"/><st c="56522"> using </st><code><st c="56529">IterativeImputer()</st></code><st c="56547"> from </st><code><st c="56553">scikit-learn</st></code><st c="56565">. When we fit the model, </st><code><st c="56590">IterativeImputer()</st></code><st c="56608"> carried out the steps that we described in the introduction of the recipe. </st><st c="56684">That is, it imputed all variables with the mean. </st><st c="56733">Then it selected one variable and set its missing values back to missing. </st><st c="56807">And finally, it fitted a Bayes regressor to estimate that variable based on the others. </st><st c="56895">It repeated this procedure for each variable. </st><st c="56941">That was one cycle of imputation. </st><st c="56975">We set it to repeat this process 10 times. </st><st c="57018">By the end of this procedure, </st><code><st c="57048">IterativeImputer()</st></code><st c="57066"> had one Bayes regressor trained to predict the values of each variable based on the other variables in the dataset. </st><st c="57183">With </st><code><st c="57188">transform()</st></code><st c="57199">, it uses the predictions of these Bayes models to impute the </st><st c="57261">missing d</st><a id="_idTextAnchor159"/><st c="57270">ata.</st></p>
			<p><code><st c="57275">Iter</st><a id="_idTextAnchor160"/><st c="57280">ativeImputer()</st></code><st c="57295"> can only impute missing data in numerical variables based on numerical variables. </st><st c="57378">If you want to use categorical variables as input, you need to encode them first. </st><st c="57460">However, keep in mind</st><a id="_idIndexMarker096"/><st c="57481"> that it will only carry </st><a id="_idIndexMarker097"/><st c="57506">out regression. </st><a id="_idTextAnchor161"/><a id="_idTextAnchor162"/><st c="57522">Hence it is </st><a id="_idTextAnchor163"/><st c="57534">not suitable to estimate missing data in discrete or </st><st c="57587">categorical variables.</st></p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor164"/><st c="57609">See also</st></h2>
			<p><st c="57618">To learn more about MICE, take a look at the </st><st c="57664">following resources:</st></p>
			<ul>
				<li><st c="57684">A multivariate technique for multiplying imputing missing values using a sequence of regression </st><st c="57781">models: </st><a href="https://www.researchgate.net/publication/244959137_A_Multivariate_Technique_for_Multiply_Imputing_Missing_Values_Using_a_Sequence_of_Regression_Models"><st c="57789">https://www.researchgate.net/publication/244959137</st></a></li>
				<li><em class="italic"><st c="57839">Multiple Imputation by Chained Equations: W</st><a id="_idTextAnchor165"/><a id="_idTextAnchor166"/><st c="57883">hat is it and how does it </st></em><em class="italic"><st c="57910">work?</st></em><st c="57915">: </st><a href="https://www.jstatsoft.org/article/download/v045i03/550"><st c="57918">https://www.jstatsoft.org/article/download/v045i03/550</st></a></li>
			</ul>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor167"/><st c="57972">Estimating missing data with nearest neighbors</st></h1>
			<p><st c="58019">Imputation with </st><strong class="bold"><st c="58036">K-Nearest Neighbors</st></strong><st c="58055"> (</st><strong class="bold"><st c="58057">KNN</st></strong><st c="58060">) involves estimating missing values</st><a id="_idIndexMarker098"/><st c="58097"> in a dataset by considering</st><a id="_idIndexMarker099"/><st c="58125"> the values of their nearest </st><a id="_idIndexMarker100"/><st c="58154">neighbors, where similarity between data points is determined based on a distance metric, such as the Euclidean distance. </st><st c="58276">It assigns the missing value the average of the nearest neighbors’ values, weighted by </st><st c="58363">their distance.</st></p>
			<p><st c="58378">Consider the following data set containing 4 variables (columns) and 11 observations (rows). </st><st c="58472">We want to impute the dark value in the fifth row of the second variable. </st><st c="58546">First, we find the row’s k-nearest neighbors, where </st><em class="italic"><st c="58598">k=3</st></em><st c="58601"> in our example, and they are highlighted by the rectangular boxes (middle panel). </st><st c="58684">Next, we take the average value shown by the closest neighbors for </st><st c="58751">variable 2.</st></p>
			<div><div><img src="img/B22396_01_11.jpg" alt="Figure 1.11 – Diagram showing a value to impute (dark box), the three closest rows to the value to impute (square boxes), and the values considered to take the average for the imputation"/><st c="58762"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="58870">Figure 1.11 – Diagram showing a value to impute (dark box), the three closest rows to the value to impute (square boxes), and the values considered to take the average for the imputation</st></p>
			<p><st c="59056">The value for the imputation</st><a id="_idIndexMarker101"/><st c="59085"> is given by (value1 × w1 + value2 × w2 + value3 × w3) / 3, where w1, w2, and w3 are proportional to the distance</st><a id="_idIndexMarker102"/><st c="59198"> of the neighbo</st><a id="_idTextAnchor168"/><a id="_idTextAnchor169"/><st c="59213">r to the data </st><st c="59228">to impute.</st></p>
			<p><st c="59238">In this recipe, we will perform KNN imputation </st><st c="59286">using scikit-learn.</st></p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor170"/><st c="59305">How to do it...</st></h2>
			<p><st c="59321">To proceed with the recipe, let’s import the required libraries and prepare </st><st c="59398">the data:</st></p>
			<ol>
				<li><st c="59407">Let’s import the required libraries, classes, </st><st c="59454">and functions:</st><pre class="source-code"><st c="59468">
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import KNNImputer</st></pre></li>				<li><st c="59611">Let’s load the dataset described in the </st><em class="italic"><st c="59652">Technical requirements</st></em><st c="59674"> section (only some </st><st c="59694">numerical variables):</st><pre class="source-code"><st c="59715">
variables = [
    "A2", "A3", "A8", "A11", "A14", "A15", </st><a id="_idTextAnchor171"/><st c="59769">"target"]
d</st><a id="_idTextAnchor172"/><st c="59780">ata = pd.read_csv(
    "credit_approval_uci.csv",
    usecols=variables,
)</st></pre></li>				<li><st c="59847">Let’s divide the data into train and </st><st c="59885">test sets:</st><pre class="source-code"><st c="59895">
X_train, X_test, y_train, y_test = train_test_split(
    data.drop("target", axis=1),
    data["target"],
    test_size=0.3,
    random_state=0,
)</st></pre></li>				<li><st c="60026">Let’s set up the imputer</st><a id="_idIndexMarker103"/><st c="60051"> to replace missing data</st><a id="_idIndexMarker104"/><st c="60075"> with the weighted mean of its closest </st><st c="60114">five neighbors:</st><pre class="source-code"><st c="60129">
imputer = KNNImputer(
    n_neighbors=5, weights="distance",
).set_output(transform="pandas")</st></pre></li>			</ol>
			<p class="callout-heading"><st c="60219">Note</st></p>
			<p class="callout"><st c="60224">The replacement values can be calculated as the uniform mean of the k-nearest neighbors, by setting </st><code><st c="60325">weights</st></code><st c="60332"> to </st><code><st c="60336">uniform</st></code><st c="60343"> or as the weighted average, as we do in the recipe. </st><st c="60396">The weight is based on the distance of the neighbor to the observation to impute. </st><st c="60478">The nearest neighbors carry </st><st c="60506">more weight.</st></p>
			<ol>
				<li value="5"><st c="60518">Find the </st><st c="60528">nearest neighbors:</st><pre class="source-code"><st c="60546">
imputer.fit(X_train)</st></pre></li>				<li><st c="60567">Replace the missing values with the weighted mean of the values shown by </st><st c="60641">the neighbors:</st><pre class="source-code"><st c="60655">
X_train_t = imputer.</st><a id="_idTextAnchor173"/><a id="_idTextAnchor174"/><a id="_idTextAnchor175"/><a id="_idTextAnchor176"/><st c="60676">transform(X_train)
X_test_t</st><a id="_idTextAnchor177"/><a id="_idTextAnchor178"/><st c="60704"> = imputer.transform(X_t</st><a id="_idTextAnchor179"/><st c="60728">est)</st></pre></li>			</ol>
			<p><st c="60733">The result is a</st><a id="_idTextAnchor180"/><st c="60749"> pandas DataFrame with the missing </st><st c="60784">data replaced.</st></p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor181"/><st c="60798">How it works...</st></h2>
			<p><st c="60814">In this recipe, we replaced</st><a id="_idIndexMarker105"/><st c="60842"> missing data with the average</st><a id="_idIndexMarker106"/><st c="60872"> value shown by each observation’s k-nearest neighbors. </st><st c="60928">We set up </st><code><st c="60938">KNNImputer()</st></code><st c="60950"> to find each observation’s five closest neighbors based on the Euclidean distance. </st><st c="61034">The replacement values were estimated as the weighted average of the values shown by the five closest neighbors for the variable to impute. </st><st c="61174">With </st><code><st c="61179">transform()</st></code><st c="61190">, the imputer calculated the replacement value and replaced the </st><st c="61254">missing data.</st></p>
		</div>
	<div></body></html>