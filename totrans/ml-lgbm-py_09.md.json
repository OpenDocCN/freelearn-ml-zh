["```py\nsess = sagemaker.Session()\nregion = sess.boto_region_name\ns3_client = boto3.client(\"s3\", region_name=region)\nsm_client = boto3.client(\"sagemaker\", region_name=region)\nsm_runtime_client = boto3.client(\"sagemaker-runtime\")\n```", "```py\nwrite_bucket = sess.default_bucket()\nwrite_prefix = \"census-income-pipeline\"\nread_bucket = \"sagemaker-data\"\nread_prefix = \"census-income\"\n```", "```py\ninput_data_key = f\"s3://{read_bucket}/{read_prefix}\"\ncensus_income_data_uri = f\"{input_data_key}/census-income.csv\"\noutput_data_uri = f\"s3://{write_bucket}/{write_prefix}/\"\nscripts_uri = f\"s3://{write_bucket}/{write_prefix}/scripts\"\n```", "```py\ntrain_model_id, train_model_version, train_scope = \"lightgbm-classification-model\", \"*\", \"training\"\nprocess_instance_type = \"ml.m5.large\"\ntrain_instance_count = 1\ntrain_instance_type = \"ml.m5.large\"\npredictor_instance_count = 1\npredictor_instance_type = \"ml.m5.large\"\nclarify_instance_count = 1\nclarify_instance_type = \"ml.m5.large\"\n```", "```py\ntrain_image_uri = retrieve(\n    region=\"us-east-1\",\n    framework=None,\n    model_id=train_model_id,\n    model_version=train_model_version,\n    image_scope=train_scope,\n    instance_type=train_instance_type\n)\n```", "```py\ntrain_instance_type_param = ParameterString(\n    name=\"TrainingInstanceType\",\n    default_value=train_instance_type)\ntrain_instance_count_param = ParameterInteger(\n    name=\"TrainingInstanceCount\",\n    default_value=train_instance_count)\ndeploy_instance_type_param = ParameterString(\n    name=\"DeployInstanceType\",\n    default_value=predictor_instance_type)\ndeploy_instance_count_param = ParameterInteger(\n    name=\"DeployInstanceCount\",\n    default_value=predictor_instance_count)\n```", "```py\nlocal_dir = \"/opt/ml/processing\"\ninput_data_path = os.path.join(\"/opt/ml/processing/census-income\", \"census-income.csv\")\nlogger.info(\"Reading claims data from {}\".format(input_data_path))\ndf = pd.read_csv(input_data_path)\n```", "```py\ntrain_output_path = os.path.join(f\"{local_dir}/train\", \"train.csv\")\nX_train.to_csv(train_output_path, index=False)\n```", "```py\ns3_client.upload_file(\n    Filename=\"src/preprocessing.py\", Bucket=write_bucket, Key=f\"{write_prefix}/scripts/preprocessing.py\"\n)\n```", "```py\nsklearn_processor = SKLearnProcessor(\n    framework_version=\"0.23-1\",\n    role=sagemaker_role,\n    instance_count=1,\n    instance_type=process_instance_type,\n    base_job_name=f\"{base_job_name_prefix}-processing\",\n)\n```", "```py\nprocess_step = ProcessingStep(\n    name=\"DataProcessing\",\n    processor=sklearn_processor,\n    inputs=[...],\n    outputs=[...],\n    job_arguments=[\n        \"--train-ratio\", \"0.8\",\n        \"--validation-ratio\", \"0.1\",\n        \"--test-ratio\", \"0.1\"\n    ],\n    code=f\"s3://{write_bucket}/{write_prefix}/scripts/preprocessing.py\"\n)\n```", "```py\ninputs = [ ProcessingInput(source=bank_marketing_data_uri, destination=\"/opt/ml/processing/bank-marketing\") ]\noutputs = [ ProcessingOutput(destination=f\"{processing_output_uri}/train_data\", output_name=\"train_data\",\n                         source=\"/opt/ml/processing/train\"), ... ]\n```", "```py\ndef install(package):\n    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\ninstall(\"lightgbm\")\nimport lightgbm as lgb\n```", "```py\ntrain_df = pd.read_csv(f\"{args.train_data_dir}/train.csv\")\nval_df = pd.read_csv(f\"{args.validation_data_dir}/validation.csv\")\nparams = {\n    \"n_estimators\": args.n_estimators,\n    \"learning_rate\": args.learning_rate,\n    \"num_leaves\": args.num_leaves,\n    \"max_bin\": args.max_bin,\n}\n```", "```py\nX, y = prepare_data(train_df)\nmodel = lgb.LGBMClassifier(**params)\nscores = cross_val_score(model, X, y, scoring=\"f1_macro\")\ntrain_f1 = scores.mean()\nmodel = model.fit(X, y)\nX_test, y_test = prepare_data(val_df)\ntest_f1 = f1_score(y_test, model.predict(X_test))\nprint(f\"[0]#011train-f1:{train_f1:.2f}\")\nprint(f\"[0]#011validation-f1:{test_f1:.2f}\")\n```", "```py\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--boosting_type\", type=str, default=\"gbdt\")\nparser.add_argument(\"--objective\", type=str, default=\"binary\")\nparser.add_argument(\"--n_estimators\", type=int, default=200)\nparser.add_argument(\"--learning_rate\", type=float, default=0.001)\nparser.add_argument(\"--num_leaves\", type=int, default=30)\nparser.add_argument(\"--max_bin\", type=int, default=300)\n```", "```py\nmetrics_data = {\"hyperparameters\": params,\n                \"binary_classification_metrics\":\n{\"validation:f1\": {\"value\": test_f1},\"train:f1\": {\"value\":\ntrain_f1}}\n}\nmetrics_location = args.output_data_dir + \"/metrics.json\"\nmodel_location = args.model_dir + \"/lightgbm-model\"\nwith open(metrics_location, \"w\") as f:\n    json.dump(metrics_data, f)\nwith open(model_location, \"wb\") as f:\n    joblib.dump(model, f)\n```", "```py\nstatic_hyperparams = {\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n}\nlgb_estimator = Estimator(\n    source_dir=\"src\",\n    entry_point=\"lightgbm_train.py\",\n    output_path=estimator_output_uri,\n    code_location=estimator_output_uri,\n    hyperparameters=static_hyperparams,\n    role=sagemaker_role,\n    image_uri=train_image_uri,\n    instance_count=train_instance_count,\n    instance_type=train_instance_type,\n    framework_version=\"1.3-1\",\n)\n```", "```py\nhyperparameter_ranges = {\n    \"n_estimators\": IntegerParameter(10, 400),\n    \"learning_rate\": ContinuousParameter(0.0001, 0.5, scaling_type=\"Logarithmic\"),\n    \"num_leaves\": IntegerParameter(2, 200),\n    \"max_bin\": IntegerParameter(50, 500)\n}\n```", "```py\ntuner_config_dict = {\n    \"estimator\": lgb_estimator,\n    \"max_jobs\": 20,\n    \"max_parallel_jobs\": 2,\n    \"objective_metric_name\": \"validation-f1\",\n    \"metric_definitions\": [{\"Name\": \"validation-f1\", \"Regex\": \"validation-f1:([0-9\\\\.]+)\"}],\n    \"hyperparameter_ranges\": hyperparameter_ranges,\n    \"base_tuning_job_name\": tuning_job_name_prefix,\n    \"strategy\": \"Random\"\n}\ntuner = HyperparameterTuner(**tuner_config_dict)\n```", "```py\ntuning_step = TuningStep(\n    name=\"LGBModelTuning\",\n    tuner=tuner,\n    inputs={\n        \"train\": TrainingInput(...),\n        \"validation\": TrainingInput(...),\n    }\n)\n```", "```py\nmodel = sagemaker.model.Model(\n    image_uri=train_image_uri,\n    model_data=tuning_step.get_top_model_s3_uri(\n        top_k=0, s3_bucket=write_bucket, prefix=model_prefix\n    ),\n    sagemaker_session=sess,\n    role=sagemaker_role\n)\ninputs = sagemaker.inputs.CreateModelInput(instance_type=deploy_instance_type_param)\ncreate_model_step = CreateModelStep(name=\"CensusIncomeModel\", model=model, inputs=inputs)\n```", "```py\nbias_config = clarify.BiasConfig(\n    label_values_or_threshold=[1], facet_name=\"Sex\", facet_values_or_threshold=[0], group_name=\"Age\"\n)\nmodel_predictions_config = sagemaker.clarify.ModelPredictedLabelConfig(probability_threshold=0.5)\nmodel_bias_check_config = ModelBiasCheckConfig(\n    data_config=model_bias_data_config,\n    data_bias_config=bias_config,\n    model_config=model_config,\n    model_predicted_label_config=model_predictions_config,\n    methods=[\"DPPL\"]\n)\n```", "```py\nmodel_bias_check_step = ClarifyCheckStep(\n    name=\"ModelBiasCheck\",\n    clarify_check_config=model_bias_check_config,\n    check_job_config=check_job_config,\n    skip_check=skip_check_model_bias_param,\n    register_new_baseline=register_new_baseline_model_bias_param,\n    supplied_baseline_constraints=supplied_baseline_constraints_model_bias_param\n)\n```", "```py\nshap_config = sagemaker.clarify.SHAPConfig(\n    seed=829,\n    num_samples=100,\n    agg_method=\"mean_abs\",\n    save_local_shap_values=True\n)\n```", "```py\nmodel_explainability_config = ModelExplainabilityCheckConfig(\n    data_config=model_explainability_data_config,\n    model_config=model_config,\n    explainability_config=shap_config\n)\n```", "```py\nmodel_explainability_step = ClarifyCheckStep(\n    name=\"ModelExplainabilityCheck\",\n    clarify_check_config=model_explainability_config,\n    check_job_config=check_job_config,\n    skip_check=skip_check_model_explainability_param,\n    register_new_baseline=register_new_baseline_model_explainability_param,\n    supplied_baseline_constraints=supplied_baseline_constraints_model_explainability_param\n)\n```", "```py\n...\n    test_f1 = f1_score(y_test, model.predict(X_test))\n    # Calculate model evaluation score\n    logger.debug(\"Calculating F1 score.\")\n    metric_dict = {\n        \"classification_metrics\": {\"f1\": {\"value\": test_f1}}\n    }\n```", "```py\n    # Save model evaluation metrics\n    output_dir = \"/opt/ml/processing/evaluation\"\n    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n    logger.info(\"Writing evaluation report with F1: %f\", test_f1)\n    evaluation_path = f\"{output_dir}/evaluation.json\"\n    with open(evaluation_path, \"w\") as f:\n        f.write(json.dumps(metric_dict))\n```", "```py\nmodel_metrics = ModelMetrics(\n    bias_post_training=MetricsSource(\n        s3_uri=model_bias_check_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\"\n    ),\n    explainability=MetricsSource(\n        s3_uri=model_explainability_step.properties.CalculatedBaselineConstraints,\n        content_type=\"application/json\"\n    ),\n)\n```", "```py\ndrift_check_baselines = DriftCheckBaselines(\n    bias_post_training_constraints=MetricsSource(   s3_uri=model_bias_check_step.properties.BaselineUsedForDriftCheckConstraints, content_type=\"application/json\",\n    ),\n    explainability_constraints=MetricsSource(        s3_uri=model_explainability_step.properties.BaselineUsedForDriftCheckConstraints, content_type=\"application/json\",\n    ),\n    explainability_config_file=FileSource(        s3_uri=model_explainability_config.monitoring_analysis_config_uri, content_type=\"application/json\",\n    ))\n```", "```py\nregister_step = RegisterModel(\n    name=\"LGBRegisterModel\",\n    estimator=lgb_estimator,\n    model_data=tuning_step.get_top_model_s3_uri(\n        top_k=0, s3_bucket=write_bucket, prefix=model_prefix\n    ),\n    content_types=[\"text/csv\"],\n    response_types=[\"text/csv\"],\n    inference_instances=[predictor_instance_type],\n    transform_instances=[predictor_instance_type],\n    model_package_group_name=model_package_group_name,\n    approval_status=model_approval_status_param,\n    model_metrics=model_metrics,\n    drift_check_baselines=drift_check_baselines\n)\n```", "```py\ncond_gte = ConditionGreaterThanOrEqualTo(\n    left=JsonGet(\n        step_name=evaluation_step.name,\n        property_file=evaluation_report,\n        json_path=\"classification_metrics.f1.value\",\n    ),\n    right=0.9,\n)\ncondition_step = ConditionStep(\n    name=\"CheckCensusIncomeLGBEvaluation\",\n    conditions=[cond_gte],\n    if_steps=[create_model_step, register_step, lambda_deploy_step],\n    else_steps=[]\n)\n```", "```py\ndef lambda_handler(event, context):\n    sm_client = boto3.client(\"sagemaker\")\n...\n    create_endpoint_config_response = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[{\n            \"VariantName\": \"Alltraffic\",\n            \"ModelName\": model_name,\n            \"InitialInstanceCount\": instance_count,\n            \"InstanceType\": instance_type,\n            \"InitialVariantWeight\": 1}])\n    create_endpoint_response = sm_client.create_endpoint(\n        EndpointName=endpoint_name,\n        EndpointConfigName=endpoint_config_name)\n```", "```py\nlambda_deploy_step = LambdaStep(\n    name=\"LambdaStepRealTimeDeploy\",\n    lambda_func=func,\n    inputs={\n        \"model_name\": pipeline_model_name,\n        \"endpoint_config_name\": endpoint_config_name,\n        \"endpoint_name\": endpoint_name,\n        \"model_package_arn\": register_step.steps[0].properties.ModelPackageArn,\n        \"role\": sagemaker_role,\n        \"instance_type\": deploy_instance_type_param,\n        \"instance_count\": deploy_instance_count_param\n    }\n)\n```", "```py\npipeline = Pipeline(\n    name=pipeline_name,\n    parameters=[process_instance_type_param,\n                train_instance_type_param,\n                train_instance_count_param,\n                deploy_instance_type_param,\n                deploy_instance_count_param,\n                clarify_instance_type_param,\n                skip_check_model_bias_param,\n                register_new_baseline_model_bias_param,                supplied_baseline_constraints_model_bias_param,\n                skip_check_model_explainability_param,                register_new_baseline_model_explainability_param,                supplied_baseline_constraints_model_explainability_param,\n                model_approval_status_param],\n```", "```py\n    steps=[\n        process_step,\n        train_step,\n        evaluation_step,\n        condition_step\n    ],\n    sagemaker_session=sess)\npipeline.upsert(role_arn=sagemaker_role)\n```", "```py\nstart_response = pipeline.start(parameters=dict(\n        SkipModelBiasCheck=True,\n        RegisterNewModelBiasBaseline=True,\n        SkipModelExplainabilityCheck=True,\n        RegisterNewModelExplainabilityBaseline=True))\n```", "```py\npredictor = sagemaker.predictor.Predictor(endpoint_name,                                           sagemaker_session=sess, serializer=CSVSerializer(),                  deserializer=CSVDeserializer())\npayload = test_df.drop([\"Target\"], axis=1).iloc[:5]\nresult = predictor.predict(payload.values)\n```"]