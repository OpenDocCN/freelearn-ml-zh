- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Divide and Conquer – Classification Using Decision Trees and Rules
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分而治之 - 使用决策树和规则进行分类
- en: When deciding between job offers, many people begin by making lists of pros
    and cons, then eliminate options using simple rules. For instance, they may decide,
    “If I have to commute for more than an hour, I will be unhappy,” or “If I make
    less than $50K, I can’t support my family.” In this way, the complex decision
    of predicting one’s future career happiness can be reduced to a series of simple
    decisions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定是否接受工作邀请时，许多人首先列出利弊清单，然后使用简单的规则消除选项。例如，他们可能会决定，“如果我必须通勤超过一小时，我会不开心，”或者“如果我收入低于5万美元，我就无法养家糊口。”通过这种方式，预测个人未来职业幸福感的复杂决策可以简化为一系列简单决策。
- en: This chapter covers decision trees and rule learners—two machine learning methods
    that also make complex decisions from sets of simple choices. These methods present
    their knowledge in the form of logical structures that can be understood with
    no statistical knowledge. This aspect makes these models particularly useful for
    business strategy and process improvement.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了决策树和规则学习器——两种机器学习方法，它们也能从简单选择集合中做出复杂决策。这些方法以逻辑结构的形式呈现其知识，无需统计知识即可理解。这一特性使得这些模型在商业策略和流程改进方面特别有用。
- en: 'By the end of this chapter, you will have learned:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学到：
- en: How trees and rules “greedily” partition data into interesting segments
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树和规则如何“贪婪”地将数据分割成有趣的片段
- en: The most common decision tree and classification rule learners, including the
    C5.0, 1R, and RIPPER algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常见的决策树和分类规则学习器，包括C5.0、1R和RIPPER算法
- en: How to use these algorithms for performing real-world classification tasks,
    such as identifying risky bank loans and poisonous mushrooms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用这些算法执行现实世界的分类任务，例如识别有风险的银行贷款和有毒蘑菇
- en: We will begin by examining decision trees and follow that with a look at classification
    rules. Then, we will summarize what we’ve learned by previewing later chapters,
    which discuss methods that use trees and rules as a foundation for more advanced
    machine learning techniques.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先检查决策树，然后看看分类规则。然后，我们将通过预览后续章节来总结我们所学的知识，这些章节讨论了使用树和规则作为更高级机器学习技术基础的方法。
- en: Understanding decision trees
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解决策树
- en: Decision tree learners are powerful classifiers that utilize a **tree structure**
    to model the relationships among the features and the potential outcomes. As illustrated
    in the following figure, this structure earned its name because it mirrors the
    way a literal tree begins, with a wide trunk at the base that splits off into
    narrower and narrower branches as it works its way upward. In much the same way,
    a decision tree classifier uses a structure of branching decisions to channel
    examples into a final predicted class value.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树学习器是强大的分类器，它们利用**树结构**来模拟特征与潜在结果之间的关系。如图所示，这种结构之所以得名，是因为它反映了真实树木的生长方式，从底部宽大的树干开始，随着向上生长，逐渐分裂成越来越窄的树枝。同样地，决策树分类器使用分支决策的结构来引导示例进入最终的预测类别值。
- en: To better understand how this works in practice, let’s consider the following
    tree, which predicts whether a job offer should be accepted. A job offer under
    consideration begins at the **root node**, from where it then passes through the
    **decision nodes**, which require choices to be made based on the attributes of
    the job. These choices split the data across **branches** that indicate the potential
    outcomes of a decision. They are depicted here as yes or no outcomes, but in other
    cases, there may be more than two possibilities.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这在实践中是如何工作的，让我们考虑以下树，它预测是否应该接受工作邀请。一个正在考虑的工作邀请从**根节点**开始，然后通过**决策节点**，这些节点需要根据工作的属性做出选择。这些选择将数据分割成表示决策潜在结果的**分支**。在这里，它们被描绘为是或否的结果，但在其他情况下，可能存在超过两种可能性。
- en: If a final decision can be made, the tree terminates in **leaf nodes** (also
    known as **terminal nodes**) that denote the action to be taken as the result
    of the series of decisions. In the case of a predictive model, the leaf nodes
    provide the expected result given the series of events in the tree.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可以做出最终决策，树将终止在**叶节点**（也称为**终端节点**），这些节点表示一系列决策的结果所采取的行动。在预测模型的情况下，叶节点提供了给定树中的事件序列的预期结果。
- en: '![](img/B17290_05_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_05_01.png)'
- en: 'Figure 5.1: A decision tree depicting the process of determining whether to
    accept a new job offer'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：一个表示确定是否接受新工作邀请过程的决策树
- en: 'A great benefit of decision tree algorithms is that the flowchart-like tree
    structure is not only for the machine’s internal use. After the model is created,
    many decision tree algorithms output the resulting structure in a human-readable
    format. This provides insight into how and why the model works or doesn’t work
    well for a particular task. This also makes decision trees particularly appropriate
    for applications in which the classification mechanism needs to be transparent
    for legal reasons, or if the results need to be shared with others to inform future
    business practices. With this in mind, some potential uses include the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树算法的一个巨大优势是，类似于流程图的树结构不仅用于机器的内部使用。在模型创建之后，许多决策树算法会将结果结构以人类可读的格式输出。这为理解模型如何以及为什么在特定任务中表现良好或不好提供了洞察。这也使得决策树特别适合于需要出于法律原因使分类机制透明或需要与他人共享结果以告知未来业务实践的应用。考虑到这一点，一些潜在用途包括以下内容：
- en: Credit scoring models in which the criteria that cause an applicant to be rejected
    need to be clearly documented and free from bias
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要明确记录并消除偏见的信用评分模型，其中导致申请人被拒绝的标准需要被清楚地记录
- en: Marketing studies of customer behavior, such as satisfaction or churn, which
    will be shared with management or advertising agencies
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 市场研究客户行为，如满意度或流失，这些将与管理层或广告机构共享
- en: Diagnosis of medical conditions based on laboratory measurements, symptoms,
    or rates of disease progression
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于实验室测量、症状或疾病进展率的医疗状况诊断
- en: Although the previous applications illustrate the value of trees in informing
    decision-making processes, this is not to suggest that their utility ends here.
    In fact, decision trees are one of the single most widely used machine learning
    techniques, and can be applied to model almost any type of data—often with excellent
    out-of-the-box performance.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的应用说明了树在告知决策过程的价值，但这并不意味着它们的效用到此为止。实际上，决策树是单一最广泛使用的机器学习技术之一，可以应用于几乎任何类型的数据——通常具有出色的即插即用性能。
- en: That said, despite their wide applicability, it is worth noting that there are
    some scenarios where trees may not be an ideal fit. This includes tasks where
    the data has many nominal features with many levels or a large number of numeric
    features. These cases may result in a very large number of decisions and an overly
    complex tree. They may also contribute to the tendency of decision trees to overfit
    data, though as we will soon see, even this weakness can be overcome by adjusting
    some simple parameters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，尽管它们的适用性很广，但值得注意的是，在某些情况下，树可能不是理想的选择。这包括数据具有许多名义特征和多个级别或大量数值特征的任务。这些情况可能导致非常多的决策和过于复杂的树。它们还可能加剧决策树过度拟合数据的倾向，尽管正如我们很快就会看到的，通过调整一些简单的参数，甚至这种弱点也可以克服。
- en: Divide and conquer
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分而治之
- en: Decision trees are built using a heuristic called **recursive partitioning**.
    This approach is also commonly known as **divide and conquer** because it splits
    the data into subsets, which are then split repeatedly into even smaller subsets,
    and so on and so forth, until the process stops when the algorithm determines
    the data within the subsets are sufficiently homogenous, or another stopping criterion
    has been met.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是使用一种称为**递归划分**的启发式方法构建的。这种方法也通常被称为**分而治之**，因为它将数据分割成子集，然后这些子集被反复分割成更小的子集，如此类推，直到算法确定子集中的数据足够同质，或者满足另一个停止标准。
- en: To see how splitting a dataset can create a decision tree, imagine a root node
    that will grow into a mature tree. At first, the root node represents the entire
    dataset, since no splitting has transpired. Here, the decision tree algorithm
    must choose a feature to split upon; ideally, it chooses the feature most predictive
    of the target class.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何分割数据集可以创建决策树，想象一个根节点，它将成长为一棵成熟的树。最初，根节点代表整个数据集，因为还没有发生分割。在这里，决策树算法必须选择一个特征进行分割；理想情况下，它选择对目标类最具有预测性的特征。
- en: The examples are then partitioned into groups according to the distinct values
    of this feature, and the first set of tree branches is formed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将示例根据该特征的独特值进行分组，并形成第一组树分支。
- en: 'Working down each branch, the algorithm continues to divide and conquer the
    data, choosing the best candidate feature each time to create another decision
    node until a stopping criterion is reached. Divide and conquer might stop at a
    node if:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着每个分支向下工作，算法继续分割和征服数据，每次选择最佳候选特征来创建另一个决策节点，直到达到停止标准。如果以下条件之一成立，分割和征服可能会在节点处停止：
- en: All (or nearly all) of the examples at the node have the same class
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点上的所有（或几乎所有）示例都属于同一类别
- en: There are no remaining features to distinguish among the examples
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有剩余的特征来区分示例
- en: The tree has grown to a predefined size limit
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树已经增长到预定义的大小限制
- en: 'To illustrate the tree-building process, let’s consider a simple example. Imagine
    that you work for a Hollywood studio, where your role is to decide whether the
    studio should move forward with producing the screenplays pitched by promising
    new authors. After returning from a vacation, your desk is piled high with proposals.
    Without the time to read each proposal cover-to-cover, you decide to develop a
    decision tree algorithm to predict whether a potential movie would fall into one
    of three categories: *Critical Success*, *Mainstream Hit*, or *Box Office Bust*.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明树构建过程，让我们考虑一个简单的例子。想象一下，你为一家好莱坞工作室工作，你的角色是决定工作室是否应该继续制作由有潜力的新作者提出的剧本。度假回来后，你的桌子堆满了提案。没有时间逐个阅读每个提案，你决定开发一个决策树算法来预测潜在电影是否会落入以下三个类别之一：*评论家成功*、*主流热门*或*票房失败*。
- en: 'To source data to create the decision tree model, you turn to the studio archives
    to examine the factors leading to the success or failure of the company’s 30 most
    recent releases. You quickly notice a relationship between the film’s estimated
    shooting budget, the number of A-list celebrities lined up for starring roles,
    and the film’s level of success. Excited about this finding, you produce a scatterplot
    to illustrate the pattern:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建决策树模型而获取数据，你转向电影制片厂的档案来检查导致公司最近30部发行作品成功或失败的因素。你很快注意到电影的预估拍摄预算、主演角色的A名单明星数量以及电影的成功程度之间存在关联。对这个发现感到兴奋，你制作了一个散点图来展示这种模式：
- en: '![A picture containing shape  Description automatically generated](img/B17290_05_02.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![包含形状的图片 自动生成描述](img/B17290_05_02.png)'
- en: 'Figure 5.2: A scatterplot depicting the relationship between a movie’s budget
    and celebrity count'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：散点图展示了电影预算与名人数量之间的关系
- en: 'Using the divide and conquer strategy, you can build a simple decision tree
    from this data. First, to create the tree’s root node, you split the feature indicating
    the number of celebrities, partitioning the movies into groups with and without
    a significant number of A-list stars:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分割和征服策略，你可以从这个数据中构建一个简单的决策树。首先，为了创建树的根节点，你分割了表示名人数量的特征，将电影分为有显著数量的A名单明星和无显著数量的A名单明星的组：
- en: '![A picture containing shape  Description automatically generated](img/B17290_05_03.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![包含形状的图片 自动生成描述](img/B17290_05_03.png)'
- en: 'Figure 5.3: The decision tree’s first split divides the data into films with
    high and low celebrity counts'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：决策树的第一次分割将数据分为高名人数量和低名人数量电影
- en: 'Next, among the group of movies with a larger number of celebrities, you make
    another split between movies with and without a high budget:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在拥有更多名人的电影组中，你在有高预算和无高预算的电影之间进行另一个分割：
- en: '![A picture containing shape  Description automatically generated](img/B17290_05_04.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![包含形状的图片 自动生成描述](img/B17290_05_04.png)'
- en: 'Figure 5.4: The decision tree’s second split further divides the films with
    a high celebrity count into those with low and high budgets'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：决策树的第二次分割进一步将高名人数量电影分为低预算和高预算两类
- en: At this point, you’ve partitioned the data into three groups. The group in the
    top-left corner of the diagram is composed entirely of critically acclaimed films.
    This group is distinguished by a high number of celebrities and a relatively low
    budget. In the top-right corner, nearly all movies are box office hits with high
    budgets and many celebrities. The final group, which has little star power but
    budgets ranging from small to large, contains the flops.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经将数据分为三个组。图的最左上角组完全由获得评论家赞誉的电影组成。这个组的特点是名人数量多而预算相对较低。在右上角，几乎所有电影都是票房大赢家，预算高且名人众多。最后一个组，虽然星光不足，但预算从小到大不等，包含了失败的作品。
- en: If desired, you could continue to divide and conquer the data by splitting it
    on increasingly specific ranges of budget and celebrity count until each of the
    currently misclassified values is correctly classified in its own tiny partition.
    However, it is not advisable to overfit a decision tree in this way. Although
    there is nothing stopping the algorithm from splitting the data indefinitely,
    overly specific decisions do not always generalize more broadly. Thus, you choose
    to avoid the problem of overfitting by stopping the algorithm here, since more
    than 80 percent of the examples in each group are from a single class. This is
    the stopping criterion for the decision tree model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，你可以通过在预算和名人数量越来越具体的范围内分割数据来继续细分和征服数据，直到当前所有错误分类的值都正确分类在其自己的小分区中。然而，以这种方式过度拟合决策树是不明智的。尽管算法可以无限分割数据，但过于具体的决策并不总是能更广泛地推广。因此，你选择在这里停止算法，因为每个组中超过80%的例子都来自单个类别。这是决策树模型的停止标准。
- en: You might have noticed that diagonal lines might have split the data even more
    cleanly. This is one limitation of the decision tree’s knowledge representation,
    which uses **axis-parallel splits**. The fact that each split considers one feature
    at a time prevents the decision tree from forming more complex decision boundaries.
    For example, a diagonal line could be created by a decision that asks, “Is the
    number of celebrities greater than the estimated budget?” If so, then “it will
    be a critical success.”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，对角线可能更干净地分割了数据。这是决策树知识表示的一个局限性，它使用**轴平行分割**。每个分割只考虑一个特征的事实阻止了决策树形成更复杂的决策边界。例如，可以通过一个询问“名人的数量是否大于估计的预算？”的决策来创建一条对角线。如果是这样，那么“它将是一个关键的成功。”
- en: The model for predicting the future success of movies can be represented in
    a simple tree, as shown in the following diagram. Each step in the tree shows
    the fraction of examples falling into each class, which shows how the data becomes
    more homogeneous as the branches get closer to a leaf. To evaluate a new movie
    script, follow the branches through each decision until the script’s success or
    failure has been predicted. Using this approach, you will be able to quickly identify
    the most promising options among the backlog of scripts and get back to more important
    work, such as writing an Academy Awards acceptance speech!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 预测电影未来成功的模型可以用一个简单的树来表示，如下面的图所示。树中的每一步都显示了落入每个类别的示例比例，这显示了随着分支接近叶子，数据如何变得更加同质化。为了评估一个新电影剧本，沿着每个决策分支前进，直到预测出剧本的成功或失败。使用这种方法，你将能够快速识别剧本库中最有希望的选项，并回到更重要的工作，例如撰写奥斯卡颁奖典礼的获奖感言！
- en: '![Diagram  Description automatically generated](img/B17290_05_05.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_05_05.png)'
- en: 'Figure 5.5: A decision tree built on historical movie data can forecast the
    performance of future movies'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：基于历史电影数据的决策树可以预测未来电影的性能
- en: Since real-world data contains more than two features, decision trees quickly
    become far more complex than this, with many more nodes, branches, and leaves.
    In the next section, you will learn about a popular algorithm to build decision
    tree models automatically.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于现实世界的数据包含超过两个特征，决策树很快就会比这复杂得多，具有更多的节点、分支和叶子。在下一节中，你将了解一个流行的自动构建决策树模型的算法。
- en: The C5.0 decision tree algorithm
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C5.0决策树算法
- en: There are numerous implementations of decision trees, but one of the most well
    known is the C5.0 algorithm. This algorithm was developed by computer scientist
    J. Ross Quinlan as an improved version of his prior algorithm, C4.5, which itself
    is an improvement over his **Iterative Dichotomiser 3** (**ID3**) algorithm. Although
    Quinlan markets C5.0 to commercial clients (see [http://www.rulequest.com/](http://www.rulequest.com/)
    for details), the source code for a single-threaded version of the algorithm was
    made public, and has therefore been incorporated into programs such as R.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有多种实现方式，但其中最著名的一种是C5.0算法。该算法由计算机科学家J. Ross Quinlan开发，是他先前算法C4.5的改进版本，而C4.5本身又是他**迭代二分器3**（**ID3**）算法的改进。尽管Quinlan将C5.0推广给商业客户（详情请见[http://www.rulequest.com/](http://www.rulequest.com/)），但该算法的单线程版本源代码已被公开，因此被整合到R等程序中。
- en: To further confuse matters, a popular Java-based open-source alternative to
    C4.5, titled **J48**, is included in R’s `RWeka` package (introduced later in
    this chapter). As the differences between C5.0, C4.5, and J48 are minor, the principles
    in this chapter apply to any of these three methods and the algorithms should
    be considered synonymous.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步混淆问题，一个流行的基于Java的开源C4.5替代品，名为**J48**，包含在R的`RWeka`包中（在本章后面介绍）。由于C5.0、C4.5和J48之间的差异很小，本章中的原则适用于这三种方法中的任何一种，并且算法应被视为同义的。
- en: The C5.0 algorithm has become the industry standard for producing decision trees
    because it does well for most types of problems directly out of the box. Compared
    to other advanced machine learning models, such as those described in *Chapter
    7*, *Black-Box Methods – Neural Networks and Support Vector Machines*, the decision
    trees built by C5.0 generally perform nearly as well but are much easier to understand
    and deploy. Additionally, as shown in the following table, the algorithm’s weaknesses
    are relatively minor and can be largely avoided.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0算法已成为生成决策树的行业标准，因为它可以直接解决大多数类型的问题。与其他高级机器学习模型相比，例如在第7章中描述的*黑盒方法 - 神经网络和支持向量机*，C5.0构建的决策树通常表现几乎一样好，但更容易理解和部署。此外，如以下表格所示，该算法的缺点相对较小，并且可以很大程度上避免。
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: An all-purpose classifier that does well on many types of problems
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种通用的分类器，在许多类型的问题上表现良好
- en: Highly automatic learning process, which can handle numeric or nominal features,
    as well as missing data
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度自动化的学习过程，可以处理数值或名义特征，以及缺失数据
- en: Excludes unimportant features
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排除不重要的特征
- en: Can be used on both small and large datasets
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于小型和大型数据集
- en: Results in a model that can be interpreted without a mathematical background
    (for relatively small trees)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的模型可以在没有数学背景的情况下进行解释（对于相对较小的树）
- en: More efficient than other complex models
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比其他复杂模型更高效
- en: '|'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Decision tree models are often biased toward splits on features having a large
    number of levels
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树模型往往偏向于具有大量级别的特征的分割
- en: It is easy to overfit or underfit the model
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易过拟合或欠拟合模型
- en: Can have trouble modeling some relationships due to reliance on axis-parallel
    splits
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于依赖于轴平行分割，可能难以模拟某些关系
- en: Small changes in training data can result in large changes to decision logic
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的小幅变化可能导致决策逻辑发生大幅变化
- en: Large trees can be difficult to interpret and the decisions they make may seem
    counterintuitive
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型树可能难以解释，它们做出的决策可能看起来不符合直觉
- en: '|'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: To keep things simple, our earlier decision tree example ignored the mathematics
    involved with how a machine would employ a divide and conquer strategy. Let’s
    explore this in more detail to examine how this heuristic works in practice.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简单，我们之前的决策树示例忽略了机器如何采用分而治之策略涉及的数学。让我们更详细地探讨这一点，以检查这种启发式方法在实际中的工作方式。
- en: Choosing the best split
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择最佳分割点
- en: The first challenge that a decision tree will face is to identify which feature
    to split upon. In the previous example, we looked for a way to split the data
    such that the resulting partitions contained examples primarily of a single class.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树将面临的第一挑战是确定要分割哪个特征。在前面的例子中，我们寻找一种分割数据的方法，使得结果分区主要包含单个类别的示例。
- en: The degree to which a subset of examples contains only a single class is known
    as **purity**, and any subset composed of only a single class is called **pure**.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一个示例子集只包含单个类别的程度被称为**纯度**，而只由单个类别组成的任何子集都称为**纯集**。
- en: There are various measurements of purity that can be used to identify the best
    decision tree splitting candidate. C5.0 uses **entropy**, a concept borrowed from
    information theory that quantifies the randomness, or disorder, within a set of
    class values. Sets with high entropy are very diverse and provide little information
    about other items that may also belong in the set, as there is no apparent commonality.
    The decision tree hopes to find splits that reduce entropy, ultimately increasing
    homogeneity within the groups.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种纯度度量可以用来识别最佳的决策树分割候选者。C5.0使用**熵**，这是从信息理论中借用的一个概念，它量化了类别值集合中的随机性或无序性。具有高熵的集合非常多样化，并且关于可能也属于该集合的其他项目提供的信息很少，因为没有明显的共同点。决策树希望找到可以减少熵的分割点，从而最终增加组内的同质性。
- en: Typically, entropy is measured in **bits**. If there are only two possible classes,
    entropy values can range from 0 to 1\. For *n* classes, entropy ranges from 0
    to *log*[2](*n*). In each case, the minimum value indicates that the sample is
    completely homogenous, while the maximum value indicates that the data are as
    diverse as possible, and no group has even a small plurality.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，熵以 **比特** 为单位测量。如果只有两个可能的类别，熵值可以从 0 到 1 变化。对于 *n* 个类别，熵的范围从 0 到 *log*[2](*n*)。在每种情况下，最小值表示样本完全同质，而最大值表示数据尽可能多样化，并且没有组有哪怕是很小的多数。
- en: 'In mathematical notion, entropy is specified as:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学概念中，熵被定义为：
- en: '![](img/B17290_05_001.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_05_001.png)'
- en: In this formula, for a given segment of data (*S*), the term *c* refers to the
    number of class levels, and *p*[i] refers to the proportion of values falling
    into the *i*th class level.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，对于给定数据段 (*S*)，术语 *c* 指的是类别级别数，而 *p*[i] 指的是落在第 *i* 个类别级别中的值的比例。
- en: 'For example, suppose we have a partition of data with two classes: red (60
    percent) and white (40 percent). We can calculate the entropy as:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个包含两个类别的数据分区：红色（60%）和白色（40%）。我们可以计算熵如下：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can visualize the entropy for all possible two-class arrangements. If we
    know the proportion of examples in one class is *x*, then the proportion in the
    other class is *(1 – x)*. Using the `curve()` function, we can then plot the entropy
    for all possible values of *x*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以可视化所有可能的二分类排列的熵。如果我们知道一个类别的示例比例是 *x*，那么另一个类别的比例是 *(1 – x)*。然后，使用 `curve()`
    函数，我们可以绘制所有可能的 *x* 值的熵：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This results in the following graph:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下图表：
- en: '![Chart, scatter chart  Description automatically generated](img/B17290_05_06.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图表，散点图  自动生成的描述](img/B17290_05_06.png)'
- en: 'Figure 5.6: The total entropy as the proportion of one class varies in a two-class
    outcome'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6：在二分类结果中，随着一个类别的比例变化，总熵
- en: As illustrated by the peak at *x = 0.50*, a 50-50 split results in the maximum
    entropy. As one class increasingly dominates the other, the entropy reduces to
    zero.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *x = 0.50* 处的峰值所示，50-50 的分裂会产生最大熵。当一个类别越来越多地主导另一个类别时，熵减少到零。
- en: 'To use entropy to determine the optimal feature to split upon, the algorithm
    calculates the change in homogeneity that would result from a split on each possible
    feature, a measure known as **information gain**. The information gain for a feature
    *F* is calculated as the difference between the entropy in the segment before
    the split (*S*[1]) and the partitions resulting from the split (*S*[2]):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用熵来确定最优的分裂特征，算法会计算在每种可能的特征上分裂所导致的同质性的变化，这种度量称为 **信息增益**。特征 *F* 的信息增益是分裂前段
    (*S*[1]) 的熵与分裂后的分区 (*S*[2]) 的熵之间的差值：
- en: InfoGain(*F*) = Entropy(S[1]) – Entropy(S[2])
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGain(*F*) = Entropy(S[1]) – Entropy(S[2])
- en: 'One complication is that after a split, the data is divided into more than
    one partition. Therefore, the function to calculate *Entropy(S*[2]*)* needs to
    consider the total entropy across all partitions resulting from the split. It
    does this by weighting each partition’s entropy according to the proportion of
    all records falling into that partition. This can be stated in a formula as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个复杂的问题是，在分裂之后，数据被分成多个分区。因此，计算 *Entropy(S*[2]*)* 的函数需要考虑分裂后所有分区的总熵。它是通过根据所有记录落在该分区中的比例来加权每个分区的熵来做到这一点的。这可以用以下公式表示：
- en: '![](img/B17290_05_002.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17290_05_002.png)'
- en: In simple terms, the total entropy resulting from a split is the sum of entropy
    of each of the *n* partitions weighted by the proportion of examples falling in
    the partition (*w*[i]).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，分裂产生的总熵是每个 *n* 个分区的熵的总和，每个分区的熵都按落在该分区中的示例比例 (*w*[i]) 加权。
- en: The higher the information gain, the better a feature is at creating homogeneous
    groups after a split on that feature. If the information gain is zero, there is
    no reduction in entropy for splitting on this feature. On the other hand, the
    maximum information gain is equal to the entropy prior to the split. This would
    imply the entropy after the split is zero, which means that the split results
    in completely homogeneous groups.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益越高，特征在分裂该特征后创建同质组的性能就越好。如果信息增益为零，则在该特征上分裂不会减少熵。另一方面，最大信息增益等于分裂前的熵。这表明分裂后的熵为零，这意味着分裂产生了完全同质的组。
- en: The previous formulas assume nominal features, but decision trees use information
    gain for splitting on numeric features as well. To do so, a common practice is
    to test various splits that divide the values into groups greater than or less
    than a threshold. This reduces the numeric feature into a two-level categorical
    feature that allows information gain to be calculated as usual. The numeric cut
    point yielding the largest information gain is chosen for the split.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的公式假设名义特征，但决策树在分割数值特征时也使用信息增益。为此，一种常见的做法是测试各种分割，将值分为大于或小于阈值的组。这把数值特征简化为两个级别的分类特征，从而允许像往常一样计算信息增益。产生最大信息增益的数值切割点被选为分割点。
- en: Though it is used by C5.0, information gain is not the only splitting criterion
    that can be used to build decision trees. Other commonly used criteria are the
    **Gini index**, **chi-squared statistic**, and **gain ratio**. For a review of
    these (and many more) criteria, refer to *An Empirical Comparison of Selection
    Measures for Decision-Tree Induction, Mingers, J, Machine Learning, 1989, Vol.
    3, pp. 319-342*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然C5.0使用了信息增益，但它不是构建决策树时可以使用的唯一分割标准。其他常用的标准包括**基尼指数**、**卡方统计量**和**增益率**。关于这些（以及许多其他）标准的综述，请参阅*Mingers,
    J, Machine Learning, 1989, Vol. 3, pp. 319-342*。
- en: Pruning the decision tree
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 剪枝决策树
- en: As mentioned earlier, a decision tree can continue to grow indefinitely, choosing
    splitting features and dividing into smaller and smaller partitions until each
    example is perfectly classified or the algorithm runs out of features to split
    on. However, if the tree grows overly large, many of the decisions it makes will
    be overly specific and the model will be overfitted to the training data. The
    process of **pruning** a decision tree involves reducing its size such that it
    generalizes better to unseen data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，决策树可以无限增长，选择分割特征并将数据分成越来越小的分区，直到每个示例都被完美分类或算法耗尽可用于分割的特征。然而，如果树长得过于庞大，它所做的许多决策将过于具体，模型将过度拟合训练数据。剪枝决策树的过程涉及减小其大小，以便更好地泛化到未见数据。
- en: One solution to this problem is to stop the tree from growing once it reaches
    a certain number of decisions or when the decision nodes contain only a small
    number of examples. This is called **early stopping** or **pre-pruning** the decision
    tree. As the tree avoids doing needless work, this is an appealing strategy. However,
    one downside to this approach is that there is no way to know whether the tree
    will miss subtle but important patterns that it would have learned had it grown
    to a larger size.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法之一是在树达到一定数量的决策或决策节点只包含少量示例时停止树的生长。这被称为**早期停止**或**预剪枝**决策树。由于树避免了做无谓的工作，这是一种吸引人的策略。然而，这种方法的一个缺点是，无法知道树是否会错过它如果长得更大可能会学习到的微妙但重要的模式。
- en: An alternative, called **post-pruning**, involves growing a tree that is intentionally
    too large and pruning leaf nodes to reduce the size of the tree to a more appropriate
    level. This is often a more effective approach than pre-pruning because it is
    quite difficult to determine the optimal depth of a decision tree without growing
    it first. Pruning the tree later allows the algorithm to be certain that all the
    important data structures were discovered.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种称为**后剪枝**的方法涉及有意生长一个过大的树，并剪枝叶节点以将树的大小减小到更合适的水平。这通常比预剪枝更有效，因为在不首先生长树的情况下很难确定决策树的最佳深度。在稍后剪枝树允许算法确信所有重要的数据结构都已发现。
- en: The implementation details of pruning operations are very technical and beyond
    the scope of this book. For a comparison of some of the available methods, see
    *A Comparative Analysis of Methods for Pruning Decision Trees, Esposito, F, Malerba,
    D, Semeraro, G, IEEE Transactions on Pattern Analysis and Machine Intelligence,
    1997, Vol. 19, pp. 476-491*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝操作的实现细节非常技术性，超出了本书的范围。关于一些可用方法的比较，请参阅*Esposito, F, Malerba, D, Semeraro, G,
    IEEE Transactions on Pattern Analysis and Machine Intelligence, 1997, Vol. 19,
    pp. 476-491*。
- en: One of the benefits of the C5.0 algorithm is that it is opinionated about pruning—it
    takes care of many of the decisions automatically using reasonable defaults. Its
    overall strategy is to post-prune the tree. It first grows a large tree that overfits
    the training data. Later, the nodes and branches that have little effect on the
    classification errors are removed. In some cases, entire branches are moved further
    up the tree or replaced by simpler decisions. These processes of grafting branches
    are known as **subtree raising** and **subtree replacement**, respectively.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0 算法的一个优点是它在剪枝方面有明确的观点——它使用合理的默认值自动处理许多决策。其整体策略是在剪枝后对树进行修剪。它首先生长出一个大的树，该树过度拟合了训练数据。随后，移除对分类错误影响较小的节点和分支。在某些情况下，整个分支会被移动到树的更高位置或被更简单的决策所取代。这些移除分支的过程分别被称为**子树提升**和**子树替换**。
- en: Getting the right balance of overfitting and underfitting is a bit of an art,
    but if model accuracy is vital, it may be worth investing some time with various
    pruning options to see if it improves the test dataset performance. As you will
    soon see, one of the strengths of the C5.0 algorithm is that it is very easy to
    adjust the training options.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在过度拟合和欠拟合之间取得正确的平衡有点像一门艺术，但如果模型准确性至关重要，那么花些时间尝试不同的剪枝选项以查看是否可以提高测试数据集的性能可能是值得的。正如你很快就会看到的，C5.0
    算法的一个优点是它非常容易调整训练选项。
- en: Example – identifying risky bank loans using C5.0 decision trees
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 使用 C5.0 决策树识别风险银行贷款
- en: The global financial crisis of 2007-2008 highlighted the importance of transparency
    and rigor in banking practices. As the availability of credit was limited, banks
    tightened their lending systems and turned to machine learning to more accurately
    identify risky loans.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 2007-2008 年的全球金融危机突出了银行实践中透明度和严谨性的重要性。由于信贷的可用性有限，银行收紧了其贷款系统，并转向机器学习以更准确地识别风险贷款。
- en: Decision trees are widely used in the banking industry due to their high accuracy
    and ability to formulate a statistical model in plain language. Since governments
    in many countries carefully monitor the fairness of lending practices, executives
    must be able to explain why one applicant was rejected for a loan while another
    was approved. This information is also useful for customers hoping to determine
    why their credit rating is unsatisfactory.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树具有高准确性和用普通语言制定统计模型的能力，因此在银行业中得到广泛应用。由于许多国家的政府仔细监控贷款实践的公平性，因此高管必须能够解释为什么某个申请人被拒绝贷款而另一个被批准。这些信息对希望确定其信用评级为何不满意的客户也很有用。
- en: It is likely that automated credit scoring models are used for credit card mailings
    and instant online approval processes. In this section, we will develop a simple
    credit approval model using C5.0 decision trees. We will also see how the model
    results can be tuned to minimize errors that result in a financial loss.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化信用评分模型可能用于信用卡邮寄和即时在线批准流程。在本节中，我们将使用 C5.0 决策树开发一个简单的信用批准模型。我们还将看到如何调整模型结果以最小化导致财务损失的错误。
- en: Step 1 – collecting data
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步 – 收集数据
- en: The motivation for our credit model is to identify factors that are linked to
    a higher risk of loan default. To do this, we must obtain data on past bank loans
    as well as information about the loan applicants that would have been available
    at the time of credit application.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们信用模型的动力在于识别与贷款违约高风险相关的因素。为此，我们必须获取关于过去银行贷款的数据以及当时在信用申请时可能可用的贷款申请人信息。
- en: Data with these characteristics are available in a dataset donated to the UCI
    Machine Learning Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))
    by Hans Hofmann of the University of Hamburg. The dataset contains information
    on loans obtained from a credit agency in Germany.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 具有这些特征的数据集由汉堡大学的 Hans Hofmann 捐赠给 UCI 机器学习仓库 ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))。该数据集包含来自德国一家信用机构的贷款信息。
- en: The dataset presented in this chapter has been modified slightly from the original
    in order to eliminate some preprocessing steps. To follow along with the examples,
    download the `credit.csv` file from the Packt Publishing GitHub repository for
    this chapter and save it to your R working directory.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的数据集已经对原始数据进行了轻微修改，以消除一些预处理步骤。为了跟随示例，请从本章的 Packt Publishing GitHub 仓库下载
    `credit.csv` 文件，并将其保存到您的 R 工作目录中。
- en: The credit dataset includes 1,000 examples of loans, plus a set of numeric and
    nominal features indicating characteristics of the loan and the loan applicant.
    A class variable indicates whether the loan went into default. Let’s see if we
    can identify any patterns that predict this outcome.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 信用数据集包括1,000个贷款示例，以及一组表示贷款和贷款申请人特征的数值和名义特征。一个类别变量表示贷款是否违约。让我们看看我们是否能识别出任何预测这种结果的模式。
- en: Step 2 – exploring and preparing the data
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 探索和准备数据
- en: 'As we have done previously, we will import the data using the `read.csv()`
    function. Now, because the character data is entirely categorical, we can set
    the `stringsAsFactors = TRUE` to automatically convert all character type columns
    to factors in the resulting data frame:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所做的那样，我们将使用`read.csv()`函数导入数据。现在，因为字符数据完全是分类的，我们可以将`stringsAsFactors =
    TRUE`设置为自动将所有字符类型列转换为结果数据框中的因子：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can check the resulting object by examining the first few lines of output
    from the `str()` function:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查`str()`函数输出的前几行来检查结果对象：
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We see the expected 1,000 observations and 17 features, which are a combination
    of factor and integer data types.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到预期的1,000个观测值和17个特征，这些特征是因子和整型数据类型的组合。
- en: 'Let’s take a look at the `table()` output for a couple of loan features that
    seem likely to predict a default. The applicant’s checking and savings account
    balances are recorded as categorical variables:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看几个似乎可以预测违约的贷款特征的`table()`输出。申请人的检查和储蓄账户余额被记录为分类变量：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The checking and savings account balances may prove to be important predictors
    of loan default status. Note that since the loan data was obtained from Germany,
    the values use the **Deutsche Mark** (**DM**), which was the currency used in
    Germany prior to the adoption of the Euro.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 检查和储蓄账户余额可能是贷款违约状态的重要预测指标。请注意，由于贷款数据来自德国，这些值使用的是**德国马克**（**DM**），这是欧元采用之前德国使用的货币。
- en: 'Some of the loan’s features are numeric, such as its duration and the amount
    of credit requested:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 一些贷款的特征是数值型的，例如其期限和请求的信贷金额：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The loan amounts ranged from 250 DM to 18,420 DM across terms of 4 to 72 months.
    They had a median amount of 2,320 DM and a median duration of 18 months.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 贷款金额从250 DM到18,420 DM不等，期限为4到72个月。它们的中间值为2,320 DM，中间期限为18个月。
- en: 'The `default` vector indicates whether the loan applicant was able to meet
    the agreed payment terms or if they went into default. A total of 30 percent of
    the loans in this dataset went into default:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`default`向量表示贷款申请人是否能够满足约定的还款条款，或者他们是否违约。在这个数据集中，总共有30%的贷款违约：'
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: A high rate of default is undesirable for a bank because it means that the bank
    is unlikely to fully recover its investment. If we are successful, our model will
    identify applicants who are at high risk of default, allowing the bank to refuse
    the credit request before the money is given.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于银行来说，高违约率是不可取的，因为这意味着银行不太可能完全收回其投资。如果我们成功，我们的模型将识别出高违约风险的申请人，使银行能够在发放资金之前拒绝信贷申请。
- en: Data preparation – creating random training and test datasets
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备 – 创建随机训练和测试数据集
- en: 'As we have done in previous chapters, we will split our data into two portions:
    a training dataset to build the decision tree and a test dataset to evaluate its
    performance on new data. We will use 90 percent of the data for training and 10
    percent for testing, which will provide us with 100 records to simulate new applicants.
    A 90-10 split is used here rather than the more common 75-25 split due to the
    relatively small size of the credit dataset; given that predicting loan defaults
    is a challenging learning task, we need as much training data as possible while
    still holding out a sufficient test sample.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的章节中所做的那样，我们将把我们的数据分成两部分：一个用于构建决策树的训练数据集和一个用于评估其在新数据上性能的测试数据集。我们将使用90%的数据进行训练，10%的数据进行测试，这将为我们提供100条记录来模拟新申请人。在这里使用90-10的分割而不是更常见的75-25分割，是因为信用数据集相对较小；鉴于预测贷款违约是一个具有挑战性的学习任务，我们需要尽可能多的训练数据，同时还要保留足够的测试样本。
- en: More sophisticated approaches for training and evaluating models with relatively
    small datasets are introduced in *Chapter 10*, *Evaluating Model Performance*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10章“评估模型性能”中介绍了用于训练和评估相对较小数据集的更复杂方法。
- en: As prior chapters used data that had been sorted in a random order, we simply
    divided the dataset into two portions by taking the first subset of records for
    training and the remaining subset for testing. In contrast, the credit dataset
    is not randomly ordered, making the prior approach unwise. Suppose that the bank
    had sorted the data by the loan amount, with the largest loans at the end of the
    file. If we used the first 90 percent for training and the remaining 10 percent
    for testing, we would be training a model on only the small loans and testing
    the model on the big loans. Obviously, this could be problematic.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前几章使用的数据是随机排序的，我们只需通过取记录的第一个子集用于训练，剩余的子集用于测试，就可以将数据集分为两部分。相比之下，信用数据集并不是随机排序的，这使得先前的做法不明智。假设银行是按照贷款金额对数据进行排序的，最大的贷款位于文件末尾。如果我们用前90%的数据用于训练，剩下的10%用于测试，那么我们将在只有小额贷款上训练模型，并在大额贷款上测试模型。显然，这可能会出现问题。
- en: We’ll solve this problem by training the model on a **random sample** of the
    credit data. A random sample is simply a process that selects a subset of records
    at random. In R, the `sample()` function is used to perform random sampling. However,
    before putting it in action, a common practice is to set a **seed** value, which
    causes the randomization process to follow a sequence that can be replicated later.
    It may seem that this defeats the purpose of generating random numbers, but there
    is a good reason for doing it this way. Providing a seed value via the `set.seed()`
    function ensures that if the analysis is repeated in the future, an identical
    result is obtained.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过在信用数据的**随机样本**上训练模型来解决此问题。随机样本简单来说就是一个随机选择记录子集的过程。在R中，`sample()`函数用于执行随机抽样。然而，在付诸实践之前，一个常见的做法是设置一个**种子**值，这会导致随机化过程遵循一个可以后来复制的序列。这似乎违背了生成随机数的初衷，但这样做有很好的理由。通过`set.seed()`函数提供种子值可以确保，如果分析在未来重复进行，将获得相同的结果。
- en: You may wonder how a so-called random process can be seeded to produce an identical
    result. This is because computers use a mathematical function called a **pseudorandom
    number generator** to create random number sequences that appear to act very random,
    but are actually quite predictable given knowledge of the previous values in the
    sequence. In practice, modern pseudorandom number sequences are virtually indistinguishable
    from true random sequences, but have the benefit that computers can generate them
    quickly and easily.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道一个所谓的随机过程如何被播种以产生相同的结果。这是因为计算机使用一种称为**伪随机数生成器**的数学函数来创建看似非常随机的随机数序列，但实际上，如果知道序列中的前一个值，它们是非常可预测的。在实践中，现代伪随机数序列几乎与真正的随机序列无法区分，但它们的好处是计算机可以快速轻松地生成它们。
- en: 'The following commands use `sample()` with a seed value. Note that the `set.seed()`
    function uses the arbitrary value `9829`. Omitting this seed will cause your training
    and testing splits to differ from those shown in the remainder of this chapter.
    The following commands select 900 values at random out of the sequence of integers
    from 1 to 1,000:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令使用带有种子值的`sample()`。请注意，`set.seed()`函数使用任意值`9829`。省略此种子值将导致您的训练和测试分割与本章其余部分所示的不同。以下命令从1到1,000的整数序列中随机选择900个值：
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'As expected, the resulting `train_sample` object is a vector of 900 random
    integers:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，生成的`train_sample`对象是一个包含900个随机整数的向量：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'By using this vector to select rows from the credit data, we can split it into
    the 90 percent training and 10 percent test datasets we desired. Recall that the
    negation operator (the `-` character) used in the selection of the test records
    tells R to select records that are not in the specified rows; in other words,
    the test data includes only the rows that are not in the training sample:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个向量从信用数据中选择行，我们可以将其分为我们想要的90%训练数据和10%测试数据集。回想一下，在测试记录的选择中使用的否定运算符（`-`字符）告诉R选择不在指定行中的记录；换句话说，测试数据只包括不在训练样本中的行：
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If randomization was done correctly, we should have about 30 percent of loans
    with default in each of the datasets:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果随机化做得正确，我们应在每个数据集中大约有30%的贷款出现违约：
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Both the training and test datasets have roughly similar distributions of loan
    defaults, so we can now build our decision tree. In the case that the proportions
    differ greatly, we may decide to resample the dataset, or attempt a more sophisticated
    sampling approach, such as those covered in *Chapter 10*, *Evaluating Model Performance*.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集在贷款违约的分布上大致相似，因此我们现在可以构建我们的决策树。如果比例差异很大，我们可能会决定重新采样数据集，或者尝试更复杂的采样方法，例如在第10章“评估模型性能”中介绍的方法。
- en: If your results do not match exactly, ensure that you ran the command `set.seed(9829)`
    immediately prior to creating the `train_sample` vector. Note that R’s default
    random number generator changed in R version 3.6.0 and your results will differ
    if this code is run on earlier versions. This also means that the results here
    are slightly different from those in prior editions of this book.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的结果不完全匹配，请确保在创建`train_sample`向量之前立即运行了`set.seed(9829)`命令。请注意，R的默认随机数生成器在R版本3.6.0中发生了变化，如果在此代码在早期版本上运行，则结果将不同。这也意味着这里的结果与本书先前版本中的结果略有不同。
- en: Step 3 – training a model on the data
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 在数据上训练模型
- en: We will use the C5.0 algorithm in the `C50` package for training our decision
    tree model. If you have not done so already, install the package with `install.packages("C50")`
    and load it to your R session using `library(C50)`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`C50`包中的C5.0算法来训练我们的决策树模型。如果您尚未安装，请使用`install.packages("C50")`安装该包，并使用`library(C50)`将其加载到R会话中。
- en: The following syntax box lists some of the most common parameters used when
    building decision trees. Compared to the machine learning approaches we have used
    previously, the C5.0 algorithm offers many more ways to tailor the model to a
    particular learning problem.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下语法框列出了在构建决策树时使用的一些最常见参数。与之前使用的机器学习方法相比，C5.0算法提供了许多更多的方式来定制模型以适应特定的学习问题。
- en: '![](img/B17290_05_07.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17290_05_07.png)'
- en: 'Figure 5.7: C5.0 decision tree syntax'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：C5.0决策树语法
- en: The `C5.0()` function uses a new syntax known as the **R formula interface**
    to specify the model to be trained. The formula syntax uses the `~` operator (known
    as the tilde) to express the relationship between a target variable and its predictors.
    The class variable to be learned goes to the left of the tilde and the predictor
    features are written on the right, separated by `+` operators.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`C5.0()`函数使用一种称为**R公式接口**的新语法来指定要训练的模型。公式语法使用`~`运算符（称为波浪号）来表示目标变量与其预测变量之间的关系。要学习的类别变量放在波浪号的左侧，预测特征写在右侧，由`+`运算符分隔。'
- en: If you would like to model the relationship between the target `y` and predictors
    `x1` and `x2`, you would write the formula as `y ~ x1 + x2`. To include all variables
    in the model, the period character is used. For example, `y ~ .` specifies the
    relationship between `y` and all other features in the dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想建模目标`y`与预测变量`x1`和`x2`之间的关系，您将公式写成`y ~ x1 + x2`。要包含模型中的所有变量，使用点字符。例如，`y ~
    .`指定了`y`与数据集中所有其他特征之间的关系。
- en: The R formula interface is used across many R functions and offers some powerful
    features to describe the relationships among predictor variables. We will explore
    some of these features in later chapters. However, if you’re eager for a preview,
    feel free to read the documentation using the `?formula` command.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: R公式接口在许多R函数中使用，并提供了一些强大的功能来描述预测变量之间的关系。我们将在后面的章节中探讨一些这些功能。然而，如果您急于预览，请随时使用`?formula`命令阅读文档。
- en: 'For the first iteration of the credit approval model, we’ll use the default
    C5.0 settings, as shown in the following code. The target class is named `default`,
    so we put it on the left-hand side of the tilde, which is followed by a period
    indicating that all other columns in the `credit_train` data frame are to be used
    as predictors:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于信用审批模型的第一次迭代，我们将使用默认的C5.0设置，如下面的代码所示。目标类别命名为`default`，所以我们将其放在波浪号`~`的左侧，后面跟着一个点表示`credit_train`数据框中的所有其他列都将用作预测变量：
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `credit_model` object now contains a C5.0 decision tree. We can see some
    basic data about the tree by typing its name:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`credit_model`对象现在包含一个C5.0决策树。我们可以通过输入其名称来查看一些关于树的基本数据：'
- en: '[PRE25]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output shows some simple facts about the tree, including the function call
    that generated it, the number of features (labeled `predictors`), and examples
    (labeled `samples`) used to grow the tree. Also listed is the tree size of `67`,
    which indicates that the tree is 67 decisions deep—quite a bit larger than the
    example trees we’ve considered so far!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了关于树的一些简单事实，包括生成它的函数调用、用于构建树的特性数量（标记为`predictors`）和示例（标记为`samples`）。还包括树的大小为`67`，这表明树有67个决策深度——比我们迄今为止考虑的示例树大得多！
- en: 'To see the tree’s decisions, we can call the `summary()` function on the model:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看树的决策，我们可以在模型上调用`summary()`函数：
- en: '[PRE27]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This results in the following output, which has been truncated to show only
    the first few lines:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致以下输出，其中已截断以仅显示前几行：
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding output shows some of the first branches in the decision tree.
    The first three lines could be represented in plain language as:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 上述输出显示了决策树的一些最初分支。前三行可以用普通语言表示为：
- en: If the checking account balance is unknown or greater than 200 DM, then classify
    as “not likely to default”
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果支票账户余额未知或超过200 DM，则将其分类为“不太可能违约”
- en: Otherwise, if the checking account balance is less than zero DM or between one
    and 200 DM...
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，如果支票账户余额小于零DM或介于1到200 DM之间...
- en: …and the credit history is perfect or very good, then classify as “likely to
    default”
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: …如果信用记录是完美或非常好的，则将其分类为“可能违约”
- en: The numbers in parentheses indicate the number of examples meeting the criteria
    for that decision and the number incorrectly classified by the decision. For instance,
    on the first line, `415/55` indicates that, of the 415 examples reaching the decision,
    55 were incorrectly classified as “not likely to default.” In other words, 55
    out of 415 applicants actually defaulted in spite of the model’s prediction to
    the contrary.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 括号中的数字表示满足该决策标准的示例数量和被该决策错误分类的示例数量。例如，在第一行中，`415/55`表示，在达到决策的415个示例中，有55个被错误地分类为“不太可能违约”。换句话说，415个申请人中有55人实际上违约了，尽管模型预测他们不会违约。
- en: Sometimes a tree results in decisions that make little logical sense. For example,
    why would an applicant whose credit history is perfect or very good be likely
    to default, while those whose checking balance is unknown are not likely to default?
    Contradictory rules like this occur sometimes. They might reflect a real pattern
    in the data, or they may be a statistical anomaly. In either case, it is important
    to investigate such strange decisions to see whether the tree’s logic makes sense
    for business use.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，一棵树会导致一些逻辑上不太合理的决策。例如，为什么信用记录完美或非常好的申请人可能会违约，而那些支票账户余额未知的人却不太可能违约？这样的矛盾规则有时会出现。它们可能反映了数据中的真实模式，或者可能是统计异常。在两种情况下，调查这些奇怪的决策以查看树的逻辑是否适合商业用途都是非常重要的。
- en: 'After the tree, the `summary(credit_model)` output displays a confusion matrix,
    which is a cross-tabulation that indicates the model’s incorrectly classified
    records in the training data:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在树之后，`summary(credit_model)`输出显示了一个混淆矩阵，它是一个交叉表，指示模型在训练数据中的错误分类记录：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `Errors` heading shows that the model correctly classified all but 118 of
    the 900 training instances for an error rate of 13.1 percent. A total of 31 actual
    `no` values were incorrectly classified as `yes` (false positives), while 87 `yes`
    values were misclassified as `no` (false negatives). Given the tendency of decision
    trees to overfit to the training data, the error rate reported here, which is
    based on training data performance, may be overly optimistic. Therefore, it is
    especially important to apply the decision tree to an unseen test dataset, which
    we will do shortly.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`Errors`标题显示，该模型正确分类了900个训练实例中的878个，错误率为13.1%。共有31个实际值为`no`的实例被错误地分类为`yes`（假阳性），而87个值为`yes`的实例被错误地分类为`no`（假阴性）。鉴于决策树倾向于过度拟合训练数据，这里报告的错误率，即基于训练数据性能的错误率，可能过于乐观。因此，将决策树应用于未见过的测试数据集尤为重要，我们将在稍后进行。'
- en: 'The output also includes a section labeled `Attribute usage`, which provides
    a general sense of the most important predictors used in the decision tree model.
    The first few lines of this output are as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 输出还包括一个标记为`Attribute usage`的部分，它提供了关于决策树模型中使用的重要预测因子的一般感觉。以下是一些输出内容的前几行：
- en: '[PRE31]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The **attribute usage** statistics in decision tree output refer to the percentage
    of rows in the training data that use the listed feature to make a final prediction.
    For example, 100 percent of rows require the `checking_balance` feature, because
    the checking account balance is used at the very first split in the tree. The
    second split uses `credit_history`, but 46.11 percent of rows were already classified
    as non-default based on the checking account balance. This leaves only 53.89 percent
    of rows that need to consider the applicant’s credit history. At the bottom of
    this list, only 12.11 percent of examples require the applicant’s age to make
    a prediction, which suggests that the applicant’s age is less important than their
    checking account balance or credit history.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树输出中的**属性使用**统计信息指的是在训练数据中使用列出的特征进行最终预测的行百分比。例如，100%的行需要`checking_balance`特征，因为检查账户余额在树的第一个分裂点被使用。第二个分裂使用`credit_history`，但46.11%的行已经根据检查账户余额被分类为非违约。这留下了只有53.89%的行需要考虑申请人的信用历史。在这个列表的底部，只有12.11%的例子需要申请人的年龄来做出预测，这表明申请人的年龄不如他们的检查账户余额或信用历史重要。
- en: This information, along with the tree structure itself, provides insight into
    how the model works. Both are readily understood, even without a statistics background.
    Of course, a model that cannot make accurate predictions is useless even if it
    is easy to understand, so we will now perform a more formal evaluation of its
    performance.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这条信息，连同树结构本身，提供了对模型工作方式的洞察。这两者都易于理解，即使没有统计学背景也是如此。当然，即使模型易于理解，如果它不能做出准确的预测，那么它也是无用的，因此我们现在将对其进行更正式的性能评估。
- en: C5.0 decision tree models can be visualized using the `plot()` function, which
    relies on functionality in the `partykit` package. Unfortunately, this is useful
    only for relatively small decision trees. For example, our decision tree can be
    visualized by typing `plot(credit_model)`, but unless you have a very large display,
    the resulting plot will likely appear as a jumbled mess due to the large number
    of nodes and splits in the tree.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0决策树模型可以使用`plot()`函数进行可视化，该函数依赖于`partykit`包中的功能。不幸的是，这仅适用于相对较小的决策树。例如，我们可以通过输入`plot(credit_model)`来可视化我们的决策树，但除非你有非常大的显示屏，否则由于树中节点和分裂的数量很大，生成的图表可能会显得杂乱无章。
- en: Step 4 – evaluating model performance
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 评估模型性能
- en: 'To apply our decision tree to the test dataset, we use the `predict()` function
    as shown in the following line of code:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要将我们的决策树应用于测试数据集，我们使用`predict()`函数，如下面的代码行所示：
- en: '[PRE32]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This creates a vector of predicted class values, which we can compare to the
    actual class values using the `CrossTable()` function in the `gmodels` package.
    Setting the `prop.c` and `prop.r` parameters to `FALSE` removes the column and
    row percentages from the table. The remaining percentage (`prop.t`) indicates
    the proportion of records in the cell out of the total number of records:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个预测类值的向量，我们可以使用`gmodels`包中的`CrossTable()`函数将其与实际类值进行比较。将`prop.c`和`prop.r`参数设置为`FALSE`将从表中删除列和行百分比。剩余的百分比（`prop.t`）表示单元格中记录数占总记录数的比例：
- en: '[PRE33]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This results in the following table:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下表格：
- en: '[PRE34]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Out of the 100 loan applications in the test set, our model correctly predicted
    that 56 did not default and 11 did default, resulting in an accuracy of 67 percent
    and an error rate of 33 percent. This is somewhat worse than its performance on
    the training data, but not unexpected, given that a model’s performance is often
    worse on unseen data. Also note that the model only correctly predicted 11 of
    the 35 actual loan defaults in the test data, or 31 percent. Unfortunately, this
    type of error is potentially a very costly mistake, as the bank loses money on
    each default. Let’s see if we can improve the result with a bit more effort.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集的100个贷款申请中，我们的模型正确预测了56个没有违约，11个违约，从而实现了67%的准确率和33%的错误率。这比它在训练数据上的表现略差，但考虑到模型在未见数据上的表现通常更差，这是可以预料的。此外，请注意，该模型在测试数据中只正确预测了35个实际贷款违约中的11个，即31%。不幸的是，这种类型的错误可能是一个代价非常高的错误，因为银行在每次违约中都会损失金钱。让我们看看我们是否可以通过更多的努力来提高结果。
- en: Step 5 – improving model performance
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步 – 提高模型性能
- en: Our model’s error rate is likely to be too high to deploy it in a real-time
    credit scoring application. In fact, if the model had predicted “no default” for
    every test case, it would have been correct 65 percent of the time—a result barely
    worse than our model but requiring much less effort! Predicting loan defaults
    using only 900 training examples seems to be a challenging problem.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的错误率可能太高，无法将其部署到实时信用评分应用中。事实上，如果模型对每个测试案例都预测“无违约”，那么它将有65%的时间是正确的——这个结果几乎与我们的模型一样，但需要付出多得多的努力！仅使用900个训练示例来预测贷款违约似乎是一个具有挑战性的问题。
- en: Making matters even worse, our model performed especially poorly at identifying
    applicants who do default on their loans. Luckily, there are a couple of simple
    ways to adjust the C5.0 algorithm that may help to improve the performance of
    the model, both overall and for the costlier type of mistakes.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，我们的模型在识别那些违约的申请人方面表现特别差。幸运的是，有几个简单的方法可以调整C5.0算法，这可能会帮助提高模型的整体性能，以及对于更昂贵的错误类型的性能。
- en: Boosting the accuracy of decision trees
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提升决策树的准确性
- en: One way the C5.0 algorithm improved upon the C4.5 algorithm was through the
    addition of **adaptive boosting**. This is a process in which many decision trees
    are built, and the trees vote on the best class for each example.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0算法在C4.5算法的基础上进行改进的一种方式是通过添加**自适应提升法**。这是一个构建许多决策树的过程，这些树对每个示例的最佳类别进行投票。
- en: 'The idea of boosting is based largely upon research by Rob Schapire and Yoav
    Freund. For more information, try searching the web for their publications or
    their textbook *Boosting: Foundations and Algorithms, Cambridge, MA, The MIT Press,
    2012*.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '提升法的想法主要基于Rob Schapire和Yoav Freund的研究。有关更多信息，请尝试在网络上搜索他们的出版物或他们的教科书 *Boosting:
    Foundations and Algorithms, Cambridge, MA, The MIT Press, 2012*。'
- en: As boosting can be applied more generally to any machine learning algorithm,
    it is covered in more detail later in this book in *Chapter 14*, *Building Better
    Learners*. For now, it suffices to say that boosting is rooted in the notion that
    by combining several weak-performing learners, you can create a team that is much
    stronger than any of the learners alone. Each of the models has a unique set of
    strengths and weaknesses, and may be better or worse at certain problems. Using
    a combination of several learners with complementary strengths and weaknesses
    can therefore dramatically improve the accuracy of a classifier.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提升法可以更普遍地应用于任何机器学习算法，它将在本书的 *第14章*，*构建更好的学习者* 中更详细地介绍。现在，只需说提升法基于这样的观点：通过结合几个表现不佳的学习者，可以创建一个比任何单个学习者都强大的团队。每个模型都有其独特的一组优势和劣势，可能在某些问题上表现更好或更差。因此，使用具有互补优势和劣势的几个学习者的组合可以显著提高分类器的准确性。
- en: The `C5.0()` function makes it easy to add boosting to our decision tree. We
    simply need to add an additional `trials` parameter indicating the number of separate
    decision trees to use in the boosted team. The `trials` parameter sets an upper
    limit; the algorithm will stop adding trees if it recognizes that additional trials
    do not seem to be improving the accuracy. We’ll start with 10 trials, a number
    that has become the de facto standard, as research suggests that this reduces
    error rates on test data by about 25 percent.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '`C5.0()` 函数使得将提升法添加到我们的决策树变得简单。我们只需添加一个额外的 `trials` 参数，表示在提升团队中使用的独立决策树的数量。`trials`
    参数设置了一个上限；如果算法认识到额外的试验似乎没有提高准确性，它将停止添加树。我们将从10次试验开始，这个数字已经成为事实上的标准，因为研究表明这可以减少测试数据上的错误率大约25%。'
- en: 'Aside from the new parameter, the command is similar to before:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 除了新的参数之外，命令与之前相似：
- en: '[PRE35]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'While examining the resulting model, we can see that the output now indicates
    the addition of boosting:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查生成的模型时，我们可以看到输出现在指示了提升法的添加：
- en: '[PRE36]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The new output shows that across the 10 iterations, our tree size shrunk. If
    you would like, you can see all 10 trees by typing `summary(credit_boost10)` at
    the command prompt. Note that some of these trees, including the tree built for
    the first trial, have one or more subtrees such as the one denoted `[S1]` in the
    following excerpt of the output:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 新的输出显示，在10次迭代中，我们的树大小缩小了。如果您愿意，可以在命令提示符中键入`summary(credit_boost10)`来查看所有10棵树。请注意，其中一些树，包括为第一次试验构建的树，包含一个或多个子树，如下面的输出摘录中所示，其中一个子树被标记为
    `[S1]`：
- en: '[PRE38]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Notice the line that says that if `age > 23`, the result is `[S1]`. To determine
    what this means, we must match `[S1]` to the corresponding subtree slightly further
    down in the output, where we see that the final decision requires several more
    steps:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到那条说如果 `age > 23`，结果将是 `[S1]` 的行。为了确定这意味着什么，我们必须将 `[S1]` 与输出中稍低位置的对应子树匹配，在那里我们看到最终的决策需要几个额外的步骤：
- en: '[PRE39]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Such subtrees are the result of post-pruning options like subtree raising and
    subtree replacement, as mentioned earlier in this chapter.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的子树是后剪枝选项（如子树提升和子树替换）的结果，如本章前面提到的。
- en: 'The tree’s `summary()` output also shows the tree’s performance on the training
    data:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 树的 `summary()` 输出还显示了该树在训练数据上的性能：
- en: '[PRE40]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The classifier made 19 mistakes on 900 training examples for an error rate
    of 2.1 percent. This is quite an improvement over the 13.1 percent training error
    rate we noted before boosting! However, it remains to be seen whether we see a
    similar improvement on the test data. Let’s take a look:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器在 900 个训练示例上犯了 19 个错误，错误率为 2.1%。这比我们之前提升前的 13.1% 训练错误率有了相当大的改进！然而，我们还需要看看测试数据上是否会有类似的改进。让我们看一下：
- en: '[PRE42]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The resulting table is as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表格如下：
- en: '[PRE43]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Here, we reduced the total error rate from 33 percent prior to boosting to 26
    percent in the boosted model. This may not seem like a large improvement, but
    it is not too far away from the 25 percent reduction we expected. That said, if
    boosting can be added this easily, why not apply it by default to every decision
    tree? The reason is twofold. First, if building a decision tree once takes a great
    deal of computation time, building many trees may be computationally impractical.
    Secondly, if the training data is very noisy, then boosting might not result in
    an improvement at all. Still, if greater accuracy is needed, it’s worth giving
    boosting a try.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将提升前的总错误率从 33% 降低到提升模型中的 26%。这可能看起来改进不大，但它离我们预期的 25% 减少并不太远。话虽如此，如果提升可以这么容易地添加，为什么不将其默认应用于每个决策树呢？原因有两个。首先，如果构建一个决策树需要大量的计算时间，构建多个树可能在计算上不切实际。其次，如果训练数据非常嘈杂，那么提升可能根本不会带来改进。尽管如此，如果需要更高的准确性，尝试提升是值得的。
- en: On the other hand, the model is still doing poorly at identifying the true defaults,
    predicting only 46 percent correctly (16 out of 35) compared to 31 percent (11
    of 35) in the simpler model. Let’s investigate one more option to see if we can
    reduce these types of costly errors.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，模型在识别真实违约方面仍然表现不佳，正确预测的只有 46%（35 个中的 16 个），而简单模型中的正确率是 31%（35 个中的 11 个）。让我们再调查一个选项，看看我们是否可以减少这些代价高昂的错误。
- en: Making some mistakes cost more than others
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一些错误比其他错误代价更高
- en: Giving a loan to an applicant who is likely to default can be an expensive mistake.
    One solution to reduce the number of false negatives may be to reject a larger
    number of borderline applicants under the assumption that the interest that the
    bank would earn from a risky loan is far outweighed by the massive loss it would
    incur if the money is not paid back at all.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 给一个可能违约的申请人贷款可能是一个代价高昂的错误。减少错误阴性的一个解决方案可能是拒绝更多的边缘申请人，假设银行从风险贷款中获得的利息远远低于如果钱全部不还所造成的巨大损失。
- en: The C5.0 algorithm allows us to assign a penalty to different types of errors
    in order to discourage a tree from making more costly mistakes. The penalties
    are designated in a **cost matrix**, which specifies how many times more costly
    each error is relative to any other.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: C5.0 算法允许我们为不同类型的错误分配惩罚，以阻止树犯更多代价高昂的错误。这些惩罚在 **成本矩阵** 中指定，该矩阵指定每个错误相对于任何其他错误的代价高多少倍。
- en: 'To begin constructing the cost matrix, we need to start by specifying the dimensions.
    Since the predicted and actual values can both take two values, `yes` or `no`,
    we need to describe a 2x2 matrix using a list of two vectors, each with two values.
    At the same time, we’ll also name the matrix dimensions to avoid confusion later:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始构建成本矩阵，我们需要首先指定维度。由于预测值和实际值都可以取两个值，`是` 或 `否`，我们需要使用两个值组成的两个向量的列表来描述一个 2x2
    的矩阵。同时，我们还将命名矩阵维度，以避免以后混淆：
- en: '[PRE44]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Examining the new object shows that our dimensions have been set up correctly:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 检查新对象显示我们的维度已经设置正确：
- en: '[PRE45]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we need to assign the penalty for the various types of errors by supplying
    four values to fill the matrix. Since R fills a matrix by filling columns one
    by one from top to bottom, we need to supply the values in a specific order:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要通过提供四个值来填充矩阵，以分配各种类型错误的惩罚。由于R是从上到下逐列填充矩阵，因此我们需要按照特定顺序提供这些值：
- en: Predicted `no`, actual `no`
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测为“否”，实际为“否”
- en: Predicted `yes`, actual `no`
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测为“是”，实际为“否”
- en: Predicted `no`, actual `yes`
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测为“否”，实际为“是”
- en: Predicted `yes`, actual `yes`
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测为“是”，实际为“是”
- en: 'Suppose we believe that a loan default costs the bank four times as much as
    a missed opportunity. Our penalty values then could be defined as:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们认为贷款违约的成本是银行错过机会的四倍。那么我们的惩罚值可以这样定义：
- en: '[PRE47]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This creates the following matrix:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这就产生了以下矩阵：
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As defined by this matrix, there is no cost assigned when the algorithm classifies
    a `no` or `yes` correctly, but a false negative has a cost of `4` versus a false
    positive’s cost of `1`. To see how this impacts classification, let’s apply it
    to our decision tree using the `costs` parameter of the `C5.0()` function. We’ll
    otherwise use the same steps as before:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如此矩阵定义，当算法正确地将“否”或“是”分类时，没有分配成本，但假阴性有4的成本，而假阳性有1的成本。为了了解这如何影响分类，让我们使用`C5.0()`函数的`costs`参数将其应用于我们的决策树。我们还将使用之前相同的步骤：
- en: '[PRE50]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This produces the following confusion matrix:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下混淆矩阵：
- en: '[PRE51]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Compared to our boosted model, this version makes more mistakes overall: 36
    percent error here versus 26 percent in the boosted case. However, the types of
    mistakes are very different. Where the previous models classified only 31 and
    46 percent of defaults correctly, in this model, *30 / 35 = 86%* of the actual
    defaults were correctly predicted to be defaults. This trade-off resulting in
    a reduction of false negatives at the expense of increasing false positives may
    be acceptable if our cost estimates were accurate.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的提升模型相比，这个版本总体上犯的错误更多：这里的错误率为36%，而在提升模型中为26%。然而，错误类型非常不同。在先前的模型中，只有31%和46%的违约被正确分类，而在本模型中，*30/35=86%*的实际违约被正确预测为违约。这种权衡在减少假阴性错误的同时增加了假阳性错误，如果我们的成本估计准确，这可能是可以接受的。
- en: Understanding classification rules
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分类规则
- en: Classification rules represent knowledge in the form of logical if-else statements
    that assign a class to unlabeled examples. They are specified in terms of an **antecedent**
    and a **consequent**, which form a statement stating that “if *this* happens,
    then *that* happens.” The antecedent comprises certain combinations of feature
    values, while the consequent specifies the class value to assign if the rule’s
    conditions are met. A simple rule might state, “if the computer is making a clicking
    sound, then it is about to fail.”
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 分类规则以逻辑if-else语句的形式表示知识，为未标记的示例分配一个类别。它们由**前件**和**后件**指定，形成一个陈述，即“如果*这个*发生，那么*那个*就会发生。”前件包括某些特征值的组合，而后件指定如果规则的条件得到满足，则分配的类别值。一个简单的规则可能声明，“如果计算机正在发出点击声，那么它即将失败。”
- en: 'Rule learners are closely related siblings of decision tree learners and are
    often used for similar types of tasks. Like decision trees, they can be used for
    applications that generate knowledge for future action, such as:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习者是决策树学习者的紧密相关的兄弟，通常用于类似类型的任务。像决策树一样，它们可以用于生成未来行动知识的应用，例如：
- en: Identifying conditions that lead to hardware failure in mechanical devices
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别导致机械设备硬件故障的条件
- en: Describing the key characteristics of groups of people for customer segmentation
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述人群群体的关键特征以进行客户细分
- en: Finding conditions that precede large drops or increases in the prices of shares
    on the stock market
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找导致股票市场价格大幅下跌或上涨的条件
- en: Rule learners do have some distinct contrasts relative to decision trees. Unlike
    a tree, which must be followed through a series of branching decisions, rules
    are propositions that can be read much like independent statements of fact. Additionally,
    for reasons that will be discussed later, the results of a rule learner can be
    more simple, direct, and easier to understand than a decision tree built on the
    same data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习者在与决策树相比有一些明显的差异。与必须通过一系列分支决策的树不同，规则是类似于独立事实陈述的命题。此外，由于以下将要讨论的原因，规则学习者的结果可能比基于相同数据的决策树更简单、更直接、更容易理解。
- en: You may have already realized that the branches of decision trees are almost
    identical to if-else statements of rule learning algorithms, and in fact, rules
    can be generated from trees. So, why bother with a separate group of rule learning
    algorithms? Read further to discover the nuances that differentiate the two approaches.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经意识到决策树的分支几乎与规则学习算法的if-else语句相同，实际上，规则可以从树中生成。那么，为什么还要有一个单独的规则学习算法组呢？继续阅读以发现区分两种方法的细微差别。
- en: Rule learners are generally applied to problems where the features are primarily
    or entirely nominal. They do well at identifying rare events, even if the rare
    event occurs only for a very specific interaction among feature values.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习器通常应用于特征主要是或完全是名义性的问题。它们在识别罕见事件方面做得很好，即使罕见事件只发生在特征值之间非常具体的交互中。
- en: Separate and conquer
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分而治之
- en: Rule learning classification algorithms utilize a heuristic known as **separate
    and conquer**. The process involves identifying a rule that covers a subset of
    examples in the training data and then separating this partition from the remaining
    data. As rules are added, additional subsets of data are separated until the entire
    dataset has been covered and no more examples remain. Although separate and conquer
    is in many ways similar to the divide and conquer heuristic covered earlier, it
    differs in subtle ways that will become clear soon.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习分类算法利用一种称为**分而治之**的启发式方法。这个过程包括识别一个覆盖训练数据中子集的规则，然后从这个分区中分离出剩余的数据。随着规则的添加，更多的数据子集被分离，直到整个数据集都被覆盖，没有更多的例子剩下。尽管分而治之在很多方面与之前提到的分而治之启发式方法相似，但它以微妙的方式有所不同，很快就会变得清楚。
- en: 'One way to imagine the rule learning process of separate and conquer is to
    imagine drilling down into the data by creating increasingly specific rules to
    identify class values. Suppose you were tasked with creating rules to identify
    whether or not an animal is a mammal. You could depict the set of all animals
    as a large space, as shown in the following diagram:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 想象分而治之的规则学习过程的一种方式是通过创建越来越具体的规则来深入数据。假设你被要求创建规则来识别动物是否是哺乳动物。你可以将所有动物集合表示为一个大的空间，如下面的图表所示：
- en: '![Venn diagram  Description automatically generated with medium confidence](img/B17290_05_08.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![维恩图  自动生成的描述，中等置信度](img/B17290_05_08.png)'
- en: 'Figure 5.8: A rule learning algorithm may help divide animals into groups of
    mammals and non-mammals'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8：一个规则学习算法可能有助于将动物分为哺乳动物和非哺乳动物组
- en: 'A rule learner begins by using the available features to find homogeneous groups.
    For example, using a feature that indicates whether the species travels via land,
    sea, or air, the first rule might suggest that any land-based animals are mammals:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习器首先使用可用的特征来寻找同质群体。例如，使用一个表示物种是否通过陆地、海洋或空中旅行的特征，第一条规则可能建议任何陆生动物都是哺乳动物：
- en: '![Diagram, venn diagram  Description automatically generated](img/B17290_05_09.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图表，维恩图  自动生成的描述](img/B17290_05_09.png)'
- en: 'Figure 5.9: A potential rule considers animals that travel on land to be mammals'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9：一个潜在的规则将陆地上旅行的动物视为哺乳动物
- en: 'Do you notice any problems with this rule? If you’re an animal lover, you might
    have realized that frogs are amphibians, not mammals. Therefore, our rule needs
    to be a bit more specific. Let’s drill down further by suggesting that mammals
    must walk on land and have a tail:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 你注意到这个规则有任何问题吗？如果你是一个动物爱好者，你可能已经意识到青蛙是两栖动物，而不是哺乳动物。因此，我们的规则需要更加具体。让我们进一步深入，建议哺乳动物必须在陆地上行走并且有尾巴：
- en: '![Diagram, venn diagram  Description automatically generated](img/B17290_05_10.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图表，维恩图  自动生成的描述](img/B17290_05_10.png)'
- en: 'Figure 5.10: A more specific rule suggests that animals that walk on land and
    have tails are mammals'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10：一个更具体的规则表明，在陆地上行走并且有尾巴的动物是哺乳动物
- en: As shown in the previous figure, the new rule results in a subset of animals
    that are entirely mammals. Thus, the subset of mammals can be separated from the
    other data and the frogs are returned to the pool of remaining animals—no pun
    intended!
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，新规则导致了一组完全由哺乳动物组成的动物子集。因此，哺乳动物的子集可以与其他数据分开，青蛙被返回到剩余动物池中——无意中开玩笑！
- en: 'An additional rule can be defined to separate out the bats, the only remaining
    mammal. A potential feature distinguishing bats from the remaining animals would
    be the presence of fur. Using a rule built around this feature, we have then correctly
    identified all the animals:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 可以定义一个额外的规则来区分蝙蝠，这是唯一剩下的哺乳动物。一个可能区分蝙蝠和其他动物的特征是毛皮的存在。使用围绕这个特征构建的规则，我们正确地识别了所有动物：
- en: '![Diagram  Description automatically generated](img/B17290_05_11.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图示  描述自动生成](img/B17290_05_11.png)'
- en: 'Figure 5.11: A rule stating that animals with fur are mammals perfectly classifies
    the remaining animals'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11：一条规则表明有毛皮的动物完美地分类了剩余的动物
- en: 'At this point, since all training instances have been classified, the rule
    learning process would stop. We learned a total of three rules:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，由于所有训练实例都已分类，规则学习过程将停止。我们总共学习了三条规则：
- en: Animals that walk on land and have tails are mammals
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在陆地上行走并带有尾巴的动物是哺乳动物
- en: If the animal does not have fur, it is not a mammal
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物没有毛皮，它就不是哺乳动物
- en: Otherwise, the animal is a mammal
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，该动物是哺乳动物
- en: The previous example illustrates how rules gradually consume larger and larger
    segments of data to eventually classify all instances. As the rules seem to cover
    portions of the data, separate and conquer algorithms are also known as **covering
    algorithms**, and the resulting rules are called covering rules. In the next section,
    we will learn how covering rules are applied in practice by examining a simple
    rule learning algorithm. We will then examine a more complex rule learner and
    apply both algorithms to a real-world problem.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的例子说明了规则如何逐渐消耗越来越大的数据段，最终对所有实例进行分类。由于规则似乎覆盖了数据的一部分，因此分离和征服算法也被称为**覆盖算法**，而由此产生的规则被称为覆盖规则。在下一节中，我们将通过检查一个简单的规则学习算法来了解覆盖规则在实际中的应用。然后我们将检查一个更复杂的规则学习器，并将这两个算法应用于一个现实世界的问题。
- en: The 1R algorithm
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1R算法
- en: Suppose a television game show has an animal hidden behind a large curtain.
    You are asked to guess whether it is a mammal and if correct, you win a large
    cash prize. You are not given any clues about the animal’s characteristics, but
    you know that a very small portion of the world’s animals are mammals. Consequently,
    you guess “non-mammal.” What do you think about your chances of winning?
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个电视游戏节目有一个动物隐藏在大型帘子后面。你被要求猜测它是否是哺乳动物，如果猜对了，你将赢得一大笔现金奖金。你没有得到任何关于动物特征的线索，但你知道世界上非常少的动物是哺乳动物。因此，你猜测“非哺乳动物”。你认为你赢得的机会有多大？
- en: Choosing this, of course, maximizes your odds of winning the prize, as it is
    the most likely outcome under the assumption the animal was chosen at random.
    Clearly, this game show is a bit ridiculous, but it demonstrates the simplest
    classifier, **ZeroR**, which is a rule learner that considers no features and
    literally learns no rules (hence the name). For every unlabeled example, regardless
    of the values of its features, it predicts the most common class. This algorithm
    has very little real-world utility, except that it provides a simple baseline
    for comparison to other, more sophisticated, rule learners.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，选择这个选项最大化了你赢得奖金的机会，因为在假设动物是随机选择的情况下，这是最可能的结果。显然，这个游戏节目有点荒谬，但它展示了最简单的分类器，**ZeroR**，这是一个不考虑任何特征的规则学习器，实际上它没有学习任何规则（因此得名）。对于每个未标记的例子，无论其特征值如何，它都预测最常见的类别。这个算法在现实世界中的实用性非常有限，除了它为比较其他更复杂的规则学习器提供了一个简单的基线。
- en: The **1R algorithm** (also known as **One Rule** or **OneR**), improves over
    ZeroR by selecting a single rule. Although this may seem overly simplistic, it
    tends to perform better than you might expect. As demonstrated in empirical studies,
    the accuracy of this algorithm can approach that of much more sophisticated algorithms
    for many real-world tasks.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '**1R算法**（也称为**One Rule**或**OneR**），通过选择一条规则来改进ZeroR。尽管这可能看起来过于简单，但它往往比你预期的表现要好。正如实证研究所证明的，这个算法的准确性可以接近许多现实世界任务中更复杂算法的准确性。'
- en: For an in-depth look at the surprising performance of 1R, see *Very Simple Classification
    Rules Perform Well on Most Commonly Used Datasets, Holte, RC, Machine Learning,
    1993, Vol. 11, pp. 63-91*.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 想要深入了解1R令人惊讶的性能，请参阅*非常简单的分类规则在大多数常用数据集上表现良好，Holte, RC, 机器学习，1993，第11卷，第63-91页*。
- en: 'The strengths and weaknesses of the 1R algorithm are shown in the following
    table:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 1R算法的优缺点如下表所示：
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Generates a single, easy-to- understand, human-readable rule
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个单一、易于理解、人类可读的规则
- en: Often performs surprisingly well
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常表现惊人地好
- en: Can serve as a benchmark for more complex algorithms
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以作为更复杂算法的基准
- en: '|'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Uses only a single feature
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只使用单个特征
- en: Probably overly simplistic
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能过于简单化
- en: '|'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The way this algorithm works is simple. For each feature, 1R divides the data
    into groups with similar values of the feature. Then, for each segment, the algorithm
    predicts the majority class. The error rate for the rule based on each feature
    is calculated and the rule with the fewest errors is chosen as the one rule.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法的工作方式很简单。对于每个特征，1R将数据划分为具有相似特征值的组。然后，对于每个段，算法预测多数类别。基于每个特征的规则错误率被计算，选择错误最少的规则作为单一规则。
- en: 'The following tables show how this would work for the animal data we looked
    at earlier:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了这对于我们之前查看的动物数据是如何工作的：
- en: '![Table  Description automatically generated](img/B17290_05_12.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B17290_05_12.png)'
- en: 'Figure 5.12: The 1R algorithm chooses the single rule with the lowest misclassification
    rate'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12：1R算法选择具有最低误分类率的单一规则
- en: 'For the *Travels By* feature, the dataset was divided into three groups: *Air*,
    *Land*, and *Sea*. Animals in the *Air* and *Sea* groups were predicted to be
    non-mammal, while animals in the *Land* group were predicted to be mammals. This
    resulted in two errors: bats and frogs.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*通过*特征，数据集被划分为三个组：*空中*、*陆地*和*海洋*。*空中*和*海洋*组的动物被预测为非哺乳动物，而*陆地*组的动物被预测为哺乳动物。这导致了两个错误：蝙蝠和青蛙。
- en: 'The *Has Fur* feature divided animals into two groups. Those with fur were
    predicted to be mammals, while those without fur were not. Three errors were counted:
    pigs, elephants, and rhinos. As the *Travels By* feature resulted in fewer errors,
    the 1R algorithm would return the following:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '*有毛皮*特征将动物分为两组。有毛皮的动物被预测为哺乳动物，而无毛皮的动物则不是。共有三个错误：猪、大象和犀牛。由于*通过*特征导致的错误较少，1R算法会返回以下结果：'
- en: If the animal travels by air, it is not a mammal
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物通过空中旅行，它就不是哺乳动物
- en: If the animal travels by land, it is a mammal
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物通过陆地旅行，它就是哺乳动物
- en: If the animal travels by sea, it is not a mammal
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果动物通过海洋旅行，它就不是哺乳动物
- en: The algorithm stops here, having found the single most important rule.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 算法在这里停止，因为它找到了最重要的单一规则。
- en: Obviously, this rule learning algorithm may be too basic for some tasks. Would
    you want a medical diagnosis system to consider only a single symptom, or an automated
    driving system to stop or accelerate your car based on only a single factor? For
    these types of tasks, a more sophisticated rule learner might be useful. We’ll
    learn about one in the following section.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种规则学习算法可能对于某些任务来说过于基础。您希望医疗诊断系统只考虑单一症状，或者自动驾驶系统只基于单一因素来停止或加速您的汽车吗？对于这些类型的任务，一个更复杂的规则学习者可能更有用。我们将在下一节中了解一个。
- en: The RIPPER algorithm
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RIPPER算法
- en: Early rule learning algorithms were plagued by a couple of problems. First,
    they were notorious for being slow, which made them ineffective for the increasing
    number of large datasets. Second, they were often prone to being inaccurate on
    noisy data.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的规则学习算法存在几个问题。首先，它们以速度慢而闻名，这使得它们对于日益增多的大型数据集无效。其次，它们通常在噪声数据上容易不准确。
- en: A first step toward solving these problems was proposed in 1994 by Johannes
    Furnkranz and Gerhard Widmer. Their **incremental reduced error pruning** (**IREP**)
    **algorithm** uses a combination of pre-pruning and post-pruning methods that
    grow very complex rules and prune them before separating the instances from the
    full dataset. Although this strategy helped the performance of rule learners,
    decision trees often still performed better.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 1994年，Johannes Furnkranz和Gerhard Widmer提出了解决这些问题的第一步。他们的**增量减少错误剪枝**（**IREP**）**算法**结合了预剪枝和后剪枝方法，这些方法可以生成非常复杂的规则，并在从完整数据集中分离实例之前对其进行剪枝。尽管这种策略有助于规则学习者的性能，但决策树通常仍然表现更好。
- en: For more information on IREP, see *Incremental Reduced Error Pruning, Furnkranz,
    J and Widmer, G, Proceedings of the 11th International Conference on Machine Learning,
    1994, pp. 70-77*.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 关于IREP的更多信息，请参阅 *Incremental Reduced Error Pruning, Furnkranz, J and Widmer,
    G, Proceedings of the 11th International Conference on Machine Learning, 1994,
    pp. 70-77*.
- en: Rule learners took another step forward in 1995 when William W. Cohen introduced
    the **repeated incremental pruning to produce error reduction** (**RIPPER**) **algorithm**,
    which improved upon IREP to generate rules that match or exceed the performance
    of decision trees.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 1995年，威廉·W·科恩引入了**重复增量剪枝以产生误差减少**（**RIPPER**）算法，该算法在IREP的基础上进行了改进，以生成与决策树性能匹配或超过性能的规则，这使得规则学习者在1995年又迈出了新的一步。
- en: For more detail on RIPPER, see *Fast Effective Rule Induction, Cohen, WW, Proceedings
    of the 12th International Conference on Machine Learning, 1995, pp. 115-123*.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 关于RIPPER的更多详细信息，请参阅*Cohen, WW，第12届国际机器学习会议论文集，1995年，第115-123页*《快速有效的规则归纳》。
- en: As outlined in the following table, the strengths and weaknesses of RIPPER are
    generally comparable to decision trees. The chief benefit is that they may result
    in a slightly more parsimonious model.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如下表概述，RIPPER的优点和缺点通常与决策树相当。主要好处是它们可能导致一个略微更节俭的模型。
- en: '| **Strengths** | **Weaknesses** |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| **优点** | **缺点** |'
- en: '|'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Generates easy-to-understand, human-readable rules
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成易于理解、人类可读的规则
- en: Efficient on large and noisy datasets
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大型和噪声数据集上效率高
- en: Generally, produces a simpler model than a comparable decision tree
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，产生的模型比可比较的决策树更简单
- en: '|'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: May result in rules that seem to defy common sense or expert knowledge
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会导致看似违反常识或专家知识的规则
- en: Not ideal for working with numeric data
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不适合处理数值数据
- en: Might not perform as well as more complex models
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能不如更复杂的模型表现得好
- en: '|'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Having evolved from several iterations of rule learning algorithms, the RIPPER
    algorithm is a patchwork of efficient heuristics for rule learning. Due to its
    complexity, a discussion of the implementation details is beyond the scope of
    this book. However, it can be understood in general terms as a three-step process:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: RIPPER算法是从几个规则学习算法的迭代中演变而来的，它是一系列用于规则学习的有效启发式方法的拼凑。由于其复杂性，对实现细节的讨论超出了本书的范围。然而，可以将其大致理解为三个步骤的过程：
- en: Grow
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生长
- en: Prune
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝
- en: Optimize
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 优化
- en: The growing phase uses the separate and conquer technique to greedily add conditions
    to a rule until it perfectly classifies a subset of data or runs out of attributes
    for splitting. Like decision trees, the information gain criterion is used to
    identify the next splitting attribute. When increasing a rule’s specificity no
    longer reduces entropy, the rule is immediately pruned. Steps one and two are
    repeated until reaching a stopping criterion, at which point the entire set of
    rules is optimized using a variety of heuristics.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 生长阶段使用分离和征服技术贪婪地向规则添加条件，直到它完美地分类数据子集或耗尽用于分割的属性。像决策树一样，使用信息增益标准来识别下一个分割属性。当增加规则的具体性不再减少熵时，规则立即被剪枝。第一步和第二步重复进行，直到达到停止标准，此时使用各种启发式方法对整个规则集进行优化。
- en: The RIPPER algorithm can create much more complex rules than the 1R algorithm
    can, as it can consider more than one feature. This means that it can create rules
    with multiple antecedents such as “if an animal flies and has fur, then it is
    a mammal.” This improves the algorithm’s ability to model complex data, but just
    like decision trees, it means that the rules can quickly become difficult to comprehend.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 与1R算法相比，RIPPER算法可以创建更复杂的规则，因为它可以考虑多个特征。这意味着它可以创建具有多个前件的规则，例如“如果一个动物会飞并且有毛皮，那么它是一种哺乳动物。”这提高了算法建模复杂数据的能力，但就像决策树一样，这也意味着规则可能很快变得难以理解。
- en: The evolution of classification rule learners didn’t stop with RIPPER. New rule
    learning algorithms are being proposed rapidly. A survey of literature shows algorithms
    called IREP++, SLIPPER, TRIPPER, among many others.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 分类规则学习者的进化并没有随着RIPPER而停止。新的规则学习算法正在迅速提出。文献综述显示，有IREP++、SLIPPER、TRIPPER等许多其他算法。
- en: Rules from decision trees
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树中的规则
- en: 'Classification rules can also be obtained directly from decision trees. Beginning
    at a leaf node and following the branches back to the root, you obtain a series
    of decisions. These can be combined into a single rule. The following figure shows
    how rules could be constructed from the decision tree for predicting movie success:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 分类规则也可以直接从决策树中获得。从叶节点开始，沿着分支回溯到根节点，你可以得到一系列决策。这些决策可以组合成一条规则。以下图显示了如何从预测电影成功的决策树构建规则：
- en: '![Diagram, shape, polygon  Description automatically generated](img/B17290_05_13.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图表，形状，多边形  自动生成的描述](img/B17290_05_13.png)'
- en: 'Figure 5.13: Rules can be generated from decision trees by following paths
    from the root node to each leaf node'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13：可以通过从根节点到每个叶节点的路径生成规则
- en: 'Following the paths from the root node down to each leaf, the rules would be:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着从根节点到每个叶节点的路径，规则如下：
- en: If the number of celebrities is low, then the movie will be a *Box Office Bust*
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果名人数量少，那么这部电影将是一个 *票房炸弹*
- en: If the number of celebrities is high and the budget is high, then the movie
    will be a *Mainstream Hit*
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果名人数量多且预算高，那么这部电影将是一个 *主流热门*
- en: If the number of celebrities is high and the budget is low, then the movie will
    be a *Critical Success*
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果名人数量多且预算低，那么这部电影将是一个 *成功的关键*
- en: For reasons that will be made clear in the following section, the chief downside
    to using a decision tree to generate rules is that the resulting rules are often
    more complex than those learned by a rule learning algorithm. The divide and conquer
    strategy employed by decision trees biases the results differently to that of
    a rule learner. On the other hand, it is sometimes more computationally efficient
    to generate rules from trees.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中将要解释的原因是，使用决策树生成规则的缺点是生成的规则通常比规则学习算法学习的规则更复杂。决策树使用的划分和征服策略与规则学习器的结果偏差不同。另一方面，有时从树中生成规则在计算上更有效。
- en: The `C5.0()` function in the `C50` package will generate a model using classification
    rules if you specify `rules = TRUE` when training the model.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `C50` 包中，使用 `C5.0()` 函数生成模型时，如果您在训练模型时指定 `rules = TRUE`，则会使用分类规则。
- en: What makes trees and rules greedy?
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么使得树和规则是贪婪的？
- en: Decision trees and rule learners are known as **greedy learners** because they
    use data on a first-come, first-served basis. Both the divide and conquer heuristic
    used by decision trees and the separate and conquer heuristic used by rule learners
    attempt to make partitions one at a time, finding the most homogeneous partition
    first, followed by the next best, and so on, until all examples have been classified.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树和规则学习器被称为 **贪婪学习器**，因为它们基于先到先得的原则使用数据。决策树使用的划分和征服启发式方法以及规则学习器使用的单独和征服启发式方法都试图一次划分一个部分，首先找到最同质的划分，然后是下一个最好的，以此类推，直到所有示例都被分类。
- en: The downside to the greedy approach is that greedy algorithms are not guaranteed
    to generate the optimal, most accurate, or smallest number of rules for a particular
    dataset. By taking the low-hanging fruit early, a greedy learner may quickly find
    a single rule that is accurate for one subset of data; however, in doing so, the
    learner may miss the opportunity to develop a more nuanced set of rules with better
    overall accuracy on the entire set of data. However, without using the greedy
    approach to rule learning, it is likely that for all but the smallest of datasets,
    rule learning would be computationally infeasible.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪方法的缺点是贪婪算法不能保证为特定数据集生成最优、最准确或最少数量的规则。通过早期获取低垂的果实，贪婪学习器可能会迅速找到一个适用于数据子集的准确规则；然而，在这样做的同时，学习器可能会错过开发一个更细致的规则集的机会，该规则集在整个数据集上具有更好的整体准确性。然而，如果不使用贪婪方法进行规则学习，对于除了最小的数据集之外的所有数据集，规则学习可能从计算上不可行。
- en: '![Diagram  Description automatically generated](img/B17290_05_14.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_05_14.png)'
- en: 'Figure 5.14: Both decision trees and classification rule learners are greedy
    algorithms'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：决策树和分类规则学习器都是贪婪算法
- en: Though both trees and rules employ greedy learning heuristics, there are subtle
    differences in how they build rules. Perhaps the best way to distinguish them
    is to note that once divide and conquer splits on a feature, the partitions created
    by the split may not be re-conquered, only further subdivided. In this way, a
    tree is permanently limited by its history of past decisions. In contrast, once
    separate and conquer finds a rule, any examples not covered by all the rule’s
    conditions may be re-conquered.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然树和规则都采用贪婪学习启发式方法，但它们构建规则的方式存在细微差别。也许最好的区分方式是注意，一旦在特征上进行了划分和征服的分割，分割创建的分区可能不会被重新征服，而只会进一步细分。这样，树就永久性地受限于其过去的决策历史。相比之下，一旦单独和征服找到一个规则，任何未覆盖到该规则所有条件的示例可能会被重新征服。
- en: 'To illustrate this contrast, consider the previous case in which we built a
    rule learner to determine whether an animal was a mammal. The rule learner identified
    three rules that perfectly classify the example animals:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种对比，考虑我们之前构建规则学习器来确定动物是否是哺乳动物的情况。规则学习器确定了三条完美分类示例动物的规则：
- en: Animals that walk on land and have tails are mammals (bears, cats, dogs, elephants,
    pigs, rabbits, rats, and rhinos)
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在陆地上行走并且有尾巴的动物是哺乳动物（熊、猫、狗、大象、猪、兔子、老鼠和犀牛）
- en: If the animal does not have fur, it is not a mammal (birds, eels, fish, frogs,
    insects, and sharks)
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果动物没有皮毛，它就不是哺乳动物（鸟类、鳗鱼、鱼类、青蛙、昆虫和鲨鱼）
- en: Otherwise, the animal is a mammal (bats)
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则，动物是哺乳动物（蝙蝠）
- en: 'In contrast, a decision tree built on the same data might have come up with
    four rules to achieve the same perfect classification:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于相同数据的决策树可能提出了四条规则来实现相同的完美分类：
- en: If an animal walks on land and has a tail, then it is a mammal (bears, cats,
    dogs, elephants, pigs, rabbits, rats, and rhinos)
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果动物在陆地上行走并且有尾巴，那么它是哺乳动物（熊、猫、狗、大象、猪、兔子、老鼠和犀牛）
- en: If an animal walks on land and does not have a tail, then it is not a mammal
    (frogs)
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果动物在陆地上行走但没有尾巴，那么它不是哺乳动物（青蛙）
- en: If the animal does not walk on land and has fur, then it is a mammal (bats)
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果动物不在陆地上行走并且有皮毛，那么它就是哺乳动物（蝙蝠）
- en: If the animal does not walk on land and does not have fur, then it is not a
    mammal (birds, insects, sharks, fish, and eels)
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果动物不在陆地上行走并且没有皮毛，那么它不是哺乳动物（鸟类、昆虫、鲨鱼、鱼类和鳗鱼）
- en: The different result across these two approaches has to do with what happens
    to the frogs after they are separated by the “walk on land” decision. Where the
    rule learner allows frogs to be re-conquered by the “do not have fur” decision,
    the decision tree cannot modify the existing partitions and therefore must place
    the frog into its own rule.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的不同结果与青蛙在“在陆地上行走”决策后发生的情况有关。规则学习器允许青蛙被“没有皮毛”的决策重新征服，而决策树不能修改现有的分区，因此必须将青蛙放入它自己的规则中。
- en: '![Diagram  Description automatically generated](img/B17290_05_15.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![图描述自动生成](img/B17290_05_15.png)'
- en: 'Figure 5.15: The handling of frogs distinguishes the divide and conquer and
    separate and conquer heuristics. The latter approach allows the frogs to be re-conquered
    by later rules.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15：青蛙的处理区分了分而治之与分离和征服启发式方法。后者允许青蛙被后来的规则重新征服。
- en: On the one hand, because rule learners can reexamine cases that were considered
    but ultimately not covered as part of prior rules, rule learners often find a
    more parsimonious set of rules than those generated by decision trees. On the
    other hand, this reuse of data means that the computational cost of rule learners
    may be somewhat higher than for decision trees.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，因为规则学习器可以重新审视那些被视为但最终未作为先前规则一部分的案件，规则学习器通常比决策树生成的规则更简洁。另一方面，这种数据重用意味着规则学习器的计算成本可能比决策树高一些。
- en: Example – identifying poisonous mushrooms with rule learners
  id: totrans-349
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - 使用规则学习器识别有毒蘑菇
- en: Each year, many people fall ill and some even die from ingesting poisonous wild
    mushrooms. Since many mushrooms are very similar to each other in appearance,
    occasionally, even experienced mushroom gatherers are poisoned.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 每年，许多人因食用有毒野生蘑菇而生病，甚至有些人因此死亡。由于许多蘑菇在外观上非常相似，有时即使是经验丰富的蘑菇采集者也会中毒。
- en: Unlike the identification of harmful plants, such as poison oak or poison ivy,
    there are no clear rules like “leaves of three, let them be” for identifying whether
    a wild mushroom is poisonous or edible.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 与识别有毒植物，如毒橡树或毒常春藤不同，没有像“三叶草，让它去吧”这样的明确规则来识别野生蘑菇是有毒的还是可食用的。
- en: Complicating matters, many traditional rules such as “poisonous mushrooms are
    brightly colored” provide dangerous or misleading information. If simple, clear,
    and consistent rules were available for identifying poisonous mushrooms, they
    could save the lives of foragers.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 事情变得更加复杂，许多传统规则，例如“有毒蘑菇颜色鲜艳”，提供了危险或误导性的信息。如果能够有简单、明确和一致的规则来识别有毒蘑菇，它们就能拯救采集者的生命。
- en: As one of the strengths of rule learning algorithms is the fact that they generate
    easy-to-understand rules, they seem like an appropriate fit for this classification
    task. However, the rules will only be as useful as they are accurate.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 规则学习算法的一个优点是它们生成的规则易于理解，这使得它们似乎非常适合这个分类任务。然而，这些规则的有效性将取决于它们的准确性。
- en: Step 1 – collecting data
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To identify rules for distinguishing poisonous mushrooms, we will use the Mushroom
    dataset by Jeff Schlimmer of Carnegie Mellon University. The raw dataset is available
    freely from the UCI Machine Learning Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset includes information on 8,124 mushroom samples from 23 species
    of gilled mushrooms listed in the *Audubon Society Field Guide to North American
    Mushrooms* (1981). In the field guide, each of the mushroom species is identified
    as “definitely edible,” “definitely poisonous,” or “likely poisonous, and not
    recommended to be eaten.” For the purposes of this dataset, the latter group was
    combined with the “definitely poisonous” group to make two classes: poisonous
    and non-poisonous. The data dictionary available on the UCI website describes
    the 22 features of the mushroom samples, including characteristics such as cap
    shape, cap color, odor, gill size and color, stalk shape, and habitat.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: This chapter uses a slightly modified version of the mushroom data. If you plan
    on following along with the example, download the `mushrooms.csv` file from the
    Packt Publishing GitHub repository for this chapter and save it to your R working
    directory.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – exploring and preparing the data
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by using `read.csv()` to import the data for our analysis. Since all
    22 features and the target class are nominal, we will set `stringsAsFactors =
    TRUE` to take advantage of the automatic factor conversion:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The output of the `str(mushrooms)` command notes that the data contains 8,124
    observations of 23 variables as the data dictionary had described. While most
    of the `str()` output is unremarkable, one feature is worth mentioning. Do you
    notice anything peculiar about the `veil_type` variable in the following line?
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'If you found it to be strange that a factor has only one level, you are correct.
    The data dictionary lists two levels for this feature: `partial` and `universal`;
    however, all examples in our data are classified as `partial`. It is likely that
    this data element was somehow coded incorrectly. In any case, since the veil type
    does not vary across samples, it does not provide any useful information for prediction.
    We will drop this variable from our analysis using the following command:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: By assigning `NULL` to the `veil_type` vector, R eliminates the feature from
    the mushrooms data frame.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'Before going much further, we should take a quick look at the distribution
    of the mushroom `type` variable in our dataset:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: About 52 percent of the mushroom samples are edible, while 48 percent are poisonous.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this experiment, we will consider the 8,214 samples in the
    mushroom data to be an exhaustive set of all the possible wild mushrooms. This
    is an important assumption because it means that we do not need to hold some samples
    out of the training data for testing purposes. We are not trying to develop rules
    that cover unforeseen types of mushrooms; we are merely trying to find rules that
    accurately depict the complete set of known mushroom types. Therefore, we can
    build and test the model on the same data.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – training a model on the data
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If we trained a hypothetical ZeroR classifier on this data, what would it predict?
    Since ZeroR ignores all of the features and simply predicts the target’s mode,
    in plain language, its rule would state that “all mushrooms are edible.” Obviously,
    this is not a very helpful classifier because it would leave a mushroom gatherer
    sick or dead for nearly half of the mushroom samples! Our rules will need to do
    much better than this benchmark in order to provide safe advice that can be published.
    At the same time, we need simple rules that are easy to remember.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: Since simple rules can still be useful, let’s see how a very simple rule learner
    performs on the mushroom data. Toward this end, we will apply the 1R classifier,
    which will identify the single feature that is the most predictive of the target
    class and use this feature to construct a rule.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: We will use the 1R implementation found in the `OneR` package by Holger von
    Jouanne-Diedrich at the Aschaffenburg University of Applied Sciences. This is
    a relatively new package, which implements 1R in native R code for speed and ease
    of use. If you don’t already have this package, it can be installed using the
    `install.packages("OneR")` command and loaded by typing `library(OneR)`.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17290_05_16.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: 1R classification rule syntax'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Like C5.0, the `OneR()` function uses the R formula syntax to specify the model
    to be trained. Using the formula `type ~ .` with `OneR()` allows our first rule
    learner to consider all possible features in the mushroom data when predicting
    the mushroom type:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'To examine the rules it created, we can type the name of the classifier object:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Examining the output, we see that the `odor` feature was selected for rule
    generation. The categories of `odor`, such as `almond`, `anise`, and so on, specify
    rules for whether the mushroom is likely to be `edible` or `poisonous`. For instance,
    if the mushroom smells `fishy`, `foul`, `musty`, `pungent`, `spicy`, or like `creosote`,
    the mushroom is likely to be poisonous. On the other hand, mushrooms with more
    pleasant smells, like `almond` and `anise`, and those with no smell at all, are
    predicted to be `edible`. For the purposes of a field guide for mushroom gathering,
    these rules could be summarized in a simple rule of thumb: “if the mushroom smells
    unappetizing, then it is likely to be poisonous.”'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – evaluating model performance
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The last line of the output notes that the rules correctly predict the edibility
    for 8,004 of the 8,124 mushroom samples, or nearly 99 percent. Anything short
    of perfection, however, runs the risk of poisoning someone if the model were to
    classify a poisonous mushroom as edible.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: 'To determine whether this occurred, let’s examine a confusion matrix of the
    predicted versus actual values. This requires us to first generate the 1R model’s
    predictions, then compare the predictions to the actual values:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Here, we can see where our rules went wrong. The table’s columns indicate the
    predicted edibility of the mushroom while the table’s rows divide the 4,208 actually
    edible mushrooms and the 3,916 actually poisonous mushrooms. Examining the table,
    we can see that although the 1R classifier did not classify any edible mushrooms
    as poisonous, it did classify 120 poisonous mushrooms as edible, which makes for
    an incredibly dangerous mistake!
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Considering that the learner utilized only a single feature, it did reasonably
    well; if you avoid unappetizing smells when foraging for mushrooms, you will almost
    always avoid a trip to the hospital. That said, close does not cut it when lives
    are involved, not to mention the field guide publisher might not be happy about
    the prospect of a lawsuit when its readers fall ill. Let’s see if we can add a
    few more rules and develop an even better classifier.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – improving model performance
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a more sophisticated rule learner, we will use `JRip()`, a Java-based implementation
    of the RIPPER algorithm. The `JRip()` function is included in the `RWeka` package,
    which gives R access to the machine learning algorithms in the Java-based Weka
    software application by Ian H. Witten and Eibe Frank.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Weka is a popular open source and full-featured graphical application for performing
    data mining and machine learning tasks—one of the earliest such tools. For more
    information on Weka, see [http://www.cs.waikato.ac.nz/~ml/weka/](http://www.cs.waikato.ac.nz/~ml/weka/).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: The `RWeka` package depends on the `rJava` package, which itself requires the
    **Java development kit** (**JDK**) to be installed on the host computer before
    installation. This can be downloaded from `https://www.java.com/` and installed
    using the instructions specific to your platform. After installing Java, use the
    `install.packages("RWeka")` command to install `RWeka` and its dependencies, then
    load the `RWeka` package using the `library(RWeka)` command.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Java is a set of programming tools available at no cost, which allows the development
    and use of cross-platform applications such as Weka. Although it was once included
    by default with many computers, this is no longer true. Unfortunately, it can
    be tricky to install, especially on Apple computers. If you are having trouble,
    be sure you have the latest Java version. Additionally, on Microsoft Windows,
    you may need to set your environment variables like `JAVA_HOME` correctly, and
    check your `PATH` settings (search the web for details). On macOS or Linux computers,
    you may also try executing `R CMD javareconf` from a terminal window then install
    the R package `rJava` from source using the R command `install.packages("rJava",
    type = "source")`. If all else fails, you may try a free Posit Cloud account ([https://posit.cloud/](https://posit.cloud/)),
    which offers an RStudio environment in which Java is already installed.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: 'With `rJava` and `RWeka` installed, the process of training a `JRip()` model
    is very similar to training a `OneR()` model, as shown in the syntax box that
    follows. This is one of the pleasant benefits of the R formula interface: the
    syntax is consistent across algorithms, which makes it simple to compare a variety
    of models.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B17290_05_17.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.17: RIPPER classification rule syntax'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s train the `JRip()` rule learner as we did with `OneR()`, allowing it
    to find rules among all of the available features:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'To examine the rules, type the name of the classifier:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The `JRip()` classifier learned a total of eight rules from the mushroom data.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: 'An easy way to read these rules is to think of them as a list of if-else statements,
    similar to programming logic. The first three rules could be expressed as:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: If the odor is foul, then the mushroom type is poisonous
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the gill size is narrow and the gill color is buff, then the mushroom type
    is poisonous
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the gill size is narrow and the odor is pungent, then the mushroom type is
    poisonous
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, the eighth rule implies that any mushroom sample that was not covered
    by the preceding seven rules is edible. Following the example of our programming
    logic, this can be read as:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Else, the mushroom is edible
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The numbers next to each rule indicate the number of instances covered by the
    rule and a count of misclassified instances. Notably, there were no misclassified
    mushroom samples using these eight rules. As a result, the number of instances
    covered by the last rule is exactly equal to the number of edible mushrooms in
    the data (N = 4,208).
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: The following figure provides a rough illustration of how the rules are applied
    to the mushroom data. If you imagine the large oval as containing all mushroom
    species, the rule learner identifies features, or sets of features, that separate
    homogeneous segments from the larger group. First, the algorithm found a large
    group of poisonous mushrooms uniquely distinguished by their foul odor. Next,
    it found smaller and more specific groups of poisonous mushrooms. By identifying
    covering rules for each of the varieties of poisonous mushrooms, all remaining
    mushrooms were edible.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to Mother Nature, each variety of mushrooms was unique enough that the
    classifier was able to achieve 100 percent accuracy.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17290_05_18.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.18: A sophisticated rule learning algorithm identified rules to perfectly
    cover all types of poisonous mushrooms'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-415
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered two classification methods that use so-called “greedy”
    algorithms to partition the data according to feature values. Decision trees use
    a divide and conquer strategy to create flowchart-like structures, while rule
    learners separate and conquer data to identify logical if-else rules. Both methods
    produce models that can be interpreted without a statistical background.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: One popular and highly configurable decision tree algorithm is C5.0\. We used
    the C5.0 algorithm to create a tree to predict whether a loan applicant will default.
    Using options for boosting and cost-sensitive errors, we were able to improve
    our accuracy and avoid risky loans that could cost the bank more money.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: We also used two rule learners, 1R and RIPPER, to develop rules for identifying
    poisonous mushrooms. The 1R algorithm used a single feature to achieve 99 percent
    accuracy in identifying potentially fatal mushroom samples. On the other hand,
    the set of eight rules generated by the more sophisticated RIPPER algorithm correctly
    identified the edibility of every mushroom.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: This merely scratches the surface of how trees and rules can be used. The next
    chapter, *Chapter 6*, *Forecasting Numeric Data – Regression Methods*, describes
    techniques known as regression trees and model trees, which use decision trees
    for numeric prediction rather than classification. In *Chapter 8*, *Finding Patterns
    – Market Basket Analysis Using Association Rules*, we will see how association
    rules—a close relative of classification rules—can be used to identify groups
    of items in transactional data. Lastly, in *Chapter 14*, *Building Better Learners*,
    we will discover how the performance of decision trees can be improved by grouping
    them together in a model known as a random forest, in addition to other advanced
    modeling techniques that rely on decision trees.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-420
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 4000 people at:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/r](https://packt.link/r)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/r.jpg)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
