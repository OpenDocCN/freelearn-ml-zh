<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Sentiment Analyser Application for Movie Reviews">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch08"/>&#13;
 Chapter 8. Sentiment Analyser Application for Movie Reviews</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>In this chapter, we describe an application to determine the sentiment of movie reviews using algorithms and methods described throughout the book. In addition, the <a id="id578" class="indexterm"/>&#13;
 <span class="strong">&#13;
<strong>Scrapy</strong>&#13;
</span>&#13;
 library will be used to collect reviews from different websites through a search engine API (Bing search engine). The text and the title of the movie review is extracted using the newspaper library or following some pre-defined extraction rules of an HTML format page. The sentiment of each review is determined using a naive Bayes classifier on the most informative words (using the X<span class="emphasis">&#13;
<em>2</em>&#13;
</span>&#13;
 measure) in the same way as in <a class="link" title="Chapter 4. Web Mining Techniques" href="text00032.html#ch04">Chapter 4</a>&#13;
 , <span class="emphasis">&#13;
<em>Web Mining Techniques</em>&#13;
</span>&#13;
 . Also, the rank of each page related to each movie query is calculated for <a id="id579" class="indexterm"/>&#13;
 completeness using the PageRank algorithm discussed in <a class="link" title="Chapter 4. Web Mining Techniques" href="text00032.html#ch04">Chapter 4</a>&#13;
 , <span class="emphasis">&#13;
<em>Web Mining Techniques</em>&#13;
</span>&#13;
 . This chapter will discuss the code used to build the application, including the Django models and views and the Scrapy scraper is used to collect data from the web pages of the movie reviews. We start by giving an example of what the web application will be and explaining the search engine API used and how we include it in the application. We then describe how we collect the movie reviews, integrating the Scrapy library into Django, the models to store the data, and the main commands to manage the application. All the code discussed in this chapter is available in the GitHub repository of the author inside the <code class="literal">chapter_8</code>&#13;
 folder at <a class="ulink" href="https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8">https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_8</a>&#13;
 .</p>&#13;
<div class="section" title="Application usage overview">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch08lvl1sec52"/>&#13;
 Application usage overview</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The<a id="id580" class="indexterm"/>&#13;
 home web page is as follows:</p>&#13;
<div class="mediaobject"><img src="Image00541.jpg" alt="Application usage overview"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The user<a id="id581" class="indexterm"/>&#13;
 can type in the movie name, if they want to know the review's sentiments and relevance. For example, we look for <span class="emphasis">&#13;
<em>Batman vs Superman Dawn of Justice</em>&#13;
</span>&#13;
 in the following screenshot:</p>&#13;
<div class="mediaobject"><img src="Image00542.jpg" alt="Application usage overview"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>The application <a id="id582" class="indexterm"/>&#13;
 collects and scrapes 18 reviews from the Bing search engine and, using the Scrapy library, it analyzes their sentiment (15 positive and 3 negative). All data is stored in Django models, ready to be used to calculate the relevance of each page using the PageRank algorithm (the links at the bottom of the page as seen in the preceding screenshot). In this case, using the PageRank algorithm, we have the following:</p>&#13;
<div class="mediaobject"><img src="Image00543.jpg" alt="Application usage overview"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>This is a list of the most relevant pages to our movie review search, setting a depth parameter 2 on the scraping crawler (refer the following section for further details). Note that to have a good result on page relevance, you have to crawl thousands of pages (the preceding screenshot shows results for around 50 crawled pages).</p>&#13;
<p>To write the application, we start the server as usual (see <a class="link" title="Chapter 6. Getting Started with Django" href="text00046.html#ch06">Chapter 6</a>&#13;
 , <span class="emphasis">&#13;
<em>Getting Started with Django</em>&#13;
</span>&#13;
 , and <a class="link" title="Chapter 7. Movie Recommendation System Web Application" href="text00050.html#page">Chapter 7</a>&#13;
 , <span class="emphasis">&#13;
<em>Movie Recommendation System Web Application</em>&#13;
</span>&#13;
 ) and the main app in Django. First, we create a folder to store all our codes, <code class="literal">movie_reviews_analyzer_app</code>&#13;
 , and then we initialize Django using the following command:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>mkdir  movie_reviews_analyzer_app</strong>&#13;

</span>&#13;


<span class="strong">
<strong>cd  movie_reviews_analyzer_app</strong>&#13;

</span>&#13;


<span class="strong">
<strong>django-admin startproject webmining_server</strong>&#13;

</span>&#13;


<span class="strong">
<strong>python manage.py startapp startapp pages</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>We set the settings in the <code class="literal">.py</code>&#13;
 file as we did in the <span class="emphasis">&#13;
<em>Settings</em>&#13;
</span>&#13;
 section of <a class="link" title="Chapter 6. Getting Started with Django" href="text00046.html#ch06">Chapter 6</a>&#13;
 , <span class="emphasis">&#13;
<em>Getting Started with Django</em>&#13;
</span>&#13;
 , and the <span class="emphasis">&#13;
<em>Application Setup</em>&#13;
</span>&#13;
 section of <a class="link" title="Chapter 7. Movie Recommendation System Web Application" href="text00050.html#page">Chapter 7</a>&#13;
 , <span class="emphasis">&#13;
<em>Movie Recommendation System Web Application</em>&#13;
</span>&#13;
 (of course, in this case the name is <code class="literal">webmining_server</code>&#13;
 instead of <code class="literal">server_movierecsys</code>&#13;
 ).</p>&#13;
<p>The sentiment<a id="id583" class="indexterm"/>&#13;
 analyzer application has the main views in the <code class="literal">.py</code>&#13;
 file in the main <code class="literal">webmining_server</code>&#13;
 folder instead of the <code class="literal">app</code>&#13;
 (pages) folder as we did previously (see <a class="link" title="Chapter 6. Getting Started with Django" href="text00046.html#ch06">Chapter 6</a>&#13;
 , <span class="emphasis">&#13;
<em>Getting Started with Django</em>&#13;
</span>&#13;
 , and <a class="link" title="Chapter 7. Movie Recommendation System Web Application" href="text00050.html#page">Chapter 7</a>&#13;
 , <span class="emphasis">&#13;
<em>Movie Recommendation System Web Application</em>&#13;
</span>&#13;
 ), because the functions now refer more to the general functioning of the server instead of the specific app (pages).</p>&#13;
<p>The last operation to make the web service operational is to create a <code class="literal">superuser</code>&#13;
 account and go live with the server:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>python manage.py createsuperuser (admin/admin)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>python manage.py runserver</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Now that the structure of the application has been explained, we can discuss the different parts in more detail starting from the search engine API used to collect URLs.</p>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Search engine choice and the application code">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch08lvl1sec53"/>&#13;
 Search engine choice and the application code</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Since scraping<a id="id584" class="indexterm"/>&#13;
 directly from the most relevant search engines such as Google, Bing, Yahoo, and others is against their term of service, we need to take initial<a id="id585" class="indexterm"/>&#13;
 review pages from their REST API (using scraping services such as Crawlera, <a class="ulink" href="http://www.crawlera.com/">http://crawlera.com/</a>&#13;
 , is also possible). We decided to use the Bing service, which allows 5,000 queries per month for free.</p>&#13;
<p>In order to do that, we register to the Microsoft Service to obtain the key needed to allow the search. Briefly, we followed these steps:</p>&#13;
<div class="orderedlist">&#13;
<ol class="orderedlist arabic">&#13;
<li class="listitem" value="1">Register online on <a class="ulink" href="https://datamarket.azure.com">https://datamarket.azure.com</a>&#13;
 .</li>&#13;
<li class="listitem" value="2">In <span class="strong">&#13;
<strong>My Account</strong>&#13;
</span>&#13;
 , take the <span class="strong">&#13;
<strong>Primary Account Key</strong>&#13;
</span>&#13;
 .</li>&#13;
<li class="listitem" value="3">Register a new application (under <span class="strong">&#13;
<strong>DEVELOPERS</strong>&#13;
</span>&#13;
 | <span class="strong">&#13;
<strong>REGISTER</strong>&#13;
</span>&#13;
 ; put <span class="strong">&#13;
<strong>Redire</strong>&#13;
</span>&#13;
 <span class="strong">&#13;
<strong>ct URI</strong>&#13;
</span>&#13;
 : <code class="literal">https://www.</code>&#13;
 <code class="literal">bing.com</code>&#13;
 )</li>&#13;
</ol>&#13;
<div style="height:10px; width: 1px"/>&#13;
</div>&#13;
<p>After that, we can write a function that retrieves as many URLs relevant to our query as we want:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">num_reviews = 30 
def bing_api(query):
    keyBing = API_KEY        # get Bing key from: https://datamarket.azure.com/account/keys
    credentialBing = 'Basic ' + (':%s' % keyBing).encode('base64')[:-1] # the "-1" is to remove the trailing "\n" which encode adds
    searchString = '%27X'+query.replace(" ",'+')+'movie+review%27'
    top = 50#maximum allowed by Bing
    
    reviews_urls = []
    if num_reviews&lt;top:
        offset = 0
        url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \
              'Query=%s&amp;$top=%d&amp;$skip=%d&amp;$format=json' % (searchString, num_reviews, offset)

        request = urllib2.Request(url)
        request.add_header('Authorization', credentialBing)
        requestOpener = urllib2.build_opener()
        response = requestOpener.open(request)
        results = json.load(response)
        reviews_urls = [ d['Url'] for d in results['d']['results']]
    else:
        nqueries = int(float(num_reviews)/top)+1
        for i in xrange(nqueries):
            offset = top*i
            if i==nqueries-1:
                top = num_reviews-offset
                url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \
                      'Query=%s&amp;$top=%d&amp;$skip=%d&amp;$format=json' % (searchString, top, offset)

                request = urllib2.Request(url)
                request.add_header('Authorization', credentialBing)
                requestOpener = urllib2.build_opener()
                response = requestOpener.open(request) 
            else:
                top=50
                url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \
                      'Query=%s&amp;$top=%d&amp;$skip=%d&amp;$format=json' % (searchString, top, offset)

                request = urllib2.Request(url)
                request.add_header('Authorization', credentialBing)
                requestOpener = urllib2.build_opener()
                response = requestOpener.open(request) 
            results = json.load(response)
            reviews_urls += [ d['Url'] for d in results['d']['results']]
    return reviews_urls</pre>&#13;
</div>&#13;
<p>The <code class="literal">API_KEY</code>&#13;
 parameter is taken from the Microsoft account, <code class="literal">query</code>&#13;
 is a string which specifies the<a id="id586" class="indexterm"/>&#13;
 movie name, and <code class="literal">num_reviews = 30</code>&#13;
 is the number of URLs returned in total from the Bing API. With the list of URLs that contain the reviews, we can now set up a scraper to extract from each web page the title and the review text using Scrapy.</p>&#13;
</div>&#13;

<div class="section" title="Scrapy setup and the application code">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch08lvl1sec54"/>&#13;
 Scrapy setup and the application code</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Scrapy is a<a id="id587" class="indexterm"/>&#13;
 Python library is used to extract content from web pages or to crawl pages linked to a given web page (see the <span class="emphasis">&#13;
<em>Web crawlers (or spiders)</em>&#13;
</span>&#13;
 section of <a class="link" title="Chapter 4. Web Mining Techniques" href="text00032.html#ch04">Chapter 4</a>&#13;
 , <span class="emphasis">&#13;
<em>Web Mining Techniques</em>&#13;
</span>&#13;
 , for more details). To install the library, type the following in the terminal:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>sudo pip install Scrapy </strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Install the executable in the <code class="literal">bin</code>&#13;
 folder:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>sudo easy_install scrapy</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>From the <code class="literal">movie_reviews_analyzer_app</code>&#13;
 folder, we initialize our Scrapy project as follows:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>scrapy startproject scrapy_spider</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>This command will create the following tree inside the <code class="literal">scrapy_spider</code>&#13;
 folder:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>├── __init__.py</strong>&#13;

</span>&#13;


<span class="strong">
<strong>├── items.py</strong>&#13;

</span>&#13;


<span class="strong">
<strong>├── pipelines.py</strong>&#13;

</span>&#13;


<span class="strong">
<strong>├── settings.py ├── spiders</strong>&#13;

</span>&#13;


<span class="strong">
<strong>├── spiders</strong>&#13;

</span>&#13;


<span class="strong">
<strong>│   ├── __init__.py</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The <code class="literal">pipelines.py</code>&#13;
 and <code class="literal">items.py</code>&#13;
 files manage how the scraped data is stored and manipulated, and they will be discussed later in the <span class="emphasis">&#13;
<em>Spiders</em>&#13;
</span>&#13;
 and <span class="emphasis">&#13;
<em>Integrate Django with Scrapy</em>&#13;
</span>&#13;
 sections. The <code class="literal">settings.py</code>&#13;
 file sets the parameters each spider (or crawler) defined in the <code class="literal">spiders</code>&#13;
 folder uses to operate. In the following two sections, we describe the main parameters and spiders used in this application.</p>&#13;
<div class="section" title="Scrapy settings">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch08lvl2sec46"/>&#13;
 Scrapy settings</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The <code class="literal">settings.py</code>&#13;
 file<a id="id588" class="indexterm"/>&#13;
 collects all the parameters used by each spider in the Scrapy project to scrape web pages. The main parameters are as follows:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<code class="literal">DEPTH_LIMIT</code>&#13;
 : The number of subsequent pages crawled following an initial URL. The default is <code class="literal">0</code>&#13;
 and it means that no limit is set.</li>&#13;
<li class="listitem">&#13;
<code class="literal">LOG_ENABLED</code>&#13;
 : To allow/deny Scrapy to log on the terminal while executing default is true.</li>&#13;
<li class="listitem">&#13;
<code class="literal">ITEM_PIPELINES = {'scrapy_spider.pipelines.ReviewPipeline': 1000,}</code>&#13;
 : The path of the pipeline function to manipulate data extracted from each web page.</li>&#13;
<li class="listitem">&#13;
<code class="literal">CONCURRENT_ITEMS = 200</code>&#13;
 : The number of concurrent items processed in the pipeline.</li>&#13;
<li class="listitem">&#13;
<code class="literal">CONCURRENT_REQUESTS = 5000</code>&#13;
 : The maximum number of simultaneous requests handled by Scrapy.</li>&#13;
<li class="listitem">&#13;
<code class="literal">CONCURRENT_REQUESTS_PER_DOMAIN = 3000</code>&#13;
 : The maximum number of simultaneous requests handled by Scrapy for each specified domain.</li>&#13;
</ul>&#13;
</div>&#13;
<p>The larger the depth, more the pages are scraped and, consequently, the time needed to scrape increases. To speed up the process, you can set high value on the last three parameters. In this application (the <code class="literal">spiders</code>&#13;
 folder), we set two spiders: a scraper to extract data from each movie review URL (<code class="literal">movie_link_results.py</code>&#13;
 ) and a crawler to generate a graph of webpages linked to the initial movie review URL (<code class="literal">recursive_link_results.py</code>&#13;
 ).</p>&#13;
</div>&#13;
<div class="section" title="Scraper">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch08lvl2sec47"/>&#13;
 Scraper</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The <a id="id589" class="indexterm"/>&#13;
 scraper on <code class="literal">movie_link_results.py</code>&#13;
 looks as follows:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from newspaper import Article</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from urlparse import urlparse</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy.selector import Selector</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy import Spider</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy.spiders import BaseSpider,CrawlSpider, Rule</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy.http import Request</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy_spider import settings</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy_spider.items import PageItem,SearchItem</strong>&#13;

</span>&#13;



<span class="strong">
<strong>unwanted_domains = ['youtube.com','www.youtube.com']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from nltk.corpus import stopwords</strong>&#13;

</span>&#13;


<span class="strong">
<strong>stopwords = set(stopwords.words('english'))</strong>&#13;

</span>&#13;



<span class="strong">
<strong>def CheckQueryinReview(keywords,title,content):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    content_list = map(lambda x:x.lower(),content.split(' '))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    title_list = map(lambda x:x.lower(),title.split(' '))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    words = content_list+title_list</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    for k in keywords:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if k in words:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            return True</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    return False</strong>&#13;

</span>&#13;



<span class="strong">
<strong>class Search(Spider):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    name = 'scrapy_spider_reviews'</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def __init__(self,url_list,search_key):#specified by -a</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.search_key = search_key</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.keywords = [w.lower() for w in search_key.split(" ") if w not in stopwords]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.start_urls =url_list.split(',')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        super(Search, self).__init__(url_list)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def start_requests(self):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for url in self.start_urls:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            yield Request(url=url, callback=self.parse_site,dont_filter=True)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def parse_site(self, response):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        ## Get the selector for xpath parsing or from newspaper</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        def crop_emptyel(arr):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            return [u for u in arr if u!=' ']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        domain = urlparse(response.url).hostname</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        a = Article(response.url)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        a.download()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        a.parse()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        title = a.title.encode('ascii','ignore').replace('\n','')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        sel = Selector(response)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if title==None:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            title = sel.xpath('//title/text()').extract()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            if len(title)&gt;0:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                title = title[0].encode('utf-8').strip().lower()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        content = a.text.encode('ascii','ignore').replace('\n','')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if content == None:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            content = 'none'</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            if len(crop_emptyel(sel.xpath('//div//article//p/text()').extract()))&gt;1:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                contents = crop_emptyel(sel.xpath('//div//article//p/text()').extract())</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                print 'divarticle'</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            ….</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            elif len(crop_emptyel(sel.xpath('/html/head/meta[@name="description"]/@content').extract()))&gt;0:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                contents = crop_emptyel(sel.xpath('/html/head/meta[@name="description"]/@content').extract())</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            content = ' '.join([c.encode('utf-8') for c in contents]).strip().lower()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #get search item </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        search_item = SearchItem.django_model.objects.get(term=self.search_key)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #save item</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if not PageItem.django_model.objects.filter(url=response.url).exists():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            if len(content) &gt; 0:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                if CheckQueryinReview(self.keywords,title,content):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                    if domain not in unwanted_domains:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        newpage = PageItem()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        newpage['searchterm'] = search_item</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        newpage['title'] = title</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        newpage['content'] = content</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        newpage['url'] = response.url</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        newpage['depth'] = 0</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        newpage['review'] = True</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        #newpage.save()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                        return newpage  </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        else:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            return null</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>We can see<a id="id590" class="indexterm"/>&#13;
 that the <code class="literal">Spider</code>&#13;
 class from <code class="literal">scrapy</code>&#13;
 is inherited by the <code class="literal">Search</code>&#13;
 class and the following standard methods have to be defined to override the standard methods:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<code class="literal">__init__</code>&#13;
 : The constructor of the spider needs to define the <code class="literal">start_urls</code>&#13;
 list that contains the URL to extract content from. In addition, we have custom variables such as <code class="literal">search_key</code>&#13;
 and <code class="literal">keywords</code>&#13;
 that store the information related to the query of the movie's title used on the search engine API.</li>&#13;
<li class="listitem">&#13;
<code class="literal">start_requests</code>&#13;
 : This function is triggered when <code class="literal">spider</code>&#13;
 is called and it declares what to do for each URL in the <code class="literal">start_urls</code>&#13;
 list; for each URL, the custom <code class="literal">parse_site</code>&#13;
 function will be called (instead of the default <code class="literal">parse</code>&#13;
 function).</li>&#13;
<li class="listitem">&#13;
<code class="literal">parse_site</code>&#13;
 : It is a custom function to parse data from each URL. To extract the title of the review and its text content, we used the newspaper library (<code class="literal">sudo pip install newspaper</code>&#13;
 ) or, if it fails, we parse the HTML file directly using some defined rules to avoid the noise due to undesired tags (each rule structure is defined with the <code class="literal">sel.xpath</code>&#13;
 command). To achieve this result, we select some popular domains (<code class="literal">rottentomatoes</code>&#13;
 , <code class="literal">cnn</code>&#13;
 , and so on) and ensure the parsing is able to extract the content from these websites (not all the extraction rules are displayed in the preceding code but they can be found as usual in the GitHub file). The data is then stored in a page <code class="literal">Django</code>&#13;
 model using the related Scrapy item and the <code class="literal">ReviewPipeline</code>&#13;
 function (see the following section).</li>&#13;
<li class="listitem">&#13;
<code class="literal">CheckQueryinReview</code>&#13;
 : This is a custom function to check whether the movie title (from the query) is contained in the content or title of each web page.</li>&#13;
</ul>&#13;
</div>&#13;
<p>To run the<a id="id591" class="indexterm"/>&#13;
 spider, we need to type in the following command from the <code class="literal">scrapy_spider</code>&#13;
 (internal) folder:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>scrapy crawl scrapy_spider_reviews -a url_list=listname -a search_key=keyname</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
</div>&#13;
<div class="section" title="Pipelines">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch08lvl2sec48"/>&#13;
 Pipelines</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The pipelines<a id="id592" class="indexterm"/>&#13;
 define what to do when a new page is scraped by the spider. In the preceding case, the <code class="literal">parse_site</code>&#13;
 function returns a <code class="literal">PageItem</code>&#13;
 object, which triggers the following pipeline (<code class="literal">pipelines.py</code>&#13;
 ):</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>class ReviewPipeline(object):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def process_item(self, item, spider):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #if spider.name == 'scrapy_spider_reviews':#not working</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           item.save()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           return item</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>This class simply saves each item (a new page in the spider notation).</p>&#13;
</div>&#13;
<div class="section" title="Crawler">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch08lvl2sec49"/>&#13;
 Crawler</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>As we<a id="id593" class="indexterm"/>&#13;
 showed in the overview (the preceding section), the relevance of the review is calculated using the PageRank algorithm after we have stored all the linked pages starting from the review's URL. The crawler <code class="literal">recursive_link_results.py</code>&#13;
 performs this operation:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>#from scrapy.spider import Spider</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy.selector import Selector</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy.contrib.spiders import CrawlSpider, Rule</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy.linkextractors import LinkExtractor</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from scrapy.http import Request</strong>&#13;

</span>&#13;



<span class="strong">
<strong>from scrapy_spider.items import PageItem,LinkItem,SearchItem</strong>&#13;

</span>&#13;



<span class="strong">
<strong>class Search(CrawlSpider):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    name = 'scrapy_spider_recursive'</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def __init__(self,url_list,search_id):#specified by -a</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #REMARK is allowed_domains is not set then ALL are allowed!!!</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.start_urls = url_list.split(',')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.search_id = int(search_id)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #allow any link but the ones with different font size(repetitions)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        self.rules = (</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            Rule(LinkExtractor(allow=(),deny=('fontSize=*','infoid=*','SortBy=*', ),unique=True), callback='parse_item', follow=True), </strong>&#13;

</span>&#13;


<span class="strong">
<strong>            )</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        super(Search, self).__init__(url_list)</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    def parse_item(self, response):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        sel = Selector(response)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        ## Get meta info from website</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        title = sel.xpath('//title/text()').extract()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if len(title)&gt;0:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            title = title[0].encode('utf-8')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        contents = sel.xpath('/html/head/meta[@name="description"]/@content').extract()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        content = ' '.join([c.encode('utf-8') for c in contents]).strip()</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        fromurl = response.request.headers['Referer']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        tourl = response.url</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        depth = response.request.meta['depth']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #get search item </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        search_item = SearchItem.django_model.objects.get(id=self.search_id)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #newpage</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if not PageItem.django_model.objects.filter(url=tourl).exists():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newpage = PageItem()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newpage['searchterm'] = search_item</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newpage['title'] = title</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newpage['content'] = content</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newpage['url'] = tourl</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newpage['depth'] = depth</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newpage.save()#cant use pipeline cause the execution can finish here</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #get from_id,to_id</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        from_page = PageItem.django_model.objects.get(url=fromurl)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        from_id = from_page.id</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        to_page = PageItem.django_model.objects.get(url=tourl)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        to_id = to_page.id</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #newlink</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if not LinkItem.django_model.objects.filter(from_id=from_id).filter(to_id=to_id).exists():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newlink = LinkItem()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newlink['searchterm'] = search_item</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newlink['from_id'] = from_id</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newlink['to_id'] = to_id</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            newlink.save()</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The <code class="literal">CrawlSpider</code>&#13;
 class<a id="id594" class="indexterm"/>&#13;
 from <code class="literal">scrapy</code>&#13;
 is inherited by the <code class="literal">Search</code>&#13;
 class, and the following standard methods have to be defined to override the standard methods (as for the spider case):</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<code class="literal">__init__</code>&#13;
 : The is a constructor of the class. The <code class="literal">start_urls</code>&#13;
 parameter defines the starting URL from which the spider will start to crawl until the <code class="literal">DEPTH_LIMIT</code>&#13;
 value is reached. The <code class="literal">rules</code>&#13;
 parameter sets the type of URL allowed/denied to scrape (in this case, the same page but with different font sizes is disregarded) and it defines the function to call to manipulate each retrieved page (<code class="literal">parse_item</code>&#13;
 ). Also, a custom variable <code class="literal">search_id</code>&#13;
 is defined, which is needed to store the ID of the query within the other data.</li>&#13;
<li class="listitem">&#13;
<code class="literal">parse_item</code>&#13;
 : This is a custom function called to store the important data from each retrieved page. A new Django item of the <code class="literal">Page</code>&#13;
 model (see the following section) from each page is created, which contains the title and content of the page (using the <code class="literal">xpath</code>&#13;
 HTML parser). To perform the PageRank algorithm, the connection from the page that links to each page and the page itself is saved as an object of the <code class="literal">Link</code>&#13;
 model using the related Scrapy item (see the following sections).</li>&#13;
</ul>&#13;
</div>&#13;
<p>To run the <a id="id595" class="indexterm"/>&#13;
 crawler, we need to type the following from the (internal) <code class="literal">scrapy_spider</code>&#13;
 folder:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>scrapy crawl scrapy_spider_recursive -a url_list=listname -a search_id=keyname</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Django models">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch08lvl1sec55"/>&#13;
 Django models</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The data<a id="id596" class="indexterm"/>&#13;
 collected using the spiders needs to be stored in a database. In <a id="id597" class="indexterm"/>&#13;
 Django, the database tables are called models and defined in the <code class="literal">models.py</code>&#13;
 file (within the <code class="literal">pages</code>&#13;
 folder). The content of this file is as follows:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from django.db import models</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from django.conf import settings</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from django.utils.translation import ugettext_lazy as _</strong>&#13;

</span>&#13;



<span class="strong">
<strong>class SearchTerm(models.Model):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    term = models.CharField(_('search'), max_length=255)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    num_reviews = models.IntegerField(null=True,default=0)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    #display term on admin panel</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def __unicode__(self):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            return self.term</strong>&#13;

</span>&#13;



<span class="strong">
<strong>class Page(models.Model):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     searchterm = models.ForeignKey(SearchTerm, related_name='pages',null=True,blank=True)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     url = models.URLField(_('url'), default='', blank=True)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     title = models.CharField(_('name'), max_length=255)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     depth = models.IntegerField(null=True,default=-1)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     html = models.TextField(_('html'),blank=True, default='')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     review = models.BooleanField(default=False)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     old_rank = models.FloatField(null=True,default=0)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     new_rank = models.FloatField(null=True,default=1)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     content = models.TextField(_('content'),blank=True, default='')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     sentiment = models.IntegerField(null=True,default=100)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     </strong>&#13;

</span>&#13;


<span class="strong">
<strong>class Link(models.Model):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     searchterm = models.ForeignKey(SearchTerm, related_name='links',null=True,blank=True)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     from_id = models.IntegerField(null=True)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>     to_id = models.IntegerField(null=True)</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Each movie<a id="id598" class="indexterm"/>&#13;
 title typed on the home page of the application is stored in the <code class="literal">SearchTerm</code>&#13;
 model, while the data of each web page is collected in an object of the <code class="literal">Page</code>&#13;
 model. Apart from the content field (HTML, title, URL, content), the sentiment of the<a id="id599" class="indexterm"/>&#13;
 review and the depth in graph network are recorded (a Boolean also indicates if the web page is a movie review page or simply a linked page). The <code class="literal">Link</code>&#13;
 model stores all the graph links between pages, which are then used by the PageRank algorithm to calculate the relevance of the reviews web pages. Note that the <code class="literal">Page</code>&#13;
 model and the <code class="literal">Link</code>&#13;
 model are both linked to the related <code class="literal">SearchTerm</code>&#13;
 through a foreign key. As usual, to write these models as database tables, we type the following commands:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>python manage.py makemigrations</strong>&#13;

</span>&#13;


<span class="strong">
<strong>python manage.py migrate</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>To populate these Django models, we need to make Scrapy interact with Django, and this is the subject of the following section.</p>&#13;
</div>&#13;

<div class="section" title="Integrating Django with Scrapy">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch08lvl1sec56"/>&#13;
 Integrating Django with Scrapy</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>To make<a id="id600" class="indexterm"/>&#13;
 paths easy to call, we remove the external <code class="literal">scrapy_spider</code>&#13;
 folder so that inside the <code class="literal">movie_reviews_analyzer_app</code>&#13;
 , the <code class="literal">webmining_server</code>&#13;
 folder<a id="id601" class="indexterm"/>&#13;
 is at the same level as the <code class="literal">scrapy_spider</code>&#13;
 folder:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>├── db.sqlite3</strong>&#13;

</span>&#13;


<span class="strong">
<strong>├── scrapy.cfg</strong>&#13;

</span>&#13;


<span class="strong">
<strong>├── scrapy_spider</strong>&#13;

</span>&#13;


<span class="strong">
<strong>│   ├── ...</strong>&#13;

</span>&#13;


<span class="strong">
<strong>│   ├── spiders</strong>&#13;

</span>&#13;


<span class="strong">
<strong>│   │   ...</strong>&#13;

</span>&#13;


<span class="strong">
<strong>└── webmining_server</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>We set the Django path into the Scrapy <code class="literal">settings.py</code>&#13;
 file:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong># Setting up django's project full path.</strong>&#13;

</span>&#13;


<span class="strong">
<strong>import sys</strong>&#13;

</span>&#13;


<span class="strong">
<strong>sys.path.insert(0, BASE_DIR+'/webmining_server')</strong>&#13;

</span>&#13;


<span class="strong">
<strong># Setting up django's settings module name.</strong>&#13;

</span>&#13;


<span class="strong">
<strong>os.environ['DJANGO_SETTINGS_MODULE'] = 'webmining_server.settings'</strong>&#13;

</span>&#13;


<span class="strong">
<strong>#import django to load models(otherwise AppRegistryNotReady: Models aren't loaded yet):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>import django</strong>&#13;

</span>&#13;


<span class="strong">
<strong>django.setup()</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Now we can install the library that will allow managing Django models from Scrapy:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>sudo pip install scrapy-djangoitem</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>In the <code class="literal">items.py</code>&#13;
 file, we write the links between Django models and Scrapy items as follows:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from scrapy_djangoitem import DjangoItem</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from pages.models import Page,Link,SearchTerm</strong>&#13;

</span>&#13;



<span class="strong">
<strong>class SearchItem(DjangoItem):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    django_model = SearchTerm</strong>&#13;

</span>&#13;


<span class="strong">
<strong>class PageItem(DjangoItem):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    django_model = Page</strong>&#13;

</span>&#13;


<span class="strong">
<strong>class LinkItem(DjangoItem):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    django_model = Link</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Each class<a id="id602" class="indexterm"/>&#13;
 inherits the <code class="literal">DjangoItem</code>&#13;
 class so that the<a id="id603" class="indexterm"/>&#13;
 original Django models declared with the <code class="literal">django_model</code>&#13;
 variable are automatically linked. The Scrapy project is now completed so we can continue our discussion explaining the Django codes that handle the data extracted by Scrapy and the Django commands needed to manage the applications.</p>&#13;
<div class="section" title="Commands (sentiment analysis model and delete queries)">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch08lvl2sec50"/>&#13;
 Commands (sentiment analysis model and delete queries)</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The application needs to manage some operations that are not allowed to the final user of the service, such <a id="id604" class="indexterm"/>&#13;
 as defining a sentiment <a id="id605" class="indexterm"/>&#13;
 analysis model and deleting a query of a movie in order to redo it instead of retrieving the existing data from memory. The following sections will explain the commands to perform these actions.</p>&#13;
</div>&#13;
<div class="section" title="Sentiment analysis model loader">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch08lvl2sec51"/>&#13;
 Sentiment analysis model loader</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>The final goal of this application is to determine the sentiment (positive or negative) of the movie reviews. To achieve that, a sentiment classifier must be built using some external data, <a id="id606" class="indexterm"/>&#13;
 and then it should be stored in memory (cache) to be used by each query request. This is the purpose of the <code class="literal">load_sentimentclassifier.py</code>&#13;
 command displayed hereafter:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>import nltk.classify.util, nltk.metrics</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from nltk.classify import NaiveBayesClassifier</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from nltk.corpus import movie_reviews</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from nltk.corpus import stopwords</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from nltk.collocations import BigramCollocationFinder</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from nltk.metrics import BigramAssocMeasures</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from nltk.probability import FreqDist, ConditionalFreqDist</strong>&#13;

</span>&#13;


<span class="strong">
<strong>import collections</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from django.core.management.base import BaseCommand, CommandError</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from optparse import make_option</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from django.core.cache import cache</strong>&#13;

</span>&#13;



<span class="strong">
<strong>stopwords = set(stopwords.words('english'))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>method_selfeatures = 'best_words_features'</strong>&#13;

</span>&#13;



<span class="strong">
<strong>class Command(BaseCommand):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    option_list = BaseCommand.option_list + (</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                make_option('-n', '--num_bestwords',     </strong>&#13;

</span>&#13;


<span class="strong">
<strong>                             dest='num_bestwords', type='int',</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                             action='store',</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                             help=('number of words with high information')),)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def handle(self, *args, **options):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>         num_bestwords = options['num_bestwords']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>         self.bestwords = self.GetHighInformationWordsChi(num_bestwords)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>         clf = self.train_clf(method_selfeatures)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>         cache.set('clf',clf)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>         cache.set('bestwords',self.bestwords)</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>At the beginning <a id="id607" class="indexterm"/>&#13;
 of the file, the variable <code class="literal">method_selfeatures</code>&#13;
 sets the method of feature selection (in this case, the features are the words in the reviews; see <a class="link" title="Chapter 4. Web Mining Techniques" href="text00032.html#ch04">Chapter 4</a>&#13;
 , <span class="emphasis">&#13;
<em>Web Mining Techniques</em>&#13;
</span>&#13;
 , for further details) used to train the classifier <code class="literal">train_clf</code>&#13;
 . The maximum number of best words (features) is defined by the input parameter <code class="literal">num_bestwords</code>&#13;
 . The classifier and the best features (<code class="literal">bestwords</code>&#13;
 ) are then stored in the cache ready to be used by the application (using the <code class="literal">cache</code>&#13;
 module). The classifier and the methods to select the best words (features) are as follows:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>    def train_clf(method):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        negidxs = movie_reviews.fileids('neg')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        posidxs = movie_reviews.fileids('pos')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if method=='stopword_filtered_words_features':</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            negfeatures = [(stopword_filtered_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            posfeatures = [(stopword_filtered_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        elif method=='best_words_features':</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            negfeatures = [(best_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            posfeatures = [(best_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        elif method=='best_bigrams_words_features':</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            negfeatures = [(best_bigrams_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            posfeatures = [(best_bigrams_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        trainfeatures = negfeatures + posfeatures</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        clf = NaiveBayesClassifier.train(trainfeatures)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        return clf</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    def stopword_filtered_words_features(self,words):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        return dict([(word, True) for word in words if word not in stopwords])</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    #eliminate Low Information Features</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def GetHighInformationWordsChi(self,num_bestwords):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        word_fd = FreqDist()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        label_word_fd = ConditionalFreqDist()</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        for word in movie_reviews.words(categories=['pos']):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            word_fd[word.lower()] +=1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            label_word_fd['pos'][word.lower()] +=1</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        for word in movie_reviews.words(categories=['neg']):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            word_fd[word.lower()] +=1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            label_word_fd['neg'][word.lower()] +=1</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        pos_word_count = label_word_fd['pos'].N()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        neg_word_count = label_word_fd['neg'].N()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        total_word_count = pos_word_count + neg_word_count</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        word_scores = {}</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for word, freq in word_fd.iteritems():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                (freq, pos_word_count), total_word_count)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                (freq, neg_word_count), total_word_count)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            word_scores[word] = pos_score + neg_score</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:num_bestwords]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        bestwords = set([w for w, s in best])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        return bestwords</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    def best_words_features(self,words):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        return dict([(word, True) for word in words if word in self.bestwords])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def best_bigrams_word_features(self,words, measure=BigramAssocMeasures.chi_sq, nbigrams=200):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        bigram_finder = BigramCollocationFinder.from_words(words)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        bigrams = bigram_finder.nbest(measure, nbigrams)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        d = dict([(bigram, True) for bigram in bigrams])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        d.update(best_words_features(words))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        return d</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Three methods <a id="id608" class="indexterm"/>&#13;
 are written to select words in the preceding code:</p>&#13;
<div class="itemizedlist">&#13;
<ul class="itemizedlist">&#13;
<li class="listitem">&#13;
<code class="literal">stopword_filtered_words_features</code>&#13;
 : Eliminates the <code class="literal">stopwords</code>&#13;
 using the <span class="strong">&#13;
<strong>Natural Language Toolkit</strong>&#13;
</span>&#13;
 (<span class="strong">&#13;
<strong>NLTK</strong>&#13;
</span>&#13;
 ) list of conjunctions and considers the rest as<a id="id609" class="indexterm"/>&#13;
 relevant words</li>&#13;
<li class="listitem">&#13;
<code class="literal">best_words_features</code>&#13;
 : Using the <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 measure (<code class="literal">NLTK</code>&#13;
 library), the most informative words related to positive or negative reviews are selected (see <a class="link" title="Chapter 4. Web Mining Techniques" href="text00032.html#ch04">Chapter 4</a>&#13;
 , <span class="emphasis">&#13;
<em>Web Mining Techniques</em>&#13;
</span>&#13;
 , for further details)</li>&#13;
<li class="listitem">&#13;
<code class="literal">best_bigrams_word_features</code>&#13;
 : Uses the <span class="emphasis">&#13;
<em>X<sup>2</sup>&#13;
</em>&#13;
</span>&#13;
 measure (<code class="literal">NLTK</code>&#13;
 library) to find the 200 most informative bigrams from the set of words (see <span class="emphasis">&#13;
<em>Chapter 4</em>&#13;
</span>&#13;
 , <span class="emphasis">&#13;
<em>Web Mining Techniques</em>&#13;
</span>&#13;
 , for further details)</li>&#13;
</ul>&#13;
</div>&#13;
<p>The chosen classifier is the Naive Bayes algorithm (see <a class="link" title="Chapter 3. Supervised Machine Learning" href="text00024.html#page">Chapter 3</a>&#13;
 , <span class="emphasis">&#13;
<em>Supervised Machine Learning</em>&#13;
</span>&#13;
 ) and the labeled text (positive, negative sentiment) is taken from the <code class="literal">NLTK.corpus</code>&#13;
 of <code class="literal">movie_reviews</code>&#13;
 . To install it, open a terminal in Python and install <code class="literal">movie_reviews</code>&#13;
 from <code class="literal">corpus</code>&#13;
 :</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>nltk.download()--&gt; corpora/movie_reviews corpus</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
</div>&#13;
<div class="section" title="Deleting an already performed query">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch08lvl2sec52"/>&#13;
 Deleting an already performed query</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Since we can <a id="id610" class="indexterm"/>&#13;
 specify different parameters (such as the feature selection method, the number of best words, and so on), we may want to perform and store again the sentiment of the reviews with different values. The <code class="literal">delete_query</code>&#13;
 command<a id="id611" class="indexterm"/>&#13;
 is needed for this purpose and it is as follows:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from pages.models import Link,Page,SearchTerm</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from django.core.management.base import BaseCommand, CommandError</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from optparse import make_option</strong>&#13;

</span>&#13;



<span class="strong">
<strong>class Command(BaseCommand):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    option_list = BaseCommand.option_list + (</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                make_option('-s', '--searchid',</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                             dest='searchid', type='int',</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                             action='store',</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                             help=('id of the search term to delete')),)</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    def handle(self, *args, **options):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>         searchid = options['searchid']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>         if searchid == None:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             print "please specify searchid: python manage.py --searchid=--"</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             #list</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             for sobj in SearchTerm.objects.all():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                 print 'id:',sobj.id,"  term:",sobj.term</strong>&#13;

</span>&#13;


<span class="strong">
<strong>         else:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             print 'delete...'</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             search_obj = SearchTerm.objects.get(id=searchid)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             pages = search_obj.pages.all()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             pages.delete()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             links = search_obj.links.all()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             links.delete()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>             search_obj.delete()</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>If we run the<a id="id612" class="indexterm"/>&#13;
 command without specifying<a id="id613" class="indexterm"/>&#13;
 the <code class="literal">searchid</code>&#13;
 (the ID of the query), the list of all the queries and related IDs will be shown. After that we can choose which query we want to delete by typing the following:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>python manage.py delete_query --searchid=VALUE</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>We can use the cached sentiment analysis model to show the user the online sentiment of the chosen movie, as we explain in the following section.</p>&#13;
</div>&#13;
<div class="section" title="Sentiment reviews analyser – Django views and HTML">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h2 class="title"><a id="ch08lvl2sec53"/>&#13;
 Sentiment reviews analyser – Django views and HTML</h2>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>Most of the code explained in this chapter (commands, Bing search engine, Scrapy, and Django models) is<a id="id614" class="indexterm"/>&#13;
 used in the function analyzer in <code class="literal">views.py</code>&#13;
 to power the home webpage shown in the <span class="emphasis">&#13;
<em>Application usage overview</em>&#13;
</span>&#13;
 section (after declaring the URL in the <code class="literal">urls.py</code>&#13;
 file as <code class="literal">url(r'^$','webmining_server.views.analyzer')</code>&#13;
 ).</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>def analyzer(request):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    context = {}</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    if request.method == 'POST':</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        post_data = request.POST</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        query = post_data.get('query', None)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if query:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            return redirect('%s?%s' % (reverse('webmining_server.views.analyzer'),</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                                urllib.urlencode({'q': query})))   </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    elif request.method == 'GET':</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        get_data = request.GET</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        query = get_data.get('q')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if not query:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            return render_to_response(</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                'movie_reviews/home.html', RequestContext(request, context))</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        context['query'] = query</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        stripped_query = query.strip().lower()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        urls = []</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if test_mode:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           urls = parse_bing_results()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        else:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           urls = bing_api(stripped_query)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if len(urls)== 0:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           return render_to_response(</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               'movie_reviews/noreviewsfound.html', RequestContext(request, context))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if not SearchTerm.objects.filter(term=stripped_query).exists():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           s = SearchTerm(term=stripped_query)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           s.save()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           try:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               #scrape</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               cmd = 'cd ../scrapy_spider &amp; scrapy crawl scrapy_spider_reviews -a url_list=%s -a search_key=%s' %('\"'+str(','.join(urls[:num_reviews]).encode('utf-8'))+'\"','\"'+str(stripped_query)+'\"')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               os.system(cmd)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           except:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               print 'error!'</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               s.delete()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        else:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           #collect the pages already scraped </strong>&#13;

</span>&#13;


<span class="strong">
<strong>           s = SearchTerm.objects.get(term=stripped_query)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #calc num pages</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        pages = s.pages.all().filter(review=True)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if len(pages) == 0:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           s.delete()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           return render_to_response(</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               'movie_reviews/noreviewsfound.html', RequestContext(request, context))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        s.num_reviews = len(pages)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        s.save()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>         </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        context['searchterm_id'] = int(s.id)</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        #train classifier with nltk</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        def train_clf(method):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            ...           </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        def stopword_filtered_words_features(words):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            ... </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #Eliminate Low Information Features</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        def GetHighInformationWordsChi(num_bestwords):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            ...            </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        bestwords = cache.get('bestwords')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if bestwords == None:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            bestwords = GetHighInformationWordsChi(num_bestwords)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        def best_words_features(words):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            ...       </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        def best_bigrams_words_features(words, measure=BigramAssocMeasures.chi_sq, nbigrams=200):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            ...</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        clf = cache.get('clf')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if clf == None:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            clf = train_clf(method_selfeatures)</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        cntpos = 0</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        cntneg = 0</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for p in pages:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            words = p.content.split(" ")</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            feats = best_words_features(words)#bigram_word_features(words)#stopword_filtered_word_feats(words)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            #print feats</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            str_sent = clf.classify(feats)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            if str_sent == 'pos':</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               p.sentiment = 1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               cntpos +=1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            else:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               p.sentiment = -1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>               cntneg +=1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            p.save()</strong>&#13;

</span>&#13;



<span class="strong">
<strong>        context['reviews_classified'] = len(pages)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        context['positive_count'] = cntpos</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        context['negative_count'] = cntneg</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        context['classified_information'] = True</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    return render_to_response(</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        'movie_reviews/home.html', RequestContext(request, context))</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The inserted <a id="id615" class="indexterm"/>&#13;
 movie title is stored in the <code class="literal">query</code>&#13;
 variable and sent to the <code class="literal">bing_api</code>&#13;
 function to collect review's URL. The URL are then scraped calling Scrapy to find the review texts, which are processed using the <code class="literal">clf</code>&#13;
 classifier model and the selected most informative words (<code class="literal">bestwords</code>&#13;
 ) retrieved from the cache (or the same model is generated again in case the cache is empty). The counts of the predicted sentiments of the reviews (<code class="literal">positive_counts</code>&#13;
 , <code class="literal">negative_counts</code>&#13;
 , and <code class="literal">reviews_classified</code>&#13;
 ) are then sent back to the <code class="literal">home.html</code>&#13;
 (the <code class="literal">templates</code>&#13;
 folder) page, which uses the following Google pie chart code:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">        &lt;h2 align = Center&gt;Movie Reviews Sentiment Analysis&lt;/h2&gt;
        &lt;div class="row"&gt;
        &lt;p align = Center&gt;&lt;strong&gt;Reviews Classified : {{ reviews_classified }}&lt;/strong&gt;&lt;/p&gt;
        &lt;p align = Center&gt;&lt;strong&gt;Positive Reviews : {{ positive_count }}&lt;/strong&gt;&lt;/p&gt;
        &lt;p align = Center&gt;&lt;strong&gt; Negative Reviews : {{ negative_count }}&lt;/strong&gt;&lt;/p&gt;
        &lt;/div&gt; 
  &lt;section&gt;
      &lt;script type="text/javascript" src="https://www.google.com/jsapi"&gt;&lt;/script&gt;
      &lt;script type="text/javascript"&gt;
        google.load("visualization", "1", {packages:["corechart"]});
        google.setOnLoadCallback(drawChart);
        function drawChart() {
          var data = google.visualization.arrayToDataTable([
            ['Sentiment', 'Number'],
            ['Positive',     {{ positive_count }}],
            ['Negative',      {{ negative_count }}]
          ]);
          var options = { title: 'Sentiment Pie Chart'};
          var chart = new google.visualization.PieChart(document.getElementById('piechart'));
          chart.draw(data, options);
        }
      &lt;/script&gt;
        &lt;p align ="Center" id="piechart" style="width: 900px; height: 500px;display: block; margin: 0 auto;text-align: center;" &gt;&lt;/p&gt;
      &lt;/div&gt;</pre>&#13;
</div>&#13;
<p>The function <code class="literal">drawChart</code>&#13;
 calls the Google <code class="literal">PieChart</code>&#13;
 visualization function, which takes as input <a id="id616" class="indexterm"/>&#13;
 the data (the positive and negative counts) to create the pie chart. To have more details about how the HTML code interacts with the Django views, refer to <a class="link" title="Chapter 6. Getting Started with Django" href="text00046.html#ch06">Chapter 6</a>&#13;
 , <span class="emphasis">&#13;
<em>Getting Started with Django</em>&#13;
</span>&#13;
 , in the <span class="emphasis">&#13;
<em>URL and views behind html web pages</em>&#13;
</span>&#13;
 section. From the result page with the sentiment counts (see the <span class="emphasis">&#13;
<em>Application usage overview</em>&#13;
</span>&#13;
 section), the PagerRank relevance of the scraped reviews can be calculated using one of the two links at the bottom of the page. The Django code behind this operation is discussed in the following section.</p>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="PageRank: Django view and the algorithm code">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch08lvl1sec57"/>&#13;
 PageRank: Django view and the algorithm code</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>To rank<a id="id617" class="indexterm"/>&#13;
 the importance of the online reviews, we have implemented the PageRank algorithm (see <a class="link" title="Chapter 4. Web Mining Techniques" href="text00032.html#ch04">Chapter 4</a>&#13;
 , <span class="emphasis">&#13;
<em>Web Mining Techniques</em>&#13;
</span>&#13;
 , in the <span class="emphasis">&#13;
<em>Ranking: PageRank algorithm</em>&#13;
</span>&#13;
 section) into the application. The <code class="literal">pgrank.py</code>&#13;
 file in the <code class="literal">pgrank</code>&#13;
 folder within<a id="id618" class="indexterm"/>&#13;
 the <code class="literal">webmining_server</code>&#13;
 folder implements the algorithm that follows:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from pages.models import Page,SearchTerm</strong>&#13;

</span>&#13;



<span class="strong">
<strong>num_iterations = 100000</strong>&#13;

</span>&#13;


<span class="strong">
<strong>eps=0.0001</strong>&#13;

</span>&#13;


<span class="strong">
<strong>D = 0.85</strong>&#13;

</span>&#13;



<span class="strong">
<strong>def pgrank(searchid):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    s = SearchTerm.objects.get(id=int(searchid))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    links = s.links.all()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    from_idxs = [i.from_id for i in links ]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    # Find the idxs that receive page rank </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    links_received = []</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    to_idxs = []</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    for l in links:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        from_id = l.from_id</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        to_id = l.to_id</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if from_id not in from_idxs: continue</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if to_id  not in from_idxs: continue</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        links_received.append([from_id,to_id])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        if to_id  not in to_idxs: to_idxs.append(to_id)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    pages = s.pages.all()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    prev_ranks = dict()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    for node in from_idxs:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        ptmp  = Page.objects.get(id=node)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        prev_ranks[node] = ptmp.old_rank</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    conv=1.</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    cnt=0</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    while conv&gt;eps or cnt&lt;num_iterations:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        next_ranks = dict()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        total = 0.0</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for (node,old_rank) in prev_ranks.items():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            total += old_rank</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            next_ranks[node] = 0.0</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #find the outbound links and send the pagerank down to each of them</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for (node, old_rank) in prev_ranks.items():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            give_idxs = []</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            for (from_id, to_id) in links_received:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                if from_id != node: continue</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                if to_id  not in to_idxs: continue</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                give_idxs.append(to_id)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            if (len(give_idxs) &lt; 1): continue</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            amount = D*old_rank/len(give_idxs)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            for id in give_idxs:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>                next_ranks[id] += amount</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        tot = 0</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for (node,next_rank) in next_ranks.items():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            tot += next_rank</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        const = (1-D)/ len(next_ranks)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for node in next_ranks:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            next_ranks[node] += const</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        tot = 0</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for (node,old_rank) in next_ranks.items():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            tot += next_rank</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        difftot = 0</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for (node, old_rank) in prev_ranks.items():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            new_rank = next_ranks[node]</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            diff = abs(old_rank-new_rank)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            difftot += diff</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        conv= difftot/len(prev_ranks)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        cnt+=1</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        prev_ranks = next_ranks</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    for (id,new_rank) in next_ranks.items():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        ptmp = Page.objects.get(id=id)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        url = ptmp.url</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    for (id,new_rank) in next_ranks.items():</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        ptmp = Page.objects.get(id=id)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        ptmp.old_rank = ptmp.new_rank</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        ptmp.new_rank = new_rank</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        ptmp.save()</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>This code <a id="id619" class="indexterm"/>&#13;
 takes all the links stores associated with the given <code class="literal">SearchTerm</code>&#13;
 object and implements the PageRank score for each page <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 at time <span class="emphasis">&#13;
<em>t</em>&#13;
</span>&#13;
 , where <span class="emphasis">&#13;
<em>P(i)</em>&#13;
</span>&#13;
 is given by the recursive equation:</p>&#13;
<div class="mediaobject"><img src="Image00544.jpg" alt="PageRank: Django view and the algorithm code"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Here, <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 is the total number of pages, and <span class="inlinemediaobject"><img src="Image00545.jpg" alt="PageRank: Django view and the algorithm code"/>&#13;
</span>&#13;
 (<span class="emphasis">&#13;
<em>N<sub>j</sub>&#13;
</em>&#13;
</span>&#13;
 is the number of out links of page <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 ) if page <span class="emphasis">&#13;
<em>j</em>&#13;
</span>&#13;
 points to <span class="emphasis">&#13;
<em>i</em>&#13;
</span>&#13;
 ; otherwise, <span class="emphasis">&#13;
<em>N</em>&#13;
</span>&#13;
 is <code class="literal">0</code>&#13;
 . The parameter <span class="emphasis">&#13;
<em>D</em>&#13;
</span>&#13;
 is the so-called <span class="strong">&#13;
<strong>damping factor</strong>&#13;
</span>&#13;
 (set to 0.85 in the preceding code), and it <a id="id620" class="indexterm"/>&#13;
 represents the probability to follow the transition given by the transition matrix <span class="emphasis">&#13;
<em>A</em>&#13;
</span>&#13;
 . The equation is iterated until the convergence parameter <code class="literal">eps</code>&#13;
 is satisfied or the maximum number of iterations, <code class="literal">num_iterations</code>&#13;
 , is reached. The algorithm is called by clicking either <span class="strong">&#13;
<strong>scrape and calculate page rank (may take a long time)</strong>&#13;
</span>&#13;
 or <span class="strong">&#13;
<strong>calculate page rank</strong>&#13;
</span>&#13;
 links at the bottom of the <code class="literal">home.html</code>&#13;
 page after the sentiment of the movie reviews has been displayed. The link is linked to the function <code class="literal">pgrank_view</code>&#13;
 in the <code class="literal">views.py</code>&#13;
 (through the declared URL in <code class="literal">urls.py: url(r'^pg-rank/(?P&lt;pk&gt;\d+)/','webmining_server.views.pgrank_view', name='pgrank_view')</code>&#13;
 ):</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>def pgrank_view(request,pk): </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    context = {}</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    get_data = request.GET</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    scrape = get_data.get('scrape','False')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    s = SearchTerm.objects.get(id=pk)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    if scrape == 'True':</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        pages = s.pages.all().filter(review=True)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        urls = []</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        for u in pages:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>            urls.append(u.url)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        #crawl</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        cmd = 'cd ../scrapy_spider &amp; scrapy crawl scrapy_spider_recursive -a url_list=%s -a search_id=%s' %('\"'+str(','.join(urls[:]).encode('utf-8'))+'\"','\"'+str(pk)+'\"')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        os.system(cmd)</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    links = s.links.all()</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    if len(links)==0:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>       context['no_links'] = True</strong>&#13;

</span>&#13;


<span class="strong">
<strong>       return render_to_response(</strong>&#13;

</span>&#13;


<span class="strong">
<strong>           'movie_reviews/pg-rank.html', RequestContext(request, context))</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    #calc pgranks</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    pgrank(pk)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    #load pgranks in descending order of pagerank</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    pages_ordered = s.pages.all().filter(review=True).order_by('-new_rank')</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    context['pages'] = pages_ordered</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    return render_to_response(</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        'movie_reviews/pg-rank.html', RequestContext(request, context)) </strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>This code <a id="id621" class="indexterm"/>&#13;
 calls the crawler to collect all the linked pages to the reviews and calculate the PageRank scores using the code discussed earlier. Then the scores are displayed in the <code class="literal">pg-rank.html</code>&#13;
 page (in descending order by page rank score) as we showed in the <span class="emphasis">&#13;
<em>Application usage overview</em>&#13;
</span>&#13;
 section of this chapter. Since this function can take a long time to process (to crawl thousands of pages), the command <code class="literal">run_scrapelinks.py</code>&#13;
 has been written to run the Scrapy crawler (the reader is invited to <a id="id622" class="indexterm"/>&#13;
 read or modify the script as they like as an exercise).</p>&#13;
</div>&#13;

<div class="section" title="Admin and API">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch08lvl1sec58"/>&#13;
 Admin and API</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>As the last part <a id="id623" class="indexterm"/>&#13;
 of the chapter, we describe briefly some possible <a id="id624" class="indexterm"/>&#13;
 admin management of the model and the implementation of an API endpoint to retrieve the data processed by the application. In the <code class="literal">pages</code>&#13;
 folder, we can set two admin interfaces in the <code class="literal">admin.py</code>&#13;
 file to check the data collected by the <code class="literal">SearchTerm</code>&#13;
 and <code class="literal">Page</code>&#13;
 models:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from django.contrib import admin</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from django_markdown.admin import MarkdownField, AdminMarkdownWidget</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from pages.models import SearchTerm,Page,Link</strong>&#13;

</span>&#13;



<span class="strong">
<strong>class SearchTermAdmin(admin.ModelAdmin):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    formfield_overrides = {MarkdownField: {'widget': AdminMarkdownWidget}}</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    list_display = ['id', 'term', 'num_reviews']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    ordering = ['-id']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>class PageAdmin(admin.ModelAdmin):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    formfield_overrides = {MarkdownField: {'widget': AdminMarkdownWidget}}</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    list_display = ['id', 'searchterm', 'url','title','content']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    ordering = ['-id','-new_rank']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>admin.site.register(SearchTerm,SearchTermAdmin)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>admin.site.register(Page,PageAdmin)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>admin.site.register(Link)</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>Note that both <code class="literal">SearchTermAdmin</code>&#13;
 and <code class="literal">PageAdmin</code>&#13;
 display objects with decreasing ID (and <code class="literal">new_rank</code>&#13;
 in the case of <code class="literal">PageAdmin</code>&#13;
 ). The following screenshot is an example:</p>&#13;
<div class="mediaobject"><img src="Image00546.jpg" alt="Admin and API"/>&#13;
</div>&#13;
<p style="clear:both; height: 1em;"/>&#13;
<p>Note that <a id="id625" class="indexterm"/>&#13;
 although it is not necessary, the <code class="literal">Link</code>&#13;
 model has also been<a id="id626" class="indexterm"/>&#13;
 included in the admin interface (<code class="literal">admin.site.register(Link)</code>&#13;
 ). More interestingly, we can set up an API endpoint to retrieve the sentiment counts related to a movie's title. In the <code class="literal">api.py</code>&#13;
 file inside the pages folder, we can have the following:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from rest_framework import views,generics</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from rest_framework.permissions import AllowAny</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from rest_framework.response import Response</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from rest_framework.pagination import PageNumberPagination</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from pages.serializers import SearchTermSerializer</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from pages.models import SearchTerm,Page</strong>&#13;

</span>&#13;



<span class="strong">
<strong>class LargeResultsSetPagination(PageNumberPagination):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    page_size = 1000</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    page_size_query_param = 'page_size'</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    max_page_size = 10000</strong>&#13;

</span>&#13;


<span class="strong">
<strong>  </strong>&#13;

</span>&#13;


<span class="strong">
<strong>class SearchTermsList(generics.ListAPIView):</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    serializer_class = SearchTermSerializer</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    permission_classes = (AllowAny,)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    pagination_class = LargeResultsSetPagination</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    </strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def get_queryset(self):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        return SearchTerm.objects.all()  </strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>class PageCounts(views.APIView):</strong>&#13;

</span>&#13;



<span class="strong">
<strong>    permission_classes = (AllowAny,)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    def get(self,*args, **kwargs):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        searchid=self.kwargs['pk']</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        reviewpages = Page.objects.filter(searchterm=searchid).filter(review=True)</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        npos = len([p for p in reviewpages if p.sentiment==1])</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        nneg = len(reviewpages)-npos</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        return Response({'npos':npos,'nneg':nneg})</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>The <code class="literal">PageCounts</code>&#13;
 class takes as input the ID of the search (the movie's title) and it returns the sentiments,<a id="id627" class="indexterm"/>&#13;
 that is, positive and negative counts, for the movie's<a id="id628" class="indexterm"/>&#13;
 reviews. To get the ID of <code class="literal">earchTerm</code>&#13;
 from a movie's title, you can either look at the admin interface or use the other API endpoint <code class="literal">SearchTermsList</code>&#13;
 ; this simply returns the list of the movies' titles together with the associated ID. The serializer is set on the <code class="literal">serializers.py</code>&#13;
 file:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>from pages.models import SearchTerm</strong>&#13;

</span>&#13;


<span class="strong">
<strong>from rest_framework import serializers</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        </strong>&#13;

</span>&#13;


<span class="strong">
<strong>class SearchTermSerializer(serializers.HyperlinkedModelSerializer):</strong>&#13;

</span>&#13;


<span class="strong">
<strong>    class Meta:</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        model = SearchTerm</strong>&#13;

</span>&#13;


<span class="strong">
<strong>        fields = ('id', 'term')</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>To call these endpoints, we can again use the swagger interface (see <a class="link" title="Chapter 6. Getting Started with Django" href="text00046.html#ch06">Chapter 6</a>&#13;
 , <span class="emphasis">&#13;
<em>Getting Started with Django</em>&#13;
</span>&#13;
 ) or use the <code class="literal">curl</code>&#13;
 command in the terminal to make these calls. For instance:</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>curl -X GET localhost:8000/search-list/</strong>&#13;

</span>&#13;


<span class="strong">
<strong>{"count":7,"next":null,"previous":null,"results":[{"id":24,"term":"the martian"},{"id":27,"term":"steve jobs"},{"id":29,"term":"suffragette"},{"id":39,"term":"southpaw"},{"id":40,"term":"vacation"},{"id":67,"term":"the revenant"},{"id":68,"term":"batman vs superman dawn of justice"}]}</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
<p>and</p>&#13;
<div class="informalexample">&#13;
<pre class="programlisting">
<span class="strong">
<strong>curl -X GET localhost:8000/pages-sentiment/68/</strong>&#13;

</span>&#13;


<span class="strong">
<strong>{"nneg":3,"npos":15}</strong>&#13;

</span>&#13;


</pre>&#13;
</div>&#13;
</div>&#13;

<div class="section" title="Summary">&#13;
<div class="titlepage">&#13;
<div>&#13;
<div>&#13;
<h1 class="title"><a id="ch08lvl1sec59"/>&#13;
 Summary</h1>&#13;
</div>&#13;
</div>&#13;
</div>&#13;
<p>In this chapter, we described a movie review sentiment analyzer web application to make you familiar with some of the algorithms and libraries we discussed in <a class="link" title="Chapter 3. Supervised Machine Learning" href="text00024.html#page">Chapter 3</a>&#13;
 , <span class="emphasis">&#13;
<em>Supervised Machine Learning</em>&#13;
</span>&#13;
 , <a class="link" title="Chapter 4. Web Mining Techniques" href="text00032.html#ch04">Chapter 4</a>&#13;
 , <span class="emphasis">&#13;
<em>Web Mining Techniques</em>&#13;
</span>&#13;
 , and <a class="link" title="Chapter 6. Getting Started with Django" href="text00046.html#ch06">Chapter 6</a>&#13;
 , <span class="emphasis">&#13;
<em>Getting Started with Django</em>&#13;
</span>&#13;
 .</p>&#13;
<p>This is the end of a journey: by reading this book and experimenting with the codes provided, you should have acquired significant practical knowledge about the most important machine learning algorithms used in the commercial environment nowadays.</p>&#13;
<p>You should be now ready to develop your own web applications and ideas using Python and some machine learning algorithms, learned by reading this book. Many challenging data-related problems are present in the real world today, waiting to be solved by people who can grasp and apply the material treated in this book, and you, who have arrived at this point, are certainly one of those people.</p>&#13;
</div>&#13;

<h1>读累了记得休息一会哦~</h1>
<p> </p>
<p><strong>公众号：古德猫宁李</strong></p>
<ul>
<li>电子书搜索下载</li>
<li>书单分享</li>
<li>书友学习交流</li>

<p> </p>
</ul>
<p><strong>网站：</strong><a href="https://www.chenjin5.com">沉金书屋 https://www.chenjin5.com</a></p>
<ul>
<li>电子书搜索下载</li>
<li>电子书打包资源分享</li>
<li>学习资源分享</li>

</ul>
</body></html>