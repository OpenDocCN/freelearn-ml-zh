<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer137">
			<h1 class="chapter-number"><a id="_idTextAnchor1057"/>8</h1>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor1058"/>AWS Application Services for AI/ML</h1>
			<p>In this chapter, you will learn about the AWS AI services for building chatbots, advanced text analysis, document analysis, transcription, and so on. This chapter has been designed in such a way that you can solve different use cases by integrating AWS AI services and get an idea of how they work. AWS is growing every day, and they are adding new AI <span class="No-Break">services regularly.</span></p>
			<p>In this chapter, you will approach different use cases programmatically or from the console. This will help you understand different APIs and how to use them. You will use S3 for storage and AWS Lambda to execute any code. The examples in this chapter are in Python, but you can use other supported languages such as Java, Node.js, .NET, PowerShell, Ruby, and <span class="No-Break">so on.</span></p>
			<p>You will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Analyzing images and videos with <span class="No-Break">Amazon Rekognition</span></li>
				<li>Text to speech with <span class="No-Break">Amazon Polly</span></li>
				<li>Speech to text with <span class="No-Break">Amazon Transcribe</span></li>
				<li>Implementing natural language processing with <span class="No-Break">Amazon Comprehend</span></li>
				<li>Translating documents with <span class="No-Break">Amazon Translate</span></li>
				<li>Extracting text from documents with <span class="No-Break">Amazon Textract</span></li>
				<li>Creating chatbots on <span class="No-Break">Amazon Lex</span></li>
				<li>Time series forecasting with <span class="No-Break">Amazon Forecast</span></li>
			</ul>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor1059"/><a id="_idTextAnchor1060"/>Technical requirements</h1>
			<p>All you need for this chapter is an <span class="No-Break">AWS account.</span></p>
			<p>You can download the code examples for this chapter from GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08"><span class="No-Break">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor1061"/><a id="_idTextAnchor1062"/>Analyzing images and videos with Amazon Rekognition</h1>
			<p>If you need to add powerful <a id="_idTextAnchor1063"/>visual analysis to your applications, then <strong class="bold">Amazon Rekognition</strong> is the service to choose. <strong class="bold">Rekognition Image</strong> lets <a id="_idTextAnchor1064"/>you easily build powerful applications to search, verify, and organize millions of images. It lets you extract motion-based context from<a id="_idTextAnchor1065"/> stored or live stream videos, and helps you analyze them. Rekognition Video also allows you to index metadata such as objects, activities, scenes, celebrities, and faces, making video searches easy. Rekognition Image uses deep neural network models to detect and label numerous objects and scenes in your images. It helps you capture text in an image, a bit like <strong class="bold">Optical Character Recognition (OCR)</strong>. A perfect <a id="_idTextAnchor1066"/>example is a T-shirt with quotes on it. If you were to take a picture of one and ask Amazon Rekognition to extract the text from it, it would be able to tell you what the text says. You can also perform celebrity recognition using Amazon Rekognition. Somebody who is not a celebrity won’t use the celebrity recognition API for their face; instead, they will use the face <span class="No-Break">comparison API.</span></p>
			<p>The official documentation, available at <a href="https://aws.amazon.com/rekognition/faqs/">https://aws.amazon.com/rekognition/faqs/</a>, states <span class="No-Break">the following:</span></p>
			<p><em class="italic">“With Rekognition Image, you only pay for the images you analyze and the face metadata you store. You will not be charged for the compute resources if, at any point of time, your </em><span class="No-Break"><em class="italic">training fails.”</em></span></p>
			<p>Some common uses of Amazon Rekognition include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Image and <span class="No-Break">video analysis</span></li>
				<li>Searchable <span class="No-Break">image library</span></li>
				<li>Face-based <span class="No-Break">user verification</span></li>
				<li><span class="No-Break">Sentiment analysis</span></li>
				<li>Text <span class="No-Break">in image</span></li>
				<li><span class="No-Break">Facial recognition</span></li>
				<li><span class="No-Break">Image moderation</span></li>
				<li>Search index for <span class="No-Break">video archives</span></li>
				<li>Easy filtering of explicit and suggestive content <span class="No-Break">in videos</span></li>
				<li>Examples of explicit nudity – sexual activity, graphical nudity, adult toys, and <span class="No-Break">so on</span></li>
				<li>Examples of suggestive content – partial<a id="_idTextAnchor1067"/> nudity, swimwear or underwear, and <span class="No-Break">so on</span></li>
			</ul>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor1068"/><a id="_idTextAnchor1069"/>Exploring the benefits of Amazon Rekognition</h2>
			<p>Here are some of the benefits of<a id="_idTextAnchor1070"/> using <span class="No-Break">Amazon Rekognition:</span></p>
			<ul>
				<li>AWS manages the infrastructure it runs on. In short, just use the API for your image analysis. You need to only focus on building and managing your deep <span class="No-Break">learning pipelines.</span><p class="list-inset">With or without knowledge of image processing, you can perform image and video analysis just by using the APIs provided in Amazon Rekognition, which can be used for any application or service on <span class="No-Break">several platforms.</span></p></li>
				<li>The Labels API’s response will identify real-world entities within an image through the DetectLabels API. These labels include city, town, table, home, garden, animal, pets, food, drink, electronics, flowers, and more. The entities are classified based on their <strong class="bold">confidence score</strong>, which indicates the probability that a given prediction is correct — the higher the score, the better. Similarly, you can use the DetectText API to extract the text in an image. Amazon Rekognition may detect multiple lines based on the gap between words. Periods do not represent the end of <span class="No-Break">a line.</span></li>
				<li>Amazon Rekognition can be integrated with AWS Kinesis Video Stream, AWS S3, and AWS Lambda for seamless and affordable image and video analysis. With the AWS IAM service, Amazon Rekognition API calls can easily be secured <span class="No-Break">and controlled.</span></li>
				<li>Low cost: you only pay for<a id="_idTextAnchor1071"/> the images and videos that <span class="No-Break">are analyzed.</span></li>
				<li>Through AWS CloudTrail, all the API calls for Amazon Rekognition can be captured as events. It captures all calls made from the console, the CLI, or code calls for APIs, which further enables the user to create Amazon SNS notifications based on <span class="No-Break">CloudTrail events.</span></li>
				<li>You can create a VPC endpoint policy for specific API calls to establish a private connection between your VPC and Amazon Rekognition. This helps you leverage enhanced security. As per the AWS Shared Responsibility Model, AWS takes care of the security of the infrastructure and software, and you have to take care of the security of your content in <span class="No-Break">the cloud.</span></li>
			</ul>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor1072"/><a id="_idTextAnchor1073"/>Getting hands-on with Amazon Rekognition</h2>
			<p>In this section, you will learn <a id="_idTextAnchor1074"/>how to integrate AWS Lambda with Amazon Rekognition to detect the labels in our image (uploaded at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Rekognition%20Demo/images">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Rekognition%20Demo/images</a>) and print the detected objects in the CloudWatch console. You will use the <strong class="source-inline">detect_labels</strong> API from Amazon Rekognition in <span class="No-Break">the code.</span></p>
			<p>You will begin by creating an IAM role <span class="No-Break">for Lambda:</span></p>
			<ol>
				<li>Navigate to the IAM <span class="No-Break">console page.</span></li>
				<li>Select <strong class="bold">Roles</strong> from the <span class="No-Break">left-hand menu.</span></li>
				<li>Select <span class="No-Break"><strong class="bold">Create role</strong></span><span class="No-Break">.</span></li>
				<li>Select <strong class="bold">Lambda</strong> from the <strong class="bold">Choose a use </strong><span class="No-Break"><strong class="bold">case</strong></span><span class="No-Break"> section.</span></li>
				<li>Add the following <span class="No-Break">managed policies:</span><ul><li><span class="No-Break"><strong class="source-inline">AmazonS3ReadOnlyAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">AmazonRekognitionFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">CloudWatchLogsFullAccess</strong></span></li></ul></li>
				<li>Name the <a id="_idTextAnchor1075"/><span class="No-Break">role </span><span class="No-Break"><strong class="source-inline">rekognition-lambda-role</strong></span><span class="No-Break">:</span><div id="_idContainer106" class="IMG---Figure"><img src="image/B21197_08_01.jpg" alt="Figure 8.1 – The Create role dialog" width="1441" height="884"/></div></li>
			</ol>
			<p class="IMG---Figure"><a id="_idTextAnchor1076"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – The Create role dialog</p>
			<p class="list-inset">Next, you will create a <span class="No-Break">Lambda function.</span></p>
			<ol>
				<li value="7">Navigate to the AWS Lambda <span class="No-Break">console page.</span></li>
				<li>Select <span class="No-Break"><strong class="bold">Create function</strong></span><span class="No-Break">.</span></li>
				<li>Create <span class="No-Break">a function:</span><ul><li>Select <strong class="bold">Author </strong><span class="No-Break"><strong class="bold">from scratch</strong></span><span class="No-Break">.</span></li><li>Give the function a name, such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">lambda-rekognition</strong></span><span class="No-Break">.</span></li><li>Choose <strong class="source-inline">Python 3.6</strong> from the <span class="No-Break"><strong class="bold">Runtime</strong></span><span class="No-Break"> dropdown.</span></li><li>Select <strong class="bold">Use an existing role</strong>. Add the name <a id="_idTextAnchor1077"/>of the role you created previously; that <span class="No-Break">is, </span><span class="No-Break"><strong class="source-inline">rekognition-lambda-role</strong></span><span class="No-Break">:</span></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B21197_08_02.jpg" alt="Figure 8.2 – Creating the Lambda function" width="1263" height="643"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor1078"/>Figure 8.2 – Creating the Lambda function</p>
			<ol>
				<li value="10">Enter the following code <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">lambda_function.py</strong></span><span class="No-Break">:</span><pre class="source-code"><strong class="source-inline">from __future__ import print_function</strong></pre><pre class="source-code"><strong class="source-inline">import boto3</strong></pre><pre class="source-code"><strong class="source-inline">def lambda_handler(event, context):</strong></pre><pre class="source-code"><strong class="source-inline">    print("========lambda_handler started=======")</strong></pre><pre class="source-code"><strong class="source-inline">    # read the bucket name (key) from the event</strong></pre><pre class="source-code"><strong class="source-inline">    name_of_the_bucket=event['Records'][0]['s3']['bucket']</strong></pre><pre class="source-code"><strong class="source-inline">['name']</strong></pre><pre class="source-code"><strong class="source-inline">    # read the object from the event</strong></pre><pre class="source-code"><strong class="source-inline">    name_of_the_photo=event['Records'][0]['s3']['object']['key']</strong></pre><pre class="source-code"><strong class="source-inline">    detect_labels(name_of_the_photo,name_of_the_bucket)</strong></pre><pre class="source-code"><strong class="source-inline">    print("Labels detected Successfully")</strong></pre><pre class="source-code"><strong class="source-inline">def detect_labels(photo, bucket):</strong></pre><pre class="source-code"><strong class="source-inline">    client=boto3.client('rekognition')</strong></pre><pre class="source-code"><strong class="source-inline">  response=client.detect_labels(Image={'S3Object':{'Bucket':bucket,'Name':photo}})</strong></pre><pre class="source-code"><strong class="source-inline">    print('Detected labels for ' + photo)</strong></pre><pre class="source-code"><strong class="source-inline">    print('==============================')</strong></pre><pre class="source-code"><strong class="source-inline">    for label in response['Labels']:</strong></pre><pre class="source-code"><strong class="source-inline">        print ("Label: " + label['Name'])</strong></pre><pre class="source-code"><strong class="source-inline">        print ("Confidence: " +</strong></pre><pre class="source-code"><strong class="source-inline">str(label['Confidence']))</strong></pre><pre class="source-code"><strong class="source-inline">        print ("Instances:")</strong></pre><pre class="source-code"><strong class="source-inline">        for instance in label['Instances']:</strong></pre><pre class="source-code"><strong class="source-inline">            print ("  Bounding box")</strong></pre><pre class="source-code"><strong class="source-inline">            print ("Top:</strong></pre><pre class="source-code"><strong class="source-inline">"+str(instance['BoundingBox']['Top']))</strong></pre><pre class="source-code"><strong class="source-inline">            print ("Left: \</strong></pre><pre class="source-code"><strong class="source-inline">"+str(instance['BoundingBox']['Left']))</strong></pre><pre class="source-code"><strong class="source-inline">            print ("Width: \</strong></pre><pre class="source-code"><strong class="source-inline">"+str(instance['BoundingBox']['Width']))</strong></pre><pre class="source-code"><strong class="source-inline">            print ("Height: \</strong></pre><pre class="source-code"><strong class="source-inline">"+str(instance['BoundingBox']['Height']))</strong></pre><pre class="source-code"><strong class="source-inline">            print ("Confidence:</strong></pre><pre class="source-code"><strong class="source-inline">"+str(instance['Confidence']))</strong></pre><pre class="source-code"><strong class="source-inline">            print()</strong></pre><pre class="source-code"><strong class="source-inline">        print ("Parents:")</strong></pre><pre class="source-code"><strong class="source-inline">        for parent in label['Parents']:</strong></pre><pre class="source-code"><strong class="source-inline">            print ("   " + parent['Name'])</strong></pre><pre class="source-code"><strong class="source-inline">        print ("----------")</strong></pre><pre class="source-code"><strong class="source-inline">        print('==============================')</strong></pre><pre class="source-code"><strong class="source-inline">    return response</strong></pre><p class="list-inset">Now, you will create a trigger for the <span class="No-Break">Lambda Function.</span></p></li>
				<li>Navigate to the AWS S3 console page. Create a bu<a id="_idTextAnchor1079"/>cket, for example, <strong class="source-inline">rekognition-test-baba</strong>, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B21197_08_03.jpg" alt="Figure 8.3 – AWS S3 console page" width="1378" height="687"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – AWS S3 console page</p>
			<ol>
				<li value="12">Click on <strong class="bold">Create folder</strong> and name it <strong class="source-inline">images</strong>. <span class="No-Break">Click </span><span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break">.</span></li>
				<li>Click the <strong class="bold">Properties</strong> tab of <span class="No-Break">our bucket.</span></li>
				<li>Scroll to <strong class="bold">Events</strong> for <span class="No-Break">that buc<a id="_idTextAnchor1080"/>ket.</span></li>
				<li>Inside the <strong class="bold">Events</strong> window, select <strong class="bold">Add notification</strong> and set the <span class="No-Break">following properties:</span><ul><li><span class="No-Break"><strong class="bold">Name</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">rekognition_event</strong></span></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object </strong><span class="No-Break"><strong class="source-inline">create events</strong></span></li><li><span class="No-Break"><strong class="bold">Prefix</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">images</strong></span><span class="No-Break"><em class="italic">/</em></span></li><li><strong class="bold">Send</strong> <strong class="bold">to</strong>: <span class="No-Break"><strong class="source-inline">Lambda Function</strong></span></li></ul><ul><li><span class="No-Break"><strong class="bold">Lambda</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">lambda-rekognition</strong></span><span class="No-Break">:</span></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B21197_08_04.jpg" alt="Figure 8.4 – S3 bucket Events window" width="1208" height="1403"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – S3 bucket Events window</p>
			<p class="list-inset">Next, you will upload the image fro<a id="_idTextAnchor1081"/>m the shared GitHub repository to the S3 bucket <span class="No-Break"><strong class="source-inline">images</strong></span><span class="No-Break"> folder.</span></p>
			<ol>
				<li value="16">As soon as you upload, you can check the <strong class="bold">Monitoring</strong> tab in the Lambda console to monitor the events, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B21197_08_05.jpg" alt="Figure 8.5 – CloudWatch monitoring the event in the Lambda console" width="1380" height="679"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – CloudWatch monitoring the event in the Lambda console</p>
			<ol>
				<li value="17">Navigate to <strong class="source-inline">CloudWatch &gt; CloudWatch Logs &gt; Log groups &gt; /aws/lambda/lambda-rekognition</strong>. Select the latest stream from all the streams listed on the AWS console and scroll down in the logs to see <span class="No-Break">your output.</span></li>
			</ol>
			<p>In this section, you learned how t<a id="_idTextAnchor1082"/>o implement the Amazon Rekognition AI service to detect objects in an image and get a confidence score for each. You will see more use cases for Amazon Rekognition in the upcoming sections, where you will detect text in images. In the next section, you will learn about Amazon’s text-to-speech service and <span class="No-Break">implement it.</span></p>
			<h1 id="_idParaDest-192">Text t<a id="_idTextAnchor1083"/><a id="_idTextAnchor1084"/>o speech with Amazon Polly</h1>
			<p><strong class="bold">Amazon Polly</strong> is all about conve<a id="_idTextAnchor1085"/>rting text into speech, and it does so by using pretrained deep learning models. It is a fully managed service, so you d<a id="_idTextAnchor1086"/>o not have to do anything. You provide the plain text as input for synthesizing or in <strong class="bold">Speech Synthesis</strong> <strong class="bold">Markup Language (SSML)</strong> format so that an audio stream is returned. It also gives you different langua<a id="_idTextAnchor1087"/>ges and voices to choose from, with both male and female options. The output audio from Amazon Polly can be saved in MP3 format for further use in the application (web or mobile) or can be a JSON output for <span class="No-Break">written speech.</span></p>
			<p>For example, if you were to input the text “Baba went to the library” into Amazon Polly, the output speech mark object would look <span class="No-Break">as follows:</span></p>
			<p><strong class="source-inline">{"</strong><span class="No-Break"><strong class="source-inline">time":370,"type":"word","start":5,"end":9,"value":"went"}</strong></span></p>
			<p>The word <strong class="source-inline">"went"</strong> begins 370 milliseconds after the audio stream begins, and starts at byte 5 and ends at byte 9 of the given <span class="No-Break">input text.</span></p>
			<p>It also returns output in <strong class="source-inline">ogg_vorbis</strong> and <strong class="source-inline">pcm</strong> format. When <strong class="source-inline">pcm</strong> is used, the content that is returned as an output is <strong class="source-inline">audio<a id="_idTextAnchor1088"/>/pcm</strong> in a signed 16-bit, 1-channe<a id="_idTextAnchor1089"/>l (mono), <span class="No-Break">little-endian format.</span></p>
			<p>Some common uses of Amazon<a id="_idTextAnchor1090"/> Polly include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Can be used as an accessibility tool for reading <span class="No-Break">web content.</span></li>
				<li>Can be integrated with Amazon Rekognition to help visually impaired people read signs. You can click a picture of the sign with text and feed it to Amazon Rekognition to extract text. The output text can be used as input for Polly, and it will return a voice <span class="No-Break">as output.</span></li>
				<li>Can be used in a public address system, where the admin team can just pass on the text to be announced and Amazon Polly does <span class="No-Break">the magic.</span></li>
				<li>By combining Amazon Polly with <strong class="bold">Amazon Connect</strong> (telephony backend service), you can build <a id="_idTextAnchor1091"/>an <strong class="source-inline">audio/video receiver</strong> <strong class="bold">(</strong><span class="No-Break"><strong class="bold">AVR)</strong></span><span class="No-Break"> system.</span></li>
				<li>Smart devices such as smart TVs, smart watches, and <strong class="bold">Internet of Things (IoT)</strong> devices can use this for <span class="No-Break">audio output.</span></li>
				<li><span class="No-Break">Narration generation.</span></li>
				<li>When combined with Amazon Lex, full-blown voice user interfaces for applications can <span class="No-Break">be developed.</span></li>
			</ul>
			<p>Now, let’s explore the benefits of <span class="No-Break">Amazon Polly.</span></p>
			<h2 id="_idParaDest-193">Explor<a id="_idTextAnchor1092"/><a id="_idTextAnchor1093"/>ing the benefits of Amazon Polly</h2>
			<p>Some of the benefits of using<a id="_idTextAnchor1094"/> Amazon Polly include <span class="No-Break">the following:</span></p>
			<ul>
				<li>This service is fully managed and does not require any admin cost to maintain or <span class="No-Break">manage resources.</span></li>
				<li>It provides an instant speech correction and <span class="No-Break">enhancement facility.</span></li>
				<li>You can develop your own access layer using the HTTP API from Amazon Polly. Development is easy due to the huge amount of language support that’s available, such as Python, Ruby, Go, C++, Java, <span class="No-Break">and Node.js.</span></li>
				<li>For certain neural voices, speech can be synthesized using the Newscaster style, to make them sound like a TV or <span class="No-Break">radio broadcaster.</span></li>
				<li>Amazon Polly also allows you to modify the pronunciation of particular words or the use of <span class="No-Break">new words.</span></li>
			</ul>
			<p>Next, you’ll get hands-on with <span class="No-Break">A<a id="_idTextAnchor1095"/>mazon Polly.</span></p>
			<h2 id="_idParaDest-194">Gettin<a id="_idTextAnchor1096"/><a id="_idTextAnchor1097"/>g hands-on with Amazon Polly</h2>
			<p>In this section, you will build <a id="_idTextAnchor1098"/>a pipeline where you can integrate AWS Lambda with Amazon Polly. The pipeline reads a text file and generates an MP3 file, saving it to another folder in the same bucket. You will monitor the task’s progress in <span class="No-Break">CloudWatch logs.</span></p>
			<p>You will begin by creating an IAM role for Lambda. Let’s <span class="No-Break">get started:</span></p>
			<ol>
				<li>Navigate to the IAM <span class="No-Break">console page.</span></li>
				<li>Select <strong class="bold">Roles</strong> from the <span class="No-Break">left-hand menu.</span></li>
				<li>Select <span class="No-Break"><strong class="bold">Create role</strong></span><span class="No-Break">.</span></li>
				<li>Select <strong class="bold">Lambda</strong> as the <span class="No-Break">trusted entity.</span></li>
				<li>Add the following <span class="No-Break">managed policies:</span><ul><li><span class="No-Break"><strong class="source-inline">AmazonS3FullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">AmazonPollyFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">CloudWatchFullAccess</strong></span></li></ul></li>
				<li>Save the role <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">polly-lambda-role</strong></span><span class="No-Break">.</span><p class="list-inset">Next, you will create a <span class="No-Break">Lambda function:</span></p></li>
				<li>Navigate to <strong class="source-inline">Lambda &gt; Functions &gt; </strong><span class="No-Break"><strong class="source-inline">Create Function</strong></span><span class="No-Break">.</span><ul><li>Name the <span class="No-Break">function </span><span class="No-Break"><strong class="source-inline">polly-lambda</strong></span></li><li>Set the runtime to <span class="No-Break"><strong class="source-inline">python 3.6</strong></span><span class="No-Break">.</span></li><li>Use an existing role; that <span class="No-Break">is, </span><span class="No-Break"><strong class="source-inline">polly-lambda-role</strong></span><span class="No-Break">.</span></li></ul></li>
				<li>Paste the code at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Rekognition%20Demo/lambda_code">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Rekognition%20Demo/lambda_code</a> into your Lambda functi<a id="_idTextAnchor1099"/>on and check its progress in the CloudWatch console. You will be using the <strong class="source-inline">start_speech_synthesis_task</strong> API from Amazon Polly for this code; it is an asynchronous <span class="No-Break">synthesis task.</span></li>
				<li>Scroll down and in the <strong class="bold">Basic Settings</strong> section, change <strong class="bold">Timeout</strong> to <strong class="source-inline">59</strong> <strong class="source-inline">sec</strong>, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.6</em>, and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break">:</span></li>
			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">The default is 3 seconds. Since this is an asynchronous operation, any retried attempts will create <span class="No-Break">more files.</span></p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B21197_08_06.jpg" alt="Figure 8.6 – Edit basic settings window" width="975" height="1115"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Edit basic settings window</p>
			<p class="list-inset">Now, you will create a bucket to trigger <span class="No-Break">an event.</span></p>
			<ol>
				<li value="10">Navigate to the AWS S3<a id="_idTextAnchor1100"/> console and create a bucket <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">polly-test-baba</strong></span><span class="No-Break">.</span></li>
				<li>Create a folder called <strong class="source-inline">input-text</strong> (in this example, you will only upload .<span class="No-Break"><strong class="source-inline">txt</strong></span><span class="No-Break"> files).</span></li>
				<li>Navigate to <strong class="source-inline">Properties &gt; Events &gt; Add notification</strong>. Fill in the required fields, as shown here, and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break">:</span><ul><li><span class="No-Break"><strong class="bold">Name</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">polly_event</strong></span></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object </strong><span class="No-Break"><strong class="source-inline">create events</strong></span></li><li><span class="No-Break"><strong class="bold">Prefix</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">input-text/</strong></span></li><li><span class="No-Break"><strong class="bold">Suffix</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">.txt</strong></span></li><li><strong class="bold">Send to</strong>: <span class="No-Break"><strong class="source-inline">Lambda Function</strong></span></li><li><span class="No-Break"><strong class="bold">Lambda</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">polly-lambda</strong></span></li></ul></li>
				<li>Next, you will upload a file to trigger an event and check its progress in CloudWatchUpload, in this case, a file c<a id="_idTextAnchor1101"/>alled <strong class="source-inline">test_file.txt</strong> in <strong class="source-inline">input-text</strong>, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.7</em>. You can download the sample file from this book’s GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Polly%20Demo/text_file"><span class="No-Break">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Polly%20Demo/text_file</span></a><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B21197_08_07.jpg" alt="Figure 8.7 – The S3 bucket after uploading a text file for further processing" width="1302" height="876"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – The S3 bucket after uploading a text file for further processing</p>
			<ol>
				<li value="14">This will trigger the Lambda <a id="_idTextAnchor1102"/>function. You can monitor your logs by going to <strong class="source-inline">CloudWatch&gt; CloudWatch Logs&gt; Log </strong><span class="No-Break"><strong class="source-inline">groups&gt; /aws/lambda/polly-lambda</strong></span><span class="No-Break">.</span></li>
				<li>Click on the latest s<a id="_idTextAnchor1103"/>tream; the log will look <span class="No-Break">as follows:</span><pre class="source-code"><strong class="source-inline">File Content:  Hello Everyone, Welcome to Dublin. How</strong></pre><pre class="source-code"><strong class="source-inline">are you doing today?</strong></pre><pre class="source-code"><strong class="source-inline">{'ResponseMetadata': {'RequestId': '74ca4afd-5844-</strong></pre><pre class="source-code"><strong class="source-inline">47d8-9664-3660a26965e4', 'HTTPStatusCode': 200,</strong></pre><pre class="source-code"><strong class="source-inline">'HTTPHeaders': {'x-amzn-requestid': '74ca4afd-5844-</strong></pre><pre class="source-code"><strong class="source-inline">47d8-9664-3660a26965e4', 'content-type':</strong></pre><pre class="source-code"><strong class="source-inline">'application/json', 'content-length': '471', 'date':</strong></pre><pre class="source-code"><strong class="source-inline">'Thu, 24 Sep 2020 18:50:57 GMT'}, 'RetryAttempts': 0},</strong></pre><pre class="source-code"><strong class="source-inline">'SynthesisTask': {'Engine': 'standard', 'TaskId':</strong></pre><pre class="source-code"><strong class="source-inline">'57548c6b-d21a-4885-962f-450952569dc7', 'TaskStatus':</strong></pre><pre class="source-code"><strong class="source-inline">'scheduled', 'OutputUri': 'https://s3.us-east-</strong></pre><pre class="source-code"><strong class="source-inline">1.amazonaws.com/polly-test-baba/output-</strong></pre><pre class="source-code"><strong class="source-inline">audio/.57548c6b-d21a-4885-962f-450952569dc7.mp3',</strong></pre><pre class="source-code"><strong class="source-inline">'CreationTime': datetime.datetime(2020, 9, 24, 18, 50,</strong></pre><pre class="source-code"><strong class="source-inline">57, 769000, tzinfo=tzlocal()), 'RequestCharacters':</strong></pre><pre class="source-code"><strong class="source-inline">59, 'OutputFormat': 'mp3', 'TextType': 'text',</strong></pre><pre class="source-code"><strong class="source-inline">'VoiceId': 'Aditi', 'LanguageCode': 'en-GB'}}</strong></pre><p class="list-inset">The logs sample is shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B21197_08_08.jpg" alt="Figure 8.8 – The logs in the CloudWatch console" width="1100" height="286"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – The logs in the CloudWatch console</p>
			<ol>
				<li value="16">It will create output in MP3 format, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.9</em>. Download and listen <span class="No-Break">to it:</span></li>
			</ol>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B21197_08_09.jpg" alt="Figure 8.9 – The output file that was created in the S3 bucket" width="1426" height="782"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.<a id="_idTextAnchor1104"/>9 – The output file that was created in the S3 bucket</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The most scalable and cost-effect<a id="_idTextAnchor1105"/>ive way for your mobile apps or web apps is to generate an AWS pre-signed URL for S3 buckets and provide it to your users. These S3 Put events asynchronously invoke downstream AI workflows to generate results and send a response to the end users. Many users can be served at the same time through this approach, and it may increase performance <span class="No-Break">and throughput.</span></p>
			<p>In this section, you learned how to impl<a id="_idTextAnchor1106"/>ement text to speech. In the next section, you will learn about Amazon Transcribe, a speech-to-text <span class="No-Break">AI service.</span></p>
			<h1 id="_idParaDest-195">Speech to t<a id="_idTextAnchor1107"/><a id="_idTextAnchor1108"/>ext with Amazon Transcribe</h1>
			<p>In the previous section, you learned ab<a id="_idTextAnchor1109"/>out text to speech. In this section, you will learn about speech to text and the service that provides it: <strong class="bold">Amazon Transcribe</strong>. It is an automatic speech reco<a id="_idTextAnchor1110"/>gnition service that uses pre-trained deep learning models, which means that you do not have to train on petabytes of data to produce a model; Amazon does this for us. You jus<a id="_idTextAnchor1111"/>t have to use the APIs that are available to transcribe audio files or video files; it supports a number of different languages and custom vocabulary too. Accuracy is the key, and through custom vocabulary, you can enhance it based on the desired domain <span class="No-Break">or industry:</span></p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B21197_08_10.jpg" alt="Figure 8.10 – Block diagram of Amazon Transcribe’s input and output" width="668" height="225"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1<a id="_idTextAnchor1112"/>0 – Block diagram of Amazon Transcribe’s input and output</p>
			<p>Some common uses of Amazon Trans<a id="_idTextAnchor1113"/>cribe include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Real-time audio streaming <span class="No-Break">and transcription</span></li>
				<li>Transcripting pre-recorded <span class="No-Break">audio files</span></li>
				<li>Enable text searching from a media file by combining AWS Elasticsearch and <span class="No-Break">Amazon Transcribe</span></li>
				<li>Performing sentiment analysis on recorded audio files for voice helpdesk (contact <span class="No-Break">center analytics)</span></li>
				<li>Channel <span class="No-Break">identification separation</span></li>
			</ul>
			<p>Next, you will explore the benefits of <a id="_idTextAnchor1114"/><span class="No-Break">Amazon Transcribe.</span></p>
			<h2 id="_idParaDest-196">Exploring th<a id="_idTextAnchor1115"/><a id="_idTextAnchor1116"/>e benefits of Amazon Transcribe</h2>
			<p>Let’s look at some of the benefits of using <span class="No-Break">Amazon Transcribe:</span></p>
			<ul>
				<li><strong class="bold">Content redaction</strong>: Customer privacy can be ensured by instructing Amazon Transcribe to identify an<a id="_idTextAnchor1117"/>d redact <strong class="bold">personally identifiable information (PII)</strong> from the language transcripts. You can filter u<a id="_idTextAnchor1118"/>nwanted words from your transcript by supplying a list of unwanted words with <strong class="source-inline">VocabularyFilterName</strong> and <strong class="source-inline">VocabularyFilterMethod</strong>, which are provided by the <strong class="source-inline">StratTranscriptionJob</strong> operation. For example, in financial organizations, this can be used to redact a <span class="No-Break">caller’s details.</span></li>
				<li><strong class="bold">Language identification</strong>: It can automatically identify the most used language in an audio file and generate transcriptions. If you have several audio files, then this service will help you classify them <span class="No-Break">by language.</span></li>
				<li><strong class="bold">Streaming transcription</strong>: You can send recorded audio files or live audio streams to Amazon Transcribe and output a stream of text in <span class="No-Break">real time.</span></li>
				<li><strong class="bold">Custom vocabulary or customized transcription</strong>: You can use your custom vocabulary list as per your custom needs to generate <span class="No-Break">accurate transcriptions.</span></li>
				<li><strong class="bold">Timestamp generation</strong>: If you want to build or add subtitles to your videos, then Amazon Transcribe can return the timestamp for each word or phrase from <span class="No-Break">the audio.</span></li>
				<li><strong class="bold">Cost effectiveness</strong>: Being a managed service, there is no <span class="No-Break">infrastructure cost.</span></li>
			</ul>
			<p>Now, let’s get hands-on with <span class="No-Break">Amazon Transcribe.</span></p>
			<h2 id="_idParaDest-197">Getting hand<a id="_idTextAnchor1119"/><a id="_idTextAnchor1120"/>s-on with Amazon Transcribe</h2>
			<p>In this section, you will build <a id="_idTextAnchor1121"/>a pipeline where you can integrate AWS Lambda with Amazon Transcribe to read an audio file stored in a folder in an S3 bucket, and then store the output JSON file in another S3 bucket. You will monitor the task’s progress in CloudWatch Logs too. You will use the <strong class="source-inline">start_transcription_job</strong> asynchronous function to start our job and you will constantly monitor the job through <strong class="source-inline">get_transcription_job</strong> until its status becomes <strong class="source-inline">COMPLETED</strong>. Let’s <span class="No-Break">get started:</span></p>
			<ol>
				<li>First, create an IAM role called <strong class="source-inline">transcribe-demo-role</strong> for the Lambda function to execute. Ensure that it can read and write from/to S3, use Amazon Transcribe, and print the output in CloudWatch logs. Add the following policies to the <span class="No-Break">IAM role:</span><ul><li><span class="No-Break"><strong class="source-inline">AmazonS3FullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">CloudWatchFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">AmazonTranscribeFullAccess</strong></span></li></ul></li>
				<li>Now, you will create a Lambda function called <strong class="source-inline">transcribe-lambda</strong> with our existing IAM role, <strong class="source-inline">transcribe-demo-role</strong>, and <span class="No-Break">save it.</span><p class="list-inset">Please make sure you change the default timeout to a higher value in the <strong class="bold">Basic</strong> <strong class="bold">settings</strong> section of your Lambda function. I have set it to 10 min and 20 sec to avoid timeout errors. You will be using an asynchronous API call called <strong class="source-inline">start_transcription_job</strong> to start the task and monitor it by using the <span class="No-Break"><strong class="source-inline">get_transcription_job</strong></span><span class="No-Break"> API.</span></p></li>
				<li>Paste the code available at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/blob/main/Chapter08/Amazon%20Transcribe%20Demo/lambda_function/lambda_function.py">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/blob/main/Chapter08/Amazon%20Transcribe%20Demo/lambda_function/lambda_function.py</a> and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Deploy</strong></span><span class="No-Break">.</span><p class="list-inset">This should give us the <span class="No-Break">following output:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B21197_08_11.jpg" alt="Figure 8.11 – The Basic settings section of our created lambda function" width="1110" height="471"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11<a id="_idTextAnchor1122"/> – The Basic settings section of our created lambda function</p>
			<ol>
				<li value="4">Next, you will be creating an S3 bucket <a id="_idTextAnchor1123"/>called <strong class="source-inline">transcribe-demo-101</strong> and a folder called <strong class="source-inline">input</strong>. Create an event by going to the <strong class="bold">Properties</strong> tab of the <strong class="bold">Create event notification</strong> section. Enter the <span class="No-Break">following details:</span><ul><li><span class="No-Break"><strong class="bold">Name</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">audio-event</strong></span></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object </strong><span class="No-Break"><strong class="source-inline">create events</strong></span></li><li><span class="No-Break"><strong class="bold">Prefix</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">input/</strong></span></li><li><strong class="bold">Destination</strong>: <span class="No-Break"><strong class="source-inline">Lambda Function</strong></span></li><li><span class="No-Break"><strong class="bold">Lambda</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">transcribe-lambda</strong></span></li></ul></li>
				<li>Upload the audio file in <strong class="source-inline">.mp4</strong> format to the <strong class="source-inline">input</strong> folder. This will trigger the Lambda function. As per the code, the output will be stored in the S3 bucket in JSON format, which you can then use to read the contents of <span class="No-Break">the file.</span></li>
				<li>Navigate to <strong class="source-inline">CloudWatch &gt; CloudWatch Logs &gt; Log groups &gt; aws/lambda/transcribe-lambda</strong>. Choose the latest strea<a id="_idTextAnchor1124"/>m from the list. It will look <span class="No-Break">as follows:</span></li>
			</ol>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B21197_08_12.jpg" alt="Figure 8.12 – The logs in a Log Stream for the specified log groups in the CloudWatch console" width="1101" height="510"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 <a id="_idTextAnchor1125"/>– The logs in a Log Stream for the specified log groups in the CloudWatch console</p>
			<ol>
				<li value="7">The output is saved to the S3 bucket in JSON format, as per the job name mentioned in your code (you can <a id="_idTextAnchor1126"/>use the S3 <strong class="source-inline">getObject</strong> API to download and <span class="No-Break">read it):</span><a id="_idTextAnchor1127"/></li>
			</ol>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B21197_08_13.jpg" alt="Figure 8.13 – The output JSON file in an S3 bucket" width="1467" height="698"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 –<a id="_idTextAnchor1128"/> The output JSON file in an S3 bucket</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">It is a best practice not to overprovision your function’s timeout settings. Always understand your code performance and set a function timeout accordingly. Overprovisioning a function timeout results in Lambda functions running longer, causing unexpected costs. If you are using asynchronous API calls in your Lambda function, then it is good to write them into SNS topics on success and trigger another Lambda function from that. If it needs human intervention, then it is suggested that you use AWS <span class="No-Break">Step Functions.</span></p>
			<p>In this section, you learned and applied Amazon Transcribe to convert speech into text. In the next section, you will learn about one of the most powerful AWS AI services you can use to get the maximum amount of insight from our <span class="No-Break">text data.</span></p>
			<h1 id="_idParaDest-198">Implementing na<a id="_idTextAnchor1129"/><a id="_idTextAnchor1130"/>tural language processing with Amazon Comprehend</h1>
			<p>This service helps you extract insights from u<a id="_idTextAnchor1131"/>nstructured text. Unstructured text information is growing exponentially. A few data source example<a id="_idTextAnchor1132"/>s are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Customer engagement</strong>: Call center, issue triage, customer surveys, and <span class="No-Break">product reviews</span></li>
				<li><strong class="bold">Business processes</strong>: Customer/vendor emails, product support messages, and operation <span class="No-Break">support feedback</span></li>
				<li><strong class="bold">Records and research</strong>: Whitepapers and <span class="No-Break">medical records</span></li>
				<li><strong class="bold">News and social media</strong>: Social media analytics, brand trends, and <span class="No-Break">correlated events</span></li>
			</ul>
			<p>Now, the question is, what can you do with this data? How can you analyze it and extract any value out of it? The answer is Amazon Comprehend, which is used to get insights from <span class="No-Break">unstructured data.</span></p>
			<p>Some common uses of Amazon Comprehend include <span class="No-Break">the fo<a id="_idTextAnchor1133"/>llowing:</span></p>
			<ul>
				<li>Information <span class="No-Break">management system</span></li>
				<li>More accurate search system on <span class="No-Break">organized topics</span></li>
				<li>Sentiment analysis <span class="No-Break">of users</span></li>
				<li>Support <span class="No-Break">ticket classification</span></li>
				<li>Language detection from a document and then translating it into English using <span class="No-Break">Amazon Translate</span></li>
				<li>Creating a system to label unstructured clinical data to assist in research and <span class="No-Break">analysis purposes</span></li>
				<li>Extracting topics from saved audio files of company meetings or <span class="No-Break">TV news</span></li>
			</ul>
			<p>Next, you’ll explore the benefits of <span class="No-Break">Amazon Compreh<a id="_idTextAnchor1134"/>end.</span></p>
			<h2 id="_idParaDest-199">Exploring the b<a id="_idTextAnchor1135"/><a id="_idTextAnchor1136"/>enefits of Amazon Comprehend</h2>
			<p>Some of the advantages of u<a id="_idTextAnchor1137"/>sing Comprehend can be seen in the <span class="No-Break">following image:</span></p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B21197_08_14.jpg" alt="Figure 8.14 – A block diagram showing Amazon Comprehend’s capabilities" width="838" height="420"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – <a id="_idTextAnchor1138"/>A block diagram showing Amazon Comprehend’s capabilities</p>
			<p>Let’s look at these in <span class="No-Break">more detail:</span></p>
			<ul>
				<li>It dete<a id="_idTextAnchor1139"/>cts the language of the text and extracts key phrases. Amazon Comprehend can be used for sentiment analysis and topic <span class="No-Break">modeling too.</span></li>
				<li>Amazon Comprehend Medical can be used to extract <span class="No-Break">medical information.</span></li>
				<li>You pay for what you use since this i<a id="_idTextAnchor1140"/>s a fully managed service; you do not have to pay for the infrastructure. You do not need to train, develop, and deploy your <span class="No-Break">own model.</span></li>
				<li>The topic modeling service works by extracting up to 100 topics. A topic is a keyword bucket so that you can see what is in the actual corpus <span class="No-Break">of documents.</span></li>
				<li>It is accurate, continuously trained, and easy <span class="No-Break">to use.</span></li>
			</ul>
			<p>Next, you’ll get hands-on with <span class="No-Break">Amazon Comprehend.</span></p>
			<h2 id="_idParaDest-200">Getting hands-on<a id="_idTextAnchor1141"/><a id="_idTextAnchor1142"/> with Amazon Comprehend</h2>
			<p>In this section, you will build a pi<a id="_idTextAnchor1143"/>peline where you can integrate AWS Lambda with Amazon Rekognition and Amazon Comprehend. You will then read an image file stored in an S3 bucket and detect the language of the text that has been extracted from the image. You will also use CloudWatch to print out the output. The following is a diagram of our <span class="No-Break">use case:</span></p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B21197_08_15.jpg" alt="Figure 8.15 – Architecture diagram of the required use case" width="721" height="291"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – A<a id="_idTextAnchor1144"/>rchitecture diagram of the required use case</p>
			<p>Let’s begin by creating an <span class="No-Break">IAM role:</span></p>
			<ol>
				<li>Navigate to the IAM <span class="No-Break">console page.</span></li>
				<li>Select <strong class="bold">Roles</strong> from the <span class="No-Break">left-hand menu.</span></li>
				<li>Select <span class="No-Break"><strong class="bold">Create role.</strong></span></li>
				<li>Select <strong class="bold">Lambda</strong> as the <span class="No-Break">trusted entity.</span></li>
				<li>Add the following <span class="No-Break">managed </span><span class="No-Break"><strong class="source-inline">policies</strong></span><span class="No-Break">:</span><ul><li><span class="No-Break"><strong class="source-inline">AmazonS3ReadOnlyAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">AmazonRekognitionFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">ComprehendFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">CloudWatchFullAccess</strong></span></li></ul></li>
				<li>Save the role <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">language-detection-from-image-role</strong></span><span class="No-Break">.</span></li>
				<li>Now, let’s create the Lambda function. Navigat<a id="_idTextAnchor1145"/>e to <strong class="source-inline">Lambda &gt; Functions &gt; </strong><span class="No-Break"><strong class="source-inline">Create Function</strong></span><span class="No-Break">.</span></li>
				<li>Name the <span class="No-Break">function </span><span class="No-Break"><strong class="source-inline">language-detection-from-image</strong></span><span class="No-Break">.</span></li>
				<li>Set the runtime to <span class="No-Break"><strong class="source-inline">Python 3.6</strong></span><span class="No-Break">.</span></li>
				<li>Use our existing role; that <span class="No-Break">is, </span><span class="No-Break"><strong class="source-inline">language-detection-from-image-role</strong></span><span class="No-Break">.</span></li>
				<li>Download the code from <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Transcribe%20Demo/lambda_function">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Transcribe%20Demo/lambda_function</a>, paste it into the function, and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Deploy</strong></span><span class="No-Break">.</span><p class="list-inset">This code will read the text from the image that you uploaded and detect the language of the text. You have used the <strong class="source-inline">detect_text</strong> API from Amazon Rekognition to detect text from an image and the <strong class="source-inline">batch_detect_dominant_language</strong> API from Amazon Comprehend to detect the language of <span class="No-Break">the text.</span></p></li>
				<li>Now, go to your AWS S3 console and create a bucket <span class="No-Break">called </span><span class="No-Break"><em class="italic">language-detection-image</em></span><span class="No-Break">.</span></li>
				<li>Create a folder called <strong class="source-inline">input-image</strong> (in this example, you will only upload <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">jpg</strong></span><span class="No-Break"> files).</span></li>
				<li>Navigate to <strong class="source-inline">Properties &gt; Events&gt; </strong><span class="No-Break"><strong class="source-inline">Add notification</strong></span><span class="No-Break">.</span></li>
				<li>Fill in the required fields in the <strong class="bold">Ev<a id="_idTextAnchor1146"/>ents</strong> section with the following information; then, click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break">:</span><ul><li><span class="No-Break"><strong class="bold">Name</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">image-upload-event</strong></span></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object </strong><span class="No-Break"><strong class="source-inline">create events</strong></span></li><li><span class="No-Break"><strong class="bold">Prefix</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">input-image/</strong></span></li><li><span class="No-Break"><strong class="bold">Suffix</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">.jpg</strong></span></li><li><strong class="bold">Send to</strong>: <span class="No-Break"><strong class="source-inline">Lambda Function</strong></span></li><li><span class="No-Break"><strong class="bold">Lambda</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">language-detection-from-image</strong></span></li></ul></li>
				<li>Navigate to <strong class="source-inline">Amazon S3&gt;language-detection-image&gt;input-image</strong>. Upload the <strong class="source-inline">sign-image.jpg</strong> image in the folder. (This file is available in this book’s GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Comprehend%20Demo/input_image"><span class="No-Break">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Comprehend%20Demo/input_image</span></a><span class="No-Break">).</span></li>
				<li>This file upload will trigger the Lambda function. You can monitor the logs from <strong class="source-inline">CloudWatch&gt; CloudWatch Logs&gt; Log </strong><span class="No-Break"><strong class="source-inline">groups&gt; /aws/lambda/language-detection-from-image</strong></span><span class="No-Break">.</span></li>
				<li>Click on the streams and select the latest one. The detected language is printed in the logs, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B21197_08_16.jpg" alt="Figure 8.16 – The logs in CloudWatch for verifying the output" width="1416" height="648"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Th<a id="_idTextAnchor1147"/>e logs in CloudWatch for verifying the output</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">It is suggested that you use batch operations such as <strong class="source-inline">BatchDetectSentiment</strong>or <strong class="source-inline">BatchDetectDominantLanguage</strong> in your production environment. This is because single API operations can cause API-level throttling. More details are available <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/comprehend/latest/dg/functionality.html"><span class="No-Break">https://docs.aws.amazon.com/comprehend/latest/dg/functionality.html</span></a><span class="No-Break">.</span></p>
			<p>In this section, you learned how to us<a id="_idTextAnchor1148"/>e Amazon Comprehend to detect the language of texts. The text is extracted into our Lambda function using Amazon Rekognition. In the next section, you will learn about translating the same text into English via <span class="No-Break">Amazon Translate.</span></p>
			<h1 id="_idParaDest-201">Translating docume<a id="_idTextAnchor1149"/><a id="_idTextAnchor1150"/>nts with Amazon Translate</h1>
			<p>Most of the time, people prefer to communi<a id="_idTextAnchor1151"/>cate in their own language, even on digital platforms. Amazon Translate is a text translation service. You can provide documents or strings of text in various languages and get it back in a different language. It uses pre-trained deep learning techniques, so you should not be worried about the models, nor how they are managed. You can make API requests and get <a id="_idTextAnchor1152"/>the <span class="No-Break">results back.</span></p>
			<p>Some common uses of Amazon Translate i<a id="_idTextAnchor1153"/>nclude <span class="No-Break">the following:</span></p>
			<ul>
				<li>If there is an organization-wide requirement to prepare documents in different languages, then Translate is the solution for converting one language <span class="No-Break">into many.</span></li>
				<li>Online chat applications can be translated in real time to provide a better <span class="No-Break">customer experience.</span></li>
				<li>To localize website content faster and more affordably into <span class="No-Break">more languages.</span></li>
				<li>Sentiment analysis can be applied to different languages once they have <span class="No-Break">been translated.</span></li>
				<li>To provide non-English language support for a news <span class="No-Break">publishing website.</span></li>
			</ul>
			<p>Next, you will explore the benefits of <span class="No-Break">Amazon Translate.</span></p>
			<h2 id="_idParaDest-202">Exploring the bene<a id="_idTextAnchor1154"/><a id="_idTextAnchor1155"/>fits of Amazon Translate</h2>
			<p>Some of the benefits of using Amazon Trans<a id="_idTextAnchor1156"/>late include <span class="No-Break">the following:</span></p>
			<ul>
				<li>It uses neural machine translation, which mimics the way the human <span class="No-Break">brain works.</span></li>
				<li>You do not need to maintain resources or infrastructures for the <span class="No-Break">Translation action.</span></li>
				<li>Produces high-quality results and maintains <span class="No-Break">their consistency.</span></li>
				<li>You can customize brand names and model names. Other unique terms too can get translated using the custom <span class="No-Break">terminology feature.</span></li>
				<li>Can be easily integrated with applications <span class="No-Break">through APIs.</span></li>
				<li>Amazon Translate scales itself when you need it to <span class="No-Break">do more.</span></li>
			</ul>
			<p>Next, you will get hands-on with <span class="No-Break">Amazon Translate.</span></p>
			<h2 id="_idParaDest-203">Getting hands-on w<a id="_idTextAnchor1157"/><a id="_idTextAnchor1158"/>ith Amazon Translate</h2>
			<p>In this section, you will build a product by integr<a id="_idTextAnchor1159"/>ating AWS Lambda with Amazon Rekognition, Amazon Comprehend, and Amazon Translate to read an image file stored in an S3 bucket. Then, you will detect the language of the text that has been extracted from the image so that you can translate it into English. You will also use CloudWatch to print the translated output. The following is a diagram of our <span class="No-Break">use case:</span></p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B21197_08_17.jpg" alt="Figure 8.17 – Architecture diagram of the required use case" width="1644" height="552"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Arc<a id="_idTextAnchor1160"/>hitecture diagram of the required use case</p>
			<p>Let’s start by creating an <span class="No-Break">IAM role:</span></p>
			<ol>
				<li>Navigate to the IAM <span class="No-Break">console page.</span></li>
				<li>Select <strong class="bold">Roles</strong> from the <span class="No-Break">left-hand menu.</span></li>
				<li>Select <span class="No-Break"><strong class="bold">Create role</strong></span><span class="No-Break">.</span></li>
				<li>Select <strong class="bold">Lambda</strong> as the <span class="No-Break">trusted entity.</span></li>
				<li>Add the following <span class="No-Break">managed policies:</span><ul><li><span class="No-Break"><strong class="source-inline">AmazonS3ReadOnlyAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">AmazonRekognitionFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">ComprehendFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">CloudWatchFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">TranslateFullAccess</strong></span></li></ul></li>
				<li>Save the role <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">language-translation-from-image</strong></span><span class="No-Break">.</span></li>
				<li>The next immediate step is to create a Lambda<a id="_idTextAnchor1161"/> function. Navigate to <strong class="source-inline">Lambda &gt; Functions &gt; </strong><span class="No-Break"><strong class="source-inline">Create Function</strong></span><span class="No-Break">.</span></li>
				<li>Name the <span class="No-Break">function </span><span class="No-Break"><strong class="source-inline">language-detection-from-image</strong></span><span class="No-Break">.</span></li>
				<li>Set the runtime to <span class="No-Break"><strong class="source-inline">Python 3.6</strong></span><span class="No-Break">.</span></li>
				<li>Use an existing role; that <span class="No-Break">is, </span><span class="No-Break"><strong class="source-inline">language-detection-from-image-role</strong></span><span class="No-Break">.</span></li>
				<li>Paste the code available at <a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/blob/main/Chapter08/Amazon%20Translate%20Demo/lambda_function/lambda_function.py">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/blob/main/Chapter08/Amazon%20Translate%20Demo/lambda_function/lambda_function.py</a> and click <strong class="bold">Deploy</strong>. You will use the <strong class="source-inline">translate_text</strong> API to translate the <span class="No-Break">input text.</span></li>
				<li>The next step is to create a bucket <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">language-translation-from-image</strong></span><span class="No-Break">.</span><p class="list-inset">Create a folder named <strong class="source-inline">image</strong>. Then, navigate to <strong class="source-inline">Properties &gt; Events&gt; </strong><span class="No-Break"><strong class="source-inline">Add notification</strong></span><span class="No-Break">.</span></p></li>
				<li>Fill in the required fields, as shown here, and click on Save (please make sure you select <strong class="source-inline">.jpg</strong> as the suffix; otherwise, it will trigger the Lambda function for any object <span class="No-Break">creation process):</span><ul><li><span class="No-Break"><strong class="bold">Name</strong></span><span class="No-Break">: </span><span class="No-Break"><em class="italic">translate-language-image</em></span></li><li><strong class="bold">Events</strong>: <strong class="source-inline">All object </strong><span class="No-Break"><strong class="source-inline">create events</strong></span></li><li><span class="No-Break"><strong class="bold">Prefix</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">image/</strong></span></li><li><span class="No-Break"><strong class="bold">Suffix</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">.jpg</strong></span></li><li><strong class="bold">Send to</strong>: <span class="No-Break"><strong class="source-inline">Lambda Function</strong></span></li><li><span class="No-Break"><strong class="bold">Lambda</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">language-translation-from-image</strong></span></li></ul></li>
				<li>Navigate to <strong class="source-inline">Amazon S3 &gt; language-detection-image &gt; input-image</strong>. Upload the <strong class="source-inline">sign-image.jpg</strong> image into the folder. This file is available in this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Translate%20Demo/input_image"><span class="No-Break">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Translate%20Demo/input_image</span></a><span class="No-Break">.</span></li>
				<li>Uploading this image will trigger the L<a id="_idTextAnchor1162"/>ambda function. You can monitor the logs by going to <strong class="source-inline">CloudWatch &gt; CloudWatch Logs &gt; Log groups &gt; /</strong><span class="No-Break"><strong class="source-inline">aws/lambda/language-translation-from-image</strong></span><span class="No-Break">.</span></li>
				<li>Click on the streams and select the latest one. It will look <span class="No-Break">as follows:</span></li>
			</ol>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B21197_08_18.jpg" alt="Figure 8.18 – The logs in CloudWatch for verifying the output" width="1319" height="413"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – The <a id="_idTextAnchor1163"/>logs in CloudWatch for verifying the output</p>
			<p>The translation is <span class="No-Break">as follows:</span></p>
			<pre class="console"><strong class="source-inline">Translation of the text from the Image :</strong></pre>
			<pre class="console"><strong class="source-inline">{'PREVENCION DEL COVID-19': 'PREVENTION OF COVID-19',</strong></pre>
			<pre class="console"><strong class="source-inline">'LAVATE LAS MANOS EVITA EL CONTACTO NO TE TOQUES oJOs,</strong></pre>
			<pre class="console"><strong class="source-inline">EVITA': 'WASHE HANDS AVOID CONTACT DO NOT TOUCH EYES',</strong></pre>
			<pre class="console"><strong class="source-inline">'60 SEGUNDOS CON CONTAGIADOS NARIZ O BOCA</strong></pre>
			<pre class="console"><strong class="source-inline">AGLOMERACIONES': '60 SECONDS WITH CONTAGIOUS NOSE OR</strong></pre>
			<pre class="console"><strong class="source-inline">MOUTH AGGLOMERATIONS', 'NO COMPARTAS NO VIAJES A MENOS</strong></pre>
			<pre class="console"><strong class="source-inline">SI TE PONES ENFERMO': "DON'T SHARE NOT TRAVEL UNLESS</strong></pre>
			<pre class="console"><strong class="source-inline">YOU GET SICK", 'CUBIERTOS NI COMIDA QUE SEA NECESARIO</strong></pre>
			<pre class="console"><strong class="source-inline">BUSCA AYUDA MEDICA': 'CUTLERY OR FOOD NEEDED SEEK</strong></pre>
			<pre class="console"><strong class="source-inline">MEDICAL HELP'}</strong></pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">For production use cases, it is recommended to use AWS Lambda with AWS Step Functions if you have dependent services or a chain <span class="No-Break">of services.</span></p>
			<p class="callout">Using the same S3 bucket to store input and output objects is not recommended. Output object creation in the same bucket may trigger recursive Lambda invocation. If you are using the same bucket, then you recommend that you use a prefix and suffix to trigger events. Similarly, you recommend using a prefix to store <span class="No-Break">output objects.</span></p>
			<p>In this section, you learned how to combine multi<a id="_idTextAnchor1164"/>ple services and chain their output to achieve a particular use case outcome. You learned how to integrate Amazon Rekognition to detect text in an image. The language can then be detected by using Amazon Comprehend. Then, you used the same input and translated it into English with the help of Amazon Translate. The translated output was then printed on CloudWatch logs for verification. In the next section, you will learn about Amazon Textract, which can be used to extract text from <span class="No-Break">a document.</span></p>
			<h1 id="_idParaDest-204">Extracting text from<a id="_idTextAnchor1165"/><a id="_idTextAnchor1166"/> documents with Amazon Textract</h1>
			<p>Manually extracting information from documents is slow, e<a id="_idTextAnchor1167"/>xpensive, and prone to errors. Traditional optical character recognition software needs a lot of customization, and it will still give erroneous output. To avoid such manual processes and errors, you should use <strong class="bold">Amazon Textract</strong>. Generally, you convert the document<a id="_idTextAnchor1168"/>s into images to detect bounding boxes around the texts in images. You then apply character recognition to read the text from it. Textract does all this for you, and also extracts text, tables, forms, and other data for you with minimal effort. If you get low-confidence results from Amazon Textract, then Amazon A2I is the <span class="No-Break">best solution.</span></p>
			<p>Textract reduces the manual effort of extracting text from millions of scanned document pages. Once the information has been captured, actions can be taken on the text, such as storing it in different data stores, analyzing sentiments, or searching for keywords. The following diagram shows how Amazon <span class="No-Break">Textract works:</span></p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B21197_08_19.jpg" alt="Figure 8.19 – Block diagram representation of Amazon Textract and how it stores its output" width="946" height="480"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 – Block<a id="_idTextAnchor1169"/> diagram representation of Amazon Textract and how it stores its output</p>
			<p>Some common uses of Amazon Textract incl<a id="_idTextAnchor1170"/>ude <span class="No-Break">the following:</span></p>
			<ul>
				<li>Documenting processing workflows to extract tables <span class="No-Break">or forms</span></li>
				<li>Creating search indexes from documents using <span class="No-Break">Amazon Elasticsearch</span></li>
				<li>Redacting personally identifiable information in a workflow; Textract identifies data types and form <span class="No-Break">labels automatically</span></li>
			</ul>
			<p>Next, you will explore the benefits of <span class="No-Break">Amazon Textract.</span></p>
			<h2 id="_idParaDest-205">Exploring the benefit<a id="_idTextAnchor1171"/><a id="_idTextAnchor1172"/>s of Amazon Textract</h2>
			<p>There are several reasons to <span class="No-Break">u<a id="_idTextAnchor1173"/>se Textract:</span></p>
			<ul>
				<li>Zero <span class="No-Break">infrastructure cost</span></li>
				<li>Fully managed service (reduced development and <span class="No-Break">management overhead)</span></li>
				<li>Helps you extract both structured and <span class="No-Break">unstructured data</span></li>
				<li>Handwritten reviews can <span class="No-Break">be analyzed</span></li>
				<li>Amazon Textract performs better than OCR apps, which use a flat bag <span class="No-Break">of words</span></li>
				<li>Next, you will get hands-on with <span class="No-Break">Amazon Textract.</span></li>
			</ul>
			<h2 id="_idParaDest-206">Getting hands-on with<a id="_idTextAnchor1174"/><a id="_idTextAnchor1175"/> Amazon Textract</h2>
			<p>In this section, you will use the Amazon Textract <a id="_idTextAnchor1176"/>API to read an image file from our S3 bucket and print the FORM details on Cloudwatch. The same can be stored in S3 in your desired format for further use or can be stored in DynamoDB as a key-value pair. Let’s <span class="No-Break">get started:</span></p>
			<ol>
				<li>First, create an IAM role called <strong class="source-inline">textract-use-case-role</strong> with the following policies. This will allow the Lambda function to execute so that it can read from S3, use Amazon Textract, and print the output in <span class="No-Break">CloudWatch logs:</span><ul><li><span class="No-Break"><strong class="source-inline">CloudWatchFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">AmazonTextractFullAccess</strong></span></li><li><span class="No-Break"><strong class="source-inline">AmazonS3ReadOnlyAccess</strong></span></li></ul></li>
				<li>Let’s create an S3 bucket called <strong class="source-inline">textract-document-analysis</strong> and upload the <strong class="source-inline">receipt.png</strong> image file. This will be used to contain the FORM details that will be extracted. The image file is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Textract%20Demo/input_doc"><span class="No-Break">https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide-Second-Edition/tree/main/Chapter08/Amazon%20Textract%20Demo/input_doc</span></a><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B21197_08_20.jpg" alt="Figure 8.20 – An S3 bucket with an image (.png) file uploaded to the input folder" width="1228" height="660"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.20 – An S3 <a id="_idTextAnchor1177"/>bucket with an image (.png) file uploaded to the input folder</p>
			<ol>
				<li value="3">The next step is to create a Lambda<a id="_idTextAnchor1178"/> function called <strong class="source-inline">read-scanned-doc</strong>, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.21</em>, with an existing execution role <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">textract-use-case-role</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B21197_08_21.jpg" alt="Figure 8.21 – The AWS Lambda Create function dialog" width="1108" height="736"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.21 – The AWS<a id="_idTextAnchor1179"/> Lambda Create function dialog</p>
			<ol>
				<li value="4">Once the function has been created, paste the following code and<a id="_idTextAnchor1180"/> deploy it. Scroll down to <strong class="bold">Basic Settings</strong> to change the default timeout to a higher value (40 seconds) to prevent timeout errors. You have used the <strong class="source-inline">analyze_document</strong> API from Amazon Textract to get the <strong class="source-inline">Table and Form</strong> details via the <strong class="source-inline">FeatureTypes</strong> parameter of <span class="No-Break">the API:</span><pre class="source-code"><strong class="source-inline">import boto3</strong></pre><pre class="source-code"><strong class="source-inline">import time</strong></pre><pre class="source-code"><strong class="source-inline">from trp import Document</strong></pre><pre class="source-code"><strong class="source-inline">textract_client=boto3.client('textract')</strong></pre><pre class="source-code"><strong class="source-inline">def lambda_handler(event, context):</strong></pre><pre class="source-code"><strong class="source-inline">    print("- - - Amazon Textract Demo - - -")</strong></pre><pre class="source-code"><strong class="source-inline">    # read the bucket name from the event</strong></pre><pre class="source-code"><strong class="source-inline">    name_of_the_bucket=event['Records'][0]['s3']['bucket'] ['name']</strong></pre><pre class="source-code"><strong class="source-inline">    # read the object from the event</strong></pre><pre class="source-code"><strong class="source-inline">    name_of_the_doc=event['Records'][0]['s3']['object']['key']</strong></pre><pre class="source-code"><strong class="source-inline">    print(name_of_the_bucket)</strong></pre><pre class="source-code"><strong class="source-inline">    print(name_of_the_doc)</strong></pre><pre class="source-code"><strong class="source-inline">    response =</strong></pre><pre class="source-code"><strong class="source-inline">textract_client.analyze_document(Document={'S3Object':</strong></pre><pre class="source-code"><strong class="source-inline">{'Bucket': name_of_the_bucket,'Name':</strong></pre><pre class="source-code"><strong class="source-inline">name_of_the_doc}},FeatureTypes=["TABLES","FORMS"])</strong></pre><pre class="source-code"><strong class="source-inline">    print(str(response))</strong></pre><pre class="source-code"><strong class="source-inline">    doc=Document(response)</strong></pre><pre class="source-code"><strong class="source-inline">    for page in doc.pages:</strong></pre><pre class="source-code"><strong class="source-inline">        # Print tables</strong></pre><pre class="source-code"><strong class="source-inline">        for table in page.tables:</strong></pre><pre class="source-code"><strong class="source-inline">            for r, row in enumerate(table.rows):</strong></pre><pre class="source-code"><strong class="source-inline">                for c, cell in enumerate(row.cells):</strong></pre><pre class="source-code"><strong class="source-inline">                    print("Table[{}][{}] =</strong></pre><pre class="source-code"><strong class="source-inline">{}".format(r, c, cell.text))</strong></pre><pre class="source-code"><strong class="source-inline">    for page in doc.pages:</strong></pre><pre class="source-code"><strong class="source-inline">        # Print fields</strong></pre><pre class="source-code"><strong class="source-inline">        print("Fields:")</strong></pre><pre class="source-code"><strong class="source-inline">        for field in page.form.fields:</strong></pre><pre class="source-code"><strong class="source-inline">            print("Key: {}, Value:</strong></pre><pre class="source-code"><strong class="source-inline">{}".format(field.key, field.value))</strong></pre><p class="list-inset">Unlike the previous examples, you will creat<a id="_idTextAnchor1181"/>e a test configuration to run <span class="No-Break">our code.</span></p></li>
				<li>Click on the dropdown left of the <span class="No-Break"><strong class="bold">Test</strong></span><span class="No-Break"> button.</span></li>
				<li>Select <strong class="bold">Configure test events</strong> and choose <strong class="bold">Create new </strong><span class="No-Break"><strong class="bold">test event</strong></span><span class="No-Break">.</span></li>
				<li>Select <strong class="bold">Amazon S3 Put</strong> from the <strong class="bold">Event </strong><span class="No-Break"><strong class="bold">template</strong></span><span class="No-Break"> dropdown.</span></li>
				<li>In the JSON body, change the highlighted values as per our bucket name and key, as <span class="No-Break">shown here:</span></li>
			</ol>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B21197_08_22.jpg" alt="Figure 8.22 – The Event template for testing the Lambda function" width="1484" height="700"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.22 – The Even<a id="_idTextAnchor1182"/>t template for testing the Lambda function</p>
			<ol>
				<li value="9">In the <strong class="bold">Event name</strong> field, name the test <span class="No-Break">configuration</span><span class="No-Break"><em class="italic"> <a id="_idTextAnchor1183"/></em></span><span class="No-Break"><strong class="source-inline">TextractDemo</strong></span><span class="No-Break">.</span></li>
				<li><span class="No-Break">Click </span><span class="No-Break"><strong class="bold">Save</strong></span><span class="No-Break">.</span></li>
				<li>Select your test configuration (<strong class="source-inline">TextractDemo</strong>) and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Test</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B21197_08_23.jpg" alt="Figure 8.23 – Selecting the test configuration before running your test" width="1178" height="322"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.23 – Selecting<a id="_idTextAnchor1184"/> the test configuration before running your test</p>
			<ol>
				<li value="12">This will trigger the Lambda function. You can monitor the logs from <strong class="source-inline">CloudWatch &gt; CloudWatch Logs &gt; Log groups &gt; /</strong><span class="No-Break"><strong class="source-inline">aws/lambda/ read-scanned-doc</strong></span><span class="No-Break">.</span></li>
				<li>Click on the streams and select the l<a id="_idTextAnchor1185"/>atest one. It will look as follows; the key-value pairs can be seen in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.24</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B21197_08_24.jpg" alt="Figure 8.24 – The logs in CloudWatch for verifying the output" width="1136" height="1002"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.24 – The logs i<a id="_idTextAnchor1186"/>n CloudWatch for verifying the output</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The most scalable and cost-effective way to generate S3 PJUT events for asynchronously invocating downstream AI workflows via Lambda is to generate an AWS pre-signed URL, and then provide it to your mobile or web application users. Many users can be served at the same time via this approach, and it may increase performance <span class="No-Break">and throughput.</span></p>
			<p class="callout">Considering the same region for your AWS <a id="_idTextAnchor1187"/>AI services and S3 bucket may improve performance and reduce network latency. AWS VPC endpoints can leverage enhanced security without using the public internet. You can store the AWS AI results in an AWS S3 bucket and encrypt the rest to attain <span class="No-Break">better security.</span></p>
			<p>In this section, you learned how to extract text from a scanned document and print the form data out of it. Unlike the other sections, you used the testing feature of a Lambda function by creating a test configuration that<a id="_idTextAnchor1188"/> includes an event template. In the next section, you will learn about creating a chatbot for organizations and learn how to <span class="No-Break">use it.</span></p>
			<h1 id="_idParaDest-207">Creating chatbots on Amazo<a id="_idTextAnchor1189"/><a id="_idTextAnchor1190"/>n Lex</h1>
			<p>Most of the features that are available in Alexa are<a id="_idTextAnchor1191"/> powered by <strong class="bold">Amazon Lex</strong>. You can easily build a chatbot using Amazon Lex. It uses natural language understanding and automat<a id="_idTextAnchor1192"/>ic speech recognition behind the scenes. An Amazon Lex bot can be created either from the console or via APIs. Its basic requirements are shown in the <span class="No-Break">upcoming diagram.</span></p>
			<p>Some common uses of Amazon Lex include <span class="No-Break">the fo<a id="_idTextAnchor1193"/>llowing:</span></p>
			<ul>
				<li>Apps that both listen and take input <span class="No-Break">as text</span></li>
				<li><span class="No-Break">Chatbots</span></li>
				<li>Conversational AI products to provide a better customer and <span class="No-Break">sales experience</span></li>
				<li>Custom business bots for assistance through AWS <span class="No-Break">Lambda functions</span></li>
				<li>Voice assistants for your call center, which can speak to a user, schedule a meeting, or request the details of <span class="No-Break">your account</span></li>
				<li>By integrating with Amazon Cognito, a few aspects such as user management, authentication, and sync across all your devices can <span class="No-Break">be controlled</span></li>
			</ul>
			<p>Next, you will explore the benefits of <span class="No-Break">Amazon Lex.</span></p>
			<h2 id="_idParaDest-208">Exploring the benefits of <a id="_idTextAnchor1194"/><a id="_idTextAnchor1195"/>Amazon Lex</h2>
			<p>Some reasons for using Lex include <span class="No-Break">the fol<a id="_idTextAnchor1196"/>lowing:</span></p>
			<ul>
				<li>Chatbots can be directly built and tested from the AWS Management Console. These chatbots can be easily integrated into Facebook Messenger, Slack, and Twilio SMS via its rich <span class="No-Break">formatting capabilities.</span></li>
				<li>Conversation logs can be stored in Amazon CloudWatch for further analysis. You can use them to monitor your bot and derive insights to improve your <span class="No-Break">user experience.</span></li>
				<li>Amazon Lex can be integrated into other AWS services such as Amazon Cognito, AWS Lambda, Amazon DynamoDB, Amazon CloudWatch, and AWS Mobile Hub to leverage application security, monitoring, user authentication, business logic, storage, and mobile app development in <span class="No-Break">AWS platforms.</span></li>
				<li>Amazon Lex chatbots can be integrated into you<a id="_idTextAnchor1197"/>r custom web applications too. You just need to build a chatbot widget and integrate it into <span class="No-Break">your UI.</span></li>
			</ul>
			<p>Next, you’ll get hands-on with <span class="No-Break">Amazon Lex.</span></p>
			<h2 id="_idParaDest-209">Getting hands-on with Amaz<a id="_idTextAnchor1198"/><a id="_idTextAnchor1199"/>on Lex</h2>
			<p>Let’s <span class="No-Break">get started:</span></p>
			<ol>
				<li>Log in <span class="No-Break">to </span><a href="https://console.aws.amazon.com/lex/"><span class="No-Break">https:/<span id="_idTextAnchor1200"/>/console.aws.amazon.com/lex/</span></a><span class="No-Break">.</span></li>
				<li>Click on <strong class="bold">Get Started</strong> and select <span class="No-Break"><strong class="bold">Custom bot</strong></span><span class="No-Break">.</span></li>
				<li>Fill in the following details and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Create</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B21197_08_25.jpg" alt="Figure 8.25 – The Create dialog of Amazon Lex" width="1185" height="938"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.25 – The Create <a id="_idTextAnchor1201"/>dialog of Amazon Lex</p>
			<ol>
				<li value="4">Click on <strong class="bold">Create Intent</strong>. A dialog will appear. Select <span class="No-Break"><strong class="bold">Create Intent</strong></span><span class="No-Break">.</span></li>
				<li>Name the new intent <strong class="source-inline">MovieIntent</strong> and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Add</strong></span><span class="No-Break">.</span></li>
				<li>Go to the <strong class="bold">Slots</strong> section and use the <span class="No-Break">following details:</span><a id="_idTextAnchor1202"/><ul><li><span class="No-Break">Name: </span><span class="No-Break"><strong class="source-inline">movie_type</strong></span></li><li>Slot <span class="No-Break">type: </span><span class="No-Break"><strong class="source-inline">AMAZON.Genre</strong></span></li><li>Prompt: <strong class="source-inline">Which movie do </strong><span class="No-Break"><strong class="source-inline">you like?</strong></span></li></ul></li>
				<li>Click on the <strong class="bold">+</strong> <span class="No-Break">in </span><span class="No-Break"><strong class="bold">Settings</strong></span><span class="No-Break">.</span><p class="list-inset">Some sample utterances can be seen in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.26</em>. In this example, <strong class="source-inline">movie_type</strong> is <span class="No-Break">my variable:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B21197_08_26.jpg" alt="Figure 8.26 – The Sample utterances section" width="1179" height="845"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.26 – The Sample u<a id="_idTextAnchor1203"/>tterances section</p>
			<ol>
				<li value="8">Scroll down to the <strong class="bold">Response</strong> section to add <span class="No-Break">a message:</span></li>
			</ol>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B21197_08_27.jpg" alt="Figure 8.27 – The Response section of Amazon Lex" width="1272" height="644"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.27<a id="_idTextAnchor1204"/> – The Response <a id="_idTextAnchor1205"/>section of Amazon Lex</p>
			<ol>
				<li value="9">Scroll down to <strong class="bold">Save Intent</strong> and click on <strong class="bold">Build</strong>. Upon successf<a id="_idTextAnchor1206"/>ully building the prompt, the following success message <span class="No-Break">will appear:</span></li>
			</ol>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B21197_08_28.jpg" alt="Figure 8.28 – The Response section of Amazon Lex" width="1140" height="633"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.28 – The Response s<a id="_idTextAnchor1207"/>ection of Amazon Lex</p>
			<ol>
				<li value="10">Now, you can test your bot, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.29</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B21197_08_29.jpg" alt="Figure 8.29 – The Test bot dialog" width="1135" height="823"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.29 – The Test bot di<a id="_idTextAnchor1208"/>alog</p>
			<p>In the next section, you will learn about Amazon Forecast and learn how to use it for different <span class="No-Break">use cases.</span></p>
			<h1 id="_idParaDest-210"><a id="_idTextAnchor1209"/>Amazon Forecast</h1>
			<p>Amazon Forecast is a powerful service that enables you to build highly accurate time-series forecasting models without the need for deep expertise in machine learning. Whether you are predicting sales, demand for inventory, or any time-dependent metric, Amazon Forecast simplifies the process, making it accessible to a <span class="No-Break">broader audience.</span></p>
			<p>Amazon Forecast is designed to tackle a variety of forecasting <span class="No-Break">challenges, including:</span></p>
			<ul>
				<li><strong class="bold">Demand forecasting</strong>: Predict future demand for products or services based on historical data, helping optimize inventory and supply <span class="No-Break">chain management.</span></li>
				<li><strong class="bold">Financial planning</strong>: Forecast financial metrics, such as revenue and expenses, aiding in budgeting and <span class="No-Break">financial decision-making.</span></li>
				<li><strong class="bold">Resource planning</strong>: Efficiently plan resources like workforce scheduling based on predicted <span class="No-Break">demand patterns.</span></li>
				<li><strong class="bold">Traffic and user engagement</strong>: Predict website or application traffic, enhancing resource allocation and <span class="No-Break">user experience.</span></li>
			</ul>
			<p>Next, you will explore the benefits of <span class="No-Break">Amazon Lex.</span></p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor1210"/>Exploring the benefits of Amazon Forecast</h2>
			<p>Some reasons for using Amazon Forecast are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Ease of use</strong>: Amazon Forecast abstracts the complexity of building accurate forecasting models. With just a few clicks, you can create, train, and deploy models without deep machine <span class="No-Break">learning expertise.</span></li>
				<li><strong class="bold">Automated machine learning</strong>: Amazon Forecast employs advanced machine learning techniques, automating the selection of algorithms and hyperparameter tuning to deliver the best <span class="No-Break">possible model.</span></li>
				<li><strong class="bold">Forecast backtesting</strong>: Enhance the reliability of your forecasts through backtesting. Amazon Forecast enables you to assess the accuracy of your models by comparing predictions against historical data. This iterative process helps fine-tune your models, adjusting hyperparameters and algorithms to achieve optimal <span class="No-Break">forecasting performance.</span></li>
				<li><strong class="bold">Scalability</strong>: Amazon Forecast seamlessly scales with your data, ensuring accurate predictions even with <span class="No-Break">vast datasets.</span></li>
				<li><strong class="bold">Integration with AWS</strong>: Leverage the power of integration with other AWS services like Amazon S3, AWS Lambda, and Amazon CloudWatch to create end-to-end forecasting solutions. Easily integrate Amazon Forecast into your existing applications and workflows, ensuring a seamless <span class="No-Break">forecasting experience.</span></li>
				<li><strong class="bold">Accuracy and precision</strong>: Amazon Forecast utilizes cutting-edge forecasting algorithms to deliver accurate and precise predictions, minimizing errors in <span class="No-Break">your forecasts.</span></li>
				<li><strong class="bold">Cost-effective</strong>: Pay only for what you use. The pay-as-you-go pricing model ensures cost-effectiveness, especially for businesses with varying <span class="No-Break">forecasting needs.</span></li>
				<li><strong class="bold">Customization</strong>: Tailor forecasting models to your specific business needs, accommodating various <span class="No-Break">forecasting scenarios.</span></li>
			</ul>
			<p>Next, you’ll get hands-on with <span class="No-Break">Amazon Lex.</span></p>
			<h2 id="_idParaDest-212"><a id="_idTextAnchor1211"/>Sales Forecasting Model with Amazon Forecast</h2>
			<p>Let’s dive into a hands-on example of using Amazon Forecast to build a sales forecasting model. In this example, you’ll predict future sales based on <span class="No-Break">historical data.</span></p>
			<p><strong class="bold">Set up your dataset</strong>: Prepare a dataset containing historical sales data, ensuring it includes relevant timestamps and corresponding <span class="No-Break">sales figures.</span></p>
			<p><strong class="bold">Create a dataset group</strong>: Use the Amazon Forecast console or API to create a dataset group, grouping related datasets <span class="No-Break">for forecasting.</span></p>
			<p><strong class="bold">Import your data</strong>: Upload your historical sales dataset to Amazon Forecast, allowing the service to learn patterns from the <span class="No-Break">provided data.</span></p>
			<p><strong class="bold">Train your model</strong>: Initiate model training using the Forecast console or API. Amazon Forecast will automatically select suitable algorithms and <span class="No-Break">optimize hyperparameters.</span></p>
			<p><strong class="bold">Generate forecasts</strong>: Once the model is trained, generate forecasts for future sales based on the patterns identified in your <span class="No-Break">historical data.</span></p>
			<p>By leveraging advanced features and implementing optimization strategies, you can elevate your Amazon Forecast experience. The flexibility and adaptability of the service allow you to tailor forecasting solutions to the specific needs of your business. For example, you can improve the precision of your forecasts by integrating external variables. Amazon Forecast allows you to include additional information, such as promotions, holidays, or economic indicators, that might impact the time series you are forecasting. By considering these external factors, your models can adapt to changing circumstances and provide more <span class="No-Break">nuanced predictions.</span></p>
			<h1 id="_idParaDest-213"><a id="_idTextAnchor1212"/>Summary</h1>
			<p>In this chapter, you lea<a id="_idTextAnchor1213"/><a id="_idTextAnchor1214"/>rned about a few of the AWS AI services that can be used to solve various problems. You used the Amazon Rekognition service, which detects objects and faces (including celebrity faces), and can also extract text from images. For text to speech, you used Amazon Polly, while for speech to text, you used Amazon Transcribe. Toward the end of this chapter, you built a chatbot in Amazon Lex and learned the usage and benefits of <span class="No-Break">Amazon Forecast.</span></p>
			<p>For language detection and translation in an image, you used Amazon Rekognition, Amazon Comprehend, and Amazon Translate. You learned how to combine all of them into one Lambda function to solve <span class="No-Break">our problem.</span></p>
			<p>For the certification exam, you do not need to remember all the APIs you used in this chapter. There may be questions on a few of the best practices that you learned or on the names of services that solve a specific problem. It is always good to practice using these AWS AI services as it will enhance your <span class="No-Break">architecting skills.</span></p>
			<p>In the next chapter, you will learn about data preparation and transformation, which is the most important aspect of <span class="No-Break">machine learning.</span></p>
			<h1 id="_idParaDest-214"><a id="_idTextAnchor1215"/>Exam Readiness Drill – Chapter Review Questions</h1>
			<p>Apart from a solid understanding of key concepts, being able to think quickly under time pressure is a skill that will help you ace your certification exam. That is why working on these skills early on in your learning journey <span class="No-Break">is key.</span></p>
			<p>Chapter review questions are designed to improve your test-taking skills progressively with each chapter you learn and review your understanding of key concepts in the chapter at the same time. You’ll find these at the end of <span class="No-Break">each chapter.</span></p>
			<p class="callout-heading">How To Access These Resources</p>
			<p class="callout">To learn how to access these resources, head over to the chapter titled <a href="B21197_11.xhtml#_idTextAnchor1477"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">Accessing the Online </em><span class="No-Break"><em class="italic">Practice Resources</em></span><span class="No-Break">.</span></p>
			<p>To open the Chapter Review Questions for this chapter, perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li>Click the link – <a href="https://packt.link/MLSC01E2_CH08"><span class="No-Break">https://packt.link/MLSC01E2_CH08</span></a><span class="No-Break">.</span><p class="list-inset">Alternatively, you can scan the following <strong class="bold">QR code</strong> (<span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.30</em></span><span class="No-Break">):</span></p></li>
			</ol>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B21197_08_30.jpg" alt="Figure 8.30 – QR code that opens Chapter Review Questions for logged-in users" width="550" height="150"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.30 – QR code that opens Chapter Review Questions for logged-in users</p>
			<ol>
				<li value="2">Once you log in, you’ll see a page similar to the one shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.31</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B21197_08_31.jpg" alt="Figure 8.31 – Chapter Review Questions for Chapter 8" width="1400" height="785"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.31 – Chapter Review Questions for Chapter 8</p>
			<ol>
				<li value="3">Once ready, start the following practice drills, re-attempting the quiz <span class="No-Break">multiple times.</span></li>
			</ol>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor1216"/>Exam Readiness Drill</h2>
			<p>For the first three attempts, don’t worry about the <span class="No-Break">time limit.</span></p>
			<h3 id="_idParaDest-216"><a id="_idTextAnchor1217"/>ATTEMPT 1</h3>
			<p>The first time, aim for at least <strong class="bold">40%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix your <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-217"><a id="_idTextAnchor1218"/>ATTEMPT 2</h3>
			<p>The second time, aim for at least <strong class="bold">60%</strong>. Look at the answers you got wrong and read the relevant sections in the chapter again to fix any remaining <span class="No-Break">learning gaps.</span></p>
			<h3 id="_idParaDest-218"><a id="_idTextAnchor1219"/>ATTEMPT 3</h3>
			<p>The third time, aim for at least <strong class="bold">75%</strong>. Once you score 75% or more, you start working on <span class="No-Break">your timing.</span></p>
			<p class="callout-heading">Tip</p>
			<p class="callout">You may take more than <strong class="bold">three</strong> attempts to reach 75%. That’s okay. Just review the relevant sections in the chapter till you <span class="No-Break">get there.</span></p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor1220"/>Working On Timing</h1>
			<p>Target: Your aim is to keep the score the same while trying to answer these questions as quickly as possible. Here’s an example of how your next attempts should <span class="No-Break">look like:</span></p>
			<table id="table001-7" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Attempt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Score</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Time Taken</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 5</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">77%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>21 mins <span class="No-Break">30 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 6</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">78%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>18 mins <span class="No-Break">34 seconds</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Attempt 7</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">76%</span></p>
						</td>
						<td class="No-Table-Style">
							<p>14 mins <span class="No-Break">44 seconds</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – Sample timing practice drills on the online platform</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The time limits shown in the above table are just examples. Set your own time limits with each attempt based on the time limit of the quiz on <span class="No-Break">the website.</span></p>
			<p>With each new attempt, your score should stay above <strong class="bold">75%</strong> while your “time taken” to complete should “decrease”. Repeat as many attempts as you want till you feel confident dealing with the <span class="No-Break">time pressure.</span></p>
		</div>
	</div>
</div>
</body></html>