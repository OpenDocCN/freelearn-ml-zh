- en: Chapter 5. Sentiment Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have explored some really cool applications in the analytics domain.
    In this chapter, we will explore the famous Natural Language Processing (NLP)
    technique, which you may have already guessed because of the name of the chapter.
    Absolutely right; we will build a sentiment analysis-based application. In general,
    everyone is familiar with sentiment analysis-based applications. If you aren't,
    then don't worry. We will discuss and understand all the necessary details.
  prefs: []
  type: TYPE_NORMAL
- en: First of all, I want to give you a basic idea about sentiment analysis. I will
    provide an example so it will be easy for you to understand. Regardless of where
    we live, we all watch movies. Nowadays, we read reviews or others' opinions on
    various social media platforms. After that, if a majority of the opinions about
    the movie are good, then we watch that movie. If the opinions are not impressive,
    we might not watch the movie. So during this entire process, our mind analyzes
    these opinions and categorizes them into either positive opinions, negative opinions,
    or neutral opinions. In this chapter, we will be performing the same kind of analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let me introduce the formal definition of sentiment analysis. Sentiment analysis
    is a technique where we consider a sentence, paragraph, document, or any information
    that is in the form of a natural language and determine whether that text's emotional
    tone is positive, negative, or neutral. We will be applying machine learning and
    Deep Learning to build a sentiment analysis application.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the problem statement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building training and testing datasets for the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering for the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the Machine Learning (ML) algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems with the existing approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to optimize the existing approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding key concepts for optimizing the approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding problems with the revised approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Best approach
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing problem statements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are living in a competitive world. Before buying any product and investing
    our time or money in anything, we try to find out what others think about that
    product(s) or service(s). We try to analyze their reviews or opinions. If we find
    them positive and trustworthy, then we buy the product and invest our money or
    time in that particular service. On the other hand, if we find these opinions
    or reviews negative, then we might not buy the product and not invest our money
    or time in that particular service. In the current era of the internet, it is
    easy to find reviews on social media platforms, blogs, news sources, and so on.
    This activity of analyzing reviews will be useful for consumers as well as makers
    of products or service providers. This is because, based on the reviews of their
    customer, they can change their product effectively, providing more satisfaction
    to their customers and make a good profit from that product or service. I have
    already given you the formal definition of sentiment analysis, so I'm not going
    to bore you with it again. Let's try to understand what the main focus of this
    chapter will be.
  prefs: []
  type: TYPE_NORMAL
- en: We will be developing a sentiment analysis application for movie reviews. During
    training, we will consider labels associated with each of the movie reviews so
    that we can train our machine learning algorithm based on the given labels. After
    training, when we pass any unseen movie reviews, then our trained machine learning
    algorithm will predict the sentiment, which means whether the provided movie review
    indicates a positive sentiment or a negative sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: We will be considering an IMDb (Internet Movie Database) movie review dataset
    to develop sentiment analysis for movie reviews. We will look at the details regarding
    the dataset in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look into our dataset. We have considered an IMDb
    dataset, which you can download at: [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).
    After clicking on this link, you can see that there is a link provided on the
    page. This link is titled Large Movie Review Dataset v1.0; we need to click on
    it. This way, we can download the IMDb dataset. Once you have downloaded the dataset,
    you need to extract the .tar.gz file. Once you extract the `.tar.gz` file, you
    can see that there are two folders inside the extracted folder and some other
    files. Let''s look at each of them in the following section.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the content of the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After extracting the dataset file, we''ll see that there are some folders and
    files inside it. We will be discussing all of the content''s meaning and what
    we will be using for our training purposes. This dataset has two folders and three
    files:'
  prefs: []
  type: TYPE_NORMAL
- en: train folder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: test folder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imdb.vocab` file'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imdbEr.txt`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: README
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train folder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This folder contains data for training. Inside this folder, there are two main
    folders. The `pos` folder contains positive movie reviews and the `neg` folder
    contains negative movie reviews. Inside the `pos` folder, there are 12,500 positive
    movie reviews. Inside the `neg` folder, there are 12,500 negative movie reviews.
    So in total, we have 25,000 movie reviews; by using them, we will train our Machine
    Learning (ML) model. For testing purposes, we can use movie reviews provided inside
    the `unsup` folder. These movie reviews are unlabeled, so we can use them for
    testing purposes or divide our labeled data into training and testing groups so
    it will be easy for us to find out how our trained ML model works.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other files inside the train folder but we are not going to use them.
    Those files carry data that already has tokenized bag-of-word (BOW) features.
    In order to get a clear picture of the folder structure, you can refer to the
    code snippet provided in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Train folder](img/B08394_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Folder structure of the train folder'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to explore the dataset in more detail, then you can refer to the
    documentation provided at: [http://www.paddlepaddle.org/docs/0.10.0/documentation/en/tutorials/sentiment_analysis/index_en.html](http://www.paddlepaddle.org/docs/0.10.0/documentation/en/tutorials/sentiment_analysis/index_en.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Test folder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This folder contains data for testing. Inside this folder, there are `pos`
    and `neg` folders, which contain positive and negative movie reviews, respectively.
    Each of the folders contains 12,500 movie reviews, so in total, we have 25,000
    movie reviews for testing. These movie reviews are labeled one so we can use this
    dataset to test our trained model. There are other `BOW` files and `url` files,
    which we will not be using. You can see the folder structure of the test folder
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Test folder](img/B08394_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Folder structure of the test folder'
  prefs: []
  type: TYPE_NORMAL
- en: imdb.vocab file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This file contains the unique words used in all movie reviews, so it is the
    vocabulary file for the `IMDb` dataset. If you open this file, then you can see
    words and observe that all of them are unique. You can see the contents of this
    file in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![imdb.vocab file](img/B08394_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.3: Contents of the imdb.vocab file'
  prefs: []
  type: TYPE_NORMAL
- en: imdbEr.txt file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This file indicates the expected rating for each token in the `imdb.vocab`
    file. This means that all these numerical values indicate the score for each individual
    word provided in the `imdb.vocab` file. If the word is positive, then the numerical
    value is a positive float number. If the word is negative, then the numerical
    value is a negative float value. You can see the contents of the file in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![imdbEr.txt file](img/B08394_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.4: The imdbEr.txt file, which has a score for each of the words given
    in the imdb.vocab file'
  prefs: []
  type: TYPE_NORMAL
- en: README
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This file contains the documentation regarding the dataset. You can get a hold
    of the basic information using this file.
  prefs: []
  type: TYPE_NORMAL
- en: Note that for developing this sentiment analysis application, we will consider
    data from only the `train` folder because processing up to 50,000 movie reviews
    takes a lot of computation power, so instead of 50,000 movie reviews, we will
    be considering only 25,000 movie reviews from the `train` folder, and we will
    hold out some movie reviews for testing. Now let's try to understand how the content
    of the movie review files has been provided.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the contents of the movie review files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inside the `pos` and `neg` folders, there are `.txt` files that contain the
    movie reviews. All the `.txt` files inside the `pos` folder are positive movie
    reviews. You can refer to the sample content provided in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding the contents of the movie review files](img/B08394_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.5: Sample movie review from the pos folder; the filename is 0_9.txt'
  prefs: []
  type: TYPE_NORMAL
- en: 'The movie reviews are provided in simple plain text. Here, we will be performing
    only a small preprocessing change, in which we rename the `pos` and `neg` folder
    names to `positiveReviews` and `negativeReviews`, respectively. This `IMDb` dataset
    had already been preprocessed, so we are not performing any extensive preprocessing.
    You can download the final training dataset by using this GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/data.tar.gz.](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/data.tar.gz)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to start building the ML model for our sentiment analysis application.
    We will perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Building the training and testing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering for the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the Machine Learning algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the baseline model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So let's try to understand all these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Building the training and testing datasets for the baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will be generating the training dataset as well as the
    testing dataset. We will iterate over the files of our dataset and consider all
    files whose names start with the digit 12 as our test dataset. So, roughly 90%
    of our dataset is considered the training dataset and 10 % of our dataset is considered
    the testing dataset. You can refer to the code for this in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building the training and testing datasets for the baseline model](img/B08394_05_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.6: Code snippet for building the training and testing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, if the filename starts with 12 then we consider the content
    of those files as the testing dataset. All files apart from these are considered
    the training dataset. You can find the code at this GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering for the baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this application, we will be using a basic statistical feature extraction
    concept in order to generate the features from raw text data. In the NLP domain,
    we need to convert raw text into a numerical format so that the ML algorithm can
    be applied to that numerical data. There are many techniques available, including
    indexing, count based vectorization, **Term Frequency - Inverse Document Frequency**
    (**TF-IDF** ), and so on. I have already discussed the concept of TF-IDF in [Chapter
    4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"), *Generate features
    using TF-IDF*:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Indexing is basically used for fast data retrieval. In indexing, we provide
    a unique identification number. This unique identification number can be assigned
    in alphabetical order or based on frequency. You can refer to this link: [http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Count-based vectorization sorts the words in alphabetical order and if a particular
    word is present then its vector value becomes 1, otherwise 0\. The size of the
    vector is the same as the vocabulary size of our training dataset. You can refer
    to the simple code by using this link: [https://github.com/jalajthanaki/NLPython/blob/6c74ddecac03b9aec740ae2e11dd8b52f11c0623/ch5/bagofwordsdemo/BOWdemo.py](https://github.com/jalajthanaki/NLPython/blob/6c74ddecac03b9aec740ae2e11dd8b52f11c0623/ch5/bagofwordsdemo/BOWdemo.py)'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are using the TF-IDF vectorizer technique from scikit-learn. The `TfidfVectorizer`
    function converts a collection of raw documents into a matrix of TF-IDF features.
    If you are new to TF-IDF, then I would recommend that you refer to [http://www.tfidf.com/](http://www.tfidf.com/)
    or [https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/5](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/5).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet provided in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature engineering for the baseline model](img/B08394_05_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.7: Code snippet for generating feature vectors by using TF-IDF'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you see in the preceding code snippet for generating feature vectors by
    using TF-IDF, we have defined some parameters, which I want to explain properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '`min_df`: This parameter provides a strict lower limit for document frequency.
    We have set this parameter to 5\. So terms that appear fewer than 5 times in the
    dataset will not be considered for generating the TF-IDF vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_df`: This parameter ignores terms that have a document frequency strictly
    higher than the given threshold. If the value of this parameter is float, then
    it represents a proportion of the document. We have set the parameter value to
    0.8, which means we are considering 80% of the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sublinear_tf`: This parameter is used to apply scaling. The value of this
    parameter is False by default . If its value is True, then the value of tf will
    be replaced with the *1+log(tf)* formula. This formula will help us perform scaling
    on our vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`use_idf`: This parameter indicates whether the IDF reweighting mechanism is
    enabled or not. By default, IDF reweighting is enabled and hence the flag value
    for this parameter is True.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two methods are used here, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fit_transform()`: By using this method, you have learned vocabulary and IDF,
    and this method returns the term-document matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transform()`: This method transforms documents into a document-term matrix.
    This method uses vocabulary and document frequency learned from the fit_transform
    method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the preceding code at this GitHub link [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: Now let's see which algorithm is best suited to building the baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the machine learning algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentiment analysis is a classification problem. There are some algorithms that
    can be really helpful for us. In movie reviews, you may discover that there are
    some phrases that appear quite frequently. If these frequently used phrases indicate
    some kind of sentiment, most likely, they are phrases that indicate a positive
    sentiment or a negative sentiment. We need to find phrases that indicate a sentiment.
    Once we find phrases that indicate sentiment, we just need to classify the sentiment
    either in a positive sentiment class or a negative sentiment class. In order to
    find out the actual sentiment class, we need to identify the probability of the
    most likely positive phrases and most likely negative phrases so that based on
    a higher probability value, we can identify that the given movie review belongs
    to a positive or a negative sentiment. The probabilities we will be taking into
    account are the prior and posterior probability values. This is the fundamental
    base of the naive Bayes algorithm. So, we will be using the multinomial naive
    Bayes algorithm. Apart from this, we will be using the **Support Vector Machine**
    (**SVM**) algorithm. We will be implementing it with different types of kernel
    tricks.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn more about naive Bayes, then you can refer to [http://www.saedsayad.com/naive_bayesian.html](http://www.saedsayad.com/naive_bayesian.html),
    and if you want to learn more about SVM, then you can refer to: [https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)
    or [https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/8/ch08lvl1sec77/understanding-ml-algorithms-and-other-concepts](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/8/ch08lvl1sec77/understanding-ml-algorithms-and-other-concepts).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the code that helps us perform training.
  prefs: []
  type: TYPE_NORMAL
- en: Training the baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at the code that helps us perform actual training
    on the training dataset. We will look at the implementation first, and then I
    will explain the code step by step. Here, we will be implementing Naive Bayes
    and SVM algorithms. For implementation, we will be using the scikit-learn library.
    You can find the code at this GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the baseline model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to understand the implementation of the baseline model, you can refer
    to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing the baseline model](img/B08394_05_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.8: Code snippet for performing training using naive Bayes and SVM'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have implemented the following four algorithms here:'
  prefs: []
  type: TYPE_NORMAL
- en: Multinomial naive Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C-support vector classification with kernel rbf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: C-support vector classification with kernel linear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear support vector classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multinomial naive Bayes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you can see in the preceding code snippet, we have used Multinomial naive
    Bayes. The multinomial naive Bayes classifier is suitable for classification with
    discrete features, which means that if the features are word counts or TF-IDF
    vectors, then we can use this classifier. The multinomial distribution normally
    requires integer feature counts. However, fractional counts such as TF-IDF might
    work as well. So, we have to apply this algorithm to `train_vectors`. The `fit()`
    method is the step where the actual training is performed. Here, we have used
    all hyper parameters by default.
  prefs: []
  type: TYPE_NORMAL
- en: C-support vector classification with kernel rbf
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have also implemented SVM with the *rbf* kernel. Kernel is a function that
    will help train the algorithm. The equation for the *rbf* kernel function is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C-support vector classification with kernel rbf](img/B08394_05_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: C-support vector classification with kernel linear
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have also implemented SVM with the linear kernel. The equation for the linear
    kernel function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C-support vector classification with kernel linear](img/B08394_05_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Linear support vector classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have also used linear support vector classification. We will use the `LinearSVC()`
    method type for implementing this classification. This is similar to SVC with
    the parameter kernel=*linear* but implemented in terms of liblinear rather than
    `libsvm`, so it has more flexibility in the choice of penalties and loss functions,
    and should scale better to a large number of samples.
  prefs: []
  type: TYPE_NORMAL
- en: For all the preceding ML algorithms, we have provided input, which are `train_vectors`
    and `train_labels`. We will test the ML model accuracy by using the test vectors
    and comparing predicted labels with actual `test_labels`. Before performing testing,
    we will decide which kind of testing parameters we will be using. So let's look
    at the testing matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the testing matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will look at the testing matrix that we should consider
    in order to evaluate the trained ML models. For the baseline approach, we will
    be using the following five testing matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F1-score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we understand these terms, let's cover some basic terms that will help
    us to understand the preceding terms.
  prefs: []
  type: TYPE_NORMAL
- en: '**True Positive** (**TP**)—If the classifier predicts that the given movie
    review carries a positive sentiment and that movie review has a positive sentiment
    in an actual scenario, then these kinds of test cases are considered TP. So, you
    can define the TP as if the test result is one that detects the condition when
    the condition is actually present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**True Negative** (**TN**)—If the classifier predicts that the given movie
    review carries a negative sentiment and that movie review has a negative sentiment
    in an actual scenario, then those kinds of test cases are considered True Negative(TN).
    So, you can define the TN as if the test result is one that does not detect the
    condition when the condition is actually absent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Positive** (**FP**)—If the classifier predicts that the given movie
    review carries a positive sentiment and that movie review has a negative sentiment
    in an actual scenario, then those kinds of test cases are considered False Positive
    (FP). So, you can define the FP as if the test result is one that detects the
    condition when the condition is actually absent. This is like a flase alram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**False Negative** (**FN**)—If the classifier predicts that the given movie
    review carries a negative sentiment and that movie review has a positive sentiment
    in an actual scenario, then those kinds of test cases are considered False Negative
    (FN). So, you can define the FN as if the test result is one that does not detect
    the condition when the condition is actually present. This is the situation where
    certain conditions have been overlooked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we will look at all five main testing matrices that use TP, TN, FP, and
    FN terms.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Precision is the ability of the classifier to assign a positive class label
    for samples that originally belong to a positive class label. Precision does not
    assign a positive class for a given sample that originally belongs to a negative
    class. The equation for generating the precision score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Precision](img/B08394_05_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Recall
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall is the ability of the classifier to find all the positive samples. The
    equation for generating recall score is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Recall](img/B08394_05_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: F1-Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'F1-score is the harmonic means of precision and recall. So you can find the
    equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![F1-Score](img/B08394_05_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Support
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Support is the number of occurrences of each class in true target labels. The
    value of support helps when it comes to calculating the average value for precision,
    recall, and F1-score. You can see the equations for calculating the average value
    of precision, recall, and F1-Score as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Support](img/B08394_05_37.jpg)![Support](img/B08394_05_38.jpg)![Support](img/B08394_05_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Actual calculation using the preceding formula will be provided in the *Testing
    the baseline mode* section. So bear with me for a while and we will see the actual
    testing result.
  prefs: []
  type: TYPE_NORMAL
- en: Training accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training accuracy guides us in order to obtain the correct direction for developing
    any ML application. We test the trained ML model on the testing dataset. When
    we perform this testing, we have actual labels of the sentiment class for each
    of the testing records, and we also have the predicted sentiment class for all
    testing records so we can compare the results. So, the set of labels predicted
    for a testing dataset must exactly match the corresponding set of labels in the
    actual testing dataset. We count the records where our predicted labels are the
    same as actual labels, and then we convert this count to a percentage.
  prefs: []
  type: TYPE_NORMAL
- en: We will see all the testing matrices for each of the implemented ML algorithms
    in the next section, and then we will decide which algorithm performs well. So
    let's look at the implementation of testing the baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the baseline model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here, we will look at the code snippet that performs the actual testing. We
    will be obtaining all the testing matrices that have been explained so far. We
    are going to test all the different ML algorithms so that we can compare the accuracy
    score.
  prefs: []
  type: TYPE_NORMAL
- en: Testing of Multinomial naive Bayes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see the testing result for the multinomial naive Bayes algorithm in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing of Multinomial naive Bayes](img/B08394_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.9: Code snippet for testing multinomial naive Bayes algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, using this algorithm we have achieved an accuracy score of 81.5%.
  prefs: []
  type: TYPE_NORMAL
- en: Testing of SVM with rbf kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see the testing result for SVM with the rbf kernel algorithm in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing of SVM with rbf kernel](img/B08394_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.10: Code snippet for testing SVM with rbf kernel'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have performed a test on the testing dataset and obtained
    an accuracy of 65.4%.
  prefs: []
  type: TYPE_NORMAL
- en: Testing SVM with the linear kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see the testing result for SVM with the linear kernel algorithm in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing SVM with the linear kernel](img/B08394_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.11: Code snippet for testing SVM with linear kernel'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have performed a test on the testing dataset and obtained
    an accuracy of 83.6%.
  prefs: []
  type: TYPE_NORMAL
- en: Testing SVM with linearSVC
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see the testing result for SVM with the linearSVC kernel algorithm
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing SVM with linearSVC](img/B08394_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.12: Code snippet for testing SVM with linearSVC kernel'
  prefs: []
  type: TYPE_NORMAL
- en: We have performed a test on the testing dataset here and obtained an accuracy
    of 83.6%.
  prefs: []
  type: TYPE_NORMAL
- en: So, after seeing the accuracy score of each of the implemented algorithms, we
    can say that SVM with linear kernel and linearSVC is performing really well. Now,
    you may wonder whether we can improve the accuracy. We can do that for sure. So
    let's discuss the problems that exist in this baseline approach.
  prefs: []
  type: TYPE_NORMAL
- en: Problem with the existing approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the baseline approach, we got great accuracy. However, we ignored the following
    points, which we can be implemented in our revised approach:'
  prefs: []
  type: TYPE_NORMAL
- en: We did not focus on word embedding-based techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning** (**DL**) algorithms such as CNN can be helpful for us'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to focus on these two points because word embedding-based techniques
    really help retain the semantics of the text. So we should use these techniques
    as well as the DL-based-algorithm, which helps us provide more accuracy because
    DL algorithms perform well when a nested data structure is involved. What do I
    mean by a nested data structure? Well, that means any written sentence or spoken
    sentence made up of phrases, phrases made of words, and so on. So, natural language
    has a nested data structure. DL algorithms help us understand the nested structure
    of the sentences from our text dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to optimize the existing approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are certain techniques that can help us improve this application. The
    key techniques that can help us improvise the baseline approach are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We can use word embedding-based techniques such as Word2Vec, glove, and so on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should also implement **Convolution Neural Networks** (**CNN** ) to get an
    idea about how a deep learning algorithm can help us
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So in the revised approach, we will be focusing on word embedding techniques
    and the Deep Learning algorithm. We will be using Keras with the TensorFlow backend.
    Before implementation, let's understand the revised approach in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding key concepts for optimizing the approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will understand the revised approach in detail, so we know
    what steps we should implement. We are using Keras, a Deep Learning library that
    provides us with high-level APIs so we can implement CNN easily. The following
    steps are involved:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing the dependencies**: In this step, we will be importing different
    dependencies such as Numpy and Keras with the TensorFlow backend. We will be using
    different APIs belonging to the Keras library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Downloading and loading the IMDb dataset**: In this step, we will be downloading
    the IMDb dataset and loading this dataset by using Keras APIs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Choosing top words and maximum text length**: In this stage, we will set
    the value of our vocabulary, which we can use during the word embedding stage.
    So, we have selected the top 10,000 words. After that, we have restricted the
    length of movie reviews to 1600.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementing word embedding**: At this stage of the code, we will be using
    default embedding techniques from Keras and generate the word vector with a length
    of 300.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Building a CNN**: In this stage, we will be making three-layer neural networks,
    where the first layer has 64 neurons, the second layer has 32 neurons, and the
    last layer has 16 neurons. Here, we are using sigmoid as an activation function.
    The Activation function introduces non-linearity to the neural network so that
    we can generate a probability score for each class using mathematical functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training and obtaining the accuracy**: Finally, we train the model and generate
    the accuracy score. We have set the epoch value to 3 and set adam as the optimization
    function and our loss function is `binary_crossentropy`. Epoch basically indicates
    how many times we need to perform training on our whole dataset. Cross-entropy
    loss measures the performance of a classifier whose output is a probability value
    between 0 and 1\. Cross-entropy loss increases as the predicted probability differs
    from the actual label. After training, we will generate the accuracy of the model.
    This training stage may take time as well as a great amount of computation power.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now let's see the code in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the revised approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see the implementation in the form of a code snippet.
    We will be following the same step that we saw in the previous section. So, without
    any delay, let''s look at the code. You can refer to this code by using this GitHub
    link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Revised_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Revised_approach.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Importing the dependencies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet in the following figure, where you can find
    the imported dependencies as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Importing the dependencies](img/B08394_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.13: Code snippet where we can see the imported dependencies'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we have used the TensorFlow backend with the Keras library.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading and loading the IMDb dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet in the following figure, where you can also
    find the code for downloading and loading the IMDb dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Downloading and loading the IMDb dataset](img/B08394_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.14: Code snippet for downloading and loading the IMDb dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We have also set the value of the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the top words and the maximum text length
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this stage, we have set the top word values as well as the maximum text
    length value. You can refer to the code snippet in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Choosing the top words and the maximum text length](img/B08394_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.15: Code snippet to set the vocabulary value and the maximum text
    length value'
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of code, we have set the top_word parameter to 10,000, and
    in the second part of the code, we have set the length of the movie review to
    1,600\. The top_word indicates the vocabulary size. From our dataset, we have
    picked the top 10,000 unique words. Most of the movie reviews have words that
    are present in our word vocabulary. Here, we are not processing very long movie
    reviews, because of that reason we have set the length of the movie review.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing word embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this stage, we have implemented the default Keras word embedding method
    in order to obtain a feature vector with a size of 300\. You can refer to the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing word embedding](img/B08394_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.16: Code snippet for obtaining word feature vector based on the word
    embedding technique'
  prefs: []
  type: TYPE_NORMAL
- en: Building a convolutional neural net (CNN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section you can refer to the code, which will help you understand the
    architecture of a neural net. Here, we have used CNN because it handles higher
    level features or a nested structure of the dataset really well. You can refer
    to the code snippet in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a convolutional neural net (CNN)](img/B08394_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.17: Code snippet for building CNN'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have built the neural network with two dense layers in it.
  prefs: []
  type: TYPE_NORMAL
- en: Training and obtaining the accuracy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this stage, we have performed the training. You can refer to the code in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and obtaining the accuracy](img/B08394_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.18: Code snippet for performing training on the training dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will be performing training three times as our epoch value is set to
    3\. As sentiment analysis is a binary problem, we have used `binary_crossentropy`
    as loss function. If you have a GPU-based computer, then the training time will
    be less; otherwise, this step is time consuming and the computation power consuming.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once training is done, we can obtain training accuracy. For training accuracy,
    you can refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training and obtaining the accuracy](img/B08394_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.19: Code snippet for obtaining training accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the accuracy is the training accuracy. Now we need to obtain testing accuracy
    because that will give us an actual idea about how well the train model is performing
    on unseen data. So let's see what the testing accuracy is.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will obtain the accuracy of the testing dataset. You can
    refer to the code snippet in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the revised approach](img/B08394_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.20: Code snippet for obtaining testing accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: After obtaining the testing accuracy, we got an accuracy value of 86.45%. This
    testing accuracy is better than our baseline approach. Now let's see what points
    we can improve in order to come up with the best approach.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding problems with the revised approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will discuss what points of the revised approach we can
    improve. These are the points that we can implement in order to obtain the best
    possible approach:'
  prefs: []
  type: TYPE_NORMAL
- en: We can use pretrained Word2Vec or glove models to generate the word vector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should use a recurrent neural network with LSTM to get better output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will understand and implement the best approach, where we
    will load the pretrained glove (global word vector) model and use an RNN and LSTM
    network.
  prefs: []
  type: TYPE_NORMAL
- en: The best approach
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are some steps that we can follow in order to obtain the best possible
    approach. In this approach, we have used a glove pretrained model and have trained
    the model using the RNN and LSTM networks. The glove model has been pretrained
    on a large dataset so that it can generate more accurate vector values for words.
    That is the reason we are using glove here. In the next section, we will look
    at the implementation of the best approach. You can find all the code at this
    GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Best_approach_sentiment_analysis.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Best_approach_sentiment_analysis.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the best approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to implement the best approach, we will be performing the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the glove model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the precomputed ID matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting the train and test datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing the trained model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading the glove model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to get the best performance, we will be using the pretrained glove
    model. You can download it at [https://nlp.stanford.edu/projects/glove/.](https://nlp.stanford.edu/projects/glove/.)
    We have already generated the binary file and saved that file as an `.npy` extension.
    This file is with the `wordsList.npy` file. You can refer to the code snippet
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the glove model](img/B08394_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.21: Code snippet for loading the glove pretrained model'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality of the word vector is 50 and this model contains word vectors
    for 400,000 words.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can refer to the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the dataset](img/B08394_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.22: Code snippet for loading the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: We have considered 25,000 movie reviews in total.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preprocessing](img/B08394_05_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.23: Code snippet for performing pre-processing'
  prefs: []
  type: TYPE_NORMAL
- en: Loading precomputed ID matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we are generating the index for each word. This process is
    computationally expensive, so I have already generated the index matrix and made
    it ready for loading. You can refer to the code snippet in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading precomputed ID matrix](img/B08394_05_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.24: Code snippet for generating matrix for word IDs'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the train and test datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will see the code snippet for generating the training and
    testing datasets. You can refer to the code snippet in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Splitting the train and test datasets](img/B08394_05_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.25: Code snippet for splitting dataset into training and testing dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have used a **recurrent neural net** (**RNN**) with **Long-Short Term Memory
    Unit** (**LSTMs**) cells as a part of their hidden states. LSTM cells are used
    to store sequential information. If you have multiple sentences, then LSTM stores
    the context of the previous or previous to previous sentences, which helps us
    improve this application. If you want to understand LSTM in detail, then you can
    refer to [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the code snippet in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a neural network](img/B08394_05_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.26: Code snippet for building RNN with LSTM'
  prefs: []
  type: TYPE_NORMAL
- en: First, we define the hyper parameters. We set the batch size to 64, LSTM units
    to 64, the number of classes to 2, and then we perform 100,000 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Training the neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, you can refer to the code snippet in the following figure,
    which is used for performing training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the neural network](img/B08394_05_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.27: Code snippet for performing training'
  prefs: []
  type: TYPE_NORMAL
- en: 'After each 10,000 iterations, we save the model. During the training, you can
    monitor the progress by using TensorBoard. You can refer to the following figure,
    which shows the progress over the period of training. You can monitor the accuracy
    and loss percentage during training, so you find out how the DL model is converging
    during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Training the neural network](img/B08394_05_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.28: Accuracy and loss graph generated during training on TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: 'Training is time consuming and computationally expensive, so with a GPU it
    may take 2- 3 hours to train the model. Therefore, you can use the pretrained
    model by downloading it from GitHub at: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz).'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the trained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once training is done, we can save the trained model. After loading this model,
    we can check its accuracy as well. You can refer to the code snippet in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Loading the trained model](img/B08394_05_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.29: Code snippet for generating testing accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: Here we have generated a testing accuracy of 91.66%.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the trained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will be passing new movie reviews, and by loading the trained
    model, we will generate the prediction of sentiment. You can refer to the code
    snippet in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the trained model](img/B08394_05_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.30: Code snippet for loading trained model and generating sentiment
    for a given sentence of the movie review'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this snippet, we have passed one sentence as part of a movie review, and
    our trained model identifies it as a negative sentiment. You can also refer to
    the code snippet in the following figure, which generates a sentiment for the
    sentence carrying a negative word in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Testing the trained model](img/B08394_05_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.31: Code snippet for loading trained model and generating sentiment
    for given sentence of the movie review'
  prefs: []
  type: TYPE_NORMAL
- en: This approach gives you great results.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to build a sentiment analysis model that gives
    us state-of-the-art results. We used an IMDb dataset that had positive and negative
    movie reviews and understood the dataset. We applied the machine learning algorithm
    in order to get the baseline model. After that, in order to optimize the baseline
    model, we changed the algorithm and applied deep-learning-based algorithms. We
    used glove, RNN, and LSTM techniques to achieve the best results. We learned how
    to build sentiment analysis applications using Deep Learning. We used TensorBoard
    to monitor our model's training progress. We also touched upon modern machine
    learning algorithms as well as Deep Learning techniques for developing sentiment
    analysis, and the Deep Learning approach works best here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used a GPU to train the neural network, so if you discover that it needs
    more computation power from your end to train the model, then you can use the
    Google cloud or **Amazon Web Services** (**AWS**) GPU-based instances. I have
    already uploaded the pretrained model, so you can directly use that as well. You
    can find the pretrained model at this GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will build a job recommendation system that will help
    people find jobs, especially related to the job profiles in which they are interested.
    For a job recommendation system, we will be using various resources for linking
    resumes, job search queries, and so on. Again, we will be developing this system
    using machine learning and Deep Learning systems.
  prefs: []
  type: TYPE_NORMAL
