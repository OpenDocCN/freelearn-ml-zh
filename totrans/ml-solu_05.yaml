- en: Chapter 5. Sentiment Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 情感分析
- en: So far, we have explored some really cool applications in the analytics domain.
    In this chapter, we will explore the famous Natural Language Processing (NLP)
    technique, which you may have already guessed because of the name of the chapter.
    Absolutely right; we will build a sentiment analysis-based application. In general,
    everyone is familiar with sentiment analysis-based applications. If you aren't,
    then don't worry. We will discuss and understand all the necessary details.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们在分析领域探索了一些真正酷的应用。在本章中，我们将探讨著名的自然语言处理（NLP）技术，你可能已经猜到了，因为章节的名称。绝对正确；我们将构建一个基于情感分析的应用。一般来说，每个人对基于情感分析的应用都很熟悉。如果你不熟悉，那么不用担心。我们将讨论和理解所有必要的细节。
- en: First of all, I want to give you a basic idea about sentiment analysis. I will
    provide an example so it will be easy for you to understand. Regardless of where
    we live, we all watch movies. Nowadays, we read reviews or others' opinions on
    various social media platforms. After that, if a majority of the opinions about
    the movie are good, then we watch that movie. If the opinions are not impressive,
    we might not watch the movie. So during this entire process, our mind analyzes
    these opinions and categorizes them into either positive opinions, negative opinions,
    or neutral opinions. In this chapter, we will be performing the same kind of analysis.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我想给你一个关于情感分析的基本概念。我会提供一个例子，这样你就能更容易理解。无论我们住在哪里，我们都会看电影。如今，我们在各种社交媒体平台上阅读评论或他人的意见。然后，如果大多数关于电影的评论都是好的，那么我们就看那部电影。如果评论不令人印象深刻，我们可能不会看那部电影。所以在这个过程中，我们的思维分析了这些意见，并将它们分类为正面意见、负面意见或中性意见。在本章中，我们将进行同样的分析。
- en: Let me introduce the formal definition of sentiment analysis. Sentiment analysis
    is a technique where we consider a sentence, paragraph, document, or any information
    that is in the form of a natural language and determine whether that text's emotional
    tone is positive, negative, or neutral. We will be applying machine learning and
    Deep Learning to build a sentiment analysis application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 让我介绍情感分析的正确定义。情感分析是一种技术，我们考虑一个句子、段落、文档或任何以自然语言形式存在的信息，并确定该文本的情感基调是正面、负面还是中性。我们将应用机器学习和深度学习来构建情感分析应用。
- en: 'We will be covering the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing the problem statement
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Understanding the dataset
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集
- en: Building training and testing datasets for the baseline model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为基线模型构建训练和测试数据集
- en: Feature engineering for the baseline model
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线模型的特征工程
- en: Selecting the Machine Learning (ML) algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择机器学习（ML）算法
- en: Training the baseline model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练基线模型
- en: Understanding the testing matrix
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: Testing the baseline model
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试基线模型
- en: Problems with the existing approach
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有方法的问题
- en: How to optimize the existing approach
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何优化现有方法
- en: Understanding key concepts for optimizing the approach
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解优化方法的关键概念
- en: Implementing the revised approach
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施修订后的方法
- en: Testing the revised approach
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试修订后的方法
- en: Understanding problems with the revised approach
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解修订后方法的问题
- en: Best approach
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳方法
- en: Implementing the best approach
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施最佳方法
- en: Summary
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: Introducing problem statements
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: We are living in a competitive world. Before buying any product and investing
    our time or money in anything, we try to find out what others think about that
    product(s) or service(s). We try to analyze their reviews or opinions. If we find
    them positive and trustworthy, then we buy the product and invest our money or
    time in that particular service. On the other hand, if we find these opinions
    or reviews negative, then we might not buy the product and not invest our money
    or time in that particular service. In the current era of the internet, it is
    easy to find reviews on social media platforms, blogs, news sources, and so on.
    This activity of analyzing reviews will be useful for consumers as well as makers
    of products or service providers. This is because, based on the reviews of their
    customer, they can change their product effectively, providing more satisfaction
    to their customers and make a good profit from that product or service. I have
    already given you the formal definition of sentiment analysis, so I'm not going
    to bore you with it again. Let's try to understand what the main focus of this
    chapter will be.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生活在一个竞争激烈的世界。在购买任何产品，或者在我们投入时间或金钱到任何事物之前，我们试图了解其他人对此产品（或服务）的看法。我们尝试分析他们的评论或意见。如果我们发现它们是积极的且值得信赖的，那么我们会购买该产品，并在该特定服务上投资我们的金钱或时间。另一方面，如果我们发现这些意见或评论是负面的，那么我们可能不会购买该产品，也不会在该特定服务上投资我们的金钱或时间。在互联网时代，在社交媒体平台、博客、新闻来源等地方找到评论很容易。这种分析评论的活动对消费者以及产品或服务提供商都有用。这是因为，基于客户的评论，他们可以有效地改变他们的产品，为他们的客户提供更多的满足，并从该产品或服务中获得良好的利润。我已经给你们提供了情感分析的正确定义，所以我不想再重复一遍。让我们尝试理解本章的主要关注点是什么。
- en: We will be developing a sentiment analysis application for movie reviews. During
    training, we will consider labels associated with each of the movie reviews so
    that we can train our machine learning algorithm based on the given labels. After
    training, when we pass any unseen movie reviews, then our trained machine learning
    algorithm will predict the sentiment, which means whether the provided movie review
    indicates a positive sentiment or a negative sentiment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发一个针对电影评论的情感分析应用程序。在训练过程中，我们将考虑与每条电影评论相关的标签，以便我们可以根据给定的标签训练我们的机器学习算法。训练完成后，当我们传递任何未见过的电影评论时，我们的训练好的机器学习算法将预测情感，这意味着提供的电影评论是否表示积极的情感或消极的情感。
- en: We will be considering an IMDb (Internet Movie Database) movie review dataset
    to develop sentiment analysis for movie reviews. We will look at the details regarding
    the dataset in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑使用 IMDb（互联网电影数据库）电影评论数据集来开发电影评论的情感分析。我们将在下一节中查看有关数据集的详细信息。
- en: Understanding the dataset
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据集
- en: 'In this section, we will look into our dataset. We have considered an IMDb
    dataset, which you can download at: [http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/).
    After clicking on this link, you can see that there is a link provided on the
    page. This link is titled Large Movie Review Dataset v1.0; we need to click on
    it. This way, we can download the IMDb dataset. Once you have downloaded the dataset,
    you need to extract the .tar.gz file. Once you extract the `.tar.gz` file, you
    can see that there are two folders inside the extracted folder and some other
    files. Let''s look at each of them in the following section.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看我们的数据集。我们考虑了一个 IMDb 数据集，您可以在以下链接下载：[http://ai.stanford.edu/~amaas/data/sentiment/](http://ai.stanford.edu/~amaas/data/sentiment/)。点击此链接后，您会看到页面上提供了一个链接。此链接的标题为
    Large Movie Review Dataset v1.0；我们需要点击它。这样，我们就可以下载 IMDb 数据集。一旦您下载了数据集，您需要解压 .tar.gz
    文件。一旦解压了 `.tar.gz` 文件，您会看到提取的文件夹中有两个文件夹和一些其他文件。让我们在下一节中查看每个部分。
- en: Understanding the content of the dataset
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据集的内容
- en: 'After extracting the dataset file, we''ll see that there are some folders and
    files inside it. We will be discussing all of the content''s meaning and what
    we will be using for our training purposes. This dataset has two folders and three
    files:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在解压数据集文件后，我们会看到其中有一些文件夹和文件。我们将讨论所有内容的含义以及我们将用于训练目的的内容。此数据集包含两个文件夹和三个文件：
- en: train folder
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: train 文件夹
- en: test folder
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: test 文件夹
- en: '`imdb.vocab` file'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imdb.vocab` 文件'
- en: '`imdbEr.txt`'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`imdbEr.txt`'
- en: README
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: README
- en: Train folder
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Train 文件夹
- en: This folder contains data for training. Inside this folder, there are two main
    folders. The `pos` folder contains positive movie reviews and the `neg` folder
    contains negative movie reviews. Inside the `pos` folder, there are 12,500 positive
    movie reviews. Inside the `neg` folder, there are 12,500 negative movie reviews.
    So in total, we have 25,000 movie reviews; by using them, we will train our Machine
    Learning (ML) model. For testing purposes, we can use movie reviews provided inside
    the `unsup` folder. These movie reviews are unlabeled, so we can use them for
    testing purposes or divide our labeled data into training and testing groups so
    it will be easy for us to find out how our trained ML model works.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件夹包含用于训练的数据。在这个文件夹内，有两个主要文件夹。`pos`文件夹包含正面电影评论，而`neg`文件夹包含负面电影评论。在`pos`文件夹内，有12,500条正面电影评论。在`neg`文件夹内，有12,500条负面电影评论。所以总共我们有25,000条电影评论；通过使用它们，我们将训练我们的机器学习（ML）模型。为了测试目的，我们可以使用`unsup`文件夹内提供的电影评论。这些电影评论是无标签的，因此我们可以使用它们进行测试，或者将我们的标记数据分成训练和测试组，这样我们就可以更容易地了解我们的训练ML模型的工作情况。
- en: 'There are other files inside the train folder but we are not going to use them.
    Those files carry data that already has tokenized bag-of-word (BOW) features.
    In order to get a clear picture of the folder structure, you can refer to the
    code snippet provided in the following figure:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练文件夹内还有其他文件，但我们不会使用它们。这些文件包含已经标记化的词袋（BOW）特征的数据。为了清楚地了解目录结构，你可以参考以下图中提供的代码片段：
- en: '![Train folder](img/B08394_05_01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![训练文件夹](img/B08394_05_01.jpg)'
- en: 'Figure 5.1: Folder structure of the train folder'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：训练文件夹的目录结构
- en: 'If you want to explore the dataset in more detail, then you can refer to the
    documentation provided at: [http://www.paddlepaddle.org/docs/0.10.0/documentation/en/tutorials/sentiment_analysis/index_en.html](http://www.paddlepaddle.org/docs/0.10.0/documentation/en/tutorials/sentiment_analysis/index_en.html).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更详细地探索数据集，可以参考以下提供的文档：[http://www.paddlepaddle.org/docs/0.10.0/documentation/en/tutorials/sentiment_analysis/index_en.html](http://www.paddlepaddle.org/docs/0.10.0/documentation/en/tutorials/sentiment_analysis/index_en.html)。
- en: Test folder
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试文件夹
- en: 'This folder contains data for testing. Inside this folder, there are `pos`
    and `neg` folders, which contain positive and negative movie reviews, respectively.
    Each of the folders contains 12,500 movie reviews, so in total, we have 25,000
    movie reviews for testing. These movie reviews are labeled one so we can use this
    dataset to test our trained model. There are other `BOW` files and `url` files,
    which we will not be using. You can see the folder structure of the test folder
    in the following figure:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件夹包含用于测试的数据。在这个文件夹内，有`pos`和`neg`文件夹，分别包含正面和负面的电影评论。每个文件夹包含12,500条电影评论，所以总共我们有25,000条电影评论用于测试。这些电影评论被标记为正面，这样我们就可以使用这个数据集来测试我们的训练模型。还有其他`BOW`文件和`url`文件，我们不会使用。你可以在以下图中看到测试文件夹的目录结构：
- en: '![Test folder](img/B08394_05_02.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![测试文件夹](img/B08394_05_02.jpg)'
- en: 'Figure 5.2: Folder structure of the test folder'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：测试文件夹的目录结构
- en: imdb.vocab file
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: imdb.vocab文件
- en: 'This file contains the unique words used in all movie reviews, so it is the
    vocabulary file for the `IMDb` dataset. If you open this file, then you can see
    words and observe that all of them are unique. You can see the contents of this
    file in the following figure:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件包含所有电影评论中使用的唯一单词，因此它是`IMDb`数据集的词汇表文件。如果你打开这个文件，你可以看到单词，并观察到它们都是唯一的。你可以在以下图中看到这个文件的内容：
- en: '![imdb.vocab file](img/B08394_05_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![imdb.vocab文件](img/B08394_05_03.jpg)'
- en: 'Figure 5.3: Contents of the imdb.vocab file'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：imdb.vocab文件的内容
- en: imdbEr.txt file
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: imdbEr.txt文件
- en: 'This file indicates the expected rating for each token in the `imdb.vocab`
    file. This means that all these numerical values indicate the score for each individual
    word provided in the `imdb.vocab` file. If the word is positive, then the numerical
    value is a positive float number. If the word is negative, then the numerical
    value is a negative float value. You can see the contents of the file in the following
    figure:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件表示`imdb.vocab`文件中每个标记的预期评分。这意味着所有这些数值都表示`imdb.vocab`文件中每个单词的评分。如果一个单词是正面的，那么数值是一个正浮点数。如果一个单词是负面的，那么数值是一个负浮点值。你可以在以下图中看到文件的内容：
- en: '![imdbEr.txt file](img/B08394_05_04.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![imdbEr.txt文件](img/B08394_05_04.jpg)'
- en: 'Figure 5.4: The imdbEr.txt file, which has a score for each of the words given
    in the imdb.vocab file'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：imdb.vocab文件中每个单词的得分的imdbEr.txt文件
- en: README
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: README
- en: This file contains the documentation regarding the dataset. You can get a hold
    of the basic information using this file.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件包含有关数据集的文档。您可以使用此文件获取基本信息。
- en: Note that for developing this sentiment analysis application, we will consider
    data from only the `train` folder because processing up to 50,000 movie reviews
    takes a lot of computation power, so instead of 50,000 movie reviews, we will
    be considering only 25,000 movie reviews from the `train` folder, and we will
    hold out some movie reviews for testing. Now let's try to understand how the content
    of the movie review files has been provided.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了开发这个情感分析应用，我们将只考虑`train`文件夹中的数据，因为处理高达50,000条电影评论需要大量的计算能力，所以我们将只考虑`train`文件夹中的25,000条电影评论，并将一些电影评论保留用于测试。现在让我们尝试理解电影评论文件的内容是如何提供的。
- en: Understanding the contents of the movie review files
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解电影评论文件的内容
- en: 'Inside the `pos` and `neg` folders, there are `.txt` files that contain the
    movie reviews. All the `.txt` files inside the `pos` folder are positive movie
    reviews. You can refer to the sample content provided in the following figure:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pos`和`neg`文件夹内，有`.txt`文件包含电影评论。`pos`文件夹内的所有`.txt`文件都是正面电影评论。您可以通过以下图中的示例内容进行参考：
- en: '![Understanding the contents of the movie review files](img/B08394_05_05.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![理解电影评论文件的内容](img/B08394_05_05.jpg)'
- en: 'Figure 5.5: Sample movie review from the pos folder; the filename is 0_9.txt'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：来自`pos`文件夹的样本电影评论；文件名为0_9.txt
- en: 'The movie reviews are provided in simple plain text. Here, we will be performing
    only a small preprocessing change, in which we rename the `pos` and `neg` folder
    names to `positiveReviews` and `negativeReviews`, respectively. This `IMDb` dataset
    had already been preprocessed, so we are not performing any extensive preprocessing.
    You can download the final training dataset by using this GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/data.tar.gz.](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/data.tar.gz)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 电影评论以简单的纯文本形式提供。在这里，我们将进行一个小型的预处理更改，即将`pos`和`neg`文件夹名称分别重命名为`positiveReviews`和`negativeReviews`。这个`IMDb`数据集已经过预处理，所以我们不会进行任何广泛的预处理。您可以通过以下GitHub链接下载最终的训练数据集：[https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/data.tar.gz](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/data.tar.gz)
- en: 'Now we need to start building the ML model for our sentiment analysis application.
    We will perform the following steps:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要开始构建我们的情感分析应用的机器学习模型。我们将执行以下步骤：
- en: Building the training and testing datasets
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建训练和测试数据集
- en: Feature engineering for the baseline model
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线模型的特征工程
- en: Selecting the Machine Learning algorithm
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择机器学习算法
- en: Training the baseline model
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练基线模型
- en: Understanding the testing matrix
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: Testing the baseline model
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试基线模型
- en: So let's try to understand all these steps.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解所有这些步骤。
- en: Building the training and testing datasets for the baseline model
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为基线模型构建训练和测试数据集
- en: 'In this section, we will be generating the training dataset as well as the
    testing dataset. We will iterate over the files of our dataset and consider all
    files whose names start with the digit 12 as our test dataset. So, roughly 90%
    of our dataset is considered the training dataset and 10 % of our dataset is considered
    the testing dataset. You can refer to the code for this in the following figure:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将生成训练数据集和测试数据集。我们将遍历数据集的文件，并将所有以数字12开头的文件视为我们的测试数据集。因此，大约90%的数据集被认为是训练数据集，10%的数据集被认为是测试数据集。您可以在以下图中的代码中参考这一点：
- en: '![Building the training and testing datasets for the baseline model](img/B08394_05_06.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![构建基线模型的训练和测试数据集](img/B08394_05_06.jpg)'
- en: 'Figure 5.6: Code snippet for building the training and testing dataset'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：构建训练和测试数据集的代码片段
- en: 'As you can see, if the filename starts with 12 then we consider the content
    of those files as the testing dataset. All files apart from these are considered
    the training dataset. You can find the code at this GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，如果文件名以12开头，则我们认为这些文件的内容是测试数据集。除了这些文件之外的所有文件都被认为是训练数据集。您可以在以下GitHub链接中找到代码：[https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb).
- en: Feature engineering for the baseline model
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基线模型的特征工程
- en: 'For this application, we will be using a basic statistical feature extraction
    concept in order to generate the features from raw text data. In the NLP domain,
    we need to convert raw text into a numerical format so that the ML algorithm can
    be applied to that numerical data. There are many techniques available, including
    indexing, count based vectorization, **Term Frequency - Inverse Document Frequency**
    (**TF-IDF** ), and so on. I have already discussed the concept of TF-IDF in [Chapter
    4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"), *Generate features
    using TF-IDF*:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个应用，我们将使用基本的统计特征提取概念来从原始文本数据中生成特征。在NLP领域，我们需要将原始文本转换为数值格式，以便将机器学习算法应用于这些数值数据。有许多技术可用，包括索引、基于计数的向量化、**词频-逆文档频率**（**TF-IDF**）等。我已在[第4章](ch04.xhtml
    "第4章。电子商务推荐系统")中讨论了TF-IDF的概念，*使用TF-IDF生成特征*：
- en: Note
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Indexing is basically used for fast data retrieval. In indexing, we provide
    a unique identification number. This unique identification number can be assigned
    in alphabetical order or based on frequency. You can refer to this link: [http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 索引主要用于快速数据检索。在索引中，我们提供一个唯一的识别号。这个唯一的识别号可以按字母顺序或基于频率分配。您可以参考此链接：[http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)
- en: 'Count-based vectorization sorts the words in alphabetical order and if a particular
    word is present then its vector value becomes 1, otherwise 0\. The size of the
    vector is the same as the vocabulary size of our training dataset. You can refer
    to the simple code by using this link: [https://github.com/jalajthanaki/NLPython/blob/6c74ddecac03b9aec740ae2e11dd8b52f11c0623/ch5/bagofwordsdemo/BOWdemo.py](https://github.com/jalajthanaki/NLPython/blob/6c74ddecac03b9aec740ae2e11dd8b52f11c0623/ch5/bagofwordsdemo/BOWdemo.py)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于计数的向量化将单词按字母顺序排序，如果某个特定单词存在，则其向量值变为1，否则为0。向量的大小与我们的训练数据集的词汇表大小相同。您可以参考以下链接中的简单代码：[https://github.com/jalajthanaki/NLPython/blob/6c74ddecac03b9aec740ae2e11dd8b52f11c0623/ch5/bagofwordsdemo/BOWdemo.py](https://github.com/jalajthanaki/NLPython/blob/6c74ddecac03b9aec740ae2e11dd8b52f11c0623/ch5/bagofwordsdemo/BOWdemo.py)
- en: Here, we are using the TF-IDF vectorizer technique from scikit-learn. The `TfidfVectorizer`
    function converts a collection of raw documents into a matrix of TF-IDF features.
    If you are new to TF-IDF, then I would recommend that you refer to [http://www.tfidf.com/](http://www.tfidf.com/)
    or [https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/5](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/5).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用scikit-learn的TF-IDF向量化技术。`TfidfVectorizer`函数将一组原始文档转换为TF-IDF特征矩阵。如果您对TF-IDF不熟悉，我建议您参考[http://www.tfidf.com/](http://www.tfidf.com/)或[https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/5](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/5)。
- en: 'You can refer to the code snippet provided in the following figure:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下图中提供的代码片段：
- en: '![Feature engineering for the baseline model](img/B08394_05_07.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![基线模型的特征工程](img/B08394_05_07.jpg)'
- en: 'Figure 5.7: Code snippet for generating feature vectors by using TF-IDF'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：使用TF-IDF生成特征向量的代码片段
- en: 'As you see in the preceding code snippet for generating feature vectors by
    using TF-IDF, we have defined some parameters, which I want to explain properly:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在前面用于生成TF-IDF特征向量的代码片段中所看到的，我们定义了一些参数，我想对此进行适当的解释：
- en: '`min_df`: This parameter provides a strict lower limit for document frequency.
    We have set this parameter to 5\. So terms that appear fewer than 5 times in the
    dataset will not be considered for generating the TF-IDF vector.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_df`：此参数为文档频率提供了一个严格的下限。我们将此参数设置为5。因此，在数据集中出现次数少于5次的术语将不会被考虑用于生成TF-IDF向量。'
- en: '`max_df`: This parameter ignores terms that have a document frequency strictly
    higher than the given threshold. If the value of this parameter is float, then
    it represents a proportion of the document. We have set the parameter value to
    0.8, which means we are considering 80% of the dataset.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_df`：此参数忽略具有严格高于给定阈值的文档频率的术语。如果此参数的值为浮点数，则表示文档的比例。我们将参数值设置为0.8，这意味着我们考虑了数据集的80%。'
- en: '`sublinear_tf`: This parameter is used to apply scaling. The value of this
    parameter is False by default . If its value is True, then the value of tf will
    be replaced with the *1+log(tf)* formula. This formula will help us perform scaling
    on our vocabulary.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sublinear_tf`: 此参数用于应用缩放。默认情况下，此参数的值为False。如果其值为True，则tf的值将被替换为*1+log(tf)*公式。此公式将帮助我们对我们词汇表进行缩放。'
- en: '`use_idf`: This parameter indicates whether the IDF reweighting mechanism is
    enabled or not. By default, IDF reweighting is enabled and hence the flag value
    for this parameter is True.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`use_idf`: 此参数表示是否启用了IDF重新加权机制。默认情况下，IDF重新加权是启用的，因此此参数的标志值为True。'
- en: 'Two methods are used here, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了两种方法，如下所示：
- en: '`fit_transform()`: By using this method, you have learned vocabulary and IDF,
    and this method returns the term-document matrix.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit_transform()`: 通过使用此方法，你已经学习了词汇和IDF，并且此方法返回词-文档矩阵。'
- en: '`transform()`: This method transforms documents into a document-term matrix.
    This method uses vocabulary and document frequency learned from the fit_transform
    method.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`transform()`: 此方法将文档转换为文档-词矩阵。此方法使用从fit_transform方法中学习到的词汇和文档频率。'
- en: You can find the preceding code at this GitHub link [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下GitHub链接中找到前面的代码 [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb)。
- en: Now let's see which algorithm is best suited to building the baseline model.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看哪种算法最适合构建基线模型。
- en: Selecting the machine learning algorithm
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择机器学习算法
- en: Sentiment analysis is a classification problem. There are some algorithms that
    can be really helpful for us. In movie reviews, you may discover that there are
    some phrases that appear quite frequently. If these frequently used phrases indicate
    some kind of sentiment, most likely, they are phrases that indicate a positive
    sentiment or a negative sentiment. We need to find phrases that indicate a sentiment.
    Once we find phrases that indicate sentiment, we just need to classify the sentiment
    either in a positive sentiment class or a negative sentiment class. In order to
    find out the actual sentiment class, we need to identify the probability of the
    most likely positive phrases and most likely negative phrases so that based on
    a higher probability value, we can identify that the given movie review belongs
    to a positive or a negative sentiment. The probabilities we will be taking into
    account are the prior and posterior probability values. This is the fundamental
    base of the naive Bayes algorithm. So, we will be using the multinomial naive
    Bayes algorithm. Apart from this, we will be using the **Support Vector Machine**
    (**SVM**) algorithm. We will be implementing it with different types of kernel
    tricks.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是一个分类问题。有一些算法对我们非常有帮助。在电影评论中，你可能会发现有些短语出现得相当频繁。如果这些常用短语表明某种情感，那么很可能是表明积极情感或消极情感的短语。我们需要找到表明情感的短语。一旦我们找到了表明情感的短语，我们只需要将情感分类为积极情感类别或消极情感类别。为了确定实际的情感类别，我们需要识别最可能出现的积极短语和最可能出现的消极短语的概率，以便基于更高的概率值，我们可以确定给定的电影评论属于积极情感还是消极情感。我们将考虑的概率是先验概率和后验概率值。这是朴素贝叶斯算法的基本基础。因此，我们将使用多项式朴素贝叶斯算法。除此之外，我们还将使用**支持向量机**（**SVM**）算法。我们将使用不同类型的核技巧来实现它。
- en: 'If you want to learn more about naive Bayes, then you can refer to [http://www.saedsayad.com/naive_bayesian.html](http://www.saedsayad.com/naive_bayesian.html),
    and if you want to learn more about SVM, then you can refer to: [https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)
    or [https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/8/ch08lvl1sec77/understanding-ml-algorithms-and-other-concepts](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/8/ch08lvl1sec77/understanding-ml-algorithms-and-other-concepts).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于朴素贝叶斯的信息，可以参考[http://www.saedsayad.com/naive_bayesian.html](http://www.saedsayad.com/naive_bayesian.html)，如果您想了解更多关于SVM的信息，可以参考：[https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/](https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/)
    或 [https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/8/ch08lvl1sec77/understanding-ml-algorithms-and-other-concepts](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/8/ch08lvl1sec77/understanding-ml-algorithms-and-other-concepts)。
- en: In the next section, we will look at the code that helps us perform training.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将查看帮助我们执行训练的代码。
- en: Training the baseline model
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练基线模型
- en: 'In this section, we will look at the code that helps us perform actual training
    on the training dataset. We will look at the implementation first, and then I
    will explain the code step by step. Here, we will be implementing Naive Bayes
    and SVM algorithms. For implementation, we will be using the scikit-learn library.
    You can find the code at this GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看帮助我们在实际训练数据集上执行实际训练的代码。我们首先查看实现，然后我将逐步解释代码。在这里，我们将实现朴素贝叶斯和SVM算法。对于实现，我们将使用scikit-learn库。您可以在以下GitHub链接找到代码：[https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Baseline_approach.ipynb)。
- en: Implementing the baseline model
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现基线模型
- en: 'In order to understand the implementation of the baseline model, you can refer
    to the following code snippet:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解基线模型实现，您可以参考以下代码片段：
- en: '![Implementing the baseline model](img/B08394_05_08.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![实现基线模型](img/B08394_05_08.jpg)'
- en: 'Figure 5.8: Code snippet for performing training using naive Bayes and SVM'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8：使用朴素贝叶斯和SVM进行训练的代码片段
- en: 'We have implemented the following four algorithms here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里实现了以下四种算法：
- en: Multinomial naive Bayes
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯
- en: C-support vector classification with kernel rbf
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带核函数rbf的C支持向量机分类
- en: C-support vector classification with kernel linear
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带核函数线性的C支持向量机分类
- en: Linear support vector classification
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性支持向量机分类
- en: Multinomial naive Bayes
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯
- en: As you can see in the preceding code snippet, we have used Multinomial naive
    Bayes. The multinomial naive Bayes classifier is suitable for classification with
    discrete features, which means that if the features are word counts or TF-IDF
    vectors, then we can use this classifier. The multinomial distribution normally
    requires integer feature counts. However, fractional counts such as TF-IDF might
    work as well. So, we have to apply this algorithm to `train_vectors`. The `fit()`
    method is the step where the actual training is performed. Here, we have used
    all hyper parameters by default.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码片段所示，我们使用了多项式朴素贝叶斯。多项式朴素贝叶斯分类器适用于具有离散特征的分类，这意味着如果特征是词频或TF-IDF向量，则可以使用此分类器。多项式分布通常需要整数特征计数。然而，如TF-IDF之类的分数计数也可能有效。因此，我们必须将此算法应用于`train_vectors`。`fit()`方法是实际训练执行的步骤。在这里，我们默认使用了所有超参数。
- en: C-support vector classification with kernel rbf
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带核函数rbf的C支持向量机分类
- en: 'We have also implemented SVM with the *rbf* kernel. Kernel is a function that
    will help train the algorithm. The equation for the *rbf* kernel function is as
    follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了具有*rbf*核的支持向量机。核是一个帮助训练算法的函数。*rbf*核函数的方程如下：
- en: '![C-support vector classification with kernel rbf](img/B08394_05_32.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![带核函数rbf的C支持向量机分类](img/B08394_05_32.jpg)'
- en: C-support vector classification with kernel linear
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带核函数线性的C支持向量机分类
- en: 'We have also implemented SVM with the linear kernel. The equation for the linear
    kernel function is as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了具有线性核的支持向量机。线性核函数的方程如下：
- en: '![C-support vector classification with kernel linear](img/B08394_05_33.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![带核函数线性的C支持向量机分类](img/B08394_05_33.jpg)'
- en: Linear support vector classification
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性支持向量分类
- en: We have also used linear support vector classification. We will use the `LinearSVC()`
    method type for implementing this classification. This is similar to SVC with
    the parameter kernel=*linear* but implemented in terms of liblinear rather than
    `libsvm`, so it has more flexibility in the choice of penalties and loss functions,
    and should scale better to a large number of samples.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用了线性支持向量分类。我们将使用`LinearSVC()`方法类型来实现这种分类。这与带有参数`kernel=linear`的SVC类似，但它是基于liblinear而不是`libsvm`实现的，因此它在选择惩罚和损失函数方面具有更大的灵活性，并且应该更好地扩展到大量样本。
- en: For all the preceding ML algorithms, we have provided input, which are `train_vectors`
    and `train_labels`. We will test the ML model accuracy by using the test vectors
    and comparing predicted labels with actual `test_labels`. Before performing testing,
    we will decide which kind of testing parameters we will be using. So let's look
    at the testing matrix.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有前面的机器学习算法，我们已经提供了输入，即`train_vectors`和`train_labels`。我们将通过使用测试向量和比较预测标签与实际的`test_labels`来测试机器学习模型的准确性。在进行测试之前，我们将决定使用哪种类型的测试参数。因此，让我们看看测试矩阵。
- en: Understanding the testing matrix
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解测试矩阵
- en: 'In this section, we will look at the testing matrix that we should consider
    in order to evaluate the trained ML models. For the baseline approach, we will
    be using the following five testing matrices:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨我们应该考虑的测试矩阵，以评估训练好的机器学习模型。对于基线方法，我们将使用以下五个测试矩阵：
- en: Precision
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度
- en: Recall
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率
- en: F1-score
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F1分数
- en: Support
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持度
- en: Training accuracy
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练准确度
- en: Before we understand these terms, let's cover some basic terms that will help
    us to understand the preceding terms.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解这些术语之前，让我们先了解一些基本术语，这将有助于我们理解前面的术语。
- en: '**True Positive** (**TP**)—If the classifier predicts that the given movie
    review carries a positive sentiment and that movie review has a positive sentiment
    in an actual scenario, then these kinds of test cases are considered TP. So, you
    can define the TP as if the test result is one that detects the condition when
    the condition is actually present.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阳性**（**TP**）—如果分类器预测给定的电影评论带有积极情感，而在实际情况下该电影评论也是积极情感，那么这类测试案例被认为是TP。因此，你可以将TP定义为测试结果检测到实际上存在的情况。'
- en: '**True Negative** (**TN**)—If the classifier predicts that the given movie
    review carries a negative sentiment and that movie review has a negative sentiment
    in an actual scenario, then those kinds of test cases are considered True Negative(TN).
    So, you can define the TN as if the test result is one that does not detect the
    condition when the condition is actually absent.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真阴性**（**TN**）—如果分类器预测给定的电影评论带有消极情感，而在实际情况下该电影评论也是消极情感，那么这类测试案例被认为是真阴性（TN）。因此，你可以将TN定义为测试结果没有检测到实际上不存在的情况。'
- en: '**False Positive** (**FP**)—If the classifier predicts that the given movie
    review carries a positive sentiment and that movie review has a negative sentiment
    in an actual scenario, then those kinds of test cases are considered False Positive
    (FP). So, you can define the FP as if the test result is one that detects the
    condition when the condition is actually absent. This is like a flase alram.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性**（**FP**）—如果分类器预测给定的电影评论带有积极情感，而在实际情况下该电影评论是消极情感，那么这类测试案例被认为是假阳性（FP）。因此，你可以将FP定义为测试结果检测到实际上不存在的情况。这就像是一个错误的警报。'
- en: '**False Negative** (**FN**)—If the classifier predicts that the given movie
    review carries a negative sentiment and that movie review has a positive sentiment
    in an actual scenario, then those kinds of test cases are considered False Negative
    (FN). So, you can define the FN as if the test result is one that does not detect
    the condition when the condition is actually present. This is the situation where
    certain conditions have been overlooked.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性**（**FN**）—如果分类器预测给定的电影评论带有消极情感，而在实际情况下该电影评论是积极情感，那么这类测试案例被认为是假阴性（FN）。因此，你可以将FN定义为测试结果没有检测到实际上存在的情况。这是某些条件被忽视的情况。'
- en: Now we will look at all five main testing matrices that use TP, TN, FP, and
    FN terms.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将查看所有五个主要测试矩阵，这些矩阵使用了TP、TN、FP和FN术语。
- en: Precision
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精确度
- en: 'Precision is the ability of the classifier to assign a positive class label
    for samples that originally belong to a positive class label. Precision does not
    assign a positive class for a given sample that originally belongs to a negative
    class. The equation for generating the precision score is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度是分类器为原本属于正类标签的样本分配正类标签的能力。精确度不会为原本属于负类标签的给定样本分配正类标签。生成精确度分数的公式如下：
- en: '![Precision](img/B08394_05_34.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![精确度](img/B08394_05_34.jpg)'
- en: Recall
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 召回率
- en: 'Recall is the ability of the classifier to find all the positive samples. The
    equation for generating recall score is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率是分类器找到所有正样本的能力。生成召回率分数的公式如下：
- en: '![Recall](img/B08394_05_35.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![召回率](img/B08394_05_35.jpg)'
- en: F1-Score
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: F1分数
- en: 'F1-score is the harmonic means of precision and recall. So you can find the
    equation as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数是精确度和召回率的调和平均值。所以你可以找到以下方程：
- en: '![F1-Score](img/B08394_05_36.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![F1分数](img/B08394_05_36.jpg)'
- en: Support
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 支持度
- en: 'Support is the number of occurrences of each class in true target labels. The
    value of support helps when it comes to calculating the average value for precision,
    recall, and F1-score. You can see the equations for calculating the average value
    of precision, recall, and F1-Score as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 支持度是每个类别在真实目标标签中出现的次数。支持度的值在计算精确度、召回率和F1分数的平均值时很有帮助。你可以看到计算精确度、召回率和F1分数平均值的公式如下：
- en: '![Support](img/B08394_05_37.jpg)![Support](img/B08394_05_38.jpg)![Support](img/B08394_05_39.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![支持度](img/B08394_05_37.jpg)![支持度](img/B08394_05_38.jpg)![支持度](img/B08394_05_39.jpg)'
- en: Actual calculation using the preceding formula will be provided in the *Testing
    the baseline mode* section. So bear with me for a while and we will see the actual
    testing result.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 实际使用前面公式进行的计算将在*测试基线模型*部分提供。所以请稍等片刻，我们将看到实际的测试结果。
- en: Training accuracy
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练准确度
- en: Training accuracy guides us in order to obtain the correct direction for developing
    any ML application. We test the trained ML model on the testing dataset. When
    we perform this testing, we have actual labels of the sentiment class for each
    of the testing records, and we also have the predicted sentiment class for all
    testing records so we can compare the results. So, the set of labels predicted
    for a testing dataset must exactly match the corresponding set of labels in the
    actual testing dataset. We count the records where our predicted labels are the
    same as actual labels, and then we convert this count to a percentage.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 训练准确度指导我们获得开发任何机器学习应用的正确方向。我们在测试数据集上测试训练好的机器学习模型。当我们进行这项测试时，我们拥有每个测试记录的实际情感类标签，我们也拥有所有测试记录的预测情感类标签，因此我们可以比较结果。所以，预测测试数据集的标签集合必须与实际测试数据集中的相应标签集合完全匹配。我们计算我们的预测标签与实际标签相同的记录，然后将这个计数转换为百分比。
- en: We will see all the testing matrices for each of the implemented ML algorithms
    in the next section, and then we will decide which algorithm performs well. So
    let's look at the implementation of testing the baseline model.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到每个实现机器学习算法的所有测试矩阵，然后我们将决定哪个算法表现良好。所以，让我们看看测试基线模型的实现。
- en: Testing the baseline model
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试基线模型
- en: Here, we will look at the code snippet that performs the actual testing. We
    will be obtaining all the testing matrices that have been explained so far. We
    are going to test all the different ML algorithms so that we can compare the accuracy
    score.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将查看执行实际测试的代码片段。我们将获取到目前为止所解释的所有测试矩阵。我们将测试所有不同的机器学习算法，以便我们可以比较准确度分数。
- en: Testing of Multinomial naive Bayes
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多项式朴素贝叶斯测试
- en: 'You can see the testing result for the multinomial naive Bayes algorithm in
    the following figure:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的图中看到多项式朴素贝叶斯算法的测试结果：
- en: '![Testing of Multinomial naive Bayes](img/B08394_05_09.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![多项式朴素贝叶斯测试](img/B08394_05_09.jpg)'
- en: 'Figure 5.9: Code snippet for testing multinomial naive Bayes algorithm'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9：测试多项式朴素贝叶斯算法的代码片段
- en: As you can see, using this algorithm we have achieved an accuracy score of 81.5%.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用这个算法我们实现了81.5%的准确度分数。
- en: Testing of SVM with rbf kernel
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用rbf核的SVM测试
- en: 'You can see the testing result for SVM with the rbf kernel algorithm in the
    following figure:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的图中看到使用rbf核的SVM算法的测试结果：
- en: '![Testing of SVM with rbf kernel](img/B08394_05_10.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![使用rbf核的SVM测试](img/B08394_05_10.jpg)'
- en: 'Figure 5.10: Code snippet for testing SVM with rbf kernel'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10：使用rbf核测试SVM的代码片段
- en: As you can see, we have performed a test on the testing dataset and obtained
    an accuracy of 65.4%.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们在测试数据集上进行了测试，并获得了65.4%的准确率。
- en: Testing SVM with the linear kernel
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性核测试SVM
- en: 'You can see the testing result for SVM with the linear kernel algorithm in
    the following figure:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下图中看到使用线性核算法的SVM的测试结果：
- en: '![Testing SVM with the linear kernel](img/B08394_05_11.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![使用线性核测试SVM](img/B08394_05_11.jpg)'
- en: 'Figure 5.11: Code snippet for testing SVM with linear kernel'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11：测试使用线性核的SVM的代码片段
- en: As you can see, we have performed a test on the testing dataset and obtained
    an accuracy of 83.6%.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们在测试数据集上进行了测试，并获得了83.6%的准确率。
- en: Testing SVM with linearSVC
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用linearSVC测试SVM
- en: 'You can see the testing result for SVM with the linearSVC kernel algorithm
    in the following figure:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下图中看到使用linearSVC核算法的SVM的测试结果：
- en: '![Testing SVM with linearSVC](img/B08394_05_12.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![使用linearSVC测试SVM](img/B08394_05_12.jpg)'
- en: 'Figure 5.12: Code snippet for testing SVM with linearSVC kernel'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12：测试使用linearSVC核的SVM的代码片段
- en: We have performed a test on the testing dataset here and obtained an accuracy
    of 83.6%.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里对测试数据集进行了测试，并获得了83.6%的准确率。
- en: So, after seeing the accuracy score of each of the implemented algorithms, we
    can say that SVM with linear kernel and linearSVC is performing really well. Now,
    you may wonder whether we can improve the accuracy. We can do that for sure. So
    let's discuss the problems that exist in this baseline approach.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在看到每个实现算法的准确率得分后，我们可以说，使用线性核和linearSVC的SVM表现非常好。现在，您可能会想知道我们是否可以提高准确率。我们当然可以做到。那么，让我们讨论这个基线方法中存在的问题。
- en: Problem with the existing approach
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现有方法的问题
- en: 'In the baseline approach, we got great accuracy. However, we ignored the following
    points, which we can be implemented in our revised approach:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在基线方法中，我们获得了很高的准确率。然而，我们忽略了以下这些点，这些点我们可以在我们的改进方法中实现：
- en: We did not focus on word embedding-based techniques
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们没有关注基于词嵌入的技术
- en: '**Deep learning** (**DL**) algorithms such as CNN can be helpful for us'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度学习**（**DL**）算法，如CNN，对我们可能有所帮助'
- en: We need to focus on these two points because word embedding-based techniques
    really help retain the semantics of the text. So we should use these techniques
    as well as the DL-based-algorithm, which helps us provide more accuracy because
    DL algorithms perform well when a nested data structure is involved. What do I
    mean by a nested data structure? Well, that means any written sentence or spoken
    sentence made up of phrases, phrases made of words, and so on. So, natural language
    has a nested data structure. DL algorithms help us understand the nested structure
    of the sentences from our text dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要关注这两个点，因为基于词嵌入的技术确实有助于保留文本的语义。因此，我们应该使用这些技术以及基于深度学习算法，这有助于我们提供更高的准确率，因为当涉及嵌套数据结构时，深度学习算法表现良好。我所说的嵌套数据结构是什么意思呢？这意味着任何由短语组成的书面句子或口头句子，短语由单词组成，依此类推。因此，自然语言具有嵌套数据结构。深度学习算法帮助我们理解文本数据集中的句子嵌套结构。
- en: How to optimize the existing approach
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何优化现有方法
- en: 'There are certain techniques that can help us improve this application. The
    key techniques that can help us improvise the baseline approach are as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有某些技术可以帮助我们改进这个应用程序。以下是可以帮助我们改进基线方法的关键技术：
- en: We can use word embedding-based techniques such as Word2Vec, glove, and so on
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用基于词嵌入的技术，如Word2Vec、glove等
- en: We should also implement **Convolution Neural Networks** (**CNN** ) to get an
    idea about how a deep learning algorithm can help us
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还应该实现**卷积神经网络**（**CNN**）来了解深度学习算法如何帮助我们
- en: So in the revised approach, we will be focusing on word embedding techniques
    and the Deep Learning algorithm. We will be using Keras with the TensorFlow backend.
    Before implementation, let's understand the revised approach in detail.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在改进方法中，我们将重点关注词嵌入技术和深度学习算法。我们将使用具有TensorFlow后端的Keras。在实施之前，让我们详细了解改进方法。
- en: Understanding key concepts for optimizing the approach
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解优化方法的关键概念
- en: 'In this section, we will understand the revised approach in detail, so we know
    what steps we should implement. We are using Keras, a Deep Learning library that
    provides us with high-level APIs so we can implement CNN easily. The following
    steps are involved:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将详细了解改进方法，以便我们知道应该实施哪些步骤。我们正在使用Keras，这是一个提供高级API的深度学习库，使我们能够轻松实现CNN。以下步骤包括：
- en: '**Importing the dependencies**: In this step, we will be importing different
    dependencies such as Numpy and Keras with the TensorFlow backend. We will be using
    different APIs belonging to the Keras library.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**导入依赖项**：在这个步骤中，我们将导入不同的依赖项，如Numpy和Keras（使用TensorFlow后端）。我们将使用Keras库的不同API。'
- en: '**Downloading and loading the IMDb dataset**: In this step, we will be downloading
    the IMDb dataset and loading this dataset by using Keras APIs'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下载和加载IMDb数据集**：在这个步骤中，我们将下载IMDb数据集，并使用Keras API加载此数据集。'
- en: '**Choosing top words and maximum text length**: In this stage, we will set
    the value of our vocabulary, which we can use during the word embedding stage.
    So, we have selected the top 10,000 words. After that, we have restricted the
    length of movie reviews to 1600.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择高频词和最大文本长度**：在这个阶段，我们将设置我们可以在词嵌入阶段使用的词汇表值。因此，我们选择了前10,000个高频词。之后，我们将电影评论的长度限制为1600个字符。'
- en: '**Implementing word embedding**: At this stage of the code, we will be using
    default embedding techniques from Keras and generate the word vector with a length
    of 300.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现词嵌入**：在代码的这个阶段，我们将使用Keras的默认嵌入技术，并生成长度为300的词向量。'
- en: '**Building a CNN**: In this stage, we will be making three-layer neural networks,
    where the first layer has 64 neurons, the second layer has 32 neurons, and the
    last layer has 16 neurons. Here, we are using sigmoid as an activation function.
    The Activation function introduces non-linearity to the neural network so that
    we can generate a probability score for each class using mathematical functions.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建CNN**：在这个阶段，我们将构建一个三层神经网络，其中第一层有64个神经元，第二层有32个神经元，最后一层有16个神经元。在这里，我们使用sigmoid作为激活函数。激活函数为神经网络引入非线性，这样我们就可以使用数学函数为每个类别生成一个概率分数。'
- en: '**Training and obtaining the accuracy**: Finally, we train the model and generate
    the accuracy score. We have set the epoch value to 3 and set adam as the optimization
    function and our loss function is `binary_crossentropy`. Epoch basically indicates
    how many times we need to perform training on our whole dataset. Cross-entropy
    loss measures the performance of a classifier whose output is a probability value
    between 0 and 1\. Cross-entropy loss increases as the predicted probability differs
    from the actual label. After training, we will generate the accuracy of the model.
    This training stage may take time as well as a great amount of computation power.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练并获得准确率**：最后，我们将训练模型并生成准确率分数。我们将epoch值设置为3，将adam作为优化函数，我们的损失函数是`binary_crossentropy`。Epoch基本上表示我们需要在全部数据集上执行训练的次数。交叉熵损失衡量的是输出概率值在0到1之间的分类器的性能。当预测概率与实际标签不同时，交叉熵损失会增加。训练完成后，我们将生成模型的准确率。这个训练阶段可能需要时间，以及大量的计算能力。'
- en: Now let's see the code in the next section.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看下一节中的代码。
- en: Implementing the revised approach
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现改进方法
- en: 'In this section, we will see the implementation in the form of a code snippet.
    We will be following the same step that we saw in the previous section. So, without
    any delay, let''s look at the code. You can refer to this code by using this GitHub
    link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Revised_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Revised_approach.ipynb).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将以代码片段的形式展示实现。我们将遵循之前章节中看到相同的步骤。所以，无需任何延迟，让我们来看看代码。您可以通过以下GitHub链接参考此代码：[https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Revised_approach.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Revised_approach.ipynb)。
- en: Importing the dependencies
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 导入依赖项
- en: 'You can refer to the code snippet in the following figure, where you can find
    the imported dependencies as well:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下图中的代码片段中找到导入的依赖项：
- en: '![Importing the dependencies](img/B08394_05_13.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![导入依赖项](img/B08394_05_13.jpg)'
- en: 'Figure 5.13: Code snippet where we can see the imported dependencies'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13：我们可以看到导入依赖项的代码片段
- en: As you can see, we have used the TensorFlow backend with the Keras library.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们使用了TensorFlow后端和Keras库。
- en: Downloading and loading the IMDb dataset
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载和加载IMDb数据集
- en: 'You can refer to the code snippet in the following figure, where you can also
    find the code for downloading and loading the IMDb dataset:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下图中参考代码片段，其中也可以找到下载和加载IMDb数据集的代码：
- en: '![Downloading and loading the IMDb dataset](img/B08394_05_14.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![下载和加载IMDb数据集](img/B08394_05_14.jpg)'
- en: 'Figure 5.14: Code snippet for downloading and loading the IMDb dataset'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14：下载和加载IMDb数据集的代码片段
- en: We have also set the value of the vocabulary.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设置了词汇的值。
- en: Choosing the top words and the maximum text length
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择顶级词汇和最大文本长度
- en: 'At this stage, we have set the top word values as well as the maximum text
    length value. You can refer to the code snippet in the following figure:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经设置了顶级词汇值以及最大文本长度值。您可以参考以下图中的代码片段：
- en: '![Choosing the top words and the maximum text length](img/B08394_05_15.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![选择顶级词汇和最大文本长度](img/B08394_05_15.jpg)'
- en: 'Figure 5.15: Code snippet to set the vocabulary value and the maximum text
    length value'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15：设置词汇值和最大文本长度值的代码片段
- en: In the first part of code, we have set the top_word parameter to 10,000, and
    in the second part of the code, we have set the length of the movie review to
    1,600\. The top_word indicates the vocabulary size. From our dataset, we have
    picked the top 10,000 unique words. Most of the movie reviews have words that
    are present in our word vocabulary. Here, we are not processing very long movie
    reviews, because of that reason we have set the length of the movie review.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的第一部分，我们将top_word参数设置为10,000，在代码的第二部分，我们将电影评论的长度设置为1,600。top_word表示词汇表的大小。从我们的数据集中，我们选择了前10,000个独特的单词。大多数电影评论中的单词都包含在我们的词词汇表中。在这里，我们不是处理非常长的电影评论，因此我们设置了电影评论的长度。
- en: Implementing word embedding
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现词嵌入
- en: 'At this stage, we have implemented the default Keras word embedding method
    in order to obtain a feature vector with a size of 300\. You can refer to the
    following code snippet:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们实现了默认的Keras词嵌入方法，以获得一个大小为300的特征向量。您可以参考以下代码片段：
- en: '![Implementing word embedding](img/B08394_05_16.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![实现词嵌入](img/B08394_05_16.jpg)'
- en: 'Figure 5.16: Code snippet for obtaining word feature vector based on the word
    embedding technique'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16：基于词嵌入技术获取词特征向量的代码片段
- en: Building a convolutional neural net (CNN)
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建卷积神经网络（CNN）
- en: 'In this section you can refer to the code, which will help you understand the
    architecture of a neural net. Here, we have used CNN because it handles higher
    level features or a nested structure of the dataset really well. You can refer
    to the code snippet in the following figure:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您可以参考代码，这将帮助您理解神经网络架构。在这里，我们使用了卷积神经网络（CNN），因为它处理数据集的高级特征或嵌套结构非常出色。您可以参考以下图中的代码片段：
- en: '![Building a convolutional neural net (CNN)](img/B08394_05_17.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![构建卷积神经网络（CNN）](img/B08394_05_17.jpg)'
- en: 'Figure 5.17: Code snippet for building CNN'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17：构建卷积神经网络（CNN）的代码片段
- en: Here, we have built the neural network with two dense layers in it.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们构建了一个包含两个密集层的神经网络。
- en: Training and obtaining the accuracy
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练和获取准确率
- en: 'At this stage, we have performed the training. You can refer to the code in
    the following figure:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经完成了训练。您可以参考以下图中的代码：
- en: '![Training and obtaining the accuracy](img/B08394_05_18.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![训练和获取准确率](img/B08394_05_18.jpg)'
- en: 'Figure 5.18: Code snippet for performing training on the training dataset'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18：在训练数据集上执行训练的代码片段
- en: Here, we will be performing training three times as our epoch value is set to
    3\. As sentiment analysis is a binary problem, we have used `binary_crossentropy`
    as loss function. If you have a GPU-based computer, then the training time will
    be less; otherwise, this step is time consuming and the computation power consuming.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将进行三次训练，因为我们的epoch值设置为3。由于情感分析是一个二元问题，我们使用了`binary_crossentropy`作为损失函数。如果您有一台基于GPU的计算机，那么训练时间会更短；否则，这一步会花费更多的时间和计算能力。
- en: 'Once training is done, we can obtain training accuracy. For training accuracy,
    you can refer to the following figure:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成训练，我们就可以获得训练准确率。对于训练准确率，您可以参考以下图：
- en: '![Training and obtaining the accuracy](img/B08394_05_19.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![训练和获取准确率](img/B08394_05_19.jpg)'
- en: 'Figure 5.19: Code snippet for obtaining training accuracy'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19：获取训练准确率的代码片段
- en: Here, the accuracy is the training accuracy. Now we need to obtain testing accuracy
    because that will give us an actual idea about how well the train model is performing
    on unseen data. So let's see what the testing accuracy is.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，准确率是训练准确率。现在我们需要获得测试准确率，因为这将给我们一个关于训练模型在未见数据上表现如何的实际想法。那么，让我们看看测试准确率是多少。
- en: Testing the revised approach
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试改进的方法
- en: 'In this section, we will obtain the accuracy of the testing dataset. You can
    refer to the code snippet in the following figure:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将获得测试数据集的准确率。您可以在以下图中的代码片段中参考：
- en: '![Testing the revised approach](img/B08394_05_20.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![测试改进方法](img/B08394_05_20.jpg)'
- en: 'Figure 5.20: Code snippet for obtaining testing accuracy'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20：获取测试准确率的代码片段
- en: After obtaining the testing accuracy, we got an accuracy value of 86.45%. This
    testing accuracy is better than our baseline approach. Now let's see what points
    we can improve in order to come up with the best approach.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得测试准确率后，我们得到了86.45%的准确率。这个测试准确率比我们的基线方法更好。现在让我们看看我们可以改进哪些方面，以提出最佳方法。
- en: Understanding problems with the revised approach
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解改进方法中的问题
- en: 'In this section, we will discuss what points of the revised approach we can
    improve. These are the points that we can implement in order to obtain the best
    possible approach:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论我们可以改进的改进方法的哪些方面。这些是我们可以实现以获得最佳可能方法的点：
- en: We can use pretrained Word2Vec or glove models to generate the word vector
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用预训练的Word2Vec或GloVe模型来生成词向量
- en: We should use a recurrent neural network with LSTM to get better output
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们应该使用具有LSTM的循环神经网络来获得更好的输出
- en: In this section, we will understand and implement the best approach, where we
    will load the pretrained glove (global word vector) model and use an RNN and LSTM
    network.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解并实现最佳方法，其中我们将加载预训练的GloVe（全局词向量）模型，并使用RNN和LSTM网络。
- en: The best approach
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳方法
- en: 'There are some steps that we can follow in order to obtain the best possible
    approach. In this approach, we have used a glove pretrained model and have trained
    the model using the RNN and LSTM networks. The glove model has been pretrained
    on a large dataset so that it can generate more accurate vector values for words.
    That is the reason we are using glove here. In the next section, we will look
    at the implementation of the best approach. You can find all the code at this
    GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Best_approach_sentiment_analysis.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Best_approach_sentiment_analysis.ipynb).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳方法，我们可以遵循以下步骤。在这个方法中，我们使用了GloVe预训练模型，并使用RNN和LSTM网络进行模型训练。GloVe模型在大型数据集上进行了预训练，以便为单词生成更准确的向量值。这就是我们在这里使用GloVe的原因。在下一节中，我们将查看最佳方法的实现。所有代码都可以在这个GitHub链接中找到：[https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Best_approach_sentiment_analysis.ipynb](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/Best_approach_sentiment_analysis.ipynb)。
- en: Implementing the best approach
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现最佳方法
- en: 'In order to implement the best approach, we will be performing the following
    steps:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现最佳方法，我们将执行以下步骤：
- en: Loading the glove model
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载GloVe模型
- en: Loading the dataset
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Preprocessing
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理
- en: Loading the precomputed ID matrix
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载预计算的ID矩阵
- en: Splitting the train and test datasets
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 划分训练和测试数据集
- en: Building a neural network
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: Training the neural network
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: Loading the trained model
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载训练模型
- en: Testing the trained model
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试训练模型
- en: Loading the glove model
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载GloVe模型
- en: 'In order to get the best performance, we will be using the pretrained glove
    model. You can download it at [https://nlp.stanford.edu/projects/glove/.](https://nlp.stanford.edu/projects/glove/.)
    We have already generated the binary file and saved that file as an `.npy` extension.
    This file is with the `wordsList.npy` file. You can refer to the code snippet
    in the following figure:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得最佳性能，我们将使用预训练的GloVe模型。您可以在[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)下载它。我们已生成二进制文件并将其保存为`.npy`扩展名。此文件与`wordsList.npy`文件一起。您可以在以下图中的代码片段中参考：
- en: '![Loading the glove model](img/B08394_05_21.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![加载GloVe模型](img/B08394_05_21.jpg)'
- en: 'Figure 5.21: Code snippet for loading the glove pretrained model'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21：加载GloVe预训练模型的代码片段
- en: Dimensionality of the word vector is 50 and this model contains word vectors
    for 400,000 words.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 词向量的维度是50，该模型包含400,000个单词的词向量。
- en: Loading the dataset
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'You can refer to the following code snippet:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下代码片段：
- en: '![Loading the dataset](img/B08394_05_22.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![加载数据集](img/B08394_05_22.jpg)'
- en: 'Figure 5.22: Code snippet for loading the dataset'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22：加载数据集的代码片段
- en: We have considered 25,000 movie reviews in total.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们总共考虑了25,000条电影评论。
- en: Preprocessing
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理
- en: 'You can refer to the code snippet in the following figure:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下图中的代码片段：
- en: '![Preprocessing](img/B08394_05_23.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![预处理](img/B08394_05_23.jpg)'
- en: 'Figure 5.23: Code snippet for performing pre-processing'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.23：执行预处理的代码片段
- en: Loading precomputed ID matrix
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载预计算的ID矩阵
- en: 'In this section, we are generating the index for each word. This process is
    computationally expensive, so I have already generated the index matrix and made
    it ready for loading. You can refer to the code snippet in the following figure:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们正在为每个单词生成索引。这个过程计算量很大，所以我已经生成了索引矩阵并使其准备好加载。您可以参考以下图中的代码片段：
- en: '![Loading precomputed ID matrix](img/B08394_05_24.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![加载预计算的ID矩阵](img/B08394_05_24.jpg)'
- en: 'Figure 5.24: Code snippet for generating matrix for word IDs'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.24：生成单词ID矩阵的代码片段
- en: Splitting the train and test datasets
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分割训练和测试数据集
- en: 'In this section, we will see the code snippet for generating the training and
    testing datasets. You can refer to the code snippet in the following figure:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到生成训练和测试数据集的代码片段。您可以参考以下图中的代码片段：
- en: '![Splitting the train and test datasets](img/B08394_05_25.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![分割训练和测试数据集](img/B08394_05_25.jpg)'
- en: 'Figure 5.25: Code snippet for splitting dataset into training and testing dataset'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.25：将数据集分割成训练集和测试集的代码片段
- en: Building a neural network
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建神经网络
- en: We have used a **recurrent neural net** (**RNN**) with **Long-Short Term Memory
    Unit** (**LSTMs**) cells as a part of their hidden states. LSTM cells are used
    to store sequential information. If you have multiple sentences, then LSTM stores
    the context of the previous or previous to previous sentences, which helps us
    improve this application. If you want to understand LSTM in detail, then you can
    refer to [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了一个**循环神经网络**（**RNN**）和作为其隐藏状态一部分的**长短期记忆单元**（**LSTMs**）。LSTM单元用于存储序列信息。如果您有多个句子，那么LSTM存储前一个或前前个句子的上下文，这有助于我们改进此应用。如果您想详细了解LSTM，可以参考[http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)。
- en: 'You can refer to the code snippet in the following figure:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下图中的代码片段：
- en: '![Building a neural network](img/B08394_05_26.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![构建神经网络](img/B08394_05_26.jpg)'
- en: 'Figure 5.26: Code snippet for building RNN with LSTM'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.26：构建具有LSTM的RNN的代码片段
- en: First, we define the hyper parameters. We set the batch size to 64, LSTM units
    to 64, the number of classes to 2, and then we perform 100,000 iterations.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义超参数。我们将批大小设置为64，LSTM单元设置为64，类别数量设置为2，然后执行100,000次迭代。
- en: Training the neural network
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'In this section, you can refer to the code snippet in the following figure,
    which is used for performing training:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您可以参考以下图中的代码片段，该代码片段用于执行训练：
- en: '![Training the neural network](img/B08394_05_27.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![训练神经网络](img/B08394_05_27.jpg)'
- en: 'Figure 5.27: Code snippet for performing training'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.27：执行训练的代码片段
- en: 'After each 10,000 iterations, we save the model. During the training, you can
    monitor the progress by using TensorBoard. You can refer to the following figure,
    which shows the progress over the period of training. You can monitor the accuracy
    and loss percentage during training, so you find out how the DL model is converging
    during training:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 每经过10,000次迭代后，我们保存模型。在训练过程中，您可以使用TensorBoard来监控进度。您可以参考以下图，它显示了训练期间的进度。您可以在训练期间监控准确率和损失百分比，以便了解深度学习模型在训练过程中的收敛情况：
- en: '![Training the neural network](img/B08394_05_28.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![训练神经网络](img/B08394_05_28.jpg)'
- en: 'Figure 5.28: Accuracy and loss graph generated during training on TensorBoard'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.28：在TensorBoard上训练过程中生成的准确率和损失图
- en: 'Training is time consuming and computationally expensive, so with a GPU it
    may take 2- 3 hours to train the model. Therefore, you can use the pretrained
    model by downloading it from GitHub at: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 训练耗时且计算量大，因此使用GPU可能需要2-3小时来训练模型。因此，您可以从GitHub下载预训练模型：[https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz)。
- en: Loading the trained model
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载训练好的模型
- en: 'Once training is done, we can save the trained model. After loading this model,
    we can check its accuracy as well. You can refer to the code snippet in the following
    figure:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成训练，我们就可以保存训练好的模型。加载此模型后，我们可以检查其准确率。您可以参考以下图中的代码片段：
- en: '![Loading the trained model](img/B08394_05_29.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![加载训练模型](img/B08394_05_29.jpg)'
- en: 'Figure 5.29: Code snippet for generating testing accuracy'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.29：生成测试准确率的代码片段
- en: Here we have generated a testing accuracy of 91.66%.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们生成了一个测试准确率为91.66%。
- en: Testing the trained model
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试训练模型
- en: 'In this section, we will be passing new movie reviews, and by loading the trained
    model, we will generate the prediction of sentiment. You can refer to the code
    snippet in the following figure:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将传递新的电影评论，并通过加载训练模型来生成情感预测。你可以参考以下图中的代码片段：
- en: '![Testing the trained model](img/B08394_05_30.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![测试训练模型](img/B08394_05_30.jpg)'
- en: 'Figure 5.30: Code snippet for loading trained model and generating sentiment
    for a given sentence of the movie review'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.30：加载训练模型并为电影评论的给定句子生成情感分析的代码片段
- en: 'In this snippet, we have passed one sentence as part of a movie review, and
    our trained model identifies it as a negative sentiment. You can also refer to
    the code snippet in the following figure, which generates a sentiment for the
    sentence carrying a negative word in it:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个片段中，我们传递了一个作为电影评论一部分的句子，我们的训练模型将其识别为负面情感。你也可以参考以下图中的代码片段，它为包含负面词汇的句子生成了情感分析：
- en: '![Testing the trained model](img/B08394_05_31.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![测试训练模型](img/B08394_05_31.jpg)'
- en: 'Figure 5.31: Code snippet for loading trained model and generating sentiment
    for given sentence of the movie review'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.31：加载训练模型并为电影评论的给定句子生成情感分析的代码片段
- en: This approach gives you great results.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法能给你带来非常好的结果。
- en: Summary
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at how to build a sentiment analysis model that gives
    us state-of-the-art results. We used an IMDb dataset that had positive and negative
    movie reviews and understood the dataset. We applied the machine learning algorithm
    in order to get the baseline model. After that, in order to optimize the baseline
    model, we changed the algorithm and applied deep-learning-based algorithms. We
    used glove, RNN, and LSTM techniques to achieve the best results. We learned how
    to build sentiment analysis applications using Deep Learning. We used TensorBoard
    to monitor our model's training progress. We also touched upon modern machine
    learning algorithms as well as Deep Learning techniques for developing sentiment
    analysis, and the Deep Learning approach works best here.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何构建一个能够给出最先进结果的情感分析模型。我们使用了一个包含正面和负面电影评论的IMDb数据集，并理解了这个数据集。我们应用了机器学习算法以获得基线模型。之后，为了优化基线模型，我们改变了算法并应用了基于深度学习的算法。我们使用了glove、RNN和LSTM技术以实现最佳结果。我们学习了如何使用深度学习构建情感分析应用。我们使用TensorBoard来监控我们模型的训练进度。我们还简要介绍了用于开发情感分析的现代机器学习算法以及深度学习技术，深度学习方法在这里效果最佳。
- en: 'We used a GPU to train the neural network, so if you discover that it needs
    more computation power from your end to train the model, then you can use the
    Google cloud or **Amazon Web Services** (**AWS**) GPU-based instances. I have
    already uploaded the pretrained model, so you can directly use that as well. You
    can find the pretrained model at this GitHub link: [https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz).'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用GPU来训练神经网络，所以如果你发现你的端需要更多的计算能力来训练模型，那么你可以使用Google Cloud或**亚马逊网络服务**（**AWS**）基于GPU的实例。我已经上传了预训练模型，所以你也可以直接使用它。你可以在以下GitHub链接中找到预训练模型：[https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz](https://github.com/jalajthanaki/Sentiment_Analysis/blob/master/models.tar.gz)。
- en: In the next chapter, we will build a job recommendation system that will help
    people find jobs, especially related to the job profiles in which they are interested.
    For a job recommendation system, we will be using various resources for linking
    resumes, job search queries, and so on. Again, we will be developing this system
    using machine learning and Deep Learning systems.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将构建一个工作推荐系统，帮助人们找到工作，特别是与他们感兴趣的工作档案相关的工作。对于工作推荐系统，我们将使用各种资源来链接简历、工作搜索查询等。同样，我们将使用机器学习和深度学习系统来开发这个系统。
