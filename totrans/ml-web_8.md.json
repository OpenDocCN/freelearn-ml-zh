["```py\n\nmkdir  movie_reviews_analyzer_app\n\ncd  movie_reviews_analyzer_app\n\ndjango-admin startproject webmining_server\n\npython manage.py startapp startapp pages\n\n```", "```py\n\npython manage.py createsuperuser (admin/admin)\n\npython manage.py runserver\n\n```", "```py\nnum_reviews = 30 \ndef bing_api(query):\n    keyBing = API_KEY        # get Bing key from: https://datamarket.azure.com/account/keys\n    credentialBing = 'Basic ' + (':%s' % keyBing).encode('base64')[:-1] # the \"-1\" is to remove the trailing \"\\n\" which encode adds\n    searchString = '%27X'+query.replace(\" \",'+')+'movie+review%27'\n    top = 50#maximum allowed by Bing\n\n    reviews_urls = []\n    if num_reviews<top:\n        offset = 0\n        url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \\\n              'Query=%s&$top=%d&$skip=%d&$format=json' % (searchString, num_reviews, offset)\n\n        request = urllib2.Request(url)\n        request.add_header('Authorization', credentialBing)\n        requestOpener = urllib2.build_opener()\n        response = requestOpener.open(request)\n        results = json.load(response)\n        reviews_urls = [ d['Url'] for d in results['d']['results']]\n    else:\n        nqueries = int(float(num_reviews)/top)+1\n        for i in xrange(nqueries):\n            offset = top*i\n            if i==nqueries-1:\n                top = num_reviews-offset\n                url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \\\n                      'Query=%s&$top=%d&$skip=%d&$format=json' % (searchString, top, offset)\n\n                request = urllib2.Request(url)\n                request.add_header('Authorization', credentialBing)\n                requestOpener = urllib2.build_opener()\n                response = requestOpener.open(request) \n            else:\n                top=50\n                url = 'https://api.datamarket.azure.com/Bing/Search/Web?' + \\\n                      'Query=%s&$top=%d&$skip=%d&$format=json' % (searchString, top, offset)\n\n                request = urllib2.Request(url)\n                request.add_header('Authorization', credentialBing)\n                requestOpener = urllib2.build_opener()\n                response = requestOpener.open(request) \n            results = json.load(response)\n            reviews_urls += [ d['Url'] for d in results['d']['results']]\n    return reviews_urls\n```", "```py\n\nsudo pip install Scrapy \n\n```", "```py\n\nsudo easy_install scrapy\n\n```", "```py\n\nscrapy startproject scrapy_spider\n\n```", "```py\n\n├── __init__.py\n\n├── items.py\n\n├── pipelines.py\n\n├── settings.py ├── spiders\n\n├── spiders\n\n│   ├── __init__.py\n\n```", "```py\n\nfrom newspaper import Article\n\nfrom urlparse import urlparse\n\nfrom scrapy.selector import Selector\n\nfrom scrapy import Spider\n\nfrom scrapy.spiders import BaseSpider,CrawlSpider, Rule\n\nfrom scrapy.http import Request\n\nfrom scrapy_spider import settings\n\nfrom scrapy_spider.items import PageItem,SearchItem\n\nunwanted_domains = ['youtube.com','www.youtube.com']\n\nfrom nltk.corpus import stopwords\n\nstopwords = set(stopwords.words('english'))\n\ndef CheckQueryinReview(keywords,title,content):\n\n content_list = map(lambda x:x.lower(),content.split(' '))\n\n title_list = map(lambda x:x.lower(),title.split(' '))\n\n words = content_list+title_list\n\n for k in keywords:\n\n if k in words:\n\n return True\n\n return False\n\nclass Search(Spider):\n\n name = 'scrapy_spider_reviews'\n\n def __init__(self,url_list,search_key):#specified by -a\n\n self.search_key = search_key\n\n self.keywords = [w.lower() for w in search_key.split(\" \") if w not in stopwords]\n\n self.start_urls =url_list.split(',')\n\n super(Search, self).__init__(url_list)\n\n def start_requests(self):\n\n for url in self.start_urls:\n\n yield Request(url=url, callback=self.parse_site,dont_filter=True)\n\n def parse_site(self, response):\n\n ## Get the selector for xpath parsing or from newspaper\n\n def crop_emptyel(arr):\n\n return [u for u in arr if u!=' ']\n\n domain = urlparse(response.url).hostname\n\n a = Article(response.url)\n\n a.download()\n\n a.parse()\n\n title = a.title.encode('ascii','ignore').replace('\\n','')\n\n sel = Selector(response)\n\n if title==None:\n\n title = sel.xpath('//title/text()').extract()\n\n if len(title)>0:\n\n title = title[0].encode('utf-8').strip().lower()\n\n content = a.text.encode('ascii','ignore').replace('\\n','')\n\n if content == None:\n\n content = 'none'\n\n if len(crop_emptyel(sel.xpath('//div//article//p/text()').extract()))>1:\n\n contents = crop_emptyel(sel.xpath('//div//article//p/text()').extract())\n\n print 'divarticle'\n\n ….\n\n elif len(crop_emptyel(sel.xpath('/html/head/meta[@name=\"description\"]/@content').extract()))>0:\n\n contents = crop_emptyel(sel.xpath('/html/head/meta[@name=\"description\"]/@content').extract())\n\n content = ' '.join([c.encode('utf-8') for c in contents]).strip().lower()\n\n #get search item \n\n search_item = SearchItem.django_model.objects.get(term=self.search_key)\n\n #save item\n\n if not PageItem.django_model.objects.filter(url=response.url).exists():\n\n if len(content) > 0:\n\n if CheckQueryinReview(self.keywords,title,content):\n\n if domain not in unwanted_domains:\n\n newpage = PageItem()\n\n newpage['searchterm'] = search_item\n\n newpage['title'] = title\n\n newpage['content'] = content\n\n newpage['url'] = response.url\n\n newpage['depth'] = 0\n\n newpage['review'] = True\n\n #newpage.save()\n\n return newpage \n\n else:\n\n return null\n\n```", "```py\n\nscrapy crawl scrapy_spider_reviews -a url_list=listname -a search_key=keyname\n\n```", "```py\n\nclass ReviewPipeline(object):\n\n def process_item(self, item, spider):\n\n #if spider.name == 'scrapy_spider_reviews':#not working\n\n item.save()\n\n return item\n\n```", "```py\n\n#from scrapy.spider import Spider\n\nfrom scrapy.selector import Selector\n\nfrom scrapy.contrib.spiders import CrawlSpider, Rule\n\nfrom scrapy.linkextractors import LinkExtractor\n\nfrom scrapy.http import Request\n\nfrom scrapy_spider.items import PageItem,LinkItem,SearchItem\n\nclass Search(CrawlSpider):\n\n name = 'scrapy_spider_recursive'\n\n def __init__(self,url_list,search_id):#specified by -a\n\n #REMARK is allowed_domains is not set then ALL are allowed!!!\n\n self.start_urls = url_list.split(',')\n\n self.search_id = int(search_id)\n\n #allow any link but the ones with different font size(repetitions)\n\n self.rules = (\n\n Rule(LinkExtractor(allow=(),deny=('fontSize=*','infoid=*','SortBy=*', ),unique=True), callback='parse_item', follow=True), \n\n )\n\n super(Search, self).__init__(url_list)\n\n def parse_item(self, response):\n\n sel = Selector(response)\n\n ## Get meta info from website\n\n title = sel.xpath('//title/text()').extract()\n\n if len(title)>0:\n\n title = title[0].encode('utf-8')\n\n contents = sel.xpath('/html/head/meta[@name=\"description\"]/@content').extract()\n\n content = ' '.join([c.encode('utf-8') for c in contents]).strip()\n\n fromurl = response.request.headers['Referer']\n\n tourl = response.url\n\n depth = response.request.meta['depth']\n\n #get search item \n\n search_item = SearchItem.django_model.objects.get(id=self.search_id)\n\n #newpage\n\n if not PageItem.django_model.objects.filter(url=tourl).exists():\n\n newpage = PageItem()\n\n newpage['searchterm'] = search_item\n\n newpage['title'] = title\n\n newpage['content'] = content\n\n newpage['url'] = tourl\n\n newpage['depth'] = depth\n\n newpage.save()#cant use pipeline cause the execution can finish here\n\n #get from_id,to_id\n\n from_page = PageItem.django_model.objects.get(url=fromurl)\n\n from_id = from_page.id\n\n to_page = PageItem.django_model.objects.get(url=tourl)\n\n to_id = to_page.id\n\n #newlink\n\n if not LinkItem.django_model.objects.filter(from_id=from_id).filter(to_id=to_id).exists():\n\n newlink = LinkItem()\n\n newlink['searchterm'] = search_item\n\n newlink['from_id'] = from_id\n\n newlink['to_id'] = to_id\n\n newlink.save()\n\n```", "```py\n\nscrapy crawl scrapy_spider_recursive -a url_list=listname -a search_id=keyname\n\n```", "```py\n\nfrom django.db import models\n\nfrom django.conf import settings\n\nfrom django.utils.translation import ugettext_lazy as _\n\nclass SearchTerm(models.Model):\n\n term = models.CharField(_('search'), max_length=255)\n\n num_reviews = models.IntegerField(null=True,default=0)\n\n #display term on admin panel\n\n def __unicode__(self):\n\n return self.term\n\nclass Page(models.Model):\n\n searchterm = models.ForeignKey(SearchTerm, related_name='pages',null=True,blank=True)\n\n url = models.URLField(_('url'), default='', blank=True)\n\n title = models.CharField(_('name'), max_length=255)\n\n depth = models.IntegerField(null=True,default=-1)\n\n html = models.TextField(_('html'),blank=True, default='')\n\n review = models.BooleanField(default=False)\n\n old_rank = models.FloatField(null=True,default=0)\n\n new_rank = models.FloatField(null=True,default=1)\n\n content = models.TextField(_('content'),blank=True, default='')\n\n sentiment = models.IntegerField(null=True,default=100)\n\nclass Link(models.Model):\n\n searchterm = models.ForeignKey(SearchTerm, related_name='links',null=True,blank=True)\n\n from_id = models.IntegerField(null=True)\n\n to_id = models.IntegerField(null=True)\n\n```", "```py\n\npython manage.py makemigrations\n\npython manage.py migrate\n\n```", "```py\n\n├── db.sqlite3\n\n├── scrapy.cfg\n\n├── scrapy_spider\n\n│   ├── ...\n\n│   ├── spiders\n\n│   │   ...\n\n└── webmining_server\n\n```", "```py\n\n# Setting up django's project full path.\n\nimport sys\n\nsys.path.insert(0, BASE_DIR+'/webmining_server')\n\n# Setting up django's settings module name.\n\nos.environ['DJANGO_SETTINGS_MODULE'] = 'webmining_server.settings'\n\n#import django to load models(otherwise AppRegistryNotReady: Models aren't loaded yet):\n\nimport django\n\ndjango.setup()\n\n```", "```py\n\nsudo pip install scrapy-djangoitem\n\n```", "```py\n\nfrom scrapy_djangoitem import DjangoItem\n\nfrom pages.models import Page,Link,SearchTerm\n\nclass SearchItem(DjangoItem):\n\n django_model = SearchTerm\n\nclass PageItem(DjangoItem):\n\n django_model = Page\n\nclass LinkItem(DjangoItem):\n\n django_model = Link\n\n```", "```py\n\nimport nltk.classify.util, nltk.metrics\n\nfrom nltk.classify import NaiveBayesClassifier\n\nfrom nltk.corpus import movie_reviews\n\nfrom nltk.corpus import stopwords\n\nfrom nltk.collocations import BigramCollocationFinder\n\nfrom nltk.metrics import BigramAssocMeasures\n\nfrom nltk.probability import FreqDist, ConditionalFreqDist\n\nimport collections\n\nfrom django.core.management.base import BaseCommand, CommandError\n\nfrom optparse import make_option\n\nfrom django.core.cache import cache\n\nstopwords = set(stopwords.words('english'))\n\nmethod_selfeatures = 'best_words_features'\n\nclass Command(BaseCommand):\n\n option_list = BaseCommand.option_list + (\n\n make_option('-n', '--num_bestwords', \n\n dest='num_bestwords', type='int',\n\n action='store',\n\n help=('number of words with high information')),)\n\n def handle(self, *args, **options):\n\n num_bestwords = options['num_bestwords']\n\n self.bestwords = self.GetHighInformationWordsChi(num_bestwords)\n\n clf = self.train_clf(method_selfeatures)\n\n cache.set('clf',clf)\n\n cache.set('bestwords',self.bestwords)\n\n```", "```py\n\n def train_clf(method):\n\n negidxs = movie_reviews.fileids('neg')\n\n posidxs = movie_reviews.fileids('pos')\n\n if method=='stopword_filtered_words_features':\n\n negfeatures = [(stopword_filtered_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]\n\n posfeatures = [(stopword_filtered_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]\n\n elif method=='best_words_features':\n\n negfeatures = [(best_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]\n\n posfeatures = [(best_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]\n\n elif method=='best_bigrams_words_features':\n\n negfeatures = [(best_bigrams_words_features(movie_reviews.words(fileids=[file])), 'neg') for file in negidxs]\n\n posfeatures = [(best_bigrams_words_features(movie_reviews.words(fileids=[file])), 'pos') for file in posidxs]\n\n trainfeatures = negfeatures + posfeatures\n\n clf = NaiveBayesClassifier.train(trainfeatures)\n\n return clf\n\n def stopword_filtered_words_features(self,words):\n\n return dict([(word, True) for word in words if word not in stopwords])\n\n #eliminate Low Information Features\n\n def GetHighInformationWordsChi(self,num_bestwords):\n\n word_fd = FreqDist()\n\n label_word_fd = ConditionalFreqDist()\n\n for word in movie_reviews.words(categories=['pos']):\n\n word_fd[word.lower()] +=1\n\n label_word_fd['pos'][word.lower()] +=1\n\n for word in movie_reviews.words(categories=['neg']):\n\n word_fd[word.lower()] +=1\n\n label_word_fd['neg'][word.lower()] +=1\n\n pos_word_count = label_word_fd['pos'].N()\n\n neg_word_count = label_word_fd['neg'].N()\n\n total_word_count = pos_word_count + neg_word_count\n\n word_scores = {}\n\n for word, freq in word_fd.iteritems():\n\n pos_score = BigramAssocMeasures.chi_sq(label_word_fd['pos'][word],\n\n (freq, pos_word_count), total_word_count)\n\n neg_score = BigramAssocMeasures.chi_sq(label_word_fd['neg'][word],\n\n (freq, neg_word_count), total_word_count)\n\n word_scores[word] = pos_score + neg_score\n\n best = sorted(word_scores.iteritems(), key=lambda (w,s): s, reverse=True)[:num_bestwords]\n\n bestwords = set([w for w, s in best])\n\n return bestwords\n\n def best_words_features(self,words):\n\n return dict([(word, True) for word in words if word in self.bestwords])\n\n def best_bigrams_word_features(self,words, measure=BigramAssocMeasures.chi_sq, nbigrams=200):\n\n bigram_finder = BigramCollocationFinder.from_words(words)\n\n bigrams = bigram_finder.nbest(measure, nbigrams)\n\n d = dict([(bigram, True) for bigram in bigrams])\n\n d.update(best_words_features(words))\n\n return d\n\n```", "```py\n\nnltk.download()--> corpora/movie_reviews corpus\n\n```", "```py\n\nfrom pages.models import Link,Page,SearchTerm\n\nfrom django.core.management.base import BaseCommand, CommandError\n\nfrom optparse import make_option\n\nclass Command(BaseCommand):\n\n option_list = BaseCommand.option_list + (\n\n make_option('-s', '--searchid',\n\n dest='searchid', type='int',\n\n action='store',\n\n help=('id of the search term to delete')),)\n\n def handle(self, *args, **options):\n\n searchid = options['searchid']\n\n if searchid == None:\n\n print \"please specify searchid: python manage.py --searchid=--\"\n\n #list\n\n for sobj in SearchTerm.objects.all():\n\n print 'id:',sobj.id,\"  term:\",sobj.term\n\n else:\n\n print 'delete...'\n\n search_obj = SearchTerm.objects.get(id=searchid)\n\n pages = search_obj.pages.all()\n\n pages.delete()\n\n links = search_obj.links.all()\n\n links.delete()\n\n search_obj.delete()\n\n```", "```py\n\npython manage.py delete_query --searchid=VALUE\n\n```", "```py\n\ndef analyzer(request):\n\n context = {}\n\n if request.method == 'POST':\n\n post_data = request.POST\n\n query = post_data.get('query', None)\n\n if query:\n\n return redirect('%s?%s' % (reverse('webmining_server.views.analyzer'),\n\n urllib.urlencode({'q': query}))) \n\n elif request.method == 'GET':\n\n get_data = request.GET\n\n query = get_data.get('q')\n\n if not query:\n\n return render_to_response(\n\n 'movie_reviews/home.html', RequestContext(request, context))\n\n context['query'] = query\n\n stripped_query = query.strip().lower()\n\n urls = []\n\n if test_mode:\n\n urls = parse_bing_results()\n\n else:\n\n urls = bing_api(stripped_query)\n\n if len(urls)== 0:\n\n return render_to_response(\n\n 'movie_reviews/noreviewsfound.html', RequestContext(request, context))\n\n if not SearchTerm.objects.filter(term=stripped_query).exists():\n\n s = SearchTerm(term=stripped_query)\n\n s.save()\n\n try:\n\n #scrape\n\n cmd = 'cd ../scrapy_spider & scrapy crawl scrapy_spider_reviews -a url_list=%s -a search_key=%s' %('\\\"'+str(','.join(urls[:num_reviews]).encode('utf-8'))+'\\\"','\\\"'+str(stripped_query)+'\\\"')\n\n os.system(cmd)\n\n except:\n\n print 'error!'\n\n s.delete()\n\n else:\n\n #collect the pages already scraped \n\n s = SearchTerm.objects.get(term=stripped_query)\n\n #calc num pages\n\n pages = s.pages.all().filter(review=True)\n\n if len(pages) == 0:\n\n s.delete()\n\n return render_to_response(\n\n 'movie_reviews/noreviewsfound.html', RequestContext(request, context))\n\n s.num_reviews = len(pages)\n\n s.save()\n\n context['searchterm_id'] = int(s.id)\n\n #train classifier with nltk\n\n def train_clf(method):\n\n ... \n\n def stopword_filtered_words_features(words):\n\n ... \n\n #Eliminate Low Information Features\n\n def GetHighInformationWordsChi(num_bestwords):\n\n ... \n\n bestwords = cache.get('bestwords')\n\n if bestwords == None:\n\n bestwords = GetHighInformationWordsChi(num_bestwords)\n\n def best_words_features(words):\n\n ... \n\n def best_bigrams_words_features(words, measure=BigramAssocMeasures.chi_sq, nbigrams=200):\n\n ...\n\n clf = cache.get('clf')\n\n if clf == None:\n\n clf = train_clf(method_selfeatures)\n\n cntpos = 0\n\n cntneg = 0\n\n for p in pages:\n\n words = p.content.split(\" \")\n\n feats = best_words_features(words)#bigram_word_features(words)#stopword_filtered_word_feats(words)\n\n #print feats\n\n str_sent = clf.classify(feats)\n\n if str_sent == 'pos':\n\n p.sentiment = 1\n\n cntpos +=1\n\n else:\n\n p.sentiment = -1\n\n cntneg +=1\n\n p.save()\n\n context['reviews_classified'] = len(pages)\n\n context['positive_count'] = cntpos\n\n context['negative_count'] = cntneg\n\n context['classified_information'] = True\n\n return render_to_response(\n\n 'movie_reviews/home.html', RequestContext(request, context))\n\n```", "```py\n        <h2 align = Center>Movie Reviews Sentiment Analysis</h2>\n        <div class=\"row\">\n        <p align = Center><strong>Reviews Classified : {{ reviews_classified }}</strong></p>\n        <p align = Center><strong>Positive Reviews : {{ positive_count }}</strong></p>\n        <p align = Center><strong> Negative Reviews : {{ negative_count }}</strong></p>\n        </div> \n  <section>\n      <script type=\"text/javascript\" src=\"img/jsapi\"></script>\n      <script type=\"text/javascript\">\n        google.load(\"visualization\", \"1\", {packages:[\"corechart\"]});\n        google.setOnLoadCallback(drawChart);\n        function drawChart() {\n          var data = google.visualization.arrayToDataTable([\n            ['Sentiment', 'Number'],\n            ['Positive',     {{ positive_count }}],\n            ['Negative',      {{ negative_count }}]\n          ]);\n          var options = { title: 'Sentiment Pie Chart'};\n          var chart = new google.visualization.PieChart(document.getElementById('piechart'));\n          chart.draw(data, options);\n        }\n      </script>\n        <p align =\"Center\" id=\"piechart\" style=\"width: 900px; height: 500px;display: block; margin: 0 auto;text-align: center;\" ></p>\n      </div>\n```", "```py\n\nfrom pages.models import Page,SearchTerm\n\nnum_iterations = 100000\n\neps=0.0001\n\nD = 0.85\n\ndef pgrank(searchid):\n\n s = SearchTerm.objects.get(id=int(searchid))\n\n links = s.links.all()\n\n from_idxs = [i.from_id for i in links ]\n\n # Find the idxs that receive page rank \n\n links_received = []\n\n to_idxs = []\n\n for l in links:\n\n from_id = l.from_id\n\n to_id = l.to_id\n\n if from_id not in from_idxs: continue\n\n if to_id  not in from_idxs: continue\n\n links_received.append([from_id,to_id])\n\n if to_id  not in to_idxs: to_idxs.append(to_id)\n\n pages = s.pages.all()\n\n prev_ranks = dict()\n\n for node in from_idxs:\n\n ptmp  = Page.objects.get(id=node)\n\n prev_ranks[node] = ptmp.old_rank\n\n conv=1.\n\n cnt=0\n\n while conv>eps or cnt<num_iterations:\n\n next_ranks = dict()\n\n total = 0.0\n\n for (node,old_rank) in prev_ranks.items():\n\n total += old_rank\n\n next_ranks[node] = 0.0\n\n #find the outbound links and send the pagerank down to each of them\n\n for (node, old_rank) in prev_ranks.items():\n\n give_idxs = []\n\n for (from_id, to_id) in links_received:\n\n if from_id != node: continue\n\n if to_id  not in to_idxs: continue\n\n give_idxs.append(to_id)\n\n if (len(give_idxs) < 1): continue\n\n amount = D*old_rank/len(give_idxs)\n\n for id in give_idxs:\n\n next_ranks[id] += amount\n\n tot = 0\n\n for (node,next_rank) in next_ranks.items():\n\n tot += next_rank\n\n const = (1-D)/ len(next_ranks)\n\n for node in next_ranks:\n\n next_ranks[node] += const\n\n tot = 0\n\n for (node,old_rank) in next_ranks.items():\n\n tot += next_rank\n\n difftot = 0\n\n for (node, old_rank) in prev_ranks.items():\n\n new_rank = next_ranks[node]\n\n diff = abs(old_rank-new_rank)\n\n difftot += diff\n\n conv= difftot/len(prev_ranks)\n\n cnt+=1\n\n prev_ranks = next_ranks\n\n for (id,new_rank) in next_ranks.items():\n\n ptmp = Page.objects.get(id=id)\n\n url = ptmp.url\n\n for (id,new_rank) in next_ranks.items():\n\n ptmp = Page.objects.get(id=id)\n\n ptmp.old_rank = ptmp.new_rank\n\n ptmp.new_rank = new_rank\n\n ptmp.save()\n\n```", "```py\n\ndef pgrank_view(request,pk): \n\n context = {}\n\n get_data = request.GET\n\n scrape = get_data.get('scrape','False')\n\n s = SearchTerm.objects.get(id=pk)\n\n if scrape == 'True':\n\n pages = s.pages.all().filter(review=True)\n\n urls = []\n\n for u in pages:\n\n urls.append(u.url)\n\n #crawl\n\n cmd = 'cd ../scrapy_spider & scrapy crawl scrapy_spider_recursive -a url_list=%s -a search_id=%s' %('\\\"'+str(','.join(urls[:]).encode('utf-8'))+'\\\"','\\\"'+str(pk)+'\\\"')\n\n os.system(cmd)\n\n links = s.links.all()\n\n if len(links)==0:\n\n context['no_links'] = True\n\n return render_to_response(\n\n 'movie_reviews/pg-rank.html', RequestContext(request, context))\n\n #calc pgranks\n\n pgrank(pk)\n\n #load pgranks in descending order of pagerank\n\n pages_ordered = s.pages.all().filter(review=True).order_by('-new_rank')\n\n context['pages'] = pages_ordered\n\n return render_to_response(\n\n 'movie_reviews/pg-rank.html', RequestContext(request, context)) \n\n```", "```py\n\nfrom django.contrib import admin\n\nfrom django_markdown.admin import MarkdownField, AdminMarkdownWidget\n\nfrom pages.models import SearchTerm,Page,Link\n\nclass SearchTermAdmin(admin.ModelAdmin):\n\n formfield_overrides = {MarkdownField: {'widget': AdminMarkdownWidget}}\n\n list_display = ['id', 'term', 'num_reviews']\n\n ordering = ['-id']\n\nclass PageAdmin(admin.ModelAdmin):\n\n formfield_overrides = {MarkdownField: {'widget': AdminMarkdownWidget}}\n\n list_display = ['id', 'searchterm', 'url','title','content']\n\n ordering = ['-id','-new_rank']\n\nadmin.site.register(SearchTerm,SearchTermAdmin)\n\nadmin.site.register(Page,PageAdmin)\n\nadmin.site.register(Link)\n\n```", "```py\n\nfrom rest_framework import views,generics\n\nfrom rest_framework.permissions import AllowAny\n\nfrom rest_framework.response import Response\n\nfrom rest_framework.pagination import PageNumberPagination\n\nfrom pages.serializers import SearchTermSerializer\n\nfrom pages.models import SearchTerm,Page\n\nclass LargeResultsSetPagination(PageNumberPagination):\n\n page_size = 1000\n\n page_size_query_param = 'page_size'\n\n max_page_size = 10000\n\nclass SearchTermsList(generics.ListAPIView):\n\n serializer_class = SearchTermSerializer\n\n permission_classes = (AllowAny,)\n\n pagination_class = LargeResultsSetPagination\n\n def get_queryset(self):\n\n return SearchTerm.objects.all() \n\nclass PageCounts(views.APIView):\n\n permission_classes = (AllowAny,)\n\n def get(self,*args, **kwargs):\n\n searchid=self.kwargs['pk']\n\n reviewpages = Page.objects.filter(searchterm=searchid).filter(review=True)\n\n npos = len([p for p in reviewpages if p.sentiment==1])\n\n nneg = len(reviewpages)-npos\n\n return Response({'npos':npos,'nneg':nneg})\n\n```", "```py\n\nfrom pages.models import SearchTerm\n\nfrom rest_framework import serializers\n\nclass SearchTermSerializer(serializers.HyperlinkedModelSerializer):\n\n class Meta:\n\n model = SearchTerm\n\n fields = ('id', 'term')\n\n```", "```py\n\ncurl -X GET localhost:8000/search-list/\n\n{\"count\":7,\"next\":null,\"previous\":null,\"results\":[{\"id\":24,\"term\":\"the martian\"},{\"id\":27,\"term\":\"steve jobs\"},{\"id\":29,\"term\":\"suffragette\"},{\"id\":39,\"term\":\"southpaw\"},{\"id\":40,\"term\":\"vacation\"},{\"id\":67,\"term\":\"the revenant\"},{\"id\":68,\"term\":\"batman vs superman dawn of justice\"}]}\n\n```", "```py\n\ncurl -X GET localhost:8000/pages-sentiment/68/\n\n{\"nneg\":3,\"npos\":15}\n\n```"]