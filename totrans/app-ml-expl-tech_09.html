<html><head></head><body>
		<div id="_idContainer142">
			<h1 id="_idParaDest-124"><em class="italic"><a id="_idTextAnchor128"/>Chapter 7</em>: Practical Exposure to Using SHAP in ML</h1>
			<p>In the previous chapter, we discussed <strong class="bold">SHapley Additive exPlanation</strong> (<strong class="bold">SHAP</strong>), which is one of the most popular model explainability frameworks. We also covered a practical example of using SHAP for explaining regression models. However, SHAP can explain other types of models trained on different types of datasets. In the previous chapter, you did receive a brief conceptual understanding of the different types of <strong class="bold">explainers</strong> available in SHAP for explaining models trained on different types of datasets. But in this chapter, you will get the practical exposure needed to apply the various types of explainers available in SHAP. </p>
			<p>More specifically, you learn how to apply <strong class="bold">TreeExplainers</strong> for explaining tree ensemble models trained on structured tabular data. You will also learn how to apply <strong class="bold">DeepExplainer</strong> and <strong class="bold">GradientExplainer</strong> SHAP with deep learning models trained on image data. As you learned in the previous chapter, the <strong class="bold">KernelExplainer</strong> in SHAP is model-agnostic, and you will get practical exposure to KernelExplainers in this chapter. We will also cover the practical aspect of using <strong class="bold">LinearExplainers</strong> on linear models. Finally, you will get to explore how SHAP can be used to explain the complicated state-of-the-art <strong class="bold">Transformer</strong> models trained on text data.</p>
			<p>In this chapter, we will cover the following important topics:</p>
			<ul>
				<li>Applying TreeExplainers to tree ensemble models</li>
				<li>Explaining deep learning models using DeepExplainer and GradientExplainer</li>
				<li>Model-agnostic explainability using KernelExplainer</li>
				<li>Exploring LinearExplainer in SHAP</li>
				<li>Explaining transformers using SHAP</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor129"/>Technical requirements </h1>
			<p>This code tutorial along with the necessary resources can be downloaded or cloned from the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07</a>. Python and Jupyter notebooks are used to implement the practical application of the theoretical concepts covered in this chapter. However, I will recommend that you run the notebooks only after you go through this chapter for a better understanding. </p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor130"/>Applying TreeExplainers to tree ensemble models</h1>
			<p>As discussed in the <a id="_idIndexMarker481"/>previous chapter, the Tree SHAP implementation can work with tree ensemble models such as <strong class="bold">Random Forests</strong>, <strong class="bold">XGBoost</strong>, and <strong class="bold">LightGBM</strong> algorithms. Now, decision trees are inherently interpretable. But tree-based ensemble learning models, either implementing boosting or bagging, are not inherently interpretable and can be quite complex to interpret. So, SHAP is one<a id="_idIndexMarker482"/> of the popular choices of algorithms used to explain such complex models. The Kernel SHAP implementation of SHAP is model-agnostic and can explain any model. However, the algorithm can be really slow with larger datasets with many features. That is why the <strong class="bold">Tree SHAP</strong> (<a href="https://arxiv.org/abs/1802.03888">https://arxiv.org/abs/1802.03888</a>) implementation of the algorithm is a high-speed exact algorithm for tree ensemble models. TreeExplainer is the fast C++ implementation of the Tree SHAP algorithm, which supports algorithms such as XGBoost, CatBoost, LightGBM, and other tree ensemble models from scikit-learn. In this section, I will cover how to apply TreeExplainer in practice.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor131"/>Installing the required Python modules</h2>
			<p>The complete tutorial is provided in the GitHub repository at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/TreeExplainers.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/TreeExplainers.ipynb</a> and I strongly recommend that you read this section and execute the code side by side. If you have followed the previous tutorials provided in the other chapters, most of the required Python packages should be installed by now. Otherwise, you can install the necessary packages using the <strong class="source-inline">pip</strong> installer:</p>
			<pre class="source-code">!pip install --upgrade numpy pandas matplotlib seaborn sklearn lightgbm shap</pre>
			<p>You can import these packages to verify their successful installations:</p>
			<pre class="source-code">import numpy as np</pre>
			<pre class="source-code">import pandas as pd</pre>
			<pre class="source-code">import seaborn as sns</pre>
			<pre class="source-code">import matplotlib.pyplot as plt</pre>
			<pre class="source-code">import sklearn</pre>
			<pre class="source-code">import lightgbm as lgb</pre>
			<pre class="source-code">import shap</pre>
			<p>For certain JavaScript-based SHAP visualizations in Jupyter notebooks, make sure to initialize the SHAP JavaScript module:</p>
			<pre class="source-code">shap.initjs()</pre>
			<p>Next, let's discuss the dataset <a id="_idIndexMarker483"/>that we are going to use for<a id="_idIndexMarker484"/> this example.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor132"/>Discussion about the dataset</h2>
			<p>For this example, we will use the German Credit Risk dataset from Kaggle (<a href="https://www.kaggle.com/uciml/german-credit">https://www.kaggle.com/uciml/german-credit</a>). This dataset is used to build a classification model for classifying<a id="_idIndexMarker485"/> good and bad credit risk. The Kaggle dataset is actually a simplified version of the original data available in UCI (<a href="https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)">https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)</a>)</p>
			<p class="callout-heading">Statlog (German Credit Data) Data Set</p>
			<p class="callout">The credit for the dataset goes to <em class="italic">Professor Dr. Hans Hofmann, Institut für Statistik und Ökonometrie Universität Hamburg</em>. </p>
			<p>Please refer to the notebook for more information on the dataset. The dataset is already provided in the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07/datasets">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07/datasets</a>. We can use the pandas Python module to load and display the dataset as a DataFrame: </p>
			<pre class="source-code">data  = pd.read_csv('datasets/german_credit_data.csv', index_col=0)</pre>
			<pre class="source-code">data.head()</pre>
			<p>The following diagram illustrates the pandas DataFrame for this data:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B18216_07_001.jpg" alt="Figure 7.1 – pandas DataFrame snapshot of the German Credit Risk dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – pandas DataFrame snapshot of the German Credit Risk dataset</p>
			<p>I do recommend that you <a id="_idIndexMarker486"/>perform a thorough <strong class="bold">Exploratory Data Analysis</strong> (<strong class="bold">EDA</strong>). You can also use pandas profiling (<a href="https://github.com/ydataai/pandas-profiling">https://github.com/ydataai/pandas-profiling</a>) as shown in <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>, for automated EDA. Since we have covered this<a id="_idIndexMarker487"/> already, I will skip the EDA part for this example.</p>
			<p>However, the dataset does have some missing values, which needs to be handled before building a model. We can check that using the following lines of code:</p>
			<pre class="source-code">sns.displot(</pre>
			<pre class="source-code">    data=data.isna().melt(value_name="missing"),</pre>
			<pre class="source-code">    y="variable",</pre>
			<pre class="source-code">    hue="missing",</pre>
			<pre class="source-code">    multiple="fill",</pre>
			<pre class="source-code">    aspect=1.5,</pre>
			<pre class="source-code">    palette='seismic'</pre>
			<pre class="source-code">)</pre>
			<pre class="source-code">plt.show()</pre>
			<p>The following visualization is generated as output:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B18216_07_002.jpg" alt="Figure 7.2 – Missing value visualization for the German Credit Risk dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Missing value visualization for the German Credit Risk dataset</p>
			<p>The dataset has around 18% missing values for the <strong class="source-inline">Saving accounts</strong> feature and 40% missing values for the <strong class="source-inline">Checking account</strong> feature. Since the percentage of missing data is high, and the features can be important, we cannot completely ignore or drop these features. Please <a id="_idIndexMarker488"/>remember that the focus of this tutorial is on model explainability using TreeExplainers. So, we will not spend too much time doing data imputation as we are not concerned with building an efficient model for this example. As both the features are categorical features, we will simply create an <strong class="source-inline">Unknown</strong> category for the missing values. This can be done by means of the following line of code:</p>
			<pre class="source-code">data.fillna('Unknown', inplace=True)</pre>
			<p>We will need to perform <strong class="bold">Label Encoding</strong> for the <a id="_idIndexMarker489"/>categorical features as we need to convert the string-like feature values to an integer format:</p>
			<pre class="source-code">from sklearn.preprocessing import LabelEncoder</pre>
			<pre class="source-code">le = LabelEncoder()</pre>
			<pre class="source-code">for feat in ['Sex', 'Housing', 'Saving accounts', 'Checking account', 'Purpose', 'Risk']:</pre>
			<pre class="source-code">    le.fit(data[feat])</pre>
			<pre class="source-code">    data[feat] = le.transform(data[feat])</pre>
			<p>Now, for this example, we will use the <strong class="bold">LightGBM algorithm</strong> (<a href="https://lightgbm.readthedocs.io/en/latest/">https://lightgbm.readthedocs.io/en/latest/</a>), which can work directly on categorical variables, and hence <a id="_idIndexMarker490"/>we do not need to perform <strong class="bold">one-hot encoding</strong>. But for other <a id="_idIndexMarker491"/>algorithms, we might need to perform one-hot encoding. Moreover, we will not perform other complex data pre-processing or feature engineering steps. I do recommend that you <a id="_idIndexMarker492"/>perform robust <em class="italic">feature engineering</em>, <em class="italic">outlier detection</em>, and <em class="italic">data normalization</em> for building efficient ML models. However, for this example, even if the model is not very accurate, we can use SHAP to generate explanations. Let's proceed to the model training part.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor133"/>Training the model</h2>
			<p>Before training the <a id="_idIndexMarker493"/>model, we will need to create the training and test sets:</p>
			<pre class="source-code">from sklearn.model_selection import train_test_split</pre>
			<pre class="source-code">features = data.drop(columns=['Risk'])</pre>
			<pre class="source-code">labels = data['Risk']</pre>
			<pre class="source-code"># Dividing the data into training-test set with 80:20 split ratio</pre>
			<pre class="source-code">x_train,x_test,y_train,y_test = \</pre>
			<pre class="source-code">train_test_split(features,labels,test_size=0.2, </pre>
			<pre class="source-code">                 random_state=123)</pre>
			<p>Since we will be using the LightGBM algorithm, we will need to create LightGBM dataset objects, which are used during the training process:</p>
			<pre class="source-code">data_train = lgb.Dataset(x_train, label=y_train, </pre>
			<pre class="source-code">                         categorical_feature=cat_features)</pre>
			<pre class="source-code">data_test = lgb.Dataset(x_test, label=y_test, </pre>
			<pre class="source-code">                        categorical_feature=cat_features)</pre>
			<p>We also need to define the <a id="_idIndexMarker494"/>model parameters as a dictionary:</p>
			<pre class="source-code">params = {</pre>
			<pre class="source-code">    'boosting_type': 'gbdt',</pre>
			<pre class="source-code">    'objective': 'binary',</pre>
			<pre class="source-code">    'metric': 'auc',</pre>
			<pre class="source-code">    'num_leaves': 20,</pre>
			<pre class="source-code">    'learning_rate': 0.05,</pre>
			<pre class="source-code">    'feature_fraction': 0.9,</pre>
			<pre class="source-code">    'bagging_fraction': 0.8,</pre>
			<pre class="source-code">    'bagging_freq': 5,</pre>
			<pre class="source-code">    'verbose': -1,</pre>
			<pre class="source-code">    'lambda_l1': 1,</pre>
			<pre class="source-code">    'lambda_l2': 1,</pre>
			<pre class="source-code">    'seed': 123</pre>
			<pre class="source-code">}</pre>
			<p>Finally, we can train the model using the parameters and dataset object created:</p>
			<pre class="source-code">model = lgb.train(params,</pre>
			<pre class="source-code">                  data_train,</pre>
			<pre class="source-code">                  num_boost_round=100,</pre>
			<pre class="source-code">                  verbose_eval=100,</pre>
			<pre class="source-code">                  valid_sets=[data_test, data_train])</pre>
			<p>We are again skipping the hyperparameter tuning process to obtain a more efficient model, but I would definitely recommend spending some time on hyperparameter tuning to get a model with<a id="_idIndexMarker495"/> higher accuracy. Now, let's proceed to the model explainability part using SHAP.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor134"/>Application of TreeExplainer in SHAP</h2>
			<p>Applying <a id="_idIndexMarker496"/>TreeExplainer in SHAP is very easy as the framework is well<a id="_idIndexMarker497"/> modularized: </p>
			<pre class="source-code">explainer = shap.TreeExplainer(model)</pre>
			<pre class="source-code">shap_values = explainer.shap_values(features)</pre>
			<p>Once we have approximated the SHAP values, we can then apply the visualization methods provided in SHAP to obtain the model's explainability. I would <a id="_idTextAnchor135"/>recommend that you refer to the <em class="italic">Visualizations in SHAP</em> section in <a href="B18216_06_ePub.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Model Interpre<a id="_idTextAnchor136"/>tability Using SHAP</em>, to refresh your memory regarding the various visualization methods that we can use with SHAP for model explainability. We will start with global explainability with summary plots.</p>
			<p><em class="italic">Figure 7.3</em> illustrates the SHAP summary plot using the SHAP values generated by TreeExplainer on this dataset:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B18216_07_003.jpg" alt="Figure 7.3 – SHAP summary plot on SHAP values generated by TreeExplainer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – SHAP summary plot on SHAP values generated by TreeExplainer</p>
			<p>As we can see from the preceding figure, the summary plot highlights the important features based on SHAP values, ordered from most important to least important. The model considered <strong class="source-inline">Checking account</strong> and <strong class="source-inline">Duration</strong> as one of the most influential features, <a id="_idIndexMarker498"/>compared to the <strong class="source-inline">Sex</strong> or <strong class="source-inline">Job</strong> features. </p>
			<p>For local<a id="_idIndexMarker499"/> explainability, we can apply the <strong class="bold">force plot</strong> and <strong class="bold">decision plot</strong> visualization<a id="_idIndexMarker500"/> methods:</p>
			<pre class="source-code"># Local explainability with force plots</pre>
			<pre class="source-code">shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], features.iloc[0,:])</pre>
			<pre class="source-code"># Local explainability with force plots</pre>
			<pre class="source-code">shap.decision_plot(explainer.expected_value[1], shap_values[1][0,:], features.iloc[0,:])</pre>
			<p>I often find decision plots to be more interpretable than force plots as decision plots show you the deviation<a id="_idIndexMarker501"/> from the mean expected value for each feature. The direction of deviation also indicates whether the feature is positively impacting the model's decision or whether it has a negative impact. But some of you might prefer force plots as well, as this indicates the positively or negatively affecting features based on their feature values and how they can impact in terms of achieving a higher prediction value or a lower prediction value.</p>
			<p><em class="italic">Figure 7.4</em> illustrates the force plot and decision plot that we have obtained:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B18216_07_004.jpg" alt="Figure 7.4 – Force and decision plots for local interpretability &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Force and decision plots for local interpretability </p>
			<p>In certain cases, understanding the inter-feature dependence becomes important as SHAP doesn't consider <a id="_idIndexMarker502"/>features in isolation to obtain the most<a id="_idIndexMarker503"/> influential features. Rather SHAP-based feature importance is estimated based on the collective impact of multiple features together. So, for analyzing the feature importance, we can try out the SHAP feature dependence plots:</p>
			<pre class="source-code"># For feature wise global interpretability</pre>
			<pre class="source-code">for col in ['Purpose', 'Age']:</pre>
			<pre class="source-code">    print(f"Feature Dependence plot for: {col}")</pre>
			<pre class="source-code">    shap.dependence_plot(col, shap_values[1], features, display_features=features)</pre>
			<p>The following diagram shows the feature dependence plot for the <strong class="source-inline">Purpose</strong> and <strong class="source-inline">Age</strong> features:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B18216_07_005.jpg" alt="Figure 7.5 – SHAP feature dependence plot for the Purpose and Age features&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – SHAP feature dependence plot for the Purpose and Age features</p>
			<p>From the SHAP in <em class="italic">Figure 7.3</em>, it was slightly surprising for me to find out that the features <strong class="source-inline">Purpose</strong> and <strong class="source-inline">Age</strong> are not as important as <strong class="source-inline">Duration</strong> or <strong class="source-inline">Credit amount</strong>. In such cases, the feature dependence <a id="_idIndexMarker504"/>plots automatically calculate the most dependent feature for a selected feature. So, from <em class="italic">Figure 7.5</em>, we can see that for both <strong class="source-inline">Purpose</strong> and <strong class="source-inline">Age</strong>, <strong class="source-inline">Credit Amount</strong> is the dependent feature, and we <a id="_idIndexMarker505"/>can also see how these features vary with the dependent feature. This justifies that collectively, <strong class="source-inline">Credit amount</strong> is more influential than <strong class="source-inline">Purpose</strong> and <strong class="source-inline">Age</strong>. </p>
			<p>You can also try out other visualization methods covered in <a href="B18216_06_ePub.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Model Interpretability Using SHAP</em>, and definitely recommend you to play around with the SHAP values generated using the TreeExplainer so that you can come up with your own custom visualization method! In the next section, we are going to apply SHAP explainers to deep learning models trained on image data.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor137"/>Explaining deep learning models using DeepExplainer and GradientExplainer</h1>
			<p>In the <a id="_idIndexMarker506"/>previous section, we <a id="_idIndexMarker507"/>covered the use of TreeExplainer in<a id="_idIndexMarker508"/> SHAP, which is a model-specific explainability method only <a id="_idIndexMarker509"/>applicable to tree ensemble models. We will now discuss GradientExplainer and DeepExplainer, two other model-specific explainers in SHAP that are mostly used with deep learning models. </p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor138"/>GradientExplainer</h2>
			<p>As discussed in <a href="B18216_02_ePub.xhtml#_idTextAnchor033"><em class="italic">Chapter 2</em></a>, <em class="italic">Model Explainability Methods</em>, one of the most widely adopted ways to explain<a id="_idIndexMarker510"/> deep learning models trained on unstructured data <a id="_idIndexMarker511"/>such as images is <strong class="bold">layer-wise relevance propagation</strong> (<strong class="bold">LRP</strong>). LRP is about analyzing the gradient flow for the intermediate<a id="_idIndexMarker512"/> layers of the deep neural network. SHAP GradientExplainers also function in a similar way. As discussed in <a href="B18216_06_ePub.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Model Interpretability Using SHAP</em>, GradientExplainer combines the idea of <em class="italic">SHAP</em>, <em class="italic">integrated gradients</em>, and <em class="italic">SmoothGrad</em> into a single expected value equation. GradientExplainer finally uses a sensitivity map-based gradient visualization method. The red pixels in the visualization map represent pixels having positive SHAP values, which increases the probability of the output class. The blue pixels represent pixels having negative SHAP values that decrease the likelihood of the output class. Now, let me walk you through the tutorial provided in the code repository: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb</a>. Please load the necessary modules and follow the detailed steps provided in the notebook tutorial as I will be covering only the important coding steps in this section for helping you to understand the flow of the code tutorial.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor139"/>Discussion on the dataset used for training the model</h2>
			<p>For this example, we will be <a id="_idIndexMarker513"/>using the SHAP ImageNet dataset, which will be used to generate the background reference required by the GradientExplainer algorithm. We will also pick up the inference image from the same dataset. However, you are always free to pick up any other image dataset or inference image of your choice:</p>
			<pre class="source-code">X,y = shap.datasets.imagenet50(resolution=224)</pre>
			<pre class="source-code">inference_image = X[[46]] </pre>
			<p>For this example, we have selected the following image as our inference image:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B18216_07_006.jpg" alt="Figure 7.6 – Inference image from SHAP ImageNet50&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Inference image from SHAP ImageNet50</p>
			<p>As we can see from the inference image, it contains many possible objects, including a man, chair, and<a id="_idIndexMarker514"/> computer. All these can be potential model outcomes and the actual outcome depends on the exact region where the model is looking to make the prediction. So, explainability is very important in such cases. Next, let's discuss the model used for this example.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor140"/>Using a pre-trained CNN model for this example</h2>
			<p>I have used a pre-trained CNN model, <strong class="bold">VGG19</strong>, as our black-box image classification model. The pre-trained model is<a id="_idIndexMarker515"/> trained on ImageNet data, and hence the training and the inference data will be consistent. The model can be loaded using the <strong class="source-inline">tensorflow</strong> Python module:</p>
			<pre class="source-code">from tensorflow.keras.applications.vgg19 import VGG19</pre>
			<pre class="source-code">model = VGG19(weights='imagenet')</pre>
			<p>Next, let's apply GradientExplainer to SHAP.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor141"/>Application of GradientExplainer in SHAP</h2>
			<p>GradientExplainer helps<a id="_idIndexMarker516"/> to map the gradient flow of intermediate layers of a <a id="_idIndexMarker517"/>deep learning<a id="_idIndexMarker518"/> model such as <strong class="bold">Convolution Neural Network</strong> (<strong class="bold">CNN</strong>) to explain the workings of the model. So, we will try to explore the 10th layer of the model and visualize the gradients based on SHAP values. The choice of the 10th layer is completely arbitrary; you can<a id="_idIndexMarker519"/> choose other<a id="_idIndexMarker520"/> layers as well:</p>
			<pre class="source-code">layer_num = 10</pre>
			<pre class="source-code"># explain how the input to the 10th layer of the model explains the top two classes</pre>
			<pre class="source-code">def map2layer(x, layer):</pre>
			<pre class="source-code">    '''</pre>
			<pre class="source-code">    Source : https://github.com/slundberg/shap</pre>
			<pre class="source-code">    '''</pre>
			<pre class="source-code">    feed_dict = dict(zip([model.layers[0].input],</pre>
			<pre class="source-code">                         [preprocess_input(x.copy())]))</pre>
			<pre class="source-code">    return K.get_session().run(model.layers[layer].input, feed_dict)</pre>
			<pre class="source-code">model_input = (model.layers[layer_num].input, </pre>
			<pre class="source-code">               model.layers[-1].output)</pre>
			<pre class="source-code">explainer = shap.GradientExplainer(model_input,</pre>
			<pre class="source-code">                                   map2layer(X, layer_num),</pre>
			<pre class="source-code">                                   local_smoothing=0)</pre>
			<pre class="source-code">shap_values, ind = explainer.shap_values(</pre>
			<pre class="source-code">    map2layer(inference_image, layer_num), </pre>
			<pre class="source-code">    ranked_outputs=4)</pre>
			<pre class="source-code"># plot the explanations</pre>
			<pre class="source-code">shap.image_plot(shap_values, inference_image, class_names)</pre>
			<p>In this example, we are trying to estimate the GradientExplainer-based SHAP values for the 10th layer of the pre-trained model. Using the SHAP image plot method, we can visualize the sensitivity map for the top 4 probable outcomes of the model, which is illustrated in <em class="italic">Figure 7.7</em>:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B18216_07_007.jpg" alt="Figure 7.7 – SHAP image plot visualization based on applying GradientExplainer to the inference image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – SHAP image plot visualization based on applying GradientExplainer to the inference image</p>
			<p>The top four predictions from the model are <strong class="bold">desktop_computer</strong>, <strong class="bold">desk</strong>, <strong class="bold">monitor</strong>, and <strong class="bold">screen</strong>. All of these are valid outcomes depending on which region the model is focusing on. Using the SHAP image plots shown in <em class="italic">Figure 7.7</em>, we can identify the exact regions contributing to<a id="_idIndexMarker521"/> the specific model prediction. The pixel regions <a id="_idIndexMarker522"/>marked in red are making a positive contribution to the specific model prediction, whereas the blue pixel regions are negatively contributing to the model predictions. You can try visualizing other layers of the model as well and analyze how the model prediction varies throughout the layers!</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor142"/>Exploring DeepExplainers</h2>
			<p>In the previous section, we<a id="_idIndexMarker523"/> covered GradientExplainers in SHAP. However, deep learning models can also be explained using DeepExplainers in SHAP based on the Deep SHAP algorithm. Deep SHAP is a high-speed implementation for estimating SHAP values for deep learning models. It uses a distribution of background samples and Shapley equations to linearize predominant non-linear operations used in deep learning models such as max, products, and softmax. </p>
			<p>The tutorial notebook provided in <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb</a> covers an example of a deep learning model trained from scratch on the CIFAR-10 dataset (<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>) for multi-class classification. The dataset contains highly compressed images of size 32x32 belonging to 10 different classes. In this section, I will skip the model training process as it is already covered in sufficient detail in the notebook tutorial. Instead, I will discuss the model explainability part using <a id="_idIndexMarker524"/>DeepExplainers, which is our primary focus. You can also try out the same tutorial with a pre-trained CNN model instead of training a model from scratch. Now, let's discuss the model explainability part.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor143"/>Application of DeepExplainer in SHAP</h2>
			<p>In order to apply <a id="_idIndexMarker525"/>DeepExplainer, we need to first form the background samples. The robustness of the explainability actually depends a lot on the<a id="_idIndexMarker526"/> selection of the background samples. For this example, we will randomly select 1,000 samples from the training data. You can increase your sample size as well, but please ensure that the background samples have no data drift between the training or the inference data by ensuring that the data collection process is consistent:</p>
			<pre class="source-code">background = x_train[np.random.choice(len(x_train), 1000, replace=False)]</pre>
			<p>Once the background samples have been selected, we can create a SHAP explainer object using the DeepExplainer method on the trained model and the background samples and estimate the SHAP values for the inference data: </p>
			<pre class="source-code">explainer = shap.DeepExplainer(model, background)</pre>
			<pre class="source-code">shap_values = explainer.shap_values(sample_x_test)</pre>
			<p>After the SHAP values are computed, we can use the SHAP image plot to visualize the pixels influencing the model in both a positive and negative manner using a similar sensitivity plot as used for GradientExplainer:</p>
			<pre class="source-code">shap.image_plot(shap_values, sample_x_test, labels, labelpad= 1)</pre>
			<p>The following figure shows the SHAP image plot for some sample inference data:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/B18216_07_008.jpg" alt="Figure 7.8 – SHAP image plot visualization after applying DeepExplainers in SHAP&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – SHAP image plot visualization after applying DeepExplainers in SHAP</p>
			<p>As we can see from <em class="italic">Figure 7.8</em>, even if the model is trained on a very compressed dataset, DeepExplainer was able to calculate SHAP values that can help us identify the regions of the<a id="_idIndexMarker527"/> image (highlighted in pinkish-red pixels) that have positively contributed to the model's prediction. The model did correctly predict the <a id="_idIndexMarker528"/>outcome as a <em class="italic">horse</em>, which is the correct classification from the compressed image. However, applying DeepExplainer was quite simple and the method was very fast compared to conventional methods to approximate SHAP values for deep learning models trained on unstructured data such as images. Next, we will learn about KernelExplainer for model agnostic explainability.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor144"/>Model-agnostic explainability using KernelExplainer</h1>
			<p>In the previous sections, we <a id="_idIndexMarker529"/>have discussed three model-specific explainers available in SHAP – TreeExplainer, GradientExplainer, and DeepExplainer. The KernelExplainer in SHAP actually makes SHAP a model-agnostic explainability approach. However, unlike the previous methods, KernelExplainer based on the Kernel SHAP algorithm is much slower, especially for large and high <a id="_idIndexMarker530"/>dimensional datasets. Kernel SHAP tries to combine ideas from Shapley values and <strong class="bold">Local Interpretable Model-agnostic Explanations (LIME)</strong> for both global and local interpretability of black-box models. Similar to the approach followed in LIME, the Kernel SHAP algorithm also creates locally linear perturbed samples and computes Shapley values of the same to identify features contributing to or against the model prediction.</p>
			<p> KernelExplainer is the<a id="_idIndexMarker531"/> practical implementation of the Kernel SHAP algorithm. The complete tutorial demonstrating the application of SHAP KernelExplainer is provided in the following notebook: <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/KernelExplainers.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/KernelExplainers.ipynb</a>. I have used the same <em class="italic">German Credit Risk dataset</em> as used for the <em class="italic">TreeExplainer tutorial</em>. Please refer to the <em class="italic">Applying TreeExplainers to tree ensemble models</em> section for a detailed discussion of the dataset and the model if you are starting from this section directly. In this <a id="_idIndexMarker532"/>section, we will discuss the application of KernelExplainers for the same problem discussed in the TreeExplainer tutorial.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor145"/>Application of KernelExplainer in SHAP</h2>
			<p>The KernelExplainer <a id="_idIndexMarker533"/>method in SHAP takes the model and the background data as the input to compute the SHAP values. For larger datasets or high dimensional<a id="_idIndexMarker534"/> datasets having many features, it is recommended to use only a subset of the training data as the background samples. Otherwise, Kernel SHAP can be a very slow algorithm and would take a lot of time to generate the SHAP values. Like the previous explainer methods covered, applying KernelExplainer is very simple and can be done using the following lines of code:</p>
			<pre class="source-code">explainer = shap.KernelExplainer(model.predict, x_train)</pre>
			<pre class="source-code">shap_values = explainer.shap_values(x_test, nsamples=100)</pre>
			<p>If we log the wall time (using <strong class="source-inline">%%time</strong> in Jupyter notebooks) for computing SHAP values and compare KernelExplainer with TreeExplainer on the same dataset, we will observe that KernelExplainer takes a significantly longer time to execute (almost 1,000 times longer in our case!). This shows that even though KernelExplainer is model-agnostic, the slowness of the algorithm is a major drawback. </p>
			<p>For explaining black-box models, the same visualization methods covered for TreeExplainer are applicable, which can be generated by the following code:</p>
			<pre class="source-code">shap.summary_plot(shap_values, x_test, plot_type='violin', show=False)</pre>
			<pre class="source-code">shap.force_plot(explainer.expected_value, shap_values[1], x_test.iloc[0,:])</pre>
			<pre class="source-code">shap.decision_plot(explainer.expected_value, shap_values[1], x_test.iloc[0,:])</pre>
			<p><em class="italic">Figure 7.9</em> shows the summary plot, decision plot, and force plots used to explain the black-box model:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/B18216_07_009.jpg" alt="Figure 7.9 – Summary plot, decision plot, and force plots obtained after using SHAP KernelExplainer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Summary plot, decision plot, and force plots obtained after using SHAP KernelExplainer</p>
			<p>The plots shown in <em class="italic">Figure 7.9</em> can be obtained using the same approach as covered for TreeExplainer. In<a id="_idIndexMarker535"/> the next section, we will cover LinearExplainer in SHAP, which<a id="_idIndexMarker536"/> is another model-specific explanation method. </p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor146"/>Exploring LinearExplainer in SHAP</h1>
			<p>LinearExplainer in SHAP is <a id="_idIndexMarker537"/>particularly developed for linear machine learning models. In the previous section, we have seen that although KernelExplainer is model-agnostic, it can be very slow. So, I think that is one of the main motivations behind using LinearExplainer to explain a linear model with independent features and even consider feature correlation. In this section, we will discuss applying the LinearExplainer method in practice. The detailed notebook tutorial is available at <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/LinearExplainers.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/LinearExplainers.ipynb</a>. We have used the same <em class="italic">Red Wine Quality dataset</em> as used for the tutorial discussed in <a href="B18216_06_ePub.xhtml#_idTextAnchor107"><em class="italic">Chapter 6</em></a>, <em class="italic">Model Interpretability Using SHAP</em>. You can refer to the same tutorial to learn more about the dataset as<a id="_idIndexMarker538"/> we will only focus on the LinearExplainer application part in this section.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor147"/>Application of LinearExplainer in SHAP</h2>
			<p>For this example, we <a id="_idIndexMarker539"/>have actually trained a linear regression model on the dataset. Similar to the other explainers, we<a id="_idIndexMarker540"/> can apply LinearExplainer using a few lines of code:</p>
			<pre class="source-code">explainer = shap.LinearExplainer(model, x_train, feature_dependence="independent")</pre>
			<pre class="source-code">shap_values = explainer.shap_values(x_test)</pre>
			<p>To explain the trained linear model, the same visualization methods covered for TreeExplainer and KernelExplainer are applicable. <em class="italic">Figure 7.10</em> shows the summary plot, feature dependence plot, and force plots used to explain the trained linear model:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B18216_07_010.jpg" alt="Figure 7.10 – Summary plot, feature dependence plot, and force plots obtained after using SHAP LinearExplainer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – Summary plot, feature dependence plot, and force plots obtained after using SHAP LinearExplainer</p>
			<p>We can obtain the visualization plots shown in <em class="italic">Figure 7.10</em> using the following lines of code:</p>
			<pre class="source-code">shap.summary_plot(shap_values, x_test, plot_type='violin', show=False)</pre>
			<pre class="source-code">shap.force_plot(explainer.expected_value, shap_values[1], x_test.iloc[0,:])</pre>
			<pre class="source-code">shap.dependence_plot("alcohol", shap_values, x_test, show=False)</pre>
			<p>I will recommend that you <a id="_idIndexMarker541"/>explore other visualization methods or even create your custom visualizations using the SHAP values generated by the LinearExplainer. Next, we will discuss applying SHAP to transformer models trained on text data.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor148"/>Explaining transformers using SHAP </h1>
			<p>In this chapter, so far we<a id="_idIndexMarker542"/> have seen examples of various SHAP explainers used to explain different types of models trained on structured and image datasets. Now, we will cover approaches to explain complicated models trained on text data. For text data, getting high accuracy with models trained on <a id="_idIndexMarker543"/>conventional <strong class="bold">Natural Language Processing (NLP)</strong> methods is always challenging. This is because extracting contextual information in sequential text data is always difficult using the classical approaches.</p>
			<p>However, with the invention of the <strong class="bold">Transformer</strong> deep learning architecture (<a href="https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/">https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/</a>), which is based on an <strong class="bold">attention mechanism</strong>, obtaining higher accuracy models trained on text data became much easier. However, transformer<a id="_idIndexMarker544"/> models are extremely complicated and it can be really difficult to interpret the workings of such models. Fortunately, being model-agnostic, SHAP can be applied with transformer models as well. </p>
			<p>So in this section, we will cover how SHAP can be applied with <em class="italic">text-based, pre-trained transformer models from Hugging Face</em> (<a href="https://github.com/huggingface/transformers">https://github.com/huggingface/transformers</a>) used for different applications. The complete tutorial can be accessed from <a href="https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining_Transformers.ipynb">https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining_Transformers.ipynb</a>. Now, let's see the first example of explaining transformer-based sentiment analysis models using SHAP. </p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor149"/>Explaining transformer-based sentiment analysis models</h2>
			<p><strong class="bold">Hugging Face</strong> (<a href="https://huggingface.co/">https://huggingface.co/</a>) provides state-of-the-art pre-trained models trained on a large <a id="_idIndexMarker545"/>amount of data. Hence, using pre-trained models from Hugging Face, AI developers can actually focus on the problem solving and application part, rather than spending too much time on<a id="_idIndexMarker546"/> tasks such as data collection, processing, and model training. The first example that we will discuss will be for doing sentiment analysis from text data. Before we proceed with the application, please ensure that you have installed the <strong class="source-inline">transformers</strong> Python module using the <strong class="source-inline">pip</strong> installer:</p>
			<pre class="source-code">!pip install --upgrade transformers</pre>
			<p>You can confirm the successful installation of the transformers framework by importing the module:</p>
			<pre class="source-code">import transformers</pre>
			<p>Now, let's load a sentiment analysis pre-trained model: </p>
			<pre class="source-code">model = transformers.pipeline('sentiment-analysis', return_all_scores=True)</pre>
			<p>We can take any sentence as an input and we will apply the model to check whether it has positive or negative sentiment. So, we will use the sentence <strong class="source-inline">"Hugging Face transformers are absolutely brilliant!"</strong> as our inference data:</p>
			<pre class="source-code">text_data = "Hugging Face transformers are absolutely brilliant!"</pre>
			<pre class="source-code">model(text_data)[0]</pre>
			<p>The model will predict the probability of the inference data being positive and negative:</p>
			<pre class="source-code">[{'label': 'NEGATIVE', 'score': 0.00013269631017465144},</pre>
			<pre class="source-code"> {'label': 'POSITIVE', 'score': 0.99986732006073}]</pre>
			<p>With a very high probability (99.99%), the model has predicted the sentence to be positive, which is a correct prediction. Now, let's apply SHAP to explain the model prediction:</p>
			<pre class="source-code">explainer = shap.Explainer(model) </pre>
			<pre class="source-code">shap_values = explainer([text_data])</pre>
			<p>Once the SHAP values are s<a id="_idIndexMarker547"/>uccessfully computed, we can apply SHAP text plot visualization and bar plot visualization to highlight words<a id="_idIndexMarker548"/> that are positively and negatively contributing to the model's prediction:</p>
			<pre class="source-code">shap.plots.text(shap_values[0,:,'POSITIVE'])</pre>
			<pre class="source-code">shap.plots.bar(shap_values[0,:,'POSITIVE'])</pre>
			<p><em class="italic">Figure 7.11</em> shows us the SHAP text plots, which look similar to force plots:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B18216_07_011.jpg" alt="Figure 7.11 – Explaining transformer models trained on text data using SHAP text plots&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11 – Explaining transformer models trained on text data using SHAP text plots</p>
			<p>As we can see from <em class="italic">Figure 7.11</em>, the words highlighted in red, such as <em class="italic">brilliant</em>, <em class="italic">absolutely</em>, and <em class="italic">Hugging</em>, make a positive contribution and increase the model prediction score, whereas the other words are lowering the model prediction and thus make a negative contribution to the model's prediction. </p>
			<p>The same inference can also be drawn from the SHAP bar plot shown in the following figure:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B18216_07_012.jpg" alt="Figure 7.12 – SHAP bar plot used to explain a transformer model trained on text data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12 – SHAP bar plot used to explain a transformer model trained on text data</p>
			<p>I find it easier to interpret <a id="_idIndexMarker549"/>bar plots, which clearly show the positive or negative impact of each word present in the sentence, as shown in <em class="italic">Figure 7.12</em>. </p>
			<p>Next, let's explore another <a id="_idIndexMarker550"/>example, in which a transformer-based multi-class classification model is trained on text data.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor150"/>Explaining a multi-class prediction transformer model using SHAP</h2>
			<p>In the previous example, we<a id="_idIndexMarker551"/> applied SHAP to explain a text-based binary classification model. Now, let's apply SHAP to explain a <a id="_idIndexMarker552"/>pre-trained transformer model used for detecting one of the following six emotions: <em class="italic">sadness</em>, <em class="italic">joy</em>, <em class="italic">love</em>, <em class="italic">anger</em>, <em class="italic">fear</em>, and <em class="italic">surprise</em>. </p>
			<p>Let's load the pre-trained transformer model for emotion detection:</p>
			<pre class="source-code">tokenizer = transformers.AutoTokenizer.from_pretrained("nateraw/bert-base-uncased-emotion", use_fast=True)</pre>
			<pre class="source-code">model = transformers.AutoModelForSequenceClassification.from_pretrained("nateraw/bert-base-uncased-emotion").cuda()</pre>
			<pre class="source-code"># build a pipeline object to do predictions</pre>
			<pre class="source-code">pipeline = transformers.pipeline("text-classification", model=model, tokenizer=tokenizer, device=0, return_all_scores=True)</pre>
			<p>Now, let's use the <a id="_idIndexMarker553"/>same inference <a id="_idIndexMarker554"/>data as in the previous example and compute the SHAP values using SHAP:</p>
			<pre class="source-code">explainer = shap.Explainer(pipeline)</pre>
			<pre class="source-code">shap_values = explainer([text_data])</pre>
			<p>We can then use the SHAP text plot to highlight words that make a positive or negative contribution to each of the six possible outcomes:</p>
			<pre class="source-code">shap.plots.text(shap_values[0])</pre>
			<p><em class="italic">Figure 7.13</em> illustrates the output of the SHAP text plot obtained from the previous line of code:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B18216_07_013.jpg" alt="Figure 7.13 – Interactive SHAP text plot highlighting the words that make a positive and negative contribution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – Interactive SHAP text plot highlighting the words that make a positive and negative contribution</p>
			<p>SHAP text plots are interactive. As we can see from <em class="italic">Figure 7.13</em>, this highlights the model's predicted outcome in red along with the words that make a positive and negative contribution to the model's decision. We can also click on other possible outcomes and visualize the influence of each word on the model's prediction. For example, if we click on <em class="italic">surprise</em> instead of <em class="italic">joy</em>, we will see that all the words other than the word <em class="italic">face</em> are highlighted in <a id="_idIndexMarker555"/>blue, as these words are contributing negatively to that specific outcome. Personally, I found this approach of explaining transformer models trained on text data using SHAP to be<a id="_idIndexMarker556"/> really interesting and efficient! Next, let's cover another interesting use case of applying SHAP to explain NLP zero-shot learning models.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor151"/>Explaining zero-shot learning models using SHAP</h2>
			<p><strong class="bold">Zero-shot learning</strong> is one of the most fascinating concepts in NLP, which involves applying models on inference data for predicting any custom category that is not used during the training process <a id="_idIndexMarker557"/>without the need for fine-tuning. You can find more information about zero-shot learning in<a id="_idIndexMarker558"/> this reference link: <a href="https://aixplain.com/2021/09/23/zero-shot-learning-in-natural-language-processing/">https://aixplain.com/2021/09/23/zero-shot-learning-in-natural-language-processing/</a>. Applying SHAP to zero-shot learning models is also very straightforward. </p>
			<p>First, we will need to load the pre-trained transformer models:</p>
			<pre class="source-code">model = AutoModelForSequenceClassification.from_pretrained("valhalla/distilbart-mnli-12-3")</pre>
			<pre class="source-code">tokenizer = AutoTokenizer.from_pretrained("valhalla/distilbart-mnli-12-3")</pre>
			<p>We will need to create a custom zero-shot learning pipeline in order for SHAP to work:</p>
			<pre class="source-code">class ZSLPipeline(ZeroShotClassificationPipeline):</pre>
			<pre class="source-code">    # Overwrite the __call__ method</pre>
			<pre class="source-code">    def __call__(self, *args):</pre>
			<pre class="source-code">        out = super().__call__(args[0], self.set_labels)[0]</pre>
			<pre class="source-code">        return [[{"label":x[0], "score": x[1]}  for x in zip(out["labels"], out["scores"])]]</pre>
			<pre class="source-code">    def set_labels(self, labels: Union[str,List[str]]):</pre>
			<pre class="source-code">        self.set_labels = labels</pre>
			<p>We will then need to define<a id="_idIndexMarker559"/> the custom labels and the inference text data and configure the new labels for <a id="_idIndexMarker560"/>the zero-shot learning model. For this example, we have selected the text <strong class="source-inline">"I love playing cricket!"</strong> as our inference data and we want our zero-shot learning model to predict whether the inference text data belongs to the <strong class="source-inline">insect</strong>, <strong class="source-inline">sports</strong>, or <strong class="source-inline">animal</strong> class:</p>
			<pre class="source-code">text = ["I love playing cricket!"]</pre>
			<pre class="source-code">labels = ["insect", "sports", "animal"]</pre>
			<pre class="source-code">model.config.label2id.update({v:k for k,v in enumerate(labels)})</pre>
			<pre class="source-code">model.config.id2label.update({k:v for k,v in enumerate(labels)})</pre>
			<pre class="source-code">pipe = ZSLPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)</pre>
			<pre class="source-code">pipe.set_labels(labels)</pre>
			<p>Once the process of setting up the zero-shot learning model is ready, we can easily apply SHAP for the model explainability:</p>
			<pre class="source-code">explainer = shap.Explainer(pipe)</pre>
			<pre class="source-code">shap_values = explainer(text)</pre>
			<p>After the SHAP values have been computed successfully, we can use text plots or bar plots for the model explainability:</p>
			<pre class="source-code">shap.plots.text(shap_values)</pre>
			<pre class="source-code">shap.plots.bar(shap_values[0,:,'sports'])</pre>
			<p><em class="italic">Figure 7.14</em> shows the bar plot <a id="_idIndexMarker561"/>visualization obtained as an output:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B18216_07_014.jpg" alt="Figure 7.14 – Bar plot visualization for the outcome of the predicted sport for the zero-shot learning model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – Bar plot visualization for the outcome of the predicted sport for the zero-shot learning model</p>
			<p>The inference text sentence – <strong class="source-inline">"I love playing cricket!"</strong> used for this example was indeed related to the <strong class="source-inline">Sports</strong> class, which was correctly predicted by the model. However, cricket is not only a sport, but an insect as well. When the phrase <strong class="source-inline">playing cricket</strong> is used, collectively it indicates that we are talking about a sport. So, these words should make a <a id="_idIndexMarker562"/>positive contribution to the model's prediction. Unfortunately, from <em class="italic">Figure 7.14</em>, we can see that both the words <strong class="source-inline">playing</strong> and <strong class="source-inline">cricket</strong> are negatively contributing with negative SHAP values. This gives us an indication that even though the model prediction is correct, this is not a very good model as the model is relying heavily on the word <strong class="source-inline">love</strong> instead of the words <strong class="source-inline">cricket</strong> or <strong class="source-inline">playing</strong>. This is a classic example that highlights the<a id="_idIndexMarker563"/> need to make <strong class="bold">Explainable AI</strong> (<strong class="bold">XAI</strong>) a mandatory part of the AI process cycle and we should not blindly trust models even if the model prediction is correct.</p>
			<p>We have now arrived at the end of this chapter, and I will summarize the important topics that we have discussed in this chapter.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor152"/>Summary</h1>
			<p>After reading this chapter, you have received some practical exposure to using SHAP with tabular structured data as well as unstructured data such as images and texts. We have discussed the different explainers available in SHAP for both model-specific and model-agnostic explainability. We have applied SHAP to explain linear models, tree ensemble models, convolution neural network models, and even transformer models in this chapter. Using SHAP, we can explain different types of models trained on different types of data. I highly recommend trying out the end-to-end tutorials provided in the GitHub code repository and exploring things in more depth to acquire deeper practical knowledge. </p>
			<p>In the next chapter, we will discuss another interesting topic of concept activation vectors and explore the practical part of applying the <strong class="bold">Testing with Concept Activation Vectors </strong>(<strong class="bold">TCAV</strong>) framework from Google AI for explaining models with human-friendly concepts.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor153"/>References</h1>
			<p>Please refer to the following resources to gain additional information:</p>
			<ul>
				<li><em class="italic">Shapley, Lloyd S. "A value for n-person games." Contributions to the Theory of Games 2.28 (1953)</em>: <a href="https://doi.org/10.1515/9781400881970-018">https://doi.org/10.1515/9781400881970-018</a></li>
				<li><em class="italic">Red Wine Quality Dataset – Kaggle</em>: <a href="https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009">https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009</a></li>
				<li><em class="italic">SHAP GitHub Project</em>: <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></li>
				<li><em class="italic">SHAP Documentation:</em> <a href="https://shap.readthedocs.io/en/latest/index.html">https://shap.readthedocs.io/en/latest/index.html</a></li>
				<li><em class="italic">Hugging Face Models</em>: <a href="https://huggingface.co/">https://huggingface.co/</a> </li>
				<li><em class="italic">Zero-Shot Learning</em>: <a href="https://en.wikipedia.org/wiki/Zero-shot_learning">https://en.wikipedia.org/wiki/Zero-shot_learning</a> </li>
			</ul>
		</div>
	</body></html>