["```py\n    $ python\n    Python 3.5.1 (default, Apr 11 2014, 13:05:11)\n    [GCC 4.8.2] on Linux\n    Type \"help\", \"copyright\", \"credits\" or \"license\" for more \n      information.\n    >>> print(\"Hello, world!\")\nHello, world!\n    >>> exit()\n\n```", "```py\n    $ conda install jupyter notebook\n\n```", "```py\n    $ jupyter notebook\n\n```", "```py\n    $ conda install scikit-learn\n\n```", "```py\nimport numpy as np \ndataset_filename = \"affinity_dataset.txt\" \nX = np.loadtxt(dataset_filename)\n\n```", "```py\nn_samples, n_features = X.shape\n\n```", "```py\nprint(X[:5])\n\n```", "```py\n[[ 0\\.  1\\.  0\\.  0\\.  0.] \n [ 1\\.  1\\.  0\\.  0\\.  0.] \n [ 0\\.  0\\.  1\\.  0\\.  1.] \n [ 1\\.  1\\.  0\\.  0\\.  0.] \n [ 0\\.  0\\.  1\\.  1\\.  1.]]\n\n```", "```py\nfeatures = [\"bread\", \"milk\", \"cheese\", \"apples\", \"bananas\"]\n\n```", "```py\nsample = X[2]\n\n```", "```py\nfrom collections import defaultdict \nvalid_rules = defaultdict(int) \ninvalid_rules = defaultdict(int) \nnum_occurences = defaultdict(int)\n\n```", "```py\nfor sample in X:\n    for premise in range(n_features):\n    if sample[premise] == 0: continue\n# Record that the premise was bought in another transaction\n    num_occurences[premise] += 1\n    for conclusion in range(n_features):\n    if premise == conclusion: \n# It makes little sense to\n    measure if X -> X.\n    continue\n    if sample[conclusion] == 1:\n# This person also bought the conclusion item\n    valid_rules[(premise, conclusion)] += 1\n\n```", "```py\nsupport = valid_rules\n\n```", "```py\nconfidence = defaultdict(float)\nfor premise, conclusion in valid_rules.keys():\n    rule = (premise, conclusion)\n    confidence[rule] = valid_rules[rule] / num_occurences [premise]\n\n```", "```py\nfor premise, conclusion in confidence:\n    premise_name = features[premise]\n    conclusion_name = features[conclusion]\n    print(\"Rule: If a person buys {0} they will also \n          buy{1}\".format(premise_name, conclusion_name))\n    print(\" - Confidence: {0:.3f}\".format\n          (confidence[(premise,conclusion)]))\n    print(\" - Support: {0}\".format(support\n                                   [(premise, \n                                     conclusion)]))\n    print(\"\")\n\n```", "```py\nfor premise, conclusion in confidence:\n    premise_name = features[premise]\n    conclusion_name = features[conclusion]\n    print(\"Rule: If a person buys {0} they will also \n          buy{1}\".format(premise_name, conclusion_name))\n    print(\" - Confidence: {0:.3f}\".format\n          (confidence[(premise,conclusion)]))\n    print(\" - Support: {0}\".format(support\n                                   [(premise, \n                                     conclusion)]))\n    print(\"\")\n\n```", "```py\nfrom operator import itemgetter \nsorted_support = sorted(support.items(), key=itemgetter(1), reverse=True)\n\n```", "```py\nsorted_confidence = sorted(confidence.items(), key=itemgetter(1),\n                           reverse=True)\nfor index in range(5):\n    print(\"Rule #{0}\".format(index + 1))\n    premise, conclusion = sorted_confidence[index][0]\n    print_rule(premise, conclusion, support, confidence, features)\n\n```", "```py\nRule #1 \nRule: If a person buys bananas they will also buy milk \n - Support: 27 \n - Confidence: 0.474 \nRule #2 \nRule: If a person buys milk they will also buy bananas \n - Support: 27 \n - Confidence: 0.519 \nRule #3 \nRule: If a person buys bananas they will also buy apples \n - Support: 27 \n - Confidence: 0.474 \nRule #4 \nRule: If a person buys apples they will also buy bananas \n - Support: 27 \n - Confidence: 0.628 \nRule #5 \nRule: If a person buys apples they will also buy cheese \n - Support: 22 \n - Confidence: 0.512\n\n```", "```py\nsorted_confidence = sorted(confidence.items(), key=itemgetter(1),\n                           reverse=True)\nfor index in range(5):\n    print(\"Rule #{0}\".format(index + 1))\n    premise, conclusion = sorted_confidence[index][0]\n    print_rule(premise, conclusion, support, confidence, features)\n\n```", "```py\nfrom matplotlib import pyplot as plt \nplt.plot([confidence[rule[0]] for rule in sorted_confidence])\n\n```", "```py\nfrom sklearn.datasets import load_iris \ndataset = load_iris() \nX = dataset.data \ny = dataset.target\n\n```", "```py\nattribute_means = X.mean(axis=0)\n\n```", "```py\nassert attribute_means.shape == (n_features,)\nX_d = np.array(X >= attribute_means, dtype='int')\n\n```", "```py\nfrom collections import defaultdict \nfrom operator import itemgetter\n\n```", "```py\ndef train_feature_value(X, y_true, feature, value):\n# Create a simple dictionary to count how frequency they give certain\npredictions\n class_counts = defaultdict(int)\n# Iterate through each sample and count the frequency of each\nclass/value pair\n for sample, y in zip(X, y_true):\n    if sample[feature] == value: \n        class_counts[y] += 1\n# Now get the best one by sorting (highest first) and choosing the\nfirst item\nsorted_class_counts = sorted(class_counts.items(), key=itemgetter(1),\n                             reverse=True)\nmost_frequent_class = sorted_class_counts[0][0]\n # The error is the number of samples that do not classify as the most\nfrequent class\n # *and* have the feature value.\n    n_samples = X.shape[1]\n    error = sum([class_count for class_value, class_count in\n                 class_counts.items()\n if class_value != most_frequent_class])\n    return most_frequent_class, error\n\n```", "```py\ndef train(X, y_true, feature): \n    # Check that variable is a valid number \n    n_samples, n_features = X.shape \n    assert 0 <= feature < n_features \n    # Get all of the unique values that this variable has \n    values = set(X[:,feature]) \n    # Stores the predictors array that is returned \n    predictors = dict() \n    errors = [] \n    for current_value in values: \n        most_frequent_class, error = train_feature_value\n        (X, y_true, feature, current_value) \n        predictors[current_value] = most_frequent_class \n        errors.append(error) \n    # Compute the total error of using this feature to classify on \n    total_error = sum(errors) \n    return predictors, total_error\n\n```", "```py\n    values = set(X[:,feature_index])\n\n```", "```py\npredictors = {} \n    errors = []\n\n```", "```py\ntotal_error = sum(errors)\nreturn predictors, total_error\n\n```", "```py\nfrom sklearn.cross_validation import train_test_split\n\n```", "```py\nXd_train, Xd_test, y_train, y_test = train_test_split(X_d, y, \n    random_state=14)\n\n```", "```py\nall_predictors = {} \nerrors = {} \nfor feature_index in range(Xd_train.shape[1]): \n    predictors, total_error = train(Xd_train,\n                                    y_train,\n                                    feature_index) \n    all_predictors[feature_index] = predictors \n    errors[feature_index] = total_error\n\n```", "```py\nbest_feature, best_error = sorted(errors.items(), key=itemgetter(1))[0]\n\n```", "```py\nmodel = {'feature': best_feature,\n         'predictor': all_predictors[best_feature]}\n\n```", "```py\nvariable = model['feature'] \npredictor = model['predictor'] \nprediction = predictor[int(sample[variable])]\n\n```", "```py\ndef predict(X_test, model):\nvariable = model['feature']\npredictor = model['predictor']\ny_predicted = np.array([predictor\n                        [int(sample[variable])] for sample\n                        in X_test])\nreturn y_predicted\n\n```", "```py\ny_predicted = predict(Xd_test, model)\n\n```", "```py\naccuracy = np.mean(y_predicted == y_test) * 100 \nprint(\"The test accuracy is {:.1f}%\".format(accuracy))\n\n```"]