["```py\n> cd {KAFKA_HOME}\n> bin/zookeeper-server-start.sh -daemon config/zookeeper.properties\n> bin/kafka-server-start.sh -daemon config/server.properties\n```", "```py\n> bin/kafka-topics.sh --create --zookeeper 192.168.56.10:2181 --replication-factor 1 --partitions 1 --topic twitter\n```", "```py\n- App Name (max 32 characters) e.g. \"Airline Sentiment Analysis\"\n- Application Description (max 200 characters) e.g. \"This App will collect tweets about airlines and apply our previously trained decision tree classifier to predict and classify the underlying sentiment of those tweets in real-time\"\n- Website URL (for attribution purposes only - if you do not have a personal website, then use the URL to your Twitter page, such as https://twitter.com/PacktPub)\n- Tell us how this app will be used (min 100 characters) e.g. \"Internal training and development purposes only, including the deployment of machine learning models in real-time. It will not be visible to customers or 3rd parties.\"\n```", "```py\n#!/usr/bin/python\n\n\"\"\" config.py: Environmental and Application Settings \"\"\"\n\n\"\"\" ENVIRONMENT SETTINGS \"\"\"\n\n# Apache Kafka\nbootstrap_servers = '192.168.56.10:9092'\ndata_encoding = 'utf-8'\n\n\"\"\" TWITTER APP SETTINGS \"\"\"\n\nconsumer_api_key = 'Enter your Twitter App Consumer API Key here'\nconsumer_api_secret = 'Enter your Twitter App Consumer API Secret Key here'\naccess_token = 'Enter your Twitter App Access Token here'\naccess_token_secret = 'Enter your Twitter App Access Token Secret here'\n\n\"\"\" SENTIMENT ANALYSIS MODEL SETTINGS \"\"\"\n\n# Name of an existing Kafka Topic to publish tweets to\ntwitter_kafka_topic_name = 'twitter'\n\n# Keywords, Twitter Handle or Hashtag used to filter the Twitter Stream\ntwitter_stream_filter = '@British_Airways'\n\n# Filesystem Path to the Trained Decision Tree Classifier\ntrained_classification_model_path = '..chapter06/models/airline-sentiment-analysis-decision-tree-classifier'\n```", "```py\nimport config\nimport tweepy\nfrom tweepy import OAuthHandler\nfrom tweepy import Stream\nfrom tweepy.streaming import StreamListener\nimport pykafka\n```", "```py\nauth = OAuthHandler(config.consumer_api_key, \n   config.consumer_api_secret)\nauth.set_access_token(config.access_token, \n   config.access_token_secret)\napi = tweepy.API(auth)\n```", "```py\nclass KafkaTwitterProducer(StreamListener):\n\n    def __init__(self):\n        self.client = pykafka.KafkaClient(config.bootstrap_servers)\n        self.producer = self.client.topics[bytes(\n           config.twitter_kafka_topic_name, \n           config.data_encoding)].get_producer()\n\n    def on_data(self, data):\n        self.producer.produce(bytes(data, config.data_encoding))\n        return True\n\n    def on_error(self, status):\n        print(status)\n        return True\n```", "```py\nprint(\"Instantiating a Twitter Stream and publishing to the '%s' \n   Kafka Topic...\" % config.twitter_kafka_topic_name)\ntwitter_stream = Stream(auth, KafkaTwitterProducer())\n```", "```py\nprint(\"Filtering the Twitter Stream based on the query '%s'...\" % \n   config.twitter_stream_filter)\ntwitter_stream.filter(track=[config.twitter_stream_filter])\n```", "```py\n> python kafka_twitter_producer.py\n $ Instantiating a Twitter Stream and publishing to the 'twitter' \n     Kafka Topic...\n $ Filtering the Twitter Stream based on the query \n     '@British_Airways'...\n```", "```py\n> cd {KAFKA_HOME}\n> bin/kafka-console-consumer.sh --bootstrap-server 192.168.56.10:9092 --topic twitter\n```", "```py\n#!/usr/bin/python\n\n\"\"\" model_pipelines.py: Pre-Processing and Feature Vectorization Spark Pipeline function definitions \"\"\"\n\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import Tokenizer\nfrom pyspark.ml.feature import StopWordsRemover\nfrom pyspark.ml.feature import HashingTF\nfrom pyspark.ml import Pipeline, PipelineModel\n\nfrom sparknlp.base import *\nfrom sparknlp.annotator import Tokenizer as NLPTokenizer\nfrom sparknlp.annotator import Stemmer, Normalizer\n\ndef preprocessing_pipeline(raw_corpus_df):\n\n    # Native MLlib Feature Transformers\n    filtered_df = raw_corpus_df.filter(\"text is not null\")\n    tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"tokens_1\")\n    tokenized_df = tokenizer.transform(filtered_df)\n    remover = StopWordsRemover(inputCol = \"tokens_1\",\n       outputCol = \"filtered_tokens\")\n\n    preprocessed_part_1_df = remover.transform(tokenized_df)\n    preprocessed_part_1_df = preprocessed_part_1_df\n       .withColumn(\"concatenated_filtered_tokens\", concat_ws(\" \",\n          col(\"filtered_tokens\")))\n\n    # spark-nlp Feature Transformers\n    document_assembler = DocumentAssembler()\n       .setInputCol(\"concatenated_filtered_tokens\")\n    tokenizer = NLPTokenizer()\n       .setInputCols([\"document\"]).setOutputCol(\"tokens_2\")\n    stemmer =    \n    Stemmer().setInputCols([\"tokens_2\"]).setOutputCol(\"stems\")\n    normalizer = Normalizer().setInputCols([\"stems\"])\n       .setOutputCol(\"normalised_stems\")\n\n    preprocessing_pipeline = Pipeline(stages = [document_assembler,\n       tokenizer, stemmer, normalizer])\n    preprocessing_pipeline_model = preprocessing_pipeline\n       .fit(preprocessed_part_1_df)\n    preprocessed_df = preprocessing_pipeline_model\n       .transform(preprocessed_part_1_df)\n    preprocessed_df.select(\"id\", \"text\", \"normalised_stems\")\n\n    # Explode and Aggregate\n    exploded_df = preprocessed_df\n       .withColumn(\"stems\", explode(\"normalised_stems\"))\n       .withColumn(\"stems\", col(\"stems\").getItem(\"result\"))\n       .select(\"id\", \"text\", \"stems\")\n\n    aggregated_df = exploded_df.groupBy(\"id\")\n       .agg(concat_ws(\" \", collect_list(col(\"stems\"))), first(\"text\"))\n       .toDF(\"id\", \"tokens\", \"text\")\n       .withColumn(\"tokens\", split(col(\"tokens\"), \" \")\n       .cast(\"array<string>\"))\n\n    # Return the final processed DataFrame\n    return aggregated_df\n\ndef vectorizer_pipeline(preprocessed_df):\n\n    hashingTF = HashingTF(inputCol = \"tokens\", outputCol = \"features\",\n       numFeatures = 280)\n    features_df = hashingTF.transform(preprocessed_df)\n\n    # Return the final vectorized DataFrame\n    return features_df\n```", "```py\nimport config\nimport model_pipelines\n```", "```py\nspark = SparkSession.builder.appName(\"Stream Processing - Real-Time Sentiment Analysis\").getOrCreate()\n```", "```py\ndecision_tree_model = DecisionTreeClassificationModel.load(\n   config.trained_classification_model_path)\n```", "```py\nschema = StructType([\n    StructField(\"created_at\", StringType()), \n    StructField(\"id\", StringType()), \n    StructField(\"id_str\", StringType()), \n    StructField(\"text\", StringType()), \n    StructField(\"retweet_count\", StringType()), \n    StructField(\"favorite_count\", StringType()), \n    StructField(\"favorited\", StringType()), \n    StructField(\"retweeted\", StringType()), \n    StructField(\"lang\", StringType()), \n    StructField(\"location\", StringType()) \n])\n```", "```py\ntweets_df = spark.readStream.format(\"kafka\")\n  .option(\"kafka.bootstrap.servers\", config.bootstrap_servers)\n  .option(\"subscribe\", config.twitter_kafka_topic_name).load()\n```", "```py\ntweets_df = tweets_df.selectExpr(\n   \"CAST(key AS STRING)\", \"CAST(value AS STRING) as json\")\n   .withColumn(\"tweet\", from_json(col(\"json\"), schema=schema))\n   .selectExpr(\"tweet.id_str as id\", \"tweet.text as text\")\n```", "```py\npreprocessed_df = model_pipelines.preprocessing_pipeline(tweets_df)\n```", "```py\nfeatures_df = model_pipelines.vectorizer_pipeline(preprocessed_df)\n```", "```py\npredictions_df = decision_tree_model.transform(features_df)\n   .select(\"id\", \"text\", \"prediction\")\n```", "```py\nquery = predictions_df.writeStream\n   .outputMode(\"complete\")\n   .format(\"console\")\n   .option(\"truncate\", \"false\")\n   .start() \nquery.awaitTermination()\n```", "```py\n> cd {SPARK_HOME}\n> bin/spark-submit --master spark://192.168.56.10:7077 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2,JohnSnowLabs:spark-nlp:1.7.0 --py-files chapter08/config.py,chapter08/model_pipelines.py chapter08/kafka_twitter_consumer.py\n```"]