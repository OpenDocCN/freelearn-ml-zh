<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">High-Performance Computing</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn about the specific aspects of virtualization that can enhance the productivity of a <strong>High Performance Computing</strong> (<strong>HPC</strong>) environment. We will focus on the capabilities provided by VMware vSphere, and how virtualization improves scientific productivity.</p>
<p>We will explore vSphere features, such as <span><strong>Single Root I/O Virtualization</strong> (<strong>SR-IOV</strong>)</span>, <span><strong>remote direct memory access</strong> (</span><strong>RDMA</strong>), and vGPU, to architect and meet the requirements for research, computing, academic, scientific, and engineering workloads.</p>
<p>This chapter covers the following topics:</p>
<ul>
<li class="h1">Virtualizing HPC applications</li>
<li class="h2">Multi-tenancy with guaranteed resources</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can download vSphere Scale-Out from <a href="https://my.vmware.com/en/web/vmware/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/6_7">https://my.vmware.com/en/web/vmware/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/6_7</a>.<a href="https://my.vmware.com/en/web/vmware/info/slug/datacenter_cloud_infrastructure/vmware_vsphere/6_7"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Virtualizing HPC applications</h1>
                </header>
            
            <article>
                
<p>This chapter describes our work at VMware to support HPC applications. The first section describes in detail many of the values identified by customers of using virtualization in HPC environments. The second section shows a few examples of how virtualization is deployed in an HPC environment, and the third discusses various aspects of performance, starting with an examination of some core aspects of performance, and then turning to throughput applications and performance for parallel-distributed <strong>M</strong><span><strong>essage Passing Interface</strong> </span>(<strong>MPI</strong>) applications. It also includes pointers to several technical publications that will be of interest to those considering virtualizing their HPC workloads.</p>
<p>The majority of HPC systems are clusters, which are aggregations of compute nodes connected via some interconnect, such as Ethernet or <strong>InfiniBand</strong> (<strong>IB</strong>). Clusters can range in size from a small handful of nodes to tens of thousands of nodes. HPC clusters exist to run HPC jobs, and the placement of those jobs in the cluster is handled by a <strong>distributed resource manager</strong> (<strong>DRM</strong>). DRM is the middleware that provides the ability for HPC users to launch their HPC jobs onto an HPC cluster in a load-balanced fashion.</p>
<p>Users typically use command-line interfaces to specify the characteristics of the job or jobs they want to run, the DRM then enqueues those requests and schedules the jobs to run on the least loaded, appropriately configured nodes in the cluster. There are many DRMs available, both open source and commercial. Examples include Grid Engine (Univa), LSF (IBM), Torque, <span><strong>Portable Batch System</strong> (<strong>PBS</strong>)</span>, and Slurm. DRMs are also called batch schedulers. IB is a high-bandwidth, low-latency interconnect, commonly used in HPC environments to boost the performance of code/applications/jobs and to increase filesystem performance. IB is not Ethernet, it does not use TCP or any of the standard networking stack, and it is currently usable in a virtual environment only via VM direct path I/O (passthrough mode).</p>
<p>The purpose of this chapter is not to explain how virtualization works. x86 virtualization has evolved from its invention at Stanford in the late 1990s using a purely software-based approach to the current situation in which both Intel and AMD have added successively more hardware support for virtualization, such as CPU, memory, and I/O. These hardware enhancements, along with increasingly sophisticated virtualization software, have greatly improved the performance for an ever-growing number of workloads. This is an important point to make because HPC people have often heard that the performance of HPC applications is very poor when they get virtualized with some cases but run extremely well, even very close to native performance, in most of the cases. Massive consolidation isn't appropriate for HPC environments, so the values of virtualization for HPC can be utilized elsewhere.</p>
<p>We will now learn the use cases for virtualization in HPC that have been identified by customers and through our own research. Because HPC includes such a wide variety of workloads and environments, some of these will resonate more with specific customers than others. HPC clusters host a single, standard OS and application stack across all hosts as uniformity enables us to schedule jobs easily by limiting the options in these environments for different use cases, such as multiple user groups needing to be served from a single shared resource. Because these traditional clusters can't satisfy the needs of multiple groups, they encourage the creation of specialized <em>islands of compute</em> scattered across an organization, which is inefficient and expensive.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Multi-tenancy with guaranteed resources</h1>
                </header>
            
            <article>
                
<p>Customers want to utilize existing investments in hardware and software such as in hypervisors or physical hardware along with the feasibility to provision directly into the public cloud. We can address this landscape, along with related services, through a service-automation solution that can manage across many platforms and many clouds. This solution can automate all manual processes of the provisioning services by abstracting the core tasks with its automation tool and then managing the access and control of these automations. Automation is very useful only if we link it with policy. Policy-based control and governance provides us with the ability to control the application of the automations that drive the cloud solution. The cloud service portal and catalog give end users self-service, on-demand access to authorized services.</p>
<p>All modern x86 systems are<span> </span><strong>non-uniform memory access</strong><span> </span>(<strong>NUMA</strong>) systems, which means that memory is attached directly to individual CPU sockets in the system. This means that access to memory from the local socket can be very fast, but access to memory that is attached to another socket will be slower because the request and data have to transit the communication pathway between the sockets. That's why it is called non-uniform memory access<span>. </span>Sometimes workloads can run a little faster in virtual environments than on bare metal. This is true especially of throughput workloads and is often the result of NUMA effects. The point isn't so much that virtualized can run faster than bare metal, but that virtualized's performance can be nearly identical to the bare-metal performance for some HPC workloads. Live migration can be used to increase the efficiency and flexibility of HPC environments. It can also be used to increase resiliency. In traditional bare-metal HPC environments, jobs are placed statically. Consider the scenario in which Application C must be scheduled, but there is currently no node with enough resources available to run it.</p>
<p>There are a two choices in a bare-metal environment:</p>
<ul>
<li>Application C can wait in the queue until Application A or B finishes</li>
<li>Application A or B could be killed to make room for Application C</li>
</ul>
<p>Either of these options reduces the cluster's job throughput. And in the case of killing jobs, the loss of work can be very expensive if the running applications are costly Independent Software Vendors' application. The solution in the virtual environment is to use live migration to move the workload to make room for Application C. This approach is relevant primarily in environments where these jobs are relatively long-running.</p>
<p>Let's look at <span>another use of live migration: to increase overall throughput on a cluster.</span><span> Consider the bare-metal environment with two jobs running. As the third job is started, it may consume more memory than expected by the user. This will bound other jobs on the system to swap, which will affect the overall performance of these jobs in negative way.</span> <strong>Dynamic Resource Scheduler</strong> <span>(</span><span><strong>DRS</strong>)</span> <span>can address these kind of situations in a virtual environment: as the third job starts to consume all available memory, DRS can shift the overloaded VM to another machine with few workloads and help the jobs to continue running with the desired performance.</span></p>
<p>This lack of guaranteed resource use for specific groups or departments is another barrier to the centralization of bare-metal environments. For example, an owner of a large island of compute will often be unwilling to donate their hardware resources to a shared pool if they cannot be guaranteed access to at least those resources when required. DRS provides the ability to address this need. Snapshots can be used to save the state of a running VM to protect against hardware failures; when a machine fails, the VM is restored and the application continues the execution from the point at which the snapshot was taken.</p>
<p>This is similar in concept to checkpoint mechanisms used in HPC, except that, rather than extracting the state of a process from a running OS, which is often subject to various limitations, we take advantage of the clean abstract that exists between the VM and the underlying virtual platform. A more advanced resiliency scheme (Proactive HA Policy) would use telemetry from the underlying system to predict upcoming hardware failures and then proactively migrate the workload from the questionable host to avoid application interruption. Examples include a fan failure on a system running an important job or an increase in the rate of detected soft memory errors, which could indicate increased probability of an upcoming hard memory error.</p>
<p>While such an approach would not likely remove the need for checkpointing, it may reduce the need. Less frequent checkpoint operations and less frequent restorations from checkpoints can increase overall job throughput on the cluster.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Critical use case – unification </h1>
                </header>
            
            <article>
                
<p>The desire of some administrators is to treat their HPC jobs as <em>just another workload</em> and move away from a partially virtual and partially physical infrastructure to an all-virtual infrastructure with more flexibility and manageability. This is a simple example of a deployment done by one of our financial services customers. In this case, the central IT department created a shared compute resource that could rent out virtual clusters to the different lines of business within the organization. Groups that need access to a cluster for some period of time would receive a set of VMs rather than a physical machine to be used for some specified time period.</p>
<p>The benefits to the organization are as follows:</p>
<ul>
<li><span><strong>Lines-of-business</strong> (</span><strong>LOBs</strong>) get the resources they need when they need them</li>
<li>Clusters and the cluster nodes can be sized to the LOBs' application requirements (for example, most process runs consume only a single CPU per job; they are serial (not parallel) jobs)</li>
<li>A central IT team can extract maximum from the available hardware</li>
<li>Hardware is flexible enough to be shared among various jobs with security compliance between users/workloads</li>
<li class="CDPAlignLeft CDPAlign"><span>Relative priorities between LOBs can be enforced by policy to ensure (for example) that groups with hard deadlines receive a higher <em>fairshare</em> of the underlying hardware resources:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4fc33cfb-9cf0-415e-8f8a-550857f6fe7c.png" style="font-size: 1em;width:35.25em;height:19.92em;"/><br/></p>
<p>There are certain benefits of configuring platform-level (and sometimes guest-level) tuning in order to achieve best performance for HPC applications on vSphere. Tuning is needed because HPC applications (unlike most Enterprise applications) are latency-sensitive. The sensitivity might be in storage, in networking, or in the communication interconnect.</p>
<p>While VMware's goal is excellent out-of-the-box performance for any application, the HPC workloads are relatively new to us, so some tuning is required. We have begun setting some of these tunables automatically by configuring vCenter advanced VM parameters where we intend to run latency-sensitive workloads in a VM. This auto-tuning will become more comprehensive over time.</p>
<p>The customer's initial <strong>Network Filesystem Storage</strong> (<strong>NFS</strong>) benchmark experience pointed directly to storage latencies in some cases. By tuning the networking stack that underlies any NFS data transfers, we were able to bring application performance directly in line, as seen in bare-metal environments. The network is by default tuned for throughput by moving large amounts of data through the network efficiently in an enterprise environment. This means that, as data packets arrive, they may not be processed immediately.</p>
<p>Typically, a small number of messages are allowed to accumulate before the system wakes up and processes the entire batch. This reduces the load on the CPU, but it slows down message delivery. It makes much more sense to spend more CPU cycles to process the packets promptly as each packet arrives in situations where the arrival of the data is a gating factor on performance. This is often the case with HPC workloads. Both the virtual and physical network devices should have coalescing turned off to make this change.</p>
<p>There is another level of coalescing, which happens at a higher level in the <strong>t</strong><span><strong>ransmission control protocol</strong> (</span><strong>TCP</strong>) stack (disable <strong>Large Receive Offload</strong> (<strong><span>LRO</span></strong>) within the guest)—<span>it</span> should be turned off. We evaluate results of some experiments to see whether the additional level of memory abstraction which is introduced by virtualization, has any effect on HPC application performance or not.</p>
<p>The special circumstance is applications that have little or no spatial data locality. This includes applications that can't effectively use their caches because they don't access memory in regular ways, such as the random-access benchmark. It doesn't affect except in a special circumstance where the overhead can be greatly reduced by using large pages within the guest operating system. Turning off EPT (RVI on AMD) and reverting to a software-based page-table approach can also help in this special circumstance. Created by the <strong>National Security Agency</strong> (<strong>NSA</strong>), it has no locality even updating random memory locations one after another.</p>
<p>These workloads incur a very large number of <strong><span>translation look-aside buffer</span></strong> (<strong>TLB</strong>) misses. Because misses in this page-table cache are frequent, the application can be slowed down if that operation is slow. Indeed, it turns out that the EPT and RVI technologies created by Intel and RVI, although implemented in hardware, are slower at handling TLB misses than the older <em>shadow page table</em> approach developed by VMware. TLB misses can be reduced by using larger pages, so turning off EPT or RVI can help in these situations.</p>
<p>The fact is that EPT and RVI perform very well in the vast majority of cases, but it pays to keep the issue in mind. <strong>High-performance LINPACK</strong> (<strong>HPL</strong>) uses caches very nicely like other HPC applications and get better performance. We can see that performance is uniformly excellent for this application type.</p>
<p>This is an important aspect of our virtualization platform for HPC customers who run applications that require large numbers of threads, and, therefore, VMs that would span multiple CPU sockets within a host.</p>
<p>ESXi has been <em>NUMA-aware</em> for many releases, which means that when it runs a VM, it is careful to place the executing thread on the same socket that is hosting the VM's memory so all memory accesses are local. When a VM spans the two sockets, we distribute similar threads across both sockets and then allocate memory on the local socket to deliver the best performance. So, even if the VM were big enough to span multiple sockets, it would not see the NUMA-ness of the underlying hardware. We introduced vNUMA to make the NUMA topology visible to the guest OS, which can then make its own optimizations based on this information. This can matter a lot from a performance perspective.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">High-performance computing cluster performances</h1>
                </header>
            
            <article>
                
<p><strong>Standard Performance Evaluation Corporation OpenMP</strong><span> (</span><strong>SPECOMP</strong><span>) is a well known HPC</span> benchmark suite for multi-threaded applications running on multiple nodes. Each benchmark (for example, Swim) is listed along with the <em>x</em> axis.</p>
<p>For each benchmark, there are three pairs of comparisons: one for a 16-vCPU VM, one for a 32-vCPU VM, and one for a 64-way VM. Default-16 means performance for a 16-way VM with no vNUMA support and vNUMA-16 means the same 16-way VM, but with vNUMA enabled. </p>
<p><strong>Ratio to Native</strong>, lower is better:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/edbb2495-532b-495b-bd7c-2e023749442e.png" style="width:27.58em;height:17.17em;"/></p>
<p>The chart shows run-times, so lower is better. We can see significant run-time drops in virtually all cases when moving from default to vNUMA. This is a hugely important feature for HPC users with a need for wide VMs. These charts show published performance results for a variety of life sciences workloads running on ESXi.</p>
<p>They show that these throughput-oriented applications run with generally under a 5% penalty when virtualized. More recent reports from customers indicate that this entire class of applications (throughput), which includes not just life sciences, but financial services, <strong>electronic design automation</strong> (<strong>EDA</strong>) chip designers, and digital content creation (movie rendering, and so on), run with well under a 5% performance degradation. Platform tuning, not application tuning, is required to achieve these results.</p>
<p>We have results reported by one of EDA (chip designer) customers who ran first a single instance of one of their EDA jobs on a bare-metal Linux node. They then ran the same Linux and the same job on ESXi and compared the results. They saw a 6% performance degradation. We believe this would be lower with additional platform tuning.</p>
<p>They then ran the second test with four instances of the application running in a single Linux instance versus four VMs running the same jobs. So, we have the same workload running in both cases. In this configuration, they discovered that the virtual jobs completed 2% sooner than the bare-metal jobs. </p>
<p>HPCC performance ratios (lower is better):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b31757da-ef1a-4feb-bab3-aaea8c483f5f.png" style="width:39.50em;height:22.33em;"/></p>
<p>This speedup usually results from a NUMA effect and an OS scheduling effect. The Linux instance must do resource balancing between the four job instances and it must also handle the NUMA issues related to this being a multi-socket system.</p>
<p>Virtualization will help us with the following advantages:</p>
<ul>
<li>Each Linux instance must handle only one job</li>
<li>Because of the ESXi scheduler's NUMA awareness, each VM would be scheduled onto a socket so none of the Linux instances needs to suffer the potential inefficiencies of dealing with NUMA issues</li>
</ul>
<p>We don't have to worry about multiple Linux instances and VMs consuming more memory: <strong>transparent page sharing</strong> (<strong>TPS</strong>) can mitigate this as the hypervisor will find common pages between VMs and share them where possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A standard Hadoop architecture </h1>
                </header>
            
            <article>
                
<p>Let's understand a standard Hadoop architecture:</p>
<ul>
<li><span><strong>Hadoop File System</strong> (<strong>HDFS</strong>)</span>: A distributed filesystem instantiated across a set of local disks attached to the compute nodes in the Hadoop cluster</li>
<li><strong>Map</strong>: The embarrassingly parallel computation that is applied to every chunk of data read from HDFS (in parallel)</li>
<li><strong>Reduce</strong>: The phase that takes map results and combines them to perform the final computation</li>
</ul>
<p>The final results are typically stored back into HDFS. The benefits of Serengeti <span>(</span>open source project) provide ease of provisioning, multi-tenancy, and flexibility to scale up or out. BDE allows Serengeti to be triggered from a vRealize blueprint, making it easy to self-provision a Hadoop cluster of a given size:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/44c841bb-5f66-47f8-890c-c43d41ed41fb.png" style="width:40.17em;height:11.25em;"/></p>
<p class="mce-root"/>
<p>The preceding diagram shows a virtualized Hadoop environment. Local disks are made available as VMDKs to the guest, Map, and Reduce tasks running in VMs on each Hadoop cluster node.</p>
<p>The next generation of our approach is one in which there are two types of VMs: Compute nodes and Data nodes. The Data node is responsible for managing the physical disks attached to the host and for HDFS. The Compute nodes run the Map and Reduce tasks. Communication between the Compute node and Data node happens through fast VM-VM communication.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standard tests</h1>
                </header>
            
            <article>
                
<p>These tests were run on a 32-node (host) cluster with local disks and 10 Gigabit Ethernet<span> </span>interconnect<strong>. </strong><span>The most important point is that four configurations were run, each solving the same problem:</span></p>
<ul>
<li><strong>Configuration 1</strong>: A 32-host physical cluster</li>
<li><strong>Configuration 2</strong>: A 32-VM virtual cluster</li>
<li><strong>Configuration 3</strong>: A 64-VM virtual cluster (two VMs per host, each using half the hardware)</li>
<li><strong>Configuration 4</strong>: A 128-VM virtual cluster (four VMs per host, with two on each socket, each with one quarter of the hardware resources)</li>
</ul>
<p>We used common benchmarks to evaluate various aspects of Hadoop performance. Here is the ratio of virtual run-time to native/physical/bare-metal run-time:</p>
<ul>
<li><strong>TeraGen</strong>: 6%, 4%, 3%</li>
<li><strong>TeraSort</strong>: 13%, 1%, -1%</li>
<li><strong>TeraValidate</strong>: 5%, 5%, 7%</li>
</ul>
<p>Generally, breaking the problem into smaller pieces and running more VMs results in performance closer to native.</p>
<p>There is a wide range of latency sensitivities found in MPI applications. The application on the <strong>Particle Mesh Ewald Molecular Dynamics</strong> (<strong>PMEMD</strong>) is a molecular dynamics code with more than 40% MPI data transfers involving messages that are a single byte long. Contrast this with <strong>Lattice Boltzmann Magneto-Hydrodynamics</strong> <span>(</span><strong>LBMHD</strong>) code, all of whose messages are greater than 1 MB in size. The first application is critically sensitive to interconnect latencies, while the second is insensitive and is instead bandwidth-bound. </p>
<p>Each application has its own communication patterns about its process flow with each other. The process/processor number is shown on both the <em>x</em> and <em>y</em> axis. The darker the data point at <em>(x, y)</em>, the more data is transferred between the <em>x</em> and <em>y</em> processes. The PMEMD pattern shows that each process communicates across the range of available processes, but significantly more communication happens with nearby/neighbor processes. There are algorithms where the displayed pattern would be a dark expanse across the entire pattern, indicating intense communication between all processes.</p>
<p>It is common in HPC bare-metal environments to bypass the kernel to get optimum bandwidth with lowest latency, as this is important for many MPI applications. Rather than using a standard TCP networking stack (which adds overhead), <strong>remote direct memory access</strong> (<strong>RDMA</strong>) devices, such as IB, allow transfers to be initiated directly to and from the application without using the <span>host CPU to transfer the data:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dbe48a47-e3fc-4819-9a4c-7a82fce445bf.png" style="width:38.25em;height:17.50em;"/></p>
<p>We can opt  for the analog way for our virtual environment. We can make the hardware device, such as IB, directly visible to the guest (rightmost <strong>rdma</strong> box in the preceding diagram) using the VM direct path I/O. It will allow the application to get direct access to the hardware, as in the bare-metal case, by using ESXi. The <strong>rdma</strong> box to the left of the VM direct path I/O case represents ongoing work within VMware to develop a vRDMA device, which is a paravirtualized RDMA device.</p>
<p>This device will continue to allow direct application hardware access for data transfers, while also continuing to support the ability to perform snapshots and to use vMotion. This represents ongoing work that will make the RDMA approach available for use by ESXi itself and by ESXi services such as vMotion, vSAN, and FT. We've shown very large performance gains for vMotion using an RDMA transport.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intel tested a variety of HPC benchmarks</h1>
                </header>
            
            <article>
                
<p><strong>High-performance computing cluster</strong> (<strong>HPCC</strong>): </p>
<p>HPC challenge benchmarks are combination of multiple benchmarks to test various attributes along with their performance of HPC systems. STAR-CD is a <strong><span>computational fluid dynamics</span></strong> (<strong>CFD</strong>) code <span>used for in-cylinder analysis</span>. These are <span class="content"><strong>Message Passing Interface</strong> (<strong>MPI</strong>)</span> codes responsible for multi-process running on multiple nodes. The results are for a small cluster in several configurations: 2-node, 16 processes; 4-node, 32 processes; 8-node, 64 processes (<em>node</em> means <em>host</em> in HPC circles). We show excellent parity with native, even for this old configuration. Intel should turn off EPT or use large page sizes for better MPIRandomAccess. We have not explored naturally-ordered ring bandwidth to understand the issue there.</p>
<p>HPCC represents a set application, such as benchmarks over a range of HPC needs. It is very positive that we do so well, even if this is admittedly at very low scale. Before we look at the STAR-CD results, it is useful to look at the messaging characteristics of the application.</p>
<p>MPI applications perform two kinds of communication between processes:</p>
<ul>
<li><strong>P2P</strong>: Individual processes send or receive data from another process</li>
<li><strong>Collective operations</strong>: Many processes participate together to transfer data in one of a set of patterns</li>
</ul>
<p>This majority of P2P messages exchanged by STAR-CD are in the 1 KB - 8 KB range. We would characterize these as medium-sized messages. As the node count is increased, more of these messages move into the 128 B-1 KB range. This means that interconnect latency becomes more of a factor as the number of nodes increases. For collective operations, there is a very clear latency-sensitivity in Star-CD since virtually all messages fall into the 0 - 128 B range.</p>
<p>The <em>y</em> axes on these two charts are different. Star-CD uses P2P messages much more often than collectives. Thus, while the collective operations are very latency-sensitive, the overall effect of latency overheads may be reduced since the collectives don't represent the majority of messages transferred by Star-CD. The results show a 15% slowdown running an A-class model (a Mercedes car body) with STAR-CD using 8 nodes and 32 processes on 2-socket nodes.</p>
<p>This is a significant slowdown for many HPC users, which is why we continue to work on characterizing and reducing latency overheads in the platform. The next set of our results is using the VM direct path I/O with QDR (40 Gigabits per second) IB and ESXi.</p>
<p>The bare-metal and VM over a wide range of message sizes use two different transfer mechanisms send and RDMA read respectively. We deliver equivalent bandwidth in the virtual case.</p>
<p>The <strong>hybrid run time</strong> (HRT, runtimes which run as kernel) latencies over a wide range of message sizes for bare-metal and virtual environment are different and depend on many factors. These results are generated using RDMA read operations. Using <strong>q<span>uad data rate</span></strong> (<strong>QDR</strong>) (40 gigabits per second) IB and ESXi, virtualization introduces about 0.7 microseconds of additional latency, which is significant at the smallest message sizes and less so for larger messages.</p>
<p>When using send/receive operations on the same hardware, ESXi latency overheads drop to about 0.4 microseconds and this disparity disappears for message sizes larger than 256 bytes. Again, the impact of these overheads on performance will depend entirely on the messaging characteristics of specific applications of interest. GPUs for computation, called <strong>general-purpose GPUs</strong> (<span><strong>GPGPU</strong>),</span> have promising results. Customers may be interested in Intel's offering in this area with their accelerator product, Intel Xeon Phi.</p>
<p>It is essential that the platform be tuned appropriately to support the required latency of these applications. We are being very transparent about what works well and what doesn't on our platform so that customers have an accurate understanding of the values and challenges of virtualizing HPC workloads and environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed the specific features of virtualization that enhance the outcome of an HPC environment. We focused on unique features above and beyond a generic virtualization platform and looked at how virtualization addresses an improved scientific productivity. The impact of virtualization on the runtime of a particular simulation or calculation may have different results, but the overall performance of a compute deployment has optimum throughput.</p>
<p>This book will help you to align the operational goals of your customers with their business objectives. Readers will learn how IT organizations can achieve both operational and business goals by providing a secure and flexible digital foundation to their businesses by addressing the key business issues. They will learn how VMware products help customers to deliver consistent and stable IT performance to the business. VMware recommend leveraging hyper-converged, virtualized resources in a solution that works out of the box to speed the deployment of a fully-virtualized infrastructure.</p>
<p>Readers should now know how a fully-virtualized infrastructure allows them to accelerate deployments and unify and streamline operations, monitoring, and IT management, while also improving the ability to scale. A software-defined infrastructure enables us to unify and ease operations, monitoring, and IT management, while improving scalability. All the solutions mentioned in this book have taken a software-defined approach to building a private cloud, <span>one that</span> extends virtualization across the entire digital infrastructure (compute, storage, and networking) through common hardware, which is managed with common existing tools and skill sets.</p>


            </article>

            
        </section>
    </body></html>