<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Speech Recognition</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Reading and plotting audio data</li>
<li>Transforming audio signals into the frequency domain</li>
<li>Generating audio signals with custom parameters</li>
<li>Synthesizing music</li>
<li>Extracting frequency domain features</li>
<li>Building <strong>hidden Markov models</strong> (<strong>HMMs)</strong></li>
<li>Building a speech recognizer</li>
<li>Building a <strong>text-to-speech</strong> (<strong>TTS</strong>) system</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="mce-root">To address the recipes in this chapter, you will need the following files (which are available on GitHub):</p>
<ul>
<li><kbd><span>read_plot.py</span></kbd></li>
<li><kbd>input_read.wav</kbd></li>
<li><kbd><span>freq_transform.py</span></kbd></li>
<li><kbd>input_freq<span>.wav</span></kbd></li>
<li><kbd>generate.py</kbd></li>
<li><kbd><span>synthesize_music.py</span></kbd></li>
<li><kbd><span>extract_freq_features.py</span></kbd></li>
<li><kbd>input_freq.wav</kbd></li>
<li><kbd><span>speech_recognizer.py</span></kbd></li>
<li><kbd>tts.py</kbd></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing speech recognition</h1>
                </header>
            
            <article>
                
<p><strong>Speech recognition</strong> refers to the process of recognizing and understanding spoken language. The input comes in the form of audio data, and the speech recognizers will process this data to extract meaningful information from it. This has a lot of practical uses, such as voice-controlled devices, the transcription of spoken language into words and security systems.</p>
<p>Speech signals are very versatile in nature. There are many variations of speech in the same language. There are different elements to speech, such as language, emotion, tone, noise, and accent. It's difficult to rigidly define a set of rules of what can constitute speech. Even with all these variations, humans are very good at understanding all of this with relative ease. Hence, we need machines to understand speech in the same way.</p>
<p>Over the last couple of decades, researchers have worked on various aspects of speech, such as identifying the speaker, understanding words, recognizing accents, and translating speech. Among all these tasks, automatic speech recognition has been the focal point for many researchers. In this chapter, we will learn how to build a <strong>speech recognizer</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading and plotting audio data</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to read an audio file and visualize the signal. This will be a good starting point, and it will give us a good understanding of the basic structure of audio signals. Before we start, we need to understand that audio files are digitized versions of actual audio signals. Actual audio signals are complex, continuous-valued waves. In order to save a digital version, we sample the signal and convert it into numbers. For example, speech is commonly sampled at 44,100 Hz. This means that each second of the signal is broken down into 44,100 parts, and the values at these timestamps are stored. In other words, you store a value every 1/44,100 seconds. As the sampling rate is high, we feel that the signal is continuous when we listen to it on our media players.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use the <kbd>wavfile</kbd> package to read the audio file from a <kbd>.wav</kbd> input file. So, we will draw the signal with a diagram.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="mce-root">We will use the following steps to read and plot audio using the <kbd>wavfile</kbd> package:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd>read_plot.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
from scipy.io import wavfile </pre>
<ol start="2">
<li>We will use the <kbd>wavfile</kbd> package to read the audio file from the <kbd>input_read.wav</kbd> input file that's already provided for you:</li>
</ol>
<pre style="padding-left: 60px"># Read the input file 
sampling_freq, audio = wavfile.read('input_read.wav') </pre>
<ol start="3">
<li>Let's print out the parameters of this signal:</li>
</ol>
<pre style="padding-left: 60px"># Print the params 
print('Shape:', audio.shape)<br/>print('Datatype:', audio.dtype)<br/>print('Duration:', round(audio.shape[0] / float(sampling_freq), 3), 'seconds')</pre>
<ol start="4">
<li>The audio signal is stored as 16-bit signed integer data; we need to normalize these values:</li>
</ol>
<pre style="padding-left: 60px"># Normalize the values 
audio = audio / (2.**15) </pre>
<ol start="5">
<li>Now, let's extract the first 30 values to plot, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Extract first 30 values for plotting 
audio = audio[:30] </pre>
<ol start="6">
<li>The <em>x</em> axis is the <strong>time axis</strong>. Let's build this axis, considering the fact that it should be scaled using the sampling frequency factor:</li>
</ol>
<pre style="padding-left: 60px"># Build the time axis 
x_values = np.arange(0, len(audio), 1) / float(sampling_freq) </pre>
<ol start="7">
<li>Convert the units to seconds, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Convert to seconds 
x_values *= 1000</pre>
<ol start="8">
<li>Let's now plot this as follows:</li>
</ol>
<pre style="padding-left: 60px"># Plotting the chopped audio signal 
plt.plot(x_values, audio, color='black') 
plt.xlabel('Time (ms)') 
plt.ylabel('Amplitude') 
plt.title('Audio signal') 
plt.show() </pre>
<ol start="9">
<li>The full code is in the <kbd>read_plot.py</kbd> file. If you run this code, you will see the following signal:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1031 image-border" src="assets/45ca3d88-69b9-4028-b6b4-6a06819e1bb9.png" style="width:33.33em;height:25.33em;"/></p>
<p style="padding-left: 60px">You will also see the following output printed on your Terminal:</p>
<pre style="padding-left: 60px"><strong>Shape: (132300,)</strong><br/><strong>Datatype: int16</strong><br/><strong>Duration: 3.0 seconds</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Wave audio files are uncompressed files. The format was introduced with Windows 3.1 as a standard format for the sound used in multimedia applications. Its technical specifications and description can be found in the <em>Multimedia Programming Interface and Data Specifications 1.0</em> document (<a href="https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf">https://www.aelius.com/njh/wavemetatools/doc/riffmci.pdf</a>). It is based on the <strong>Resource Interchange File Format</strong> (<strong>RIFF</strong>) specifications that were introduced in 1991, constituting a metaformat for multimedia files running in the Windows environment. The RIFF structure organizes blocks of data in sections called chunks, each of which describes a characteristic of the WAV file (such as the sample rate, the bit rate, and the number of audio channels), or contains the values of the samples (in this case, we are referring to chunk data). The chunks are 32 bit (with some exceptions).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To read the WAV file, the <kbd>scipy.io.wavfile.read()</kbd> function was used. This function returns data<span> from a WAV file along with </span>the sample rate. The returned sample rate is a Python integer, and the data is returned as a NumPy array with a datatype that corresponds to the file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to the official documentation of the <kbd>scipy.io.wavfile.read()</kbd> function: <a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html">https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html</a></span></li>
<li><span>Refer to </span><em>WAV</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/WAV">https://en.wikipedia.org/wiki/WAV</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transforming audio signals into the frequency domain</h1>
                </header>
            
            <article>
                
<p>Audio signals consist of a complex mixture of sine waves of different frequencies, amplitudes, and phases. Sine waves are also referred to as <strong>sinusoids</strong>. There is a lot of information that is hidden in the frequency content of an audio signal. In fact, an audio signal is heavily characterized by its frequency content. The whole world of speech and music is based on this fact. Before you proceed further, you will need some knowledge of <strong>Fourier transforms</strong>. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will see how to <span>transform an audio signal into the frequency domain. To do this, the <kbd>numpy.fft.fft()</kbd> function is used. This function computes the one-dimensional <em>n</em>-point <strong>discrete Fourier transform</strong> (<strong>DFT</strong>) with the efficient <strong>fast Fourier transform</strong> (<strong>FFT</strong>) algorithm.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to transform audio signals into the frequency domain:</p>
<ol>
<li>Create a new Python file and import the following package (the full code is in the <kbd>freq_transform.py</kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
from scipy.io import wavfile 
import matplotlib.pyplot as plt </pre>
<ol start="2">
<li>Read the <kbd>input_freq.wav</kbd> file that is already provided for you:</li>
</ol>
<pre style="padding-left: 60px"># Read the input file 
sampling_freq, audio = wavfile.read('input_freq.wav') </pre>
<ol start="3">
<li>Normalize the signal, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Normalize the values 
audio = audio / (2.**15) </pre>
<ol start="4">
<li>The audio signal is just a NumPy array. So, you can extract the length using the following code:</li>
</ol>
<pre style="padding-left: 60px"># Extract length 
len_audio = len(audio)</pre>
<ol start="5">
<li>Let's apply the Fourier transform. The Fourier transform signal is mirrored along the center, so we just need to take the first half of the transformed signal. Our end goal is to extract the power signal, so we square the values in the signal in preparation for this:</li>
</ol>
<pre style="padding-left: 60px"># Apply Fourier transform 
transformed_signal = np.fft.fft(audio) 
half_length = np.ceil((len_audio + 1) / 2.0) 
transformed_signal = abs(transformed_signal[0:int(half_length)]) 
transformed_signal /= float(len_audio) 
transformed_signal **= 2</pre>
<ol start="6">
<li>Extract the length of the signal, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Extract length of transformed signal 
len_ts = len(transformed_signal) </pre>
<ol start="7">
<li>We need to double the signal according to the length of the signal:</li>
</ol>
<pre style="padding-left: 60px"># Take care of even/odd cases 
if len_audio % 2: 
    transformed_signal[1:len_ts] *= 2 
else: 
    transformed_signal[1:len_ts-1] *= 2 </pre>
<ol start="8">
<li>The power signal is extracted using the following formula:</li>
</ol>
<pre style="padding-left: 60px"># Extract power in dB 
power = 10 * np.log10(transformed_signal) </pre>
<ol start="9">
<li>The <em>x</em> axis is the time axis; we need to scale this according to the sampling frequency and then convert this into seconds:</li>
</ol>
<pre style="padding-left: 60px"># Build the time axis 
x_values = np.arange(0, half_length, 1) * (sampling_freq / len_audio) / 1000.0 </pre>
<ol start="10">
<li>Plot the signal, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Plot the figure 
plt.figure() 
plt.plot(x_values, power, color='black') 
plt.xlabel('Freq (in kHz)') 
plt.ylabel('Power (in dB)') 
plt.show()</pre>
<ol start="11">
<li>If you run this code, you will see the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1032 image-border" src="assets/70c4c4b5-2abc-46bc-b29b-9d57fba262ad.png" style="width:30.50em;height:22.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The sound spectrum is a graphical representation of the sound level, normally in <strong>decibels</strong> (<span><strong>dB</strong>)</span>, depending on the frequency in Hz. If the sound to be analyzed is a so-called pure sound (signal at a single frequency constant over time), for example, a perfect sine wave, the signal spectrum will have a single component at the sine wave frequency, with a certain level in dB. In reality, any real signal consists of a large number of sinusoidal components of amplitude that are continuously variable over time. For these signals, it is impossible to analyze pure tones because there are always fractions of the signal energy that are difficult to represent with sinusoids. In fact, the representation of a signal as the sum of sinusoidal harmonic components, according to the Fourier transform theorem, is only valid for stationary signals, which often do not correspond to real sounds.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The frequency analysis of the sounds is based on the Fourier transform theorem. That is, any periodic signal can be generated by summing together so many sinusoidal signals (called harmonics) having multiple whole frequencies of the frequency of the periodic signal (called fundamental frequency).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to the official documentation of the <kbd>numpy.fft.fft()</kbd> function: <a href="https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.fft.fft.html">https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.fft.fft.html</a><br/></span></li>
<li class="mce-root"><span>Refer to </span><em>The Fourier Transform</em>: <a href="http://www.thefouriertransform.com/">http://www.thefouriertransform.com/</a></li>
<li class="mce-root"><span>Refer to <em>Time-frequency representations</em> (from Aalto University): <a href="https://mycourses.aalto.fi/pluginfile.php/145214/mod_resource/content/3/slides_05_time-frequency_representations.pdf">https://mycourses.aalto.fi/pluginfile.php/145214/mod_resource/content/3/slides_05_time-frequency_representations.pdf</a></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating audio signals with custom parameters</h1>
                </header>
            
            <article>
                
<p>Sound is a particular type of wave in which a variation of pressure that is induced by a vibrating body (that is, a sound source) propagates in the surrounding medium (usually air). Some examples of sound sources include the following:</p>
<ul>
<li class="mce-root">Musical instruments in which the vibrating part can be a struck string (such as a guitar), or rubbed with a bow (such as the violin).</li>
<li class="mce-root">Our vocal cords that are made to vibrate from the air that comes out of the lungs and give rise to the voice.</li>
<li class="mce-root">Any phenomenon that causes a movement of air (such as the beating wings of a bird, an airplane that breaks down the supersonic barrier, a bomb that explodes, or a hammer beating on an anvil) having appropriate physical characteristics.</li>
</ul>
<p>To reproduce sound through electronic equipment, it is necessary to transform it into an analogue sound that is an electric current that originates from the transformation by conversion of the mechanical energy of the sound wave into electrical energy. In order to be able to use the sound signals with the computer, it is necessary to transfigure the analogue in a digital signal originating from the transformation of the analog sound into an audio signal represented by a flow of 0 and 1 (bit).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will use <span>NumPy to generate audio signals. As we discussed earlier, audio signals are complex mixtures of sinusoids. So, we will bear this in mind when we generate our own audio signal.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to generate audio signals with custom parameters:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the<span> </span><kbd>generate.py</kbd><span> </span>file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
from scipy.io.wavfile import write </pre>
<ol start="2">
<li>We need to define the output file where the generated audio will be stored:</li>
</ol>
<pre style="padding-left: 60px"># File where the output will be saved 
output_file = 'output_generated.wav' </pre>
<ol start="3">
<li>Let's now specify the audio generation parameters. We want to generate a 3-second long signal with a sampling frequency of 44,100, and a tonal frequency of 587 Hz. The values on the time axis will go from <em>-2*pi</em> to <em>2*pi</em>:</li>
</ol>
<pre style="padding-left: 60px"># Specify audio parameters 
duration = 3  # seconds 
sampling_freq = 44100  # Hz 
tone_freq = 587 
min_val = -2 * np.pi 
max_val = 2 * np.pi </pre>
<ol start="4">
<li>Let's generate the time axis and the audio signal. The audio signal is a simple sinusoid with the previously mentioned parameters:</li>
</ol>
<pre style="padding-left: 60px"># Generate audio 
t = np.linspace(min_val, max_val, duration * sampling_freq) 
audio = np.sin(2 * np.pi * tone_freq * t)</pre>
<ol start="5">
<li>Now, let's add some noise to the signal:</li>
</ol>
<pre style="padding-left: 60px"># Add some noise 
noise = 0.4 * np.random.rand(duration * sampling_freq) 
audio += noise</pre>
<ol start="6">
<li>We need to scale the values to 16-bit integers before we store them:</li>
</ol>
<pre style="padding-left: 60px"># Scale it to 16-bit integer values 
scaling_factor = pow(2,15) - 1 
audio_normalized = audio / np.max(np.abs(audio)) 
audio_scaled = np.int16(audio_normalized * scaling_factor)</pre>
<ol start="7">
<li>Write this signal to the output file:</li>
</ol>
<pre style="padding-left: 60px"># Write to output file 
write(output_file, sampling_freq, audio_scaled) </pre>
<ol start="8">
<li>Plot the signal using the first 100 values:</li>
</ol>
<pre style="padding-left: 60px"># Extract first 100 values for plotting 
audio = audio[:100] </pre>
<ol start="9">
<li>Generate the time axis, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Build the time axis 
x_values = np.arange(0, len(audio), 1) / float(sampling_freq) </pre>
<ol start="10">
<li>Convert the time axis into seconds:</li>
</ol>
<pre style="padding-left: 60px"># Convert to seconds 
x_values *= 1000 </pre>
<ol start="11">
<li>Plot the signal, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Plotting the chopped audio signal 
plt.plot(x_values, audio, color='black') 
plt.xlabel('Time (ms)') 
plt.ylabel('Amplitude') 
plt.title('Audio signal') 
plt.show()</pre>
<p class="mce-root"/>
<ol start="12">
<li>If you run this code, you will get the following output:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1033 image-border" src="assets/6ada8825-3c7c-4b24-b6eb-a6a4227b51e0.png" style="width:31.00em;height:23.58em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In this recipe, we used the NumPy library to generate audio signals. We have seen that a digital sound is a sequence of numbers, so generating a sound will be enough to build an array that represents a musical tone. First, we set the filename to where the output will be saved. Then, we specified the audio parameters. Thus, we generated audio using a sine wave. We then added some noise, so we resized to 16-bit integer values. In the end, we wrote the signal on the output file.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>In the coding of a signal, each value assigned to the single sample is represented in bits. Each bit corresponds to a dynamic range of 6 dB. The higher the number of bits used, the higher the range of dB that can be represented by the single sample.</p>
<p>Some of the typical values are as follows:</p>
<ul>
<li>8 bits per sample that correspond to 256 levels.</li>
<li>16 bits per sample (the number used for CDs) that correspond to 65,636 levels.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to the official documentation of the NumPy library: <a href="http://www.numpy.org/">http://www.numpy.org/</a></span></li>
<li><span>Refer to </span><em>Sine wave</em> (from Wikipedia): <a href="https://en.wikipedia.org/wiki/Sine_wave">https://en.wikipedia.org/wiki/Sine_wave</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Synthesizing music</h1>
                </header>
            
            <article>
                
<p>In traditional musical instruments, sound is produced by the vibration of mechanical parts. In synthetic instruments, vibration is described by functions over time, called signals, which express the variation in the time of the acoustic pressure. Sound synthesis is a process that allows you to generate the sound artificially. The parameters by which the timbre of the sound is determined differ according to the type of synthesis that is used for the generation, and can be provided directly by the composer, or with actions on appropriate input devices, or derived from the analysis of pre-existing sounds.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will see how to<span> synthesize some music. To do this, we will use various notes, such as </span><em>A</em><span>, </span><em>G</em><span>, and </span><em>D</em><span>, along with their corresponding frequencies, to generate some simple music.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's take a look at how to synthesize some music:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd><span>synthesize_music.py</span></kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import json 
import numpy as np 
from scipy.io.wavfile import write </pre>
<ol start="2">
<li>Define a function to synthesize a tone, based on input parameters:</li>
</ol>
<pre style="padding-left: 60px"># Synthesize tone 
def synthesizer(freq, duration, amp=1.0, sampling_freq=44100):</pre>
<ol start="3">
<li>Build the time axis values:</li>
</ol>
<pre>    # Build the time axis 
    <span>t = np.linspace(0, duration, round(duration * sampling_freq))</span> </pre>
<ol start="4">
<li>Construct the audio sample using the input arguments, such as amplitude and frequency:</li>
</ol>
<pre>    # Construct the audio signal 
    audio = amp * np.sin(2 * np.pi * freq * t) 
 
    return audio.astype(np.int16)  </pre>
<ol start="5">
<li>Let's define the main function. You've been provided with a JSON file, called <kbd>tone_freq_map.json</kbd>, which contains some notes along with their frequencies:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    tone_map_file = 'tone_freq_map.json' </pre>
<ol start="6">
<li>Load that file, as follows:</li>
</ol>
<pre>    # Read the frequency map 
    with open(tone_map_file, 'r') as f: 
        tone_freq_map = json.loads(f.read()) </pre>
<ol start="7">
<li>Now, let's assume that we want to generate a <kbd>G</kbd> note for a duration of two seconds:</li>
</ol>
<pre>    # Set input parameters to generate 'G' tone 
    input_tone = 'G' 
    duration = 2     # seconds 
    amplitude = 10000 
    sampling_freq = 44100    # Hz </pre>
<ol start="8">
<li>Call the function with the following parameters:</li>
</ol>
<pre>    # Generate the tone 
    synthesized_tone = synthesizer(tone_freq_map[input_tone], duration, amplitude, sampling_freq) </pre>
<ol start="9">
<li>Write the generated signal into the output file, as follows:</li>
</ol>
<pre>    # Write to the output file 
    write('output_tone.wav', sampling_freq, synthesized_tone)</pre>
<div style="padding-left: 60px" class="packt_infobox">A single tone <kbd>.wav</kbd> file is generated (<kbd>output_tone.wav</kbd>). <span>Open this file in a media player and listen to it. That's the <em>G</em> note! </span></div>
<ol start="10">
<li>Now, let's do something more interesting. Let's generate some notes in sequence to give it a musical feel. Define a note sequence along with their durations in seconds:</li>
</ol>
<pre>    # Tone-duration sequence 
    tone_seq = [('D', 0.3), ('G', 0.6), ('C', 0.5), ('A', 0.3), ('Asharp', 0.7)] </pre>
<ol start="11">
<li>Iterate through this list and call the synthesizer function for each of them:</li>
</ol>
<pre>    # Construct the audio signal based on the chord sequence 
    output = np.array([]) 
    for item in tone_seq: 
        input_tone = item[0] 
        duration = item[1] 
        synthesized_tone = synthesizer(tone_freq_map[input_tone], duration, amplitude, sampling_freq) 
        output = np.append(output, synthesized_tone, axis=0) <br/>    output = output.astype(np.int16)</pre>
<ol start="12">
<li>Write the signal to the output file:</li>
</ol>
<pre>    # Write to the output file 
    write('output_tone_seq.wav', sampling_freq, output) </pre>
<ol start="13">
<li>You can now open the <kbd>output_tone_seq.wav</kbd> file in your media player and listen to it. You can feel the music!</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">Music is a work of ingenuity and creativity that is difficult to explain in a nutshell. Musicians read a piece of music recognizing the notes as they are placed on the stave. By analogy, we can regard the synthesis of sound as a sequence of the characteristic frequencies of the known ones. In this recipe, we have used this procedure to synthesize a short sequence of notes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">To generate music<span> artificially,</span> the synthesizer is used. All synthesizers have the following basic components that work together to create a sound:</p>
<ul>
<li class="mce-root">An oscillator that generates the waveform and changes the tone</li>
<li class="mce-root">A filter that cuts out some frequencies in the wave to change the timbre</li>
<li class="mce-root">An amplifier that controls the volume of the signal</li>
<li class="mce-root">A modulator to create effects</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to the official documentation of the NumPy library: <a href="http://www.numpy.org/">http://www.numpy.org/</a></span></li>
<li class="mce-root"><span>Refer to the official documentation of the<span> </span></span><kbd>scipy.io.wavfile.write()</kbd> fun<span>ction: <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.write.html">https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.io.wavfile.read.html</a></span></li>
<li><span>Refer to </span><em>Frequencies of Musical Notes</em> (from <span>Michigan Technological University</span>): <a href="http://pages.mtu.edu/~suits/notefreqs.html">http://pages.mtu.edu/~suits/notefreqs.html</a></li>
<li><span>Refer to </span><em>Principles of Sound Synthesis</em> (from the University of Salford):<span> </span><a href="http://www.acoustics.salford.ac.uk/acoustics_info/sound_synthesis/">http://www.acoustics.salford.ac.uk/acoustics_info/sound_synthesis/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting frequency domain features</h1>
                </header>
            
            <article>
                
<p>In the <em>Transforming audio signals into the frequency domain</em> recipe, we discussed how to convert a signal into the frequency domain. In most modern speech recognition systems, people use frequency domain features. After you convert a signal into the frequency domain, you need to convert it into a usable form. <strong>Mel Frequency Cepstral Coefficients</strong> (<strong>MFCC</strong>) is a good way to do this. MFCC takes the power spectrum of a signal and then uses a combination of filter banks and <strong>discrete cosine transform</strong> (<strong>DCT</strong>) to extract the features.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will see how to <span>use the </span><kbd>python_speech_features</kbd><span> package to extract frequency domain features. You can find the installation instructions at </span><a href="http://python-speech-features.readthedocs.org/en/latest"><span class="URLPACKT">http://python-speech-features.readthedocs.org/en/latest</span></a><span>. So, let's take a look at how to extract MFCC features.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to extract frequency domain features:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd><span>extract_freq_features.py</span></kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import numpy as np 
import matplotlib.pyplot as plt 
from scipy.io import wavfile  
from python_speech_features import mfcc, logfbank </pre>
<ol start="2">
<li>Read the <kbd>input_freq.wav</kbd> input file that is already provided for you:</li>
</ol>
<pre style="padding-left: 60px"># Read input sound file 
sampling_freq, audio = wavfile.read("input_freq.wav") </pre>
<ol start="3">
<li>Extract the MFCC and filter bank features, as follows:</li>
</ol>
<pre style="padding-left: 60px"># Extract MFCC and Filter bank features 
mfcc_features = mfcc(audio, sampling_freq) 
filterbank_features = logfbank(audio, sampling_freq) </pre>
<ol start="4">
<li>Print the parameters to see how many windows were generated:</li>
</ol>
<pre style="padding-left: 60px"># Print parameters 
print('MFCC:\nNumber of windows =', mfcc_features.shape[0])<br/>print('Length of each feature =', mfcc_features.shape[1])<br/>print('\nFilter bank:\nNumber of windows =', filterbank_features.shape[0])<br/>print('Length of each feature =', filterbank_features.shape[1])</pre>
<ol start="5">
<li>Let's now visualize the MFCC features. We need to transform the matrix so that the time domain is horizontal:</li>
</ol>
<pre style="padding-left: 60px"># Plot the features 
mfcc_features = mfcc_features.T 
plt.matshow(mfcc_features) 
plt.title('MFCC') </pre>
<ol start="6">
<li>Now, let's visualize the filter bank features. Again, we need to transform the matrix so that the time domain is horizontal:</li>
</ol>
<pre style="padding-left: 60px">filterbank_features = filterbank_features.T 
plt.matshow(filterbank_features) 
plt.title('Filter bank') 
plt.show() </pre>
<ol start="7">
<li>If you run this code, you will get the following output for MFCC features:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1034 image-border" src="assets/9f6de9bc-6a8b-4f22-b167-265da6b87b8f.png" style="width:105.58em;height:38.33em;"/></p>
<p style="padding-left: 60px">The filter bank features will look like the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1035 image-border" src="assets/c447803e-13e9-42d9-85f3-00e03bbe3f3e.png" style="width:37.25em;height:26.08em;"/></p>
<p style="padding-left: 60px">You will get the following output on your Terminal:</p>
<pre style="padding-left: 60px" class="CDPAlignLeft CDPAlign"><strong>MFCC:</strong><br/><strong>Number of windows = 40</strong><br/><strong>Length of each feature = 13</strong><br/><br/><strong>Filter bank:</strong><br/><strong>Number of windows = 40</strong><br/><strong>Length of each feature = 26</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The <strong>cepstrum</strong> is the result of the Fourier transform applied to the dB spectrum of a signal. Its name is derived from the reversal of the first four letters of the word <strong>spectrum</strong>. It was defined in 1963 by Bogert et al. Thus, the cepstrum of a signal is the Fourier transform of the log value of the Fourier transform of the signal. </p>
<p class="mce-root">The graph of the cepstrum is used to analyze the rates of change of the spectral content of a signal. Originally, it was invented to analyze earthquakes, explosions, and the responses to radar signals. It is currently a very effective tool for discriminating the human voice in music informatics. For these applications, the spectrum is first transformed through the frequency bands of the Mel scale. The result is the spectral coefficient Mel, or MFCCs. It is used for voice identification and pitch detection algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">The cepstrum is used to separate the part of the signal that contains the excitation information from the transfer function performed by the larynx. The lifter action (filtering in the frequency domain) has as its objective the separation of the excitation signal from the transfer function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to the official documentation o</span>f the <kbd>python_speech_features</kbd> pack<span>age: <a href="https://python-speech-features.readthedocs.io/en/latest/">https://python-speech-features.readthedocs.io/en/latest/</a></span></li>
<li><span>Refer to the </span>MFCC tutorial: <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building HMMs</h1>
                </header>
            
            <article>
                
<p>We are now ready to discuss speech recognition. We will use HMMs to perform speech recognition; HMMs are great at modeling time series data. As an audio signal is a time series signal, HMMs perfectly suit our needs. An HMM is a model that represents probability distributions over sequences of observations. We assume that the outputs are generated by hidden states. So, our goal is to find these hidden states so that we can model the signal. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will see how to build an HMM using the <kbd>hmmlearn</kbd> package. <span>Before you proceed, you will need to install the </span><kbd>hmmlearn</kbd><span> package</span><span>.</span> <span>Let's take a look at how to build HMMs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build HMMs:</p>
<ol>
<li>Create a new Python file <span>and define a class to model HMMs</span> (the full code is in the <kbd><span>speech_recognizer.py</span></kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px"># Class to handle all HMM related processing 
class HMMTrainer(object): </pre>
<ol start="2">
<li>Let's initialize the class; we will use Gaussian HMMs to model our data. The <kbd>n_components</kbd> parameter defines the number of hidden states. <kbd>cov_type</kbd> defines the type of covariance in our transition matrix, and <kbd>n_iter</kbd> indicates the number of iterations it will go through before it stops training:</li>
</ol>
<pre>    def __init__(self, model_name='GaussianHMM', n_components=4, cov_type='diag', n_iter=1000): </pre>
<p style="padding-left: 60px">The choice of the preceding parameters depends on the problem at hand. You need to have an understanding of your data in order to select these parameters in a smart way.</p>
<ol start="3">
<li>Initialize the variables, as follows:</li>
</ol>
<pre>        self.model_name = model_name 
        self.n_components = n_components 
        self.cov_type = cov_type 
        self.n_iter = n_iter 
        self.models = [] </pre>
<ol start="4">
<li>Define the model with the following parameters:</li>
</ol>
<pre>        if self.model_name == 'GaussianHMM': 
            self.model = hmm.GaussianHMM(n_components=self.n_components,  
                    covariance_type=self.cov_type, n_iter=self.n_iter) 
        else: 
            raise TypeError('Invalid model type') </pre>
<ol start="5">
<li>The input data is a NumPy array, where each element is a feature vector consisting of <em>k </em>dimensions:</li>
</ol>
<pre>    # X is a 2D numpy array where each row is 13D 
    def train(self, X): 
        np.seterr(all='ignore') 
        self.models.append(self.model.fit(X))</pre>
<ol start="6">
<li>Define a method to extract the score, based on the model:</li>
</ol>
<pre>    # Run the model on input data 
    def get_score(self, input_data): 
        return self.model.score(input_data) </pre>
<ol start="7">
<li>We built a class to handle HMM training and prediction, but we need some data to see it in action. We will use it in the next recipe to build a speech recognizer. </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">HMM is a model where the system is assumed to be a Markov process with unobserved states. A stochastic process is called Markovian when, having chosen a certain instance of <em>t</em> for observation, the evolution of the process, starting with <em>t</em>, depends only on <em>t</em>, and does not depend in any way on the previous instances. Thus, a process is Markovian when, given the moment of observation, only a particular instance determines the future evolution of the process, and that evolution does not depend on the past.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">An HMM is, therefore, a Markov chain in which states are not directly observable. More precisely, it can be <span><span>understood </span></span>as follows:</p>
<ul>
<li class="mce-root">The chain has a number of states</li>
<li class="mce-root">The states evolve according to a Markov chain</li>
<li class="mce-root">Each state generates an event with a certain probability distribution that depends only on the state</li>
<li class="mce-root">The event is observable, but the state is not</li>
</ul>
<p>HMMs are particularly known for their applications in the recognition of the temporal pattern of spoken speeches, handwriting, texture recognition, and bioinformatics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to the official documentation of the <kbd>hmmlearn</kbd> package: <a href="https://hmmlearn.readthedocs.io/en/latest/">https://hmmlearn.readthedocs.io/en/latest/</a></span></li>
<li><span>Refer to </span><em>A Tutorial on Hidden Markov Models (</em>by Lawrence R. Rabiner): <a href="https://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf">https://www.robots.ox.ac.uk/~vgg/rg/slides/hmm.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a speech recognizer</h1>
                </header>
            
            <article>
                
<p>Speech recognition is the process by which human oral language is recognized, and subsequently processed through a computer, or, more specifically, through a special speech recognition system. Speech recognition systems are used for automated voice applications in the context of telephone applications (such as automatic call centers) for dictation systems, which allow the dictation of speeches to the computer, for control systems of the navigation system satellite, or for a phone in a car via voice commands.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>We need a database of speech files to build our speech recognizer. We will use the database available at </span><a href="https://code.google.com/archive/p/hmm-speech-recognition/downloads"><span class="URLPACKT">https://code.google.com/archive/p/hmm-speech-recognition/downloads</span></a><span>. This contains 7 different words, where each word has 15 audio files associated with it. Download the ZIP file and extract the folder that contains the Python file (rename the folder that contains the data as <kbd>data</kbd>). This is a small dataset, but it is sufficient in understanding how to build a speech recognizer that can recognize 7 different words. We need to build an HMM model for each class. When we want to identify the word in a new input file, we need to run all the models on this file and pick the one with the best score. We will use the HMM class that we built in the previous recipe.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a speech recognizer:</p>
<ol>
<li>Create a new Python file and import the following packages (the full code is in the <kbd><span>speech_recognizer.py</span></kbd> file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import os 
import argparse  
 
import numpy as np 
from scipy.io import wavfile  
from hmmlearn import hmm 
from python_speech_features import mfcc </pre>
<ol start="2">
<li>Define a function to parse the input arguments in the command line:</li>
</ol>
<pre style="padding-left: 60px"># Function to parse input arguments 
def build_arg_parser():<br/>    parser = argparse.ArgumentParser(description='Trains the HMM classifier')<br/>    parser.add_argument("--input-folder", dest="input_folder", required=True,<br/>            help="Input folder containing the audio files in subfolders")<br/>    return parser</pre>
<ol start="3">
<li>Let's use the <kbd>HMMTrainer</kbd> class defined in the previous <em>Building HMMs</em> recipe:</li>
</ol>
<pre style="padding-left: 60px">class HMMTrainer(object):<br/> def __init__(self, model_name='GaussianHMM', n_components=4, cov_type='diag', n_iter=1000):<br/> self.model_name = model_name<br/> self.n_components = n_components<br/> self.cov_type = cov_type<br/> self.n_iter = n_iter<br/> self.models = []<br/><br/>if self.model_name == 'GaussianHMM':<br/> self.model = hmm.GaussianHMM(n_components=self.n_components, <br/> covariance_type=self.cov_type, n_iter=self.n_iter)<br/> else:<br/> raise TypeError('Invalid model type')<br/><br/># X is a 2D numpy array where each row is 13D<br/> def train(self, X):<br/> np.seterr(all='ignore')<br/> self.models.append(self.model.fit(X))<br/><br/># Run the model on input data<br/> def get_score(self, input_data):<br/> return self.model.score(input_data)</pre>
<ol start="4">
<li>Define the main function, and parse the input arguments:</li>
</ol>
<pre style="padding-left: 60px">if __name__=='__main__': 
    args = build_arg_parser().parse_args() 
    input_folder = args.input_folder </pre>
<ol start="5">
<li>Initiate the variable that will hold all the HMM models:</li>
</ol>
<pre>    hmm_models = [] </pre>
<ol start="6">
<li>Parse the input directory that contains all the database's audio files:</li>
</ol>
<pre>    # Parse the input directory 
    for dirname in os.listdir(input_folder):</pre>
<p class="mce-root"/>
<ol start="7">
<li>Extract the name of the subfolder:</li>
</ol>
<pre>        # Get the name of the subfolder  
        subfolder = os.path.join(input_folder, dirname) 
 
        if not os.path.isdir(subfolder):  
            continue </pre>
<ol start="8">
<li>The name of the subfolder is the label of this class; extract it using the following code:</li>
</ol>
<pre>        # Extract the label 
        label = subfolder[subfolder.rfind('/') + 1:] </pre>
<ol start="9">
<li>Initialize the variables for training:</li>
</ol>
<pre>        # Initialize variables 
        X = np.array([]) 
        y_words = [] </pre>
<ol start="10">
<li>Iterate through the list of audio files in each subfolder:</li>
</ol>
<pre>        # Iterate through the audio files (leaving 1 file for testing in each class) 
        for filename in [x for x in os.listdir(subfolder) if x.endswith('.wav')][:-1]: </pre>
<ol start="11">
<li>Read each audio file, as follows:</li>
</ol>
<pre>            # Read the input file 
            filepath = os.path.join(subfolder, filename) 
            sampling_freq, audio = wavfile.read(filepath) </pre>
<ol start="12">
<li>Extract the MFCC features, as follows:</li>
</ol>
<pre>            # Extract MFCC features 
            mfcc_features = mfcc(audio, sampling_freq) </pre>
<ol start="13">
<li>Keep appending this to the <kbd>X</kbd> variable, as follows:</li>
</ol>
<pre>            # Append to the variable X 
            if len(X) == 0: 
                X = mfcc_features 
            else: 
                X = np.append(X, mfcc_features, axis=0)</pre>
<ol start="14">
<li>Append the corresponding label too, as follows:</li>
</ol>
<pre>            # Append the label 
            y_words.append(label)    </pre>
<ol start="15">
<li>Once you have extracted features from all the files in the current class, train and save the HMM model. As HMM is a generative model for unsupervised learning, we don't need labels to build HMM models for each class. We explicitly assume that separate HMM models will be built for each class:</li>
</ol>
<pre>        # Train and save HMM model 
        hmm_trainer = HMMTrainer() 
        hmm_trainer.train(X) 
        hmm_models.append((hmm_trainer, label)) 
        hmm_trainer = None </pre>
<ol start="16">
<li>Get a list of test files that were not used for training:</li>
</ol>
<pre>    # Test files 
    input_files = [ 
            'data/pineapple/pineapple15.wav', 
            'data/orange/orange15.wav', 
            'data/apple/apple15.wav', 
            'data/kiwi/kiwi15.wav' 
            ] </pre>
<ol start="17">
<li>Parse the input files, as follows:</li>
</ol>
<pre>    # Classify input data 
    for input_file in input_files: </pre>
<ol start="18">
<li>Read in each audio file, as follows:</li>
</ol>
<pre>        # Read input file 
        sampling_freq, audio = wavfile.read(input_file) </pre>
<ol start="19">
<li>Extract the MFCC features, as follows:</li>
</ol>
<pre>        # Extract MFCC features 
        mfcc_features = mfcc(audio, sampling_freq) </pre>
<ol start="20">
<li>Define the variables to store the maximum score and the output label:</li>
</ol>
<pre>        # Define variables 
        max_score = float('-inf')
        output_label = None</pre>
<ol start="21">
<li>Iterate through all the models and run the input file through each of them:</li>
</ol>
<pre>        # Iterate through all HMM models and pick  
        # the one with the highest score 
        for item in hmm_models: 
            hmm_model, label = item </pre>
<ol start="22">
<li>Extract the score and store the maximum score:</li>
</ol>
<pre>            score = hmm_model.get_score(mfcc_features) 
            if score &gt; max_score: 
                max_score = score 
                output_label = label </pre>
<ol start="23">
<li>Print the true and predicted labels:</li>
</ol>
<pre>         # Print the output<br/>        print("True:", input_file[input_file.find('/')+1:input_file.rfind('/')])<br/>        print("Predicted:", output_label)</pre>
<ol start="24">
<li>The full code is in the <kbd>speech_recognizer.py</kbd> file. Run this file using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ python speech_recognizer.py --input-folder data</strong></pre>
<p style="padding-left: 60px">The following results are returned on your Terminal:</p>
<pre style="padding-left: 60px"><strong>True: pineapple</strong><br/><strong>Predicted: data\pineapple</strong><br/><strong>True: orange</strong><br/><strong>Predicted: data\orange</strong><br/><strong>True: apple</strong><br/><strong>Predicted: data\apple</strong><br/><strong>True: kiwi</strong><br/><strong>Predicted: data\kiwi</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we created a speech recognition system using an HMM. To do this, we first created a function to analyze input arguments. Then, a class was used to handle all HMM-related processing. Thus, we have classified the input data and then predicted the label of the test data. Finally, we printed the results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">A voice recognition system is based on a comparison of the input audio, which is appropriately processed, with a database created during system training. In practice, the software application tries to identify the word spoken by the speaker, looking for a similar sound in the database, and checking which word corresponds. Naturally, it is a very complex operation. Moreover, it is not done on whole words, but on the phonemes that compose them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to the official documentation of the <kbd>hmmlearn</kbd> package:<span> </span><a href="https://hmmlearn.readthedocs.io/en/latest/">https://hmmlearn.readthedocs.io/en/latest/</a></span></li>
<li class="mce-root"><span>Refer to the official documentation of the</span> <kbd>python_speech_features</kbd><span><span> </span>package: <a href="https://python-speech-features.readthedocs.io/en/latest/">https://python-speech-features.readthedocs.io/en/latest/</a></span></li>
<li class="mce-root"><span>Refer to the </span><em>Argparse Tutorial</em>: <a href="https://docs.python.org/3/howto/argparse.html">https://docs.python.org/3/howto/argparse.html</a></li>
<li><span>Refer to </span><em>Fundamentals of Speech Recognition: A Short Course</em> (from Mississippi State University): <a href="http://www.iitg.ac.in/samudravijaya/tutorials/fundamentalOfASR_picone96.pdf">http://www.iitg.ac.in/samudravijaya/tutorials/fundamentalOfASR_picone96.pdf</a></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a TTS system</h1>
                </header>
            
            <article>
                
<p>Speech synthesis is the technique that is used for the artificial reproduction of the human voice. A system used for this purpose is called a speech synthesizer and can be implemented by software or hardware. Speech synthesis systems are also known as TTS systems due to their ability to convert text into speech. There are also systems that convert phonetic symbols into speech.</p>
<p>Speech synthesis can be achieved by concatenating recordings of vocals stored in a database. The various systems of speech synthesis differ according to the size of the stored voice samples. That is, a system that stores single phonemes or double phonemes allows you to obtain the maximum number of combinations at the expense of overall clarity, while other systems which are designed for a specific use repeat themselves, to record whole words or entire sentences in order to achieve a high-quality result.</p>
<p>A synthesizer can create a completely synthetic voice using vocal traits and other human characteristics. The quality of a speech synthesizer is evaluated on the basis of both the resemblance to the human voice and its level of comprehensibility. A TTS conversion program with good performance can play an important role in accessibility; for example, by allowing people with impaired vision or dyslexia to listen to documents written on the computer. For this type of application (since the early 1980s), many operating systems have included speech synthesis functions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this recipe, we will introduce the Python library that allows us to create TTS systems. We will run the <kbd>pyttsx</kbd> cross-platform TTS wrapper library.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's see how to build a TTS system:</p>
<ol>
<li>First, we must install <kbd>pyttsx</kbd> for the Python 3 library (offline TTS for Python 3) and its relative dependencies:</li>
</ol>
<pre style="padding-left: 60px"><strong>$ pip install pyttsx3</strong></pre>
<ol start="2">
<li>To avoid possible errors, it is also necessary to install the <kbd>pypiwin32</kbd> library:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>$ pip install pypiwin32</span></strong></pre>
<ol start="3">
<li>Create a new Python file and import the <kbd>pyttsx3</kbd> package (the full code is in the<span> </span><kbd><span>tts.py</span></kbd><span> </span>file that's already provided for you):</li>
</ol>
<pre style="padding-left: 60px">import pyttsx3;</pre>
<ol start="4">
<li>We create an engine instance that will use the specified driver:</li>
</ol>
<pre style="padding-left: 60px">engine = pyttsx3.init();</pre>
<ol start="5">
<li>To change the speech rate, use the following commands:</li>
</ol>
<pre style="padding-left: 60px">rate = engine.getProperty('rate')<br/>engine.setProperty('rate', rate-50)</pre>
<p class="mce-root"/>
<ol start="6">
<li>To<span> </span>change the voice of the speaker, use the following commands:</li>
</ol>
<pre style="padding-left: 60px">voices = engine.getProperty('voices')<br/>engine.setProperty('voice', 'TTS_MS_EN-US_ZIRA_11.0')</pre>
<ol start="7">
<li>Now, we will use the <kbd>say</kbd> method to queue a command to speak an utterance. The speech is output according to the properties set before this command in the queue:</li>
</ol>
<pre style="padding-left: 60px">engine.say("You are reading the Python Machine Learning Cookbook");<br/>engine.say("I hope you like it.");</pre>
<ol start="8">
<li>Finally, we will invoke the <kbd>runAndWait()</kbd> method. This method blocks while processing all currently queued commands and invokes callbacks for engine notifications appropriately. It returns when all commands queued before this call are emptied from the queue:</li>
</ol>
<pre style="padding-left: 60px">engine.runAndWait();</pre>
<p style="padding-left: 60px">At this point, a different voice will read the text supplied by us.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="mce-root">A speech synthesis system or engine is composed of two parts: a frontend and a backend. The frontend part deals with the conversion of the text into phonetic symbols, while the backend part interprets the phonetic symbols and reads them, thus, transforming them into an artificial voice. The frontend has two key functions; first, it performs an analysis of the written text to convert all numbers, abbreviations, and abbreviations into words in full. This preprocessing step is referred to as tokenization. The second function consists of converting each word into its corresponding phonetic symbols and performing the linguistic analysis of the revised text, subdividing it into prosodic units, that is, into prepositions, sentences, and periods. The process of assigning phonetic transcription to words is called conversion from text to phoneme, or from grapheme to phoneme.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="mce-root">An evolution of the classic TTS system is called <kbd>WaveNet</kbd>, and it seems to know how to speak, articulate accents, and pronounce a whole sentence fluently. WaveNet is a deep neural network that generates raw audio. It was created by researchers at the London-based artificial intelligence firm, DeepMind. WaveNet uses a deep generative model for sound waves that can imitate any human voice. The sentences pronounced by WaveNet sound 50% more similar to a human voice than the more advanced TTS. To demonstrate this, samples were created in English and Mandarin, and using the <strong>Mean Opinion Scores</strong> (<strong><span>MOS</span></strong>) system, which is now a standard in audio evaluation, samples of artificial intelligence were compared to those generated by normal TTS, parametric-TTS, and also with respect to the samples of real voices.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li class="mce-root"><span>Refer to the official documentation of</span> the <kbd>pyttsx3</kbd> pa<span>ckage: <a href="https://pyttsx3.readthedocs.io/en/latest/index.html">https://pyttsx3.readthedocs.io/en/latest/index.html</a></span></li>
<li><span>Refer to <em>Text to Speech: A Simple Tutorial</em> (by D. Sasirekha and E. Chandra): <a href="https://pdfs.semanticscholar.org/e7ad/2a63458653ac965fe349fe375eb8e2b70b02.pdf">https://pdfs.semanticscholar.org/e7ad/2a63458653ac965fe349fe375eb8e2b70b02.pdf</a></span></li>
<li><span>Refer to </span><em>WaveNet: A Generative Model for Raw Audio</em> (from Google DeepMind): <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>