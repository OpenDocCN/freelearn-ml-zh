<html><head></head><body>
		<div id="_idContainer072">
			<h1 id="_idParaDest-92" class="chapter-number"><a id="_idTextAnchor018"/>6</h1>
			<h1 id="_idParaDest-93">Detecting Machine-Generated Text</h1>
			<p>In the previous chapter, we discussed deepfakes, which are synthetic media that can depict a person in a video and show the person to be saying or doing things that they did not say or do. Using powerful deep learning methods, it has been possible to create realistic deepfakes that cannot be distinguished from real media. Similar to such deepfakes, machine learning models have also succeeded in creating fake text – text that is generated by a model but appears to be written by a human. While the technology has been used to power chatbots and develop question-answering systems, it has also found its use in several <span class="No-Break">nefarious applications.</span></p>
			<p>Generative text models can be used to enhance bots and fake profiles on social networking sites. Given a prompt text, the model can be used to write messages, posts, and articles, thus adding credibility to the bot. A bot can now pretend to be a real person, and a victim might be fooled because of the realistic-appearing chat messages. These models allow customization by style, tone, sentiment, domain, and even political leaning. It is easily possible to provide a prompt and generate a news-style article; such articles can be used to spread misinformation. Models can be automated and deployed at scale on the internet, which means that there can be millions of fake profiles pretending to be real people, and millions of Twitter accounts generating and posting misleading articles. Detecting automated text is an important problem on the <span class="No-Break">internet today.</span></p>
			<p>This chapter will explore the fundamentals of generative models, how they can be used to create text, and techniques to <span class="No-Break">detect them.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Text <span class="No-Break">generation models</span></li>
				<li><span class="No-Break">Naïve detection</span></li>
				<li>Transformer methods for detecting <span class="No-Break">automated text</span></li>
			</ul>
			<p>By the end of this chapter, you will have a firm understanding of text generation models and approaches to detecting <span class="No-Break">bot-generated text.</span></p>
			<h1 id="_idParaDest-94">Technical requirements</h1>
			<p>You can find the code files for this chapter on GitHub <span class="No-Break">at</span><a href=" https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%206"><span class="No-Break"> </span><span class="No-Break">https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%206</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-95">Text generation models</h1>
			<p>In the previous chapter, we saw how machine learning models can be trained to generate images of people. The images generated were so realistic that it was impossible in most cases to tell them <a id="_idIndexMarker447"/>apart from real images with the naked eye. Along similar lines, machine learning models have made great progress in the area of text generation as well. It is now possible to generate high-quality text in an automated fashion using deep learning models. Just like images, this text is so well written that it is not possible to distinguish it from <span class="No-Break">human-generated text.</span></p>
			<p>Fundamentally, a language model is a machine learning system that is able to look at a part of a sentence and predict what comes next. The words predicted are appended to the existing sentence, and this newly formed sentence is used to predict what will come next. The process continues recursively until a specific token denoting the end of the text is generated. Note that when we say that the next word is predicted, in reality, the model generates a probability distribution over possible output words. Language models can also operate at the <span class="No-Break">character level.</span></p>
			<p>Most text generation models take in a prompt text as input. Trained on massive datasets (such as all Wikipedia articles or entire books), the models have learned to produce text based on these prompts. Training on different kinds of text (stories, biographies, technical articles, and news articles) enables models to generate those specific kinds <span class="No-Break">of text.</span></p>
			<p>To see the power of AI-based text generation with your own eyes, explore the open source text generator called <strong class="bold">Grover</strong>. This is a <a id="_idIndexMarker448"/>tool that was produced by researchers at the University of Washington and allows you to produce a real-looking news article based on any given prompt. The website provides an interface as shown in the following figure, where you can provide a prompt title and an <span class="No-Break">author name:</span></p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B19327_06_01.jpg" alt="Figure 6.1 – The Grover interface to generate text"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – The Grover interface to generate text</p>
			<p>Once you set these<a id="_idIndexMarker449"/> parameters, Grover will generate an article that appears real. Here is the article that was generated with our <span class="No-Break">given prompt:</span></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Link found between autism </strong><span class="No-Break"><strong class="bold">and vaccines</strong></span></p>
							<p>May 29, 2019 - <span class="No-Break">Sofia Ojeda</span></p>
							<p>Written by Sofia <span class="No-Break">Ojeda, CNN</span></p>
							<p>A paper published in the journal Genetics and Human Behavior claims to find a “significant link” between autism and a type of booster vaccine given in childhood. The paper focuses on the mercury-based preservative thimerosal and considers the possibility that the injectable vaccine contributes to autism-related symptoms. These include poor social skills, language delays and seizures, according to <span class="No-Break">the paper.</span></p>
							<p>Thimerosal, which is commonly used as a preservative in vaccines, was phased out of children’s vaccines in 2010. In 2010, the Food and Drug Administration declared that thimerosal was safe. “We don’t know of any adverse effects associated with the endocrine-disrupting, mercury-containing preservative in vaccines, and believe that this outcome is beneficial to public health,” said Dr. Lisa M. Collins, president of the American Academy of Pediatrics, in an organization statement issued <span class="No-Break">in 2010.</span></p>
							<p>The new study, led by researchers at Tel Aviv University, is based on case studies of two women with autistic children and one non-autistic mother. The researchers found that autism symptoms tended to worsen after only one dose of the thimerosal-containing hepatitis B booster vaccine. “The use of unadjuvanted Hepatitis B booster vaccines, against which the only adverse effects are fatigue, headache, cough, and fever, remains a substantial and unrefined source of vaccine-related injury and disability worldwide,” the study <span class="No-Break">authors wrote.</span></p>
							<p>Using a mathematical model, the study authors described multiple risks associated with the use of unadjuvanted injectable vaccines. Among them, vaccines increased the risk of seizures, such as those associated with autism; autoimmune disorders <span class="No-Break">and cancer.</span></p>
							<p>“There are other types of vaccines that pose similar risk to autism, including rotavirus vaccines and the polio vaccine,” said lead author Professor Uri Ayalon, head of the pediatrics department at Tel <span class="No-Break">Aviv University.</span></p>
							<p>The authors also found that the autism risk increased by twofold in children aged 3 to 10 who received the hepatitis B booster vaccine against Hepatitis B, regardless of the following vaccination schedules: aged 9 months or older with only one dose; or under 4 months old and only one dose. “Since these vaccines are widely used, regardless of age, it may be important to limit its utilization,” the <span class="No-Break">researchers wrote.</span></p>
							<p>Lead author Dr. Yonatan Schulmann said there were no apparent risks associated with a standard influenza vaccination. “The flu vaccine probably represents an acceptable source of vaccine-related injury and disability,” he said. “This is not true for most vaccines. The flu vaccine is relatively inexpensive (free of charges) and has no significant health effects,” <span class="No-Break">he said.</span></p>
							<p>The timing of vaccination is also important, said Schulmann. “Autism spectrum disorders are most often diagnosed in early adolescence, the upper age range at which it is most likely that vaccination data is available,” he said. Furthermore, the authors said they found no clear differences between children who received hepatitis B vaccine against Hepatitis B and <span class="No-Break">other children.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Note how the article has<a id="_idIndexMarker450"/> the stylistic features that you would typically expect from journalistic writings. The sentence construction is grammatically correct and the whole text reads as a coherent article. There are quotes from researchers and professors who are subject matter experts, complete with statistics and experimental results cited. Overall, the article could pass off as something written by <span class="No-Break">a human.</span></p>
			<h2 id="_idParaDest-96">Understanding GPT</h2>
			<p>GPT stands for <strong class="bold">Generative Pretrained Transformer</strong>, and GPT models have dazzled the NLP world because they can generate <a id="_idIndexMarker451"/>coherent essays that are beyond those produced by traditional language models such <a id="_idIndexMarker452"/>as those based on <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>). GPT models are also based on the transformer architecture (recall the BERT architecture that we used for malware detection was also based on <span class="No-Break">the transformer).</span></p>
			<p>Recall the concepts of attention that we introduced in <a href="B19327_03.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Malware Detection Using Transformers and BERT</em>. We introduced two kinds of blocks – the encoder and decoder – both of which were built using transformers that leveraged the attention mechanism. The transformer encoder had a self-attention layer followed by a fully connected feed-forward neural network. The decoder layer was similar except that it had an additional masked self-attention layer that ensured that the transformer did not attend to the future tokens (which would defeat the purpose of the language model). For example, if the decoder decodes the fourth word, it will attend to all words up to the third predicted word and all the words in <span class="No-Break">the input.</span></p>
			<p>In general, GPT models use only the decoder blocks, which are stacked one after the other. When a token is fed into the model, it is converted into an embedding representation using a matrix lookup. Additionally, a positional encoding is added to it to indicate the sequence of words/tokens. The two matrices (embedding and positional encoding) are parts of the pretrained models we use. When the first token is passed to the model, it gets converted into a vector using the embedding lookup and positional encoding matrices. It passes through the first decoder block, which performs self-attention, passes the output to the neural network layer, and forwards the output to the next <span class="No-Break">decoder block.</span></p>
			<p>After processing by the <a id="_idIndexMarker453"/>final decoder, the output vector is multiplied with the embedding matrix to obtain a probability distribution over the output token to be produced. This probability distribution can be used to select the next word. The most straightforward strategy is to choose the word with the highest probability – however, we run the risk of being stuck in a loop. For instance, if the tokens produced so far are “<em class="italic">The man and</em>” and we always select the word with the highest probability, we might end up producing “<em class="italic">The man and the man and the man and the </em><span class="No-Break"><em class="italic">man…..</em></span><span class="No-Break">” indefinitely.</span></p>
			<p>To avoid this, we apply a top-<em class="italic">K</em> sampling. We select the top <em class="italic">K</em> words (based on the probability) and sample a word from them, where words with a higher score have a higher chance of being selected. Since this process is non-deterministic, the model does not end up in the loop of choosing the same set of words again and again. The process continues until a certain number of tokens has been produced, or the end-of-string token <span class="No-Break">is found.</span></p>
			<p>Generation by GPT models can be either conditional or unconditional. To see generation in action, we can use the Write with Transformer (<a href="https://transformer.huggingface.co/doc/gpt2-large">https://transformer.huggingface.co/doc/gpt2-large</a>) web app developed by Hugging Face, which uses GPT-2. The website allows you to simulate both conditional and <span class="No-Break">unconditional generation.</span></p>
			<p>In conditional generation, we provide the model with a set of words as a prompt, which is used to seed the generation. This initial set of words provides the context used to drive the rest of the text, <span class="No-Break">as shown:</span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B19327_06_02.jpg" alt="Figure 6.2 – Generating text with a prompt"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Generating text with a prompt</p>
			<p>On the other hand, in unconditional generation, we just provide the <strong class="source-inline">&lt;s&gt;</strong> token, which is used to indicate<a id="_idIndexMarker454"/> the start of a string, and allow the model to freely produce what it wants. If you press the <em class="italic">Tab</em> key on Write With Transformer, you should see such unconditional <span class="No-Break">samples generated:</span></p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B19327_06_03.jpg" alt="Figure 6.3 – Generating text without prompts"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Generating text without prompts</p>
			<p>There have been multiple versions of GPT models released by OpenAI, the latest one that has made the news being ChatGPT, based on GPT 3.5. In an upcoming section, we will use ChatGPT to create our own dataset of <span class="No-Break">fake news.</span></p>
			<h1 id="_idParaDest-97">Naïve detection</h1>
			<p>In this section, we will focus <a id="_idIndexMarker455"/>on naïve methods for detecting bot-generated text. We will first create our own dataset, extract features, and then apply machine learning models to determine whether a particular text is machine-generated <span class="No-Break">or not.</span></p>
			<h2 id="_idParaDest-98">Creating the dataset</h2>
			<p>The task we will focus on is<a id="_idIndexMarker456"/> detecting bot-generated fake news. However, the concepts and techniques we will learn are fairly generic and can be applied to<a id="_idIndexMarker457"/> parallel tasks such as detecting bot-generated tweets, reviews, posts, and so on. As such a dataset is not readily available to the public, we will create <span class="No-Break">our own.</span></p>
			<p>How are we creating our dataset? We will use the News Aggregator dataset (<a href="https://archive.ics.uci.edu/ml/datasets/News+Aggregator">https://archive.ics.uci.edu/ml/datasets/News+Aggregator</a>) from the UCI Dataset Repository. The dataset contains a set of news articles (that is, links to the articles on the web). We will scrape these articles, and these are our human-generated articles. Then, we will use the article title as a prompt to seed generation by GPT-2, and generate an article that will be on the same theme and topic, but generated by GPT-2! This makes up our <span class="No-Break">positive class.</span></p>
			<h3>Scraping real articles</h3>
			<p>The News Aggregator <a id="_idIndexMarker458"/>dataset from UCI contains information on over 420k news articles. It was developed for research purposes by scientists at the Roma Tre University in Italy. News articles span multiple categories such as business, health, entertainment, and science and technology. For each article, we have the title and the URL of the article online. You will need to download the dataset from the UCI Machine Learning Repository <span class="No-Break">website (</span><a href="https://archive.ics.uci.edu/ml/datasets/News+Aggregator"><span class="No-Break">https://archive.ics.uci.edu/ml/datasets/News+Aggregator</span></a><span class="No-Break">).</span></p>
			<p>Take a look at the data using the <strong class="source-inline">head()</strong> functionality (note that you will have to change the path according to how you store the <span class="No-Break">file locally):</span></p>
			<pre class="source-code">
import pandas as pd
path = "/content/UCI-News-Aggregator-Classifier/data/uci-news-aggregator.csv"
df = pd.read_csv(path)
df.head()</pre>
			<p>This will show you the <a id="_idIndexMarker459"/>first five rows of the DataFrame. As you can see in the following screenshot, we have an ID to refer to each row and the title and URL of the news article. We also have the hostname (the website where the article appeared) and the timestamp, which denotes the time when the news was published. The <strong class="bold">STORY</strong> field contains an ID that is used to indicate a cluster containing similar <span class="No-Break">news stories.</span></p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B19327_06_04.jpg" alt="Figure 6.4 – UCI News Aggregator data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – UCI News Aggregator data</p>
			<p>Let us take a look at the distribution of the articles <span class="No-Break">across categories:</span></p>
			<pre class="source-code">
df["CATEGORY"].value_counts().plot(kind = 'bar')</pre>
			<p>This will produce the <span class="No-Break">following result:</span></p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B19327_06_05.jpg" alt="Figure 6.5 – News article distribution by category"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – News article distribution by category</p>
			<p>From the documentation, we see that the categories <strong class="bold">e</strong>, <strong class="bold">b</strong>, <strong class="bold">t</strong>, and <strong class="bold">m</strong> represent entertainment, business, technology, and health, respectively. Entertainment has the highest number of <a id="_idIndexMarker460"/>articles, followed by business and technology (which are similar), and health has <span class="No-Break">the least.</span></p>
			<p>Similarly, we can also inspect the top domains where the articles <span class="No-Break">come from:</span></p>
			<pre class="source-code">
df["HOSTNAME"].value_counts()[:20].plot(kind = 'bar')</pre>
			<p>You will get the <span class="No-Break">following output:</span></p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B19327_06_06.jpg" alt="Figure 6.6 – Distribution of news articles across sources"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Distribution of news articles across sources</p>
			<p>In order to scrape the article from the website, we would need to simulate a browser session using a browser tool such as Selenium, find the article text by parsing the HTML source, and then extract it. Fortunately, there is a library in Python that does all of this for us. The <strong class="source-inline">Newspaper</strong> Python package (<a href="https://github.com/codelucas/newspaper/">https://github.com/codelucas/newspaper/</a>) provides an interface for downloading and parsing news articles. It can extract text, keywords, author names, summaries, and images from the HTML source of an article. It has support for multiple languages including English, Spanish, Russian, and German. You can also use a general-purpose web scraping library such as <strong class="source-inline">BeautifulSoup</strong>, but the <strong class="source-inline">Newspaper</strong> library is designed specifically to capture news articles and hence provides a lot of functions that we would have had to write custom if <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">BeautifulSoup</strong></span><span class="No-Break">.</span></p>
			<p>To create our dataset <a id="_idIndexMarker461"/>of real articles, we will iterate through the News Aggregator DataFrame and use the <strong class="source-inline">Newspaper</strong> library to extract the text for each article. Note that the dataset has upward of 420k articles – for the purposes of demonstration, we will sample 1,000 articles randomly from the dataset. For each article, we will use the <strong class="source-inline">Newspaper</strong> library to scrape the text. We will create a directory to hold <span class="No-Break">these articles.</span></p>
			<p>First, let us create the <span class="No-Break">directory structure:</span></p>
			<pre class="source-code">
import os
root = "./articles"
fake = os.path.join(root, "fake")
real = os.path.join(root, "real")
for dir in [root, real, fake]:
  if not os.path.exists(dir):
    os.mkdir(dir)</pre>
			<p>Now, let us sample articles from the 400k articles we have. In order to avoid bias and overfitting, we should not focus on a particular category. Rather, our goal should be to sample uniformly at random so we have a well-distributed dataset across all four categories. This general principle also applies to other areas where you are designing machine learning models; the more diverse your dataset is, the better the generalization. We will sample 250 articles from each of our <span class="No-Break">4 categories:</span></p>
			<pre class="source-code">
df2 = df.groupby('CATEGORY').apply(lambda x: x.sample(250))</pre>
			<p>If you check the distribution now, you will see that it is equal across <span class="No-Break">all categories:</span></p>
			<pre class="source-code">
df2["CATEGORY"].value_counts().plot(kind='bar')</pre>
			<p>You can see the distribution clearly in the <span class="No-Break">following plot:</span></p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B19327_06_07.jpg" alt="Figure 6.7 – Distribution of sampled articles"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Distribution of sampled articles</p>
			<p>We will now iterate<a id="_idIndexMarker462"/> through this DataFrame and scrape each article. We will scrape the article, read the text, and save it into a file in the real directory we created earlier. Note that this is essentially a web scraper – as different websites have different HTML structures, the newspaper library may hit some errors. Certain websites may also block scrapers. For such articles, we will print out a message with the article URL. In practice, when such a situation is encountered, data scientists will fill the gap manually if the number of missing articles is <span class="No-Break">small enough:</span></p>
			<pre class="source-code">
from newspaper import Article
URL_LIST = df2["URL"].tolist()
TITLE_LIST = df2["TITLE"].tolist()
for id_url, article_url in enumerate(URL_LIST):
  article = Article(article_url)
  try:
    # Download and parse article
    article.download()
    article.parse()
    text = article.text
    # Save to file
    filename = os.path.join(real, "Article_{}.txt".format(id_url))
    article_title = TITLE_LIST[id_url]
    with open(filename, "w") as text_file:
      text_file.write(" %s \n %s" % (article_title, text))
  except:
    print("Could not download the article at: {}".format(article_url))</pre>
			<p>Now we have our real<a id="_idIndexMarker463"/> articles downloaded locally. It’s time to get into the good stuff – creating our set of <span class="No-Break">fake articles!</span></p>
			<h3>Using GPT to create a dataset</h3>
			<p>In this section, we will use GPT-3 to create our own dataset of machine-generated text. OpenAI, a San Francisco-based<a id="_idIndexMarker464"/> artificial intelligence research lab, developed GPT-3, a pretrained universal language model that utilizes deep learning<a id="_idIndexMarker465"/> transformers to create text that is remarkably human-like. Released in 2020, GPT-3 has made headlines in various industries, as its potential use cases are virtually limitless. With the help of the GPT-3 API family and ChatGPT, individuals have used it to write fiction and poetry, code websites, respond to customer feedback, improve grammar, translate languages, generate dialog, optimize tax deductions, and automate A/B testing, among other things. The model’s high-quality results have <span class="No-Break">impressed many.</span></p>
			<p>We can use the <strong class="source-inline">transformers</strong> library from HuggingFace to download and run inference on ChatGPT models. To do this, we can first load the model <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from transformers import pipeline
generator = pipeline('text-generation',
                     model='EleutherAI/gpt-neo-2.7B')</pre>
			<p>This will download the<a id="_idIndexMarker466"/> model for you locally. Note that this involves downloading a sizeable model from the online repository, and hence will<a id="_idIndexMarker467"/> take quite some time. The time taken to execute will depend on your system usage, resources, and network speed at <span class="No-Break">the time.</span></p>
			<p>We can generate a sample text using this new model. For example, if we want to generate a poem about flowers, we can do <span class="No-Break">the following:</span></p>
			<pre class="source-code">
chat_prompt = 'Generate a five-line poem about flowers'
model_output = generator(prompt,
                 max_length=100)
response = model_output[0]['generated_text']
print(response)</pre>
			<p>And this gave me the following poem (note that the results may differ <span class="No-Break">for you):</span></p>
			<pre class="source-code">
Flowers bloom in gardens bright,
Their petals open to the light,
Their fragrance sweet and pure,
A colorful feast for eyes to lure,
Nature's art, forever to endure.</pre>
			<p>We have already downloaded and initialized the model we want. Now, we can iterate through our list of article titles and generate articles one by one by passing the title as a seed prefix. Just like the scraped articles, each article must be saved into a text file so that we can later access it<a id="_idIndexMarker468"/> <span class="No-Break">for training:</span></p>
			<pre class="source-code">
for id_title, title in enumerate(TITLE_LIST):
  # Generate the article
  article = generator(title, max_length = 500)[0]["generated_text"]
  # Save to file
  filename = os.path.join(fake, "Article_{}.txt".format(id_url))
  with open(filename, "w") as text_file:
      text_file.write(" %s \n %s" % (title, text))</pre>
			<p>All that is left now is to<a id="_idIndexMarker469"/> read all of the data we have into a common array or list, which can then be used in all of our experiments. We will read each file in the real directory and add it to an array. At the same time, we will keep appending <strong class="source-inline">0</strong> (indicating a real article) to another array that holds labels. We will repeat the same process with the fake articles and append <strong class="source-inline">1</strong> as <span class="No-Break">the label:</span></p>
			<pre class="source-code">
X = []
Y = []
for file in os.listdir(real):
  try:
    with open(file, "r") as article_file:
      article = file.read()
      X.append(article)
      Y.append(0)
  except:
    print("Error reading: {}".format(file))
    continue
for file in os.listdir(fake):
  try:
    with open(file, "r") as article_file:
      article = file.read()
      X.append(article)
      Y.append(1)
  except:
    print("Error reading: {}".format(file))
    continue</pre>
			<p>Now, we have our <a id="_idIndexMarker470"/>text in the <strong class="source-inline">X</strong> list and associated labels in the <strong class="source-inline">Y</strong> list. Our dataset <span class="No-Break">is ready!</span></p>
			<h2 id="_idParaDest-99">Feature exploration</h2>
			<p>Now that we have our dataset, we <a id="_idIndexMarker471"/>want to build a machine learning <a id="_idIndexMarker472"/>model to detect bot-generated news articles. Recall that machine learning algorithms are mathematical models and, therefore, operate on numbers; they cannot operate directly on text! Let us now extract some features from <span class="No-Break">the text.</span></p>
			<p>This section will focus on hand-crafting features – the process where subject matter experts theorize potential differences between the two classes and build features that will effectively capture the differences. There is no unified technique for doing this; data scientists experiment with several features based on domain knowledge to identify the <span class="No-Break">best ones.</span></p>
			<p>Here, we are concerned with text data – so let us engineer a few features from that domain. Prior work in NLP and linguistics has analyzed human writing and identified certain characteristics. We will engineer three features based on <span class="No-Break">prior research.</span></p>
			<h3>Function words</h3>
			<p>These are supporting words in the text that do not contribute to meaning but add continuity and flow to the sentence. They are generally determiners (<em class="italic">the</em>, <em class="italic">an</em>, <em class="italic">many</em>, <em class="italic">a little</em>, and <em class="italic">none</em>), conjunctions (<em class="italic">and</em> and <em class="italic">but</em>), prepositions (<em class="italic">around</em>, <em class="italic">within</em>, and <em class="italic">on</em>), pronouns (<em class="italic">he</em>, <em class="italic">her</em>, and <em class="italic">their</em>), auxiliary verbs (<em class="italic">be</em>, <em class="italic">have</em>, and <em class="italic">do</em>), modal auxiliary (<em class="italic">can</em>, <em class="italic">should</em>, <em class="italic">could</em>, and <em class="italic">would</em>), qualifiers (<em class="italic">really</em> and <em class="italic">quite</em>), or question words (<em class="italic">how</em> and <em class="italic">why</em>). Linguistic studies have shown that every human uses these unpredictably, so there might be randomness in the usage pattern. As our feature, we will count the number of function <a id="_idIndexMarker473"/>words that we see in the sentence, and then normalize it by the length of the sentence <span class="No-Break">in words.</span></p>
			<p>We will use a file that contains a list of the top function words and read the list of all function words. Then, we will count the function words in each text and normalize this count by the length. We will wrap this up in a function that can be used to featurize multiple instances of text. Note that as the list of function words would be the same for all texts, we do not need to repeat it in each function call – we will keep that part outside <span class="No-Break">the function:</span></p>
			<pre class="source-code">
FUNCTION_WORD_FILE = '../static/function_words.txt'
with open(FUNCTION_WORD_FILE,'r') as fwf:
  k = fwf.readlines()
  func_words = [w.rstrip() for w in k]
  #There might be duplicates!
  func_words = list(set(func_words))
def calculate_function_words(text):
  function_word_counter = 0
  text_length = len(text.split(' '))
  for word in func_words:
    function_word_counter = function_word_counter + text.count(word)
  if text_length == 0:
    feature = 0
  else:
    feature = function_word_counter / total_length
  return feature</pre>
			<h3>Punctuation</h3>
			<p>Punctuation<a id="_idIndexMarker474"/> symbols (commas, periods, question marks, exclamations, and semi-colons) set the tone of the text and inform how it should be read. Prior research has shown that the count of punctuation symbols may be an important feature in detecting bot-generated text. We will first compile a list of punctuation symbols (readily available in the Python <strong class="source-inline">string</strong> package). Similar to the function words, we will count the occurrences of punctuation and normalize them by length. Note that this time, however, we need to normalize by the length in terms of the number of characters as opposed <span class="No-Break">to words:</span></p>
			<pre class="source-code">
def calculate_punctuation(text):
  punctuations = =[ k for k in string.punctuation]
  punctuation_counter = 0
  total_length = len(text.split())
  for punc in punctuations:
    punctuation_counter = punctuation_counter + text.count(punc)
  if text_length == 0:
    feature = 0
  else:
    feature = punctuation_counter / total_length
  return feature</pre>
			<h3>Readability</h3>
			<p>Research in early childhood education has studied text in detail and derived several metrics that indicate how<a id="_idIndexMarker475"/> readable a particular blob of text is. These metrics analyze the vocabulary and complexity of the text and determine the ease with which a reader can read and understand the text. There are several measures of readability defined in prior literature (<a href="https://en.wikipedia.org/wiki/Readability">https://en.wikipedia.org/wiki/Readability</a>), but we will be using the most popular one called the <strong class="bold">Automated Readability Index</strong> (<strong class="bold">ARI</strong>) (<a href="https://readabilityformulas.com/automated-readability-index.php">https://readabilityformulas.com/automated-readability-index.php</a>). It depends on two factors – word<a id="_idIndexMarker476"/> difficulty (the number of letters per word) and sentence difficulty (the number of words per sentence), and is calculated <span class="No-Break">as follows:</span></p>
			<p><span class="_-----MathTools-_Math_Variable">A</span><span class="_-----MathTools-_Math_Variable">R</span><span class="_-----MathTools-_Math_Variable">I</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">4.71</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">h</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">0.5</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">w</span><span class="_-----MathTools-_Math_Variable">o</span><span class="_-----MathTools-_Math_Variable">r</span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol">#</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">t</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">n</span><span class="_-----MathTools-_Math_Variable">c</span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Variable">s</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number">21.43</span></span></p>
			<p>In theory, the ARI represents the approximate age needed to understand the text. We will now develop a function that calculates the ARI for our input text, and wrap it into a function like we did for the <span class="No-Break">previous features:</span></p>
			<pre class="source-code">
def calculate_ari(text):
  chars = len(text.split())
  words = len(text.split(' '))
  sentences = len(text.split('.'))
  if words == 0 or sentences == 0:
    feature = 0
  else:
    feature = 4.71* (chars / words) + 0.5* (words / sentences) - 21.43
  return feature</pre>
			<p>This completes our discussion of naive feature extraction. In the next section, we will use these features to<a id="_idIndexMarker477"/> train and evaluate machine <span class="No-Break">learning models.</span></p>
			<h2 id="_idParaDest-100">Using machine learning models for detecting text</h2>
			<p>We have now <a id="_idIndexMarker478"/>hand-crafted three different features: punctuation <a id="_idIndexMarker479"/>counts, function word counts, and the readability index. We also defined functions for each. Now, we are ready to apply these to our dataset and build models. Recall that the <strong class="source-inline">X</strong> array contains all of our text. We want to represent each text sample using a three-element vector (as we have <span class="No-Break">three features):</span></p>
			<pre class="source-code">
X_Features = []
for x in X:
  feature_vector = []
  feature_vector.append(calculate_function_words(x))
  feature_vector.append(calculate_punctuation(x))
  feature_vector.append(calculate_ari(x))
  X_Features.append(feature_vector)</pre>
			<p>Now, each text sample is represented by a three-element vector in <strong class="source-inline">X_Features</strong>. The first, second, and third elements represent the normalized function word count, punctuation count, and ARI, respectively. Note that this order is arbitrary – you may choose your own order as it does not affect the <span class="No-Break">final model.</span></p>
			<p>Our features are ready, so now we will do the usual. We begin by splitting our data into training and test sets. We then fit a model on the training data and evaluate its performance on the test data. In previous chapters, we used the confusion matrix function to plot the confusion matrix and visually observe the true positives, false positives, true negatives, and false negatives. We will now build another function on top of it that will take in these values and calculate metrics of interest. We will calculate the true positives, false positives, true negatives, and false negatives, and then calculate the accuracy, precision, recall, and F1<a id="_idIndexMarker480"/> score. We will return all of these as <span class="No-Break">a dictionary:</span></p>
			<pre class="source-code">
from sklearn.metrics import confusion_matrix
def evaluate_model(actual, predicted):
  confusion = confusion_matrix(actual, predicted)
  tn, fp, fn, tp = confusion.ravel()
  total = tp + fp + tn + fn
  accuracy = 100 * (tp + tn) / total
  if tp + fp != 0:
    precision = tp / (tp + fp)
  else:
    precision = 0
  if tp + fn != 0:
    recall = tp / (tp + fn)
  else:
    recall = 0
  if precision == 0 or recall == 0:
    f1 = 0
  else:
    f1 = 2 * precision * recall / (precision + recall)
  evaluation = { 'accuracy': accuracy,'precision': precision,'recall': recall,'f1': f1}
  return evaluation</pre>
			<p>Let us split the <a id="_idIndexMarker481"/>data into training <span class="No-Break">and testing:</span></p>
			<pre class="source-code">
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_Features, Y)</pre>
			<p>Now, we will fit a model <a id="_idIndexMarker482"/>on the training data, and evaluate its performance on the test data. Here, we will use random forests, logistic regression, SVM, and a deep <span class="No-Break">neural network.</span></p>
			<p>The logistic regression classifier is a statistical model that expresses the probability of an input belonging to a particular class as a linear combination of features. Specifically, the model produces a<a id="_idIndexMarker483"/> linear combination of inputs (just like linear regression) and applies a sigmoid to this combination to obtain an <span class="No-Break">output probability:</span></p>
			<pre class="source-code">
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train, Y_train)
Y_predicted = model.predict(X_test)
print(evaluate_model(Y_test, Y_pred))</pre>
			<p>Random forests are ensemble classifiers consisting of multiple decision trees. Each tree is a hierarchical structure with nodes as conditions and leaves as class labels. A classification label is derived by following the path of the tree through the root. The random forest contains multiple such trees, each trained on a random sample of data <span class="No-Break">and features:</span></p>
			<pre class="source-code">
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators = 100)
model.fit(X_train, Y_train)
Y_predicted = model.predict(X_test)
print(evaluate_model(Y_test, Y_pred))</pre>
			<p>A <strong class="bold">multilayer perceptron</strong> (<strong class="bold">MLP</strong>) is a fully connected deep neural network, with multiple hidden layers. The input data undergoes<a id="_idIndexMarker484"/> transformations through these layers, and the final layer is a sigmoid <a id="_idIndexMarker485"/>or softmax function, which generates the probability of the data belonging to a <span class="No-Break">particular class:</span></p>
			<pre class="source-code">
from sklearn.neural_network import MLPClassifier
model = MLPClassifier(hidden_layer_sizes = (50, 25, 10),max_iter = 100,activation = 'relu',solver = 'adam',random_state = 123)
model.fit(X_train, Y_train)
Y_predicted = model.predict(X_test)
print(evaluate_model(Y_test, Y_pred))</pre>
			<p>The SVM constructs a <a id="_idIndexMarker486"/>decision boundary between two classes such that<a id="_idIndexMarker487"/> the best classification accuracy is obtained. In case the boundary is not linear, the SVM transforms the features into a higher dimensional space and obtains a <span class="No-Break">non-linear boundary:</span></p>
			<pre class="source-code">
from sklearn import svm
model = svm.SVC(kernel='linear')
model.fit(X_train, Y_train)
Y_predicted = model.predict(X_test)
print(evaluate_model(Y_test, Y_pred))</pre>
			<p>Running this code should print out the evaluation dictionaries for each model, which tells you the accuracy, recall, and precision. You can also plot the confusion matrix (as we did in previous chapters) to visually see the false positives and negatives, and get an overall sense of how good the <span class="No-Break">model is.</span></p>
			<h2 id="_idParaDest-101">Playing around with the model</h2>
			<p>We have<a id="_idIndexMarker488"/> explored here only three features – however, the possibilities for hand-crafted features are endless. I encourage you to experiment by adding more features to the mix. Examples of some features are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Length of <span class="No-Break">the text</span></li>
				<li>Number of <span class="No-Break">proper nouns</span></li>
				<li>Number of <span class="No-Break">numeric characters</span></li>
				<li>Average <span class="No-Break">sentence length</span></li>
				<li>Number of times the letter <em class="italic">q</em> <span class="No-Break">was used</span></li>
			</ul>
			<p>This is certainly not<a id="_idIndexMarker489"/> an exhaustive list, and you should experiment by adding other features to see whether the model's <span class="No-Break">performance improves.</span></p>
			<h2 id="_idParaDest-102">Automatic feature extraction</h2>
			<p>In the previous section, we <a id="_idIndexMarker490"/>discussed how features can be engineered from text. However, hand-crafting features might not always be the best idea. This is because it requires expert knowledge. In this case, data scientists or machine<a id="_idIndexMarker491"/> learning engineers alone will not be able to design these features – they will need experts from linguistics and language studies to identify the nuances of language and suggest appropriate features such as the readability index. Additionally, the process is time-consuming; each feature has to be identified, implemented, and tested one after <span class="No-Break">the other.</span></p>
			<p>We will now explore some methods for automatic feature extraction from text. This means that we do not manually design features such as the punctuation count, readability index, and so on. We will use existing models and techniques, which can take in the input text and generate a feature vector <span class="No-Break">for us.</span></p>
			<h3>TF-IDF</h3>
			<p><strong class="bold">Term Frequency – Inverse Document Frequency</strong> (<strong class="bold">TF-IDF</strong>) is a commonly used technique in natural language processing to<a id="_idIndexMarker492"/> convert text into numeric features. Every word in the text is assigned a score that indicates how important the word is in that<a id="_idIndexMarker493"/> text. This is done by multiplying <span class="No-Break">two metrics:</span></p>
			<ul>
				<li><strong class="bold">Term Frequency</strong>: How frequently does the word appear in the text sample? This can be normalized by the length of the text in words, as texts that differ in length by a large number can cause skews. The term frequency measures how common a word is in this <span class="No-Break">particular text.</span></li>
				<li><strong class="bold">Inverse Document Frequency</strong>: How frequently does the word appear in the rest of the corpus? First, the number of text samples containing this word is obtained. The total number of samples is divided by this number. Simply put, IDF is the inverse of the fraction of text samples containing the word. IDF measures how common the word is in the rest of <span class="No-Break">the corpus.</span></li>
			</ul>
			<p>For every word in each text, the TF-IDF score is a statistical measure of the importance of the word to the sentence. A word that is common in a text but rare in the rest of the corpus is surely important and a distinguishing characteristic of the text, and will have a high TF-IDF score. Alternately, a word that is very common in the corpus (that is, present in nearly all text samples) will not be a distinguishing one – it will have a low <span class="No-Break">TF-IDF score.</span></p>
			<p>In order to convert the text into a vector, we first calculate the TF-IDF score of each word in each text. Then, we replace the word with a sequence of TF-IDF scores corresponding to the words. The <strong class="source-inline">scikit-learn</strong> library provides us with an implementation of TF-IDF vectorization out of <span class="No-Break">the box.</span></p>
			<p>Note a fine nuance here: the goal of our experiment is to build a model for bot detection that can be used to classify new text as being generated by bots or not. Thus, when we are training, we have no idea about the test data that will come in the future. To ensure that we simulate this, we will do the TF-IDF score calculation over only the training data. When we vectorize the test data, we will simply use the calculated scores as <span class="No-Break">a lookup:</span></p>
			<pre class="source-code">
from sklearn.feature_extraction.text import TfidfVectorizer
tf_idf = TfidfVectorizer()
X_train_TFIDF = tf_idf.fit_transform(X_train)
X_test_TFIDF = tf_idf.transform(X_test)</pre>
			<p>You can manually inspect a few samples from the generated list. What do they <span class="No-Break">look like?</span></p>
			<p>Now that we have the<a id="_idIndexMarker494"/> feature vectors, we can use them to train the classification models. The overall procedure remains the same: initialize a model, fit a model on the training data, and evaluate it on the testing data. The MLP example is shown here; however, you could replace this with any of the models <span class="No-Break">we discussed:</span></p>
			<pre class="source-code">
from sklearn.neural_network import MLPClassifier
model = MLPClassifier(hidden_layer_sizes = (300, 200, 100),max_iter = 100,activation = 'relu',solver = 'adam',random_state = 123)
model.fit(X_train_TFIDF, Y_train)
Y_predicted = model.predict(X_test_TFIDF)
print(evaluate_model(Y_test, Y_pred))</pre>
			<p>How does the performance of this model compare to the performance of the same model with handcrafted features? How about the performance of the <span class="No-Break">other models?</span></p>
			<h3>Word embeddings</h3>
			<p>The TF-IDF approach is <a id="_idIndexMarker495"/>considered to be what we call a <em class="italic">bag of words</em> approach in machine learning terms. Each word is scored<a id="_idIndexMarker496"/> based on its presence, irrespective of the order in which it appears. Word embeddings are numeric representations of words assigned such that words that are similar in meaning have similar embeddings – the numeric representations are close to each other in the feature space. The most fundamental technique used to to generate<a id="_idIndexMarker497"/> word embeddings is <span class="No-Break">called </span><span class="No-Break"><strong class="bold">Word2Vec</strong></span><span class="No-Break">.</span></p>
			<p>Word2Vec embeddings are produced by a shallow neural network. Recall that the last layer of a classification model is a sigmoid or softmax layer for producing an output probability distribution. This softmax layer operates on the features it receives from the pre-final layer – these features can be treated as high-dimensional representations of the input. If we chop off the last layer, the neural network without the classification layer can be used to extract <span class="No-Break">these embeddings.</span></p>
			<p>Word2Vec can work in one of <span class="No-Break">two ways:</span></p>
			<ul>
				<li><strong class="bold">Continuous Bag of Words</strong>: A neural network model is trained to predict the next word in a sentence. Input sentences <a id="_idIndexMarker498"/>are broken down to <a id="_idIndexMarker499"/>generate training examples. For example, if the text corpus contains the sentence <em class="italic">I went to walk the dog</em>, then <strong class="source-inline">X</strong> = <em class="italic">I went to walk the</em> and <strong class="source-inline">Y</strong> = <em class="italic">dog</em> would be one <span class="No-Break">training example.</span></li>
				<li><strong class="bold">Skip-Gram</strong>: This is the<a id="_idIndexMarker500"/> more widely used technique. Instead of <a id="_idIndexMarker501"/>predicting the target word, we train a <a id="_idIndexMarker502"/>model to predict the surrounding words. For example, if the text corpus contains the sentence <em class="italic">I went to walk the dog</em>, then our input would be <em class="italic">walk</em> and the output would be a prediction (or probabilistic prediction) of the surrounding two or more words. Because of this design, the model learns to generate similar embeddings for similar words. After the model is trained, we can pass the word of interest as an input, and use the features of the final layer as <span class="No-Break">our embedding.</span></li>
			</ul>
			<p>Note that while this is still a<a id="_idIndexMarker503"/> classification task, it is not<a id="_idIndexMarker504"/> supervised learning. Rather, it is a self-supervised approach. We have no ground truth, but by framing the problem uniquely, we generate our own <span class="No-Break">ground truth.</span></p>
			<p>We will now build our word embedding model using the <strong class="source-inline">gensim</strong> Python library. We will fit the model on our training data, and then vectorize each sentence using the embeddings. After we have the vectors, we can fit and evaluate <span class="No-Break">the models.</span></p>
			<p>First, we fit the model on training data. Because of the way Word2Vec operates, we need to combine our texts into a list of sentences and then tokenize it <span class="No-Break">into words:</span></p>
			<pre class="source-code">
import nltk
nltk.download('punkt')
corpus = []
for x in X_train:
  # Split into sentences
  sentences_tokens = nltk.sent_tokenize(x)
  # Split each sentence into words
  word_tokens = [nltk.word_tokenize(sent) for sent in sentences_tokens]
  # Add to corpus
  corpus = corpus + word_tokens</pre>
			<p>Now, we can fit the<a id="_idIndexMarker505"/> embedding model. By passing in the <strong class="source-inline">vector_size</strong> parameter, we control the size of the generated embedding. The<a id="_idIndexMarker506"/> larger the size, the more the expressive the power of <span class="No-Break">the embeddings:</span></p>
			<pre class="source-code">
from gensim.models import Word2Vec
model = Word2Vec(corpus, min_count=1, vector_size = 30)</pre>
			<p>We now have the embedding model and can start using it to tokenize the text. Here, we have two strategies. One strategy is that we can calculate the embedding for all the words in the text and simply average them to find the mean embedding for the text. Here’s how we would <span class="No-Break">do this:</span></p>
			<pre class="source-code">
X_train_vector_mean = []
for x in X_train:
  # Create a 30-element vector with all zeroes
  vector = [0 for _ in range(30)]
  # Create a vector for out-of-vocab words
  oov = [0 for _ in range(30)]
  words = x.split(' ')
  for word in words:
    if word in model.wv.vocab:
      # Word is present in the vocab
      vector = np.sum([vector, model[word]], axis = 0)
    else:
      # Out of Vocabulary
      vector = np.sum([vector, oov], axis = 0)
  # Calculate the mean
  mean_vector = vector / len(words)
  X_train_vector_mean.append(mean_vector)</pre>
			<p>The <strong class="source-inline">X_train_vector_mean</strong> array now holds an embedding representation for each text in our corpus. The same process<a id="_idIndexMarker507"/> can be repeated to generate the feature set with <span class="No-Break">test data.</span></p>
			<p>The second strategy is, instead <a id="_idIndexMarker508"/>of averaging the vectors, we append them one after the other. This retains more expressive power as it takes into account the order of words in the sentence. However, each text will have a different length and we require a fixed-size vector. Therefore, we take only a fixed number of words from the text and concatenate <span class="No-Break">their embeddings.</span></p>
			<p>Here, we set the maximum number of words to be <strong class="source-inline">40</strong>. If a text has more than 40 words, we will consider only the first 40. If it has less than 40 words, we will consider all of the words and pad the remaining elements of the vector <span class="No-Break">with zeros:</span></p>
			<pre class="source-code">
X_train_vector_appended = []
max_words = 40
for x in X_train:
  words = x.split(' ')
  num_words = max(max_words, len(words))
  feature_vector = []
  for word in words[:num_words]:
    if word in model.wv.vocab:
      # Word is present in the vocab
      vector = np.sum([vector, model[word]], axis = 0)
    else:
      # Out of Vocabulary
      vector = np.sum([vector, oov], axis = 0)
    feature_vector = feature_vector + vector
  if num_words &lt; max_words:
    pads = [0 for _ in range(30*(max_words-num_words))]
    feature_vector = feature_vector + pads
  X_train_vector_appended.append(feature_vector)</pre>
			<p>The same code snippet<a id="_idIndexMarker509"/> can be repeated with the test data as <a id="_idIndexMarker510"/>well. Remember that the approach you use (averaging or appending) has to be consistent across training and <span class="No-Break">testing data.</span></p>
			<p>Now that the features are ready, we train and evaluate the model as usual. Here’s how you would do it with an MLP; this is easily extensible to other models we <span class="No-Break">have seen:</span></p>
			<pre class="source-code">
from sklearn.neural_network import MLPClassifier
model = MLPClassifier(hidden_layer_sizes = (1000, 700, 500, 200),max_iter = 100,activation = 'relu',solver = 'adam',random_state = 123)
model.fit(X_train_vector_appended, Y_train)
Y_predicted = model.predict(X_test_vector_appended)
print(evaluate_model(Y_test, Y_pred))</pre>
			<p>Note that, here, the dimensions of the hidden layers we passed to the model are different from before. In the very first example with hand-crafted features, our feature vector was only three-dimensional. However, in this instance, every text instance will be represented by 40 words, and each word represented by a 30-dimensional embedding, meaning that the feature vector has 1,200 elements. The higher number of neurons in the hidden layer helps handle the high-dimensional <span class="No-Break">feature space.</span></p>
			<p>As an exercise, you are <a id="_idIndexMarker511"/>encouraged to experiment with three changes and check whether there is an improvement in the <span class="No-Break">model </span><span class="No-Break"><a id="_idIndexMarker512"/></span><span class="No-Break">performance:</span></p>
			<ul>
				<li>The size of the word embeddings, which has been set to <strong class="source-inline">30</strong> for now. What happens to the model performance as you increase or decrease <span class="No-Break">this number?</span></li>
				<li>The number of words has been chosen as 0. What happens if this is reduced <span class="No-Break">or increased?</span></li>
				<li>Using the MLP, how does the model performance change as you vary the number <span class="No-Break">of layers?</span></li>
				<li>Combine word embeddings and TF-IDF. Instead of a simple average, calculate a weighted average where the embedding for each word is weighted by the TF-IDF score. This will ensure that more important words influence the average more. How does this affect <span class="No-Break">model performance?</span></li>
			</ul>
			<h1 id="_idParaDest-103">Transformer methods for detecting automated text</h1>
			<p>In the previous sections, we have<a id="_idIndexMarker513"/> used traditional hand-crafted features, automated bag of words features, as well as embedding<a id="_idIndexMarker514"/> representations for text classification. We saw the power of BERT as a language model in the previous chapter. While describing BERT, we referenced that the embeddings generated by BERT can be used for downstream classification tasks. In this section, we will extract BERT embeddings for our <span class="No-Break">classification task.</span></p>
			<p>The embeddings generated by BERT are different from those generated by the Word2Vec model. Recall that in BERT, we use the masked language model and a transformer-based architecture based on attention. This means that the embedding of a word depends on the context in which it occurs; based on the surrounding words, BERT knows which other words to pay attention to and generate <span class="No-Break">the embedding.</span></p>
			<p>In traditional word embeddings, a word will have the same embedding, irrespective of the context. The word <em class="italic">match</em> will have the same embedding in the sentence <em class="italic">They were a perfect match!</em> and <em class="italic">I lit a match last night</em>. BERT, on the other hand, conditions the embeddings based on context. The word <em class="italic">match</em> would have different embeddings in these <span class="No-Break">two sentences.</span></p>
			<p>Recall that we have already <a id="_idIndexMarker515"/>used BERT once, for malware detection. There are two major differences in how we use it now versus when<a id="_idIndexMarker516"/> we implemented it for <span class="No-Break">malware detection:</span></p>
			<ul>
				<li>Previously, we used BERT in the fine-tuning mode. This means that we used the entire transformer architecture initialized with pretrained weights, added a neural network on top of it, and trained the whole model end to end. The pretrained model enabled learning sequence features, and the fine-tuning helped adapt it to the specific task. However, now we will use BERT only as a feature extractor. We will load a pretrained model, run the sentence through it, and use the pre-final layer to construct <span class="No-Break">our features.</span></li>
				<li>In the previous chapter, we used TensorFlow for implementing BERT. Now, we will use PyTorch, a deep learning framework developed by researchers from Facebook. This provides a much more intuitive, straightforward, and understandable interface to design and run deep neural networks. It also has a <strong class="source-inline">transformers</strong> library, which provides easy implementations of all <span class="No-Break">pretrained models.</span></li>
			</ul>
			<p>First, we will initialize the BERT model and set it to evaluation mode. In the evaluation mode, there is no learning, just inferencing. Therefore, we need only the forward pass and <span class="No-Break">no backpropagation:</span></p>
			<pre class="source-code">
import torch
from pytorch_transformers import BertTokenizer
from pytorch_transformers import BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base uncased',
output_hidden_states=True)
model.eval()</pre>
			<p>We will now prepare our data in the format needed by BERT. This includes adding the two special tokens to indicate <a id="_idIndexMarker517"/>the start and separation. Then, we will run the model in inference mode to obtain the embeddings (hidden states). Recall that when we used Word2Vec embeddings, we averaged the<a id="_idIndexMarker518"/> embeddings for each word. In the case of BERT embeddings, we have <span class="No-Break">multiple choices:</span></p>
			<ul>
				<li>Use just the last hidden state as <span class="No-Break">the embedding:</span><pre class="source-code">
X_train_BERT = []</pre><pre class="source-code">
for x in X_train:</pre><pre class="source-code">
  # Add CLS and SEP</pre><pre class="source-code">
  marked_text = "[CLS] " + x + " [SEP]"</pre><pre class="source-code">
  # Split the sentence into tokens.</pre><pre class="source-code">
  tokenized_text = tokenizer.tokenize(marked_text)</pre><pre class="source-code">
  # Map the token strings to their vocabulary indices.</pre><pre class="source-code">
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</pre><pre class="source-code">
  tokens_tensor = torch.tensor([indexed_tokens])</pre><pre class="source-code">
  with torch.no_grad():</pre><pre class="source-code">
    outputs = model(tokens_tensor)</pre><pre class="source-code">
    feature_vector = outputs[0]</pre><pre class="source-code">
  X_train_BERT.append(feature_vector)</pre></li>
				<li>Use the sum of all hidden states as <span class="No-Break">the embedding:</span><pre class="source-code">
X_train_BERT = []</pre><pre class="source-code">
for x in X_train:</pre><pre class="source-code">
  # Add CLS and SEP</pre><pre class="source-code">
  marked_text = "[CLS] " + x + " [SEP]"</pre><pre class="source-code">
  # Split the sentence into tokens.</pre><pre class="source-code">
  tokenized_text = tokenizer.tokenize(marked_text)</pre><pre class="source-code">
  # Map the token strings to their vocabulary indeces.</pre><pre class="source-code">
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</pre><pre class="source-code">
  tokens_tensor = torch.tensor([indexed_tokens])</pre><pre class="source-code">
  with torch.no_grad():</pre><pre class="source-code">
    outputs = model(tokens_tensor)</pre><pre class="source-code">
    hidden_states = outputs[2]</pre><pre class="source-code">
    feature_vector = torch.stack(hidden_states).sum(0)</pre><pre class="source-code">
  X_train_BERT.append(feature_vector)</pre></li>
				<li>Use the sum of <a id="_idIndexMarker519"/>the last four layers <a id="_idIndexMarker520"/>as <span class="No-Break">an embedding:</span><pre class="source-code">
X_train_BERT = []</pre><pre class="source-code">
for x in X_train:</pre><pre class="source-code">
  # Add CLS and SEP</pre><pre class="source-code">
  marked_text = "[CLS] " + x + " [SEP]"</pre><pre class="source-code">
  # Split the sentence into tokens.</pre><pre class="source-code">
  tokenized_text = tokenizer.tokenize(marked_text)</pre><pre class="source-code">
  # Map the token strings to their vocabulary indeces.</pre><pre class="source-code">
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</pre><pre class="source-code">
  tokens_tensor = torch.tensor([indexed_tokens])</pre><pre class="source-code">
  with torch.no_grad():</pre><pre class="source-code">
    outputs = model(tokens_tensor)</pre><pre class="source-code">
    hidden_states = outputs[2]</pre><pre class="source-code">
    feature_vector = torch.stack(hidden_states[-4:]).sum(0)</pre><pre class="source-code">
  X_train_BERT.append(feature_vector)</pre></li>
				<li>Concatenate the <a id="_idIndexMarker521"/>last four layers and <a id="_idIndexMarker522"/>use that as <span class="No-Break">the embedding:</span><pre class="source-code">
X_train_BERT = []</pre><pre class="source-code">
for x in X_train:</pre><pre class="source-code">
  # Add CLS and SEP</pre><pre class="source-code">
  marked_text = "[CLS] " + x + " [SEP]"</pre><pre class="source-code">
  # Split the sentence into tokens.</pre><pre class="source-code">
  tokenized_text = tokenizer.tokenize(marked_text)</pre><pre class="source-code">
  # Map the token strings to their vocabulary indeces.</pre><pre class="source-code">
  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)</pre><pre class="source-code">
  tokens_tensor = torch.tensor([indexed_tokens])</pre><pre class="source-code">
  with torch.no_grad():</pre><pre class="source-code">
    outputs = model(tokens_tensor)</pre><pre class="source-code">
    hidden_states = outputs[2]</pre><pre class="source-code">
    feature_vector = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)</pre><pre class="source-code">
  X_train_BERT.append(feature_vector)</pre></li>
			</ul>
			<p>Once we have the BERT features, we train and evaluate the model using our usual methodology. We will show an example of an MLP here, but the same process can be repeated for all <span class="No-Break">the </span><span class="No-Break"><a id="_idIndexMarker523"/></span><span class="No-Break">classifiers:</span></p>
			<pre class="source-code">
from sklearn.neural_network import MLPClassifier
model = MLPClassifier(hidden_layer_sizes = (1000, 700, 500, 200),max_iter = 100,activation = 'relu',solver = 'adam',random_state = 123)
model.fit(X_train_BERT, Y_train)
Y_predicted = model.predict(X_test_BERT)
print(evaluate_model(Y_test, Y_pred))</pre>
			<p>This completes our<a id="_idIndexMarker524"/> analysis of how transformers can be used to detect <span class="No-Break">machine-generated text.</span></p>
			<h2 id="_idParaDest-104">Compare and contrast</h2>
			<p>By now, we have explored <a id="_idIndexMarker525"/>several techniques for detecting bot-generated news. Here’s a list of all <span class="No-Break">of them:</span></p>
			<ul>
				<li>Hand-crafted features such as function words, punctuation words, and automated <span class="No-Break">readability index</span></li>
				<li>TF-IDF scores <span class="No-Break">for words</span></li>
				<li><span class="No-Break">Word2Vec embeddings:</span><ul><li>Averaged across the text for <span class="No-Break">all words</span></li><li>Concatenated for each word across <span class="No-Break">the text</span></li></ul></li>
				<li><span class="No-Break">BERT embeddings:</span><ul><li>Using only the last <span class="No-Break">hidden state</span></li><li>Using the sum of all <span class="No-Break">hidden states</span></li><li>Using the sum of the last four <span class="No-Break">hidden states</span></li><li>Using the concatenation of the last four <span class="No-Break">hidden states</span></li></ul></li>
			</ul>
			<p>We can see that we have eight feature sets at our disposal. Additionally, we experimented with four <a id="_idIndexMarker526"/>different models: random forests, logistic regression, SVM, and deep neural network (MLP). This means that we have a total of 32 configurations (feature set <strong class="source-inline">x</strong> model) that we can use for building a classifier to detect bot-generated <span class="No-Break">fake news.</span></p>
			<p>I leave it up to you to construct this 8x4 matrix and determine which is the best approach among all <span class="No-Break">of them!</span></p>
			<h1 id="_idParaDest-105">Summary</h1>
			<p>In this chapter, we described approaches and techniques for detecting bot-generated fake news. With the rising prowess of artificial intelligence and the widespread availability of language models, attackers are using automated text generation to run bots on social media. These sock-puppet accounts can generate real-looking responses, posts, and, as we saw, even news-style articles. Data scientists in the security space, particularly those working in the social media domain, will often be up against attackers who leverage AI to spew out text and carpet-bomb <span class="No-Break">a platform.</span></p>
			<p>This chapter aims to equip practitioners against such adversaries. We began by understanding how text generation exactly works and created our own dataset for machine learning experiments. We then used a variety of features (hand-crafted, TF-IDF, and word embeddings) to detect the bot-generated text. Finally, we used contextual embeddings to build <span class="No-Break">improved mechanisms.</span></p>
			<p>In the next chapter, we will study the problem of authorship attribution and obfuscation and the social and technical issues <span class="No-Break">surrounding it.</span></p>
		</div>
	</body></html>