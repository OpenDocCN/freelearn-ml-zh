<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Natural Language Processing</h1>
                </header>
            
            <article>
                
<p>How fast has the world been changing? Well, technology and data have been changing just as quickly. With the advent of the internet and social media, our entire outlook on data has changed. Initially, the scope of most data analytics revolved around structured data. However, due to so much unstructured data being pumped in through the internet and social media, the spectrum of analytics has broadened. Large amounts of text data, images, sound, and video data are being generated every second. They contain lots of information that needs to be synthesized for business. Natural language processing is a technique through which we enable a machine to understand text or speech. Although unstructured data has a wide range, the scope of this chapter will be to expose you to text analytics.</p>
<p>Structured data is typically made up of fixed observations and fixed columns set up in relational databases or in a spreadsheet, whereas unstructured data doesn't have any structure, and it can't be set up in a relational database; rather, it needs a NoSQL database, example, video, text, and so on.</p>
<p>In this chapter, you will learn about the following topics:</p>
<ul>
<li>The document term matrix</li>
<li>Different approaches to looking at text</li>
<li>Sentiment analysis</li>
<li>Topic modeling</li>
<li>The Bayesian technique</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Text corpus</h1>
                </header>
            
            <article>
                
<p>A text corpus is text data that forms out of a single document or group of documents and can come from any language, such as English, German, Hindi, and so on. In today's world, most of the textual data flows from social media, such as Facebook, Twitter, blogging sites, and other platforms. Mobile applications have now been added to the list of such sources. The larger size of a corpus, which is called <strong>corpora,</strong> makes the analytics more accurate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentences</h1>
                </header>
            
            <article>
                
<p>A corpus can be broken into units, which are called <strong>sentences</strong>. Sentences hold the meaning and context of the corpus, once we combine them together. Sentence formation takes place with the help of parts of speech. Every sentence is separated from other sentences by a delimiter, such as a period, which we can make use of to break it up further. This is called <strong>sentence tokenization</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Words</h1>
                </header>
            
            <article>
                
<p>Words are the smallest unit of corpuses and take the shape of sentences when we put them in order by following the parts of speech. When we break down the sentences into words, it is called <strong>word tokenization</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bags of words</h1>
                </header>
            
            <article>
                
<p>When we have text as input data, we can't go ahead and work with raw text. Hence, it's imperative for that text input data to get converted into numbers or vectors of numbers, in order to make it usable for a number of algorithms.</p>
<p>A bag of words model is one of the ways to make the text usable for the algorithms. Essentially, it is a representation of text that works on the occurrence of words in the document. It has nothing to do with the structure, order, and location; this model only looks for the count of the words as a feature.</p>
<p>The thought process behind this model is that having similar content means having a similar document.</p>
<p>The different steps to be taken in the bag of words model are as follows:</p>
<ul>
<li><strong>Building the corpus</strong>: In this step, the documents are collected and combined together to form a corpus. For example, the famous song from the TV series Friends has been used here as a corpus:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><em>I will be there for you<br/>
When the rain starts to pour<br/>
I will be there for you<br/>
Like I have been there before<br/>
I will be there for you</em></p>
<p style="padding-left: 60px">Let's consider each line of this song as a separate document.<em><br/></em></p>
<ul>
<li><strong>Vocabulary building</strong>: In this step, we figure out the unique words in the corpus and create a list of them:
<ul>
<li>I</li>
<li>will</li>
<li>be</li>
<li>there</li>
<li>for</li>
<li>you</li>
<li>when</li>
<li>the</li>
<li>rain</li>
<li>starts</li>
<li>to</li>
<li>pour</li>
<li>like</li>
<li>have</li>
<li>been</li>
<li>before</li>
</ul>
</li>
<li><strong>Document vector creation</strong>: Now, it's time to convert each document of text into a vector.</li>
</ul>
<p>The simple way to do this is through a Boolean route. This means that raw text will be transformed into a document vector, with the help of the presence/absence of that text in the respective document.</p>
<p>For example, if the first line of the song is <span>turned into a document containing</span> <em>I will be there for you</em>, then the document vector will turn out as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p> </p>
</td>
<td>
<p><strong>Document vector</strong></p>
</td>
</tr>
<tr>
<td>
<p>I</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>will</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>be</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>there</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>for</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>you</p>
</td>
<td>
<p>1</p>
</td>
</tr>
<tr>
<td>
<p>when</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>the</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>rain</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>starts</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>to</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>pour</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>like</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>have</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>been</p>
</td>
<td>
<p>0</p>
</td>
</tr>
<tr>
<td>
<p>before</p>
</td>
<td>
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>All the words that are present in the document are marked as 1, and the rest are marked as 0.</p>
<p>Hence, the document vector for the first sentence is <em>[1,1,1,1,1,1,0,0,0,0,0,0,0,0,0,0]</em>.</p>
<p>Similarly, the document vector for the second sentence is <em>[0,0,0,0,0,0,1,1,1,1,1,1,0,0,0,0]</em>.</p>
<p>As the size of the corpus continues to increase, the number of zeros in the document vector will rise, as well. As a result of that, it induces sparsity in the vector and it becomes a sparse vector. Computing a sparse vector becomes really challenging for various algorithms. <span>Data cleansing is one of the ways to counter it, to some extent</span>:</p>
<ul>
<li><strong>Cleansing the text</strong>: This would involve transforming all of the corpus into a single case (either upper (preferably) or lower). The punctuation must be taken out of the corpus. Stemming, which means finding the root words of the text, can be incorporated, and will be able to reduce the unique words in the corpus. Also, removal of stop words, such as <em>is</em><span> and</span> <em>of</em>, <span>might be able to abate the pain of sparsity.</span></li>
</ul>
<ul>
<li><strong>Count vector</strong>: There is another way to create the document vector, with the help of the frequency of the words appearing in the document. Let's suppose that there is a corpus comprised of N documents and T tokens (words) have been extracted. These T tokens will form our dictionary. Hence, the dimension of the count vector matrix will turn out to be N X T. Every row contains the frequency of tokens (words) in that respective document comprising the dictionary.</li>
</ul>
<p>For example, let's suppose that we have three documents:</p>
<ul>
<li><strong>N1</strong>: Count vector has got count in it</li>
<li><strong>N2</strong>: Is count vector better than the Boolean way of creating feature vector?</li>
<li><strong>N3</strong>: Creation of feature vector is very important</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>After removing <kbd>stopwords</kbd>, the count vector matrix turns out like the following table:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 3.27548%">
<p> </p>
</td>
<td style="width: 10%">
<p><strong>count</strong></p>
</td>
<td style="width: 7%">
<p><strong>vector</strong></p>
</td>
<td style="width: 3%">
<p><strong>got</strong></p>
</td>
<td style="width: 4%">
<p><strong>it</strong></p>
</td>
<td style="width: 10%">
<p><strong>better</strong></p>
</td>
<td style="width: 5%">
<p><strong>than</strong></p>
</td>
<td style="width: 10%">
<p><strong>Boolean</strong></p>
</td>
<td style="width: 4%">
<p><strong>way</strong></p>
</td>
<td style="width: 10%">
<p><strong>creating</strong></p>
</td>
<td style="width: 8%">
<p><strong>feature</strong></p>
</td>
<td style="width: 10%">
<p><strong>creation</strong></p>
</td>
<td style="width: 12%">
<p><strong>important</strong></p>
</td>
</tr>
<tr>
<td style="width: 3.27548%">
<p>N1</p>
</td>
<td style="width: 10%">
<p>2</p>
</td>
<td style="width: 7%">
<p>1</p>
</td>
<td style="width: 3%">
<p>1</p>
</td>
<td style="width: 4%">
<p>1</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 5%">
<p>0</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 4%">
<p>0</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 8%">
<p>0</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 12%">
<p>0</p>
</td>
</tr>
<tr>
<td style="width: 3.27548%">
<p>N2</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 7%">
<p>2</p>
</td>
<td style="width: 3%">
<p>0</p>
</td>
<td style="width: 4%">
<p>0</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 5%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 4%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 8%">
<p>1</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 12%">
<p>0</p>
</td>
</tr>
<tr>
<td style="width: 3.27548%">
<p>N3</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 7%">
<p>1</p>
</td>
<td style="width: 3%">
<p>0</p>
</td>
<td style="width: 4%">
<p>0</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 5%">
<p>0</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 4%">
<p>0</p>
</td>
<td style="width: 10%">
<p>0</p>
</td>
<td style="width: 8%">
<p>1</p>
</td>
<td style="width: 10%">
<p>1</p>
</td>
<td style="width: 12%">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now, take a look at the matrix dimension carefully; since <em>N=3</em> and <em>T=12</em>, that makes this a matrix of 3 x 12.</p>
<p>We will look at how the matrix formation has taken place. For document N1, the number of times the count has occurred in it is 2, the number of times the vector has come is 1, and so on. Taking these frequencies, we enter these values. A similar process has been completed for the other two documents, as well.</p>
<p>However, this has a drawback. A highly frequent word might start to dominate the document, and the corpus, too, which will result in having limited information extracted out of the features. To counter this, <span><strong>term frequency inverse-document frequency</strong> (</span><strong>TF-IDF</strong>) has been introduced.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TF-IDF</h1>
                </header>
            
            <article>
                
<p>As we understood the limitation of count vectorization that a highly frequent word might spoil the party. Hence, the idea is to penalize the frequent words occurring in most of the documents by assigning them a lower weight and increasing the weight of the words that appear in a subset of documents. This is the principle upon which TF-IDF works.</p>
<p>TF-IDF is a measure of how important a term is with respect to a document and the entire corpus (collection of documents):</p>
<p class="CDPAlignCenter CDPAlign"><em>TF-IDF(term) = TF(term)* IDF(term)</em></p>
<p><strong>Term frequency</strong> (<strong>TF</strong>) is the frequency of the word appearing in the document out of all the words in the same document. For example, if there are 1,000 words in a document and we have to find out the <em>TF</em> of a word <em>NLP</em> that has appeared 50 times in that very document, we use the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>TF(NLP)= 50/1000=0.05</em></p>
<p class="mce-root"/>
<p>Hence, we can conclude the following:</p>
<p class="CDPAlignCenter CDPAlign"><em>TF(term) = Number of times the term appears in the document/total number of terms in the document</em></p>
<p>In the preceding example , comprised of three documents, <em>N1</em>, <em>N2</em>, and <em>N3</em>, if the <em>TF</em> of the term <em>count</em> in the document <em>N1</em> needs to be found, it will turn out to be like the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><em>TF(count) N1= 2/ (2+1+1+1) = 2/5 = 0.4</em></p>
<p>It indicates the contribution of words to the document.</p>
<p>However,  IDF is an indicator of how significant this term is for the entire corpus:</p>
<p class="CDPAlignCenter CDPAlign"><em>IDF("count") = log(Total number of documents/Number of documents containing the term "count")</em></p>
<p class="CDPAlignCenter CDPAlign"><em>IDF("count") = log(3/2)= 0.17</em></p>
<p>Now, let's calculate the IDF for the term <em>vector</em>:</p>
<p class="CDPAlignCenter CDPAlign"><em>IDF("vector")=log(3/3)= 0</em></p>
<p>How do we interpret this? It implies that if the same word has appeared in all of the documents, then it is not relevant to a particular document. But, if the word appears only in a subset of documents, this means that it holds some relevance to those documents in which it exists.</p>
<p>Let's calculate the TF-IDF for <em>count</em> and <em>vector</em>, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><em>TF-IDF(count) for Document N1= TF(count)*IDF(count)= 0.4 * 0.17 = 0.068</em></p>
<p class="CDPAlignCenter CDPAlign"><em>TF-IDF(vector) for Document N1 = TF(vector)* IDF(vector)= (1/5)*0 = 0</em></p>
<p>It is quite evident that, since it assigns more weight to the <em>count</em> in <em>N1</em>, it is more important than the <em>vector</em>. The higher the weight value, the rarer the term. The smaller the weight, the more common the term. Search engines makes use of TF-IDF to retrieve the relevant documents pertaining to a query.</p>
<p class="mce-root">Now, we will look at how to execute the count vectorizer and TF-IDF vectorizer in Python.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Executing the count vectorizer</h1>
                </header>
            
            <article>
                
<p>The following are the steps for executing the <kbd>CountVectorizer</kbd>:</p>
<ol>
<li>Import the library required for the count vectorizer:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.feature_extraction.text import CountVectorizer</pre>
<ol start="2">
<li>Make a list of the text:</li>
</ol>
<pre style="padding-left: 60px">text = [" Machine translation automatically translate text from one human language to another text"]</pre>
<ol start="3">
<li class="mce-root">Tokenize the list of the text and build the vocabulary:</li>
</ol>
<pre style="padding-left: 60px">vectorizer.fit(text)</pre>
<p style="padding-left: 60px">You will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-562 image-border" src="assets/c20b2298-018c-4ca4-8e27-761cbdd6be03.png" style="width:34.25em;height:6.83em;"/></p>
<ol start="4">
<li>Let's take a look at the vocabulary that was created:</li>
</ol>
<pre style="padding-left: 60px">print(vectorizer.vocabulary_)</pre>
<p style="padding-left: 60px">We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b7b02501-31a1-4008-a878-46fddfdfed4d.png"/></p>
<ol start="5">
<li>Now, we have to encode it, as follows:</li>
</ol>
<pre style="padding-left: 60px">vector = vectorizer.transform(text)</pre>
<ol start="6">
<li>Let's get a summary of the vector and find out the term matrix:</li>
</ol>
<pre style="padding-left: 60px">print(type(vector))<br/>print(vector.toarray())</pre>
<p style="padding-left: 60px">We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-579 image-border" src="assets/4ecef6c4-0f67-42a4-ad3d-11ebebe2c372.png" style="width:18.42em;height:2.75em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Executing TF-IDF in Python</h1>
                </header>
            
            <article>
                
<p>The following are the steps for executing TF-IDF in Python:</p>
<ol>
<li>Import the library, as follows:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.feature_extraction.text import TfidfVectorizer</pre>
<ol start="2">
<li>Let's make a corpus by adding four documents, as follows:</li>
</ol>
<pre style="padding-left: 60px">corpus = ['First document', 'Second document','Third document','First and second document' ]</pre>
<ol start="3">
<li>Let's set up the vectorizer:</li>
</ol>
<pre style="padding-left: 60px">vectorizer = TfidfVectorizer()</pre>
<ol start="4">
<li>We extract the features out of the text as follows:</li>
</ol>
<pre style="padding-left: 60px">X = vectorizer.fit_transform(corpus)<br/>print(vectorizer.get_feature_names())<br/>print(X.shape)</pre>
<p style="padding-left: 60px">The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-574 image-border" src="assets/3c8fc2b7-1107-4330-8b93-ec961fd601d2.png" style="width:24.08em;height:2.75em;"/></p>
<ol start="5">
<li>Here comes the document term matrix; every list indicates a document:</li>
</ol>
<pre style="padding-left: 60px">X.toarray()</pre>
<p style="padding-left: 60px">We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-567 image-border" src="assets/123a887f-48b6-4607-8985-7aec3b856655.png" style="width:36.33em;height:4.92em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis</h1>
                </header>
            
            <article>
                
<p>Sentiment analysis is one of the application areas of natural language processing. It is widely in use across industries and domains, and there is a big need for it in the industry. Every organization is aiming to focus customers and their needs. Hence, to understand voice and sentiment, the customer turns out to be the prime goal, as knowing the pulse of the customers leads to revenue generation. Nowadays, customers voice their sentiments through Twitter, Facebook, or blogs. It takes some work to refine that textual data and make it consumable. Let's look at how to do it in Python.</p>
<p>Here, verbatims of cinegoers have been taken from IMDB. This is shared on GitHub, too.</p>
<p>We will launch the libraries , as follows:</p>
<pre>import numpy as np<br/> import pandas as pd<br/> import seaborn as sns<br/> import matplotlib.pyplot as plt<br/> sns.set(color_codes=True)<br/> import os<br/> print(os.listdir())</pre>
<p>We will load the dataset, as follows:</p>
<pre>data= pd.read_csv("imdb_master.csv",encoding = "ISO-8859-1")</pre>
<p>Now, let's explore the data and its dimensions:</p>
<pre>print(data.head())<br/>print(data.shape)</pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-565 image-border" src="assets/dc6c039b-041d-4f4e-8f57-1d31af5f0253.png" style="width:33.50em;height:13.25em;"/></p>
<p>We only need two variables, <kbd>review</kbd> and <kbd>label</kbd>, to build the model. We will just keep both of them in the data. A new dataframe has been created , as follows:</p>
<pre>Newdata= data[["review","label"]]<br/>Newdata.shape</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-566 image-border" src="assets/7ede2db9-69bd-4f52-a7a3-e6b5a060f5a6.png" style="width:8.33em;height:2.25em;"/></p>
<p>Now, this is the step where we need to check how many categories are in <kbd>label</kbd>, as we are only interested in keeping the positive and negative ones:</p>
<pre>g= Newdata.groupby("label")<br/>g.count()</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-572 image-border" src="assets/b6923b05-aaf6-4e0f-85bf-ef5b502faf68.png" style="width:7.08em;height:8.67em;"/></p>
<p>Now, it's clear that there are three categories and we will get rid of <kbd>unsup</kbd>, as follows:</p>
<pre>sent=["neg","pos"]<br/><br/>Newdata = Newdata[Newdata.label.isin(sent)]<br/>Newdata.head()</pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2ae43214-b171-45f9-ab0b-10879fc26aef.png" style="width:26.00em;height:11.92em;"/></p>
<p class="mce-root"/>
<p>Our data has now been set up. However, since we got rid of a few rows, we will reset the index of the data, as it sometimes causes some issues:</p>
<pre>print(len(Newdata))<br/>Newdata=Newdata.reset_index(drop=True) Newdata.head()</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2ae43214-b171-45f9-ab0b-10879fc26aef.png" style="width:25.83em;height:11.83em;"/></p>
<p class="mce-root">We are done with it. Now, we will encode the <kbd>label</kbd> <span>variable </span>in order to make it usable for machine learning models. We have to use <kbd>LabelEncode</kbd> for that, as follows:</p>
<pre>from sklearn.preprocessing import LabelEncoder<br/> labelencoder = LabelEncoder()<br/> Newdata["label"] = labelencoder.fit_transform(Newdata["label"])</pre>
<p>We have to work on cleansing part of the data, in order to make it clean and standard, as follows:</p>
<pre>Newdata["Clean_review"]= Newdata['review'].str.replace("[^a-zA-Z#]", " ")<br/><br/>Newdata.head()</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5381500b-d813-4742-9ba4-8ac1167130dd.png" style="width:42.92em;height:11.42em;"/></p>
<p>Here, we are trying to get rid of the words that are less than <kbd>3</kbd> in length as the idea is that most of the words that are less than <kbd>3</kbd> in length don't have much of an impact on the meaning:</p>
<pre>Newdata['Clean_review'] = Newdata['Clean_review'].apply(lambda x: ' '.join([w for w in x.split() if len(w)&gt;3]))<br/> Newdata.shape</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-563 image-border" src="assets/e0614fa7-5f4b-408c-97e8-d513ad32aff4.png" style="width:6.58em;height:2.17em;"/></p>
<p>The tokenization of the data can now take place, as follows:</p>
<pre>tokenized_data = Newdata['Clean_review'].apply(lambda x: x.split())<br/> tokenized_data.shape</pre>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-560 image-border" src="assets/2b38f515-be3b-449b-9060-73f9e57224c0.png" style="width:5.67em;height:2.50em;"/></p>
<p>We are making use of stemming, in order to get rid of different variations of the same words. For example, we will look at satisfying, satisfy, and satisfied, as follows:</p>
<pre>from nltk.stem.porter import *<br/> stemmer = PorterStemmer()<br/> tokenized_data = tokenized_data.apply(lambda x: [stemmer.stem(i) for i in x])<br/> tokenized_data.head()</pre>
<p class="CDPAlignLeft CDPAlign">The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-570 image-border" src="assets/b1ae0d82-9419-440d-a8b2-69cdab829bce.png" style="width:27.67em;height:7.42em;"/></p>
<p>After stemming, we have to join the data back, as we are heading towards producing a word cloud:</p>
<pre>for i in range(len(tokenized_data)):<br/> tokenized_data[i] = ' '.join(tokenized_data[i])<br/><br/>tokenized_data.head()</pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a2ef8e36-ba0f-4e13-a0c2-aa610dabf9e0.png" style="width:26.08em;height:6.25em;"/></p>
<p>Here, the tokenized data has been combined with the old <kbd>Newdata</kbd> <span>dataframe:</span></p>
<pre>Newdata["Clean_review2"]= tokenized_data<br/> Newdata.head()</pre>
<p>The following is the output for the preceding code:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-573 image-border" src="assets/f56c6aec-532d-47e5-934e-1653cd4ebd63.png" style="width:52.75em;height:9.33em;"/></p>
<p>A word cloud combining all of the words together has been produced:</p>
<pre>all_words = ' '.join([str(text) for text in Newdata['Clean_review2']])<br/> from wordcloud import WordCloud<br/> wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)<br/> plt.figure(figsize=(10, 7))<br/> plt.imshow(wordcloud, interpolation="bilinear")<br/> plt.axis('off')<br/> plt.show()</pre>
<p>The output can be seen as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f99639e7-85e6-44a0-be40-80d09d359a4a.png" style="width:18.08em;height:11.50em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, we will make a word cloud for negative and positive sentiments separately, as follows:</p>
<ul>
<li>For <kbd>Negative</kbd> sentiments, we will use the following:</li>
</ul>
<pre style="padding-left: 60px">Negative =' '.join([text for text in Newdata['Clean_review2'][Newdata['label'] == 0]])<br/> wordcloud1= WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(Negative)<br/> plt.figure(figsize=(10, 7))<br/> plt.imshow(wordcloud1, interpolation="bilinear")<br/> plt.title("Word Cloud- Negative")<br/> plt.axis('off')<br/> plt.show()</pre>
<p style="padding-left: 60px">The following output shows a <span>word cloud for <kbd>Negative</kbd> sentiments:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/65d91099-624b-46f6-a53d-8f582e57862a.png" style="width:32.50em;height:21.92em;"/></p>
<ul>
<li>We will use the following for <kbd>Positive</kbd> sentiments:</li>
</ul>
<pre style="padding-left: 60px">Positive=' '.join([text for text in Newdata['Clean_review2'][Newdata['label'] == 1]])<br/> wordcloud2 = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(Positive)<br/> plt.figure(figsize=(10, 7))<br/> plt.imshow(wordcloud, interpolation="bilinear")<br/> plt.title("Word Cloud-Positive")<br/> plt.axis('off')<br/> plt.show()</pre>
<p style="padding-left: 60px"><span>The following output shows a </span><span>word cloud for <kbd>Positive</kbd> sentiments:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2f4b243f-f440-4218-8d86-eb80fadd5631.png" style="width:36.33em;height:23.83em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment classification</h1>
                </header>
            
            <article>
                
<p>We will take two approaches to sentiment classification (positive and negative), as follows:</p>
<ul>
<li>TF-IDF</li>
<li>Count vectorization</li>
</ul>
<p>Let's see which one gives us the better result.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TF-IDF feature extraction</h1>
                </header>
            
            <article>
                
<p>The following code will provide us with the TF-IDF feature extraction:</p>
<pre>from sklearn.feature_extraction.text import TfidfVectorizer<br/>tfidf= TfidfVectorizer(max_df=0.9,min_df= 2, max_features=1000,<br/>                        stop_words="english")<br/>tfidfV = tfidf.fit_transform(Newdata['Clean_review2'])<br/><br/>tfidf.vocabulary_</pre>
<p>We will get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-571 image-border" src="assets/2ca570fe-5e99-4234-85b2-42105f2ef38b.png" style="width:9.92em;height:21.58em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Count vectorizer bag of words feature extraction</h1>
                </header>
            
            <article>
                
<p>The following code will show the count vectorizer for a bag of words:</p>
<pre>from sklearn.feature_extraction.text import CountVectorizer<br/> bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')<br/> # bag-of-words<br/> bow = bow_vectorizer.fit_transform(Newdata['Clean_review2'])</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model building count vectorization</h1>
                </header>
            
            <article>
                
<p>For building count vectorization we can split the data into train and test dataset as follows:</p>
<pre>from sklearn.linear_model import LogisticRegression<br/> from sklearn.model_selection import train_test_split<br/> from sklearn.metrics import f1_score,accuracy_score<br/> # splitting data into training and validation set<br/> xtrain, xtest, ytrain, ytest = train_test_split(bow, Newdata['label'], random_state=42, test_size=0.3)<br/> lreg = LogisticRegression()<br/> lreg.fit(xtrain, ytrain) # training the model<br/> prediction = lreg.predict_proba(xtest) # predicting on the validation set<br/> prediction_int = prediction[:,1] &gt;= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0<br/> prediction_int = prediction_int.astype(np.int)<br/> print("F1 Score-",f1_score(ytest, prediction_int))<br/> print("Accuracy-",accuracy_score(ytest,prediction_int))</pre>
<p>We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-564 image-border" src="assets/91c1681e-3872-4632-a10b-20d98e6e2fb8.png" style="width:13.42em;height:2.83em;"/></p>
<p>Here, we attain an accuracy of 84%. Let's see how the TF-IDF approach fares:</p>
<pre>from sklearn.linear_model import LogisticRegression<br/> # splitting data into training and validation set<br/> xtraintf, xtesttf, ytraintf, ytesttf = train_test_split(tfidfV, Newdata['label'], random_state=42, test_size=0.3)<br/> lreg = LogisticRegression()<br/> lreg.fit(xtraintf, ytraintf) # training the model<br/> prediction = lreg.predict_proba(xtesttf) # predicting on the test set<br/> prediction_int = prediction[:,1] &gt;= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0<br/> prediction_int = prediction_int.astype(np.int)<br/> print("F1 Score-",f1_score(ytest, prediction_int))<br/> print("Accuracy-",accuracy_score(ytest,prediction_int))</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-569 image-border" src="assets/b27c588c-06aa-44ec-b149-2012a0477b3d.png" style="width:14.58em;height:3.25em;"/></p>
<p>Here, the accuracy turns out to be 83.8% (a little less than the count vectorizer).</p>
<p>This completes building a model for sentiment classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Topic modeling </h1>
                </header>
            
            <article>
                
<p>Modeling is a methodology that's used to identify a topic and derive hidden patterns exhibited by a text corpus. Topic modeling resembles clustering, as we provide the number of topics as a hyperparameter (similar to the one used in clustering), which happens to be the number of clusters (k-means). Through this, we try to extract the number of topics or texts having some weights assigned to them.</p>
<p>The application of modeling lies in the area of document clustering, dimensionality reduction, information retrieval, and feature selection.</p>
<p>There are multiple ways to perform this, as follows:</p>
<ul>
<li><strong>Latent dirichlet allocation</strong> (<strong>LDA</strong>): It's based on probabilistic graphical models</li>
<li><strong>Latent semantic analysis</strong> (<strong>LSA</strong>): It works on linear algebra (singular value decomposition)</li>
<li><strong>Non-negative matrix factorization</strong>: It's based on linear algebra</li>
</ul>
<p>We will primarily discuss LDA, which is <span>considered </span>the most popular of all. </p>
<p>LDA is a matrix factorization technique that works on an assumption that documents are formed out of a number of topics, and, in turn, topics are formed out of words.</p>
<p>Having read the previous sections, you should be aware that any corpus can be represented as a document-term matrix. The following matrix shows a corpus of <strong>M</strong> documents and a vocabulary size of <strong>N</strong> words that makes an <strong>M x N matrix</strong>. All of the cells in this matrix have the frequency of the words in that particular document:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-578 image-border" src="assets/a70489d5-5907-4d88-a0a1-4f5cbe7d3e8f.png" style="width:67.42em;height:34.75em;"/></p>
<p>This <strong>M x N matrix of Document &amp; Words</strong> gets translated into two matrices by LDA: <strong>M x X matrix of Documents &amp; Topics</strong> and <strong>X x N matrix of Topics &amp; Words</strong>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LDA architecture</h1>
                </header>
            
            <article>
                
<p><span>In the LDA architecture, there are M number of documents having an N number of words, that get processed through the black strip called <strong>LDA</strong>. It delivers <strong>X Topics</strong> with <strong>Cluster of words</strong>. Each topic has psi distribution of words out of topics. Finally, it also comes up with a distribution of topics out of documents, which is denoted by phi.</span></p>
<p><span>The following diagram illustrates LDA:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-576 image-border" src="assets/b9500217-80f6-44c9-b757-32062bde9987.png" style="width:61.83em;height:30.25em;"/></p>
<p><span>With regard to the <strong>Alpha</strong> and <strong>Beta</strong> hyperparameters: alpha represents document-topic concentration and beta represents topic-word concentration. The higher the value of alpha, the more topics we get out of documents. On the other hand, the higher the value of beta, the more words there are in a topic. These can be tweaked based on the domain knowledge.</span></p>
<p>LDA iterates through each word of every document and assigns and adjusts a topic for it. A new topic <em>X</em> is assigned to it, on the basis of the product of two probabilities: <em>p1= (topic t/document d),</em> which means the proportion of the words of a document assigned to topic t, and <em>p2=(word w/topic t),</em> which refers to the proportion of assignments to topic <em>t</em> spread over all the documents, which has the word w associated with it.</p>
<p>With the number of passes, a good distribution of topic-word and topic-documents is attained.</p>
<p>Let's look at how it's executed in Python:</p>
<ol>
<li>In this step, we are loading <kbd>dataset = fetch_20newsgroups</kbd>, which comes from <kbd>sklearn</kbd>:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.datasets import fetch_20newsgroups<br/> dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))<br/> documents = dataset.data</pre>
<ol start="2">
<li>In this step, we will clean the dataset. In order to do that, the <kbd>stopwords</kbd> and <kbd>WordNetLemmatizer</kbd> functions are required. Hence, the relevant libraries are must be loaded, as follows:</li>
</ol>
<pre style="padding-left: 60px">from nltk.corpus import stopwords<br/>from nltk.stem.wordnet import WordNetLemmatizer<br/>import string</pre>
<ol start="3">
<li>Ensure that you have downloaded the following dictionaries:</li>
</ol>
<pre style="padding-left: 60px">import nltk<br/>nltk.download("stopwords")<br/>nltk.download("wordnet")</pre>
<ol start="4">
<li>Here, a <kbd>clean</kbd> <span>function </span>is created to put the words in lowercase. Remove the <kbd>stopwords</kbd> and pick the words that have a length greater than <kbd>3</kbd>. Also, it makes it punctuation-free. Finally, lemmatize it , as follows:</li>
</ol>
<pre style="padding-left: 60px">stop = set(stopwords.words('english'))<br/>punc = set(string.punctuation)<br/>lemma = WordNetLemmatizer()<br/>def clean(doc):<br/>     stopw_free = " ".join([i for i in doc.lower().split() if i not in stop and len(i)&gt;3])<br/>     punc_free = ''.join(ch for ch in stop_free if ch not in punc)<br/>     lemmatized = " ".join(lemma.lemmatize(word) for word in punc_free.split())<br/>     return lemmatized<br/> doc_clean = [clean(doc).split() for doc in documents]</pre>
<ol start="5">
<li>Now, we have to make the document term matrix with the help of the <kbd>gensim</kbd> library. This library will also enable us to carry out LDA:</li>
</ol>
<pre style="padding-left: 60px">import gensim<br/>from gensim import corpora</pre>
<ol start="6">
<li>A document term matrix based on a bag of words is created here:</li>
</ol>
<pre style="padding-left: 60px">corpus = corpora.Dictionary(doc_clean)<br/> doc_term_matrix = [corpus.doc2bow(doc) for doc in doc_clean]</pre>
<ol start="7">
<li>Here, a similar matrix is being created with the help of TF-IDF:</li>
</ol>
<pre style="padding-left: 60px">from gensim import models<br/>tfidf = models.TfidfModel(doc_term_matrix)<br/>corpus_tfidf = tfidf[doc_term_matrix]</pre>
<ol start="8">
<li>Let's set up the model with a TF-IDF matrix. The number of topics has been given as <kbd>10</kbd>:</li>
</ol>
<pre style="padding-left: 60px">lda_model1 = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=corpus, passes=2, workers=2)</pre>
<ol start="9">
<li>Let's take a look at the topic with words:</li>
</ol>
<pre style="padding-left: 60px">print(lda_model1.print_topics(num_topics=5, num_words=5))</pre>
<p style="padding-left: 60px">The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1026 image-border" src="assets/62a149b1-0e00-455a-9feb-f329a77498d1.png" style="width:112.58em;height:5.00em;"/></p>
<ol start="10">
<li>A similar exercise will be done for the bag of words; later, we will compare it:</li>
</ol>
<pre style="padding-left: 60px">lda_model2 = gensim.models.LdaMulticore(doc_term_matrix, num_topics=10, id2word=corpus, passes=2, workers=2)<br/><br/>print(lda_model2.print_topics(num_topics=5, num_words=5))</pre>
<p style="padding-left: 60px">We get the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-577 image-border" src="assets/ae422b0d-be41-4c91-8387-ba7e26aa1c15.png" style="width:112.58em;height:5.25em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Log perplexity is a measure of how good an LDA model is. The lower the value of the perplexity, the better the model is:</p>
<pre>print("lda_model 1- Perplexity:-",lda_model.log_perplexity(corpus_tfidf))<br/>print("lda_model 2- Perplexity:-",lda_model2.log_perplexity(doc_term_matrix))</pre>
<p>The output for the log perplexity is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-561 image-border" src="assets/522d8ec6-6359-444e-aed4-e2b7c52528cd.png" style="width:20.92em;height:2.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Visualizing the LDA</h1>
                </header>
            
            <article>
                
<p>In order to visualize the data, we can use the following code:</p>
<pre>import pyLDAvis<br/>import pyLDAvis.gensim<br/>import matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>pyLDAvis.enable_notebook()<br/>visual1= pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, corpus)<br/>visual1</pre>
<p>The output will be as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-973 image-border" src="assets/06c44009-bf6c-47e8-9b71-83eeeab8da61.png" style="width:57.42em;height:25.17em;"/></p>
<p><span>We can enable the notebook here, as follows:</span></p>
<pre><span>pyLDAvis.enable_notebook()</span><br/> <span>visual2= pyLDAvis.gensim.prepare(lda_model2, doc_term_matrix, corpus)</span><br/> <span>visual2</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-575 image-border" src="assets/6afc488d-22f8-41f0-82b1-b6b7accbb05a.png" style="width:67.58em;height:41.17em;"/></p>
<p>Let's try to interpret this.</p>
<p>On the left hand side, we have the topics, and on the right, we have the terms/words:</p>
<ul>
<li>A bigger circle size means more frequent topics.</li>
<li>Topics that are overlapping or <span>closer to one another </span>are similar.</li>
<li>Upon selecting a topic, the most representative words for the selected topic can be seen. This reflects how frequent the word is. One can toggle the weight of each property by using the slider.</li>
<li>Hovering over a topic will provide the contribution of words to the topic on the right and upon clicking on the word, we will see the circle size changing, which reflects how frequent that term is in that topic.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Naive Bayes technique in text classification</h1>
                </header>
            
            <article>
                
<p class="mce-root">Naive Bayes is a supervised classification algorithm that is based on Bayes theorem. It is a probabilistic algorithm. But, you might be wondering why it is called <strong>Naive</strong>. It is so because this algorithm works on an assumption that all the features are independent of each other. However, we are cognizant of the fact that independence of features might not be there in a real-world scenario. For example, if we are trying to detect whether an email is spam or not, all we look for are the keywords associated with spams such as Lottery, Award, and so on. Based on these, we extract those relevant features from the email and say that if given spam-related features, the email will be classified as spam.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Bayes theorem</h1>
                </header>
            
            <article>
                
<p class="mce-root">The Bayes theorem helps us in finding posterior probability, given a certain condition:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>P(A|B)= P(B|A) * P(A)/P(B)</em></p>
<p><em>A</em> and <em>B</em> can be deemed as the target and features, respectively.</p>
<p>Where, <em>P(A|B)</em>: posterior probability, which implies the probability of event <em>A</em>, given that <em>B</em> has taken place:</p>
<ul>
<li><em>P(B|A)</em>: The likelihood that implies the probability of feature <em>B</em>, given the target <em>A</em></li>
<li><em>P(A)</em>: The prior probability of target <em>A</em></li>
<li><em>P(B)</em>: The prior probability of feature <em>B</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How the Naive Bayes classifier works</h1>
                </header>
            
            <article>
                
<p>We will try to understand all of this by looking at the example of the Titanic. While the Titanic was sinking, a few of the categories had priority over others, in terms of being saved. We have the following dataset (it is a Kaggle dataset):</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr style="height: 64px">
<td style="height: 64px">
<p><strong>Person category</strong></p>
</td>
<td style="height: 64px">
<p><strong>Survival chance</strong></p>
</td>
</tr>
<tr style="height: 50px">
<td style="height: 50px">
<p>Woman</p>
</td>
<td style="height: 50px">
<p>Yes</p>
</td>
</tr>
<tr style="height: 59px">
<td style="height: 59px">
<p>Kid</p>
</td>
<td style="height: 59px">
<p>Yes</p>
</td>
</tr>
<tr style="height: 59.5469px">
<td style="height: 59.5469px">
<p>Kid</p>
</td>
<td style="height: 59.5469px">
<p>Yes</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Man</p>
</td>
<td style="height: 64px">
<p>No</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Woman</p>
</td>
<td style="height: 64px">
<p>Yes</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Woman</p>
</td>
<td style="height: 64px">
<p>Yes</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Man</p>
</td>
<td style="height: 64px">
<p>No</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Man</p>
</td>
<td style="height: 64px">
<p>Yes</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Kid</p>
</td>
<td style="height: 64px">
<p>Yes</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Woman</p>
</td>
<td style="height: 64px">
<p>No</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Kid</p>
</td>
<td style="height: 64px">
<p>No</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Woman</p>
</td>
<td style="height: 64px">
<p>No</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Man</p>
</td>
<td style="height: 64px">
<p>Yes</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Man</p>
</td>
<td style="height: 64px">
<p>No</p>
</td>
</tr>
<tr style="height: 64px">
<td style="height: 64px">
<p>Woman</p>
</td>
<td style="height: 64px">
<p>Yes</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Now, let's prepare a likelihood table for the preceding information:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td style="width: 13.7741%">
<p> </p>
</td>
<td style="width: 18.4573%">
<p> </p>
</td>
<td style="width: 19.0083%">
<p><strong>Survival chance</strong></p>
</td>
<td style="width: 11.5702%">
<p> </p>
</td>
<td style="width: 18.595%">
<p> </p>
</td>
<td style="width: 8.81543%">
<p> </p>
</td>
<td style="width: 6.88705%"/>
</tr>
<tr>
<td style="width: 13.7741%"/>
<td style="width: 18.4573%">
<p> </p>
</td>
<td style="width: 19.0083%">
<p>No</p>
</td>
<td style="width: 11.5702%">
<p>Yes</p>
</td>
<td style="width: 18.595%">
<p>Grand Total</p>
</td>
<td style="width: 8.81543%">
<p> </p>
</td>
<td style="width: 6.88705%">
<p> </p>
</td>
</tr>
<tr>
<td style="width: 13.7741%">
<p>Category</p>
</td>
<td style="width: 18.4573%">
<p>Kid</p>
</td>
<td style="width: 19.0083%">
<p>1</p>
</td>
<td style="width: 11.5702%">
<p>3</p>
</td>
<td style="width: 18.595%">
<p>4</p>
</td>
<td style="width: 8.81543%">
<p>4/15=</p>
</td>
<td style="width: 6.88705%">
<p>0.27</p>
</td>
</tr>
<tr>
<td style="width: 13.7741%">
<p>Man</p>
</td>
<td style="width: 18.4573%">
<p>3</p>
</td>
<td style="width: 19.0083%">
<p>2</p>
</td>
<td style="width: 11.5702%">
<p>5</p>
</td>
<td style="width: 18.595%">
<p>5/15=</p>
</td>
<td style="width: 8.81543%">
<p>0.33</p>
</td>
<td style="width: 6.88705%"/>
</tr>
<tr>
<td style="width: 13.7741%">
<p>Woman</p>
</td>
<td style="width: 18.4573%">
<p>2</p>
</td>
<td style="width: 19.0083%">
<p>4</p>
</td>
<td style="width: 11.5702%">
<p>6</p>
</td>
<td style="width: 18.595%">
<p>6/15=</p>
</td>
<td style="width: 8.81543%">
<p>0.40</p>
</td>
<td style="width: 6.88705%"/>
</tr>
<tr>
<td style="width: 13.7741%">
<p> </p>
</td>
<td style="width: 18.4573%">
<p>Grand Total</p>
</td>
<td style="width: 19.0083%">
<p>6</p>
</td>
<td style="width: 11.5702%">
<p>9</p>
</td>
<td style="width: 18.595%">
<p>15</p>
</td>
<td style="width: 8.81543%">
<p> </p>
</td>
<td style="width: 6.88705%">
<p> </p>
</td>
</tr>
<tr>
<td style="width: 13.7741%">
<p> </p>
</td>
<td style="width: 18.4573%">
<p> </p>
</td>
<td style="width: 19.0083%">
<p>6/15</p>
</td>
<td style="width: 11.5702%">
<p>9/15</p>
</td>
<td style="width: 18.595%">
<p> </p>
</td>
<td style="width: 8.81543%">
<p> </p>
</td>
<td style="width: 6.88705%">
<p> </p>
</td>
</tr>
<tr>
<td style="width: 13.7741%">
<p> </p>
</td>
<td style="width: 18.4573%">
<p> </p>
</td>
<td style="width: 19.0083%">
<p>0.40</p>
</td>
<td style="width: 11.5702%">
<p>0.6</p>
</td>
<td style="width: 18.595%">
<p> </p>
</td>
<td style="width: 8.81543%">
<p> </p>
</td>
<td style="width: 6.88705%">
<p> </p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root"/>
<p>Let's find out which category of people had the maximum chance of survival:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>Kid - P(Yes|Kid)= P(Kid|Yes) * P(Yes)/P(Kid)</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>P(Kid|Yes) = 3/9= 0.3</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>P(Yes) = 9/15 =0.6</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>P(Kid)= 4/15 =0.27</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>P(Yes|kid) = 0.33 *0.6/0.27=0.73</em></p>
<p class="CDPAlignCenter CDPAlign"><em>Woman - P(Yes|Woman)= P(Woman|Yes) * P(Yes)/P(Woman)</em></p>
<p class="CDPAlignCenter CDPAlign"><em>P(Woman|Yes) = 4/9= 0.44</em></p>
<p class="CDPAlignCenter CDPAlign"><em>P(Yes) = 9/15 =0.6</em></p>
<p class="CDPAlignCenter CDPAlign"><em> P(Woman)= 6/15 =0.4</em></p>
<p class="CDPAlignCenter CDPAlign"><em>P(Yes|Woman) = 0.44 *0.6/0.4=0.66</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>Man - P(Yes|Man)= P(Man|Yes) * P(Yes)/P(Man)</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>P(Man|Yes) = 2/9= 0.22</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em> P(Yes) = 9/15 =0.6</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em> P(Man)= 6/15 =0.33</em></p>
<p class="mce-root CDPAlignCenter CDPAlign"><em>P(Yes|Man) = 0.22 *0.6/0.33=0.4</em></p>
<p class="mce-root">So, we can see that a child had the maximum chance of survival and a man the least chance.</p>
<p class="mce-root">Let's perform the sentiment classification with the help of Naive Bayes, and see whether the result is better or worse:</p>
<pre>from sklearn.naive_bayes import MultinomialNB<br/># splitting data into training and validation set<br/>xtraintf, xtesttf, ytraintf, ytesttf = train_test_split(tfidfV, Newdata['label'], random_state=42, test_size=0.3)<br/>NB= MultinomialNB()<br/>NB.fit(xtraintf, ytraintf)<br/>prediction = NB.predict_proba(xtesttf) # predicting on the test set<br/>prediction_int = prediction[:,1] &gt;= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0<br/>prediction_int = prediction_int.astype(np.int)<br/>print("F1 Score-",f1_score(ytest, prediction_int))<br/>print("Accuracy-",accuracy_score(ytest,prediction_int))</pre>
<p>The output is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-568 image-border" src="assets/395e9ce6-4752-4128-9ecd-410866ff1142.png" style="width:14.50em;height:3.25em;"/></p>
<p>Here, we can see that our previous results were better than the Naive Bayes results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we studied c<span>orpus building techniques</span> <span>that consists of sentences and words, which includes a bag of words to make the texts usable for the algorithms.</span> You <span>also learned about TF-IDF and how important a term is with respect to a document and the entire corpus. We went over sentiment analysis, along with classification and TF-IDF feature extraction.</span></p>
<p>You were also introduced to topic modeling and evaluating models, which includes visualizing LDA. We covered the Bayes theorem and working with the Naive Bayes classifier. In the next chapter, you will learn about <span>temporal and sequential pattern discovery.</span></p>


            </article>

            
        </section>
    </body></html>