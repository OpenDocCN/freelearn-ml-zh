- en: Deep Belief – Deep Networks and Dreaming
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度信念 – 深度网络与梦境
- en: We've all heard of deep learning, but how many of us know what a **Deep Belief
    Network** is? Let's start this chapter by answering that very question. A Deep
    Belief Network is a very advanced form of machine learning, one whose meaning
    is rapidly evolving. As a machine learning developer, it's important that you
    have a bit of exposure to this concept so that you are familiar with it when you
    encounter it or it encounters you!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都听说过深度学习，但有多少人知道**深度信念网络**是什么？让我们从这个章节开始，回答这个问题。深度信念网络是一种非常高级的机器学习方法，其含义正在迅速演变。作为一名机器学习开发者，了解这个概念很重要，这样当你遇到它或它遇到你时，你会熟悉它！
- en: In machine learning, a Deep Belief Network is technically a deep neural network.
    We should state that the meaning of **deep**, when it comes to deep learning or
    deep belief, means that the network is composed of multiple layers (hidden units).
    In a Deep Belief Network, these connections span internally between each neuron
    within a layer, but not between different layers. A Deep Belief Network can be
    trained to learn unsupervised in order to probabilistically reconstruct the network's
    inputs. The layers then function as 'feature detectors' to recognize or classify
    images, letters, and so on. You can also watch a Deep Belief Network dream, which
    is a very interesting topic in and of itself.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，深度信念网络在技术上是一种深度神经网络。我们应该指出，当提到深度学习或深度信念时，“深度”的含义是指网络由多个层（隐藏单元）组成。在深度信念网络中，这些连接在层内的每个神经元之间进行，但不在不同层之间。深度信念网络可以被训练以无监督地学习，以便以概率重建网络的输入。然后这些层作为“特征检测器”来识别或分类图像、字母等。你还可以观察深度信念网络做梦，这是一个非常有趣的话题。
- en: 'In this chapter we will cover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Restricted Boltzmann Machines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: Creating and training a Deep Belief Network in C#
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用C#创建和训练深度信念网络
- en: Restricted Boltzmann Machines
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 受限玻尔兹曼机
- en: One popular method of constructing a Deep Belief Network is to comprise it as
    a layered collection of **Restricted Boltzmann Machines** (**RBMs**). These RMBs
    function as auto-encoders, with each hidden layer, serving as the visible layer
    for the next. This composition leads to a fast, layer-by-layer and unsupervised
    training procedure. The Deep Belief Network will have layers of RBMs for the pre-train
    phase, and then a feedforward network for the fine-tune phase. The first step
    of the training will be to learn a layer of features from the visible units. The
    next step is to take the activations from the previously trained features and
    make them the new visible units. We then repeat the process so that we can learn
    more features in the second hidden layer. The process then continues for all hidden
    layers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 构建深度信念网络的一种流行方法是将其构建为一个由**受限玻尔兹曼机**（**RBMs**）组成的分层集合。这些RBMs作为自编码器运行，每个隐藏层都作为下一层的可见层。这种结构导致了一种快速、逐层和无监督的训练过程。深度信念网络在预训练阶段将包含RBM层，然后在微调阶段使用前馈网络。训练的第一步是从可见单元学习一层特征。下一步是将之前训练的特征的激活作为新的可见单元。然后我们重复这个过程，以便在第二个隐藏层中学习更多特征。然后这个过程会继续应用于所有隐藏层。
- en: We should provide two notes of information here.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应该提供两条信息。
- en: First, we should explain a bit about what an auto-encoder is and does. Auto-encoders
    are at the heart of what is known as **representational learning**. They encode
    input, which is usually compressed vectors of significant features, as well as
    data for reconstructing via unsupervised learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该解释一下什么是自编码器以及它做什么。自编码器是所谓**表示学习**的核心。它们编码输入，通常是显著特征的压缩向量，以及通过无监督学习重建的数据。
- en: Second, we should note that stacking RBMs within a Deep Belief Network is but
    one way to approach this. Stacking Restricted Linear Units (ReLUs) with dropout
    and training, and then accompanying that with backpropagation, has once again
    become state of the art. I say once again because 30 years ago, the supervised
    approach was the way to go. Rather than let the algorithm look at all the data
    and determine the feature of interest, sometimes we as humans can actually better
    find the feature we want.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们应该注意，在深度信念网络中堆叠RBMs只是处理这个问题的一种方法。堆叠带有dropout的受限线性单元（ReLUs）并进行训练，然后配合反向传播，这再次成为了最先进的技术。我说再次是因为30年前，监督方法是唯一可行的方法。与其让算法查看所有数据并确定感兴趣的特征，有时我们作为人类实际上可以更好地找到我们想要的特征。
- en: 'What I would consider the two most significant properties of Deep Belief Networks
    are as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为深度信念网络最显著的两个特性如下：
- en: There is an efficient, layer-by-layer process for learning top-down, generative
    weights. It determines how variables in one layer depend on variables in the layers
    above it.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在一个高效、逐层的学习过程，用于学习自上而下的生成权重。它决定了某一层的变量如何依赖于其上层的变量。
- en: After the learning is complete, the values of the variables in every layer can
    easily be inferred by a single, bottom-up pass which starts with an observed data
    vector in the bottom layer and uses the generative weights in reverse direction
    to reconstruct the data.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习完成后，可以通过从底层观察到的数据向量开始的单个自下而上的遍历，很容易地推断出每一层变量的值，并使用生成权重反向重建数据。
- en: With that said, let's now talk about RBMs as well as Boltzmann machines in general.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，现在让我们也来谈谈RBM以及一般的霍尔兹曼机。
- en: 'A Boltzmann machine is a recurrent neural network with binary units and undirected
    edges between these units. For those of you who weren''t paying attention in your
    graph theory class, undirected means the edges (or links) are bidirectional, they
    are not pointing in any specific direction. For those not experienced in graph
    theory, the following is a diagram of an undirected graph with undirected edges:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 霍尔兹曼机是一种具有二元单元和单元之间无向边的循环神经网络。对于那些在图论课程中没注意听讲的同学，无向意味着边（或链接）是双向的，它们不指向任何特定方向。对于那些不熟悉图论的人来说，以下是一个具有无向边的无向图的示意图：
- en: '![](img/f6107d56-83c7-438d-9796-8d0d9372a8d7.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f6107d56-83c7-438d-9796-8d0d9372a8d7.png)'
- en: Boltzmann machines were one of the first neural networks capable of learning
    internal representations, and given enough time, they can solve difficult problems.
    They are, however, not good at scaling, which leads us to our next topic, RBMs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 霍尔兹曼机是第一批能够学习内部表示的神经网络之一，并且如果给定足够的时间，它们可以解决难题。然而，它们在扩展方面并不擅长，这让我们转向下一个主题，即RBMs。
- en: RBMs were introduced to deal with the Boltzmann Machines' inability to scale.
    They have hidden layers, with connections restricted between each hidden unit
    but not outside those units, which helps with efficient learning. More formally,
    we must dive into a little bit of graph theory to properly explain this.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: RBM被引入来处理霍尔兹曼机无法扩展的问题。它们有隐藏层，隐藏单元之间的连接受到限制，但不在这些单元之外，这有助于高效学习。更正式地说，我们必须稍微深入研究一下图论，才能正确解释这一点。
- en: RBMs must have their neurons form what is known as a **bipartite graph**, a
    more advanced form of graph theory; a pair of nodes from each of the two groups
    of units (visible and hidden layers) may have a symmetric connection between them.
    There can be no connections between the nodes within any group. A bipartite graph,
    sometimes called a **biograph**, is a set of graph vertices decomposed into two
    disjoint sets such that no two vertices within the same set are adjacent.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: RBM必须让它们的神经元形成所谓的**二分图**，这是一种更高级的图论形式；两组单元（可见层和隐藏层）中的每一对节点之间可能存在对称连接。任何一组内的节点之间不能有连接。二分图，有时称为**双图**，是一组图顶点分解为两个不相交的集合，使得同一集合内的两个顶点不相邻。
- en: Here is a good example that will help visualize this topic.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个很好的例子，可以帮助可视化这个主题。
- en: 'Note that there are no connections within the same set (red on the left or
    black on the right), but there are connections between the two sets:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，同一集合内（左侧的红色或右侧的黑色）没有连接，但两个集合之间存在连接：
- en: '![](img/1f5eb229-6889-43c9-858a-889d54e19878.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1f5eb229-6889-43c9-858a-889d54e19878.png)'
- en: More formally, an RBM is what is known as a **symmetrical bipartite graph*.***
    This is because inputs from all visible nodes are passed to all hidden nodes.
    We say symmetrical because each visible node relates to a hidden node; bipartite
    because there are two layers; and graph because, well, it's a graph, or a collection
    of nodes if you prefer!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，RBM被称为**对称二分图**。这是因为所有可见节点的输入都传递给所有隐藏节点。我们称之为对称，因为每个可见节点都与一个隐藏节点相关联；二分是因为有两个层次；而图是因为，嗯，它是一个图，或者如果你更喜欢，它是一组节点！
- en: 'Imagine for a second that our RBM is presented images of cats and dogs, and
    we have two output nodes, one for each animal. On our forward learning pass, our
    RBM asks itself "*With the pixels I am seeing, should I send stronger weight signals
    for the cat or for the dog?*" On the backward pass, it wonders "*Being a dog,
    which distribution of pixels should I see?*" That, my friends, was today''s lesson
    on joint probability: the simultaneous probability of *X* given *A* and *A* given
    *X*. In our case, this joint probability is expressed as the weights between the
    two layers and is an important aspect of RBMs.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们的RBM被呈现了猫和狗的图像，并且我们有两个输出节点，一个用于每种动物。在我们的正向学习过程中，我们的RBM会问自己“*看到这些像素，我应该为猫还是狗发送更强的权重信号？*”在反向过程中，它会思考“*作为一个狗，我应该看到什么样的像素分布？*”朋友们，这就是今天关于联合概率的教训：给定*A*的*X*和给定*X*的*A*的同时概率。在我们的案例中，这个联合概率以两层之间的权重表示，并且是RBMs的一个重要方面。
- en: With today's mini lessons in joint probability and graph theory behind us, we'll
    now talk about **reconstruction**, which is an important piece of what RBMs do.
    In the example we have been discussing, we are learning which groups of pixels
    occur (meaning being *on*) for a set of images. When a hidden layer node is activated
    by a significant weight (whatever that is determined to be to turn it *on*), it
    represents co-occurrences of something happening, in our case, the dog or the
    cat. Pointy ears + round face + small eyes might be what we are looking for if
    the image is a cat. Big ears + long tail + big nose may make the image a dog.
    These activations represent what our RBM "thinks" the original data looks like.
    For all intents and purposes, we are in fact reconstructing the original data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了今天的联合概率和图论的小课程之后，我们现在将讨论**重建**，这是RBMs（限制玻尔兹曼机）所做的重要部分。在我们一直在讨论的例子中，我们正在学习哪些像素组在一系列图像中发生（意味着处于*开启*状态）。当一个隐藏层节点被一个显著权重激活（无论这个权重是如何确定的以将其*开启*），它代表了某些事件同时发生的共现，在我们的案例中，是狗或猫。如果图像是一只猫，尖耳朵+圆脸+小眼睛可能就是我们要找的特征。大耳朵+长尾巴+大鼻子可能使图像成为一只狗。这些激活代表了我们的RBM“认为”原始数据看起来像什么。从所有目的和用途来看，我们实际上正在重建原始数据。
- en: We should also quickly point out that an RBM has two biases instead of one.
    This is very important as this is what distinguishes it from other auto-encoding
    algorithms. The hidden bias helps our RBM produce the activations we need when
    it's on the forward pass, and the visible layer bias helps learn the correct reconstructions
    on the backward pass. The hidden bias is important because its main job is to
    ensure that some of the nodes fire no matter how sparse our data might be. You
    will see how this impacts the way a Deep Belief Network dreams a little later
    on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该迅速指出，RBM有两个偏差而不是一个。这一点非常重要，因为它将RBM与其他自动编码算法区分开来。隐藏偏差帮助我们的RBM在正向过程中产生所需的激活，而可见层偏差帮助在反向过程中学习正确的重建。隐藏偏差很重要，因为它的主要任务是确保在数据可能非常稀疏的情况下，某些节点仍然会激活。你将在稍后看到这如何影响深度信念网络做梦的方式。
- en: Layering
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次化
- en: Once our RBM learns the structure of the input data, which is related to the
    activations made in our first hidden layer, the data gets passed down to the next
    hidden layer. The first hidden layer then becomes the new visible layer. The activations
    we created in the hidden layer now become our inputs. They will be multiplied
    by the weights in the new hidden layer to produce another set of activations.
    This process continues through all the hidden layers in our network. The hidden
    layer becomes the visible layer, we have another hidden layer whose weights we
    will use, and we repeat. Each new hidden layer results in adjusted weights, until
    we get to the point where we can recognize the input from the previous layer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的RBM学会了输入数据的结构，这与我们在第一隐藏层中做出的激活有关，数据就会传递到下一个隐藏层。第一隐藏层随后成为新的可见层。我们在隐藏层中创建的激活现在成为我们的输入。它们将被新的隐藏层中的权重相乘，产生另一组激活。这个过程会继续通过我们网络中的所有隐藏层。隐藏层变成可见层，我们有了另一个我们将使用的权重的隐藏层，我们重复这个过程。每个新的隐藏层都会导致权重的调整，直到我们达到可以识别来自前一层的输入的点。
- en: To elaborate just a bit more (helping you in your quest to remain buzzword-compliant),
    this is technically called **unsupervised, greedy, layer-wise training**. No input
    is required to improve the weights of each layer, which means no outside influence
    of any type is involved. This further means we should be able to use our algorithm
    to train on unsupervised data that has not been seen previously. As we have continually
    stressed, *the more the data we have, the better our results*! As each layer gets
    better and hopefully more accurate, we are in a much better position to increase
    our learning through each hidden layer, with the weights having the responsibility
    of guiding us to the correct image classification along the way.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更详细地说明（帮助你保持术语的合规性），这从技术上讲被称为**无监督、贪婪、分层训练**。不需要输入来改进每一层的权重，这意味着没有任何类型的外部影响。这进一步意味着我们应该能够使用我们的算法在之前未见过的不监督数据上进行训练。正如我们一直强调的那样，*数据越多，我们的结果越好*！随着每一层变得更好，希望也更准确，我们就有更好的位置通过每一隐藏层增加我们的学习，权重在这个过程中负责引导我们到达正确的图像分类。
- en: But as we discuss reconstruction, we should point out that each time a number
    (weight) in our reconstruction effort is non-zero, that is an indication that
    our RBM has learned something from the data. In a sense, you can treat the returned
    numbers exactly as you would treat a percentage indicator. The higher the number,
    the more confident the algorithm is of what it is seeing. Remember, we have the
    master dataset that we are trying to get back to, and we have a reference dataset
    to use in our reconstruction efforts. As our RBM iterates over each image, it
    doesn't yet know what image it is dealing with; that's what it is trying to determine.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我们讨论重建时，我们应该指出，在我们重建努力中，每当一个数字（权重）不为零，这表明我们的RBM已经从数据中学习到了一些东西。从某种意义上说，你可以将返回的数字当作你对待百分比指示器一样来处理。数字越高，算法对其所看到的东西就越有信心。记住，我们有一个主数据集，我们试图恢复到这个数据集，并且我们有一个参考数据集用于我们的重建工作。随着我们的RBM遍历每一张图像，它还不知道它正在处理什么图像；这正是它试图确定的事情。
- en: Let's take a brief moment to clarify something. When we say we are using a greedy
    algorithm, what we really mean is that our RBM will take the shortest path to
    achieve the best result. We will sample random pixels from the image we see, and
    test which ones lead us to the correct answer. The RBM will test each hypothesis
    against the master dataset (test set), which is our correct end goal. Keep in
    mind that each image is just a set of pixels we're trying to classify. Those pixels
    house features and characteristics of data. For example, a pixel can have different
    shades of light, wherein dark pixels perhaps indicate borders, light pixels perhaps
    indicate numbers, and so forth.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要澄清一下。当我们说我们使用贪婪算法时，我们真正意思是我们的RBM将采取最短路径以实现最佳结果。我们将从我们看到的图像中采样随机像素，并测试哪些像素能引导我们到达正确答案。RBM将测试每个假设与主数据集（测试集）的对比，这是我们正确的最终目标。记住，每张图像只是我们试图分类的一组像素。这些像素包含了数据的特征和特性。例如，一个像素可以有不同亮度的阴影，其中深色像素可能表示边界，浅色像素可能表示数字，等等。
- en: But what happens when things don't go our way? What happens if whatever we learn
    at any given step is not correct? Should this occur, it would mean that our algorithm
    has guessed incorrectly. Our course of action is then to go back and try again.
    This is not as bad, nor as time-consuming, as it may seem. Of course, there is
    a temporal cost associated with an incorrect hypothesis, but the end goal is that
    we must increase our learning efficiency and reduce our error with each stage.
    Each weighted connection that was wrong will be penalized like what we did in
    reinforcement learning. These connections will decrease in weight and no longer
    be as strong. Hopefully, the next pass through will increase our accuracy while
    decreasing our error, and the stronger the weight, the more the influence it will
    have.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 但当事情不按我们的意愿发展时会发生什么？如果我们在任何给定步骤中学到的任何东西都不正确，会发生什么？如果发生这种情况，这意味着我们的算法猜错了。我们的行动方案是回过头去再试一次。这并不像看起来那么糟糕，也不那么耗时。当然，一个错误假设有一个时间成本，但最终目标是我们必须提高我们的学习效率并减少每个阶段的错误。每个错误的加权连接将像我们在强化学习中做的那样受到惩罚。这些连接的权重将减少，不再那么强大。希望下一次遍历将提高我们的准确性，同时减少错误，权重越强，它的影响就越大。
- en: So, let's take a hypothetical scenario and think aloud for a second. Let's say
    we are classifying numeric images, meaning numbers. Some images will have curves,
    such as 2, 3, 6, 8, 9, and so on. Other numbers, such as 1, 4 and 7, will not.
    Knowledge such as this is very important, because our RBM, will use it to continue
    to improve its learning and reduce error. If we think we're dealing with the number
    2, then the weights to the path that indicate this to us will be more heavily
    weighted than others. This is a drastic oversimplification, but hopefully it's
    enough to help you understand what we are about to embark upon.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们假设一个场景，并且稍微思考一下。假设我们正在对数字图像进行分类，即数字。一些图像会有曲线，例如2、3、6、8、9等等。其他数字，如1、4和7，则不会有。这样的知识非常重要，因为我们的RBM将利用它来继续改进其学习并减少错误。如果我们认为我们正在处理数字2，那么指向这个方向的权重将比其他权重更重。这是一个极端的简化，但希望这足以帮助你理解我们即将开始的事情。
- en: As we put all this together, we now have the theoretical framework for a Deep
    Belief Network. Although we have delved into more theory than other chapters,
    as you see our example program working, it will all start to make sense. And you
    will be much better prepared to use it in your applications, knowing what's happening
    behind the scenes. Remember, black hole versus black box!
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将所有这些放在一起时，我们现在有了深度信念网络的理论框架。尽管我们比其他章节更深入地探讨了理论，但当你看到我们的示例程序运行时，一切都将开始变得有意义。而且你将更好地准备好在你的应用中使用它，了解幕后发生的事情。记住，黑洞与黑箱！
- en: To show you about both Deep Belief Networks and RBMs, we are going to use the
    fantastic open source software SharpRBM written by Mattia Fagerlund. This software
    is an incredible contribution to the open source community, and I have no doubt
    you will spend hours, if not days, working with it. This software comes with some
    very incredible demos. For this chapter, we will use the Letter Classification
    Demo.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示深度信念网络和RBMs，我们将使用由Mattia Fagerlund编写的出色开源软件SharpRBM。这款软件是对开源社区的巨大贡献，我毫不怀疑你将花费数小时，甚至数天的时间与之共事。这款软件附带了一些非常令人惊叹的演示。对于本章，我们将使用字母分类演示。
- en: The following screenshot is of our deep belief test application. Ever wonder
    what a computer dreams of when it's sleeping? Well my friend, you are about to
    find out!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是我们深度信念测试应用。你是否好奇当计算机在睡眠时它在想什么？好吧，我的朋友，你即将找到答案！
- en: '![](img/96b442c2-cc23-40d2-804b-93be033833d3.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/96b442c2-cc23-40d2-804b-93be033833d3.png)'
- en: 'As usual, we will also use ReflectInsight to provide us with a behind-the-scenes
    look into what is going on:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们也会使用ReflectInsight来提供幕后发生的事情的视角：
- en: '![](img/f78e506e-9303-4f84-9db3-2439e859a16e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f78e506e-9303-4f84-9db3-2439e859a16e.png)'
- en: The first thing you will notice about our demo application is that there is
    a lot going on. Let's take a moment and break it down into smaller chunks.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先会注意到我们的演示应用中有很多事情在进行。让我们花点时间将其分解成更小的部分。
- en: 'In the upper-left corner of the program screen is the area where we designate
    the layer that we want to train. We have three hidden layers, all of which need
    proper training before testing. We can train each layer one at a time, starting
    with the first layer. You may train for as long or as little as you like, but
    the more you train, the better your system will be:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在程序屏幕的左上角是我们要指定的层，我们有三层隐藏层，所有这些层在测试之前都需要适当的训练。我们可以一次训练一层，从第一层开始。你可以根据自己的喜好训练多长时间，但训练越多，你的系统会越好：
- en: '![](img/6f26651d-7b4a-4ae7-af21-aa68a286482a.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6f26651d-7b4a-4ae7-af21-aa68a286482a.png)'
- en: 'The next section following our training options is our progress. As we are
    training, all pertinent information, such as generation, reconstruction error,
    detector error, and learning rate, is displayed here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练选项之后是下一节，我们的进度。在我们训练的过程中，所有相关信息，如生成、重建错误、检测器错误和学习率，都显示在这里：
- en: '![](img/2d0b445f-cde3-4a56-9a75-476a9d084326.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2d0b445f-cde3-4a56-9a75-476a9d084326.png)'
- en: 'The next section is the drawing of our feature detectors, which will update
    themselves throughout training if the Draw checkbox is checked:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节是关于我们特征检测器的绘图，如果勾选了“绘制”复选框，它将在整个训练过程中更新自己：
- en: '![](img/2ee0d8b4-17bd-45f1-ae42-f1e12880e9bf.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2ee0d8b4-17bd-45f1-ae42-f1e12880e9bf.png)'
- en: 'As you begin training a layer, you will notice that the reconstructions and
    feature detectors are basically empty. They will refine themselves as your training
    progresses. Remember, we are reconstructing what we already know to be true! As
    the training continues, the reconstructed digits become more and more defined,
    along with our feature detector:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始训练一层时，你会注意到重建和特征检测器基本上是空的。随着你的训练进展，它们会自我完善。记住，我们正在重建我们已经知道是真实的内容！随着训练的继续，重建的数字变得越来越清晰，我们的特征检测器也随之变得更加明显：
- en: '![](img/91cb9918-4fc0-4c6b-bab7-86cd979531fc.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91cb9918-4fc0-4c6b-bab7-86cd979531fc.png)'
- en: 'Here is a snapshot from the application during training. As you can see, it
    is on generation 31 and the reconstructed digits are very well defined. They are
    still not complete or correct, but you can see just how much progress we are making:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是应用程序在训练过程中的一个快照。正如你所见，它处于第31代，重建的数字非常清晰。它们仍然不完整或不正确，但你可以看到我们正在取得多大的进步：
- en: '![](img/262f7890-b936-4657-98c7-a676cdcb747f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/262f7890-b936-4657-98c7-a676cdcb747f.png)'
- en: What does a computer dream?
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机做梦是什么？
- en: '*What does a computer dream when it dreams*? is a famous saying. The intuition
    for us will be a feature which allows us to see what the computer is thinking
    during its reconstruction phase. As the program is trying to reconstruct our digits,
    the feature detectors themselves will take various forms throughout the process.
    It is these forms that we display in the dream window (indicated by the red circle):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: “*当计算机做梦时，它在想什么*？” 这是一句著名的话。对我们来说，这将是一个特性，它允许我们在计算机重建阶段看到它在想什么。当程序试图重建我们的数字时，特征检测器本身在整个过程中会呈现各种形式。我们就是在梦境窗口（用红色圆圈表示）中显示这些形式：
- en: '![](img/031211f7-91d7-49ca-b5fa-b747c14b9516.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/031211f7-91d7-49ca-b5fa-b747c14b9516.png)'
- en: 'Well, we have spent quite a bit of time looking at screenshots of our application.
    I think it''s time that we now look at the code. Let''s start by looking at how
    we create the `DeepBeliefNetwork` object itself:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我们已经花了很多时间查看我们应用程序的截图。我认为现在是时候看看代码了。让我们先看看我们是如何创建 `DeepBeliefNetwork` 对象本身的：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once this is created, we need to create our network trainer, and we do this
    based on the weights from the layer that we are training:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 创建完成后，我们需要创建我们的网络训练器，我们根据正在训练的层的权重来做这件事：
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Both of these objects are used within our main `TrainNetwork` loop, the part
    of our application where most of the activity happens. This loop will continue
    until told to stop.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个对象都在我们的主 `TrainNetwork` 循环中使用，这是我们应用程序中活动发生的主要部分。这个循环将持续进行，直到被通知停止。
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the preceding code, we highlighted the `trainer.Train()` function, which
    is an array-based learning algorithm that looks like this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们突出了 `trainer.Train()` 函数，这是一个基于数组的机器学习算法，其形式如下：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code uses parallel processing (highlighted section) to train single cases
    in parallel. This function is responsible for handling the changing of input and
    hidden layers, as we discussed at the beginning of the chapter. It uses the `TrainOnSingleCase`
    function, which looks like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用并行处理（突出部分）来并行训练单个案例。这个函数负责处理输入和隐藏层的转换，正如我们在本章开头所讨论的。它使用 `TrainOnSingleCase`
    函数，其形式如下：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we accumulate the errors during processing, which is the difference
    between what our model should believe and what it actually does. Obviously, the
    lower the error rate the better, for the most accurate reconstruction of our images.
    The `AccumulateErrors` function is shown here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在处理过程中累积错误，这是我们的模型应该相信的内容与它实际执行的内容之间的差异。显然，错误率越低越好，这样我们才能更准确地重建我们的图像。`AccumulateErrors`
    函数如下所示：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Summary
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Well folks, there you have it! In this chapter, you learned about RBMs, a little
    bit of graph theory, and how to create and train a Deep Belief Network in C#.
    Your buzzword-compliant checklist is almost complete! I recommend that you experiment
    with the code, train your network layers to different thresholds, and watch how
    your computer dreams while reconstructing. Remember, the more you train the better,
    so spend time with each layer to ensure it has enough data to do an accurate job
    of reconstruction.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，朋友们，这就是全部内容！在本章中，你学习了 RBMs、一点图论，以及如何在 C# 中创建和训练深度信念网络。你的 buzzword-compliant
    检查清单几乎已经完成！我建议你尝试代码，将网络层训练到不同的阈值，并观察你的计算机在重建过程中是如何“做梦”的。记住，训练越多越好，所以花时间在每个层上，确保它有足够的数据来完成准确的重建工作。
- en: 'A quick note of warning: if you enable drawing of your feature detectors and
    reconstructed inputs, you will notice a huge decrease in performance. If you are
    trying to train your layers, you may wish to train them without visualizations
    first in order to reduce the time required. Trust me, with visualizations it will
    feel like an eternity if you train each level to a high iteration! Feel free to
    continually save your network as you progress. Good luck, and happy dreaming!'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简短的警告：如果你启用了绘制你的特征检测器和重建输入的功能，你会注意到性能会有大幅下降。如果你正在尝试训练你的层，你可能希望在首先不进行可视化的情况下训练它们，以减少所需的时间。相信我，如果你将每个级别训练到高迭代次数，那么使用可视化将感觉像永恒！在你进步的过程中，随时保存你的网络。祝你好运，祝你梦想成真！
- en: In the next chapter, we will learn about micro benchmarking and get to use one
    of the most powerful open source micro benchmarking toolkit ever written!
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习微基准测试，并有机会使用有史以来最强大的开源微基准测试工具包之一！
- en: References
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Mattias Fagerlund: [https://lotsacode.wordpress.com/2010/09/14/sharprbm-restricted-boltzmann-machines-in-c-net/#comments](https://lotsacode.wordpress.com/2010/09/14/sharprbm-restricted-boltzmann-machines-in-c-net/#comments)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mattias Fagerlund: [https://lotsacode.wordpress.com/2010/09/14/sharprbm-restricted-boltzmann-machines-in-c-net/#comments](https://lotsacode.wordpress.com/2010/09/14/sharprbm-restricted-boltzmann-machines-in-c-net/#comments)'
- en: 'Nykamp DQ, *Undirected graph definition*, from *Math Insight*: [http://mathinsight.org/definition/undirected_graph](http://mathinsight.org/definition/undirected_graph)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nykamp DQ, *无向图定义*, 来自 *Math Insight*: [http://mathinsight.org/definition/undirected_graph](http://mathinsight.org/definition/undirected_graph)'
