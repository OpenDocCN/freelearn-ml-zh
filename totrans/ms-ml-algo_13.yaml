- en: Deep Belief Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to present two probabilistic generative models
    that employ a set of latent variables to represent a specific data generation
    process. **Restricted Boltzmann Machines** (**RBMs**), proposed in 1986, are the
    building blocks of a more complex model, called a **Deep Belief Network** (**DBN**),
    which is capable of capturing complex relationships among features at different
    levels (in a way not dissimilar to a deep convolutional network). Both models
    can be used in unsupervised and supervised scenarios as preprocessors or, as is
    usual with DBN, fine-tuning the parameters using a standard backpropagation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Markov random fields** (**MRF**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contrastive Divergence** (**CD-k**) algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBN with supervised and unsupervised examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MRF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a set of random variables, *x[i]*, organized in an undirected
    graph, *G=(V, E)*, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78d1aaaf-ec2e-4308-af39-97b91d0f6f98.png)'
  prefs: []
  type: TYPE_IMG
- en: Example of a probabilistic undirected graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Two random variables, *a* and *b*, are conditionally independent given the
    random variable, *c* if:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d40802c-56e7-4a0f-aef4-c9a4a5758b21.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, consider the graph again; if all generic couples of subsets of variables
    *S[i]* and *S[j]* are conditionally independent given a separating subset, *S[k]*
    (so that all connections between variables belonging to *S[i]* to variables belonging
    to *S[j]* pass through *S[k]*), the graph is called a **Markov random field**
    (**MRF**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given *G=(V, E)*, a subset containing vertices such that every couple is adjacent
    is called a **clique** (the set of all cliques is often denoted as *cl(G))*. For
    example, consider the graph shown previously; *(x[0], x[1])* is a clique and if
    *x[0]* and *x[5]* were connected, *(x[0], x[1], x[5])* would be a clique. A **maximal
    clique** is a clique that cannot be expanded by adding new vertices. A particular
    family of MRF is made up of all those graphs whose joint probability distribution
    can be factorized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3a6781e8-0fcd-45e0-8fc7-3fc62a442155.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, *α* is the normalizing constant and the product is extended to
    the set of all maximal cliques. According to the **Hammersley–Clifford** theorem
    (for further information, please refer to *Proof of Hammersley-Clifford Theorem*, *Cheung
    S., University of Kentucky, 2008*), if the joint probability density function
    is strictly positive, the MRF can be factorized and all the *ρ[i]* functions are
    strictly positive too. Hence *p(x)*, after some straightforward manipulations
    based on the properties of logarithms, can be rewritten as a **Gibbs** (or **Boltzmann**)
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/caff2327-6603-415c-9a9b-805e948b04af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The term *E(x)* is called energy, as it derives from the first application
    of such a distribution in statistical physics. *1/Z* is now the normalizing constant
    employing the standard notation. In our scenarios, we always consider graphs containing
    observed *(x[i])* and latent variables *(h[j])*. Therefore, it''s useful to express
    the joint probability as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fd98304-a2bb-4dc8-a801-9e354591b72a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Whenever it''s necessary to marginalize to obtain *p(x)*, we can simply sum
    over *h[j]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7aa19f63-e8ff-46ac-906c-acbaa30e0007.png)'
  prefs: []
  type: TYPE_IMG
- en: RBMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A RBM (originally called **Harmonium**) is a neural model proposed by Smolensky
    (in *I**nformation processing in dynamical systems: Foundations of harmony theory, **Smolensky
    P., Parallel Distributed Processing, Vol 1, The MIT Press*) that is made up of
    a layer of input (observable) neurons and a layer of hidden (latent) neurons.
    A generic structure is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e23e3da-c002-4238-bd80-aa05e33d7ba7.png)'
  prefs: []
  type: TYPE_IMG
- en: Structure of Restricted Boltzmann Machine
  prefs: []
  type: TYPE_NORMAL
- en: 'As the undirected graph is bipartite (there are no connections between neurons
    belonging to the same layer), the underlying probabilistic structure is MRF. In
    the original model (even if this is not a restriction), all the neurons are assumed
    to be Bernoulli-distributed *(x[i], h[i] = {0, 1})*, with a bias, *b[i]* (for
    the observed units) and *c[j]* (for the latent neurons). The resulting energy
    function is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/204d3b4b-ad7e-4053-8488-26cd2fb2586e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A RBM is a probabilistic generative model that can learn a data-generating
    process, *p[data]*, which is represented by the observed units but exploits the
    presence of the latent variables in order to model all the internal relationships.
    If we summarized all the parameters in a single vector, *θ = {w[ij], b[i], c[j]}*,
    the Gibbs distribution becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1683c289-da77-49c0-9c58-4e473d85ddbe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The training goal of a RBM is to maximize the log-likelihood with respect to
    an input distribution. Hence, the first step is determining *L(θ; x)* after the
    marginalization of the previous expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0caa878b-b9a4-48c6-b8ac-8ef0e40261e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we need to maximize the log-likelihood, it''s useful to compute the gradient
    with respect to *θ*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/773cb0ac-76e3-4105-bc66-f69e1f97022c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Applying the chain rule of derivatives, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da3493f6-5cad-4768-b4cb-8571af2f0bac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the conditional and joint probability equalities, the previous expression
    becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86e3b4e3-2e5d-45d8-9d21-f2ebd696ddcb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering the full joint probability, after some tedious manipulations (which
    we omit), it''s possible to derive the following expressions (*σ(•)* is the sigmoid
    function):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed282e75-cbb0-4076-b3b6-1ac21c45924e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, we can compute the gradient of the log-likelihood with respect
    to each single parameter, *w[ij]*, *b**[i]*, and *c**[j]*. Starting with *w[ij]*,
    and considering that *∇[wij] E(x, h; θ) = -x[i]h[j]*, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f7b51aa-8366-498c-b37a-ca9042c5d5ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The expression can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28b42155-e075-4942-83c0-555285d9340e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, considering that all the units are Bernoulli-distributed, and isolating
    only the *j^(th)* hidden unit, it''s possible to apply the simplification:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5097a2ee-27cf-4eba-8a95-2fd06a5b75f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the gradient becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8d12c1d4-c022-4040-a344-d6993cc18acb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Analogously, we can derive the gradient of *L* with respect to *b*[*i* ]and
    *c[j]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6265c30f-5188-4089-a8b9-4ba746d8a035.png)'
  prefs: []
  type: TYPE_IMG
- en: Hence, the first term of every gradient is very easy to compute, while the second
    one requires summing over all observed values. As this operation is impracticable,
    the only feasible alternative is an approximation based on sampling, using a method
    such as Gibbs sampling (for further information, see [Chapter 4](0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml),
    *Bayesian Networks and Hidden Markov Models*). However, as this algorithm samples
    from the conditionals *p(x|h)* and *p(h|x)*, rather than from the full joint distribution *p(x,
    h)*, it requires the associated Markov chain to reach its stationary distribution, *π*,
    in order to provide valid samples. As we don't know how many sampling steps are
    required to reach *π*, Gibbs sampling can also be an unfeasible solution because
    of its potentially high computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to solve this problem, Hinton proposed (in *A Practical Guide to Training
    Restricted Boltzmann Machines, **Hinton G., Dept. Computer Science, University
    of Toronto*) an alternative algorithm called **CD-k**. The idea is very simple
    but extremely effective: instead of waiting for the Markov chain to reach the
    stationary distribution, we sample a fixed number of times starting from a training
    sample at *t=0 x^((0))* and computing *h^((1))* by sampling from *p(h^((1))|x^((0)))*.
    Then, the hidden vector is employed to sample the reconstruction, *x^((2))*, from
    *p(x^((2))|h^((1)))*. This procedure can be repeated any number of times, but
    in practice, a single sampling step is normally enough to ensure quite good accuracy.
    At this point, the gradient of the log-likelihood is approximated as (considering
    *t* steps):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/965a0a11-806a-41f2-bd50-f53c1bc01e98.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The single gradients with respect to *w[ij]*, *b**[i]*, and *c**[j]* can be
    easily obtained considering the preceding procedure. The term *contrastive* derives
    from the approximation of the gradient of *L* computed at *x*^(*(0)* )with a weighted
    difference between a term called the *positive gradient* and another defined as the
    *negative gradient*. This approach is analogous to the approximation of a derivative
    with this incremental ratio:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f8d9d1c3-6a85-42b1-a79d-55f7ea07d8af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The complete RBM training algorithm, based on a single-step CD-k is (assuming
    that there are *M* training samples):'
  prefs: []
  type: TYPE_NORMAL
- en: Set the number, *N[h]*, of hidden units
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a number of epochs, *N[e]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a `learning_rate`* η* (for example, *η = 0.01*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *e=1* to *N[e]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *Δw = 0*, *Δb = 0*, and *Δc = 0*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i=1* to *M*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *h^((i))* from *p(h|x^((i)))*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a reconstruction *x^((i+1))* from *p(x^((i+1))|h^((i)))*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Accumulate the updates for weights and biases:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Δw += p(h = 1|x^((i))**)x^((i)) - p(h = 1|x^((i+1)))x^((i+1))* (as outer product)'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Δb += x^((i)) - x^((i+1))*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Δc += p(h = 1|x^((i))) - p(h = 1|x^((i+1)))*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update weights and biases:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*w += ηΔw*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*b += ηΔb*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*c += ηΔc*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The outer product between two vectors is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9249259-6881-455f-8b01-1091e0686677.png)'
  prefs: []
  type: TYPE_IMG
- en: If vector *a* has an *(n, 1)* shape and *b* has an *(m, 1)* shape, the result
    is a matrix with a *(n, m)* shape.
  prefs: []
  type: TYPE_NORMAL
- en: DBNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Belief or Bayesian network is a concept already explored in [Chapter 4](0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml),
    *Bayesian Networks and Hidden Markov Models*. In this particular case, we are
    going to consider Belief Networks where there are visible and latent variables,
    organized into homogeneous layers. The first layer always contains the input (visible)
    units, while all the remaining ones are latent. Hence, a DBN can be structured
    as a stack of RBMs, where each hidden layer is also the visible one of the subsequent
    RBM, as shown in the following diagram (the number of units can be different for
    each layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48c9ddc4-bedc-4d1b-8287-bc07533adaf2.png)'
  prefs: []
  type: TYPE_IMG
- en: Structure of a generic Deep Belief Network
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning procedure is usually greedy and step-wise (as proposed in *A fast
    learning algorithm for deep belief nets*,*Hinton G. E.*, *Osindero S.*, *Teh Y.
    W.*, *Neural Computation, 18/7*). The first RBM is trained with the dataset and
    optimized to reconstruct the original distribution using the CD-k algorithm. At
    this point, the internal (hidden) representations are employed as input for the
    next RBM, and so on until all the blocks are fully trained. In this way, the DBN
    is forced to create subsequent internal representations of the dataset that can
    be used for different purposes. Of course, when the model is trained, it''s possible
    to infer from the recognition (inverse) model sampling from the hidden layers
    and compute the activation probability as (*x* represents a generic cause):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08b8c50d-ce2e-42a8-a5e2-7569624636ff.png)'
  prefs: []
  type: TYPE_IMG
- en: As a DBN is always a generative process, in an unsupervised scenario, it can
    perform a component analysis/dimensionality reduction with an approach that is
    based on the idea of creating a chain of sub-processes, which are able to rebuild
    an internal representation. While a single RBM focuses on a single hidden layer
    and hence cannot learn sub-features, a DBN greedily learns how to represent each
    sub-feature vector using a refined hidden distribution. The concept behind this
    process is not very different from a cascade of convolutional layers, with the
    main difference that in this case, the learning procedure is greedy. Another distinction
    with methods such as PCA is that we don't know exactly how the internal representation
    is built. As the latent variables are optimized by maximizing the log-likelihood,
    there are possibly many optimal points but we cannot easily impose constraints
    on them. However, DBNs show very powerful properties in different scenarios, even
    if their computational cost is normally considerably higher than other methods.
    One of the main problems (common to the majority of deep learning methods) concerns
    the right choice of hidden units in every layer. As they represent latent variables,
    their number is a crucial factor for the success of a training procedure. The
    right choice is not immediate, because it's necessary to know the complexity of
    the data-generating process, however, as a rule of thumb, I suggest starting with
    a couple of layers containing 32/64 units and proceeding to increase the number
    of hidden neurons and the layers until the desired accuracy is reached (in the
    same way, I suggest starting with a small learning rate, for example, 0.01 -,
    increasing it if necessary).
  prefs: []
  type: TYPE_NORMAL
- en: As the first RBM is responsible for reconstructing the original dataset, it's
    very useful to monitor the log-likelihood (or the error) after each epoch in order
    to understand whether the process is learning correctly (decreasing error) or
    it's saturating the capacity. It's clear that an initial bad reconstruction leads
    to subsequently worse representations. As the learning process is greedy, in an
    unsupervised task there's no way to improve the performance of lower layers when
    the previous training steps are finished therefore, I always suggest tuning up
    the parameters so that the first reconstruction is very accurate. Of course, all
    the considerations about overfitting are still valid, so, it's also important
    to monitor the generalization ability with validation samples. However, in a component
    analysis, we assume we're working with a distribution that is representative of
    the underlying data-generating process, so the risk of finding before-seen features
    should be minimal.
  prefs: []
  type: TYPE_NORMAL
- en: In a supervised scenario, there are generally two options whose first step is
    always a greedy training of the DBN. However, the first approach performs a subsequent
    refinement using a standard algorithm, such as backpropagation (considering the
    whole architecture as a single deep network), while the second one uses the last
    internal representation as the input of a separate classifier. It goes without
    saying that the first method has many more degrees of freedom because it works
    with a pre-trained network whose weights can be adjusted until the validation
    accuracy reaches its maximum value. In this case, the first greedy step works
    with the same assumption that has been empirically confirmed by observing the
    internal behavior of deep models (similar to convolutional networks). The first
    layers learn how to detect low-level features, while all the subsequent ones increase
    the details. Therefore, the backpropagation step presumably starts from a point
    that is already quite close to the optimum and can converge more quickly. Conversely,
    the second approach is analogous to applying the kernel trick to a standard **Support
    Vector Machine** (**SVM**). In fact, the external classifier is generally a very
    simple one (such as a logistic regression or an SVM) and the increased accuracy
    is normally due to an improved linear separability obtained by projecting the
    original samples onto a sub-space (often higher-dimensional) where they can be
    easily classified. In general, this method yields worse performance than the first
    one because there's no way to tune up the parameters once the DBN is trained.
    Therefore, when the final projections are not suitable for a linear classification,
    it's necessary to employ more complex models and the resulting computational cost
    can be very high without a proportional performance gain. As deep learning is
    generally based on the concept of end-to-end learning, training the whole network
    can be useful to implicitly include the pre-processing steps in the complete structure,
    which becomes a black box that associates input samples with specific outcomes.
    On the other hand, whenever an explicit pipeline is requested, greedy-training
    the DBN and employing a separate classifier could be a more suitable solution.
  prefs: []
  type: TYPE_NORMAL
- en: Example of unsupervised DBN in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we are going to use a Python library freely available on GitHub
    ([https://github.com/albertbup/deep-belief-network](https://github.com/albertbup/deep-belief-network))
    that allows working with supervised and unsupervised DBN using NumPy (CPU-only)
    or Tensorflow (CPU or GPU support) with the standard Scikit-Learn interface. Our
    goal is to create a lower-dimensional representation of a subset of the `mnist`
    dataset (as the training process can be quite slow, we''ll limit it to `400` samples).
    The first step is loading (using the Keras helper function), shuffling, and normalizing
    the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can create an instance of the the `UnsupervisedDBN` class,
    setting three layers with respectively `512`, `256`, and `64` sigmoid units (as
    we want to bind the values between `0` and `1`). The learning rate, *η* (`learning_rate_rbm`),
    is set equal to `0.05`, the batch size (`batch_size`) to `64`, and the number
    of epochs for each RBM (`n_epochs_rbm`) to `100`. The default value for the number
    of CD-k steps is `1`, but it''s possible to change it using the `contrastive_divergence_iter` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training process is complete, the `X_dbn` array contains the values
    sampled from the last hidden layer. Unfortunately, this library doesn''t implement
    an inverse transformation method, but we can use the t-SNE algorithm to project
    the distribution onto a bidimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The corresponding plot is shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2709e21a-b1ee-4023-95ea-211d043f6c88.png)'
  prefs: []
  type: TYPE_IMG
- en: t-SNE plot of the last DBN hidden layer distribution (64-dimensional)
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, even if there are still a few anomalies, the hidden low-dimensional
    representation is globally coherent with the original dataset because the group
    containing the same digits is organized in compact clusters that preserve some
    geometrical properties. For example, the group containing the digits representing
    a **1** is very close to the one containing the images of 7s, as well as the groups
    of 3s and 8s. This result confirms that a DBN can be successfully employed as
    a preprocessing layer for classification purposes, but in this case, rather than
    reducing the dimensionality, it's often preferable to increase it, in order to
    exploit the redundancy to use a simpler linear classifier (to better understand
    this concept, think about augmenting a dataset with polynomial features). I invite
    you to test this ability by preprocessing the whole MNIST dataset and then classifying
    it using a logistic regression, comparing the results with a direct approach.
  prefs: []
  type: TYPE_NORMAL
- en: The library can be installed using the `pip install git+git://github.com/albertbup/deep-belief-network.git` command
    (NumPy or Tensorflow CPU) or `pip install git+git://github.com/albertbup/deep-belief-network.git@master_gpu`
    (Tensorflow GPU). In both cases, the commands will also install Tensorflow and
    other dependencies that are often present in common Python distributions (such
    as Anaconda); therefore, in order to limit the installation only to the DBN component,
    it's necessary to add the `--no-deps` attribute to the `pip` command. For further
    information, please refer to the GitHub page.
  prefs: []
  type: TYPE_NORMAL
- en: Example of Supervised DBN with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we are going to employ the KDD Cup '99 dataset (provided by
    Scikit-Learn), which contains the logs generated by an intrusion detection system
    exposed to normal and dangerous network activities. We are focusing only on the
    `smtp` sub-dataset, which is the smallest one, because, as explained before, the
    training process can be very long. This dataset is not extremely complex and it
    can be successfully classified with simpler methods; however, the example has
    only a didactic purpose and can be useful for understanding how to work with this
    kind of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to load the dataset, encode the labels (which are strings),
    and standardize the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can create train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is based on an instance of the `SupervisedDBNClassification` class,
    which implements the backpropagation method. The parameters are very similar to
    the unsupervised case, but now we can also specify the **stochastic gradient descent**
    (**SGD**) learning rate (`learning_rate`), the number of backpropagation epochs
    (`n_iter_backprop`), and an optional dropout (`dropout_p`). The algorithm performs
    an initial greedy training (whose computational cost is normally higher than the
    SGD phase), followed by a fine-tuning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training process is finished, we can evaluate performance on the test
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The validation accuracy is `1.0` (there are no misclassifications), but this
    is really a simple dataset that needs only a few minutes of training. I invite
    you to test the performance of a DBN in the classification of the MNIST/Fashion
    MNIST dataset, comparing the results with the one obtained using a deep convolutional
    network. In this case, it''s important to monitor the reconstruction error of
    each RBM, trying to minimize it before running the backpropagation phase. At the
    end of this exercise, you should be able to answer this question: which is preferable,
    an end-to-end or a preprocessing-based approach?'
  prefs: []
  type: TYPE_NORMAL
- en: When running these experiments, where there's an intensive use of sampling,
    I always suggest setting the random seed (and keeping it constant) in NumPy in
    order to guarantee reproducibility (using the `np.random.seed(...)` command).
    As this library also works with Tensorflow, it's necessary to repeat the operation
    using the `tf.set_random_seed(...)` command.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we presented the MRF as the underlying structure of an RBM.
    An MRF is represented as an undirected graph whose vertices are random variables.
    In particular, for our purposes, we considered MRFs whose joint probability can
    be expressed as a product of the positive functions of each random variable. The
    most common distribution, based on an exponential, is called the Gibbs (or Boltzmann)
    distribution and it is particularly suitable for our problems because the logarithm
    cancels the exponential, yielding simpler expressions.
  prefs: []
  type: TYPE_NORMAL
- en: An RBM is a simple bipartite, undirected graph, made up of visible and latent
    variables, with connections only between different groups. The goal of this model
    is to learn a probability distribution, thanks to the presence of hidden units
    that can model the unknown relationships. Unfortunately, the log-likelihood, although
    very simple, cannot be easily optimized because the normalization term requires
    summing over all the input values. For this reason, Hinton proposed an alternative
    algorithm, called CD-k, which outputs an approximation of the gradient of the
    log-likelihood based on a fixed number (normally 1) of Gibbs sampling steps.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking multiple RBMs allows modeling DBNs, where the hidden layer of each
    block is also the visible layer of the following one. DBN can be trained using
    a greedy approach, maximizing the log-likelihood of each RBM in sequence. In an
    unsupervised scenario, a DBN is able to extract the features of a data-generating
    process in a hierarchical way, and therefore the application includes component
    analysis and dimensionality reduction. In a supervised scenario, a DBN can be
    greedily pre-trained and fine-tuned using the backpropagation algorithm (considering
    the whole network) or sometimes using a preprocessing step in a pipeline where
    the classifier is generally a very simple model (such as a logistic regression).
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 14](51bcd684-080e-4354-b2ed-60430bd15f6d.xhtml), *Introduction
    to Reinforcement Learning*, we are going to introduce the concept of reinforcement
    learning, discussing the most important elements of systems that can autonomously
    learn to play a game or allow a robot to walk, jump, and perform tasks that are
    extremely difficult to model and control using classic methods.
  prefs: []
  type: TYPE_NORMAL
