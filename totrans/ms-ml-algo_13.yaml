- en: Deep Belief Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度信念网络
- en: In this chapter, we are going to present two probabilistic generative models
    that employ a set of latent variables to represent a specific data generation
    process. **Restricted Boltzmann Machines** (**RBMs**), proposed in 1986, are the
    building blocks of a more complex model, called a **Deep Belief Network** (**DBN**),
    which is capable of capturing complex relationships among features at different
    levels (in a way not dissimilar to a deep convolutional network). Both models
    can be used in unsupervised and supervised scenarios as preprocessors or, as is
    usual with DBN, fine-tuning the parameters using a standard backpropagation algorithm.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍两种使用一组潜在变量来表示特定数据生成过程的概率生成模型。**受限玻尔兹曼机**（**RBMs**），于1986年提出，是更复杂模型（称为**深度信念网络**（**DBN**））的构建块，能够捕捉不同层次特征之间的复杂关系（在某种程度上与深度卷积网络相似）。这两种模型都可以在无监督和监督场景中使用作为预处理程序，或者，如DBN通常所做的那样，使用标准反向传播算法调整参数。
- en: 'In particular, we will discuss:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将讨论：
- en: '**Markov random fields** (**MRF**)'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫随机场**（**MRF**）'
- en: RBM
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBM
- en: '**Contrastive Divergence** (**CD-k**) algorithm'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对比发散**（**CD-k**）算法'
- en: DBN with supervised and unsupervised examples
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有监督和无监督示例的DBN
- en: MRF
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MRF
- en: 'Let''s consider a set of random variables, *x[i]*, organized in an undirected
    graph, *G=(V, E)*, as shown in the following diagram:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一组随机变量，*x[i]*，它们在一个无向图*G=(V, E)*中组织，如下图所示：
- en: '![](img/78d1aaaf-ec2e-4308-af39-97b91d0f6f98.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/78d1aaaf-ec2e-4308-af39-97b91d0f6f98.png)'
- en: Example of a probabilistic undirected graph
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 概率无向图的示例
- en: 'Two random variables, *a* and *b*, are conditionally independent given the
    random variable, *c* if:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个随机变量*a*和*b*在随机变量*c*的条件下是条件独立的，那么：
- en: '![](img/3d40802c-56e7-4a0f-aef4-c9a4a5758b21.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3d40802c-56e7-4a0f-aef4-c9a4a5758b21.png)'
- en: Now, consider the graph again; if all generic couples of subsets of variables
    *S[i]* and *S[j]* are conditionally independent given a separating subset, *S[k]*
    (so that all connections between variables belonging to *S[i]* to variables belonging
    to *S[j]* pass through *S[k]*), the graph is called a **Markov random field**
    (**MRF**).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，再次考虑这个图；如果所有变量子集*S[i]*和*S[j]*的通用配对在分离子集*S[k]*（这样属于*S[i]*的变量与属于*S[j]*的变量之间的所有连接都通过*S[k]*）的条件下是条件独立的，那么这个图被称为**马尔可夫随机场**（**MRF**）。
- en: 'Given *G=(V, E)*, a subset containing vertices such that every couple is adjacent
    is called a **clique** (the set of all cliques is often denoted as *cl(G))*. For
    example, consider the graph shown previously; *(x[0], x[1])* is a clique and if
    *x[0]* and *x[5]* were connected, *(x[0], x[1], x[5])* would be a clique. A **maximal
    clique** is a clique that cannot be expanded by adding new vertices. A particular
    family of MRF is made up of all those graphs whose joint probability distribution
    can be factorized as:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 给定*G=(V, E)*，一个包含所有相邻顶点的子集称为**团**（所有团的集合通常表示为*cl(G)*）。例如，考虑之前显示的图；（x[0]，x[1]）是一个团，如果x[0]和x[5]相连，那么（x[0]，x[1]，x[5]）将是一个团。**最大团**是一个不能通过添加新顶点来扩展的团。一个特定的MRF家族由所有那些联合概率分布可以分解为以下形式的图组成：
- en: '![](img/3a6781e8-0fcd-45e0-8fc7-3fc62a442155.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3a6781e8-0fcd-45e0-8fc7-3fc62a442155.png)'
- en: 'In this case, *α* is the normalizing constant and the product is extended to
    the set of all maximal cliques. According to the **Hammersley–Clifford** theorem
    (for further information, please refer to *Proof of Hammersley-Clifford Theorem*, *Cheung
    S., University of Kentucky, 2008*), if the joint probability density function
    is strictly positive, the MRF can be factorized and all the *ρ[i]* functions are
    strictly positive too. Hence *p(x)*, after some straightforward manipulations
    based on the properties of logarithms, can be rewritten as a **Gibbs** (or **Boltzmann**)
    distribution:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，*α*是归一化常数，乘积扩展到所有最大团集合。根据**汉密尔顿-克利福德定理**（有关更多信息，请参阅*Cheung S.*，肯塔基大学，2008年的*汉密尔顿-克利福德定理证明*），如果联合概率密度函数是严格正的，MRF可以分解，所有*ρ[i]*函数也是严格正的。因此，基于对数性质的一些简单操作后，*p(x)*可以重写为**吉布斯**（或**玻尔兹曼**）分布：
- en: '![](img/caff2327-6603-415c-9a9b-805e948b04af.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/caff2327-6603-415c-9a9b-805e948b04af.png)'
- en: 'The term *E(x)* is called energy, as it derives from the first application
    of such a distribution in statistical physics. *1/Z* is now the normalizing constant
    employing the standard notation. In our scenarios, we always consider graphs containing
    observed *(x[i])* and latent variables *(h[j])*. Therefore, it''s useful to express
    the joint probability as:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 *E(x)* 被称为能量，因为它源自统计物理中此类分布的第一个应用。*1/Z* 现在是使用标准符号表示的正则化常数。在我们的场景中，我们始终考虑包含观测值
    *(x[i])* 和潜在变量 *(h[j])* 的图。因此，将联合概率表示为是有用的：
- en: '![](img/0fd98304-a2bb-4dc8-a801-9e354591b72a.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0fd98304-a2bb-4dc8-a801-9e354591b72a.png)'
- en: 'Whenever it''s necessary to marginalize to obtain *p(x)*, we can simply sum
    over *h[j]*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 每当需要边缘化以获得 *p(x)* 时，我们可以简单地对 *h[j]* 求和：
- en: '![](img/7aa19f63-e8ff-46ac-906c-acbaa30e0007.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7aa19f63-e8ff-46ac-906c-acbaa30e0007.png)'
- en: RBMs
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RBMs
- en: 'A RBM (originally called **Harmonium**) is a neural model proposed by Smolensky
    (in *I**nformation processing in dynamical systems: Foundations of harmony theory, **Smolensky
    P., Parallel Distributed Processing, Vol 1, The MIT Press*) that is made up of
    a layer of input (observable) neurons and a layer of hidden (latent) neurons.
    A generic structure is shown in the following diagram:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: RBM（最初称为**Harmonium**）是由Smolensky提出的神经网络模型（在*《信息处理在动态系统中的基础：和谐理论的基础》**，Smolensky
    P.，并行分布式处理，第1卷，麻省理工学院出版社*），由一层输入（可观测）神经元和一层隐藏（潜在）神经元组成。以下图表显示了其通用结构：
- en: '![](img/4e23e3da-c002-4238-bd80-aa05e33d7ba7.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4e23e3da-c002-4238-bd80-aa05e33d7ba7.png)'
- en: Structure of Restricted Boltzmann Machine
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 限制玻尔兹曼机的结构
- en: 'As the undirected graph is bipartite (there are no connections between neurons
    belonging to the same layer), the underlying probabilistic structure is MRF. In
    the original model (even if this is not a restriction), all the neurons are assumed
    to be Bernoulli-distributed *(x[i], h[i] = {0, 1})*, with a bias, *b[i]* (for
    the observed units) and *c[j]* (for the latent neurons). The resulting energy
    function is:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无向图是二分图（同一层的神经元之间没有连接），其潜在的概率结构是MRF。在原始模型中（即使这不是一个限制），所有神经元都被假定为伯努利分布 *(x[i],
    h[i] = {0, 1})*，带有偏置，*b[i]*（对于观测单元）和*c[j]*（对于潜在神经元）。得到的能量函数是：
- en: '![](img/204d3b4b-ad7e-4053-8488-26cd2fb2586e.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/204d3b4b-ad7e-4053-8488-26cd2fb2586e.png)'
- en: 'A RBM is a probabilistic generative model that can learn a data-generating
    process, *p[data]*, which is represented by the observed units but exploits the
    presence of the latent variables in order to model all the internal relationships.
    If we summarized all the parameters in a single vector, *θ = {w[ij], b[i], c[j]}*,
    the Gibbs distribution becomes:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: RBM是一种概率生成模型，可以学习数据生成过程，*p[data]*，它由观测单元表示，但利用潜在变量的存在来模拟所有内部关系。如果我们把所有参数总结成一个向量，*θ
    = {w[ij], b[i], c[j]}*，吉布斯分布变为：
- en: '![](img/1683c289-da77-49c0-9c58-4e473d85ddbe.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1683c289-da77-49c0-9c58-4e473d85ddbe.png)'
- en: 'The training goal of a RBM is to maximize the log-likelihood with respect to
    an input distribution. Hence, the first step is determining *L(θ; x)* after the
    marginalization of the previous expression:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RBM的训练目标是最大化输入分布相对于对数似然。因此，第一步是在对前一个表达式进行边缘化后确定*L(θ; x)*：
- en: '![](img/0caa878b-b9a4-48c6-b8ac-8ef0e40261e4.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0caa878b-b9a4-48c6-b8ac-8ef0e40261e4.png)'
- en: 'As we need to maximize the log-likelihood, it''s useful to compute the gradient
    with respect to *θ*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要最大化对数似然，计算相对于*θ*的梯度是有用的：
- en: '![](img/773cb0ac-76e3-4105-bc66-f69e1f97022c.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/773cb0ac-76e3-4105-bc66-f69e1f97022c.png)'
- en: 'Applying the chain rule of derivatives, we get:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 应用导数的链式法则，我们得到：
- en: '![](img/da3493f6-5cad-4768-b4cb-8571af2f0bac.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/da3493f6-5cad-4768-b4cb-8571af2f0bac.png)'
- en: 'Using the conditional and joint probability equalities, the previous expression
    becomes:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用条件概率和联合概率等式，前面的表达式变为：
- en: '![](img/86e3b4e3-2e5d-45d8-9d21-f2ebd696ddcb.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/86e3b4e3-2e5d-45d8-9d21-f2ebd696ddcb.png)'
- en: 'Considering the full joint probability, after some tedious manipulations (which
    we omit), it''s possible to derive the following expressions (*σ(•)* is the sigmoid
    function):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到完整的联合概率，经过一些繁琐的操作（此处省略），可以推导出以下表达式（*σ(•)* 是Sigmoid函数）：
- en: '![](img/ed282e75-cbb0-4076-b3b6-1ac21c45924e.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ed282e75-cbb0-4076-b3b6-1ac21c45924e.png)'
- en: 'At this point, we can compute the gradient of the log-likelihood with respect
    to each single parameter, *w[ij]*, *b**[i]*, and *c**[j]*. Starting with *w[ij]*,
    and considering that *∇[wij] E(x, h; θ) = -x[i]h[j]*, we get:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以计算对数似然相对于每个单个参数的梯度，*w[ij]*，*b**[i]*，和*c**[j]*。从*w[ij]*开始，考虑到*∇[wij]
    E(x, h; θ) = -x[i]h[j]*，我们得到：
- en: '![](img/8f7b51aa-8366-498c-b37a-ca9042c5d5ae.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8f7b51aa-8366-498c-b37a-ca9042c5d5ae.png)'
- en: 'The expression can be rewritten as:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该表达式可以重写为：
- en: '![](img/28b42155-e075-4942-83c0-555285d9340e.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/28b42155-e075-4942-83c0-555285d9340e.png)'
- en: 'Now, considering that all the units are Bernoulli-distributed, and isolating
    only the *j^(th)* hidden unit, it''s possible to apply the simplification:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到所有单元都是伯努利分布的，并且仅隔离第 *j* 个隐藏单元，可以应用以下简化：
- en: '![](img/5097a2ee-27cf-4eba-8a95-2fd06a5b75f5.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5097a2ee-27cf-4eba-8a95-2fd06a5b75f5.png)'
- en: 'Therefore, the gradient becomes:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，梯度变为：
- en: '![](img/8d12c1d4-c022-4040-a344-d6993cc18acb.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8d12c1d4-c022-4040-a344-d6993cc18acb.png)'
- en: 'Analogously, we can derive the gradient of *L* with respect to *b*[*i* ]and
    *c[j]*:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以推导出 *L* 对 *b[i*  ]和 *c[j]* 的梯度：
- en: '![](img/6265c30f-5188-4089-a8b9-4ba746d8a035.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6265c30f-5188-4089-a8b9-4ba746d8a035.png)'
- en: Hence, the first term of every gradient is very easy to compute, while the second
    one requires summing over all observed values. As this operation is impracticable,
    the only feasible alternative is an approximation based on sampling, using a method
    such as Gibbs sampling (for further information, see [Chapter 4](0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml),
    *Bayesian Networks and Hidden Markov Models*). However, as this algorithm samples
    from the conditionals *p(x|h)* and *p(h|x)*, rather than from the full joint distribution *p(x,
    h)*, it requires the associated Markov chain to reach its stationary distribution, *π*,
    in order to provide valid samples. As we don't know how many sampling steps are
    required to reach *π*, Gibbs sampling can also be an unfeasible solution because
    of its potentially high computational cost.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个梯度的第一个项非常容易计算，而第二个项则需要对所有观察到的值进行求和。由于这个操作不切实际，唯一可行的替代方案是基于采样的近似，使用如吉布斯采样（更多信息，见[第4章](0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml)，*贝叶斯网络和隐马尔可夫模型*)。然而，因为这个算法从条件分布
    *p(x|h)* 和 *p(h|x)* 中采样，而不是从完整的联合分布 *p(x, h)* 中采样，它需要相关的马尔可夫链达到其平稳分布 *π*，以便提供有效的样本。由于我们不知道需要多少采样步骤才能达到
    *π*，吉布斯采样也可能因为其可能的高计算成本而成为不可行的解决方案。
- en: 'In order to solve this problem, Hinton proposed (in *A Practical Guide to Training
    Restricted Boltzmann Machines, **Hinton G., Dept. Computer Science, University
    of Toronto*) an alternative algorithm called **CD-k**. The idea is very simple
    but extremely effective: instead of waiting for the Markov chain to reach the
    stationary distribution, we sample a fixed number of times starting from a training
    sample at *t=0 x^((0))* and computing *h^((1))* by sampling from *p(h^((1))|x^((0)))*.
    Then, the hidden vector is employed to sample the reconstruction, *x^((2))*, from
    *p(x^((2))|h^((1)))*. This procedure can be repeated any number of times, but
    in practice, a single sampling step is normally enough to ensure quite good accuracy.
    At this point, the gradient of the log-likelihood is approximated as (considering
    *t* steps):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Hinton在《*训练受限玻尔兹曼机的实用指南，**Hinton G**.，多伦多大学计算机科学系）中提出了一种名为**CD-k**的替代算法。这个想法非常简单但非常有效：我们不是等待马尔可夫链达到平稳分布，而是从训练样本
    *t=0 x^((0))* 开始固定次数采样，并通过从 *p(h^((1))|x^((0)))* 中采样来计算 *h^((1))*。然后，使用隐藏向量从 *p(x^((2))|h^((1)))*
    中采样重建 *x^((2))*。这个程序可以重复任意次数，但在实践中，通常只需要一个采样步骤就能确保相当高的精度。在这个点上，对数似然梯度的近似可以表示为（考虑
    *t* 步）：
- en: '![](img/965a0a11-806a-41f2-bd50-f53c1bc01e98.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/965a0a11-806a-41f2-bd50-f53c1bc01e98.png)'
- en: 'The single gradients with respect to *w[ij]*, *b**[i]*, and *c**[j]* can be
    easily obtained considering the preceding procedure. The term *contrastive* derives
    from the approximation of the gradient of *L* computed at *x*^(*(0)* )with a weighted
    difference between a term called the *positive gradient* and another defined as the
    *negative gradient*. This approach is analogous to the approximation of a derivative
    with this incremental ratio:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 相对于 *w[ij]*，*b[i]* 和 *c[j]* 的单梯度可以很容易地通过前面的程序获得。术语 *对比度* 来自于在 *x*^(*(0)*  )处计算的
    *L* 的梯度近似，通过一个称为 *正梯度* 的项和一个称为 *负梯度* 的项之间的加权差。这种方法类似于用这个增量比来近似导数：
- en: '![](img/f8d9d1c3-6a85-42b1-a79d-55f7ea07d8af.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f8d9d1c3-6a85-42b1-a79d-55f7ea07d8af.png)'
- en: 'The complete RBM training algorithm, based on a single-step CD-k is (assuming
    that there are *M* training samples):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基于单步 CD-k 的完整 RBM 训练算法（假设有 *M* 个训练样本）：
- en: Set the number, *N[h]*, of hidden units
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '设置隐藏单元的数量，*N[h]* '
- en: Set a number of epochs, *N[e]*
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '设置一个epoch的数量，*N[e]* '
- en: Set a `learning_rate`* η* (for example, *η = 0.01*)
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置学习率 `learning_rate`* η*（例如，*η = 0.01*）
- en: 'For *e=1* to *N[e]*:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *e=1* 到 *N[e]*：
- en: Set *Δw = 0*, *Δb = 0*, and *Δc = 0*
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *Δw = 0*，*Δb = 0*，和 *Δc = 0*
- en: 'For *i=1* to *M*:'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *M*：
- en: Sample *h^((i))* from *p(h|x^((i)))*
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *p(h|x^((i)))* 中采样 *h^((i))*
- en: Sample a reconstruction *x^((i+1))* from *p(x^((i+1))|h^((i)))*
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *p(x^((i+1))|h^((i)))* 中采样重建 *x^((i+1))*
- en: 'Accumulate the updates for weights and biases:'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 累加权重和偏置的更新：
- en: '*Δw += p(h = 1|x^((i))**)x^((i)) - p(h = 1|x^((i+1)))x^((i+1))* (as outer product)'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Δw += p(h = 1|x^((i)))x^((i)) - p(h = 1|x^((i+1)))x^((i+1))*（作为外积）'
- en: '*Δb += x^((i)) - x^((i+1))*'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Δb += x^((i)) - x^((i+1))*'
- en: '*Δc += p(h = 1|x^((i))) - p(h = 1|x^((i+1)))*'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Δc += p(h = 1|x^((i))) - p(h = 1|x^((i+1)))*'
- en: 'Update weights and biases:'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重和偏置：
- en: '*w += ηΔw*'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*w += ηΔw*'
- en: '*b += ηΔb*'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*b += ηΔb*'
- en: '*c += ηΔc*'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*c += ηΔc*'
- en: 'The outer product between two vectors is defined as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 两个向量之间的外积定义为：
- en: '![](img/c9249259-6881-455f-8b01-1091e0686677.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9249259-6881-455f-8b01-1091e0686677.png)'
- en: If vector *a* has an *(n, 1)* shape and *b* has an *(m, 1)* shape, the result
    is a matrix with a *(n, m)* shape.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果向量 *a* 的形状为 *(n, 1)*，而 *b* 的形状为 *(m, 1)*，则结果是形状为 *(n, m)* 的矩阵。
- en: DBNs
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBNs
- en: 'A Belief or Bayesian network is a concept already explored in [Chapter 4](0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml),
    *Bayesian Networks and Hidden Markov Models*. In this particular case, we are
    going to consider Belief Networks where there are visible and latent variables,
    organized into homogeneous layers. The first layer always contains the input (visible)
    units, while all the remaining ones are latent. Hence, a DBN can be structured
    as a stack of RBMs, where each hidden layer is also the visible one of the subsequent
    RBM, as shown in the following diagram (the number of units can be different for
    each layer):'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 信念网络或贝叶斯网络是一个已在[第4章](0870cfd8-26b1-4e7a-9bf6-c81c84ac46b3.xhtml)“贝叶斯网络和隐马尔可夫模型”中探讨的概念。在这种情况下，我们将考虑具有可见和潜在变量，并组织成同质层的信念网络。第一层始终包含输入（可见）单元，而所有其余的都是潜在变量。因此，深度信念网络（DBN）可以结构化为堆叠的RBM，其中每个隐藏层也是后续RBM的可见层，如下面的图所示（每层的单元数可能不同）：
- en: '![](img/48c9ddc4-bedc-4d1b-8287-bc07533adaf2.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/48c9ddc4-bedc-4d1b-8287-bc07533adaf2.png)'
- en: Structure of a generic Deep Belief Network
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通用深度信念网络的架构
- en: 'The learning procedure is usually greedy and step-wise (as proposed in *A fast
    learning algorithm for deep belief nets*,*Hinton G. E.*, *Osindero S.*, *Teh Y.
    W.*, *Neural Computation, 18/7*). The first RBM is trained with the dataset and
    optimized to reconstruct the original distribution using the CD-k algorithm. At
    this point, the internal (hidden) representations are employed as input for the
    next RBM, and so on until all the blocks are fully trained. In this way, the DBN
    is forced to create subsequent internal representations of the dataset that can
    be used for different purposes. Of course, when the model is trained, it''s possible
    to infer from the recognition (inverse) model sampling from the hidden layers
    and compute the activation probability as (*x* represents a generic cause):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 学习过程通常是贪婪的、逐步的（如Hinton G. E.、Osindero S.、Teh Y. W.在*《深度信念网的快速学习算法》*中提出）。第一个RBM使用数据集进行训练，并使用CD-k算法优化以重建原始分布。在此阶段，内部（隐藏）表示被用作下一个RBM的输入，依此类推，直到所有块都完全训练。这样，DBN被迫创建后续内部表示，这些表示可以用于不同的目的。当然，当模型训练时，可以从识别（逆）模型中采样隐藏层，并计算激活概率（*x*代表一个通用原因）：
- en: '![](img/08b8c50d-ce2e-42a8-a5e2-7569624636ff.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/08b8c50d-ce2e-42a8-a5e2-7569624636ff.png)'
- en: As a DBN is always a generative process, in an unsupervised scenario, it can
    perform a component analysis/dimensionality reduction with an approach that is
    based on the idea of creating a chain of sub-processes, which are able to rebuild
    an internal representation. While a single RBM focuses on a single hidden layer
    and hence cannot learn sub-features, a DBN greedily learns how to represent each
    sub-feature vector using a refined hidden distribution. The concept behind this
    process is not very different from a cascade of convolutional layers, with the
    main difference that in this case, the learning procedure is greedy. Another distinction
    with methods such as PCA is that we don't know exactly how the internal representation
    is built. As the latent variables are optimized by maximizing the log-likelihood,
    there are possibly many optimal points but we cannot easily impose constraints
    on them. However, DBNs show very powerful properties in different scenarios, even
    if their computational cost is normally considerably higher than other methods.
    One of the main problems (common to the majority of deep learning methods) concerns
    the right choice of hidden units in every layer. As they represent latent variables,
    their number is a crucial factor for the success of a training procedure. The
    right choice is not immediate, because it's necessary to know the complexity of
    the data-generating process, however, as a rule of thumb, I suggest starting with
    a couple of layers containing 32/64 units and proceeding to increase the number
    of hidden neurons and the layers until the desired accuracy is reached (in the
    same way, I suggest starting with a small learning rate, for example, 0.01 -,
    increasing it if necessary).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种始终是生成过程的DBN，在无监督场景中，它可以通过创建一系列子过程的方法来执行成分分析/降维，这些子过程能够重建内部表示。而单个RBM专注于单个隐藏层，因此无法学习子特征，DBN则贪婪地学习如何使用精细化的隐藏分布来表示每个子特征向量。这一过程背后的概念与卷积层级联的概念并没有很大区别，主要区别在于在这种情况下，学习过程是贪婪的。与PCA等方法的另一个区别是，我们并不确切知道内部表示是如何构建的。由于潜在变量是通过最大化对数似然进行优化的，因此可能存在许多最优解，但我们无法轻易地对它们施加约束。然而，DBNs在不同的场景中表现出非常强大的特性，尽管它们的计算成本通常比其他方法高得多。其中一个主要问题（与大多数深度学习方法都有关）是正确选择每一层的隐藏单元。因为它们代表潜在变量，所以它们的数量是训练过程成功的关键因素。正确的选择并不立即显现，因为需要了解数据生成过程的复杂性。然而，作为一个经验法则，我建议从包含32/64个单元的几层开始，然后逐步增加隐藏神经元的数量和层数，直到达到所需的准确度（同样，我建议从一个较小的学习率开始，例如0.01，如果需要则增加）。
- en: As the first RBM is responsible for reconstructing the original dataset, it's
    very useful to monitor the log-likelihood (or the error) after each epoch in order
    to understand whether the process is learning correctly (decreasing error) or
    it's saturating the capacity. It's clear that an initial bad reconstruction leads
    to subsequently worse representations. As the learning process is greedy, in an
    unsupervised task there's no way to improve the performance of lower layers when
    the previous training steps are finished therefore, I always suggest tuning up
    the parameters so that the first reconstruction is very accurate. Of course, all
    the considerations about overfitting are still valid, so, it's also important
    to monitor the generalization ability with validation samples. However, in a component
    analysis, we assume we're working with a distribution that is representative of
    the underlying data-generating process, so the risk of finding before-seen features
    should be minimal.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于第一个RBM负责重建原始数据集，因此监控每个epoch后的对数似然（或错误）非常有用，以便了解过程是否正确学习（错误减少）或者是否已经饱和了容量。很明显，初始的重建不良会导致随后的表示更差。由于学习过程是贪婪的，在无监督任务中，一旦完成之前的训练步骤，就没有办法提高较低层的性能，因此，我总是建议调整参数，使得第一次重建非常准确。当然，关于过拟合的所有考虑仍然有效，因此，监控验证样本的泛化能力也很重要。然而，在成分分析中，我们假设我们正在处理一个能够代表潜在数据生成过程的分布，因此发现先前未见特征的风险应该是最小的。
- en: In a supervised scenario, there are generally two options whose first step is
    always a greedy training of the DBN. However, the first approach performs a subsequent
    refinement using a standard algorithm, such as backpropagation (considering the
    whole architecture as a single deep network), while the second one uses the last
    internal representation as the input of a separate classifier. It goes without
    saying that the first method has many more degrees of freedom because it works
    with a pre-trained network whose weights can be adjusted until the validation
    accuracy reaches its maximum value. In this case, the first greedy step works
    with the same assumption that has been empirically confirmed by observing the
    internal behavior of deep models (similar to convolutional networks). The first
    layers learn how to detect low-level features, while all the subsequent ones increase
    the details. Therefore, the backpropagation step presumably starts from a point
    that is already quite close to the optimum and can converge more quickly. Conversely,
    the second approach is analogous to applying the kernel trick to a standard **Support
    Vector Machine** (**SVM**). In fact, the external classifier is generally a very
    simple one (such as a logistic regression or an SVM) and the increased accuracy
    is normally due to an improved linear separability obtained by projecting the
    original samples onto a sub-space (often higher-dimensional) where they can be
    easily classified. In general, this method yields worse performance than the first
    one because there's no way to tune up the parameters once the DBN is trained.
    Therefore, when the final projections are not suitable for a linear classification,
    it's necessary to employ more complex models and the resulting computational cost
    can be very high without a proportional performance gain. As deep learning is
    generally based on the concept of end-to-end learning, training the whole network
    can be useful to implicitly include the pre-processing steps in the complete structure,
    which becomes a black box that associates input samples with specific outcomes.
    On the other hand, whenever an explicit pipeline is requested, greedy-training
    the DBN and employing a separate classifier could be a more suitable solution.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督场景中，通常有两种选择，它们的第一个步骤总是DBN的贪婪训练。然而，第一种方法使用标准算法（如反向传播，将整个架构视为单个深度网络）进行后续的细化，而第二种方法则使用最后一个内部表示作为单独分类器的输入。不言而喻，第一种方法具有更多的自由度，因为它与一个预训练的网络一起工作，其权重可以调整，直到验证准确率达到最大值。在这种情况下，第一个贪婪步骤与通过观察深度模型的内部行为（类似于卷积网络）经验证的同一种假设一起工作。第一层学习如何检测低级特征，而所有随后的层都增加了细节。因此，反向传播步骤可能从已经非常接近最优点的位置开始，可以更快地收敛。相反，第二种方法类似于将核技巧应用于标准的**支持向量机**（**SVM**）。实际上，外部分类器通常非常简单（如逻辑回归或SVM），而提高的准确度通常是由于通过将原始样本投影到子空间（通常是高维空间）而获得的更好的线性可分性，在那里它们可以很容易地被分类。一般来说，这种方法比第一种方法性能更差，因为没有方法可以调整DBN训练后的参数。因此，当最终投影不适合线性分类时，有必要采用更复杂的模型，而计算成本可能会非常高，而性能增益却不成比例。由于深度学习通常基于端到端学习的概念，因此训练整个网络可以隐式地将预处理步骤包含在完整结构中，这成为一个将输入样本与特定结果关联的黑盒。另一方面，每当需要显式管道时，贪婪训练DBN并使用单独的分类器可能是一个更合适的解决方案。
- en: Example of unsupervised DBN in Python
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中无监督DBN的示例
- en: 'In this example, we are going to use a Python library freely available on GitHub
    ([https://github.com/albertbup/deep-belief-network](https://github.com/albertbup/deep-belief-network))
    that allows working with supervised and unsupervised DBN using NumPy (CPU-only)
    or Tensorflow (CPU or GPU support) with the standard Scikit-Learn interface. Our
    goal is to create a lower-dimensional representation of a subset of the `mnist`
    dataset (as the training process can be quite slow, we''ll limit it to `400` samples).
    The first step is loading (using the Keras helper function), shuffling, and normalizing
    the dataset:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用GitHub上免费提供的Python库（[https://github.com/albertbup/deep-belief-network](https://github.com/albertbup/deep-belief-network)），该库允许使用NumPy（仅CPU）或Tensorflow（CPU或GPU支持）以及标准的Scikit-Learn接口来处理监督和无监督的DBN。我们的目标是创建`mnist`数据集子集的较低维度的表示（由于训练过程可能相当慢，我们将限制为`400`个样本）。第一步是加载（使用Keras辅助函数）、打乱和归一化数据集：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'At this point, we can create an instance of the the `UnsupervisedDBN` class,
    setting three layers with respectively `512`, `256`, and `64` sigmoid units (as
    we want to bind the values between `0` and `1`). The learning rate, *η* (`learning_rate_rbm`),
    is set equal to `0.05`, the batch size (`batch_size`) to `64`, and the number
    of epochs for each RBM (`n_epochs_rbm`) to `100`. The default value for the number
    of CD-k steps is `1`, but it''s possible to change it using the `contrastive_divergence_iter` parameter:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以创建一个 `UnsupervisedDBN` 类的实例，设置三个层，分别有 `512`、`256` 和 `64` 个 sigmoid
    单元（因为我们想将值绑定在 `0` 和 `1` 之间）。学习率 *η* (`learning_rate_rbm`) 设置为 `0.05`，批量大小 (`batch_size`)
    为 `64`，每个 RBM 的训练轮数 (`n_epochs_rbm`) 为 `100`。CD-k 步骤的默认值是 `1`，但可以通过 `contrastive_divergence_iter`
    参数进行更改：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once the training process is complete, the `X_dbn` array contains the values
    sampled from the last hidden layer. Unfortunately, this library doesn''t implement
    an inverse transformation method, but we can use the t-SNE algorithm to project
    the distribution onto a bidimensional space:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，`X_dbn` 数组将包含从最后一个隐藏层采样的值。不幸的是，这个库没有实现逆变换方法，但我们可以使用 t-SNE 算法将分布投影到二维空间：
- en: '[PRE2]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The corresponding plot is shown in the following graph:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的图表如下所示：
- en: '![](img/2709e21a-b1ee-4023-95ea-211d043f6c88.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2709e21a-b1ee-4023-95ea-211d043f6c88.png)'
- en: t-SNE plot of the last DBN hidden layer distribution (64-dimensional)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 图展示了最后一个 DBN 隐藏层分布（64 维）
- en: As you can see, even if there are still a few anomalies, the hidden low-dimensional
    representation is globally coherent with the original dataset because the group
    containing the same digits is organized in compact clusters that preserve some
    geometrical properties. For example, the group containing the digits representing
    a **1** is very close to the one containing the images of 7s, as well as the groups
    of 3s and 8s. This result confirms that a DBN can be successfully employed as
    a preprocessing layer for classification purposes, but in this case, rather than
    reducing the dimensionality, it's often preferable to increase it, in order to
    exploit the redundancy to use a simpler linear classifier (to better understand
    this concept, think about augmenting a dataset with polynomial features). I invite
    you to test this ability by preprocessing the whole MNIST dataset and then classifying
    it using a logistic regression, comparing the results with a direct approach.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，尽管仍然存在一些异常，但隐藏的低维表示与原始数据集全局上一致，因为包含相同数字的组被组织成紧凑的簇，并保留了一些几何属性。例如，包含表示数字
    **1** 的组的簇与包含 7 的图像的组非常接近，以及 3 和 8 的组。这一结果证实，DBN 可以成功用作分类目的的预处理层，但在此情况下，与其减少维度，不如通常更倾向于增加维度，以便利用冗余来使用更简单的线性分类器（为了更好地理解这个概念，想想通过添加多项式特征来增强数据集）。我邀请您通过预处理整个
    MNIST 数据集，然后使用逻辑回归对其进行分类，并将结果与直接方法进行比较来测试这一能力。
- en: The library can be installed using the `pip install git+git://github.com/albertbup/deep-belief-network.git` command
    (NumPy or Tensorflow CPU) or `pip install git+git://github.com/albertbup/deep-belief-network.git@master_gpu`
    (Tensorflow GPU). In both cases, the commands will also install Tensorflow and
    other dependencies that are often present in common Python distributions (such
    as Anaconda); therefore, in order to limit the installation only to the DBN component,
    it's necessary to add the `--no-deps` attribute to the `pip` command. For further
    information, please refer to the GitHub page.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令安装库：`pip install git+git://github.com/albertbup/deep-belief-network.git`（NumPy
    或 Tensorflow CPU）或 `pip install git+git://github.com/albertbup/deep-belief-network.git@master_gpu`（Tensorflow
    GPU）。在两种情况下，命令还会安装 Tensorflow 和其他常见 Python 发行版中通常存在的依赖项（如 Anaconda）；因此，为了仅安装 DBN
    组件，需要在 `pip` 命令中添加 `--no-deps` 属性。有关更多信息，请参阅 GitHub 页面。
- en: Example of Supervised DBN with Python
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Python 的监督 DBN 示例
- en: In this example, we are going to employ the KDD Cup '99 dataset (provided by
    Scikit-Learn), which contains the logs generated by an intrusion detection system
    exposed to normal and dangerous network activities. We are focusing only on the
    `smtp` sub-dataset, which is the smallest one, because, as explained before, the
    training process can be very long. This dataset is not extremely complex and it
    can be successfully classified with simpler methods; however, the example has
    only a didactic purpose and can be useful for understanding how to work with this
    kind of data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用KDD Cup '99数据集（由Scikit-Learn提供），该数据集包含一个入侵检测系统在正常和危险网络活动下生成的日志。我们只关注`smtp`子数据集，这是最小的子集，因为，如前所述，训练过程可能非常耗时。这个数据集并不特别复杂，可以用更简单的方法成功分类；然而，这个例子只有一个教学目的，并且对于理解如何处理这类数据可能是有用的。
- en: 'The first step is to load the dataset, encode the labels (which are strings),
    and standardize the values:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是加载数据集，对标签（字符串类型）进行编码，并对值进行标准化：
- en: '[PRE3]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'At this point, we can create train and test sets:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以创建训练集和测试集：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The model is based on an instance of the `SupervisedDBNClassification` class,
    which implements the backpropagation method. The parameters are very similar to
    the unsupervised case, but now we can also specify the **stochastic gradient descent**
    (**SGD**) learning rate (`learning_rate`), the number of backpropagation epochs
    (`n_iter_backprop`), and an optional dropout (`dropout_p`). The algorithm performs
    an initial greedy training (whose computational cost is normally higher than the
    SGD phase), followed by a fine-tuning:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 模型基于`SupervisedDBNClassification`类的实例，该类实现了反向传播方法。参数与无监督情况非常相似，但现在我们还可以指定**随机梯度下降**（**SGD**）学习率（`learning_rate`）、反向传播的迭代次数（`n_iter_backprop`）以及可选的dropout（`dropout_p`）。算法执行一个初始的贪婪训练（其计算成本通常高于SGD阶段），然后进行微调：
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Once the training process is finished, we can evaluate performance on the test
    set:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，我们就可以在测试集上评估性能：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The validation accuracy is `1.0` (there are no misclassifications), but this
    is really a simple dataset that needs only a few minutes of training. I invite
    you to test the performance of a DBN in the classification of the MNIST/Fashion
    MNIST dataset, comparing the results with the one obtained using a deep convolutional
    network. In this case, it''s important to monitor the reconstruction error of
    each RBM, trying to minimize it before running the backpropagation phase. At the
    end of this exercise, you should be able to answer this question: which is preferable,
    an end-to-end or a preprocessing-based approach?'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 验证准确率为`1.0`（没有误分类），但这实际上是一个简单的数据集，只需要几分钟的训练。我邀请您测试DBN在MNIST/Fashion MNIST数据集分类中的性能，并将结果与使用深度卷积网络获得的结果进行比较。在这种情况下，重要的是要监控每个RBM的重建误差，尝试在运行反向传播阶段之前将其最小化。完成这个练习后，你应该能够回答这个问题：端到端方法与基于预处理的方法哪个更可取？
- en: When running these experiments, where there's an intensive use of sampling,
    I always suggest setting the random seed (and keeping it constant) in NumPy in
    order to guarantee reproducibility (using the `np.random.seed(...)` command).
    As this library also works with Tensorflow, it's necessary to repeat the operation
    using the `tf.set_random_seed(...)` command.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行这些实验时，由于采样使用频繁，我总是建议在NumPy中设置随机种子（并保持其恒定），以确保可重复性（使用`np.random.seed(...)`命令）。由于这个库也与Tensorflow一起工作，因此需要使用`tf.set_random_seed(...)`命令重复此操作。
- en: Summary
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we presented the MRF as the underlying structure of an RBM.
    An MRF is represented as an undirected graph whose vertices are random variables.
    In particular, for our purposes, we considered MRFs whose joint probability can
    be expressed as a product of the positive functions of each random variable. The
    most common distribution, based on an exponential, is called the Gibbs (or Boltzmann)
    distribution and it is particularly suitable for our problems because the logarithm
    cancels the exponential, yielding simpler expressions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们介绍了MRF作为RBM的底层结构。MRF表示为一个无向图，其顶点是随机变量。特别是，为了我们的目的，我们考虑了联合概率可以表示为每个随机变量正函数乘积的MRF。基于指数的最常见分布称为Gibbs（或Boltzmann）分布，它特别适合我们的问题，因为对数消除了指数，从而得到更简单的表达式。
- en: An RBM is a simple bipartite, undirected graph, made up of visible and latent
    variables, with connections only between different groups. The goal of this model
    is to learn a probability distribution, thanks to the presence of hidden units
    that can model the unknown relationships. Unfortunately, the log-likelihood, although
    very simple, cannot be easily optimized because the normalization term requires
    summing over all the input values. For this reason, Hinton proposed an alternative
    algorithm, called CD-k, which outputs an approximation of the gradient of the
    log-likelihood based on a fixed number (normally 1) of Gibbs sampling steps.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: RBM是一个简单的二分无向图，由可见变量和潜在变量组成，只在不同组之间有连接。这个模型的目标是通过隐藏单元的存在来学习一个概率分布，这些隐藏单元可以模拟未知关系。不幸的是，尽管对数似然非常简单，但无法轻易优化，因为归一化项需要对所有输入值求和。因此，Hinton提出了一个替代算法，称为CD-k，该算法基于固定数量的Gibbs采样步骤（通常是1）输出对数似然梯度的近似值。
- en: Stacking multiple RBMs allows modeling DBNs, where the hidden layer of each
    block is also the visible layer of the following one. DBN can be trained using
    a greedy approach, maximizing the log-likelihood of each RBM in sequence. In an
    unsupervised scenario, a DBN is able to extract the features of a data-generating
    process in a hierarchical way, and therefore the application includes component
    analysis and dimensionality reduction. In a supervised scenario, a DBN can be
    greedily pre-trained and fine-tuned using the backpropagation algorithm (considering
    the whole network) or sometimes using a preprocessing step in a pipeline where
    the classifier is generally a very simple model (such as a logistic regression).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠多个RBMs可以建模DBNs，其中每个块的隐藏层也是下一个块的可见层。DBN可以通过贪婪方法进行训练，依次最大化每个RBM的对数似然。在无监督场景中，DBN能够以分层的方式提取数据生成过程的特点，因此应用包括成分分析和降维。在监督场景中，DBN可以使用反向传播算法（考虑整个网络）或有时使用管道中的预处理步骤进行贪婪预训练和微调，其中分类器通常是一个非常简单的模型（例如逻辑回归）。
- en: In the next chapter, [Chapter 14](51bcd684-080e-4354-b2ed-60430bd15f6d.xhtml), *Introduction
    to Reinforcement Learning*, we are going to introduce the concept of reinforcement
    learning, discussing the most important elements of systems that can autonomously
    learn to play a game or allow a robot to walk, jump, and perform tasks that are
    extremely difficult to model and control using classic methods.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第14章](51bcd684-080e-4354-b2ed-60430bd15f6d.xhtml)，*强化学习导论*中，我们将介绍强化学习的概念，讨论能够自主学习玩游戏或使机器人行走、跳跃以及执行使用经典方法难以建模和控制的任务的系统的最重要的元素。
