["```py\nconda env create â€“f mlewp-chapter03.yml \n```", "```py\npip install tensorflow-macos \n```", "```py\npip install tensorflow-metal \n```", "```py\nbrew install swig \n```", "```py\npip install auto-sklearn \n```", "```py\ndata = [['Bleach'], ['Cereal'], ['Toilet Roll']] \n```", "```py\nfrom sklearn import preprocessing\nordinal_enc = preprocessing.OrdinalEncoder()\nordinal_enc.fit(data)\n# Print returns [[0.]\n#    [1.]\n#    [2.]]\nprint(ordinal_enc.transform(data)) \n```", "```py\nonehot_enc = preprocessing.OneHotEncoder()\nonehot_enc.fit(data)\n# Print returns [[1\\. 0\\. 0.]\n#    [0\\. 1\\. 0.]\n#    [0\\. 0\\. 1.]]\nprint(onehot_enc.transform(data).toarray()) \n```", "```py\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.linear_model import RidgeClassifier\n    from sklearn import metrics\n    from sklearn.datasets import load_wine\n    from sklearn.pipeline import make_pipeline\n    X, y = load_wine(return_X_y=True) \n    ```", "```py\n    X_train, X_test, y_train, y_test =\\\n    train_test_split(X, y, test_size=0.30, random_state=42) \n    ```", "```py\n    no_scale_clf = make_pipeline(RidgeClassifier(tol=1e-2,\n                                                 solver=\"sag\"))\n    no_scale_clf.fit(X_train, y_train)\n    y_pred_no_scale = no_scale_clf.predict(X_test) \n    ```", "```py\n    std_scale_clf = make_pipeline(StandardScaler(), RidgeClassifier(tol=1e-2, solver=\"sag\"))\n    std_scale_clf.fit(X_train, y_train)\n    y_pred_std_scale = std_scale_clf.predict(X_test) \n    ```", "```py\n    print('\\nAccuracy [no scaling]')\n    print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, y_pred_no_\n           scale)))\n    print('\\nClassification Report [no scaling]')\n    print(metrics.classification_report(y_test, y_pred_no_scale)) \n    ```", "```py\n    Accuracy [no scaling]75.93%\n    Classification Report [no scaling]\n                  precision    recall  f1-score   support\n               0       0.90      1.00      0.95        19\n               1       0.66      1.00      0.79        21\n               2       1.00      0.07      0.13        14\n        accuracy                           0.76        54\n       macro avg       0.85      0.69      0.63        54\n    weighted avg       0.83      0.76      0.68        54 \n    ```", "```py\n    print('\\nAccuracy [scaling]')\n    print('{:.2%}\\n'.format(metrics.accuracy_score(y_test, y_pred_std_scale)))\n    print('\\nClassification Report [scaling]')\n    print(metrics.classification_report(y_test, y_pred_std_scale)) \n    ```", "```py\n    Accuracy [scaling]\n    98.15%\n    Classification Report [scaling]\n                  precision    recall  f1-score   support\n               0       0.95      1.00      0.97        19\n               1       1.00      0.95      0.98        21\n               2       1.00      1.00      1.00        14\n        accuracy                           0.98        54\n       macro avg       0.98      0.98      0.98        54\n    weighted avg       0.98      0.98      0.98        54 \n    ```", "```py\npip install alibi\npip install alibi-detect \n```", "```py\n    from sklearn.datasets import load_wine\n    from sklearn.model_selection import train_test_split\n    import alibi\n    from alibi_detect.cd import TabularDrift \n    ```", "```py\n    wine_data = load_wine()\n    feature_names = wine_data.feature_names\n    X, y = wine_data.data, wine_data.target\n    X_ref, X_test, y_ref, y_test = train_test_split(X, y, test_size=0.50,\n                                                   random_state=42) \n    ```", "```py\n    cd = TabularDrift(X_ref=X_ref, p_val=.05 ) \n    ```", "```py\n    preds = cd.predict(X_test)\n    labels = ['No', 'Yes']\n    print('Drift: {}'.format(labels[preds['data']['is_drift']])) \n    ```", "```py\n    X_test_cal_error = 1.1*X_test\n    preds = cd.predict(X_test_cal_error)\n    labels = ['No', 'Yes']\n    print('Drift: {}'.format(labels[preds['data']['is_drift']])) \n    ```", "```py\n    cd = TabularDrift(X_ref=y_ref, p_val=.05 ) \n    ```", "```py\n    preds = cd.predict(y_test)\n    labels = ['No', 'Yes']\n    print('Drift: {}'.format(labels[preds['data']['is_drift']])) \n    ```", "```py\n    y_test_cal_error = 1.1*y_test\n    preds = cd.predict(y_test_cal_error)\n    labels = ['No', 'Yes']\n    print('Drift: {}'.format(labels[preds['data']['is_drift']])) \n    ```", "```py\n    from alibi_detect.cd import MMDDriftOnline \n    ```", "```py\n    ert = 50\n    window_size = 10\n    cd = MMDDriftOnline(X_ref, ert, window_size, backend='pytorch',\n                        n_bootstraps=2500) \n    ```", "```py\n    cd.predict(x)['data'] ['is_drift'] \n    ```", "```py\nimport pandas as pd\nfeature_names = rf[:-1].get_feature_names_out()\nmdi_importances = pd.Series(rf[-1].feature_importances_,\n                            index=feature_names).sort_values(ascending=True) \n```", "```py\nfrom sklearn.inspection import permutation_importance\nresult = permutation_importance(\n    rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=2\n    )\nsorted_importances_idx = result.importances_mean.argsort()\nimportances = pd.DataFrame(\n    result.importances[sorted_importances_idx].T,\n    columns=X.columns[sorted_importances_idx]) \n```", "```py\npip install shap \n```", "```py\nexplainer = shap.Explainer(rf, predict, X_test)\nshap_values = explainer(X_test) \n```", "```py\nshap.plots.bar(shap_values) \n```", "```py\n    pip install evidently \n    ```", "```py\n    from evidently.report import Report \n    ```", "```py\n    from evidently.metric_preset import DataDriftPreset \n    ```", "```py\n    data_drift_report = Report(metrics=[\n    DataDriftPreset(), \n        ])\n    report.run(\n        reference_data=X_ref,\n       current_data=X_ref\n    ) \n    ```", "```py\n    data_drift_report.save_json('data_drift_report.json')\n    data_drift_report.save_html('data_drift_report.xhtml') \n    ```", "```py\n    from Hyperopt import hp\n\n    space = {\n        'warm_start' : hp.choice('warm_start', [True, False]),\n        'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n        'tol' : hp.uniform('tol', 0.00001, 0.0001),\n        'C' : hp.uniform('C', 0.05, 2.5),\n        'solver' : hp.choice('solver', ['newton-cg', 'lbfgs',\n                                        'liblinear']),\n        'max_iter' : hp.choice('max_iter', range(10,500))\n        } \n    ```", "```py\n    def objective(params, n_folds, X, y):\n        # Perform n_fold cross validation with hyperparameters\n        clf = LogisticRegression(**params, random_state=42)\n        scores = cross_val_score(clf, X, y, cv=n_folds, scoring=\n                                 'f1_macro')\n        # Extract the best score\n        max_score = max(scores)\n        # Loss must be minimized\n        loss = 1 - max_score\n        # Dictionary with information for evaluation\n        return {'loss': loss, 'params': params, 'status': STATUS_OK} \n    ```", "```py\n    # Trials object to track progress\n    trials = Trials()\n    # Optimize\n    best = fmin(\n        fn=partial(objective, n_folds=n_folds, X=X_train, y=y_train),\n        space=space,\n        algo=tpe.suggest,\n        max_evals=16,\n        trials=trials\n        ) \n    ```", "```py\n    {'C': 0.26895003542493234,\n    'fit_intercept': 1,\n    'max_iter': 452,\n    'solver': 2,\n    'tol': 1.863336145787027e-05,\n    'warm_start': 1} \n    ```", "```py\n    def objective(trial, n_folds, X, y):\n        \"\"\"Objective function for tuning logistic regression hyperparameters\"\"\"\n        params = {\n            'warm_start': \n            trial.suggest_categorical('warm_start', [True, False]),\n            'fit_intercept': \n            trial.suggest_categorical('fit_intercept', [True, False]),\n            'tol': trial.suggest_uniform('tol', 0.00001, 0.0001),\n            'C': trial.suggest_uniform('C', 0.05, 2.5),\n            'solver': trial.suggest_categorical('solver', ['newton-cg',\n                                                'lbfgs', 'liblinear']),\n            'max_iter': trial.suggest_categorical('max_iter', \n                                                   range(10, 500))\n        }\n        # Perform n_fold cross validation with hyperparameters\n        clf = LogisticRegression(**params, random_state=42)\n        scores = cross_val_score(clf, X, y, cv=n_folds, \n                                 scoring='f1_macro')\n        # Extract the best score\n        max_score = max(scores)\n        # Loss must be minimized\n        loss = 1 - max_score\n        # Dictionary with information for evaluation\n        return loss \n    ```", "```py\n    n_folds = 5\n    X, y = datasets.make_classification(n_samples=100000, n_features=20,n_informative=2, n_redundant=2)\n    train_samples = 100  # Samples used for training the models\n    X_train = X[:train_samples]\n    X_test = X[train_samples:]\n    y_train = y[:train_samples]\n    y_test = y[train_samples:] \n    ```", "```py\n    from optuna.samplers import TPESampler\n    study = optuna.create_study(direction='minimize', sampler=TPESampler())\n    study.optimize(partial(objective, n_folds=n_folds, X=X_train, y=y_\n                           train), n_trials=16) \n    ```", "```py\n    {'warm_start': False,\n    'fit_intercept': False,\n    'tol': 9.866562116436095e-05,\n    'C': 0.08907657649508408,\n    'solver': 'newton-cg',\n    'max_iter': 108} \n    ```", "```py\n    import numpy as np\n    import sklearn.datasets\n    import sklearn.metrics\n    import autosklearn.classification \n    ```", "```py\n    automl = autosklearn.classification.AutoSklearnClassifier(\n        time_left_for_this_task=60,\n        per_run_time_limit=30\n        ) \n    ```", "```py\n    automl.fit(X_train, y_train, dataset_name='wine') \n    ```", "```py\n    print(automl.show_models()) \n    ```", "```py\n    print(automl.sprint_statistics()) \n    ```", "```py\n    predictions = automl.predict(X_test) \n    ```", "```py\n    sklearn.metrics.accuracy_score(y_test, predictions) \n    ```", "```py\n    import autokeras as ak \n    ```", "```py\n    clf = ak.StructuredDataClassifier(max_trials=5) \n    ```", "```py\n    clf.fit(x=X_train, y=y_train) \n    ```", "```py\n    accuracy=clf.evaluate(x=X_train, y=y_train) \n    ```", "```py\nmlflow server \\\n    --backend-store-uri sqlite:///mlflow.db \\\n    --default-artifact-root ./artifacts \\\n    --host 0.0.0.0 \n```", "```py\n    with mlflow.start_run(run_name=\"YOUR_RUN_NAME\") as run:\n        params = {'tol': 1e-2, 'solver': 'sag'}\n        std_scale_clf = make_pipeline(StandardScaler(),\n                                      RidgeClassifier(**params))\n        std_scale_clf.fit(X_train, y_train)\n        y_pred_std_scale = std_scale_clf.predict(X_test)\n        mlflow.log_metrics({\n                 \"accuracy\":\n                  metrics.accuracy_score(y_test, y_pred_std_scale),\n                 \"precision\":\n                  metrics.precision_score(y_test, y_pred_std_scale, \n                                          average=\"macro\"),\n                 \"f1\": \n                  metrics.f1_score(y_test, y_pred_std_scale,\n                                   average=\"macro\"),\n                 \"recall\":\n                  metrics.recall_score(y_test, y_pred_std_scale, \n                                       average=\"macro\"),\n        })\n    mlflow.log_params(params) \n    ```", "```py\n     mlflow.sklearn.log_model(\n                sk_model=std_scale_clf,\n                artifact_path=\"sklearn-model\",\n                registered_model_name=\"sk-learn-std-scale-clf\"\n            ) \n    ```", "```py\n    model_name = \"sk-learn-std-scale-clf\"\n    model_version = 1\n    model = mlflow.pyfunc.load_model(\n        model_uri=f\"models:/{model_name}/{model_version}\"\n        )\n    model.predict(X_test) \n    ```", "```py\n    stage = 'Staging'\n    model = mlflow.pyfunc.load_model(\n        model_uri=f\"models:/{model_name}/{stage}\"\n        ) \n    ```", "```py\n    client = MlflowClient()\n    client.transition_model_version_stage(\n        name=\"sk-learn-std-scale-clf\",\n        version=1,\n        stage=\"Production\"\n        ) \n    ```", "```py\n    from sklearn.compose import ColumnTransformer\n    from sklearn.pipeline import Pipeline \n    ```", "```py\n    from sklearn.impute import SimpleImputer \n    ```", "```py\n    numeric_features = ['age', 'balance']\n    numeric_transformer = Pipeline(steps=[\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())]) \n    ```", "```py\n    categorical_features = ['job', 'marital', 'education', 'contact',\n                            'housing', 'loan', 'default','day']\n    categorical_transformer = OneHotEncoder(handle_unknown='ignore') \n    ```", "```py\n    preprocessor = ColumnTransformer(\n        transformers=[\n            ('num', numeric_transformer, numeric_features),\n            ('cat', categorical_transformer, categorical_features)]) \n    ```", "```py\n    clf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                       ('classifier', LogisticRegression())]) \n    ```", "```py\n    clf_pipeline.fit(X_train, y_train) \n    ```", "```py\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass AddToColumsTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, addition = 0.0, columns=None):\n        self.addition = addition\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        transform_columns = list(X.columns)\n        if self.columns:\n            transform_columns = self.columns\n        X[transform_columns] = X[transform_columns] + self.addition\n        return X \n```", "```py\npipeline = Pipeline(\n    steps=[\n        (\"add_float\", AddToColumnsTransformer(0.5, columns=[\"col1\",\"col2\", \n                                                            \"col3\"]))\n    ]\n) \n```", "```py\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\ndef add_number(X, columns=None, number=None):\n    if columns == None and number == None:\n        return X\n    X[columns] = X[columns] + number\npipeline = Pipeline(\n    steps=[\n        (\n            \"add_float\",\n            FunctionTransformer(\n                add_number, kw_args={\"columns\": [\"col1\", \"col2\", \"col3\"],\n                                     \"number\": 0.5}),\n        )\n    ]\n) \n```", "```py\n    from pyspark.ml import Pipeline, PipelineModel\n    categoricalColumns = [\"job\", \"marital\", \"education\", \"contact\",\n                          \"housing\", \"loan\", \"default\", \"day\"]\n\n    for categoricalCol in categoricalColumns:\n        stringIndexer = StringIndexer(inputCol=categoricalCol,\n                                      outputCol=categoricalCol +\n                                      \"Index\").setHandleInvalid(\"keep\")\n        encoder = OneHotEncoder(\n                  inputCols=[stringIndexer.getOutputCol()],\n                  outputCols=[categoricalCol + \"classVec\"]\n                  )\n        stages += [stringIndexer, encoder] \n    ```", "```py\n    numericalColumns = [\"age\", \"balance\"]\n    numericalColumnsImputed = [x + \"_imputed\" for x in numericalColumns]\n    imputer = Imputer(inputCols=numericalColumns, outputCols=numericalColumnsImputed)\n    stages += [imputer] \n    ```", "```py\n    from pyspark.ml.feature import StandardScaler\n\n    numericalAssembler = VectorAssembler(\n        inputCols=numericalColumnsImputed, \n        outputCol='numerical_cols_imputed')\n    stages += [numericalAssembler]\n    scaler = StandardScaler(inputCol='numerical_cols_imputed',\n                            outputCol=\"numerical_cols_imputed_scaled\")\n    stages += [scaler] \n    ```", "```py\n    assemblerInputs = [c + \"classVec\" for c in categoricalColumns] +\\ [\"numerical_cols_imputed_scaled\"]\n    assembler = VectorAssembler(inputCols=assemblerInputs,\n                                outputCol=\"features\")\n    stages += [assembler] \n    ```", "```py\n    lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\",\n                            maxIter=10)\n    stages += [lr]\n    (trainingData, testData) = data.randomSplit([0.7, 0.3], seed=100)\n    clfPipeline = Pipeline().setStages(stages).fit(trainingData)\n    clfPipeline.transform(testData) \n    ```", "```py\nclfPipeline.save(path) \n```", "```py\nfrom pyspark.ml import Pipeline\nclfPipeline = Pipeline().load(path) \n```"]