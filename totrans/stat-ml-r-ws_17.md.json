["```py\n\n    set.seed(1)\n    random_prob = runif(1, min = 0, max = 1)\n    >>> random_prob\n    0.2655087\n    ```", "```py\n\n    prop_success = 0.2\n    >>> random_prob < prop_success\n    FALSE\n    ```", "```py\n\n    n_samples = 10\n    data = c()\n    for(sample_idx in 1:n_samples) {\n      data[sample_idx] <- runif(1, min = 0, max = 1) < prop_success\n    }\n    >>> data\n    FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n    ```", "```py\n\n    data = as.numeric(data)\n    >>> data\n    0 0 0 0 0 0 0 0 1 0\n    ```", "```py\n\nset.seed(1)\n>>> rbinom(n = n_samples, size = 1, prob = prop_success)\n```", "```py\n\nset.seed(1)\nprop_successes = runif(n_samples, min = 0.0, max = 0.2)\n>>> prop_successes\n0.05310173 0.07442478 0.11457067 0.18164156 0.04033639 0.17967794 0.18893505 0.13215956 0.12582281 0.01235725\n```", "```py\n\nlibrary(ggplot2)\n# Sample 1000 draws from Beta(35,55) prior\nprior_A = rbeta(n = 1000, shape1 = 35, shape2 = 55)\n# Store the results in a data frame\nprior_sim = data.frame(prior_A)\n# Construct a density plot of the prior sample\nggplot(prior_sim, aes(x = prior_A)) +\n  geom_density()\n```", "```py\n\n# Sample draws from the Beta(1,1) prior\nprior_B = rbeta(n = 1000, shape1 = 1, shape2 = 1)\n# Sample draws from the Beta(100,100) prior\nprior_C = rbeta(n = 1000, shape1 = 100, shape2 = 100)\n# Combine the results in a single data frame\nprior_all = data.frame(samples = c(prior_A, prior_B, prior_C),\n                        priors = rep(c(\"A\",\"B\",\"C\"), each = 1000))\n# Plot the 3 priors\nggplot(prior_all, aes(x = samples, fill = priors)) +\n  geom_density(alpha = 0.5)\n```", "```py\n\n# observed data\nx = c(1, 2, 3, 4, 5)\ny = c(2, 3, 5, 6, 7)\n# parameter value\nb = 0.8\n# calculate the predicted values\ny_pred = b * x\n# calculate the residuals\nresiduals = y - y_pred\n```", "```py\n\nlog_likelihood = -0.5 * length(y) * log(2 * pi) - 0.5 * sum(residuals^2)\nlog_likelihood\n-18.09469\n```", "```py\n\nlibrary(ggridges)\n# Define a vector of 1000 p values\np_grid = seq(from = 0, to = 1, length.out = 1000)\n# Simulate 10 trials for each p in p_grid, each trial has 1000 samples\nsim_result = rbinom(n = 1000, size = 10, prob = p_grid)\n# Collect results in a data frame\nlikelihood_sim = data.frame(p_grid, sim_result)\n# Density plots of p_grid grouped by sim_result\nggplot(likelihood_sim, aes(x = p_grid, y = sim_result, group = sim_result)) +\n  geom_density_ridges()\n```", "```py\n\n    library(rjags)\n    # define the model\n    bayes_model = \"model{\n        # Likelihood model for X\n        X ~ dbin(p, n)\n        # Prior model for p\n        p ~ dbeta(a, b)\n    }\"\n    Compile the model using the bayes.model() function.\n    # compile the model\n    bayes_jags = jags.model(textConnection(bayes_model),\n                            data = list(a = 1, b = 1, X = 3, n = 10))\n    ```", "```py\n\n    # simulate the posterior\n    bayes_sim = coda.samples(model = bayes_jags, variable.names = c(\"p\"), n.iter = 10000)\n    ```", "```py\n\n    # plot the posterior\n    plot(bayes_sim, trace = FALSE, xlim = c(0,1), ylim = c(0,3))\n    ```", "```py\n\n    library(coda)\n    set.seed(1)\n    mu_true = 2\n    sd_true = 1\n    n = 100\n    data = rnorm(n, mean = mu_true, sd = sd_true)\n    ```", "```py\n\n    model_string = \"model {\n        for (i in 1:n) {\n            y[i] ~ dnorm(mu, prec)\n        }\n        mu ~ dnorm(0, 0.1)\n        sigma ~ dunif(0, 10)\n        prec <- pow(sigma, -2)\n    }\"\n    ```", "```py\n\n    data_jags = list(y = data, n = n)\n    model = jags.model(textConnection(model_string), data = data_jags)\n    update(model, 1000)  # burn-in\n    ```", "```py\n\n    params = c(\"mu\", \"sigma\")\n    samples = coda.samples(model, params, n.iter = 10000)\n    # print summary statistics for the posterior samples\n    >>> summary(samples)\n    Iterations = 2001:12000\n    Thinning interval = 1\n    Number of chains = 1\n    Sample size per chain = 10000\n    1\\. Empirical mean and standard deviation for each variable,\n       plus standard error of the mean:\n            Mean      SD  Naive SE Time-series SE\n    mu    2.1066 0.09108 0.0009108      0.0009108\n    sigma 0.9097 0.06496 0.0006496      0.0008996\n    2\\. Quantiles for each variable:\n           2.5%    25%    50%    75% 97.5%\n    mu    1.929 2.0453 2.1064 2.1666 2.287\n    sigma 0.792 0.8652 0.9055 0.9513 1.046\n    ```", "```py\n\n    >>> plot(samples)\n    ```", "```py\n\n# Store the chains in a data frame\nmcmc_chains <- data.frame(samples[[1]], iter = 1:10000)\n# Check out the head\n>>> head(mcmc_chains)\n        mu     sigma iter\n1 2.159540 0.8678513    1\n2 2.141280 0.8719263    2\n3 1.975057 0.8568497    3\n4 2.054670 0.9313297    4\n5 2.144810 1.0349093    5\n6 2.001104 1.0597861    6\n```", "```py\n\n# Use plot() to construct trace plots\n>>> plot(samples, density = FALSE)\n```", "```py\n\n# Trace plot the first 100 iterations of the mu chain\n>>> ggplot(mcmc_chains[1:100, ], aes(x = iter, y = mu)) +\n  geom_line() +\n  theme(axis.title.x = element_text(size = 20),  # Increase x-axis label size\n        axis.title.y = element_text(size = 20))  # Increase y-axis label size\n```", "```py\n\n# Use plot() to construct density plots\n>>> plot(samples, trace = FALSE)\n```", "```py\n\nmodel2 = jags.model(textConnection(model_string), data = data_jags, n.chains = 4)\n# simulate the posterior\nsamples2 <- coda.samples(model = model2, variable.names = params, n.iter = 1000)\n```", "```py\n\n# Construct trace plots\n>>> plot(samples2, density = FALSE)\n```", "```py\n\n>>> summary(samples2)\nIterations = 1001:2000\nThinning interval = 1\nNumber of chains = 4\nSample size per chain = 1000\n1\\. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n        Mean      SD Naive SE Time-series SE\nmu    2.1052 0.09046  0.00143        0.00144\nsigma 0.9089 0.06454  0.00102        0.00134\n2\\. Quantiles for each variable:\n        2.5%    25%   50%    75% 97.5%\nmu    1.9282 2.0456 2.104 2.1655 2.282\nsigma 0.7952 0.8626 0.906 0.9522 1.041\n```", "```py\n\n    # load the necessary libraries\n    library(rjags)\n    library(coda)\n    # define the model\n    model = \"model{\n        # Define model for data Y[i]\n        for(i in 1:length(Y)) {\n          Y[i] ~ dnorm(m[i], s^(-2))\n          m[i] <- a + b * X[i]\n        }\n        # Define the a, b, s priors\n        a ~ dnorm(0, 0.5^(-2))\n        b ~ dnorm(1, 0.5^(-2))\n        s ~ dunif(0, 20)\n    }\"\n    ```", "```py\n\n    # compile the model\n    model = jags.model(textConnection(model),\n                        data = list(Y = mtcars$wt, X = mtcars$hp),\n                        n.chains = 3,\n                        inits = list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 100))\n    ```", "```py\n\n    # burn-in\n    update(model, 1000)\n    ```", "```py\n\n    # generate MCMC samples\n    samples = coda.samples(model, variable.names = c(\"a\", \"b\", \"s\"), n.iter = 5000)\n    ```", "```py\n\n    # check convergence using trace plot\n    >>> plot(samples)\n    ```", "```py\n\n    # Get the posterior estimates\n    posterior_estimates = summary(samples)\n    # Calculate the mean for each parameter\n    a_mean = posterior_estimates$statistics[\"a\", \"Mean\"]\n    b_mean = posterior_estimates$statistics[\"b\", \"Mean\"]\n    # Plot the prediction line\n    ggplot(mtcars, aes(x = hp, y = wt)) +\n      geom_point() +\n      geom_abline(intercept = a_mean, slope = b_mean) +\n      labs(title = \"Bayesian Linear Regression\",\n           x = \"Horsepower\",\n           y = \"Weight\") +\n      theme(plot.title = element_text(hjust = 0.5))\n    ```", "```py\n\n    # Extract samples\n    a_samples = as.matrix(samples[, \"a\"])\n    b_samples = as.matrix(samples[, \"b\"])\n    # Calculate credible intervals\n    a_hpd = coda::HPDinterval(coda::as.mcmc(a_samples))\n    b_hpd = coda::HPDinterval(coda::as.mcmc(b_samples))\n    # Plot histograms and credible intervals\n    par(mfrow=c(2,1))  # Create 2 subplots\n    # Parameter a\n    hist(a_samples, freq=FALSE, xlab=\"a\", main=\"Posterior distribution of a\", col=\"lightgray\")\n    abline(v=a_hpd[1,1], col=\"red\", lwd=2)  # Lower limit of the credible interval\n    abline(v=a_hpd[1,2], col=\"red\", lwd=2)  # Upper limit of the credible interval\n    # Parameter b\n    hist(b_samples, freq=FALSE, xlab=\"b\", main=\"Posterior distribution of b\", col=\"lightgray\")\n    abline(v=b_hpd[1,1], col=\"red\", lwd=2)  # Lower limit of the credible interval\n    abline(v=b_hpd[1,2], col=\"red\", lwd=2)  # Upper limit of the credible interval\n    ```", "```py\n\n    # make posterior predictions\n    # Obtain the mean values of the MCMC samples for each parameter\n    a_mean = mean(samples[[1]][,\"a\"])\n    b_mean = mean(samples[[1]][,\"b\"])\n    # New input (e.g., horsepower = 120)\n    new_input = 120\n    # Prediction\n    predicted_weight = a_mean + b_mean * new_input\n    print(predicted_weight)\n    # Predictive distribution\n    predicted_weights = samples[[1]][,\"a\"] + samples[[1]][,\"b\"] * new_input\n    # Plot the predictive distribution\n    >>> hist(predicted_weights, breaks = 30, main = \"Posterior predictive distribution\", xlab = \"Predicted weight\")\n    ```", "```py\n\n    # define the model\n    model = \"model{\n        # Define model for data Y[i]\n        for(i in 1:length(Y)) {\n          Y[i] ~ dnorm(mu[am[i]+1], s^(-2))\n        }\n        # Define the mu, s priors\n        for(j in 1:2){\n          mu[j] ~ dnorm(20, 10^(-2))\n        }\n        s ~ dunif(0, 20)\n    }\"\n    ```", "```py\n\n    # compile the model\n    model = jags.model(textConnection(model),\n                        data = list(Y = mtcars$mpg, am = mtcars$am),\n                        n.chains = 3,\n                        inits = list(.RNG.name = \"base::Wichmann-Hill\", .RNG.seed = 100))\n    # burn-in\n    update(model, 1000)\n    # generate MCMC samples\n    samples = coda.samples(model, variable.names = c(\"mu\", \"s\"), n.iter = 5000)\n    # check convergence using trace plot\n    >>> plot(samples)\n    ```", "```py\n\n    # Get the posterior estimates\n    posterior_estimates = summary(samples)\n    # Calculate the mean for each parameter\n    mu1_mean = posterior_estimates$statistics[\"mu[1]\", \"Mean\"]\n    mu2_mean = posterior_estimates$statistics[\"mu[2]\", \"Mean\"]\n    # Plot the prediction line\n    ggplot(mtcars, aes(x = as.factor(am), y = mpg)) +\n      geom_jitter(width = 0.2) +\n      geom_hline(aes(yintercept = mu1_mean, color = \"Automatic\"), linetype = \"dashed\") +\n      geom_hline(aes(yintercept = mu2_mean, color = \"Manual\"), linetype = \"dashed\") +\n      scale_color_manual(name = \"Transmission\", values = c(\"Automatic\" = \"red\", \"Manual\" = \"blue\")) +\n      labs(title = \"Bayesian Linear Regression\",\n           x = \"Transmission (0 = automatic, 1 = manual)\",\n           y = \"Miles Per Gallon (mpg)\") +\n      theme(plot.title = element_text(hjust = 0.5),\n            legend.position = \"bottom\")\n    ```"]