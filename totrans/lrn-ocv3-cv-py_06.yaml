- en: Chapter 6. Retrieving Images and Searching Using Image Descriptors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the human eyes and brain, OpenCV can detect the main features of
    an image and extract these into so-called image descriptors. These features can
    then be used as a database, enabling image-based searches. Moreover, we can use
    keypoints to stitch images together and compose a bigger image (think of putting
    together many pictures to form a 360 degree panorama).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter shows you how to detect features of an image with OpenCV and make
    use of them to match and search images. Throughout the chapter, we will take sample
    images and detect their main features, and then try to find a sample image contained
    in another image using homography.
  prefs: []
  type: TYPE_NORMAL
- en: Feature detection algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of algorithms that can be used to detect and extract features,
    and we will explore most of them. The most common algorithms used in OpenCV are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Harris**: This algorithm is useful to detect corners'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SIFT**: This algorithm is useful to detect blobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SURF**: This algorithm is useful to detect blobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FAST**: This algorithm is useful to detect corners'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BRIEF**: This algorithm is useful to detect blobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ORB**: This algorithm stands for **Oriented FAST and Rotated BRIEF**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Matching features can be performed with the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Brute-Force matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FLANN-based matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatial verification can then be performed with homography.
  prefs: []
  type: TYPE_NORMAL
- en: Defining features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What is a feature exactly? Why is a particular area of an image classifiable
    as a feature, while others are not? Broadly speaking, a feature is an area of
    interest in the image that is unique or easily recognizable. As you can imagine,
    corners and high-density areas are good features, while patterns that repeat themselves
    a lot or low-density areas (such as a blue sky) are not. Edges are good features
    as they tend to divide two regions of an image. A blob (an area of an image that
    greatly differs from its surrounding areas) is also an interesting feature.
  prefs: []
  type: TYPE_NORMAL
- en: Most feature detection algorithms revolve around the identification of corners,
    edges, and blobs, with some also focusing on the concept of a **ridge**, which
    you can conceptualize as the symmetry axis of an elongated object (think, for
    example, about identifying a road in an image).
  prefs: []
  type: TYPE_NORMAL
- en: Some algorithms are better at identifying and extracting features of a certain
    type, so it's important to know what your input image is so that you can utilize
    the best tool in your OpenCV belt.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting features – corners
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's start by identifying corners by utilizing `CornerHarris`, and let's do
    this with an example. If you continue studying OpenCV beyond this book, you'll
    find that—for many a reason—chessboards are a common subject of analysis in computer
    vision, partly because a chequered pattern is suited to many types of feature
    detections, and maybe because chess is pretty popular among geeks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s our sample image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting features – corners](img/image00216.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'OpenCV has a very handy utility function called `cornerHarris`, which detects
    corners in an image. The code to illustrate this is incredibly simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s analyze the code: after the usual imports, we load the chessboard image
    and turn it to grayscale so that `cornerHarris` can compute it. Then, we call
    the `cornerHarris` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The most important parameter here is the third one, which defines the aperture
    of the Sobel operator. The Sobel operator performs the change detection in the
    rows and columns of an image to detect edges, and it does this using a kernel.
    The OpenCV `cornerHarris` function uses a Sobel operator whose aperture is defined
    by this parameter. In plain english, it defines how sensitive corner detection
    is. It must be between 3 and 31 and be an odd value. At value 3, all those diagonal
    lines in the black squares of the chessboard will register as corners when they
    touch the border of the square. At 23, only the corners of each square will be
    detected as corners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, in places where a red mark in the corners is detected, tweaking the second
    parameter in `cornerHarris` will change this, that is, the smaller the value,
    the smaller the marks indicating corners.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Detecting features – corners](img/image00217.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Great, we have corner points marked, and the result is meaningful at first glance;
    all the corners are marked in red.
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction and description using DoG and SIFT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The preceding technique, which uses `cornerHarris`, is great to detect corners
    and has a distinct advantage because corners are corners; they are detected even
    if the image is rotated.
  prefs: []
  type: TYPE_NORMAL
- en: However, if we reduce (or increase) the size of an image, some parts of the
    image may lose or even gain a corner quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, look at the following corner detections of the F1 Italian Grand
    Prix track:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature extraction and description using DoG and SIFT](img/image00218.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here''s a smaller version of the same screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature extraction and description using DoG and SIFT](img/image00219.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: You will notice how corners are a lot more condensed; however, we didn't only
    gain corners, we also lost some! In particular, look at the **Variante Ascari**
    chicane that squiggles at the end of the NW/SE straight part of the track. In
    the larger version of the image, both the entrance and the apex of the double
    bend were detected as corners. In the smaller image, the apex is not detected
    as such. The more we reduce the image, the more likely it is that we're going
    to lose the entrance to that chicane too.
  prefs: []
  type: TYPE_NORMAL
- en: 'This loss of features raises an issue; we need an algorithm that will work
    regardless of the scale of the image. Enter **SIFT**: while **Scale-Invariant
    Feature Transform** may sound a bit mysterious, now that we know what problem
    we''re trying to resolve, it actually makes sense. We need a function (a transform)
    that will detect features (a feature transform) and will not output different
    results depending on the scale of the image (a scale-invariant feature transform).
    Note that SIFT does not detect keypoints (which is done with Difference of Gaussians),
    but it describes the region surrounding them by means of a feature vector.'
  prefs: []
  type: TYPE_NORMAL
- en: A quick introduction to **Difference of Gaussians** (**DoG**) is in order; we
    have already talked about low pass filters and blurring operations, specifically
    with the `cv2.GaussianBlur()` function. DoG is the result of different Gaussian
    filters applied to the same image. In [Chapter 3](part0023.xhtml#aid-LTSU1 "Chapter 3. Processing
    Images with OpenCV 3"), *Processing Images with OpenCV 3*, we applied this technique
    to compute a very efficient edge detection, and the idea is the same. The final
    result of a DoG operation contains areas of interest (keypoints), which are then
    going to be described through SIFT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how SIFT behaves in an image full of corners and features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature extraction and description using DoG and SIFT](img/image00220.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, the beautiful panorama of Varese (Lombardy, Italy) also gains a computer
    vision meaning. Here''s the code used to obtain this processed image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After the usual imports, we load the image that we want to process. To make
    this script generic, we will take the image path as a command-line argument using
    the `sys` module of Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We then turn the image into grayscale. At this stage, you may have gathered
    that most processing algorithms in Python need a grayscale feed in order to work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create a SIFT object and compute the grayscale image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an interesting and important process; the SIFT object uses DoG to detect
    keypoints and computes a feature vector for the surrounding regions of each keypoint.
    As the name of the method clearly gives away, there are two main operations performed:
    detection and computation. The return value of the operation is a tuple containing
    keypoint information (keypoints) and the descriptor.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we process this image by drawing the keypoints on it and displaying
    it with the usual `imshow` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in the `drawKeypoints` function, we pass a flag that has a value
    of 4\. This is actually the `cv2` module property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This code enables the drawing of circles and orientation of each keypoint.
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of a keypoint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s take a quick look at the definition, from the OpenCV documentation,
    of the keypoint class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Some properties are more self-explanatory than others, but let''s not take
    anything for granted and go through each one:'
  prefs: []
  type: TYPE_NORMAL
- en: The `pt` (point) property indicates the *x* and *y* coordinates of the keypoint
    in the image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `size` property indicates the diameter of the feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `angle` property indicates the orientation of the feature as shown in the
    preceding processed image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `response` property indicates the strength of the keypoint. Some features
    are classified by SIFT as stronger than others, and `response` is the property
    you would check to evaluate the strength of a feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `octave` property indicates the layer in the pyramid where the feature was
    found. To fully explain this property, we would need to write an entire chapter
    on it, so I will only introduce the basic concept. The SIFT algorithm operates
    in a similar fashion to face detection algorithms in that, it processes the same
    image sequentially but alters the parameters of the computation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, the scale of the image and neighboring pixels are parameters that
    change at each iteration (`octave`) of the algorithm. So, the `octave` property
    indicates the layer at which the keypoint was detected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, the object ID is the ID of the keypoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction and detection using Fast Hessian and SURF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Computer vision is a relatively recent branch of computer science and many algorithms
    and techniques are of recent invention. SIFT is in fact only 16 years old, having
    been published by David Lowe in 1999.
  prefs: []
  type: TYPE_NORMAL
- en: SURF is a feature detection algorithm published in 2006 by Herbert Bay, which
    is several times faster than SIFT, and it is partially inspired by it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that both SIFT and SURF are patented algorithms and, for this reason, are
    made available in the `xfeatures2d` module of OpenCV.
  prefs: []
  type: TYPE_NORMAL
- en: It is not particularly relevant to this book to understand how SURF works under
    the hood, as much as we can use it in our applications and make the best of it.
    What is important to understand is that SURF is an OpenCV class that operates
    keypoint detection with the Fast Hessian algorithm and extraction with SURF, much
    like the SIFT class in OpenCV operating keypoint detection with DoG and extraction
    with SIFT.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the good news is that as a feature detection algorithm, the API of SURF
    does not differ from SIFT. Therefore, we can simply edit the previous script to
    dynamically choose a feature detection algorithm instead of rewriting the entire
    program.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we only support two algorithms for now, there is no need to find a particularly
    elegant solution to the evaluation of the algorithm to be used and we''ll use
    the simple `if` blocks, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the result using SURF with a threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature extraction and detection using Fast Hessian and SURF](img/image00221.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This image has been obtained by processing it with a SURF algorithm using a
    Hessian threshold of 8000\. To be precise, I ran the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The higher the threshold, the less features identified, so play around with
    the values until you reach an optimal detection. In the preceding case, you can
    clearly see how individual buildings are detected as features.
  prefs: []
  type: TYPE_NORMAL
- en: In a process similar to the one we adopted in [Chapter 4](part0036.xhtml#aid-12AK81
    "Chapter 4. Depth Estimation and Segmentation"), *Depth Estimation and Segmentation*,
    when we were calculating disparity maps, try—as an exercise—to create a trackbar
    to feed the value of the Hessian threshold to the SURF instance, and see the number
    of features increase and decrease in an inversely proportional fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's examine corner detection with FAST, the BRIEF keypoint descriptor,
    and ORB (which uses the two) and put feature detection to good use.
  prefs: []
  type: TYPE_NORMAL
- en: ORB feature detection and feature matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If SIFT is young, and SURF younger, ORB is in its infancy. ORB was first published
    in 2011 as a fast alternative to SIFT and SURF.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm was published in the paper, *ORB: an efficient alternative to
    SIFT or SURF*, and is available in the PDF format at [http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf](http://www.vision.cs.chubu.ac.jp/CV-R/pdf/Rublee_iccv2011.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: ORB mixes techniques used in the FAST keypoint detection and the BRIEF descriptor,
    so it is definitely worth taking a quick look at FAST and BRIEF first. We will
    then talk about Brute-Force matching—one of the algorithms used for feature matching—and
    show an example of feature matching.
  prefs: []
  type: TYPE_NORMAL
- en: FAST
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Features from Accelerated Segment Test** (**FAST**) algorithm works in
    a clever way; it draws a circle around including 16 pixels. It then marks each
    pixel brighter or darker than a particular threshold compared to the center of
    the circle. A corner is defined by identifying a number of contiguous pixels marked
    as brighter or darker.
  prefs: []
  type: TYPE_NORMAL
- en: 'FAST implements a high-speed test, which attempts at quickly skipping the whole
    16-pixel test. To understand how this test works, let''s take a look at this screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FAST](img/image00222.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, three out of four of the test pixels (pixels number **1**, **9**,
    **5**, and **13**) must be within (or beyond) the threshold (and, therefore, marked
    as brighter or darker) and one must be in the opposite side of the threshold.
    If all four are marked as brighter or darker, or two are and two are not, the
    pixel is not a candidate corner.
  prefs: []
  type: TYPE_NORMAL
- en: FAST is an incredibly clever algorithm, but not devoid of weaknesses, and to
    compensate these weaknesses, developers analyzing images can implement a machine
    learning approach, feeding a set of images (relevant to your application) to the
    algorithm so that corner detection is optimized.
  prefs: []
  type: TYPE_NORMAL
- en: Despite this, FAST depends on a threshold, so the developer's input is always
    necessary (unlike SIFT).
  prefs: []
  type: TYPE_NORMAL
- en: BRIEF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Binary** **Robust Independent Elementary Features** (**BRIEF**), on the other
    hand, is not a feature detection algorithm, but a descriptor. We have not yet
    explored this concept, so let''s explain what a descriptor is, and then look at
    BRIEF.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice that when we previously analyzed images with SIFT and SURF,
    the heart of the entire process is the call to the `detectAndCompute` function.
    This function operates two different steps: detection and computation, and they
    return two different results if coupled in a tuple.'
  prefs: []
  type: TYPE_NORMAL
- en: The result of detection is a set of keypoints; the result of the computation
    is the descriptor. This means that the OpenCV's SIFT and SURF classes are both
    detectors and descriptors (although, remember that the original algorithms are
    not! OpenCV's SIFT is really DoG plus SIFT and OpenCV's SURF is really Fast Hessian
    plus SURF).
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint descriptors are a representation of the image that serves as the gateway
    to feature matching because you can compare the keypoint descriptors of two images
    and find commonalities.
  prefs: []
  type: TYPE_NORMAL
- en: BRIEF is one of the fastest descriptors currently available. The theory behind
    BRIEF is actually quite complicated, but suffice to say that BRIEF adopts a series
    of optimizations that make it a very good choice for feature matching.
  prefs: []
  type: TYPE_NORMAL
- en: Brute-Force matching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Brute-Force matcher is a descriptor matcher that compares two descriptors
    and generates a result that is a list of matches. The reason why it's called Brute-Force
    is that there is little optimization involved in the algorithm; all the features
    in the first descriptor are compared to the features in the second descriptor,
    and each comparison is given a distance value and the best result is considered
    a match.
  prefs: []
  type: TYPE_NORMAL
- en: This is why it's called Brute-Force. In computing, the term, **brute-force**,
    is often associated with an approach that prioritizes the exhaustion of all possible
    combinations (for example, all the possible combinations of characters to crack
    a password) over some clever and convoluted algorithmical logic. OpenCV provides
    a `BFMatcher` object that does just that.
  prefs: []
  type: TYPE_NORMAL
- en: Feature matching with ORB
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a general idea of what FAST and BRIEF are, we can understand
    why the team behind ORB (at the time composed by Ethan Rublee, Vincent Rabaud,
    Kurt Konolige, and Gary R. Bradski) chose these two algorithms as a foundation
    for ORB.
  prefs: []
  type: TYPE_NORMAL
- en: 'In their paper, the authors aim at achieving the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: The addition of a fast and accurate orientation component to FAST
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The efficient computation of oriented BRIEF features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analysis of variance and correlation of oriented BRIEF features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A learning method to decorrelate BRIEF features under rotational invariance,
    leading to better performance in nearest-neighbor applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aside from very technical jargon, the main points are quite clear; ORB aims
    at optimizing and speeding up operations, including the very important step of
    utilizing BRIEF in a rotation-aware fashion so that matching is improved even
    in situations where a training image has a very different rotation to the query
    image.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, though, I bet you have had enough of the theory and want to sink
    your teeth in some feature matching, so let's go look at some code.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an avid listener of music, the first example that comes to my mind is to
    get the logo of a band and match it to one of the band''s albums:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let's now examine this code step by step.
  prefs: []
  type: TYPE_NORMAL
- en: After the usual imports, we load two images (the query image and the training
    image).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you have probably seen the loading of images with a second parameter
    with the value of 0 being passed. This is because `cv2.imread` takes a second
    parameter that can be one of the following flags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `cv2.IMREAD_GRAYSCALE` is equal to `0`, so you can pass the
    flag itself or its value; they are the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the image we''ve loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature matching with ORB](img/image00223.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is another image that we''ve loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature matching with ORB](img/image00224.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we proceed to creating the ORB feature detector and descriptor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In a similar fashion to what we did with SIFT and SURF, we detect and compute
    the keypoints and descriptors for both images.
  prefs: []
  type: TYPE_NORMAL
- en: The theory at this point is pretty simple; iterate through the descriptors and
    determine whether they are a match or not, and then calculate the quality of this
    match (distance) and sort the matches so that we can display the top *n* matches
    with a degree of confidence that they are, in fact, matching features on both
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '`BFMatcher`, as described in Brute-Force matching, does this for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'At this stage, we already have all the information we need, but as computer
    vision enthusiasts, we place quite a bit of importance on visually representing
    data, so let''s draw these matches in a `matplotlib` chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature matching with ORB](img/image00225.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Using K-Nearest Neighbors matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of algorithms that can be used to detect matches so that
    we can draw them. One of them is **K-Nearest Neighbors** (**KNN**). Using different
    algorithms for different tasks can be really beneficial, because each algorithm
    has strengths and weaknesses. Some may be more accurate than others, some may
    be faster or less computationally expensive, so it's up to you to decide which
    one to use, depending on the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you have hardware constraints, you may choose an algorithm that
    is less costly. If you're developing a real-time application, you may choose the
    fastest algorithm, regardless of how heavy it is on the processor or memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: Among all the machine learning algorithms, KNN is probably the simplest, and
    while the theory behind it is interesting, it is well out of the scope of this
    book. Instead, we will simply show you how to use KNN in your application, which
    is not very different from the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Crucially, the two points where the script differs to switch to KNN are in
    the way we calculate matches with the Brute-Force matcher, and the way we draw
    these matches. The preceding example, which has been edited to use KNN, looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The final result is somewhat similar to the previous one, so what is the difference
    between `match` and `knnMatch`? The difference is that `match` returns best matches,
    while KNN returns *k* matches, giving the developer the option to further manipulate
    the matches obtained with `knnMatch`.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you could iterate through the matches and apply a ratio test so
    that you can filter out matches that do not satisfy a user-defined condition.
  prefs: []
  type: TYPE_NORMAL
- en: FLANN-based matching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, we are going to take a look at **Fast Library for Approximate Nearest
    Neighbors** (**FLANN**). The official Internet home of FLANN is at [http://www.cs.ubc.ca/research/flann/](http://www.cs.ubc.ca/research/flann/).
  prefs: []
  type: TYPE_NORMAL
- en: Like ORB, FLANN has a more permissive license than SIFT or SURF, so you can
    freely use it in your project. Quoting the website of FLANN,
  prefs: []
  type: TYPE_NORMAL
- en: '*"FLANN is a library for performing fast approximate nearest neighbor searches
    in high dimensional spaces. It contains a collection of algorithms we found to
    work best for nearest neighbor search and a system for automatically choosing
    the best algorithm and optimum parameters depending on the dataset.*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*FLANN is written in C++ and contains bindings for the following languages:
    C, MATLAB and Python."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In other words, FLANN possesses an internal mechanism that attempts at employing
    the best algorithm to process a dataset depending on the data itself. FLANN has
    been proven to be 10 times times faster than other nearest neighbors search software.
  prefs: []
  type: TYPE_NORMAL
- en: FLANN is even available on GitHub at [https://github.com/mariusmuja/flann](https://github.com/mariusmuja/flann).
    In my experience, I've found FLANN-based matching to be very accurate and fast
    as well as friendly to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of FLANN-based feature matching:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Some parts of the preceding script will be familiar to you at this stage (import
    of modules, image loading, and creation of a SIFT object).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The interesting part is the declaration of the FLANN matcher, which follows
    the documentation at [http://www.cs.ubc.ca/~mariusm/uploads/FLANN/flann_manual-1.6.pdf](http://www.cs.ubc.ca/~mariusm/uploads/FLANN/flann_manual-1.6.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'We find that the FLANN matcher takes two parameters: an `indexParams` object
    and a `searchParams` object. These parameters, passed in a dictionary form in
    Python (and a struct in C++), determine the behavior of the index and search objects
    used internally by FLANN to compute the matches.'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we could have chosen between `LinearIndex`, `KTreeIndex`, `KMeansIndex`,
    `CompositeIndex`, and `AutotuneIndex`, and we chose `KTreeIndex`. Why? This is
    because it's a simple enough index to configure (only requires the user to specify
    the number of kernel density trees to be processed; a good value is between 1
    and 16) and clever enough (the kd-trees are processed in parallel). The `searchParams`
    dictionary only contains one field (checks) that specifies the number of times
    an index tree should be traversed. The higher the value, the longer it takes to
    compute the matching, but it will also be more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: In reality, a lot depends on the input that you feed the program with. I've
    found that 5 kd-trees and 50 checks always yield a respectably accurate result,
    while only taking a short time to complete.
  prefs: []
  type: TYPE_NORMAL
- en: After the creation of the FLANN matcher and having created the matches array,
    matches are then filtered according to the test described by Lowe in his paper,
    *Distinctive Image Features from Scale-Invariant Keypoints*, available at [https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf](https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: In its chapter, *Application to object recognition*, Lowe explains that not
    all matches are "good" ones, and that filtering according to an arbitrary threshold
    would not yield good results all the time. Instead, Dr. Lowe explains,
  prefs: []
  type: TYPE_NORMAL
- en: '*"The probability that a match is correct can be determined by taking the ratio
    of distance from the closest neighbor to the distance of the second closest."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the preceding example, discarding any value with a distance greater than
    0.7 will result in just a few good matches being filtered out, while getting rid
    of around 90 percent of false matches.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s unveil the result of a practical example of FLANN. This is the query
    image that I''ve fed the script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FLANN-based matching](img/image00226.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the training image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FLANN-based matching](img/image00227.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Here, you may notice that the image contains the query image at position (5,
    3) of this grid.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the FLANN processed result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FLANN-based matching](img/image00228.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: A perfect match!!
  prefs: []
  type: TYPE_NORMAL
- en: FLANN matching with homography
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First of all, what is homography? Let''s read a definition from the Internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"A relation between two figures, such that to any point of the one corresponds
    one and but one point in the other, and vise versa. Thus, a tangent line rolling
    on a circle cuts two fixed tangents of the circle in two sets of points that are
    homographic."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'If you—like me—are none the wiser from the preceding definition, you will probably
    find this explanation a bit clearer: homography is a condition in which two figures
    find each other when one is a perspective distortion of the other.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike all the previous examples, let''s first take a look at what we want
    to achieve so that we can fully understand what homography is. Then, we''ll go
    through the code. Here''s the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![FLANN matching with homography](img/image00229.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the screenshot, we took a subject on the left, correctly
    identified in the image on the right-hand side, drew matching lines between keypoints,
    and even drew a white border showing the perspective deformation of the seed subject
    in the right-hand side of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When compared to the previous FLANN-based matching example, the only difference
    (and this is where all the action happens) is in the `if` block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what happens in this code step by step: firstly, we make sure that
    we have at least a certain number of good matches (the minimum required to compute
    a homography is four), which we will arbitrarily set at 10 (in real life, you
    would probably use a higher value than this):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we find the keypoints in the original image and the training image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we find the homography:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Note that we create `matchesMask`, which will be used in the final drawing of
    the matches so that only points lying within the homography will have matching
    lines drawn.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage, we simply have to calculate the perspective distortion of the
    original object into the second picture so that we can draw the border:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: And we then proceed to draw as per all our previous examples.
  prefs: []
  type: TYPE_NORMAL
- en: A sample application – tattoo forensics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's conclude this chapter with a real-life (or kind of) example. Imagine that
    you're working for the Gotham forensics department and you need to identify a
    tattoo. You have the original picture of the tattoo (imagine this coming from
    a CCTV footage) belonging to a criminal, but you don't know the identity of the
    person. However, you possess a database of tattoos, indexed with the name of the
    person to whom the tattoo belongs.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s divide the task in two parts: save image descriptors to files first,
    and then, scan these for matches against the picture we are using as a query image.'
  prefs: []
  type: TYPE_NORMAL
- en: Saving image descriptors to file
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first thing we will do is save image descriptors to an external file. This
    is so that we don't have to recreate the descriptors every time we want to scan
    two images for matches and homography.
  prefs: []
  type: TYPE_NORMAL
- en: In our application, we will scan a folder for images and create the corresponding
    descriptor files so that we have them readily available for future searches.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create descriptors and save them to a file, we will use a process we have
    used a number of times in this chapter, namely load an image, create a feature
    detector, detect, and compute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In this script, we pass the folder name where all our images are contained,
    and then create all the descriptor files in the same folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy has a very handy `save()` utility, which dumps array data into a file
    in an optimized way. To generate the descriptors in the folder containing your
    script, run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that `cPickle/pickle` are more popular libraries for Python object serialization.
    However, in this particular context, we are trying to limit ourselves to the usage
    of OpenCV and Python with NumPy and SciPy.
  prefs: []
  type: TYPE_NORMAL
- en: Scanning for matches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have descriptors saved to files, all we need to do is to repeat
    the homography process on all the descriptors and find a potential match to our
    query image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the process we will put in place:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading a query image and creating a descriptor for it (`tattoo_seed.jpg`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scanning the folder with descriptors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each descriptor, computing a FLANN-based match
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the number of matches is beyond an arbitrary threshold, including the file
    of potential culprits (remember we're investigating a crime)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of all the culprits, electing the one with the highest number of matches as
    the potential suspect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s inspect the code to achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: I saved this script as `scan_for_matches.py`. The only element of novelty in
    this script is the use of `numpy.load(filename)`, which loads an `npy` file into
    an `np` array.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the script produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were to represent this graphically, this is what we would see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scanning for matches](img/image00230.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about detecting features in images and extracting
    them into descriptors. We explored a number of algorithms available in OpenCV
    to accomplish this task, and then applied them to real-life scenarios to understand
    a real-world application of the concepts we explored.
  prefs: []
  type: TYPE_NORMAL
- en: We are now familiar with the concept of detecting features in an image (or a
    video frame), which is a good foundation for the next chapter.
  prefs: []
  type: TYPE_NORMAL
