["```py\n# loading the required packages\nlibrary(ggplot2)\nlibrary(reshape2)\n# distribution of arms or actions having normally distributed\n# rewards with small variance\n# The data represents a standard, ideal situation i.e.\n# normally distributed rewards, well seperated from each other.\nmean_reward = c(5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5, 25, 26)\nreward_dist = c(function(n) rnorm(n = n, mean = mean_reward[1], sd = 2.5),\n                function(n) rnorm(n = n, mean = mean_reward[2], sd = 2.5),\n                function(n) rnorm(n = n, mean = mean_reward[3], sd = 2.5),\n                function(n) rnorm(n = n, mean = mean_reward[4], sd = 2.5),\n                function(n) rnorm(n = n, mean = mean_reward[5], sd = 2.5),\n                function(n) rnorm(n = n, mean = mean_reward[6], sd = 2.5),\n                function(n) rnorm(n = n, mean = mean_reward[7], sd = 2.5),\n                function(n) rnorm(n = n, mean = mean_reward[8], sd = 2.5),\n                function(n) rnorm(n = n, mean = mean_reward[9], sd = 2.5),\n                function(n) rnorm(n= n, mean = mean_reward[10], sd = 2.5))\n#preparing simulation data\ndataset = matrix(nrow = 10000, ncol = 10)\nfor(i in 1:10){\n  dataset[, i] = reward_dist[[i]](n = 10000)\n}\n# assigning column names\ncolnames(dataset) <- 1:10\n# viewing the dataset that is just created with simulated data\nView(dataset)\n```", "```py\n# creating a melted dataset with arm and reward combination\ndataset_p = melt(dataset)[, 2:3]\ncolnames(dataset_p) <- c(\"Bandit\", \"Reward\")\n# converting the arms column in the dataset to nominal type\ndataset_p$Bandit = as.factor(dataset_p$Bandit)\n# viewing the dataset that is just melted\nView(dataset_p)\n```", "```py\n#ploting the distributions of rewards from bandits\nggplot(dataset_p, aes(x = Reward, col = Bandit, fill = Bandit)) +\n  geom_density(alpha = 0.3) +\n  labs(title = \"Reward from different bandits\")\n```", "```py\n# implementing upper confidence bound algorithm\nUCB <- function(N = 1000, reward_data){\n  d = ncol(reward_data)\n  bandit_selected = integer(0)\n  numbers_of_selections = integer(d)\n  sums_of_rewards = integer(d)\n  total_reward = 0\n  for (n in 1:N) {\n    max_upper_bound = 0\n    for (i in 1:d) {\n      if (numbers_of_selections[i] > 0){\n        average_reward = sums_of_rewards[i] / numbers_of_selections[i]\n        delta_i = sqrt(2 * log(1 + n * log(n)^2) /\nnumbers_of_selections[i])\n        upper_bound = average_reward + delta_i\n      } else {\n        upper_bound = 1e400\n      }\n      if (upper_bound > max_upper_bound){\n        max_upper_bound = upper_bound\n        bandit = i\n      }\n    }\n    bandit_selected = append(bandit_selected, bandit)\n    numbers_of_selections[bandit] = numbers_of_selections[bandit] + 1\n    reward = reward_data[n, bandit]\n    sums_of_rewards[bandit] = sums_of_rewards[bandit] + reward\n    total_reward = total_reward + reward\n  }\n  return(list(total_reward = total_reward, bandit_selected bandit_selected, numbers_of_selections = numbers_of_selections, sums_of_rewards = sums_of_rewards))\n}\n# running the UCB algorithm on our\n# hypothesized arms with normal distributions\nUCB(N = 1000, reward_data = dataset)\n```", "```py\n$total_reward\n       1\n25836.91\n$numbers_of_selections\n [1]   1   1   1   1   1   1   2   1  23 968\n$sums_of_rewards\n [1]     4.149238    10.874230     5.998070    11.951624    18.151797    21.004781    44.266832    19.370479   563.001692\n[10] 25138.139942\n```", "```py\n# Thompson sampling algorithm\nrnormgamma <- function(n, mu, lambda, alpha, beta){\n  if(length(n) > 1)\n    n <- length(n)\n  tau <- rgamma(n, alpha, beta)\n  x <- rnorm(n, mu, 1 / (lambda * tau))\n  data.frame(tau = tau, x = x)\n}\nT.samp <- function(N = 500, reward_data, mu0 = 0, v = 1, alpha = 2,\nbeta = 6){\n  d = ncol(reward_data)\n  bandit_selected = integer(0)\n  numbers_of_selections = integer(d)\n  sums_of_rewards = integer(d)\n  total_reward = 0\n  reward_history = vector(\"list\", d)\n  for (n in 1:N){\n    max_random = -1e400\n    for (i in 1:d){\n      if(numbers_of_selections[i] >= 1){\n        rand = rnormgamma(1,\n                          (v * mu0 + numbers_of_selections[i] * mean(reward_history[[i]])) / (v + numbers_of_selections[i]),\n                          v + numbers_of_selections[i],\n                          alpha + numbers_of_selections[i] / 2,\n                          beta + (sum(reward_history[[i]] - mean(reward_history[[i]])) ^ 2) / 2 + ((numbers_of_selections[i] * v) / (v + numbers_of_selections[i])) * (mean(reward_history[[i]]) - mu0) ^ 2 / 2)$x\n      }else {\n        rand = rnormgamma(1, mu0, v, alpha, beta)$x\n      }\n      if(rand > max_random){\n        max_random = rand\n        bandit = i\n      }\n    }\n    bandit_selected = append(bandit_selected, bandit)\n    numbers_of_selections[bandit] = numbers_of_selections[bandit] + 1\n    reward = reward_data[n, bandit]\n    sums_of_rewards[bandit] = sums_of_rewards[bandit] + reward\n    total_reward = total_reward + reward\n    reward_history[[bandit]] = append(reward_history[[bandit]], reward)\n  }\n  return(list(total_reward = total_reward, bandit_selected = bandit_selected, numbers_of_selections = numbers_of_selections, sums_of_rewards = sums_of_rewards))\n}\n# Applying Thompson sampling using normal-gamma prior and Normal likelihood to estimate posterior distributions\nT.samp(N = 1000, reward_data = dataset, mu0 = 40)\n```", "```py\n$total_reward\n      10\n24434.24\n$numbers_of_selections\n [1]  16  15  15  14  14  17  16  19  29 845\n$sums_of_rewards\n [1]    80.22713   110.09657   141.14346   171.41301   212.86899   293.30138   311.12230   423.93256   713.54105 21976.59855\n```", "```py\n# Distribution of bandits / actions having normally distributed rewards with large variance\n# This data represents an ideal but more unstable situation: normally distributed rewards with much larger variance,\n# thus not well separated from each other.\nmean_reward = c(5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5, 25, 26)\nreward_dist = c(function(n) rnorm(n = n, mean = mean_reward[1], sd = 20),\n                function(n) rnorm(n = n, mean = mean_reward[2], sd = 20),\n                function(n) rnorm(n = n, mean = mean_reward[3], sd = 20),\n                function(n) rnorm(n = n, mean = mean_reward[4], sd = 20),\n                function(n) rnorm(n = n, mean = mean_reward[5], sd = 20),\n                function(n) rnorm(n = n, mean = mean_reward[6], sd = 20),\n                function(n) rnorm(n = n, mean = mean_reward[7], sd = 20),]\n                function(n) rnorm(n = n, mean = mean_reward[8], sd = 20),\n                function(n) rnorm(n = n, mean = mean_reward[9], sd = 20),\n                function(n) rnorm(n = n, mean = mean_reward[10], sd = 20))\n#preparing simulation data\ndataset = matrix(nrow = 10000, ncol = 10)\nfor(i in 1:10){\n  dataset[, i] = reward_dist[[i]](n = 10000)\n}\ncolnames(dataset) <- 1:10\ndataset_p = melt(dataset)[, 2:3]\ncolnames(dataset_p) <- c(\"Bandit\", \"Reward\")\ndataset_p$Bandit = as.factor(dataset_p$Bandit)\n#plotting the distributions of rewards from bandits\nggplot(dataset_p, aes(x = Reward, col = Bandit, fill = Bandit)) +\n  geom_density(alpha = 0.3) +\n  labs(title = \"Reward from different bandits\")\n```", "```py\n# Applying UCB on rewards with higher variance\nUCB(N = 1000, reward_data = dataset)\n```", "```py\n$total_reward\n       1\n25321.39\n$numbers_of_selections\n [1]   1   1   1   3   1   1   2   6 903  81\n$sums_of_rewards\n [1]     2.309649    -6.982907   -24.654597    49.186498     8.367174   -16.211632    31.243270   104.190075 23559.216706  1614.725305\n```", "```py\n# Applying Thompson sampling on rewards with higher variance\nT.samp(N = 1000, reward_data = dataset, mu0 = 40)\n```", "```py\n$total_reward\n       2\n24120.94\n$numbers_of_selections\n [1]  16  15  14  15  15  17  20  21 849  18\n$sums_of_rewards\n [1]    94.27878    81.42390   212.00717   181.46489   140.43908   249.82014   368.52864   397.07629 22090.20740 305.69191\n```", "```py\n# Distribution of bandits / actions with rewards of different distributions\n# This data represents an more chaotic (possibly more realistic) situation:\n# rewards with different distribution and different variance.\nmean_reward = c(5, 7.5, 10, 12.5, 15, 17.5, 20, 22.5, 25, 26)\nreward_dist = c(function(n) rnorm(n = n, mean = mean_reward[1], sd = 20),\n                function(n) rgamma(n = n, shape = mean_reward[2] / 2, rate\n                 = 0.5),\n                function(n) rpois(n = n, lambda = mean_reward[3]),\n                function(n) runif(n = n, min = mean_reward[4] - 20, max = mean_reward[4] + 20),\n                function(n) rlnorm(n = n, meanlog = log(mean_reward[5]) - 0.25, sdlog = 0.5),\n                function(n) rnorm(n = n, mean = mean_reward[6], sd = 20),\n                function(n) rexp(n = n, rate = 1 / mean_reward[7]),\n                function(n) rbinom(n = n, size = mean_reward[8] / 0.5, prob = 0.5),\n                function(n) rnorm(n = n, mean = mean_reward[9], sd = 20),\n                function(n) rnorm(n = n, mean = mean_reward[10], sd = 20))\n#preparing simulation data\ndataset = matrix(nrow = 10000, ncol = 10)\nfor(i in 1:10){\n  dataset[, i] = reward_dist[[i]](n = 10000)\n}\ncolnames(dataset) <- 1:10\ndataset_p = melt(dataset)[, 2:3]\ncolnames(dataset_p) <- c(\"Bandit\", \"Reward\")\ndataset_p$Bandit = as.factor(dataset_p$Bandit)\n#plotting the distributions of rewards from bandits\nggplot(dataset_p, aes(x = Reward, col = Bandit, fill = Bandit)) +\n  geom_density(alpha = 0.3) +\n  labs(title = \"Reward from different bandits\")\n```", "```py\n# Applying UCB on rewards with different distributions\nUCB(N = 1000, reward_data = dataset)\n```", "```py\n$total_reward\n       1\n22254.18\n$numbers_of_selections\n [1]   1   1   1   1   1   1   1 926  61   6\n$sums_of_rewards\n [1]     6.810026     3.373098     8.000000    12.783859    12.858791    11.835287     1.616978 20755.000000 1324.564987   117.335467\n```", "```py\n# Applying Thompson sampling on rewards with different distributions\nT.samp(N = 1000, reward_data = dataset, mu0 = 40)\n```", "```py\n$total_reward\n       2\n24014.36\n$numbers_of_selections\n [1]  16  14  14  14  14  15  14  51 214 634\n$sums_of_rewards\n [1]    44.37095   127.57153   128.00000   142.66207   191.44695   169.10430   150.19486  1168.00000  5201.69130 16691.32118\n```"]