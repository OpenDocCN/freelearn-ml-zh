- en: Human Pose Estimation with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to cover human pose estimation with TensorFlow
    using the DeeperCut algorithm. We will learn single-person and multi-person pose
    detection using the DeeperCut and ArtTrack models. Later, we will also learn how
    to use the model with videos and retrain it to use it for the customized images
    in our projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Pose estimation with DeeperCut and ArtTrack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single-person pose detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-person pose detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Videos and retraining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pose estimation using DeeperCut and ArtTrack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human pose estimation is the process of estimating the configuration of the
    body (pose) from an image or video. It includes landmarks (points), which are
    similar to joints such as the feet, ankles, chin, shoulder, elbows, hands, head,
    and so on. We will be doing this automatically using deep learning. If you consider
    a face, the landmarks are relatively rigid or, rather, relatively constant from
    face to face, such as the relative position of the eyes to the nose, the mouth
    to the chin, and so forth.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following photo provides an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2c43966-d8da-481e-aff1-ff313b244730.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Although the body structure remains the same, our bodies aren''t rigid. So,
    we need to detect the different parts of our body relative to the other parts.
    For example, detecting the feet relative to the knee is very challenging compared
    to facial detection. Also, we can move our hands and feet, which can lead to a
    wide variety of positions. The following picture gives an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85561526-7b48-492b-b381-eb42bfddb855.png)'
  prefs: []
  type: TYPE_IMG
- en: This was very difficult until we had some breakthroughs in computer vision from
    different groups around the world. Different code has been developed to carry
    out pose estimation, but we will cover an algorithm called **DeeperCut**.
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to MPII Human Pose Models ([pose.mpi-inf.mpg.de](http://pose.mpi-inf.mpg.de))
    for detailed information.
  prefs: []
  type: TYPE_NORMAL
- en: 'DeeperCut was developed by a group in Germany at the Max Planck Society, in
    conjunction with Stanford University, who released their algorithm and published
    papers. It is recommended to checkout their paper *DeepCut: Joint Subset Partition
    and Labeling for Multi Person Pose Estimation*, which gives an overview of an
    earlier algorithm, before DeeperCut, where they talk about how they detected body
    parts and how they ran an optimization algorithm to achieve good results. You
    can also refer to their subsequent paper, *DeeperCuts*: *a deeper, stronger and
    faster multi person pose estimation model*, which was published by the same group
    of authors, as this will cover a lot of the technical details. We will definitely
    not get exact results, but you can determine things with a reasonable amount of
    probability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the GitHub page, [https://github.com/eldar/pose-tensorflow](https://github.com/eldar/pose-tensorflow),
    there is the public implementation of their code, which covers DeeperCut and a
    new version called ArtTrack. It is articulated multi-person tracking in the wild,
    and you can see the output result in the following photo:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aaf57d4e-3c54-4ea2-8635-9385de2c2ab3.png)'
  prefs: []
  type: TYPE_IMG
- en: We are going to run a modified version of the code, which is made to run in
    the Jupyter Notebook environment and is made for all learning purposes, so it
    should be a little easier than just getting it straight from GitHub. We will learn
    exactly how we can run the code and use it in our own projects. All of the pre-trained
    models are included here: [https://github.com/eldar/pose-tensorflow.](https://github.com/eldar/pose-tensorflow)
  prefs: []
  type: TYPE_NORMAL
- en: Single-person pose detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have an overview of human pose estimation and the new DeeperCut
    algorithm, we can run the code for single-person pose detection and check that
    out in the Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with single-person detection. Before starting, we need to make
    sure that we are using a clean kernel. You can restart your kernel, or you can
    use the hotkeys to do the same. You can then press the *0* key twice when you're
    in command mode, which is opposed to edit mode when you're actually editing the
    cells.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with our single-person detection code, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The exclamation mark means execute a shell command. This will install a couple
    of libraries that you might not have, and if you have Python 3 installed in your
    system, you might need to change the command to `pip 3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next cell, we will call the `%pylab notebook` function, which will allow
    us to look at images with some useful widgets in the notebook, as well as load
    some numerical libraries, such as `numpy` and so forth. We will do some general
    imports, such as `os`, `sys` and `cv2`. To do some annotations, we will use `imageio` for
    the `imread` function and get everything from `randint`. You don''t need to import
    `numpy` because we have already used `%pylab notebook`, but in case you want to
    copy and paste this code outside of the notebook, you will need it. Then, we need
    to import `tensorflow`, which already has some glued utilities here that come
    from the `pose-tensorflow` repository. The code, for your reference, is shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We then execute the preceding cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now set up pose prediction, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: It will start our session and load our model. We will be using a pre-trained
    model, which you can quickly access from the GitHub repository. The `tf.Session()`
    will start the TensorFlow session and save it to the `sess` variable, which we're
    going to return. Note that when you run this function, it's going to leave the
    TensorFlow session open, so if you want to move on and do something else, such
    as load a new model, then you will have to close the session or restart. It's
    useful here because we're going to be looking at multiple images and it will be
    slower if you load the session every single time. We will then take the configuration,
    which loads the corresponding model and variables, and is going to return the
    necessary values in order to actually run the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we extract CNN outputs using the `extract_cnn_outputs` function. In the
    output, we''ll get joint locations to know where everything is, exactly relative
    to something else. We want a nice ordered 2D array where we know the X and Y locations
    of where the ankles, hands, or shoulders are present. This is demonstrated in
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This is going to take the output from the neural network (which is kind of
    unintelligible) and put it in a form we can actually use. Then, we will feed the
    output to something else, or just visualize it in this case. `argmax_pose_predict`
    is complementary to what we did before. It is another utility function that is
    going to help us understand the output, which is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Let's now execute that cell in which we have defined the functions. It will
    run instantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will load the configuration file, which is `demo/pose_cfg.yaml`,
    and `setup_pose_prediction(cfg)` will return `sess`, `inputs`, and `outputs`.
    This is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When we run the preceding code, it will leave the TensorFlow session open and
    it is recommended to run it only once to avoid errors, or you might have to restart
    the kernel. So, if the command gets executed, we understand that the model has
    been loaded, as you can see in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll see how to actually apply the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For our model, we have to give our file a name. So, we have a directory called
    `testcases` with a bunch of stock photos of people in various poses, which we
    will be using for our test. We then need to load the `standing-leg-lift.jpg` image
    in a suitable format. We will convert the image to something that TensorFlow actually
    needs. The input is like an `image_batch`, which is going to expand the dimensions
    along the `0` axis. So, just create an array that TensorFlow can actually use.
    Then, `outputs_np` will run the session, extract the CNN output in the next line,
    and then do the actual pose prediction. The `pose` variable is the best to use
    here. We should then execute the cell and hit *Esc* button to get into the command
    mode. Then, we need to create a new cell; type `pose` and hit C*trl *+ E*nter*.
    We will then get the following 2D array output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e66b51af-effe-45c2-8b51-20e4e815f1fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output gives us the `x` and `y` coordinates corresponding to the joints
    such as wrists, ankles, knees, head, chin, shoulders, and so on. From this, we
    get the `x` coordinate, `y` coordinate, and matching score. We do not need the
    sub-pixel level, so we can round it to the nearest integer. In the following example,
    you can see that we have labeled our opposing joints with numbers and drawn lines
    between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We need to create a `pose2D` label here, and then we are going to extract the
    x and y coordinates in the first two columns. We will make a copy using `image.copy()`,
    because we want our annotated image to be separate from our original image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will run the following code to show our original image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We are now going to learn how to annotate the original image. We're going to
    create a copy of the image and then we're going to iterate it over the first six
    joints and draw lines between them. It starts on the ankle, `1`, goes up through
    the hips, and then goes down to the the other ankle. Numbers `6` through `11`
    will be the arms and shoulders, and the last two points are the chin and the top
    of the head. We're now going to connect all these points with lines from our `pose2D`.
    We actually don't have points for the `waist` and the `collar`, but we can easily
    estimate those from the midpoints of the hips and the shoulders, which is useful
    for completing the skeleton.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following code, which helps us estimate the midpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now draw a spine by drawing a point from the waist to the collar, and
    the collar to the chin. We can also label these joints to show exactly what we
    are joining, and this will help in your customized application. We are going to
    label the joints, create the figure, show the annotated image, and deal with random
    colors. The following screenshot shows what the output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbe433a8-4db9-4dcc-b044-b7a2d51bad78.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, 1 is the right ankle, but it could be the left ankle depending on which
    way the person's facing. So, all the links are joined except for 13, which is
    a bit occluded here, and 14, which is slightly out of the image. The nice thing
    about this is that it potentially works even if other joints are occluded (for
    instance, if they're off-screen or covered up by something). You will notice that
    the image is simple with a flat background, flat floor, and a simple pose and
    clothes. The code will also work with more complicated images, and if you have
    any trouble reading the details, you can use the widgets here and zoom in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try using different images and analyze our results, which are shown
    in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following shows the photo we will be testing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2eb4e31-b79b-46d9-b13a-301bab289c87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When we run our model again, using a different image, we get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19445ba0-e0f2-4b91-be29-1b3ab509e062.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we take an image of a guy with crossed arms, We get the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68002f06-88dc-43a5-8da1-c0a1019254c7.png)'
  prefs: []
  type: TYPE_IMG
- en: The result is very good, even though the arms are crossed.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's take a look at a few difficult images. This might not give us the
    accurate results of a complete motion capture pose estimation solution, but is
    still very impressive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select `acrobatic.jpeg`, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6d30678-98a9-45f0-bbed-2b148f6e243e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The output we get when we run this photo is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a998f24-5ca7-4468-a184-b9e35e12a17c.png)'
  prefs: []
  type: TYPE_IMG
- en: It looks as if it found the joints, more or less, but did not connect them properly.
    It shows that the guy's head is on his hand, which is touching the ground. We
    can see that the results are not that good. But we cannot expect accurate results
    for all images, even though this is state of the art.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-person pose detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's move from single-person pose detection to multi-person pose detection.
    With single-person pose detection, we saw that the code will take an image of
    a single person and generate pose estimation with all the joints labeled. We will
    now learn a more advanced model called ArtTrack, which will allow us to count
    the number of people, find people, and estimate their poses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the code for multi-person pose detection, which is shown in
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is a little more complicated. We will first list our directories here using
    the `!ls` command in the current directory, where you will find a file called
    `compile.sh`.
  prefs: []
  type: TYPE_NORMAL
- en: We need to run this file because there are some binary dependencies in this
    module. But this is a shell script file, and you might face some issues on macOS
    or Linux. So, in order to generate those files/commands that are OS-specific,
    you will need to run that script. For Windows, those binary files have already
    been generated. So, if you are using the latest version of Python and TensorFlow,
    then the files will be compatible and the binary should work.
  prefs: []
  type: TYPE_NORMAL
- en: If it does not work, you will need to download and install Visual Studio Community.
    There are some instructions that you can follow at [https://github.com/eldar/pose-tensorflow](https://github.com/eldar/pose-tensorflow) under
    the `demo` code section for multi-person pose.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have everything up and running, we can start with our example. Also,
    as we have already discussed, we need to make sure that we restart the kernel.
    This is important because if you have your session already open for running a
    different project, TensorFlow might not be able to compute the code as the previous
    model is loaded. It is always a good practice to start from a fresh kernel.
  prefs: []
  type: TYPE_NORMAL
- en: We will run our `%pylab notebook` to make our visualizations and numerix. The
    code works similarly to what we have already covered, where we have the boilerplate
    and load a pre-trained model. The pre-trained model is already included, so we
    don't need to download it. The code will execute within a second because of TensorFlow,
    and we will get the modules imported and load the repositories as well. Also,
    we need to load the model and actually do the predictions separately. If we hit
    C*trl *+ S*hift *+ *-*,we can separate the predictions into different cells to
    make it look neat.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the first cell, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2797da1-e46c-40d2-8f76-5cc565d2b363.png)'
  prefs: []
  type: TYPE_IMG
- en: This is not a big error message, and is because `imread` was defined here; the
    Notebook clobbers that and just gives you a warning message. We can just rerun
    that code to ignore the warning and get a tidy output.
  prefs: []
  type: TYPE_NORMAL
- en: In this cell, we are going load the configuration file for multiple people provided
    by the authors of ArtTrack/DeeperCut.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following line loads the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, the following line creates the model and loads it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'When we execute that, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75cf5251-4345-430b-8250-fac3e0b66805.png)'
  prefs: []
  type: TYPE_IMG
- en: We will keep the session open here so that we can keep running different things
    and quickly run through different frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now run our code for some test cases that actually have multiple people,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We need to go to the `np.array` and convert it to a flat array network to compute
    the predictions with `sess.run`, and then extract the CNN output and `detections`
    using the model utilities. We will not label the bones here, but we will instead
    label the joints with numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run the code, we get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19c5a46a-bb1f-4893-b003-02abe003965d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is a simple image of multiple people, in plain dress and with a flat background.
    This actually worked. However, the numbers aren''t the same as before. Previously,
    number 1 corresponded to the right ankle and went up through 2, 3, 4, 5, and 6,
    and then 7 was the right wrist, and so on. So, the numbers are different, and
    there are more of them, which actually detects more joints because they have multiple
    numbers for the face, so there are multiple points here. Let''s zoom in to check
    the details, as shown in the following picture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53647dd1-9068-4d16-b90c-91ae7364ca89.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we have the facial landmarks as 1, 2, 3, 4, and 5, and hence this could
    be used in conjunction with the dlib detector, which is covered in [Chapter 6](c59fb392-c966-4da6-987a-625378474e71.xhtml),
    *Facial Feature Tracking and Classification with dlib*. If we want to know somebody's
    facial expression, in addition to the full-body landmark detectors and their pose,
    then this could be done here. We can also get a really thorough description of
    which way people are facing and exactly what they're doing within the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try another `exercise_class.jpeg` image, which gives us the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5770e208-4320-4fd7-8d02-c2567cc740b4.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see how multiple points are present on the knees for the lady on
    the extreme right. It is still a good result.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try one more image, which we saw previously on the GitHub page, `gym.png`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ab0b140-7c79-4dda-952f-4f795d32db80.png)'
  prefs: []
  type: TYPE_IMG
- en: This does detect the body parts here. So, let's try using this model to detect
    the pose for a single person. Do you think it will work? The answer is *yes*,
    it does work. You must be wondering why we would use the previous model if this
    is available. This model is slightly more computationally efficient, so if you
    know you only have one person, you don't actually need it, because this algorithm
    provides the number of people.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can select the photo of a single person from among the photos available.
    For example, we''ll select `mountain_pose.jpg`, which gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a40eaebf-7ee6-440f-9b1a-491c13fd042f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It will also show the number of people, as demonstrated by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c96952fb-1a85-4c57-9c85-272547f8ff93.png)'
  prefs: []
  type: TYPE_IMG
- en: But, if you use the multi-person detector for a single person, it might be prone
    to over-fitting and detecting more people than are actually in the image. So,
    if you already know there is only one person, then it may still be a good idea
    to just use that original model rather than the ArtTrack model. But if it does
    work, try both, or use whatever is best for your application. However, this might
    not work perfectly for complex images and a complicated variety of poses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try one last `island_dance.jpeg` image. The following screenshot shows
    the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca3d7ae4-b8d8-4b8f-a451-b77bd2361b8f.png)'
  prefs: []
  type: TYPE_IMG
- en: Retraining the human pose estimation model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now discuss how to handle videos and retrain our human pose estimation
    network. We have already covered face detection and how to apply a model to a
    video. Opening a video is pretty straightforward and OpenCV provides a mechanism
    for that. It''s basically doing the same thing one frame at a time. The following
    example shows the code for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we need to create a `cv2` capture device, then open the file, and while
    reading the file, we should load the image and run the network on the image. Please
    refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Using a good GPU, we should be able to do the computation in few frames per
    second, if not 30 to 60 FPS, depending on your hardware. You should be able to
    do it almost in real time.
  prefs: []
  type: TYPE_NORMAL
- en: For training your model, you need to first make sure that you have good hardware
    and a lot of time. First, you need to download the ImageNet and ResNet models.
    Then, you need to go through the steps and instructions on the [https://github.com/eldar/pose-tensorflow/blob/master/models/README.md](https://github.com/eldar/pose-tensorflow/blob/master/models/README.md) page.
    You will need a lot of data, so you can use the data they provide. Using your
    own data could be time consuming and difficult to obtain, but it is possible.
    You can refer to the previous link provided for complete instructions.
  prefs: []
  type: TYPE_NORMAL
- en: The instructions here use MATLAB at one point to convert the data, although
    there are ways to do that in Python and train the model with the MS COCO dataset.
    This is similar to what we did in [Chapter 2](d5bf725b-9e79-4865-96e9-338481599464.xhtml),
    *Image Captioning with TensorFlow* and it also provides instructions on how to
    train the model with your own data set. This involves a lot of work and a lot
    of computational power. You can try this or use what has already been provided
    in the pre-trained model, which can do a lot of things.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned the basics of human pose estimation and then used
    the DeeperCut and ArtTrack models in our project for human pose estimation. Using
    these models, we carried out single-person and multi-person pose detection. Towards
    the end of the chapter, we learned how to use the model with videos and retrained
    the model for customized images.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 5](c8202017-404a-42aa-9f24-93488e3abd0a.xhtml), *Handwritten
    Digit Recognition with scikit-learn and TensorFlow*, we will learn handwritten
    digit recognition with scikit-learn and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
