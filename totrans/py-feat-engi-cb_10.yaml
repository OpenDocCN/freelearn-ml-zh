- en: <st c="0">10</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3">Creating Features from a Time Series with tsfresh</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="53">Throughout this book, we’ve discussed feature engineering methods
    and tools tailored for tabular and relational datasets.</st> <st c="176">In this
    chapter, we will shift our focus to time-series data.</st> <st c="238">A time
    series is a sequence of observations taken sequentially over time.</st> <st c="312">Examples
    include energy generation and demand, temperature, air pollutant concentration,
    stock prices, and sales revenue.</st> <st c="434">Each of these examples represents
    a variable and their values change</st> <st c="503">over time.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="513">The widespread availability of affordable sensors capable of measuring
    motion, movement, humidity, glucose, and other parameters has significantly increased
    the amount of temporally annotated data.</st> <st c="712">These time series can
    be utilized in various classification tasks.</st> <st c="779">For instance, by
    analyzing the electricity usage pattern of a household at a given time interval,
    we can infer whether a particular appliance was being used.</st> <st c="937">Similarly,
    the signal of an ultrasound sensor can help determine the probability of a (gas)
    pipeline failure, and the characteristics of a sound wavelength can help predict
    whether a listener will like a song.</st> <st c="1147">Time-series data is also
    valuable for regression tasks.</st> <st c="1203">For example, signals from machinery
    sensors can be used to predict the remaining useful life of</st> <st c="1299">the
    device.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="1310">To use time series with traditional supervised machine learning
    models, such as linear and logistic regression, or decision-tree-based algorithms,
    we need to map each time series into a well-defined feature vector that captures
    its characteristics.</st> <st c="1560">Time-series patterns, including trends,
    seasonality, and periodicity, among other things, can be captured by a combination
    of simple and complex mathematical operations.</st> <st c="1730">Simple calculations
    include, for instance, taking the mean and the standard deviation of the time
    series.</st> <st c="1836">More complex methods include determining correlation
    or entropy, for example.</st> <st c="1914">In addition, we can apply non-linear
    time-series analysis functions to decompose the time-series signal, for example,
    Fourier or wavelet transformations, and use the parameters of these functions
    as features of the</st> <st c="2129">supervised models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2147">Creating features from time series can be very time-consuming;
    we need to apply various signal processing and time-series analysis algorithms
    to identify and extract meaningful features.</st> <st c="2335">The</st> `<st c="2339">tsfresh</st>`
    <st c="2346">Python package, which stands for</st> `<st c="2597">tsfresh</st>`
    <st c="2604">includes a feature selection algorithm that identifies the most predictive
    features for a given time series.</st> <st c="2714">By automating the application
    of complex time-series methods,</st> `<st c="2776">tsfresh</st>` <st c="2783">bridges
    the gap between signal-processing experts and machine learning practitioners,
    making it easier to extract valuable features from</st> <st c="2921">time-series
    data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="2938">In this chapter, we will learn how to automatically create hundreds
    of features from time-series data by utilizing</st> `<st c="3054">tsfresh</st>`<st
    c="3061">. Following that, we will discuss how to fine-tune this feature creation
    process by selecting the most relevant features, extracting different features
    from different time series, and integrating the feature creation process into
    a scikit-learn pipeline to classify</st> <st c="3327">time-series data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3344">In this chapter, we will go through the</st> <st c="3385">following
    recipes:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3403">Extracting hundreds of features automatically from a</st> <st c="3457">time
    series</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3468">Automatically creating and selecting predictive features from</st>
    <st c="3531">time-series data</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3547">Extracting different features from different</st> <st c="3593">time
    series</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3604">Creating a subset of features identified through</st> <st c="3654">feature
    selection</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3671">Embedding feature creation into a</st> <st c="3706">scikit-learn
    pipeline</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="3727">Technical requirements</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="3750">In this chapter, we will use the open source</st> `<st c="3796">tsfresh</st>`
    <st c="3803">Python library.</st> <st c="3820">You can install</st> `<st c="3836">tsfresh</st>`
    <st c="3843">with</st> `<st c="3849">pip</st>` <st c="3852">by executing</st>
    `<st c="3866">pip</st>` `<st c="3870">install tsfresh</st>`<st c="3885">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3886">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="3891">If you have an old Microsoft operating system, you may need to
    update the Microsoft C++ build tools to proceed with the</st> `<st c="4012">tsfresh</st>`
    <st c="4019">package’s installation.</st> <st c="4044">Follow the steps in this
    thread to do</st> <st c="4082">so:</st> [<st c="4086">https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst</st>](https://stackoverflow.com/questions/64261546/how-to-solve-error-microsoft-visual-c-14-0-or-greater-is-required-when-inst)<st
    c="4206">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4207">We will work</st> <st c="4220">with the</st> **<st c="4230">Occupancy
    Detection</st>** <st c="4249">dataset from the UCI Machine Learning Repository,
    available at</st> [<st c="4313">http://archive.ics.uci.edu/ml/datasets/Occupancy+Detection</st>](http://archive.ics.uci.edu/ml/datasets/Occupancy+Detection)
    <st c="4371">and licensed under a Creative Commons Attribution 4.0 International
    (CC BY 4.0) license:</st> [<st c="4461">https://creativecommons.org/licenses/by/4.0/legalcode</st>](https://creativecommons.org/licenses/by/4.0/legalcode)<st
    c="4514">. The corresponding citation for this data is</st> <st c="4560">as follows:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4571">Candanedo, Luis.</st> <st c="4589">(2016).</st> <st c="4597">Occupancy
    Detection.</st> <st c="4618">UCI Machine Learning</st> <st c="4639">Repository.</st>
    [<st c="4651">https://doi.org/10.24432/C5X01N</st>](https://doi.org/10.24432/C5X01N)<st
    c="4682">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4683">I downloaded and modified the data as shown in this</st> <st c="4736">notebook:</st>
    [<st c="4746">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="4885">For a copy of the modified dataset and a target variable, check
    out the files called</st> `<st c="4971">occupancy.csv</st>` <st c="4984">and</st>
    `<st c="4989">occupancy_target.csv</st>`<st c="5009">, available at the following</st>
    <st c="5038">link:</st> [<st c="5044">https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh</st>](https://github.com/PacktPublishing/Python-Feature-engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5151">The Occupancy Detection dataset contains time-series data taken
    over 135 hours at one-minute intervals.</st> <st c="5256">The variables measure
    the temperature, humidity,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/38.png)
    <st c="5305"><st c="5309">level, and light consumption in an office.</st> <st
    c="5352">Camera footage was used to determine whether someone was in the office.</st>
    <st c="5424">The target variable shows whether the office was occupied at any
    one hour.</st> <st c="5499">If the target takes the value</st> `<st c="5529">1</st>`<st
    c="5530">, it means that the office was occupied during that hour; otherwise,
    it takes the</st> <st c="5612">value</st> `<st c="5618">0</st>`<st c="5619">.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5620">The dataset with the time series and that with the target variable
    have different numbers of rows.</st> <st c="5720">The time-series dataset contains
    135 hours of records at one-minute intervals – that is, 8,100 rows.</st> <st c="5821">The
    target has only 135 rows, with a label indicating whether the office was occupied
    at each of the</st> <st c="5922">135 hours.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5932">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="5937">Check out the notebook in this book’s GitHub repository for plots
    of the different time series to become familiar with the</st> <st c="6061">dataset:</st>
    [<st c="6070">https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb</st>](https://github.com/PacktPublishing/Python-Feature-Engineering-Cookbook-Third-Edition/blob/main/ch10-tsfresh/prepare-occupancy-dataset.ipynb)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6209">Extracting hundreds of features automatically from a time series</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="6274">Time series</st> <st c="6287">a</st><st c="6288">re data points
    indexed in time</st> <st c="6319">order.</st> <st c="6326">Analyzing time-series
    sequences allows us to make various predictions.</st> <st c="6397">For example,
    sensor data can be used to predict pipeline failures, sound data can help identify
    music genres, health history or personal measurements such as glucose levels can
    indicate whether a person is sick, and, as we will show in this recipe, patterns
    of light usage, humidity, and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/39.png)
    <st c="6685"><st c="6689">levels can determine whether an office</st> <st c="6728">is
    occupied.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="6740">To tra</st><st c="6747">in regression and classification models
    using traditional machine learning algorithms, such as linear regression or random
    forests, we require a dataset of size</st> *<st c="6909">M x N</st>*<st c="6914">,
    where M is the number of rows and N is the number of features or columns.</st>
    <st c="6990">However, with time-series data, what we have is a collection of</st>
    *<st c="7054">M</st>* <st c="7055">time series, and each time series has multiple
    rows indexed chronologically.</st> <st c="7133">To use time series in supervised
    learning models, each time series needs to be mapped into a well-defined feature
    vector,</st> *<st c="7255">N</st>*<st c="7256">, as shown in the</st> <st c="7274">following
    diagram</st><st c="7291">:</st>
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Diagram showing the process of feature creation from a time
    series for classification or regression](img/B22396_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="7294">Figure 10.1 – Diagram showing the process of feature creation from
    a time series for classification or regression</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="7407">The</st><st c="7411">se feature vectors, which are represented
    as</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/40.png)<st
    c="7457"><st c="7458">,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/41.png)<st
    c="7460"><st c="7461">, and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/42.png)
    <st c="7467"><st c="7468">in</st> *<st c="7472">Figure 10</st>**<st c="7481">.1</st>*<st
    c="7483">, should capture the characteristics of the time series.</st> <st c="7540">For
    example,</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/43.png)<st
    c="7553"><st c="7554">could be the mean value of the time series and</st> ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/44.png)<st
    c="7601"><st c="7602">its variance.</st> <st c="7616">We can crea</st><st c="7627">te
    many features to characterize the time series concerning the distribution of data
    points, correlation properties, stationarity, or entropy, among others.</st> <st
    c="7785">Therefore, the feature vector, N, can be constructed by applying a series</st>
    <st c="7859">of</st> **<st c="7862">characterization methods</st>** <st c="7886">t</st><st
    c="7888">hat take a time series as input and return one or more scalars as output.</st>
    <st c="7962">The mean, or the sum, takes the time-series sequence as input and
    returns a single scalar as output, with the mean value of the time series or the
    sum of its values.</st> <st c="8128">We can also fit a linear trend to the time-series
    sequence, which will return two scalars – one with the slope and one with</st>
    <st c="8252">the intercep</st><st c="8264">t.</st></st></st></st></st></st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="8267">tsfresh</st>` <st c="8275">applies 63 characterization methods
    to a time series, each of which returns one or more scalars, therefore resulting
    in more than 750 features for any given time series.</st> <st c="8446">In this
    recipe, we will use</st> `<st c="8474">tsfresh</st>` <st c="8481">to transform
    time-series data into an M x N feature</st> <st c="8534">table, which</st> <st
    c="8547">we will then use to predict</st> <st c="8575">office occupa</st><st c="8588">ncy.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="8593">Getting ready</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="8607">In this recipe, we wil</st><st c="8630">l use the Occupancy Detection
    dataset described in the</st> *<st c="8686">Technical requirements</st>* <st c="8708">section.</st>
    <st c="8718">This dataset contains measurements of temperature, humidity,</st>
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/45.png)
    <st c="8779"><st c="8782">level, and light consumption in an office taken at one-minute
    intervals.</st> <st c="8855">There are 135 hours of measurements, and each hour
    is flagged with a unique identifier.</st> <st c="8943">There is also a dataset
    with a target variable that indicates in which of these 135 hours the office was
    occupied.</st> <st c="9058">Let’s load the data and make some plot</st><st c="9096">s
    to understand</st> <st c="9113">its patterns:</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9126">Let’s load</st> `<st c="9138">pandas</st>` <st c="9144">and</st>
    `<st c="9149">matplotlib</st>`<st c="9159">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="9213">Load the dataset and display the first</st> <st c="9253">five rows:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="9328">In the following figure, we can see the dataset containing a unique
    identifier, followed by the date and time of the measurements and values for five
    time series capturing temperature, humidity, lights, and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/39.png)
    <st c="9536"><st c="9540">levels in</st> <st c="9550">the offi</st><st c="9558">ce:</st></st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.2 – DataFrame with the time-series data](img/B22396_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="9934">Figure 10.2 – DataFrame with the time-series data</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="9983">Let’s creat</st><st c="9995">e a</st> <st c="10000">function to</st>
    <st c="10011">plot the time series from</st> *<st c="10038">step 2</st>* <st c="10044">at
    a given hour (the</st> `<st c="10066">id</st>` <st c="10068">column is a unique
    identifier for each of the 135 hours</st> <st c="10125">of records):</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="10587">Let’s</st> <st c="10593">plot the time series corresponding to
    an hour when the office was</st> <st c="10660">not occupied:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="10692">In the following figure, we can see the time-series values during
    the second hour of</st> <st c="10778">records, when the office</st> <st c="10802">was</st>
    <st c="10807">empty:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 10.3 – Time-series values during the second hour of data collection\
    \ when the office was empt\uFEFFy](img/B22396_10_03.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="11177">Figure 10.3 – Time-series values during the second hour of data
    collection when the office was empt</st><st c="11276">y</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11278">Note that the lights were off, and that</st> <st c="11318">is
    why we see the flat line at 0 in the plot of</st> `<st c="11366">light</st>` <st
    c="11371">consumption in the</st> <st c="11391">top-right corner.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="11408">Now, let’s plot the time-series data corresponding to an hour
    when the office</st> <st c="11487">was occupied:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="11520">In the following figure, we can see the time-series values during
    the fifteenth hour of records, when the office</st> <st c="11634">was occu</st><st
    c="11642">pied:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Time-series values during the fifteenth hour of data collection,
    when the office was occupied](img/B22396_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="12022">Figure 10.4 – Time-series values during the fifteenth hour of
    data collection, when the office was occupied</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12129">Notice that the lights were on this time (</st><st c="12172">top-right
    panel).</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12190">In this recipe, we will extract features from each of these one-hour
    windows of time-series data, capturing</st> <st c="12298">various aspects of their</st>
    <st c="12324">character</st><st c="12333">istics.</st> <st c="12342">From each
    of these 60-minute time-series segments, we will automatically generate more than
    750 features using</st> `<st c="12453">tsfresh</st>`<st c="12460">, ensuring a
    comprehensive representation of the</st> <st c="12509">data’s prop</st><st c="12520">erties.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12528">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="12544">We will begin by automatically creating hundreds of features from
    one time series,</st> `<st c="12628">lights</st>`<st c="12634">, and then use
    those features to predict whether the office was occupied at any</st> <st c="12714">given
    hour:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="12725">Let’s import the required Python libraries</st> <st c="12769">and
    functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13056">Load the dataset described in the</st> *<st c="13091">Technical</st>*
    *<st c="13101">requirements</st>* <st c="13113">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13177">L</st><st c="13179">oad the target variable into a</st> `<st c="13210">pandas</st>`
    <st c="13216">Series:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13293">Let’s create hundreds of features automatically for each hour
    of light records using</st> `<st c="13379">tsfresh</st>`<st c="13386">. To create
    features from the</st> `<st c="13416">light</st>` <st c="13421">variable, we pass
    the DataFrame containing this variable and the unique identifier for each time
    series to the</st> `<st c="13533">extract_features</st>` <st c="13549">function</st>
    <st c="13559">from</st> `<st c="13564">tsfresh</st>`<st c="13571">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="13638">If we execute</st> `<st c="13653">features.shape</st>`<st c="13667">,
    we’ll obtain</st> `<st c="13682">(135, 789)</st>` <st c="13692">corresponding
    to the size</st> <st c="13719">of</st> <st c="13722">the resulting DataFrame,
    where each row represents an hour of records and each column one of the features
    created by</st> `<st c="13839">tsfresh</st>`<st c="13846">. There are 789 features
    that characterize light consumption at any given hour.</st> <st c="13926">Go ahead
    and execute</st> `<st c="13947">features.head()</st>` <st c="13962">to get a view
    of the resulting DataFrame.</st> <st c="14005">For space reasons, we can’t display
    the entire DataFrame in the book.</st> <st c="14075">So, instead, we will explore
    some of</st> <st c="14112">the features.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="14125">Let’s capture the names of five of the created features in</st>
    <st c="14185">an array:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14226">If we execute</st> `<st c="14241">feats</st>`<st c="14246">, we’ll
    see the names of five features corresponding to the mean, length, standard deviation,
    coefficient of variation, and variance of the light consumption</st> <st c="14404">per
    hour:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14549">Now, let’s display the values of the features from</st> *<st c="14601">step
    5</st>* <st c="14607">for the first</st> <st c="14622">five hours:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="14656">In the following DataFrame, we see the features extracted from
    the time series for the first</st> <st c="14749">five hours of</st> <st c="14764">light</st>
    <st c="14770">co</st><st c="14772">nsumption:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Features created for each hour of light consumption](img/B22396_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="15066">Figure 10.5 – Features created for each hour of light consumption</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15131">Looking at the mean value of light consumption in</st> *<st c="15182">Figure
    10</st>**<st c="15191">.4</st>*<st c="15193">, we can see that the lights were
    on during the first hour, and then</st> <st c="15262">off in the following four
    hours.</st> <st c="15295">The length of the time series is 60 because we have
    60 minutes of records</st> <st c="15369">per hour.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="15378">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="15383">tsfresh</st>` <st c="15391">applies 63 feature creation methods
    to a time series.</st> <st c="15446">Based on the characteristics of the time
    series, such as its length or its variability, some of the methods will return
    missing values or infinite values.</st> <st c="15601">For example, in</st> *<st
    c="15617">Figure 10</st>**<st c="15626">.4</st>*<st c="15628">, w</st><st c="15631">e
    see that the variation coefficient could not be calculated for those hours where
    the light consumption is constant.</st> <st c="15750">And the variance is also</st>
    `<st c="15775">0</st>` <st c="15776">in those cases.</st> <st c="15793">In fact,
    for our dataset, many of the resulting features contain only</st> `<st c="15863">NaN</st>`
    <st c="15866">values, or are constant, like the length, and are therefore not
    useful for training machine</st> <st c="15959">learning models.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="15975">tsfresh</st>` <st c="15983">includes an imputation function
    to impute features that contain</st> `<st c="16048">NaN</st>` <st c="16051">values.</st>
    <st c="16060">Let’s go ahead and impute</st> <st c="16086">our features:</st>'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="16116">The</st> `<st c="16121">impute</st>` <st c="16127">function from</st>
    `<st c="16142">tsfresh</st>` <st c="16149">replaces</st> `<st c="16159">NaN</st>`<st
    c="16162">,</st> `<st c="16164">-Inf</st>`<st c="16168">, and</st> `<st c="16174">Inf</st>`
    <st c="16177">values with the variable’s median, minimum, or maximum</st> <st
    c="16233">values, respectively.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="16254">Let’s use these features to train a logistic regression model
    and predict whether the office</st> <st c="16348">was occupied.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="16361">Let’s begin</st> <st c="16373">by separating the</st> <st c="16391">dataset
    into training and</st> <st c="16418">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="16528">Now, let’s set up and train a logistic regression model, and then
    evaluate</st> <st c="16604">its performance:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="16755">In t</st><st c="16760">he following output, we see the values
    of evaluation metrics that are commonly used for classification analysis, which
    suggests that the created features ar</st><st c="16917">e useful for predicting</st>
    <st c="16942">office occupancy:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17108">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17113">To keep the recipe simple, I have not optimized the model hyperparameters
    or tuned the probability threshold – things that we normally do to ensure our
    models</st> <st c="17273">are accurate.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17286">To finish</st> <st c="17297">off, let’s</st> <st c="17308">extract
    features from every time series, that is,</st> `<st c="17358">light</st>`<st c="17363">,</st>
    `<st c="17365">temperature</st>`<st c="17376">,</st> `<st c="17378">humidity</st>`<st
    c="17386">, and</st> `<st c="17392">co2</st>`<st c="17395">, and this time, we
    will impute the features right after</st> <st c="17452">the extraction:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="17561">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17566">In</st> *<st c="17570">step 10</st>*<st c="17577">, we indicated
    that we want to sort our time series based on the timestamp containing the time
    and date of the measurement, by passing the</st> `<st c="17716">date</st>` <st
    c="17720">variable to the</st> `<st c="17737">column_sort</st>` <st c="17748">parameter.</st>
    <st c="17760">This is useful when our time series are not equidistant or not ordered
    chronologically.</st> <st c="17848">If we leave this parameter set to</st> `<st
    c="17882">None</st>`<st c="17886">,</st> `<st c="17888">tsfresh</st>` <st c="17895">assumes
    that the time series are ordered</st> <st c="17937">and eq</st><st c="17943">uidistant.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="17954">The output of</st> *<st c="17969">step 10</st>* <st c="17976">consists
    of a DataFrame with 135 rows, containing 3,945 features (execute</st> `<st c="18051">features.shape</st>`
    <st c="18065">to check that out) that characterize the five original time series
    – temperature, light, humidity and its ratio, and</st> ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>C</mml:mi><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/47.png)
    <st c="18183"><st c="18187">in the office.</st> <st c="18202">These</st> <st c="18207">features
    were imputed in</st> *<st c="18233">step 10</st>*<st c="18240">, so you can go
    ahead and use this DataFrame to train another logistic</st> <st c="18311">regression</st>
    <st c="18321">model to predict</st> <st c="18339">o</st><st c="18340">ffice occupancy.</st></st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18356">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="18372">In this recipe, we used</st> `<st c="18397">tsfresh</st>` <st
    c="18404">to automatically create hundreds of features from five time series,
    and then used those features to train a logistic regression model to predict whether
    the office</st> <st c="18569">was occupied.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18582">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18587">To create features with</st> `<st c="18612">tsfresh</st>`<st c="18619">,
    the time-series interval from which we want to extract features must be marked</st>
    <st c="18700">with a</st> `<st c="18755">id</st>` <st c="18757">variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="18767">To create features from time series, we used the</st> `<st c="18817">extract_features</st>`
    <st c="18833">function from</st> `<st c="18848">tsfresh</st>`<st c="18855">. This
    function takes the DataFrame containing the time series and the unique identifier
    as input and returns a DataFrame containing the extracted features</st> <st c="19011">as
    output.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="19021">extract_features</st>` <st c="19038">has three key parameters:</st>
    `<st c="19065">column_id</st>`<st c="19074">,</st> `<st c="19076">column_sort</st>`<st
    c="19087">, and</st> `<st c="19093">impute_function</st>`<st c="19108">.</st>
    `<st c="19110">column_id</st>` <st c="19119">receives the name of the column with
    the unique identifier for each sequence that’ll be used to extract features.</st>
    `<st c="19234">column_sort</st>` <st c="19245">is used to reorder the time series
    before extracting features.</st> <st c="19309">When</st> `<st c="19314">column_sort</st>`
    <st c="19325">is left to</st> `<st c="19337">None</st>`<st c="19341">,</st> `<st
    c="19343">tsfresh</st>` <st c="19350">assumes that the data is ordered chronologically
    and that the timestamps are equidistant.</st> <st c="19441">In</st> *<st c="19444">step
    10</st>*<st c="19451">, we passed the</st> `<st c="19467">date</st>` <st c="19471">variable
    as the sorting variable, which informs</st> `<st c="19520">tsfresh</st>` <st c="19527">how
    to sort the data before extracting</st> <st c="19567">the features.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19580">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19585">In our dataset, leaving</st> `<st c="19610">column_sort</st>`
    <st c="19621">set to</st> `<st c="19629">None</st>` <st c="19633">or passing the</st>
    `<st c="19649">date</st>` <st c="19653">variable made no difference, because our
    time series were already ordered chronologically and the timestamps were equidistant.</st>
    <st c="19781">If this is not the case in your time series, use this parameter
    to create</st> <st c="19855">features correctly.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="19874">Finally,</st> `<st c="19884">extract_features</st>` <st c="19900">also
    accepts the</st> `<st c="19918">impute</st>` <st c="19924">function through the</st>
    `<st c="19946">impute_function</st>` <st c="19961">parameter, to</st> <st c="19975">automatically</st>
    <st c="19990">remove infinite and</st> `<st c="20010">NaN</st>` <st c="20013">values
    from the created features.</st> <st c="20048">Will discuss additional parameters
    of</st> `<st c="20086">extract_features</st>` <st c="20102">in the</st> <st c="20110">coming
    recipes.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20125">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20130">For more details</st> <st c="20148">about the</st> `<st c="20158">extract_features</st>`
    <st c="20174">function,</st> <st c="20185">visit</st> [<st c="20191">https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#module-tsfresh.feature_extraction.extraction</st>](https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#module-tsfresh.feature_extraction.extraction)<st
    c="20312">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20313">The</st> `<st c="20318">im</st><st c="20320">pute</st>` <st c="20325">function,
    which can be used independently, as we did in</st> *<st c="20382">step 7</st>*<st
    c="20388">, or within the</st> `<st c="20404">extract_features</st>` <st c="20420">function,
    as we did in</st> *<st c="20444">step 10</st>*<st c="20451">, replaced</st> `<st
    c="20462">NAN</st>`<st c="20465">,</st> `<st c="20467">-Inf</st>`<st c="20471">,
    and</st> `<st c="20477">Inf</st>` <st c="20480">values</st> <st c="20487">with
    the variable’s median, minimum, or maximum values, respectively.</st> <st c="20558">If
    the feature contains only</st> `<st c="20587">NaN</st>` <st c="20590">values,
    they are replaced by zeroes.</st> <st c="20628">The imputation occurs in place
    – that is, in the same DataFrame that is</st> <st c="20700">being imputed.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="20714">The</st> `<st c="20719">extract_features</st>` <st c="20735">function
    returns a DataFrame containing as many rows as unique identifiers in the data.</st>
    <st c="20824">In our case, it returned a DataFrame with 135 rows.</st> <st c="20876">The
    columns of the resulting DataFrame correspond to the 789 values that were returned
    by 63 characterization methods applied to each of the 135 60-minute</st> <st c="21031">time
    series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21043">In</st> *<st c="21047">step 5</st>*<st c="21053">, we explored
    some of the resulting features, which captured the time series mean, variance,
    and coefficient of variation, as well as their length.</st> <st c="21201">Let’s
    explore a few more of the</st> <st c="21233">resulting features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="21252">Some of the created variables are self-explanatory.</st> <st c="21305">For
    example, the</st> `<st c="21322">'light__skewness'</st>` <st c="21339">and</st>
    `<st c="21344">'light__kurtosis'</st>` <st c="21361">variables contain the skewness
    and kurtosis coefficients, which characterize the data distribution.</st> <st
    c="21462">The</st> `<st c="21466">'light__has_duplicate_max'</st>`<st c="21492">,</st>
    `<st c="21494">'light__has_duplicate_min'</st>`<st c="21520">, and</st> `<st c="21526">'light__has_duplicate'</st>`
    <st c="21548">variables indicate whether the time series has duplicated values
    or duplicated minimum or maximum values within the time interval.</st> <st c="21680">The</st>
    `<st c="21684">'light__quantile__q_0.1'</st>`<st c="21708">,</st> `<st c="21710">'light__quantile__q_0.2'</st>`<st
    c="21734">, and</st> `<st c="21740">'light__quantile__q_0.3'</st>` <st c="21764">variables
    display the different quantile values of the time series.</st> <st c="21833">Finally,
    the</st> `<st c="21846">'light__autocorrelation__lag_0'</st>`<st c="21877">,</st>
    `<st c="21879">'light__autocorrelation__lag_1'</st>`<st c="21910">, and</st> `<st
    c="21916">'light__autocorrelation__lag_2'</st>` <st c="21947">variables show the
    autocorrelation of the time series with its past values, lagged by 0, 1, or 2
    steps – information that is generally useful</st> <st c="22090">i</st><st c="22091">n
    forecasting.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22105">Other characterization methods return fe</st><st c="22146">atures
    obtained from signal processing</st> <st c="22185">algorithms, such</st> <st c="22203">as</st>
    <st c="22206">the continuous wavelet transform for the Ricker wavelet, which returns
    the</st> `<st c="22281">'light__cwt_coefficients__coeff_0__w_2__widths_(2, 5,
    10, 20)'</st>`<st c="22343">,</st> `<st c="22345">'light__cwt_coefficients__coeff_0__w_5__widths_(2,
    5, 10, 20)'</st>`<st c="22407">,</st> `<st c="22409">'light__cwt_coefficients__coeff_0__w_10__widths_(2,
    5, 10, 20)'</st>`<st c="22472">, and</st> `<st c="22478">'light__cwt_coefficients__coeff_0__w_20__widths_(2,
    5, 10, 20)'</st>` <st c="22541">features,</st> <st c="22552">among others.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22565">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22570">We can’t discuss each of these feature characterization methods
    or their outputs in detail in this book because there are too many.</st> <st c="22703">You
    can find more details about the transformations</st> <st c="22755">supported by</st>
    `<st c="22768">tsfresh</st>` <st c="22775">and their formulation</st> <st c="22798">at</st>
    [<st c="22801">https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html</st>](https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html)<st
    c="22877">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="22878">Some of the features that are automatically created by</st> `<st
    c="22934">tsfresh</st>` <st c="22941">may not make sense or even be possible to
    calculate for some time series because they require a certain length or data variability,
    or the time series must meet certain distribution assumptions.</st> <st c="23137">Therefore,
    the suitability of the features will depend on the nature of the</st> <st c="23213">time
    series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23225">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23230">You can decide which features to extract from your time series
    based on domain knowledge, or by creating all possible features and then applying
    feature selection algorithms or following up with data analysis.</st> <st c="23441">In
    fact, from our dataset, many of the resulting features were either constant or
    contained only missing data.</st> <st c="23552">Hence, we can reduce the feature
    space to informative features by taking those fe</st><st c="23633">atures out
    of</st> <st c="23648">the data.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23657">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="23666">For more</st> <st c="23675">details about</st> `<st c="23690">tsfresh</st>`<st
    c="23697">, check</st> <st c="23705">out the article Christ M., Braun N., , Neuffer
    J., and Kempa-Liehr A., (2018).</st> *<st c="23784">Time Series FeatuRe Extraction
    on basis of Scalable Hypothesis tests (tsfresh – A Python package).</st> <st c="23883">Neurocomputing
    307 (2018).</st> <st c="23910">Pages</st>* *<st c="23916">72-77.</st>* [<st c="23922">https://dl.acm.org/doi/10.1</st><st
    c="23950">016/j.neucom.2018.03.067</st>](https://dl.acm.org/doi/10.1016/j.neucom.2018.03.067)<st
    c="23975">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="23976">Automatically creating and selecting predictive features fr</st><st
    c="24036">om time-series data</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="24056">In the previous</st> <st c="24073">recipe, we</st> <st c="24084">automatically
    extracted several hundred features from time-series variables using</st> `<st
    c="24166">tsfresh</st>`<st c="24173">. If we have more than one time-series variable,
    we can easily end up with a dataset containing thousands of features.</st> <st
    c="24292">In addition, many of the resulting features had only missing data or
    were constant and were therefore not useful for training mach</st><st c="24422">ine</st>
    <st c="24427">learning models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="24443">When we create classiﬁcation and regression models to solve real-life
    problems, we often want our models to take a small number of relevant features
    as input to produce interpretable machine learning outputs.</st> <st c="24653">Simpler
    models have many advantages.</st> <st c="24690">First, their output is easier
    to interpret.</st> <st c="24734">Second, simpler models are cheaper to store and
    faster to train.</st> <st c="24799">They also return th</st><st c="24818">eir</st>
    <st c="24823">outputs faster.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="24838">tsfresh</st>` <st c="24846">includes a highly parallelizable
    feature selection algorithm based on non-parametric statistical hypothesis tests,
    which can be executed at the back of the feature creation procedure to quickly
    remove irrelevant features.</st> <st c="25069">The feature selection procedure
    utilizes different tests for</st> <st c="25129">different features.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="25149">tsfresh</st>` <st c="25157">uses the following tests to</st>
    <st c="25186">select features:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25202">Fisher’s exact test of independence, if both the feature and the
    target</st> <st c="25275">are binary</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="25285">Kolmogorov-Smirnov test, if either the feature or the target</st>
    <st c="25347">is binary</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="25356">Kendall rank test, if neither the feature nor the target</st>
    <st c="25414">is binary</st>
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="25423">The advantage of these tests is that they are non-parametric,
    and thus make no assumptions on the underlying distribution of the variables</st>
    <st c="25563">being tested.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25576">The result of these tests is a vector of p-values that measures
    the significance of the association between each feature and the target.</st>
    <st c="25714">These p-values are then evaluated based on</st> <st c="25756">the</st>
    <st c="25761">Benjamini-Yekutieli procedure to decide which features</st> <st
    c="25816">to keep.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25824">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="25829">For more details abo</st><st c="25850">ut</st> `<st c="25854">tsfresh</st>`<st
    c="25861">’s featu</st><st c="25870">re selection procedure, check out the article
    Christ, Kempa-Liehr, and Feindt,</st> *<st c="25950">Distributed and parallel
    time series feature extraction for industrial big data applications</st>*<st c="26042">.
    Asian Machine Learning Conference (ACML) 2016, Workshop on Learning on Big Data
    (WLBD), Hamilton (New Zealand),</st> <st c="26156">arXiv,</st> [<st c="26163">https://arxiv.org/abs/1610.07717v1</st>](https://arxiv.org/abs/1610.07717v1)<st
    c="26197">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26198">In this recipe, we will automatically create hundreds of features
    from various time series, and then select the most relevant f</st><st c="26326">eatures
    by</st> <st c="26338">utilizing</st> `<st c="26348">tsfresh</st>`<st c="26355">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26356">How</st> <st c="26360">to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="26372">We will begin by automatically creating and selecting features
    from one time series,</st> `<st c="26458">lights</st>`<st c="26464">, and then
    we will automate the procedure for multiple</st> <st c="26519">time series:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="26531">Let’s import the required Python libraries</st> <st c="26575">and
    functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="26907">Load the</st> <st c="26916">dataset</st> <st c="26925">and the
    target variable described in the</st> *<st c="26966">Technical</st>* *<st c="26976">requirements</st>*
    <st c="26988">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="27122">Let’s create hundreds of features automatically for each hour
    of</st> `<st c="27188">light</st>` <st c="27193">use records and impute the</st>
    <st c="27221">resulting features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="27331">The output of the previous step is a DataFrame with 135 rows and
    789 columns, corresponding to the features created from each hour of</st> <st
    c="27466">light consumption.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="27484">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27489">For more details about</st> *<st c="27513">step 3</st>*<st c="27519">,
    or the Occupancy Detection dataset, check out the</st> *<st c="27571">Extracting
    hundreds of features automatically from a time</st>* *<st c="27629">seri</st><st
    c="27633">es</st>* <st c="27636">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="27644">Now, let’s select the features based on the non-parametric tests
    that we mentioned in the introduction of</st> <st c="27751">this recipe:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="27803">If we execute</st> `<st c="27818">len(features)</st>`<st c="27831">,
    we’ll see the value</st> `<st c="27853">135</st>`<st c="27856">, which means that
    from the 789 features created in</st> *<st c="27908">step 3</st>*<st c="27914">,
    only 135 are statistically significant.</st> <st c="27956">Go ahead and execute</st>
    `<st c="27977">features.head()</st>` <st c="27992">to display the first five rows
    of the</st> <st c="28031">resulting DataFrame.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="28051">For space reasons, we will only display the first</st> <st c="28102">five
    features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="28169">In</st> <st c="28172">the</st> <st c="28176">following DataFrame,
    we see the values of the first five features for the first</st> <st c="28257">five
    hours of</st> <st c="28271">light consumption:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.6 – DataFrame with five of the selected features created from each
    hour of light consumption](img/B22396_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: <st c="28549">Figure 10.6 – DataFrame with five of the selected features created
    from each hour of light consumption</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28651">Check the discussion in the</st> *<st c="28680">How it works…</st>*
    <st c="28693">section for a more detailed analysis of the DataFrame resulting
    from</st> *<st c="28763">step 4</st>*<st c="28769">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="28770">Now, we will use the features from</st> *<st c="28806">step 4</st>*
    <st c="28812">to train a logistic regression model and predict whether the office
    was occupied.</st> <st c="28895">Let’s begin by separating the dataset into training
    and</st> <st c="28951">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="29061">Let’s set</st> <st c="29072">up</st> <st c="29074">and train a
    logistic regression model and then evaluate</st> <st c="29131">its performance:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="29298">In the following output, we see the values of commonly used evaluation
    metrics for classification analysis.</st> <st c="29407">These suggest that the
    selected features are useful for predicting</st> <st c="29474">office occupancy:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: features = extract_relevant_features(
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: X,
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: y,
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: column_id="id",
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: column_sort="date",
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: )
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**<st c="30226">Note</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30231">The parameters of</st> `<st c="30250">extract_relevant_features</st>`
    <st c="30275">are very similar to those of</st> `<st c="30305">extract_features</st>`<st
    c="30321">. Note, however, that the former will automatically perform imputation
    to be able to proceed with the feature selection.</st> <st c="30442">We discussed
    the parameters of</st> `<st c="30473">extract_features</st>` <st c="30489">in
    the</st> *<st c="30497">Extracting hundreds of features automatically from time</st>*
    *<st c="30553">series</st>* <st c="30559">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30567">The output of</st> *<st c="30582">step 8</st>* <st c="30588">consists
    of a DataFrame with 135 rows and 968 features, from the original 3,945 that are
    returned by default by</st> `<st c="30701">tsfresh</st>` <st c="30708">(you can
    check that out by executing</st> `<st c="30746">features.shape</st>`<st c="30760">).</st>
    <st c="30764">Go ahead and use this DataFrame to train another logistic regressio</st><st
    c="30831">n model to predict</st> <st c="30851">office occupancy</st><st c="30867">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="30868">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="30884">In this</st> <st c="30892">recipe, we</st> <st c="30904">created
    hundreds of features from a time series and then selected the most relevant features
    based on non-parametric statistical tests.</st> <st c="31040">The feature creation
    and selection procedures were carried out automatically</st> <st c="31117">by</st>
    `<st c="31120">tsfresh</st>`<st c="31127">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31128">To create the features, we used</st> `<st c="31161">tsfresh</st>`<st
    c="31168">’s</st> `<st c="31172">extract_features</st>` <st c="31188">function,
    which we described in detail in the</st> *<st c="31235">Extracting hundreds of
    features automatical</st><st c="31278">ly from a time</st>* *<st c="31294">series</st>*
    <st c="31300">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31308">To select features, we used the</st> `<st c="31341">select_features</st>`
    <st c="31356">function, also from</st> `<st c="31377">tsfresh</st>`<st c="31384">.
    This function applies different statistical tests, depending on the nature of
    the feature and the target.</st> <st c="31492">Briefly, if the feature and target
    are binary, it tests their relationship with Fisher’s exact test.</st> <st c="31593">If
    either the feature or the target is binary, and the other variable is continuous,
    it tests their relationship by using the Kolmogorov-Smirnov test.</st> <st c="31744">If
    neither the features nor the target is binary, it uses the Kendall</st> <st c="31814">rank
    test.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="31824">The result of these tests is a vector with one p-value per feature.</st>
    <st c="31893">Next,</st> `<st c="31899">tsfresh</st>` <st c="31906">applies the
    Benjamini-Yekutieli procedure, which aims to reduce the false discovery rate,
    to select which features to keep based on the p-values.</st> <st c="32053">This
    feature selection procedure has some advantages, the main one being that statistical
    tests are fast to compute, and therefore the selection algorithm is scalable and
    can be parallelized.</st> <st c="32245">Another advantage is that the tests are
    non-parametric and hence suitable for linear and</st> <st c="32334">non-linear
    models.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32352">However, feature selection methods that evaluate each feature
    individually are unable to remove redundant features.</st> <st c="32469">In fact,
    many of the features automatically created by</st> `<st c="32524">tsfresh</st>`
    <st c="32531">will be highly correlated, like those capturing the different quantiles
    of light consumption.</st> <st c="32626">Hence, they will show similar p-values
    and be retained.</st> <st c="32682">But in practice, we only need one or a few
    of them to capture the information of the time series.</st> <st c="32780">I’d
    recommend following up the</st> `<st c="32811">tsfresh</st>` <st c="32818">selection
    procedure with alternative feature selection methods that are able to pick up</st>
    <st c="32907">feature interactions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="32928">Finally, in</st> *<st c="32941">step 8</st>*<st c="32947">, we
    combined the feature creation step (</st>*<st c="32988">step 3</st>*<st c="32995">)
    with the feature selection step (</st>*<st c="33031">step 4</st>*<st c="33038">)
    by using the</st> `<st c="33054">extract_relevant_features</st>` <st c="33079">function.</st>
    `<st c="33090">extract_relevant_features</st>` <st c="33115">applies the</st>
    `<st c="33128">extract_features</st>` <st c="33144">function to create the features
    from each time series and imputes them.</st> <st c="33217">Next, it applies the</st>
    `<st c="33238">select_features</st>` <st c="33253">function to return a DataFrame
    containing one row per unique identifier, and the features that were selected
    for each time series.</st> <st c="33385">Note that different features can</st>
    <st c="33417">be</st> <st c="33421">selected for different</st> <st c="33444">time
    series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33456">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="33465">The selection algorithm from</st> `<st c="33495">tsfresh</st>`
    <st c="33502">offers a quick method to remove irrelevant features.</st> <st c="33556">However,
    it does not find the best feature subset for the classification or regression
    task.</st> <st c="33649">Other feature selection methods can be applied at the
    back of</st> `<st c="33711">tsfresh</st>`<st c="33718">’s algorithm to reduce
    the feature</st> <st c="33754">space further.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33768">For more details on feature selection algorithms, check out the
    book</st> *<st c="33838">Feature Selection in Machine Learning with Python</st>*
    <st c="33887">by Soledad Galli on</st> <st c="33908">Leanpub:</st> [<st c="33917">https://leanpub.com</st><st
    c="33936">/feature-selection-in-machine-learning/</st>](https://leanpub.com/feature-selection-in-machine-learning/)<st
    c="33976">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="33977">Extracting diffe</st><st c="33994">rent features from different
    time series</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`<st c="34035">tsfresh</st>` <st c="34043">extracts</st> <st c="34053">many
    features based on the time-series characteristics and distribution, such as their
    correlation properties, stationarity, and entropy.</st> <st c="34191">It also
    applies non-linear time-series analysis functions, which decompose the time-series
    signal through, for example, Fourier or wavelet transformations.</st> <st c="34347">Depending
    on the nature of the time series, some of these transformations make more sense
    than others.</st> <st c="34450">For example, wavelength decomposition methods
    can make sense for time series resulting from signals or sensors but are not always
    useful for time series representing sales or</st> <st c="34625">stock prices.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34638">In this recipe, we will discuss how to optimize the feature extracti</st><st
    c="34707">on procedure to extract specific features from each time series, and
    then use th</st><st c="34788">ese features to predict</st> <st c="34813">office
    occupancy.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="34830">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`<st c="34846">tsfresh</st>` <st c="34854">accesses the methods that will be
    used to create features through a dictionary that contains the method names as
    keys and, if they need a parameter, it has the parameter as a value.</st> `<st
    c="35037">tsfresh</st>` <st c="35044">includes some predefined dictionaries as
    well.</st> <st c="35092">We’ll explore these predefined dictionaries first, which
    can be accessed through the</st> `<st c="35177">settings</st>` <st c="35185">module:</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="35193">Let’s import the required Python libraries and functions and the</st>
    `<st c="35259">settings</st>` <st c="35267">module:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="35559">Load the dataset and the target variable described in the</st>
    *<st c="35618">Technical</st>* *<st c="35628">requirements</st>* <st c="35640">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`<st c="35773">tsfresh</st>` <st c="35781">includes</st> <st c="35791">three
    main dictionaries that control the feature creation output:</st> `<st c="35857">settings.ComprehensiveFCParameters</st>`<st
    c="35891">,</st> `<st c="35893">settings.EfficientFCParameters</st>`<st c="35923">,
    and</st> `<st c="35929">settings.MinimalFCParameters</st>`<st c="35957">. Here,
    we’ll explore the dictionary that returns the fewest features.</st> <st c="36028">You
    can repeat the steps to explore the</st> <st c="36068">additional dictionaries.</st>'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="36092">Display the feature creation methods that will be applied when
    using the dictionary that returns the</st> <st c="36194">fewest features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36277">In the output of</st> *<st c="36295">step 3</st>*<st c="36301">,
    we see a dictionary with the feature extraction method names as keys, and the
    parameters used by those methods, if any,</st> <st c="36423">as values:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="36643">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36648">Go ahead and explore the other two predefined dictionaries,</st>
    `<st c="36709">settings.ComprehensiveFCParameters</st>` <st c="36743">and</st>
    `<st c="36748">settings.EfficientFCParameters</st>`<st c="36778">, by ad</st><st
    c="36785">apting the code from</st> *<st c="36807">step 3</st>*<st c="36813">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="36814">Now, let’s use the dictionary from</st> *<st c="36850">step 3</st>*
    <st c="36856">to extract only those features from the</st> `<st c="36897">light</st>`
    <st c="36902">time</st> <st c="36907">series and then display the shape of the</st>
    <st c="36949">resulting DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37087">The output of</st> *<st c="37102">step 4</st>* <st c="37108">is</st>
    `<st c="37112">(135, 10)</st>`<st c="37121">, which means that only 10 features
    were created for each of th</st><st c="37184">e 135 hours of light</st> <st c="37206">consumption
    data.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="37223">Let’s display the</st> <st c="37242">resulting DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="37278">We see the values of the resulting features for the first five
    hours of lig</st><st c="37354">ht consumption in the</st> <st c="37377">following
    DataFrame:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 10.7 – DataFrame with the features created \uFEFFfor each hour of\
    \ light consumption](img/B22396_10_07.jpg)"
  prefs: []
  type: TYPE_IMG
- en: <st c="37945">Figure 10.7 – DataFrame with the features created</st> <st c="37995">for
    each hour of light consumption</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38029">Now, we will use these features to train a logistic regression
    model to predict whether</st> <st c="38118">the office</st> <st c="38129">was
    occupied.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38142">Let’s begin by separating the dataset into training and</st> <st
    c="38199">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38309">Now, let’s set up and train a logistic regression model, and then
    evaluate</st> <st c="38385">its performance:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="38538">In the following output, we see the evaluation metrics that are
    commonly used for classification analysis.</st> <st c="38646">These suggest that
    the selected features are useful for predicting</st> <st c="38713">office occupancy:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**<st c="38879">Note</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="38884">Because light consumption is a very good indicator of office occupancy,
    with very simple features, we can obtain a predictive logistic</st> <st c="39020">regression
    model.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39037">Now, let’s</st> <st c="39049">learn how to specify the creation
    of different features for different</st> <st c="39119">time series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="39131">Let’s create a dictionary with the names of the methods that we
    want to use to create features from the</st> `<st c="39236">light</st>` <st c="39241">time
    series.</st> <st c="39255">We enter the method’s names as keys, and if the methods
    take a parameter, we pass it as an additional dictionary to the corresponding
    key; otherwise, we pass</st> `<st c="39413">None</st>` <st c="39417">as</st> <st
    c="39421">the values:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39552">Now, let’s create a dictionary with the features that we want
    to create from the</st> `<st c="39634">co2</st>` <st c="39637">time series:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39729">Let’s</st> <st c="39735">combine these dictionaries into a</st>
    <st c="39770">new dictionary:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="39851">Finally, let’s use the dictionary from</st> *<st c="39891">step
    10</st>* <st c="39898">to create the features from both</st> <st c="39932">time
    series:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40063">The output of</st> *<st c="40078">step 11</st>* <st c="40085">consists
    of a DataFrame with 135 rows and 8 features.</st> <st c="40140">If we execute</st>
    `<st c="40154">features.columns</st>`<st c="40170">, we will see the names of
    the</st> <st c="40201">created features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="40439">N</st><st c="40441">ote that in the output from</st> *<st c="40469">step
    11</st>*<st c="40476">, different variables have been create</st><st c="40514">d
    from each of the</st> `<st c="40534">light</st>` <st c="40539">and</st> `<st c="40544">co2</st>`
    <st c="40547">time series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40560">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="40576">In this</st> <st c="40585">recipe, we extracted specific features
    from our time-series data.</st> <st c="40651">First, we created features based
    on a predefined dictionary that comes with</st> `<st c="40727">tsfresh</st>`<st
    c="40734">. Next, we created our own dictionary, specifying the creation of different
    features for different</st> <st c="40833">time series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="40845">The</st> `<st c="40850">tsfresh</st>` <st c="40857">package comes
    with some predefined dictionaries that can be accessed through the</st> `<st c="40939">settings</st>`
    <st c="40947">module.</st> <st c="40956">The</st> `<st c="40960">MinimalFCParameters</st>`
    <st c="40979">dictionary is used to create 10 simple features based on basic statistical
    parameters of the time-series distribution, such as the mean, median, standard
    deviation, variance, sum of its values, count (or length), and minimum and maximum
    values.</st> <st c="41225">In</st> *<st c="41228">step 3</st>*<st c="41234">,
    we displayed the dictionary, with the method names as keys, and, as these methods
    do not require additional parameters, each key had</st> `<st c="41369">None</st>`
    <st c="41373">as</st> <st c="41377">the value.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="41387">tsfresh</st>` <st c="41395">has two additional predefined dictionaries.</st>
    `<st c="41440">EfficientFCParameters</st>` <st c="41461">is used to apply methods
    that are fast to compute, whereas</st> `<st c="41521">ComprehensiveFCParameters</st>`
    <st c="41546">returns all possible features and is the one used by default by
    the</st> `<st c="41615">extract_</st><st c="41623">features</st>` <st c="41632">function.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41642">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41647">For more details about the predefined dictionaries, check out</st>
    `<st c="41710">tsfresh</st>`<st c="41717">’s</st> <st c="41721">documentation:</st>
    [<st c="41736">https://tsfresh.readthedocs.io/en/latest</st><st c="41776">/text/feature_extraction_setti</st><st
    c="41807">ngs.html</st>](https://tsfresh.readthedocs.io/en/latest/text/feature_extraction_settings.html)
  prefs: []
  type: TYPE_NORMAL
- en: <st c="41816">By using these predefined dictionaries in the</st> `<st c="41863">default_fc_parameters</st>`
    <st c="41884">parameter of</st> `<st c="41898">tsfresh</st>`<st c="41905">’s</st>
    `<st c="41909">extract_features</st>` <st c="41925">function, we can create specific
    features from one or more time series, as we did in</st> *<st c="42011">step 4</st>*<st
    c="42017">. Note that</st> `<st c="42029">default_fc_parameters</st>` <st c="42050">instructs</st>
    `<st c="42061">extract_features</st>` <st c="42077">to create the same features
    from</st> *<st c="42111">all</st>* <st c="42114">the time series.</st> <st c="42132">What
    if we want to extract different features from different</st> <st c="42193">time
    series?</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42205">To create different features for different time series, we can
    use the</st> `<st c="42277">kind_to_fc_parameters</st>` <st c="42298">parameter
    of</st> `<st c="42312">tsfresh</st>`<st c="42319">’s</st> `<st c="42323">extract_features</st>`
    <st c="42339">function.</st> <st c="42350">This parameter takes a dictionary of
    dictionaries, specifying the methods to apply to each</st> <st c="42441">time
    series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="42453">In</st> *<st c="42457">step 8</st>*<st c="42463">, we created
    a dictionary to specify the creation of specific features from the</st> `<st c="42543">light</st>`
    <st c="42548">time series.</st> <st c="42562">Note that the</st> `<st c="42576">"sum_values"</st>`
    <st c="42588">and</st> `<st c="42593">"mean"</st>` <st c="42599">methods take</st>
    `<st c="42613">None</st>` <st c="42617">as values, but the</st> `<st c="42637">quantile</st>`
    <st c="42645">method needs additional parameters corresponding to the quantiles
    that should be returned from the time series.</st> <st c="42758">In</st> *<st
    c="42761">step 9</st>*<st c="42767">, we created a dictionary to specify the creation
    of features from the</st> `<st c="42838">co2</st>` <st c="42841">time series.</st>
    <st c="42855">In</st> *<st c="42858">step 10</st>*<st c="42865">, we combined
    both dictionaries into one that takes the name of the time series as the key and
    the feature creation dictionaries as values.</st> <st c="43005">Then, we passed
    this dictionary to the</st> `<st c="43044">kind_to_fc_parameters</st>` <st c="43065">parameter
    of</st> `<st c="43079">tsfresh</st>`<st c="43086">’s</st> `<st c="43090">extract_features</st>`
    <st c="43106">function.</st> <st c="43117">This way of specifying features is
    suitable if we use domain knowledge to create the features, or if we only create
    a small number</st> <st c="43248">of features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43260">Do we need to type each method by hand into a dictionary if we
    want to create multiple features for various</st> <st c="43368">time series?</st>
    <st c="43382">Not really.</st> <st c="43394">In the following recipe, we will
    learn how to specify which features</st> <st c="43462">to create based on features
    selected</st> <st c="43500">by Lasso.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43509">Creating a subset of features identified through feature selection</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="43576">In the</st> *<st c="43584">Automatically creating and selecting
    predictive features from time-series data</st>* <st c="43662">recipe, we</st>
    <st c="43673">learned</st> <st c="43682">how to select relevant features using</st>
    `<st c="43720">tsfresh</st>`<st c="43727">. We also discussed the limitations
    of</st> `<st c="43766">tsfresh</st>`<st c="43773">’s selection procedures and
    suggested following up with alternative feature selection methods to identify
    predictive featu</st><st c="43896">res while</st> <st c="43907">avoiding redundancy.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="43927">In this recipe, we will create and select features using</st>
    `<st c="43985">tsfresh</st>`<st c="43992">. Following that, we will reduce the
    feature space further by utilizing Lasso regularization.</st> <st c="44086">Then,
    we will learn how to create a dictionary from the selected feature names to trigger
    the creatio</st><st c="44187">n of those features</st> *<st c="44208">only</st>*
    <st c="44212">from future</st> <st c="44225">time series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44237">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="44253">Let’s begin by importing the necessary libraries and getting the</st>
    <st c="44319">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="44333">Let’s import</st> <st c="44347">the</st> <st c="44350">required
    libraries</st> <st c="44370">and functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44627">Load the Occupancy Detection dataset described in the</st> *<st
    c="44682">Technical</st>* *<st c="44692">requirements</st>* <st c="44704">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="44838">Create and select features from our five time series and then
    display the shape of the</st> <st c="44926">resulting DataFrame:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="45043">The output of</st> *<st c="45058">step 3</st>* <st c="45064">is</st>
    `<st c="45068">(135, 968)</st>`<st c="45078">, indicating that 968 features were
    returned from the five original time series, for each hour</st> <st c="45173">of
    records.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="45184">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45189">We discussed the function from</st> *<st c="45221">step 3</st>*
    <st c="45227">in the</st> *<st c="45235">Automatically creating and selecting
    pred</st><st c="45276">ictive features from time-series</st>* *<st c="45310">data</st>*
    <st c="45314">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45322">Let’s reduce</st> <st c="45336">the</st> <st c="45340">feature
    space further by selecting features with</st> <st c="45389">Lasso regularization.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="45410">Set up logistic regression with Lasso regularization, which is
    the</st> `<st c="45478">"l1"</st>` <st c="45482">penalty.</st> <st c="45492">I
    also set some additional</st> <st c="45519">parameters arbitrarily:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="45644">Let’s set up a transformer to retain those features whose logistic
    regression coefficients are different</st> <st c="45750">from 0:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="45789">Train the logistic regression model and select</st> <st c="45837">the
    features:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="45876">Now, capture the selected features in</st> <st c="45915">a variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="45970">If we</st> <st c="45976">execute</st> `<st c="45985">features</st>`<st
    c="45993">, we’ll</st> <st c="46001">see the names of the</st> <st c="46022">selected
    features:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="46596">To extract just the features from</st> *<st c="46631">step 6</st>*
    <st c="46637">from the time series, we need to capture the feature creation method
    name and corresponding parameters in a dictionary.</st> <st c="46758">We can do
    this automatically from the feature names</st> <st c="46810">with</st> `<st c="46815">tsfresh</st>`<st
    c="46822">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="46907">If we execute</st> `<st c="46922">kind_to_fc_parameters</st>`<st
    c="46943">, we’ll see the dictionary that was created from the</st> <st c="46996">names
    of the features from</st> *<st c="47023">step 6</st>*<st c="47029">:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="47457">Now, we can use</st> <st c="47473">the dictionary from</st> *<st
    c="47494">step 8</st>* <st c="47500">together with the</st> `<st c="47519">extract_features</st>`
    <st c="47535">function to create only those features from</st> <st c="47580">our
    dataset:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="47707">The new DataFrame, which can be displayed by executing</st> `<st
    c="47763">features.head()</st>`<st c="47778">, only contains the 12 features that
    were selected by Las</st><st c="47835">so.</st> <st c="47840">Go ahead and corroborate
    the re</st><st c="47871">sult on</st> <st c="47880">your computer.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="47894">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="47910">In this recipe, we</st> <st c="47930">created 968 features</st>
    <st c="47951">from 5 time series.</st> <st c="47971">Next, we reduced the feature
    space to 12 features by using Lasso regularization.</st> <st c="48052">Finally,
    we captured the specifications of the selected features in a dictionary so that,
    looking forward, we only created those features from our</st> <st c="48199">time
    series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48211">To automatically create and select features with</st> `<st c="48261">tsfresh</st>`<st
    c="48268">, we used the</st> `<st c="48282">extract_relevant_features</st>` <st
    c="48307">function, which we described in detail in the</st> *<st c="48354">Automatically
    creating and selecting predictive features from time-series</st>* *<st c="48428">data</st>*
    <st c="48432">recipe.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="48440">Lasso regularization has the intrinsic ability to reduce some
    of the coefficients of the logistic regression model to 0\.</st> <st c="48562">The
    contribution of the features whose coefficient is 0 to the prediction of office
    occupancy is null and can therefore be removed.</st> <st c="48694">The</st> `<st
    c="48698">SelectFromModel()</st>` <st c="48715">class can identify and remove
    those features.</st> <st c="48762">We set up an instance of</st> `<st c="48787">SelectFromModel()</st>`
    <st c="48804">with a logistic regression model that used Lasso regularization
    to find the model coefficients.</st> <st c="48901">With</st> `<st c="48906">fit()</st>`<st
    c="48911">,</st> `<st c="48913">SelectFromModel()</st>` <st c="48930">trained
    the logistic regression model using the 968 features created from our time series
    and identified those whose coefficients were different from 0\.</st> <st c="49084">Then,
    with the</st> `<st c="49099">get_feature_names_out()</st>` <st c="49122">method,
    we captured the names of the selected features in a</st> <st c="49183">new variable.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49196">To create only the 12 features selected by Lasso regularization,
    we created a dictionary from the variable names by using the</st> `<st c="49323">from_columns()</st>`
    <st c="49337">function from</st> `<st c="49352">tsfresh</st>`<st c="49359">. This
    function returned a dictionary with the variables from which features were selected
    as keys.</st> <st c="49459">The values were additional dictionaries, containing
    the methods used to create features as keys, and the parameters used, if any,
    as values.</st> <st c="49600">To create the new features, we used this</st> <st
    c="49641">dictionary</st> <st c="49652">together with the</st> `<st c="49670">extract_features</st>`
    <st c="49686">function.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49696">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49701">In</st> *<st c="49705">step 9</st>*<st c="49711">, we passed the
    entire dataset</st> <st c="49742">to the</st> `<st c="49749">extract_features</st>`
    <st c="49765">function.</st> <st c="49776">The resulting features only contained
    features extracted from three of the</st> <st c="49851">five time series.</st>
    <st c="49869">The additional two time series</st> <st c="49900">were ignored.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="49913">Embe</st><st c="49918">dding feature creation into a scikit-learn
    pipeline</st>
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: <st c="49970">Throughout this</st> <st c="49986">chapter, we’ve discussed how
    to automatical</st><st c="50030">ly create and select features from time-series
    data by utilizing</st> `<st c="50096">tsfresh</st>`<st c="50103">. Then, we used
    these features to train a classification model to predict whether an office was
    occupied at any</st> <st c="50215">given hour.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="50226">tsfresh</st>` <st c="50234">includes</st> *<st c="50244">wrapper</st>*
    <st c="50251">classes around its main functions,</st> `<st c="50287">extract_features</st>`
    <st c="50303">and</st> `<st c="50308">extract_relevant_features</st>`<st c="50333">,
    to make the creation and selection of features compatible with the</st> <st c="50402">scikit-learn
    pipeline.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50424">In this recipe, we will set up a scikit-learn pipeline that extracts
    features from time series using</st> `<st c="50526">tsfresh</st>` <st c="50533">and
    then trains a logistic re</st><st c="50563">gression model with those features
    to predict</st> <st c="50610">office occupancy.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50627">How to do it...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="50643">Let’s begin by importing the necessary libraries and getting the</st>
    <st c="50709">dataset ready:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="50723">Let’s import the required libraries</st> <st c="50760">and functions:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="51048">Load the Occupancy Detection dataset described in the</st> *<st
    c="51103">Technical</st>* *<st c="51113">requirements</st>* <st c="51125">section:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="51259">Create an empty DataFrame that contains the index of the</st>
    <st c="51317">target variable:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="51367">Now, let’s split</st> <st c="51384">the DataFrame from</st> *<st
    c="51404">step 3</st>* <st c="51410">and the target from</st> *<st c="51431">step
    2</st>* <st c="51437">into training and</st> <st c="51456">test sets:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="51543">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="51548">X_train</st>` <st c="51556">and</st> `<st c="51561">X_test</st>`
    <st c="51567">will be used as containers to store the features created by</st>
    `<st c="51628">tsfresh</st>`<st c="51635">. They are needed for the functionality
    of</st> `<st c="51678">RelevantFe</st><st c="51688">atureAugmenter()</st>` <st
    c="51705">that we will discuss in the</st> <st c="51734">coming steps.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="51747">Let’s create a dictionary specifying the features to extract from
    each time series (I defined the following</st> <st c="51856">features arbitrarily):</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="52750">We discussed the parameters of this dictionary in the</st> *<st
    c="52805">Extrac</st><st c="52811">ting different features from different ti</st><st
    c="52853">me</st>* *<st c="52857">series</st>* <st c="52863">recipe.</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: <st c="52871">Let’s set</st> <st c="52882">up</st> `<st c="52885">RelevantFeatureAugmenter()</st>`<st
    c="52911">, which is a wrapper around the</st> `<st c="52943">extract_relevant_features</st>`
    <st c="52968">function, to create the features specified in</st> *<st c="53015">step
    5</st>*<st c="53021">:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="53144">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53149">To create all possible features, use the</st> `<st c="53191">FeatureAugmenter()</st>`
    <st c="53209">class instead in</st> *<st c="53227">step 6</st>*<st c="53233">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="53234">Let’s combine the feature creation instance from</st> *<st c="53284">step
    6</st>* <st c="53290">with a logistic regression model in a</st> <st c="53329">scikit-learn
    pipeline:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="53462">Now, let’s</st> <st c="53473">tell</st> `<st c="53479">RelevantFeatureAugmenter()</st>`
    <st c="53505">which dataset it needs to use to create</st> <st c="53546">the feat</st><st
    c="53554">ures:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="53611">Let’s fit the pipeline, which will trigger the feature creation
    process, followed by the training of the logistic</st> <st c="53726">regression
    model:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="53770">Now, let’s obtain predictions using the time series in the test
    set and evaluate the model’s performance through a</st> <st c="53886">classification
    report:</st>
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: <st c="53968">We can see the</st> <st c="53984">output of</st> *<st c="53994">step</st>*
    *<st c="53999">10</st>* <st c="54001">here:</st>
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**<st c="54156">The values of the classification report suggest that the extracted
    features are suitabl</st><st c="54244">e for predicting whether the office is
    occupied at any</st> <st c="54300">given hour.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54311">How it works...</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="54327">In this recipe, we combined creating features from a time series
    with</st> `<st c="54398">tsfresh</st>` <st c="54405">with training a machine learning
    algorith</st><st c="54447">m from the scikit-learn library in</st> <st c="54483">a
    pip</st><st c="54488">eline.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="54495">The</st> `<st c="54500">tsfresh</st>` <st c="54507">library includes
    two wrapper classes around its main functions to make the feature creation process
    compatible with the scikit-learn pipeline.</st> <st c="54651">In this recipe,
    we used the</st> `<st c="54679">RelevantFeatureAugmenter()</st>` <st c="54705">class,
    which wraps the</st> `<st c="54729">extract_relevant_features</st>` <st c="54754">function
    to create and then select features from a</st> <st c="54806">time series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: '`<st c="54818">RelevantFeatureAugmenter()</st>` <st c="54845">works as follows;
    with</st> `<st c="54869">fit()</st>`<st c="54874">, it creates and selects features
    by using</st> `<st c="54917">extract_relevant_features</st>`<st c="54942">. The
    names of the selected features are then stored internally in the transformer.</st>
    <st c="55026">With</st> `<st c="55031">transform()</st>`<st c="55042">,</st> `<st
    c="55044">RelevantFeatureAugmenter()</st>` <st c="55070">creates the selected
    features from the</st> <st c="55110">time series.</st>'
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55122">We overrode the default</st> <st c="55147">functionality of</st>
    `<st c="55164">RelevantFeatureAugmenter()</st>` <st c="55190">by passing a dictionary
    with the features we wanted to create to its</st> `<st c="55260">kind_to_fc_parameters</st>`
    <st c="55281">parameter.</st> <st c="55293">Therefore, with</st> `<st c="55309">transform()</st>`<st
    c="55320">,</st> `<st c="55322">RelevantFeatureAugmenter()</st>` <st c="55348">created
    the indicated features from the</st> <st c="55389">time series.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55401">To create all features from the time series,</st> `<st c="55447">tsfresh</st>`
    <st c="55454">includes the</st> `<st c="55468">FeatureAugmenter()</st>` <st c="55486">class,
    which has the same functionality as</st> `<st c="55530">RelevantFeatureAugmenter()</st>`<st
    c="55556">, but without the feature</st> <st c="55582">selection step.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="55597">Both</st> `<st c="55603">RelevantFeatureAugmenter()</st>` <st
    c="55629">and</st> `<st c="55634">FeatureAugmenter()</st>` <st c="55652">need
    two DataFrames to work.</st> <st c="55682">The first DataFrame contains the time-series
    data and the unique identifiers (we loaded this DataFrame in</st> *<st c="55788">step
    2</st>*<st c="55794">).</st> <st c="55798">The second DataFrame should be empty
    and contain the unique identifiers</st> *<st c="55870">in its index</st>* <st
    c="55882">(we created this DataFrame in</st> *<st c="55913">step 3</st>*<st c="55919">).</st>
    <st c="55923">The features are extracted from the first DataFrame with the time
    series (when applying</st> `<st c="56011">transform()</st>`<st c="56022">) and
    subsequently added to the second DataFrame, which is then used to train the logistic
    regression or obtain</st> <st c="56135">its predictions.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56151">Note</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56156">The index of the empty DataFrame is used by</st> `<st c="56201">RelevantFeatureAugmenter()</st>`
    <st c="56227">and</st> `<st c="56232">FeatureAugmenter()</st>` <st c="56250">to
    identify the time series from which to extract the features.</st> <st c="56315">Hence,
    when applying</st> `<st c="56336">fit()</st>` <st c="56341">while passing</st>
    `<st c="56356">X_train</st>`<st c="56363">, features were extracted from time
    series whose</st> `<st c="56412">id</st>` <st c="56414">value was in the training
    set.</st> <st c="56446">After that, the model was evaluated by observing predictions
    made using the test set, which triggered the creation of features from time series
    whose</st> `<st c="56596">id</st>` <st c="56598">value was</st> <st c="56609">in</st>
    `<st c="56612">X_test</st>`<st c="56618">.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56619">When we used</st> `<st c="56633">fit()</st>` <st c="56638">on
    the pipeline, we created features from our raw time series and trained a logistic
    regression model with the resulting features.</st> <st c="56770">With the</st>
    `<st c="56779">predict()</st>` <st c="56788">method, we created features from
    the test set and obtained</st> <st c="56848">the predictions of the</st> <st c="56870">logistic
    regression based on</st> <st c="56900">those features.</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="56915">See also</st>
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: <st c="56924">For more details about the classes</st> <st c="56960">and procedures
    used in this recipe, visit the</st> <st c="57006">following links:</st>
  prefs: []
  type: TYPE_NORMAL
- en: <st c="57022">The</st> `<st c="57027">tsfresh</st>` <st c="57034">documentation:</st>
    [<st c="57050">https://tsfresh.readthedocs.io/en/latest/api/tsfresh.transformers.html#tsfresh.transformers.relevant_feature_augmenter.RelevantFeatureAugmenter</st>](https://tsfresh.readthedocs.io/en/latest/api/tsfresh.transformers.html#tsfresh.transformers.relevant_feature_augmenter.RelevantFeatureAugmenter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: <st c="57193">A Jupyter notebook with a</st> <st c="57220">demo:</st> [<st c="57226">https://github.com/blue-yonder/tsfresh/blob/main/notebooks/02%20sklearn%20Pipeline.ipynb</st>](https://github.com/blue-yonder/tsfresh/blob/main/notebooks/02%20sklearn%20Pipeline.ipynb)******
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
