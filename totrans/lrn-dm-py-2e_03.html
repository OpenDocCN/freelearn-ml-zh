<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Predicting Sports Winners with Decision Trees</h1>
            </header>

            <article>
                
<p>In this chapter, we will look at predicting the winner of sports matches using a different type of classification algorithm to the ones we have seen so far: <strong>decision trees</strong>. These algorithms have a number of advantages over other algorithms. One of the main advantages is that they are readable by humans, allowing for their use in human-driven decision making. In this way, decision trees can be used to learn a procedure, which could then be given to a human to perform if needed. Another advantage is that they work with a variety of features, including categorical, which we will see in this chapter.</p>
<p><span><span><span>We will cover the following topics in this chapter:</span></span></span></p>
<ul>
<li><span><span><span>Using the</span></span></span> pandas<span><span><span> library for loading and manipulating data</span></span></span></li>
<li><span><span><span>Decision trees for classification</span></span></span></li>
<li><span><span><span>Random forests to improve upon decision trees</span></span></span></li>
<li><span><span><span>Using real-world datasets in data mining</span></span></span></li>
<li><span><span><span>Creating new features and testing them in a robust framework</span></span></span></li>
</ul>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Loading the dataset</h1>
            </header>

            <article>
                
<p><span><span><span>In this chapter, we will look at predicting the winner of games of the <strong>National Basketball Association</strong> (<strong>NBA</strong>). Matches in the NBA are often close and can be decided at the last minute, making predicting the winner quite difficult. Many sports share this characteristic, whereby the (generally) <span><span>better team</span></span> could be beaten by another team on the right day.</span></span></span></p>
<p><span><span><span>Various research into predicting the winner suggests that there may be an upper limit to sports outcome prediction accuracy which, depending on the sport, is between 70 percent and 80 percent. There is a significant amount of research being performed into sports prediction, often through data mining or statistics-based methods.</span></span></span></p>
<p>In this chapter, we are going to have a look at an entry level basketball match prediction algorithm, using decision trees for determining whether a team will win a given match. Unfortunately, it doesn't quite make as much profit as the models that sports betting agencies use, which are often a bit more advanced, more complex, and ultimately, more accurate.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Collecting the data</h1>
            </header>

            <article>
                
<p><span><span><span>The data we will be using is the match history data for the NBA for the 2015-2016 season. The website  <a href="http://basketball-reference.com/"><span><span><span><span><span>http://basketball-reference.com</span></span></span></span></span></a> contains a significant number of resources and statistics collected from the NBA and other leagues. To download the dataset, perform the following steps:</span></span></span></p>
<ol>
<li><span><span><span>Navigate to <a href="http://www.basketball-reference.com/leagues/NBA_2016_games.html" target="_blank">http://www.basketball-reference.com/leagues/NBA_2016_games.html</a>  in your web browser.</span></span></span></li>
<li><span><span><span>Click</span></span></span> <span class="packt_screen">Share &amp; more.</span></li>
<li>Click <span class="packt_screen">Get table as CSV (for Excel).</span></li>
<li><span><span><span>Copy the data, including the heading, into a text file named</span></span></span> <kbd>basketball.csv</kbd>.</li>
<li>Repeat this process for the other months, except do not copy the heading.</li>
</ol>
<p><span><span><span>This will give you a CSV file containing the results from each game of this season of the NBA. Your file should contain 1316 games and a total of 1317 lines in the file, including the header line.</span></span></span></p>
<p><span><span><span>CSV files are text files where each line contains a new row and each value is separated by a comma (hence the name). CSV files can be created manually by typing into a text editor and saving with a <kbd><span><span><span><span><span>.csv</span></span></span></span></span></kbd> extension. They can be opened in any program that can read text files but can also be opened in Excel as a spreadsheet. Excel (and other spreadsheet programs) can usually convert a spreadsheet to CSV as well.</span></span></span></p>
<p><span><span><span>We will load the file with the <span><span><span><kbd>pandas</kbd> </span></span></span></span></span></span>library, which is an incredibly useful library for manipulating data. Python also contains a built-in library called <kbd>csv</kbd> that supports reading and writing CSV files. However, we will use pandas, which provides more powerful functions that we will use later in the chapter for creating new features.</p>
<div class="packt_infobox"><span><span><span><span><span><span>For this chapter, you will need to install</span></span></span></span></span></span> pandas<span><span><span><span><span><span>. The easiest way to install it is to use Anaconda's</span></span></span></span></span></span> <kbd>conda</kbd> <span><span><span><span><span><span>installer</span></span></span><span><span><span>, as you did in</span></span></span> <span><span><span><span><span><a href="">Chapter 1</a>,</span></span></span></span></span> <em>Getting Started with data mining to install scikit-learn</em><span><span><span>:<br/></span></span></span></span></span></span><kbd><span><span><span><span><span><span>$ conda install pandas<br/></span></span></span></span></span></span></kbd><span><span><span><span><span><span>If you have difficulty in installing</span></span></span></span></span></span> pandas<span><span><span><span><span><span>, head to the project's website at</span></span></span> <span><span><span><span><span><a href="http://pandas.pydata.org/getpandas.html"><span><span><span>http://pandas.pydata.org/getpandas.html</span></span></span></a></span></span></span></span></span> <span><span><span>and read the installation instructions for your system.</span></span></span></span></span></span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using pandas to load the dataset</h1>
            </header>

            <article>
                
<p><span><span><span>The</span></span></span> <kbd>pandas</kbd> <span><span><span>library is a library for loading, managing, and manipulating data. It handles data structures behind-the-scenes and supports data analysis functions, such as computing the mean and grouping data by value.</span></span></span></p>
<p><span><span><span>When doing multiple data mining experiments, you will find that you write many of the same functions again and again, such as reading files and extracting features. Each time this reimplementation happens, you run the risk of introducing bugs. Using a high-quality library such as <kbd>pandas</kbd> significantly reduces the amount of work needed to do these functions, and also gives you more confidence in using well-tested code to underly your own programs.</span></span></span></p>
<p><span><span><span>Throughout this book, we will be using</span></span></span> pandas <span><span><span>a lot, introducing use cases as we go and new functions as needed.</span></span></span></p>
<p><span><span><span>We can load the dataset using the </span></span></span><kbd>read_csv</kbd> <span><span><span>function:</span></span></span></p>
<pre>
<strong>import pandas as pd</strong><br/><strong>data_filename = "basketball.csv"</strong><br/><strong>dataset = pd.read_csv(data_filename)</strong>
</pre>
<p><span><span><span>The result of this is a</span></span></span> pandas <strong>DataFrame</strong><span><span><span>, and it has some useful functions that we will use later on. Looking at the resulting dataset, we can see some issues. </span></span></span><span><span><span>Type the following and run the code to see the first five rows of the dataset:</span></span></span></p>
<pre>
dataset.head(5)
</pre>
<p><span><span><span>Here's the output:</span></span></span></p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" src="assets/B06162OS_03_01.png"/></div>
<p><span><span><span>Just reading the data with no parameters resulted in quite a usable dataset, but it has some issues which we will address in the next section.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Cleaning up the dataset</h1>
            </header>

            <article>
                
<p><span><span><span>After looking at the output, we can see a number of problems:</span></span></span></p>
<ul>
<li><span><span><span>The date is just a string and not a date object</span></span></span></li>
<li><span><span><span>From visually inspecting the results, the headings aren't complete or correct</span></span></span></li>
</ul>
<p><span><span><span>These issues come from the data and we could fix this by altering the data itself. However, in doing this, we could forget the steps we took or misapply them; that is, we can't replicate our results. As with the previous section where we used pipelines to track the transformations we made to a dataset, we will use</span></span></span> pandas <span><span><span>to apply transformations to the raw data itself.</span></span></span></p>
<p><span><span><span>The</span></span></span> <kbd>pandas.read_csv</kbd> <span><span><span>function has parameters to fix each of these issues, which we can specify when loading the file. We can also change the headings after loading the file, as shown in the following code:</span></span></span></p>
<pre>
dataset = pd.read_csv(data_filename, parse_dates=["Date"]) dataset.columns<br/>        = ["Date", "Start (ET)", "Visitor Team", "VisitorPts", <br/>           "Home Team", "HomePts", "OT?", "Score Type", "Notes"]
</pre>
<p><span><span><span>The results have significantly improved, as we can see if we print out the resulting data frame:</span></span></span></p>
<pre>
dataset.head()
</pre>
<p><span><span><span>The output is as follows:</span></span></span></p>
<div class="CDPAlignCenter CDPAlign"><span><img class=" image-border" src="assets/B06162OS_03_02.png"/></span></div>
<p><span><span><span>Even in well-compiled data sources such as this one, you need to make some adjustments. Different systems have different nuances, resulting in data files that are not quite compatible with each other. When loading a dataset for the first time, always check the data loaded (even if it's a known format) and also check the data types of the data. In</span></span></span> pandas<span><span><span>, this can be done with the following code:</span></span></span></p>
<pre>
print(dataset.dtypes)
</pre>
<p><span><span><span>Now that we have our dataset in a consistent format, we can compute a <strong>baseline</strong>, which is an easy way to get a good accuracy on a given problem. Any decent data mining solution should beat this baseline figure.</span></span></span></p>
<div class="packt_infobox"><span><span><span>For a product recommendation system, a good baseline is to simply <em>recommend the most popular product</em>.<br/>
For a classification task, it can be to <em>always predict the most frequent task</em>, or alternatively applying a very simple classification algorithm like <strong>OneR</strong>.</span></span></span></div>
<p><span><span><span>For our dataset, each match has two teams: a home team and a visitor team. An obvious baseline for this task is 50 percent, which is our expected accuracy if we simply guessed a winner at random. In other words, choosing the predicted winning team randomly will (over time) result in an accuracy of around 50 percent. With a little domain knowledge, however, we can use a better baseline for this task, which we will see in the next section.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Extracting new features</h1>
            </header>

            <article>
                
<p><span><span><span>We will now extract some features from this dataset by combining and comparing the existing data. First, we need to specify our class value, which will give our classification algorithm something to compare against to see if its prediction is correct or not. This could be encoded in a number of ways; however, for this application, we will specify our class as 1 if the home team wins and 0 if the visitor team wins. In basketball, the team with the most points wins. So, while the data set doesn't specify who wins directly, we can easily compute it.</span></span></span></p>
<p><span><span><span>We can specify the data set by the following:</span></span></span></p>
<pre>
dataset["HomeWin"] = dataset["VisitorPts"] &lt; dataset["HomePts"]
</pre>
<p><span><span><span>We then copy those values into a</span></span></span> NumPy array <span><span><span>to use later for our</span></span></span> scikit-learn classifiers. <span><span><span>There is not currently a clean integration between</span></span></span> pandas <span><span><span>and</span></span></span> scikit-learn<span><span><span>, but they work nicely together through the use of</span></span></span> NumPy arrays<span><span><span>. While we will use</span></span></span> pandas <span><span><span>to extract features, we will need to extract the values to use them with scikit-learn:</span></span></span></p>
<pre>
y_true = dataset["HomeWin"].values
</pre>
<p><span><span><span>The preceding array now holds our class values in a format that scikit-learn can read.</span></span></span></p>
<p>By the way, the better baseline figure for sports prediction is to predict the home team in every game. Home teams are shown to have an advantage in nearly all sports across the world. How big is this advantage? Let's have a look:</p>
<pre>
dataset["HomeWin"].mean()
</pre>
<p>The resulting value, around 0.59, indicates that the home team wins 59 percent of games on average. This is higher than 50 percent from random chance and is a simple rule that applies to most sports.</p>
<p><span><span><span>We can also start creating some features to use in our data mining for the input values (the <kbd>X</kbd> array). While sometimes we can just throw the raw data into our classifier, we often need to derive continuous numerical or categorical features from our data.</span></span></span></p>
<p><span><span><span>For our current dataset, we can't really use the features already present (in their current form) to do a prediction. We wouldn't know the scores of a game before we would need to predict the outcome of the game, so we can not use them as features. While this might sound obvious, it can be easy to miss.</span></span></span></p>
<p><span><span><span>The first two features we want to create to help us predict which team will win are whether either of those two teams won their previous game. This would roughly approximate which team is currently playing well.</span></span></span></p>
<p><span><span><span>We will compute this feature by iterating through the rows in order and recording which team won. When we get to a new row, we look up whether the team won the last time we saw them.</span></span></span></p>
<p><span><span><span>We first create a (default) dictionary to store the team's last result:</span></span></span></p>
<pre>
from collections import defaultdict <br/>won_last = defaultdict(int)
</pre>
<p>We then create a new feature on our dataset to store the results of our new features:</p>
<pre>
dataset["HomeLastWin"] = 0<br/>dataset["VisitorLastWin"] = 0
</pre>
<p><span><span><span>The key of this dictionary will be the team and the value will be whether they won their previous game. We can then iterate over all the rows and update the current row with the team's last result:</span></span></span></p>
<pre>
for index, row in dataset.iterrows():<br/>    home_team = row["Home Team"]<br/>    visitor_team = row["Visitor Team"]<br/>    row["HomeLastWin"] = won_last[home_team]<br/>    dataset.set_value(index, "HomeLastWin", won_last[home_team])<br/>    dataset.set_value(index, "VisitorLastWin", won_last[visitor_team])<br/>    won_last[home_team] = int(row["HomeWin"])<br/>    won_last[visitor_team] = 1 - int(row["HomeWin"])
</pre>
<p><span><span><span>Note that the preceding code relies on our dataset being in chronological order. Our dataset is in order; however, if you are using a dataset that is not in order, you will need to replace <kbd><span><span><span><span><span>dataset.iterrows()</span></span></span></span></span></kbd> with <kbd><span><span><span><span><span>dataset.sort("Date").iterrows()</span></span></span></span></span></kbd>.</span></span></span></p>
<p>Those last two lines in the loop update our dictionary with either a 1 or a 0, depending on which team won the <em>current</em> game. This information is used for the next game each team plays.</p>
<p><span><span><span>After the preceding code runs, we will have two new features:</span></span></span> <kbd>HomeLastWin</kbd> <span><span><span>and</span></span></span> <kbd>VisitorLastWin</kbd><span><span><span>. Have a look at the dataset using</span></span></span> <kbd>dataset.head(6)</kbd> <span><span><span>to see an example of a home team and a visitor team that won their recent game. Have a look at other parts of the dataset using the panda's indexer:</span></span></span></p>
<pre>
dataset.ix[1000:1005]
</pre>
<p><span><span><span>Currently, this gives a false value to all teams (including the previous year's champion!) when they are first seen. We could improve this feature using the previous year's data, but we will not do that in this chapter.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Decision trees</h1>
            </header>

            <article>
                
<div class="packt_infobox"><span><span><span>Decision trees are a class of supervised learning algorithms like a flow chart that consists of a sequence of nodes, where the values for a sample are used to make a decision on the next node to go to.  </span></span></span></div>
<p class="CDPAlignLeft CDPAlign">The following example gives a very good idea of how decision trees are a class of supervised learning algorithms:</p>
<div class="CDPAlignCenter CDPAlign"><img class=" image-border" height="172" src="assets/B06162OS_03_03.jpg" width="253"/></div>
<p><span><span><span>As with most classification algorithms, there are two stages to using them:</span></span></span></p>
<ul>
<li><span><span><span>The first stage is the</span></span></span> <strong>training</strong> <span><span><span>stage, where a tree is built using training data. While the nearest neighbor algorithm from the previous chapter did not have a training phase, it is needed for decision trees. In this way, the nearest neighbor algorithm is a lazy learner, only doing any work when it needs to make a prediction. In contrast, decision trees, like most classification methods, are eager learners, undertaking work at the training stage and therefore needing to do less in the predicting stage.</span></span></span></li>
<li><span><span><span>The second stage is the</span></span></span> <strong>predicting</strong> <span><span><span>stage, where the trained tree is used to predict the classification of new samples. Using the previous example tree, a data point of <kbd><span><span><span><span><span>["is raining", "very windy"]</span></span></span></span></span></kbd> would be classed as</span></span></span> <em>bad weather</em><span><span><span>.</span></span></span></li>
</ul>
<div class="packt_infobox"><span><span><span>There are many algorithms for creating decision trees. Many of these algorithms are iterative. They start at the base node and decide the best feature to use for the first decision, then go to each node and choose the next best feature, and so on. This process is stopped at a certain point when it is decided that nothing more can be gained from extending the tree further.</span></span></span></div>
<p><span><span><span>The</span></span></span> <kbd>scikit-learn</kbd> <span><span><span>package implements the <strong>Classification and Regression Trees</strong> (<strong>CART</strong>) algorithm as its default dDecision tree class, which can use both categorical and continuous features.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Parameters in decision trees</h1>
            </header>

            <article>
                
<p><span><span><span>One of the most important parameters for a Decision Tree is the <strong>stopping criterion</strong>. When the tree building is nearly completed, the final few decisions can often be somewhat arbitrary and rely on only a small number of samples to make their decision. Using such specific nodes can result in trees that significantly overfit the training data. Instead, a stopping criterion can be used to ensure that the Decision Tree does not reach this exactness.</span></span></span></p>
<p><span><span><span>Instead of using a stopping criterion, the tree could be created in full and then trimmed. This trimming process removes nodes that do not provide much information to the overall process. This is known as <strong>pruning</strong> and results in a model that generally does better on new datasets because it hasn't overfitted the training data.</span></span></span></p>
<p><span><span><span>The decision tree implementation in</span></span></span> scikit-learn <span><span><span>provides a method to stop the building of a tree using the following options:</span></span></span></p>
<ul>
<li><span><span><span><kbd><strong><span class="packt_screen">min_samples_split</span></strong></kbd>: This specifies how many samples are needed in order to create a new node in the Decision Tree</span></span></span></li>
<li><span><span><span><kbd><strong><span class="packt_screen">min_samples_leaf</span></strong></kbd>: This specifies how many samples must be resulting from a node for it to stay</span></span></span></li>
</ul>
<p><span><span><span>The first dictates whether a decision node will be created, while the second dictates whether a decision node will be kept.</span></span></span></p>
<p><span><span><span>Another parameter for decision trees is the criterion for creating a decision. <strong>Gini impurity</strong> and<strong> information gain</strong> are two popular options for this parameter:</span></span></span></p>
<ul>
<li><span><span><span><strong>Gini impurity</strong>: This is a measure of how often a decision node would incorrectly predict a sample's class</span></span></span></li>
<li><span><span><span><strong>Information gain</strong>: This uses information-theory-based entropy to indicate how much extra information is gained by the decision node</span></span></span></li>
</ul>
<p>These parameter values do approximately the same thing--decide which rule and value to use to split a node into subnodes. The value itself is simply which metric to use to determine that split, however this can make a significant impact on the final models.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using decision trees</h1>
            </header>

            <article>
                
<p><span><span><span>We can import the <kbd><span><span><span><span><span>DecisionTreeClassifier</span></span></span></span></span></kbd> class and create a Decision Tree using</span></span></span> scikit-learn<span><span><span>:</span></span></span></p>
<pre>
<span><span><span>from sklearn.tree import DecisionTreeClassifier<br/>clf = DecisionTreeClassifier(random_state=14)</span></span></span>
</pre>
<div class="packt_tip"><span><span><span>We used 14 for our</span></span></span> <kbd><span><span><span><span><span><span><span><span>random_state</span></span></span></span></span></span></span></span></kbd> <span><span><span>again and will do so for most of the book. Using the same random seed allows for replication of experiments. However, with your experiments, you should mix up the random state to ensure that the algorithm's performance is not tied to the specific value.</span></span></span></div>
<p><span><span><span>We now need to extract the dataset from our</span></span></span> pandas <span><span><span>data frame in order to use it with our</span></span></span> <kbd>scikit-learn</kbd> <span><span><span>classifier. We do this by specifying the columns we wish to use and using the <span><span><span><span><span>values</span></span></span></span></span> parameter of a view of the data frame. The following code creates a dataset using our last win values for both the home team and the visitor team:</span></span></span></p>
<pre>
<span><span><span>X_previouswins = dataset[["HomeLastWin", "VisitorLastWin"]].values</span></span></span>
</pre>
<p><span><span><span>Decision trees are estimators, as introduced in <span><span><a href="">Chapter 2</a></span></span>, <em>Classifying using </em></span></span></span><em>scikit-learn</em> <span><span><span><em>Estimators</em>, and therefore have <kbd><span><span><span><span><span>fit</span></span></span></span></span></kbd> and <kbd><span><span><span><span><span>predict</span></span></span></span></span></kbd> methods. We can also use the <kbd><span><span><span><span><span>cross_val_score</span></span></span></span></span></kbd> method to get the average score (as we did previously):</span></span></span></p>
<pre>
<span><span><span>from sklearn.cross_validation import cross_val_score<br/>import numpy as np<br/>scores = cross_val_score(clf, X_previouswins, y_true,<br/>scoring='accuracy')<br/></span></span></span><span><span><span>print("Accuracy: {0:.1f}%".format(np.mean(scores) * 100))</span></span></span>
</pre>
<p><span><span><span>This scores 59.4 percent: we are better than choosing randomly! However, we aren't beating our other baseline of just choosing the home team. In fact, we are pretty much exactly the same. We should be able to do better. <strong>Feature engineering</strong> is one of the most difficult tasks in data mining, and choosing <em>good</em> <strong>features</strong> is key to getting good outcomes—more so than choosing the right algorithm!</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Sports outcome prediction</h1>
            </header>

            <article>
                
<p><span><span><span>We may be able to do better by trying other features. We have a method for testing how accurate our models are. The</span></span></span> <kbd>cross_val_score</kbd> <span><span><span>method allows us to try new features.</span></span></span></p>
<p><span><span><span>There are many possible features we could use, but we will try the following questions:</span></span></span></p>
<ul>
<li><span><span><span>Which team is considered better generally?</span></span></span></li>
<li><span><span><span>Which team won their last encounter?</span></span></span></li>
</ul>
<p><span><span><span>We will also try putting the raw teams into the algorithm, to check whether the algorithm can learn a model that checks how different teams play against each other.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Putting it all together</h1>
            </header>

            <article>
                
<p><span><span><span>For the first feature, we will create a feature that tells us if the home team is generally <span><span>better</span></span> than the visitors. To do this, we will load the standings (also called a ladder in some sports) from the NBA in the previous season. A team will be considered better if it ranked higher in 2015 than the other team.</span></span></span></p>
<p><span><span><span>To obtain the standings data, perform the following steps:</span></span></span></p>
<ol>
<li><span><span><span>Navigate to <span><span><span><span><span><a href="http://www.basketball-reference.com/leagues/NBA_2015_standings.html" target="_blank">http://www.basketball-reference.com/leagues/NBA_2015_standings.html</a></span></span></span></span></span> in your web browser.</span></span></span></li>
<li><span><span><span>Select <span><span class="packt_screen">Expanded Standings</span></span> to get a single list for the entire league.</span></span></span></li>
<li><span><span><span>Click on the <span class="packt_screen"><span>Export</span> link</span>.</span></span></span></li>
<li><span><span><span>Copy the text and save it in a text/CSV file called <kbd>standings.csv</kbd> in your data folder.</span></span></span></li>
</ol>
<p><span><span><span>Back in your Jupyter Notebook, enter the following lines into a new cell. You'll need to ensure that the file was saved into the location pointed to by the <span><span><span><span><span>data_folder</span></span></span></span></span> variable. The code is as follows:</span></span></span></p>
<pre>
<span><span><span>import os<br/>standings_filename = os.path.join(data_folder, "standings.csv")<br/></span></span></span><span><span><span>standings = pd.read_csv(standings_filename, skiprows=1)</span></span></span>
</pre>
<p><span><span><span>You can view the ladder by just typing <span><span><span><span><span>standings</span></span></span></span></span> into a new cell and running<br/>
the code:</span></span></span></p>
<pre>
<span><span><span>standings.head()</span></span></span>
</pre>
<p><span><span><span>The output is as follows:</span></span></span></p>
<div class="CDPAlignCenter CDPAlign"><span><img class=" image-border" src="assets/B06162OS_03_04.png"/></span></div>
<p><span><span><span>Next, we create a new feature using a similar pattern to the previous feature. We iterate over the rows, looking up the standings for the home team and visitor team. The code is as follows:</span></span></span></p>
<pre>
<span><span><span>dataset["HomeTeamRanksHigher"] = 0<br/>for index, row in dataset.iterrows():<br/>    home_team = row["Home Team"]<br/>    visitor_team = row["Visitor Team"]<br/>    home_rank = standings[standings["Team"] == home_team]["Rk"].values[0]<br/>    visitor_rank = standings[standings["Team"] == visitor_team]["Rk"].values[0]<br/>    row["HomeTeamRanksHigher"] = int(home_rank &gt; visitor_rank)<br/>    dataset.set_value(index, "HomeTeamRanksHigher", int(home_rank &lt; visitor_rank))</span></span></span>
</pre>
<p><span><span><span>Next, we use the <kbd><span><span><span><span><span>cross_val_score</span></span></span></span></span></kbd> function to test the result. First, we extract the dataset:</span></span></span></p>
<pre>
<span><span><span>X_homehigher = dataset[["HomeLastWin", "VisitorLastWin", "HomeTeamRanksHigher"]].values</span></span></span>
</pre>
<p><span><span><span>Then, we create a new <kbd><span><span><span><span><span>DecisionTreeClassifier</span></span></span></span></span></kbd> and run the evaluation:</span></span></span></p>
<pre>
<span><span><span>clf = DecisionTreeClassifier(random_state=14)<br/></span></span></span><span><span><span>scores = cross_val_score(clf, X_homehigher, y_true, scoring='accuracy')<br/></span></span></span><span><span><span>print("Accuracy: {0:.1f}%".format(np.mean(scores) * 100))</span></span></span>
</pre>
<p><span><span><span>This now scores 60.9 percent  even better than our previous result, and now better than just choosing the home team every time. Can we do better?</span></span></span></p>
<p><span><span><span>Next, let's test which of the two teams won their last match against each other. While rankings can give some hints on who won (the higher ranked team is more likely to win), sometimes teams play better against other teams. There are many reasons for this--for example, some teams may have strategies or players that work against specific teams really well. Following our previous pattern, we create a dictionary to store the winner of the past game and create a new feature in our data frame. The code is as follows:</span></span></span></p>
<pre>
last_match_winner = defaultdict(int)<br/>dataset["HomeTeamWonLast"] = 0<br/><br/>for index, row in dataset.iterrows():<br/>    home_team = row["Home Team"]<br/>    visitor_team = row["Visitor Team"]<br/>    teams = tuple(sorted([home_team, visitor_team])) # Sort for a consistent ordering<br/>    # Set in the row, who won the last encounter<br/>    home_team_won_last = 1 if last_match_winner[teams] == row["Home Team"] else 0<br/>    dataset.set_value(index, "HomeTeamWonLast", home_team_won_last)<br/>    # Who won this one?<br/>    winner = row["Home Team"] if row["HomeWin"] else row["Visitor Team"]<br/>    last_match_winner[teams] = winner
</pre>
<p>This feature works much like our previous rank-based feature. However, instead of looking up the ranks, this features creates a tuple called <kbd>teams</kbd>, and then stores the previous result in a dictionary. When those two teams play each other next, it recreates this tuple, and looks up the previous result. Our code doesn't differentiate between home games and visitor games, which might be a useful improvement to look at implementing.</p>
<p><span><span><span>Next, we need to evaluate. The process is pretty similar to before, except we add the new feature into the extracted values:</span></span></span></p>
<pre>
X_lastwinner = dataset[[ "HomeTeamWonLast", "HomeTeamRanksHigher", "HomeLastWin", "VisitorLastWin",]].values<br/>clf = DecisionTreeClassifier(random_state=14, criterion="entropy")<br/><br/>scores = cross_val_score(clf, X_lastwinner, y_true, scoring='accuracy')<br/><br/>print("Accuracy: {0:.1f}%".format(np.mean(scores) * 100))
</pre>
<p><span><span><span>This scores 62.2 percent. Our results are getting better and better.</span></span></span></p>
<p><span><span><span>Finally, we will check what happens if we throw a lot of data at the Decision Tree, and see if it can learn an effective model anyway. We will enter the teams into the tree and check whether a <span>Decision Tree </span>can learn to incorporate that information.</span></span></span></p>
<p><span><span><span>While <span>decision tree</span>s are capable of learning from categorical features, the implementation in</span></span></span> <kbd>scikit-learn</kbd> <span><span><span>requires those features to be encoded as numbers and features, instead of string values. We can use the <kbd><span><span><span><span><span>LabelEncoder</span></span></span></span></span></kbd> <strong>transformer</strong> to convert the string-based team names into assigned integer values. The code is as follows:</span></span></span></p>
<pre>
from sklearn.preprocessing import LabelEncoder<br/>encoding = LabelEncoder()<br/>encoding.fit(dataset["Home Team"].values)<br/>home_teams = encoding.transform(dataset["Home Team"].values)<br/>visitor_teams = encoding.transform(dataset["Visitor Team"].values)<br/>X_teams = np.vstack([home_teams, visitor_teams]).T
</pre>
<p>We should use the same transformer for encoding both the home team and visitor teams. This is so that the same team gets the same integer value as both a home team and visitor team. While this is not critical to the performance of this application, it is important and failing to do this may degrade the performance of future models.</p>
<p><span><span><span>These integers can be fed into the Decision Tree, but they will still be interpreted as continuous features by <kbd><span><span><span><span><span>DecisionTreeClassifier</span></span></span></span></span></kbd>. For example, teams may be allocated integers from 0 to 16. The algorithm will see teams 1 and 2 as being similar, while teams 4 and 10 will be very different--but this makes no sense as all. All of the teams are different from each other--two teams are either the same or they are not!</span></span></span></p>
<p><span><span><span>To fix this inconsistency, we use the <kbd><span><span><span><span><span>OneHotEncoder</span></span></span></span></span></kbd> <strong>transformer</strong> to encode these integers into a number of binary features. Each binary feature will be a single value for the feature. For example, if the NBA team Chicago Bulls is allocated as integer 7 by the <kbd><span><span><span><span><span>LabelEncoder</span></span></span></span></span></kbd>, then the seventh feature returned by the <kbd><span><span><span><span><span>OneHotEncoder</span></span></span></span></span></kbd> will be a 1 if the team is <span><span>Chicago Bulls</span></span> and 0 for all other features/teams. This is done for every possible value, resulting in a much larger dataset. The code is as follows:</span></span></span></p>
<pre>
from sklearn.preprocessing import OneHotEncoder<br/>onehot = OneHotEncoder()<br/>X_teams = onehot.fit_transform(X_teams).todense()
</pre>
<p><span><span><span>Next, we run the Decision Tree as before on the new dataset:</span></span></span></p>
<pre>
clf = DecisionTreeClassifier(random_state=14)<br/>scores = cross_val_score(clf, X_teams, y_true, scoring='accuracy')<br/>print("Accuracy: {0:.1f}%".format(np.mean(scores) * 100))
</pre>
<p><span><span><span>This scores an accuracy of 62.8 percent. The score is better still, even though the information given is just the teams playing. It is possible that the larger number of features were not handled properly by the decision trees. For this reason, we will try changing the algorithm and see if that helps. Data mining can be an iterative process of trying new algorithms and features.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Random forests</h1>
            </header>

            <article>
                
<div class="packt_infobox"><span><span><span>A single Decision Tree can learn quite complex functions. However, decision trees are prone to overfitting--learning rules that work only for the specific training set and don't generalize well to new data.</span></span></span></div>
<p><span><span><span>One of the ways that we can adjust for this is to limit the number of rules that it learns. For instance, we could limit the depth of the tree to just three layers. Such a tree will learn the best rules for splitting the dataset at a global level, but won't learn highly specific rules that separate the dataset into highly accurate groups. This trade-off results in trees that may have a good generalization, but an overall slightly poorer performance on the training dataset.</span></span></span></p>
<p><span><span><span>To compensate for this, we could create many of these <em>limited</em> decision trees and then ask each to predict the class value. We could take a majority vote and use that answer as our overall prediction.</span></span></span> Random Forests <span><span><span>is an algorithm developed from this insight.</span></span></span></p>
<p><span><span><span>There are two problems with the aforementioned procedure. The first problem is that building decision trees is largely deterministic—using the same input will result in the same output each time. We only have one training dataset, which means our input (and therefore the output) will be the same if we try to build multiple trees. We can address this by choosing a random subsample of our dataset, effectively creating <span><span><span><span><span>new</span></span></span></span></span> training sets. This process is called</span></span></span> <strong>bagging</strong><span><span><span><span class="packt_screen"> </span></span></span></span> and it can be very effective in many situations in data mining.</p>
<p><span><span><span>The second problem we might run into with creating many decision trees from similar data is that the features that are used for the first few decision nodes in our tree will tend to be similar. Even if we choose random subsamples of our training data, it is still quite possible that the decision trees built will be largely the same. To compensate for this, we also choose a random subset of the features to perform our data splits on.</span></span></span></p>
<p><span><span><span>Then, we have randomly built trees using randomly chosen samples, using (nearly) randomly chosen features. This is a random forest and, perhaps <span><span>unintuitively</span></span>, this algorithm is very effective for many datasets, with little need to tune many parameters of the model.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">How do ensembles work?</h1>
            </header>

            <article>
                
<p><span><span><span>The randomness inherent in random forests may make it seem like we are leaving the results of the algorithm up to chance. However, we apply the benefits of averaging to nearly randomly built decision trees, resulting in an algorithm that reduces the variance of the result.</span></span></span></p>
<div class="packt_infobox"><span><span><span><strong>Variance</strong> is the error introduced by variations in the training dataset on the algorithm. Algorithms with a high variance (such as decision trees) can be greatly affected by variations to the training dataset. This results in models that have the problem of overfitting.</span></span></span> In contrast, <strong>bias</strong> is the error introduced by assumptions in the algorithm rather than anything to do with the dataset, that is, if we had an algorithm that presumed that all features would be normally distributed, then our algorithm may have a high error if the features were not.</div>
<p>Negative impacts from bias can be reduced by analyzing the data to see if the classifier's data model matches that of the actual data.</p>
<p>To use an extreme example, a classifier that always predicts true, regardless of the input, has a very high bias. A classifier that always predicts randomly would have a very high variance. Each classifier has a high degree of error but of a different nature.</p>
<p><span><span><span>By averaging a large number of decision trees, this variance is greatly reduced. This results, at least normally, in a model with a higher overall accuracy and better predictive power. The trade-offs are an increase in time and an increase in the bias of the algorithm.</span></span></span></p>
<p><span><span><span>In general, ensembles work on the assumption that errors in prediction are effectively random and that those errors are quite different from one classifier to another. By averaging the results across many models, these random errors are canceled out—leaving the true prediction. We will see many more ensembles in action throughout the rest of the book.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Setting parameters in Random Forests</h1>
            </header>

            <article>
                
<p><span><span><span>The Random Forest implementation in</span></span></span> scikit-learn <span><span><span>is called <kbd><span><span><span><span><span>RandomForestClassifier</span></span></span></span></span></kbd>, and it has a number of parameters. As Random Forests use many instances of <kbd><span><span><span><span><span>DecisionTreeClassifier</span></span></span></span></span></kbd>, they share many of the same parameters such as the <kbd>criterion</kbd> (Gini Impurity or Entropy/information gain), <kbd><span><span><span><span><span>max_features</span></span></span></span></span></kbd>, and <kbd><span><span><span><span><span>min_samples_split</span></span></span></span></span></kbd>.</span></span></span></p>
<p><span><span><span>There are some new parameters that are used in the ensemble process:</span></span></span></p>
<ul>
<li><span><span><span><kbd><span><span><span><span><span>n_estimators</span></span></span></span></span></kbd>: This dictates how many decision trees should be built. A higher value will take longer to run, but will (probably) result in a higher accuracy.</span></span></span></li>
<li><span><span><span><kbd><span><span><span><span><span>oob_score</span></span></span></span></span></kbd>: If true, the method is tested using samples that aren't in the random subsamples chosen for training the decision trees.</span></span></span></li>
<li><span><span><span><kbd><span><span><span><span><span>n_jobs</span></span></span></span></span></kbd>: This specifies the number of cores to use when training the decision trees in parallel.</span></span></span></li>
</ul>
<p><span><span><span>The</span></span></span> <kbd>scikit-learn</kbd> <span><span><span>package uses a library called</span></span></span> <strong>Joblib</strong> <span><span><span>for inbuilt parallelization. This parameter dictates how many cores to use. By default, only a single core is used--if you have more cores, you can increase this, or set it to -1 to use all cores.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Applying random forests</h1>
            </header>

            <article>
                
<p><span><span><span>Random forests in</span></span></span> scikit-learn <span><span><span>use the</span></span></span> <strong>Estimator</strong> <span><span><span>interface, allowing us to use almost the exact same code as before to do cross-fold validation:</span></span></span></p>
<pre>
from sklearn.ensemble import RandomForestClassifier<br/>clf = RandomForestClassifier(random_state=14)<br/>scores = cross_val_score(clf, X_teams, y_true, scoring='accuracy')<br/>print("Accuracy: {0:.1f}%".format(np.mean(scores) * 100))
</pre>
<p><span><span><span>This results in an immediate benefit of 65.3 percent, up by 2.5 points by just swapping the classifier.</span></span></span></p>
<p><span><span><span>Random forests, using subsets of the features, should be able to learn more effectively with more features than normal decision trees. We can test this by throwing more features at the algorithm and seeing how it goes:</span></span></span></p>
<pre>
X_all = np.hstack([X_lastwinner, X_teams])<br/>clf = RandomForestClassifier(random_state=14)<br/>scores = cross_val_score(clf, X_all, y_true, scoring='accuracy')<br/>print("Accuracy: {0:.1f}%".format(np.mean(scores) * 100))
</pre>
<p><span><span><span>This results in 63.3 percent—a drop in performance! One cause is the randomness inherent in random forests only chose some features to use rather than others. Further, there are many more features in  <kbd>X_teams</kbd> than in <kbd>X_lastwinner</kbd>, and having the extra features results in less relevant information being used. That said, don't get too excited by small changes in percentages, either up or down. Changing the random state value will have more of an impact on the accuracy than the slight difference between these feature sets that we just observed. Instead, you should run many tests with different random states, to get a good sense of the mean and spread of accuracy values.</span></span></span></p>
<p><span><span><span>We can also try some other parameters using the <kbd><span><span><span><span><span>GridSearchCV</span></span></span></span></span></kbd> class, as we introduced in <a href="b453da40-2d85-4978-aca0-3121de2f984e.xhtml" target="_blank"><span><span>Chapter 2</span></span></a>, <em>Classifying using </em></span></span></span><em>scikit-learn Estimators</em><span><span><span>:</span></span></span></p>
<pre>
from sklearn.grid_search import GridSearchCV<br/><br/>parameter_space = {<br/> "max_features": [2, 10, 'auto'],<br/> "n_estimators": [100, 200],<br/> "criterion": ["gini", "entropy"],<br/> "min_samples_leaf": [2, 4, 6],<br/>}<br/><br/>clf = RandomForestClassifier(random_state=14)<br/>grid = GridSearchCV(clf, parameter_space)<br/>grid.fit(X_all, y_true)<br/>print("Accuracy: {0:.1f}%".format(grid.best_score_ * 100))
</pre>
<p><span><span><span>This has a much better accuracy of 67.4 percent!</span></span></span></p>
<p><span><span><span>If we wanted to see the parameters used, we can print out the best model that was found in the grid search. The code is as follows:</span></span></span></p>
<pre>
<span><span><span>print(grid.best_estimator_)</span></span></span>
</pre>
<p><span><span><span>The result shows the parameters that were used in the best scoring model:</span></span></span></p>
<pre>
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',<br/>            max_depth=None, max_features=2, max_leaf_nodes=None,<br/>            min_samples_leaf=2, min_samples_split=2,<br/>            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,<br/>            oob_score=False, random_state=14, verbose=0, warm_start=False)
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Engineering new features</h1>
            </header>

            <article>
                
<p><span><span><span>In the previous few examples, we saw that changing the features can have quite a large impact on the performance of the algorithm. Through our small amount of testing, we had more than 10 percent variance just from the features.</span></span></span></p>
<p><span><span><span>You can create features that come from a simple function in</span></span></span> pandas <span><span><span>by doing something like this:</span></span></span></p>
<pre>
<span><span><span>dataset["New Feature"] = feature_creator()</span></span></span>
</pre>
<p><span><span><span>The <span><span><span><span><span>feature_creator</span></span></span></span></span> function must return a list of the feature's value for each sample in the dataset. A common pattern is to use the dataset as a parameter:</span></span></span></p>
<pre>
<span><span><span>dataset["New Feature"] = feature_creator(dataset)</span></span></span>
</pre>
<p><span><span><span>You can create those features more directly by setting all the values to a single default value, like 0 in the next line:</span></span></span></p>
<pre>
<span><span><span>dataset["My New Feature"] = 0</span></span></span>
</pre>
<p><span><span><span>You can then iterate over the dataset, computing the features as you go. We used<br/>
this format in this chapter to create many of our features:</span></span></span></p>
<pre>
<span><span><span>for index, row in dataset.iterrows():<br/></span></span></span><span><span><span>    home_team = row["Home Team"]<br/></span></span></span><span><span><span>    visitor_team = row["Visitor Team"]<br/></span></span></span><span><span><span>    # Some calculation here to alter row<br/>    dataset.set_value(index, "FeatureName", feature_value)</span></span></span>
</pre>
<p><span><span><span>Keep in mind that this pattern isn't very efficient. If you are going to do this, try all of your features at once.</span></span></span></p>
<div class="packt_infobox"><span><span><span>A common <em>best practice</em> is to touch every sample as little as possible, preferably only once.</span></span></span></div>
<p><span><span><span>Some example features that you could try and implement are as follows:</span></span></span></p>
<ul>
<li><span><span><span>How many days has it been since each team's previous match? Teams may be tired if they play too many games in a short time frame.</span></span></span></li>
<li><span><span><span>How many games of the last five did each team win? This will give a more stable form of the</span></span></span> <kbd>HomeLastWin</kbd> <span><span><span>and</span></span></span> <kbd>VisitorLastWin</kbd> <span><span><span>features we extracted earlier (and can be extracted in a very similar way).</span></span></span></li>
<li><span><span><span>Do teams have a good record when visiting certain other teams? For instance, one team may play well in a particular stadium, even if they are the visitors.</span></span></span></li>
</ul>
<p><span><span><span>If you are facing trouble extracting features of these types, check the</span></span></span> pandas<span><span><span>documentation at <span><span><span><span><span><a href="http://pandas.pydata.org/pandas-docs/stable/">http://pandas.pydata.org/pandas-docs/stable/</a></span></span></span></span></span> for help. Alternatively, you can try an online forum such as Stack Overflow for assistance.</span></span></span></p>
<p><span><span><span>More extreme examples could use player data to estimate the strength of each team's sides to predict who won. These types of complex features are used every day by gamblers and sports betting agencies to try to turn a profit by predicting the outcome of sports matches.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p><span><span><span>In this chapter, we extended our use of</span></span></span> scikit-learn's <span><span><span>classifiers to perform classification and introduced the <kbd>pandas</kbd>library to manage our data. We analyzed real-world data on basketball results from the NBA, saw some of the problems that even well-curated data introduces, and created new features for our analysis.</span></span></span></p>
<p><span><span><span>We saw the effect that good features have on performance and used an ensemble algorithm, random forests, to further improve the accuracy. To take these concepts further, try to create your own features and test them out. Which features perform better? If you have trouble coming up with features, think about what other datasets can be included. For example, if key players are injured, this might affect the results of a specific match and cause a better team to lose.</span></span></span></p>
<p><span><span><span>In the next chapter, we will extend the affinity analysis that we performed in the first chapter to create a program to find similar books. We will see how to use algorithms for ranking and also use an approximation to improve the scalability of data mining.</span></span></span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>