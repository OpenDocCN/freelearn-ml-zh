["```py\nfunc affine(weights [][]float64, inputs []float64) []float64 {\n  return activation(matVecMul(weights, inputs))\n}\n```", "```py\nimport (\n  G \"gorgonia.org/gorgonia\"\n)\n\nvar Float tensor.Float = tensor.Float64\nfunc main() {\n  g := G.NewGraph()\n  x := G.NewMatrix(g, Float, G.WithName(\"x\"), G.WithShape(N, 728))\n  w := G.NewMatrix(g, Float, G.WithName(\"w\"), G.WithShape(728, 800), \n       G.WithInit(G.Uniform(1.0)))\n  b := G.NewMatrix(g, Float, G.WithName(\"b\"), G.WithShape(N, 800), \n       G.WithInit(G.Zeroes()))\n  xw, _ := G.Mul(x, w)\n  xwb, _ := G.Add(xw, b)\n  act, _ := G.Sigmoid(xwb)\n\n  w2 := G.NewMatrix(g, Float, G.WithName(\"w2\"), G.WithShape(800, 10), \n        G.WithInit(G.Uniform(1.0)))\n  b2 := G.NewMatrix(g, Float, G.WithName(\"b2\"), G.WithShape(N, 10), \n        G.WithInit(G.Zeroes()))\n  xw2, _ := G.Mul(act, w2)\n  xwb2, _ := G.Add(xw2, b2)\n  sm, _ := G.SoftMax(xwb2)\n}\n```", "```py\nfunc (nn *NN) Predict(a tensor.Tensor) (int, error) {\n  if a.Dims() != 1 {\n    return nil, errors.New(\"Expected a vector\")\n  }\n\n  var m maybe\n  act0 := m.sigmoid(m.matVecMul(nn.hidden, a))\n  pred := m.sigmoid(m.matVecMul(nn.final, act0))\n  if m.err != nil {\n    return -1, m.err\n  }\n  return argmax(pred.Data().([]float64)), nil\n}\n```", "```py\n// X is the image, Y is a one hot vector\nfunc (nn *NN) Train(x, y tensor.Tensor, learnRate float64) (cost float64, err error) {\n  // predict\n  var m maybe\n  m.reshape(x, s.Shape()[0], 1)\n  m.reshape(y, 10, 1)\n  act0 := m.sigmoid(m.matmul(nn.hidden, x))\n  pred := m.sigmoid(m.matmul(nn.final, act0))\n\n  // backpropagation.\n  outputErrors := m.sub(y, pred))\n  cost = sum(outputErrors.Data().([]float64))\n\n  hidErrs := m.do(func() (tensor.Tensor, error) {\n    if err := nn.final.T(); err != nil {\n      return nil, err\n    }\n    defer nn.final.UT()\n    return tensor.MatMul(nn.final, outputErrors)\n  })\n  dpred := m.mul(m.dsigmoid(pred), outputErrors, tensor.UseUnsafe())\n  dpred_dfinal := m.dmatmul(outputErrors, act0)\n    if err := act0.T(); err != nil {\n      return nil, err\n    }\n    defer act0.UT()\n    return tensor.MatMul(outputErrors, act0)\n  })\n\n  m.reshape(m.mul(hidErrs, m.dsigmoid(act0), tensor.UseUnsafe()), \n                  hidErrs.Shape()[0], 1)\n  dcost_dhidden := m.do(func() (tensor.Tensor, error) {\n    if err := x.T(); err != nil {\n      return nil, err\n    }\n    defer x.UT()\n    return tensor.MatMul(hidErrs, x)\n  })\n\n  // gradient update\n  m.mul(dpred_dfinal, learnRate, tensor.UseUnsafe())\n  m.mul(dcost_dhidden, learnRate, tensor.UseUnsafe())\n  m.add(nn.final, dpred_dfinal, tensor.UseUnsafe())\n  m.add(nn.hidden, dcost_dhidden, tensor.UseUnsafe())\n  return cost, m.err\n}\n```", "```py\nfunc (nn *NN) fwd(x tensor.Tensor) (act0, pred tensor.Tensor, err error) {\n  var m maybe\n  m.reshape(x, s.Shape()[0], 1)\n  act0 := m.sigmoid(m.matmul(nn.hidden, x))\n  pred := m.sigmoid(m.matmul(nn.final, act0))\n  return act0, pred, m.err\n}\n```", "```py\nfunc (nn *NN) Predict(a tensor.Tensor) (int, error) {\n  if a.Dims() != 1 {\n    return nil, errors.New(\"Expected a vector\")\n  }\n\n  var err error\n  var pred tensor.Tensor\n  if _, pred, err = nn.fwd(a); err!= nil {\n    return -1, err\n  }\n  return argmax(pred.Data().([]float64)), nil\n}\n```", "```py\n// X is the image, Y is a one hot vector\nfunc (nn *NN) Train(x, y tensor.Tensor, learnRate float64) (cost float64, err error) {\n  // predict\n  var act0, pred tensor.Tensor\n  if act0, pred, err = nn.fwd(); err != nil {\n    return math.Inf(1), err\n  }\n\n  var m maybe\n  m.reshape(y, 10, 1)\n  // backpropagation.\n  outputErrors := m.sub(y, pred))\n  cost = sum(outputErrors.Data().([]float64))\n\n  hidErrs := m.do(func() (tensor.Tensor, error) {\n    if err := nn.final.T(); err != nil {\n      return nil, err\n    }\n    defer nn.final.UT()\n    return tensor.MatMul(nn.final, outputErrors)\n  })\n  dpred := m.mul(m.dsigmoid(pred), outputErrors, tensor.UseUnsafe())\n  dpred_dfinal := m.dmatmul(outputErrors, act0)\n    if err := act0.T(); err != nil {\n      return nil, err\n    }\n    defer act0.UT()\n    return tensor.MatMul(outputErrors, act0)\n  })\n\n  m.reshape(m.mul(hidErrs, m.dsigmoid(act0), tensor.UseUnsafe()), \n                  hidErrs.Shape()[0], 1)\n  dcost_dhidden := m.do(func() (tensor.Tensor, error) {\n    if err := x.T(); err != nil {\n      return nil, err\n    }\n    defer x.UT()\n    return tensor.MatMul(hidErrs, x)\n  })\n\n  // gradient update\n  m.mul(dpred_dfinal, learnRate, tensor.UseUnsafe())\n  m.mul(dcost_dhidden, learnRate, tensor.UseUnsafe())\n  m.add(nn.final, dpred_dfinal, tensor.UseUnsafe())\n  m.add(nn.hidden, dcost_dhidden, tensor.UseUnsafe())\n  return cost, m.err\n}\n```", "```py\nimport (\n  G \"gorgonia.org/gorgonia\"\n)\n\nvar Float tensor.Float = tensor.Float64\nfunc main() {\n  g := G.NewGraph()\n  x := G.NewMatrix(g, Float, G.WithName(\"x\"), G.WithShape(N, 728))\n  w := G.NewMatrix(g, Float, G.WithName(\"w\"), G.WithShape(728, 800), \n                   G.WithInit(G.Uniform(1.0)))\n  b := G.NewMatrix(g, Float, G.WithName(\"b\"), G.WithShape(N, 800), \n                   G.WithInit(G.Zeroes()))\n  xw, _ := G.Mul(x, w)\n  xwb, _ := G.Add(xw, b)\n  act, _ := G.Sigmoid(xwb)\n\n  w2 := G.NewMatrix(g, Float, G.WithName(\"w2\"), G.WithShape(800, 10), \n                    G.WithInit(G.Uniform(1.0)))\n  b2 := G.NewMatrix(g, Float, G.WithName(\"b2\"), G.WithShape(N, 10),  \n                    G.WithInit(G.Zeroes()))\n  xw2, _ := G.Mul(act, w2)\n  xwb2, _ := G.Add(xw2, b2)\n  sm, _ := G.SoftMax(xwb2)\n}\n```", "```py\n// Image holds the pixel intensities of an image.\n// 255 is foreground (black), 0 is background (white).\ntype RawImage []byte\n\n// Label is a digit label in 0 to 9\ntype Label uint8\n\nconst numLabels = 10\nconst pixelRange = 255\n\nconst (\n  imageMagic = 0x00000803\n  labelMagic = 0x00000801\n  Width = 28\n  Height = 28\n)\n\nfunc readLabelFile(r io.Reader, e error) (labels []Label, err error) {\n  if e != nil {\n    return nil, e\n  }\n\n  var magic, n int32\n  if err = binary.Read(r, binary.BigEndian, &magic); err != nil {\n    return nil, err\n  }\n  if magic != labelMagic {\n    return nil, os.ErrInvalid\n  }\n  if err = binary.Read(r, binary.BigEndian, &n); err != nil {\n    return nil, err\n  }\n  labels = make([]Label, n)\n  for i := 0; i < int(n); i++ {\n    var l Label\n    if err := binary.Read(r, binary.BigEndian, &l); err != nil {\n      return nil, err\n    }\n    labels[i] = l\n  }\n  return labels, nil\n}\n\nfunc readImageFile(r io.Reader, e error) (imgs []RawImage, err error) {\n  if e != nil {\n    return nil, e\n  }\n\n  var magic, n, nrow, ncol int32\n  if err = binary.Read(r, binary.BigEndian, &magic); err != nil {\n    return nil, err\n  }\n  if magic != imageMagic {\n    return nil, err /*os.ErrInvalid*/\n  }\n  if err = binary.Read(r, binary.BigEndian, &n); err != nil {\n    return nil, err\n  }\n  if err = binary.Read(r, binary.BigEndian, &nrow); err != nil {\n    return nil, err\n  }\n  if err = binary.Read(r, binary.BigEndian, &ncol); err != nil {\n    return nil, err\n  }\n  imgs = make([]RawImage, n)\n  m := int(nrow * ncol)\n  for i := 0; i < int(n); i++ {\n    imgs[i] = make(RawImage, m)\n    m_, err := io.ReadFull(r, imgs[i])\n    if err != nil {\n      return nil, err\n    }\n    if m_ != int(m) {\n      return nil, os.ErrInvalid\n    }\n  }\n return imgs, nil\n```", "```py\nfunc pixelWeight(px byte) float64 {\n    retVal := (float64(px) / 255 * 0.999) + 0.001\n    if retVal == 1.0 {\n        return 0.999\n    }\n    return retVal\n}\nfunc reversePixelWeight(px float64) byte {\n    return byte(((px - 0.001) / 0.999) * 255)\n}\nfunc prepareX(M []RawImage) (retVal tensor.Tensor) {\n    rows := len(M)\n    cols := len(M[0])\n\n    b := make([]float64, 0, rows*cols)\n    for i := 0; i < rows; i++ {\n        for j := 0; j < len(M[i]); j++ {\n            b = append(b, pixelWeight(M[i][j]))\n        }\n    }\n    return tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(b))\n}\nfunc prepareY(N []Label) (retVal tensor.Tensor) {\n    rows := len(N)\n    cols := 10\n\n    b := make([]float64, 0, rows*cols)\n    for i := 0; i < rows; i++ {\n        for j := 0; j < 10; j++ {\n            if j == int(N[i]) {\n                b = append(b, 0.999)\n            } else {\n                b = append(b, 0.001)\n            }\n        }\n    }\n    return tensor.New(tensor.WithShape(rows, cols), tensor.WithBacking(b))\n}\nfunc visualize(data tensor.Tensor, rows, cols int, filename string) (err error) {\n    N := rows * cols\n\n    sliced := data\n    if N > 1 {\n        sliced, err = data.Slice(makeRS(0, N), nil) // data[0:N, :] in python\n        if err != nil {\n            return err\n        }\n    }\n\n    if err = sliced.Reshape(rows, cols, 28, 28); err != nil {\n        return err\n    }\n\n    imCols := 28 * cols\n    imRows := 28 * rows\n    rect := image.Rect(0, 0, imCols, imRows)\n    canvas := image.NewGray(rect)\n\n    for i := 0; i < cols; i++ {\n        for j := 0; j < rows; j++ {\n            var patch tensor.Tensor\n            if patch, err = sliced.Slice(makeRS(i, i+1), makeRS(j,  \n                                         j+1)); err != nil {\n                return err\n            }\n\n            patchData := patch.Data().([]float64)\n            for k, px := range patchData {\n                x := j*28 + k%28\n                y := i*28 + k/28\n                c := color.Gray{reversePixelWeight(px)}\n                canvas.Set(x, y, c)\n            }\n        }\n    }\n\n    var f io.WriteCloser\n    if f, err = os.Create(filename); err != nil {\n        return err\n    }\n\n    if err = png.Encode(f, canvas); err != nil {\n        f.Close()\n        return err\n    }\n\n    if err = f.Close(); err != nil {\n        return err\n    }\n    return nil\n}\n```", "```py\nfunc main() {\n  kb := []float64{\n    1 / 16.0, 1 / 8.0, 1 / 16.0,\n    1 / 8.0, 1 / 4.0, 1 / 8.0,\n    1 / 16.0, 1 / 8.0, 1 / 16.0,\n  }\n  k := tensor.New(tensor.WithShape(3,3), tensor.WithBacking(kb))\n\n  for _, row := range imgIt {\n    for j, px := range row {\n      var acc float64\n\n      for _, krow := range kIt {\n        for _, kpx := range krow {\n          acc += px * kpx \n        }\n      }\n      row[j] = acc\n    }\n  }\n}\n```", "```py\ntype convnet struct {\n    g                  *gorgonia.ExprGraph\n    w0, w1, w2, w3, w4 *gorgonia.Node // weights. the number at the back indicates which layer it's used for\n    d0, d1, d2, d3     float64        // dropout probabilities\n\n    out    *gorgonia.Node\n    outVal gorgonia.Value\n}\n```", "```py\nfunc newConvNet(g *gorgonia.ExprGraph) *convnet {\n  w0 := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(32, 1, 3, 3), \n                 gorgonia.WithName(\"w0\"),    \n                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n  w1 := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(64, 32, 3, 3), \n                 gorgonia.WithName(\"w1\"),  \n                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n  w2 := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(128, 64, 3, 3), \n                 gorgonia.WithName(\"w2\"), \n                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n  w3 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(128*3*3, 625), \n                 gorgonia.WithName(\"w3\"), \n                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n  w4 := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(625, 10), \n                 gorgonia.WithName(\"w4\"), \n                 gorgonia.WithInit(gorgonia.GlorotN(1.0)))\n  return &convnet{\n    g: g,\n    w0: w0,\n    w1: w1,\n    w2: w2,\n    w3: w3,\n    w4: w4,\n\n    d0: 0.2,\n    d1: 0.2,\n    d2: 0.2,\n    d3: 0.55,\n  }\n}\n```", "```py\n// This function is particularly verbose for educational reasons. In reality, you'd wrap up the layers within a layer struct type and perform per-layer activations\nfunc (m *convnet) fwd(x *gorgonia.Node) (err error) {\n    var c0, c1, c2, fc *gorgonia.Node\n    var a0, a1, a2, a3 *gorgonia.Node\n    var p0, p1, p2 *gorgonia.Node\n    var l0, l1, l2, l3 *gorgonia.Node\n\n    // LAYER 0\n    // here we convolve with stride = (1, 1) and padding = (1, 1),\n    // which is your bog standard convolution for convnet\n    if c0, err = gorgonia.Conv2d(x, m.w0, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n        return errors.Wrap(err, \"Layer 0 Convolution failed\")\n    }\n    if a0, err = gorgonia.Rectify(c0); err != nil {\n        return errors.Wrap(err, \"Layer 0 activation failed\")\n    }\n    if p0, err = gorgonia.MaxPool2D(a0, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n        return errors.Wrap(err, \"Layer 0 Maxpooling failed\")\n    }\n    if l0, err = gorgonia.Dropout(p0, m.d0); err != nil {\n        return errors.Wrap(err, \"Unable to apply a dropout\")\n    }\n\n    // Layer 1\n    if c1, err = gorgonia.Conv2d(l0, m.w1, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n        return errors.Wrap(err, \"Layer 1 Convolution failed\")\n    }\n    if a1, err = gorgonia.Rectify(c1); err != nil {\n        return errors.Wrap(err, \"Layer 1 activation failed\")\n    }\n    if p1, err = gorgonia.MaxPool2D(a1, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n        return errors.Wrap(err, \"Layer 1 Maxpooling failed\")\n    }\n    if l1, err = gorgonia.Dropout(p1, m.d1); err != nil {\n        return errors.Wrap(err, \"Unable to apply a dropout to layer 1\")\n    }\n\n    // Layer 2\n    if c2, err = gorgonia.Conv2d(l1, m.w2, tensor.Shape{3, 3}, []int{1, 1}, []int{1, 1}, []int{1, 1}); err != nil {\n        return errors.Wrap(err, \"Layer 2 Convolution failed\")\n    }\n    if a2, err = gorgonia.Rectify(c2); err != nil {\n        return errors.Wrap(err, \"Layer 2 activation failed\")\n    }\n    if p2, err = gorgonia.MaxPool2D(a2, tensor.Shape{2, 2}, []int{0, 0}, []int{2, 2}); err != nil {\n        return errors.Wrap(err, \"Layer 2 Maxpooling failed\")\n    }\n    log.Printf(\"p2 shape %v\", p2.Shape())\n\n    var r2 *gorgonia.Node\n    b, c, h, w := p2.Shape()[0], p2.Shape()[1], p2.Shape()[2], p2.Shape()[3]\n    if r2, err = gorgonia.Reshape(p2, tensor.Shape{b, c * h * w}); err != nil {\n        return errors.Wrap(err, \"Unable to reshape layer 2\")\n    }\n    log.Printf(\"r2 shape %v\", r2.Shape())\n    if l2, err = gorgonia.Dropout(r2, m.d2); err != nil {\n        return errors.Wrap(err, \"Unable to apply a dropout on layer 2\")\n    }\n\n    // Layer 3\n    if fc, err = gorgonia.Mul(l2, m.w3); err != nil {\n        return errors.Wrapf(err, \"Unable to multiply l2 and w3\")\n    }\n    if a3, err = gorgonia.Rectify(fc); err != nil {\n        return errors.Wrapf(err, \"Unable to activate fc\")\n    }\n    if l3, err = gorgonia.Dropout(a3, m.d3); err != nil {\n        return errors.Wrapf(err, \"Unable to apply a dropout on layer 3\")\n    }\n\n    // output decode\n    var out *gorgonia.Node\n    if out, err = gorgonia.Mul(l3, m.w4); err != nil {\n        return errors.Wrapf(err, \"Unable to multiply l3 and w4\")\n    }\n    m.out, err = gorgonia.SoftMax(out)\n    gorgonia.Read(m.out, &m.outVal)\n    return\n}\n```", "```py\nfunc main() {\n    flag.Parse()\n    parseDtype()\n    imgs, err := readImageFile(os.Open(\"train-images-idx3-ubyte\"))\n    if err != nil {\n        log.Fatal(err)\n    }\n    labels, err := readLabelFile(os.Open(\"train-labels-idx1-ubyte\"))\n    if err != nil {\n        log.Fatal(err)\n    }\n\n    inputs := prepareX(imgs)\n    targets := prepareY(labels)\n\n    // the data is in (numExamples, 784).\n    // In order to use a convnet, we need to massage the data\n    // into this format (batchsize, numberOfChannels, height, width).\n    //\n    // This translates into (numExamples, 1, 28, 28).\n    //\n    // This is because the convolution operators actually understand height and width.\n    //\n    // The 1 indicates that there is only one channel (MNIST data is black and white).\n    numExamples := inputs.Shape()[0]\n    bs := *batchsize\n\n    if err := inputs.Reshape(numExamples, 1, 28, 28); err != nil {\n        log.Fatal(err)\n    }\n    g := gorgonia.NewGraph()\n    x := gorgonia.NewTensor(g, dt, 4, gorgonia.WithShape(bs, 1, 28, 28), gorgonia.WithName(\"x\"))\n    y := gorgonia.NewMatrix(g, dt, gorgonia.WithShape(bs, 10), gorgonia.WithName(\"y\"))\n    m := newConvNet(g)\n    if err = m.fwd(x); err != nil {\n        log.Fatalf(\"%+v\", err)\n    }\n    losses := gorgonia.Must(gorgonia.HadamardProd(m.out, y))\n    cost := gorgonia.Must(gorgonia.Mean(losses))\n    cost = gorgonia.Must(gorgonia.Neg(cost))\n\n    // we wanna track costs\n    var costVal gorgonia.Value\n    gorgonia.Read(cost, &costVal)\n\n    if _, err = gorgonia.Grad(cost, m.learnables()...); err != nil {\n        log.Fatal(err)\n    }\n```", "```py\n    losses := gorgonia.Must(gorgonia.HadamardProd(m.out, y))\n    cost := gorgonia.Must(gorgonia.Mean(losses))\n    cost = gorgonia.Must(gorgonia.Neg(cost))\n```", "```py\nfunc (m *convnet) learnables() gorgonia.Nodes {\n    return gorgonia.Nodes{m.w0, m.w1, m.w2, m.w3, m.w4}\n}\n```", "```py\n    vm := gorgonia.NewTapeMachine(g, \n        gorgonia.WithPrecompiled(prog, locMap), \n        gorgonia.BindDualValues(m.learnables()...))\n    solver := gorgonia.NewRMSPropSolver(gorgonia.WithBatchSize(float64(bs)))\n    defer vm.Close()\n```", "```py\n\n Instructions:\n 0 loadArg 0 (x) to CPU0\n 1 loadArg 1 (y) to CPU1\n 2 loadArg 2 (w0) to CPU2\n 3 loadArg 3 (w1) to CPU3\n 4 loadArg 4 (w2) to CPU4\n 5 loadArg 5 (w3) to CPU5\n 6 loadArg 6 (w4) to CPU6\n 7 im2col<(3,3), (1, 1), (1,1) (1, 1)> [CPU0] CPU7 false false false\n 8 Reshape(32, 9) [CPU2] CPU8 false false false\n 9 Reshape(78400, 9) [CPU7] CPU7 false true false\n 10 Alloc Matrix float64(78400, 32) CPU9\n 11 A × Bᵀ [CPU7 CPU8] CPU9 true false true\n 12 DoWork\n 13 Reshape(100, 28, 28, 32) [CPU9] CPU9 false true false\n 14 Aᵀ{0, 3, 1, 2} [CPU9] CPU9 false true false\n 15 const 0 [] CPU10 false false false\n 16 >= true [CPU9 CPU10] CPU11 false false false\n 17 ⊙ false [CPU9 CPU11] CPU9 false true false\n 18 MaxPool{100, 32, 28, 28}(kernel: (2, 2), pad: (0, 0), stride: (2, \n                             2)) [CPU9] CPU12 false false false\n 19 0(0, 1) - (100, 32, 14, 14) [] CPU13 false false false\n 20 const 0.2 [] CPU14 false false false\n 21 > true [CPU13 CPU14] CPU15 false false false\n 22 ⊙ false [CPU12 CPU15] CPU12 false true false\n 23 const 5 [] CPU16 false false false\n 24 ÷ false [CPU12 CPU16] CPU12 false true false\n 25 im2col<(3,3), (1, 1), (1,1) (1, 1)> [CPU12] CPU17 false false false\n 26 Reshape(64, 288) [CPU3] CPU18 false false false\n 27 Reshape(19600, 288) [CPU17] CPU17 false true false\n 28 Alloc Matrix float64(19600, 64) CPU19\n 29 A × Bᵀ [CPU17 CPU18] CPU19 true false true\n 30 DoWork\n 31 Reshape(100, 14, 14, 64) [CPU19] CPU19 false true false\n 32 Aᵀ{0, 3, 1, 2} [CPU19] CPU19 false true false\n 33 >= true [CPU19 CPU10] CPU20 false false false\n 34 ⊙ false [CPU19 CPU20] CPU19 false true false\n 35 MaxPool{100, 64, 14, 14}(kernel: (2, 2), pad: (0, 0), stride: (2, \n                             2)) [CPU19] CPU21 false false false\n 36 0(0, 1) - (100, 64, 7, 7) [] CPU22 false false false\n 37 > true [CPU22 CPU14] CPU23 false false false\n 38 ⊙ false [CPU21 CPU23] CPU21 false true false\n 39 ÷ false [CPU21 CPU16] CPU21 false true false\n 40 im2col<(3,3), (1, 1), (1,1) (1, 1)> [CPU21] CPU24 false false false\n 41 Reshape(128, 576) [CPU4] CPU25 false false false\n 42 Reshape(4900, 576) [CPU24] CPU24 false true false\n 43 Alloc Matrix float64(4900, 128) CPU26\n 44 A × Bᵀ [CPU24 CPU25] CPU26 true false true\n 45 DoWork\n 46 Reshape(100, 7, 7, 128) [CPU26] CPU26 false true false\n 47 Aᵀ{0, 3, 1, 2} [CPU26] CPU26 false true false\n 48 >= true [CPU26 CPU10] CPU27 false false false\n 49 ⊙ false [CPU26 CPU27] CPU26 false true false\n 50 MaxPool{100, 128, 7, 7}(kernel: (2, 2), pad: (0, 0), stride: (2, \n                            2)) [CPU26] CPU28 false false false\n 51 Reshape(100, 1152) [CPU28] CPU28 false true false\n 52 0(0, 1) - (100, 1152) [] CPU29 false false false\n 53 > true [CPU29 CPU14] CPU30 false false false\n 54 ⊙ false [CPU28 CPU30] CPU28 false true false\n 55 ÷ false [CPU28 CPU16] CPU28 false true false\n 56 Alloc Matrix float64(100, 625) CPU31\n 57 A × B [CPU28 CPU5] CPU31 true false true\n 58 DoWork\n 59 >= true [CPU31 CPU10] CPU32 false false false\n 60 ⊙ false [CPU31 CPU32] CPU31 false true false\n 61 0(0, 1) - (100, 625) [] CPU33 false false false\n 62 const 0.55 [] CPU34 false false false\n 63 > true [CPU33 CPU34] CPU35 false false false\n 64 ⊙ false [CPU31 CPU35] CPU31 false true false\n 65 const 1.8181818181818181 [] CPU36 false false false\n 66 ÷ false [CPU31 CPU36] CPU31 false true false\n 67 Alloc Matrix float64(100, 10) CPU37\n 68 A × B [CPU31 CPU6] CPU37 true false true\n 69 DoWork\n 70 exp [CPU37] CPU37 false true false\n 71 Σ[1] [CPU37] CPU38 false false false\n 72 SizeOf=10 [CPU37] CPU39 false false false\n 73 Repeat[1] [CPU38 CPU39] CPU40 false false false\n 74 ÷ false [CPU37 CPU40] CPU37 false true false\n 75 ⊙ false [CPU37 CPU1] CPU37 false true false\n 76 Σ[0 1] [CPU37] CPU41 false false false\n 77 SizeOf=100 [CPU37] CPU42 false false false\n 78 SizeOf=10 [CPU37] CPU43 false false false\n 79 ⊙ false [CPU42 CPU43] CPU44 false false false\n 80 ÷ false [CPU41 CPU44] CPU45 false false false\n 81 neg [CPU45] CPU46 false false false\n 82 DoWork\n 83 Read CPU46 into 0xc43ca407d0\n 84 Free CPU0\n Args: 11 | CPU Memories: 47 | GPU Memories: 0\n CPU Mem: 133594448 | GPU Mem []\n ```", "```py\n\nPrinting the program allows you to actually have a feel for the complexity of the neural network. At 84 instructions, the convnet is among the simpler programs I've seen. However, there are quite a few expensive operations, which would inform us quite a bit about how long each run would take. This output also tells us roughly how many bytes of memory will be used: 133594448 bytes, or 133 megabytes.\n\nNow it's time to talk about, gradient descent. Gorgonia comes with a number of gradient descent solvers. For this project, we'll be using the RMSProp algorithm. So, we create a solver by calling `solver := gorgonia.NewRMSPropSolver(gorgonia.WithBatchSize(float64(bs)))`. Because we are planning to perform our operations in batches, we should correct the solver by providing it the batch size, lest the solver overshoots its target.\n\n![](img/4c1d9844-845b-4a62-aed4-e0074809e3cd.png)\n\nTo run the neural network, we simply run it for a number of epochs (which is passed in as an argument to the program):\n\n```", "```py\n\nBecause I was feeling a bit fancy, I decided to add a progress bar to track the progress. To do so, I'm using `cheggaaa/pb.v1` as the library to draw a progress bar. To install it, simply run `go get gopkg.in/cheggaaa/pb.v1` and to use it, simply add `import \"gopkg.in/cheggaaa/pb.v1` in the imports.\n\nThe rest is fairly straightforward. From the training dataset, we slice out a small portion of it (specifically, we slice out `bs` rows). Because our program takes a rank-4 tensor as an input, the data has to be reshaped to `xVal.(*tensor.Dense).Reshape(bs, 1, 28, 28)`.\n\nFinally, we feed the value into the function by using `gorgonia.Let`. Where `gorgonia.Read` reads a value out from the execution environment, `gorgonia.Let` puts a value into the execution environment. After which, `vm.RunAll()` executes the program, evaluating the mathematical function. As a programmed and intentional side-effect, each call to `vm.RunAll()` will populate the cost value into `costVal`.\n\nOnce the equation has been evaluated, this also means that the variables of the equation are now ready to be updated. As such, we use `solver.Step(gorgonia.NodesToValueGrads(m.learnables()))` to perform the actual gradient updates. After this, `vm.Reset()` is called to reset the VM state, ready for its next iteration.\n\nGorgonia in general, is pretty efficient. In the current version as this book was written, it managed to use all eight cores in my CPU as shown here:\n\n![](img/7b1b4faf-4ecf-4a20-8fe5-6aef1eb035d4.png)\n\n# Testing\n\nOf course we'd have to test our neural network.\n\nFirst we load up the testing data:\n\n```", "```py\n\nIn the last line, we visualize the test data to ensure that we do indeed have the correct dataset:\n\n![](img/1f8e92fd-bd9f-4382-a9d2-1a9fc82433cc.png)\n\nThen we have the main testing loop. Do observe that it's extremely similar to the training loop - because it's the same neural network!\n\n```", "```py\n\nOne difference is in the following snippet:\n\n```"]