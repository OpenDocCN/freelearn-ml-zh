- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dealing with Edge Cases and Rare Events in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the field of **machine learning**, it is crucial to identify and handle edge
    cases properly. Edge cases refer to instances in your dataset that are significantly
    different from the majority of the data, and they can have a substantial impact
    on the performance and reliability of your machine learning models. Rare events
    can be challenging for machine learning models due to class imbalance problems
    as they might not have enough data to learn patterns effectively. Class imbalance
    occurs when one class (the rare event) is significantly underrepresented compared
    to the other class(es). Traditional machine learning algorithms tend to perform
    poorly in such scenarios because they may be biased toward the majority class,
    leading to lower accuracy in identifying rare events.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore various techniques and approaches to detect
    edge cases in machine learning and data, using Python code examples. We’ll explore
    statistical techniques, including using visualizations and other measures such
    as Z-scores, to analyze data distributions and identify potential outliers. We
    will also focus on methodologies such as isolation forests and semi-supervised
    methods such as **autoencoders** to uncover anomalies and irregular patterns in
    your datasets. We will learn how to address class imbalance and enhance model
    performance through techniques such as oversampling, undersampling, and generating
    synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: In the latter half of this chapter, we’ll understand the importance of adjusting
    the learning process to account for imbalanced classes, especially in scenarios
    where the rare event carries significant consequences. We’ll explore the significance
    of selecting appropriate evaluation metrics, emphasizing those that account for
    class imbalance, to ensure a fair and accurate assessment of model performance.
    Finally, we’ll understand how ensemble methods such as bagging, boosting, and
    stacking help us enhance the robustness of models, particularly in scenarios where
    rare events play a crucial role.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following key topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Importance of detecting rare events and edge cases in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation and resampling techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing evaluation metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importance of detecting rare events and edge cases in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Detecting rare events and edge cases is crucial in machine learning for several
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision-making in critical scenarios**: Rare events often represent critical
    scenarios or anomalies that require immediate attention or special treatment.
    For instance, in medical diagnosis, rare diseases or extreme cases might need
    urgent intervention. Accurate detection of these events can lead to better decision-making
    and prevent adverse consequences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unbalanced datasets**: Many real-world datasets suffer from class imbalance,
    where one class (often the rare event) is significantly underrepresented compared
    to the other classes. This can lead to biased models that perform poorly on the
    minority class. Detecting rare events helps identify the need for special handling,
    such as using resampling techniques or employing appropriate evaluation metrics
    to ensure fair evaluation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fraud detection**: In fraud detection applications, rare events often correspond
    to fraudulent transactions or activities. Detecting these rare cases is crucial
    for preventing financial losses and ensuring the security of financial systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quality control and anomaly detection**: In manufacturing and industrial
    processes, detecting rare events can help identify faulty or anomalous products
    or processes. This enables timely intervention to improve product quality and
    maintain operational efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predictive maintenance**: In predictive maintenance, detecting edge cases
    can indicate potential equipment failures or abnormal behaviors, allowing proactive
    maintenance to reduce downtime and increase productivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model generalization**: By accurately identifying and handling rare events,
    machine learning models can better generalize to unseen data and handle real-world
    scenarios effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer behavior analysis**: In marketing and customer analysis, detecting
    rare events can reveal unusual patterns or behaviors of interest, such as identifying
    high-value customers or detecting potential churners.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and intrusion detection**: In cybersecurity, rare events may indicate
    security breaches or cyber-attacks. Detecting and responding to these events in
    real time is essential for ensuring the safety and integrity of digital systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environmental monitoring**: In environmental applications, rare events might
    signify unusual ecological conditions or natural disasters. Detecting such events
    aids in disaster preparedness and environmental monitoring efforts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss different statistical methods to analyze data distributions and
    identify potential outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistical methods provide valuable tools for identifying outliers and anomalies
    in our data, aiding in data preprocessing and decision-making. In this section,
    we’ll talk about how to use methods such as Z-scores, **Interquartile Range**
    (**IQR**), box plots, and scatter plots to uncover anomalies in our data.
  prefs: []
  type: TYPE_NORMAL
- en: Z-scores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Z-scores, also known as standard scores, are a statistical measure that indicates
    how many standard deviations a data point is away from the mean of the data. Z-scores
    are used to standardize data and allow for comparisons between different datasets,
    even if they have different units or scales. They are particularly useful in detecting
    outliers and identifying extreme values in a dataset. The formula to calculate
    the Z-score for a data point *x* in a dataset with mean *μ* and standard deviation
    *σ* is presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: Z = (x − μ) / σ
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Z* is the Z-score of the data point *x*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is the value of the data point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*μ* is the mean of the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*σ* is the standard deviation of the dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Z-scores are widely used to detect outliers in a dataset. Data points with Z-scores
    that fall outside a certain threshold (e.g., *Z* > 3 or *Z* < -3) are considered
    outliers. These outliers can represent extreme values or measurement errors in
    the data. By transforming the data into Z-scores, the mean becomes 0, and the
    standard deviation becomes 1, resulting in a standardized distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Z-scores are employed in normality testing to assess whether a dataset follows
    a normal (Gaussian) distribution. If the dataset follows a normal distribution,
    approximately 68% of the data points should have Z-scores between -1 and 1, about
    95% between -2 and 2, and nearly all between -3 and 3\. In hypothesis testing,
    Z-scores are used to compute *p*-values and make inferences about population parameters.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Z-tests are commonly used for sample mean comparisons when the
    population standard deviation is known. Z-scores can be useful in anomaly detection
    where we want to identify data points that deviate significantly from the norm.
    High Z-scores may indicate anomalous behavior or rare events in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore this concept in Python using the *Loan Prediction* dataset. Let’s
    start by loading the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we see the sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – The df DataFrame](img/B19297_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – The df DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have loaded the dataset, we can calculate Z-scores on some of the
    numerical features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We leverage Z-scores to detect outliers in our dataset effectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By setting a Z-score threshold of 3, we can identify outliers by examining if
    any rows have values that lie beyond the Z-score boundaries of greater than 3
    or less than -3\. This approach allows us to pinpoint data points that deviate
    significantly from the mean and enables us to take appropriate actions to handle
    these outliers, ensuring the integrity of our data and the accuracy of subsequent
    analyses and models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output DataFrame of outliers using this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – The resulting outlier_rows DataFrame](img/B19297_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – The resulting outlier_rows DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Z-scores make it easier to spot outliers accurately. They act
    as a helpful tool, guiding us to understand data patterns better. In the next
    section on IQR, we will further explore alternative methods for detecting and
    addressing outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Interquartile Range (IQR)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IQR is a statistical measure used to describe the spread or dispersion of a
    dataset. It is particularly useful in identifying and handling outliers and understanding
    the central tendency of the data. The IQR is defined as the range between the
    first quartile (Q1) and the third quartile (Q3) of a dataset. Quartiles are points
    that divide a dataset into four equal parts, each containing 25% of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IQR can be calculated using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: IQR = Q3 − Q1
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: Q1 is the first quartile (25th percentile), representing the value below which
    25% of the data lies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q3 is the third quartile (75th percentile), representing the value below which
    75% of the data lies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IQR is commonly used to identify outliers in a dataset. Data points that fall
    below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are considered outliers and may warrant
    further investigation. IQR provides valuable information about the distribution
    of the data. It helps to understand the spread of the middle 50% of the dataset
    and can be used to assess the symmetry of the distribution. When comparing datasets,
    IQR can be used to assess differences in the spread of data between two or more
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a simple example of how to calculate the IQR for a dataset using Python
    and NumPy on the Loan Prediction dataset. To begin with outlier detection, we
    calculate the IQR for numerical features. By defining a threshold, typically 1.5
    times the IQR based on Tukey’s method, which involves calculating the IQR as the
    difference between the third quartile (Q3) and the first quartile (Q1), we can
    identify outliers. Tukey’s method is a robust statistical technique that aids
    in detecting data points that deviate significantly from the overall distribution,
    providing a reliable measure for identifying potential anomalies in the dataset.
    Once outliers are flagged for all numerical features, we can display the rows
    with outlier values, which aids in further analysis and potential data treatment.
    Using the IQR and Tukey’s method together facilitates effective outlier detection
    and helps ensure the integrity of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output DataFrame of outliers using this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – The resulting outlier_rows DataFrame](img/B19297_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – The resulting outlier_rows DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, the exploration of IQR provides a valuable technique for identifying
    outliers, particularly effective in capturing the central tendencies of a dataset.
    While IQR proves advantageous in scenarios where a focus on the middle range is
    paramount, it’s essential to recognize its limitations in capturing the entire
    data distribution. In the upcoming section on box plots, we will delve into a
    graphical representation that complements IQR, offering a visual tool to better
    understand data dispersion and outliers in diverse contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Box plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Box plots, also known as box-and-whisker plots, are a graphical representation
    of the distribution of a dataset. They provide a quick and informative way to
    visualize the spread and skewness of the data, identify potential outliers, and
    compare multiple datasets. Box plots are particularly useful when dealing with
    continuous numerical data and can be used to gain insights into the central tendency
    and variability of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the components of a box plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Box (IQR)**: The box represents the IQR, which is the range between the first
    quartile (Q1) and the third quartile (Q3) of the data. It spans the middle 50%
    of the dataset and provides a visual representation of the data’s spread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Median (Q2)**: The median, represented by a horizontal line inside the box,
    indicates the central value of the dataset. It divides the data into two equal
    halves, with 50% of the data points below and 50% above the median.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Whiskers**: Whiskers extend from the edges of the box to the furthest data
    points that lie within the “whisker length.” The length of the whiskers is typically
    determined by a factor (for example, 1.5 times the IQR) and is used to identify
    potential outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outliers**: Data points lying beyond whiskers are considered outliers and
    are usually plotted individually as individual points or circles. They are data
    points that deviate significantly from the central distribution and may warrant
    further investigation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the benefits of box plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visualizing data distribution**: Box plots offer an intuitive way to see
    the spread and skewness of the data, as well as identify any potential data clusters
    or gaps'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Comparing datasets**: Box plots are useful for comparing multiple datasets
    side by side, allowing for easy comparisons of central tendencies and variabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier detection**: Box plots facilitate outlier detection by highlighting
    data points that lie beyond whiskers, helping identify unusual or extreme values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling skewed data**: Box plots are robust to the influence of extreme
    values and can handle skewed data distributions more effectively than traditional
    mean and standard deviation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement this approach using the `matplotlib` library in Python. Here,
    we will produce two box plots for `ApplicantIncome` and `LoanAmount`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the plot outputs. We see that there are some possible outliers in
    these columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Box plots for ApplicantIncome (top) and LoanAmount (bottom)](img/B19297_09_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Box plots for ApplicantIncome (top) and LoanAmount (bottom)
  prefs: []
  type: TYPE_NORMAL
- en: In summarizing the use of box plots, we find them to be a powerful visual aid,
    complementing IQR in portraying both central tendencies and data dispersion. While
    box plots excel in providing a holistic view, it’s crucial to acknowledge that
    they may not capture all nuances of complex datasets. In the next section on scatter
    plots, we will explore a versatile graphical tool that offers a broader perspective,
    facilitating the identification of relationships and patterns between variables
    in our data.
  prefs: []
  type: TYPE_NORMAL
- en: Scatter plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scatter plots are a popular and versatile data visualization technique used
    to explore the relationship between two continuous numerical variables. They provide
    a clear visual representation of how one variable (the independent variable) affects
    or influences another (the dependent variable). Scatter plots are especially useful
    in identifying patterns, correlations, clusters, and outliers in data, making
    them an essential tool in data analysis and **exploratory data** **analysis**
    (**EDA**).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at how scatter plots are constructed and their key characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Scatter plot construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To create a scatter plot, the values of two numerical variables are plotted
    as points on a Cartesian coordinate system. Each point represents a data observation,
    where the *x* coordinate corresponds to the value of the independent variable
    and the *y* coordinate corresponds to the value of the dependent variable. Multiple
    data points collectively form a scatter plot that provides insights into the relationship
    between the two variables.
  prefs: []
  type: TYPE_NORMAL
- en: Key characteristics of scatter plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some key characteristics of scatter plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Correlation**: Scatter plots help us assess the correlation or relationship
    between two variables. If points on the plot appear to form a clear trend or pattern
    (e.g., a linear or non-linear trend), it suggests a significant correlation between
    the variables. If points are scattered randomly, there might be no or weak correlation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster analysis**: Scatter plots can reveal clusters of data points, indicating
    potential subgroups or patterns within the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outlier detection**: Scatter plots facilitate outlier detection by identifying
    data points that lie far away from the main cluster of points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data spread**: The spread or distribution of data points along the *x* and
    *y* axes provides insights into the variability of the variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualizing regression lines**: In some cases, a regression line can be fitted
    to the scatter plot to model the relationship between the variables and make predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement this in Python on the `ApplicantIncome` and `LoanAmount` columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output showcasing the scatter plot results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Scatter plot showcasing ApplicantIncome and LoanAmount](img/B19297_09_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Scatter plot showcasing ApplicantIncome and LoanAmount
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, some points are notably distant from the majority of the population,
    suggesting the presence of potential outliers. These outlying data points stand
    apart from the overall pattern, warranting further investigation to understand
    their significance and impact on the relationship between the two variables. Identifying
    and handling outliers is crucial for ensuring accurate data analysis and model
    performance. By visualizing the scatter plot, we can gain valuable insights into
    data distribution and correlations, paving the way for effective decision-making
    and data exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Anomaly detection is a specific approach to detecting rare events, where the
    focus is on identifying instances that significantly deviate from the norm or
    normal behavior. Anomalies can be caused by rare events, errors, or unusual patterns
    that are not typical in the dataset. This technique is particularly useful when
    there is limited or no labeled data for rare events. Common anomaly detection
    algorithms include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised methods**: Techniques such as Isolation Forest and **One-Class
    SVM**) can be used to identify anomalies in data without requiring labeled examples
    of the rare event.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-supervised methods**: These approaches combine normal and abnormal data
    during training but have only a limited number of labeled anomalies. Autoencoders
    and variational autoencoders are examples of semi-supervised anomaly detection
    algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervised methods**: If a small number of labeled anomalies are available,
    **supervised learning** algorithms such as Random Forest, **Support Vector Machines**
    (**SVM**), and neural networks can be used for anomaly detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s understand these methods in detail with Python code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised method using Isolation Forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Isolation Forest is an efficient and effective algorithm used for anomaly detection
    in **unsupervised learning** scenarios. It works by isolating anomalies or rare
    events in the data by constructing isolation trees (random decision trees) that
    separate the anomalies from the majority of the normal data points. It was introduced
    by Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou in their 2008 paper titled *Isolation
    Forest*. Here are some key concepts and features of the Isolation Forest algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Random partitioning**: Isolation Forest uses a random partitioning strategy
    to create isolation trees. At each step of constructing a tree, a random feature
    is selected, and a random split value within the range of the selected feature’s
    values is chosen to create a node. This random partitioning leads to shorter paths
    for anomalies, making them easier to isolate from normal data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Path length**: The key idea behind Isolation Forest is that anomalies are
    isolated into smaller partitions with fewer data points, while normal data points
    are distributed more uniformly across larger partitions. The average path length
    of a data point to reach an anomaly in a tree is used as a measure of its “isolation.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly score**: Based on the average path length, each data point is assigned
    an anomaly score. The anomaly score represents how easily the data point can be
    isolated or separated from the rest of the data. Shorter average path lengths
    correspond to higher anomaly scores, indicating that the data point is more likely
    to be an anomaly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`auto`,” which estimates the contamination based on the dataset’s size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few advantages of Isolation Forest are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Isolation Forest is computationally efficient and scalable, making it suitable
    for large datasets. It does not require a large number of trees to achieve good
    performance, reducing the computational overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is relatively insensitive to the number of dimensions/features,
    which is particularly advantageous when dealing with high-dimensional datasets.
    Isolation Forest is an unsupervised learning algorithm, making it suitable for
    scenarios where labeled anomaly data is scarce or unavailable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are, however, some limitations of Isolation Forest:'
  prefs: []
  type: TYPE_NORMAL
- en: Isolation Forest may not perform well on datasets with multiple clusters of
    anomalies or when anomalies are close to the majority of normal data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with most unsupervised algorithms, Isolation Forest may produce false positives
    (normal data points misclassified as anomalies) and false negatives (anomalies
    misclassified as normal data points).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement this approach in Python with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` library is imported as `pd` to handle data in tabular format, and
    `IsolationForest` is imported from the `sklearn.ensemble` module for performing
    anomaly detection using the Isolation Forest algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`numerical_features` list is defined, containing the names of the numerical
    columns to be used for anomaly detection. These columns are `''ApplicantIncome''`,
    `''CoapplicantIncome''`, and `''LoanAmount''`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_anomaly` DataFrame is created by extracting the columns specified in `numerical_features`
    from the original `df` DataFrame. This new DataFrame will be used for anomaly
    detection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_anomaly` DataFrame, the `fillna()` method is used with the mean of each
    column. This ensures that any missing values are replaced with the mean value
    of their respective columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`IsolationForest(contamination=''auto'', random_state=42)`. The `contamination`
    parameter is set to `''auto''`, which means it will automatically detect the percentage
    of outliers in the dataset. The `random_state` parameter is set to `42` to ensure
    reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_anomaly` data using the `fit_predict()` method. This method simultaneously
    fits the model to the data and predicts whether each data point is an outlier
    or not. The predictions are stored in the `anomaly_predictions` array.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`anomaly_predictions` array contains predicted labels for each data point:
    `-1` for anomalies (outliers) and `1` for inliers (non-outliers). These predictions
    are added as a new `''IsAnomaly''` column to the original `df` DataFrame.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df` DataFrame where `IsAnomaly` is equal to -1, indicating the presence of
    outliers. The resulting DataFrame contains all rows with anomalies, which can
    then be further analyzed or processed as needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now view the DataFrame with rows that the model has predicted as anomalies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – The resulting anomalies DataFrame](img/B19297_09_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – The resulting anomalies DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Isolation Forest stands out as a powerful and efficient tool
    for anomaly detection, particularly in scenarios where anomalies are rare and
    distinctly different from normal instances. Its ability to isolate anomalies through
    the creation of random trees makes it a valuable asset in various applications,
    from fraud detection to network security. However, it’s essential to acknowledge
    the algorithm’s limits. Isolation Forest might face challenges when anomalies
    are not well separated or when datasets are highly dimensional. In the next section,
    we will explore autoencoders – a semi-supervised method for anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised methods using autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anomaly detection using autoencoders is an unsupervised learning approach that
    leverages neural networks (NNs) to detect anomalies in data. Autoencoders are
    a type of NN architecture designed to reconstruct the input data from a compressed
    representation. In anomaly detection, we exploit the fact that autoencoders struggle
    to reconstruct anomalous instances, making them useful for identifying unusual
    patterns or outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Autoencoders consist of two main components: an encoder and a decoder. The
    encoder compresses the input data into a lower-dimensional representation called
    the “latent space,” while the decoder tries to reconstruct the original input
    from this representation. The encoder and decoder are typically symmetric, and
    the network is trained to minimize reconstruction errors.'
  prefs: []
  type: TYPE_NORMAL
- en: In anomaly detection, we train the autoencoder on normal data without anomalies.
    Since the autoencoder learns to reconstruct normal data, it will be less capable
    of reconstructing anomalies, leading to higher reconstruction errors for anomalous
    instances. This property allows us to use the reconstruction error as an anomaly
    score.
  prefs: []
  type: TYPE_NORMAL
- en: During training, we compare the original input (for example, numerical features)
    to the reconstructed output. The difference between the two is the reconstruction
    error. A low reconstruction error indicates that the input is close to the normal
    data distribution, while a high reconstruction error suggests that the input is
    likely an anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: After training the autoencoder, we need to set a threshold to distinguish between
    normal and anomalous instances based on the reconstruction error. There are several
    methods to set the threshold, such as percentile-based or using validation data.
    The threshold will depend on the desired trade-off between false positives and
    false negatives, which can be adjusted based on the application’s requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are flexible and can capture complex patterns in the data, making
    them suitable for high-dimensional data with non-linear relationships. They can
    handle both global and local anomalies, meaning they can detect anomalies that
    differ from the majority of data points and anomalies within specific regions
    of the data. Autoencoders are capable of unsupervised learning, which is advantageous
    when labeled anomaly data is limited or unavailable. As with other unsupervised
    methods, autoencoders may produce false positives (normal data misclassified as
    anomalies) and false negatives (anomalies misclassified as normal data). They
    may struggle to detect anomalies that are very similar to the normal data, as
    the reconstruction error might not be significantly different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example implementation of this approach using the TensorFlow library
    and the Loan Prediction dataset in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_loan_prediction.csv` file using `pd.read_csv()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`numerical_features` list containing the names of the numerical columns to
    be used for anomaly detection. These columns are `''ApplicantIncome''`, `''CoapplicantIncome''`,
    and `''LoanAmount''`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_anomaly` DataFrame is created by extracting the columns specified in `numerical_features`
    from the original `df` DataFrame. This new DataFrame will be used for anomaly
    detection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X_anomaly` DataFrame are replaced with the mean value of their respective
    columns using the `fillna()` method:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_anomaly` are standardized using `StandardScaler()`. Standardization scales
    the features to have zero mean and unit variance, which is important for training
    machine learning models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_train`) and testing (`X_test`) sets using the `train_test_split()` function.
    The original indices of the data are also stored in `original_indices`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Dense` layers. The model is trained using the `fit()` method, with the **mean
    squared error** (**MSE**) as the loss function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_test`, and reconstruction errors are calculated as the mean squared difference
    between the original and reconstructed data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_test`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`1` in `anomaly_predictions`; otherwise, it is assigned `0`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`anomaly_df` DataFrame is created with the anomaly predictions and the corresponding
    index from `X_test_df`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`df` DataFrame using the `merge()` method, adding the `''IsAnomaly''` column
    to `df`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''IsAnomaly''` column is present in `df`. If present, it displays rows where
    `''IsAnomaly''` is equal to `1`, indicating the presence of anomalies. If not
    present, it prints `"No` `anomalies detected."`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting DataFrame is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.7 – The resulting IsAnomaly DataFrame](img/B19297_09_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – The resulting IsAnomaly DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: In summary, autoencoders prove to be a versatile and powerful tool for anomaly
    detection, capturing nuanced patterns that may elude traditional methods. Their
    ability to discover subtle anomalies within complex data structures makes them
    invaluable in diverse domains, including image analysis, cybersecurity, and industrial
    quality control.
  prefs: []
  type: TYPE_NORMAL
- en: However, the effectiveness of autoencoders is contingent on various factors.
    The architecture’s complexity and the selection of hyperparameters can influence
    performance, requiring careful tuning for optimal results. In the next section,
    we will understand how SVMs can be used in anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised methods using SVMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SVMs are a powerful class of supervised learning algorithms commonly used for
    classification tasks. When applied to anomaly detection, SVMs prove to be effective
    in separating normal instances from anomalies by finding a hyperplane with a maximum
    margin. Here is how SVMs work under the hood:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperplane definition**: In a two-dimensional space, a hyperplane is a flat,
    two-dimensional subspace. SVM aims to find a hyperplane that best separates the
    dataset into two classes — normal and anomalous. This hyperplane is positioned
    to maximize the margin, which is the distance between the hyperplane and the nearest
    data points of each class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision boundary**: The hyperplane serves as a decision boundary that separates
    instances of one class from another. In a binary classification scenario, instances
    on one side of the hyperplane are classified as belonging to one class, and those
    on the other side are classified as belonging to the other class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel trick**: SVM can handle complex relationships in the data through
    the use of a kernel function. In many real-world scenarios, the relationship between
    features may not be linear. SVM addresses this by using a kernel function. This
    function transforms the input data into a higher-dimensional space, making it
    easier to find a hyperplane that effectively separates the classes. Commonly used
    kernel functions include the linear kernel (for linearly separable data), polynomial
    kernel, **radial basis function** (**RBF**) or Gaussian kernel, and sigmoid kernel.
    The choice of kernel depends on the nature of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimal hyperplane**: SVM aims to find the hyperplane that maximizes the
    margin, which is the distance between the hyperplane and the nearest data points
    of each class. The larger the margin, the more robust and generalizable the model
    is likely to be. Support vectors are data points that lie closest to the decision
    boundary. They play a crucial role in defining the optimal hyperplane and the
    margin. SVM focuses on these support vectors during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement a Python example of SVM in anomaly detection using our Loan
    Prediction dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_loan_prediction.csv` file using `pd.read_csv()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Data preprocessing**: We will do some basic data preprocessing tasks, including
    handling missing values. For this anomaly detection example, we simplify the analysis
    by excluding categorical variables. In a more complex analysis, you might choose
    to encode and include these variables if they are deemed relevant to your specific
    anomaly detection task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a train-test split for the SVM model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Standardize the features using `StandardScaler` from scikit-learn to ensure
    that all features have the same scale:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the One-Class SVM model for anomaly detection. Adjust the *nu* parameter
    based on the expected proportion of outliers in your dataset. The *nu* parameter
    represents an upper bound on the fraction of margin errors and a lower bound on
    the fraction of support vectors. It essentially controls the proportion of outliers
    or anomalies the algorithm should consider. Choosing an appropriate value for
    *nu* is crucial, and it depends on the characteristics of your dataset and the
    expected proportion of anomalies. Here are some guidelines to help you select
    the *nu* parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Understand the nature of anomalies**: Assess the domain knowledge and characteristics
    of your dataset. Understand the expected proportion of anomalies. If anomalies
    are rare, a smaller value of *nu* might be appropriate.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experiment with a range of values**: Start by experimenting with a range
    of *nu* values, such as 0.01, 0.05, 0.1, 0.2, and so on. You can adjust this range
    based on your understanding of the data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consider the dataset size**: The size of your dataset can also influence
    the choice of *nu*. For larger datasets, a smaller value might be suitable, while
    for smaller datasets, a relatively larger value may be appropriate.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Balance false positives and false negatives**: Depending on the application,
    you might prioritize minimizing false positives or false negatives. Adjust *nu*
    accordingly to achieve the desired balance.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will implement an experiment with a range of values for *nu*. We will specify
    a list of *nu* values that we want to experiment with. These values represent
    the upper bound of the fraction of margin errors and the lower bound of the fraction
    of support vectors in the One-Class SVM model. We then create an empty list to
    store the mean decision function values for each *nu* value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each *nu* value in the list, train a One-Class SVM model with that *nu*
    value. Retrieve the decision function values for the test set and calculate the
    mean decision function value. Append the mean decision function value to the list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Identify the index of the *nu* value that corresponds to the highest mean decision
    function value. Then, retrieve the best *nu* value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a final One-Class SVM model using the best *nu* value and train it on
    the scaled training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now use this model to predict anomalies on the `X_test_scaled` test dataset.
    This line creates a binary representation of the predictions (`y_pred`) by mapping
    -1 to 1 (indicating anomalies) and any other value (typically 1) to 0 (indicating
    normal instances). This is done because the One-Class SVM model often assigns
    -1 to anomalies and 1 to normal instances. We will store this in a new DataFrame
    as `df_with_anomalies`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the confusion matrix and accuracy score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print the following report:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Output classification report](img/B19297_09_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Output classification report
  prefs: []
  type: TYPE_NORMAL
- en: 'Print DataFrame rows predicted as anomalies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will output the following DataFrame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.9 – The df_with_anomalies DataFrame](img/B19297_09_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – The df_with_anomalies DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve walked through the process of implementing anomaly detection
    using SVM in Python. Anomaly detection using SVM can be adapted for various datasets
    with clear anomalies, making it a valuable tool for outlier identification. In
    the next section, we will explore data augmentation and resampling techniques
    for identifying edge cases and rare events.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation and resampling techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Class imbalance is a common issue in datasets with rare events. Class imbalance
    can adversely affect the model’s performance, as the model tends to be biased
    toward the majority class. To address this, we will explore two resampling techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Oversampling**: Increasing the number of instances in the minority class
    by generating synthetic samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Undersampling**: Reducing the number of instances in the majority class to
    balance class distribution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss these resampling techniques in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling using SMOTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Synthetic Minority Over-sampling TEchnique** (**SMOTE**) is a widely used
    resampling method for addressing class imbalance in machine learning datasets,
    especially when dealing with rare events or minority classes. SMOTE helps to generate
    synthetic samples for the minority class by interpolating between existing minority
    class samples. This technique aims to balance class distribution by creating additional
    synthetic instances, thereby mitigating the effects of class imbalance. In a dataset
    with class imbalance, the minority class contains significantly fewer instances
    than the majority class. This can lead to biased model training, where the model
    tends to favor the majority class and performs poorly on the minority class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key steps of a SMOTE algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying minority class instances**: The first step of SMOTE is to identify
    instances belonging to the minority class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Selecting nearest neighbors**: For each minority class instance, SMOTE selects
    its *k* nearest neighbors (commonly chosen through the **k-nearest neighbors**
    (**KNN**) algorithm). These neighbors are used to create synthetic samples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Creating synthetic samples**: For each minority class instance, SMOTE generates
    synthetic samples along the line connecting the instance to its *k* nearest neighbors
    in the feature space. Synthetic samples are created by adding a random fraction
    (usually between 0 and 1) of the feature differences between the instance and
    its neighbors. This process effectively introduces variability to synthetic samples.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Combining with the original data**: The synthetic samples are combined with
    the original minority class instances, resulting in a resampled dataset with a
    more balanced class distribution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SMOTE helps to address class imbalance without discarding any data, as it generates
    synthetic samples rather than removing instances from the majority class. It increases
    the information available to the model, potentially improving the model’s ability
    to generalize to the minority class. SMOTE is straightforward to implement and
    is available in popular libraries such as imbalanced-learn in Python.
  prefs: []
  type: TYPE_NORMAL
- en: While SMOTE is effective in many cases, it might not always perform optimally
    for highly imbalanced datasets or datasets with complex decision boundaries. Generating
    too many synthetic samples can lead to overfitting on the training data, so it
    is crucial to choose an appropriate value for the number of nearest neighbors
    (*k*). SMOTE may introduce some noise and may not be as effective if the minority
    class is too sparse or scattered in the feature space. SMOTE can be combined with
    other techniques, such as undersampling the majority class or using different
    resampling ratios, to achieve better performance. It is essential to evaluate
    the model’s performance on appropriate metrics (for example, precision, recall,
    or F1-score) to assess the impact of SMOTE and other techniques on the model’s
    ability to detect rare events.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement this approach in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_loan_prediction.csv` file using `pd.read_csv()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`''Loan_Status''` column in the dataset contains `''Y''` and `''N''` categorical
    values, which represent loan approval (`''Y''`) and rejection (`''N''`). To convert
    this categorical target variable into a numerical format, `''Y''` is mapped to
    `1` and `''N''` is mapped to `0` using the `map()` function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`fillna()` method. This ensures that any missing values in the dataset are
    replaced with the mean value of their respective columns:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X` feature set. The `select_dtypes()` method is used to include only columns
    with `float` and `int` data types while excluding non-numerical columns such as
    `''Loan_Status''` and `''Loan_ID''`. The `y` target variable is set to `''Loan_Status''`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`X_train`, `y_train`) and testing (`X_test`, `y_test`) sets using the `train_test_split()`
    function from scikit-learn. The training set consists of 80% of the data, while
    the testing set contains 20% of the data. The `random_state` parameter is set
    to `42` to ensure reproducibility:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`SMOTE(random_state=42`). SMOTE is then applied to the training data using
    the `fit_resample()` method. This method oversamples the minority class (loan
    rejection) by generating synthetic samples, creating a balanced dataset. The resulting
    resampled data is stored in `X_train_resampled` and `y_train_resampled`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`RandomForestClassifier(random_state=42)`. The classifier is trained on the
    resampled data (`X_train_resampled`, `y_train_resampled`) using the `fit()` method.
    The trained classifier is used to make predictions on the test data (`X_test`)
    using the `predict()` method. Predictions are stored in `y_pred`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A classification report is generated using the `classification_report()` function
    from scikit-learn. The classification report provides precision, recall, F1-score,
    and support for each class (loan approval and rejection) based on predictions
    (`y_pred`) and true labels (`y_test`) from the test set. The classification report
    is initially returned in a dictionary format. The code converts this dictionary
    to a `clf_report` DataFrame using `pd.DataFrame()`, making it easier to work with
    the data. The `clf_report` DataFrame is transposed using the `.T` attribute to
    have classes (`0` and `1`) as rows and evaluation metrics (precision, recall,
    F1-score, and support) as columns. This transposition provides a more convenient
    and readable format for further analysis or presentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s print the classification report to understand how good the model is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Classification report generated by the preceding code](img/B19297_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Classification report generated by the preceding code
  prefs: []
  type: TYPE_NORMAL
- en: The report indicates that the model performs moderately well in identifying
    instances of class 0, with a precision of 73.33%. However, recall is relatively
    lower at 51.16%, indicating that the model might miss some actual instances of
    class 0\. The model excels in identifying instances of class 1, with a high precision
    of 77.42% and a very high recall of 90.00%. The weighted average metrics consider
    the class imbalance, providing a balanced evaluation across both classes. The
    overall accuracy of the model is 76.42%, indicating the percentage of correctly
    predicted instances.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s explore undersampling – another method to address
    class imbalance problems in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling using RandomUnderSampler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Handling class imbalance with `RandomUnderSampler` is an effective approach
    to address the challenge of imbalanced datasets, where one class significantly
    outweighs the other class(es). In such cases, traditional machine learning algorithms
    may struggle to learn from the data and tend to be biased toward the majority
    class, leading to poor performance on the minority class or rare events.
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` is a resampling technique that aims to balance class distribution
    by randomly removing instances from the majority class until class proportions
    become more balanced. By reducing the number of instances in the majority class,
    `RandomUnderSampler` ensures that the minority class is represented more proportionately,
    making it easier for the model to detect and learn patterns related to rare events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key points about handling class imbalance with `RandomUnderSampler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` is a type of data-level resampling method. Data-level
    resampling techniques involve manipulating the training data to balance class
    distribution. In `RandomUnderSampler`, instances from the majority class are randomly
    selected and removed, resulting in a smaller dataset with a balanced class distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` directly removes instances from the majority class without
    altering minority class instances. This approach helps preserve information from
    the minority class, making it easier for the model to focus on learning patterns
    associated with rare events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` is the loss of information from the majority class. By
    removing instances randomly, some informative instances may be discarded, potentially
    leading to a reduction in the model’s ability to generalize on the majority class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` is computationally efficient since it simply involves
    randomly removing instances from the majority class. This makes it faster compared
    to some other resampling methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` can be effective in certain scenarios, it might not always
    be the best choice, especially if the majority class contains important patterns
    and information. Careful consideration of the problem and dataset characteristics
    is crucial when selecting the appropriate resampling technique.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` can be used in combination with other techniques. For
    example, one can apply `RandomUnderSampler` first and then use `RandomOverSampler`
    (oversampling) to further balance class distribution. This approach helps in achieving
    a more balanced representation of both classes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation and model selection**: When handling class imbalance, it is essential
    to evaluate the model’s performance on relevant metrics such as precision, recall,
    F1-score, and **Area Under the ROC Curve** (**AUC-ROC**). These metrics provide
    a comprehensive assessment of the model’s ability to handle rare events and edge
    cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement this approach using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train_loan_prediction.csv` file using `pd.read_csv()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The `''Loan_Status''` column in the dataset contains `''Y''` and `''N''` categorical
    values, which represent loan approval (`''Y''`) and rejection (`''N''`). To convert
    this categorical target variable into numerical format, `''Y''` is mapped to 1
    and `''N''` is mapped to 0 using the `map()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code applies mean imputation to all columns with missing values
    in the dataset using the `fillna()` method. This ensures that any missing values
    in the dataset are replaced with the mean value of their respective columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The code selects only numerical columns from the dataset to build the `X` feature
    set. The `select_dtypes()` method is used to include only columns with data types
    `float` and `int` data types while excluding non-numerical columns such as `''Loan_Status''`
    and `''Loan_ID''`. The `y` target variable is set to `''Loan_Status''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The data is split into training (`X_train`, `y_train`) and testing (`X_test`,
    `y_test`) sets using the `train_test_split()` function from scikit-learn. The
    training set consists of 80% of the data, while the testing set contains 20% of
    the data. The `random_state` parameter is set to `42` to ensure reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We then instantiate `RandomUnderSampler` and apply it to the training data
    using the `fit_resample()` method. This method undersamples the majority class
    (loan approval) to create a balanced dataset. The resulting resampled data is
    stored in `X_train_resampled` and `y_train_resampled`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'A Random Forest classifier is then trained on the resampled data (`X_train_resampled`,
    `y_train_resampled`) using the `fit()` method. The trained classifier is used
    to make predictions on the test data (`X_test`) using the `predict()` method.
    Predictions are stored in `y_pred`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s inspect the classification report to assess the model performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Classification report of the model’s performance](img/B19297_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Classification report of the model’s performance
  prefs: []
  type: TYPE_NORMAL
- en: The model shows decent performance for both classes, with higher precision,
    recall, and F1-score for class `1` compared to class `0`.The weighted average
    considers the imbalance in class distribution, providing a more representative
    measure of overall performance. The accuracy score of 0.7805% suggests that the
    model correctly predicted the class for approximately 78% of instances in the
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s understand cost-sensitive learning and explore its
    crucial role in scenarios where rare events bear significant consequences.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-sensitive learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cost-sensitive learning is a machine learning approach that takes into account
    costs associated with misclassifications of different classes during the model
    training process. In traditional machine learning, the focus is on maximizing
    overall accuracy, but in many real-world scenarios, misclassifying certain classes
    can have more severe consequences than misclassifying others.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a medical diagnosis application, misdiagnosing a severe disease
    as not present (false negative) could have more significant consequences than
    misdiagnosing a mild condition as present (false positive). In fraud detection,
    incorrectly flagging a legitimate transaction as fraudulent (false positive) might
    inconvenience the customer, while failing to detect actual fraudulent transactions
    (false negative) could lead to significant financial losses.
  prefs: []
  type: TYPE_NORMAL
- en: Cost-sensitive learning addresses these imbalances in costs by assigning different
    misclassification costs to different classes. By incorporating these costs into
    the training process, the model is encouraged to prioritize minimizing the overall
    misclassification cost rather than simply optimizing accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several approaches to implementing cost-sensitive learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Modifying loss functions**: The loss function used during model training
    can be modified to incorporate class-specific misclassification costs. The goal
    is to minimize the expected cost, which is a combination of misclassification
    costs and the model’s predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Class weights**: Another approach is to assign higher weights to the minority
    class or the class with higher misclassification costs. This technique can be
    applied to various classifiers, such as decision trees, random forests, and SVMs,
    to emphasize learning from the minority class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling techniques**: In addition to assigning weights, resampling techniques
    such as oversampling the minority class or undersampling the majority class can
    also be used to balance class distribution and improve the model’s ability to
    learn from rare events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Threshold adjustment**: By adjusting the classification threshold, we can
    control the trade-off between precision and recall, allowing us to make predictions
    that are more sensitive to the minority class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Ensemble methods such as cost-sensitive boosting combine
    multiple models to focus on hard-to-classify instances and assign higher weights
    to misclassified samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive learning is especially important in scenarios where the class
    imbalance is severe and the consequences of misclassification are critical. By
    taking into account costs associated with different classes, the model can make
    more informed decisions and improve overall performance in detecting rare events
    and handling edge cases.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that cost-sensitive learning requires careful consideration
    of the cost matrix, as incorrectly specified costs can lead to unintended results.
    Proper validation and evaluation of the model on relevant metrics, considering
    real-world costs, are crucial to ensure the effectiveness and reliability of cost-sensitive
    learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now demonstrate cost-sensitive learning using the Loan Prediction dataset
    in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the required libraries and datasets using `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now need to perform data preprocessing to handle missing values and convert
    target variables to numeric data types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For this example, we will use only numeric columns. We will then split the
    dataset into `train` and `test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We first calculate the class weights based on the inverse of class frequencies
    in the training data. The higher the frequency of a class, the lower its weight,
    and vice versa. This way, the model assigns higher importance to the minority
    class (rare events) and is more sensitive to its correct prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we train the Random Forest classifier with the `class_weight` parameter
    set to the calculated class weights. This modification allows the classifier to
    consider the class weights during the training process, effectively implementing
    cost-sensitive learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After training the model, we make predictions on the test data and evaluate
    the classifier’s performance using the classification report, which provides precision,
    recall, F1-score, and support for each class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s view the classification report and assess the Random Forest classifier’s
    performance:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Classification report of the Random Forest classifier’s performance](img/B19297_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Classification report of the Random Forest classifier’s performance
  prefs: []
  type: TYPE_NORMAL
- en: In cost-sensitive learning, you would typically define a cost matrix that quantifies
    misclassification costs for each class and use it to guide the model’s training.
    The results from the classification report can help you identify areas where adjustments
    may be needed to align the model with specific cost considerations in your application.
    A cost matrix is especially useful in situations where the costs of false positives
    and false negatives are not equal. If the cost of false positives is higher, consider
    raising the decision threshold. If the cost of false negatives is higher, consider
    lowering the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s understand which evaluation metrics are used for
    detecting edge cases and rare events.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing evaluation metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with edge cases and rare events in machine learning, selecting
    the right evaluation metrics is crucial to accurately assess the performance of
    the model. Traditional evaluation metrics, such as accuracy, may not be sufficient
    in imbalanced datasets where the class of interest (the rare event) is vastly
    outnumbered by the majority class. In imbalanced datasets, where the rare event
    is a minority class, traditional evaluation metrics such as accuracy can be misleading.
    For instance, if a dataset has 99% of the majority class and only 1% of the rare
    event, a model that predicts all instances as the majority class will still achieve
    an accuracy of 99%, which is deceptively high. However, such a model would be
    ineffective in detecting the rare event. To address this issue, we need evaluation
    metrics that focus on the model’s performance in correctly identifying the rare
    event, even at the expense of a decrease in accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some evaluation metrics that are more suitable for detecting edge
    cases and rare events:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: Precision measures the accuracy of positive predictions made
    by the model. It is the ratio of true positive (correctly predicted rare event)
    to the sum of true positive and false positive (incorrectly predicted rare event
    as the majority class). High precision indicates that the model is cautious in
    making positive predictions and has a low false positive rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall (sensitivity)**: Recall measures the proportion of true positives
    predicted by the model out of all actual positive instances. It is the ratio of
    true positive to the sum of true positive and false negative (incorrectly predicted
    majority class as the rare event). High recall indicates that the model is capable
    of capturing a significant portion of rare event instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1-score**: The F1-score is the harmonic mean of precision and recall. It
    provides a balance between the two metrics and is especially useful when there
    is an imbalance between precision and recall. F1-score penalizes models that prioritize
    either precision or recall at the expense of the other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Area Under the Receiver Operating Characteristic (ROC-AUC)**: ROC-AUC is
    a performance metric used to evaluate binary classification models. It measures
    the area under the ROC curve, which plots the true positive rate (recall) against
    the false positive rate as the classification threshold changes. A higher ROC-AUC
    indicates better model performance, especially in detecting rare events.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, let’s delve into ensemble techniques and understand their
    crucial role in machine learning models, especially when dealing with data containing
    edge cases and rare events.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble techniques are powerful methods used to improve the performance of
    machine learning models, particularly in scenarios with imbalanced datasets, rare
    events, and edge cases. These techniques combine multiple base models to create
    a more robust and accurate final prediction. Let’s discuss some popular ensemble
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bootstrap aggregating** (**bagging**) is an ensemble technique that creates
    multiple bootstrap samples (random subsets with replacement) from the training
    data and trains a separate base model on each sample. The final prediction is
    obtained by averaging or voting the predictions of all base models. Bagging is
    particularly useful when dealing with high variance and complex models, as it
    reduces overfitting and enhances the model’s generalization ability. Here are
    the key concepts associated with bagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap sampling**: The bagging process begins by creating multiple random
    subsets of the training data through a process called bootstrap sampling. Bootstrap
    sampling involves randomly selecting data points from the original dataset with
    replacements. As a result, some data points may appear more than once in a subset,
    while others may be left out.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Base model training**: For each bootstrap sample, a base model (learner)
    is trained independently on that particular subset of the training data. The base
    models can be any machine learning algorithm, such as decision trees, random forests,
    or SVMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregating predictions**: Once all base models are trained, they are used
    to make predictions on new, unseen data. For classification tasks, the final prediction
    is typically determined by majority voting, where the class that receives the
    most votes across the base models is chosen. In regression tasks, the final prediction
    is obtained by averaging the predictions from all base models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few benefits of bagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance reduction**: Bagging helps reduce variance in the model by combining
    predictions from multiple models trained on different subsets of the data. This
    results in a more stable and robust model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting prevention**: By training each base model on different subsets
    of the data, bagging prevents individual models from overfitting to noise in the
    training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model generalization**: Bagging improves the model’s generalization ability
    by reducing bias and variance, leading to better performance on unseen data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelism**: Since the base models are trained independently, bagging is
    amenable to parallel processing, making it computationally efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest is a popular example of the bagging technique. In Random Forest,
    the base models are decision trees, and the predictions from multiple decision
    trees are combined to make the final prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of implementing bagging using Random Forest in Python on
    the Loan Prediction dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: In conclusion, bagging techniques offer a robust and effective strategy for
    handling edge cases. By aggregating predictions from multiple base models, bagging
    not only enhances the overall model stability but also fortifies its ability to
    accurately identify and address edge cases, contributing to a more resilient and
    reliable predictive framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore boosting, another method to handle edge
    cases and rare events.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boosting is an ensemble technique that builds base models sequentially, with
    each subsequent model focusing on misclassified instances of the previous model.
    It assigns higher weights to misclassified instances, thus giving more attention
    to rare events. Popular boosting algorithms include **Adaptive Boosting** (**AdaBoost**),
    Gradient Boosting, and XGBoost. Boosting aims to create a strong learner by combining
    weak learners iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how boosting works:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Base model training**: Boosting starts by training a base model (also known
    as a weak learner) on the entire training dataset. Weak learners are usually simple
    models with limited predictive power, such as decision stumps (a decision tree
    with a single split).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Weighted training**: After the first model is trained, data points that were
    misclassified by the model are assigned higher weights. This means that the subsequent
    model will pay more attention to those misclassified data points, attempting to
    correct their predictions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative training**: Boosting follows an iterative approach. For each iteration
    (or boosting round), a new weak learner is trained on the updated training data
    with adjusted weights. The weak learners are then combined to create a strong
    learner, which improves its predictive performance compared to the individual
    weak learners.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Weighted voting**: During the final prediction, the weak learners’ predictions
    are combined with weighted voting, where models with higher accuracy have more
    influence on the final prediction. This allows the boosting algorithm to focus
    on difficult-to-classify instances and improve the model’s sensitivity to rare
    events.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The benefits of boosting are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increased accuracy**: Boosting improves the model’s accuracy by focusing
    on the most challenging instances in the dataset and refining predictions over
    multiple iterations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness**: Boosting reduces the model’s sensitivity to noise and outliers
    in the data by iteratively adjusting weights and learning from previous mistakes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model adaptation**: Boosting adapts well to different types of data and can
    handle complex relationships between features and the target variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble diversity**: Boosting creates a diverse ensemble of weak learners,
    which results in better generalization and reduced overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at an example of boosting using AdaBoost.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost is a popular boosting algorithm that is commonly used in practice.
    In AdaBoost, the base models are typically decision stumps, and the model’s weights
    are adjusted after each iteration to emphasize misclassified instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of implementing boosting using AdaBoost in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: In summary, the application of boosting techniques emerges as a robust strategy
    for handling edge cases. Through its iterative approach, boosting empowers models
    to focus on instances that pose challenges, ultimately enhancing their ability
    to discern and accurately predict rare events.
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore how to use the stacking method to detect and handle edge
    cases and rare events in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stacking is an advanced ensemble learning technique that combines the predictions
    of multiple base models by training a meta-model on their outputs. Stacking aims
    to leverage the strengths of different base models to create a more accurate and
    robust final prediction. It is a form of “learning to learn” where the meta-model
    learns how to best combine the predictions of the base models. The base models
    act as “learners,” and their predictions become the input features for the meta-model,
    which makes the final prediction. Stacking can often improve performance by capturing
    complementary patterns from different base models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the model methodology:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Base model training**: The stacking process starts by training multiple diverse
    base models on the training dataset. These base models can be different types
    of machine learning algorithms or even the same algorithm with different hyperparameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Base model predictions**: Once the base models are trained, they are used
    to make predictions on the same training data (in-sample predictions) or a separate
    validation dataset (out-of-sample predictions).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Meta-model training**: The predictions from the base models are then combined
    to create a new dataset that serves as the input for the meta-model. Each base
    model’s predictions become a new feature in this dataset. The meta-model is trained
    on this new dataset along with the true target labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Final prediction**: During the final prediction phase, the base models make
    predictions on the new, unseen data. These predictions are then used as input
    features for the meta-model, which makes the final prediction.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stacking has the following benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved predictive performance**: Stacking leverages the complementary strengths
    of different base models, potentially leading to better overall predictive performance
    compared to using individual models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduction of bias and variance**: Stacking can reduce the model’s bias and
    variance by combining multiple models, leading to improved generalization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Stacking allows the use of diverse base models, making it
    suitable for various types of data and problems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble diversity**: Stacking creates a diverse ensemble by using various
    base models, which can help prevent overfitting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example of implementing stacking using scikit-learn in Python using
    the Loan Prediction dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the required libraries and load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now need to perform data preprocessing to handle missing values and convert
    target variables to numeric data types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For simplicity, we will use only numeric columns for this example. We then
    split the dataset into `train` and `test`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`RandomForestClassifier` and `GradientBoostingClassifier`, are instantiated
    with `RandomForestClassifier(random_state=42)` and `GradientBoostingClassifier(random_state=42)`
    respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These base models are trained on the training data (`X_train`, `y_train`) using
    the `fit()` method, as seen next. The trained base models are used to make predictions
    on the test data (`X_test`) using the `predict()` method. Predictions from both
    base models are stored in `pred_base_model_1` and `pred_base_model_2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`stacking_X_train` dataset is created by combining the predictions from the
    base models (`pred_base_model_1` and `pred_base_model_2`). This new dataset will
    be used as input features for the meta-model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`LogisticRegression()`. The meta-model is trained on the new dataset (`stacking_X_train`)
    and the true labels from the test set (`y_test`) using the `fit()` method. The
    meta-model learns to combine predictions of the base models and make the final
    prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`new_unseen_data`) is created by randomly selecting 20% of the test data (`X_test`)
    using the `sample()` method. The base models are used to make predictions on the
    new, unseen data (`new_unseen_data`) using the `predict()` method. Predictions
    from both base models for the new data are stored in `new_pred_base_model_1` and
    `new_pred_base_model_2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`stacking_new_unseen_data` dataset is created by combining predictions from
    the base models (`new_pred_base_model_1` and `new_pred_base_model_2`) for the
    new, unseen data. This new dataset will be used as input features for the meta-model
    to make the final prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`stacking_new_unseen_data`) using the `predict()` method. The `final_prediction`
    variable holds the predicted classes (0 or 1) based on the meta-model’s decision:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In summary, this code demonstrates the concept of stacking, where base models
    (Random Forest and Gradient Boosting) are trained on the original data, their
    predictions are used as input features for a meta-model (Logistic Regression),
    and the final prediction is made using the meta-model on new, unseen data. Stacking
    allows the models to work together and can potentially improve the prediction
    performance compared to using the base models alone.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the critical aspect of detecting rare events and
    edge cases in machine learning. Rare events, by their infrequency, hold significant
    implications across various domains and necessitate special attention. We delved
    into several techniques and methodologies that equip us to effectively identify
    and handle these uncommon occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical methods, such as Z-scores and IQR, provide powerful tools to pinpoint
    outliers and anomalies in our data. These methods aid in establishing meaningful
    thresholds for identifying rare events, enabling us to distinguish significant
    data points from noise.
  prefs: []
  type: TYPE_NORMAL
- en: We also explored machine learning-based anomaly detection techniques, such as
    isolation forest and autoencoders. These methods leverage unsupervised learning
    to identify patterns and deviations that diverge from the majority of the data,
    making them well suited for detecting rare events in complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we discussed the significance of resampling methods such as SMOTE
    and `RandomUnderSampler` to tackle class imbalances. These techniques enable us
    to create balanced datasets that enhance the performance of models in identifying
    rare events while preserving data integrity.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we uncovered the potential of ensemble techniques, including stacking,
    bagging, and boosting, in augmenting the capabilities of our models for detecting
    edge cases. The combined power of multiple models through ensembles enhances generalization
    and model robustness.
  prefs: []
  type: TYPE_NORMAL
- en: It is crucial to select appropriate evaluation metrics, especially in the presence
    of rare events, to ensure fair assessment and accurate model performance evaluation.
    Metrics such as precision, recall, F1-score, and AUC-ROC provide comprehensive
    insights into model performance and guide decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting rare events and edge cases has far-reaching implications across diverse
    domains, including medical diagnosis, fraud detection, predictive maintenance,
    and environmental monitoring. By employing effective techniques to identify and
    handle these infrequent occurrences, we enhance the reliability and efficiency
    of machine learning applications.
  prefs: []
  type: TYPE_NORMAL
- en: As we conclude this chapter, let us recognize the significance of this skill
    in real-world scenarios. Detecting rare events empowers us to make informed decisions,
    protect against potential risks, and leverage the full potential of machine learning
    to drive positive impact in a multitude of fields.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore some of the challenges faced by the data-centric
    approach to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 4: Getting Started with Data-Centric ML'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now you may have realized that shifting to a data-centric approach to ML
    involves not just adapting your own ways of working, but also influencing those
    around you – a task that’s far from simple. In this part, we explore both the
    technical and non-technical hurdles you might encounter during the development
    and deployment of models, and reveal how adopting a data-centric approach can
    aid in overcoming these obstacles.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19297_10.xhtml#_idTextAnchor164)*, Kick-Starting Your Journey
    in Data-Centric Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
