- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Dealing with Edge Cases and Rare Events in Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器学习中处理边缘案例和罕见事件
- en: In the field of **machine learning**, it is crucial to identify and handle edge
    cases properly. Edge cases refer to instances in your dataset that are significantly
    different from the majority of the data, and they can have a substantial impact
    on the performance and reliability of your machine learning models. Rare events
    can be challenging for machine learning models due to class imbalance problems
    as they might not have enough data to learn patterns effectively. Class imbalance
    occurs when one class (the rare event) is significantly underrepresented compared
    to the other class(es). Traditional machine learning algorithms tend to perform
    poorly in such scenarios because they may be biased toward the majority class,
    leading to lower accuracy in identifying rare events.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在**机器学习**领域，正确识别和处理边缘情况至关重要。边缘情况指的是与数据集大多数数据显著不同的实例，它们可能会对机器学习模型的性能和可靠性产生重大影响。由于类别不平衡问题，罕见事件可能对机器学习模型具有挑战性，因为它们可能没有足够的数据来有效地学习模式。类别不平衡发生在某一类别（罕见事件）相对于其他类别（们）显著代表性不足时。传统的机器学习算法在这种场景下往往表现不佳，因为它们可能偏向于多数类别，导致在识别罕见事件时准确性降低。
- en: In this chapter, we will explore various techniques and approaches to detect
    edge cases in machine learning and data, using Python code examples. We’ll explore
    statistical techniques, including using visualizations and other measures such
    as Z-scores, to analyze data distributions and identify potential outliers. We
    will also focus on methodologies such as isolation forests and semi-supervised
    methods such as **autoencoders** to uncover anomalies and irregular patterns in
    your datasets. We will learn how to address class imbalance and enhance model
    performance through techniques such as oversampling, undersampling, and generating
    synthetic data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨使用Python代码示例检测机器学习和数据中边缘案例的各种技术和方法。我们将探讨统计技术，包括使用可视化和其他措施如Z分数来分析数据分布并识别潜在的异常值。我们还将关注隔离森林和半监督方法，如**自编码器**，以揭示数据集中的异常和不规则模式。我们将学习如何通过过采样、欠采样和生成合成数据等技术来解决类别不平衡并提高模型性能。
- en: In the latter half of this chapter, we’ll understand the importance of adjusting
    the learning process to account for imbalanced classes, especially in scenarios
    where the rare event carries significant consequences. We’ll explore the significance
    of selecting appropriate evaluation metrics, emphasizing those that account for
    class imbalance, to ensure a fair and accurate assessment of model performance.
    Finally, we’ll understand how ensemble methods such as bagging, boosting, and
    stacking help us enhance the robustness of models, particularly in scenarios where
    rare events play a crucial role.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后半部分，我们将了解调整学习过程以考虑不平衡类别的必要性，特别是在罕见事件具有重大后果的场景中。我们将探讨选择适当的评估指标的重要性，强调那些考虑类别不平衡的指标，以确保对模型性能的公平和准确评估。最后，我们将了解集成方法，如bagging、boosting和stacking如何帮助我们增强模型的鲁棒性，特别是在罕见事件发挥关键作用的场景中。
- en: 'The following key topics will be covered:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 以下关键主题将涵盖：
- en: Importance of detecting rare events and edge cases in machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习中检测罕见事件和边缘案例的重要性
- en: Statistical methods
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计方法
- en: Anomaly detection
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测
- en: Data augmentation and resampling techniques
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强和重采样技术
- en: Cost-sensitive learning
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本敏感学习
- en: Choosing evaluation metrics
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择评估指标
- en: Ensemble techniques
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成技术
- en: Importance of detecting rare events and edge cases in machine learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器学习中检测罕见事件和边缘案例的重要性
- en: 'Detecting rare events and edge cases is crucial in machine learning for several
    reasons:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，检测罕见事件和边缘情况至关重要，原因有以下几点：
- en: '**Decision-making in critical scenarios**: Rare events often represent critical
    scenarios or anomalies that require immediate attention or special treatment.
    For instance, in medical diagnosis, rare diseases or extreme cases might need
    urgent intervention. Accurate detection of these events can lead to better decision-making
    and prevent adverse consequences.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键场景中的决策**：罕见事件通常代表需要立即关注或特殊处理的临界场景或异常情况。例如，在医疗诊断中，罕见疾病或极端病例可能需要紧急干预。准确检测这些事件可以导致更好的决策并防止不良后果。'
- en: '**Unbalanced datasets**: Many real-world datasets suffer from class imbalance,
    where one class (often the rare event) is significantly underrepresented compared
    to the other classes. This can lead to biased models that perform poorly on the
    minority class. Detecting rare events helps identify the need for special handling,
    such as using resampling techniques or employing appropriate evaluation metrics
    to ensure fair evaluation.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不平衡数据集**：许多现实世界的数据集存在类别不平衡的问题，其中一个类别（通常是罕见事件）与其他类别相比显著代表性不足。这可能导致在少数类别上表现不佳的偏见模型。检测罕见事件有助于确定需要特殊处理的需求，例如使用重采样技术或采用适当的评估指标以确保公平评估。'
- en: '**Fraud detection**: In fraud detection applications, rare events often correspond
    to fraudulent transactions or activities. Detecting these rare cases is crucial
    for preventing financial losses and ensuring the security of financial systems.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈检测**：在欺诈检测应用中，罕见事件通常对应于欺诈交易或活动。检测这些罕见案例对于防止财务损失和确保金融系统的安全至关重要。'
- en: '**Quality control and anomaly detection**: In manufacturing and industrial
    processes, detecting rare events can help identify faulty or anomalous products
    or processes. This enables timely intervention to improve product quality and
    maintain operational efficiency.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**质量控制与异常检测**：在制造和工业过程中，检测罕见事件可以帮助识别有缺陷或异常的产品或流程。这使及时干预成为可能，从而提高产品质量并保持运营效率。'
- en: '**Predictive maintenance**: In predictive maintenance, detecting edge cases
    can indicate potential equipment failures or abnormal behaviors, allowing proactive
    maintenance to reduce downtime and increase productivity.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测性维护**：在预测性维护中，检测边缘案例可以表明潜在的设备故障或异常行为，从而允许主动维护以减少停机时间并提高生产力。'
- en: '**Model generalization**: By accurately identifying and handling rare events,
    machine learning models can better generalize to unseen data and handle real-world
    scenarios effectively.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型泛化**：通过准确识别和处理罕见事件，机器学习模型可以更好地泛化到未见数据，并有效地处理现实世界场景。'
- en: '**Customer behavior analysis**: In marketing and customer analysis, detecting
    rare events can reveal unusual patterns or behaviors of interest, such as identifying
    high-value customers or detecting potential churners.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户行为分析**：在营销和客户分析中，检测罕见事件可以揭示有趣的异常模式或行为，例如识别高价值客户或检测潜在的流失者。'
- en: '**Security and intrusion detection**: In cybersecurity, rare events may indicate
    security breaches or cyber-attacks. Detecting and responding to these events in
    real time is essential for ensuring the safety and integrity of digital systems.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全和入侵检测**：在网络安全领域，罕见事件可能表明安全漏洞或网络攻击。实时检测和应对这些事件对于确保数字系统的安全和完整性至关重要。'
- en: '**Environmental monitoring**: In environmental applications, rare events might
    signify unusual ecological conditions or natural disasters. Detecting such events
    aids in disaster preparedness and environmental monitoring efforts.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境监测**：在环境应用中，罕见事件可能表明不寻常的生态条件或自然灾害。检测此类事件有助于灾害准备和环境监测工作。'
- en: Let’s discuss different statistical methods to analyze data distributions and
    identify potential outliers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论不同的统计方法来分析数据分布并识别潜在的异常值。
- en: Statistical methods
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计方法
- en: Statistical methods provide valuable tools for identifying outliers and anomalies
    in our data, aiding in data preprocessing and decision-making. In this section,
    we’ll talk about how to use methods such as Z-scores, **Interquartile Range**
    (**IQR**), box plots, and scatter plots to uncover anomalies in our data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 统计方法为我们识别数据中的异常值和异常情况提供了宝贵的工具，有助于数据预处理和决策制定。在本节中，我们将讨论如何使用诸如Z分数、**四分位数范围**（**IQR**）、箱线图和散点图等方法来揭示数据中的异常情况。
- en: Z-scores
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Z分数
- en: 'Z-scores, also known as standard scores, are a statistical measure that indicates
    how many standard deviations a data point is away from the mean of the data. Z-scores
    are used to standardize data and allow for comparisons between different datasets,
    even if they have different units or scales. They are particularly useful in detecting
    outliers and identifying extreme values in a dataset. The formula to calculate
    the Z-score for a data point *x* in a dataset with mean *μ* and standard deviation
    *σ* is presented here:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Z 分数，也称为标准分数，是一种统计量，表示数据点与数据均值之间的标准差数。Z 分数用于标准化数据，并允许在不同数据集之间进行比较，即使它们有不同的单位或尺度。它们在检测异常值和识别数据集中的极端值方面特别有用。计算数据集中数据点
    *x* 的 Z 分数的公式如下：
- en: Z = (x − μ) / σ
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Z = (x − μ) / σ
- en: 'Here, the following applies:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，以下规则适用：
- en: '*Z* is the Z-score of the data point *x*'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Z* 是数据点 *x* 的 Z 分数'
- en: '*x* is the value of the data point'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* 是数据点的值'
- en: '*μ* is the mean of the dataset'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*μ* 是数据集的均值'
- en: '*σ* is the standard deviation of the dataset'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σ* 是数据集的标准差'
- en: Z-scores are widely used to detect outliers in a dataset. Data points with Z-scores
    that fall outside a certain threshold (e.g., *Z* > 3 or *Z* < -3) are considered
    outliers. These outliers can represent extreme values or measurement errors in
    the data. By transforming the data into Z-scores, the mean becomes 0, and the
    standard deviation becomes 1, resulting in a standardized distribution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Z 分数广泛用于检测数据集中的异常值。具有超出一定阈值（例如，*Z* > 3 或 *Z* < -3）的 Z 分数的数据点被认为是异常值。这些异常值可能代表数据中的极端值或测量误差。通过将数据转换为
    Z 分数，均值变为 0，标准差变为 1，从而得到一个标准化的分布。
- en: Z-scores are employed in normality testing to assess whether a dataset follows
    a normal (Gaussian) distribution. If the dataset follows a normal distribution,
    approximately 68% of the data points should have Z-scores between -1 and 1, about
    95% between -2 and 2, and nearly all between -3 and 3\. In hypothesis testing,
    Z-scores are used to compute *p*-values and make inferences about population parameters.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Z 分数用于正态性检验，以评估数据集是否遵循正态（高斯）分布。如果数据集遵循正态分布，则大约 68% 的数据点的 Z 分数应在 -1 和 1 之间，约
    95% 在 -2 和 2 之间，几乎所有数据点都在 -3 和 3 之间。在假设检验中，Z 分数用于计算 *p*-值并对总体参数进行推断。
- en: For example, Z-tests are commonly used for sample mean comparisons when the
    population standard deviation is known. Z-scores can be useful in anomaly detection
    where we want to identify data points that deviate significantly from the norm.
    High Z-scores may indicate anomalous behavior or rare events in the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当总体标准差已知时，Z 测试通常用于样本均值比较。Z 分数在异常检测中很有用，当我们想要识别与规范显著偏离的数据点时。高 Z 分数可能表明数据中的异常行为或罕见事件。
- en: 'Let’s explore this concept in Python using the *Loan Prediction* dataset. Let’s
    start by loading the dataset:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 *Loan Prediction* 数据集在 Python 中探索这个概念。让我们首先加载数据集：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, we see the sample dataset:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到样本数据集：
- en: '![Figure 9.1 – The df DataFrame](img/B19297_09_1.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – df DataFrame](img/B19297_09_1.jpg)'
- en: Figure 9.1 – The df DataFrame
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – df DataFrame
- en: 'Now that we have loaded the dataset, we can calculate Z-scores on some of the
    numerical features:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了数据集，我们可以在一些数值特征上计算 Z 分数：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We leverage Z-scores to detect outliers in our dataset effectively:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用 Z 分数有效地检测数据集中的异常值：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: By setting a Z-score threshold of 3, we can identify outliers by examining if
    any rows have values that lie beyond the Z-score boundaries of greater than 3
    or less than -3\. This approach allows us to pinpoint data points that deviate
    significantly from the mean and enables us to take appropriate actions to handle
    these outliers, ensuring the integrity of our data and the accuracy of subsequent
    analyses and models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置 Z 分数阈值为 3，我们可以通过检查是否有任何行的值超出 Z 分数边界（大于 3 或小于 -3）来识别异常值。这种方法使我们能够确定与均值显著偏离的数据点，并使我们能够采取适当的措施来处理这些异常值，确保数据的完整性和后续分析和模型的准确性。
- en: 'Here is the output DataFrame of outliers using this approach:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法得到的输出 DataFrame 包含异常值如下：
- en: '![Figure 9.2 – The resulting outlier_rows DataFrame](img/B19297_09_2.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 结果的 outlier_rows DataFrame](img/B19297_09_2.jpg)'
- en: Figure 9.2 – The resulting outlier_rows DataFrame
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 结果的 outlier_rows DataFrame
- en: In conclusion, Z-scores make it easier to spot outliers accurately. They act
    as a helpful tool, guiding us to understand data patterns better. In the next
    section on IQR, we will further explore alternative methods for detecting and
    addressing outliers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Z分数使得准确识别异常值变得更加容易。它们作为一个有用的工具，帮助我们更好地理解数据模式。在下一节关于四分位数的讨论中，我们将进一步探讨检测和解决异常值的替代方法。
- en: Interquartile Range (IQR)
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 四分位距（IQR）
- en: IQR is a statistical measure used to describe the spread or dispersion of a
    dataset. It is particularly useful in identifying and handling outliers and understanding
    the central tendency of the data. The IQR is defined as the range between the
    first quartile (Q1) and the third quartile (Q3) of a dataset. Quartiles are points
    that divide a dataset into four equal parts, each containing 25% of the data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: IQR是一种用于描述数据集分布或分散程度的统计量。它在识别和处理异常值以及理解数据的中心趋势方面特别有用。IQR定义为数据集的第一四分位数（Q1）和第三四分位数（Q3）之间的范围。四分位数是将数据集分为四个相等部分，每个部分包含25%的数据的点。
- en: 'The IQR can be calculated using the following formula:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下公式计算IQR：
- en: IQR = Q3 − Q1
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: IQR = Q3 − Q1
- en: 'Here, the following applies:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，以下规则适用：
- en: Q1 is the first quartile (25th percentile), representing the value below which
    25% of the data lies
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q1是第一四分位数（25百分位数），表示低于此值的25%的数据。
- en: Q3 is the third quartile (75th percentile), representing the value below which
    75% of the data lies
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q3是第三四分位数（75百分位数），表示低于此值的75%的数据。
- en: IQR is commonly used to identify outliers in a dataset. Data points that fall
    below Q1 - 1.5 * IQR or above Q3 + 1.5 * IQR are considered outliers and may warrant
    further investigation. IQR provides valuable information about the distribution
    of the data. It helps to understand the spread of the middle 50% of the dataset
    and can be used to assess the symmetry of the distribution. When comparing datasets,
    IQR can be used to assess differences in the spread of data between two or more
    datasets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: IQR通常用于识别数据集中的异常值。低于Q1 - 1.5 * IQR或高于Q3 + 1.5 * IQR的数据点被认为是异常值，可能需要进一步调查。IQR提供了关于数据分布的有价值信息。它有助于理解数据集中间50%的分布，可以用来评估分布的对称性。在比较数据集时，IQR可以用来评估两个或更多数据集之间数据分散的差异。
- en: 'Here’s a simple example of how to calculate the IQR for a dataset using Python
    and NumPy on the Loan Prediction dataset. To begin with outlier detection, we
    calculate the IQR for numerical features. By defining a threshold, typically 1.5
    times the IQR based on Tukey’s method, which involves calculating the IQR as the
    difference between the third quartile (Q3) and the first quartile (Q1), we can
    identify outliers. Tukey’s method is a robust statistical technique that aids
    in detecting data points that deviate significantly from the overall distribution,
    providing a reliable measure for identifying potential anomalies in the dataset.
    Once outliers are flagged for all numerical features, we can display the rows
    with outlier values, which aids in further analysis and potential data treatment.
    Using the IQR and Tukey’s method together facilitates effective outlier detection
    and helps ensure the integrity of the dataset:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个简单的例子，说明如何使用Python和NumPy在贷款预测数据集上计算数据集的IQR。首先进行异常值检测，我们计算数值特征的IQR。通过定义一个阈值，通常是Tukey方法中IQR的1.5倍，该方法涉及计算IQR为第三四分位数（Q3）和第一四分位数（Q1）之间的差值，我们可以识别异常值。Tukey方法是一种稳健的统计技术，有助于检测与整体分布显著偏离的数据点，为识别数据集中的潜在异常提供了一个可靠的度量。一旦所有数值特征都被标记为异常值，我们就可以显示具有异常值的行，这有助于进一步分析和潜在的数据处理。使用IQR和Tukey方法一起可以促进有效的异常值检测，并有助于确保数据集的完整性：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the output DataFrame of outliers using this approach:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用此方法得到的异常值输出DataFrame：
- en: '![Figure 9.3 – The resulting outlier_rows DataFrame](img/B19297_09_3.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 结果的outlier_rows DataFrame](img/B19297_09_3.jpg)'
- en: Figure 9.3 – The resulting outlier_rows DataFrame
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 结果的outlier_rows DataFrame
- en: In conclusion, the exploration of IQR provides a valuable technique for identifying
    outliers, particularly effective in capturing the central tendencies of a dataset.
    While IQR proves advantageous in scenarios where a focus on the middle range is
    paramount, it’s essential to recognize its limitations in capturing the entire
    data distribution. In the upcoming section on box plots, we will delve into a
    graphical representation that complements IQR, offering a visual tool to better
    understand data dispersion and outliers in diverse contexts.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，探索四分位数间距（IQR）为识别异常值提供了一种有价值的技术，尤其是在捕捉数据集的中心趋势方面特别有效。虽然IQR在关注中间范围至关重要的情况下具有优势，但认识到它在捕捉整个数据分布方面的局限性是至关重要的。在接下来的关于箱线图的章节中，我们将深入研究一种补充IQR的图形表示，提供一种在多种情境下更好地理解数据分散和异常值的视觉工具。
- en: Box plots
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 箱线图
- en: Box plots, also known as box-and-whisker plots, are a graphical representation
    of the distribution of a dataset. They provide a quick and informative way to
    visualize the spread and skewness of the data, identify potential outliers, and
    compare multiple datasets. Box plots are particularly useful when dealing with
    continuous numerical data and can be used to gain insights into the central tendency
    and variability of the data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图，也称为箱形图，是数据集分布的图形表示。它们提供了一种快速且信息丰富的可视化方式，可以直观地查看数据的分布和偏斜，识别潜在的异常值，并比较多个数据集。箱线图在处理连续数值数据时特别有用，可以用来深入了解数据的中心趋势和变异性。
- en: 'Here are the components of a box plot:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是箱线图的组成部分：
- en: '**Box (IQR)**: The box represents the IQR, which is the range between the first
    quartile (Q1) and the third quartile (Q3) of the data. It spans the middle 50%
    of the dataset and provides a visual representation of the data’s spread.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**箱线图（IQR）**：箱线图表示IQR，即数据的第一四分位数（Q1）和第三四分位数（Q3）之间的范围。它覆盖了数据集的中间50%，并提供了数据分布的视觉表示。'
- en: '**Median (Q2)**: The median, represented by a horizontal line inside the box,
    indicates the central value of the dataset. It divides the data into two equal
    halves, with 50% of the data points below and 50% above the median.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中位数（Q2）**：中位数，由箱内的水平线表示，指示数据集的中心值。它将数据分为两个相等的部分，其中50%的数据点低于中位数，50%的数据点高于中位数。'
- en: '**Whiskers**: Whiskers extend from the edges of the box to the furthest data
    points that lie within the “whisker length.” The length of the whiskers is typically
    determined by a factor (for example, 1.5 times the IQR) and is used to identify
    potential outliers.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**须**：须从箱的边缘延伸到位于“须长度”内的最远数据点。须的长度通常由一个因子（例如，IQR的1.5倍）确定，并用于识别潜在的异常值。'
- en: '**Outliers**: Data points lying beyond whiskers are considered outliers and
    are usually plotted individually as individual points or circles. They are data
    points that deviate significantly from the central distribution and may warrant
    further investigation.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值**：位于须外的数据点被认为是异常值，通常单独作为单独的点或圆圈绘制。它们是显著偏离中心分布的数据点，可能需要进一步调查。'
- en: 'The following are the benefits of box plots:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些箱线图的优点：
- en: '**Visualizing data distribution**: Box plots offer an intuitive way to see
    the spread and skewness of the data, as well as identify any potential data clusters
    or gaps'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化数据分布**：箱线图提供了一种直观的方式来查看数据的分布和偏斜，以及识别任何潜在的数据簇或缺口。'
- en: '**Comparing datasets**: Box plots are useful for comparing multiple datasets
    side by side, allowing for easy comparisons of central tendencies and variabilities'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**比较数据集**：箱线图在并排比较多个数据集时很有用，允许轻松比较中心趋势和变异性。'
- en: '**Outlier detection**: Box plots facilitate outlier detection by highlighting
    data points that lie beyond whiskers, helping identify unusual or extreme values'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值检测**：箱线图通过突出显示位于须外的数据点来促进异常值检测，有助于识别异常或极端值。'
- en: '**Handling skewed data**: Box plots are robust to the influence of extreme
    values and can handle skewed data distributions more effectively than traditional
    mean and standard deviation'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理偏斜数据**：箱线图对极端值的影响具有鲁棒性，并且比传统的均值和标准差更有效地处理偏斜数据分布。'
- en: 'Let’s implement this approach using the `matplotlib` library in Python. Here,
    we will produce two box plots for `ApplicantIncome` and `LoanAmount`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Python中的`matplotlib`库来实现这种方法。在这里，我们将为`ApplicantIncome`和`LoanAmount`生成两个箱线图：
- en: '[PRE4]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are the plot outputs. We see that there are some possible outliers in
    these columns:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是绘图输出。我们看到在这些列中存在一些可能的异常值：
- en: '![Figure 9.4 – Box plots for ApplicantIncome (top) and LoanAmount (bottom)](img/B19297_09_4.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – ApplicantIncome（顶部）和LoanAmount（底部）的箱线图](img/B19297_09_4.jpg)'
- en: Figure 9.4 – Box plots for ApplicantIncome (top) and LoanAmount (bottom)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – ApplicantIncome（顶部）和LoanAmount（底部）的箱线图
- en: In summarizing the use of box plots, we find them to be a powerful visual aid,
    complementing IQR in portraying both central tendencies and data dispersion. While
    box plots excel in providing a holistic view, it’s crucial to acknowledge that
    they may not capture all nuances of complex datasets. In the next section on scatter
    plots, we will explore a versatile graphical tool that offers a broader perspective,
    facilitating the identification of relationships and patterns between variables
    in our data.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 总结箱线图的使用，我们发现它们是一种强大的视觉辅助工具，与四分位数范围（IQR）一起描绘了数据的中心趋势和数据分散。虽然箱线图在提供整体视图方面表现出色，但重要的是要认识到它们可能无法捕捉到复杂数据集的所有细微差别。在下一节中，我们将探讨一种多功能的图形工具，它提供了一个更广阔的视角，有助于识别数据中变量之间的关系和模式。
- en: Scatter plots
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 散点图
- en: Scatter plots are a popular and versatile data visualization technique used
    to explore the relationship between two continuous numerical variables. They provide
    a clear visual representation of how one variable (the independent variable) affects
    or influences another (the dependent variable). Scatter plots are especially useful
    in identifying patterns, correlations, clusters, and outliers in data, making
    them an essential tool in data analysis and **exploratory data** **analysis**
    (**EDA**).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图是一种流行且多功能的数据可视化技术，用于探索两个连续数值变量之间的关系。它们提供了一个清晰的视觉表示，说明一个变量（自变量）如何影响或影响另一个变量（因变量）。散点图在识别数据中的模式、相关性、簇和异常值方面特别有用，是数据分析中**探索性数据分析**（EDA）的一个基本工具。
- en: Let’s now look at how scatter plots are constructed and their key characteristics.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨散点图的构建及其关键特征。
- en: Scatter plot construction
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 散点图构建
- en: To create a scatter plot, the values of two numerical variables are plotted
    as points on a Cartesian coordinate system. Each point represents a data observation,
    where the *x* coordinate corresponds to the value of the independent variable
    and the *y* coordinate corresponds to the value of the dependent variable. Multiple
    data points collectively form a scatter plot that provides insights into the relationship
    between the two variables.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建散点图，需要将两个数值变量的值绘制在笛卡尔坐标系上的点。每个点代表一个数据观测值，其中*x*坐标对应自变量的值，*y*坐标对应因变量的值。多个数据点共同形成一个散点图，可以提供关于两个变量之间关系的见解。
- en: Key characteristics of scatter plots
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 散点图的关键特征
- en: 'The following are some key characteristics of scatter plots:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些散点图的关键特征：
- en: '**Correlation**: Scatter plots help us assess the correlation or relationship
    between two variables. If points on the plot appear to form a clear trend or pattern
    (e.g., a linear or non-linear trend), it suggests a significant correlation between
    the variables. If points are scattered randomly, there might be no or weak correlation.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相关性**：散点图帮助我们评估两个变量之间的相关性或关系。如果图上的点似乎形成了一个清晰的趋势或模式（例如，线性或非线性趋势），则表明变量之间存在显著的相关性。如果点分布随机，可能没有或相关性较弱。'
- en: '**Cluster analysis**: Scatter plots can reveal clusters of data points, indicating
    potential subgroups or patterns within the data.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类分析**：散点图可以揭示数据点的簇，表明数据中可能存在的子组或模式。'
- en: '**Outlier detection**: Scatter plots facilitate outlier detection by identifying
    data points that lie far away from the main cluster of points.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值检测**：散点图通过识别远离主要数据点群的数据点来促进异常值检测。'
- en: '**Data spread**: The spread or distribution of data points along the *x* and
    *y* axes provides insights into the variability of the variables.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分布**：数据点在*x*轴和*y*轴上的分布提供了关于变量变异性的见解。'
- en: '**Visualizing regression lines**: In some cases, a regression line can be fitted
    to the scatter plot to model the relationship between the variables and make predictions.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化回归线**：在某些情况下，可以将回归线拟合到散点图上，以模拟变量之间的关系并做出预测。'
- en: 'Let’s implement this in Python on the `ApplicantIncome` and `LoanAmount` columns:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Python的`ApplicantIncome`和`LoanAmount`列上实现这一功能：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the output showcasing the scatter plot results:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是展示散点图结果的输出：
- en: '![Figure 9.5 – Scatter plot showcasing ApplicantIncome and LoanAmount](img/B19297_09_5.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 展示ApplicantIncome和LoanAmount的散点图](img/B19297_09_5.jpg)'
- en: Figure 9.5 – Scatter plot showcasing ApplicantIncome and LoanAmount
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 展示申请收入和贷款金额的散点图
- en: As you can see, some points are notably distant from the majority of the population,
    suggesting the presence of potential outliers. These outlying data points stand
    apart from the overall pattern, warranting further investigation to understand
    their significance and impact on the relationship between the two variables. Identifying
    and handling outliers is crucial for ensuring accurate data analysis and model
    performance. By visualizing the scatter plot, we can gain valuable insights into
    data distribution and correlations, paving the way for effective decision-making
    and data exploration.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，一些点明显偏离了大多数人群，这表明可能存在潜在的异常值。这些异常数据点与整体模式相区别，需要进一步调查以了解其重要性和对两个变量之间关系的影响。识别和处理异常值对于确保准确的数据分析和模型性能至关重要。通过可视化散点图，我们可以获得关于数据分布和关联的有价值见解，为有效的决策和数据探索铺平道路。
- en: Anomaly detection
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常检测
- en: 'Anomaly detection is a specific approach to detecting rare events, where the
    focus is on identifying instances that significantly deviate from the norm or
    normal behavior. Anomalies can be caused by rare events, errors, or unusual patterns
    that are not typical in the dataset. This technique is particularly useful when
    there is limited or no labeled data for rare events. Common anomaly detection
    algorithms include the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测是检测罕见事件的一种特定方法，其重点是识别与规范或正常行为显著偏离的实例。异常可能由罕见事件、错误或数据集中不典型的异常模式引起。当罕见事件的数据有限或没有标记数据时，这种技术特别有用。常见的异常检测算法包括以下：
- en: '**Unsupervised methods**: Techniques such as Isolation Forest and **One-Class
    SVM**) can be used to identify anomalies in data without requiring labeled examples
    of the rare event.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督方法**：例如隔离森林和**单类SVM**等技术可以在不要求罕见事件的标记示例的情况下用于识别数据中的异常。'
- en: '**Semi-supervised methods**: These approaches combine normal and abnormal data
    during training but have only a limited number of labeled anomalies. Autoencoders
    and variational autoencoders are examples of semi-supervised anomaly detection
    algorithms.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督方法**：这些方法在训练期间结合正常和异常数据，但只有有限数量的标记异常。自编码器和变分自编码器是半监督异常检测算法的例子。'
- en: '**Supervised methods**: If a small number of labeled anomalies are available,
    **supervised learning** algorithms such as Random Forest, **Support Vector Machines**
    (**SVM**), and neural networks can be used for anomaly detection.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督方法**：如果可用少量标记的异常，可以使用**监督学习**算法，如随机森林、**支持向量机**（SVM）和神经网络进行异常检测。'
- en: Let’s understand these methods in detail with Python code examples.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过Python代码示例详细了解这些方法。
- en: Unsupervised method using Isolation Forest
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用隔离森林的无监督方法
- en: 'Isolation Forest is an efficient and effective algorithm used for anomaly detection
    in **unsupervised learning** scenarios. It works by isolating anomalies or rare
    events in the data by constructing isolation trees (random decision trees) that
    separate the anomalies from the majority of the normal data points. It was introduced
    by Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou in their 2008 paper titled *Isolation
    Forest*. Here are some key concepts and features of the Isolation Forest algorithm:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 隔离森林是一种用于无监督学习场景中异常检测的高效且有效的算法。它通过构建隔离树（随机决策树）来隔离数据中的异常或罕见事件，这些树将异常与大多数正常数据点分开。它是由Fei
    Tony Liu、Kai Ming Ting和Zhi-Hua Zhou在2008年发表的论文《Isolation Forest》中引入的。以下是隔离森林算法的一些关键概念和特性：
- en: '**Random partitioning**: Isolation Forest uses a random partitioning strategy
    to create isolation trees. At each step of constructing a tree, a random feature
    is selected, and a random split value within the range of the selected feature’s
    values is chosen to create a node. This random partitioning leads to shorter paths
    for anomalies, making them easier to isolate from normal data points.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机划分**：隔离森林使用随机划分策略来创建隔离树。在构建树的每个步骤中，选择一个随机特征，并在所选特征的值范围内选择一个随机分割值来创建一个节点。这种随机划分导致异常的路径变短，使得它们更容易从正常数据点中隔离出来。'
- en: '**Path length**: The key idea behind Isolation Forest is that anomalies are
    isolated into smaller partitions with fewer data points, while normal data points
    are distributed more uniformly across larger partitions. The average path length
    of a data point to reach an anomaly in a tree is used as a measure of its “isolation.”'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anomaly score**: Based on the average path length, each data point is assigned
    an anomaly score. The anomaly score represents how easily the data point can be
    isolated or separated from the rest of the data. Shorter average path lengths
    correspond to higher anomaly scores, indicating that the data point is more likely
    to be an anomaly.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`auto`,” which estimates the contamination based on the dataset’s size.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A few advantages of Isolation Forest are listed here:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Isolation Forest is computationally efficient and scalable, making it suitable
    for large datasets. It does not require a large number of trees to achieve good
    performance, reducing the computational overhead.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm is relatively insensitive to the number of dimensions/features,
    which is particularly advantageous when dealing with high-dimensional datasets.
    Isolation Forest is an unsupervised learning algorithm, making it suitable for
    scenarios where labeled anomaly data is scarce or unavailable.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are, however, some limitations of Isolation Forest:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Isolation Forest may not perform well on datasets with multiple clusters of
    anomalies or when anomalies are close to the majority of normal data points.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with most unsupervised algorithms, Isolation Forest may produce false positives
    (normal data points misclassified as anomalies) and false negatives (anomalies
    misclassified as normal data points).
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement this approach in Python with the following steps:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas` library is imported as `pd` to handle data in tabular format, and
    `IsolationForest` is imported from the `sklearn.ensemble` module for performing
    anomaly detection using the Isolation Forest algorithm:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`numerical_features` list is defined, containing the names of the numerical
    columns to be used for anomaly detection. These columns are `''ApplicantIncome''`,
    `''CoapplicantIncome''`, and `''LoanAmount''`:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '`X_anomaly` DataFrame is created by extracting the columns specified in `numerical_features`
    from the original `df` DataFrame. This new DataFrame will be used for anomaly
    detection:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`X_anomaly` DataFrame, the `fillna()` method is used with the mean of each
    column. This ensures that any missing values are replaced with the mean value
    of their respective columns:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`IsolationForest(contamination=''auto'', random_state=42)`. The `contamination`
    parameter is set to `''auto''`, which means it will automatically detect the percentage
    of outliers in the dataset. The `random_state` parameter is set to `42` to ensure
    reproducibility:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`X_anomaly` data using the `fit_predict()` method. This method simultaneously
    fits the model to the data and predicts whether each data point is an outlier
    or not. The predictions are stored in the `anomaly_predictions` array.'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`anomaly_predictions` array contains predicted labels for each data point:
    `-1` for anomalies (outliers) and `1` for inliers (non-outliers). These predictions
    are added as a new `''IsAnomaly''` column to the original `df` DataFrame.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df` DataFrame where `IsAnomaly` is equal to -1, indicating the presence of
    outliers. The resulting DataFrame contains all rows with anomalies, which can
    then be further analyzed or processed as needed:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now view the DataFrame with rows that the model has predicted as anomalies:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – The resulting anomalies DataFrame](img/B19297_09_6.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – The resulting anomalies DataFrame
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Isolation Forest stands out as a powerful and efficient tool
    for anomaly detection, particularly in scenarios where anomalies are rare and
    distinctly different from normal instances. Its ability to isolate anomalies through
    the creation of random trees makes it a valuable asset in various applications,
    from fraud detection to network security. However, it’s essential to acknowledge
    the algorithm’s limits. Isolation Forest might face challenges when anomalies
    are not well separated or when datasets are highly dimensional. In the next section,
    we will explore autoencoders – a semi-supervised method for anomaly detection.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised methods using autoencoders
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anomaly detection using autoencoders is an unsupervised learning approach that
    leverages neural networks (NNs) to detect anomalies in data. Autoencoders are
    a type of NN architecture designed to reconstruct the input data from a compressed
    representation. In anomaly detection, we exploit the fact that autoencoders struggle
    to reconstruct anomalous instances, making them useful for identifying unusual
    patterns or outliers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Autoencoders consist of two main components: an encoder and a decoder. The
    encoder compresses the input data into a lower-dimensional representation called
    the “latent space,” while the decoder tries to reconstruct the original input
    from this representation. The encoder and decoder are typically symmetric, and
    the network is trained to minimize reconstruction errors.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: In anomaly detection, we train the autoencoder on normal data without anomalies.
    Since the autoencoder learns to reconstruct normal data, it will be less capable
    of reconstructing anomalies, leading to higher reconstruction errors for anomalous
    instances. This property allows us to use the reconstruction error as an anomaly
    score.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: During training, we compare the original input (for example, numerical features)
    to the reconstructed output. The difference between the two is the reconstruction
    error. A low reconstruction error indicates that the input is close to the normal
    data distribution, while a high reconstruction error suggests that the input is
    likely an anomaly.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们比较原始输入（例如，数值特征）与重建输出。两者之间的差异是重建误差。低重建误差表明输入接近正常数据分布，而高重建误差则表明输入可能是异常。
- en: After training the autoencoder, we need to set a threshold to distinguish between
    normal and anomalous instances based on the reconstruction error. There are several
    methods to set the threshold, such as percentile-based or using validation data.
    The threshold will depend on the desired trade-off between false positives and
    false negatives, which can be adjusted based on the application’s requirements.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练自动编码器后，我们需要设置一个阈值来区分正常和异常实例，基于重建误差。有几种方法可以设置阈值，例如基于百分位数或使用验证数据。阈值将取决于所需的假阳性与假阴性之间的权衡，这可以根据应用程序的要求进行调整。
- en: Autoencoders are flexible and can capture complex patterns in the data, making
    them suitable for high-dimensional data with non-linear relationships. They can
    handle both global and local anomalies, meaning they can detect anomalies that
    differ from the majority of data points and anomalies within specific regions
    of the data. Autoencoders are capable of unsupervised learning, which is advantageous
    when labeled anomaly data is limited or unavailable. As with other unsupervised
    methods, autoencoders may produce false positives (normal data misclassified as
    anomalies) and false negatives (anomalies misclassified as normal data). They
    may struggle to detect anomalies that are very similar to the normal data, as
    the reconstruction error might not be significantly different.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器灵活且能够捕捉数据中的复杂模式，这使得它们适用于具有非线性关系的多维数据。它们可以处理全局和局部异常，这意味着它们可以检测与大多数数据点不同的异常以及数据特定区域内的异常。自动编码器能够进行无监督学习，这在标记异常数据有限或不可用的情况下具有优势。与其他无监督方法一样，自动编码器可能会产生假阳性（将正常数据错误地分类为异常）和假阴性（将异常数据错误地分类为正常数据）。它们可能难以检测与正常数据非常相似的异常，因为重建误差可能没有显著差异。
- en: 'Let’s see an example implementation of this approach using the TensorFlow library
    and the Loan Prediction dataset in Python:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用 TensorFlow 库和 Python 中的 Loan Prediction 数据集来查看这种方法的一个示例实现：
- en: '`train_loan_prediction.csv` file using `pd.read_csv()`:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `pd.read_csv()` 读取 `train_loan_prediction.csv` 文件：
- en: '[PRE12]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`numerical_features` list containing the names of the numerical columns to
    be used for anomaly detection. These columns are `''ApplicantIncome''`, `''CoapplicantIncome''`,
    and `''LoanAmount''`:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含用于异常检测的数值列名称的 `numerical_features` 列表。这些列是 `'ApplicantIncome'`、`'CoapplicantIncome'`
    和 `'LoanAmount'`：
- en: '[PRE13]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`X_anomaly` DataFrame is created by extracting the columns specified in `numerical_features`
    from the original `df` DataFrame. This new DataFrame will be used for anomaly
    detection.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过从原始 `df` DataFrame 中提取 `numerical_features` 中指定的列创建 `X_anomaly` DataFrame。这个新的
    DataFrame 将用于异常检测。
- en: '`X_anomaly` DataFrame are replaced with the mean value of their respective
    columns using the `fillna()` method:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `fillna()` 方法将 `X_anomaly` DataFrame 中的列替换为其各自列的平均值：
- en: '[PRE14]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`X_anomaly` are standardized using `StandardScaler()`. Standardization scales
    the features to have zero mean and unit variance, which is important for training
    machine learning models:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `StandardScaler()` 对 `X_anomaly` 进行标准化。标准化将特征缩放到具有零均值和单位方差，这对于训练机器学习模型很重要：
- en: '[PRE15]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`X_train`) and testing (`X_test`) sets using the `train_test_split()` function.
    The original indices of the data are also stored in `original_indices`:'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `train_test_split()` 函数对 (`X_train`) 和测试 (`X_test`) 集进行划分。数据原始索引也存储在 `original_indices`
    中：
- en: '[PRE16]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`Dense` layers. The model is trained using the `fit()` method, with the **mean
    squared error** (**MSE**) as the loss function:'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Dense` 层。模型使用 `fit()` 方法进行训练，**均方误差**（**MSE**）作为损失函数：'
- en: '[PRE17]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`X_test`, and reconstruction errors are calculated as the mean squared difference
    between the original and reconstructed data:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_test`，并计算重建误差为原始数据和重建数据之间的均方差异：'
- en: '[PRE18]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`X_test`:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_test`:'
- en: '[PRE19]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`1` in `anomaly_predictions`; otherwise, it is assigned `0`:'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 `anomaly_predictions` 中的值为 `1`，否则将其分配为 `0`：
- en: '[PRE20]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`anomaly_df` DataFrame is created with the anomaly predictions and the corresponding
    index from `X_test_df`:'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用 `anomaly_df` DataFrame 创建包含异常预测和对应索引的 `X_test_df`:'
- en: '[PRE21]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`df` DataFrame using the `merge()` method, adding the `''IsAnomaly''` column
    to `df`.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''IsAnomaly''` column is present in `df`. If present, it displays rows where
    `''IsAnomaly''` is equal to `1`, indicating the presence of anomalies. If not
    present, it prints `"No` `anomalies detected."`:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting DataFrame is as follows:'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.7 – The resulting IsAnomaly DataFrame](img/B19297_09_7.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – The resulting IsAnomaly DataFrame
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: In summary, autoencoders prove to be a versatile and powerful tool for anomaly
    detection, capturing nuanced patterns that may elude traditional methods. Their
    ability to discover subtle anomalies within complex data structures makes them
    invaluable in diverse domains, including image analysis, cybersecurity, and industrial
    quality control.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: However, the effectiveness of autoencoders is contingent on various factors.
    The architecture’s complexity and the selection of hyperparameters can influence
    performance, requiring careful tuning for optimal results. In the next section,
    we will understand how SVMs can be used in anomaly detection.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Supervised methods using SVMs
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SVMs are a powerful class of supervised learning algorithms commonly used for
    classification tasks. When applied to anomaly detection, SVMs prove to be effective
    in separating normal instances from anomalies by finding a hyperplane with a maximum
    margin. Here is how SVMs work under the hood:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '**Hyperplane definition**: In a two-dimensional space, a hyperplane is a flat,
    two-dimensional subspace. SVM aims to find a hyperplane that best separates the
    dataset into two classes — normal and anomalous. This hyperplane is positioned
    to maximize the margin, which is the distance between the hyperplane and the nearest
    data points of each class.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decision boundary**: The hyperplane serves as a decision boundary that separates
    instances of one class from another. In a binary classification scenario, instances
    on one side of the hyperplane are classified as belonging to one class, and those
    on the other side are classified as belonging to the other class.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kernel trick**: SVM can handle complex relationships in the data through
    the use of a kernel function. In many real-world scenarios, the relationship between
    features may not be linear. SVM addresses this by using a kernel function. This
    function transforms the input data into a higher-dimensional space, making it
    easier to find a hyperplane that effectively separates the classes. Commonly used
    kernel functions include the linear kernel (for linearly separable data), polynomial
    kernel, **radial basis function** (**RBF**) or Gaussian kernel, and sigmoid kernel.
    The choice of kernel depends on the nature of the data.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimal hyperplane**: SVM aims to find the hyperplane that maximizes the
    margin, which is the distance between the hyperplane and the nearest data points
    of each class. The larger the margin, the more robust and generalizable the model
    is likely to be. Support vectors are data points that lie closest to the decision
    boundary. They play a crucial role in defining the optimal hyperplane and the
    margin. SVM focuses on these support vectors during training.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement a Python example of SVM in anomaly detection using our Loan
    Prediction dataset:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '`train_loan_prediction.csv` file using `pd.read_csv()`:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**Data preprocessing**: We will do some basic data preprocessing tasks, including
    handling missing values. For this anomaly detection example, we simplify the analysis
    by excluding categorical variables. In a more complex analysis, you might choose
    to encode and include these variables if they are deemed relevant to your specific
    anomaly detection task:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Create a train-test split for the SVM model:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Standardize the features using `StandardScaler` from scikit-learn to ensure
    that all features have the same scale:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Train the One-Class SVM model for anomaly detection. Adjust the *nu* parameter
    based on the expected proportion of outliers in your dataset. The *nu* parameter
    represents an upper bound on the fraction of margin errors and a lower bound on
    the fraction of support vectors. It essentially controls the proportion of outliers
    or anomalies the algorithm should consider. Choosing an appropriate value for
    *nu* is crucial, and it depends on the characteristics of your dataset and the
    expected proportion of anomalies. Here are some guidelines to help you select
    the *nu* parameter:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Understand the nature of anomalies**: Assess the domain knowledge and characteristics
    of your dataset. Understand the expected proportion of anomalies. If anomalies
    are rare, a smaller value of *nu* might be appropriate.'
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experiment with a range of values**: Start by experimenting with a range
    of *nu* values, such as 0.01, 0.05, 0.1, 0.2, and so on. You can adjust this range
    based on your understanding of the data.'
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consider the dataset size**: The size of your dataset can also influence
    the choice of *nu*. For larger datasets, a smaller value might be suitable, while
    for smaller datasets, a relatively larger value may be appropriate.'
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Balance false positives and false negatives**: Depending on the application,
    you might prioritize minimizing false positives or false negatives. Adjust *nu*
    accordingly to achieve the desired balance.'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will implement an experiment with a range of values for *nu*. We will specify
    a list of *nu* values that we want to experiment with. These values represent
    the upper bound of the fraction of margin errors and the lower bound of the fraction
    of support vectors in the One-Class SVM model. We then create an empty list to
    store the mean decision function values for each *nu* value:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'For each *nu* value in the list, train a One-Class SVM model with that *nu*
    value. Retrieve the decision function values for the test set and calculate the
    mean decision function value. Append the mean decision function value to the list:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Identify the index of the *nu* value that corresponds to the highest mean decision
    function value. Then, retrieve the best *nu* value:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Create a final One-Class SVM model using the best *nu* value and train it on
    the scaled training data:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We now use this model to predict anomalies on the `X_test_scaled` test dataset.
    This line creates a binary representation of the predictions (`y_pred`) by mapping
    -1 to 1 (indicating anomalies) and any other value (typically 1) to 0 (indicating
    normal instances). This is done because the One-Class SVM model often assigns
    -1 to anomalies and 1 to normal instances. We will store this in a new DataFrame
    as `df_with_anomalies`:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Print the confusion matrix and accuracy score:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This will print the following report:'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Output classification report](img/B19297_09_8.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Output classification report
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Print DataFrame rows predicted as anomalies:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This will output the following DataFrame:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.9 – The df_with_anomalies DataFrame](img/B19297_09_9.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – The df_with_anomalies DataFrame
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve walked through the process of implementing anomaly detection
    using SVM in Python. Anomaly detection using SVM can be adapted for various datasets
    with clear anomalies, making it a valuable tool for outlier identification. In
    the next section, we will explore data augmentation and resampling techniques
    for identifying edge cases and rare events.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation and resampling techniques
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Class imbalance is a common issue in datasets with rare events. Class imbalance
    can adversely affect the model’s performance, as the model tends to be biased
    toward the majority class. To address this, we will explore two resampling techniques:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '**Oversampling**: Increasing the number of instances in the minority class
    by generating synthetic samples'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Undersampling**: Reducing the number of instances in the majority class to
    balance class distribution'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss these resampling techniques in more detail.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling using SMOTE
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Synthetic Minority Over-sampling TEchnique** (**SMOTE**) is a widely used
    resampling method for addressing class imbalance in machine learning datasets,
    especially when dealing with rare events or minority classes. SMOTE helps to generate
    synthetic samples for the minority class by interpolating between existing minority
    class samples. This technique aims to balance class distribution by creating additional
    synthetic instances, thereby mitigating the effects of class imbalance. In a dataset
    with class imbalance, the minority class contains significantly fewer instances
    than the majority class. This can lead to biased model training, where the model
    tends to favor the majority class and performs poorly on the minority class.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the key steps of a SMOTE algorithm:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying minority class instances**: The first step of SMOTE is to identify
    instances belonging to the minority class.'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Selecting nearest neighbors**: For each minority class instance, SMOTE selects
    its *k* nearest neighbors (commonly chosen through the **k-nearest neighbors**
    (**KNN**) algorithm). These neighbors are used to create synthetic samples.'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Creating synthetic samples**: For each minority class instance, SMOTE generates
    synthetic samples along the line connecting the instance to its *k* nearest neighbors
    in the feature space. Synthetic samples are created by adding a random fraction
    (usually between 0 and 1) of the feature differences between the instance and
    its neighbors. This process effectively introduces variability to synthetic samples.'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Combining with the original data**: The synthetic samples are combined with
    the original minority class instances, resulting in a resampled dataset with a
    more balanced class distribution.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: SMOTE helps to address class imbalance without discarding any data, as it generates
    synthetic samples rather than removing instances from the majority class. It increases
    the information available to the model, potentially improving the model’s ability
    to generalize to the minority class. SMOTE is straightforward to implement and
    is available in popular libraries such as imbalanced-learn in Python.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: While SMOTE is effective in many cases, it might not always perform optimally
    for highly imbalanced datasets or datasets with complex decision boundaries. Generating
    too many synthetic samples can lead to overfitting on the training data, so it
    is crucial to choose an appropriate value for the number of nearest neighbors
    (*k*). SMOTE may introduce some noise and may not be as effective if the minority
    class is too sparse or scattered in the feature space. SMOTE can be combined with
    other techniques, such as undersampling the majority class or using different
    resampling ratios, to achieve better performance. It is essential to evaluate
    the model’s performance on appropriate metrics (for example, precision, recall,
    or F1-score) to assess the impact of SMOTE and other techniques on the model’s
    ability to detect rare events.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s implement this approach in Python:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '`train_loan_prediction.csv` file using `pd.read_csv()`:'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`''Loan_Status''` column in the dataset contains `''Y''` and `''N''` categorical
    values, which represent loan approval (`''Y''`) and rejection (`''N''`). To convert
    this categorical target variable into a numerical format, `''Y''` is mapped to
    `1` and `''N''` is mapped to `0` using the `map()` function:'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '`fillna()` method. This ensures that any missing values in the dataset are
    replaced with the mean value of their respective columns:'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '`X` feature set. The `select_dtypes()` method is used to include only columns
    with `float` and `int` data types while excluding non-numerical columns such as
    `''Loan_Status''` and `''Loan_ID''`. The `y` target variable is set to `''Loan_Status''`:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`X_train`, `y_train`) and testing (`X_test`, `y_test`) sets using the `train_test_split()`
    function from scikit-learn. The training set consists of 80% of the data, while
    the testing set contains 20% of the data. The `random_state` parameter is set
    to `42` to ensure reproducibility:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '`SMOTE(random_state=42`). SMOTE is then applied to the training data using
    the `fit_resample()` method. This method oversamples the minority class (loan
    rejection) by generating synthetic samples, creating a balanced dataset. The resulting
    resampled data is stored in `X_train_resampled` and `y_train_resampled`:'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '`RandomForestClassifier(random_state=42)`. The classifier is trained on the
    resampled data (`X_train_resampled`, `y_train_resampled`) using the `fit()` method.
    The trained classifier is used to make predictions on the test data (`X_test`)
    using the `predict()` method. Predictions are stored in `y_pred`:'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'A classification report is generated using the `classification_report()` function
    from scikit-learn. The classification report provides precision, recall, F1-score,
    and support for each class (loan approval and rejection) based on predictions
    (`y_pred`) and true labels (`y_test`) from the test set. The classification report
    is initially returned in a dictionary format. The code converts this dictionary
    to a `clf_report` DataFrame using `pd.DataFrame()`, making it easier to work with
    the data. The `clf_report` DataFrame is transposed using the `.T` attribute to
    have classes (`0` and `1`) as rows and evaluation metrics (precision, recall,
    F1-score, and support) as columns. This transposition provides a more convenient
    and readable format for further analysis or presentation:'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s print the classification report to understand how good the model is:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Classification report generated by the preceding code](img/B19297_09_10.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Classification report generated by the preceding code
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: The report indicates that the model performs moderately well in identifying
    instances of class 0, with a precision of 73.33%. However, recall is relatively
    lower at 51.16%, indicating that the model might miss some actual instances of
    class 0\. The model excels in identifying instances of class 1, with a high precision
    of 77.42% and a very high recall of 90.00%. The weighted average metrics consider
    the class imbalance, providing a balanced evaluation across both classes. The
    overall accuracy of the model is 76.42%, indicating the percentage of correctly
    predicted instances.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s explore undersampling – another method to address
    class imbalance problems in machine learning.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling using RandomUnderSampler
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Handling class imbalance with `RandomUnderSampler` is an effective approach
    to address the challenge of imbalanced datasets, where one class significantly
    outweighs the other class(es). In such cases, traditional machine learning algorithms
    may struggle to learn from the data and tend to be biased toward the majority
    class, leading to poor performance on the minority class or rare events.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` is a resampling technique that aims to balance class distribution
    by randomly removing instances from the majority class until class proportions
    become more balanced. By reducing the number of instances in the majority class,
    `RandomUnderSampler` ensures that the minority class is represented more proportionately,
    making it easier for the model to detect and learn patterns related to rare events.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some key points about handling class imbalance with `RandomUnderSampler`:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` is a type of data-level resampling method. Data-level
    resampling techniques involve manipulating the training data to balance class
    distribution. In `RandomUnderSampler`, instances from the majority class are randomly
    selected and removed, resulting in a smaller dataset with a balanced class distribution.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` directly removes instances from the majority class without
    altering minority class instances. This approach helps preserve information from
    the minority class, making it easier for the model to focus on learning patterns
    associated with rare events.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` is the loss of information from the majority class. By
    removing instances randomly, some informative instances may be discarded, potentially
    leading to a reduction in the model’s ability to generalize on the majority class.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` is computationally efficient since it simply involves
    randomly removing instances from the majority class. This makes it faster compared
    to some other resampling methods.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` can be effective in certain scenarios, it might not always
    be the best choice, especially if the majority class contains important patterns
    and information. Careful consideration of the problem and dataset characteristics
    is crucial when selecting the appropriate resampling technique.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` can be used in combination with other techniques. For
    example, one can apply `RandomUnderSampler` first and then use `RandomOverSampler`
    (oversampling) to further balance class distribution. This approach helps in achieving
    a more balanced representation of both classes.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Evaluation and model selection**: When handling class imbalance, it is essential
    to evaluate the model’s performance on relevant metrics such as precision, recall,
    F1-score, and **Area Under the ROC Curve** (**AUC-ROC**). These metrics provide
    a comprehensive assessment of the model’s ability to handle rare events and edge
    cases.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s implement this approach using Python:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '`train_loan_prediction.csv` file using `pd.read_csv()`:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The `''Loan_Status''` column in the dataset contains `''Y''` and `''N''` categorical
    values, which represent loan approval (`''Y''`) and rejection (`''N''`). To convert
    this categorical target variable into numerical format, `''Y''` is mapped to 1
    and `''N''` is mapped to 0 using the `map()` function:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The following code applies mean imputation to all columns with missing values
    in the dataset using the `fillna()` method. This ensures that any missing values
    in the dataset are replaced with the mean value of their respective columns:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The code selects only numerical columns from the dataset to build the `X` feature
    set. The `select_dtypes()` method is used to include only columns with data types
    `float` and `int` data types while excluding non-numerical columns such as `''Loan_Status''`
    and `''Loan_ID''`. The `y` target variable is set to `''Loan_Status''`:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The data is split into training (`X_train`, `y_train`) and testing (`X_test`,
    `y_test`) sets using the `train_test_split()` function from scikit-learn. The
    training set consists of 80% of the data, while the testing set contains 20% of
    the data. The `random_state` parameter is set to `42` to ensure reproducibility:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We then instantiate `RandomUnderSampler` and apply it to the training data
    using the `fit_resample()` method. This method undersamples the majority class
    (loan approval) to create a balanced dataset. The resulting resampled data is
    stored in `X_train_resampled` and `y_train_resampled`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'A Random Forest classifier is then trained on the resampled data (`X_train_resampled`,
    `y_train_resampled`) using the `fit()` method. The trained classifier is used
    to make predictions on the test data (`X_test`) using the `predict()` method.
    Predictions are stored in `y_pred`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let’s inspect the classification report to assess the model performance:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Classification report of the model’s performance](img/B19297_09_11.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Classification report of the model’s performance
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: The model shows decent performance for both classes, with higher precision,
    recall, and F1-score for class `1` compared to class `0`.The weighted average
    considers the imbalance in class distribution, providing a more representative
    measure of overall performance. The accuracy score of 0.7805% suggests that the
    model correctly predicted the class for approximately 78% of instances in the
    test set.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s understand cost-sensitive learning and explore its
    crucial role in scenarios where rare events bear significant consequences.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Cost-sensitive learning
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cost-sensitive learning is a machine learning approach that takes into account
    costs associated with misclassifications of different classes during the model
    training process. In traditional machine learning, the focus is on maximizing
    overall accuracy, but in many real-world scenarios, misclassifying certain classes
    can have more severe consequences than misclassifying others.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a medical diagnosis application, misdiagnosing a severe disease
    as not present (false negative) could have more significant consequences than
    misdiagnosing a mild condition as present (false positive). In fraud detection,
    incorrectly flagging a legitimate transaction as fraudulent (false positive) might
    inconvenience the customer, while failing to detect actual fraudulent transactions
    (false negative) could lead to significant financial losses.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Cost-sensitive learning addresses these imbalances in costs by assigning different
    misclassification costs to different classes. By incorporating these costs into
    the training process, the model is encouraged to prioritize minimizing the overall
    misclassification cost rather than simply optimizing accuracy.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several approaches to implementing cost-sensitive learning:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '**Modifying loss functions**: The loss function used during model training
    can be modified to incorporate class-specific misclassification costs. The goal
    is to minimize the expected cost, which is a combination of misclassification
    costs and the model’s predictions.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Class weights**: Another approach is to assign higher weights to the minority
    class or the class with higher misclassification costs. This technique can be
    applied to various classifiers, such as decision trees, random forests, and SVMs,
    to emphasize learning from the minority class.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling techniques**: In addition to assigning weights, resampling techniques
    such as oversampling the minority class or undersampling the majority class can
    also be used to balance class distribution and improve the model’s ability to
    learn from rare events.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Threshold adjustment**: By adjusting the classification threshold, we can
    control the trade-off between precision and recall, allowing us to make predictions
    that are more sensitive to the minority class.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble methods**: Ensemble methods such as cost-sensitive boosting combine
    multiple models to focus on hard-to-classify instances and assign higher weights
    to misclassified samples.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost-sensitive learning is especially important in scenarios where the class
    imbalance is severe and the consequences of misclassification are critical. By
    taking into account costs associated with different classes, the model can make
    more informed decisions and improve overall performance in detecting rare events
    and handling edge cases.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that cost-sensitive learning requires careful consideration
    of the cost matrix, as incorrectly specified costs can lead to unintended results.
    Proper validation and evaluation of the model on relevant metrics, considering
    real-world costs, are crucial to ensure the effectiveness and reliability of cost-sensitive
    learning algorithms.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now demonstrate cost-sensitive learning using the Loan Prediction dataset
    in Python:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the required libraries and datasets using `pandas`:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We now need to perform data preprocessing to handle missing values and convert
    target variables to numeric data types:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'For this example, we will use only numeric columns. We will then split the
    dataset into `train` and `test`:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We first calculate the class weights based on the inverse of class frequencies
    in the training data. The higher the frequency of a class, the lower its weight,
    and vice versa. This way, the model assigns higher importance to the minority
    class (rare events) and is more sensitive to its correct prediction:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, we train the Random Forest classifier with the `class_weight` parameter
    set to the calculated class weights. This modification allows the classifier to
    consider the class weights during the training process, effectively implementing
    cost-sensitive learning:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'After training the model, we make predictions on the test data and evaluate
    the classifier’s performance using the classification report, which provides precision,
    recall, F1-score, and support for each class:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let’s view the classification report and assess the Random Forest classifier’s
    performance:'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Classification report of the Random Forest classifier’s performance](img/B19297_09_12.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Classification report of the Random Forest classifier’s performance
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: In cost-sensitive learning, you would typically define a cost matrix that quantifies
    misclassification costs for each class and use it to guide the model’s training.
    The results from the classification report can help you identify areas where adjustments
    may be needed to align the model with specific cost considerations in your application.
    A cost matrix is especially useful in situations where the costs of false positives
    and false negatives are not equal. If the cost of false positives is higher, consider
    raising the decision threshold. If the cost of false negatives is higher, consider
    lowering the threshold.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s understand which evaluation metrics are used for
    detecting edge cases and rare events.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Choosing evaluation metrics
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When dealing with edge cases and rare events in machine learning, selecting
    the right evaluation metrics is crucial to accurately assess the performance of
    the model. Traditional evaluation metrics, such as accuracy, may not be sufficient
    in imbalanced datasets where the class of interest (the rare event) is vastly
    outnumbered by the majority class. In imbalanced datasets, where the rare event
    is a minority class, traditional evaluation metrics such as accuracy can be misleading.
    For instance, if a dataset has 99% of the majority class and only 1% of the rare
    event, a model that predicts all instances as the majority class will still achieve
    an accuracy of 99%, which is deceptively high. However, such a model would be
    ineffective in detecting the rare event. To address this issue, we need evaluation
    metrics that focus on the model’s performance in correctly identifying the rare
    event, even at the expense of a decrease in accuracy.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some evaluation metrics that are more suitable for detecting edge
    cases and rare events:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**: Precision measures the accuracy of positive predictions made
    by the model. It is the ratio of true positive (correctly predicted rare event)
    to the sum of true positive and false positive (incorrectly predicted rare event
    as the majority class). High precision indicates that the model is cautious in
    making positive predictions and has a low false positive rate.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall (sensitivity)**: Recall measures the proportion of true positives
    predicted by the model out of all actual positive instances. It is the ratio of
    true positive to the sum of true positive and false negative (incorrectly predicted
    majority class as the rare event). High recall indicates that the model is capable
    of capturing a significant portion of rare event instances.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1-score**: The F1-score is the harmonic mean of precision and recall. It
    provides a balance between the two metrics and is especially useful when there
    is an imbalance between precision and recall. F1-score penalizes models that prioritize
    either precision or recall at the expense of the other.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Area Under the Receiver Operating Characteristic (ROC-AUC)**: ROC-AUC is
    a performance metric used to evaluate binary classification models. It measures
    the area under the ROC curve, which plots the true positive rate (recall) against
    the false positive rate as the classification threshold changes. A higher ROC-AUC
    indicates better model performance, especially in detecting rare events.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, let’s delve into ensemble techniques and understand their
    crucial role in machine learning models, especially when dealing with data containing
    edge cases and rare events.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble techniques
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensemble techniques are powerful methods used to improve the performance of
    machine learning models, particularly in scenarios with imbalanced datasets, rare
    events, and edge cases. These techniques combine multiple base models to create
    a more robust and accurate final prediction. Let’s discuss some popular ensemble
    techniques.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Bootstrap aggregating** (**bagging**) is an ensemble technique that creates
    multiple bootstrap samples (random subsets with replacement) from the training
    data and trains a separate base model on each sample. The final prediction is
    obtained by averaging or voting the predictions of all base models. Bagging is
    particularly useful when dealing with high variance and complex models, as it
    reduces overfitting and enhances the model’s generalization ability. Here are
    the key concepts associated with bagging:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrap sampling**: The bagging process begins by creating multiple random
    subsets of the training data through a process called bootstrap sampling. Bootstrap
    sampling involves randomly selecting data points from the original dataset with
    replacements. As a result, some data points may appear more than once in a subset,
    while others may be left out.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Base model training**: For each bootstrap sample, a base model (learner)
    is trained independently on that particular subset of the training data. The base
    models can be any machine learning algorithm, such as decision trees, random forests,
    or SVMs.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Aggregating predictions**: Once all base models are trained, they are used
    to make predictions on new, unseen data. For classification tasks, the final prediction
    is typically determined by majority voting, where the class that receives the
    most votes across the base models is chosen. In regression tasks, the final prediction
    is obtained by averaging the predictions from all base models.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few benefits of bagging:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '**Variance reduction**: Bagging helps reduce variance in the model by combining
    predictions from multiple models trained on different subsets of the data. This
    results in a more stable and robust model.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting prevention**: By training each base model on different subsets
    of the data, bagging prevents individual models from overfitting to noise in the
    training set.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model generalization**: Bagging improves the model’s generalization ability
    by reducing bias and variance, leading to better performance on unseen data.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parallelism**: Since the base models are trained independently, bagging is
    amenable to parallel processing, making it computationally efficient.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random Forest is a popular example of the bagging technique. In Random Forest,
    the base models are decision trees, and the predictions from multiple decision
    trees are combined to make the final prediction.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of implementing bagging using Random Forest in Python on
    the Loan Prediction dataset:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In conclusion, bagging techniques offer a robust and effective strategy for
    handling edge cases. By aggregating predictions from multiple base models, bagging
    not only enhances the overall model stability but also fortifies its ability to
    accurately identify and address edge cases, contributing to a more resilient and
    reliable predictive framework.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore boosting, another method to handle edge
    cases and rare events.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boosting is an ensemble technique that builds base models sequentially, with
    each subsequent model focusing on misclassified instances of the previous model.
    It assigns higher weights to misclassified instances, thus giving more attention
    to rare events. Popular boosting algorithms include **Adaptive Boosting** (**AdaBoost**),
    Gradient Boosting, and XGBoost. Boosting aims to create a strong learner by combining
    weak learners iteratively.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how boosting works:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '**Base model training**: Boosting starts by training a base model (also known
    as a weak learner) on the entire training dataset. Weak learners are usually simple
    models with limited predictive power, such as decision stumps (a decision tree
    with a single split).'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Weighted training**: After the first model is trained, data points that were
    misclassified by the model are assigned higher weights. This means that the subsequent
    model will pay more attention to those misclassified data points, attempting to
    correct their predictions.'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterative training**: Boosting follows an iterative approach. For each iteration
    (or boosting round), a new weak learner is trained on the updated training data
    with adjusted weights. The weak learners are then combined to create a strong
    learner, which improves its predictive performance compared to the individual
    weak learners.'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Weighted voting**: During the final prediction, the weak learners’ predictions
    are combined with weighted voting, where models with higher accuracy have more
    influence on the final prediction. This allows the boosting algorithm to focus
    on difficult-to-classify instances and improve the model’s sensitivity to rare
    events.'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The benefits of boosting are as follows:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '**Increased accuracy**: Boosting improves the model’s accuracy by focusing
    on the most challenging instances in the dataset and refining predictions over
    multiple iterations'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustness**: Boosting reduces the model’s sensitivity to noise and outliers
    in the data by iteratively adjusting weights and learning from previous mistakes'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model adaptation**: Boosting adapts well to different types of data and can
    handle complex relationships between features and the target variable'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble diversity**: Boosting creates a diverse ensemble of weak learners,
    which results in better generalization and reduced overfitting'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s look at an example of boosting using AdaBoost.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost is a popular boosting algorithm that is commonly used in practice.
    In AdaBoost, the base models are typically decision stumps, and the model’s weights
    are adjusted after each iteration to emphasize misclassified instances.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of implementing boosting using AdaBoost in Python:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: In summary, the application of boosting techniques emerges as a robust strategy
    for handling edge cases. Through its iterative approach, boosting empowers models
    to focus on instances that pose challenges, ultimately enhancing their ability
    to discern and accurately predict rare events.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore how to use the stacking method to detect and handle edge
    cases and rare events in machine learning.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Stacking
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stacking is an advanced ensemble learning technique that combines the predictions
    of multiple base models by training a meta-model on their outputs. Stacking aims
    to leverage the strengths of different base models to create a more accurate and
    robust final prediction. It is a form of “learning to learn” where the meta-model
    learns how to best combine the predictions of the base models. The base models
    act as “learners,” and their predictions become the input features for the meta-model,
    which makes the final prediction. Stacking can often improve performance by capturing
    complementary patterns from different base models.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the model methodology:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '**Base model training**: The stacking process starts by training multiple diverse
    base models on the training dataset. These base models can be different types
    of machine learning algorithms or even the same algorithm with different hyperparameters.'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Base model predictions**: Once the base models are trained, they are used
    to make predictions on the same training data (in-sample predictions) or a separate
    validation dataset (out-of-sample predictions).'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Meta-model training**: The predictions from the base models are then combined
    to create a new dataset that serves as the input for the meta-model. Each base
    model’s predictions become a new feature in this dataset. The meta-model is trained
    on this new dataset along with the true target labels.'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Final prediction**: During the final prediction phase, the base models make
    predictions on the new, unseen data. These predictions are then used as input
    features for the meta-model, which makes the final prediction.'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Stacking has the following benefits:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '**Improved predictive performance**: Stacking leverages the complementary strengths
    of different base models, potentially leading to better overall predictive performance
    compared to using individual models'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduction of bias and variance**: Stacking can reduce the model’s bias and
    variance by combining multiple models, leading to improved generalization'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Stacking allows the use of diverse base models, making it
    suitable for various types of data and problems'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ensemble diversity**: Stacking creates a diverse ensemble by using various
    base models, which can help prevent overfitting'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s an example of implementing stacking using scikit-learn in Python using
    the Loan Prediction dataset:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the required libraries and load the dataset:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We now need to perform data preprocessing to handle missing values and convert
    target variables to numeric data types:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'For simplicity, we will use only numeric columns for this example. We then
    split the dataset into `train` and `test`:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '`RandomForestClassifier` and `GradientBoostingClassifier`, are instantiated
    with `RandomForestClassifier(random_state=42)` and `GradientBoostingClassifier(random_state=42)`
    respectively:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'These base models are trained on the training data (`X_train`, `y_train`) using
    the `fit()` method, as seen next. The trained base models are used to make predictions
    on the test data (`X_test`) using the `predict()` method. Predictions from both
    base models are stored in `pred_base_model_1` and `pred_base_model_2`:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '`stacking_X_train` dataset is created by combining the predictions from the
    base models (`pred_base_model_1` and `pred_base_model_2`). This new dataset will
    be used as input features for the meta-model:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '`LogisticRegression()`. The meta-model is trained on the new dataset (`stacking_X_train`)
    and the true labels from the test set (`y_test`) using the `fit()` method. The
    meta-model learns to combine predictions of the base models and make the final
    prediction:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '`new_unseen_data`) is created by randomly selecting 20% of the test data (`X_test`)
    using the `sample()` method. The base models are used to make predictions on the
    new, unseen data (`new_unseen_data`) using the `predict()` method. Predictions
    from both base models for the new data are stored in `new_pred_base_model_1` and
    `new_pred_base_model_2`:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '`stacking_new_unseen_data` dataset is created by combining predictions from
    the base models (`new_pred_base_model_1` and `new_pred_base_model_2`) for the
    new, unseen data. This new dataset will be used as input features for the meta-model
    to make the final prediction:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '`stacking_new_unseen_data`) using the `predict()` method. The `final_prediction`
    variable holds the predicted classes (0 or 1) based on the meta-model’s decision:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: In summary, this code demonstrates the concept of stacking, where base models
    (Random Forest and Gradient Boosting) are trained on the original data, their
    predictions are used as input features for a meta-model (Logistic Regression),
    and the final prediction is made using the meta-model on new, unseen data. Stacking
    allows the models to work together and can potentially improve the prediction
    performance compared to using the base models alone.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-391
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the critical aspect of detecting rare events and
    edge cases in machine learning. Rare events, by their infrequency, hold significant
    implications across various domains and necessitate special attention. We delved
    into several techniques and methodologies that equip us to effectively identify
    and handle these uncommon occurrences.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Statistical methods, such as Z-scores and IQR, provide powerful tools to pinpoint
    outliers and anomalies in our data. These methods aid in establishing meaningful
    thresholds for identifying rare events, enabling us to distinguish significant
    data points from noise.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: We also explored machine learning-based anomaly detection techniques, such as
    isolation forest and autoencoders. These methods leverage unsupervised learning
    to identify patterns and deviations that diverge from the majority of the data,
    making them well suited for detecting rare events in complex datasets.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we discussed the significance of resampling methods such as SMOTE
    and `RandomUnderSampler` to tackle class imbalances. These techniques enable us
    to create balanced datasets that enhance the performance of models in identifying
    rare events while preserving data integrity.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we uncovered the potential of ensemble techniques, including stacking,
    bagging, and boosting, in augmenting the capabilities of our models for detecting
    edge cases. The combined power of multiple models through ensembles enhances generalization
    and model robustness.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: It is crucial to select appropriate evaluation metrics, especially in the presence
    of rare events, to ensure fair assessment and accurate model performance evaluation.
    Metrics such as precision, recall, F1-score, and AUC-ROC provide comprehensive
    insights into model performance and guide decision-making.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Detecting rare events and edge cases has far-reaching implications across diverse
    domains, including medical diagnosis, fraud detection, predictive maintenance,
    and environmental monitoring. By employing effective techniques to identify and
    handle these infrequent occurrences, we enhance the reliability and efficiency
    of machine learning applications.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: As we conclude this chapter, let us recognize the significance of this skill
    in real-world scenarios. Detecting rare events empowers us to make informed decisions,
    protect against potential risks, and leverage the full potential of machine learning
    to drive positive impact in a multitude of fields.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore some of the challenges faced by the data-centric
    approach to machine learning.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 4: Getting Started with Data-Centric ML'
  id: totrans-401
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By now you may have realized that shifting to a data-centric approach to ML
    involves not just adapting your own ways of working, but also influencing those
    around you – a task that’s far from simple. In this part, we explore both the
    technical and non-technical hurdles you might encounter during the development
    and deployment of models, and reveal how adopting a data-centric approach can
    aid in overcoming these obstacles.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapter:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B19297_10.xhtml#_idTextAnchor164)*, Kick-Starting Your Journey
    in Data-Centric Machine Learning*'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
