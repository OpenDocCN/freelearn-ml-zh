- en: '*Chapter 16*: K-Means and DBSCAN Clustering'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data clustering allows us to organize unlabeled data into groups of observations
    with more in common with other members of the group than with observations outside
    of the group. There are a surprisingly large number of applications for clustering,
    either as the final model of a machine learning pipeline or as input for another
    model. This includes market research, image processing, and document classification.
    We sometimes also use clustering to improve exploratory data analysis or to create
    more meaningful visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: K-means and **density-based spatial clustering of applications with noise**
    (**DBSCAN**) clustering, like **principal component analysis** (**PCA**), are
    unsupervised learning algorithms. There are no labels to use as the basis for
    predictions. The purpose of the algorithm is to identify instances that hang together
    based on their features. Instances that are in close proximity to each other,
    and further away from other instances, can be considered to be in a cluster. There
    are a number of ways to gauge proximity. **Partition-based clustering**, such
    as k-means, and **density-based clustering**, such as DBSCAN, are two of the more
    popular approaches. We will explore those approaches in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will go over the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The key concepts of k-means and DBSCAN clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing DBSCAN clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will mainly stick to the pandas, NumPy, and scikit-learn libraries in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The key concepts of k-means and DBSCAN clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With k-means clustering, we identify *k* clusters, each with a center, or **centroid**.
    The centroid is the point that minimizes the total squared distance between it
    and the other data points in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: An example with made-up data should help here. The data points in *Figure 16.1*
    seem to be in three clusters. (It is not usually that easy to visualize the number
    of clusters, *k*.)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.1 – Data points with three discernible clusters ](img/B17978_16_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.1 – Data points with three discernible clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'We perform the following steps to construct the clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: Assign a random point as the center of each cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the distance of each point from the center of each cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign data points to a cluster based on their proximity to the center point.
    These first three steps are illustrated in *Figure 16.2*. The points with an **X**
    are the randomly chosen cluster centers (with *k* set at 3). Data points that
    are closer to the cluster center point than to other cluster center points get
    assigned to that cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 16.2 – Random points assigned as the center of the cluster ](img/B17978_16_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.2 – Random points assigned as the center of the cluster
  prefs: []
  type: TYPE_NORMAL
- en: Calculate a new center point for the new cluster. This is illustrated in *Figure
    16.3*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 16.3 – New cluster centers calculated ](img/B17978_16_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.3 – New cluster centers calculated
  prefs: []
  type: TYPE_NORMAL
- en: Repeat steps 2 through 4 until there is not much change in the centers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: K-means clustering is a very popular algorithm for clustering for several reasons.
    It is quite intuitive and typically quite fast. It does have some disadvantages,
    however. It processes every data point as part of a cluster, so the clusters can
    be yanked around by extreme values. It also assumes clusters will have spherical
    shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The evaluation of unsupervised models is less clear than with supervised models,
    as we do not have a target with which to compare our predictions. A fairly common
    metric for clustering models is the **silhouette score**. The silhouette score
    is the mean silhouette coefficient for all instances. The **silhouette coefficient**
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17978_16_0011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B17978_16_002.png) is the mean distance to all instances of the
    next closest cluster for the ith instance, and ![](img/B17978_16_003.png) is the
    mean distance to the instances of the assigned cluster. This coefficient ranges
    from -1 to 1, with scores near 1 meaning that the instance is well within the
    assigned cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Another metric to evaluate our clusters is the **inertia score**. This is the
    sum of squared distances between each instance and its centroid. This distance
    will decrease as we increase the number of clusters but there are eventually diminishing
    marginal returns from increasing the number of clusters. The change in inertia
    score with *k* is often visualized using an **elbow plot**. This plot is called
    an elbow plot because the slope gets much closer to 0 as we increase *k*, so close
    that it resembles an elbow. This is shown in *Figure 16.4*. In this case, we would
    choose a value of *k* near the elbow.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.4 – An elbow plot with inertia and k ](img/B17978_16_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.4 – An elbow plot with inertia and k
  prefs: []
  type: TYPE_NORMAL
- en: Another metric often used when evaluating a clustering model is the **Rand index**.
    The Rand index tells us how frequently two clusterings have assigned the same
    cluster to instances. Values for the Rand index will range between 0 and 1\. We
    typically use an adjusted Rand index, which corrects for chance in the similarity
    calculation. Values for the adjusted Rand index can sometimes be negative.
  prefs: []
  type: TYPE_NORMAL
- en: '**DBSCAN** takes a different approach to clustering. For each instance, it
    counts the number of instances within a specified distance of that instance. All
    instances within ɛ of an instance are said to be in that instance’s ɛ-neighborhood.
    When the number of instances in an ɛ-neighborhood equals or exceeds the minimum
    samples value that we specify, that instance is considered a core instance and
    the ɛ-neighborhood is considered a cluster. Any instance that is more than ɛ from
    another instance is considered noise. This is illustrated in *Figure 16.5*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.5 – DBSCAN clustering with minimum samples = five ](img/B17978_16_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.5 – DBSCAN clustering with minimum samples = five
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several advantages of this density-based approach. The clusters do
    not need to be spherical. They can take any shape. We do not need to guess at
    the number of clusters, though we do need to provide a value for ɛ. Outliers are
    just interpreted as noise and so do not impact the clusters. (This last point
    hints at another useful application of DBSCAN: identifying anomalies.)'
  prefs: []
  type: TYPE_NORMAL
- en: We will use DBSCAN for clustering later in this chapter. First, we will examine
    how to do clustering with k-means, including how to choose a good value for k.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use k-means with some of the same data that we used with the supervised
    learning models that we developed in earlier chapters. The difference is that
    there is no longer a target for us to predict. Rather, we are interested in how
    certain instances hang together. Think of how people arrange themselves in groups
    during a stereotypical high school lunch break and you kind of get a general idea.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to do much of the same preprocessing work that we did with supervised
    learning models. We will start with that in this section. We will work with data
    on income gaps between women and men, labor force participation rates, educational
    attainment, teenage birth frequency, and female participation in politics at the
    highest level.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The income gap dataset is made available for public use by the *United Nations
    Development Program* at [https://www.kaggle.com/datasets/undp/human-development](https://www.kaggle.com/datasets/undp/human-development).
    There is one record per country with aggregate employment, income, and education
    data by gender for 2015.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build a k-means clustering model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We load the familiar libraries. We also load the `KMeans` and `silhouette_score`
    modules. Recall that the silhouette score is often used to evaluate how good a
    job our model has done of clustering. We also load `rand_score`, which will allow
    us to compute the Rand index of similarity between different clusterings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we load the income gap data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s look at some descriptive statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should also look at some correlations. The education ratio (the ratio of
    female educational level to male educational level) and the human development
    ratio are highly correlated, as are gender inequality and the adolescent birth
    rate, as well as the income ratio and the labor force participation ratio:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.6 – A heat map of the correlation matrix ](img/B17978_16_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.6 – A heat map of the correlation matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to scale the data before running our model. We also use **KNN imputation**
    to handle the missing values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we are ready to run the k-means clustering. We specify a value for the
    number of clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After fitting the model, we can generate a silhouette score. Our silhouette
    score is not great. This suggests that our clusters are not very far apart. Later,
    we will look to see whether we can get a better score with more or fewer clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a closer look at the clusters. We can use the `labels_` attribute
    to get the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We could have used the `fit_predict` method instead to get the clusters, like
    so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is helpful to examine how the clusters differ in terms of the values of
    their features. Cluster 0 countries have much higher maternal mortality and adolescent
    birth rate values than countries in the other clusters. Cluster 1 countries have
    very low maternal mortality and high income per capita. Cluster 2 countries have
    very low labor force participation ratios (the ratio of female labor force participation
    to male labor force participation) and income ratios. Recall that we have scaled
    the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the `cluster_centers_` attribute to get the center of each cluster.
    There are nine values representing the center for each of the three clusters,
    since we used nine features for the clustering:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We plot the clusters by some of their features, as well as the center. We place
    the number for the cluster at the centroid for that cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.7 – A 3D scatter plot of three clusters  ](img/B17978_16_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.7 – A 3D scatter plot of three clusters
  prefs: []
  type: TYPE_NORMAL
- en: We can see here that the cluster 0 countries have higher maternal mortality
    and higher adolescent birth rates. Cluster 0 countries have lower income ratios.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have assumed that the best number of clusters to use for our model
    is three. Let’s build a five-cluster model and see how those results look.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The silhouette score has declined from the three-cluster model. That could
    be an indicator that at least some of the clusters are very close together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the new clusters to get a better sense of where they are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.8 – A 3D scatter plot of five clusters ](img/B17978_16_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.8 – A 3D scatter plot of five clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use a statistic called the Rand index to measure the similarity between
    the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We tried three-cluster and five-cluster models, but were either of those a
    good choice? Let’s look at scores for a range of *k* values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot the inertia scores with an elbow plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.9 – An elbow plot of inertia scores ](img/B17978_16_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.9 – An elbow plot of inertia scores
  prefs: []
  type: TYPE_NORMAL
- en: 'We also create a plot of the silhouette scores:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.10 – An plot of silhouette scores ](img/B17978_16_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.10 – An plot of silhouette scores
  prefs: []
  type: TYPE_NORMAL
- en: The elbow plot suggests that a value of *k* around 6 or 7 would be best. We
    start to get diminishing returns in inertia at *k* values above that. The silhouette
    score plot suggests a smaller *k*, as there is a sharp decline in silhouette scores
    after that.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering helped us make sense of our data on the gap between women
    and men in terms of income, education, and employment by country. We can now see
    how certain features hang together, in a way that the simple correlations we did
    earlier did not reveal. This largely assumed, however, that our clusters have
    a spherical shape, and we had to do some work to confirm that our value of *k*
    was the best. We will not have any of the same issues with DBSCAN clustering,
    so we will try that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing DBSCAN clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DBSCAN is a very flexible approach to clustering. We just need to specify a
    value for ɛ, also referred to as **eps**. As we have discussed, the ɛ value determines
    the size of the ɛ-neighborhood around an instance. The minimum samples hyperparameter
    indicates how many instances around an instance are needed for it to be considered
    a core instance.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We use DBSCAN to cluster the same income gap data that we worked with in the
    previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build a DBSCAN clustering model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading familiar libraries, plus the `DBSCAN` module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We import the code to load and preprocess the wage income data that we worked
    with in the previous section. Since that code is unchanged, there is no need to
    repeat it here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to preprocess the data and fit a DBSCAN model . We have chosen
    an eps value of 0.35 here largely through trial and error. We could have also
    looped over a range of eps values and compared silhouette score:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the `labels_` attribute to see the clusters. We have 17 noise instances,
    those with a cluster of -1\. The remaining observations are in one of two clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s take a closer look at which features are associated with each cluster.
    Cluster 1 countries are very different from cluster 0 countries in `maternalmortality`,
    `adolescentbirthrate`, and `genderinequality`. These were important features with
    the k-means clustering as well, but there is one fewer cluster with DBSCAN and
    the overwhelming majority of instances fall into one cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s visualize the clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.11 – A 3D scatter plot of the cluster for each country ](img/B17978_16_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.11 – A 3D scatter plot of the cluster for each country
  prefs: []
  type: TYPE_NORMAL
- en: DBSCAN is an excellent tool for clustering, particularly when the characteristics
    of our data mean that k-means clustering is not a good option; for example, when
    the clusters are not spherical. It also has the advantage of not being influenced
    by outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We sometimes need to organize our instances into groups with similar characteristics.
    This can be useful even when there is no target to predict. We can use the clusters
    created for visualizations, as we did in this chapter. Since the clusters are
    easy to interpret, we can use them to hypothesize why some features move together.
    We can also use the clustering results in subsequent analysis.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter explored two popular clustering techniques, k-means and DBSCAN.
    Both techniques are intuitive, efficient, and handle clustering reliably.
  prefs: []
  type: TYPE_NORMAL
