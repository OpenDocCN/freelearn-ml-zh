<html><head></head><body>
		<div id="_idContainer113">
			<h1 id="_idParaDest-138"><a id="_idTextAnchor159"/>Chapter 8: APIs and Microservice Management</h1>
			<p><a id="_idTextAnchor160"/>In this chapter, you will learn about APIs and microservice management. So far, we have deployed ML applications that are served as APIs. Now we will look into how to develop, organize, manage, and serve APIs. You will learn the principles of API and microservice design for ML inference so that you can design your own custom ML solutions. </p>
			<p>In this chapter, we will learn by doing as we build a microservice using FastAPI and Docker and serve it as an API. For this, we will go through the fundamentals of designing an API and microservice for an ML model trained previously (in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>). Lastly, we will reflect on some key principles, challenges, and tips to design a robust and scalable microservice and API for test and production environments. The following topics will be covered in this chapter:</p>
			<ul>
				<li><a id="_idTextAnchor161"/>Introduction to APIs and microservices</li>
				<li>The need for microservices for ML</li>
				<li>Old is gold – REST API-based microservices</li>
				<li>Hands-on implementation of serving an ML model as an API</li>
				<li>Developing a microservice using Docker</li>
				<li>Testing the API service</li>
			</ul>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor162"/>Introduction to APIs and microservices</h1>
			<p>APIs and <a id="_idIndexMarker595"/>microservices are powerful tools that help to enable your <strong class="bold">ML</strong> (<strong class="bold">ML</strong>) models <a id="_idIndexMarker596"/>to become useful in production or legacy systems for serving the models or communicating with other components of the system. Using APIs and microservices, you can design a robust and scalable ML solution to cater to your business needs. Let's take a look at what APIs and microservices are and how they realize your model's potential in the real world.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor163"/>What is an Application Programming Interface (API)?</h2>
			<p>An <strong class="bold">API</strong> is the gateway that enables developers to <a id="_idIndexMarker597"/>communicate with an application. APIs enable two things:</p>
			<ul>
				<li>Access to an application's data</li>
				<li>The use of an application's functionality</li>
			</ul>
			<p>By accessing and communicating with application data and functionalities, APIs have enabled the world's electronics, applications, and web pages to communicate with each other in order to work together to accomplish business or operations-centric tasks. </p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B16572_08_01.jpg" alt="Figure 8.1 – Workings of an API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Workings of an API</p>
			<p>In <em class="italic">Figure 8.1</em>, we can see the role of an API as it enables access to application data (from the database) and communication with third parties or other applications such as mobile applications (for mobile users), weather applications (on mobile or the web), electric cars, and so on. APIs have <a id="_idIndexMarker598"/>been in operation since the dawn of computers, intending <a id="_idIndexMarker599"/>to enable inter-application communication. Over <a id="_idIndexMarker600"/>time, we have seen developers come to a consensus with protocols such as <strong class="bold">Simple Object Access Protocol</strong> (<strong class="bold">SOAP</strong>) and <strong class="bold">Representational State Transfer</strong> (<strong class="bold">REST</strong>) in the early 2000s. In recent years, a generation of new types of API protocols <a id="_idIndexMarker601"/>have been developed, such as <strong class="bold">Remote Procedure Call</strong> (<strong class="bold">RPC</strong>) and GraphQL as seen in the following table:</p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/Table_011.jpg" alt="Table 8.1 – API protocols comparison &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 8.1 – API protocols comparison </p>
			<p>It is valuable to understand the mainstream API protocols if you are a developer of applications (hosted on the cloud or communicating with other services). It helps you design your APIs as per your business or functionality needs. As a programmer, count yourself fortunate to <a id="_idIndexMarker602"/>have many API protocols at your disposal, as 20 years ago, only SOAP and REST were available. Now a variety of choices are at your disposal depending on your needs, for example, GraphQL, Thrift, and JSON-RPC. These protocols have various advantages and drawbacks, making it easy to find the best suited to your situation. </p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor164"/>Microservices</h2>
			<p>Microservices are a <a id="_idIndexMarker603"/>modern way of designing and deploying apps to run a service. Microservices enable distributed applications rather than one big monolithic application where functionalities are broken up into smaller fragments (called microservices). A microservice is an individual application in a microservice architecture. This is contrary to centralized or monolithic architectures, where all functionalities are <a id="_idIndexMarker604"/>tied up together in one big app. Microservices have grown in popularity due to <strong class="bold">Service-Oriented Architecture</strong> (<strong class="bold">SOA</strong>), an alternative to developing traditional monolithic (singular and self-sufficient applications).</p>
			<p>Microservices gained massive adoption as they enable developers to develop, integrate, and maintain applications with ease. Eventually, this comes down to the fact that individual functionalities are treated independently, at first permitting you to develop an individual functionality of a service step by step. Lastly, it allows you to work on each functionality independently while integrating the whole system to orchestrate the service. This way, you can add, improve, or fix it without risking breaking the entire application. Microservices are valuable for bigger companies since they allow teams to work on isolated things without any complicated organization. In <em class="italic">Figure 8.2</em>, we can see the difference between monoliths and microservices. Microservices enable distributed applications compared to monoliths, which are non-distributed applications: </p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B16572_08_02.jpg" alt="Figure 8.2 – Microservices versus monoliths&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Microservices versus monoliths</p>
			<p>Software development <a id="_idIndexMarker605"/>teams are empowered to work independently and within well-understood service responsibilities. Microservices-based architecture encourages software development teams to take ownership of their services or modules. One possible downside to microservices-based architecture is if you break an application up into parts, there is a severe need for those parts to communicate effectively in order to keep the service running.</p>
			<p>The relationship between APIs and microservices is fascinating as it has two sides. As a result of microservices-based architecture, an API is a direct outcome of implementing that architecture in your application. Whereas at the same time, an API is an essential tool for communicating between services in a microservices-based architecture to function efficiently. Let's have a look at the next section, where we will glance through some examples of ML applications. </p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor165"/>The need for microservices for ML</h1>
			<p>To understand the need for microservices-based architecture for ML applications, let's <a id="_idIndexMarker606"/>look at a hypothetical use case and go through various phases of developing a ML application for the use case.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor166"/>Hypothetical use case</h2>
			<p>A large car repair facility needs a solution to <a id="_idIndexMarker607"/>estimate the number of cars in the facility and their accurate positions. A bunch of IP cameras is installed in the repair stations for monitoring the facility. Design an ML system to monitor and manage the car repair facility.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor167"/>Stage 1 – Proof of concept (a monolith)</h2>
			<p>A quick PoC is developed <a id="_idIndexMarker608"/>in a typical case using available <a id="_idIndexMarker609"/>data points and applying ML to showcase and validate the use case and prove to the business stakeholders that ML can solve their problems or improve their business. </p>
			<p>In our hypothetical use case, a monolith Python app is developed that does the following: </p>
			<ul>
				<li>Fetches streams from all cameras</li>
				<li>Determines the positions of cars (head or tail) from each camera </li>
				<li>Aggregates all estimations into a facility state estimator</li>
			</ul>
			<p>We can see in <em class="italic">Figure 8.3</em>, the app is dockerized and deployed to the server:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B16572_08_03.jpg" alt="Figure 8.3 – Monolith ML application (PoC for hypothetical use case)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Monolith ML application (PoC for hypothetical use case)</p>
			<p>All cameras are connected to this server via the local network. The algorithms for <a id="_idIndexMarker610"/>car position estimation and the facility state estimator work but need further improvements, and overall the PoC works. This monolith app is highly prone to crashing due to the instability of the cameras, the local network, and other errors. Such instabilities can be handled better by microservices. Let's see this in practice in stage 2.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor168"/>Stage 2 – Production (microservices)</h2>
			<p>In this stage, an <a id="_idIndexMarker611"/>application that is less prone to crashing is essential to run the car repair facility's monitoring operations continuously. For this reason, a monolith application is replaced with microservices-based architecture as shown in <em class="italic">Figure 8.4</em>: </p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B16572_08_04.jpg" alt="Figure 8.4 – Microservices (production-ready application for hypothetical use case)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Microservices (production-ready application for hypothetical use case)</p>
			<p>The application is fragmented into multiple services in the following manner: </p>
			<ul>
				<li>Video stream collector.</li>
				<li><strong class="bold">Image processor</strong>: This aggregates images – it receives, processes, and caches images, and generates packets for further processing.</li>
				<li><strong class="bold">Position classifier</strong>: Estimates a car's position (head or tail) parked in the repair facility.</li>
				<li><strong class="bold">Facility setup estimator</strong>: This asynchronously <a id="_idIndexMarker612"/>receives car position estimations and calibrates the facility setup and sends real-time data to the cloud.</li>
				<li>The cloud collects and stores data using MQTT (a standard lightweight, publish-subscribe network protocol that transports messages between devices). The data is portrayed on a dashboard for the car facility operators to analyze operations. </li>
			</ul>
			<p>All of the communication between each microservice is facilitated using APIs. The advantages of microservice architecture are that if any of the services crash or errors take place, that particular microservice is spawned to replace the failed one to keep the whole service running. Secondly, each microservice can be maintained and improved continuously by a dedicated team (of data scientists, developers, and DevOps engineers), unlike coordinating teams, to work on a monolithic system.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor169"/>Old is gold – REST API-based microservices</h1>
			<p>Old is gold. Plus, it's better to start somewhere where there are various API protocols. The <strong class="bold">Representational State Transfer</strong> (<strong class="bold">REST</strong>) protocol has <a id="_idIndexMarker613"/>become a gold standard for many applications over the years, and it's not so very <a id="_idIndexMarker614"/>different for ML applications today. The majority of companies prefer developing their ML applications based on the REST API protocol. </p>
			<p>A REST API or RESTful API is based on REST, an architectural method used to communicate mainly in web services development.</p>
			<p>RESTful APIs are widely used; companies such as Amazon, Google, LinkedIn, and Twitter use them. Serving our ML models via RESTful APIs has many benefits, such as the following: </p>
			<ul>
				<li>Serve predictions on the fly to multiple users.</li>
				<li>Add more instances to scale up the application behind a load balancer.</li>
				<li>Possibly combine multiple models using different API endpoints.</li>
				<li>Separate our model operating environment from the user-facing environment.</li>
				<li>Enable microservices-based architecture. Hence, teams can work independently to develop and enhance the services. </li>
			</ul>
			<p>A RESTful API uses existing HTTP methodologies that are defined by the RFC 2616 protocol. <em class="italic">Table 8.2</em> summarizes the HTTP methods in combination with their CRUD operations and purpose in ML applications.</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/Table_02.jpg" alt="Table 8.2 – REST API HTTP methods&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Table 8.2 – REST API HTTP methods</p>
			<p>The fundamental HTTP methods are <strong class="source-inline">GET</strong>, <strong class="source-inline">POST</strong>, <strong class="source-inline">PUT</strong>, <strong class="source-inline">PATCH</strong>, and <strong class="source-inline">DELETE</strong>. These methods <a id="_idIndexMarker615"/>correspond to <strong class="bold">CRUD</strong> operations such as <strong class="source-inline">create</strong>, <strong class="source-inline">read</strong>, <strong class="source-inline">update</strong>, and <strong class="source-inline">delete</strong>. Using these methods, we can develop RESTful APIs to serve ML models. RESTful APIs have gained <a id="_idIndexMarker616"/>significant adoption due to drivers such as OpenAPI. The OpenAPI Specification is a standardized REST API description format. It has become a standardized format for humans and machines; it enables REST API understandability and provides extended tooling such as API validation, testing, and an interactive documentation generator. In practice, the OpenAPI file enables you to describe an entire API with critical information such as the following:</p>
			<ul>
				<li>Available endpoints (<strong class="source-inline">/names</strong>) and operations on each endpoint (<strong class="source-inline">GET /names</strong>, <strong class="source-inline">POST /names</strong>)</li>
				<li>Input and output for each operation (operation parameters)</li>
				<li>Authentication methods</li>
				<li>Developer documentation</li>
				<li>Terms of use, license, and other information</li>
			</ul>
			<p>You can find more <a id="_idIndexMarker617"/>about OpenAPI on this site: <a href="https://swagger.io/specification/">https://swagger.io/specification/</a>.</p>
			<p>In the next section, we will develop a RESTful API to serve an ML model and test it using an OpenAPI based interface called Swagger UI.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor170"/>Hands-on implementation of serving an ML model as an API</h1>
			<p>In this section, we will apply the principles of APIs and microservices that we have learned <a id="_idIndexMarker618"/>previously (in the section <em class="italic">Introduction to APIs and microservices</em>) and develop a RESTful API service to serve the ML model. The ML model we'll <a id="_idIndexMarker619"/>serve will be for the business problem (weather prediction using ML) we worked on previously. We will use the FastAPI framework to serve the model as an API and Docker to containerize the API service into a microservice. </p>
			<p>FastAPI is a framework for deploying ML models. It is easy and fast to code and enables high performance with features such as asynchronous calls and data integrity checks. FastAPI is easy to use and follows the OpenAPI Specification, making it easy to test and validate APIs. Find out <a id="_idIndexMarker620"/>more about FastAPI here: <a href="https://fastapi.tiangolo.com/.">https://fastapi.tiangolo.com/.</a></p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor171"/>API design and development </h2>
			<p>We will <a id="_idIndexMarker621"/>develop the API service and run it on a local computer. (This could also be developed on the VM <a id="_idIndexMarker622"/>we created earlier in the Azure Machine learning workspace. For learning, it is recommended to practice it locally for ease.) </p>
			<p>To get started, clone the book repository on your PC or laptop and go to the <strong class="source-inline">08_API_Microservices</strong> folder. We will use these files to build the API service:</p>
			<p class="source-code">Learn_MLOps</p>
			<p class="source-code">├──08_API_Microservices</p>
			<p class="source-code">│   ├── Dockerfile</p>
			<p class="source-code">    ├── app</p>
			<p class="source-code">            └── variables.py</p>
			<p class="source-code">            └── weather_api.py</p>
			<p class="source-code">            └── requirements.txt</p>
			<p class="source-code">            └── artifacts</p>
			<p class="source-code">                       └── model-scaler.pkl</p>
			<p class="source-code">                       └── svc.onnx</p>
			<p>The files listed in the directory tree for the folder <strong class="source-inline">08_API_Microservices</strong> include a Dockerfile (used to build a Docker image and container from the <strong class="source-inline">FASTAPI</strong> service) and a folder named <strong class="source-inline">app</strong>. The <strong class="source-inline">app</strong> folder contains the files <strong class="source-inline">weather_api.py</strong> (contains the <a id="_idIndexMarker623"/>code for API endpoint <a id="_idIndexMarker624"/>definitions), <strong class="source-inline">variables.py</strong> (contains the input variables definition), and <strong class="source-inline">requirements.txt</strong> (contains Python packages needed for running the API service), and a folder with model artifacts such as a model scaler (used to scale incoming data) and a serialized model file (<strong class="source-inline">svc.onnx</strong>). </p>
			<p>The model was serialized previously, in the model training and evaluation stage, as seen in <a href="B16572_05_Final_JM_ePub.xhtml#_idTextAnchor093"><em class="italic">Chapter 5</em></a>, <em class="italic">Model Evaluation and Packaging</em>. The model is downloaded and placed in the folder from the model registry in the Azure Machine learning workspace (<strong class="source-inline">Learn_MLOps</strong>) as shown in <em class="italic">Figure 8.3</em>:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B16572_08_05.jpg" alt="Figure 8.5 – Download the serialized model file&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.5 – Download the serialized model file</p>
			<p>You can <a id="_idIndexMarker625"/>replace the <strong class="source-inline">svc.onnx</strong> and <strong class="source-inline">model-scalar.pkl</strong> files with the files you have trained in your <a id="_idIndexMarker626"/>Azure Machine learning workspace or else just continue using these files for quick experimentation. Now we will look into the code of each file. Let's start with <strong class="source-inline">variables.py</strong>.</p>
			<h3>variables.py</h3>
			<p>We use only one package for <a id="_idIndexMarker627"/>defining input variables. The package we use is called <strong class="source-inline">pydantic</strong>; it is a data validation and settings management package using Python-type annotations. Using <strong class="source-inline">pydantic</strong>, we will define input variables in the class named <strong class="source-inline">WeatherVariables</strong> used for the <strong class="source-inline">fastAPI</strong> service: </p>
			<p class="source-code">from pydantic import BaseModel</p>
			<p class="source-code">class WeatherVariables(BaseModel):</p>
			<p class="source-code">                        temp_c: float </p>
			<p class="source-code">                        humidity: float </p>
			<p class="source-code">                        wind_speed_kmph: float </p>
			<p class="source-code">                        wind_bearing_degree: float</p>
			<p class="source-code">                        visibility_km: float </p>
			<p class="source-code">                        pressure_millibars: float </p>
			<p class="source-code">                        current_weather_condition: float</p>
			<p>In the <strong class="source-inline">WeatherVariables</strong> class, define variables and their types as shown in the preceding code. The same variables that were used for training the model in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>, will be used for inference. We define those input variables here as <strong class="source-inline">temp_c</strong>, <strong class="source-inline">humidity</strong>, <strong class="source-inline">wind_speed_kmph</strong>, <strong class="source-inline">wind_bearing_degree</strong>, <strong class="source-inline">visibility_km</strong>, <strong class="source-inline">pressure_millibars</strong>, and <strong class="source-inline">current_weather_condition</strong>. Data types for these variables are defined as <strong class="source-inline">float</strong>. We will import the <strong class="source-inline">WeatherVariables</strong> class and <a id="_idIndexMarker628"/>use the defined input variables in the <strong class="source-inline">fastAPI</strong> service. Let's look at how we can use the variables defined in the <strong class="source-inline">WeatherVariables</strong> class in the <strong class="source-inline">fastAPI</strong> service using the <strong class="source-inline">Weather_api.py</strong> file.  </p>
			<h3>Weather_api.py</h3>
			<p>This file is used to <a id="_idIndexMarker629"/>define the <strong class="source-inline">fastAPI</strong> service. The needed model artifacts are imported and used to serve API endpoints to infer the model for making predictions in real time or in production: </p>
			<ol>
				<li>We start by importing the required packages as follows:<p class="source-code">import uvicorn</p><p class="source-code">from fastapi import FastAPI</p><p class="source-code">from variables import WeatherVariables</p><p class="source-code">import numpy</p><p class="source-code">import pickle</p><p class="source-code">import pandas as pd</p><p class="source-code">import onnxruntime as rt</p><p>We imported the required packages, such as <strong class="source-inline">uvicorn</strong> (an ASGI server implementation package), <strong class="source-inline">fastapi</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">pickle</strong>, <strong class="source-inline">pandas</strong>, and <strong class="source-inline">onnxruntime</strong> (used to deserialize and infer <strong class="source-inline">onnx</strong> models). </p><p class="callout-heading">Note </p><p class="callout">We imported the <strong class="source-inline">WeatherVariables</strong> class previously created in the <strong class="source-inline">variables.py</strong> file. We will use the variables defined in this file for procuring input data for the <strong class="source-inline">fastAPI</strong> service. </p></li>
				<li>Next, we create an <strong class="source-inline">app</strong> object. You will notice some syntactic similarities of <strong class="source-inline">fastAPI</strong> with the Flask web framework (if you have ever used Flask). <p>For instance, in the next step, we create the <strong class="source-inline">app</strong> object using the function <strong class="source-inline">FastAPI()</strong> to create the <strong class="source-inline">app</strong> object. Creating an <strong class="source-inline">app</strong> object is similar to how we do it via the <strong class="source-inline">Flask</strong> example: from Flask, import <strong class="source-inline">Flask</strong> and then we use the <strong class="source-inline">Flask</strong> function to create the <strong class="source-inline">app</strong> object in the manner <strong class="source-inline">app = Flask ()</strong>. You will notice such <a id="_idIndexMarker630"/>similarities as we build API endpoints using <strong class="source-inline">fastAPI</strong>:</p><p class="source-code">app = FastAPI()</p><p class="source-code"># Load model scalar</p><p class="source-code">pickle_in = open("artifacts/model-scaler.pkl", "rb")</p><p class="source-code">scaler = pickle.load(pickle_in)</p><p class="source-code"># Load the model</p><p class="source-code">sess = rt.InferenceSession("artifacts/svc.onnx")</p><p class="source-code">input_name = sess.get_inputs()[0].name</p><p class="source-code">label_name = sess.get_outputs()[0].name</p></li>
				<li>After creating the <strong class="source-inline">app</strong> object, we will import the necessary model artifacts for inference in the endpoints. <strong class="source-inline">Pickle</strong> is used to deserialize the data scaler file <strong class="source-inline">model-scaler.pkl</strong>. This file was used to train the model (in <a href="B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 4</em></a>, <em class="italic">Machine Learning Pipelines</em>), and now we'll use it to scale the incoming data before model inference. We will use the previously trained support vector classifier model, which was serialized into the file named <strong class="source-inline">scv.onnx</strong> (we can access and download the file as shown in <em class="italic">Figure 8.3</em>). </li>
				<li><strong class="source-inline">ONNX</strong> Runtime is used to load the serialized model into inference sessions (<strong class="source-inline">input_name</strong> and <strong class="source-inline">label_name</strong>) for making ML model predictions. Next, we can move to the core part of defining the API endpoints to infer the ML model. To begin, we make a <strong class="source-inline">GET</strong> request to the index route using the wrapper function <strong class="source-inline">@app.get('/')</strong>:  <p class="source-code">@app.get('/')</p><p class="source-code">def index():</p><p class="source-code">    return {'Hello': 'Welcome to weather prediction service, access the api     docs and test the API at http://0.0.0.0/docs.'}</p><p>A function named <strong class="source-inline">index()</strong> is defined for the index route. It returns the welcome <a id="_idIndexMarker631"/>message, pointing to the docs link. This message is geared toward guiding the users to the docs link to access and test the API endpoints. </p></li>
				<li>Next, we will define the core API endpoint, <strong class="source-inline">/predict</strong>, which is used to infer the ML model. A wrapper function, <strong class="source-inline">@app.post('/predict')</strong>, is used to make a <strong class="source-inline">POST</strong> request: <p class="source-code">@app.post('/predict')</p><p class="source-code">def predict_weather(data: WeatherVariables):</p><p class="source-code">    data = data.dict()</p><p class="source-code">    # fetch input data using data varaibles</p><p class="source-code">    temp_c = data['temp_c']</p><p class="source-code">    humidity = data['humidity']</p><p class="source-code">    wind_speed_kmph = data['wind_speed_kmph']</p><p class="source-code">    wind_bearing_degree = data['wind_bearing_degree']</p><p class="source-code">    visibility_km = data['visibility_km']</p><p class="source-code">    pressure_millibars = data['pressure_millibars']</p><p class="source-code">    current_weather_condition = data['current_weather_condition']</p><p>A function named <strong class="source-inline">predict_weather()</strong> is initiated for the endpoint <strong class="source-inline">/predict</strong>. Inside the function, we have created a variable called <strong class="source-inline">data</strong> that will capture the input data; this variable captures the <strong class="source-inline">JSON</strong> data we are getting through the <strong class="source-inline">POST</strong> request and points to <strong class="source-inline">WeatherVariables</strong>. As soon as we do the <strong class="source-inline">POST</strong> request, all the <a id="_idIndexMarker632"/>variables in the incoming data will be mapped to variables in the <strong class="source-inline">WeatherVariables</strong> class from the <strong class="source-inline">variables.py</strong> file. </p></li>
				<li>Next, we convert the data into a dictionary, fetch each input variable from the dictionary, and compress them into a <strong class="source-inline">numpy</strong> array variable, <strong class="source-inline">data_to_pred</strong>. We will use this variable to scale the data and infer the ML model: <p class="source-code">    data_to_pred = numpy.array([[temp_c, humidity, wind_speed_kmph,         </p><p class="source-code">wind_bearing_degree,visibility_km, pressure_millibars, </p><p class="source-code">current_weather_condition]])</p><p class="source-code">    # Scale input data</p><p class="source-code">    data_to_pred = scaler.fit_transform(data_to_pred.reshape(1, 7))</p><p class="source-code">  # Model inference</p><p class="source-code">    prediction = sess.run(</p><p class="source-code">        [label_name], {input_name: data_to_pred.astype(numpy.float32)})[0]</p><p>The data (<strong class="source-inline">data_to_pred</strong>) is reshaped and scaled using the scaler loaded previously using the <strong class="source-inline">fit_transform()</strong> function. </p></li>
				<li>Next, the model inference step, which is the key step, is performed by inferencing scaled data to the model, as shown in the preceding code. The prediction inferred from the model is then returned as the output to the <strong class="source-inline">prediction</strong> variable: <p class="source-code">if(prediction[0] &gt; 0.5):</p><p class="source-code">        prediction = "Rain"</p><p class="source-code">    else:</p><p class="source-code">        prediction = "No_Rain"</p><p class="source-code">    return {</p><p class="source-code">        'prediction': prediction</p><p class="source-code">    }    </p><p>Lastly, we will convert the <a id="_idIndexMarker633"/>model inference into a human-readable format by suggesting <strong class="source-inline">rain</strong> or <strong class="source-inline">no_rain</strong> based on the ML model's predictions and return the <strong class="source-inline">prediction</strong> for the <strong class="source-inline">POST</strong> call to the <strong class="source-inline">/predict</strong> endpoint. This brings us to the end of the <strong class="source-inline">weather_api.py</strong> file. When a <strong class="source-inline">POST</strong> request is made by passing input data, the service returns the model prediction in the form of <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>. The service will return <strong class="source-inline">rain</strong> or <strong class="source-inline">not_rain</strong> based on the model prediction. When you get such a prediction, your service is working and is robust enough to serve production needs. </p></li>
			</ol>
			<h3>Requirement.txt</h3>
			<p>This text file contains all the <a id="_idIndexMarker634"/>packages needed to run the <strong class="source-inline">fastAPI</strong> service: </p>
			<p class="source-code">numpy</p>
			<p class="source-code">fastapi</p>
			<p class="source-code">uvicorn</p>
			<p class="source-code">scikit-learn==0.20.3</p>
			<p class="source-code">pandas</p>
			<p class="source-code">onnx</p>
			<p class="source-code">onnxruntime</p>
			<p>These packages should be installed in the environment where you would like to run the API service. We will use <strong class="source-inline">numpy</strong>, <strong class="source-inline">fastapi</strong> (an ML framework for creating robust APIs), <strong class="source-inline">uvicorn</strong> (an AGSI server), <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">onnx</strong>, and <strong class="source-inline">onnxruntime</strong> (to deserialize and infer <strong class="source-inline">onnx</strong> models) to <a id="_idIndexMarker635"/>run the FastAPI service. To deploy and run the API service in a standardized way, we will use Docker to run the FastAPI service in a Docker container. </p>
			<p>Next, let's look at how to create a Dockerfile for the service.   </p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor172"/>Developing a microservice using Docker</h1>
			<p>In this section, we <a id="_idIndexMarker636"/>will package the FastAPI service in a <a id="_idIndexMarker637"/>standardized way using Docker. This way, we can deploy the Docker image or container on the deployment target of your choice within around 5 minutes. </p>
			<p>Docker has several advantages, such as replicability, security, development simplicity, and so on. We can use the official Docker image of <strong class="source-inline">fastAPI</strong> (<strong class="source-inline">tiangolo/uvicorn-gunicorn-fastapi</strong>) from Docker Hub. Here is a snippet of the Dockerfile:</p>
			<p class="source-code">FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7</p>
			<p class="source-code">COPY ./app /app</p>
			<p class="source-code">RUN pip install -r requirements.txt</p>
			<p class="source-code">EXPOSE 80</p>
			<p class="source-code">CMD ["uvicorn", "weather_api:app", "--host", "0.0.0.0", "--port", "80"]</p>
			<p>Firstly, we use an official <strong class="source-inline">fastAPI</strong> Docker image from Docker Hub by using the <strong class="source-inline">FROM</strong> command and pointing to the image – <strong class="source-inline">tiangolo/uvicorn-gunicorn-fastapi:python3.7</strong>. The image uses Python 3.7, which is compatible with <strong class="source-inline">fastAPI</strong>. Next, we copy the <strong class="source-inline">app</strong> folder into a directory named <strong class="source-inline">app</strong> inside <strong class="source-inline">docker image/container</strong>. After the folder <strong class="source-inline">app</strong> is copied inside the Docker image/container, we will install the necessary packages listed in the file <strong class="source-inline">requirements.txt</strong> by using the <strong class="source-inline">RUN</strong> command. </p>
			<p>As the <strong class="source-inline">uvicorn</strong> server (AGSI server) for <strong class="source-inline">fastAPI</strong> uses port <strong class="source-inline">80</strong> by default, we will <strong class="source-inline">EXPOSE</strong> port <strong class="source-inline">80</strong> for the Docker image/container. Lastly, we will spin up the server inside the <a id="_idIndexMarker638"/>Docker image/container using the command <strong class="source-inline">CMD "uvicorn weather_api:app –host 0.0.0.0 –port 80"</strong>. This <a id="_idIndexMarker639"/>command points to the <strong class="source-inline">weather_api.py</strong> file to access the <strong class="source-inline">fastAPI</strong> app object for the service and host it on port <strong class="source-inline">80</strong> of the image/container. </p>
			<p>Congrats, you are almost there. Now we will test the microservice for readiness and see whether and how it works.</p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor173"/>Testing the API </h1>
			<p>To test the API for <a id="_idIndexMarker640"/>readiness, we will perform the following steps:</p>
			<ol>
				<li value="1">Let's start by building the Docker image. For this, a prerequisite is to have Docker installed. Go to your terminal or Command Prompt and clone the repository to your desired location and access the folder <strong class="source-inline">08_API_Microservices</strong>. Execute the following Docker command to build the Docker image: <p class="source-code"><strong class="bold">docker </strong><strong class="bold">build -t fastapi .</strong></p><p>Execution of the <strong class="source-inline">build</strong> command will start building the Docker image following the steps listed in the Dockerfile. The image is tagged with the name <strong class="source-inline">fastapi</strong>. After successful execution of the <strong class="source-inline">build</strong> command, you can validate whether the image is built and tagged successfully or not using the <strong class="source-inline">docker images</strong> command. It will output the information as follows, after successfully building the image:</p><p class="source-code"><strong class="bold">(base) user ~ docker images   </strong></p><p class="source-code"><strong class="bold">REPOSITORY   TAG       IMAGE ID       CREATED          SIZE</strong></p><p class="source-code"><strong class="bold">fastapi      latest    1745e964f57f   56 seconds ago   1.31GB</strong></p></li>
				<li>Run a Docker container locally. Now, we can spawn a running Docker container from the Docker image created previously. To run a Docker container, we use the <strong class="source-inline">RUN</strong> command:<p class="source-code"><strong class="bold">docker run -d –name weathercontainer -p 80:80 fastapi</strong></p><p>A Docker container is spawned from the <strong class="source-inline">fastapi</strong> Docker image. The name of the running container is <strong class="source-inline">weathercontainer</strong> and its port <strong class="source-inline">80</strong> is mapped to port <strong class="source-inline">80</strong> of the local computer. The container will run in the background as we have used <strong class="source-inline">-d</strong> in the <strong class="source-inline">RUN</strong> command. Upon successfully running a container, a container ID is output on the terminal, for example, <strong class="source-inline">2729ff7a385b0a255c63cf03ec9b0e1411ce4426c9c49e8db 4883e0cf0fde567</strong>.</p></li>
				<li>Test the <a id="_idIndexMarker641"/>API service using sample data. We will check whether the container is running successfully or not. To check this, use the following command: <p class="source-code"><strong class="bold">docker container ps</strong></p><p>This will list all the running containers as follows:</p><p class="source-code"><strong class="bold">(base) user ~ docker container ps</strong></p><p class="source-code"><strong class="bold">CONTAINER ID   IMAGE     COMMAND                  CREATED          STATUS</strong></p><p class="source-code"><strong class="bold">          PORTS                NAMES</strong></p><p class="source-code"><strong class="bold">2729ff7a385b   fastapi   "uvicorn weather_api…"   </strong></p><p class="source-code"><strong class="bold">18 seconds ago   Up 17 seconds   0.0.0.0:80-&gt;80/tcp   weathercontainer</strong></p><p>We can see that the container from the image <strong class="source-inline">fastapi</strong> is mapped and successfully running on port <strong class="source-inline">80</strong> of the local machine. We can access the service and test it from the browser on our local machine at the address <strong class="source-inline">0.0.0.0:80</strong>.</p><p class="callout-heading">Note</p><p class="callout">If you have no response or errors when you run or test your API service, you may have to disable CORS validation from browsers such as Chrome, Firefox, and Brave or add an extension (for example, go to the Chrome Web Store and search for one) that will disable CORS validation for running and testing APIs locally. By default, you don't need to disable CORS; do it only if required.</p><p>You will see the message that follows:</p><div id="_idContainer110" class="IMG---Figure"><img src="image/B16572_08_06.jpg" alt="Figure 8.6 – FastAPI service running on local port 80&#13;&#10;"/></div><p class="figure-caption">Figure 8.6 – FastAPI service running on local port 80</p><p>FastAPI uses the OpenAPI (read more: <a href="https://www.openapis.org/">https://www.openapis.org/</a>, <a href="https://swagger.io/specification/">https://swagger.io/specification/</a>) Specification to serve the model. The <strong class="bold">OpenAPI Specification</strong> (<strong class="bold">OAS</strong>) is a <a id="_idIndexMarker642"/>standard, language-agnostic interface for RESTful APIs. Using OpenAPI <a id="_idIndexMarker643"/>features, we can access API documentation and get a broad overview of the API. You can access the API docs and test the API at <strong class="source-inline">0.0.0.0:80/docs</strong> and it will direct you to a Swagger-based UI (it uses the OAS) to test your API. </p></li>
				<li>Now, test the <strong class="source-inline">/predict</strong> endpoint (by selecting the endpoint and clicking the <strong class="bold">Try it out</strong> button) using input data of your choice, as shown in <em class="italic">Figure 8.6</em>:<div id="_idContainer111" class="IMG---Figure"><img src="image/B16572_08_07.jpg" alt="Figure 8.7 – Input for the request body of the FastAPI service&#13;&#10;"/></div><p class="figure-caption">Figure 8.7 – Input for the request body of the FastAPI service</p></li>
				<li>Click <strong class="bold">Execute</strong> to make a <strong class="source-inline">POST</strong> call and <a id="_idIndexMarker644"/>test the endpoint. The input is inferred with the model in the service and the model prediction <strong class="source-inline">Rain</strong> or <strong class="source-inline">No_Rain</strong> is the output of the <strong class="source-inline">POST</strong> call, as shown in <em class="italic">Figure 8.7</em>: </li>
			</ol>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B16572_08_08.jpg" alt="Figure 8.8 – Output for the POST call (/predict endpoint)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.8 – Output for the POST call (/predict endpoint)</p>
			<p>The successful <a id="_idIndexMarker645"/>execution of the <strong class="source-inline">POST</strong> call for the <strong class="source-inline">/predict</strong> API will result in the output model prediction as shown in <em class="italic">Figure 8.6</em>. The model running in the Docker container outputs the weather condition as <strong class="source-inline">Rain</strong> for the <strong class="source-inline">POST</strong> call. Congratulations, you have successfully spawned a <strong class="source-inline">fastAPI</strong> container and tested it. This exercise should have equipped you with the skills to build, deploy, and test ML-based API services for your use cases. </p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor174"/>Summary</h1>
			<p>In this chapter, we have learned the key principles of API design and microservice deployment in production. We touched upon the basics of API design methods and learned about FastAPI. For our business problem, we have learned by doing a practical implementation of developing an API service in the <em class="italic">Hands-on implementation of serving an ML model as an API</em> section using FastAPI and Docker. Using the practical knowledge gained in this chapter, you can design and develop robust API services to serve your ML models. Developing API services for ML models is a stepping stone to take ML models to production. </p>
			<p>In <a id="_idTextAnchor175"/>the next chapter, we will delve into the concepts of testing and security. We will implement a testing method to test the robustness of an API service using Locust. Let's go!</p>
		</div>
	</body></html>