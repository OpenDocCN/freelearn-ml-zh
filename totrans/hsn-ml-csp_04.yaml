- en: Risk versus Reward – Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will go a little deeper and learn about one of the hot
    topics in machine learning: reinforcement learning. We will cover several exciting
    examples to show how you can use this in your application. We''ll go over a few
    algorithms, and then after our first, more formal example, we will take you to
    a final exciting example that you are sure to enjoy!'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overviewing reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Q-learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SARSA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running our application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tower of Hanoi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overviewing reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned in [Chapter 1](7a1f2cca-1be5-426a-8e8a-6a4a3828cd76.xhtml), *Machine
    Learning Basics*, reinforcement learning is a case where the machine is trained
    for a specific outcome with the sole purpose of maximizing efficiency and/or performance.
    The algorithm is rewarded for making correct decisions and penalized for making
    incorrect ones, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33096271-1bd8-4af9-b961-c9d556df32c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Continual training is used to constantly improve performance. The focus here
    is on performance, meaning somehow finding a balance between unseen data and what
    the algorithms have already learned. The algorithm applies an action to its environment,
    receives a reward or a penalty based on what it has done, repeats the process,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We're going to dive right into the application in this chapter, and we're going
    to use the incredible Accord.NET open source machine learning framework to highlight
    how we can use reinforcement learning to help an autonomous object get from its
    starting location, depicted by a black object, to a desired end point, depicted
    by a red object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept is similar, although on a much lower scale of complexity, to what
    autonomous vehicles do to get you from point A to point B. Our example will allow
    you to use maps of various complexity, meaning various obstacles may appear in
    between your autonomous object and the desired location. Let''s look at our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e7b2b7c-9d43-4f03-9d8e-4e1d9576a2b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, you can see that we have a very basic map loaded, one with no obstacles
    but only exterior confining walls. The black block (start) is our autonomous object
    and the red block (stop) is our destination. Our goal in this application is to
    navigate the walls to get to our desired location. If our next move puts us onto
    a white block, our algorithm will be rewarded. If our next move puts us into a
    wall, it will be penalized. From this, our autonomous object should be able to
    get to its destination. The question is: how fast can it learn? In this example,
    there are absolutely no obstacles in its path, so there should be no issues solving
    the problem in the shortest number of moves possible.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is another example of a somewhat more complicated map for our
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c05f744-440e-460c-a01b-07de77b354c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Types of learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'On the right-hand side of our application are our settings, as seen in the
    following screenshot. The first thing that we see is the learning algorithm. In
    this application, we will be dealing with two distinct learning algorithms, **Q-learning**
    and **State-Action-Reward-State-Action** (**SARSA**). Let''s briefly discuss both
    of these algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c0a67d9-da76-4634-b528-5f839ad71d04.png)'
  prefs: []
  type: TYPE_IMG
- en: Q-learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Q-learning can identify an optimal action (that which has the highest value
    in each state) while in a given state without having a completely defined model
    of the environment. It is also great at handling problems with stochastic transitions
    and rewards without requiring tweaking or adaptations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the mathematical intuition for Q-learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57e66621-c1c9-4b6b-9fd7-463cb2612e4b.png)'
  prefs: []
  type: TYPE_IMG
- en: Perhaps it's easier to comprehend if we provide a very high-level abstract example.
    The agent starts at state 1\. It then performs action 1 and gets reward 1\. Next,
    it looks around and sees what the maximum possible reward for an action in state
    2 is; it uses that to update the value of action 1\. And so on!
  prefs: []
  type: TYPE_NORMAL
- en: SARSA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SARSA** (you can already guess where this one is, going by the name) works
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: The agent starts at state 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It then performs action 1 and gets reward 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, it moves on to state 2, performs action 2, and gets reward 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the agent goes back and updates the value of action 1
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As you can see, the difference in the two algorithms is in the way the future
    reward is found. Q-learning uses the highest action possible from state 2, while
    SARSA uses the value of the action that is actually taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the mathematical intuition for SARSA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a07b00f1-71ee-4b11-aee4-7d5eb98c7e82.png)'
  prefs: []
  type: TYPE_IMG
- en: Running our application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For now, let's start using our application with our default parameters. Simply
    click on the Start button and the learning will commence. Once this is complete,
    you will be able to click on the Show Solution button, and the learned path will
    be animated from start to finish.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking on Start will begin the learning stage and continue until the black
    object reaches its goal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24bfed96-785a-4460-b149-73ccf8778c20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here you will see that as the learning progresses, we are sending the output
    to `ReflectInsight` to help us see and learn what the algorithm is doing internally.
    You see that for each iteration, different object positions are being evaluated,
    and so are their actions and rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2221e39-43ce-497d-9929-65eb7dcfc453.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the learning is complete, we can click on the Show Solution button to
    replay the final solution. When complete, the black object will sit atop the red
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a2ea377-0fdb-44ab-8753-94ba7fd68298.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s look at the code from our application. There are two methods of
    learning that we highlighted previously. Here''s how Q-learning looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'How does SARSA learning differ? Let''s take a look at the `while` loop of SARSA
    learning and understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our last step is to see how we can animate the solution. This will be needed
    for us to see that our algorithm achieved its goal. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: And here is our `while` loop where all the magic happens!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's break this down into more digestible sections. The first thing that we
    do is establish our tabu policy. If you are not familiar with tabu searching,
    note that it is designed to enhance the performance of a local search by relaxing
    its rule. At each step, sometimes worsening a move is acceptable if there are
    no alternatives (moves with reward).
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, prohibitions (tabu) are put in place to ensure that the algorithm
    does not return to the previously visited solution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Next, we have to position our agent and prepare the map.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c364af85-1701-4fa3-a222-40c17923fc6d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is our main execution loop, which will show the animated solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Tower of Hanoi
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we've discussed Q-learning, I want to spend the rest of this chapter highlighting
    some fantastic work done by Kenan Deen. His Tower of Hanoi solution is a great
    example of how you can use reinforcement learning to solve real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: This form of reinforcement learning is more formally known as a **Markov Decision
    Process** (**MDP**). An MDP is a discrete-time stochastic control process, which
    means that at each time step, the process is in state *x*. The decision maker
    may choose any available action for that state, and the process will respond at
    the next time step by randomly moving into a new state and giving the decision
    maker a reward. The probability that the process moves into its new state is determined
    by the chosen action. So, the next state depends on the current state and the
    decision maker's action. Given the state and the action, the next move is completely
    independent of all previous states and actions.
  prefs: []
  type: TYPE_NORMAL
- en: The Tower of Hanoi consists of three rods and several sequentially sized disks
    in the leftmost rod. The objective is to move all the disks from the leftmost
    rod to the rightmost one **with the fewest possible number of moves**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two important rules you have to follow are that you can move only one disk
    at a time, and you can''t put a bigger disk on top of a smaller one; that is,
    in any rod, the order of disks must always be from the largest disk at the bottom
    to the smallest disk at the top, depicted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c35f9c3-1da9-4304-bec3-adb147155875.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say we are using three disks, as pictured just now. In this scenario,
    there are 3³ possible states, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4c79d9df-58e4-4e23-b5f8-3f99cce61828.png)'
  prefs: []
  type: TYPE_IMG
- en: The total number of all possible states in a Tower of Hanoi puzzle is 3 raised
    to the number of disks.
  prefs: []
  type: TYPE_NORMAL
- en: '*||S|| = 3^n*'
  prefs: []
  type: TYPE_NORMAL
- en: Where ||*S*|| is the number of elements in the set states, and *n* is the number
    of disks.
  prefs: []
  type: TYPE_NORMAL
- en: So, in our example, we have *3 x 3 x 3 = 27* unique possible states of the distribution
    of disks over the three rods, including empty rods; but two empty rods can be
    in a state at max.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the total number of states being defined, here are all the possible actions
    our algorithm has available to move from one state to another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd82cb57-d7fd-4e8f-8f1f-2214abf7135f.png)'
  prefs: []
  type: TYPE_IMG
- en: The least possible number of moves for this puzzle is:*LeastPossibleMoves =
    2^n - 1*
  prefs: []
  type: TYPE_NORMAL
- en: Where *n* is the number of disks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Q-learning algorithm can be formally defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96bd1e3a-69a6-466b-b761-7b13295edd00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this Q-learning algorithm, we have the following variables being used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q matrix**: A 2D array that, at first, is populated with a fixed value for
    all elements (usually 0). It is used to hold the calculated policy over all states;
    that is, for every state, it holds the rewards for the respective possible actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**R matrix:** A 2D array that holds the initial rewards and allows the program
    to determine the list of possible actions for a specific state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Discount factor:** Determines the policy of the agent in how it deals with
    rewards. A discount factor closer to 0 will make the agent greedy by only considering
    current rewards, while a discount factor approaching 1 will make it more strategic
    and farsighted for better rewards in the long run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We should briefly highlight some of the methods of our Q-learning class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Init`: Called for generation of all possible states as well as for the start
    of the learning process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Learn`: Has sequential steps for the learning process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`InitRMatrix`: This initializes the reward matrix with one of these values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`0`: We do **not** have information about the reward when taking this action
    in this state'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`X`: There is no way to take this action in this state'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`100`: This is our big reward in the final state, where we want to go'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TrainQMatrix`: Contains the actual iterative value update rule of the Q matrix.
    When completed, we expect to have a trained agent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NormalizeQMatrix`: This normalizes the values of the Q matrix making them
    percentages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Test`: Provides textual input from the user and displays the optimal shortest
    path to solve the puzzle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look deeper into our `TrainQMatrix` code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the application with three disks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f64ce769-eca7-4410-a4e6-43b5ea7fdf3d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Running the application with four disks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b105397f-e116-46e2-88ba-cd5b1b64c195.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And here''s running with seven disks. The optimal number of moves is 127, so
    you can see how fast the solution can multiply the possible combinations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f284c705-27e8-4e11-b992-c8c3ca150248.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about reinforcement learning, various types of learning
    algorithms that go with it, and how you can apply it to real-world learning problems.
    In the next chapter, we're going to jump into fuzzy logic and see not only what
    it means, but also how we can apply it to everyday problems.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Wikipedia, Creative Commons ShareAlike License
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watkins, C.J.C.H. (1989), *Learning from Delayed Rewards* (Ph.D. thesis), Cambridge
    University
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Online Q-Learning using Connectionist Systems*, Rummery & Niranjan (1994)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wiering, Marco; Schmidhuber, Jürgen (1998-10-01), *Fast Online Q(λ)*. *Machine
    Learning*. **33** (1): 105-115'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Copyright (c) 2009-2017, Accord.NET Authors at: `authors@accord-framework.net`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kenan Deen, [https://kenandeen.wordpress.com/](https://kenandeen.wordpress.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
