<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer076">
			<p><a id="_idTextAnchor050"/></p>
			<h1 id="_idParaDest-51"><em class="italic"><a id="_idTextAnchor051"/>Chapter 3</em>: Exploring Regression with TPOT</h1>
			<p>In this chapter, you'll get hands-on experience with automated regression modeling through three datasets. You will learn how to handle regression tasks with TPOT in an automated manner with tons of practical examples, tips, and advice.</p>
			<p>We will go through essential topics such as dataset loading, exploratory data analysis, and basic data preparation first. Then, we'll get our hands dirty with TPOT. You will learn how to train models in an automated way and how to evaluate those models.</p>
			<p>Before training models automatically, we will see how good performance can be obtained with basic models, such as linear regression. These models will serve as a baseline that TPOT needs to outperform.</p>
			<p>This chapter will cover the following topics:</p>
			<ul>
				<li>Applying automated regression modeling to the fish market dataset</li>
				<li>Applying automated regression modeling to the insurance dataset</li>
				<li>Applying automated regression modeling to the vehicle dataset</li>
			</ul>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor052"/>Technical requirements</h1>
			<p>To complete this chapter, you will need a computer with Python and TPOT installed. The previous chapter demonstrated how to set up the environment from scratch for both standalone Python installation and installation through Anaconda. Refer to <a href="B16954_02_Final_SK_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 2</em></a>, <em class="italic">Deep Dive into TPOT</em>, for detailed instructions on environment setup.</p>
			<p>You can download the source code and datasets for this chapter here: <a href="https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter03">https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter03</a><a href="https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter3%20"/></p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor053"/>Applying automated regression modeling to the fish market dataset</h1>
			<p>This section demonstrates how to apply machine learning automation with TPOT to a regression <a id="_idIndexMarker146"/>dataset. The section uses the fish market dataset (<a href="https://www.kaggle.com/aungpyaeap/fish-market">https://www.kaggle.com/aungpyaeap/fish-market</a>) for exploration and regression modeling. The <a id="_idIndexMarker147"/>goal is to predict the weight of a fish. You <a id="_idIndexMarker148"/>will learn how to load the dataset, visualize it, adequately prepare it, and how to find the best machine learning pipeline with TPOT:</p>
			<ol>
				<li>The first thing to do is to load in the required libraries and load in the dataset. With regards to the libraries, you'll need <strong class="source-inline">numpy</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">m</strong><strong class="source-inline">atplotlib</strong>, and <strong class="source-inline">seaborn</strong>. Additionally, the <strong class="source-inline">rcParams</strong> module is imported with <strong class="source-inline">matplotlib</strong> to tweak the plot stylings a bit. You can find the code for this step in the following block:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sns</p><p class="source-code">from matplotlib import rcParams</p><p class="source-code">rcParams['axes.spines.top'] = False</p><p class="source-code">rcParams['axes.spines.right'] = False</p><p class="source-code">df = pd.read_csv('data/Fish.csv')</p><p class="source-code">df.head()</p><p>Here's how the first couple of rows look (the result from calling the <strong class="source-inline">head()</strong> method):</p><div id="_idContainer038" class="IMG---Figure"><img src="Images/B16954_03_001.jpg" alt="Figure 3.1 – First five rows of the fish market dataset&#13;&#10;" width="1170" height="416"/></div><p class="figure-caption">Figure 3.1 – First five rows of the fish market dataset</p></li>
				<li>Exploratory <a id="_idIndexMarker149"/>data analysis comes in next. It's not a hard requirement for using TPOT, but you should always be aware of how your data looks. The first thing of interest <a id="_idIndexMarker150"/>is missing values. Here's how to check for them:<p class="source-code">df.isnull().sum()</p><p>And here's the corresponding output:</p><div id="_idContainer039" class="IMG---Figure"><img src="Images/B16954_03_002.jpg" alt="Figure 3.2 – Count of missing values per column&#13;&#10;" width="286" height="329"/></div><p class="figure-caption">Figure 3.2 – Count of missing values per column</p><p>As you can see, there are no missing values. This makes the data preparation process much easier and shorter.</p></li>
				<li>The next step is to check how the target variable is distributed. For this dataset, we are <a id="_idIndexMarker151"/>trying to predict <strong class="source-inline">Weight</strong>. Here's the code for drawing a simple histogram:<p class="source-code">plt.figure(figsize=(12, 7))</p><p class="source-code">plt.title('Target variable (Weight) distribution', size=20)</p><p class="source-code">plt.xlabel('Weight', size=14)</p><p class="source-code">plt.ylabel('Count', size=14)</p><p class="source-code">plt.hist(df['Weight'], bins=15, color='#4f4f4f', ec='#040404');</p><p>And here's how the histogram looks:</p><div id="_idContainer040" class="IMG---Figure"><img src="Images/B16954_03_003.jpg" alt="Figure 3.3 – Histogram of the target variable (Weight)&#13;&#10;" width="1261" height="780"/></div><p class="figure-caption">Figure 3.3 – Histogram of the target variable (Weight)</p><p>Most <a id="_idIndexMarker152"/>of the fish are light, but there are a couple of heavy ones present. Let's explore species further to get a better grasp. </p></li>
				<li>The following code prints how many instances of a specific species there are (the number and percentage of the total), and also prints average and standard deviation for every attribute. To be more precise, a subset of the original dataset is kept where the species equals the specified species. Afterward, the number of records, total percentage, mean, and standard deviation are printed for every column in the subset.<p>This <a id="_idIndexMarker153"/>function <a id="_idIndexMarker154"/>is then called for every unique species:</p><p class="source-code">def describe_species(species):</p><p class="source-code">    subset = df[df['Species'] == species]</p><p class="source-code">    print(f'============ {species.upper()} ============')</p><p class="source-code">    print(f'Count: {len(subset)}')</p><p class="source-code">    print(f'Pct. total: {(len(subset) / len(df) * 100):.2f}%')</p><p class="source-code">    for column in df.columns[1:]:</p><p class="source-code">        avg = np.round(subset[column].mean(), 2)</p><p class="source-code">        sd = np.round(subset[column].std(), 2)</p><p class="source-code">        print(f'Avg. {column:&gt;7}: {avg:6} +/- {sd:6}')</p><p class="source-code">for species in df['Species'].unique():</p><p class="source-code">    describe_species(species)</p><p class="source-code">    print()</p><p>Here's the corresponding output:</p><div id="_idContainer041" class="IMG---Figure"><img src="Images/B16954_03_004.jpg" alt="Figure 3.4 – Feature exploration for every fish species&#13;&#10;" width="1280" height="698"/></div><p class="figure-caption">Figure 3.4 – Feature exploration for every fish species</p></li>
				<li>Finally, let's <a id="_idIndexMarker155"/>check for correlation between attributes. Correlation can be calculated only for numerical attributes. The following snippet shows you how to <a id="_idIndexMarker156"/>visualize a correlation matrix with the <strong class="source-inline">seaborn</strong> library:<p class="source-code">plt.figure(figsize=(12, 9))</p><p class="source-code">plt.title('Correlation matrix', size=20)</p><p class="source-code">sns.heatmap(df.corr(), annot=True, cmap='Blues');</p><p>Here's the correlation matrix:</p><div id="_idContainer042" class="IMG---Figure"><img src="Images/B16954_03_005.jpg" alt="Figure 3.5 – Correlation matrix of features&#13;&#10;" width="1051" height="863"/></div><p class="figure-caption">Figure 3.5 – Correlation matrix of features</p><p>You can do more in the exploratory data analysis process, but we'll stop here. This book shows you how to build automated models with TPOT, so we should spend most of the time there.</p></li>
				<li>There's <a id="_idIndexMarker157"/>one step left to do before modeling, and that is data preparation. We can't pass non-numerical <a id="_idIndexMarker158"/>attributes to the pipeline optimizer. We'll convert them to dummy variables for simplicity's sake and merge them with the original data afterward. Here's the code for doing so:<p class="source-code">species_dummies = pd.get_dummies(df['Species'], drop_first=True, prefix='Is')</p><p class="source-code">df = pd.concat([species_dummies, df], axis=1)</p><p class="source-code">df.drop('Species', axis=1, inplace=True)</p><p class="source-code">df.head()</p><p>And here's how the dataset looks now:</p><div id="_idContainer043" class="IMG---Figure"><img src="Images/B16954_03_006.jpg" alt="Figure 3.6 – First five rows of the fish market dataset after data preparation&#13;&#10;" width="1228" height="265"/></div><p class="figure-caption">Figure 3.6 – First five rows of the fish market dataset after data preparation</p><p>As you can see, we deleted the <strong class="source-inline">Species</strong> column because it's not needed anymore. Let's begin with the modeling next.</p></li>
				<li>To start, we need to make a couple of imports and decide on the scoring strategy. TPOT comes with a couple of regression scoring metrics. The default one is <strong class="source-inline">neg_mean_squared_error</strong>. We can't escape the negative metric, but we <a id="_idIndexMarker159"/>can at <a id="_idIndexMarker160"/>least make it be in the same units as the target variable is. It makes no sense to predict <a id="_idIndexMarker161"/>weight and keep track of errors in weight squared. That's where <strong class="bold">Root Mean Squared Error</strong> (<strong class="bold">RMSE</strong>) comes into play. It is a simple metric that calculates the square root of the previously discussed mean squared error. Due to the square root operations, we're tracking errors in the original units (weight) instead of squared units (weight squared). We will define it with the help of lambda functions:<p class="source-code">from tpot import TPOTRegressor</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.metrics import mean_squared_error, make_scorer</p><p class="source-code">rmse = lambda y, y_hat: np.sqrt(mean_squared_error(y, y_hat))</p></li>
				<li>Next on the requirement list is the train test split. We will keep 75% of the data for training and evaluate on the rest:<p class="source-code">X = df.drop('Weight', axis=1)</p><p class="source-code">y = df['Weight']</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(</p><p class="source-code">    X, y, test_size=0.25, random_state=42</p><p class="source-code">)</p><p>Here's how many instances are in the train and test sets, respectively:</p><div id="_idContainer044" class="IMG---Figure"><img src="Images/B16954_03_007.jpg" alt="Figure 3.7 – Number of instances in the training and test sets&#13;&#10;" width="120" height="19"/></div><p class="figure-caption">Figure 3.7 – Number of instances in the training and test sets</p></li>
				<li>Next, let's <a id="_idIndexMarker162"/>make a model with the linear regression algorithm. This model is just a baseline <a id="_idIndexMarker163"/>that TPOT needs to outperform:<p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">lm = LinearRegression()</p><p class="source-code">lm.fit(X_train, y_train)</p><p class="source-code">lm_preds = lm.predict(X_test)</p><p class="source-code">rmse(y_test, lm_preds)</p><p>Here's the corresponding RMSE value for linear regression on the test set:</p><div id="_idContainer045" class="IMG---Figure"><img src="Images/B16954_03_008.jpg" alt="Figure 3.8 – RMSE score for the linear regression model (baseline)&#13;&#10;" width="380" height="45"/></div><p class="figure-caption">Figure 3.8 – RMSE score for the linear regression model (baseline)</p><p>The baseline model is wrong by 82 units of weight on average. Not bad, considering we have weights up to 1,500. </p></li>
				<li>Next, let's fit a TPOT pipeline optimization model. We will use our RMSE scorer and <a id="_idIndexMarker164"/>perform the optimization for 10 minutes. You can optimize for more time, but 10 minutes <a id="_idIndexMarker165"/>should outperform the baseline model:<p class="source-code">rmse_scorer = make_scorer(rmse, greater_is_better=False)</p><p class="source-code">pipeline_optimizer = TPOTRegressor(</p><p class="source-code">    scoring=rmse_scorer,</p><p class="source-code">    max_time_mins=10,</p><p class="source-code">    random_state=42</p><p class="source-code">)</p><p class="source-code">pipeline_optimizer.fit(X_train, y_train)</p><p>After the optimization has finished, here's the output that's shown in the console:</p><div id="_idContainer046" class="IMG---Figure"><img src="Images/B16954_03_009.jpg" alt="Figure 3.9 – TPOT regressor output&#13;&#10;" width="1313" height="84"/></div><p class="figure-caption">Figure 3.9 – TPOT regressor output</p></li>
				<li>Here's how to obtain the RMSE score:<p class="source-code">pipeline_optimizer.score(X_test, y_test)</p><p>And here is the corresponding output:</p><div id="_idContainer047" class="IMG---Figure"><img src="Images/B16954_03_010.jpg" alt="Figure 3.10 – RMSE score for TPOT optimized pipeline model&#13;&#10;" width="384" height="41"/></div><p class="figure-caption">Figure 3.10 – RMSE score for TPOT optimized pipeline model</p><p>Don't <a id="_idIndexMarker166"/>worry about the minus sign before the number. The actual RMSE is 73.35 units of weight. The TPOT model outperformed the baseline one. That's all you need to know. TPOT <a id="_idIndexMarker167"/>gives us access to the best pipeline through the <strong class="source-inline">fitted_pipeline_</strong> attribute. Here's how it looks:</p><div id="_idContainer048" class="IMG---Figure"><img src="Images/B16954_03_011.jpg" alt="Figure 3.11 – Full TPOT pipeline&#13;&#10;" width="1314" height="372"/></div><p class="figure-caption">Figure 3.11 – Full TPOT pipeline</p></li>
				<li>As a final step, we can export the pipeline to a Python file. Here's how:<p class="source-code">pipeline_optimizer.export('fish_pipeline.py')</p><p>Here's what the file looks like:</p></li>
			</ol>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="Images/B16954_03_012.jpg" alt="Figure 3.12 – Source code of the TPOT pipeline&#13;&#10;" width="1096" height="792"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – Source code of the TPOT pipeline</p>
			<p>You can now use this file to make predictions on new, unseen data.</p>
			<p>In this <a id="_idIndexMarker168"/>section, you've <a id="_idIndexMarker169"/>built your first automated machine learning pipeline with TPOT on a simple dataset. Most of the time, in practice, the steps you take will look similar. It's the data cleaning and preparation where things differ. Always make sure to prepare your dataset adequately before passing it to TPOT. Sure, TPOT does many things for you, but it can't turn garbage data into a usable model. </p>
			<p>In the next section, you'll see how to apply TPOT to the medical insurance dataset.</p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor054"/>Applying automated regression modeling to the insurance dataset</h1>
			<p>This <a id="_idIndexMarker170"/>section demonstrates <a id="_idIndexMarker171"/>how to apply an automated machine learning solution to a slightly more complicated dataset. You <a id="_idIndexMarker172"/>will use the medical insurance cost dataset (<a href="https://www.kaggle.com/mirichoi0218/insurance">https://www.kaggle.com/mirichoi0218/insurance</a>) to predict how much insurance will cost based on a couple of predictor variables. You will learn how to load the dataset, perform exploratory data analysis, how to prepare it, and how to find the best machine learning pipeline with TPOT: </p>
			<ol>
				<li value="1">As with the previous example, the first step is to load in the libraries and the dataset. We'll need <strong class="source-inline">numpy</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">m</strong><strong class="source-inline">atplotlib</strong>, and <strong class="source-inline">seaborn</strong> to start with the analysis. Here's how to import the libraries and load the dataset:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sns</p><p class="source-code">from matplotlib import rcParams</p><p class="source-code">rcParams['axes.spines.top'] = False</p><p class="source-code">rcParams['axes.spines.right'] = False</p><p class="source-code">df = pd.read_csv('data/insurance.csv')</p><p class="source-code">df.head()</p><p>The first five rows are shown in the following figure:</p><div id="_idContainer050" class="IMG---Figure"><img src="Images/B16954_03_013.jpg" alt="Figure 3.13 – First five rows of the insurance dataset&#13;&#10;" width="1054" height="376"/></div><p class="figure-caption">Figure 3.13 – First five rows of the insurance dataset</p></li>
				<li>We'll <a id="_idIndexMarker173"/>continue with the exploratory data analysis. As with the previous example, we'll <a id="_idIndexMarker174"/>first check for the number of missing values. Here's the code for doing so:<p class="source-code">df.isnull().sum()</p><p>The following figure shows counts of missing values per column:</p><div id="_idContainer051" class="IMG---Figure"><img src="Images/B16954_03_014.jpg" alt="Figure 3.14 – Missing value counts per column for the insurance dataset&#13;&#10;" width="298" height="304"/></div><p class="figure-caption">Figure 3.14 – Missing value counts per column for the insurance dataset</p><p>As you can see, there are no missing values. </p></li>
				<li>We're trying to predict the <strong class="source-inline">charges</strong> column with this dataset, so let's quickly check what type of values we can expect there. A histogram seems like an easy enough option. Here's the code needed for drawing one:<p class="source-code">plt.figure(figsize=(12, 7))</p><p class="source-code">plt.title('Target variable (charges) distribution', size=20)</p><p class="source-code">plt.xlabel('Charge', size=14)</p><p class="source-code">plt.ylabel('Count', size=14)</p><p class="source-code">plt.hist(df['charges'], bins=15, color='#4f4f4f', ec='#040404');</p><p>And here's the resulting histogram:</p><div id="_idContainer052" class="IMG---Figure"><img src="Images/B16954_03_015.jpg" alt="Figure 3.15 – Distribution of the target variable&#13;&#10;" width="1269" height="788"/></div><p class="figure-caption">Figure 3.15 – Distribution of the target variable</p><p>So, values <a id="_idIndexMarker175"/>even go <a id="_idIndexMarker176"/>above $60,000.00. Most of them are lower, so it will be interesting to see how the model will handle it.</p></li>
				<li>Let's dive deeper into the analysis and explore other variables. The goal is to see the average insurance costs for every categorical variable segment. We'll use the median as an average value, as it's less prone to outliers.<p>The easiest way to approach this analysis is to make a function that makes a bar chart for the specified column. The following function will come in handy for this example and many others in the future. It calculates a median from a grouped dataset and visualizes a bar chart with a title, labels, a legend, and text on top <a id="_idIndexMarker177"/>of the bars. You can use this function in general to visualize medians of some variable <a id="_idIndexMarker178"/>after a grouping operation is performed. It's best suited for categorical variables:</p><p class="source-code">def make_bar_chart(column, title, ylabel, xlabel, y_offset=0.12, x_offset=700):</p><p class="source-code">    ax = df.groupby(column).median()[['charges']].plot(</p><p class="source-code">        kind='bar', figsize=(10, 6), fontsize=13, color='#4f4f4f'</p><p class="source-code">    )</p><p class="source-code">    ax.set_title(title, size=20, pad=30)</p><p class="source-code">    ax.set_ylabel(ylabel, fontsize=14)</p><p class="source-code">    ax.set_xlabel(xlabel, fontsize=14)</p><p class="source-code">    ax.get_legend().remove()</p><p class="source-code">                  </p><p class="source-code">    for i in ax.patches:</p><p class="source-code">        ax.text(i.get_x() + x_offset, i.get_height() + y_offset, f'${str(round(i.get_height(), 2))}', fontsize=15)</p><p class="source-code">    return ax</p></li>
				<li>Let's now use this function to visualize the median insurance cost for smokers and non-smokers. Here's the code:<p class="source-code">make_bar_chart(</p><p class="source-code">    column='smoker',</p><p class="source-code">    title='Median insurance charges for smokers and non-smokers',</p><p class="source-code">    ylabel='Insurance charge ($)',</p><p class="source-code">    xlabel='Do they smoke?',</p><p class="source-code">    y_offset=700,</p><p class="source-code">    x_offset=0.12</p><p class="source-code">)</p><p>And <a id="_idIndexMarker179"/>here's <a id="_idIndexMarker180"/>the corresponding visualization:</p><div id="_idContainer053" class="IMG---Figure"><img src="Images/B16954_03_016.jpg" alt="Figure 3.16 – Median insurance charges for smokers and non-smokers&#13;&#10;" width="1188" height="783"/></div><p class="figure-caption">Figure 3.16 – Median insurance charges for smokers and non-smokers</p><p>As you can see, smokers pay an insurance fee several times higher than non-smokers. </p></li>
				<li>Let's make a similar-looking visualization for comparing median insurance costs between genders:<p class="source-code">make_bar_chart(</p><p class="source-code">    column='sex',</p><p class="source-code">    title='Median insurance charges between genders',</p><p class="source-code">    ylabel='Insurance charge ($)',</p><p class="source-code">    xlabel='Gender',</p><p class="source-code">    y_offset=200,</p><p class="source-code">    x_offset=0.15</p><p class="source-code">)</p><p>You <a id="_idIndexMarker181"/>can see <a id="_idIndexMarker182"/>the visualization here:</p><div id="_idContainer054" class="IMG---Figure"><img src="Images/B16954_03_017.jpg" alt="Figure 3.17 – Median insurance charges between genders&#13;&#10;" width="1136" height="817"/></div><p class="figure-caption">Figure 3.17 – Median insurance charges between genders</p><p>Not much of a difference here. </p></li>
				<li>But <a id="_idIndexMarker183"/>what will <a id="_idIndexMarker184"/>happen if we compare median insurance costs by the number of children? The following code snippet does just that:<p class="source-code">make_bar_chart(</p><p class="source-code">    column='children',</p><p class="source-code">    title='Median insurance charges by number of children',</p><p class="source-code">    ylabel='Insurance charge ($)',</p><p class="source-code">    xlabel='Number of children',</p><p class="source-code">    y_offset=200,</p><p class="source-code">    x_offset=-0.15</p><p class="source-code">)</p><p>Here's how the costs are distributed:</p><div id="_idContainer055" class="IMG---Figure"><img src="Images/B16954_03_018.jpg" alt="Figure 3.18 – Median insurance charges by number of children&#13;&#10;" width="1135" height="763"/></div><p class="figure-caption">Figure 3.18 – Median insurance charges by number of children</p><p>The <a id="_idIndexMarker185"/>insurance costs seem to go up until the fifth child. Maybe there aren't that many families with five children. Can you confirm that on your own?</p></li>
				<li>What <a id="_idIndexMarker186"/>about the region? Here's the code for visualizing median insurance costs by region:<p class="source-code">make_bar_chart(</p><p class="source-code">    column='region',</p><p class="source-code">    title='Median insurance charges by region',</p><p class="source-code">    ylabel='Insurance charge ($)',</p><p class="source-code">    xlabel='Region',</p><p class="source-code">    y_offset=200,</p><p class="source-code">    x_offset=0</p><p class="source-code">)</p><p>The cost distribution per region is shown in the following figure:</p><div id="_idContainer056" class="IMG---Figure"><img src="Images/B16954_03_019.jpg" alt="Figure 3.19 – Median insurance charges by region&#13;&#10;" width="1021" height="742"/></div><p class="figure-caption">Figure 3.19 – Median insurance charges by region</p><p>The values don't differ that much. </p><p>We've <a id="_idIndexMarker187"/>made a <a id="_idIndexMarker188"/>decent amount of visualizations and explored the dataset. It's now time to prepare it and apply machine learning models.</p></li>
				<li>There are a couple of things we need to do for this dataset to be machine learning ready. First, we'll have to remap string values to integers for the columns <strong class="source-inline">sex</strong> and <strong class="source-inline">smoker</strong>. Then, we'll need to create dummy variables for the <strong class="source-inline">region</strong> column. This step is necessary because TPOT can't understand raw textual data.<p>Here's <a id="_idIndexMarker189"/>the code <a id="_idIndexMarker190"/>snippet that does the necessary preparation:</p><p class="source-code">df['sex'] = [1 if x == 'female' else 0 for x in df['sex']]</p><p class="source-code">df.rename(columns={'sex': 'is_female'}, inplace=True)</p><p class="source-code">df['smoker'] = [1 if x == 'yes' else 0 for x in df['smoker']]</p><p class="source-code">region_dummies = pd.get_dummies(df['region'], drop_first=True, prefix='region')</p><p class="source-code">df = pd.concat([region_dummies, df], axis=1)</p><p class="source-code">df.drop('region', axis=1, inplace=True)</p><p class="source-code">df.head()</p><p>Calling the <strong class="source-inline">head()</strong> function results in the dataset shown in the following figure:</p><div id="_idContainer057" class="IMG---Figure"><img src="Images/B16954_03_020.jpg" alt="Figure 3.20 – Insurance dataset after preparation&#13;&#10;" width="1232" height="268"/></div><p class="figure-caption">Figure 3.20 – Insurance dataset after preparation</p></li>
				<li>The dataset is now ready for predictive modeling. Before we do so, let's check for <a id="_idIndexMarker191"/>variable correlations with the target variable. The following snippet draws the correlation <a id="_idIndexMarker192"/>matrix with annotations:<p class="source-code">plt.figure(figsize=(12, 9))</p><p class="source-code">plt.title('Correlation matrix', size=20)</p><p class="source-code">sns.heatmap(df.corr(), annot=True, cmap='Blues');</p><p>The corresponding correlation matrix is shown in the following figure:</p><div id="_idContainer058" class="IMG---Figure"><img src="Images/B16954_03_021.jpg" alt="Figure 3.21 – Insurance dataset correlation matrix&#13;&#10;" width="1048" height="890"/></div><p class="figure-caption">Figure 3.21 – Insurance dataset correlation matrix</p><p>Next stop – predictive modeling.</p></li>
				<li>As <a id="_idIndexMarker193"/>before, the <a id="_idIndexMarker194"/>first step is to make a train/test split. The following code snippet shows you how to do that:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">X = df.drop('charges', axis=1)</p><p class="source-code">y = df['charges']</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(</p><p class="source-code">    X, y, test_size=0.25, random_state=42</p><p class="source-code">)</p><p class="source-code">y_train.shape, y_test.shape</p><p>The number of training and testing instances is shown in the following figure:</p><div id="_idContainer059" class="IMG---Figure"><img src="Images/B16954_03_022.jpg" alt="Figure 3.22 – Number of instances in train and test sets&#13;&#10;" width="387" height="63"/></div><p class="figure-caption">Figure 3.22 – Number of instances in train and test sets</p></li>
				<li>We'll first make a baseline model with a linear regression algorithm. It will serve as something TPOT must outperform. You'll find a code snippet for training a baseline model here:<p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.metrics import r2_score, mean_squared_error</p><p class="source-code">rmse = lambda y, y_hat: np.sqrt(mean_squared_error(y, y_hat))</p><p class="source-code">lm = LinearRegression()</p><p class="source-code">lm.fit(X_train, y_train)</p><p class="source-code">lm_preds = lm.predict(X_test)</p><p class="source-code">print(f'R2   = {r2_score(y_test, lm_preds):.2f}')</p><p class="source-code">print(f'RMSE = {rmse(y_test, lm_preds):.2f}')</p><p>Coefficient of determination (R2) and root mean squared error (RMSE) values are shown in the following figure:</p><div id="_idContainer060" class="IMG---Figure"><img src="Images/B16954_03_023.jpg" alt="Figure 3.23 – R2 and RMSE for the linear regression model&#13;&#10;" width="332" height="85"/></div><p class="figure-caption">Figure 3.23 – R2 and RMSE for the linear regression model</p><p>On average, a simple linear regression model is wrong by $5,926.02. This simple model captures 77% of the variance in the dataset. </p></li>
				<li>We can <a id="_idIndexMarker195"/>further explore the linear regression model's feature importance by examining the assigned weights (coefficients). <p>The <a id="_idIndexMarker196"/>following code snippet prints the variable name and its corresponding coefficient:</p><p class="source-code">for i, column in enumerate(df.columns[:-1]):</p><p class="source-code">    coef = np.round(lm.coef_[i], 2)</p><p class="source-code">    print(f'{column:17}: {coef:8}')</p><p>The output is shown in the following figure:</p><div id="_idContainer061" class="IMG---Figure"><img src="Images/B16954_03_024.jpg" alt="Figure 3.24 – Coefficients of a linear regression model&#13;&#10;" width="603" height="325"/></div><p class="figure-caption">Figure 3.24 – Coefficients of a linear regression model</p><p>As you can see, the column with the largest coefficient is <strong class="source-inline">smoker</strong>. That makes sense, as it confirms our visualization made in the exploratory data analysis phase.</p></li>
				<li>It's now <a id="_idIndexMarker197"/>time to bring in the big guns. We'll use the TPOT library to produce an automated <a id="_idIndexMarker198"/>machine learning pipeline. We'll optimize the pipeline for R2 score this time, but feel free to stick with RMSE or any other metric.<p>The following code snippet imports the TPOT library, instantiates it, and fits the pipeline:</p><p class="source-code">from tpot import TPOTRegressor</p><p class="source-code">pipeline_optimizer = TPOTRegressor(</p><p class="source-code">    scoring='r2',</p><p class="source-code">    max_time_mins=10,</p><p class="source-code">    random_state=42,</p><p class="source-code">    verbosity=2</p><p class="source-code">)</p><p class="source-code">pipeline_optimizer.fit(X_train, y_train)</p><p>After 10 minutes, you should see the following output in your notebook:</p><div id="_idContainer062" class="IMG---Figure"><img src="Images/B16954_03_025.jpg" alt="Figure 3.25 – TPOT score per generation&#13;&#10;" width="1278" height="494"/></div><p class="figure-caption">Figure 3.25 – TPOT score per generation</p><p>The score on the training set started to increase in the last couple of generations. You'd likely get a slightly better model if you gave TPOT more time to train.</p></li>
				<li>The R2 <a id="_idIndexMarker199"/>score on the test set can be obtained with the following code:<p class="source-code">pipeline_optimizer.score(X_test, y_test)</p><p>The <a id="_idIndexMarker200"/>score is shown in the following figure:</p><div id="_idContainer063" class="IMG---Figure"><img src="Images/B16954_03_026.jpg" alt="Figure 3.26 – TPOT R2 score on the test set&#13;&#10;" width="414" height="40"/></div><p class="figure-caption">Figure 3.26 – TPOT R2 score on the test set</p></li>
				<li>You can obtain R2 and RMSE values for the test set manually. The following code snippet shows you how:<p class="source-code">tpot_preds = pipeline_optimizer.predict(X_test)</p><p class="source-code">print(f'R2   = {r2_score(y_test, tpot_preds):.2f}')</p><p class="source-code">print(f'RMSE = {rmse(y_test, tpot_preds):.2f}')</p><p>The corresponding scores are shown here:</p><div id="_idContainer064" class="IMG---Figure"><img src="Images/B16954_03_027.jpg" alt="Figure 3.27 – TPOT R2 and RMSE scores on the test set&#13;&#10;" width="326" height="79"/></div><p class="figure-caption">Figure 3.27 – TPOT R2 and RMSE scores on the test set</p></li>
				<li>As the <a id="_idIndexMarker201"/>last step, we'll <a id="_idIndexMarker202"/>export the optimized pipeline to a Python file. The following code snippet does it:<p class="source-code">pipeline_optimizer.export('insurance_pipeline.py')</p><p>The Python code for the optimized pipeline is shown here:</p></li>
			</ol>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="Images/B16954_03_028.jpg" alt="Figure 3.28 – TPOT optimized pipeline for the insurance dataset&#13;&#10;" width="1116" height="959"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.28 – TPOT optimized pipeline for the insurance dataset</p>
			<p>You <a id="_idIndexMarker203"/>can now use this file to make predictions on new, unseen data. It would be best to leave the pipeline to perform the optimization for as long as needed, but even 10 minutes was enough to produce good-quality models.</p>
			<p>This section <a id="_idIndexMarker204"/>showed you how to build automated pipelines optimized for different metrics and with a bit more verbose output printed to the console. As you can see, the code for optimization is more or less identical. It's the data preparation that changes drastically from project to project, and that's where you'll spend most of your time.</p>
			<p>In the next section, you'll see how to apply TPOT to the vehicle dataset.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Applying automated regression modeling to the vehicle dataset</h1>
			<p>This section shows how to develop an automated machine learning model on the most complex <a id="_idIndexMarker205"/>dataset thus far. You will <a id="_idIndexMarker206"/>use the vehicle dataset (<a href="https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho">https://www.kaggle.com/nehalbirla/vehicle-dataset-from-cardekho</a>), so download it if <a id="_idIndexMarker207"/>you haven't already. The goal is to predict the selling price based on the various predictors, such as year made and kilometers driven. </p>
			<p>This time, we won't focus on exploratory data analysis. You can do that on your own if you've followed the last two examples. Instead, we'll concentrate on dataset preparation and model training. There's a lot of work required to transform this dataset into something ready for machine learning, so let's get started right away:</p>
			<ol>
				<li value="1">Once again, the first step is to load in the libraries and the dataset. The requirements are the same as with previous examples. You'll need <strong class="source-inline">numpy</strong>, <strong class="source-inline">p</strong><strong class="source-inline">andas</strong>, <strong class="source-inline">matplotlib</strong>, and <strong class="source-inline">seaborn</strong>. Here's how to import the libraries and load the dataset:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import seaborn as sns</p><p class="source-code">from matplotlib import rcParams</p><p class="source-code">rcParams['axes.spines.top'] = False</p><p class="source-code">rcParams['axes.spines.right'] = False</p><p class="source-code">df = pd.read_csv('data/Car.csv')</p><p class="source-code">df.head()</p><p>Calling the <strong class="source-inline">head()</strong> function displays the first five rows. You can see how they look in the following figure:</p><div id="_idContainer066" class="IMG---Figure"><img src="Images/B16954_03_029.jpg" alt="Figure 3.29 – First five rows of the vehicle dataset&#13;&#10;" width="1273" height="298"/></div><p class="figure-caption">Figure 3.29 – First five rows of the vehicle dataset</p></li>
				<li>The <a id="_idIndexMarker208"/>dataset has <a id="_idIndexMarker209"/>a lot of columns, and not all of them are shown in <em class="italic">Figure 3.29</em>. The next step in the data preparation phase is to check for missing values. The following code snippet does that:<p class="source-code">df.isnull().sum()</p><p>The results are shown in the following figure:</p><div id="_idContainer067" class="IMG---Figure"><img src="Images/B16954_03_030.jpg" alt="Figure 3.30 – Count of missing values for the vehicle dataset&#13;&#10;" width="384" height="483"/></div><p class="figure-caption">Figure 3.30 – Count of missing values for the vehicle dataset</p><p>Some of the values are missing, and we'll address this issue with the simplest approach – by removing them. </p></li>
				<li>Removing missing values might not always be the best option. You should always investigate why the values are missing and if they can (or should) be somehow filled. This book focuses on machine learning automation, so we won't do that here.<p>Here's <a id="_idIndexMarker210"/>how you <a id="_idIndexMarker211"/>can drop the missing values:</p><p class="source-code">df.dropna(inplace=True)</p><p class="source-code">df.isnull().sum()</p><p>Executing the preceding code results in the following count:</p><div id="_idContainer068" class="IMG---Figure"><img src="Images/B16954_03_031.jpg" alt="Figure 3.31 – Removing missing values from the vehicle dataset&#13;&#10;" width="348" height="487"/></div><p class="figure-caption">Figure 3.31 – Removing missing values from the vehicle dataset</p></li>
				<li>There are no missing values now, but that doesn't mean we're done with data preparation. Here's the list of steps required to make this dataset suitable for machine learning:<ul><li>Convert the <strong class="source-inline">transmission</strong> column to an integer – 1 if <em class="italic">manual</em>, 0 otherwise. Also, rename the column to <strong class="source-inline">is_manual</strong>.</li><li>Remap the <strong class="source-inline">owner</strong> column to integers. Check the <strong class="source-inline">remap_owner()</strong> function for further clarifications.</li><li>Extract car brand, mileage, engine, and max power from the corresponding attributes. The value of interest for all of the mentioned attributes is everything before the first space.</li><li>Create <a id="_idIndexMarker212"/>dummy variables from the attributes <strong class="source-inline">name</strong>, <strong class="source-inline">fuel</strong>, and <strong class="source-inline">seller_type</strong>.</li><li>Concatenate the original dataset with dummy variables and drop unnecessary attributes.<p>Here <a id="_idIndexMarker213"/>is the code for the <strong class="source-inline">remap_owner()</strong> function:</p><p class="source-code">def remap_owner(owner):</p><p class="source-code">    if owner == 'First Owner': return 1</p><p class="source-code">    elif owner == 'Second Owner': return 2</p><p class="source-code">    elif owner == 'Third Owner': return 3</p><p class="source-code">    elif owner == 'Fourth &amp; Above Owner': return 4</p><p class="source-code">    else: return 0</p><p>And here is the code for performing all of the mentioned transformations:</p><p class="source-code">df['transmission'] = [1 if x == 'Manual' else 0 for x in df['transmission']]</p><p class="source-code">df.rename(columns={'transmission': 'is_manual'}, inplace=True)</p><p class="source-code">df['owner'] = df['owner'].apply(remap_owner)</p><p class="source-code">df['name'] = df['name'].apply(lambda x: x.split()[0])</p><p class="source-code">df['mileage'] = df['mileage'].apply(lambda x: x.split()[0]).astype(float)</p><p class="source-code">df['engine'] = df['engine'].apply(lambda x: x.split()[0]).astype(int)</p><p class="source-code">df['max_power'] = df['max_power'].apply(lambda x: x.split()[0]).astype(float)</p><p class="source-code">brand_dummies = pd.get_dummies(df['name'], drop_first=True, prefix='brand')</p><p class="source-code">fuel_dummies = pd.get_dummies(df['fuel'], drop_first=True, prefix='fuel')</p><p class="source-code">seller_dummies = pd.get_dummies(df['seller_type'], drop_first=True, prefix='seller')</p><p class="source-code">df.drop(['name', 'fuel', 'seller_type', 'torque'], axis=1, inplace=True)</p><p class="source-code">df = pd.concat([df, brand_dummies, fuel_dummies, seller_dummies], axis=1)</p><p>After <a id="_idIndexMarker214"/>applying <a id="_idIndexMarker215"/>the transformations, the dataset looks like this:</p></li></ul></li>
			</ol>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="Images/B16954_03_032.jpg" alt="Figure 3.32 – Prepared vehicle dataset&#13;&#10;" width="1267" height="358"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.32 – Prepared vehicle dataset</p>
			<p>Data in this format can be passed to a machine learning algorithm. Let's do that next.</p>
			<ol>
				<li value="5">As always, we'll start with the train test split. The following code snippet shows you how to perform it on this dataset:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">X = df.drop('selling_price', axis=1)</p><p class="source-code">y = df['selling_price']</p><p class="source-code">X_train, X_test, y_train, y_test = train_test_split(</p><p class="source-code">    X, y, test_size=0.25, random_state=42</p><p class="source-code">)</p><p class="source-code">y_train.shape, y_test.shape</p><p>You <a id="_idIndexMarker216"/>can see how many instances are in both sets in <em class="italic">Figure 3.33</em>:</p><div id="_idContainer070" class="IMG---Figure"><img src="Images/B16954_03_033.jpg" alt="Figure 3.33 – Number of instances in train and test sets&#13;&#10;" width="368" height="41"/></div><p class="figure-caption">Figure 3.33 – Number of instances in train and test sets</p><p>As <a id="_idIndexMarker217"/>you can see, this is a much larger dataset than we had before.</p></li>
				<li>We won't <a id="_idIndexMarker218"/>use your standard metrics for evaluating regression models (R2 and RMSE) this time. We'll use <strong class="bold">Mean Absolute Percentage Error</strong> (<strong class="bold">MAPE</strong>) instead. The MAPE metric measures the error of a predictive model as a percentage. The metric can be calculated as an average of the absolute difference between the actual and predicted values divided by the actual values. You can also optionally multiply this value by 100 to get the actual percentage. It's a great evaluation metric if there are no outliers (extremes) in the data. This metric isn't built into the <strong class="source-inline">scikit-learn</strong> library, so we'll have to implement it manually. Here's how:<p class="source-code">def mape(y, y_hat): </p><p class="source-code">    y, y_hat = np.array(y), np.array(y_hat)</p><p class="source-code">    return np.mean(np.abs((y - y_hat) / y)) * 100</p></li>
				<li>And <a id="_idIndexMarker219"/>now it's time to make a baseline model. Once again, it will be a linear regression model, evaluated <a id="_idIndexMarker220"/>on the test set with R2 and MAPE metrics. Here's the code for implementing the baseline model:<p class="source-code">from sklearn.linear_model import LinearRegression</p><p class="source-code">from sklearn.metrics import r2_score</p><p class="source-code">lm = LinearRegression()</p><p class="source-code">lm.fit(X_train, y_train)</p><p class="source-code">lm_preds = lm.predict(X_test)</p><p class="source-code">print(f'R2   = {r2_score(y_test, lm_preds):.2f}')</p><p class="source-code">print(f'MAPE = {mape(y_test, lm_preds):.2f}')</p><p>The corresponding results are shown in the following figure:</p><div id="_idContainer071" class="IMG---Figure"><img src="Images/B16954_03_034.jpg" alt="Figure 3.34 – R2 and MAPE for the baseline model&#13;&#10;" width="258" height="78"/></div><p class="figure-caption">Figure 3.34 – R2 and MAPE for the baseline model</p><p>On average, the baseline model is wrong by 43%. It's a lot, but we have to start somewhere. </p></li>
				<li>Let's take a look at the linear regression model coefficient to determine which features are important. Here's the code for obtaining coefficients:<p class="source-code">for i, column in enumerate(df.columns[:-1]):</p><p class="source-code">    coef = np.round(lm.coef_[i], 2)</p><p class="source-code">    print(f'{column:20}: {coef:12}')</p><p>And <a id="_idIndexMarker221"/>here are the coefficients:</p><div id="_idContainer072" class="IMG---Figure"><img src="Images/B16954_03_035.jpg" alt="Figure 3.35 – Baseline model coefficients&#13;&#10;" width="1274" height="874"/></div><p class="figure-caption">Figure 3.35 – Baseline model coefficients</p><p>Just <a id="_idIndexMarker222"/>take a moment to appreciate how interpretable this is. The higher the year, the newer the car is, which results in a higher price. The more kilometers the vehicle has driven, the more the price decreases. It also looks like cars with automatic transmissions cost more. You get the point. Interpretability is something that linear regression offers. But it lacks accuracy. That's what TPOT will improve.</p></li>
				<li>Let's <a id="_idIndexMarker223"/>fit a TPOT <a id="_idIndexMarker224"/>model next and optimize it for MAPE score. We'll train the model for 10 minutes on every available CPU core (indicated by <strong class="source-inline">n_jobs=-1</strong>):<p class="source-code">from tpot import TPOTRegressor</p><p class="source-code">from sklearn.metrics import make_scorer</p><p class="source-code">mape_scorer = make_scorer(mape, greater_is_better=False)</p><p class="source-code">pipeline_optimizer = TPOTRegressor(</p><p class="source-code">    scoring=mape_scorer,</p><p class="source-code">    max_time_mins=10,</p><p class="source-code">    random_state=42,</p><p class="source-code">    verbosity=2,</p><p class="source-code">    n_jobs=-1</p><p class="source-code">)</p><p class="source-code">pipeline_optimizer.fit(X_train, y_train)</p><p>The output you'll see after 10 minutes is shown in the following figure:</p><div id="_idContainer073" class="IMG---Figure"><img src="Images/B16954_03_036.jpg" alt="Figure 3.36 – Output of a TPOT optimization process&#13;&#10;" width="1267" height="197"/></div><p class="figure-caption">Figure 3.36 – Output of a TPOT optimization process</p><p>It looks like 10 minutes wasn't nearly enough for TPOT to give its best. </p><p>The resulting pipeline is shown in the following figure:</p><div id="_idContainer074" class="IMG---Figure"><img src="Images/B16954_03_037.jpg" alt="Figure 3.37 – Best fitted pipeline after 10 minutes&#13;&#10;" width="1255" height="238"/></div><p class="figure-caption">Figure 3.37 – Best fitted pipeline after 10 minutes</p></li>
				<li>And <a id="_idIndexMarker225"/>now the <a id="_idIndexMarker226"/>moment of truth – did the MAPE decrease? Here's the code to find out:<p class="source-code">tpot_preds = pipeline_optimizer.predict(X_test)</p><p class="source-code">print(f'R2   = {r2_score(y_test, tpot_preds):.2f}')</p><p class="source-code">print(f'MAPE = {mape(y_test, tpot_preds):.2f}')</p><p>The output is shown in the following figure:</p></li>
			</ol>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="Images/B16954_03_038.jpg" alt="Figure 3.38 – R2 and MAPE for the TPOT optimized model&#13;&#10;" width="249" height="69"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.38 – R2 and MAPE for the TPOT optimized model</p>
			<p>As you can see, TPOT decreased the error significantly and increased the goodness of fit (R2) simultaneously. Just as expected.</p>
			<p>The final code-along section showed you how easy it is to train automated models on a more complex dataset. The procedure is more or less identical, depending on the metric you're optimizing for, but it's the data preparation phase that makes all the difference.</p>
			<p>If you spend more time preparing and analyzing the data, and maybe removing some noisy data, you <a id="_idIndexMarker227"/>will get <a id="_idIndexMarker228"/>better results, guaranteed! That's mainly the case when a lot of columns contain text data. A lot of features can be extracted from there.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor056"/>Summary</h1>
			<p>This was the first purely hands-on chapter in the book. You've connected the theory from the previous chapters with practice. You've built not one, but three fully automated machine learning models. Without any kind of doubt, you should now be able to use TPOT to solve any type of regression problem.</p>
			<p>As with most things in data science and machine learning, 90% of the work boils down to data preparation. TPOT can make this percentage even higher because less time is spent designing and tweaking the models. Use this extra time wisely, and get yourself fully acquainted with the dataset. There's no way around it.</p>
			<p>In the next chapter, you'll see how to build automated machine learning models for classification datasets. That chapter will also be entirely hands-on. Later, in <a href="B16954_05_Final_SK_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 5</em></a>, <em class="italic">Parallel Training with TPOT and Dask</em>, we'll combine both theory and practice.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/>Q&amp;A</h1>
			<ol>
				<li value="1">Which type of data visualization lets you explore the distribution of a continuous variable?</li>
				<li>Explain R2, RMSE, and MAPE metrics.</li>
				<li>Can you use a custom scoring function with TPOT? If yes, how?</li>
				<li>Why is it essential to build baseline models first? Which algorithm is considered as a "baseline" for regression tasks?</li>
				<li>What do the coefficients of a linear regression model tell you?</li>
				<li>How do you use all CPU cores when training TPOT models?</li>
				<li>Can you use TPOT to obtain the Python code of the best pipeline?</li>
			</ol>
		</div>
	</div></body></html>