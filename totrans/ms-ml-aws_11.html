<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing Deep Learning with TensorFlow on AWS</h1>
                </header>
            
            <article>
                
<p>TensorFlow is a very popular deep learning framework that can be used to train deep neural networks, such as those described in the previous chapter.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>About TensorFlow</li>
<li>TensorFlow as a general machine learning library</li>
<li>Training and serving the TensorFlow model through SageMaker</li>
<li>Creating a custom neural net with TensorFlow</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">About TensorFlow</h1>
                </header>
            
            <article>
                
<p>TensorFlow is a library for deep learning, first released by Google in 2015. Initially, it included a core library that allowed users to work with tensors (multidimensional arrays) in symbolic form, thus enabling low-level neural network design and training at high performance. Nowadays, it's a fully fledged deep learning library that allows data scientists to build models for complex problems, such as image recognition, using high-level primitives. You can also use TensorFlow for solving standard machine learning problems, such as the ones we've been considering in the past chapters. TensorFlow has similar abstractions to the ones we have been using in <kbd>scikit-learn</kbd>, Apache Spark, and SageMaker. For example, it allows the user to create classification models or regression models using high-level abstractions, such as estimators, predictors, and evaluators.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow as a general machine learning library</h1>
                </header>
            
            <article>
                
<p><span> In this section we will </span> show how we use TensorFlow to create a regression model for the house estimation problem of<span> </span><a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>,<span> </span><em>Predicting House Value with Regression Algorithms</em>. To get started, we will first launch a SageMaker notebook and choose the TensorFlow kernel (<kbd>conda_tensorflow_p36</kbd>), which has all the necessary TensorFlow dependencies needed for this section:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-700 image-border" src="assets/3d9cd933-f996-44b9-972a-bf56df7f2ace.png" style="width:17.25em;height:37.75em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, let's consider the estimation problem from <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms</em>. Recall that we had a set of indicators (age of the house, distance to nearest center, and so on) to estimate the median value of the house (expressed in the <kbd>medv</kbd> column, which is our target feature), as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-701 image-border" src="assets/d2519263-cc3e-49ff-90f5-505b78a5769b.png" style="width:111.67em;height:19.17em;"/></p>
<p>In <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms</em>, we identified 11 learning features to use for predicting the target feature (<kbd>medv</kbd>):</p>
<pre>training_features = ['crim', 'zn', 'indus', 'chas', 'nox',             'rm', 'age', 'dis', 'tax', 'ptratio', 'lstat']<br/><br/>label = 'medv'</pre>
<p>With this information, we define a TensorFlow linear regressor capable of solving our regression problem with a pre-built neural net:</p>
<pre>tf_regressor = tf.estimator.LinearRegressor(<br/>    feature_columns=[tf.feature_column.numeric_column('inputs', <br/>                                  shape=(11,))])</pre>
<p><span>For the regressor, we decided to create a single-feature</span> input<span>, which assembles the rest of the features into a vector of numbers that will represent the input layer. It is also possible to create one named feature per training feature (as we did in <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms</em>), but we'll just have a single vector feature to simplify the prediction service discussed at the end of the section.</span></p>
<p>To construct a regressor, we need to pass in the TensorFlow feature columns, which can be of several different kinds. The <kbd>tf.feature_column</kbd> package provides functions to construct different kinds of columns, depending on the encoding being used by the model (for example, categorical, bucketized, and so on.). The feature columns inform the model on the expected format of the data being submitted as input. In our case, we will just tell the model to expect vector rows of length 11.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>To construct the actual data to be passed into the model, we need to create a matrix. The <kbd>pandas</kbd> library has a convenient method, <kbd>as_matrix()</kbd>, so we'll slice the training features and build a matrix:</p>
<pre>training_df[training_features].as_matrix()</pre>
<p><span> Similarly, we'll create the vector of features:</span></p>
<pre>training_df[label].as_matrix()</pre>
<p><span>Once we have these two things, we can start plugging the data into the model. TensorFlow expects the data to be fed by defining a function that knows how to source the data into tensors (the building blocks of TensorFlow that represents a multidimensional array).</span></p>
<p>The following is the code block for plugging in the data:</p>
<pre>training_input_fn = tf.estimator.inputs.numpy_input_fn(<br/>    x={'inputs': training_df[training_features].as_matrix()},<br/>    y=training_df[label].as_matrix(),<br/>    shuffle=False,<br/>    batch_size=1,<br/>    num_epochs=100,<br/>    queue_capacity=1000,<br/>    num_threads=1)</pre>
<p>The <kbd>tf.estimator.inputs.numpy_input_fn</kbd> utility is able to construct such a function by providing the training matrix and target feature vectors. It will also create partitions of the data for running through the network a number of epochs. It also allows the user to pick the size of the batch (recall the mini-batch method mentioned in <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms</em>, for stochastic gradient descent) and other data feeding parameters. In essence, the underlying regressor's neural network relies on the <kbd>training_input_fn</kbd> function for creating the input tensors at each stage of the algorithm.</p>
<p>Likewise, we create a similar function for feeding the testing data, in preparation for model evaluation:</p>
<pre>test_input_fn = tf.estimator.inputs.numpy_input_fn(<br/>    x={'inputs': test_df[training_features].as_matrix()},<br/>    y=test_df[label].as_matrix(),<br/>    shuffle=False,<br/>    batch_size=1)</pre>
<p>To train the model, we call the usual <kbd>fit()</kbd> method, providing the function we created for sourcing the data:</p>
<pre>tf_regressor.train(input_fn=training_input_fn, steps=50000)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<div class="packt_infobox"><span>The <kbd>steps</kbd> argument is a limit we can impose on the number of total steps. A step, here, is one gradient descent update for one batch. Hence, each epoch runs a number of steps. </span></div>
<p>Once it completes the training, TensorFlow will output the loss metric in the <kbd>final</kbd> epoch:</p>
<pre><strong>INFO:</strong>tensorflow:Loss<strong> for </strong>final<strong> step: 1.1741621.</strong></pre>
<p>We can then evaluate the accuracy of our model by running the test dataset (by providing the test dataset sourcing function):</p>
<pre>tf_regressor.evaluate(input_fn=test_input_fn)<strong><br/></strong></pre>
<p>The preceding code generates the following output:</p>
<pre><strong>{'average_loss': 37.858795,<br/></strong><strong>'label/mean': 22.91492,<br/></strong><strong>'loss': 37.858795,<br/></strong><strong>'prediction/mean': 21.380392,<br/></strong><strong>'global_step': 26600}</strong></pre>
<p>The average loss depends on the units of the target feature, so let's look at building a scatter plot like the one we created in <a href="eeb8abad-c8a9-40f2-8639-a9385d95f80f.xhtml">Chapter 3</a>, <em>Predicting House Value with Regression Algorithms</em>, to compare actual versus predicted house values. To do that, we first need to obtain <kbd>predictions</kbd>.</p>
<p>We simply call the <kbd>predict()</kbd> function to get <kbd>predictions</kbd>, again providing the test dataset sourcing function:</p>
<pre>predictions = tf_regressor.predict(input_fn=test_input_fn)</pre>
<p><span>The <kbd>predictions</kbd> returned a value that is actually a Python generator of single-value vectors, so we can obtain a list of <kbd>predictions</kbd> by constructing the list-through-list comprehension:</span></p>
<pre>predicted_values = [prediction['predictions'][0] for prediction in predictions]<br/><br/></pre>
<p><span>We can thus examine <kbd>predicted_values</kbd>:</span></p>
<pre>predicted_values[:5]</pre>
<p><span>The preceding code generates the following output:</span></p>
<pre><strong>[22.076485, 23.075985, 17.803957, 20.629128, 28.749748]</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can plug in the predicted values as a column to our original <kbd>pandas</kbd> test dataframe:</p>
<pre>test_df['prediction'] = predicted_values</pre>
<p><span>This allows us to use the pandas plotting method to create the chart:</span></p>
<pre>test_df.plot(kind='scatter', x=label, y='prediction')</pre>
<p>We can see the result in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-702 image-border" src="assets/18617ff1-e19b-487e-be7b-4b62070de8c1.png" style="width:27.17em;height:18.58em;"/></p>
<p><span>Note that there is a clear correlation. To improve the performance, we would have to tune our regression model, the size of the batches, steps, epochs, and so on.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training and serving the TensorFlow model through SageMaker</h1>
                </header>
            
            <article>
                
<p>Instead of training the model in a notebook instance, we train the model using the SageMaker infrastructure. In previous chapters, we used built-in estimators, such as BlazingText, XGBoost, and <strong>Factorization Machines</strong> (<strong>FMs</strong>). In this section, we will show how we can build our own TensorFlow models and train them through SageMaker, much like we did with these pre-built models. To do this, we just have to teach SageMaker how our TensorFlow model should be constructed and comply with some conventions regarding the format, location, and structure of the data. Through a Python script, we specify all of this.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>SageMaker will rely on this Python script to perform the training within SageMaker training instances:</p>
<pre>import sagemaker<br/>from sagemaker import get_execution_role<br/>import json<br/>import boto3<br/>from sagemaker.tensorflow import TensorFlow<br/><br/>sess = sagemaker.Session()<br/>role = get_execution_role()<br/>tf_estimator = TensorFlow(entry_point='tf_train.py', role=role,<br/>                          train_instance_count=1,      train_instance_type='ml.m5.large',<br/>                          framework_version='1.12', py_version='py3')<br/>tf_estimator.fit('s3://mastering-ml-aws/chapter8/train-data/')</pre>
<p>The first few lines in the preceding code block are the usual imports and session creation necessary for getting started with SageMaker. The next important thing is the creation of a TensorFlow estimator. Note how we provide the constructor with a Python script, TensorFlow version, and Python version, as well as the usual parameters for instance number and type.</p>
<p>Upon calling the <kbd>tf_estimator.fit(training_data_s3_path)</kbd> function, SageMaker will do the following tasks:</p>
<ol>
<li>Launch an EC2 instance (server).</li>
<li>Download the S3 data to a local directory.</li>
<li>Call the <kbd>tf_train.py</kbd> Python script to train the model. The Python script is expected to store the model on a certain local directory of the EC2 instance.</li>
<li>Package the stored model in a <kbd>.tar.gz</kbd> file and upload it to S3. Additionally, it will create an Amazon container and SageMaker model identifier.</li>
</ol>
<p>Hence, the training happens on a SageMaker managed server, but the model it produces is a SageMaker compatible model, which can be used to serve predictions or run batch transform jobs, like the ones we worked with in previous chapters.</p>
<p>Let's take a look at the <kbd>tf_train.py</kbd> Python script, which is responsible for the model training and saving the model.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This Python script must receive some information from the SageMaker container. In particular, it must receive the following:</p>
<ul>
<li>The local directory where SageMaker has downloaded the data (from S3)</li>
<li>The location where the Python script needs to store the trained model</li>
<li>Other hyperparameters needed by the model (we will not dive into this yet and work with just fixed values, but we will show in <a href="7de65295-dd1f-4eb3-af00-3868ed7e2df9.xhtml">Chapter 14</a>, <em>Optimizing</em> <em>Models in Spark and SageMaker</em>, how these can be used for hyperparameter tuning)</li>
</ul>
<p>Take a look at the following code:</p>
<pre>import pandas as pd<br/>import argparse<br/>import os<br/>import tensorflow as tf<br/><br/>if __name__ == '__main__':<br/>    parser = argparse.ArgumentParser()<br/>    parser.add_argument('--epochs', type=int, default=100)<br/>    parser.add_argument('--batch_size', type=int, default=1)<br/>    parser.add_argument('--steps', type=int, default=12000)<br/>    parser.add_argument('--model_dir', type=str)<br/>    parser.add_argument('--local_model_dir', type=str,                                 <br/>default=os.environ.get('SM_MODEL_DIR'))<br/>    parser.add_argument('--train', type=str,       default=os.environ.get('SM_CHANNEL_TRAINING'))<br/><br/>    args, _ = parser.parse_known_args()<br/>    housing_df = pd.read_csv(args.train + '/train.csv')<br/>    training_features = ['crim', 'zn', 'indus', 'chas', 'nox', <br/>                         'rm', 'age', 'dis', 'tax', 'ptratio', 'lstat']<br/>    label = 'medv'<br/>    tf_regressor = tf.estimator.LinearRegressor(<br/>        feature_columns=[tf.feature_column.numeric_column('inputs', <br/>                                  shape=(11,))])<br/>    training_input_fn = tf.estimator.inputs.numpy_input_fn(<br/>        x={'inputs': housing_df[training_features].as_matrix()},<br/>        y=housing_df[label].as_matrix(),<br/>        shuffle=False,<br/>        batch_size=args.batch_size,<br/>        num_epochs=args.epochs,<br/>        queue_capacity=1000,<br/>        num_threads=1)<br/>    tf_regressor.train(input_fn=training_input_fn, steps=args.steps)<br/><br/><br/>    def serving_input_fn():<br/>        feature_spec = tf.placeholder(tf.float32, shape=[1, 11])<br/>        return tf.estimator.export.build_parsing_serving_input_receiver_fn(<br/>                {'input': feature_spec})()<br/><br/><br/>    tf_regressor.export_savedmodel(export_dir_base=args.local_model_dir + '/export/Servo',<br/>                                   serving_input_receiver_fn=serving_input_fn)</pre>
<p>The first part of the script is just setting up an argument parser. Since SageMaker calls this script as a black box, it needs to be able to inject such arguments to the script. With these arguments, it can train the TensorFlow model. You might notice that the training is exactly the same as what we did in the previous section. The only new part is saving the model and the definition of a new kind of function (<kbd>serving_input_fn</kbd>). This function has a similar purpose to the ones we used for training and testing, but instead, it will be used at the serving time (that is, each time a prediction request is made to the service). It is responsible for defining the necessary transformation from an input tensor placeholder to the features expected by the model. The <kbd>tf.estimator.export.build_parsing_serving_input_receiver_fn</kbd> utility can conveniently build a function for such purposes. It builds a function that expects <kbd>tf.Example</kbd> (a <kbd>protobuf</kbd>-serialized dictionary of features) fed into a string placeholder, so that it can parse such examples into feature tensors. In our case, we just have a single vector as input, so the transformation is straightforward. The last line in our script saves the model into the location requested by SageMaker through the <kbd>local_model_dir</kbd> argument. In order for the deserialization and unpacking to work, the convention is to save the model in a <kbd>/export/Servo</kbd> subdirectory.</p>
<p>Once we run the <kbd>fit()</kbd> command, we can deploy the model as usual:</p>
<pre>predictor = tf_estimator.deploy(instance_type='ml.m5.large', initial_instance_count=1)</pre>
<div class="packt_infobox"><span>For this example, we used a non-GPU instance type, but these are largely recommended for serious serving and training. We will dive into this in <a href="691fc3d8-e4b8-4e3f-a8d9-e13f53f058c4.xhtml">Chapter 15</a>, <em>Tuning Clusters for Machine Learning</em><em>.</em></span></div>
<p>The <kbd>deploy()</kbd> command will launch a container capable of serving the model we constructed. However, constructing the payload to send to such service is not as trivial as the examples in the previous chapter, as we need to construct <kbd>tf.Example</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>At prediction time we want to obtain the price given a specific feature vector. Suppose we want to find the price for these features:</p>
<pre>features_vector = [0.00632, 18.0, 2.31, 0.0, 0.538, 6.575, 65.2, 4.09, 296.0, 15.3, 4.98]</pre>
<p><span>The first step is to construct a <kbd>tf.train.Example</kbd> instance, which in our case consists of a single feature called <kbd>inputs</kbd> with the floating point values of <kbd>features_vector</kbd>:</span></p>
<pre>model_input = tf.train.Example(features=tf.train.Features(<br/><br/>    feature={"inputs": tf.train.Feature(float_list=tf.train.FloatList(value=features_vector))}))</pre>
<p>The next step is to serialize the <kbd>model_input protobuf</kbd> message using <kbd>SerializeToString</kbd>:</p>
<pre>model_input = model_input.SerializeToString()</pre>
<p><span>Since this is really a string of bytes, we need to further encode <kbd>model_input</kbd> so that it can be sent in the payload as a string without special characters. We use <kbd>base64</kbd> encoding to do such a thing:</span></p>
<pre>encoded = base64.b64encode(model_input).decode()</pre>
<p>Lastly, we call our <kbd>predictor</kbd> service by assembling a JSON request:</p>
<pre>predictor.predict('{"inputs":[{"b64":"%s"}]}' % encoded)</pre>
<p><span>Note there is a special convention used for sending base64, encoded <kbd>protobuf</kbd> examples by creating a dictionary keyed with <kbd>b64</kbd>. The output decoded from JSON is a dictionary with the following prediction:</span></p>
<pre><strong>{'outputs': [[24.7537]]}</strong></pre>
<div class="packt_infobox">The <kbd>inputs</kbd> and <kbd>outputs</kbd> payload JSON keys are part of the contract for SageMaker and should not be confused with the name of our single feature, <kbd>inputs</kbd>, which can be an arbitrary string.</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a custom neural net with TensorFlow </h1>
                </header>
            
            <article>
                
<p>In the previous section, <em>Training and serving the TensorFlow model through SageMaker</em>, we used the high-level library of TensorFlow to construct a regression model using a <kbd>LinearRegressor</kbd>. In this section, we will show how we can construct an actual neural network using the Keras library from TensorFlow. Keras facilitates the design of neural networks by hiding some of the complexity behind the core (low-level) TensorFlow library.</p>
<p>In this chapter, we will use the ubiquitous MNIST dataset, which consists of a series of images of handwritten digits along with the real label (values between 0 and 1). The MNIST dataset can be downloaded from <a href="https://www.kaggle.com/c/digit-recognizer/data">https://www.kaggle.com/c/digit-recognizer/data</a>.<a href="https://www.kaggle.com/c/digit-recognizer/data"/></p>
<p>The dataset comes as a CSV with 784 columns corresponding to each of the pixels in the 28 x 28 image. The values for each column represent the strength of the pixel in a gray scale from 0 to 255. It also has an additional column for the label with a value between 0 and 9, corresponding to the actual digit.</p>
<p>Let's download the dataset and do our usual splitting into testing and training using <kbd>pandas</kbd> and <kbd>scikit-learn</kbd>:</p>
<pre>import pandas as pd<br/>from sklearn.model_selection import train_test_split<br/><br/>mnist_df = pd.read_csv('mnist/train.csv')<br/>train_df, test_df = train_test_split(mnist_df, shuffle=False) </pre>
<p>We can inspect the dataset through <kbd>train.head()</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-703 image-border" src="assets/144dc974-87ff-464d-97f6-8521f09e3286.png" style="width:160.17em;height:18.92em;"/></p>
<p>As we can see, the columns are labeled <kbd>pixelX</kbd>, where <kbd>x</kbd> is a number between <kbd>0</kbd> and <kbd>783</kbd>. Let's define the names of these columns in distinct variables:</p>
<pre>pixel_columns = ['pixel' + str(i) for i in range(0, 784)]<br/>label_column = 'label'</pre>
<p>Each row in this dataset becomes a training example and thus represent the input layer of our network. On the other end of the network, we will have 10 nodes, each representing the probability of each digit given each input vector. For our example, we will just use one middle layer.</p>
<p>The following diagram depicts our network structure:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-704 image-border" src="assets/cd1bc0d1-990d-4431-9d49-ca56f9657017.png" style="width:20.42em;height:15.83em;"/></p>
<p>To define such a network in Keras is very simple:</p>
<pre>import tensorflow as tf<br/>from tensorflow import keras<br/><br/>model = keras.Sequential([<br/>    keras.layers.InputLayer(input_shape=(784,), batch_size=5),<br/>    keras.layers.Dense(256, activation=tf.nn.relu),<br/>    keras.layers.Dense(10, activation=tf.nn.softmax)<br/>])</pre>
<p>Note how easy it is to define such a model. It consists of three layers:</p>
<ul>
<li>An input layer, where each vector is of size 784, and each gradient descent update will feed a mini-batch of five examples</li>
<li>A middle dense layer (meaning each node will connect to every other node in the next layer) with a <strong>Rectified Linear Unit</strong> (<strong>ReLU</strong>) activation function on each node</li>
<li>An output layer of size 10 using a softmax activation function (as we want a probability distribution over the digits)</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In addition to defining the network through a sequence of layers, TensorFlow will need to compile the model. This basically entails providing the kind of optimization method to use, the <kbd>loss</kbd> function, and the required metrics:</p>
<pre>model.compile(optimizer='adam',<br/>              loss='sparse_categorical_crossentropy',<br/>              metrics=['accuracy'])</pre>
<p>The next stage will be to fit the model with our data. In order to feed the dataset into TensorFlow, we need to create <kbd>numpy</kbd> matrices, where each row is a training instance and each column represents a node in the input layer. Conveniently, the <kbd>pandas</kbd> method <kbd>dataframe.as_matrix()</kbd> does exactly that, so we will slice the dataset to include the training columns and construct such a matrix. Additionally, we will normalize the matrix to have each grayscale value between 0 and 1:</p>
<pre>import numpy as np<br/><br/>vectorized_normalization_fn = np.vectorize(lambda x: x / 255.0)<br/>normalized_matrix = <br/>      vectorized_normalization_fn(train_df<br/>[pixel_columns].as_matrix())</pre>
<p><span>Likewise, we obtain the <kbd>labels</kbd> vector by transforming the <kbd>pandas</kbd> series into a vector of digits:</span></p>
<pre>labels = train_df[label_column].as_matrix()<br/><br/></pre>
<p><span>Now that we have our training matrix and labels, we are ready to fit our model. </span>We do this by simply calling <kbd>fit()</kbd> and providing the labeled training data:</p>
<pre>model.fit(normalized_matrix, labels, epochs=3)</pre>
<p><span>The training will end with the loss and accuracy metrics on the training dataset:</span></p>
<pre><strong>Epoch 3/3<br/></strong><strong>31500/31500 [==============================] - 16s 511us/sample - loss: 0.0703 - acc: 0.9775</strong></pre>
<p>In order to determine whether our model is overfitting (that is, it just learns how to classify the images in our training dataset but fails to generalize over new images), we need to test our model in the testing dataset. For this, we will perform the same transformations we made on our training dataset, but for the test dataset.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The <kbd>evaluate()</kbd> function of our model will provide accuracy evaluation metrics:</p>
<pre>normalized_test_matrix = vectorized_normalization_fn(test_df[pixel_columns].as_matrix())<br/>test_labels = test_df[label_column].as_matrix()<br/>_, test_acc = model.evaluate(normalized_test_matrix, test_labels)<br/><br/>print('Accuracy on test dataset:', test_acc)</pre>
<p>The preceding code generates the following output:</p>
<pre><strong>Accuracy on test dataset: 0.97</strong></pre>
<p>Note that our simple model is, in fact, fairly accurate. Let's examine a few images in the testing dataset to see how the prediction matches the actual digit. For doing this, we will plot the images and compare them to the predicted digit by performing the following steps:</p>
<ol>
<li>First, we will define a function that obtains the predicted label for a particular row (<kbd>index</kbd>) on our testing dataset matrix:</li>
</ol>
<pre style="padding-left: 60px">def predict_digit(index):<br/>    predictions = model.predict(normalized_test_matrix[index:index + 1])<br/>    return np.argmax(predictions, axis=1)[0]<br/><br/></pre>
<p style="padding-left: 60px"><span><kbd>model.predict()</kbd> will obtain the predictions given a matrix of features. In this case, we just need one single row, so we slice our matrix into a single row to obtain the prediction for just the index in question. The predictions will be a vector of 10 components, each representing the strength of each digit. We use the <kbd>argmax</kbd> function to find the digit that maximizes the strength (that is, finding the most probable digit).</span></p>
<ol start="2">
<li>Next, we define a function, <kbd>show_image()</kbd>, which, given an index, will plot the image:</li>
</ol>
<pre style="padding-left: 60px">from IPython.display import display<br/>from PIL import Image<br/><br/>def show_image(index):<br/>    print("predicted digit: %d" % predict_digit(index))<br/>    print("digit image:")<br/>    vectorized_denormalization_fn = np.vectorize(lambda x: np.uint8(x * 255.0))<br/>    img_matrix = normalized_test_matrix[index].reshape(28, 28)<br/>    img_matrix = vectorized_denormalization_fn(img_matrix)<br/>    img = Image.fromarray(img_matrix, mode='L')<br/>    display(img)<br/><br/></pre>
<p style="padding-left: 60px"><span>We rely on the <kbd>PIL</kbd> library to perform the plotting. In order to plot the image, we need to denormalize our values back to the 0-255 range and reshape the 784 pixels into a 28x28 image.</span></p>
<p>Let's examine a few instances in the following screenshots:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-705 image-border" src="assets/39b4fed9-a53d-4633-8af1-6d0fd46d561b.png" style="width:11.08em;height:8.08em;"/></p>
<p>And the second instance:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-706 image-border" src="assets/757408e6-a4fa-48d0-9342-1d07f478acb2.png" style="width:9.17em;height:6.67em;"/></p>
<p>The following images were not able to be recognized correctly by the model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-707 image-border" src="assets/bf05441b-f53d-491c-b1dc-093e51b57567.png" style="width:12.25em;height:8.83em;"/></p>
<p><span>And the second instance:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-708 image-border" src="assets/490e171f-14d4-4147-ac00-87a704f37b3c.png" style="width:11.25em;height:8.33em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>You may probably agree that even a human could make similar mistakes.</p>
<p>So, how can we build a service on top of our <kbd>model</kbd>?</p>
<p>One simple way to do this is to create an <kbd>estimator</kbd> instance from our <kbd>model</kbd>:</p>
<pre>estimator = tf.keras.estimator.model_to_estimator(model)</pre>
<p><span>Recall that <kbd>LinearRegressor</kbd> we used in the previous section was also an <kbd>estimator</kbd> instance, so the same process for training, serializing, and serving the model would apply starting from this <kbd>estimator</kbd> instance.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we went through the process of creating two different TensorFlow models: one using the high-level library of estimators, and the other using Keras to build a custom neural network. In the process, we also showed how SageMaker can seamlessly handle the training and serving of TensorFlow models.</p>
<p>In the next chapter, <em>Image Classification and Detection with SageMaker</em>,  we will show how to use deep learning out-of-the-box on AWS to detect and recognize images.  </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exercises</h1>
                </header>
            
            <article>
                
<p>The following are the questions for this chapter:</p>
<ul>
<li>What is the difference between an epoch, batch, and step?</li>
<li>How would you design a network that would be able to provide recommendations for the theme park dataset considered in <a href="c940bfe6-b849-4179-b8f8-65e5d44652d6.xhtml">Chapter 6</a>, <em>Analyzing Visitor Patterns to Make Recommendations</em>?</li>
<li>How would you build a network that is capable of classifying the ads in <a href="ccd8e969-f651-4fb9-8ef2-026286577e70.xhtml">Chapter 5</a>, <em>Customer Segmentation Using Clustering Algorithms</em>, as clicks/not-clicks?</li>
</ul>


            </article>

            
        </section>
    </body></html>