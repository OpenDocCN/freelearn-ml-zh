- en: Chapter 3. Bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees were introduced in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    and then applied to five different classification problems. Here, they can be
    seen to work better for some databases more than others. We had almost only used
    the `default` settings for the `rpart` function when constructing decision trees.
    This chapter begins with the exploration of some options that are likely to improve
    the performance of the decision tree. The previous chapter introduced the `bootstrap`
    method, used mainly for statistical methods and models. In this chapter, we will
    use it for trees. The method is generally accepted as a machine learning technique.
    Bootstrapping decision trees is widely known as *bagging*. A similar kind of classification
    method is k-nearest neighborhood classification, abbreviated as *k*-NN. We will
    introduce this method in the third section and apply the bagging technique for
    this method in the concluding section of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification trees and related pruning/improvement methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bagging the classification tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction and application of the *k*-NN classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k*-NN bagging extension'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the following libraries in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`class`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FNN`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ipred`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mlbench`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rpar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification trees and pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A classification tree is a particular type of decision tree, and its focus is
    mainly on classification problems. Breiman, et al. (1984) invented the decision
    tree and Quinlan (1984) independently introduced the C4.5 algorithm. Both of these
    had a lot in common, but we will focus on the Breiman school of decision trees.
    Hastie, et al. (2009) gives a comprehensive treatment of decision trees, and Zhang
    and Singer (2010) offer a treatise on the recursive partitioning methods. An intuitive
    and systematic R programmatic development of the trees can be found in [Chapter
    9](part0062_split_000.html#1R42S1-2006c10fab20488594398dc4871637ee "Chapter 9. Ensembling
    Regression Models"), *Ensembling Regression Models*, of Tattar (2017).
  prefs: []
  type: TYPE_NORMAL
- en: A classification tree has many arguments that can be fine-tuned for improving
    performance. However, we will first simply construct the classification tree with
    default settings and visualize the tree. The `rpart` function from the `rpart`
    package can create classification, regression, as well as survival trees. The
    function first inspects whether the regress and is a categorical, numerical, or
    survival object and accordingly sets up the respective classification, regression,
    or survival trees as well as using the relevant split function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **German credit** dataset is loaded and the exercise of splitting the data
    into train and test parts is carried out as in earlier settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can refer to [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*
    to understand how the R code is performed. Now, using the training dataset and
    the chosen formula, we will create the first classification tree. The `rpart`
    function applied on the formula and dataset will create a classification tree.
    The tree is visualized using the `plot` function, and the `uniform=TRUE` option
    ensures that the display aligns the splits at their correct hierarchical levels.
    Furthermore, the text function will display the variable names at the split points
    and `use.n=TRUE` will give the distribution of the Ys at the node. Finally, the
    fitted classification tree is then used to `predict` the good/bad loans for the
    test dataset, comparisons are made for the test sample, and we find the accuracy
    of the tree is 70.61%, which is the same as in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The plot of the preceding code is the left tree display, `DT-01`, of the next
    diagram. It can be seen from the display that there are way too many terminal
    nodes and there also appears to be many splits, meaning that there is a chance
    that we are overfitting the data. Some of the terminal nodes have as few as seven
    observations while many terminal nodes have less than 20 observations. Consequently,
    there is room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next iteration of the decision tree, we ask the tree algorithm not to
    split further if we have less than 30 observations in the node `(minsplit=30)`,
    and that the minimum bucket size `(minbucket=15)` must be at least 15\. This change
    should give us an improvisation over the tree, `DT_01`. For the new tree, we will
    again check the change in accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Classification trees and pruning](img/00169.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Classification trees for German data'
  prefs: []
  type: TYPE_NORMAL
- en: The `DT-02` tree appears cleaner than `DT-01`, and each terminal node has a
    considerably good number of observations. Importantly, the accuracy is improved
    to `0.7252 - 0.7061 = 0.0191`, or about 2%, which is an improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **complexity parameter**, **Cp**, is an important aspect of the trees and
    we will now use it in improving the classification tree. Using the argument `cp=0.005`
    along with the `minsplit` and `minbucket`, we will try to improve the performance
    of the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The performance has now changed from `0.7252` to `0.7316`, and this is an improvement
    again. The tree complexity structure does not seem to have changed much in `DT-03`,
    the left-side tree of the following diagram. We now carry out two changes simultaneously.
    First, we change the split criteria from Gini to information, and then we add
    a loss matrix for misclassification.
  prefs: []
  type: TYPE_NORMAL
- en: 'What is a loss matrix for misclassification? If a good loan is identified or
    predicted by the model as a good loan in this way, we have no misclassification.
    Furthermore, if a bad loan is classified as a bad loan, it is also the correct
    decision identified by the algorithm. The consequence of misclassifying a good
    loan as bad is not the same as classifying the bad as good. For instance, if a
    bad customer is granted a loan, the loss would be a four-six digit revenue loss,
    while a good customer who is denied a loan might apply again in 3 months'' time.
    If you ran `matrix( c(0,200,500,0),byrow = TRUE,nrow=2)`, the output would be
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that the penalty for misclassifying a good loan as bad is 200 while
    misclassifying a bad loan as good is 500\. Penalties help a lot and gives weight
    to the classification problem. With this option and the split criteria, we set
    up the next classification tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Classification trees and pruning](img/00170.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Classification trees with further options'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the decision tree `DT-04` appears to have far fewer splits than `DT-01-03`,
    and does not appear to over-train the data.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see many options that can be used to tweak the decision trees,
    and some of these are appropriate for the classification tree only. However, with
    some of the parameters tweaking might need expert knowledge, although it is nice
    to be aware of the options. Note how the order of the variable importance changes
    from one decision tree to another. Given the data and the tree structure, how
    reliably can we determine the variable importance of a given variable? This question
    will be covered in the next section. A general way of improving the performance
    of the decision tree will also be explored next.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Bagging** stands for **B**oostap **AGG**regat**ING**. This was invented by
    Breiman (1994). Bagging is an example of an *homogeneous ensemble* and this is
    because the base learning algorithm remains as the classification tree. Here,
    each bootstrap tree will be a base learner. This also means that when we bootstrapped
    the linear regression model in [Chapter 2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "Chapter 2. Bootstrapping"), *Bootstrapping*, we actually performed an ensemble
    there. A few remarks with regards to combining the results of multiple trees is
    in order here.'
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods combine the outputs from multiple models, also known as base
    learners, and produce a single result. A benefit of this approach is that if each
    of these base learners possesses a *desired property*, then the combined result
    will have increased stability. If a certain base learner is over-trained in a
    specific region of the covariate space, the other base learner will nullify such
    an undesired prediction. It is the increased stability that is expected from the
    ensemble, and bagging many times helps to improve the performance of fitted values
    from a given set of models. Berk (2016), Seni and Elder (2010), and Hastie, et
    al. (2009) may be referred to for more details.
  prefs: []
  type: TYPE_NORMAL
- en: '*A basic result!* If *N* observations are drawn with replacements from *N*
    units, then 37% of the observations are left out on average.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This result is an important one. Since we will be carrying out a bootstrap
    method, it means that on average for each tree, we will have a holdout sample
    of 37%. A brief simulation program will compute the probability for us. For an
    *N* value ranging from 11 to 100, we will draw a sample with a replacement, find
    how many indexes are left out, and divide the number by *N* to obtain the empirical
    probability of the number of units left out of this simulation. The empirical
    probability is obtained via *B = 100,000* a number of times, and that average
    is reported as the probability of any individual not being selected in a draw
    of *N* items drawn from *N* items with replacements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Consequently, we can see that in a sample of *N* drawn from *N* items with replacements,
    approximately 0.37 or 37% observations are not selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bagging algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Draw a random sample of size *N* with replacements from the data consisting
    of *N* observations. The selected random sampleis called a **bootstrap sample***.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a classification tree from the bootstrap sample.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign a class to each terminal node and store the predicted class of each observation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat steps 1-3 a large number of times, for example, *B*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign each observation to a final class by a majority vote over the set of
    trees.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main purpose of the bagging procedure is to reduce instability, and this
    is mainly achieved through the `bootstrap` method. As noted earlier, and proven
    by a simulation program, when we draw *N* observations with replacements from
    *N* observations, on average, 37% observations would be excluded from the sample.
    The `bagging` method takes advantage of this sampling with a replacement technique
    and we call the unselected observations **out-of-bag** (**OOB**) observations.
    As with the `bootstrap` method, the resampling technique gives us multiple estimates
    of the different parameters and, using such a sampling distribution, it is then
    possible to carry out the appropriate statistical inference. For a more rigorous
    justification of this technique, the original Breiman (1996) paper is a good read.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*A word of caution*: Suppose an observation has been an out-of-bag observation
    for, say, 100 times out of 271 trees. In the 100th instance when the observation
    is marked for testing purposes, the observation might be classified as `TRUE`
    in *70* of them. It is then tempting to conclude for the observation that *P(TRUE)
    = 70/100 = 0.7*. Such an interpretation can be misleading since the samples in
    the bag are not independent.'
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed to the software implementation, two important remarks from
    Hastie, et al. (2009) are in order. First, the bagging technique helps in reducing
    the variance of the estimated prediction function and it also works for high-variance,
    low-bias models, such as trees. Second, the core aim of bagging in aggregation
    is to average many noisy models that are unbiased, hence the subsequent reduction
    in variance. As with the bootstrap method, bagging reduces bias as it is a smoothing
    method. We will now illustrate the bagging method using the German credit data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `bagging` function from the `ipred` package will help in setting up the
    procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Aggregation has helped here by significantly increasing the accuracy. Note that
    each run of the bagging method will result in a different answer. This is mainly
    because each tree is set up with different samples and the seeds generating the
    random samples are allowed to change dynamically. Until now, made attempts to
    fix the seeds at a certain number to reproduce the results. Going forward, the
    seeds will seldom be fixed. As an exercise to test your knowledge, work out what
    the `keepx` and `coob options` specified in the bagging function are. Do this
    by using `?bagging`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to the German credit problem! We have created *B = 500* trees and for
    some crazy reason we want to view all of them. Of course, Packt (the publisher
    of this book) would probably be a bit annoyed if the author insisted on printing
    all 500 trees, and besides, the trees would look ugly as they are not pruned.
    The program must be completed with respect to the book size constraints. With
    that in mind, let''s kick off with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following steps were performed in the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: We will first invoke the PDF device.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A new file is then created in the `Output` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we begin a loop from `1` to `B`. The `bagging` object constitutes of `B
    = 500` trees and in a temporary object, `tt`, we store the details of the ith
    tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We then plot that tree using the `plot` function by extracting the tree details
    out of `tt`, and adding the relevant text associated with the nodes and splits
    of that tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After the loop is completed, the `dev.off` line is run, which will then save
    the `GC2_Bagging_Trees.pdf` file. This portable document file will consist of
    500 trees.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A lot has been said about the benefits of bootstrapping and a lot has also
    been illustrated in the previous chapter. However, putting aside the usual advantages
    of bagging, which are shown in many blogs and references, we will show here how
    to also get reliable inference of the variable importance. It can be easily seen
    that the variable importance varies a lot across the trees. This is not a problem,
    however. When we are asked about the overall reliability of the `variable importance`
    in decision trees for each variable, we can now look at their values across the
    trees and carry out the inference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding program needs an explanation. Recollect that `variable.importance`
    is displayed in descending order. If you have gone through the `GC2_Bagging_Trees.pdf`
    file (even a cursory look will do), it can be seen that different trees have different
    variables to the primary split, and consequently the order of importance for variables
    would be different. Thus, we first save the order of variable that we need in
    `vnames` object, and then order for each tree `variable.importance[vnames]` by
    the same order as in `vnames`. Each tree in the loop is extracted with `$mtrees`
    and `$btree$variable.importance` to do what is required. Thus, the `VI data.frame`
    object now consists of the variable importance of all 500 trees set up by the
    bagging procedure. The `colMeans` gives the desired aggregate of the importance
    across the 500 trees, and the desired statistical inference can be carried out
    by looking across the detailed information in the `VI` frame. Note that the last
    three variables have `NA` in the aggregated mean. The reason for the `NA` result
    is that in some classification trees, these variables provide no gains whatsoever
    and are not even among any surrogate split. We can quickly discover how many trees
    contain no information on the importance of these three variables, and then repeat
    the calculation of the `colMeans` using the `na.rm=TRUE` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous section, we explored a host of options in `minsplit`, `minbucket`,
    `split`, and `loss` arguments. Can bagging incorporate these metrics? Using the
    `control` argument for the `bagging` function, we will now improvise on the earlier
    results. The choice of the additional parameters is kept the same as earlier.
    After fitting the bagging object, we inspect the accuracy, and then write the
    classification trees to the `GC2_Bagging_Trees_02.pdf` file. Clearly, the trees
    in this file are much more readable than the counterparts in the `GC2_Bagging_Trees.pdf`
    file, and expectedly so. The variable information table is also obtained for the
    `B = 500` trees with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of trees for bagging has been chosen arbitrarily at 500\. There
    is no particular reason for this. We will now see how the accuracy over test data
    changes over the number of trees chosen. The inspection will happen over the choice
    of the number of trees varying from 1 to 25 at an increment of 1, and then by
    incrementing by 25 to 50, 75, …, 475, 500\. The reader is left to make sense out
    of this plot. Meanwhile, the following program is straightforward and does not
    require further explanation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output that is generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](img/00171.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Bagging accuracy over the number of trees'
  prefs: []
  type: TYPE_NORMAL
- en: Analytical techniques don't belong to the family of alchemies. If such a procedure
    is invented, we won't have to worry about modeling. The next example shows that
    bagging can also go wrong.
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging is not a guaranteed recipe!**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first edition of his book, Berk (2016) cautions the reader not to fall
    prey to the proclaimed superiority of the newly invented methods. Recollect the
    Pima Indians Diabetes problem introduced in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*.
    The accuracy table in there shows that the decision tree gives an accuracy of
    `0.7588`. We now apply the bagging method for the same partition and compute the
    accuracy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The overall accuracy here is `0.7548` while that of the single tree classification
    model was `0.7588`, which means that bagging has decreased the accuracy. This
    is not a worry if one understands the purpose of bagging. The purpose was always
    to increase the stability of the predictions, and, as such, we would rather accept
    that bagging would have decreased the variance term.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is a very important technique, and you need not restrict it to classification
    trees alone, but should rather go ahead and explore with other regression methods
    too, such as neural networks and support vector machines. In the next section,
    we will introduce the different approach of k-nearest neighbors (*k*-NN).
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-NN classifier'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    we became familiar with a variety of classification models. Some readers might
    already be familiar with the *k*-NN model. The *k*-NN classifier is one of the
    most simple, intuitive, and non-assumptive models. The name of the model itself
    suggests how it might be working - nearest neighborhoods! And that''s preceded
    by *k*! Thus, if we have *N* points in a study, we find the *k*-nearest points
    in neighborhood, and then make a note of the class of the k-neighbors. The majority
    class of the *k-*neighbors is then assigned to the unit. In case of regression,
    the average of the neighbors is assigned to the unit. The following is a visual
    depiction of *k*-NN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![k-NN classifier](img/00172.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Visual depiction of *k*-NN'
  prefs: []
  type: TYPE_NORMAL
- en: The top left part of the visual depiction of *k*-NN shows the scatterplot of
    27 observations, 16 of which are circles and the remaining 11 are squares. The
    circles are marked in orange ![k-NN classifier](img/00173.jpeg) while the squares
    are marked in blue ![k-NN classifier](img/00174.jpeg). Suppose we choose to set
    up a classifier based on k = 3 neighbors. For every point, we then find its three
    neighbors and assign the majority color to it. Thus, if a circle remains orange,
    it has been correctly identified, and, similarly, if a square is correctly identified,
    its color will remain blue. However, if a circle point has two or more of its
    three nearest neighbors as squares, its color will change to blue and this will
    be denoted by ![k-NN classifier](img/00175.jpeg). Similarly, the color of an incorrectly
    classified square will change to orange ![k-NN classifier](img/00176.jpeg). In
    the top-right block of the preceding diagram, we have the 3-NN predictions, while
    the 7-NN predictions can be found in the lower-left panel of the diagram. Note
    that with 3-NN, we have five misclassifications while with 7-NN, we have seven.
    Thus, increasing the number of the nearest neighbors does not mean that there
    will be an increase in the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom right panel is a 1-NN, which will always give a perfect classification.
    However, it is akin to a broken clock that shows the time perfectly only twice
    a day.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing waveform data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we will perform the analyses for the waveform data. Repeat the code from
    [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee "Chapter 1. Introduction
    to Ensemble Techniques"), *Introduction to Ensemble Techniques*, to obtain the
    waveform data and then partition it for the train and test parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'A simple scatterplot display of the two variables `X.1` and `X.8` by the classes
    is given in the following diagram. A lot of interplay of colors can be seen in
    the display and it does not appear that any form of logistic regression model
    or decision tree will help the case here. It might be that *k*-NN will be useful
    in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing waveform data](img/00177.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Scatterplot for waveform data'
  prefs: []
  type: TYPE_NORMAL
- en: 'For `k = 10`, we first set up a *k*-NN model for the waveform data. The `knn`
    function from the `class` package is used here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy of 90.32% looks promising, and it is better than all the models
    considered in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    except for the support vector machine. We will expand the search grid for k over
    2 to 50:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note that for k = 40, we get the maximum accuracy. Why? We will put the *k*-NN
    in a bag in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-NN bagging'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *k-NN* classifier introduced a classification model in the previous section.
    We can make this robust using the bootstrap method. The broader algorithm remains
    the same. As with the typical bootstrap method, we can always write a program
    consisting of the loop and depending on the number of required bootstrap samples,
    or bags, the control can be specified easily. However, here we will use a function
    from the `FNN` R package. The `ownn` function is useful for carrying out the bagging
    method on the *k*-NN classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ownn` function requires all variables in the dataset to be numeric. However,
    we do have many variables that are factor variables. Consequently, we need to
    tweak the data so that we can use the `ownn` function. The covariate data from
    the training and test dataset are first put together using the `rbind` function.
    Using the `model.matrix` function with the formula `~.-1`, we convert all factor
    variables into numeric variables. The important question here is how does the
    `model.matrix` function work? To keep the explanation simple, if a factor variable
    has m levels, it will create m – 1 new binary variables, which will span the *m*
    dimensions. The reason why we combine the training and test covariates is that
    if any factor has less levels in any of the partition, the number of variables
    will be unequal and we would not be able to adopt the model built on the training
    dataset on the test dataset. After obtaining the covariate matrix with all numeric
    variables, we will split the covariates into the train and test regions again,
    specify the *k*-NN set up with the `ownn` function, and predict the accuracy as
    a consequence of using the bagging method. The program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `train`, `test`, `cl`, and `testcl` arguments are straightforward to follow;
    see `ownn`, and we will vary the number of neighbors on a grid of 5-50\. Did we
    specify the number of bags or the bootstrap samples? Now, the bagging is carried
    out and the bagged predictions are given. It seems that there might be an approximation
    of the estimate as the package and the function clearly says that the predictions
    are based on bagging:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following accuracy plot shows that the model accuracy is stabilizing after
    about 20 neighborhoods. Thus, we have carried out the *k*-NN bagging technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![k-NN bagging](img/00178.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Accuracy of the *k*-NN bagging'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging is essentially an ensembling method that consists of homogeneous base
    learners. Bagging was introduced as a bootstrap aggregation method, and we saw
    some of the advantages of the bootstrap method in [Chapter 2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "Chapter 2. Bootstrapping"), *Bootstrapping*. The advantage of the bagging method
    is the stabilization of the predictions. This chapter began with modifications
    for the classification tree, and we saw different methods of improvising the performance
    of a decision tree so that the tree does not overfit the data. The bagging of
    the decision tress and the related tricks followed in the next section. We then
    introduced *k*-NN as an important classifier and illustrated it with a simple
    example. The chapter concluded with the bagging extension of the *k*-NN classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging helps in reducing the variance of the decision trees. However, the trees
    of the two bootstrap samples are correlated since a lot of common observations
    generate them. In the next chapter, we will look at innovative resampling, which
    will uncorrelate two decision trees.
  prefs: []
  type: TYPE_NORMAL
