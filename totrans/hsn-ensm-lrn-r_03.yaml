- en: Chapter 3. Bagging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。Bagging
- en: Decision trees were introduced in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    and then applied to five different classification problems. Here, they can be
    seen to work better for some databases more than others. We had almost only used
    the `default` settings for the `rpart` function when constructing decision trees.
    This chapter begins with the exploration of some options that are likely to improve
    the performance of the decision tree. The previous chapter introduced the `bootstrap`
    method, used mainly for statistical methods and models. In this chapter, we will
    use it for trees. The method is generally accepted as a machine learning technique.
    Bootstrapping decision trees is widely known as *bagging*. A similar kind of classification
    method is k-nearest neighborhood classification, abbreviated as *k*-NN. We will
    introduce this method in the third section and apply the bagging technique for
    this method in the concluding section of the chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee "第1章。集成技术介绍")，*集成技术介绍*中介绍，然后应用于五个不同的分类问题。在这里，可以看到它们在某些数据库上的表现比其他数据库更好。我们在构建决策树时几乎只使用了`default`设置。这一章将从探索一些可能提高决策树性能的选项开始。上一章介绍了`bootstrap`方法，主要用于统计方法和模型。在这一章中，我们将使用它来构建树。这种方法通常被视为一种机器学习技术。决策树的bootstrapping方法通常被称为*bagging*。类似的一种分类方法是k-最近邻分类，简称*k*-NN。我们将在第三部分介绍这种方法，并在本章的最后一部分应用bagging技术。
- en: 'In this chapter, we will cover the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Classification trees and related pruning/improvement methods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类树和相关剪枝/改进方法
- en: Bagging the classification tree
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging分类树
- en: Introduction and application of the *k*-NN classifier
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*-NN分类器的介绍和应用'
- en: '*k*-NN bagging extension'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*-NN bagging扩展'
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the following libraries in the chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将使用以下库：
- en: '`class`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class`'
- en: '`FNN`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FNN`'
- en: '`ipred`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ipred`'
- en: '`mlbench`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlbench`'
- en: '`rpar`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpar`'
- en: Classification trees and pruning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类树和剪枝
- en: A classification tree is a particular type of decision tree, and its focus is
    mainly on classification problems. Breiman, et al. (1984) invented the decision
    tree and Quinlan (1984) independently introduced the C4.5 algorithm. Both of these
    had a lot in common, but we will focus on the Breiman school of decision trees.
    Hastie, et al. (2009) gives a comprehensive treatment of decision trees, and Zhang
    and Singer (2010) offer a treatise on the recursive partitioning methods. An intuitive
    and systematic R programmatic development of the trees can be found in [Chapter
    9](part0062_split_000.html#1R42S1-2006c10fab20488594398dc4871637ee "Chapter 9. Ensembling
    Regression Models"), *Ensembling Regression Models*, of Tattar (2017).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树是决策树的一种特定类型，其主要关注分类问题。Breiman等人（1984年）发明了决策树，Quinlan（1984年）独立地引入了C4.5算法。这两者有很多共同之处，但我们将重点关注Breiman的决策树学派。Hastie等人（2009年）对决策树进行了全面论述，Zhang和Singer（2010年）对递归分割方法进行了论述。Tattar（2017年）的*集成回归模型*第9章中可以找到对树的一个直观和系统的R程序开发。
- en: A classification tree has many arguments that can be fine-tuned for improving
    performance. However, we will first simply construct the classification tree with
    default settings and visualize the tree. The `rpart` function from the `rpart`
    package can create classification, regression, as well as survival trees. The
    function first inspects whether the regress and is a categorical, numerical, or
    survival object and accordingly sets up the respective classification, regression,
    or survival trees as well as using the relevant split function.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类树有许多参数可以调整以改善性能。然而，我们首先将使用默认设置简单地构建分类树并可视化树。`rpart`包中的`rpart`函数可以创建分类、回归以及生存树。该函数首先检查regress是否为分类、数值或生存对象，并相应地设置相应的分类、回归或生存树，同时使用相关的分割函数。
- en: 'The **German credit** dataset is loaded and the exercise of splitting the data
    into train and test parts is carried out as in earlier settings:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 加载了**德国信用**数据集，并按照早期设置执行了将数据分为训练和测试部分的操作：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can refer to [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*
    to understand how the R code is performed. Now, using the training dataset and
    the chosen formula, we will create the first classification tree. The `rpart`
    function applied on the formula and dataset will create a classification tree.
    The tree is visualized using the `plot` function, and the `uniform=TRUE` option
    ensures that the display aligns the splits at their correct hierarchical levels.
    Furthermore, the text function will display the variable names at the split points
    and `use.n=TRUE` will give the distribution of the Ys at the node. Finally, the
    fitted classification tree is then used to `predict` the good/bad loans for the
    test dataset, comparisons are made for the test sample, and we find the accuracy
    of the tree is 70.61%, which is the same as in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee "第1章。集成技术简介")，*集成技术简介*，了解R代码是如何执行的。现在，使用训练数据集和选定的公式，我们将创建第一个分类树。将`rpart`函数应用于公式和数据集将创建一个分类树。使用`plot`函数可视化树，`uniform=TRUE`选项确保显示正确对齐分割的层次级别。此外，文本函数将在分割点显示变量名，`use.n=TRUE`将给出节点Y的分布。最后，使用拟合的分类树对测试数据集的贷款好坏进行预测，对测试样本进行比较，我们发现树的准确率为70.61%，与[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第1章。集成技术简介")，*集成技术简介*中的相同：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The plot of the preceding code is the left tree display, `DT-01`, of the next
    diagram. It can be seen from the display that there are way too many terminal
    nodes and there also appears to be many splits, meaning that there is a chance
    that we are overfitting the data. Some of the terminal nodes have as few as seven
    observations while many terminal nodes have less than 20 observations. Consequently,
    there is room for improvement.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 前一段代码的图示是下一张图的左侧树形图，`DT-01`。从显示中可以看出，终端节点太多，似乎也有很多分割，这意味着我们可能过度拟合了数据。一些终端节点只有七个观测值，而许多终端节点的观测值少于20个。因此，还有改进的空间。
- en: 'In the next iteration of the decision tree, we ask the tree algorithm not to
    split further if we have less than 30 observations in the node `(minsplit=30)`,
    and that the minimum bucket size `(minbucket=15)` must be at least 15\. This change
    should give us an improvisation over the tree, `DT_01`. For the new tree, we will
    again check the change in accuracy:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树的下一个迭代中，我们要求树算法在节点`(minsplit=30)`的观测值少于30个时不要进一步分割，并且最小桶大小`(minbucket=15)`必须至少为15。这个更改应该会改善树`DT_01`。对于新树，我们再次检查准确率的变化：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Classification trees and pruning](img/00169.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![分类树和剪枝](img/00169.jpeg)'
- en: 'Figure 1: Classification trees for German data'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：德国数据的分类树
- en: The `DT-02` tree appears cleaner than `DT-01`, and each terminal node has a
    considerably good number of observations. Importantly, the accuracy is improved
    to `0.7252 - 0.7061 = 0.0191`, or about 2%, which is an improvement.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`DT-02`树看起来比`DT-01`更干净，每个终端节点的观测值数量也相当好。重要的是，准确率提高了`0.7252 - 0.7061 = 0.0191`，即大约2%，这是一个改进。'
- en: 'The **complexity parameter**, **Cp**, is an important aspect of the trees and
    we will now use it in improving the classification tree. Using the argument `cp=0.005`
    along with the `minsplit` and `minbucket`, we will try to improve the performance
    of the tree:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**复杂度参数**，**Cp**，是树的一个重要方面，我们现在将使用它来改进分类树。使用`cp=0.005`参数以及`minsplit`和`minbucket`，我们将尝试提高树的表现：'
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The performance has now changed from `0.7252` to `0.7316`, and this is an improvement
    again. The tree complexity structure does not seem to have changed much in `DT-03`,
    the left-side tree of the following diagram. We now carry out two changes simultaneously.
    First, we change the split criteria from Gini to information, and then we add
    a loss matrix for misclassification.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 性能现在已从`0.7252`提高到`0.7316`，这又是一个改进。在`DT-03`中，即下一张图的左侧树中，树复杂度结构似乎没有太大变化。我们现在同时进行两个更改。首先，我们将分割标准从Gini改为信息，然后添加一个错误矩阵以处理误分类。
- en: 'What is a loss matrix for misclassification? If a good loan is identified or
    predicted by the model as a good loan in this way, we have no misclassification.
    Furthermore, if a bad loan is classified as a bad loan, it is also the correct
    decision identified by the algorithm. The consequence of misclassifying a good
    loan as bad is not the same as classifying the bad as good. For instance, if a
    bad customer is granted a loan, the loss would be a four-six digit revenue loss,
    while a good customer who is denied a loan might apply again in 3 months'' time.
    If you ran `matrix( c(0,200,500,0),byrow = TRUE,nrow=2)`, the output would be
    the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是误分类的损失矩阵？如果模型以这种方式将一笔好贷款识别或预测为好贷款，则没有误分类。此外，如果将一笔坏贷款分类为坏贷款，这也是算法识别出的正确决策。将一笔好贷款误分类为坏贷款的后果与将坏贷款分类为好贷款的后果不同。例如，如果向一个坏客户发放贷款，损失将是四到六位数的收入损失，而一个被拒绝贷款的好客户可能在三个月后再次申请。如果你运行`matrix(c(0,200,500,0),
    byrow = TRUE, nrow=2)`，输出将是以下内容：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This means that the penalty for misclassifying a good loan as bad is 200 while
    misclassifying a bad loan as good is 500\. Penalties help a lot and gives weight
    to the classification problem. With this option and the split criteria, we set
    up the next classification tree:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着将一笔好贷款误分类为坏贷款的惩罚是200，而将一笔坏贷款误分类为好贷款的惩罚是500。惩罚在很大程度上有助于并给分类问题增加了权重。使用此选项和分割标准，我们设置了下一个分类树：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Classification trees and pruning](img/00170.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![分类树和剪枝](img/00170.jpeg)'
- en: 'Figure 2: Classification trees with further options'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：具有更多选项的分类树
- en: Note that the decision tree `DT-04` appears to have far fewer splits than `DT-01-03`,
    and does not appear to over-train the data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，决策树`DT-04`似乎比`DT-01-03`有更少的分割，并且看起来没有过度训练数据。
- en: Here, we can see many options that can be used to tweak the decision trees,
    and some of these are appropriate for the classification tree only. However, with
    some of the parameters tweaking might need expert knowledge, although it is nice
    to be aware of the options. Note how the order of the variable importance changes
    from one decision tree to another. Given the data and the tree structure, how
    reliably can we determine the variable importance of a given variable? This question
    will be covered in the next section. A general way of improving the performance
    of the decision tree will also be explored next.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到许多可以用来调整决策树的选项，其中一些仅适用于分类树。然而，调整一些参数可能需要专业知识，尽管了解这些选项是很好的。注意变量重要性在不同决策树之间的变化顺序。给定数据和树结构，我们如何可靠地确定给定变量的变量重要性？这个问题将在下一节中讨论。接下来还将探讨提高决策树性能的一般方法。
- en: Bagging
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging
- en: '**Bagging** stands for **B**oostap **AGG**regat**ING**. This was invented by
    Breiman (1994). Bagging is an example of an *homogeneous ensemble* and this is
    because the base learning algorithm remains as the classification tree. Here,
    each bootstrap tree will be a base learner. This also means that when we bootstrapped
    the linear regression model in [Chapter 2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "Chapter 2. Bootstrapping"), *Bootstrapping*, we actually performed an ensemble
    there. A few remarks with regards to combining the results of multiple trees is
    in order here.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bagging**代表**B**oostap **AGG**regat**ING**。这是由Breiman（1994年）发明的。Bagging是**同质集成**的一个例子，这是因为基础学习算法仍然是分类树。在这里，每个自助树将是一个基础学习器。这也意味着当我们对[第2章](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "第2章。自助")中的线性回归模型进行自助时，*自助*，我们实际上在那里执行了一个集成。关于结合多棵树的结果的一些评论也是必要的。'
- en: Ensemble methods combine the outputs from multiple models, also known as base
    learners, and produce a single result. A benefit of this approach is that if each
    of these base learners possesses a *desired property*, then the combined result
    will have increased stability. If a certain base learner is over-trained in a
    specific region of the covariate space, the other base learner will nullify such
    an undesired prediction. It is the increased stability that is expected from the
    ensemble, and bagging many times helps to improve the performance of fitted values
    from a given set of models. Berk (2016), Seni and Elder (2010), and Hastie, et
    al. (2009) may be referred to for more details.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法结合了多个模型（也称为基学习器）的输出，并产生一个单一的结果。这种方法的一个好处是，如果每个基学习器都具备*所需属性*，那么组合结果将具有更高的稳定性。如果某个基学习器在协变量空间的一个特定区域过度训练，其他基学习器将消除这种不希望的预测。集成方法期望的是更高的稳定性，多次带包装有助于提高给定模型集的拟合值性能。Berk（2016）、Seni和Elder（2010）、Hastie等人（2009）可以参考以获取更多详细信息。
- en: '*A basic result!* If *N* observations are drawn with replacements from *N*
    units, then 37% of the observations are left out on average.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*一个基本结果!* 如果从*N*个单位中用放回抽样方法抽取*N*个观测值，那么平均有37%的观测值被排除在外。'
- en: 'This result is an important one. Since we will be carrying out a bootstrap
    method, it means that on average for each tree, we will have a holdout sample
    of 37%. A brief simulation program will compute the probability for us. For an
    *N* value ranging from 11 to 100, we will draw a sample with a replacement, find
    how many indexes are left out, and divide the number by *N* to obtain the empirical
    probability of the number of units left out of this simulation. The empirical
    probability is obtained via *B = 100,000* a number of times, and that average
    is reported as the probability of any individual not being selected in a draw
    of *N* items drawn from *N* items with replacements:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果非常重要。由于我们将执行一个bootstrap方法，这意味着平均每个树，我们将有一个37%的保留样本。一个简短的模拟程序将为我们计算概率。对于*N*值从11到100的范围，我们将进行放回抽样，找出有多少索引被排除，然后将该数字除以*N*以获得从*N*个单位中放回抽取的*N*个单位的模拟中未排除单位的经验概率。经验概率是通过*B
    = 100,000*多次获得，并将该平均值报告为在*N*个单位的放回抽取中未选中任何单个单位的概率：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Consequently, we can see that in a sample of *N* drawn from *N* items with replacements,
    approximately 0.37 or 37% observations are not selected.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到，从*N*个单位的放回抽取中抽取的样本中，大约有0.37或37%的观测值未被选中。
- en: 'The bagging algorithm is given as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 带包装算法如下：
- en: Draw a random sample of size *N* with replacements from the data consisting
    of *N* observations. The selected random sampleis called a **bootstrap sample***.*
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从由*N*个观测值组成的数据中抽取一个大小为*N*的随机样本，并放回。所选的随机样本称为**bootstrap样本***.*
- en: Construct a classification tree from the bootstrap sample.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从bootstrap样本中构建一个分类树。
- en: Assign a class to each terminal node and store the predicted class of each observation.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个终端节点分配一个类别，并存储每个观测值的预测类别。
- en: Repeat steps 1-3 a large number of times, for example, *B*.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复步骤1-3多次，例如，*B*。
- en: Assign each observation to a final class by a majority vote over the set of
    trees.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对树集进行多数投票，将每个观测值分配给一个最终类别。
- en: The main purpose of the bagging procedure is to reduce instability, and this
    is mainly achieved through the `bootstrap` method. As noted earlier, and proven
    by a simulation program, when we draw *N* observations with replacements from
    *N* observations, on average, 37% observations would be excluded from the sample.
    The `bagging` method takes advantage of this sampling with a replacement technique
    and we call the unselected observations **out-of-bag** (**OOB**) observations.
    As with the `bootstrap` method, the resampling technique gives us multiple estimates
    of the different parameters and, using such a sampling distribution, it is then
    possible to carry out the appropriate statistical inference. For a more rigorous
    justification of this technique, the original Breiman (1996) paper is a good read.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 带包装过程的主要目的是减少不稳定性，这主要通过`bootstrap`方法实现。如前所述，并通过模拟程序证明，当我们从*N*个观测值中用放回抽样方法抽取*N*个观测值时，平均而言，37%的观测值将被排除在样本之外。`bagging`方法利用这种放回抽样的技术，我们将未选择的观测值称为**袋外**（**OOB**）观测值。与`bootstrap`方法一样，重抽样技术为我们提供了不同参数的多个估计值，利用这种抽样分布，我们可以进行适当的统计推断。为了更严格地证明这种技术，原始的Breiman（1996）论文值得一读。
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*A word of caution*: Suppose an observation has been an out-of-bag observation
    for, say, 100 times out of 271 trees. In the 100th instance when the observation
    is marked for testing purposes, the observation might be classified as `TRUE`
    in *70* of them. It is then tempting to conclude for the observation that *P(TRUE)
    = 70/100 = 0.7*. Such an interpretation can be misleading since the samples in
    the bag are not independent.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意事项**：假设一个观测值在 271 棵树中有 100 次是袋外观测值。在第 100 次标记为测试目的时，该观测值可能被分类为 `TRUE` 的有
    70 次。因此，对于该观测值，可能会得出结论 *P(TRUE) = 70/100 = 0.7*。这种解释可能会误导，因为袋中的样本不是独立的。'
- en: Before we proceed to the software implementation, two important remarks from
    Hastie, et al. (2009) are in order. First, the bagging technique helps in reducing
    the variance of the estimated prediction function and it also works for high-variance,
    low-bias models, such as trees. Second, the core aim of bagging in aggregation
    is to average many noisy models that are unbiased, hence the subsequent reduction
    in variance. As with the bootstrap method, bagging reduces bias as it is a smoothing
    method. We will now illustrate the bagging method using the German credit data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行软件实现之前，Hastie 等人（2009年）提出了两个重要的评论。首先，bagging 技术有助于减少估计预测函数的方差，并且它也适用于高方差、低偏差的模型，如树模型。其次，bagging
    在聚合中的核心目标是平均许多无偏的噪声模型，因此随后方差减少。与自助法一样，bagging 作为一种平滑方法，可以减少偏差。现在，我们将使用德国信贷数据来阐述
    bagging 方法。
- en: 'The `bagging` function from the `ipred` package will help in setting up the
    procedure:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 `ipred` 包的 `bagging` 函数将有助于设置程序：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Aggregation has helped here by significantly increasing the accuracy. Note that
    each run of the bagging method will result in a different answer. This is mainly
    because each tree is set up with different samples and the seeds generating the
    random samples are allowed to change dynamically. Until now, made attempts to
    fix the seeds at a certain number to reproduce the results. Going forward, the
    seeds will seldom be fixed. As an exercise to test your knowledge, work out what
    the `keepx` and `coob options` specified in the bagging function are. Do this
    by using `?bagging`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合在这里通过显著提高准确性有所帮助。请注意，每次运行 bagging 方法都会得到不同的答案。这主要是因为每棵树都是使用不同的样本设置的，并且允许生成随机样本的种子动态变化。到目前为止，已经尝试将种子固定在某个特定数值以重现结果。向前看，种子很少会被固定。作为一个测试你知识的练习，找出
    bagging 函数中指定的 `keepx` 和 `coob` 选项的含义。通过使用 `?bagging` 来完成这个练习。
- en: 'Back to the German credit problem! We have created *B = 500* trees and for
    some crazy reason we want to view all of them. Of course, Packt (the publisher
    of this book) would probably be a bit annoyed if the author insisted on printing
    all 500 trees, and besides, the trees would look ugly as they are not pruned.
    The program must be completed with respect to the book size constraints. With
    that in mind, let''s kick off with the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 回到德国信贷问题！我们已经创建了 *B = 500* 棵树，出于某种疯狂的原因，我们想查看所有这些树。当然，Packt（本书的出版商）可能会对作者坚持打印所有
    500 棵树感到有些恼火，而且，由于这些树没有经过修剪，它们看起来会很丑。程序必须符合书籍大小的限制。考虑到这一点，让我们从以下代码开始：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following steps were performed in the preceding code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤在先前的代码中已执行：
- en: We will first invoke the PDF device.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将调用 PDF 设备。
- en: A new file is then created in the `Output` folder.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在 `Output` 文件夹中创建了一个新文件。
- en: Next, we begin a loop from `1` to `B`. The `bagging` object constitutes of `B
    = 500` trees and in a temporary object, `tt`, we store the details of the ith
    tree.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从 `1` 到 `B` 开始一个循环。`bagging` 对象由 `B = 500` 棵树组成，在一个临时对象 `tt` 中，我们存储第 i
    棵树的细节。
- en: We then plot that tree using the `plot` function by extracting the tree details
    out of `tt`, and adding the relevant text associated with the nodes and splits
    of that tree.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用 `plot` 函数绘制该树，通过从 `tt` 中提取树细节，并添加与该树的节点和分割相关的相关文本。
- en: After the loop is completed, the `dev.off` line is run, which will then save
    the `GC2_Bagging_Trees.pdf` file. This portable document file will consist of
    500 trees.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环完成后，运行 `dev.off` 行，这将保存 `GC2_Bagging_Trees.pdf` 文件。这个便携式文档文件将包含 500 棵树。
- en: 'A lot has been said about the benefits of bootstrapping and a lot has also
    been illustrated in the previous chapter. However, putting aside the usual advantages
    of bagging, which are shown in many blogs and references, we will show here how
    to also get reliable inference of the variable importance. It can be easily seen
    that the variable importance varies a lot across the trees. This is not a problem,
    however. When we are asked about the overall reliability of the `variable importance`
    in decision trees for each variable, we can now look at their values across the
    trees and carry out the inference:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自举法的益处已经有很多讨论，并且在上一章中也进行了很多说明。然而，除了在许多博客和参考资料中展示的常规优势之外，我们在这里还将展示如何获得变量重要性的可靠推断。很容易看出，变量重要性在树之间有很大的差异。但这并不是问题。当我们被问到每个变量的决策树中变量重要性的整体可靠性时，我们现在可以查看它们在树中的值并执行推断：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding program needs an explanation. Recollect that `variable.importance`
    is displayed in descending order. If you have gone through the `GC2_Bagging_Trees.pdf`
    file (even a cursory look will do), it can be seen that different trees have different
    variables to the primary split, and consequently the order of importance for variables
    would be different. Thus, we first save the order of variable that we need in
    `vnames` object, and then order for each tree `variable.importance[vnames]` by
    the same order as in `vnames`. Each tree in the loop is extracted with `$mtrees`
    and `$btree$variable.importance` to do what is required. Thus, the `VI data.frame`
    object now consists of the variable importance of all 500 trees set up by the
    bagging procedure. The `colMeans` gives the desired aggregate of the importance
    across the 500 trees, and the desired statistical inference can be carried out
    by looking across the detailed information in the `VI` frame. Note that the last
    three variables have `NA` in the aggregated mean. The reason for the `NA` result
    is that in some classification trees, these variables provide no gains whatsoever
    and are not even among any surrogate split. We can quickly discover how many trees
    contain no information on the importance of these three variables, and then repeat
    the calculation of the `colMeans` using the `na.rm=TRUE` option:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述程序需要解释。回想一下，`variable.importance`是按降序显示的。如果你已经看过`GC2_Bagging_Trees.pdf`文件（即使是粗略地看），你会看到不同的树有不同的主要分割变量，因此变量的重要性顺序也会不同。因此，我们首先将需要的变量顺序保存在`vnames`对象中，然后按照`vnames`中的顺序对每个树的`variable.importance[vnames]`进行排序。循环中的每个树都通过`$mtrees`和`$btree$variable.importance`提取出来以完成所需操作。因此，`VI
    data.frame`对象现在由自举过程设置的500棵树的变量重要性组成。`colMeans`给出了500棵树的重要性汇总，通过查看`VI`框架中的详细信息可以进行所需的统计推断。请注意，最后三个变量在汇总平均值中有`NA`。`NA`结果的原因是在某些分类树中，这些变量根本没有任何增益，甚至不在任何替代分割中。我们可以快速发现有多少棵树没有这些三个变量的重要性信息，然后使用`na.rm=TRUE`选项重复计算`colMeans`：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the previous section, we explored a host of options in `minsplit`, `minbucket`,
    `split`, and `loss` arguments. Can bagging incorporate these metrics? Using the
    `control` argument for the `bagging` function, we will now improvise on the earlier
    results. The choice of the additional parameters is kept the same as earlier.
    After fitting the bagging object, we inspect the accuracy, and then write the
    classification trees to the `GC2_Bagging_Trees_02.pdf` file. Clearly, the trees
    in this file are much more readable than the counterparts in the `GC2_Bagging_Trees.pdf`
    file, and expectedly so. The variable information table is also obtained for the
    `B = 500` trees with the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探索了`minsplit`、`minbucket`、`split`和`loss`参数的各种选项。自举法能否包含这些指标？使用`bagging`函数的`control`参数，我们现在将对之前的结果进行改进。附加参数的选择与之前保持一致。在拟合自举对象后，我们检查准确性，然后将分类树写入`GC2_Bagging_Trees_02.pdf`文件。显然，这个文件中的树比`GC2_Bagging_Trees.pdf`文件中的树更容易阅读，这是预料之中的。以下代码也获得了`B
    = 500`棵树的变量信息表：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The number of trees for bagging has been chosen arbitrarily at 500\. There
    is no particular reason for this. We will now see how the accuracy over test data
    changes over the number of trees chosen. The inspection will happen over the choice
    of the number of trees varying from 1 to 25 at an increment of 1, and then by
    incrementing by 25 to 50, 75, …, 475, 500\. The reader is left to make sense out
    of this plot. Meanwhile, the following program is straightforward and does not
    require further explanation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging的树的数量已被任意选择为500。没有特别的原因。我们现在将看到测试数据的准确率如何随着树的数量而变化。检查将在树的数量从1到25，每次增加1的情况下进行，然后增加25到50，75，……，475，500。读者需要自己理解这个图。同时，以下程序很简单，不需要进一步解释：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following is the output that is generated:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是由以下生成的输出：
- en: '![Bagging](img/00171.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![Bagging](img/00171.jpeg)'
- en: 'Figure 3: Bagging accuracy over the number of trees'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Bagging准确率与树的数量
- en: Analytical techniques don't belong to the family of alchemies. If such a procedure
    is invented, we won't have to worry about modeling. The next example shows that
    bagging can also go wrong.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 分析技术不属于炼金术的范畴。如果发明了这样的程序，我们就不必担心建模了。下一个例子将表明，Bagging也可能出错。
- en: '**Bagging is not a guaranteed recipe!**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bagging不是一个保证的食谱！**'
- en: 'In the first edition of his book, Berk (2016) cautions the reader not to fall
    prey to the proclaimed superiority of the newly invented methods. Recollect the
    Pima Indians Diabetes problem introduced in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*.
    The accuracy table in there shows that the decision tree gives an accuracy of
    `0.7588`. We now apply the bagging method for the same partition and compute the
    accuracy as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的书的第一个版本中，Berk（2016）警告读者不要成为新发明方法宣称的优越性的牺牲品。回想一下在[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第1章。集成技术简介")中引入的皮马印第安人糖尿病问题，“集成技术简介”。那里的准确率表显示决策树给出了`0.7588`的准确率。我们现在应用Bagging方法对同一分区进行计算，并计算准确率如下：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The overall accuracy here is `0.7548` while that of the single tree classification
    model was `0.7588`, which means that bagging has decreased the accuracy. This
    is not a worry if one understands the purpose of bagging. The purpose was always
    to increase the stability of the predictions, and, as such, we would rather accept
    that bagging would have decreased the variance term.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的总体准确率是`0.7548`，而单个树分类模型的准确率是`0.7588`，这意味着Bagging降低了准确率。如果理解了Bagging的目的，这并不是一个担忧。目的始终是增加预测的稳定性，因此我们宁愿接受Bagging会降低方差项。
- en: Bagging is a very important technique, and you need not restrict it to classification
    trees alone, but should rather go ahead and explore with other regression methods
    too, such as neural networks and support vector machines. In the next section,
    we will introduce the different approach of k-nearest neighbors (*k*-NN).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging是一个非常重要的技术，你不需要仅将其限制在分类树上，而应该继续探索其他回归方法，例如神经网络和支持向量机。在下一节中，我们将介绍k最近邻（*k*-NN）的不同方法。
- en: '*k*-NN classifier'
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*k*-NN分类器'
- en: 'In [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    we became familiar with a variety of classification models. Some readers might
    already be familiar with the *k*-NN model. The *k*-NN classifier is one of the
    most simple, intuitive, and non-assumptive models. The name of the model itself
    suggests how it might be working - nearest neighborhoods! And that''s preceded
    by *k*! Thus, if we have *N* points in a study, we find the *k*-nearest points
    in neighborhood, and then make a note of the class of the k-neighbors. The majority
    class of the *k-*neighbors is then assigned to the unit. In case of regression,
    the average of the neighbors is assigned to the unit. The following is a visual
    depiction of *k*-NN:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee "第1章。集成技术简介")中，我们熟悉了各种分类模型。一些读者可能已经熟悉了*k*-NN模型。*k*-NN分类器是最简单、最直观和非假设性的模型之一。模型的名字本身就暗示了它可能的工作方式——最近的邻域！而且这还伴随着*k*！因此，如果我们有一个研究中的*N*个点，我们就会找到邻域中的*k*个最近点，然后记录下k个邻居的类别。然后，将k个邻居的多数类别分配给单元。在回归的情况下，将邻居的平均值分配给单元。以下是对*k*-NN的视觉描述：
- en: '![k-NN classifier](img/00172.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![k-NN分类器](img/00172.jpeg)'
- en: 'Figure 4: Visual depiction of *k*-NN'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：*k*-NN 的可视化表示
- en: The top left part of the visual depiction of *k*-NN shows the scatterplot of
    27 observations, 16 of which are circles and the remaining 11 are squares. The
    circles are marked in orange ![k-NN classifier](img/00173.jpeg) while the squares
    are marked in blue ![k-NN classifier](img/00174.jpeg). Suppose we choose to set
    up a classifier based on k = 3 neighbors. For every point, we then find its three
    neighbors and assign the majority color to it. Thus, if a circle remains orange,
    it has been correctly identified, and, similarly, if a square is correctly identified,
    its color will remain blue. However, if a circle point has two or more of its
    three nearest neighbors as squares, its color will change to blue and this will
    be denoted by ![k-NN classifier](img/00175.jpeg). Similarly, the color of an incorrectly
    classified square will change to orange ![k-NN classifier](img/00176.jpeg). In
    the top-right block of the preceding diagram, we have the 3-NN predictions, while
    the 7-NN predictions can be found in the lower-left panel of the diagram. Note
    that with 3-NN, we have five misclassifications while with 7-NN, we have seven.
    Thus, increasing the number of the nearest neighbors does not mean that there
    will be an increase in the accuracy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-NN 的可视化表示的左上部分显示了 27 个观察值的散点图，其中 16 个是圆圈，剩下的 11 个是正方形。圆圈用橙色标记 ![k-NN 分类器](img/00173.jpeg)，而正方形用蓝色标记
    ![k-NN 分类器](img/00174.jpeg)。假设我们选择基于 k = 3 个邻居来设置一个分类器。然后，对于每个点，我们找到它的三个邻居，并将大多数颜色分配给它。因此，如果一个圆圈保持橙色，它已经被正确识别，同样，如果一个正方形被正确识别，它的颜色将保持蓝色。然而，如果一个圆圈点有三个最近的邻居中有两个或更多是正方形，它的颜色将变为蓝色，这将被标记为
    ![k-NN 分类器](img/00175.jpeg)。同样，一个错误分类的正方形的颜色将变为橙色 ![k-NN 分类器](img/00176.jpeg)。在前面的图的右上块中，我们有
    3-NN 的预测，而 7-NN 的预测可以在图的左下角找到。注意，在 3-NN 中，我们有五个错误分类，而在 7-NN 中，我们有七个。因此，增加最近邻居的数量并不意味着准确率会增加。'
- en: The bottom right panel is a 1-NN, which will always give a perfect classification.
    However, it is akin to a broken clock that shows the time perfectly only twice
    a day.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 右下角的图是 1-NN，它总是给出完美的分类。然而，它就像一个每天只准确显示两次时间的坏钟。
- en: Analyzing waveform data
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析波形数据
- en: 'Next, we will perform the analyses for the waveform data. Repeat the code from
    [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee "Chapter 1. Introduction
    to Ensemble Techniques"), *Introduction to Ensemble Techniques*, to obtain the
    waveform data and then partition it for the train and test parts:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对波形数据进行分析。重复 [第 1 章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第 1 章。集成技术简介") 中的代码，即 *集成技术简介*，以获取波形数据，然后将其分为训练和测试部分：
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A simple scatterplot display of the two variables `X.1` and `X.8` by the classes
    is given in the following diagram. A lot of interplay of colors can be seen in
    the display and it does not appear that any form of logistic regression model
    or decision tree will help the case here. It might be that *k*-NN will be useful
    in this case:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了两个变量 `X.1` 和 `X.8` 通过类别进行的一个简单的散点图显示。在显示中可以看到很多颜色的交互，看起来任何形式的逻辑回归模型或决策树都不会帮助这种情况。可能
    *k*-NN 在这种情况下会有所帮助：
- en: '![Analyzing waveform data](img/00177.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![分析波形数据](img/00177.jpeg)'
- en: 'Figure 5: Scatterplot for waveform data'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：波形数据的散点图
- en: 'For `k = 10`, we first set up a *k*-NN model for the waveform data. The `knn`
    function from the `class` package is used here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `k = 10`，我们首先为波形数据建立了一个 *k*-NN 模型。这里使用了 `class` 包中的 `knn` 函数：
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The accuracy of 90.32% looks promising, and it is better than all the models
    considered in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    except for the support vector machine. We will expand the search grid for k over
    2 to 50:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 90.32% 的准确率看起来很有希望，并且优于 [第 1 章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第 1 章。集成技术简介") 中考虑的所有模型，即 *集成技术简介*，除了支持向量机。我们将扩大 k 的搜索范围，从 2 到 50：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that for k = 40, we get the maximum accuracy. Why? We will put the *k*-NN
    in a bag in the next section.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于 k = 40，我们得到了最大的准确率。为什么？我们将在下一节中将 *k*-NN 放入一个袋中。
- en: '*k*-NN bagging'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*k*-NN 袋装'
- en: The *k-NN* classifier introduced a classification model in the previous section.
    We can make this robust using the bootstrap method. The broader algorithm remains
    the same. As with the typical bootstrap method, we can always write a program
    consisting of the loop and depending on the number of required bootstrap samples,
    or bags, the control can be specified easily. However, here we will use a function
    from the `FNN` R package. The `ownn` function is useful for carrying out the bagging
    method on the *k*-NN classifier.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，*k*-NN 分类器引入了一个分类模型。我们可以使用自助法使其更加稳健。更广泛的算法保持不变。与典型的自助法一样，我们可以始终编写一个由循环组成的程序，根据所需的
    bootstrap 样本数或袋数，控制可以很容易地指定。然而，这里我们将使用 `FNN` R 包中的一个函数。`ownn` 函数对于在 *k*-NN 分类器上执行袋装方法非常有用。
- en: 'The `ownn` function requires all variables in the dataset to be numeric. However,
    we do have many variables that are factor variables. Consequently, we need to
    tweak the data so that we can use the `ownn` function. The covariate data from
    the training and test dataset are first put together using the `rbind` function.
    Using the `model.matrix` function with the formula `~.-1`, we convert all factor
    variables into numeric variables. The important question here is how does the
    `model.matrix` function work? To keep the explanation simple, if a factor variable
    has m levels, it will create m – 1 new binary variables, which will span the *m*
    dimensions. The reason why we combine the training and test covariates is that
    if any factor has less levels in any of the partition, the number of variables
    will be unequal and we would not be able to adopt the model built on the training
    dataset on the test dataset. After obtaining the covariate matrix with all numeric
    variables, we will split the covariates into the train and test regions again,
    specify the *k*-NN set up with the `ownn` function, and predict the accuracy as
    a consequence of using the bagging method. The program is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`ownn` 函数要求数据集中的所有变量都必须是数值型。然而，我们确实有很多是因子变量的。因此，我们需要调整数据，以便可以使用 `ownn` 函数。首先，使用
    `rbind` 函数将训练集和测试集的协变量数据合并在一起。使用 `model.matrix` 函数和公式 `~.-1`，我们将所有因子变量转换为数值变量。这里的关键问题是
    `model.matrix` 函数是如何工作的？为了使解释简单，如果一个因子变量有 m 个水平，它将创建 m - 1 个新的二元变量，这些变量将跨越 *m*
    维度。我们之所以将训练集和测试集的协变量合并在一起，是因为如果任何因子在任何分区中的水平较少，变量的数量将不相等，我们就无法在测试集上采用基于训练集构建的模型。在获得所有数值变量的协变量矩阵后，我们再次将协变量分为训练和测试区域，使用
    `ownn` 函数指定 *k*-NN 设置，并预测使用袋装方法的准确度。程序如下：'
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `train`, `test`, `cl`, and `testcl` arguments are straightforward to follow;
    see `ownn`, and we will vary the number of neighbors on a grid of 5-50\. Did we
    specify the number of bags or the bootstrap samples? Now, the bagging is carried
    out and the bagged predictions are given. It seems that there might be an approximation
    of the estimate as the package and the function clearly says that the predictions
    are based on bagging:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`train`、`test`、`cl` 和 `testcl` 参数很容易理解；请参阅 `ownn`，我们将在一个 5-50 的网格上改变邻居的数量。我们是否指定了袋数或
    bootstrap 样本数？现在，袋装操作已完成，并给出了袋装预测。看起来估计可能存在近似，因为包和函数明确表示预测是基于 bagging 的：'
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following accuracy plot shows that the model accuracy is stabilizing after
    about 20 neighborhoods. Thus, we have carried out the *k*-NN bagging technique:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的准确度图显示，模型准确度在大约 20 个邻域后趋于稳定。因此，我们已经执行了 *k*-NN 袋装技术：
- en: '![k-NN bagging](img/00178.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![k-NN bagging](img/00178.jpeg)'
- en: 'Figure 6: Accuracy of the *k*-NN bagging'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：*k*-NN 袋装方法的准确度
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Bagging is essentially an ensembling method that consists of homogeneous base
    learners. Bagging was introduced as a bootstrap aggregation method, and we saw
    some of the advantages of the bootstrap method in [Chapter 2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "Chapter 2. Bootstrapping"), *Bootstrapping*. The advantage of the bagging method
    is the stabilization of the predictions. This chapter began with modifications
    for the classification tree, and we saw different methods of improvising the performance
    of a decision tree so that the tree does not overfit the data. The bagging of
    the decision tress and the related tricks followed in the next section. We then
    introduced *k*-NN as an important classifier and illustrated it with a simple
    example. The chapter concluded with the bagging extension of the *k*-NN classifier.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging本质上是一种集成学习方法，由同质的基本学习器组成。Bagging作为一种自助聚合方法被引入，我们在[第二章](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "第二章. 自助聚合")中看到了自助方法的一些优点，*自助聚合*。Bagging方法的优点是预测的稳定性。本章从对分类树的修改开始，我们看到了改进决策树性能的不同方法，以便树不会过度拟合数据。决策树的Bagging和相关技巧在下一节中介绍。然后我们介绍了*k*-NN作为重要的分类器，并用一个简单的例子进行了说明。本章以*k*-NN分类器的Bagging扩展结束。
- en: Bagging helps in reducing the variance of the decision trees. However, the trees
    of the two bootstrap samples are correlated since a lot of common observations
    generate them. In the next chapter, we will look at innovative resampling, which
    will uncorrelate two decision trees.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging有助于减少决策树的方差。然而，由于许多共同观察结果生成它们，两个自助样本的树是相关的。在下一章中，我们将探讨创新的重新采样，这将使两个决策树不相关。
