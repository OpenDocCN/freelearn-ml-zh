- en: Chapter 3. Bagging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。Bagging
- en: Decision trees were introduced in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    and then applied to five different classification problems. Here, they can be
    seen to work better for some databases more than others. We had almost only used
    the `default` settings for the `rpart` function when constructing decision trees.
    This chapter begins with the exploration of some options that are likely to improve
    the performance of the decision tree. The previous chapter introduced the `bootstrap`
    method, used mainly for statistical methods and models. In this chapter, we will
    use it for trees. The method is generally accepted as a machine learning technique.
    Bootstrapping decision trees is widely known as *bagging*. A similar kind of classification
    method is k-nearest neighborhood classification, abbreviated as *k*-NN. We will
    introduce this method in the third section and apply the bagging technique for
    this method in the concluding section of the chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee "第1章。集成技术介绍")，*集成技术介绍*中介绍，然后应用于五个不同的分类问题。在这里，可以看到它们在某些数据库上的表现比其他数据库更好。我们在构建决策树时几乎只使用了`default`设置。这一章将从探索一些可能提高决策树性能的选项开始。上一章介绍了`bootstrap`方法，主要用于统计方法和模型。在这一章中，我们将使用它来构建树。这种方法通常被视为一种机器学习技术。决策树的bootstrapping方法通常被称为*bagging*。类似的一种分类方法是k-最近邻分类，简称*k*-NN。我们将在第三部分介绍这种方法，并在本章的最后一部分应用bagging技术。
- en: 'In this chapter, we will cover the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Classification trees and related pruning/improvement methods
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类树和相关剪枝/改进方法
- en: Bagging the classification tree
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging分类树
- en: Introduction and application of the *k*-NN classifier
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*-NN分类器的介绍和应用'
- en: '*k*-NN bagging extension'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k*-NN bagging扩展'
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the following libraries in the chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将使用以下库：
- en: '`class`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`class`'
- en: '`FNN`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FNN`'
- en: '`ipred`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ipred`'
- en: '`mlbench`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlbench`'
- en: '`rpar`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpar`'
- en: Classification trees and pruning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类树和剪枝
- en: A classification tree is a particular type of decision tree, and its focus is
    mainly on classification problems. Breiman, et al. (1984) invented the decision
    tree and Quinlan (1984) independently introduced the C4.5 algorithm. Both of these
    had a lot in common, but we will focus on the Breiman school of decision trees.
    Hastie, et al. (2009) gives a comprehensive treatment of decision trees, and Zhang
    and Singer (2010) offer a treatise on the recursive partitioning methods. An intuitive
    and systematic R programmatic development of the trees can be found in [Chapter
    9](part0062_split_000.html#1R42S1-2006c10fab20488594398dc4871637ee "Chapter 9. Ensembling
    Regression Models"), *Ensembling Regression Models*, of Tattar (2017).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树是决策树的一种特定类型，其主要关注分类问题。Breiman等人（1984年）发明了决策树，Quinlan（1984年）独立地引入了C4.5算法。这两者有很多共同之处，但我们将重点关注Breiman的决策树学派。Hastie等人（2009年）对决策树进行了全面论述，Zhang和Singer（2010年）对递归分割方法进行了论述。Tattar（2017年）的*集成回归模型*第9章中可以找到对树的一个直观和系统的R程序开发。
- en: A classification tree has many arguments that can be fine-tuned for improving
    performance. However, we will first simply construct the classification tree with
    default settings and visualize the tree. The `rpart` function from the `rpart`
    package can create classification, regression, as well as survival trees. The
    function first inspects whether the regress and is a categorical, numerical, or
    survival object and accordingly sets up the respective classification, regression,
    or survival trees as well as using the relevant split function.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类树有许多参数可以调整以改善性能。然而，我们首先将使用默认设置简单地构建分类树并可视化树。`rpart`包中的`rpart`函数可以创建分类、回归以及生存树。该函数首先检查regress是否为分类、数值或生存对象，并相应地设置相应的分类、回归或生存树，同时使用相关的分割函数。
- en: 'The **German credit** dataset is loaded and the exercise of splitting the data
    into train and test parts is carried out as in earlier settings:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 加载了**德国信用**数据集，并按照早期设置执行了将数据分为训练和测试部分的操作：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can refer to [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*
    to understand how the R code is performed. Now, using the training dataset and
    the chosen formula, we will create the first classification tree. The `rpart`
    function applied on the formula and dataset will create a classification tree.
    The tree is visualized using the `plot` function, and the `uniform=TRUE` option
    ensures that the display aligns the splits at their correct hierarchical levels.
    Furthermore, the text function will display the variable names at the split points
    and `use.n=TRUE` will give the distribution of the Ys at the node. Finally, the
    fitted classification tree is then used to `predict` the good/bad loans for the
    test dataset, comparisons are made for the test sample, and we find the accuracy
    of the tree is 70.61%, which is the same as in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The plot of the preceding code is the left tree display, `DT-01`, of the next
    diagram. It can be seen from the display that there are way too many terminal
    nodes and there also appears to be many splits, meaning that there is a chance
    that we are overfitting the data. Some of the terminal nodes have as few as seven
    observations while many terminal nodes have less than 20 observations. Consequently,
    there is room for improvement.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next iteration of the decision tree, we ask the tree algorithm not to
    split further if we have less than 30 observations in the node `(minsplit=30)`,
    and that the minimum bucket size `(minbucket=15)` must be at least 15\. This change
    should give us an improvisation over the tree, `DT_01`. For the new tree, we will
    again check the change in accuracy:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Classification trees and pruning](img/00169.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Classification trees for German data'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: The `DT-02` tree appears cleaner than `DT-01`, and each terminal node has a
    considerably good number of observations. Importantly, the accuracy is improved
    to `0.7252 - 0.7061 = 0.0191`, or about 2%, which is an improvement.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'The **complexity parameter**, **Cp**, is an important aspect of the trees and
    we will now use it in improving the classification tree. Using the argument `cp=0.005`
    along with the `minsplit` and `minbucket`, we will try to improve the performance
    of the tree:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The performance has now changed from `0.7252` to `0.7316`, and this is an improvement
    again. The tree complexity structure does not seem to have changed much in `DT-03`,
    the left-side tree of the following diagram. We now carry out two changes simultaneously.
    First, we change the split criteria from Gini to information, and then we add
    a loss matrix for misclassification.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'What is a loss matrix for misclassification? If a good loan is identified or
    predicted by the model as a good loan in this way, we have no misclassification.
    Furthermore, if a bad loan is classified as a bad loan, it is also the correct
    decision identified by the algorithm. The consequence of misclassifying a good
    loan as bad is not the same as classifying the bad as good. For instance, if a
    bad customer is granted a loan, the loss would be a four-six digit revenue loss,
    while a good customer who is denied a loan might apply again in 3 months'' time.
    If you ran `matrix( c(0,200,500,0),byrow = TRUE,nrow=2)`, the output would be
    the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是误分类的损失矩阵？如果模型以这种方式将一笔好贷款识别或预测为好贷款，则没有误分类。此外，如果将一笔坏贷款分类为坏贷款，这也是算法识别出的正确决策。将一笔好贷款误分类为坏贷款的后果与将坏贷款分类为好贷款的后果不同。例如，如果向一个坏客户发放贷款，损失将是四到六位数的收入损失，而一个被拒绝贷款的好客户可能在三个月后再次申请。如果你运行`matrix(c(0,200,500,0),
    byrow = TRUE, nrow=2)`，输出将是以下内容：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This means that the penalty for misclassifying a good loan as bad is 200 while
    misclassifying a bad loan as good is 500\. Penalties help a lot and gives weight
    to the classification problem. With this option and the split criteria, we set
    up the next classification tree:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着将一笔好贷款误分类为坏贷款的惩罚是200，而将一笔坏贷款误分类为好贷款的惩罚是500。惩罚在很大程度上有助于并给分类问题增加了权重。使用此选项和分割标准，我们设置了下一个分类树：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Classification trees and pruning](img/00170.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![分类树和剪枝](img/00170.jpeg)'
- en: 'Figure 2: Classification trees with further options'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：具有更多选项的分类树
- en: Note that the decision tree `DT-04` appears to have far fewer splits than `DT-01-03`,
    and does not appear to over-train the data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，决策树`DT-04`似乎比`DT-01-03`有更少的分割，并且看起来没有过度训练数据。
- en: Here, we can see many options that can be used to tweak the decision trees,
    and some of these are appropriate for the classification tree only. However, with
    some of the parameters tweaking might need expert knowledge, although it is nice
    to be aware of the options. Note how the order of the variable importance changes
    from one decision tree to another. Given the data and the tree structure, how
    reliably can we determine the variable importance of a given variable? This question
    will be covered in the next section. A general way of improving the performance
    of the decision tree will also be explored next.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到许多可以用来调整决策树的选项，其中一些仅适用于分类树。然而，调整一些参数可能需要专业知识，尽管了解这些选项是很好的。注意变量重要性在不同决策树之间的变化顺序。给定数据和树结构，我们如何可靠地确定给定变量的变量重要性？这个问题将在下一节中讨论。接下来还将探讨提高决策树性能的一般方法。
- en: Bagging
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging
- en: '**Bagging** stands for **B**oostap **AGG**regat**ING**. This was invented by
    Breiman (1994). Bagging is an example of an *homogeneous ensemble* and this is
    because the base learning algorithm remains as the classification tree. Here,
    each bootstrap tree will be a base learner. This also means that when we bootstrapped
    the linear regression model in [Chapter 2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "Chapter 2. Bootstrapping"), *Bootstrapping*, we actually performed an ensemble
    there. A few remarks with regards to combining the results of multiple trees is
    in order here.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**Bagging**代表**B**oostap **AGG**regat**ING**。这是由Breiman（1994年）发明的。Bagging是**同质集成**的一个例子，这是因为基础学习算法仍然是分类树。在这里，每个自助树将是一个基础学习器。这也意味着当我们对[第2章](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "第2章。自助")中的线性回归模型进行自助时，*自助*，我们实际上在那里执行了一个集成。关于结合多棵树的结果的一些评论也是必要的。'
- en: Ensemble methods combine the outputs from multiple models, also known as base
    learners, and produce a single result. A benefit of this approach is that if each
    of these base learners possesses a *desired property*, then the combined result
    will have increased stability. If a certain base learner is over-trained in a
    specific region of the covariate space, the other base learner will nullify such
    an undesired prediction. It is the increased stability that is expected from the
    ensemble, and bagging many times helps to improve the performance of fitted values
    from a given set of models. Berk (2016), Seni and Elder (2010), and Hastie, et
    al. (2009) may be referred to for more details.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '*A basic result!* If *N* observations are drawn with replacements from *N*
    units, then 37% of the observations are left out on average.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'This result is an important one. Since we will be carrying out a bootstrap
    method, it means that on average for each tree, we will have a holdout sample
    of 37%. A brief simulation program will compute the probability for us. For an
    *N* value ranging from 11 to 100, we will draw a sample with a replacement, find
    how many indexes are left out, and divide the number by *N* to obtain the empirical
    probability of the number of units left out of this simulation. The empirical
    probability is obtained via *B = 100,000* a number of times, and that average
    is reported as the probability of any individual not being selected in a draw
    of *N* items drawn from *N* items with replacements:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Consequently, we can see that in a sample of *N* drawn from *N* items with replacements,
    approximately 0.37 or 37% observations are not selected.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'The bagging algorithm is given as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Draw a random sample of size *N* with replacements from the data consisting
    of *N* observations. The selected random sampleis called a **bootstrap sample***.*
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Construct a classification tree from the bootstrap sample.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign a class to each terminal node and store the predicted class of each observation.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat steps 1-3 a large number of times, for example, *B*.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assign each observation to a final class by a majority vote over the set of
    trees.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main purpose of the bagging procedure is to reduce instability, and this
    is mainly achieved through the `bootstrap` method. As noted earlier, and proven
    by a simulation program, when we draw *N* observations with replacements from
    *N* observations, on average, 37% observations would be excluded from the sample.
    The `bagging` method takes advantage of this sampling with a replacement technique
    and we call the unselected observations **out-of-bag** (**OOB**) observations.
    As with the `bootstrap` method, the resampling technique gives us multiple estimates
    of the different parameters and, using such a sampling distribution, it is then
    possible to carry out the appropriate statistical inference. For a more rigorous
    justification of this technique, the original Breiman (1996) paper is a good read.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*A word of caution*: Suppose an observation has been an out-of-bag observation
    for, say, 100 times out of 271 trees. In the 100th instance when the observation
    is marked for testing purposes, the observation might be classified as `TRUE`
    in *70* of them. It is then tempting to conclude for the observation that *P(TRUE)
    = 70/100 = 0.7*. Such an interpretation can be misleading since the samples in
    the bag are not independent.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意事项**：假设一个观测值在 271 棵树中有 100 次是袋外观测值。在第 100 次标记为测试目的时，该观测值可能被分类为 `TRUE` 的有
    70 次。因此，对于该观测值，可能会得出结论 *P(TRUE) = 70/100 = 0.7*。这种解释可能会误导，因为袋中的样本不是独立的。'
- en: Before we proceed to the software implementation, two important remarks from
    Hastie, et al. (2009) are in order. First, the bagging technique helps in reducing
    the variance of the estimated prediction function and it also works for high-variance,
    low-bias models, such as trees. Second, the core aim of bagging in aggregation
    is to average many noisy models that are unbiased, hence the subsequent reduction
    in variance. As with the bootstrap method, bagging reduces bias as it is a smoothing
    method. We will now illustrate the bagging method using the German credit data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行软件实现之前，Hastie 等人（2009年）提出了两个重要的评论。首先，bagging 技术有助于减少估计预测函数的方差，并且它也适用于高方差、低偏差的模型，如树模型。其次，bagging
    在聚合中的核心目标是平均许多无偏的噪声模型，因此随后方差减少。与自助法一样，bagging 作为一种平滑方法，可以减少偏差。现在，我们将使用德国信贷数据来阐述
    bagging 方法。
- en: 'The `bagging` function from the `ipred` package will help in setting up the
    procedure:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 `ipred` 包的 `bagging` 函数将有助于设置程序：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Aggregation has helped here by significantly increasing the accuracy. Note that
    each run of the bagging method will result in a different answer. This is mainly
    because each tree is set up with different samples and the seeds generating the
    random samples are allowed to change dynamically. Until now, made attempts to
    fix the seeds at a certain number to reproduce the results. Going forward, the
    seeds will seldom be fixed. As an exercise to test your knowledge, work out what
    the `keepx` and `coob options` specified in the bagging function are. Do this
    by using `?bagging`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合在这里通过显著提高准确性有所帮助。请注意，每次运行 bagging 方法都会得到不同的答案。这主要是因为每棵树都是使用不同的样本设置的，并且允许生成随机样本的种子动态变化。到目前为止，已经尝试将种子固定在某个特定数值以重现结果。向前看，种子很少会被固定。作为一个测试你知识的练习，找出
    bagging 函数中指定的 `keepx` 和 `coob` 选项的含义。通过使用 `?bagging` 来完成这个练习。
- en: 'Back to the German credit problem! We have created *B = 500* trees and for
    some crazy reason we want to view all of them. Of course, Packt (the publisher
    of this book) would probably be a bit annoyed if the author insisted on printing
    all 500 trees, and besides, the trees would look ugly as they are not pruned.
    The program must be completed with respect to the book size constraints. With
    that in mind, let''s kick off with the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 回到德国信贷问题！我们已经创建了 *B = 500* 棵树，出于某种疯狂的原因，我们想查看所有这些树。当然，Packt（本书的出版商）可能会对作者坚持打印所有
    500 棵树感到有些恼火，而且，由于这些树没有经过修剪，它们看起来会很丑。程序必须符合书籍大小的限制。考虑到这一点，让我们从以下代码开始：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following steps were performed in the preceding code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤在先前的代码中已执行：
- en: We will first invoke the PDF device.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将调用 PDF 设备。
- en: A new file is then created in the `Output` folder.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，在 `Output` 文件夹中创建了一个新文件。
- en: Next, we begin a loop from `1` to `B`. The `bagging` object constitutes of `B
    = 500` trees and in a temporary object, `tt`, we store the details of the ith
    tree.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们从 `1` 到 `B` 开始一个循环。`bagging` 对象由 `B = 500` 棵树组成，在一个临时对象 `tt` 中，我们存储第 i
    棵树的细节。
- en: We then plot that tree using the `plot` function by extracting the tree details
    out of `tt`, and adding the relevant text associated with the nodes and splits
    of that tree.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用 `plot` 函数绘制该树，通过从 `tt` 中提取树细节，并添加与该树的节点和分割相关的相关文本。
- en: After the loop is completed, the `dev.off` line is run, which will then save
    the `GC2_Bagging_Trees.pdf` file. This portable document file will consist of
    500 trees.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环完成后，运行 `dev.off` 行，这将保存 `GC2_Bagging_Trees.pdf` 文件。这个便携式文档文件将包含 500 棵树。
- en: 'A lot has been said about the benefits of bootstrapping and a lot has also
    been illustrated in the previous chapter. However, putting aside the usual advantages
    of bagging, which are shown in many blogs and references, we will show here how
    to also get reliable inference of the variable importance. It can be easily seen
    that the variable importance varies a lot across the trees. This is not a problem,
    however. When we are asked about the overall reliability of the `variable importance`
    in decision trees for each variable, we can now look at their values across the
    trees and carry out the inference:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自举法的益处已经有很多讨论，并且在上一章中也进行了很多说明。然而，除了在许多博客和参考资料中展示的常规优势之外，我们在这里还将展示如何获得变量重要性的可靠推断。很容易看出，变量重要性在树之间有很大的差异。但这并不是问题。当我们被问到每个变量的决策树中变量重要性的整体可靠性时，我们现在可以查看它们在树中的值并执行推断：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding program needs an explanation. Recollect that `variable.importance`
    is displayed in descending order. If you have gone through the `GC2_Bagging_Trees.pdf`
    file (even a cursory look will do), it can be seen that different trees have different
    variables to the primary split, and consequently the order of importance for variables
    would be different. Thus, we first save the order of variable that we need in
    `vnames` object, and then order for each tree `variable.importance[vnames]` by
    the same order as in `vnames`. Each tree in the loop is extracted with `$mtrees`
    and `$btree$variable.importance` to do what is required. Thus, the `VI data.frame`
    object now consists of the variable importance of all 500 trees set up by the
    bagging procedure. The `colMeans` gives the desired aggregate of the importance
    across the 500 trees, and the desired statistical inference can be carried out
    by looking across the detailed information in the `VI` frame. Note that the last
    three variables have `NA` in the aggregated mean. The reason for the `NA` result
    is that in some classification trees, these variables provide no gains whatsoever
    and are not even among any surrogate split. We can quickly discover how many trees
    contain no information on the importance of these three variables, and then repeat
    the calculation of the `colMeans` using the `na.rm=TRUE` option:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述程序需要解释。回想一下，`variable.importance`是按降序显示的。如果你已经看过`GC2_Bagging_Trees.pdf`文件（即使是粗略地看），你会看到不同的树有不同的主要分割变量，因此变量的重要性顺序也会不同。因此，我们首先将需要的变量顺序保存在`vnames`对象中，然后按照`vnames`中的顺序对每个树的`variable.importance[vnames]`进行排序。循环中的每个树都通过`$mtrees`和`$btree$variable.importance`提取出来以完成所需操作。因此，`VI
    data.frame`对象现在由自举过程设置的500棵树的变量重要性组成。`colMeans`给出了500棵树的重要性汇总，通过查看`VI`框架中的详细信息可以进行所需的统计推断。请注意，最后三个变量在汇总平均值中有`NA`。`NA`结果的原因是在某些分类树中，这些变量根本没有任何增益，甚至不在任何替代分割中。我们可以快速发现有多少棵树没有这些三个变量的重要性信息，然后使用`na.rm=TRUE`选项重复计算`colMeans`：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In the previous section, we explored a host of options in `minsplit`, `minbucket`,
    `split`, and `loss` arguments. Can bagging incorporate these metrics? Using the
    `control` argument for the `bagging` function, we will now improvise on the earlier
    results. The choice of the additional parameters is kept the same as earlier.
    After fitting the bagging object, we inspect the accuracy, and then write the
    classification trees to the `GC2_Bagging_Trees_02.pdf` file. Clearly, the trees
    in this file are much more readable than the counterparts in the `GC2_Bagging_Trees.pdf`
    file, and expectedly so. The variable information table is also obtained for the
    `B = 500` trees with the following code:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探索了`minsplit`、`minbucket`、`split`和`loss`参数的各种选项。自举法能否包含这些指标？使用`bagging`函数的`control`参数，我们现在将对之前的结果进行改进。附加参数的选择与之前保持一致。在拟合自举对象后，我们检查准确性，然后将分类树写入`GC2_Bagging_Trees_02.pdf`文件。显然，这个文件中的树比`GC2_Bagging_Trees.pdf`文件中的树更容易阅读，这是预料之中的。以下代码也获得了`B
    = 500`棵树的变量信息表：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The number of trees for bagging has been chosen arbitrarily at 500\. There
    is no particular reason for this. We will now see how the accuracy over test data
    changes over the number of trees chosen. The inspection will happen over the choice
    of the number of trees varying from 1 to 25 at an increment of 1, and then by
    incrementing by 25 to 50, 75, …, 475, 500\. The reader is left to make sense out
    of this plot. Meanwhile, the following program is straightforward and does not
    require further explanation:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following is the output that is generated:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '![Bagging](img/00171.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Bagging accuracy over the number of trees'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Analytical techniques don't belong to the family of alchemies. If such a procedure
    is invented, we won't have to worry about modeling. The next example shows that
    bagging can also go wrong.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '**Bagging is not a guaranteed recipe!**'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first edition of his book, Berk (2016) cautions the reader not to fall
    prey to the proclaimed superiority of the newly invented methods. Recollect the
    Pima Indians Diabetes problem introduced in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*.
    The accuracy table in there shows that the decision tree gives an accuracy of
    `0.7588`. We now apply the bagging method for the same partition and compute the
    accuracy as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The overall accuracy here is `0.7548` while that of the single tree classification
    model was `0.7588`, which means that bagging has decreased the accuracy. This
    is not a worry if one understands the purpose of bagging. The purpose was always
    to increase the stability of the predictions, and, as such, we would rather accept
    that bagging would have decreased the variance term.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Bagging is a very important technique, and you need not restrict it to classification
    trees alone, but should rather go ahead and explore with other regression methods
    too, such as neural networks and support vector machines. In the next section,
    we will introduce the different approach of k-nearest neighbors (*k*-NN).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-NN classifier'
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    we became familiar with a variety of classification models. Some readers might
    already be familiar with the *k*-NN model. The *k*-NN classifier is one of the
    most simple, intuitive, and non-assumptive models. The name of the model itself
    suggests how it might be working - nearest neighborhoods! And that''s preceded
    by *k*! Thus, if we have *N* points in a study, we find the *k*-nearest points
    in neighborhood, and then make a note of the class of the k-neighbors. The majority
    class of the *k-*neighbors is then assigned to the unit. In case of regression,
    the average of the neighbors is assigned to the unit. The following is a visual
    depiction of *k*-NN:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![k-NN classifier](img/00172.jpeg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Visual depiction of *k*-NN'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: The top left part of the visual depiction of *k*-NN shows the scatterplot of
    27 observations, 16 of which are circles and the remaining 11 are squares. The
    circles are marked in orange ![k-NN classifier](img/00173.jpeg) while the squares
    are marked in blue ![k-NN classifier](img/00174.jpeg). Suppose we choose to set
    up a classifier based on k = 3 neighbors. For every point, we then find its three
    neighbors and assign the majority color to it. Thus, if a circle remains orange,
    it has been correctly identified, and, similarly, if a square is correctly identified,
    its color will remain blue. However, if a circle point has two or more of its
    three nearest neighbors as squares, its color will change to blue and this will
    be denoted by ![k-NN classifier](img/00175.jpeg). Similarly, the color of an incorrectly
    classified square will change to orange ![k-NN classifier](img/00176.jpeg). In
    the top-right block of the preceding diagram, we have the 3-NN predictions, while
    the 7-NN predictions can be found in the lower-left panel of the diagram. Note
    that with 3-NN, we have five misclassifications while with 7-NN, we have seven.
    Thus, increasing the number of the nearest neighbors does not mean that there
    will be an increase in the accuracy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: The bottom right panel is a 1-NN, which will always give a perfect classification.
    However, it is akin to a broken clock that shows the time perfectly only twice
    a day.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing waveform data
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we will perform the analyses for the waveform data. Repeat the code from
    [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee "Chapter 1. Introduction
    to Ensemble Techniques"), *Introduction to Ensemble Techniques*, to obtain the
    waveform data and then partition it for the train and test parts:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A simple scatterplot display of the two variables `X.1` and `X.8` by the classes
    is given in the following diagram. A lot of interplay of colors can be seen in
    the display and it does not appear that any form of logistic regression model
    or decision tree will help the case here. It might be that *k*-NN will be useful
    in this case:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![Analyzing waveform data](img/00177.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Scatterplot for waveform data'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: 'For `k = 10`, we first set up a *k*-NN model for the waveform data. The `knn`
    function from the `class` package is used here:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The accuracy of 90.32% looks promising, and it is better than all the models
    considered in [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    except for the support vector machine. We will expand the search grid for k over
    2 to 50:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that for k = 40, we get the maximum accuracy. Why? We will put the *k*-NN
    in a bag in the next section.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '*k*-NN bagging'
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *k-NN* classifier introduced a classification model in the previous section.
    We can make this robust using the bootstrap method. The broader algorithm remains
    the same. As with the typical bootstrap method, we can always write a program
    consisting of the loop and depending on the number of required bootstrap samples,
    or bags, the control can be specified easily. However, here we will use a function
    from the `FNN` R package. The `ownn` function is useful for carrying out the bagging
    method on the *k*-NN classifier.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ownn` function requires all variables in the dataset to be numeric. However,
    we do have many variables that are factor variables. Consequently, we need to
    tweak the data so that we can use the `ownn` function. The covariate data from
    the training and test dataset are first put together using the `rbind` function.
    Using the `model.matrix` function with the formula `~.-1`, we convert all factor
    variables into numeric variables. The important question here is how does the
    `model.matrix` function work? To keep the explanation simple, if a factor variable
    has m levels, it will create m – 1 new binary variables, which will span the *m*
    dimensions. The reason why we combine the training and test covariates is that
    if any factor has less levels in any of the partition, the number of variables
    will be unequal and we would not be able to adopt the model built on the training
    dataset on the test dataset. After obtaining the covariate matrix with all numeric
    variables, we will split the covariates into the train and test regions again,
    specify the *k*-NN set up with the `ownn` function, and predict the accuracy as
    a consequence of using the bagging method. The program is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `train`, `test`, `cl`, and `testcl` arguments are straightforward to follow;
    see `ownn`, and we will vary the number of neighbors on a grid of 5-50\. Did we
    specify the number of bags or the bootstrap samples? Now, the bagging is carried
    out and the bagged predictions are given. It seems that there might be an approximation
    of the estimate as the package and the function clearly says that the predictions
    are based on bagging:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following accuracy plot shows that the model accuracy is stabilizing after
    about 20 neighborhoods. Thus, we have carried out the *k*-NN bagging technique:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![k-NN bagging](img/00178.jpeg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Accuracy of the *k*-NN bagging'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bagging is essentially an ensembling method that consists of homogeneous base
    learners. Bagging was introduced as a bootstrap aggregation method, and we saw
    some of the advantages of the bootstrap method in [Chapter 2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "Chapter 2. Bootstrapping"), *Bootstrapping*. The advantage of the bagging method
    is the stabilization of the predictions. This chapter began with modifications
    for the classification tree, and we saw different methods of improvising the performance
    of a decision tree so that the tree does not overfit the data. The bagging of
    the decision tress and the related tricks followed in the next section. We then
    introduced *k*-NN as an important classifier and illustrated it with a simple
    example. The chapter concluded with the bagging extension of the *k*-NN classifier.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Bagging helps in reducing the variance of the decision trees. However, the trees
    of the two bootstrap samples are correlated since a lot of common observations
    generate them. In the next chapter, we will look at innovative resampling, which
    will uncorrelate two decision trees.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
