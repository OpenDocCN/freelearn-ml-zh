- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Oversampling Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, we often donâ€™t have enough samples of the minority class.
    One solution might be to gather more samples of such a class. For example, in
    the problem of detecting whether a patient has cancer or not, if we donâ€™t have
    enough samples of the cancer class, we can wait for some time to gather more samples.
    However, such a strategy is not always feasible or sensible and can be time-consuming.
    In such cases, we can augment our data by using various techniques. One such technique
    is oversampling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the concept of oversampling, discuss when
    to use it, and the various techniques to perform it. We will also demonstrate
    how to utilize these techniques through the `imbalanced-learn` library APIs and
    compare their performance using some classical machine learning models. Finally,
    we will conclude with some practical advice on which techniques tend to work best
    under specific real-world conditions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Random oversampling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE variants
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ADASYN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance comparison of various oversampling methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidance for using various oversampling techniques
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oversampling in multi-class classification
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will utilize common libraries such as `numpy`, `scikit-learn`,
    and `imbalanced-learn`. The code and notebooks for this chapter are available
    on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02).
    You can just fire up the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapterâ€™s notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: What is oversampling?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sampling** involves selecting a subset of observations from a larger set
    of observations. In this chapter, weâ€™ll initially focus on binary classification
    problems with two classes: the positive class and the negative class. The minority
    class has significantly fewer instances than the majority class. Later in this
    chapter, we will explore multi-class classification problems. Toward the end of
    this chapter, we will look into oversampling for multi-class classification problems.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '**Oversampling** is a data balancing technique that generates more samples
    of the minority class. However, this can be easily scaled to work for any number
    of classes where there are multiple classes with an imbalance. *Figure 2**.1*
    shows how samples of minority and majority classes are imbalanced (**a**) initially
    and balanced (**b**) after applying an oversampling technique:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 â€“ An increase in the number of minority class samples after oversampling
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '*Why is oversampling needed*, you ask? It is required so that we give the model
    enough samples of the minority class to learn from it. If we offer too few instances
    of the minority class, the model may choose to ignore these minority class examples
    and focus solely on the majority class examples. This, in turn, would lead to
    the model not being able to learn the decision boundary well.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s generate a two-class imbalanced dataset with a 1:99 ratio using the `sklearn`
    libraryâ€™s `make_classification` API, which creates a normally distributed set
    of points for each class. This will generate an imbalanced dataset of two classes:
    one being the minority class with label 1 and the other being the majority class
    with label 0\. We will apply various oversampling techniques throughout this chapter
    to balance this dataset:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code generates 100 examples of class 1 and 9,900 examples of class 0 with
    an imbalance ratio of 1:99\. By plotting the dataset, we can see how the examples
    are distributed:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 â€“ The dataset with an imbalance ratio of 1:99
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we understood the need for oversampling. We also generated
    a synthetic imbalanced binary classification dataset to demonstrate the application
    of various oversampling techniques.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Random oversampling
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest strategy to balance the imbalance in a dataset is to randomly choose
    samples of the minority class and repeat or duplicate them. This is also called
    **random oversampling** **with replacement**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: To increase the number of minority class observations, we can replicate the
    minority class data observations enough times to balance the two classes. Does
    this sound too trivial? Yes, but it works. By increasing the number of minority
    class samples, random oversampling reduces the bias toward the majority class.
    This helps the model learn the patterns and characteristics of the minority class
    more effectively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: We will use random oversampling from the `imbalanced-learn` library. The `fit_resample`
    API from the `RandomOverSampler` class resamples the original dataset and balances
    it. The `sampling_strategy` parameter is used to specify the new ratio of various
    classes. For example, we could say `sampling_strategy=1.0` to have an equal number
    of the two classes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various ways to specify `sampling_strategy`, such as a float value,
    string value, or `dict` â€“ for example, {0: 50, 1: 50}:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is the output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So, we went from a ratio of 1:99 to 1:1, which is what we expected with `sampling_strategy=1.0`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s plot the oversampled dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 â€“ Dataset oversampled using RandomOverSampler (label 1 examples appear
    unchanged due to overlap)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: After applying random oversampling, the examples with label 1 overlap each other,
    creating the impression that nothing has changed. Repeating the same data point
    over and over can cause the model to memorize the specific data points and not
    be able to generalize to new, unseen examples. The `shrinkage` parameter in `RandomOverSampler`
    lets us perturb or shift each point by a small amount.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨éšæœºè¿‡é‡‡æ ·åï¼Œæ ‡ç­¾ä¸º1çš„ç¤ºä¾‹ä¼šç›¸äº’é‡å ï¼Œç»™äººä¸€ç§æ²¡æœ‰å˜åŒ–çš„æ„Ÿè§‰ã€‚åå¤é‡å¤ç›¸åŒçš„æ•°æ®ç‚¹å¯èƒ½å¯¼è‡´æ¨¡å‹è®°ä½ç‰¹å®šçš„æ•°æ®ç‚¹ï¼Œè€Œæ— æ³•æ¨å¹¿åˆ°æ–°çš„ã€æœªè§è¿‡çš„ç¤ºä¾‹ã€‚`RandomOverSampler`ä¸­çš„`shrinkage`å‚æ•°è®©æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªå°é‡æ‰°åŠ¨æˆ–ç§»åŠ¨æ¯ä¸ªç‚¹ã€‚
- en: The value of the `shrinkage` parameter has to be greater than or equal to 0
    and can be `float` or `dict`. If a `float` data type is used, the same shrinkage
    factor will be used for all classes. If a `dict` data type is used, the shrinkage
    factor will be specific for each class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`shrinkage`å‚æ•°çš„å€¼å¿…é¡»å¤§äºæˆ–ç­‰äº0ï¼Œå¯ä»¥æ˜¯`float`æˆ–`dict`ç±»å‹ã€‚å¦‚æœä½¿ç”¨`float`æ•°æ®ç±»å‹ï¼Œç›¸åŒçš„æ”¶ç¼©å› å­å°†ç”¨äºæ‰€æœ‰ç±»åˆ«ã€‚å¦‚æœä½¿ç”¨`dict`æ•°æ®ç±»å‹ï¼Œæ”¶ç¼©å› å­å°†é’ˆå¯¹æ¯ä¸ªç±»åˆ«å…·ä½“æŒ‡å®šã€‚'
- en: 'In *Figure 2**.4*, we can observe the impact of random oversampling with `shrinkage=0.2`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨**å›¾2**.4ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°`shrinkage=0.2`çš„éšæœºè¿‡é‡‡æ ·çš„å½±å“ï¼š
- en: '![](img/B17259_02_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](img/B17259_02_04.jpg)'
- en: Figure 2.4 â€“ Result of applying random oversampling with shrinkage=0.2
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾2.4 â€“ åº”ç”¨éšæœºè¿‡é‡‡æ ·ï¼ˆshrinkage=0.2ï¼‰çš„ç»“æœ
- en: Toward the end of this chapter, we will compare the performance of random oversampling
    with various other oversampling techniques across multiple models and datasets.
    This will provide insights into their effectiveness in real-world applications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« çš„ç»“å°¾ï¼Œæˆ‘ä»¬å°†æ¯”è¾ƒéšæœºè¿‡é‡‡æ ·ä¸å…¶ä»–å¤šç§è¿‡é‡‡æ ·æŠ€æœ¯åœ¨å¤šä¸ªæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚è¿™å°†ä¸ºæˆ‘ä»¬æä¾›å…³äºå®ƒä»¬åœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§çš„è§è§£ã€‚
- en: ğŸš€ Random oversampling in production at Grab
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸš€ Grabåœ¨ç”Ÿäº§ä¸­ä½¿ç”¨éšæœºè¿‡é‡‡æ ·
- en: Grab, a ride-hailing and food delivery service in Southeast Asia, developed
    an image collection platform [1] for storing and retrieving imagery and map data.
    A key feature of this platform was its ability to automatically detect and blur
    **Personally Identifiable Information** (**PII**), such as faces and license plates,
    in street-level images. This was essential for maintaining user privacy. The dataset
    that was used for this purpose had a significant imbalance, with far more negative
    samples (images without PII) than positive ones (images with PII). Manual annotation
    was not feasible, so they turned to machine learning to solve this problem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Grabæ˜¯ä¸€å®¶ä¸œå—äºšçš„æ‰“è½¦å’Œé£Ÿå“é…é€æœåŠ¡å…¬å¸ï¼Œå¼€å‘äº†ä¸€ä¸ªç”¨äºå­˜å‚¨å’Œæ£€ç´¢å›¾åƒå’Œåœ°å›¾æ•°æ®çš„å›¾åƒæ”¶é›†å¹³å°[1]ã€‚è¯¥å¹³å°çš„ä¸€ä¸ªå…³é”®ç‰¹æ€§æ˜¯èƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹å’Œæ¨¡ç³Šè¡—æ™¯å›¾åƒä¸­çš„**ä¸ªäººèº«ä»½ä¿¡æ¯**ï¼ˆ**PII**ï¼‰ï¼Œå¦‚äººè„¸å’Œè½¦ç‰Œã€‚è¿™å¯¹äºç»´æŠ¤ç”¨æˆ·éšç§è‡³å…³é‡è¦ã€‚ç”¨äºæ­¤ç›®çš„çš„æ•°æ®é›†å­˜åœ¨æ˜¾è‘—çš„ä¸å¹³è¡¡ï¼Œè´Ÿæ ·æœ¬ï¼ˆæ²¡æœ‰PIIçš„å›¾åƒï¼‰è¿œå¤šäºæ­£æ ·æœ¬ï¼ˆæœ‰PIIçš„å›¾åƒï¼‰ã€‚æ‰‹åŠ¨æ ‡æ³¨ä¸å¯è¡Œï¼Œæ‰€ä»¥ä»–ä»¬è½¬å‘æœºå™¨å­¦ä¹ æ¥è§£å†³æ­¤é—®é¢˜ã€‚
- en: To address the data imbalance, Grab employed the random oversampling technique
    to increase the number of positive samples, thereby enhancing the performance
    of their machine learning model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è§£å†³æ•°æ®ä¸å¹³è¡¡ï¼ŒGrabé‡‡ç”¨äº†éšæœºè¿‡é‡‡æ ·æŠ€æœ¯æ¥å¢åŠ æ­£æ ·æœ¬çš„æ•°é‡ï¼Œä»è€Œæé«˜äº†ä»–ä»¬çš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: Problems with random oversampling
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: éšæœºè¿‡é‡‡æ ·çš„é—®é¢˜
- en: Random oversampling can often lead to overfitting of the model since the generated
    synthetic observations get repeated, and the model sees the same observations
    again and again. Shrinkage tries to handle that in some sense, but it may be challenging
    to come up with an apt value of shrinkage, and shrinkage doesnâ€™t care if the generated
    synthetic samples overlap with the majority class samples, which can lead to other
    problems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºè¿‡é‡‡æ ·å¾€å¾€ä¼šå¯¼è‡´æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œå› ä¸ºç”Ÿæˆçš„åˆæˆè§‚å¯Ÿå€¼ä¼šé‡å¤ï¼Œæ¨¡å‹ä¼šä¸€æ¬¡åˆä¸€æ¬¡åœ°çœ‹åˆ°ç›¸åŒçš„è§‚å¯Ÿå€¼ã€‚æ”¶ç¼©è¯•å›¾åœ¨æŸç§ç¨‹åº¦ä¸Šå¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œä½†å¯èƒ½å¾ˆéš¾æ‰¾åˆ°ä¸€ä¸ªåˆé€‚çš„æ”¶ç¼©å€¼ï¼Œè€Œä¸”æ”¶ç¼©å¹¶ä¸å…³å¿ƒç”Ÿæˆçš„åˆæˆæ ·æœ¬æ˜¯å¦ä¸å¤šæ•°ç±»æ ·æœ¬é‡å ï¼Œè¿™å¯èƒ½å¯¼è‡´å…¶ä»–é—®é¢˜ã€‚
- en: In the previous section, we learned about the most basic and practical technique
    for applying oversampling to balance a dataset and reduce bias toward the majority
    class. Many times, random oversampling itself might give us such a high boost
    to our modelâ€™s performance that we may not even need to apply more advanced techniques.
    In production settings, it would also be beneficial to keep things plain and simple
    until we are ready to introduce more complexity in the pipeline. As they say,
    â€œpremature optimization is the root of all evil,â€ so we start with something simple,
    so long as it does improve our modelâ€™s performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we will explore some alternative techniques, such
    as SMOTE and ADASYN, which adopt a different approach to oversampling and alleviate
    some of the problems associated with the random oversampling technique.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main problem with random oversampling is that it duplicates the observations
    from the minority class. This can often cause overfitting. **Synthetic Minority
    Oversampling Technique** (**SMOTE**) [2] solves this problem of duplication by
    using a technique called **interpolation**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation involves creating new data points in the range of known data points.
    Think of interpolation as being similar to the process of reproduction in biology.
    In reproduction, two individuals come together to produce a new individual with
    traits of both of them. Similarly, in interpolation, we pick two observations
    from the dataset and create a new observation by choosing a random point on the
    line joining the two selected points.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'We oversample the minority class by interpolating synthetic examples. That
    prevents the duplication of minority samples while generating new synthetic observations
    similar to the known points. *Figure 2**.5* depicts how SMOTE works:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_05.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 â€“ Working of SMOTE
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: The majority and minority class samples are plotted (left)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synthetic samples are generated by taking a random point on the line joining
    a minority sample to two nearest neighbor majority class samples (right)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE was originally designed for continuous inputs. To keep the explanations
    simple, weâ€™ll start with continuous inputs and discuss other kinds of inputs later.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: First, we will examine the functioning of SMOTE and explore any potential disadvantages
    associated with this technique.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: How SMOTE works
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SMOTE algorithm works as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: It considers only the samples from the minority class.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It trains KNN on the minority samples. A typical value of `k` is 5.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each minority sample, a line is drawn between the point and each of its
    KNN examples.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each such line segment, a point on the segment is randomly picked to create
    a new synthetic example.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Letâ€™s use SMOTE using APIs from the `imbalanced-learn` library:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The oversampled dataset looks like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_06.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 â€“ Oversampling using SMOTE
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš€ Oversampling techniques in production at Microsoft
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application at Microsoft [3], machine learning was employed
    to forecast **Live Site Incidents** (**LSIs**) for early detection and escalation
    of incidents for engineering teams. Every day, a high volume of incidents was
    being generated, most of which started as low-severity issues. Due to limited
    resources, it was impractical for engineering teams to investigate all incidents,
    leading to potential delays in mitigating critical issues until they had a significant
    customer impact.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this, Microsoft employed machine learning to forecast which LSIs
    could escalate into severe problems, aiming for proactive identification and early
    resolution. The challenge was the data imbalance in the training set: out of approximately
    40,000 incidents, fewer than 2% escalated to high severity. Microsoft used two
    different oversampling techniquesâ€” bagged classification (covered in [*Chapter
    4*](B17259_04.xhtml#_idTextAnchor120), *Ensemble Methods*), and SMOTE, which were
    the most effective in improving the modelâ€™s performance. They used a two-step
    pipeline for balancing classes: first, oversampling with **SMOTE** and then undersampling
    with **RandomUnderSampler** (covered in [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079),
    *Undersampling Methods*). The pipeline automatically selected the optimal sampling
    ratios for both steps, and SMOTE performed better when combined with undersampling.
    The resulting end-to-end automated model was designed to be generic, making it
    applicable across different teams within or outside Microsoft, provided historical
    incidents were available for learning. The LSI insight tool used this model, which
    was adopted by various engineering teams.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the limitations of using SMOTE.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Problems with SMOTE
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SMOTE has its pitfalls â€“ for example, it can add noise to an already noisy
    dataset. It can also lead to class overlap issues as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'SMOTE generates minority class samples without considering the majority class
    distribution, which may increase the overlap between the classes. In *Figure 2**.7*,
    weâ€™re plotting the binary classification imbalanced dataset before and after applying
    SMOTE. We can see a lot of overlap between the two classes after applying SMOTE:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_02_07.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 â€“ Binary classification dataset before (left) and after (right) applying
    SMOTE (see the overlap between two classes on the right)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: The other case may be that you have a huge amount of data, and running SMOTE
    may increase the runtime of your pipeline.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem 1 can be solved by using the SMOTE variant Borderline-SMOTE (discussed
    in the next section).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about SMOTE, which uses the nearest neighbor technique
    to generate synthetic samples of the minority class. Sometimes, SMOTE may perform
    better than random oversampling since it exploits the proximity to other minority
    class samples to generate new samples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE variants
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, letâ€™s look at some of the SMOTE variants, such as Borderline-SMOTE, SMOTE-NC,
    and SMOTEN. These variants apply the SMOTE algorithm to samples of a certain kind
    and may not always be applicable.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Borderline-SMOTE
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Borderline-SMOTE [4] is a variation of SMOTE that generates synthetic samples
    from the minority class samples that are near the classification boundary, which
    divides the majority class from the minority class.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Why consider samples on the classification boundary?
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea is that the examples near the classification boundary are more prone
    to misclassification than those far away from the decision boundary. Producing
    more such minority samples along the boundary would help the model learn better
    about the minority class. Intuitively, it is also true that the points away from
    the classification boundary likely wonâ€™t make the model a better classifier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: 'Hereâ€™s a step-by-step algorithm for Borderline-SMOTE:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: We run a KNN algorithm over the whole dataset.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we divide the minority class points into three categories:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Noise* points are minority class examples that have all the neighbors from
    the majority class. These points are buried among majority-class neighbors. They
    are likely outliers and can safely be ignored as â€œnoise.â€'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Safe* points have more minority-class neighbors than majority-class neighbors.
    Such observations donâ€™t contain much information and can be safely ignored.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Danger* points have more majority-class neighbors than minority-class neighbors.
    This implies that such observations are on or close to the boundary between the
    two classes.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we train a KNN model only on the minority class examples.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we apply the SMOTE algorithm to the `Danger` points. Note that the
    neighbors of these `Danger` points may or may not be marked as `Danger`.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As shown in *Figure 2**.8*, Borderline-SMOTE focuses on the danger class points
    for synthetic data generation:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_08.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 â€“ The Borderline-SMOTE algorithm uses only danger points to generate
    synthetic samples. Danger points have more majority-class neighbors than minority-class
    ones
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.9* shows how Borderline-SMOTE focuses on the minority class samples
    that are near the classification boundary, which separates the majority and minority
    classes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_09.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 â€“ Illustrating Borderline-SMOTE
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: a) Plots of majority and minority class samples
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: b) Synthetic samples generated using neighbors near the classification boundary
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Letâ€™s see how we can use Borderline-SMOTE from the `imbalanced-learn` library
    to perform oversampling of the data:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Can you guess the problem with focusing solely on data points on the decision
    boundary of the two classes?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this technique focuses so heavily on a very small number of points on
    the boundary, the points inside the minority class clusters are not sampled at
    all:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_10.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 â€“ The Borderline-SMOTE algorithm utilizing danger points, with more
    majority- than minority-class neighbors, to generate synthetic samples
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about Borderline-SMOTE, which generates synthetic
    minority class samples by focusing on the samples that are close to the classification
    boundary of the majority and minority classes, which, in turn, may help in improving
    the discrimination power of the model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: ğŸš€ Oversampling techniques in production at Amazon
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application, Amazon used machine learning to optimize packaging
    types for products, aiming to reduce waste while ensuring product safety [5].
    In their training dataset, which featured millions of product and package combinations,
    Amazon faced a significant class imbalance, with as few as 1% of the examples
    representing unsuitable product-package pairings (minority class).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle this imbalance, Amazon used various oversampling techniques:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '- Borderline-SMOTE oversampling, which resulted in a 4%-7% increase in PR-AUC
    but increased the training time by 25%-35%.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '- A hybrid of random oversampling and random undersampling, where they randomly
    oversampled the minority class and undersampled the majority class. It led to
    a 6%-10% improvement in PR-AUC and increased the training time by up to 25%.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The best-performing technique was two-phase learning with random undersampling
    (discussed in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep
    Learning Methods)*, which improved PR-AUC by 18%-24% with no increase in training
    time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: They mentioned that the effectiveness of a technique in dealing with dataset
    imbalance is both domain- and dataset-specific. This real-world example underscores
    the effectiveness of oversampling techniques in tackling class imbalance issues.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn about another oversampling technique, called ADASYN, that
    oversamples examples near boundaries and in other low-density regions without
    completely ignoring data points that do not lie on the boundary.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: ADASYN
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While SMOTE doesnâ€™t distinguish between the density distribution of minority
    class samples, **Adaptive Synthetic Sampling** (**ADASYN**) [6] focuses on harder-to-classify
    minority class samples since they are in a low-density area. ADASYN uses a weighted
    distribution of the minority class based on the difficulty of classifying the
    observations. This way, more synthetic data is generated from harder samples:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_11.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 â€“ Illustration of how ADASYN works
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: a) The majority and minority class samples are plotted
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b) Synthetic samples are generated depending on the hardness factor (explained
    later)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While SMOTE uses all samples from the minority class for oversampling uniformly,
    in ADASYN, the observations that are harder to classify are used more often.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Another difference between the two techniques is that, unlike SMOTE, ADASYN
    also uses the majority class observations while training KNN. It then decides
    the hardness of samples based on how many majority observations are its neighbors.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Working of ADASYN
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ADASYN follows a simple algorithm. Here is the step-by-step working of ADASYN:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: First, it trains a KNN on the entire dataset.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each observation of the minority class, we find the hardness factor. This
    factor tells us how difficult it is to classify that data point. The hardness
    factor, denoted by r, is the ratio of the number of majority class neighbors with
    the total number of neighbors. Here, r = MÂ _Â KÂ , where M is the count of majority
    class neighbors and K is the total number of nearest neighbors.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each minority observation, we generate synthetic samples proportional to
    the hardness factor by drawing a line between the minority observation and its
    neighbors (neighbors could be from the majority class or minority class). The
    harder it is to classify a data point, the more synthetic samples will be created
    for it.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Letâ€™s see how we can use the ADASYN API from the `imbalanced-learn` library
    to perform oversampling of the data:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the output:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/B17259_02_12.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 â€“ ADASYN prioritizes harder samples and incorporates majority class
    examples in KNN to assess sample hardness
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_13.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 â€“ A memory aid summarizing various oversampling techniques
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about ADASYN. Next, letâ€™s see how we can deal with
    cases when our data contains categorical features.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Categorical features and SMOTE variants (SMOTE-NC and SMOTEN)
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What if your data contains categorical features? A categorical feature can
    take one of a limited or fixed number of possible values, and itâ€™s a parallel
    to enumerations (enums) in computer science. These could be nominal categorical
    features that lack a natural order (for example, hair color, ethnicity, and so
    on) or ordinal categorical features that have an inherent order (for example,
    low, medium, and high):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 â€“ Categorical data and its types with examples
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: For ordinal features, we can just encode them via sklearnâ€™s `OrdinalEncoder`,
    which assigns the categories to the values 0, 1, 2, and so on.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For nominal features, none of the SMOTE variants we have learned so far will
    work. However, `RandomOverSampler` can handle nominal features too:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: However, SMOTE, by default, works only on continuous data and cannot be directly
    used on categorical data. *Why?* Thatâ€™s because SMOTE works by generating a random
    point on the line joining two different data points of the minority class (also
    called interpolation). If our data is categorical and has values of â€œyesâ€ and
    â€œno,â€ we would first need to transform such values into numbers. Even when we
    do so, say â€œyesâ€ is mapped to 1 and â€œnoâ€ is mapped to 0, the interpolation via
    SMOTE may end up producing a new point of 0.3, which does not map to any real
    category.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Also, we cannot use the `shrinkage` parameter in `RandomOverSampler` with categorical
    data because this parameter is designed only for continuous values.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'However, two variants of SMOTE can deal with categorical features:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '`imbalanced-learn` to oversample our dataset. The first item in the dataset
    is categorical, and the second item is continuous:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the output:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Synthetic Minority Oversampling Technique for Nominal** (**SMOTEN**) is used
    for nominal categorical data. SMOTEN performs the majority vote similar to SMOTE-NC
    for all the features. It considers all features as nominal categorical, and the
    feature value of new samples is decided by taking the most frequent category of
    the nearest neighbors. The distance metric thatâ€™s used for calculating the nearest
    neighbors is called the **Value Distance Metric** (**VDM**). VDM computes the
    distance between two attribute values by considering the distribution of class
    labels associated with each value. It is based on the idea that two attribute
    values are more similar if they have similar distributions of class labels. This
    way, VDM can capture the underlying relationships between categorical attributes
    and their corresponding class labels.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Letâ€™s look at some example code that uses SMOTEN:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the output:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In *Table 2.1*, we can see SMOTE, SMOTEN, and SMOTENC, with a few examples
    for each technique to demonstrate the difference between them:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** **of SMOTE** | **Features Supported** | **Example Data** |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| SMOTE | Only numerical | features: [2.3, 4.5, 1.2], label: 0features: [3.4,
    2.2, 5.1], label: 1 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| SMOTEN | Categorical(nominal or ordinal) | features: [â€˜greenâ€™, â€˜squareâ€™],
    label: 0features: [â€˜redâ€™, â€˜circleâ€™], label: 1 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| SMOTENC | Numerical or categorical(nominal or ordinal) | features: [2.3,
    â€˜greenâ€™, â€˜smallâ€™, â€˜squareâ€™], label: 0features: [3.4, â€˜redâ€™, â€˜largeâ€™, â€˜circleâ€™],
    label: 1 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: Table 2.1 â€“ SMOTE and some of its common variants with example data
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we should use SMOTENC when we have a mix of categorical and continuous
    data types, while SMOTEN can only be used when all the columns are categorical.
    You might be curious about how the various oversampling methods compare with each
    other in terms of model performance. Weâ€™ll explore this topic in the next section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Model performance comparison of various oversampling methods
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Letâ€™s examine how some popular models perform with the different oversampling
    techniques weâ€™ve discussed. Weâ€™ll use two datasets for this comparison: one synthetic
    and one real-world dataset. Weâ€™ll evaluate the performance of four oversampling
    techniques, as well as no sampling, using logistic regression and random forest
    models.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all the related code in this bookâ€™s GitHub repository. In *Figure
    2**.15* and *Figure 2**.16*, we can see the average precision score values for
    both models on the two datasets:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_15.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 â€“ Performance comparison of various oversampling techniques on a
    synthetic dataset
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_16.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 â€“ Performance comparison of various oversampling techniques on the
    thyroid_sick dataset
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these plots, we can draw some useful conclusions:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '**Effectiveness of oversampling**: In general, using oversampling techniques
    seems to improve the average precision score compared to not using any sampling
    (NoSampling).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm sensitivity**: The effectiveness of oversampling techniques varies
    depending on the machine learning algorithm used. For example, random forest seems
    to benefit more from oversampling techniques than logistic regression, especially
    on synthetic data.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thyroid_sick` dataset but showed variations in the synthetic data.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thyroid_sick` data'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For random forest, Borderline-SMOTE had the highest average precision score
    on synthetic data
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thyroid_sick` data.*   **No clear winner**: There is no single oversampling
    technique that outperforms all others across all conditions. The choice of technique
    may depend on the specific algorithm and dataset being used.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the models used here are not tuned with the best hyperparameters.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the hyperparameters of random forest and logistic regression models may
    improve the models' performance further.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: In general, there is no single technique that will always do better than the
    rest. We have multiple variables at play here, namely the â€œmodelâ€ and the â€œdata.â€
    Most of the time, the only way to know is to try out a bunch of these techniques
    and find the one that works the best for our model and data. You may find yourself
    curious about how to choose from the numerous oversampling options available.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Guidance for using various oversampling techniques
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, letâ€™s review some guidelines on how to navigate through the various oversampling
    techniques we went over and how these techniques differ from each other:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Train a model without applying any sampling techniques. This will be our model
    with baseline performance. Any oversampling technique we apply is expected to
    give a boost to this performance.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with random oversampling and add some shrinkage too. We may have to play
    with some values of shrinkage to see if the modelâ€™s performance improves.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When we have categorical features, we have a couple of options:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert all categorical features into numerical features first using one-hot
    encoding, label encoding, feature hashing, or other feature transformation techniques.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (Only for nominal categorical features) Use SMOTENC and SMOTEN directly on the
    data.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply various oversampling techniques â€“ random oversampling, SMOTE, Borderline-SMOTE,
    and ADASYN â€“ and measure the modelâ€™s performance on metrics applicable to your
    problem, such as the average precision score, ROC-AUC, precision, recall, F1 score,
    and more.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since oversampling alters the distribution of the training dataset, which is
    not the case for the test set or the real world, using oversampling can potentially
    generate biased predictions. After using oversampling, it can be essential to
    recalibrate our modelâ€™s probability scores depending on the application. Recalibration
    of the model corrects any bias introduced by altering the class distribution,
    ensuring more reliable decision-making when deployed. Similarly, adjusting the
    classification threshold is key for accurate model interpretation, especially
    with imbalanced datasets. For more details on recalibration and threshold adjustment,
    please see [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279), *Model Calibration*,
    and [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive* *Learning*,
    respectively.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When to avoid oversampling
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to Data Imbalance
    in Machine Learning*, we discussed scenarios where data imbalance may not be a
    concern. Those considerations should be revisited before you opt for oversampling
    techniques. Despite criticisms, the applicability of oversampling should be evaluated
    on a case-by-case basis. Here are some additional technical considerations to
    keep in mind when choosing to apply oversampling techniques:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational cost**: Oversampling increases the datasetâ€™s size, leading
    to higher computational demands in terms of processing time and hardware resources.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality**: If the minority class data is noisy or has many outliers,
    oversampling can introduce more noise, reducing model reliability.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classifier limitations**: In scenarios with system constraints, such as extremely
    low latency, or when dealing with legacy systems, the use of strong classifiers
    (complex and more accurate models) may not be feasible. In these cases, we may
    be limited to using weak classifiers. Weak classifiers are simpler and less accurate
    but require fewer computational resources and have lower runtime latency. In such
    situations, oversampling can be beneficial [7]. For strong classifiers, oversampling
    may offer diminishing returns, and optimizing the decision threshold could sometimes
    serve as a simpler, less resource-intensive alternative.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider these factors when deciding whether to use oversampling methods for
    imbalanced datasets.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 2.2* summarizes the key ideas, pros, and cons of various oversampling
    techniques. This can help you better evaluate which oversampling method to choose:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **SMOTE** | **Borderline-SMOTE** | **ADASYN** | **SMOTE-NC** **and SMOTEN**
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| Key idea | Choose random points on the line joining the nearest neighbors
    of minority class examples. | Choose the minority samples on the boundary between
    the majority and minority classes. Perform SMOTE for such samples on the boundary.
    | Automatically decides the number of minority class samples to generate according
    to density distribution. More points are generated where the density distribution
    is low. | It performs a majority vote for the categorical features. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| Pro | Usually reduces false negatives. | Creates synthetic samples that are
    not naÃ¯ve copies of the known data. | It cares about the density distribution
    of different classes. | It works with categorical data. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| Con | Overlapping classes may occur and can introduce more noise to data.
    This may not work well with high-dimensional data or multi-class classification
    problems. | It does not care about the distribution of minority class examples.
    | It focuses on areas where there is overlap between classes. It may focus too
    much on outliers, resulting in poor model performance. | The same as SMOTE. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: Table 2.2 â€“ Summarizing the various oversampling techniques that were discussed
    in this chapter
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at some general guidelines to apply the various oversampling
    techniques we learned about in this chapter and the pros and cons of using them.
    Next, we will look at how to extend the various oversampling methods to multi-class
    classification problems.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling in multi-class classification
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In multi-class classification problems, we have more than two classes or labels
    to be predicted, and hence more than one class may be imbalanced. This adds some
    more complexity to the problem. However, we can apply the same techniques to multi-class
    classification problems as well. The `imbalanced-learn` library provides the option
    to deal with multi-class classification in almost all the supported methods. We
    can choose from various sampling strategies using the `sampling_strategy` parameter.
    For multi-class classification, we can pass some fixed string values (called built-in
    strategies) to the `sampling_strategy` parameter in the SMOTE API. We can also
    pass a dictionary with the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Keys as the class labels
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values as the number of samples of that class
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the built-in strategies for `sampling_strategy` when using the parameter
    as a string:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: The `minority` strategy resamples only the minority class.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `not minority` strategy resamples all classes except the minority class.
    This may be helpful in the case of multi-class imbalance, where we have more than
    two classes and multiple classes are imbalanced, but we donâ€™t want to touch the
    minority class.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `not majority` strategy resamples all classes except the majority class.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `all` strategy resamples all classes.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `auto` strategy is the same as the `not` `majority` strategy.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following code shows the usage of SMOTE for multi-class classification using
    various sampling strategies.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'First, letâ€™s create a dataset containing 100 samples with three classes that
    have weights of 0.1, 0.4, and 0.5:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As expected, our dataset contains the three classes in the ratio 10:40:50 for
    classes 0, 1, and 2, respectively.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, letâ€™s apply SMOTE with the â€œ*minority*â€ sampling strategy. This will oversample
    the class with the least number of samples:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since class 0 previously had the least number of samples, the â€œ*minority*â€ sampling
    strategy only oversampled class 0, making the number of samples equal to the number
    of samples in the majority class.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, weâ€™re using a dictionary for oversampling. Here, for
    each class label (0, 1, or 2) as `key` in the `sampling_strategy` dictionary,
    we have the number of desired samples for each targeted class as `value`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is the output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Tip
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Please note that when using `dict` within `sampling_strategy`, the number of
    desired samples for each class should be greater than or equal to the original
    number of samples. Otherwise, the `fit_resample` API will throw an exception.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw how to extend oversampling strategies to handle cases
    when we have imbalanced datasets with more than two classes. Most of the time,
    the â€œautoâ€ `sampling_strategy` would be good enough and would balance all the
    classes.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through various oversampling techniques for dealing
    with imbalanced datasets and applied them using Pythonâ€™s `imbalanced-learn` library
    (also called `imblearn`). We also saw the internal workings of some of the techniques
    by implementing them from scratch. While random oversampling generates new minority
    class samples by duplicating them, SMOTE-based techniques work by choosing random
    samples in the direction of nearest neighbors of the minority class samples. Though
    oversampling can potentially overfit the model on your data, it usually has more
    pros than cons, depending on the data and model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: We applied them to some of the synthesized and publicly available datasets and
    benchmarked their performance and effectiveness. We saw how different oversampling
    techniques may lead to model performance on a varying scale, so it becomes crucial
    to try a few different oversampling techniques to decide on the one thatâ€™s most
    optimal for our data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: If you feel intrigued by the prospect of discovering oversampling approaches
    relevant to deep learning models, we invite you to check out [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*, where weâ€™ll discuss data-level techniques
    within the realm of deep learning.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go over various undersampling techniques.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explore the two variants of SMOTE, namely KMeans-SMOTE and SVM-SMOTE, from the
    `imbalanced-learn` library, not discussed in this chapter. Compare their performance
    with vanilla SMOTE, Borderline-SMOTE, and ADASYN using the logistic regression
    and random forest models.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a classification problem with two classes, letâ€™s say the minority class
    to majority class ratio is 1:20\. How should we balance this dataset? Should we
    apply the balancing technique at test or evaluation time? Please provide a reason
    for your answer.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Letâ€™s say we are trying to build a model that can estimate whether a person
    can be granted a bank loan or not. Out of the 5,000 observations we have, only
    500 people got the loan approved. To balance the dataset, we duplicate the approved
    people data and then split it into train, test, and validation datasets. Are there
    any issues with using this approach?
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data normalization helps in dealing with data imbalance. Is this true? Why or
    why not?
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explore the various oversampling APIs available from the `imbalanced-learn`
    library here: [https://imbalanced-learn.org/stable/references/over_sampling.html](https://imbalanced-learn.org/stable/references/over_sampling.html).
    Pay attention to the various parameters of each of the APIs.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Protecting Personal Data in Grabâ€™s Imagery* (2021), [https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery).'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, *SMOTE: Synthetic
    Minority Over-sampling Technique*, jair, vol. 16, pp. 321â€“357, Jun. 2002, doi:
    10.1613/jair.953.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Live Site Incident escalation forecast* (2023), [https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178).'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H. Han, W.-Y. Wang, and B.-H. Mao, *Borderline-SMOTE: A New Over-Sampling Method
    in Imbalanced Data Sets Learning*, in Advances in Intelligent Computing, D.-S.
    Huang, X.-P. Zhang, and G.-B. Huang, Eds., in Lecture Notes in Computer Science,
    vol. 3644\. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 878â€“887\.
    doi: 10.1007/11538059_91.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'P. Meiyappan and M. Bales, *Position Paper: Reducing Amazonâ€™s packaging waste
    using multimodal deep learning*, (2021), article: [https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste](https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste),
    paper: [https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning](https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning).'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Haibo He, Yang Bai, E. A. Garcia, and Shutao Li, *ADASYN: Adaptive synthetic
    sampling approach for imbalanced learning*, in 2008 IEEE International Joint Conference
    on Neural Networks (IEEE World Congress on Computational Intelligence), Hong Kong,
    China: IEEE, Jun. 2008, pp. 1322â€“1328\. doi: 10.1109/IJCNN.2008.4633969.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Y. Elor and H. Averbuch-Elor, *To SMOTE, or not to SMOTE?*, arXiv, May 11,
    2022\. Accessed: Feb. 19, 2023\. [Online]. Available at [http://arxiv.org/abs/2201.08528](http://arxiv.org/abs/2201.08528).'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
