- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Oversampling Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过采样方法
- en: In machine learning, we often don’t have enough samples of the minority class.
    One solution might be to gather more samples of such a class. For example, in
    the problem of detecting whether a patient has cancer or not, if we don’t have
    enough samples of the cancer class, we can wait for some time to gather more samples.
    However, such a strategy is not always feasible or sensible and can be time-consuming.
    In such cases, we can augment our data by using various techniques. One such technique
    is oversampling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们往往没有足够的少数类样本。一个可能的解决方案可能是收集此类更多的样本。例如，在检测患者是否患有癌症的问题中，如果我们没有足够的癌症类样本，我们可以等待一段时间来收集更多样本。然而，这种策略并不总是可行或明智，并且可能耗时。在这种情况下，我们可以通过使用各种技术来增强我们的数据。其中一种技术就是过采样。
- en: In this chapter, we will introduce the concept of oversampling, discuss when
    to use it, and the various techniques to perform it. We will also demonstrate
    how to utilize these techniques through the `imbalanced-learn` library APIs and
    compare their performance using some classical machine learning models. Finally,
    we will conclude with some practical advice on which techniques tend to work best
    under specific real-world conditions.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍过采样的概念，讨论何时使用它，以及执行它的各种技术。我们还将通过`imbalanced-learn`库的API演示如何利用这些技术，并使用一些经典的机器学习模型比较它们的性能。最后，我们将总结一些实际建议，说明哪些技术在特定现实世界条件下效果最佳。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Random oversampling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机过采样
- en: SMOTE
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTE
- en: SMOTE variants
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTE变体
- en: ADASYN
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ADASYN
- en: Model performance comparison of various oversampling methods
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种过采样方法的模型性能比较
- en: Guidance for using various oversampling techniques
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种过采样技术的指南
- en: Oversampling in multi-class classification
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类分类中的过采样
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will utilize common libraries such as `numpy`, `scikit-learn`,
    and `imbalanced-learn`. The code and notebooks for this chapter are available
    on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02).
    You can just fire up the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapter’s notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用如`numpy`、`scikit-learn`和`imbalanced-learn`等常用库。本章的代码和笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02)。您只需点击本章笔记本顶部的**在Colab中打开**图标，或通过[https://colab.research.google.com](https://colab.research.google.com)使用笔记本的GitHub
    URL启动，即可启动GitHub笔记本。
- en: What is oversampling?
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是过采样？
- en: '**Sampling** involves selecting a subset of observations from a larger set
    of observations. In this chapter, we’ll initially focus on binary classification
    problems with two classes: the positive class and the negative class. The minority
    class has significantly fewer instances than the majority class. Later in this
    chapter, we will explore multi-class classification problems. Toward the end of
    this chapter, we will look into oversampling for multi-class classification problems.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**采样**涉及从更大的观察集中选择观察子集。在本章中，我们最初将关注具有两个类别的二分类问题：正类和负类。少数类的实例数量显著少于多数类。在本章的后面部分，我们将探讨多类分类问题。在本章的结尾，我们将探讨多类分类问题的过采样。'
- en: '**Oversampling** is a data balancing technique that generates more samples
    of the minority class. However, this can be easily scaled to work for any number
    of classes where there are multiple classes with an imbalance. *Figure 2**.1*
    shows how samples of minority and majority classes are imbalanced (**a**) initially
    and balanced (**b**) after applying an oversampling technique:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**过采样**是一种数据平衡技术，它为少数类生成更多的样本。然而，这可以很容易地扩展到适用于任何有多个类别且存在不平衡的类别。*图2.1*显示了在应用过采样技术之前，少数类和多数类的样本是不平衡的（**a**）以及之后平衡的情况（**b**）：'
- en: '![](img/B17259_02_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_01.jpg)'
- en: Figure 2.1 – An increase in the number of minority class samples after oversampling
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 过采样后少数类样本数量的增加
- en: '*Why is oversampling needed*, you ask? It is required so that we give the model
    enough samples of the minority class to learn from it. If we offer too few instances
    of the minority class, the model may choose to ignore these minority class examples
    and focus solely on the majority class examples. This, in turn, would lead to
    the model not being able to learn the decision boundary well.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问：“为什么需要过采样？”这是必要的，以便我们给模型足够的少数类样本来从中学习。如果我们提供的少数类实例太少，模型可能会选择忽略这些少数类示例，而只关注多数类示例。这反过来又会导致模型无法很好地学习决策边界。
- en: 'Let’s generate a two-class imbalanced dataset with a 1:99 ratio using the `sklearn`
    library’s `make_classification` API, which creates a normally distributed set
    of points for each class. This will generate an imbalanced dataset of two classes:
    one being the minority class with label 1 and the other being the majority class
    with label 0\. We will apply various oversampling techniques throughout this chapter
    to balance this dataset:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`sklearn`库的`make_classification` API生成一个1:99比例的两类不平衡数据集，该API为每个类别创建一个正态分布的点集。这将生成两个类的不平衡数据集：一个是有标签1的少数类，另一个是有标签0的多数类。在本章中，我们将应用各种过采样技术来平衡这个数据集：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code generates 100 examples of class 1 and 9,900 examples of class 0 with
    an imbalance ratio of 1:99\. By plotting the dataset, we can see how the examples
    are distributed:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码生成了100个类别1的例子和9,900个类别0的例子，不平衡比率为1:99。通过绘制数据集，我们可以看到例子是如何分布的：
- en: '![](img/B17259_02_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_02.jpg)'
- en: Figure 2.2 – The dataset with an imbalance ratio of 1:99
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 不平衡比率为1:99的数据集
- en: In this section, we understood the need for oversampling. We also generated
    a synthetic imbalanced binary classification dataset to demonstrate the application
    of various oversampling techniques.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们了解了过采样的必要性。我们还生成了一个合成的非平衡二分类数据集，以展示各种过采样技术的应用。
- en: Random oversampling
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机过采样
- en: The simplest strategy to balance the imbalance in a dataset is to randomly choose
    samples of the minority class and repeat or duplicate them. This is also called
    **random oversampling** **with replacement**.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡数据集中不平衡的最简单策略是随机选择少数类的样本并重复或复制它们。这也被称为**随机过采样****带替换**。
- en: To increase the number of minority class observations, we can replicate the
    minority class data observations enough times to balance the two classes. Does
    this sound too trivial? Yes, but it works. By increasing the number of minority
    class samples, random oversampling reduces the bias toward the majority class.
    This helps the model learn the patterns and characteristics of the minority class
    more effectively.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加少数类观察的数量，我们可以复制少数类数据观察结果足够多次以平衡两个类。这听起来太简单了吗？是的，但它是有效的。通过增加少数类样本的数量，随机过采样减少了向多数类的偏差。这有助于模型更有效地学习少数类的模式和特征。
- en: We will use random oversampling from the `imbalanced-learn` library. The `fit_resample`
    API from the `RandomOverSampler` class resamples the original dataset and balances
    it. The `sampling_strategy` parameter is used to specify the new ratio of various
    classes. For example, we could say `sampling_strategy=1.0` to have an equal number
    of the two classes.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自`imbalanced-learn`库的随机过采样。`RandomOverSampler`类的`fit_resample` API重新采样原始数据集并使其平衡。`sampling_strategy`参数用于指定各种类的新比率。例如，我们可以将`sampling_strategy=1.0`指定为两个类具有相同数量的例子。
- en: 'There are various ways to specify `sampling_strategy`, such as a float value,
    string value, or `dict` – for example, {0: 50, 1: 50}:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '有多种方式可以指定`sampling_strategy`，例如浮点值、字符串值或`dict` – 例如，{0: 50, 1: 50}：'
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is the output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: So, we went from a ratio of 1:99 to 1:1, which is what we expected with `sampling_strategy=1.0`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从1:99的比例变为了1:1，这正是我们期望的`sampling_strategy=1.0`的结果。
- en: 'Let’s plot the oversampled dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制过采样后的数据集：
- en: '![](img/B17259_02_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_03.jpg)'
- en: Figure 2.3 – Dataset oversampled using RandomOverSampler (label 1 examples appear
    unchanged due to overlap)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – 使用RandomOverSampler（RandomOverSampler）过采样后的数据集（标签1的示例由于重叠而未改变）
- en: After applying random oversampling, the examples with label 1 overlap each other,
    creating the impression that nothing has changed. Repeating the same data point
    over and over can cause the model to memorize the specific data points and not
    be able to generalize to new, unseen examples. The `shrinkage` parameter in `RandomOverSampler`
    lets us perturb or shift each point by a small amount.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 应用随机过采样后，标签为1的示例会相互重叠，给人一种没有变化的感觉。反复重复相同的数据点可能导致模型记住特定的数据点，而无法推广到新的、未见过的示例。`RandomOverSampler`中的`shrinkage`参数让我们可以通过一个小量扰动或移动每个点。
- en: The value of the `shrinkage` parameter has to be greater than or equal to 0
    and can be `float` or `dict`. If a `float` data type is used, the same shrinkage
    factor will be used for all classes. If a `dict` data type is used, the shrinkage
    factor will be specific for each class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`shrinkage`参数的值必须大于或等于0，可以是`float`或`dict`类型。如果使用`float`数据类型，相同的收缩因子将用于所有类别。如果使用`dict`数据类型，收缩因子将针对每个类别具体指定。'
- en: 'In *Figure 2**.4*, we can observe the impact of random oversampling with `shrinkage=0.2`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图2**.4中，我们可以观察到`shrinkage=0.2`的随机过采样的影响：
- en: '![](img/B17259_02_04.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_02_04.jpg)'
- en: Figure 2.4 – Result of applying random oversampling with shrinkage=0.2
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – 应用随机过采样（shrinkage=0.2）的结果
- en: Toward the end of this chapter, we will compare the performance of random oversampling
    with various other oversampling techniques across multiple models and datasets.
    This will provide insights into their effectiveness in real-world applications.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的结尾，我们将比较随机过采样与其他多种过采样技术在多个模型和数据集上的性能。这将为我们提供关于它们在实际应用中的有效性的见解。
- en: 🚀 Random oversampling in production at Grab
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Grab在生产中使用随机过采样
- en: Grab, a ride-hailing and food delivery service in Southeast Asia, developed
    an image collection platform [1] for storing and retrieving imagery and map data.
    A key feature of this platform was its ability to automatically detect and blur
    **Personally Identifiable Information** (**PII**), such as faces and license plates,
    in street-level images. This was essential for maintaining user privacy. The dataset
    that was used for this purpose had a significant imbalance, with far more negative
    samples (images without PII) than positive ones (images with PII). Manual annotation
    was not feasible, so they turned to machine learning to solve this problem.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Grab是一家东南亚的打车和食品配送服务公司，开发了一个用于存储和检索图像和地图数据的图像收集平台[1]。该平台的一个关键特性是能够自动检测和模糊街景图像中的**个人身份信息**（**PII**），如人脸和车牌。这对于维护用户隐私至关重要。用于此目的的数据集存在显著的不平衡，负样本（没有PII的图像）远多于正样本（有PII的图像）。手动标注不可行，所以他们转向机器学习来解决此问题。
- en: To address the data imbalance, Grab employed the random oversampling technique
    to increase the number of positive samples, thereby enhancing the performance
    of their machine learning model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决数据不平衡，Grab采用了随机过采样技术来增加正样本的数量，从而提高了他们的机器学习模型的性能。
- en: Problems with random oversampling
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机过采样的问题
- en: Random oversampling can often lead to overfitting of the model since the generated
    synthetic observations get repeated, and the model sees the same observations
    again and again. Shrinkage tries to handle that in some sense, but it may be challenging
    to come up with an apt value of shrinkage, and shrinkage doesn’t care if the generated
    synthetic samples overlap with the majority class samples, which can lead to other
    problems.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随机过采样往往会导致模型过拟合，因为生成的合成观察值会重复，模型会一次又一次地看到相同的观察值。收缩试图在某种程度上处理这个问题，但可能很难找到一个合适的收缩值，而且收缩并不关心生成的合成样本是否与多数类样本重叠，这可能导致其他问题。
- en: In the previous section, we learned about the most basic and practical technique
    for applying oversampling to balance a dataset and reduce bias toward the majority
    class. Many times, random oversampling itself might give us such a high boost
    to our model’s performance that we may not even need to apply more advanced techniques.
    In production settings, it would also be beneficial to keep things plain and simple
    until we are ready to introduce more complexity in the pipeline. As they say,
    “premature optimization is the root of all evil,” so we start with something simple,
    so long as it does improve our model’s performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了应用过采样平衡数据集和减少对多数类偏差的最基本和实用的技术。很多时候，随机过采样本身可能会极大地提高我们模型的表现，以至于我们可能甚至不需要应用更高级的技术。在生产环境中，在准备引入更多复杂性之前，保持事情简单明了也是有利的。正如他们所说，“过早优化是万恶之源”，所以我们从简单的事情开始，只要它能提高我们模型的表现。
- en: In the subsequent sections, we will explore some alternative techniques, such
    as SMOTE and ADASYN, which adopt a different approach to oversampling and alleviate
    some of the problems associated with the random oversampling technique.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的章节中，我们将探讨一些替代技术，例如SMOTE和ADASYN，它们采用不同的过采样方法，并缓解了与随机过采样技术相关的一些问题。
- en: SMOTE
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SMOTE
- en: The main problem with random oversampling is that it duplicates the observations
    from the minority class. This can often cause overfitting. **Synthetic Minority
    Oversampling Technique** (**SMOTE**) [2] solves this problem of duplication by
    using a technique called **interpolation**.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 随机过采样的主要问题是它会重复少数类的观察结果。这通常会导致过拟合。**合成少数过采样技术**（**SMOTE**）[2]通过使用称为**插值**的技术来解决这种重复问题。
- en: Interpolation involves creating new data points in the range of known data points.
    Think of interpolation as being similar to the process of reproduction in biology.
    In reproduction, two individuals come together to produce a new individual with
    traits of both of them. Similarly, in interpolation, we pick two observations
    from the dataset and create a new observation by choosing a random point on the
    line joining the two selected points.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 插值涉及在已知数据点的范围内创建新的数据点。将插值想象成类似于生物学中的繁殖过程。在繁殖中，两个个体结合在一起产生一个具有两者特征的新个体。同样，在插值中，我们从数据集中选择两个观察结果，并通过选择两个选定点之间线上的随机点来创建一个新的观察结果。
- en: 'We oversample the minority class by interpolating synthetic examples. That
    prevents the duplication of minority samples while generating new synthetic observations
    similar to the known points. *Figure 2**.5* depicts how SMOTE works:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过插值合成示例来过采样少数类。这防止了少数样本的重复，同时生成与已知点相似的新的合成观察结果。*图2.5*展示了SMOTE是如何工作的：
- en: '![](img/B17259_02_05.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_05.jpg)'
- en: Figure 2.5 – Working of SMOTE
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – SMOTE的工作原理
- en: 'Here, we can see the following:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到以下内容：
- en: The majority and minority class samples are plotted (left)
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数类和少数类样本被绘制出来（左侧）
- en: The synthetic samples are generated by taking a random point on the line joining
    a minority sample to two nearest neighbor majority class samples (right)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在连接少数样本和两个最近邻多数类样本的线上的随机点生成合成样本（右侧）
- en: SMOTE was originally designed for continuous inputs. To keep the explanations
    simple, we’ll start with continuous inputs and discuss other kinds of inputs later.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE最初是为连续输入设计的。为了保持解释的简单性，我们将从连续输入开始，稍后再讨论其他类型的输入。
- en: First, we will examine the functioning of SMOTE and explore any potential disadvantages
    associated with this technique.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将检查SMOTE的功能，并探讨与此技术相关的任何潜在缺点。
- en: How SMOTE works
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SMOTE是如何工作的
- en: 'The SMOTE algorithm works as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE算法的工作原理如下：
- en: It considers only the samples from the minority class.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它只考虑少数类的样本。
- en: It trains KNN on the minority samples. A typical value of `k` is 5.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在少数样本上训练KNN。`k`的典型值是5。
- en: For each minority sample, a line is drawn between the point and each of its
    KNN examples.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个少数样本，从该点到其KNN示例之间画一条线。
- en: For each such line segment, a point on the segment is randomly picked to create
    a new synthetic example.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这样的线段，随机选择线段上的一个点来创建一个新的合成示例。
- en: 'Let’s use SMOTE using APIs from the `imbalanced-learn` library:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`imbalanced-learn`库的API来应用SMOTE：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The oversampled dataset looks like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 过采样后的数据集看起来像这样：
- en: '![](img/B17259_02_06.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_06.jpg)'
- en: Figure 2.6 – Oversampling using SMOTE
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – 使用SMOTE进行过采样
- en: 🚀 Oversampling techniques in production at Microsoft
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 微软生产中的过采样技术
- en: In a real-world application at Microsoft [3], machine learning was employed
    to forecast **Live Site Incidents** (**LSIs**) for early detection and escalation
    of incidents for engineering teams. Every day, a high volume of incidents was
    being generated, most of which started as low-severity issues. Due to limited
    resources, it was impractical for engineering teams to investigate all incidents,
    leading to potential delays in mitigating critical issues until they had a significant
    customer impact.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在微软的一个实际应用[3]中，机器学习被用于预测**现场事件**（**LSIs**），以实现事件的早期检测和升级，这对于工程团队来说至关重要。每天都会产生大量的事件，其中大部分最初是低严重性问题。由于资源有限，工程团队调查所有事件是不切实际的，这可能导致在事件对客户产生重大影响之前，缓解关键问题的潜在延迟。
- en: 'To address this, Microsoft employed machine learning to forecast which LSIs
    could escalate into severe problems, aiming for proactive identification and early
    resolution. The challenge was the data imbalance in the training set: out of approximately
    40,000 incidents, fewer than 2% escalated to high severity. Microsoft used two
    different oversampling techniques— bagged classification (covered in [*Chapter
    4*](B17259_04.xhtml#_idTextAnchor120), *Ensemble Methods*), and SMOTE, which were
    the most effective in improving the model’s performance. They used a two-step
    pipeline for balancing classes: first, oversampling with **SMOTE** and then undersampling
    with **RandomUnderSampler** (covered in [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079),
    *Undersampling Methods*). The pipeline automatically selected the optimal sampling
    ratios for both steps, and SMOTE performed better when combined with undersampling.
    The resulting end-to-end automated model was designed to be generic, making it
    applicable across different teams within or outside Microsoft, provided historical
    incidents were available for learning. The LSI insight tool used this model, which
    was adopted by various engineering teams.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，微软采用了机器学习来预测哪些LSIs可能会升级为严重问题，目标是进行主动识别和早期解决。挑战在于训练集中的数据不平衡：在约40,000个事件中，不到2%升级为高严重性。微软使用了两种不同的过采样技术——袋分类（在第[*第4章*](B17259_04.xhtml#_idTextAnchor120)，*集成方法*）和SMOTE，这些技术在提高模型性能方面最为有效。他们使用了两步流程来平衡类别：首先，使用**SMOTE**进行过采样，然后使用**RandomUnderSampler**（在第[*第3章*](B17259_03.xhtml#_idTextAnchor079)，*欠采样方法*）进行欠采样。该流程自动选择了两步的最优采样比率，并且当与欠采样结合时，SMOTE表现更佳。所得到的端到端自动化模型被设计为通用型，使其适用于微软内部或外部的不同团队，前提是可用历史事件进行学习。LSI洞察工具使用了这个模型，并被各个工程团队采用。
- en: Next, we will look at the limitations of using SMOTE.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨使用SMOTE的局限性。
- en: Problems with SMOTE
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SMOTE的问题
- en: 'SMOTE has its pitfalls – for example, it can add noise to an already noisy
    dataset. It can also lead to class overlap issues as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SMOTE有其陷阱——例如，它可能会向已经嘈杂的数据集中添加噪声。它也可能导致以下类重叠问题：
- en: 'SMOTE generates minority class samples without considering the majority class
    distribution, which may increase the overlap between the classes. In *Figure 2**.7*,
    we’re plotting the binary classification imbalanced dataset before and after applying
    SMOTE. We can see a lot of overlap between the two classes after applying SMOTE:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SMOTE在生成少数类样本时没有考虑多数类的分布，这可能会增加类别之间的重叠。在*图2*.7中，我们绘制了应用SMOTE前后的二分类不平衡数据集。我们可以看到应用SMOTE后两个类别之间有很多重叠：
- en: '![](img/B17259_02_07.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_02_07.jpg)'
- en: Figure 2.7 – Binary classification dataset before (left) and after (right) applying
    SMOTE (see the overlap between two classes on the right)
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 – 应用SMOTE前（左）和后（右）的二分类数据集（右图中两个类别的重叠部分）
- en: The other case may be that you have a huge amount of data, and running SMOTE
    may increase the runtime of your pipeline.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种情况可能是你拥有大量数据，运行SMOTE可能会增加你管道的运行时间。
- en: Problem 1 can be solved by using the SMOTE variant Borderline-SMOTE (discussed
    in the next section).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 问题1可以通过使用SMOTE变体Borderline-SMOTE（将在下一节中讨论）来解决。
- en: In this section, we learned about SMOTE, which uses the nearest neighbor technique
    to generate synthetic samples of the minority class. Sometimes, SMOTE may perform
    better than random oversampling since it exploits the proximity to other minority
    class samples to generate new samples.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了SMOTE，它使用最近邻技术来生成少数类的合成样本。有时，SMOTE可能比随机过采样表现得更好，因为它利用了与其他少数类样本的邻近性来生成新的样本。
- en: SMOTE variants
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SMOTE变体
- en: Now, let’s look at some of the SMOTE variants, such as Borderline-SMOTE, SMOTE-NC,
    and SMOTEN. These variants apply the SMOTE algorithm to samples of a certain kind
    and may not always be applicable.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一些SMOTE的变体，例如Borderline-SMOTE、SMOTE-NC和SMOTEN。这些变体将SMOTE算法应用于特定类型的样本，并且不一定总是适用。
- en: Borderline-SMOTE
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Borderline-SMOTE
- en: Borderline-SMOTE [4] is a variation of SMOTE that generates synthetic samples
    from the minority class samples that are near the classification boundary, which
    divides the majority class from the minority class.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Borderline-SMOTE [4]是SMOTE的一种变体，它从靠近分类边界的少数类样本中生成合成样本，该边界将多数类与少数类分开。
- en: Why consider samples on the classification boundary?
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么要考虑分类边界的样本？
- en: The idea is that the examples near the classification boundary are more prone
    to misclassification than those far away from the decision boundary. Producing
    more such minority samples along the boundary would help the model learn better
    about the minority class. Intuitively, it is also true that the points away from
    the classification boundary likely won’t make the model a better classifier.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 理念是靠近分类边界的例子比远离决策边界的例子更容易误分类。在边界附近产生更多的这种少数样本将有助于模型更好地学习少数类。直观上，远离分类边界的点可能不会使模型成为一个更好的分类器。
- en: 'Here’s a step-by-step algorithm for Borderline-SMOTE:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Borderline-SMOTE的逐步算法：
- en: We run a KNN algorithm over the whole dataset.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在整个数据集上运行KNN算法。
- en: 'Then, we divide the minority class points into three categories:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将少数类点分为三类：
- en: '*Noise* points are minority class examples that have all the neighbors from
    the majority class. These points are buried among majority-class neighbors. They
    are likely outliers and can safely be ignored as “noise.”'
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声**点是所有邻居都是多数类的少数类例子。这些点被埋在多数类邻居中。它们可能是异常值，可以安全地被忽略作为“噪声”。'
- en: '*Safe* points have more minority-class neighbors than majority-class neighbors.
    Such observations don’t contain much information and can be safely ignored.'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全**点比多数类邻居有更多的少数类邻居。这样的观察结果包含的信息不多，可以安全地忽略。'
- en: '*Danger* points have more majority-class neighbors than minority-class neighbors.
    This implies that such observations are on or close to the boundary between the
    two classes.'
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**危险**点比少数类邻居有更多的多数类邻居。这表明这样的观察结果位于或接近两类之间的边界。'
- en: Then, we train a KNN model only on the minority class examples.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们仅在少数类例子上训练KNN模型。
- en: Finally, we apply the SMOTE algorithm to the `Danger` points. Note that the
    neighbors of these `Danger` points may or may not be marked as `Danger`.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将SMOTE算法应用于`危险`点。请注意，这些`危险`点的邻居可能被标记为`危险`，也可能不是。
- en: 'As shown in *Figure 2**.8*, Borderline-SMOTE focuses on the danger class points
    for synthetic data generation:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如**图2.8**所示，Borderline-SMOTE在生成合成数据时专注于危险类点：
- en: '![](img/B17259_02_08.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_08.jpg)'
- en: Figure 2.8 – The Borderline-SMOTE algorithm uses only danger points to generate
    synthetic samples. Danger points have more majority-class neighbors than minority-class
    ones
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – Borderline-SMOTE算法仅使用危险点来生成合成样本。危险点比少数类邻居有更多的多数类邻居
- en: '*Figure 2**.9* shows how Borderline-SMOTE focuses on the minority class samples
    that are near the classification boundary, which separates the majority and minority
    classes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '**图2.9**展示了Borderline-SMOTE如何专注于靠近分类边界的少数类样本，该边界将多数类和少数类分开：'
- en: '![](img/B17259_02_09.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_09.jpg)'
- en: Figure 2.9 – Illustrating Borderline-SMOTE
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 – Borderline-SMOTE的说明
- en: 'Here, we can see the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到以下情况：
- en: a) Plots of majority and minority class samples
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: a) 多数类和少数类样本的图
- en: b) Synthetic samples generated using neighbors near the classification boundary
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: b) 使用靠近分类边界的邻居生成的合成样本
- en: 'Let’s see how we can use Borderline-SMOTE from the `imbalanced-learn` library
    to perform oversampling of the data:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用`imbalanced-learn`库中的Borderline-SMOTE来执行数据过采样：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Can you guess the problem with focusing solely on data points on the decision
    boundary of the two classes?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你能猜出只关注两类决策边界上的数据点的问题吗？
- en: 'Since this technique focuses so heavily on a very small number of points on
    the boundary, the points inside the minority class clusters are not sampled at
    all:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种技术如此侧重于边界上非常少数的点，少数类簇内的点根本就没有被采样：
- en: '![](img/B17259_02_10.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_10.jpg)'
- en: Figure 2.10 – The Borderline-SMOTE algorithm utilizing danger points, with more
    majority- than minority-class neighbors, to generate synthetic samples
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 – 利用危险点（多数类邻居多于少数类）的Borderline-SMOTE算法生成合成样本
- en: In this section, we learned about Borderline-SMOTE, which generates synthetic
    minority class samples by focusing on the samples that are close to the classification
    boundary of the majority and minority classes, which, in turn, may help in improving
    the discrimination power of the model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了边界SMOTE，它通过关注接近多数和少数类别分类边界的样本来生成合成少数类别样本，这反过来又可能有助于提高模型的判别能力。
- en: 🚀 Oversampling techniques in production at Amazon
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 亚马逊生产中的过采样技术
- en: In a real-world application, Amazon used machine learning to optimize packaging
    types for products, aiming to reduce waste while ensuring product safety [5].
    In their training dataset, which featured millions of product and package combinations,
    Amazon faced a significant class imbalance, with as few as 1% of the examples
    representing unsuitable product-package pairings (minority class).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，亚马逊使用机器学习优化产品的包装类型，旨在减少浪费同时确保产品安全[5]。在他们的训练数据集中，包含了数百万种产品和包装组合，亚马逊面临显著的类别不平衡，其中只有1%的示例代表不适合的产品-包装配对（少数类别）。
- en: 'To tackle this imbalance, Amazon used various oversampling techniques:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这种不平衡，亚马逊使用了各种过采样技术：
- en: '- Borderline-SMOTE oversampling, which resulted in a 4%-7% increase in PR-AUC
    but increased the training time by 25%-35%.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '- 边界SMOTE过采样，导致PR-AUC提高了4%-7%，但训练时间增加了25%-35%。'
- en: '- A hybrid of random oversampling and random undersampling, where they randomly
    oversampled the minority class and undersampled the majority class. It led to
    a 6%-10% improvement in PR-AUC and increased the training time by up to 25%.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '- 随机过采样和随机欠采样的混合，其中它们随机过采样少数类，并欠采样多数类。这导致了PR-AUC提高了6%-10%，但训练时间增加了高达25%。'
- en: The best-performing technique was two-phase learning with random undersampling
    (discussed in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep
    Learning Methods)*, which improved PR-AUC by 18%-24% with no increase in training
    time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 表现最好的技术是两阶段学习与随机欠采样（在第[*第7章*](B17259_07.xhtml#_idTextAnchor205)，*数据级深度学习方法)*中讨论），它将PR-AUC提高了18%-24%，而没有增加训练时间。
- en: They mentioned that the effectiveness of a technique in dealing with dataset
    imbalance is both domain- and dataset-specific. This real-world example underscores
    the effectiveness of oversampling techniques in tackling class imbalance issues.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提到，处理数据集不平衡的技术效果既与领域相关，也与数据集特定。这个现实世界的例子强调了过采样技术在解决类别不平衡问题上的有效性。
- en: Next, we will learn about another oversampling technique, called ADASYN, that
    oversamples examples near boundaries and in other low-density regions without
    completely ignoring data points that do not lie on the boundary.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习另一种过采样技术，称为ADASYN，它通过对边界附近和其他低密度区域的示例进行过采样，而不会完全忽略不在边界上的数据点。
- en: ADASYN
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ADASYN
- en: 'While SMOTE doesn’t distinguish between the density distribution of minority
    class samples, **Adaptive Synthetic Sampling** (**ADASYN**) [6] focuses on harder-to-classify
    minority class samples since they are in a low-density area. ADASYN uses a weighted
    distribution of the minority class based on the difficulty of classifying the
    observations. This way, more synthetic data is generated from harder samples:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SMOTE 不区分少数类别样本的密度分布，**自适应合成采样**（**ADASYN**）[6]则专注于难以分类的少数类别样本，因为它们位于低密度区域。ADASYN根据分类观察的难度，使用少数类别的加权分布。这样，从更难样本中生成更多的合成数据：
- en: '![](img/B17259_02_11.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_11.jpg)'
- en: Figure 2.11 – Illustration of how ADASYN works
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 – ADASYN工作原理的说明
- en: 'Here, we can see the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到以下内容：
- en: a) The majority and minority class samples are plotted
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: a) 绘制了多数类和少数类样本
- en: b) Synthetic samples are generated depending on the hardness factor (explained
    later)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: b) 根据硬度因子（稍后解释）生成合成样本
- en: While SMOTE uses all samples from the minority class for oversampling uniformly,
    in ADASYN, the observations that are harder to classify are used more often.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SMOTE 使用少数类中的所有样本进行均匀过采样，但在 ADASYN 中，更难分类的观察结果被更频繁地使用。
- en: Another difference between the two techniques is that, unlike SMOTE, ADASYN
    also uses the majority class observations while training KNN. It then decides
    the hardness of samples based on how many majority observations are its neighbors.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种技术之间的另一个区别是，与SMOTE不同，ADASYN在训练KNN时也使用多数类观测值。然后，它根据有多少多数观测值是其邻居来决定样本的硬度。
- en: Working of ADASYN
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ADASYN的工作原理
- en: 'ADASYN follows a simple algorithm. Here is the step-by-step working of ADASYN:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ADASYN遵循一个简单的算法。以下是ADASYN的逐步工作原理：
- en: First, it trains a KNN on the entire dataset.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它在整个数据集上训练一个KNN。
- en: For each observation of the minority class, we find the hardness factor. This
    factor tells us how difficult it is to classify that data point. The hardness
    factor, denoted by r, is the ratio of the number of majority class neighbors with
    the total number of neighbors. Here, r = M _ K , where M is the count of majority
    class neighbors and K is the total number of nearest neighbors.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于少数类的每个观测值，我们找到硬度因子。这个因子告诉我们分类该数据点有多困难。硬度因子，用r表示，是多数类邻居数与邻居总数的比率。在这里，r = M
    / K，其中M是多数类邻居的数量，K是最近邻的总数。
- en: For each minority observation, we generate synthetic samples proportional to
    the hardness factor by drawing a line between the minority observation and its
    neighbors (neighbors could be from the majority class or minority class). The
    harder it is to classify a data point, the more synthetic samples will be created
    for it.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个少数类观测值，我们通过在少数类观测值及其邻居（邻居可以是多数类或少数类）之间画线来生成与硬度因子成比例的合成样本。数据点分类越困难，为其创建的合成样本就越多。
- en: 'Let’s see how we can use the ADASYN API from the `imbalanced-learn` library
    to perform oversampling of the data:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用来自`imbalanced-learn`库的ADASYN API进行数据的过采样：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the output:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](img/B17259_02_12.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_12.jpg)'
- en: Figure 2.12 – ADASYN prioritizes harder samples and incorporates majority class
    examples in KNN to assess sample hardness
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 – ADASYN优先考虑较难分类的样本，并在KNN中包含多数类示例以评估样本硬度
- en: '![](img/B17259_02_13.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_13.jpg)'
- en: Figure 2.13 – A memory aid summarizing various oversampling techniques
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 – 总结各种过采样技术的记忆辅助图
- en: In this section, we learned about ADASYN. Next, let’s see how we can deal with
    cases when our data contains categorical features.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了关于ADASYN的内容。接下来，让我们看看当我们的数据包含分类特征时，我们该如何处理这些情况。
- en: Categorical features and SMOTE variants (SMOTE-NC and SMOTEN)
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类特征和SMOTE变体（SMOTE-NC和SMOTEN）
- en: 'What if your data contains categorical features? A categorical feature can
    take one of a limited or fixed number of possible values, and it’s a parallel
    to enumerations (enums) in computer science. These could be nominal categorical
    features that lack a natural order (for example, hair color, ethnicity, and so
    on) or ordinal categorical features that have an inherent order (for example,
    low, medium, and high):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据包含分类特征呢？分类特征可以取有限或固定数量的可能值，这与计算机科学中的枚举（enums）类似。这些可能是无序的分类特征，例如头发颜色、种族等，或者是有序的分类特征，例如低、中、高：
- en: '![](img/B17259_02_14.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_02_14.jpg)'
- en: Figure 2.14 – Categorical data and its types with examples
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 – 带有示例的分类数据和其类型
- en: For ordinal features, we can just encode them via sklearn’s `OrdinalEncoder`,
    which assigns the categories to the values 0, 1, 2, and so on.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于有序特征，我们可以通过sklearn的`OrdinalEncoder`对其进行编码，它将类别分配给值0、1、2等。
- en: 'For nominal features, none of the SMOTE variants we have learned so far will
    work. However, `RandomOverSampler` can handle nominal features too:'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于名义特征，我们之前学到的所有SMOTE变体都不会起作用。然而，`RandomOverSampler`也可以处理名义特征：
- en: '[PRE9]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is the output:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE10]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: However, SMOTE, by default, works only on continuous data and cannot be directly
    used on categorical data. *Why?* That’s because SMOTE works by generating a random
    point on the line joining two different data points of the minority class (also
    called interpolation). If our data is categorical and has values of “yes” and
    “no,” we would first need to transform such values into numbers. Even when we
    do so, say “yes” is mapped to 1 and “no” is mapped to 0, the interpolation via
    SMOTE may end up producing a new point of 0.3, which does not map to any real
    category.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，默认情况下，SMOTE仅在连续数据上工作，不能直接用于分类数据。*为什么？* 这是因为SMOTE通过生成连接少数类两个不同数据点的线上的随机点（也称为插值）来工作。如果我们的数据是分类的，并且有“是”和“否”这样的值，我们首先需要将这些值转换为数字。即使我们这样做，比如“是”映射到1，“否”映射到0，SMOTE的插值可能会产生一个0.3的新点，这并不映射到任何真实类别。
- en: Also, we cannot use the `shrinkage` parameter in `RandomOverSampler` with categorical
    data because this parameter is designed only for continuous values.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于`RandomOverSampler`中的`shrinkage`参数仅设计用于连续值，因此我们无法在分类数据中使用该参数。
- en: 'However, two variants of SMOTE can deal with categorical features:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，SMOTE的两个变体可以处理分类特征：
- en: '`imbalanced-learn` to oversample our dataset. The first item in the dataset
    is categorical, and the second item is continuous:'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`imbalanced-learn`对数据进行过采样。数据集的第一项是分类的，第二项是连续的：
- en: '[PRE11]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the output:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE12]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Synthetic Minority Oversampling Technique for Nominal** (**SMOTEN**) is used
    for nominal categorical data. SMOTEN performs the majority vote similar to SMOTE-NC
    for all the features. It considers all features as nominal categorical, and the
    feature value of new samples is decided by taking the most frequent category of
    the nearest neighbors. The distance metric that’s used for calculating the nearest
    neighbors is called the **Value Distance Metric** (**VDM**). VDM computes the
    distance between two attribute values by considering the distribution of class
    labels associated with each value. It is based on the idea that two attribute
    values are more similar if they have similar distributions of class labels. This
    way, VDM can capture the underlying relationships between categorical attributes
    and their corresponding class labels.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用于名义数据的合成少数过采样技术**（**SMOTEN**）用于名义分类数据。SMOTEN对所有特征执行与SMOTE-NC类似的多数投票。它将所有特征视为名义分类，新样本的特征值通过选择最近邻中最频繁的类别来决定。用于计算最近邻的距离度量称为**值距离度量**（**VDM**）。VDM通过考虑与每个值关联的类别标签分布来计算两个属性值之间的距离。基于这样的想法，如果两个属性值的类别标签分布相似，则这两个属性值更相似。这样，VDM可以捕捉分类属性及其对应类别标签之间的潜在关系。'
- en: 'Let’s look at some example code that uses SMOTEN:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看使用SMOTEN的一些示例代码：
- en: '[PRE13]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the output:'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE14]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In *Table 2.1*, we can see SMOTE, SMOTEN, and SMOTENC, with a few examples
    for each technique to demonstrate the difference between them:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在*表2.1*中，我们可以看到SMOTE、SMOTEN和SMOTENC，以及每种技术的一些示例，以展示它们之间的差异：
- en: '| **Type** **of SMOTE** | **Features Supported** | **Example Data** |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| **SMOTE类型** | **支持的特性** | **示例数据** |'
- en: '| --- | --- | --- |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SMOTE | Only numerical | features: [2.3, 4.5, 1.2], label: 0features: [3.4,
    2.2, 5.1], label: 1 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| SMOTE | 仅数值 | 特征：[2.3, 4.5, 1.2]，标签：0特征：[3.4, 2.2, 5.1]，标签：1 |'
- en: '| SMOTEN | Categorical(nominal or ordinal) | features: [‘green’, ‘square’],
    label: 0features: [‘red’, ‘circle’], label: 1 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| SMOTEN | 分类（名义或有序） | 特征：[‘green’, ‘square’]，标签：0特征：[‘red’, ‘circle’]，标签：1
    |'
- en: '| SMOTENC | Numerical or categorical(nominal or ordinal) | features: [2.3,
    ‘green’, ‘small’, ‘square’], label: 0features: [3.4, ‘red’, ‘large’, ‘circle’],
    label: 1 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| SMOTENC | 数值或分类（名义或有序） | 特征：[2.3, ‘green’, ‘small’, ‘square’]，标签：0特征：[3.4,
    ‘red’, ‘large’, ‘circle’]，标签：1 |'
- en: Table 2.1 – SMOTE and some of its common variants with example data
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 – SMOTE及其一些常见变体与示例数据
- en: In summary, we should use SMOTENC when we have a mix of categorical and continuous
    data types, while SMOTEN can only be used when all the columns are categorical.
    You might be curious about how the various oversampling methods compare with each
    other in terms of model performance. We’ll explore this topic in the next section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，当我们有分类和连续数据类型的混合时，应使用SMOTENC，而SMOTEN只能用于所有列都是分类的情况。你可能对各种过采样方法在模型性能方面的比较感到好奇。我们将在下一节探讨这个话题。
- en: Model performance comparison of various oversampling methods
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 各种过采样方法的模型性能比较
- en: 'Let’s examine how some popular models perform with the different oversampling
    techniques we’ve discussed. We’ll use two datasets for this comparison: one synthetic
    and one real-world dataset. We’ll evaluate the performance of four oversampling
    techniques, as well as no sampling, using logistic regression and random forest
    models.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一些流行的模型在不同过采样技术下的表现。我们将使用两个数据集进行比较：一个是合成数据集，另一个是真实世界数据集。我们将使用逻辑回归和随机森林模型评估四种过采样技术以及无采样的性能。
- en: 'You can find all the related code in this book’s GitHub repository. In *Figure
    2**.15* and *Figure 2**.16*, we can see the average precision score values for
    both models on the two datasets:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到所有相关代码。在*图2.15*和*图2.16*中，我们可以看到两个数据集上两种模型的平均精确度得分值：
- en: '![](img/B17259_02_15.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_02_15.jpg)'
- en: Figure 2.15 – Performance comparison of various oversampling techniques on a
    synthetic dataset
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15 – 在合成数据集上各种过采样技术的性能比较
- en: '![](img/B17259_02_16.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_02_16.jpg)'
- en: Figure 2.16 – Performance comparison of various oversampling techniques on the
    thyroid_sick dataset
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 – 在thyroid_sick数据集上各种过采样技术的性能比较
- en: 'Based on these plots, we can draw some useful conclusions:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些图表，我们可以得出一些有用的结论：
- en: '**Effectiveness of oversampling**: In general, using oversampling techniques
    seems to improve the average precision score compared to not using any sampling
    (NoSampling).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过采样的有效性**：总的来说，使用过采样技术似乎比不使用任何采样（NoSampling）提高了平均精确度得分。'
- en: '**Algorithm sensitivity**: The effectiveness of oversampling techniques varies
    depending on the machine learning algorithm used. For example, random forest seems
    to benefit more from oversampling techniques than logistic regression, especially
    on synthetic data.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法敏感性**：过采样技术的有效性取决于所使用的机器学习算法。例如，随机森林似乎比逻辑回归从过采样技术中受益更多，尤其是在合成数据上。'
- en: '`thyroid_sick` dataset but showed variations in the synthetic data.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thyroid_sick`数据集但在合成数据中显示了变化。'
- en: '`thyroid_sick` data'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thyroid_sick`数据'
- en: For random forest, Borderline-SMOTE had the highest average precision score
    on synthetic data
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于随机森林，Borderline-SMOTE在合成数据上具有最高的平均精确度得分。
- en: '`thyroid_sick` data.*   **No clear winner**: There is no single oversampling
    technique that outperforms all others across all conditions. The choice of technique
    may depend on the specific algorithm and dataset being used.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thyroid_sick`数据.*   **没有明显的胜者**：没有一种过采样技术在所有条件下都优于其他技术。技术的选择可能取决于所使用的特定算法和数据集。'
- en: Please note that the models used here are not tuned with the best hyperparameters.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里使用的模型没有使用最佳超参数进行调整。
- en: Tuning the hyperparameters of random forest and logistic regression models may
    improve the models' performance further.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 调整随机森林和逻辑回归模型的超参数可以进一步提高模型性能。
- en: In general, there is no single technique that will always do better than the
    rest. We have multiple variables at play here, namely the “model” and the “data.”
    Most of the time, the only way to know is to try out a bunch of these techniques
    and find the one that works the best for our model and data. You may find yourself
    curious about how to choose from the numerous oversampling options available.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，没有一种技术总是比其他技术表现得更好。这里有几个变量在起作用，即“模型”和“数据”。大多数时候，唯一知道的方法是尝试这些技术中的一系列，并找到最适合我们模型和数据的那个。你可能会对如何从众多过采样选项中进行选择感到好奇。
- en: Guidance for using various oversampling techniques
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用各种过采样技术的指南
- en: 'Now, let’s review some guidelines on how to navigate through the various oversampling
    techniques we went over and how these techniques differ from each other:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回顾一下如何导航我们讨论过的各种过采样技术以及这些技术如何彼此不同的一些指南：
- en: Train a model without applying any sampling techniques. This will be our model
    with baseline performance. Any oversampling technique we apply is expected to
    give a boost to this performance.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不应用任何采样技术来训练模型。这将是我们具有基线性能的模型。我们应用的任何过采样技术都预期会提高这种性能。
- en: Start with random oversampling and add some shrinkage too. We may have to play
    with some values of shrinkage to see if the model’s performance improves.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随机过采样开始，并添加一些收缩。我们可能需要调整一些收缩的值，看看模型性能是否有所改善。
- en: 'When we have categorical features, we have a couple of options:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们具有分类特征时，我们有几种选择：
- en: Convert all categorical features into numerical features first using one-hot
    encoding, label encoding, feature hashing, or other feature transformation techniques.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先将所有分类特征转换为数值特征，使用独热编码、标签编码、特征哈希或其他特征转换技术。
- en: (Only for nominal categorical features) Use SMOTENC and SMOTEN directly on the
    data.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: （仅适用于名义分类特征）直接在数据上使用SMOTENC和SMOTEN。
- en: Apply various oversampling techniques – random oversampling, SMOTE, Borderline-SMOTE,
    and ADASYN – and measure the model’s performance on metrics applicable to your
    problem, such as the average precision score, ROC-AUC, precision, recall, F1 score,
    and more.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用各种过采样技术——随机过采样、SMOTE、Borderline-SMOTE和ADASYN——并在适用于您问题的指标上衡量模型的性能，例如平均精度得分、ROC-AUC、精确率、召回率、F1分数等。
- en: Since oversampling alters the distribution of the training dataset, which is
    not the case for the test set or the real world, using oversampling can potentially
    generate biased predictions. After using oversampling, it can be essential to
    recalibrate our model’s probability scores depending on the application. Recalibration
    of the model corrects any bias introduced by altering the class distribution,
    ensuring more reliable decision-making when deployed. Similarly, adjusting the
    classification threshold is key for accurate model interpretation, especially
    with imbalanced datasets. For more details on recalibration and threshold adjustment,
    please see [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279), *Model Calibration*,
    and [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive* *Learning*,
    respectively.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于过采样改变了训练数据集的分布，而测试集或现实世界并非如此，使用过采样可能会产生潜在的偏差预测。使用过采样后，根据应用重新校准模型的概率得分可能至关重要。模型的重新校准纠正了由于改变类别分布而引入的任何偏差，确保在部署时做出更可靠的决策。同样，调整分类阈值对于准确解释模型至关重要，尤其是在不平衡数据集的情况下。有关重新校准和阈值调整的更多详细信息，请参阅[*第十章*](B17259_10.xhtml#_idTextAnchor279)《模型校准》和[*第五章*](B17259_05.xhtml#_idTextAnchor151)《成本敏感学习》。
- en: When to avoid oversampling
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免过采样的时机
- en: 'In [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to Data Imbalance
    in Machine Learning*, we discussed scenarios where data imbalance may not be a
    concern. Those considerations should be revisited before you opt for oversampling
    techniques. Despite criticisms, the applicability of oversampling should be evaluated
    on a case-by-case basis. Here are some additional technical considerations to
    keep in mind when choosing to apply oversampling techniques:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第一章*](B17259_01.xhtml#_idTextAnchor015)《机器学习中的数据不平衡介绍》中，我们讨论了数据不平衡可能不是问题的场景。在您选择过采样技术之前，应该重新审视这些考虑因素。尽管存在批评，但过采样的适用性应该根据具体情况评估。以下是一些在选择应用过采样技术时需要考虑的额外技术因素：
- en: '**Computational cost**: Oversampling increases the dataset’s size, leading
    to higher computational demands in terms of processing time and hardware resources.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算成本**：过采样增加了数据集的大小，导致处理时间和硬件资源方面的计算需求更高。'
- en: '**Data quality**: If the minority class data is noisy or has many outliers,
    oversampling can introduce more noise, reducing model reliability.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量**：如果少数类数据有噪声或许多异常值，过采样可能会引入更多噪声，降低模型可靠性。'
- en: '**Classifier limitations**: In scenarios with system constraints, such as extremely
    low latency, or when dealing with legacy systems, the use of strong classifiers
    (complex and more accurate models) may not be feasible. In these cases, we may
    be limited to using weak classifiers. Weak classifiers are simpler and less accurate
    but require fewer computational resources and have lower runtime latency. In such
    situations, oversampling can be beneficial [7]. For strong classifiers, oversampling
    may offer diminishing returns, and optimizing the decision threshold could sometimes
    serve as a simpler, less resource-intensive alternative.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器限制**：在系统约束场景中，例如极低延迟或处理遗留系统时，使用强大的分类器（复杂且更准确的模型）可能不可行。在这种情况下，我们可能只能使用弱分类器。弱分类器更简单、精度较低，但需要较少的计算资源，并且具有更低的运行时延迟。在这种情况下，过采样可能是有益的[7]。对于强大的分类器，过采样可能带来递减的回报，有时优化决策阈值可能是一个更简单、资源消耗更少的替代方案。'
- en: Consider these factors when deciding whether to use oversampling methods for
    imbalanced datasets.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定是否使用过采样方法来解决不平衡数据集时，考虑以下因素。
- en: '*Table 2.2* summarizes the key ideas, pros, and cons of various oversampling
    techniques. This can help you better evaluate which oversampling method to choose:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 2.2* 总结了各种过采样技术的关键思想、优点和缺点。这可以帮助你更好地评估选择哪种过采样方法：'
- en: '|  | **SMOTE** | **Borderline-SMOTE** | **ADASYN** | **SMOTE-NC** **and SMOTEN**
    |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | **SMOTE** | **Borderline-SMOTE** | **ADASYN** | **SMOTE-NC** 和 **SMOTEN**
    |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Key idea | Choose random points on the line joining the nearest neighbors
    of minority class examples. | Choose the minority samples on the boundary between
    the majority and minority classes. Perform SMOTE for such samples on the boundary.
    | Automatically decides the number of minority class samples to generate according
    to density distribution. More points are generated where the density distribution
    is low. | It performs a majority vote for the categorical features. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 核心思想 | 在连接少数类样本最近邻的线上选择随机点。 | 在多数类和少数类之间的边界上选择少数样本。对这样的样本在边界上执行 SMOTE。 |
    根据密度分布自动决定要生成的少数类样本数量。在密度分布低的地方生成更多点。 | 它对分类特征进行多数投票。 |'
- en: '| Pro | Usually reduces false negatives. | Creates synthetic samples that are
    not naïve copies of the known data. | It cares about the density distribution
    of different classes. | It works with categorical data. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 优点 | 通常减少假阴性。 | 创建的合成样本不是已知数据的简单复制。 | 它关注不同类别的密度分布。 | 它适用于分类数据。 |'
- en: '| Con | Overlapping classes may occur and can introduce more noise to data.
    This may not work well with high-dimensional data or multi-class classification
    problems. | It does not care about the distribution of minority class examples.
    | It focuses on areas where there is overlap between classes. It may focus too
    much on outliers, resulting in poor model performance. | The same as SMOTE. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 缺点 | 可能会发生重叠的类别，并可能向数据中引入更多噪声。这可能不适合高维数据或多类别分类问题。 | 它不关心少数类样本的分布。 | 它关注类别之间重叠的区域。它可能过分关注异常值，导致模型性能不佳。
    | 与 SMOTE 相同。 |'
- en: Table 2.2 – Summarizing the various oversampling techniques that were discussed
    in this chapter
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2 – 总结本章讨论的各种过采样技术
- en: In this section, we looked at some general guidelines to apply the various oversampling
    techniques we learned about in this chapter and the pros and cons of using them.
    Next, we will look at how to extend the various oversampling methods to multi-class
    classification problems.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了如何应用本章中学习到的各种过采样技术以及使用它们的优缺点。接下来，我们将探讨如何将各种过采样方法扩展到多类别分类问题。
- en: Oversampling in multi-class classification
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类别分类中的过采样
- en: 'In multi-class classification problems, we have more than two classes or labels
    to be predicted, and hence more than one class may be imbalanced. This adds some
    more complexity to the problem. However, we can apply the same techniques to multi-class
    classification problems as well. The `imbalanced-learn` library provides the option
    to deal with multi-class classification in almost all the supported methods. We
    can choose from various sampling strategies using the `sampling_strategy` parameter.
    For multi-class classification, we can pass some fixed string values (called built-in
    strategies) to the `sampling_strategy` parameter in the SMOTE API. We can also
    pass a dictionary with the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在多类别分类问题中，我们有超过两个类别或标签需要预测，因此可能存在多个类别不平衡。这给问题增加了更多复杂性。然而，我们也可以将相同的技巧应用于多类别分类问题。`imbalanced-learn`
    库提供了在几乎所有支持的方法中处理多类别分类的选项。我们可以通过使用 `sampling_strategy` 参数选择各种采样策略。对于多类别分类，我们可以在
    SMOTE API 中的 `sampling_strategy` 参数传递一些固定的字符串值（称为内置策略）。我们还可以传递一个包含以下内容的字典：
- en: Keys as the class labels
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以类别标签作为键
- en: Values as the number of samples of that class
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以该类别的样本数量作为值
- en: 'Here are the built-in strategies for `sampling_strategy` when using the parameter
    as a string:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用参数作为字符串时，以下是一些内置的 `sampling_strategy` 样本策略：
- en: The `minority` strategy resamples only the minority class.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minority` 策略仅重新采样少数类。'
- en: The `not minority` strategy resamples all classes except the minority class.
    This may be helpful in the case of multi-class imbalance, where we have more than
    two classes and multiple classes are imbalanced, but we don’t want to touch the
    minority class.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`not minority` 策略重新采样除了少数类以外的所有类别。在多类别不平衡的情况下，这可能是有帮助的，因为我们有超过两个类别，并且多个类别不平衡，但我们不想触及少数类。'
- en: The `not majority` strategy resamples all classes except the majority class.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`not majority`策略重新采样所有类别，除了多数类别。'
- en: The `all` strategy resamples all classes.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`all`策略重新采样所有类别。'
- en: The `auto` strategy is the same as the `not` `majority` strategy.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`auto`策略与`not``majority`策略相同。'
- en: The following code shows the usage of SMOTE for multi-class classification using
    various sampling strategies.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了使用各种采样策略进行多类分类时SMOTE的使用方法。
- en: 'First, let’s create a dataset containing 100 samples with three classes that
    have weights of 0.1, 0.4, and 0.5:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建一个包含100个样本的数据集，其中三个类别的权重分别为0.1、0.4和0.5：
- en: '[PRE15]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is the output:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE16]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As expected, our dataset contains the three classes in the ratio 10:40:50 for
    classes 0, 1, and 2, respectively.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，我们的数据集包含三个类别，分别为类别0、1和2，其比例为10:40:50。
- en: 'Now, let’s apply SMOTE with the “*minority*” sampling strategy. This will oversample
    the class with the least number of samples:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们应用带有“*少数*”采样策略的SMOTE。这将增加最少样本数的类别：
- en: '[PRE17]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the output:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE18]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since class 0 previously had the least number of samples, the “*minority*” sampling
    strategy only oversampled class 0, making the number of samples equal to the number
    of samples in the majority class.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 由于类别0之前样本数最少，因此“*少数*”采样策略只对类别0进行了过采样，使得样本数等于多数类别的样本数。
- en: 'In the following code, we’re using a dictionary for oversampling. Here, for
    each class label (0, 1, or 2) as `key` in the `sampling_strategy` dictionary,
    we have the number of desired samples for each targeted class as `value`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们使用字典进行过采样。在这里，对于`sampling_strategy`字典中的每个类别标签（0、1或2）作为`key`，我们都有每个目标类别的期望样本数作为`value`：
- en: '[PRE19]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is the output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE20]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Tip
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Please note that when using `dict` within `sampling_strategy`, the number of
    desired samples for each class should be greater than or equal to the original
    number of samples. Otherwise, the `fit_resample` API will throw an exception.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当在`sampling_strategy`中使用`dict`时，每个类别的期望样本数应大于或等于原始样本数。否则，`fit_resample`API将抛出异常。
- en: In this section, we saw how to extend oversampling strategies to handle cases
    when we have imbalanced datasets with more than two classes. Most of the time,
    the “auto” `sampling_strategy` would be good enough and would balance all the
    classes.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了如何将过采样策略扩展到处理具有两个以上类别的数据不平衡情况。大多数情况下，“auto”`采样策略`就足够好了，并且会平衡所有类别。
- en: Summary
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went through various oversampling techniques for dealing
    with imbalanced datasets and applied them using Python’s `imbalanced-learn` library
    (also called `imblearn`). We also saw the internal workings of some of the techniques
    by implementing them from scratch. While random oversampling generates new minority
    class samples by duplicating them, SMOTE-based techniques work by choosing random
    samples in the direction of nearest neighbors of the minority class samples. Though
    oversampling can potentially overfit the model on your data, it usually has more
    pros than cons, depending on the data and model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了处理不平衡数据集的各种过采样技术，并使用Python的`imbalanced-learn`库（也称为`imblearn`）进行了应用。我们还通过从头实现一些技术来了解这些技术的内部工作原理。虽然过采样可能会潜在地使模型对数据进行过拟合，但它通常比缺点多，具体取决于数据和模型。
- en: We applied them to some of the synthesized and publicly available datasets and
    benchmarked their performance and effectiveness. We saw how different oversampling
    techniques may lead to model performance on a varying scale, so it becomes crucial
    to try a few different oversampling techniques to decide on the one that’s most
    optimal for our data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将它们应用于一些合成和公开可用的数据集，并对其性能和有效性进行了基准测试。我们看到了不同的过采样技术如何可能导致模型性能在各个尺度上变化，因此尝试几种不同的过采样技术以决定最适合我们数据的方法变得至关重要。
- en: If you feel intrigued by the prospect of discovering oversampling approaches
    relevant to deep learning models, we invite you to check out [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*, where we’ll discuss data-level techniques
    within the realm of deep learning.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你被发现与深度学习模型相关的过采样方法所吸引，我们邀请你查看[*第7章*](B17259_07.xhtml#_idTextAnchor205)，*数据级深度学习方法*，我们将讨论深度学习领域内的数据级技术。
- en: In the next chapter, we will go over various undersampling techniques.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍各种欠采样技术。
- en: Exercises
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Explore the two variants of SMOTE, namely KMeans-SMOTE and SVM-SMOTE, from the
    `imbalanced-learn` library, not discussed in this chapter. Compare their performance
    with vanilla SMOTE, Borderline-SMOTE, and ADASYN using the logistic regression
    and random forest models.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索`imbalanced-learn`库中的SMOTE的两个变体，即KMeans-SMOTE和SVM-SMOTE，本章未讨论。使用逻辑回归和随机森林模型比较它们与vanilla
    SMOTE、Borderline-SMOTE和ADASYN的性能。
- en: For a classification problem with two classes, let’s say the minority class
    to majority class ratio is 1:20\. How should we balance this dataset? Should we
    apply the balancing technique at test or evaluation time? Please provide a reason
    for your answer.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一个有两个类别的分类问题，假设少数类与多数类的比例是1:20。我们应该如何平衡这个数据集？我们应该在测试或评估时间应用平衡技术吗？请提供你的答案的理由。
- en: Let’s say we are trying to build a model that can estimate whether a person
    can be granted a bank loan or not. Out of the 5,000 observations we have, only
    500 people got the loan approved. To balance the dataset, we duplicate the approved
    people data and then split it into train, test, and validation datasets. Are there
    any issues with using this approach?
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们正在尝试构建一个模型，用于估计一个人是否能获得银行贷款。在我们拥有的5,000个观测值中，只有500人的贷款获得了批准。为了平衡数据集，我们将批准的人的数据进行复制，然后将其分为训练集、测试集和验证集。使用这种方法是否存在任何问题？
- en: Data normalization helps in dealing with data imbalance. Is this true? Why or
    why not?
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据归一化有助于处理数据不平衡。这是真的吗？为什么或为什么不？
- en: 'Explore the various oversampling APIs available from the `imbalanced-learn`
    library here: [https://imbalanced-learn.org/stable/references/over_sampling.html](https://imbalanced-learn.org/stable/references/over_sampling.html).
    Pay attention to the various parameters of each of the APIs.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里探索`imbalanced-learn`库中可用的各种过采样API：[https://imbalanced-learn.org/stable/references/over_sampling.html](https://imbalanced-learn.org/stable/references/over_sampling.html)。请注意每个API的各种参数。
- en: References
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Protecting Personal Data in Grab’s Imagery* (2021), [https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery).'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*《Grab的图像中保护个人数据》*（2021年），[https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery)。'
- en: 'N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, *SMOTE: Synthetic
    Minority Over-sampling Technique*, jair, vol. 16, pp. 321–357, Jun. 2002, doi:
    10.1613/jair.953.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'N. V. Chawla，K. W. Bowyer，L. O. Hall，和 W. P. Kegelmeyer，*SMOTE：合成少数类过采样技术*，jair，第16卷，第321-357页，2002年6月，doi:
    10.1613/jair.953。'
- en: '*Live Site Incident escalation forecast* (2023), [https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178).'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*《实时网站事件升级预测》*（2023年），[https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178)。'
- en: 'H. Han, W.-Y. Wang, and B.-H. Mao, *Borderline-SMOTE: A New Over-Sampling Method
    in Imbalanced Data Sets Learning*, in Advances in Intelligent Computing, D.-S.
    Huang, X.-P. Zhang, and G.-B. Huang, Eds., in Lecture Notes in Computer Science,
    vol. 3644\. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 878–887\.
    doi: 10.1007/11538059_91.'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'H. Han，W.-Y. Wang，和B.-H. Mao，*Borderline-SMOTE：不平衡数据集学习中的新过采样方法*，在《智能计算进展》中，D.-S.
    Huang，X.-P. Zhang，和G.-B. Huang，编，在《计算机科学讲座笔记》第3644卷。柏林，海德堡：Springer Berlin Heidelberg，2005年，第878-887页。doi:
    10.1007/11538059_91。'
- en: 'P. Meiyappan and M. Bales, *Position Paper: Reducing Amazon’s packaging waste
    using multimodal deep learning*, (2021), article: [https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste](https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste),
    paper: [https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning](https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning).'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: P. Meiyappan和M. Bales，*立场文件：使用多模态深度学习减少亚马逊的包装浪费*，（2021年），文章：[https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste](https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste)，论文：[https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning](https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning)。
- en: 'Haibo He, Yang Bai, E. A. Garcia, and Shutao Li, *ADASYN: Adaptive synthetic
    sampling approach for imbalanced learning*, in 2008 IEEE International Joint Conference
    on Neural Networks (IEEE World Congress on Computational Intelligence), Hong Kong,
    China: IEEE, Jun. 2008, pp. 1322–1328\. doi: 10.1109/IJCNN.2008.4633969.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Haibo He, Yang Bai, E. A. Garcia 和 Shutao Li，*ADASYN：用于不平衡学习的自适应合成采样方法*，载于2008年IEEE国际神经网络联合会议（IEEE计算智能世界大会），中国香港：IEEE，2008年6月，第1322–1328页。doi:
    10.1109/IJCNN.2008.4633969。'
- en: 'Y. Elor and H. Averbuch-Elor, *To SMOTE, or not to SMOTE?*, arXiv, May 11,
    2022\. Accessed: Feb. 19, 2023\. [Online]. Available at [http://arxiv.org/abs/2201.08528](http://arxiv.org/abs/2201.08528).'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Elor 和 H. Averbuch-Elor, *SMOTE 是否必要？*，arXiv，2022年5月11日。访问时间：2023年2月19日。[在线]。可在[http://arxiv.org/abs/2201.08528](http://arxiv.org/abs/2201.08528)获取。
