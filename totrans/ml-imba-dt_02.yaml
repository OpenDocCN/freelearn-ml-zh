- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Oversampling Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In machine learning, we often don‚Äôt have enough samples of the minority class.
    One solution might be to gather more samples of such a class. For example, in
    the problem of detecting whether a patient has cancer or not, if we don‚Äôt have
    enough samples of the cancer class, we can wait for some time to gather more samples.
    However, such a strategy is not always feasible or sensible and can be time-consuming.
    In such cases, we can augment our data by using various techniques. One such technique
    is oversampling.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the concept of oversampling, discuss when
    to use it, and the various techniques to perform it. We will also demonstrate
    how to utilize these techniques through the `imbalanced-learn` library APIs and
    compare their performance using some classical machine learning models. Finally,
    we will conclude with some practical advice on which techniques tend to work best
    under specific real-world conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Random oversampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE variants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ADASYN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance comparison of various oversampling methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidance for using various oversampling techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oversampling in multi-class classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will utilize common libraries such as `numpy`, `scikit-learn`,
    and `imbalanced-learn`. The code and notebooks for this chapter are available
    on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/master/chapter02).
    You can just fire up the GitHub notebook using Google Colab by clicking on the
    **Open in Colab** icon at the top of this chapter‚Äôs notebook or by launching it
    from [https://colab.research.google.com](https://colab.research.google.com) using
    the GitHub URL of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: What is oversampling?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sampling** involves selecting a subset of observations from a larger set
    of observations. In this chapter, we‚Äôll initially focus on binary classification
    problems with two classes: the positive class and the negative class. The minority
    class has significantly fewer instances than the majority class. Later in this
    chapter, we will explore multi-class classification problems. Toward the end of
    this chapter, we will look into oversampling for multi-class classification problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Oversampling** is a data balancing technique that generates more samples
    of the minority class. However, this can be easily scaled to work for any number
    of classes where there are multiple classes with an imbalance. *Figure 2**.1*
    shows how samples of minority and majority classes are imbalanced (**a**) initially
    and balanced (**b**) after applying an oversampling technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 ‚Äì An increase in the number of minority class samples after oversampling
  prefs: []
  type: TYPE_NORMAL
- en: '*Why is oversampling needed*, you ask? It is required so that we give the model
    enough samples of the minority class to learn from it. If we offer too few instances
    of the minority class, the model may choose to ignore these minority class examples
    and focus solely on the majority class examples. This, in turn, would lead to
    the model not being able to learn the decision boundary well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs generate a two-class imbalanced dataset with a 1:99 ratio using the `sklearn`
    library‚Äôs `make_classification` API, which creates a normally distributed set
    of points for each class. This will generate an imbalanced dataset of two classes:
    one being the minority class with label 1 and the other being the majority class
    with label 0\. We will apply various oversampling techniques throughout this chapter
    to balance this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates 100 examples of class 1 and 9,900 examples of class 0 with
    an imbalance ratio of 1:99\. By plotting the dataset, we can see how the examples
    are distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 ‚Äì The dataset with an imbalance ratio of 1:99
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we understood the need for oversampling. We also generated
    a synthetic imbalanced binary classification dataset to demonstrate the application
    of various oversampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Random oversampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The simplest strategy to balance the imbalance in a dataset is to randomly choose
    samples of the minority class and repeat or duplicate them. This is also called
    **random oversampling** **with replacement**.
  prefs: []
  type: TYPE_NORMAL
- en: To increase the number of minority class observations, we can replicate the
    minority class data observations enough times to balance the two classes. Does
    this sound too trivial? Yes, but it works. By increasing the number of minority
    class samples, random oversampling reduces the bias toward the majority class.
    This helps the model learn the patterns and characteristics of the minority class
    more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: We will use random oversampling from the `imbalanced-learn` library. The `fit_resample`
    API from the `RandomOverSampler` class resamples the original dataset and balances
    it. The `sampling_strategy` parameter is used to specify the new ratio of various
    classes. For example, we could say `sampling_strategy=1.0` to have an equal number
    of the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are various ways to specify `sampling_strategy`, such as a float value,
    string value, or `dict` ‚Äì for example, {0: 50, 1: 50}:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: So, we went from a ratio of 1:99 to 1:1, which is what we expected with `sampling_strategy=1.0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs plot the oversampled dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 ‚Äì Dataset oversampled using RandomOverSampler (label 1 examples appear
    unchanged due to overlap)
  prefs: []
  type: TYPE_NORMAL
- en: After applying random oversampling, the examples with label 1 overlap each other,
    creating the impression that nothing has changed. Repeating the same data point
    over and over can cause the model to memorize the specific data points and not
    be able to generalize to new, unseen examples. The `shrinkage` parameter in `RandomOverSampler`
    lets us perturb or shift each point by a small amount.
  prefs: []
  type: TYPE_NORMAL
- en: The value of the `shrinkage` parameter has to be greater than or equal to 0
    and can be `float` or `dict`. If a `float` data type is used, the same shrinkage
    factor will be used for all classes. If a `dict` data type is used, the shrinkage
    factor will be specific for each class.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 2**.4*, we can observe the impact of random oversampling with `shrinkage=0.2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 ‚Äì Result of applying random oversampling with shrinkage=0.2
  prefs: []
  type: TYPE_NORMAL
- en: Toward the end of this chapter, we will compare the performance of random oversampling
    with various other oversampling techniques across multiple models and datasets.
    This will provide insights into their effectiveness in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: üöÄ Random oversampling in production at Grab
  prefs: []
  type: TYPE_NORMAL
- en: Grab, a ride-hailing and food delivery service in Southeast Asia, developed
    an image collection platform [1] for storing and retrieving imagery and map data.
    A key feature of this platform was its ability to automatically detect and blur
    **Personally Identifiable Information** (**PII**), such as faces and license plates,
    in street-level images. This was essential for maintaining user privacy. The dataset
    that was used for this purpose had a significant imbalance, with far more negative
    samples (images without PII) than positive ones (images with PII). Manual annotation
    was not feasible, so they turned to machine learning to solve this problem.
  prefs: []
  type: TYPE_NORMAL
- en: To address the data imbalance, Grab employed the random oversampling technique
    to increase the number of positive samples, thereby enhancing the performance
    of their machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with random oversampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random oversampling can often lead to overfitting of the model since the generated
    synthetic observations get repeated, and the model sees the same observations
    again and again. Shrinkage tries to handle that in some sense, but it may be challenging
    to come up with an apt value of shrinkage, and shrinkage doesn‚Äôt care if the generated
    synthetic samples overlap with the majority class samples, which can lead to other
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, we learned about the most basic and practical technique
    for applying oversampling to balance a dataset and reduce bias toward the majority
    class. Many times, random oversampling itself might give us such a high boost
    to our model‚Äôs performance that we may not even need to apply more advanced techniques.
    In production settings, it would also be beneficial to keep things plain and simple
    until we are ready to introduce more complexity in the pipeline. As they say,
    ‚Äúpremature optimization is the root of all evil,‚Äù so we start with something simple,
    so long as it does improve our model‚Äôs performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we will explore some alternative techniques, such
    as SMOTE and ADASYN, which adopt a different approach to oversampling and alleviate
    some of the problems associated with the random oversampling technique.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main problem with random oversampling is that it duplicates the observations
    from the minority class. This can often cause overfitting. **Synthetic Minority
    Oversampling Technique** (**SMOTE**) [2] solves this problem of duplication by
    using a technique called **interpolation**.
  prefs: []
  type: TYPE_NORMAL
- en: Interpolation involves creating new data points in the range of known data points.
    Think of interpolation as being similar to the process of reproduction in biology.
    In reproduction, two individuals come together to produce a new individual with
    traits of both of them. Similarly, in interpolation, we pick two observations
    from the dataset and create a new observation by choosing a random point on the
    line joining the two selected points.
  prefs: []
  type: TYPE_NORMAL
- en: 'We oversample the minority class by interpolating synthetic examples. That
    prevents the duplication of minority samples while generating new synthetic observations
    similar to the known points. *Figure 2**.5* depicts how SMOTE works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 ‚Äì Working of SMOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The majority and minority class samples are plotted (left)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The synthetic samples are generated by taking a random point on the line joining
    a minority sample to two nearest neighbor majority class samples (right)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SMOTE was originally designed for continuous inputs. To keep the explanations
    simple, we‚Äôll start with continuous inputs and discuss other kinds of inputs later.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will examine the functioning of SMOTE and explore any potential disadvantages
    associated with this technique.
  prefs: []
  type: TYPE_NORMAL
- en: How SMOTE works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The SMOTE algorithm works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It considers only the samples from the minority class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It trains KNN on the minority samples. A typical value of `k` is 5.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each minority sample, a line is drawn between the point and each of its
    KNN examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each such line segment, a point on the segment is randomly picked to create
    a new synthetic example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let‚Äôs use SMOTE using APIs from the `imbalanced-learn` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The oversampled dataset looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 ‚Äì Oversampling using SMOTE
  prefs: []
  type: TYPE_NORMAL
- en: üöÄ Oversampling techniques in production at Microsoft
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application at Microsoft [3], machine learning was employed
    to forecast **Live Site Incidents** (**LSIs**) for early detection and escalation
    of incidents for engineering teams. Every day, a high volume of incidents was
    being generated, most of which started as low-severity issues. Due to limited
    resources, it was impractical for engineering teams to investigate all incidents,
    leading to potential delays in mitigating critical issues until they had a significant
    customer impact.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this, Microsoft employed machine learning to forecast which LSIs
    could escalate into severe problems, aiming for proactive identification and early
    resolution. The challenge was the data imbalance in the training set: out of approximately
    40,000 incidents, fewer than 2% escalated to high severity. Microsoft used two
    different oversampling techniques‚Äî bagged classification (covered in [*Chapter
    4*](B17259_04.xhtml#_idTextAnchor120), *Ensemble Methods*), and SMOTE, which were
    the most effective in improving the model‚Äôs performance. They used a two-step
    pipeline for balancing classes: first, oversampling with **SMOTE** and then undersampling
    with **RandomUnderSampler** (covered in [*Chapter 3*](B17259_03.xhtml#_idTextAnchor079),
    *Undersampling Methods*). The pipeline automatically selected the optimal sampling
    ratios for both steps, and SMOTE performed better when combined with undersampling.
    The resulting end-to-end automated model was designed to be generic, making it
    applicable across different teams within or outside Microsoft, provided historical
    incidents were available for learning. The LSI insight tool used this model, which
    was adopted by various engineering teams.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the limitations of using SMOTE.
  prefs: []
  type: TYPE_NORMAL
- en: Problems with SMOTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SMOTE has its pitfalls ‚Äì for example, it can add noise to an already noisy
    dataset. It can also lead to class overlap issues as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SMOTE generates minority class samples without considering the majority class
    distribution, which may increase the overlap between the classes. In *Figure 2**.7*,
    we‚Äôre plotting the binary classification imbalanced dataset before and after applying
    SMOTE. We can see a lot of overlap between the two classes after applying SMOTE:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17259_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 ‚Äì Binary classification dataset before (left) and after (right) applying
    SMOTE (see the overlap between two classes on the right)
  prefs: []
  type: TYPE_NORMAL
- en: The other case may be that you have a huge amount of data, and running SMOTE
    may increase the runtime of your pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problem 1 can be solved by using the SMOTE variant Borderline-SMOTE (discussed
    in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about SMOTE, which uses the nearest neighbor technique
    to generate synthetic samples of the minority class. Sometimes, SMOTE may perform
    better than random oversampling since it exploits the proximity to other minority
    class samples to generate new samples.
  prefs: []
  type: TYPE_NORMAL
- en: SMOTE variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let‚Äôs look at some of the SMOTE variants, such as Borderline-SMOTE, SMOTE-NC,
    and SMOTEN. These variants apply the SMOTE algorithm to samples of a certain kind
    and may not always be applicable.
  prefs: []
  type: TYPE_NORMAL
- en: Borderline-SMOTE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Borderline-SMOTE [4] is a variation of SMOTE that generates synthetic samples
    from the minority class samples that are near the classification boundary, which
    divides the majority class from the minority class.
  prefs: []
  type: TYPE_NORMAL
- en: Why consider samples on the classification boundary?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea is that the examples near the classification boundary are more prone
    to misclassification than those far away from the decision boundary. Producing
    more such minority samples along the boundary would help the model learn better
    about the minority class. Intuitively, it is also true that the points away from
    the classification boundary likely won‚Äôt make the model a better classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here‚Äôs a step-by-step algorithm for Borderline-SMOTE:'
  prefs: []
  type: TYPE_NORMAL
- en: We run a KNN algorithm over the whole dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we divide the minority class points into three categories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Noise* points are minority class examples that have all the neighbors from
    the majority class. These points are buried among majority-class neighbors. They
    are likely outliers and can safely be ignored as ‚Äúnoise.‚Äù'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Safe* points have more minority-class neighbors than majority-class neighbors.
    Such observations don‚Äôt contain much information and can be safely ignored.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Danger* points have more majority-class neighbors than minority-class neighbors.
    This implies that such observations are on or close to the boundary between the
    two classes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we train a KNN model only on the minority class examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we apply the SMOTE algorithm to the `Danger` points. Note that the
    neighbors of these `Danger` points may or may not be marked as `Danger`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As shown in *Figure 2**.8*, Borderline-SMOTE focuses on the danger class points
    for synthetic data generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 ‚Äì The Borderline-SMOTE algorithm uses only danger points to generate
    synthetic samples. Danger points have more majority-class neighbors than minority-class
    ones
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2**.9* shows how Borderline-SMOTE focuses on the minority class samples
    that are near the classification boundary, which separates the majority and minority
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 ‚Äì Illustrating Borderline-SMOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: a) Plots of majority and minority class samples
  prefs: []
  type: TYPE_NORMAL
- en: b) Synthetic samples generated using neighbors near the classification boundary
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs see how we can use Borderline-SMOTE from the `imbalanced-learn` library
    to perform oversampling of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Can you guess the problem with focusing solely on data points on the decision
    boundary of the two classes?
  prefs: []
  type: TYPE_NORMAL
- en: 'Since this technique focuses so heavily on a very small number of points on
    the boundary, the points inside the minority class clusters are not sampled at
    all:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 ‚Äì The Borderline-SMOTE algorithm utilizing danger points, with more
    majority- than minority-class neighbors, to generate synthetic samples
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about Borderline-SMOTE, which generates synthetic
    minority class samples by focusing on the samples that are close to the classification
    boundary of the majority and minority classes, which, in turn, may help in improving
    the discrimination power of the model.
  prefs: []
  type: TYPE_NORMAL
- en: üöÄ Oversampling techniques in production at Amazon
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world application, Amazon used machine learning to optimize packaging
    types for products, aiming to reduce waste while ensuring product safety [5].
    In their training dataset, which featured millions of product and package combinations,
    Amazon faced a significant class imbalance, with as few as 1% of the examples
    representing unsuitable product-package pairings (minority class).
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle this imbalance, Amazon used various oversampling techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Borderline-SMOTE oversampling, which resulted in a 4%-7% increase in PR-AUC
    but increased the training time by 25%-35%.'
  prefs: []
  type: TYPE_NORMAL
- en: '- A hybrid of random oversampling and random undersampling, where they randomly
    oversampled the minority class and undersampled the majority class. It led to
    a 6%-10% improvement in PR-AUC and increased the training time by up to 25%.'
  prefs: []
  type: TYPE_NORMAL
- en: The best-performing technique was two-phase learning with random undersampling
    (discussed in [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205), *Data-Level Deep
    Learning Methods)*, which improved PR-AUC by 18%-24% with no increase in training
    time.
  prefs: []
  type: TYPE_NORMAL
- en: They mentioned that the effectiveness of a technique in dealing with dataset
    imbalance is both domain- and dataset-specific. This real-world example underscores
    the effectiveness of oversampling techniques in tackling class imbalance issues.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn about another oversampling technique, called ADASYN, that
    oversamples examples near boundaries and in other low-density regions without
    completely ignoring data points that do not lie on the boundary.
  prefs: []
  type: TYPE_NORMAL
- en: ADASYN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While SMOTE doesn‚Äôt distinguish between the density distribution of minority
    class samples, **Adaptive Synthetic Sampling** (**ADASYN**) [6] focuses on harder-to-classify
    minority class samples since they are in a low-density area. ADASYN uses a weighted
    distribution of the minority class based on the difficulty of classifying the
    observations. This way, more synthetic data is generated from harder samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 ‚Äì Illustration of how ADASYN works
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: a) The majority and minority class samples are plotted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: b) Synthetic samples are generated depending on the hardness factor (explained
    later)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While SMOTE uses all samples from the minority class for oversampling uniformly,
    in ADASYN, the observations that are harder to classify are used more often.
  prefs: []
  type: TYPE_NORMAL
- en: Another difference between the two techniques is that, unlike SMOTE, ADASYN
    also uses the majority class observations while training KNN. It then decides
    the hardness of samples based on how many majority observations are its neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: Working of ADASYN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ADASYN follows a simple algorithm. Here is the step-by-step working of ADASYN:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it trains a KNN on the entire dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each observation of the minority class, we find the hardness factor. This
    factor tells us how difficult it is to classify that data point. The hardness
    factor, denoted by r, is the ratio of the number of majority class neighbors with
    the total number of neighbors. Here, r = M¬†_¬†K¬†, where M is the count of majority
    class neighbors and K is the total number of nearest neighbors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each minority observation, we generate synthetic samples proportional to
    the hardness factor by drawing a line between the minority observation and its
    neighbors (neighbors could be from the majority class or minority class). The
    harder it is to classify a data point, the more synthetic samples will be created
    for it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let‚Äôs see how we can use the ADASYN API from the `imbalanced-learn` library
    to perform oversampling of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B17259_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 ‚Äì ADASYN prioritizes harder samples and incorporates majority class
    examples in KNN to assess sample hardness
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 ‚Äì A memory aid summarizing various oversampling techniques
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about ADASYN. Next, let‚Äôs see how we can deal with
    cases when our data contains categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical features and SMOTE variants (SMOTE-NC and SMOTEN)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What if your data contains categorical features? A categorical feature can
    take one of a limited or fixed number of possible values, and it‚Äôs a parallel
    to enumerations (enums) in computer science. These could be nominal categorical
    features that lack a natural order (for example, hair color, ethnicity, and so
    on) or ordinal categorical features that have an inherent order (for example,
    low, medium, and high):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 ‚Äì Categorical data and its types with examples
  prefs: []
  type: TYPE_NORMAL
- en: For ordinal features, we can just encode them via sklearn‚Äôs `OrdinalEncoder`,
    which assigns the categories to the values 0, 1, 2, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For nominal features, none of the SMOTE variants we have learned so far will
    work. However, `RandomOverSampler` can handle nominal features too:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: However, SMOTE, by default, works only on continuous data and cannot be directly
    used on categorical data. *Why?* That‚Äôs because SMOTE works by generating a random
    point on the line joining two different data points of the minority class (also
    called interpolation). If our data is categorical and has values of ‚Äúyes‚Äù and
    ‚Äúno,‚Äù we would first need to transform such values into numbers. Even when we
    do so, say ‚Äúyes‚Äù is mapped to 1 and ‚Äúno‚Äù is mapped to 0, the interpolation via
    SMOTE may end up producing a new point of 0.3, which does not map to any real
    category.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we cannot use the `shrinkage` parameter in `RandomOverSampler` with categorical
    data because this parameter is designed only for continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, two variants of SMOTE can deal with categorical features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`imbalanced-learn` to oversample our dataset. The first item in the dataset
    is categorical, and the second item is continuous:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Synthetic Minority Oversampling Technique for Nominal** (**SMOTEN**) is used
    for nominal categorical data. SMOTEN performs the majority vote similar to SMOTE-NC
    for all the features. It considers all features as nominal categorical, and the
    feature value of new samples is decided by taking the most frequent category of
    the nearest neighbors. The distance metric that‚Äôs used for calculating the nearest
    neighbors is called the **Value Distance Metric** (**VDM**). VDM computes the
    distance between two attribute values by considering the distribution of class
    labels associated with each value. It is based on the idea that two attribute
    values are more similar if they have similar distributions of class labels. This
    way, VDM can capture the underlying relationships between categorical attributes
    and their corresponding class labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let‚Äôs look at some example code that uses SMOTEN:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In *Table 2.1*, we can see SMOTE, SMOTEN, and SMOTENC, with a few examples
    for each technique to demonstrate the difference between them:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** **of SMOTE** | **Features Supported** | **Example Data** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SMOTE | Only numerical | features: [2.3, 4.5, 1.2], label: 0features: [3.4,
    2.2, 5.1], label: 1 |'
  prefs: []
  type: TYPE_TB
- en: '| SMOTEN | Categorical(nominal or ordinal) | features: [‚Äògreen‚Äô, ‚Äòsquare‚Äô],
    label: 0features: [‚Äòred‚Äô, ‚Äòcircle‚Äô], label: 1 |'
  prefs: []
  type: TYPE_TB
- en: '| SMOTENC | Numerical or categorical(nominal or ordinal) | features: [2.3,
    ‚Äògreen‚Äô, ‚Äòsmall‚Äô, ‚Äòsquare‚Äô], label: 0features: [3.4, ‚Äòred‚Äô, ‚Äòlarge‚Äô, ‚Äòcircle‚Äô],
    label: 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 ‚Äì SMOTE and some of its common variants with example data
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we should use SMOTENC when we have a mix of categorical and continuous
    data types, while SMOTEN can only be used when all the columns are categorical.
    You might be curious about how the various oversampling methods compare with each
    other in terms of model performance. We‚Äôll explore this topic in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Model performance comparison of various oversampling methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let‚Äôs examine how some popular models perform with the different oversampling
    techniques we‚Äôve discussed. We‚Äôll use two datasets for this comparison: one synthetic
    and one real-world dataset. We‚Äôll evaluate the performance of four oversampling
    techniques, as well as no sampling, using logistic regression and random forest
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find all the related code in this book‚Äôs GitHub repository. In *Figure
    2**.15* and *Figure 2**.16*, we can see the average precision score values for
    both models on the two datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 ‚Äì Performance comparison of various oversampling techniques on a
    synthetic dataset
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17259_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 ‚Äì Performance comparison of various oversampling techniques on the
    thyroid_sick dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these plots, we can draw some useful conclusions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Effectiveness of oversampling**: In general, using oversampling techniques
    seems to improve the average precision score compared to not using any sampling
    (NoSampling).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithm sensitivity**: The effectiveness of oversampling techniques varies
    depending on the machine learning algorithm used. For example, random forest seems
    to benefit more from oversampling techniques than logistic regression, especially
    on synthetic data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thyroid_sick` dataset but showed variations in the synthetic data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thyroid_sick` data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For random forest, Borderline-SMOTE had the highest average precision score
    on synthetic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thyroid_sick` data.*   **No clear winner**: There is no single oversampling
    technique that outperforms all others across all conditions. The choice of technique
    may depend on the specific algorithm and dataset being used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that the models used here are not tuned with the best hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the hyperparameters of random forest and logistic regression models may
    improve the models' performance further.
  prefs: []
  type: TYPE_NORMAL
- en: In general, there is no single technique that will always do better than the
    rest. We have multiple variables at play here, namely the ‚Äúmodel‚Äù and the ‚Äúdata.‚Äù
    Most of the time, the only way to know is to try out a bunch of these techniques
    and find the one that works the best for our model and data. You may find yourself
    curious about how to choose from the numerous oversampling options available.
  prefs: []
  type: TYPE_NORMAL
- en: Guidance for using various oversampling techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs review some guidelines on how to navigate through the various oversampling
    techniques we went over and how these techniques differ from each other:'
  prefs: []
  type: TYPE_NORMAL
- en: Train a model without applying any sampling techniques. This will be our model
    with baseline performance. Any oversampling technique we apply is expected to
    give a boost to this performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start with random oversampling and add some shrinkage too. We may have to play
    with some values of shrinkage to see if the model‚Äôs performance improves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When we have categorical features, we have a couple of options:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert all categorical features into numerical features first using one-hot
    encoding, label encoding, feature hashing, or other feature transformation techniques.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: (Only for nominal categorical features) Use SMOTENC and SMOTEN directly on the
    data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply various oversampling techniques ‚Äì random oversampling, SMOTE, Borderline-SMOTE,
    and ADASYN ‚Äì and measure the model‚Äôs performance on metrics applicable to your
    problem, such as the average precision score, ROC-AUC, precision, recall, F1 score,
    and more.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since oversampling alters the distribution of the training dataset, which is
    not the case for the test set or the real world, using oversampling can potentially
    generate biased predictions. After using oversampling, it can be essential to
    recalibrate our model‚Äôs probability scores depending on the application. Recalibration
    of the model corrects any bias introduced by altering the class distribution,
    ensuring more reliable decision-making when deployed. Similarly, adjusting the
    classification threshold is key for accurate model interpretation, especially
    with imbalanced datasets. For more details on recalibration and threshold adjustment,
    please see [*Chapter 10*](B17259_10.xhtml#_idTextAnchor279), *Model Calibration*,
    and [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive* *Learning*,
    respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When to avoid oversampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 1*](B17259_01.xhtml#_idTextAnchor015), *Introduction to Data Imbalance
    in Machine Learning*, we discussed scenarios where data imbalance may not be a
    concern. Those considerations should be revisited before you opt for oversampling
    techniques. Despite criticisms, the applicability of oversampling should be evaluated
    on a case-by-case basis. Here are some additional technical considerations to
    keep in mind when choosing to apply oversampling techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computational cost**: Oversampling increases the dataset‚Äôs size, leading
    to higher computational demands in terms of processing time and hardware resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality**: If the minority class data is noisy or has many outliers,
    oversampling can introduce more noise, reducing model reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classifier limitations**: In scenarios with system constraints, such as extremely
    low latency, or when dealing with legacy systems, the use of strong classifiers
    (complex and more accurate models) may not be feasible. In these cases, we may
    be limited to using weak classifiers. Weak classifiers are simpler and less accurate
    but require fewer computational resources and have lower runtime latency. In such
    situations, oversampling can be beneficial [7]. For strong classifiers, oversampling
    may offer diminishing returns, and optimizing the decision threshold could sometimes
    serve as a simpler, less resource-intensive alternative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider these factors when deciding whether to use oversampling methods for
    imbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 2.2* summarizes the key ideas, pros, and cons of various oversampling
    techniques. This can help you better evaluate which oversampling method to choose:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **SMOTE** | **Borderline-SMOTE** | **ADASYN** | **SMOTE-NC** **and SMOTEN**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Key idea | Choose random points on the line joining the nearest neighbors
    of minority class examples. | Choose the minority samples on the boundary between
    the majority and minority classes. Perform SMOTE for such samples on the boundary.
    | Automatically decides the number of minority class samples to generate according
    to density distribution. More points are generated where the density distribution
    is low. | It performs a majority vote for the categorical features. |'
  prefs: []
  type: TYPE_TB
- en: '| Pro | Usually reduces false negatives. | Creates synthetic samples that are
    not na√Øve copies of the known data. | It cares about the density distribution
    of different classes. | It works with categorical data. |'
  prefs: []
  type: TYPE_TB
- en: '| Con | Overlapping classes may occur and can introduce more noise to data.
    This may not work well with high-dimensional data or multi-class classification
    problems. | It does not care about the distribution of minority class examples.
    | It focuses on areas where there is overlap between classes. It may focus too
    much on outliers, resulting in poor model performance. | The same as SMOTE. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.2 ‚Äì Summarizing the various oversampling techniques that were discussed
    in this chapter
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at some general guidelines to apply the various oversampling
    techniques we learned about in this chapter and the pros and cons of using them.
    Next, we will look at how to extend the various oversampling methods to multi-class
    classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling in multi-class classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In multi-class classification problems, we have more than two classes or labels
    to be predicted, and hence more than one class may be imbalanced. This adds some
    more complexity to the problem. However, we can apply the same techniques to multi-class
    classification problems as well. The `imbalanced-learn` library provides the option
    to deal with multi-class classification in almost all the supported methods. We
    can choose from various sampling strategies using the `sampling_strategy` parameter.
    For multi-class classification, we can pass some fixed string values (called built-in
    strategies) to the `sampling_strategy` parameter in the SMOTE API. We can also
    pass a dictionary with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Keys as the class labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values as the number of samples of that class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are the built-in strategies for `sampling_strategy` when using the parameter
    as a string:'
  prefs: []
  type: TYPE_NORMAL
- en: The `minority` strategy resamples only the minority class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `not minority` strategy resamples all classes except the minority class.
    This may be helpful in the case of multi-class imbalance, where we have more than
    two classes and multiple classes are imbalanced, but we don‚Äôt want to touch the
    minority class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `not majority` strategy resamples all classes except the majority class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `all` strategy resamples all classes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `auto` strategy is the same as the `not` `majority` strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following code shows the usage of SMOTE for multi-class classification using
    various sampling strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let‚Äôs create a dataset containing 100 samples with three classes that
    have weights of 0.1, 0.4, and 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As expected, our dataset contains the three classes in the ratio 10:40:50 for
    classes 0, 1, and 2, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let‚Äôs apply SMOTE with the ‚Äú*minority*‚Äù sampling strategy. This will oversample
    the class with the least number of samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Since class 0 previously had the least number of samples, the ‚Äú*minority*‚Äù sampling
    strategy only oversampled class 0, making the number of samples equal to the number
    of samples in the majority class.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we‚Äôre using a dictionary for oversampling. Here, for
    each class label (0, 1, or 2) as `key` in the `sampling_strategy` dictionary,
    we have the number of desired samples for each targeted class as `value`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Please note that when using `dict` within `sampling_strategy`, the number of
    desired samples for each class should be greater than or equal to the original
    number of samples. Otherwise, the `fit_resample` API will throw an exception.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we saw how to extend oversampling strategies to handle cases
    when we have imbalanced datasets with more than two classes. Most of the time,
    the ‚Äúauto‚Äù `sampling_strategy` would be good enough and would balance all the
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through various oversampling techniques for dealing
    with imbalanced datasets and applied them using Python‚Äôs `imbalanced-learn` library
    (also called `imblearn`). We also saw the internal workings of some of the techniques
    by implementing them from scratch. While random oversampling generates new minority
    class samples by duplicating them, SMOTE-based techniques work by choosing random
    samples in the direction of nearest neighbors of the minority class samples. Though
    oversampling can potentially overfit the model on your data, it usually has more
    pros than cons, depending on the data and model.
  prefs: []
  type: TYPE_NORMAL
- en: We applied them to some of the synthesized and publicly available datasets and
    benchmarked their performance and effectiveness. We saw how different oversampling
    techniques may lead to model performance on a varying scale, so it becomes crucial
    to try a few different oversampling techniques to decide on the one that‚Äôs most
    optimal for our data.
  prefs: []
  type: TYPE_NORMAL
- en: If you feel intrigued by the prospect of discovering oversampling approaches
    relevant to deep learning models, we invite you to check out [*Chapter 7*](B17259_07.xhtml#_idTextAnchor205),
    *Data-Level Deep Learning Methods*, where we‚Äôll discuss data-level techniques
    within the realm of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go over various undersampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explore the two variants of SMOTE, namely KMeans-SMOTE and SVM-SMOTE, from the
    `imbalanced-learn` library, not discussed in this chapter. Compare their performance
    with vanilla SMOTE, Borderline-SMOTE, and ADASYN using the logistic regression
    and random forest models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a classification problem with two classes, let‚Äôs say the minority class
    to majority class ratio is 1:20\. How should we balance this dataset? Should we
    apply the balancing technique at test or evaluation time? Please provide a reason
    for your answer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let‚Äôs say we are trying to build a model that can estimate whether a person
    can be granted a bank loan or not. Out of the 5,000 observations we have, only
    500 people got the loan approved. To balance the dataset, we duplicate the approved
    people data and then split it into train, test, and validation datasets. Are there
    any issues with using this approach?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data normalization helps in dealing with data imbalance. Is this true? Why or
    why not?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Explore the various oversampling APIs available from the `imbalanced-learn`
    library here: [https://imbalanced-learn.org/stable/references/over_sampling.html](https://imbalanced-learn.org/stable/references/over_sampling.html).
    Pay attention to the various parameters of each of the APIs.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Protecting Personal Data in Grab‚Äôs Imagery* (2021), [https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, *SMOTE: Synthetic
    Minority Over-sampling Technique*, jair, vol. 16, pp. 321‚Äì357, Jun. 2002, doi:
    10.1613/jair.953.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Live Site Incident escalation forecast* (2023), [https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178](https://medium.com/data-science-at-microsoft/live-site-incident-escalation-forecast-566763a2178).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'H. Han, W.-Y. Wang, and B.-H. Mao, *Borderline-SMOTE: A New Over-Sampling Method
    in Imbalanced Data Sets Learning*, in Advances in Intelligent Computing, D.-S.
    Huang, X.-P. Zhang, and G.-B. Huang, Eds., in Lecture Notes in Computer Science,
    vol. 3644\. Berlin, Heidelberg: Springer Berlin Heidelberg, 2005, pp. 878‚Äì887\.
    doi: 10.1007/11538059_91.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'P. Meiyappan and M. Bales, *Position Paper: Reducing Amazon‚Äôs packaging waste
    using multimodal deep learning*, (2021), article: [https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste](https://www.amazon.science/latest-news/deep-learning-machine-learning-computer-vision-applications-reducing-amazon-package-waste),
    paper: [https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning](https://www.amazon.science/publications/position-paper-reducing-amazons-packaging-wasteusing-multimodal-deep-learning).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Haibo He, Yang Bai, E. A. Garcia, and Shutao Li, *ADASYN: Adaptive synthetic
    sampling approach for imbalanced learning*, in 2008 IEEE International Joint Conference
    on Neural Networks (IEEE World Congress on Computational Intelligence), Hong Kong,
    China: IEEE, Jun. 2008, pp. 1322‚Äì1328\. doi: 10.1109/IJCNN.2008.4633969.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Y. Elor and H. Averbuch-Elor, *To SMOTE, or not to SMOTE?*, arXiv, May 11,
    2022\. Accessed: Feb. 19, 2023\. [Online]. Available at [http://arxiv.org/abs/2201.08528](http://arxiv.org/abs/2201.08528).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
