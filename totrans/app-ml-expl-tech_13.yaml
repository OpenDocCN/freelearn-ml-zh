- en: '*Chapter 10*: XAI Industry Best Practices'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first section of this book, we discussed various concepts related to
    **Explainable AI** (**XAI**). These concepts were established through years of
    research, considering various application domains of **artificial intelligence**
    (**AI**). However, the need for XAI for industrial applications has been felt
    very recently as AI adoption in industrial use cases is increasing. Unfortunately,
    the general awareness of XAI for industrial use cases is still lacking due to
    certain challenges and gaps in how to implement human-friendly explainability
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: In *Section 2*, *Practical Problem Solving*, we covered many XAI Python frameworks
    that are popularly used for interpreting the working of **machine learning** (**ML**)
    models. However, only understanding how to apply the XAI Python frameworks in
    practice is not sufficient for industrial problems. Industrial problems require
    solutions that are scalable and sustainable. So, it is very important for us to
    discuss the best practices of XAI for scalable and sustainable AI solutions used
    for industrial problems.
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, XAI has evolved a lot. From being a topic of academic research,
    XAI is now a powerful tool in the toolkit of AI and ML industrial practitioners.
    However, XAI has many open challenges on which the research community is still
    working to bring AI closer to end users. So, we will discuss the existing challenges
    of XAI and the general recommendations for designing an explainable ML system
    while considering the open challenges. Also, the quality of AI/ML systems is as
    good as the quality of the underlying data. Therefore, we will also focus on the
    importance of adopting a data-first approach for model explainability.
  prefs: []
  type: TYPE_NORMAL
- en: The XAI research community believes that XAI is a multi-disciplinary perspective
    that should be centered around the end user. So, we will discuss the concept of
    **interactive machine learning** (**IML**) to create high user engagement for
    industrial AI systems. Finally, we will cover the importance of providing actionable
    suggestions and insights using AI/ML as an approach to decipher the complex nature
    of AI models, thereby making AI explainable and increasing users' trust.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the previous chapters, in this chapter, we will not focus on the practical
    applications or learn a new XAI framework. Instead, our goal is to understand
    the best practices of XAI for industrial use cases. So, in this chapter, we are
    going to discuss the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Open challenges of XAI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidelines for designing explainable ML systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adopting a data-first approach for explainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emphasizing IML for explainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emphasizing prescriptive insights for explainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, let's find out more about these topics next.
  prefs: []
  type: TYPE_NORMAL
- en: Open challenges of XAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As briefly discussed, there have been some significant advances in the field
    of XAI. XAI is no longer just a topic of academic research; the availability of
    XAI frameworks has made XAI an essential tool for industrial practitioners. But
    are these frameworks sufficient to increase AI adoption? Unfortunately, the answer
    is no. XAI is yet to mature further as there are certain open challenges that,
    once resolved, can significantly bridge the gap between AI and the end user. Let''s
    discuss these open challenges next:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Shifting focus between the model developer and the end user*: After exploring
    many XAI frameworks throughout this book, you might have also felt that the explainability
    provided by most of the frameworks requires technical knowledge of ML, mathematics,
    or statistics to truly understand the working of the model. This is because the
    explainability methods or algorithms were primarily designed for ML experts or
    model developers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As more and more end users start utilizing AI models and systems, the need for
    non-technical human-friendly explanations is growing. So, for industrial applications,
    dynamically shifting the focus between the model developer and the end user is
    a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: To a non-technical end user, a simple explanation method such as feature importance
    visualization can become really complicated unless explicit information is provided.
    In order to mitigate this challenge, the general recommendation is to design user-centric
    AI systems. Similar to any software application or system, the user should be
    involved in the development process early on to understand their requirements
    and include their expertise while designing the application and not post-production
    of the application.
  prefs: []
  type: TYPE_NORMAL
- en: '*Lack of stakeholder participation*: From the previous point, although the
    recommended action is to involve the end users early on in the development process
    of the AI system, onboarding a stakeholder in the development process can also
    be a challenge. For most industrial use cases, AI solutions are developed in isolation
    without involving the final stakeholder(s). Following the design principles from
    the field of **Human-Computer Interaction** (**HCI**), the user should be involved
    in the loop during the development process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considering high stake domains such as healthcare, finance, legal and regulatory,
    getting stakeholders and domain experts can be an extremely tedious and expensive
    process. The stakeholder's availability can be a challenge. Their interest or
    motivation to participate in the development process can be low even with necessary
    incentives and compensation. Due to these difficulties in onboarding end users
    into the development process, designing a user-centric AI system is difficult.
  prefs: []
  type: TYPE_NORMAL
- en: The most recommended action to tackle this challenge is through a collaboration
    between industry and academia. Usually, academic institutions such as medical
    schools, law schools, or other universities have broader access to real participants
    or students who belong to the respective fields and can be *pseudo* participants.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how XAI is a multi-disciplinary perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18216_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – XAI is a multi-disciplinary perspective
  prefs: []
  type: TYPE_NORMAL
- en: '*Application-specific challenges*: Different application domains need explainability
    of different types. For example, in an AI-based loan approval system, influence-based
    or example-based feature explanations can be really helpful. However, for an application
    to detect COVID-19 infections from X-ray images, highlighting or localizing the
    region of the infection can be more helpful. So, each application can have its
    own requirement and definition of explainability and, thus, any general XAI framework
    might not be very effective.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lack of quantitative evaluation metrics*: The quantitative evaluation of explanation
    methods has been an important research topic. Unfortunately, there is still no
    tool or framework that exists that can quantitatively evaluate the quality of
    explanation methods. This is mostly because many diverse AI algorithms are at
    work on different types of data. Consequently, there are many definitions of model
    explainability and many approaches for XAI. So, it is very hard to generalize
    quantitative evaluation metrics that can work with all of the different explanation
    methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Currently, qualitative evaluation methods such as *Trust*, *Usefulness*, *Actionability*,
    *Coherence with prior beliefs*, *Impact*, and more are used. To learn more about
    these metrics, take a look at *Understanding Machines: Explainable AI* from *Accenture
    Labs*, which is available at [https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf](https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf).
    Additionally, take a look at *Explanation in Artificial Intelligence: Insights
    from the Social Sciences* from *Tim Miller*, which is available at [https://arxiv.org/pdf/1706.07269.pdf](https://arxiv.org/pdf/1706.07269.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: The qualitative evaluation methods are, indeed, user-centric and use the principles
    of HCI to collect feedback from the end user, but usually, quantitative metrics
    are more useful when comparing different methods. However, I am hopeful that tools
    such as *Quantus* ([https://github.com/understandable-machine-intelligence-lab/Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus)),
    which is used to evaluate explanation methods for neural networks, will mature
    significantly in a few years and it will be easier to evaluate explanation methods.
  prefs: []
  type: TYPE_NORMAL
- en: '*Lack of actionable explanations*: Most explanation methods don''t provide
    actionable insights to the end user. So, designing explainable AI/ML systems that
    can provide actionable explanations can be challenging. Counterfactual explanations,
    what-if analysis, and interactive visualization-based explanations are the only
    explanation methods that allow the user to observe the change in outcome when
    the input features are altered. I would recommend increasing the usage of these
    actionable explanation methods to develop explainable AI/ML systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lack of contextual explanations*: Any ML algorithm that is deployed in production
    depends on the specific use case and the underlying data. Due to this, there is
    always a trade-off between explainability, model performance, fairness, and privacy.
    So, understanding the context of explainability is an existing challenge that
    any general XAI framework cannot provide accurately. So, the recommendation to
    mitigate this challenge is to design personalized explainable ML systems for a
    specific use case rather than a generalized implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want to explore more in this area, you can take a look at *Verma et
    al.''s* work, *Pitfalls of Explainable ML: An Industry Perspective* ([https://arxiv.org/abs/2106.07758](https://arxiv.org/abs/2106.07758)),
    to learn more about the typical challenges of XAI. All of these open challenges
    are interesting research problems that you can explore to help the research community
    progress in this field. Now that we have discussed the open challenges of XAI,
    next, let''s discuss the guidelines for designing explainable ML systems for industrial
    use cases, considering the open challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: Guidelines for designing explainable ML systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will discuss the recommended guidelines for designing an
    explainable ML system from an industry perspective while considering the open
    challenges of XAI, as discussed in the previous section. All of these guidelines
    have been carefully collated from various publications, conference keynotes, and
    panel discussions from various experts in the field of XAI, ML, and software systems.
    It is true that every ML and AI problem is unique in its own way, and so, it is
    hard to generalize any recommendations. But many AI organizations have adopted
    the following list of guidelines for designing explainable and user-friendly ML
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Identify the target audience of XAI and their usability context*: The definition
    of explainability depends on the user using the AI system. *Arrieta et al*., in
    their work *Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities,
    and Challenges toward Responsible AI*, have highlighted the importance of identifying
    the target audience of XAI when designing explainable AI systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AI system can have different audiences such as technical stakeholders (that
    is, data scientists, ML experts, product owners, and developers), business stakeholders
    (that is, managers and executive leaders), domain experts (that is, doctors, lawyers,
    insurance agents, and more), legal and regulatory agencies, and non-technical
    end users. Every audience might have a different need for explainability, so accordingly,
    the explanation methods should try to address the best needs of the audience.
  prefs: []
  type: TYPE_NORMAL
- en: As a preliminary step, identifying the target audience of the explainable system
    along with the situation or context in which they are going to use the system
    helps a lot in the design process. For example, for medical experts relying on
    ML models to predict the risk of diabetes, the choice of explanation methods depends
    on their actual needs. If their need is to suggest actions to improve the health
    conditions of diabetic patients, then counterfactual examples can be really useful.
    However, if their purpose is to find out the factors that are leading to the increase
    in the risk of diabetes, then feature-based explanations methods are more relevant.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.2* illustrates the various target audience of explainable AI systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18216_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Identifying the target audience of XAI
  prefs: []
  type: TYPE_NORMAL
- en: '*Shortlisting the XAI techniques based on the user''s needs*: Once the target
    audience and their usability context have been identified, along with the necessary
    technical details about the type of the dataset (for instance, tabular, images,
    or textual) and the ML algorithm used for training the model, shortlisting a list
    of possible explanation methods, as covered in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033),
    *Model Explainability Methods*, is very important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These shortlisted explanation methods should fit in with the software system
    that the target users will use to interact with the AI models. This means the
    explanation techniques should be well integrated with the software applications
    or interfaces and should even be considered during the design process of the software
    for a consistent user experience.
  prefs: []
  type: TYPE_NORMAL
- en: '*Human-centered XAI: An iterative process of translating and evaluating XAI
    in specific domains involving the end user*. Similar to the design life cycle
    of a software system using HCI, XAI is also an iterative process. It should be
    human-centered and should be evaluated continuously to assess the impact. In the
    *User-centric system design using XAI* section of [*Chapter 11*](B18216_11_ePub.xhtml#_idTextAnchor217), *End
    User-Centered Artificial Intelligence,* I have included other important aspects
    to consider for a human-centered XAI design process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The importance of the feedback loop in XAI*: All explainable AI systems should
    have the option to capture the end user''s feedback to assess the impact, relevance,
    effectiveness, and trust of the explanations provided by the system. It is never
    possible to consider all edge cases and all preferences of the end users during
    the design and the initial development process. But using the feedback loop, developers
    can collect specific feedback about the explanation methods and modify them if
    needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The importance of scalability in the design process*: Similar to serving ML
    models for production systems, explainability should also be served in modular
    and scalable approaches. The best way to serve model explanations is by designing
    **scalable web APIs** to be deployed in centralized cloud servers. So, when XAI
    is implemented in practice, do make sure that the explanations are being served
    through web APIs so that they can be easily integrated with any software interface
    or application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Toggling between the data, the interface, and actionable insights*: It has
    been observed by many experts that, for end users, their satisfaction with the
    model explanation method is a trade-off between how well the explanation is being
    connected to the underlying dataset (or their prior beliefs), how the users are
    able to interact with the ML system to gain more confidence in it, and how well
    the explanations encourage them to take actions to get their desired output. **Data-centric
    XAI**, **IML**, and **actionable explanations** are broader research topics that
    should be considered when designing the explainable AI system for industrial use
    cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we have learned about the open challenges of XAI and discussed the design
    guidelines considering the open challenges. We now have a fair idea of what to
    consider when designing explainable ML systems. Next, let's elaborate on the last
    recommended guideline in the upcoming sections to carefully understand why it
    is important. Let's start our discussion with the importance of using a data-centric
    approach for explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting a data-first approach for explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B18216_03_ePub.xhtml#_idTextAnchor053), *Data-Centric Approaches*,
    we discussed the importance and various techniques of **Data-Centric XAI**. Now,
    in this section, we will elaborate on how adopting a data-first approach for explainability
    helps in gaining users' trust in industrial use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Data-centric AI is based on the fundamental idea that *the quality of the ML
    model is as good as the quality of the underlying dataset used for training the
    model*. For industrial use cases, dealing with poor-quality datasets is a major
    challenge for most data scientists. Unfortunately, data quality is often ignored
    as data scientists and ML experts are expected to cast their *magic* of ML to
    build models that are close to 100% accurate. Consequently, ML experts simply
    try to follow **model-centric approaches** such as tuning hyperparameters or using
    complex algorithms to boost model performance. Even if the model performance increases
    slightly, with the increase in complexity, explainability decreases. The lack
    of explainability increases the skepticism of the business stakeholders. Also,
    issues relating to data quality such as the presence of *data anomalies*, *data
    leakage*, *data drift*, and other issues, as discussed in [*Chapter 3*](B18216_03_ePub.xhtml#_idTextAnchor053),
    *Data-Centric Approaches*, significantly increase. *In that case, what do we do?*
  prefs: []
  type: TYPE_NORMAL
- en: The answer is to adopt a data-centric approach to explain the ML process. Using
    data-centric explainability methods such as **exploratory data analysis** (**EDA**),
    we can extract insights about the dataset such as any interesting patterns, correlations,
    monotonicity, or trends from the features used in the dataset. EDA and data analysis
    between the training data and the inference data also helps you to identify data
    quality issues. If there are issues in the dataset, it is always recommended that
    you inform the business stakeholder about the limitations of poor data quality
    and set the expectations correctly about the model performance. So, even if the
    model predictions are not correct, the business stakeholder will understand the
    limitations instead of doubting the ML system.
  prefs: []
  type: TYPE_NORMAL
- en: But *why don't we try out the other XAI frameworks and methods covered throughout
    this book*? *How would adopting a data-first approach for explainability help*?
    Well, you can and you should try out other relevant XAI methods if applicable,
    but data-centric explainability is always easier to explain to a non-technical
    user. Especially, with the *data profiling method*, as discussed in [*Chapter
    3*](B18216_03_ePub.xhtml#_idTextAnchor053), *Data-Centric Approaches*, we can
    identify the range of values of features present in the dataset for each category
    (if there is a classification problem) or each bin of the prediction variable
    (if there is a regression problem) and compare the model predictions with the
    profiled values. Simple comparisons with the profiled values are easier to understand
    as compared to complicated mathematical concepts such as *Shapley values* or other
    algorithms used in XAI frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason why data-centric approaches are preferable is because of the
    user's trust in historical data. Generally, it is observed that most stakeholders
    have more trust in historical data as compared to AI models. For example, in the
    spring season, if an AI weather forecasting model predicts the occurrence of snowfall,
    most end users would be hesitant to trust the prediction. That's because spring
    is always associated with sunshine and flowers blooming due to the observations
    throughout the world over many years. But if the model also indicates the occurrence
    of snowfall in the last few years during the same time or even indicates that
    there was snowfall in close proximity in the last few days, the user's trust would
    be greater. So, it is recommended that you, first, explore data-centric explainability
    and then look at other explainability methods for any industrial ML problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how data-centric XAI can be very close to
    the natural ways of providing explainability, thereby improving the ease of understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – The importance of a data-centric approach for explainability'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_10_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – The importance of a data-centric approach for explainability
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss interactive ML to boost the end user's trust.
  prefs: []
  type: TYPE_NORMAL
- en: Emphasizing IML for explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: IML is the paradigm of designing intelligent user interfaces to facilitate ML
    and AI algorithms with the help of user interactions. Using IML to steer the usage
    of ML systems to increase the trust of the end user has been an important research
    topic for the AI and HCI research community over the last few years. Many works
    of research literature recommend using IML to increase user engagement for AI
    systems. *Recent Research Advances on Interactive Machine Learning* by *Jiang
    et al*. ([https://arxiv.org/abs/1811.04548](https://arxiv.org/abs/1811.04548))
    talks about some of the significant progress that has been made in the field of
    IML and how it is closely associated with the increasing trust and transparency
    of ML algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'IML is another interesting approach that is used by the XAI community to explain
    ML models. Even in frameworks such as *DALEX* and *Explainerdashboards*, as covered
    in [*Chapter 9*](B18216_09_ePub.xhtml#_idTextAnchor172), *Other Popular XAI Frameworks*,
    providing interactive dashboards and web interfaces that end users can interact
    with to explore the data, model, and predictions are considered as a way for model
    explainability. IML helps the user in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Explore the dataset through graphs and visuals, thereby making it easier for
    the user to observe and remember key insights from the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gain more confidence about the ML systems, as the intelligent user interfaces
    allow the user to make changes and observe the outcome. It makes it easier for
    the user to figure out how the model behaves while considering any changes in
    input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, what-if analysis and local explainability are improved when interactive
    interfaces are provided.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IML gives more control to the user to explore the system, and IML usually considers
    a user-centric design process for providing customized interfaces tailor-made
    for a specific use case.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In short, IML improves the user experience and, thus, helps to boost the adoption
    of AI models. I would strongly recommend using interactive user interfaces as
    part of explainable ML systems along with serving model explainability using modularized
    web APIs. You can read the following article to find out more about the usefulness
    of IML for business problems: [https://hub.packtpub.com/what-is-interactive-machine-learning/](https://hub.packtpub.com/what-is-interactive-machine-learning/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the difference between conventional ML and
    IML:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10\. 4 – Comparing conventional ML with IML'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_10_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10\. 4 – Comparing conventional ML with IML
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding diagram, using IML, the end user can directly
    interact with the intelligent user interface to get predictions, explanations,
    and insights. Next, let's discuss the importance of prescriptive insights for
    explainable ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: Emphasizing prescriptive insights for explainability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prescriptive insight is a popular jargon used in data analysis. It means providing
    actionable recommendations derived from the dataset to achieve the desired outcome.
    It is often considered to be a catalyst in the entire process of data-driven decision-making.
    In the context of XAI, explanation methods such as *counterfactual examples*,
    *data-centric XAI*, and *what-if analysis* are prominently used for providing
    actionable suggestions to the user.
  prefs: []
  type: TYPE_NORMAL
- en: Along with counterfactuals, the concept of **actionable recourse in ML** is
    also used for generating prescriptive insights. **Actionable recourse** is the
    ability of a user to alter the prediction of an ML model by modifying the features
    that are actionable. But *how is it different from counterfactuals?* Actionable
    recourse can be considered to be an extension of the idea of counterfactual examples,
    which uses actionable features instead of all the features present in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now, *what do we mean by actionable features?* Considering a practical scenario,
    it is not feasible for us to change all the features present in a dataset in any
    direction to reach the desired outcome. For example, features such as *age*, *gender*,
    and *race* cannot be changed in any direction to obtain the desired output. Unfortunately,
    algorithms used for generating counterfactual examples do not consider the practical
    feasibility of changing a feature.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose that an ML model is being used to estimate the risk of diabetes.
    For a diabetic patient, if we want to use counterfactual examples to recommend
    how to reduce the risk of diabetes, it is not practically feasible for the patient
    to reduce their age by 10 years or change their gender to decrease the risk. So,
    these are non-actionable features. Even though theoretically altering these features
    can change the model prediction, it is not practical to change these features.
    Therefore, the concept of actionable recourse is more like a controlled counterfactual
    generation process that is applied to actionable features and considers a practically
    feasible boundary condition for the feature values.
  prefs: []
  type: TYPE_NORMAL
- en: To generate prescriptive insights, I would recommend that you use actionable
    recourse as it considers the practical feasibility and difficulty of altering
    a feature value to get the desired outcome. You can find more about actionable
    recourse from *Ustun et al.'s* work, *Actionable Recourse in Linear Classification*
    ([https://arxiv.org/abs/1809.06514](https://arxiv.org/abs/1809.06514)), along
    with their GitHub project at [https://github.com/ustunb/actionable-recourse](https://github.com/ustunb/actionable-recourse).
  prefs: []
  type: TYPE_NORMAL
- en: 'But *are prescriptive insights really necessary in XAI*? Well, the answer is
    *yes*! The following list of reasons explains why prescriptive insights are important
    in XAI:'
  prefs: []
  type: TYPE_NORMAL
- en: Prescriptive insights are actions suggested to the user to get the desired result.
    In most industrial use cases, explainability is incomplete if the user is unaware
    of how to reach their desired outcome.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating prescriptive insights is a proactive method for explaining the working
    of ML models. That's because it allows the user to take necessary proactive actions
    rather than trusting the passive explanations provided to them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It increases the user's faith in the system by giving a sense of control over
    the system. Using actionable explanations, the user is empowered to alter the
    model prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It increases the ability of business stakeholders for making data-driven decisions
    for the organization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the main reasons why you should always consider generating explanations
    that are actionable when designing explainable AI systems for industrial problems.
    *Figure 10.5* illustrates how prescriptive insights using XAI can provide actionable
    recommendations for the user to get their desired outcome:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – The importance of prescriptive insights for explainability'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_10_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – The importance of prescriptive insights for explainability
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have arrived at the end of this chapter. Let's summarize the topics
    discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focused on the best practices for designing explainable AI systems
    for industrial problems. In this chapter, we discussed the open challenges of
    XAI and the necessary design guidelines for explainable ML systems, considering
    the open challenges. We also highlighted the importance of considering data-centric
    approaches of explainability, IML, and prescriptive insights for designing explainable
    AI/ML systems.
  prefs: []
  type: TYPE_NORMAL
- en: If you are a technical expert, architect, or business leader responsible for
    using AI to solve industrial problems, this chapter has helped you to learn some
    of the most important guidelines for designing explainable AI/ML systems considering
    the open challenges in XAI. If you are a researcher in the field of AI or HCI,
    some of the open challenges discussed in the chapter could be interesting research
    topics to consider. Finding solutions to these challenges can lead to significant
    progress in the field of XAI.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover the principles of **End User-Centered Artificial
    Intelligence** to bridge the AI-end user gap.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For additional information about the topics covered in this chapter, please
    refer to the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Pitfalls of Explainable ML: An Industry Perspective*: [https://arxiv.org/abs/2106.07758](https://arxiv.org/abs/2106.07758)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Quantus framework in GitHub: [https://github.com/understandable-machine-intelligence-lab/Quantus](https://github.com/understandable-machine-intelligence-lab/Quantus)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Explanation in Artificial Intelligence: Insights from the Social Sciences*:
    [https://arxiv.org/pdf/1706.07269.pdf](https://arxiv.org/pdf/1706.07269.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network
    Explanations*: [https://arxiv.org/abs/2202.06861](https://arxiv.org/abs/2202.06861)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understanding Machines: Explainable AI* from *Accenture Labs*: [https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf](https://www.accenture.com/_acnmedia/pdf-85/accenture-understanding-machines-explainable-ai.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Actionable Recourse in Linear Classification* by *Ustun et al*:[https://arxiv.org/abs/1809.06514](https://arxiv.org/abs/1809.06514)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Actionable recourse in ML: [https://github.com/ustunb/actionable-recourse](https://github.com/ustunb/actionable-recourse)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Questioning the AI: Informing Design Practices for Explainable AI User Experiences*
    by *Liao et al*: [https://dl.acm.org/doi/10.1145/3313831.3376590](https://dl.acm.org/doi/10.1145/3313831.3376590)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Advances and Open Questions in Explainable AI (XAI): A practical perspective
    from an HCI researcher* by *Q. Vera Liao*: [http://qveraliao.com/aaai_panel.pdf](http://qveraliao.com/aaai_panel.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
