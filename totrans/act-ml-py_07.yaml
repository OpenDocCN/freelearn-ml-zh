- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Leveraging Active Learning for Big Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用主动学习处理大数据
- en: In this chapter, we will explore how to use **machine learning** (**ML**) to
    deal with big data, such as videos. The task of developing ML models for video
    analysis comes with its own set of unique challenges. Videos, being inherently
    large, pose significant hurdles in terms of efficient processing. Video analysis
    using ML has become an increasingly important technique across many industries
    and applications. From autonomous vehicles that rely on computer vision models
    to analyze road conditions in real-time video feeds, to security systems that
    can automatically detect suspicious activity, ML is revolutionizing what’s possible
    with video data. These models can automate time-consuming manual analysis and
    provide scalable video understanding. Implementing performant and scalable video
    analysis pipelines involves surmounting key hurdles such as an enormous amount
    of data labeling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何使用**机器学习**（**ML**）来处理大数据，例如视频。开发视频分析的ML模型带来了一系列独特的挑战。视频本身很大，在高效处理方面提出了重大障碍。使用ML进行视频分析已成为许多行业和应用的越来越重要的技术。从依赖计算机视觉模型分析实时视频流中的道路条件的自动驾驶汽车，到能够自动检测可疑活动的安全系统，ML正在改变视频数据所能实现的可能性。这些模型可以自动化耗时的人工分析并提供可扩展的视频理解。实现高效和可扩展的视频分析管道需要克服关键障碍，如大量数据标注。
- en: We will guide you through a cutting-edge ML method that will aid you in selecting
    the most informative frames for labeling, thereby enhancing the overall accuracy
    and efficacy of the analysis.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将引导您了解一种前沿的ML方法，这将帮助您选择最具信息量的帧进行标注，从而提高分析的整体准确性和效率。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Implementing ML models for video analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现视频分析的ML模型
- en: Selecting the most informative frames with Lightly
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Lightly选择最具信息量的帧
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, you will need to install the following packages:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要安装以下软件包：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will also need the following imports:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要以下导入：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, you need to create a Lightly account and set up your API token, as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要创建一个Lightly账户并设置您的API令牌，如下所示：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, you must set up the Lightly client to connect to the API:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您必须设置Lightly客户端以连接到API：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This demo was run on an AWS SageMaker notebook (`ml.g4dn.2xlarge instance`).
    This instance has a GPU and enough memory to run this demo because we need access
    to Docker, which is not possible in Google Colab.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个演示是在AWS SageMaker笔记本（`ml.g4dn.2xlarge实例`）上运行的。这个实例有一个GPU和足够的内存来运行这个演示，因为我们需要访问Docker，这在Google
    Colab中是不可能的。
- en: An Encord account ([https://app.encord.com/](https://app.encord.com/)) is also
    required if you want to send the selected frames to an annotation platform.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想将选定的帧发送到标注平台，还需要一个Encord账户（[https://app.encord.com/](https://app.encord.com/））。
- en: Implementing ML models for video analysis
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现视频分析的ML模型
- en: Active ML plays a transformative role in managing big data projects by strategically
    optimizing the data annotation process, thereby enhancing model performance with
    less manual effort. For instance, in large-scale image recognition tasks, such
    as identifying specific objects across millions of social media photos, active
    learning can significantly reduce the workload by pinpointing images that are
    most likely to refine the model’s capabilities. Similarly, in **natural language
    processing** (**NLP**) applications, dealing with vast amounts of text data from
    sources such as news articles, forums, and customer feedback, active ML helps
    in selectively annotating documents that add the most value to understanding complex
    language nuances or sentiments. This approach not only streamlines the effort
    required in annotating massive datasets but also ensures that models trained on
    such data are more accurate, efficient, and capable of handling the real-world
    variability inherent in big data sources. Extending this methodology to video
    analysis, active ML becomes even more pivotal due to the added complexity and
    volume of data within video content. Active ML can be leveraged to identify key
    frames or segments that significantly contribute to the model’s learning, dramatically
    reducing the annotation burden while ensuring comprehensive understanding and
    analysis of video data. This targeted approach in video projects not only conserves
    resources but also accelerates the development of sophisticated video analysis
    models that are capable of performing tasks such as activity recognition, event
    detection, and sentiment analysis with higher precision and efficiency.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃式机器学习（Active ML）在管理大数据项目方面发挥着变革性的作用，通过战略性地优化数据标注过程，从而以更少的手动努力提高模型性能。例如，在大型图像识别任务中，如从数百万社交媒体照片中识别特定对象，活跃学习可以通过定位最有可能提升模型能力的图像，显著减少工作量。同样，在**自然语言处理**（NLP）应用中，处理来自新闻文章、论坛和客户反馈等来源的大量文本数据，活跃式机器学习有助于选择性地标注对理解复杂语言细微差别或情感最有价值的文档。这种方法不仅简化了标注大规模数据集所需的工作量，而且确保了基于此类数据训练的模型更加准确、高效，并且能够处理大数据源中固有的现实世界变化。将这种方法扩展到视频分析中，由于视频内容中增加了复杂性和数据量，活跃式机器学习变得更加关键。活跃式机器学习可以用来识别对模型学习有显著贡献的关键帧或片段，在确保全面理解和分析视频数据的同时，大幅减少标注负担。这种在视频项目中采用的有针对性的方法不仅节省了资源，而且加速了复杂视频分析模型的发展，这些模型能够以更高的精度和效率执行活动识别、事件检测和情感分析等任务。
- en: In this section, we will explore how to leverage active ML for developing an
    ML model for videos. ML video analysis systems require implementing strategies
    to efficiently curate video frames. Videos are usually large files, and annotating
    all frames in the videos is impossible. Moreover, depending on the **frames per
    second** (**FPS**) rate, videos often contain a significant amount of duplicated
    data, which would be a waste of time and money to label. Common practice is to
    label at an FPS rate of 1, instead of 30 for example, to reduce the number of
    frames to label. However, this is not an optimal solution as it will lead to similar
    frames being annotated, an imbalance in the classes that are represented, and
    a lack of diversity in the data selected. Plus, many of the frames that will be
    labeled if such a pipeline is in place probably don’t need to be labeled in the
    first place because the model might already perform very well on some of those
    frames. Thus, labeling frames on which the model is confident and correct is a
    waste of time and money.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何利用活跃式机器学习开发用于视频的机器学习模型。机器学习视频分析系统需要实施策略来高效地整理视频帧。视频通常是大型文件，标注视频中的所有帧是不可能的。此外，根据**每秒帧数**（FPS）的速率，视频往往包含大量重复数据，如果对这样的数据进行标注，将浪费时间和金钱。常见的做法是以1
    FPS的速率进行标注，而不是像30 FPS那样，以减少需要标注的帧数。然而，这并不是一个最佳解决方案，因为它会导致相似帧被标注，代表类别的平衡性失衡，以及所选数据缺乏多样性。此外，如果实施这样的流程，将被标注的许多帧可能一开始就无需标注，因为模型可能已经在某些帧上表现得很出色。因此，标注模型有信心且正确的帧是浪费时间和金钱。
- en: 'In other words, it is infeasible to manually label all frames in video datasets
    for ML, making active learning crucial due to the following factors:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，手动标注视频数据集中的所有帧对于机器学习来说是不切实际的，由于以下因素，活跃学习变得至关重要：
- en: '**Data volume**: Video data consists of a large number of frames, which makes
    comprehensive manual labeling extremely time-consuming and expensive. For instance,
    labeling objects in all frames of just 10 minutes of a 30 FPS video would require
    labeling 18,000 images.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据量**：视频数据由大量帧组成，这使得全面的手动标注变得极其耗时且昂贵。例如，对30 FPS视频仅10分钟的每一帧进行标注就需要标注18,000张图片。'
- en: '**Redundancy**: Consecutive video frames are highly redundant as they contain
    almost identical content. It is inefficient to manually label this repetitive
    data.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**冗余**：连续的视频帧高度冗余，因为它们包含几乎相同的内容。手动标注这种重复数据是不高效的。'
- en: '**Cost**: The expense of hiring human labelers to meticulously annotate video
    frames would be exorbitant, rendering the majority of projects economically unfeasible.
    Labeling fees for just 10 hours of video could amount to thousands of dollars.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：雇佣人工标注员仔细标注视频帧的费用会非常高昂，使得大多数项目在经济上不可行。仅10小时的视频标注费用就可能高达数千美元。'
- en: This is where active ML is invaluable. It optimizes the labeling effort by identifying
    the most informative frames that are likely to improve the model, as we have seen
    in previous chapters. Human labelers can then focus exclusively on these high-value
    frames. By directing the labeling process to maximize performance gains, considerably
    fewer frames require manual annotation, making video ML viable and cost-effective.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是主动机器学习非常有价值的地方。它通过识别最有可能提高模型性能的最具信息量的帧来优化标注工作，正如我们在前面的章节中看到的。人工标注员可以专注于这些高价值帧。通过将标注过程引导至最大化性能提升，需要手动标注的帧数大大减少，使得视频机器学习变得可行且经济高效。
- en: In summary, exhaustive manual video data labeling is impractical and economically
    unfeasible. Active learning provides crucial optimization for labeling so that
    models can be trained to analyze video in a feasible, affordable, and adaptable
    manner.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，全面手动视频数据标注既不实际，在经济上也不可行。主动学习为标注提供了关键优化，使得模型能够以可行、经济和适应性强的方式分析视频。
- en: Now, let’s explore a real-world example with a commonly used active ML tool
    called **Lightly** ([https://www.lightly.ai/](https://www.lightly.ai/)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个使用常见主动机器学习工具**Lightly**（[https://www.lightly.ai/](https://www.lightly.ai/)）的实例来探讨现实世界的情况。
- en: Selecting the most informative frames with Lightly
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Lightly 选择最具信息量的帧
- en: In this section, we will use an active ML tool called **Lightly**. Lightly is
    a data curation tool that’s equipped with a web platform that enables users to
    choose the optimal subset of samples for maximizing model accuracy. Lightly’s
    algorithms can process substantial volumes of data, such as 10 million images
    or 10 thousand videos, in less than 24 hours.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用一个名为**Lightly**的主动机器学习工具。Lightly是一个数据整理工具，它配备了一个网络平台，使用户能够选择最优的样本子集以最大化模型精度。Lightly的算法可以在不到24小时内处理大量数据，例如1000万张图片或1万段视频。
- en: The web app allows users to explore their datasets using filters such as sharpness,
    luminance, contrast, file size, and more. They can then use these filters to explore
    correlations between these characteristics.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 网络应用程序允许用户使用诸如锐度、亮度、对比度、文件大小等过滤器来探索他们的数据集。然后，他们可以使用这些过滤器来探索这些特征之间的相关性。
- en: Users can also search for similar images or objects within the app and look
    into the embeddings (**principal component analysis** (**PCA**), **T-distributed
    stochastic neighbor embedding** (**TSNE**), and **uniform manifold approximation
    and projection** (**UMAP**)). Embeddings refers to vector representations of images
    that are learned by deep neural networks. They capture visual features and semantics
    of the images in a way that allows similarities and relationships between images
    to be analyzed. When images are passed through a convolutional neural network,
    the final layer before classification is typically a dense representation of the
    image features. This dense layer outputs a vector with hundreds or thousands of
    dimensions for each image. This vector is called an embedding. Images with similar
    features will have embeddings that are close or nearby when mapped in the embedding
    space. Images with very different features will be farther apart in the embedding
    space.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用户还可以在应用内搜索相似图像或对象，并查看嵌入（**主成分分析**（**PCA**），**T分布随机近邻嵌入**（**TSNE**），和**均匀流形近似与投影**（**UMAP**））。嵌入是指由深度神经网络学习到的图像的向量表示。它们以允许分析图像之间相似性和关系的方式捕捉图像的视觉特征和语义。当图像通过卷积神经网络时，分类前的最后一层通常是图像特征的密集表示。这个密集层为每个图像输出一个具有数百或数千维度的向量。这个向量被称为嵌入。在嵌入空间中映射时，具有相似特征的图像的嵌入将更接近或相邻。具有非常不同特征的图像在嵌入空间中将更远。
- en: 'There are a few techniques that can be used to visualize these high-dimensional
    embeddings in two or three dimensions for human analysis:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种技术可以将这些高维嵌入可视化到二维或三维空间，以便于人类分析：
- en: '**PCA**: PCA reduces the dimensions of embeddings down to 2D or 3D so that
    they can be plotted. It projects them onto the dimensions that capture the most
    variance. Images with similar prominent visual features will appear closer together
    after PCA projection.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PCA**：PCA将嵌入的维度降低到二维或三维，以便于绘制。它将它们投影到捕捉最大变差的维度上。在PCA投影后，具有相似显著视觉特征的图像将更靠近。'
- en: '**TSNE**: TSNE is a technique that represents high-dimensional embeddings in
    lower dimensions while keeping similar images close and dissimilar images apart.
    The 2D or 3D mappings attempt to model the local structure of the higher dimensional
    space.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TSNE**：TSNE是一种技术，在保持相似图像靠近和不相似图像分开的同时，将高维嵌入表示在较低维度。二维或三维映射试图模拟更高维空间中的局部结构。'
- en: '**UMAP**: UMAP is a more recent technique that can preserve global data structure
    in the projections better than TSNE in many cases. It maps images with similar
    embeddings nearby and dissimilar ones farther apart in the projection.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UMAP**：UMAP是一种更近期的技术，在很多情况下比TSNE更好地保留投影中的全局数据结构。它将具有相似嵌入的图像映射到投影附近，将不相似的图像映射得更远。'
- en: Embeddings capture image features and semantics in vectors. Techniques such
    as PCA, TSNE, and UMAP then project these high-dimensional vectors down to 2D
    or 3D so that they can be visualized and analyzed for similarity relationships
    between images. The Lightly app leverages these projections to enable image similarity
    searches.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入以向量的形式捕捉图像特征和语义。PCA、TSNE和UMAP等技术然后将这些高维向量投影到二维或三维，以便于可视化和分析图像之间的相似性关系。Lightly应用利用这些投影来实现图像相似性搜索。
- en: Using Lightly to select the best frames to label for object detection
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Lightly选择用于对象检测的最佳帧进行标记
- en: To select the best frames, we need to conduct a series of steps.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择最佳帧，我们需要进行一系列步骤。
- en: Dataset and pre-trained model
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集和预训练模型
- en: 'In this example, we will use a video of a dog running after a ball. We will
    only be using one video, as depicted in *Figure 5**.1*, for demo purposes. This
    video can be found in this book’s GitHub repository ([https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/blob/main/Chapter_5/videos/project_demo/dog_running_ball.mp4](https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/blob/main/Chapter_5/videos/project_demo/dog_running_ball.mp4)):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用一只狗追球的视频。出于演示目的，我们只会使用一个视频，如*图5.1*所示。这个视频可以在本书的GitHub仓库中找到（[https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/blob/main/Chapter_5/videos/project_demo/dog_running_ball.mp4](https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/blob/main/Chapter_5/videos/project_demo/dog_running_ball.mp4)）：
- en: '![Figure 5.1 – Video used for testing our Lightly demo](img/B21789_05_1.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 用于测试我们的Lightly演示的视频](img/B21789_05_1.jpg)'
- en: Figure 5.1 – Video used for testing our Lightly demo
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 用于测试我们的Lightly演示的视频
- en: Our main goal in this section is to select the most informative frames in this
    video using the different sampling strategies that Lightly offers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们的主要目标是使用 Lightly 提供的不同采样策略选择这个视频中最具信息量的帧。
- en: Our video called `dog_running_ball.mp4` is stored in a subfolder called `project_demo`
    under a folder called `videos`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的视频 `dog_running_ball.mp4` 存储在名为 `videos` 的文件夹下的一个名为 `project_demo` 的子文件夹中。
- en: 'Once, we have the video in our `videos/project_demo` folder, the next step
    is to load a pre-trained object detection model:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在 `videos/project_demo` 文件夹中有视频，下一步就是加载一个预训练的对象检测模型：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This pre-trained model from `ultralytics` supports 80 classes that we can visualize
    with the following command:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个来自 `ultralytics` 的预训练模型支持 80 个类别，我们可以使用以下命令进行可视化：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This returns the following output:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下输出：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In our case, we know we are dealing with a video of a dog running after a ball,
    so we will focus on the `dog` and `sports` `ball` classes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们这个例子中，我们知道我们正在处理一个狗追球的视频，因此我们将关注 `狗` 和 `运动` `球` 类别。
- en: 'Then, we must prepare some of the variables for our Lightly run, such as the
    path to the predictions output folder, the task’s name, and the important classes
    that we want to focus on in this run to improve our model (`dog` and `sports ball`):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须为 Lightly 运行准备一些变量，例如预测输出文件夹的路径、任务的名称以及我们在这轮运行中想要关注的、以改进模型的重要类别（`狗` 和
    `运动球`）：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Our classes here are `16` and `32`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里的类别是 `16` 和 `32`。
- en: Creating the required Lightly files
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建所需的 Lightly 文件
- en: 'We will be saving our inference predictions in a subfolder called `predictions`;
    our task name is `yolov8_demo_dog_detection`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把我们的推理预测保存到一个名为 `predictions` 的子文件夹中；我们的任务名称是 `yolov8_demo_dog_detection`：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we need to create all the configuration files that Lightly will use:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要创建 Lightly 将使用的所有配置文件：
- en: The `tasks.json` file, which specifies the name of our current task. A task
    name is the name of the corresponding subfolder in the folder predictions.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tasks.json` 文件，它指定了我们当前任务的名称。任务名称是预测文件夹中相应子文件夹的名称。'
- en: The `schema.json` file, which allows Lightly to know the format of the predictions.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schema.json` 文件，它允许 Lightly 了解预测的格式。'
- en: The metadata `schema.json` file, which contains the names of the videos in our
    `videos` folder.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含我们 `videos` 文件夹中视频名称的元数据 `schema.json` 文件。
- en: The code to create these configuration files can be found in the [*Chapter 5*](B21789_05.xhtml#_idTextAnchor069)
    section in this book’s GitHub repository ([https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5](https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5)).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 创建这些配置文件的代码可以在本书 GitHub 仓库的[*第5章*](B21789_05.xhtml#_idTextAnchor069)部分找到([https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5](https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5))。
- en: We can now run object detection inference using the pre-trained model we loaded
    earlier.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用我们之前加载的预训练模型运行对象检测推理。
- en: Inference
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理
- en: 'We’ll run object detection inference on our test video with a low confidence
    threshold of 0.3 as we want to have lower confidence scores. The code has been
    set up to handle more than one video in the subfolder. However, for testing purposes,
    we only have one video. We will skip predictions that are not part of the important
    classes:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以低置信度阈值 0.3 在我们的测试视频上运行对象检测推理，因为我们想要较低的置信度分数。代码已经设置好以处理子文件夹中的多个视频。然而，出于测试目的，我们只有一个视频。我们将跳过不属于重要类别的预测：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `populate_predictions_json_files` function is defined in the code in this
    book’s GitHub repository ([https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5](https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5)).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`populate_predictions_json_files` 函数定义在这本书的 GitHub 仓库中的代码里([https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5](https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5))。'
- en: 'Once we have run this code, we’ll receive the outputs of the inference run
    in the format supported by Lightly in the `predictions` subfolder, as shown in
    *Figure 5**.2*:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行了这段代码，我们将在 `predictions` 子文件夹中以 Lightly 支持的格式收到推理运行的输出，如图 *图 5**.2* 所示：
- en: '![Figure 5.2 – Lightly JSON predictions (snapshot)](img/B21789_05_2.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2 – Lightly JSON 预测（快照）](img/B21789_05_2.jpg)'
- en: Figure 5.2 – Lightly JSON predictions (snapshot)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – Lightly JSON 预测（快照）
- en: 'If we take a look at one of those files, the information the JSON files contain
    is the result of the inference. This includes the coordinates of the bounding
    boxes, along with the corresponding class ID and confidence score, as shown in
    *Figure 5**.3*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看这些文件之一，JSON 文件包含的信息是推理的结果。这包括边界框的坐标，以及相应的类别 ID 和置信度分数，如图 *图 5.3* 所示：
- en: '![Figure 5.3 – Prediction JSON file for the first frame in the format expected
    by Lightly](img/B21789_05_3.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – Lightly 预测 JSON 文件格式，用于第一帧](img/B21789_05_3.jpg)'
- en: Figure 5.3 – Prediction JSON file for the first frame in the format expected
    by Lightly
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – Lightly 预测 JSON 文件格式，用于第一帧
- en: We are now ready to schedule the active Lightly ML run.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好安排活动 Lightly 机器学习运行。
- en: Schedule the active ML run
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安排活动机器学习运行
- en: 'We can register the Lightly worker using the following code, which returns
    the ID of our Lightly worker:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码注册 Lightly 工作进程，该代码返回我们的 Lightly 工作进程的 ID：
- en: Note
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If a worker with this name already exists, the ID of the existing worker will
    be returned.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果已存在具有此名称的工作进程，则将返回现有工作进程的 ID。
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Lightly uses Docker to run active ML with the `lightly/worker:latest` image.
    It can be pulled using the command line:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Lightly 使用 Docker 通过 `lightly/worker:latest` 镜像运行活动机器学习。可以使用命令行拉取：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Docker is an open platform for developing, shipping, and running applications
    within software containers. It allows code to be packaged with all its dependencies
    and libraries into a standardized unit for software development. Using Docker
    containers eliminates compatibility issues that arise from differences between
    environments. In short, it enables easy replicability when running scripts because
    the environment in the Docker image is already set up with the correct installed
    packages.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 是一个开源平台，用于在软件容器内开发、运输和运行应用程序。它允许将代码及其所有依赖项和库打包成一个标准化的单元，用于软件开发。使用 Docker
    容器消除了由环境差异引起的不兼容性问题。简而言之，它使得在运行脚本时易于复制，因为 Docker 镜像中的环境已经配置了正确的已安装包。
- en: 'Next, we need to schedule our active ML run. This process involves several
    steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要安排我们的活动机器学习运行。这个过程涉及几个步骤：
- en: 'Create a Lightly dataset called `demo_dataset`:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `demo_dataset` 的 Lightly 数据集：
- en: '[PRE12]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Set up the data sources by using the `project_demo` project as the input or
    output locations for data. In our case, we are using the local storage option
    ([https://docs.lightly.ai/docs/local-storage](https://docs.lightly.ai/docs/local-storage))
    for demo purposes, but ideally, you should use the cloud service option ([https://docs.lightly.ai/docs/cloud-storage](https://docs.lightly.ai/docs/cloud-storage)),
    which uses either AWS, Azure, or GCP:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `project_demo` 项目作为数据输入或输出位置来设置数据源。在我们的案例中，我们使用本地存储选项 ([https://docs.lightly.ai/docs/local-storage](https://docs.lightly.ai/docs/local-storage))
    进行演示，但理想情况下，您应该使用云服务选项 ([https://docs.lightly.ai/docs/cloud-storage](https://docs.lightly.ai/docs/cloud-storage))，该选项使用
    AWS、Azure 或 GCP：
- en: '[PRE13]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Schedule the active ML run with the selected strategies we want to use: a strategy
    to find diverse objects, a strategy to balance the class ratios, and a strategy
    to use a prediction score for the object’s frequencies and least confident results.
    We are sampling five samples in this example and are trying to reach a 50/50 balance
    between the `dog` and `sports` `ball` classes:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安排我们想要使用的活动机器学习运行计划：一个用于寻找不同对象的策略，一个用于平衡类别比例的策略，以及一个用于使用预测分数来表示对象的频率和最不自信的结果的策略。在这个例子中，我们采样了五个样本，并试图在`狗`和`运动``球`类别之间达到
    50/50 的平衡：
- en: '[PRE14]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, organize the local files so that they match what Lightly is expecting:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，组织本地文件，使其与 Lightly 期望的格式相匹配：
- en: '[PRE15]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When we take a look at the folders, we’ll see the following structure:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们查看文件夹时，我们会看到以下结构：
- en: '![Figure 5.4 – Folders organized for Lightly local storage](img/B21789_05_4.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 为 Lightly 本地存储组织的文件夹](img/B21789_05_4.jpg)'
- en: Figure 5.4 – Folders organized for Lightly local storage
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 为 Lightly 本地存储组织的文件夹
- en: We are now ready to start the run.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好开始运行。
- en: Starting the worker and the active ML run
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动工作进程和活动机器学习运行
- en: 'We need to create a function that will be used to initiate the worker with
    the organized folders that are mounted in the Docker container:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个函数，该函数将用于在 Docker 容器中挂载的已组织文件夹中启动工作进程：
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s start the run:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始运行：
- en: '[PRE17]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can check the progress of our run on the Lightly platform, as shown in *Figure
    5**.5*, as well as by looking at the output of the previous code:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在 Lightly 平台上检查我们运行的进度，如图 *图 5.5* 所示，以及通过查看前一段代码的输出：
- en: '![ Figure 5.5 – Lightly view of the active ML run](img/B21789_05_5.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – Lightly 活动机器学习运行的视图](img/B21789_05_5.jpg)'
- en: Figure 5.5 – Lightly view of the active ML run
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – Lightly 活动机器学习运行的视图
- en: Once completed, we have access to a detailed report of the run and all the necessary
    logs, such as the memory log and default log. We can also view our dataset in
    the Lightly web application ([https://app.lightly.ai/dataset](https://app.lightly.ai/dataset)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以访问运行详细的报告和所有必要的日志，例如内存日志和默认日志。我们还可以在 Lightly 网络应用程序中查看我们的数据集（[https://app.lightly.ai/dataset](https://app.lightly.ai/dataset)）。
- en: 'Let’s explore our results. In *Figure 5**.6*, we can see that we have several
    new subfolders, including `frames` and `crops`. Those folders contain the selected
    frames and crops (cropped bounding boxes):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索我们的结果。在 *图 5*.6 中，我们可以看到我们有几个新的子文件夹，包括 `frames` 和 `crops`。这些文件夹包含所选的帧和裁剪（裁剪边界框）：
- en: '![Figure 5.6 – Resulting folder after the Lightly active ML run](img/B21789_05_6.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – Lightly 活动机器学习运行后的结果文件夹](img/B21789_05_6.jpg)'
- en: Figure 5.6 – Resulting folder after the Lightly active ML run
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – Lightly 活动机器学习运行后的结果文件夹
- en: 'Let’s visualize the selected frames:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化所选的帧：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This returns the images shown in *Figure 5**.7*:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了 *图 5*.7 中显示的图像：
- en: '![Figure 5.7 – The five most informative frames that were chosen by the Lightly
    active ML run](img/B21789_05_7.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – Lightly 活动机器学习运行选择的五个最有信息量的帧](img/B21789_05_7.jpg)'
- en: Figure 5.7 – The five most informative frames that were chosen by the Lightly
    active ML run
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – Lightly 活动机器学习运行选择的五个最有信息量的帧
- en: We can also explore the report, which gives us a lot of information about our
    new subset of samples.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以探索报告，它为我们提供了关于我们新样本子集的大量信息。
- en: 'The report is saved under `lightly/project_demo/.lightly/runs/run_id`. We can
    view the run ID and then copy the `report.pdf` file locally, as shown in *Figure
    5**.8*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 报告保存在 `lightly/project_demo/.lightly/runs/run_id` 下。我们可以查看运行 ID，然后像 *图 5*.8
    中所示的那样在本地复制 `report.pdf` 文件：
- en: '![Figure 5.8 – Copying the report document](img/B21789_05_8.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 复制报告文档](img/B21789_05_8.jpg)'
- en: Figure 5.8 – Copying the report document
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 复制报告文档
- en: There is a lot of information in the report, so we will only focus on certain
    sections.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 报告中信息量很大，所以我们只关注某些部分。
- en: 'First, let’s take a quick look at what we started with and what we ended up
    with. As shown in *Figure 5**.9*, we had 290 frames and one video and created
    a subset of five frames only, which corresponds to our request for five samples.
    Note that the sample ratio can be selected with `n_samples`, but it can also be
    selected as a percentage of the data with `proportionSamples`. So, to select 30%
    of the data, we can set the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们快速看一下我们开始时的内容和最终的结果。如图 *图 5*.9 所示，我们最初有 290 帧和一个视频，只创建了一个包含五个帧的子集，这与我们请求的五个样本相对应。请注意，可以通过
    `n_samples` 选择样本比例，也可以通过 `proportionSamples` 选择数据百分比。因此，要选择 30% 的数据，我们可以设置以下内容：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can do this instead of running the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样做，而不是运行以下操作：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here’s the output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 5.9 – Lightly report – dataset information](img/B21789_05_9.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – Lightly 报告 – 数据集信息](img/B21789_05_9.jpg)'
- en: Figure 5.9 – Lightly report – dataset information
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – Lightly 报告 – 数据集信息
- en: 'Now, let’s examine the embeddings plots, as shown in *Figure 5**.10*. Upon
    examining both the UMAP and PCA embeddings plots, we’ll see the absence of distinct
    clusters, suggesting a lack of consistency among the frames. This inconsistency
    can be attributed to the video’s dynamic filming conditions, including varying
    angles, distances to the dog, and changes in lighting due to shaded and non-shaded
    areas being encountered while moving around and following the dog. These factors
    contribute to the diverse visual input captured in the frames:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们检查嵌入图，如图 *图 5*.10 所示。在检查 UMAP 和 PCA 嵌入图后，我们会看到没有明显的聚类，这表明帧之间缺乏一致性。这种不一致性可以归因于视频的动态拍摄条件，包括不同的角度、与狗的距离以及由于在移动和跟随狗的过程中遇到有阴影和无阴影区域而造成的照明变化。这些因素导致在帧中捕获了多样化的视觉输入：
- en: '![Figure 5.10 – Lightly report – embeddings plots](img/B21789_05_10.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10 – Lightly 报告 – 嵌入图](img/B21789_05_10.jpg)'
- en: Figure 5.10 – Lightly report – embeddings plots
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – 轻量级报告 – 嵌入图
- en: 'With *Figure 5**.11*, we have a better understanding of which frames were selected
    by the active ML algorithm. We can see that it did a good job of selecting frames
    from diverse locations in the embeddings space:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 *图 5*.11，我们更好地理解了活动机器学习算法选择的帧。我们可以看到它很好地从嵌入空间的不同位置选择了帧：
- en: '![Figure 5.11 – Lightly report – embeddings plots selected frames](img/B21789_05_11.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11 – 轻量级报告 – 嵌入图选定帧](img/B21789_05_11.jpg)'
- en: Figure 5.11 – Lightly report – embeddings plots selected frames
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 轻量级报告 – 嵌入图选定帧
- en: 'In *Figure 5**.12*, we can observe that the class distribution has been effectively
    balanced: the `dog` class accounts for 54.5%, while the `sports ball` class accounts
    for 45.5% of the selection, closely aligning with our intended 50/50 class balance
    ratio. This balance was achieved thanks to our configured class balance ratios.
    Nonetheless, attaining such equilibrium often presents challenges, particularly
    when one class significantly outnumbers the others in the dataset:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.12* 中，我们可以观察到类别分布已经得到了有效平衡：`狗` 类占 54.5%，而 `运动球` 类占 45.5% 的选择，与我们的 50/50
    类别平衡比率非常接近。这种平衡得益于我们配置的类别平衡比率。尽管如此，达到这种平衡往往具有挑战性，尤其是在数据集中一个类别显著多于其他类别时：
- en: '![Figure 5.12 – Lightly report – class distribution](img/B21789_05_12.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.12 – 轻量级报告 – 类别分布](img/B21789_05_12.jpg)'
- en: Figure 5.12 – Lightly report – class distribution
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 轻量级报告 – 类别分布
- en: 'Now, we can explore some of the model predictions, as shown in *Figure 5**.13*.
    Here, we have several examples of predictions with different numbers of detections.
    Overall, the model demonstrates strong performance, consistently identifying the
    dog within the frames. However, it appears to struggle more with detecting the
    ball. Notably, the ball was recognized in one of the sample frames, which is an
    encouraging sign. This discrepancy in detection accuracy likely comes from the
    nature of the ball used in the test; it deviates from the conventional sports
    balls, such as tennis or soccer balls, on which the original model was trained.
    This context helps explain the observed variations in model performance and can
    be fixed by labeling the ball and re-training the model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以探索一些模型预测结果，如图 *图 5.13* 所示。在这里，我们有不同检测数量的预测示例。总体而言，模型表现出强大的性能，始终在帧内识别出狗。然而，它似乎在检测球方面遇到了更多困难。值得注意的是，球在其中一个样本帧中被识别出来，这是一个令人鼓舞的迹象。这种检测准确性的差异很可能是由于测试中使用的球的本性；它偏离了原始模型训练时使用的传统运动球，如网球或足球。这个上下文有助于解释观察到的模型性能变化，可以通过标记球并重新训练模型来修复：
- en: '![Figure 5.13 – Lightly report – examples of the model’s predictions](img/B21789_05_13.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13 – 轻量级报告 – 模型预测示例](img/B21789_05_13.jpg)'
- en: Figure 5.13 – Lightly report – examples of the model’s predictions
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – 轻量级报告 – 模型预测示例
- en: 'Finally, *Figure 5**.14* shows the selected frames throughout the video. Each
    selected frame corresponds to a vertical line, and we can confirm that there are
    five lines for our five selected frames:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*图 5.14* 展示了视频中的选定帧。每个选定的帧对应一条垂直线，我们可以确认有五条线对应我们选定的五个帧：
- en: '![Figure 5.14 – Lightly report – video sampling densities](img/B21789_05_14.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – 轻量级报告 – 视频采样密度](img/B21789_05_14.jpg)'
- en: Figure 5.14 – Lightly report – video sampling densities
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – 轻量级报告 – 视频采样密度
- en: Now that we have the most informative frames we wish to label, we can send them
    to the annotation platform used in the project, such as Encord Annotate ([https://docs.encord.com/docs/annotate-overview](https://docs.encord.com/docs/annotate-overview))
    or Roboflow ([https://roboflow.com/](https://roboflow.com/)), as presented in
    [*Chapter 3*](B21789_03.xhtml#_idTextAnchor040), *Managing the Human in the Loop*.
    In this example, we will use Encord Annotate because it offers a feature to visualize
    selected frames as videos.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了希望标记的最具信息量的帧，我们可以将它们发送到项目中使用的注释平台，例如 Encord Annotate ([https://docs.encord.com/docs/annotate-overview](https://docs.encord.com/docs/annotate-overview))
    或 Roboflow ([https://roboflow.com/](https://roboflow.com/))，如 *第 3 章* 中所述，*管理闭环中的人类*。在这个例子中，我们将使用
    Encord Annotate，因为它提供了一个将选定帧可视化为视频的功能。
- en: Before running this code, you need to create an Encord SSH private key by following
    the documentation ([https://docs.encord.com/reference/authentication-sdk](https://docs.encord.com/reference/authentication-sdk))
    they provide.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此代码之前，您需要根据他们提供的文档创建 Encord SSH 私钥 ([https://docs.encord.com/reference/authentication-sdk](https://docs.encord.com/reference/authentication-sdk))。
- en: 'Then, you can authenticate using the following code:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用以下代码进行验证：
- en: '[PRE21]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The next step is to create an Encord dataset ([https://docs.encord.com/docs/annotate-datasets](https://docs.encord.com/docs/annotate-datasets))
    with the name of our project – that is, `project_demo`. This Encord dataset is
    the data that we want to label:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个名为我们项目名称的 Encord 数据集（[https://docs.encord.com/docs/annotate-datasets](https://docs.encord.com/docs/annotate-datasets)）
    – 即 `project_demo`。这个 Encord 数据集是我们想要标注的数据：
- en: '[PRE22]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will return a dictionary with the `title`, `type`, `dataset_hash`, `user_hash`,
    and `backing_folder_uuid` values of the created dataset. Here, we are using Encord’s
    cloud storage, but you could also use custom cloud storage. For example, if you’re
    using AWS S3, then you can use `StorageLocation.AWS` instead.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个字典，包含创建的数据集的 `title`、`type`、`dataset_hash`、`user_hash` 和 `backing_folder_uuid`
    值。在这里，我们使用 Encord 的云存储，但你也可以使用自定义云存储。例如，如果你使用 AWS S3，那么你可以使用 `StorageLocation.AWS`
    代替。
- en: 'Now, we can query the dataset hash because it will be necessary when uploading
    the images:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以查询数据集哈希，因为在上传图像时将需要它：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we can populate the dataset with our selected frames:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以用我们选择的帧填充数据集：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This will return a dictionary containing the `data_hash`, `backing_item_uuid`,
    and `title` values of the uploaded data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个包含上传数据的 `data_hash`、`backing_item_uuid` 和 `title` 值的字典。
- en: Note that we used `create_video=True` so that we can create a compressed video
    from the image groups; these are called image sequences ([https://docs.encord.com/docs/annotate-supported-data#image-sequences](https://docs.encord.com/docs/annotate-supported-data#image-sequences)).
    This is beneficial when visualizing the frames as it helps maintain the temporal
    context of the videos and is usually very helpful for the labelers. It also allows
    the labelers to use features such as automated labeling ([https://docs.encord.com/docs/automated-labeling](https://docs.encord.com/docs/automated-labeling)),
    which includes interpolation. This helps speed up the labeling process considerably
    by automatically estimating the location of labels between two manually labeled
    frames.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用了 `create_video=True`，这样我们就可以从图像组创建一个压缩视频；这些被称为图像序列（[https://docs.encord.com/docs/annotate-supported-data#image-sequences](https://docs.encord.com/docs/annotate-supported-data#image-sequences)）。当可视化帧时，这有助于保持视频的时间上下文，并且通常对标注者非常有帮助。它还允许标注者使用诸如自动标注（[https://docs.encord.com/docs/automated-labeling](https://docs.encord.com/docs/automated-labeling)）等特性，包括插值。这通过自动估计两个手动标注帧之间的标签位置，大大加快了标注过程。
- en: 'At this point, we can view our dataset on the Encord web application, in the
    `Index/Datasets` section ([https://app.encord.com/datasets](https://app.encord.com/datasets)),
    as shown in *Figure 5**.15*. We can observe that the images are saved as `img_sequence`,
    which means that they will be displayed as a video:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以在 Encord 网络应用程序的 `Index/Datasets` 部分查看我们的数据集（[https://app.encord.com/datasets](https://app.encord.com/datasets)），如图
    *图 5*.15 所示。我们可以观察到图像被保存为 `img_sequence`，这意味着它们将以视频的形式显示：
- en: '![Figure 5.15 – Encord dataset with our five selected frames saved as an image
    sequence](img/B21789_05_15.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 使用我们选择的五个帧保存为图像序列的 Encord 数据集](img/B21789_05_15.jpg)'
- en: Figure 5.15 – Encord dataset with our five selected frames saved as an image
    sequence
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 使用我们选择的五个帧保存为图像序列的 Encord 数据集
- en: 'In Encord, we define the ontology ([https://docs.encord.com/reference/ontologies-sdk](https://docs.encord.com/reference/ontologies-sdk))
    that we want to use for this annotation project, as shown in *Figure 5**.16*.
    We introduced the concept of ontologies in the *Designing interactive learning
    systems and workflows* section of [*Chapter 3*](B21789_03.xhtml#_idTextAnchor040),
    *Managing the Human in* *the Loop*:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Encord 中，我们定义了用于此标注项目的本体（[https://docs.encord.com/reference/ontologies-sdk](https://docs.encord.com/reference/ontologies-sdk)），如图
    *图 5*.16 所示。我们在 *第 3 章* 的 *设计交互式学习系统和工作流程* 部分介绍了本体的概念：
- en: '![Figure 5.16 – Created ontology in Encord with our two classes, dog and ball](img/B21789_05_16.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – 在 Encord 中创建的包含我们的两个类别，狗和球的本体](img/B21789_05_16.jpg)'
- en: Figure 5.16 – Created ontology in Encord with our two classes, dog and ball
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – 在 Encord 中创建的包含我们的两个类别，狗和球的本体
- en: 'From the page visualized in *Figure 5**.16*, we can copy the ontology ID and
    use it to create the Encord Annotate project ([https://docs.encord.com/docs/annotate-overview](https://docs.encord.com/docs/annotate-overview)):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 从 *图 5*.16 中可视化的页面，我们可以复制本体 ID 并使用它来创建 Encord Annotate 项目（[https://docs.encord.com/docs/annotate-overview](https://docs.encord.com/docs/annotate-overview)）：
- en: '[PRE25]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We’ll see the following:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到以下内容：
- en: '![Figure 5.17 – Encord Annotate project of our project_demo samples](img/B21789_05_17.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – Encord Annotate 项目中的 project_demo 样本](img/B21789_05_17.jpg)'
- en: Figure 5.17 – Encord Annotate project of our project_demo samples
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – Encord Annotate 项目中的 project_demo 样本
- en: 'Our data is now ready to be labeled. We can view our Encord Annotate project
    in the `Annote/Annotation projects` section ([https://app.encord.com/projects](https://app.encord.com/projects))
    of the web app. Then, in the `project_demo` annotation project, we can view our
    selected frames as a video, as shown in *Figure 5**.18*:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在已准备好进行标注。我们可以在 Web 应用的 `Annote/标注项目` 部分查看我们的 Encord Annotate 项目（[https://app.encord.com/projects](https://app.encord.com/projects)）。然后，在
    `project_demo` 标注项目中，我们可以将所选帧作为视频查看，如图 *5.18* 所示：
- en: '![Figure 5.18 – Labeling view of the selected frames'' image sequence in Encord](img/B21789_05_18.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18 – Encord 中所选帧图像序列的标注视图](img/B21789_05_18.jpg)'
- en: Figure 5.18 – Labeling view of the selected frames' image sequence in Encord
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 – Encord 中所选帧图像序列的标注视图
- en: 'In the view presented in *Figure 5**.18*, we can see that all the frames are
    presented as a video with a slider that the users can use to navigate the frames.
    There are also the two classes (dog and ball) that we defined in our ontology
    in the **Classes** section that labelers can select to label the frames, as shown
    in *Figure 5**.19*:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 5.18* 所示的视图中，我们可以看到所有帧都作为视频呈现，用户可以使用滑块来浏览帧。还有我们在本体论中的 **类别** 部分定义的两个类别（狗和球），标注者可以选择来标注帧，如图
    *5.19* 所示：
- en: '![Figure 5.19 – Example of annotations on one of the selected frames](img/B21789_05_19.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.19 – 在所选帧上标注的示例](img/B21789_05_19.jpg)'
- en: Figure 5.19 – Example of annotations on one of the selected frames
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 – 在所选帧上标注的示例
- en: From this page, labelers can use the automated labeling feature mentioned previously
    and easily label the objects; when they are done, they can submit the results
    for review.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个页面，标注者可以使用之前提到的自动标注功能轻松标注对象；完成标注后，他们可以提交结果以供审查。
- en: 'You can also access the selected frames using the following code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用以下代码访问所选帧：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This returns the following output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下输出：
- en: '[PRE27]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This is correct! Now, let’s print the results so that we have a better understanding:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是正确的！现在，让我们打印结果，以便我们更好地理解：
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This returns the following output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回以下输出：
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: These URLs can be used to send the selected frames to the annotations platform
    used for the project. Note that since we are using local storage for the demo,
    the data isn’t easily accessible to annotation platforms and cloud services should
    be used instead. The local data can also be visualized in Lightly by serving `localhost`
    ([https://docs.lightly.ai/docs/local-storage#optional-view-local-data-in-lightly-platform](https://docs.lightly.ai/docs/local-storage#optional-view-local-data-in-lightly-platform)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 URL 可以用来将所选帧发送到用于项目的标注平台。请注意，由于我们在演示中使用本地存储，数据不容易被标注平台和云服务访问，因此应使用云服务。本地数据也可以通过在
    Lightly 中提供 `localhost` 来可视化（[https://docs.lightly.ai/docs/local-storage#optional-view-local-data-in-lightly-platform](https://docs.lightly.ai/docs/local-storage#optional-view-local-data-in-lightly-platform)）。
- en: In this section, we used Lightly to select the most informative frames in a
    test video of a dog running after a ball using several strategies. These strategies
    included finding diverse objects, balancing class ratios, using prediction scores
    for object frequencies, and considering the least confident results. Lightly has
    a lot of other features to improve these results and well-organized documentation
    ([https://docs.lightly.ai/](https://docs.lightly.ai/)).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用 Lightly 通过几种策略选择了一只狗追球的测试视频中最具信息量的帧。这些策略包括寻找多样化的对象、平衡类别比率、使用预测分数来计算对象频率，以及考虑最不自信的结果。Lightly
    还有很多其他功能来改进这些结果，并且有很好的组织文档（[https://docs.lightly.ai/](https://docs.lightly.ai/)）。
- en: Next up, we will talk about how we can use Lightly for **self-supervised** **learning**
    (**SSL**).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何使用 Lightly 进行**自监督学习**（**SSL**）。
- en: SSL with active ML
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有主动机器学习的 SSL
- en: Lightly offers other useful features, such as **SSL**, which allows users to
    fine-tune an SSL model on their data before embedding the images. SSL algorithms
    exploit the structure and context within unlabeled images or videos to generate
    surrogate supervised signals that enable models to discover powerful visual representations
    on their own. For example, models may be trained to recognize spatial patterns,
    colorizations, rotations, or temporal ordering as pretext objectives before fine-tuning
    downstream tasks. In essence, SSL allows models to take advantage of vast volumes
    of unlabeled video and images to uncover useful features and patterns within the
    data itself, avoiding reliance on manual labeling, which can be infeasible at
    scale. The models automatically supervise their feature learning through carefully
    designed pretext tasks while harnessing aspects such as temporal continuity in
    a video. So, this Lightly feature can be extremely beneficial when developing
    models for a specific domain, such as medical videos. The additional training
    step improves the quality of the embeddings because the model can adapt to the
    specific domain without requiring more labeling.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Lightly提供了其他有用的功能，例如**SSL**，它允许用户在嵌入图像之前，在他们的数据上微调SSL模型。SSL算法利用未标记图像或视频中的结构和上下文来生成替代监督信号，使模型能够自行发现强大的视觉表示。例如，模型可能被训练在微调下游任务之前，以识别空间模式、着色、旋转或时间顺序作为先验目标。本质上，SSL允许模型利用大量的未标记视频和图像来发现数据本身中的有用特征和模式，避免对人工标记的依赖，这在规模上可能是不切实际的。模型通过精心设计的先验任务自动监督其特征学习，同时利用视频中的时间连续性等特性。因此，当开发特定领域的模型时，例如医学视频，这个Lightly功能可以非常有益。额外的训练步骤提高了嵌入的质量，因为模型可以在不要求更多标记的情况下适应特定领域。
- en: 'Enabling SSL is simple and only requires adding a couple of lines to our code
    in the `worker_config` and `lightly_config` subdictionaries, both of which are
    part of the `scheduled_run_id` dictionary:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 启用SSL很简单，只需在我们的`worker_config`和`lightly_config`子字典中添加几行代码，这两个子字典都是`scheduled_run_id`字典的一部分：
- en: '[PRE30]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, we configured the active ML run to perform SSL training for 10 epochs
    on the CPU before generating the embeddings.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们配置了主动的机器学习运行，在生成嵌入之前，在CPU上执行10个时期的SSL训练。
- en: Now, let’s take a look at our outputs. The frames that were selected are mostly
    different from the ones we selected previously – that is, `dog_running_ball-024-mp4.png`,
    `dog_running_ball-101-mp4.png`, `dog_running_ball-033-mp4.png`, `dog_running_ball-224-mp4.png`,
    and `dog_running_ball-049-mp4.png` – compared to `dog_running_ball-024-mp4.png`,
    `dog_running_ball-183-mp4.png`, `dog_running_ball-151-mp4.png`, `dog_running_ball-194-mp4.png`,
    and `dog_running_ball-180-mp4.png`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的输出。被选中的帧大多与之前我们选择的帧不同——即`dog_running_ball-024-mp4.png`、`dog_running_ball-101-mp4.png`、`dog_running_ball-033-mp4.png`、`dog_running_ball-224-mp4.png`和`dog_running_ball-049-mp4.png`——与`dog_running_ball-024-mp4.png`、`dog_running_ball-183-mp4.png`、`dog_running_ball-151-mp4.png`、`dog_running_ball-194-mp4.png`和`dog_running_ball-180-mp4.png`相比。
- en: 'So, only frame `024` was selected again. *Figure 5**.20* shows the five most
    informative frames:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，只有帧`024`再次被选中。*图5.20*显示了五个最有信息量的帧：
- en: '![Figure 5.20 – Selecting the five most informative frames via a Lightly active
    ML SSL run](img/B21789_05_20.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图5.20 – 通过Lightly主动机器学习SSL运行选择五个最有信息量的帧](img/B21789_05_20.jpg)'
- en: Figure 5.20 – Selecting the five most informative frames via a Lightly active
    ML SSL run
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 – 通过Lightly主动机器学习SSL运行选择五个最有信息量的帧
- en: The frame that was selected in both the SSL and non-SSL runs is highlighted
    at the borders.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在SSL和非SSL运行中都被选中的帧在边缘被突出显示。
- en: '*Figure 5**.20* shows that the addition of the SSL step has noticeably altered
    the selection criteria for frames. Predominantly, frames chosen post-SSL tend
    to feature the dog at a further distance, contrasting sharply with those selected
    without SSL, which mainly consisted of close-ups showcasing the dog holding the
    ball. This shift underscores the impact of SSL on the model’s focus and frame
    selection preferences:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.20*显示，添加SSL步骤明显改变了帧的选择标准。主要的是，SSL之后选择的帧往往显示狗在更远的距离，与未使用SSL选择的帧形成鲜明对比，后者主要是特写镜头，展示了狗拿着球。这种转变突出了SSL对模型关注点和帧选择偏好的影响：'
- en: '![Figure 5.21 – Lightly report – comparing video sampling densities between
    the non-SSL run (top image) and the SSL run (bottom image)](img/B21789_05_21.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图5.21 – Lightly报告 – 比较非SSL运行（顶部图像）和SSL运行（底部图像）之间的视频采样密度](img/B21789_05_21.jpg)'
- en: Figure 5.21 – Lightly report – comparing video sampling densities between the
    non-SSL run (top image) and the SSL run (bottom image)
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21 – 简要报告 – 比较非SSL运行（顶部图像）和SSL运行（底部图像）之间的视频采样密度
- en: 'Upon examining the new embedding plots shown in *Figure 5**.21*, it is evident
    that the embeddings model performs better in clustering the frames. Despite this
    improvement, the clusters are not yet sharply defined, suggesting that extending
    the number of epochs in the SSL training could further refine this aspect:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查*图5.21**.21*中显示的新嵌入图后，很明显，嵌入模型在聚类帧方面表现更好。尽管有这种改进，但聚类仍然没有明确界定，这表明在SSL训练中增加epoch的数量可以进一步细化这一方面：
- en: '![Figure 5.22 – New embeddings plots with the SSL active ML run](img/B21789_05_22.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图5.22 – SSL活跃机器学习运行的新嵌入图](img/B21789_05_22.jpg)'
- en: Figure 5.22 – New embeddings plots with the SSL active ML run
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22 – SSL活跃机器学习运行的新嵌入图
- en: Incorporating the Lightly SSL feature into the ML pipeline is a straightforward
    addition that can provide significant benefits for field-specific data. By leveraging
    this advanced technique, we observed that the embeddings that were generated by
    the model were notably improved after undergoing SSL in our test. This enhancement
    not only enhances the overall performance but also ensures that the pipeline is
    optimized to handle the unique characteristics of the data being processed.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 将Lightly SSL功能纳入机器学习管道是一个简单添加，可以为特定领域的数据提供显著的好处。通过利用这项高级技术，我们观察到在测试中经过SSL处理的模型生成的嵌入得到了显著改善。这种增强不仅提高了整体性能，还确保了管道针对正在处理的数据的独特特征进行了优化。
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to use Lightly to efficiently select the most
    informative frames in videos to improve object detection models using diverse
    sampling strategies. We also saw how to send these selected frames to the labeling
    platform Encord, thereby completing an end-to-end use case. Finally, we explored
    how to further enhance sampling by incorporating an SSL step into the active ML
    pipeline.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用Lightly通过不同的采样策略高效地选择视频中最具信息量的帧，以改进目标检测模型。我们还看到了如何将这些选定的帧发送到标记平台Encord，从而完成一个端到端的使用案例。最后，我们探讨了如何通过将SSL步骤纳入活跃的机器学习管道来进一步优化采样。
- en: Moving forward, our focus will shift to exploring how to effectively evaluate,
    monitor, and test the active ML pipeline. This step is essential in ensuring that
    the pipeline remains robust and reliable throughout its deployment. By implementing
    comprehensive evaluation strategies, we can assess the performance of the pipeline
    against predefined metrics and benchmarks. Additionally, continuous monitoring
    will allow us to identify any potential issues or deviations from expected behavior,
    enabling us to take proactive measures to maintain optimal performance.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的工作中，我们的重点将转向探索如何有效地评估、监控和测试活跃的机器学习管道。这一步骤对于确保管道在整个部署过程中保持稳健和可靠至关重要。通过实施全面的评估策略，我们可以评估管道相对于预定义的指标和基准的性能。此外，持续的监控将使我们能够识别任何潜在的问题或与预期行为不符的情况，从而使我们能够采取主动措施以保持最佳性能。
- en: Furthermore, rigorous testing of the active ML pipeline is essential to verify
    its functionality and validate its accuracy. Through systematic testing procedures,
    we can ensure that the pipeline behaves consistently under various scenarios and
    input conditions. This will involve designing and executing diverse test cases
    that cover a wide range of potential use cases and edge scenarios.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对活跃的机器学习管道进行严格的测试对于验证其功能并验证其准确性至关重要。通过系统化的测试程序，我们可以确保管道在各种场景和输入条件下表现一致。这将涉及设计并执行各种测试用例，涵盖广泛潜在的使用案例和边缘场景。
- en: By thoroughly evaluating, monitoring, and testing the active ML pipeline, we
    can instill confidence in its reliability and performance. This robust framework
    will enable us to make informed decisions and drive valuable insights from the
    processed data, ultimately leading to improved outcomes and enhanced decision-making
    capabilities in the field-specific domain.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 通过彻底评估、监控和测试活跃的机器学习管道，我们可以对其可靠性和性能建立信心。这个稳健的框架将使我们能够做出明智的决定，并从处理的数据中获得有价值的见解，最终导致特定领域领域的成果改进和决策能力增强。
