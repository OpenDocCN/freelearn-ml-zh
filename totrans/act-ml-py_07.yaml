- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Leveraging Active Learning for Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore how to use **machine learning** (**ML**) to
    deal with big data, such as videos. The task of developing ML models for video
    analysis comes with its own set of unique challenges. Videos, being inherently
    large, pose significant hurdles in terms of efficient processing. Video analysis
    using ML has become an increasingly important technique across many industries
    and applications. From autonomous vehicles that rely on computer vision models
    to analyze road conditions in real-time video feeds, to security systems that
    can automatically detect suspicious activity, ML is revolutionizing what’s possible
    with video data. These models can automate time-consuming manual analysis and
    provide scalable video understanding. Implementing performant and scalable video
    analysis pipelines involves surmounting key hurdles such as an enormous amount
    of data labeling.
  prefs: []
  type: TYPE_NORMAL
- en: We will guide you through a cutting-edge ML method that will aid you in selecting
    the most informative frames for labeling, thereby enhancing the overall accuracy
    and efficacy of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ML models for video analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the most informative frames with Lightly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will need to install the following packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also need the following imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you need to create a Lightly account and set up your API token, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you must set up the Lightly client to connect to the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This demo was run on an AWS SageMaker notebook (`ml.g4dn.2xlarge instance`).
    This instance has a GPU and enough memory to run this demo because we need access
    to Docker, which is not possible in Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: An Encord account ([https://app.encord.com/](https://app.encord.com/)) is also
    required if you want to send the selected frames to an annotation platform.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ML models for video analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Active ML plays a transformative role in managing big data projects by strategically
    optimizing the data annotation process, thereby enhancing model performance with
    less manual effort. For instance, in large-scale image recognition tasks, such
    as identifying specific objects across millions of social media photos, active
    learning can significantly reduce the workload by pinpointing images that are
    most likely to refine the model’s capabilities. Similarly, in **natural language
    processing** (**NLP**) applications, dealing with vast amounts of text data from
    sources such as news articles, forums, and customer feedback, active ML helps
    in selectively annotating documents that add the most value to understanding complex
    language nuances or sentiments. This approach not only streamlines the effort
    required in annotating massive datasets but also ensures that models trained on
    such data are more accurate, efficient, and capable of handling the real-world
    variability inherent in big data sources. Extending this methodology to video
    analysis, active ML becomes even more pivotal due to the added complexity and
    volume of data within video content. Active ML can be leveraged to identify key
    frames or segments that significantly contribute to the model’s learning, dramatically
    reducing the annotation burden while ensuring comprehensive understanding and
    analysis of video data. This targeted approach in video projects not only conserves
    resources but also accelerates the development of sophisticated video analysis
    models that are capable of performing tasks such as activity recognition, event
    detection, and sentiment analysis with higher precision and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore how to leverage active ML for developing an
    ML model for videos. ML video analysis systems require implementing strategies
    to efficiently curate video frames. Videos are usually large files, and annotating
    all frames in the videos is impossible. Moreover, depending on the **frames per
    second** (**FPS**) rate, videos often contain a significant amount of duplicated
    data, which would be a waste of time and money to label. Common practice is to
    label at an FPS rate of 1, instead of 30 for example, to reduce the number of
    frames to label. However, this is not an optimal solution as it will lead to similar
    frames being annotated, an imbalance in the classes that are represented, and
    a lack of diversity in the data selected. Plus, many of the frames that will be
    labeled if such a pipeline is in place probably don’t need to be labeled in the
    first place because the model might already perform very well on some of those
    frames. Thus, labeling frames on which the model is confident and correct is a
    waste of time and money.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, it is infeasible to manually label all frames in video datasets
    for ML, making active learning crucial due to the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data volume**: Video data consists of a large number of frames, which makes
    comprehensive manual labeling extremely time-consuming and expensive. For instance,
    labeling objects in all frames of just 10 minutes of a 30 FPS video would require
    labeling 18,000 images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Redundancy**: Consecutive video frames are highly redundant as they contain
    almost identical content. It is inefficient to manually label this repetitive
    data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost**: The expense of hiring human labelers to meticulously annotate video
    frames would be exorbitant, rendering the majority of projects economically unfeasible.
    Labeling fees for just 10 hours of video could amount to thousands of dollars.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is where active ML is invaluable. It optimizes the labeling effort by identifying
    the most informative frames that are likely to improve the model, as we have seen
    in previous chapters. Human labelers can then focus exclusively on these high-value
    frames. By directing the labeling process to maximize performance gains, considerably
    fewer frames require manual annotation, making video ML viable and cost-effective.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, exhaustive manual video data labeling is impractical and economically
    unfeasible. Active learning provides crucial optimization for labeling so that
    models can be trained to analyze video in a feasible, affordable, and adaptable
    manner.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explore a real-world example with a commonly used active ML tool
    called **Lightly** ([https://www.lightly.ai/](https://www.lightly.ai/)).
  prefs: []
  type: TYPE_NORMAL
- en: Selecting the most informative frames with Lightly
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will use an active ML tool called **Lightly**. Lightly is
    a data curation tool that’s equipped with a web platform that enables users to
    choose the optimal subset of samples for maximizing model accuracy. Lightly’s
    algorithms can process substantial volumes of data, such as 10 million images
    or 10 thousand videos, in less than 24 hours.
  prefs: []
  type: TYPE_NORMAL
- en: The web app allows users to explore their datasets using filters such as sharpness,
    luminance, contrast, file size, and more. They can then use these filters to explore
    correlations between these characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Users can also search for similar images or objects within the app and look
    into the embeddings (**principal component analysis** (**PCA**), **T-distributed
    stochastic neighbor embedding** (**TSNE**), and **uniform manifold approximation
    and projection** (**UMAP**)). Embeddings refers to vector representations of images
    that are learned by deep neural networks. They capture visual features and semantics
    of the images in a way that allows similarities and relationships between images
    to be analyzed. When images are passed through a convolutional neural network,
    the final layer before classification is typically a dense representation of the
    image features. This dense layer outputs a vector with hundreds or thousands of
    dimensions for each image. This vector is called an embedding. Images with similar
    features will have embeddings that are close or nearby when mapped in the embedding
    space. Images with very different features will be farther apart in the embedding
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a few techniques that can be used to visualize these high-dimensional
    embeddings in two or three dimensions for human analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**PCA**: PCA reduces the dimensions of embeddings down to 2D or 3D so that
    they can be plotted. It projects them onto the dimensions that capture the most
    variance. Images with similar prominent visual features will appear closer together
    after PCA projection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TSNE**: TSNE is a technique that represents high-dimensional embeddings in
    lower dimensions while keeping similar images close and dissimilar images apart.
    The 2D or 3D mappings attempt to model the local structure of the higher dimensional
    space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UMAP**: UMAP is a more recent technique that can preserve global data structure
    in the projections better than TSNE in many cases. It maps images with similar
    embeddings nearby and dissimilar ones farther apart in the projection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embeddings capture image features and semantics in vectors. Techniques such
    as PCA, TSNE, and UMAP then project these high-dimensional vectors down to 2D
    or 3D so that they can be visualized and analyzed for similarity relationships
    between images. The Lightly app leverages these projections to enable image similarity
    searches.
  prefs: []
  type: TYPE_NORMAL
- en: Using Lightly to select the best frames to label for object detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To select the best frames, we need to conduct a series of steps.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset and pre-trained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this example, we will use a video of a dog running after a ball. We will
    only be using one video, as depicted in *Figure 5**.1*, for demo purposes. This
    video can be found in this book’s GitHub repository ([https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/blob/main/Chapter_5/videos/project_demo/dog_running_ball.mp4](https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/blob/main/Chapter_5/videos/project_demo/dog_running_ball.mp4)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Video used for testing our Lightly demo](img/B21789_05_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Video used for testing our Lightly demo
  prefs: []
  type: TYPE_NORMAL
- en: Our main goal in this section is to select the most informative frames in this
    video using the different sampling strategies that Lightly offers.
  prefs: []
  type: TYPE_NORMAL
- en: Our video called `dog_running_ball.mp4` is stored in a subfolder called `project_demo`
    under a folder called `videos`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once, we have the video in our `videos/project_demo` folder, the next step
    is to load a pre-trained object detection model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This pre-trained model from `ultralytics` supports 80 classes that we can visualize
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In our case, we know we are dealing with a video of a dog running after a ball,
    so we will focus on the `dog` and `sports` `ball` classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we must prepare some of the variables for our Lightly run, such as the
    path to the predictions output folder, the task’s name, and the important classes
    that we want to focus on in this run to improve our model (`dog` and `sports ball`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Our classes here are `16` and `32`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the required Lightly files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will be saving our inference predictions in a subfolder called `predictions`;
    our task name is `yolov8_demo_dog_detection`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to create all the configuration files that Lightly will use:'
  prefs: []
  type: TYPE_NORMAL
- en: The `tasks.json` file, which specifies the name of our current task. A task
    name is the name of the corresponding subfolder in the folder predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `schema.json` file, which allows Lightly to know the format of the predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metadata `schema.json` file, which contains the names of the videos in our
    `videos` folder.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code to create these configuration files can be found in the [*Chapter 5*](B21789_05.xhtml#_idTextAnchor069)
    section in this book’s GitHub repository ([https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5](https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5)).
  prefs: []
  type: TYPE_NORMAL
- en: We can now run object detection inference using the pre-trained model we loaded
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll run object detection inference on our test video with a low confidence
    threshold of 0.3 as we want to have lower confidence scores. The code has been
    set up to handle more than one video in the subfolder. However, for testing purposes,
    we only have one video. We will skip predictions that are not part of the important
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `populate_predictions_json_files` function is defined in the code in this
    book’s GitHub repository ([https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5](https://github.com/PacktPublishing/Active-Machine-Learning-with-Python/tree/main/Chapter_5)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have run this code, we’ll receive the outputs of the inference run
    in the format supported by Lightly in the `predictions` subfolder, as shown in
    *Figure 5**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Lightly JSON predictions (snapshot)](img/B21789_05_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – Lightly JSON predictions (snapshot)
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take a look at one of those files, the information the JSON files contain
    is the result of the inference. This includes the coordinates of the bounding
    boxes, along with the corresponding class ID and confidence score, as shown in
    *Figure 5**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Prediction JSON file for the first frame in the format expected
    by Lightly](img/B21789_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – Prediction JSON file for the first frame in the format expected
    by Lightly
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to schedule the active Lightly ML run.
  prefs: []
  type: TYPE_NORMAL
- en: Schedule the active ML run
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can register the Lightly worker using the following code, which returns
    the ID of our Lightly worker:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If a worker with this name already exists, the ID of the existing worker will
    be returned.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Lightly uses Docker to run active ML with the `lightly/worker:latest` image.
    It can be pulled using the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Docker is an open platform for developing, shipping, and running applications
    within software containers. It allows code to be packaged with all its dependencies
    and libraries into a standardized unit for software development. Using Docker
    containers eliminates compatibility issues that arise from differences between
    environments. In short, it enables easy replicability when running scripts because
    the environment in the Docker image is already set up with the correct installed
    packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to schedule our active ML run. This process involves several
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Lightly dataset called `demo_dataset`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set up the data sources by using the `project_demo` project as the input or
    output locations for data. In our case, we are using the local storage option
    ([https://docs.lightly.ai/docs/local-storage](https://docs.lightly.ai/docs/local-storage))
    for demo purposes, but ideally, you should use the cloud service option ([https://docs.lightly.ai/docs/cloud-storage](https://docs.lightly.ai/docs/cloud-storage)),
    which uses either AWS, Azure, or GCP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Schedule the active ML run with the selected strategies we want to use: a strategy
    to find diverse objects, a strategy to balance the class ratios, and a strategy
    to use a prediction score for the object’s frequencies and least confident results.
    We are sampling five samples in this example and are trying to reach a 50/50 balance
    between the `dog` and `sports` `ball` classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, organize the local files so that they match what Lightly is expecting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we take a look at the folders, we’ll see the following structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Folders organized for Lightly local storage](img/B21789_05_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Folders organized for Lightly local storage
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to start the run.
  prefs: []
  type: TYPE_NORMAL
- en: Starting the worker and the active ML run
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to create a function that will be used to initiate the worker with
    the organized folders that are mounted in the Docker container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s start the run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the progress of our run on the Lightly platform, as shown in *Figure
    5**.5*, as well as by looking at the output of the previous code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 5.5 – Lightly view of the active ML run](img/B21789_05_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Lightly view of the active ML run
  prefs: []
  type: TYPE_NORMAL
- en: Once completed, we have access to a detailed report of the run and all the necessary
    logs, such as the memory log and default log. We can also view our dataset in
    the Lightly web application ([https://app.lightly.ai/dataset](https://app.lightly.ai/dataset)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore our results. In *Figure 5**.6*, we can see that we have several
    new subfolders, including `frames` and `crops`. Those folders contain the selected
    frames and crops (cropped bounding boxes):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Resulting folder after the Lightly active ML run](img/B21789_05_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – Resulting folder after the Lightly active ML run
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s visualize the selected frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the images shown in *Figure 5**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – The five most informative frames that were chosen by the Lightly
    active ML run](img/B21789_05_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – The five most informative frames that were chosen by the Lightly
    active ML run
  prefs: []
  type: TYPE_NORMAL
- en: We can also explore the report, which gives us a lot of information about our
    new subset of samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The report is saved under `lightly/project_demo/.lightly/runs/run_id`. We can
    view the run ID and then copy the `report.pdf` file locally, as shown in *Figure
    5**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Copying the report document](img/B21789_05_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – Copying the report document
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot of information in the report, so we will only focus on certain
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s take a quick look at what we started with and what we ended up
    with. As shown in *Figure 5**.9*, we had 290 frames and one video and created
    a subset of five frames only, which corresponds to our request for five samples.
    Note that the sample ratio can be selected with `n_samples`, but it can also be
    selected as a percentage of the data with `proportionSamples`. So, to select 30%
    of the data, we can set the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can do this instead of running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Lightly report – dataset information](img/B21789_05_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Lightly report – dataset information
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s examine the embeddings plots, as shown in *Figure 5**.10*. Upon
    examining both the UMAP and PCA embeddings plots, we’ll see the absence of distinct
    clusters, suggesting a lack of consistency among the frames. This inconsistency
    can be attributed to the video’s dynamic filming conditions, including varying
    angles, distances to the dog, and changes in lighting due to shaded and non-shaded
    areas being encountered while moving around and following the dog. These factors
    contribute to the diverse visual input captured in the frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Lightly report – embeddings plots](img/B21789_05_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 – Lightly report – embeddings plots
  prefs: []
  type: TYPE_NORMAL
- en: 'With *Figure 5**.11*, we have a better understanding of which frames were selected
    by the active ML algorithm. We can see that it did a good job of selecting frames
    from diverse locations in the embeddings space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Lightly report – embeddings plots selected frames](img/B21789_05_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 – Lightly report – embeddings plots selected frames
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 5**.12*, we can observe that the class distribution has been effectively
    balanced: the `dog` class accounts for 54.5%, while the `sports ball` class accounts
    for 45.5% of the selection, closely aligning with our intended 50/50 class balance
    ratio. This balance was achieved thanks to our configured class balance ratios.
    Nonetheless, attaining such equilibrium often presents challenges, particularly
    when one class significantly outnumbers the others in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – Lightly report – class distribution](img/B21789_05_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 – Lightly report – class distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can explore some of the model predictions, as shown in *Figure 5**.13*.
    Here, we have several examples of predictions with different numbers of detections.
    Overall, the model demonstrates strong performance, consistently identifying the
    dog within the frames. However, it appears to struggle more with detecting the
    ball. Notably, the ball was recognized in one of the sample frames, which is an
    encouraging sign. This discrepancy in detection accuracy likely comes from the
    nature of the ball used in the test; it deviates from the conventional sports
    balls, such as tennis or soccer balls, on which the original model was trained.
    This context helps explain the observed variations in model performance and can
    be fixed by labeling the ball and re-training the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – Lightly report – examples of the model’s predictions](img/B21789_05_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 – Lightly report – examples of the model’s predictions
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, *Figure 5**.14* shows the selected frames throughout the video. Each
    selected frame corresponds to a vertical line, and we can confirm that there are
    five lines for our five selected frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – Lightly report – video sampling densities](img/B21789_05_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 – Lightly report – video sampling densities
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the most informative frames we wish to label, we can send them
    to the annotation platform used in the project, such as Encord Annotate ([https://docs.encord.com/docs/annotate-overview](https://docs.encord.com/docs/annotate-overview))
    or Roboflow ([https://roboflow.com/](https://roboflow.com/)), as presented in
    [*Chapter 3*](B21789_03.xhtml#_idTextAnchor040), *Managing the Human in the Loop*.
    In this example, we will use Encord Annotate because it offers a feature to visualize
    selected frames as videos.
  prefs: []
  type: TYPE_NORMAL
- en: Before running this code, you need to create an Encord SSH private key by following
    the documentation ([https://docs.encord.com/reference/authentication-sdk](https://docs.encord.com/reference/authentication-sdk))
    they provide.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you can authenticate using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to create an Encord dataset ([https://docs.encord.com/docs/annotate-datasets](https://docs.encord.com/docs/annotate-datasets))
    with the name of our project – that is, `project_demo`. This Encord dataset is
    the data that we want to label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This will return a dictionary with the `title`, `type`, `dataset_hash`, `user_hash`,
    and `backing_folder_uuid` values of the created dataset. Here, we are using Encord’s
    cloud storage, but you could also use custom cloud storage. For example, if you’re
    using AWS S3, then you can use `StorageLocation.AWS` instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can query the dataset hash because it will be necessary when uploading
    the images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can populate the dataset with our selected frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This will return a dictionary containing the `data_hash`, `backing_item_uuid`,
    and `title` values of the uploaded data.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we used `create_video=True` so that we can create a compressed video
    from the image groups; these are called image sequences ([https://docs.encord.com/docs/annotate-supported-data#image-sequences](https://docs.encord.com/docs/annotate-supported-data#image-sequences)).
    This is beneficial when visualizing the frames as it helps maintain the temporal
    context of the videos and is usually very helpful for the labelers. It also allows
    the labelers to use features such as automated labeling ([https://docs.encord.com/docs/automated-labeling](https://docs.encord.com/docs/automated-labeling)),
    which includes interpolation. This helps speed up the labeling process considerably
    by automatically estimating the location of labels between two manually labeled
    frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can view our dataset on the Encord web application, in the
    `Index/Datasets` section ([https://app.encord.com/datasets](https://app.encord.com/datasets)),
    as shown in *Figure 5**.15*. We can observe that the images are saved as `img_sequence`,
    which means that they will be displayed as a video:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Encord dataset with our five selected frames saved as an image
    sequence](img/B21789_05_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 – Encord dataset with our five selected frames saved as an image
    sequence
  prefs: []
  type: TYPE_NORMAL
- en: 'In Encord, we define the ontology ([https://docs.encord.com/reference/ontologies-sdk](https://docs.encord.com/reference/ontologies-sdk))
    that we want to use for this annotation project, as shown in *Figure 5**.16*.
    We introduced the concept of ontologies in the *Designing interactive learning
    systems and workflows* section of [*Chapter 3*](B21789_03.xhtml#_idTextAnchor040),
    *Managing the Human in* *the Loop*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Created ontology in Encord with our two classes, dog and ball](img/B21789_05_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 – Created ontology in Encord with our two classes, dog and ball
  prefs: []
  type: TYPE_NORMAL
- en: 'From the page visualized in *Figure 5**.16*, we can copy the ontology ID and
    use it to create the Encord Annotate project ([https://docs.encord.com/docs/annotate-overview](https://docs.encord.com/docs/annotate-overview)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.17 – Encord Annotate project of our project_demo samples](img/B21789_05_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 – Encord Annotate project of our project_demo samples
  prefs: []
  type: TYPE_NORMAL
- en: 'Our data is now ready to be labeled. We can view our Encord Annotate project
    in the `Annote/Annotation projects` section ([https://app.encord.com/projects](https://app.encord.com/projects))
    of the web app. Then, in the `project_demo` annotation project, we can view our
    selected frames as a video, as shown in *Figure 5**.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.18 – Labeling view of the selected frames'' image sequence in Encord](img/B21789_05_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 – Labeling view of the selected frames' image sequence in Encord
  prefs: []
  type: TYPE_NORMAL
- en: 'In the view presented in *Figure 5**.18*, we can see that all the frames are
    presented as a video with a slider that the users can use to navigate the frames.
    There are also the two classes (dog and ball) that we defined in our ontology
    in the **Classes** section that labelers can select to label the frames, as shown
    in *Figure 5**.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.19 – Example of annotations on one of the selected frames](img/B21789_05_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 – Example of annotations on one of the selected frames
  prefs: []
  type: TYPE_NORMAL
- en: From this page, labelers can use the automated labeling feature mentioned previously
    and easily label the objects; when they are done, they can submit the results
    for review.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also access the selected frames using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is correct! Now, let’s print the results so that we have a better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This returns the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: These URLs can be used to send the selected frames to the annotations platform
    used for the project. Note that since we are using local storage for the demo,
    the data isn’t easily accessible to annotation platforms and cloud services should
    be used instead. The local data can also be visualized in Lightly by serving `localhost`
    ([https://docs.lightly.ai/docs/local-storage#optional-view-local-data-in-lightly-platform](https://docs.lightly.ai/docs/local-storage#optional-view-local-data-in-lightly-platform)).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we used Lightly to select the most informative frames in a
    test video of a dog running after a ball using several strategies. These strategies
    included finding diverse objects, balancing class ratios, using prediction scores
    for object frequencies, and considering the least confident results. Lightly has
    a lot of other features to improve these results and well-organized documentation
    ([https://docs.lightly.ai/](https://docs.lightly.ai/)).
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we will talk about how we can use Lightly for **self-supervised** **learning**
    (**SSL**).
  prefs: []
  type: TYPE_NORMAL
- en: SSL with active ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lightly offers other useful features, such as **SSL**, which allows users to
    fine-tune an SSL model on their data before embedding the images. SSL algorithms
    exploit the structure and context within unlabeled images or videos to generate
    surrogate supervised signals that enable models to discover powerful visual representations
    on their own. For example, models may be trained to recognize spatial patterns,
    colorizations, rotations, or temporal ordering as pretext objectives before fine-tuning
    downstream tasks. In essence, SSL allows models to take advantage of vast volumes
    of unlabeled video and images to uncover useful features and patterns within the
    data itself, avoiding reliance on manual labeling, which can be infeasible at
    scale. The models automatically supervise their feature learning through carefully
    designed pretext tasks while harnessing aspects such as temporal continuity in
    a video. So, this Lightly feature can be extremely beneficial when developing
    models for a specific domain, such as medical videos. The additional training
    step improves the quality of the embeddings because the model can adapt to the
    specific domain without requiring more labeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enabling SSL is simple and only requires adding a couple of lines to our code
    in the `worker_config` and `lightly_config` subdictionaries, both of which are
    part of the `scheduled_run_id` dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here, we configured the active ML run to perform SSL training for 10 epochs
    on the CPU before generating the embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at our outputs. The frames that were selected are mostly
    different from the ones we selected previously – that is, `dog_running_ball-024-mp4.png`,
    `dog_running_ball-101-mp4.png`, `dog_running_ball-033-mp4.png`, `dog_running_ball-224-mp4.png`,
    and `dog_running_ball-049-mp4.png` – compared to `dog_running_ball-024-mp4.png`,
    `dog_running_ball-183-mp4.png`, `dog_running_ball-151-mp4.png`, `dog_running_ball-194-mp4.png`,
    and `dog_running_ball-180-mp4.png`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, only frame `024` was selected again. *Figure 5**.20* shows the five most
    informative frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.20 – Selecting the five most informative frames via a Lightly active
    ML SSL run](img/B21789_05_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 – Selecting the five most informative frames via a Lightly active
    ML SSL run
  prefs: []
  type: TYPE_NORMAL
- en: The frame that was selected in both the SSL and non-SSL runs is highlighted
    at the borders.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5**.20* shows that the addition of the SSL step has noticeably altered
    the selection criteria for frames. Predominantly, frames chosen post-SSL tend
    to feature the dog at a further distance, contrasting sharply with those selected
    without SSL, which mainly consisted of close-ups showcasing the dog holding the
    ball. This shift underscores the impact of SSL on the model’s focus and frame
    selection preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.21 – Lightly report – comparing video sampling densities between
    the non-SSL run (top image) and the SSL run (bottom image)](img/B21789_05_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 – Lightly report – comparing video sampling densities between the
    non-SSL run (top image) and the SSL run (bottom image)
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon examining the new embedding plots shown in *Figure 5**.21*, it is evident
    that the embeddings model performs better in clustering the frames. Despite this
    improvement, the clusters are not yet sharply defined, suggesting that extending
    the number of epochs in the SSL training could further refine this aspect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.22 – New embeddings plots with the SSL active ML run](img/B21789_05_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 – New embeddings plots with the SSL active ML run
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating the Lightly SSL feature into the ML pipeline is a straightforward
    addition that can provide significant benefits for field-specific data. By leveraging
    this advanced technique, we observed that the embeddings that were generated by
    the model were notably improved after undergoing SSL in our test. This enhancement
    not only enhances the overall performance but also ensures that the pipeline is
    optimized to handle the unique characteristics of the data being processed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use Lightly to efficiently select the most
    informative frames in videos to improve object detection models using diverse
    sampling strategies. We also saw how to send these selected frames to the labeling
    platform Encord, thereby completing an end-to-end use case. Finally, we explored
    how to further enhance sampling by incorporating an SSL step into the active ML
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, our focus will shift to exploring how to effectively evaluate,
    monitor, and test the active ML pipeline. This step is essential in ensuring that
    the pipeline remains robust and reliable throughout its deployment. By implementing
    comprehensive evaluation strategies, we can assess the performance of the pipeline
    against predefined metrics and benchmarks. Additionally, continuous monitoring
    will allow us to identify any potential issues or deviations from expected behavior,
    enabling us to take proactive measures to maintain optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, rigorous testing of the active ML pipeline is essential to verify
    its functionality and validate its accuracy. Through systematic testing procedures,
    we can ensure that the pipeline behaves consistently under various scenarios and
    input conditions. This will involve designing and executing diverse test cases
    that cover a wide range of potential use cases and edge scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: By thoroughly evaluating, monitoring, and testing the active ML pipeline, we
    can instill confidence in its reliability and performance. This robust framework
    will enable us to make informed decisions and drive valuable insights from the
    processed data, ultimately leading to improved outcomes and enhanced decision-making
    capabilities in the field-specific domain.
  prefs: []
  type: TYPE_NORMAL
