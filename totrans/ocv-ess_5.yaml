- en: Chapter 5. Focusing on the Interesting 2D Features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In most images, the most useful information is around certain zones that typically
    correspond to salient points and regions. In most applications, local processing
    around these salient points is sufficient as long as these points are stable and
    distinctive. In this chapter, we will cover a basic introduction to the 2D salient
    points and features offered by OpenCV. It is important to note the difference
    between detectors and descriptors. **Detectors** only extract interest points
    (local features) on an image, while descriptors obtain relevant information about
    the neighborhood of these points. **Descriptors**, as their name suggests, describe
    the image by proper features. They describe an interest point in a way that is
    invariant to change in lighting and to small perspective deformations. This can
    be used to match them with other descriptors (typically extracted from other images).
    For this purpose, matchers are used. This, in turn, can be used to detect objects
    and infer the camera transformation between two images. First, we show the internal
    structure of the interest points and provide an explanation of the 2D features
    and descriptor extraction. Finally, the chapter deals with matching, that is,
    putting 2D features of different images into correspondence.
  prefs: []
  type: TYPE_NORMAL
- en: Interest points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Local features, also called interest points, are characterized by sudden changes
    of intensity in the region. These local features are usually classified in edges,
    corners, and blobs. OpenCV encapsulates interesting point information in the `KeyPoint`
    class, which contains the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: The coordinates of the interest point (the `Point2f` type)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diameter of the meaningful keypoint neighborhood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Orientation of the keypoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strength of the keypoint, which depends on the keypoint detector that is selected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pyramid layer (octave) from which the keypoint has been extracted; octaves are
    used in some descriptors such as `SIFT`, `SURF`, `FREAK`, or `BRISK`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object ID used to perform clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OpenCV handles several local feature detector implementations through the `FeatureDetector`
    abstract class and its `Ptr<FeatureDetector> FeatureDetector::create(const string&
    detectorType)` method or through the algorithm class directly. In the first case,
    the type of detector is specified (see the following diagram where the detectors
    used in this chapter are indicated in red color). Detectors and the types of local
    features that they detect are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FAST` (`FastFeatureDetector`): This feature detects corners and blobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`STAR` (`StarFeatureDetector`): This feature detects edges, corners, and blobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SIFT` (`SiftFeatureDetector`): This feature detects corners and blobs (part
    of the `nonfree` module)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SURF` (`SurfFeatureDetector`): This feature detects corners and blobs (part
    of the `nonfree` module)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ORB` (`OrbFeatureDetector`): This feature detects corners and blobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BRISK` (`BRISK`): This feature detects corners and blobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MSER` (`MserFeatureDetector`): This feature detects blobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GFTT` (`GoodFeaturesToTrackDetector`): This feature detects edges and corners'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HARRIS` (`GoodFeaturesToTrackDetector`): This feature detects edges and corners
    (with the Harris detector enabled)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dense` (`DenseFeatureDetector`): This feature detects the features that are
    distributed densely and regularly on the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SimpleBlob` (`SimpleBlobDetector`): This feature detects blobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Feature detectors](img/00028.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 2D feature detectors in OpenCV
  prefs: []
  type: TYPE_NORMAL
- en: We should note that some of these detectors, such as `SIFT`, `SURF`, `ORB`,
    and `BRISK`, are also descriptors.
  prefs: []
  type: TYPE_NORMAL
- en: Keypoint detection is performed by the `void FeatureDetector::detect(const Mat&
    image, vector<KeyPoint>& keypoints, const Mat& mask)` function, which is another
    method of the `FeatureDetector` class. The first parameter is the input image
    where the keypoints will be detected. The second parameter corresponds to the
    vector where the keypoints will be stored. The last parameter is optional and
    represents an input mask image in which we can specify where to look for keypoints.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Matthieu Labbé has implemented a Qt-based open source application where you
    can test OpenCV's corner detectors, feature extractors, and matching algorithms
    in a nice GUI. It is available at [https://code.google.com/p/find-object/](https://code.google.com/p/find-object/).
  prefs: []
  type: TYPE_NORMAL
- en: The first interest points were historically corners. In 1977, Moravec defined
    corners as interest points where there is a large intensity variation in several
    directions (45 degrees). These interest points were used by Moravec to find matching
    regions in consecutive image frames. Later, in 1988, Harris improved Moravec's
    algorithm using the Taylor expansion to approximate the shifted intensity variation.
    Afterwards, other detectors appeared, such as the detector based on **difference
    of Gaussians** (**DoG**) and **determinant of the Hessian** (**DoH**) (for example,
    `SIFT` or `SURF`, respectively) or the detector based on Moravec's algorithm,
    but considering continuous intensity values in a pixel neighborhood such as `FAST`
    or `BRISK` (scale-space FAST).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lu, in her personal blog, *LittleCheeseCake*, explains some of the most popular
    detectors and descriptors in detail. The blog is available at [http://littlecheesecake.me/blog/13804625/feature-detectors-and-descriptors](http://littlecheesecake.me/blog/13804625/feature-detectors-and-descriptors).
  prefs: []
  type: TYPE_NORMAL
- en: The FAST detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The corner detector is based on the **Features from Accelerated Segment Test**
    (**FAST**) algorithm. It was designed to be very efficient, targeting real-time
    applications. The method is based on considering a circle of 16 pixels (neighborhood)
    around a candidate corner p. The FAST detector will consider p as a corner if
    there is a set of contiguous pixels in the neighborhood that all are brighter
    than p+T or darker than p-T, T being a threshold value. This threshold must be
    properly selected.
  prefs: []
  type: TYPE_NORMAL
- en: OpenCV implements the FAST detector in the `FastFeatureDetector()` class, which
    is a wrapper class for the `FAST()` method. To use this class, we must include
    the `features2d.hpp` header file in our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we show a code example where the corners are detected using the `FAST`
    method with different threshold values. The `FASTDetector` code example is shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The explanation of the code is given as follows. In this and the following
    examples, we usually perform the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the 2D feature detector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Detect keypoints in the image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw the keypoints obtained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In our sample, `FastFeatureDetector(int threshold=1, bool nonmaxSuppression=
    true, type=FastFeatureDetector::TYPE_9_16)` is the function where the detector
    parameters, such as threshold value, non-maximum suppression, and neighborhoods,
    are defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following three types of neighborhoods can be selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FastFeatureDetector::TYPE_9_16`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FastFeatureDetector::TYPE_7_12`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`FastFeatureDetector::TYPE_5_8`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These neighborhoods define the number of neighbors (16, 12, or 8) and the total
    number of contiguous pixels (9, 7, or 5) needed to consider the corner (keypoint)
    valid. An example of `TYPE_9_16` is shown in the next screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our code, the threshold values `80` and `100` have been selected, while
    the rest of the parameters have their default values, `nonmaxSuppression=true`
    and `type=FastFeatureDetector::TYPE_9_16`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Keypoints are detected and saved using the `void detect(const Mat& image, vector<KeyPoint>&
    keypoints, const Mat& mask=Mat())` function. In our case, we create the following
    two FAST feature detectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '`detector1` saves its keypoints in the `keypoints1` vector'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`detector2` saves its keypoints in the `keypoints2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `void drawKeypoints(const Mat& image, const vector<KeyPoint>& keypoints,
    Mat& outImage, const Scalar& color=Scalar::all(-1), int flags=DrawMatchesFlags::DEFAULT)`
    function draws the keypoints in the image. The `color` parameter allows us to
    define a color of keypoints, and with the `Scalar:: all(-1)` option, each keypoint
    will be drawn with a different color.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The keypoints are drawn using the two threshold values on the image. We will
    notice a small difference in the number of keypoints detected. This is due to
    the threshold value in each case. The following screenshot shows a corner detected
    in the sample with a threshold value of 80, which is not detected with a threshold
    value of 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The FAST detector](img/00029.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Keypoint detected with a threshold value of 80 (in the left-hand side). The
    same corner is not detected with a threshold value of 100 (in the right-hand side).
  prefs: []
  type: TYPE_NORMAL
- en: 'The difference is due to the fact that the FAST feature detectors are created
    with the default type, that is, `TYPE_9_16`. In the example, the p pixel takes
    a value of 228, so at least nine contiguous pixels must be brighter than p+T or
    darker than p-T. The following screenshot shows the neighborhood pixel values
    in this specific keypoint. The condition of nine contiguous pixels is met if we
    use a threshold value of 80\. However, the condition is not met with a threshold
    value of 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The FAST detector](img/00030.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Keypoint pixel values and contiguous pixels all darker than p-T (228-80=148)
    with a threshold value of 80
  prefs: []
  type: TYPE_NORMAL
- en: The SURF detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Speeded Up Robust Features** (**SURF**) detector is based on a Hessian
    matrix to find the interest points. For this purpose, SURF divides the image in
    different scales (levels and octaves) using second-order Gaussian kernels and
    approximates these kernels with a simple box filter. This filter box is mostly
    interpolated in scale and space in order to provide the detector with the scale-invariance
    properties. SURF is a faster approximation of the classic **Scale Invariant Feature
    Transform** (**SIFT**) detector. Both the SURF and SIFT detectors are patented,
    so OpenCV includes them separately in their `nonfree/nonfree.hpp` header file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `SURFDetector` code shows an example where the keypoints are
    detected using the SURF detector with a different number of Gaussian pyramid octaves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the preceding example (and subsequent ones), some portions of code are not
    repeated for simplicity because they are the same as in previous examples.
  prefs: []
  type: TYPE_NORMAL
- en: The explanation of the code is given as follows. `SURFFeatureDetector(double
    hessianThreshold, int nOctaves, int nOctaveLayers, bool extended, bool upright)`
    is the main function used to create a SURF detector where we can define the parameter
    values of the detector, such as the Hessian threshold, the number of Gaussian
    pyramid octaves, number of images within each octave of a Gaussian pyramid, number
    of elements in the descriptor, and the orientation of each feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'A high threshold value extracts less keypoints but with more accuracy. A low
    threshold value extracts more keypoints but with less accuracy. In this case,
    we have used a large Hessian threshold (`3500`) to show a reduced number of keypoints
    in the image. Also, the number of octaves changes for each image (2 and 5, respectively).
    A larger number of octaves also select keypoints with a larger size. The following
    screenshot shows the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The SURF detector](img/00031.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The SURF detector with two Gaussian pyramid octaves (in the left-hand side)
    and the SURF detector with five Gaussian pyramid octaves (in the right-hand side)
  prefs: []
  type: TYPE_NORMAL
- en: Again, we use the `drawKeypoints` function to draw the keypoints detected, but
    in this case, as the SURF detector has orientation properties, the `DrawMatchesFlags`
    parameter is defined as `DRAW_RICH_KEYPOINTS`. Then, the `drawKeypoints` function
    draws each keypoint with its size and orientation.
  prefs: []
  type: TYPE_NORMAL
- en: The ORB detector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Binary Robust Independent Elementary Features** (**BRIEF**) is a descriptor
    based on binary strings; it does not find interest points. The **Oriented FAST
    and Rotated BRIEF** (**ORB**) detector is a union of the FAST detector and BRIEF
    descriptor and is considered an alternative to the patented SIFT and SURF detectors.
    The ORB detector uses the FAST detector with pyramids to detect interest points
    and then uses the HARRIS algorithm to rank the features and retain the best ones.
    OpenCV also allows us to use the FAST algorithm to rank the features, but normally,
    this produces less stable keypoints. The following `ORBDetector` code shows a
    simple and clear example of this difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![The ORB detector](img/00032.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The ORB detector with the FAST algorithm to select the 300 best features (in
    the left-hand side) and the HARRIS detector to select the 300 best features (in
    the right-hand side)
  prefs: []
  type: TYPE_NORMAL
- en: 'The explanation of the code is given as follows. The `OrbFeatureDetector(int
    nfeatures=500, float scaleFactor=1.2f, int nlevels=8, int edgeThreshold=31, int
    firstLevel=0, int WTA_K=2, int scoreType=ORB:: HARRIS_SCORE, int patchSize=31)`
    function is the class constructor where we can specify the maximum number of features
    to retain the scale, number of levels, and type of detector (`HARRIS_SCORE` or
    `FAST_SCORE`) used to rank the features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following proposed code example shows the difference between the HARRIS
    and FAST algorithms to rank features; the result is shown in the preceding screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The HARRIS corner detector is used more than FAST to rank features, because
    it rejects edges and provides a reasonable score. The rest of the functions are
    the same as in the previous detector examples, keypoint detection and drawing.
  prefs: []
  type: TYPE_NORMAL
- en: The KAZE and AKAZE detectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The KAZE and AKAZE detectors will be included in the upcoming OpenCV 3.0.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenCV 3.0 is not yet available. Again, if you want to test this code and use
    the KAZE and AKAZE features, you can work with the latest version already available
    in the OpenCV git repository at [http://code.opencv.org/projects/opencv/repository](http://code.opencv.org/projects/opencv/repository).
  prefs: []
  type: TYPE_NORMAL
- en: The KAZE detector is a method that can detect 2D features in a nonlinear scale
    space. This method allows us to keep important image details and remove noise.
    **Additive Operator Splitting** (**AOS**) schemes are used for nonlinear scale
    space. AOS schemes are efficient, stable, and parallelizable. The algorithm computes
    the response of a Hessian matrix at multiple scale levels to detect keypoints.
    On the other hand, the **Accelerated-KAZE** (**AKAZE**) feature detector uses
    fast explicit diffusion to build a nonlinear scale space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in the `KAZEDetector` code, we see an example of the new KAZE and AKAZE
    feature detectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `KAZE::KAZE(bool extended, bool upright)` function is the KAZE class constructor
    in which two parameters can be selected: `extended` and `upright`. The `extended`
    parameter adds the option to select between 64 or 128 descriptors, while the `upright`
    parameter allows us to select rotation or no invariant. In this case, we use both
    parameters with a `true` value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the `AKAZE::AKAZE(DESCRIPTOR_TYPE descriptor_type, int descriptor_size=0,
    int descriptor_channels=3)` function is the AKAZE class constructor. This function
    gets the descriptor type, descriptor size, and the channels as input arguments.
    For the descriptor type, the following enumeration is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the results obtained with this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The KAZE and AKAZE detectors](img/00033.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: The KAZE detector (in the left-hand side) and the AKAZE detector (in the right-hand
    side)
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Eugene Khvedchenya's *Computer Vision Talks* blog contains useful reports that
    compare different keypoints in terms of robustness and efficiency. See the posts
    at [http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/](http://computer-vision-talks.com/articles/2012-08-18-a-battle-of-three-descriptors-surf-freak-and-brisk/)
    and [http://computer-vision-talks.com/articles/2011-07-13-comparison-of-the-opencv-feature-detection-algorithms/](http://computer-vision-talks.com/articles/2011-07-13-comparison-of-the-opencv-feature-detection-algorithms/).
  prefs: []
  type: TYPE_NORMAL
- en: Feature descriptor extractors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Descriptors describe local image regions and are invariant to image transformations
    such as rotation, scale or translation. They provide a measure and distance function
    for a small patch around an interest point. Therefore, whenever the similarity
    between two image patches needs to be estimated, we compute their descriptors
    and measure their distance. In OpenCV, the basic Mat type is used to represent
    a collection of descriptors, where each row is a keypoint descriptor.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are the following two possibilities to use a feature descriptor extractor:'
  prefs: []
  type: TYPE_NORMAL
- en: The `DescriptorExtractor` common interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm class directly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (See the following diagram where the descriptors used in this chapter are indicated
    in red color.)
  prefs: []
  type: TYPE_NORMAL
- en: The common interface allows us to switch easily between different algorithms.
    This can be very useful when choosing an algorithm to solve a problem, as the
    results of each algorithm can be compared with no effort. On the other hand, depending
    on the algorithm, there are several parameters that can be tweaked only using
    its class.
  prefs: []
  type: TYPE_NORMAL
- en: '![Feature descriptor extractors](img/00034.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 2D feature descriptors in OpenCV
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Ptr<DescriptorExtractor> DescriptorExtractor::create(const String& descriptorExtractorType)`
    function creates a new descriptor extractor of the selected type. Descriptors
    can be grouped in two families: float and binary. Float descriptors store float
    values in a vector; this can lead to a high memory usage. On the other hand, binary
    descriptors store binary strings, thus enabling faster processing times and a
    reduced memory footprint. The current implementation supports the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'SIFT: This implementation supports the float descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SURF: This implementation supports the float descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BRIEF: This implementation supports the binary descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'BRISK: This implementation supports the binary descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ORB: This implementation supports the binary descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'FREAK: This implementation supports the binary descriptor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KAZE: This implementation supports the binary descriptor (new in OpenCV 3.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AKAZE: This implementation supports the binary descriptor (new in OpenCV 3.0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other important function of `DescriptorExtractor` is `void DescriptorExtractor::compute(InputArray
    image, vector<KeyPoint>& keypoints, OutputArray descriptors)`, which computes
    the descriptors for a set of keypoints detected in an image on the previous step.
    There is a variant of the function that accepts an image set.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that it is possible to mix feature detectors and descriptor extractors
    from different algorithms. However, it is recommended that you use both methods
    from the same algorithm, as they should fit better together.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptor matchers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`DescriptorMatcher` is an abstract base class to match keypoint descriptors
    that, as happens with `DescriptorExtractor`, make programs more flexible than
    using matchers directly. With the `Ptr<DescriptorMatcher> DescriptorMatcher::create(const
    string& descriptorMatcherType)` function, we can create a descriptor matcher of
    the desired type. The following are the supported types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**BruteForce-L1**: This is used for float descriptors. It uses L1 distance
    and is efficient and fast.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BruteForce**: This is used for float descriptors. It uses L2 distance and
    can be better than L1, but it needs more CPU usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BruteForce-SL2**: This is used for float descriptors and avoids square root
    computation from L2, which requires high CPU usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BruteForce-Hamming**: This is used for binary descriptors and calculates
    the Hamming distance between the compared descriptors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BruteForce-Hamming(2)**: This is used for binary descriptors (2 bits version).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FlannBased**: This is used for float descriptors and is faster than brute
    force by pre-computing acceleration structures (as in DB engines) at the cost
    of using more memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `void DescriptorMatcher::match(InputArray queryDescriptors, InputArray trainDescriptors,
    vector<DMatch>& matches, InputArray mask=noArray())` and `void DescriptorMatcher::knnMatch(InputArray
    queryDescriptors, InputArray trainDescriptors, vector<vector<DMatch>>& matches,
    int k, InputArray mask=noArray(), bool compactResult=false)` functions give the
    best k matches for each descriptor, k being 1 for the first function.
  prefs: []
  type: TYPE_NORMAL
- en: The `void DescriptorMatcher::radiusMatch(InputArray queryDescriptors, InputArray
    trainDescriptors, vector<vector<DMatch>>& matches, float maxDistance, InputArray
    mask=noArray(), bool compactResult=false)` function also finds the matches for
    each query descriptor but not farther than the specified distance. The major drawback
    of this method is that the magnitude of this distance is not normalized, and it
    depends on the feature extractor and descriptor used.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to get the best results, we recommend that you use matchers along with
    descriptors of the same type. Although it is possible to mix binary descriptors
    with float matchers and the other way around, the results might be inaccurate.
  prefs: []
  type: TYPE_NORMAL
- en: Matching the SURF descriptors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SURF descriptors belong to the family of oriented gradients descriptors. They
    encode statistical knowledge about the geometrical shapes present in the patch
    (via histograms of oriented gradients/Haar-like features). They are considered
    as a more efficient substitution for SIFT. They are the best known multiscale
    feature description approaches, and their accuracy has been widely tested. They
    have two main drawbacks though:'
  prefs: []
  type: TYPE_NORMAL
- en: They are patented
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are slower than binary descriptors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is a common pipeline in every descriptor matching application that uses
    the components explained earlier in this chapter. It performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute interest points in both images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract descriptors from the two generated interest point sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a matcher to find connections between descriptors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter the results to remove bad matches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the `matchingSURF` example that follows this pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The explanation of the code is given as follows. As we described earlier, following
    the application pipeline implies performing these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step to be performed is to detect interest points in the input images.
    In this example, the common interface is used to create a SURF detector with the
    line `Ptr<FeatureDetector> detector = FeatureDetector::create("SURF")`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, the interest points are detected, and a descriptor extractor is
    created using the common interface `Ptr<DescriptorExtractor> extractor = DescriptorExtractor::create(
    "SURF")`. The SURF algorithm is also used to compute the descriptors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The next step is to match the descriptors of both images, and for this purpose,
    a descriptor matcher is created using the common interface, too. The line, `Ptr<DescriptorMatcher>
    matcher = DescriptorMatcher::create("FlannBased")`, creates a new matcher based
    on the Flann algorithm, which is used to match the descriptors in the following
    way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, the results are filtered. Note that two matching sets are computed,
    as a cross-checking filter is performed afterwards. This filtering only stores
    the matches that appear in both sets when using the input images as query and
    train images. In the following screenshot, we can see the difference when a filter
    is used to discard matches:![Matching the SURF descriptors](img/00035.jpeg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Results after matching SURF descriptors with and without a filter
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Matching the AKAZE descriptors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KAZE and AKAZE are novel descriptors included in the upcoming OpenCV 3.0\. According
    to published tests, both outperform the previous detectors included in the library
    by improving repeatability and distinctiveness for common 2D image-matching applications.
    AKAZE is much faster than KAZE while obtaining comparable results, so if speed
    is critical in an application, AKAZE should be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `matchingAKAZE` example matches descriptors of this novel algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The explanation of the code is given as follows. The first two steps are quite
    similar to the previous example; the feature detector and descriptor extractor
    are created through their common interfaces. We only change the string parameter
    passed to the constructor, as this time, the AKAZE algorithm is used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A BruteForce matcher that uses Hamming distance is used this time, as AKAZE
    is a binary descriptor.
  prefs: []
  type: TYPE_NORMAL
- en: It is created by executing `Ptr<DescriptorMatcher> matcher = DescriptorMatcher::create("BruteForce-Hamming")`.
    The `matcher.knnMatch(descriptors1, descriptors2, matches, 2)` function computes
    the matches between the image descriptors. It is noteworthy to mention the last
    integer parameter, as it is necessary for the filter processing executed afterwards.
    This filtering is called Ratio Test, and it computes the goodness of the best
    match between the goodness of the second best match. To be considered as a good
    match, this value must be higher than a certain ratio, which can be set in a range
    of values between 0 and 1\. If the ratio tends to be 0, the correspondence between
    descriptors is stronger.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we can see the output when matching a book cover
    in an image where the book appears rotated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matching the AKAZE descriptors](img/00036.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Matching AKAZE descriptors in a rotated image
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the result when the book does not appear in
    the second image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matching the AKAZE descriptors](img/00037.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Matching AKAZE descriptors when the train image does not appear
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have covered a widely used OpenCV component. Local features
    are a key part of relevant computer vision algorithms such as object recognition,
    object tracking, image stitching, and camera calibration. An introduction and
    several samples have been provided, thus covering interest points detection using
    different algorithms, extraction of descriptors from interest points, matching
    descriptors, and filtering the results.
  prefs: []
  type: TYPE_NORMAL
- en: What else?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The powerful Bag-of-Words object categorization framework has not been included.
    This is actually an additional step to what we have covered in this chapter, as
    extracted descriptors are clustered and used to perform categorization. A complete
    sample can be found at `[opencv_source_code]/samples/cpp/bagofwords_classification.cpp`.
  prefs: []
  type: TYPE_NORMAL
