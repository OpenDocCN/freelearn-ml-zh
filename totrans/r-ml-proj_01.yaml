- en: Exploring the Machine Learning Landscape
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索机器学习领域
- en: '**Machine learning** (**ML**) is an amazing subfield of **Artificial Intelligence**
    (**AI**) that tries to mimic the learning behavior of humans. Similar to the way
    a baby learns by observing the examples it encounters, an ML algorithm learns
    the outcome or response to a future incident by observing the data points that
    are provided as input to it.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）是**人工智能**（**AI**）的一个令人惊叹的子领域，它试图模仿人类的认知学习行为。类似于婴儿通过观察遇到的例子来学习的方式，机器学习算法通过观察作为输入提供的数据点来学习未来事件的结果或响应。'
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: ML versus software engineering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习与软件工程
- en: Types of ML methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习方法的类型
- en: ML terminology—a quick review
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习术语——快速回顾
- en: ML project pipeline
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习项目流程
- en: Learning paradigm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习范式
- en: Datasets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集
- en: ML versus software engineering
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习与软件工程
- en: With most people transitioning from traditional software engineering practice
    to ML, it is important to understand the underlying difference between both areas.
    Superficially, both of these areas seem to generate some sort of code to perform
    a particular task. An interesting fact to observe is that, unlike software engineering
    where a programmer explicitly writes a program with various responses based on
    several conditions, the ML algorithm infers the rules of the game by observing
    the input examples. The rules that are learned are further used for better decision
    making when new input data is fed to the system.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大多数人从传统的软件工程实践转向机器学习，理解这两个领域之间的潜在差异非常重要。表面上，这两个领域似乎都生成某种代码以执行特定任务。一个有趣的事实是，与软件工程中程序员根据几个条件明确编写程序不同，机器学习算法通过观察输入示例来推断游戏的规则。学到的规则随后被用于当向系统提供新的输入数据时进行更好的决策。
- en: 'As you can observe in the following diagram, automatically inferring the actions
    from data without manual intervention is the key differentiator between ML and
    traditional programming:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可以在以下图表中观察到的，在没有人工干预的情况下自动从数据中推断动作是机器学习与传统编程之间的关键区别：
- en: '![](img/604a99b5-c97b-4bdb-a20c-e53a09d707f9.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/604a99b5-c97b-4bdb-a20c-e53a09d707f9.png)'
- en: Another key differentiator of ML from traditional programming is that the knowledge
    acquired through ML is able to generalize beyond the training samples by successfully
    interpreting data that the algorithm has never seen before, while a program coded
    in traditional programming can only perform the responses that were included as
    part of the code.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与传统编程的另一个关键区别是，通过机器学习获得的知识能够通过成功解释算法以前从未见过的数据来推广到训练样本之外，而用传统编程编写的程序只能执行代码中包含的响应。
- en: Yet another differentiator is that in software engineering, there are certain
    specific ways to solve a problem at hand. Given an algorithm developed based on
    certain assumptions of inputs and the conditions incorporated, you will be able
    to guarantee the output that will be obtained given an input. In the ML world,
    it is not possible to provide such assurances on the output obtained from the
    algorithms. It is also very difficult in the ML world to confirm if a particular
    technique is better than another without actually trying both the techniques on
    the dataset for the problem at hand.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个区别是，在软件工程中，有解决手头问题的特定方法。给定基于某些输入假设和包含条件的算法，你将能够保证在给定输入的情况下获得输出。在机器学习世界中，无法对算法获得的输出提供此类保证。在机器学习世界中，要确认某种技术是否优于另一种技术，而不实际在处理问题的数据集上尝试这两种技术，也是非常困难的。
- en: ML and software engineering are not the same! ML projects may involve some software
    engineering in them, but ML cannot be considered to be the same as software engineering.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习与软件工程不同！机器学习项目可能涉及一些软件工程，但机器学习不能被视为与软件工程相同。
- en: 'While there is more than one formal definition that exists for ML, the following
    mentioned are a few key definitions encountered often:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在多个关于机器学习的正式定义，以下是一些经常遇到的关键定义：
- en: '"Machine learning is the science of getting computers to act without being
    explicitly programmed."'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '"机器学习是让计算机在没有明确编程的情况下行动的科学。"'
- en: —Stanford
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: —斯坦福
- en: '"Machine learning is based on algorithms that can learn from data without relying
    on rules-based programming."'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '"机器学习基于可以无规则编程依赖从数据中学习的算法。"'
- en: —McKinsey and Co.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: —麦肯锡公司
- en: With the rise of data as the fuel of the future, the terms AI, ML, data mining,
    data science, and data analytics are used interchangeably by industry practitioners.
    It is important to understand the key differences between these terms to avoid
    confusion.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据成为未来动力的兴起，行业从业者将AI、ML、数据挖掘、数据科学和数据分析这些术语互换使用。理解这些术语之间的关键区别以避免混淆是很重要的。
- en: The terms AI, ML, data mining, data science, and data analytics, though used
    interchangeably, are not the same!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然AI、ML、数据挖掘、数据科学和数据分析这些术语被互换使用，但它们并不相同！
- en: 'Let''s take a look at the following terms:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看以下术语：
- en: '**AI**: AI is a paradigm where machines are able to perform tasks in a smart
    way. It may be observed that in the definition of AI, it is not specified whether
    the smartness of machines may be achieved manually or automatically. Therefore,
    it is safe to assume that even a program written with several `if...else` or `switch...case` statements
    that has then been infused with a machine to carry out tasks may be considered
    to be AI.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AI**：AI是一种范式，其中机器能够以智能的方式执行任务。在AI的定义中，并没有指定机器的智能是手动实现还是自动实现。因此，可以安全地假设，即使是一个用几个`if...else`或`switch...case`语句编写的程序，如果它被注入了机器来执行任务，也可以被认为是AI。'
- en: '**ML**: ML, on the other hand, is a way for the machine to achieve smartness
    by learning from the data that is provided as input and, thereby, we have a smart
    machine performing a task. It may be observed that ML achieves the same objective
    of AI except that the smartness is achieved automatically. Therefore, it can be
    concluded that ML is simply a way to achieve AI.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ML**：另一方面，ML是通过从提供作为输入的数据中学习来实现机器智能的一种方式，因此我们有一个智能机器在执行任务。可以观察到，ML实现了与AI相同的目标，只是智能是通过自动实现的。因此，可以得出结论，ML仅仅是实现AI的一种方式。'
- en: '**Data mining**: Data mining is a specific field that focuses on discovering
    the unknown properties of the datasets. The primary objective of data mining is
    to extract rules from large amounts of data provided as input, whereas in ML,
    an algorithm not only infers rules from the data input, but also uses the rules
    to perform predictions on any new, incoming data.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据挖掘**：数据挖掘是一个专注于发现数据集未知属性的特定领域。数据挖掘的主要目标是提取从大量输入数据中得出的规则，而在机器学习中，算法不仅从数据输入中推断规则，还使用这些规则对任何新的、传入的数据进行预测。'
- en: '**Data analytics**: Data analytics is a field that encompasses performing fundamental
    descriptive statistics, data visualization, and data points communication for
    conclusions. Data analytics may be considered to be a basic level within data
    science. It is normal for practitioners to perform data analytics on the input
    data provided for data mining or ML exercises. Such analysis on data is generally
    termed as **exploratory data analysis (EDA)**.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分析**：数据分析是一个涵盖执行基本描述性统计、数据可视化和数据点沟通以得出结论的领域。数据分析可以被认为是数据科学的一个基本层次。从业者对数据挖掘或机器学习练习提供的输入数据进行数据分析是正常的。这种数据分析通常被称为**探索性数据分析（EDA）**。'
- en: '**Data science**: Data science is an umbrella term that includes data analytics,
    data mining, ML, and any specific domain expertise pertaining to the field of
    work. Data science is a concept that includes several aspects of handling the
    data such as acquiring the data from one or more sources, data cleansing, data
    preparation, and creating new data points based on existing data. It includes
    performing data analytics. It also encompasses using one or more data mining or
    ML techniques on the data to infer knowledge to create an algorithm that performs
    a task on unseen data. This concept also includes deploying the algorithm in a
    way that it is useful to perform the designated tasks in the future.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据科学**：数据科学是一个包括数据分析和数据挖掘、机器学习以及与工作领域相关的任何特定领域专长的总称。数据科学是一个涉及处理数据的多个方面的概念，例如从一个或多个来源获取数据、数据清洗、数据准备以及根据现有数据创建新的数据点。它包括执行数据分析。它还包括在数据上使用一个或多个数据挖掘或机器学习技术来推断知识，以创建一个在未见数据上执行任务的算法。这个概念还包括以使其在未来执行指定任务有用的方式部署算法。'
- en: 'The following is a Venn diagram which demonstrates the skills required by a
    professional working in the data science ambit. It has three circles, each of
    which defines a specific skill that a data science professional should have:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个维恩图，展示了在数据科学领域工作的专业人士所需具备的技能。它有三个圆圈，每个圆圈定义了数据科学专业人士应具备的特定技能：
- en: '![](img/8e4fdea3-d6b8-4e8a-bb6e-3ebfbda482ad.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8e4fdea3-d6b8-4e8a-bb6e-3ebfbda482ad.png)'
- en: 'Let''s explore the following skills mentioned in the preceding diagram:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索前面图表中提到的以下技能：
- en: '**Math & Statistic Knowledge**: This skill is required to analyze the statistical
    properties of the data.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数学与统计知识**：这项技能是分析数据的统计属性所必需的。'
- en: '**Hacking Skills**: Programming skills play a key role in order to process
    the data in a quick manner. The ML algorithm is applied to create an output that
    will perform the prediction on unseen data.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**黑客技能**：编程技能在快速处理数据中起着关键作用。机器学习算法被应用于创建输出，该输出将对未见数据进行预测。'
- en: '**Substantive Expertise**: This skill refers to the domain expertise in the
    field of the problem at hand. It helps the professional to be able to provide
    proper inputs to the system from which it can learn and to assess the appropriateness
    of the inputs and results obtained.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实质性专业知识**：这项技能指的是对当前问题领域领域专业知识。它帮助专业人士能够向系统提供适当的输入，以便从中学习，并评估输入和获得的结果的适当性。'
- en: To be a successful data science professional you need to have math, programming skills,
    as well as knowledge of the business domain.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为一名成功的数据科学专业人士，你需要具备数学、编程技能以及对商业领域的了解。
- en: As we can see, AI, data science, data analytics, data mining, and ML are all
    interlinked. All of these areas are the most in-demand domains in the industry
    right now. The right skill sets in combination with real-world experience will
    lead to a strong career in these areas which are currently trending. As ML forms
    the core of the leading space, the next section explores the various types of
    ML methods that may be applied to several real-world problems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，人工智能、数据科学、数据分析、数据挖掘和机器学习都是相互关联的。所有这些领域目前都是行业中最热门的领域。结合正确的技能集和实际经验，将引领你在这些当前趋势领域拥有强大的职业生涯。由于机器学习构成了领先领域的基础，下一节将探讨可能应用于多个实际问题的各种机器学习方法。
- en: 'ML is everywhere! Most of the time, we may be using something that is ML-based
    but don’t realize its existence or the influence that it has on our lives! Let''s
    explore together some very popular devices or applications that we experience
    on a daily basis, which are powered by ML:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习无处不在！大多数时候，我们可能在使用基于机器学习的东西，但并没有意识到它的存在或它对我们生活的影响！让我们一起探索一些我们每天都能体验到的非常流行的设备或应用程序，它们由机器学习驱动：
- en: '**Virtual personal assistants** (**VPAs**) such as **Google Allo**, **Alexa**,
    **Google Now**, **Google Home**, **Siri**, and so on'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟个人助理**（**VPAs**）如**Google Allo**、**Alexa**、**Google Now**、**Google Home**、**Siri**等等'
- en: Smart maps that show you traffic predictions, given your source and destination
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示你起点和终点交通预测的智能地图
- en: Demand-based price surging in Uber or similar transportation services
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Uber或类似交通服务中基于需求的价格上涨
- en: Automated video surveillance in airports, railway stations, and other public
    places
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机场、火车站和其他公共场所的自动视频监控
- en: Face recognition of individuals in pictures posted on social media sites such
    as Facebook
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在社交媒体网站如Facebook上发布的图片中的人脸识别
- en: Personalized news feeds served to you on Facebook
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Facebook上为你提供的个性化新闻推送
- en: Advertisements served to you on YouTube
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在YouTube上为你提供的广告
- en: People you may know suggestions on Facebook and other similar sites
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Facebook和其他类似网站上你可能认识的人的建议
- en: Job recommendations on LinkedIn, based on your profile
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据你的个人资料在LinkedIn上的工作推荐
- en: Automated responses on Google Mail
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Google Mail上的自动回复
- en: Chatbots that you converse with in online customer support forums
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在在线客户支持论坛中与你交谈的聊天机器人
- en: Search engine results filtering
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎结果过滤
- en: Email spam filtering
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邮件垃圾邮件过滤
- en: Of course, the list does not end here. The preceding applications mentioned
    are just a few of the basic ones that illustrate the influence that ML has on
    our lives today. It is not astonishing to quote that there is no subject area
    that ML has not touched!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个列表并没有结束。前面提到的应用只是基本的一些，它们展示了机器学习对我们今天生活的影响。引用没有哪个学科领域机器学习没有触及，这并不令人惊讶！
- en: The topics in this section are by no means an exhaustive description of ML,
    but just a quick touch point to get us started on a journey of exploration. Now
    that we have a basic understanding of what ML is and where it can be applied,
    let's delve deeper into other ML-related topics in the next section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的主题绝不是机器学习的详尽描述，而只是为了让我们开始探索之旅的一个快速接触点。现在我们已经对机器学习是什么以及它可以在哪里应用有了基本的了解，让我们在下一节中深入探讨其他机器学习相关主题。
- en: Types of ML methods
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习方法的类型
- en: Several types of tasks that aim at solving real-world problems can be achieved
    thanks to ML. An ML method generally means a group of specific types of algorithms
    that are suitable for solving a particular kind of problem and the method addresses
    any constraints that the problem brings along with it. For example, a constraint
    of a particular problem could be the availability of labeled data that can be
    provided as input to the learning algorithm.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以解决多种旨在解决现实世界问题的任务。一般来说，机器学习方法意味着一组特定的算法，这些算法适合解决特定类型的问题，并且该方法解决了问题带来的任何约束。例如，特定问题的约束可能是可用于作为学习算法输入的有标签数据的可用性。
- en: Essentially, the popular ML methods are supervised learning, unsupervised learning,
    semi-supervised learning, reinforcement learning, and transfer learning. The rest
    of this section details each of these methods.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，流行的机器学习方法是监督学习、无监督学习、半监督学习、强化学习和迁移学习。本节其余部分将详细说明这些方法。
- en: Supervised learning
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: A supervised learning algorithm is applied when one is very clear about the
    result that needs to be achieved from a problem, however one is unsure about the
    relationships between the data that affects the output. We would like the ML algorithm
    that we apply on the data to perceive these relationships between different data
    elements so as to achieve the desired output.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个人对需要从问题中实现的结果非常清楚，但对影响输出的数据之间的关系不确定时，就会应用监督学习算法。我们希望我们应用在数据上的机器学习算法能够感知不同数据元素之间的关系，以便实现预期的输出。
- en: 'The concept can be better explained with an example—at a bank, prior to extending
    a loan, they would like to predict if a loan applicant would pay the loan back. In
    this case, the problem is very clear. If a loan is extended to a prospective customer
    X, there are two possibilities: that X would successfully repay the loan or X
    would not repay the loan. The bank would like to use ML to identify the category
    into which customer X falls; that is, a successful repayer of the loan or a repayment
    defaulter.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念可以通过一个例子来更好地解释——在银行，在发放贷款之前，他们希望预测贷款申请人是否会偿还贷款。在这种情况下，问题非常明确。如果向潜在客户X发放贷款，有两种可能性：X会成功偿还贷款或X不会偿还贷款。银行希望使用机器学习来识别客户X所属的类别；即，成功的贷款偿还者或违约的贷款偿还者。
- en: While the problem definition that is to be solved is clear, please note that
    the features of a customer that will contribute to successful loan repayment or
    non-repayment are not clear and this is something we would like the ML algorithm
    to learn by observing the patterns in the data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然要解决的问题定义是清晰的，但请注意，对客户将有助于成功偿还或未偿还贷款的特征并不明确，这是我们希望机器学习算法通过观察数据中的模式来学习的东西。
- en: The major challenge here is that we need to provide input data that represents
    both customers that repaid their loans successfully and also customers that failed
    to repay. The bank can simply look at the historical data to get the records of
    customers in both categories and then label each record as paid or unpaid categories
    as appropriate.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的主要挑战是我们需要提供代表成功偿还贷款的客户和未能偿还贷款的客户的数据。银行可以简单地查看历史数据以获取两类客户的记录，然后根据适当的情况将每条记录标记为已支付或未支付类别。
- en: The records, thus labeled, now become input to a supervised learning algorithm
    so that it can learn the patterns of both categories of customers. The process
    of learning from the labeled data is called **training** and the output obtained
    (algorithm) from the learning process is called a **model**. Ideally, the bank
    would keep some part of the labeled data aside from training data so as to be
    able to test the model created, and this data is termed as **test data**. It should
    be no surprise that the labeled data that is used for training the model is called
    **training data**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因此标记的记录现在成为监督学习算法的输入，以便它可以学习两类客户的模式。从标记数据中学习的过程称为**训练**，学习过程中获得（算法）输出称为**模型**。理想情况下，银行会从训练数据中保留一部分标记数据，以便能够测试创建的模型，这种数据被称为**测试数据**。应该不会令人惊讶的是，用于训练模型的标记数据被称为**训练数据**。
- en: Once the model has been built, measurements are obtained by testing the model
    with test data to ensure the model yields a satisfactory level of performance,
    otherwise model-building iterations are carried out until the desired model performance
    is obtained. The model that achieved the desired performance on test data can
    be used by the bank to infer if any new loan applicant will be a future defaulter
    at all and, if so, make a better decision in terms of extending a loan to that
    applicant.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型建立，通过用测试数据测试模型来获取测量结果，以确保模型达到令人满意的性能水平，否则进行模型构建迭代，直到获得所需的模型性能。在测试数据上达到所需性能的模型可以由银行用来推断任何新的贷款申请人是否会成为未来的违约者，如果是的话，就可以在向该申请人提供贷款方面做出更好的决策。
- en: 'In a nutshell, supervised ML algorithms are employed when the objective is
    very clear and labeled data is available as input for the algorithm to learn the
    patterns from. The following diagram summarizes the supervised learning process:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，当目标非常明确且作为算法学习模式的输入有标签数据时，会使用监督机器学习算法。以下图表总结了监督学习过程：
- en: '![](img/db813db4-c27a-4c2c-80cf-3ba2fa40e95e.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/db813db4-c27a-4c2c-80cf-3ba2fa40e95e.png)'
- en: Supervised learning can be further divided into two categories, namely **classification**
    and **regression**. The prediction of a bank loan defaulter explained in this
    section is an example of classification and it aims to predict a label of a nominal
    type such as yes or no. On the other hand, it is also possible to predict numeric
    values (continuous values) and this type of prediction is called regression. An
    example of regression is predicting the monthly rental of a home in a prime location
    of a city based on features such as the demand for houses in the area, the number
    of bedrooms, the dimensions of the house, and accessibility to public transportation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习可以进一步分为两类，即**分类**和**回归**。本节中解释的银行贷款违约者预测是一个分类的例子，它旨在预测一个名义类型的标签，如是或否。另一方面，也可以预测数值（连续值），这种预测称为回归。回归的一个例子是根据该地区的房屋需求、卧室数量、房屋尺寸和公共交通的可达性来预测城市黄金地段房屋的月租金。
- en: Several supervised learning algorithms exist, and a few popularly known algorithms
    in this area include **classification and regression trees** (**CART**), logistic
    regression, linear regression, Naive Bayes, neural networks, **k-nearest neighbors**
    (**KNN**), and **support vector machine** (**SVM**).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种监督学习算法，该领域一些广为人知的算法包括**分类和回归树**（**CART**）、逻辑回归、线性回归、朴素贝叶斯、神经网络、**k-最近邻**（**KNN**）和**支持向量机**（**SVM**）。
- en: Unsupervised learning
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: The availability of labeled data is not very common and manually labeling data
    is also not cheap. This is the situation where unsupervised learning comes into
    play.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 标签数据的可用性并不常见，手动标注数据也不便宜。这就是无监督学习发挥作用的情况。
- en: For example, one small boutique firm wants to roll out a promotion to its customers,
    who are registered on their Facebook page. While the business objective is clear—that
    a promotion needs to be rolled out to customers—it is unclear as to which customer
    falls under which group. Unlike the supervised learning method where prior knowledge
    existed in terms of bad debtors and good debtors, in this case there are no such
    clues.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一家小型精品公司想要向在其Facebook页面上注册的客户推出促销活动。虽然业务目标很明确——需要向客户推出促销活动——但不清楚哪些客户属于哪个群体。与监督学习方法中存在关于不良债务人和良好债务人的先验知识不同，在这种情况下没有这样的线索。
- en: When the customer information is given as input to unsupervised learning algorithms,
    it tries to identify the patterns in the data and thereby groups the data of the
    customers with similar kinds of attributes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当将客户信息作为输入提供给无监督学习算法时，它试图识别数据中的模式，从而将具有相似属性的客户数据进行分组。
- en: Birds of the same feather flock together is the principle followed in customer
    grouping with unsupervised learning.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 同类鸟儿聚在一起，这是在无监督学习中客户分组的原理。
- en: 'The reasoning behind the formation of these organic groups from the grouping
    exercise may not be very intuitive. It may take some research to identify the
    factors that contributed to the gathering of a set of customers in a group. Most
    of the time, this research is manual and the data points in each group need verifying.
    This research may form the basis to determine the groups to which the particular
    promotion at hand needs to be rolled out. This application of unsupervised learning
    is called **clustering**. The following diagram shows the application of unsupervised
    ML to cluster the data points:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些有机群体形成的推理可能不是非常直观。可能需要一些研究来识别导致一组客户聚集在群体中的因素。大多数时候，这项研究是手动的，每个群体中的数据点需要验证。这项研究可能成为确定特定促销活动需要推出的群体的基础。这种无监督学习的应用称为**聚类**。以下图表显示了无监督机器学习在聚类数据点中的应用：
- en: '![](img/1cff7f3c-dd14-4b08-b656-b4607abb9fe4.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1cff7f3c-dd14-4b08-b656-b4607abb9fe4.png)'
- en: There are a number of clustering algorithms. However, the most popular ones
    are namely, k-means clustering, k-modes clustering, hierarchical clustering, fuzzy
    clustering, and so on.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多聚类算法。然而，最受欢迎的包括k-means聚类、k-modes聚类、层次聚类、模糊聚类等。
- en: Other forms of unsupervised learning do exist. For example, in retail industry,
    an unsupervised learning method called **association rule mining** is applied
    on customer purchases to identify the goods that are purchased together. In this
    case, unlike supervised learning, there is no need for labels at all. The task
    involved only requires the ML algorithm to identify the latent associations between
    the products that are billed together by customers. Having the information from
    association rule mining helps retailers place the products that are bought together
    in proximity. The idea is that customers can be intuitively encouraged to buy
    the extra products.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其他形式的无监督学习确实存在。例如，在零售行业，一种称为**关联规则挖掘**的无监督学习方法被应用于客户购买行为，以识别一起购买的物品。在这种情况下，与监督学习不同，根本不需要标签。所涉及的任务只需要机器学习算法识别客户一起计费的产品之间的潜在关联。从关联规则挖掘中获得的信息有助于零售商将一起购买的产品放置在附近。其理念是，可以直观地鼓励顾客购买额外的产品。
- en: A priori, **equivalence class transformation** (**Eclat**), and **frequency
    pattern growth** (**FPG**) are popular among the several algorithms that exist
    to perform association rule mining.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在现有的执行关联规则挖掘的算法中，**等价类转换**（**Eclat**）和**频率模式增长**（**FPG**）是流行的算法。
- en: Yet another form of unsupervised learning is anomaly detection or outlier detection.
    The goal of the exercise is to identify data points that do not belong to the
    rest of the elements that are given as input to the unsupervised learning algorithm.
    Similar to association rule mining, due to the nature of the problem at hand,
    there is no requirement for labels to be made use of by the algorithm to achieve
    the goal.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种形式的无监督学习是异常检测或离群点检测。该练习的目标是识别不属于无监督学习算法输入的其他元素的点。与关联规则挖掘类似，由于问题的性质，算法不需要使用标签来实现目标。
- en: Fraud detection is an important application of anomaly detection in the credit
    cards industry. Credit card transactions are monitored in real time and any spurious
    transaction patterns are flagged immediately to avoid losses to the credit card
    user as well as the credit card provider. The unusual pattern that is monitored
    for could be a huge transaction in a foreign currency rather than that of a normal
    currency in which the particular customer generally transacts. It could be transactions
    in physical stores located in two different continents on the same day. The general
    idea is to be able to flag up a pattern that is a deviation from the norm.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 欺诈检测是信用卡行业中异常检测的重要应用。信用卡交易实时监控，任何可疑的交易模式都会立即标记，以避免信用卡用户和信用卡提供商遭受损失。监控的不寻常模式可能是在外币中进行的巨额交易，而不是特定客户通常交易的正常货币。这可能是同一天在两个不同大陆的实体店中的交易。一般想法是能够标记出偏离常规的模式。
- en: K-means clustering and one-class SVM are two well-known unsupervised ML algorithms
    that are used to observe abnormalities in the population.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类和单类SVM是两种著名的无监督机器学习算法，用于观察人群中的异常情况。
- en: Overall, it may be understood that unsupervised learning is unarguably a very
    important method, given that labeled data used for training is a scarce resource.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，可以理解，无监督学习无疑是非常重要的一种方法，考虑到用于训练的标记数据是一种稀缺资源。
- en: Semi-supervised learning
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督学习
- en: Semi-supervised learning is a hybrid of both supervised and unsupervised methods.
    ML requires large amounts of data for training. Most of the time, a directly proportional
    relationship is observed between the amount of data used for model training and
    the performance of the model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习是监督和无监督方法的混合体。机器学习需要大量的数据用于训练。大多数时候，观察到模型训练所使用的数据量与模型性能之间存在直接的正比关系。
- en: In niche domains such as medical imagining, a large amount of image data (MRIs,
    x-rays, CT scans) is available. However, the time and availability of qualified
    radiologists to label these images is scarce. In this situation, we might end
    up getting only a few images labeled by radiologists.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在如医学成像等利基领域，有大量的图像数据（MRI、X光、CT扫描）可用。然而，合格的放射科医生的时间和可用性来标记这些图像是稀缺的。在这种情况下，我们可能只能获得少数由放射科医生标记的图像。
- en: 'Semi-supervised learning takes advantage of the few labeled images by building
    an initial model that is used to label the large amount of unlabeled data that
    exists in the domain. Once the large amount of labeled data is available, a supervised
    ML algorithm may be used to train and create a final model that is used for prediction
    tasks on the unseen data. The following diagram illustrates the steps involved
    in semi-supervised learning:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习通过构建一个初始模型来利用少量标记图像，该模型用于标记领域中存在的大量未标记数据。一旦有大量标记数据可用，可以使用监督机器学习算法来训练和创建一个用于在未见数据上的预测任务的最终模型。以下图表说明了半监督学习涉及到的步骤：
- en: '![](img/73aa5ccf-6d2b-4ede-a32f-cefe1c973f03.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/73aa5ccf-6d2b-4ede-a32f-cefe1c973f03.png)'
- en: Speech analysis, protein synthesis, and web content classifications are certain
    areas where large amounts of unlabeled data and fewer amounts of labeled data
    are available. Semi-supervised learning is applied in these areas with successful
    results.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 语音分析、蛋白质合成和网页内容分类是某些领域，在这些领域中，存在大量未标记的数据和较少的标记数据。在这些领域应用半监督学习取得了成功的结果。
- en: '**Generative adversarial networks** (**GANs**), **semi-supervised support vector
    machines** (**S3VMs**), graph-based methods, and **Markov** chain methods are
    well-known methods among others in the semi-supervised ML area.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）、**半监督支持向量机**（**S3VMs**）、基于图的方法和**马尔可夫**链方法是半监督机器学习领域中众所周知的方法。'
- en: Reinforcement learning
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: '**Reinforcement learning** (**RL**) is an ML method that is neither supervised
    learning nor unsupervised learning. In this method, a reward definition is provided
    as input to this kind of a learning algorithm at the start. As the algorithm is
    not provided with labeled data for training, this type of learning algorithm cannot
    be categorized as supervised learning. On the other hand, it is not categorized
    as unsupervised learning, as the algorithm is fed with information on reward definition
    that guides the algorithm through taking the steps to solve the problem at hand.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是一种既不是监督学习也不是无监督学习的机器学习方法。在这种方法中，一开始就向这种学习算法提供奖励定义作为输入。由于算法没有提供用于训练的标记数据，这种学习算法不能归类为监督学习。另一方面，它也不归类为无监督学习，因为算法被提供了关于奖励定义的信息，这些信息指导算法通过采取解决问题的步骤。'
- en: Reinforcement learning aims to improve the strategies used to solve any problem
    continuously by relying on the feedback received. The goal is to maximize the
    rewards while taking steps to solve the problem. The rewards obtained are computed
    by the algorithm itself going by the rewards and penalty definitions. The idea
    is to achieve optimal steps that maximize the rewards to solve the problem at
    hand.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习旨在通过依赖收到的反馈来不断改进解决任何问题的策略。目标是最大化奖励，同时采取解决问题的步骤。获得的奖励由算法本身根据奖励和惩罚定义来计算。其理念是实现最优步骤，以最大化奖励来解决当前问题。
- en: 'The following diagram is an illustration depicting a robot automatically determining
    the ideal behavior through a reinforcement learning method within the specific
    context of fire:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了通过强化学习方法在火灾特定情境下自动确定理想行为的机器人：
- en: '![](img/bbe12641-25e0-49dd-95d5-55f01b9f58aa.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bbe12641-25e0-49dd-95d5-55f01b9f58aa.png)'
- en: A machine outplaying humans in an Atari video game is termed as one of the foremost
    success stories of reinforcement learning. To achieve this feat, a large number
    of example games played by humans are fed as input to the algorithm that learned
    the steps to take to maximize the reward. The reward in this case is the final
    score. The algorithm, post learning from the example inputs, just simulated the
    pattern at each step of the game that eventually maximized the score obtained.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一台机器在Atari视频游戏中超越人类被称为强化学习最显著的成功故事之一。为了实现这一壮举，将大量人类玩过的示例游戏作为输入提供给算法，该算法学习了采取最大化奖励的步骤。在这种情况下，奖励是最终得分。算法在从示例输入中学习后，仅模拟了游戏中每个步骤的模式，最终最大化了获得的分数。
- en: 'Though it might appear that reinforcement learning can be applied to game scenarios
    only, there are numerous use cases for this method in industry as well. The following
    examples mentioned are three such use cases:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然强化学习可能看起来只能应用于游戏场景，但在工业界也有许多这种方法的应用案例。以下提到的例子是其中三个这样的案例：
- en: Dynamic pricing of goods and services based on spontaneous supply and demand
    targeted at achieving profit maximization is achieved through a variant of reinforcement
    learning called **Q-learning**.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于自发的供需关系动态定价商品和服务，以实现利润最大化，是通过强化学习的一种变体**Q学习**来实现的。
- en: Effective use of space in warehouses is a key challenge faced by inventory management
    professionals. Market demand fluctuations, the large availability of inventory
    stocks, and delays in refilling the inventory are the key constraints that affect
    space utilization. Reinforcement learning algorithms are used to optimize the
    time to procure inventory as well as to reduce the time to retrieve the goods
    from warehouses, thereby directly impacting the space management issue referred
    to as a problem in the inventory management area.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在仓库中有效利用空间是库存管理专业人士面临的关键挑战。市场需求波动、大量库存的可用性和库存补充的延迟是影响空间利用的关键约束。强化学习算法用于优化采购库存的时间以及从仓库中检索商品的时间，从而直接影响到被称为库存管理领域的空间管理问题。
- en: Prolonged treatments and differential drug administration is required in medical
    science to treat diseases such as cancer. The treatments are highly personalized,
    based on the characteristics of the patient. Treatment often involves variations
    of the treatment strategy at various stages. This kind of treatment plan is typically
    referred to as a **dynamic treatment regime** (**DTR**). Reinforcement learning
    helps with processing the clinical trials data to come up with the appropriate
    personalized DTR for the patient, based on the characteristics of the patient
    that are fed in as inputs to the reinforcement learning algorithm.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在医学科学中，为了治疗如癌症等疾病，需要长期治疗和差异化的药物管理。治疗高度个性化，基于患者的特征。治疗通常涉及在各个阶段的治疗策略的变化。这种治疗计划通常被称为**动态治疗制度**（**DTR**）。强化学习有助于处理临床试验数据，根据输入到强化学习算法的患者特征，提出适当的个性化DTR。
- en: There are four very popular reinforcement learning algorithms, namely Q-learning,
    **state-action-reward-state-action** (**SARSA**), **deep Q network** (**DQN**),
    and **deep deterministic policy gradient** (**DDPG**).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有四种非常流行的强化学习算法，分别是Q学习、**状态-动作-奖励-状态-动作**（**SARSA**）、**深度Q网络**（**DQN**）和**深度确定性策略梯度**（**DDPG**）。
- en: Transfer learning
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习
- en: The reusability of code is one of the fundamental concepts of **object-oriented
    programming** (**OOP**) and it is pretty popular in the software-engineering world.
    Similarly, transfer learning involves reusing a model built to achieve a specific
    task to solve another related task.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的可重用性是**面向对象编程**（**OOP**）的基本概念之一，在软件工程领域非常流行。同样，迁移学习涉及重用为完成特定任务而构建的模型来解决另一个相关任务。
- en: It is understandable that to achieve better performance measurements, ML models
    need to be trained on large amounts of labeled data. The availability of fewer
    amounts of data means less training and the result is a model with suboptimal
    performance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要达到更好的性能测量，机器学习模型需要在大量标记数据上进行训练是理所当然的。数据量较少意味着训练较少，结果是性能次优的模型。
- en: 'Transfer learning attempts to solve the problems arising from the availability
    of fewer amounts of data by reusing the knowledge obtained by a different related
    model. Having fewer data points available to train a model should not impede building
    a better model, which is the core concept behind transfer learning. The following
    diagram is an illustration showing the purpose of transfer learning in an image
    recognition task that classifies dog and cat images:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习试图通过重用不同相关模型获得的知识来解决数据量较少时出现的问题。拥有较少的数据点来训练模型不应该阻碍构建更好的模型，这是迁移学习的核心概念。以下图表展示了迁移学习在图像识别任务中的目的，该任务用于分类狗和猫的图像：
- en: '![](img/cbce050e-7b24-4339-bb63-ffa0ddbff955.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cbce050e-7b24-4339-bb63-ffa0ddbff955.png)'
- en: In this task, a neural network model is involved with detecting the edges, color
    blob detection, and so on in the first few layers. Only at the progressive layers
    (maybe in the last few layers) does the model attempt to identify the facial features
    of dogs or cats in order to classify them as one of the targets (a dog or a cat).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，一个神经网络模型涉及到在第一层中检测边缘、颜色块检测等。只有在渐进层（可能在最后几层）中，模型才会尝试识别狗或猫的面部特征，以便将它们分类为目标之一（狗或猫）。
- en: It may be observed that the tasks of identifying edges and color blobs are not
    specific to cats' and dogs' images. The knowledge to infer edges or color blobs
    may be generally inferred even if a model is trained on non-dog or non-cat images.
    Eventually, if this knowledge is clubbed with knowledge derived from inferring
    cat faces versus dog faces, even if they are small in number, we will have a better
    model than the suboptimal model obtained by training on fewer images.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会观察到，识别边缘和颜色块的任务并不特定于猫和狗的图像。即使模型是在非狗或非猫的图像上训练的，也可能一般地推断出边缘或颜色块的知识。最终，如果将这种知识结合从推断猫脸与狗脸中得出的知识，即使数量较少，我们也将拥有比在较少图像上训练得到的次优模型更好的模型。
- en: In the case of a dogs-cats classifier, first, a model is trained on a large
    set of images that are not confined to cats' and dogs' images. The model is then
    taken and the last few layers are retrained on the dogs' and cats' faces. The
    model, thus obtained, is then tested and used post evidencing performance measurements
    that are satisfactory.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在狗-猫分类器的情况下，首先，在一个大型图像集上训练一个模型，这些图像不仅限于猫和狗的图像。然后，将模型取来，在狗和猫的脸上重新训练最后几层。因此获得的模型经过测试并使用后，证明了性能测量结果是令人满意的。
- en: The concept of transfer learning is used not just for image-related tasks. Another
    example of it being used is in **natural language processing** (**NLP**) where
    it can perform sentiment analysis on text data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的概念不仅用于图像相关任务。另一个例子是它在**自然语言处理**（**NLP**）中的应用，它可以对文本数据进行情感分析。
- en: Assume a company that launched a new product has a concept that never existed
    before (say, for now, a flying car). The task is to analyze the tweets related
    to the new product and identify each of them as being of positive, negative, or
    neural sentiment. It may be observed that prior, labeled tweets are unavailable
    in the flying car's domain. In such cases, we can take a model built based on
    the labeled data of generic product reviews for several products and domains.
    We can reuse the model by supplementing it with flying-car-domain-specific terminology
    to avail a new model. This new model will be finally used for testing and deploying
    to analyze sentiment on the tweets obtained about the newly launched flying cars.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一家公司推出了一款前所未有的新产品（比如说，现在是一架飞行汽车）。任务是分析与新产品质量相关的推文，并将每一条推文识别为正面、负面或中性情感。可能会观察到，在飞行汽车领域，之前标记的推文是不可用的。在这种情况下，我们可以使用基于多个产品和领域通用产品评论标记数据的模型。我们可以通过补充飞行汽车领域特定的术语来重用该模型，从而获得一个新的模型。这个新模型最终将用于测试和部署，以分析关于新推出的飞行汽车的推文中的情感。
- en: 'It is possible to achieve transfer learning through the following two ways:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过以下两种方式实现迁移学习：
- en: By reusing one's own model
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重用自己训练的模型
- en: By reusing a pretrained model
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重用预训练模型
- en: Pretrained models are models built by various organizations or individuals as
    part of their research work or as part of a competition. These models are generally
    very complex and are trained on large amounts of data. They are also optimized
    to perform their tasks with high precision. These models may take days or weeks
    to train on modern hardware. Organizations or individuals often release these
    models under permissive license for reuse. Such pretrained models can be downloaded
    and reused through the transfer-learning paradigm. This will effectively make
    use of the vast existing knowledge that the pretrained models possess, which would
    otherwise be hard to attain for an individual with limited hardware resources
    and amounts of data to train.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型是由各种组织或个人作为其研究工作或作为比赛的一部分构建的模型。这些模型通常非常复杂，并且在大量的数据上进行了训练。它们也被优化以高精度执行其任务。这些模型在现代硬件上训练可能需要几天或几周的时间。组织或个人通常会以许可许可证的形式发布这些模型以供重用。通过迁移学习范式可以下载并重用这些预训练模型。这将有效地利用预训练模型所拥有的大量现有知识，这对于有限的硬件资源和数据量的个人来说可能难以获得。
- en: 'There are several pretrained models made available by various parties. The
    following described are some of the popular pretrained models:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 有多个预训练模型由不同的机构提供。以下描述的是一些流行的预训练模型：
- en: '**Inception-V3 model**: This model has been trained on ImageNet as part of
    a large visual recognition challenge. The competition required the participants
    to classify a given image into one of 1,000 classes. Some of the classes include
    the names of animals and object names.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Inception-V3模型**：这个模型作为大型视觉识别挑战的一部分在ImageNet上进行了训练。比赛要求参与者将给定的图像分类为1000个类别之一。其中一些类别包括动物名称和物体名称。'
- en: '**MobileNet**: This pretrained model has been built by Google and it is meant
    to perform object detection using the ImageNet database. The architecture is designed
    for mobiles.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MobileNet**：这个预训练模型是由谷歌构建的，旨在使用ImageNet数据库进行目标检测。其架构是为移动设备设计的。'
- en: '**VCG Face**: This is a pretrained model built for face recognition.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VCG Face**：这是一个为面部识别构建的预训练模型。'
- en: '**VCG 16**: This is a pretrained model trained on the **MS COCO** dataset.
    This one accomplishes image captioning; that is, given an input image, it generates
    a caption describing the image''s contents.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VCG 16**：这是一个在**MS COCO**数据集上训练的预训练模型。这个模型实现了图像描述；也就是说，给定一个输入图像，它生成一个描述图像内容的字幕。'
- en: '**Google''s Word2Vec model and Stanford''s GloVe model**: These pretrained
    models take text as input and produce word vectors as output. Distributed word
    vectors offer one form of representing documents for NLP or ML applications.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**谷歌的Word2Vec模型和斯坦福的GloVe模型**：这些预训练模型以文本作为输入，并生成单词向量作为输出。分布式单词向量是表示文档的一种形式，用于自然语言处理（NLP）或机器学习（ML）应用。'
- en: Now that we have a basic understanding of various possible ML methods, in the
    next section, we focus on quickly reviewing the key terminology used in ML.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对各种可能的机器学习方法有了基本的了解，在下一节中，我们将快速回顾机器学习中使用的核心术语。
- en: ML terminology – a quick review
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习术语 – 快速回顾
- en: In this section, we take the popular ML terms and review them. This non-exhaustive
    review will helps us as a quick refresher and enable us to follow the projects
    covered by this book without any hiccups.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了流行的机器学习术语。这个非详尽的回顾将帮助我们快速复习，并使我们能够顺利地跟随本书涵盖的项目。
- en: Deep learning
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习
- en: This is a revolutionary trend and has become a super-hot topic in recent times
    in the ML world. It is a category of ML algorithms that use **artificial neural
    networks** (**ANNs**) with multiple hidden layers of neurons to address problems.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个革命性的趋势，在最近的机器学习（ML）世界中已经成为一个超级热门的话题。它是一类使用具有多个隐藏层神经元的**人工神经网络**（**ANNs**）来解决问题的机器学习算法。
- en: Superior results are obtained by applying deep learning to several real-world
    problems. **Convolutional neural networks** (**CNNs**), **recurrent neural networks **(**RNNs**)
    **autoencoders **(**AEs**), **generative adversarial networks** (**GANs**), and **deep
    belief networks** (**DBNs**) are some of the popular deep learning methods.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将深度学习应用于几个现实世界问题，可以获得优越的结果。**卷积神经网络**（**CNNs**）、**循环神经网络**（**RNNs**）、**自编码器**（**AEs**）、**生成对抗网络**（**GANs**）和**深度信念网络**（**DBNs**）是一些流行的深度学习方法。
- en: Big data
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据
- en: The term refers to large volumes of data that combine both structured data types
    (rows and columns similar to a table) and unstructured data types (text documents,
    voice recordings, image data, and so on). Due to the volume of data, it does not
    fit into the main memory of the hardware where ML algorithms need to be executed.
    Separate strategies are needed to work on these large volumes of data. Distributed
    processing of the data and combining the results (typically called **MapReduce**)
    is one strategy. It is also possible to process just enough data sequentially that
    can fit in a main memory each time and store the results somewhere on a hard drive;
    we need to repeat this process until the entirety of the data is processed completely.
    After the data processing, the results need to be combined to avail the final
    results of all the data that has been processed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语指的是大量数据，这些数据结合了结构化数据类型（类似于表格的行和列）和非结构化数据类型（文本文档、语音录音、图像数据等）。由于数据量很大，它不适合机器学习算法需要执行的主存储器。需要单独的策略来处理这些大量数据。数据分布式处理和结果合并（通常称为**MapReduce**）是一种策略。也有可能每次只处理足够的数据，使其能够适应主存储器，并将结果存储在硬盘上的某个地方；我们需要重复这个过程，直到完全处理完所有数据。数据处理后，需要将结果合并，以获得所有已处理数据的最终结果。
- en: Special technologies such as Hadoop and Spark are required to perform ML on
    big data. Needless to say, you will need to hone specialized skills in order to
    apply ML algorithms successfully using these technologies on big data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要在大数据上执行机器学习，需要特殊技术，如Hadoop和Spark。不用说，你需要磨练专门技能，以便成功地将这些技术应用于大数据上的机器学习算法。
- en: Natural language processing
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: This is an application area of ML that aims for computers to comprehend human
    languages such as English, French, and Mandarin. NLP applications enable users
    to interact with computers using spoken languages.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机器学习的一个应用领域，旨在使计算机理解人类语言，如英语、法语和普通话。自然语言处理应用允许用户使用口语与计算机交互。
- en: Chatbot, speech synthesis, machine translation, text classification and clustering,
    text generation, and text summarization are some of the popular applications of
    NLP.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天机器人、语音合成、机器翻译、文本分类和聚类、文本生成和文本摘要是一些流行的自然语言处理应用。
- en: Computer vision
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: This field of ML tries to mimic human vision. The aim is to enable computers
    to see, process, and determine the objects in images or videos. Deep learning
    and the availability of powerful hardware has led to the rise of very powerful
    applications in this area of ML.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个机器学习领域试图模仿人类的视觉。目标是使计算机能够看到、处理和确定图像或视频中的对象。深度学习和强大硬件的可用性导致了该领域机器学习非常强大应用的出现。
- en: Autonomous vehicles such as self-driving cars, object recognition, object tracking,
    motion analysis, and the restoration of images are some of the applications of
    computer vision.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶汽车等自动驾驶汽车、物体识别、物体跟踪、运动分析和图像恢复是计算机视觉的一些应用。
- en: Cost function
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 成本函数
- en: Cost function, loss function, or error function are used interchangeably by
    practitioners. Each is used to define and measure the error of a model. The objective
    for the ML algorithm is to minimize the loss from the dataset.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数、损失函数或误差函数在实践中可以互换使用。每个都用于定义和衡量模型的误差。机器学习算法的目标是最小化数据集中的损失。
- en: Some of the examples of cost function are square loss that is used in linear
    regression, hinge loss that is used in support vector machines and 0/1 loss used
    to measure accuracy in classification algorithms.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数的一些例子包括用于线性回归的平方损失、用于支持向量机的hinge损失以及用于分类算法中准确度测量的0/1损失。
- en: Model accuracy
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型准确度
- en: Accuracy is one of the popular metrics used to measure the performance of ML
    models. The measurement is easy to understand and helps the practitioner to communicate
    the goodness of a model very easily to its business users.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度是用于衡量机器学习模型性能的流行指标之一。这个测量方法易于理解，并有助于从业者非常容易地向其商业用户传达模型的良好性能。
- en: Generally, this metric is used for classification problems. Accuracy is measured
    as the number of correct predictions divided by the total number of predictions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，这个指标用于分类问题。准确度是通过正确预测的数量除以总预测数量来衡量的。
- en: Confusion matrix
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: This is a table that describes the classification model's performance. It is
    an *n* rows, *n* columns matrix where *n* represents the number of classes that
    are predicted by the classification model. It is formed by noting down the number
    of correct and incorrect predictions by the model when compared to the actual
    label.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个描述分类模型性能的表格。它是一个 *n* 行 *n* 列的矩阵，其中 *n* 代表分类模型预测的类别数量。它通过记录模型在比较实际标签时的正确和错误预测数量而形成。
- en: 'Confusion matrices are better explained with an example—assume that there are
    100 images in a dataset where there are 50 dog images and 50 cat images. A model
    that is built to classify images as cat images or dog images is given this dataset.
    The output from the model showed that 40 dog images are classified correctly and
    20 cat images are predicted correctly. The following table is the confusion matrix
    construction from the prediction output of the model:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵可以通过一个例子更好地解释——假设数据集中有100张图片，其中包含50张狗图片和50张猫图片。一个旨在将图像分类为猫图像或狗图像的模型被给出这个数据集。模型的输出显示，40张狗图片被正确分类，20张猫图片被正确预测。以下表格是模型预测输出的混淆矩阵构建：
- en: '| Model predicted labels | **Actual labels** |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 模型预测标签 | **实际标签** |'
- en: '|  | cats | dogs |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | 猫 | 狗 |'
- en: '| cats | 20 | 30 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 猫 | 20 | 30 |'
- en: '| dogs | 10 | 40 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 狗 | 10 | 40 |'
- en: Predictor variables
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测变量
- en: These variables are otherwise called **independent variables** or **x-values**.
    These are the input variables that help to predict the dependent or target or
    response variable.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量也被称为**独立变量**或**x值**。这些是帮助预测因变量或目标或响应变量的输入变量。
- en: In a house rent prediction use case, the size of the house in square feet, the
    number of bedrooms, the number of houses available unoccupied in the region, the
    proximity to public transport, the accessibility to facilities such as hospitals
    and schools are all some examples of predictor variables that determine the rental
    cost of the house.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在房屋租赁预测用例中，房屋的面积（平方英尺）、卧室数量、该地区空置房屋的数量、距离公共交通的远近、以及医院和学校等设施的可达性都是一些预测变量，它们决定了房屋的租金。
- en: Response variable
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 响应变量
- en: Dependent variables or target or y-values are all interchangeably used by practitioners
    as alternatives for the term **response variable**. This is the variable the model
    predicts as output based on the independent variables that are provided as input
    to the model.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 实践者将因变量或目标或y值互换地用作**响应变量**的替代词。这是模型根据作为模型输入的独立变量预测的输出变量。
- en: In the house rent prediction use case, the rent predicted is the response variable.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在房屋租赁预测用例中，预测的租金是响应变量。
- en: Dimensionality reduction
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: Feature reduction (or feature selection) or dimensionality reduction is the
    process of reducing the input set of independent variables to obtain a lesser
    number of variables that are really required by the model to predict the target.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 特征减少（或特征选择）或降维是将独立变量的输入集减少到所需变量数量更少的过程，这些变量是模型预测目标所需的。
- en: In certain cases, it is possible to represent multiple dependent variables by
    combining them together without losing much information. For example, instead
    of having two independent variables such as the length of a rectangle and the
    breath of a rectangle, the dimensions can be represented by only one variable
    called the area that represents both the length and breadth of the rectangle.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，可以通过组合多个因变量来表示它们，而不会丢失太多信息。例如，与其有两个独立变量，如矩形的长度和矩形的宽度，不如用一个称为面积的单个变量来表示这些维度，该变量代表矩形的长度和宽度。
- en: 'The following mentioned are the multiple reasons we need to perform a dimensionality
    reduction on a given input dataset:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们需要对给定输入数据集执行降维的多个原因：
- en: To aid data compression, therefore accommodate the data in a smaller amount
    of disk space.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了帮助数据压缩，因此适应更小的磁盘空间。
- en: The time to process the data is reduced as fewer dimensions are used to represent
    the data.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更少的维度来表示数据时，数据处理的时间会减少。
- en: It removes redundant features from datasets. Redundant features are typically
    known as **multicollinearity** in data.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它从数据集中移除了冗余特征。冗余特征通常在数据中被称为**多重共线性**。
- en: Reducing the data to fewer dimensions helps visualize the data through graphs
    and charts.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据减少到更少的维度有助于通过图表和图表可视化数据。
- en: Dimensionality reduction removes noisy features from the dataset which, in turn,
    improves the model performance.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降维从数据集中移除噪声特征，从而提高模型性能。
- en: There are many ways by which dimensionality reduction can be attained in a dataset.
    The use of filters, such as information gain filters, and symmetric attribute
    evaluation filters, is one way. Genetic-algorithm-based selection and **principal
    component analysis** (**PCA**) are other popular techniques used to achieve dimensionality
    reduction. Hybrid methods do exist to attain feature selection.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中实现降维有许多方法。使用过滤器，例如信息增益过滤器和对称属性评估过滤器，是一种方法。基于遗传算法的选择和**主成分分析**（**PCA**）是其他流行的技术，用于实现降维。存在混合方法来实现特征选择。
- en: Class imbalance problem
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类不平衡问题
- en: Let's assume that one needs to build a classifier that identifies cat and dog
    images. The problem has two classes namely cat and dog. If one were to train a
    classification model, training data is required. The training data in this case
    is based on images of dogs and cats given as input so a supervised learning model
    can learn the features of dogs versus cats.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 假设需要构建一个分类器，用于识别猫和狗的图像。这个问题有两个类别，即猫和狗。如果训练一个分类模型，则需要训练数据。在这种情况下，训练数据是基于作为输入提供的狗和猫的图像，以便监督学习模型可以学习狗与猫的特征。
- en: It may so happen that if there are 100 images available for training in the
    dataset and 95 of them are dog pictures, five of them are cat pictures. This kind
    of unequal representation of different classes in a training dataset is termed
    as a class imbalance problem.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会出现这样的情况：如果数据集中有100张用于训练的图像，其中95张是狗的图片，5张是猫的图片。这种不同类别在训练数据集中的不平等表示被称为类不平衡问题。
- en: Most ML techniques work best when the number of examples in each class are roughly
    equal. One can employ certain techniques to counter class imbalance problems in
    data. One technique is to reduce the majority class (images of dogs) samples and
    make them equal to the minority class (images of cats). In this case, there is
    information loss as a lot of the dog images go unused. Another option is to generate
    synthetic data similar to the data for the minority class (images of cats) so
    as to make the number of data samples equal to the majority class. **Synthetic
    minority over-sampling technique** (**SMOTE**) is a very popular technique for
    generating synthetic data.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习技术在工作时，每个类别的示例数量大致相等时效果最好。可以采用某些技术来应对数据中的类不平衡问题。一种技术是减少多数类（狗的图像）样本，使它们与少数类（猫的图像）相等。在这种情况下，会有信息损失，因为许多狗的图像未被使用。另一种选择是生成与少数类（猫的图像）数据相似的人工数据，以便使数据样本的数量与多数类相等。**合成少数过采样技术**（**SMOTE**）是生成人工数据的一种非常流行的技术。
- en: It may be noted that accuracy is not a good metric for evaluating the performance
    of models where the training dataset experiences class imbalance problems. Assume
    a model built based on a class-imbalanced dataset that predicts a majority class
    for any test sample that it is asked to predict on. In this case, one gets 95%
    accuracy as roughly 95% of the images are dog images in the test dataset. But
    this performance can only be termed as a hoax as the model does not have any discriminative
    power—it just predicts dog as the class for any image it needs to predict about.
    In this case, it just happened that every image is predicted as a dog, but still
    the model got away with a very high accuracy indicating that it is a great model,
    whether it is in reality or not!
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，准确率不是评估训练数据集存在类不平衡问题时模型性能的好指标。假设基于类不平衡数据集构建的模型预测任何测试样本为多数类。在这种情况下，得到95%的准确率，因为测试数据集中大约95%的图像是狗的图像。但这个性能只能被称为骗局，因为模型没有任何区分能力——它只是预测任何需要预测的图像为狗的类别。在这种情况下，所有图像都被预测为狗，但模型仍然以非常高的准确率逃脱，这表明它是一个很好的模型，无论它是否真的是这样！
- en: There are several other performance metrics available to use in a situation
    where a class imbalance is a problem, F1 score and the **area under the curve
    of the receiver operating characteristic** (**AUCROC**) are some of the popular
    ones.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在类不平衡问题时，有几种其他性能指标可供使用，F1分数和**受试者工作特征曲线下面积**（**AUCROC**）是一些流行的指标。
- en: Model bias and variance
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型偏差和方差
- en: While several ML algorithms are available to build models, model selection can
    be done on the basis of the bias and variance errors that the models produce.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有几种机器学习算法可以构建模型，但可以根据模型产生的偏差和方差误差来选择模型。
- en: Bias error occurs when the model has a limited capability to learn the true
    signals from a dataset provided as input to it. Having a highly biased model essentially
    means the model is consistent but inaccurate on average.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差误差发生在模型从提供给它的数据集中学习真实信号的能力有限时。一个高度偏差的模型本质上意味着模型在平均情况下是一致的但不够准确。
- en: Variance errors occur when the models are too sensitive to the training datasets
    with which they are trained. Having high variance in a model essentially means
    that the trained model will produce high accuracies on any test dataset on average,
    but their predictions are inconsistent.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 方差误差发生在模型对其训练数据集过于敏感时。一个模型具有高方差本质上意味着训练好的模型在平均情况下对任何测试数据集都会产生高精度，但它们的预测结果不一致。
- en: Underfitting and overfitting
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合
- en: Underfitting and overfitting are the concepts closely associated with bias and
    variance. These two are the biggest causes for the poor performance of the models,
    therefore a practitioner has to pay very close attention to these issues while
    building ML models.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合是与偏差和方差密切相关的概念。这两个是模型表现不佳的最大原因，因此，在构建机器学习模型时，从业者必须非常关注这些问题。
- en: A situation where the model does not perform well with both training data as
    well as test data is termed as underfitting. This situation can be detected by
    observing high training errors and test errors. Having an underfitting problem
    means that the ML algorithm chosen to fit the model is not suitable to model the
    features of the training data. Therefore, the only remedy is to try other kinds
    of ML algorithms to model the data.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型在训练数据和测试数据上表现都不好时，这种情况被称为欠拟合。这种情况可以通过观察高训练误差和高测试误差来检测。存在欠拟合问题意味着用于拟合模型的机器学习算法不适合对训练数据的特征进行建模。因此，唯一的补救措施是尝试其他类型的机器学习算法来建模数据。
- en: Overfitting is a situation where the model learned the features of the training
    data so well that it fails to generalize on other unseen data. In an overfitting
    model, noise or random fluctuations in the training data are considered as true
    signals by the model and it looks for these patterns in unseen data as well, therefore
    impacting the poor model performance.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是一种情况，其中模型对训练数据的特征学习得如此之好，以至于它无法泛化到其他未见数据。在过拟合模型中，模型将训练数据中的噪声或随机波动视为真实信号，并在未见数据中也寻找这些模式，因此影响了模型的表现。
- en: Overfitting is more prevalent in non-parametric and non-linear models such as
    decision trees, and neural networks. Pruning the trees is one remedy to overcome
    the problem. Another remedial measure is a technique called **dropout** where
    some of the features learned from the model are dropped randomly from the model
    therefore making the model more generalizable to unseen data. Regularization is
    yet another technique to resolve overfitting problems. This is attained by penalizing
    the coefficients of the model so that the model generalizes better. L1 penalty
    and L2 penalty are the types of penalties through which regularization can be
    performed in regression scenarios.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合在非参数和非线性模型中更为普遍，如决策树和神经网络。剪枝是克服这一问题的补救措施之一。另一种补救措施是称为**dropout**的技术，其中从模型中随机丢弃一些学习到的特征，从而使模型对未见数据更具泛化能力。正则化是解决过拟合问题的另一种技术。这是通过对模型系数进行惩罚来实现的，从而使模型更好地泛化。L1惩罚和L2惩罚是正则化在回归场景中可以执行的类型。
- en: The goal for a practitioner is to ensure that the model neither overfits nor
    underfits. To achieve this, it is essential to learn when to stop training the
    ML data. One could plot the training error and validation error (an error that
    is measured on a small portion of the training dataset that is kept aside) on
    a chart and identify the point where the training data keeps decreasing, however
    the validation error starts to rise.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于从业者来说，目标是确保模型既不过拟合也不欠拟合。为了实现这一点，学习何时停止训练机器学习数据至关重要。可以在图表上绘制训练误差和验证误差（在一个小部分训练数据集上测量的误差，这部分数据被保留），并确定训练数据持续下降，而验证误差开始上升的点。
- en: At times, obtaining performance measurement on training data and expecting a
    similar measurement to be obtained on unseen data may not work. A more realistic
    training and test performance estimate is to be obtained from a model by adopting
    a data-resampling technique called k-fold cross validation. The *k* in k-fold
    cross validation refers to a number; examples include 3-fold cross validation,
    5-fold cross validation, and 10-fold cross validation. The **k-fold cross validation**
    technique involves dividing the training data into *k* parts and running the training
    process *k* + 1 times. In each iteration, the training is performed on *k* - 1
    partitions of the data and the *k*^(th) partition is used exclusively for testing.
    It may be noted that the *k*^(th) partition for testing and *k* - 1 partitions
    for training are shuffled in each iteration, therefore the training data and testing
    data do not stay constant in each iteration. This approach enables getting a pessimistic
    measurement of performance that can be expected from the model on the unseen data
    in the future.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在训练数据上获得性能度量并期望在未见过的数据上获得相似度量可能不起作用。通过采用称为k折交叉验证的数据重采样技术，可以从模型中获得更现实的训练和测试性能估计。k折交叉验证中的*k*代表一个数字；例如，包括3折交叉验证、5折交叉验证和10折交叉验证。**k折交叉验证**技术涉及将训练数据分成*k*部分，并运行训练过程*k*+1次。在每次迭代中，训练在数据*k*
    - 1个分区上进行，而*k*^(th)分区则专门用于测试。需要注意的是，在每次迭代中，用于测试的*k*^(th)分区和用于训练的*k* - 1个分区都会进行洗牌，因此训练数据和测试数据在每个迭代中都不会保持不变。这种方法使得能够获得对未来未见数据中模型可预期的性能的悲观度量。
- en: 10-fold cross validation with 10 runs to obtain model performance is considered
    to be a gold standard estimate for a model's performance among practitioners.
    Estimating the model's performance in this way is always recommended in industrial
    setups and for critical ML applications.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，使用10折交叉验证和10次运行来获得模型性能被认为是对模型性能的黄金标准估计。在工业设置和关键机器学习应用中，始终推荐以这种方式估计模型性能。
- en: Data preprocessing
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: This is essentially a step that is adopted in the early stages of an ML project
    pipeline. Data preprocessing involves transforming the raw data in a format that
    is acceptable as input by ML algorithms.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是在机器学习项目管道的早期阶段采用的一个步骤。数据预处理涉及将原始数据转换为机器学习算法可接受的输入格式。
- en: Feature hashing, missing values imputation, transforming variables from numeric
    to nominal, and vice versa, are a few data preprocessing steps among the numerous
    things that can be done to data during preprocessing.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 特征哈希、缺失值插补、将变量从数值型转换为名义型以及相反操作，是数据预处理阶段众多可执行操作中的几个步骤。
- en: Raw text documents' transformation into word vectors is an example of data preprocessing.
    The word vectors thus obtained can be fed to an ML algorithm to achieve documents
    classification or documents clustering.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始文本文档转换为词向量是数据预处理的一个例子。因此获得的词向量可以输入到机器学习算法中，以实现文档分类或文档聚类。
- en: Holdout sample
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保留样本
- en: While working on a training dataset, a small portion of the data is kept aside
    for testing the performance of the models. The small portion of data is unseen
    data (not used in training), therefore one can rely on the measurements obtained
    for this data. The measurements obtained can be used to tune the parameters of
    the model or just to report out the performance of the model so as to set expectations
    in terms of what level of performance can be expected from the model.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理训练数据集时，会保留一小部分数据用于测试模型的性能。这部分小数据是未见数据（未用于训练），因此可以依赖为此数据获得的度量。获得的度量可以用来调整模型的参数，或者只是报告模型的性能，以便设定期望，即从模型中可以期望达到何种性能水平。
- en: It may be noted that the performance measurement reported out on the basis of
    a holdout sample is not as robust an estimate as that of a k-fold cross validation
    estimate. This is because there could be some unknown biases that could have crept
    in during the random split of the holdout set from the original dataset. Also,
    there are also no guarantees that the holdout dataset has a representation of
    all the classes involved in the training dataset. If we need representation of
    all classes in the holdout dataset, then a special technique called a **stratified
    holdout sample** needs to be applied. This ensures that there is representation
    for all classes in the holdout dataset. It is obvious that a performance measurement
    obtained from a stratified holdout sample is a better estimate of performance
    than that of the estimate of performance obtained from a nonstratified holdout
    sample.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要注意的是，基于保留样本的性能测量并不像k折交叉验证估计那样稳健。这是因为，在从原始数据集中随机分割保留集的过程中，可能存在一些未知的偏差。此外，也没有保证保留数据集代表了训练数据集中涉及的所有类别。如果我们需要在保留数据集中代表所有类别，那么需要应用一种称为**分层保留样本**的特殊技术。这确保了在保留数据集中有所有类别的代表。显然，从分层保留样本中获得的表现测量比从非分层保留样本中获得的表现测量更准确。
- en: 70%-30%, 80%-20%, and 90%-10% are generally the sets of training data-holdout
    data splits observed in ML projects.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 70%-30%，80%-20%，和90%-10%通常是机器学习项目中观察到的训练数据-保留数据分割的集合。
- en: Hyperparameter tuning
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超参数调整
- en: ML or deep learning algorithms take hyperparameters as input prior to training
    the model. Each algorithm comes with its own set of hyperparameters and some algorithms
    may have zero hyperparameters.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习或深度学习算法在训练模型之前将超参数作为输入。每个算法都附带其自己的超参数集，并且某些算法可能没有超参数。
- en: Hyperparameter tuning is an important step in model building. Each of the ML
    algorithms comes with some default hyperparameter values that are generally used
    to build an initial model, unless the practitioner manually overrides the hyperparameters.
    Setting the right combination of hyperparameters and the right hyperparameter
    values for the model greatly improves the performance of the model in most cases.
    Hence, it is strongly recommended that one does hyperparameter tuning as part
    of ML model building. Searching through the possible universe of hyperparameter
    values is a very time-consuming task.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整是模型构建的重要步骤。每个机器学习算法都附带一些默认的超参数值，通常用于构建初始模型，除非从业者手动覆盖超参数。为模型设置正确的超参数组合和正确的超参数值在大多数情况下可以极大地提高模型的表现。因此，强烈建议将超参数调整作为机器学习模型构建的一部分。搜索可能的超参数值宇宙是一个非常耗时的工作。
- en: The *k* in k-means clustering and k-nearest neighbors classification, the number
    of tress and the depth of tress in random forest, and *eta* in XGBoost are all
    examples of hyperparameters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类和k-最近邻分类中的*k*，随机森林中的树的数量和树的深度，以及XGBoost中的*eta*都是超参数的例子。
- en: '**Grid search** and **Bayesian** optimization-based hyperparameter tuning are
    two popular methods of hyperparameter tuning among practitioners.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**网格搜索**和基于**贝叶斯**优化的超参数调整是实践中两种流行的超参数调整方法。'
- en: Performance metrics
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能指标
- en: A model needs to be evaluated on unseen data to assess its goodness. The term
    goodness may be expressed in several ways and these ways are termed as model performance
    metrics.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 模型需要在未见过的数据上进行评估以评估其好坏。好坏可以用几种方式表达，这些方式被称为模型性能指标。
- en: Several metrics exist to report the performance of models. Accuracy, precision,
    recall, F-score, sensitivity, specificity, AUROC curve, **root mean squared error**
    (**RMSE**), Hamming loss, and **mean squared error** (**MSE**) are some of the
    popular model performance metrics among others.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多个指标来报告模型的性能。准确率、精确度、召回率、F分数、灵敏度、特异性、AUROC曲线、**均方根误差**（**RMSE**）、汉明损失和**均方误差**（**MSE**）是其他指标中一些流行的模型性能指标。
- en: Feature engineering
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程
- en: Feature engineering is the art of creating new features either from existing
    data in the dataset or by procuring additional data from an external data source.
    It is done with the intent that adding additional features improves the model
    performance. Feature engineering generally requires domain expertise and in-depth
    business problem understanding.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是从数据集中的现有数据或通过从外部数据源获取额外数据来创建新特征的艺术。这是出于添加额外特征以提高模型性能的目的。特征工程通常需要领域专业知识和对业务问题的深入理解。
- en: Let's take a look at an example of feature engineering—for a bank that is working
    on a loan defaulter prediction project, sourcing and supplementing the training
    dataset with information on the unemployment trends of the region for the past
    few months might improve the performance of the model.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个特征工程的例子——对于一个正在从事贷款违约预测项目的银行，从过去几个月的区域失业趋势信息中获取和补充训练数据集可能会提高模型的性能。
- en: Model interpretability
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型可解释性
- en: Often, in a business environment when ML models are built, just reporting the
    performance measurements obtained to confirm the goodness of the model may not
    be enough. The stakeholders generally are inquisitive to understand the *whys*
    of the model, that is, what are the factors contributing to the model's performance?
    In other words, the stakeholders want to understand the causes of the effects.
    Essentially, the expectation from the stakeholders is to understand the importance
    of various features in the model and the direction in which each of the variables
    impacts the model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在商业环境中，当构建机器学习模型时，仅仅报告获得的性能度量以确认模型的良好性可能是不够的。利益相关者通常渴望了解模型的“为什么”，即是什么因素导致了模型的性能？换句话说，利益相关者希望了解影响的原因。本质上，利益相关者的期望是了解模型中各种特征的重要性以及每个变量对模型影响的方向。
- en: For example, does a feature of *time spent on exercising every day* in the dataset
    for a cancer prediction model have any impact on the model predictions at all?
    If so, *does time spent on exercising every day* push the prediction in a negative
    direction or positive direction?
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数据集中每天锻炼时间这一特征对癌症预测模型的预测是否有任何影响？如果有，那么每天锻炼时间会推动预测向负面方向还是正面方向？
- en: While the example might sound simple to generate an answer for, in real-world
    ML projects, model interpretability is not so very simple due to the complex relationships
    between variables. It is seldom that one feature, in its isolation, impacts the
    prediction in any one direction. It is indeed a **combination** of features that
    impact the prediction outcome. Thus, it is even more difficult to explain to what
    extent the feature is impacting the prediction.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个例子听起来很简单，可以生成答案，但在现实世界的机器学习项目中，由于变量之间的复杂关系，模型的可解释性并不那么简单。很少有一个特征在孤立的情况下对预测产生任何方向的影响。实际上，是特征的**组合**影响了预测结果。因此，解释特征对预测影响程度就更加困难。
- en: Linear models are generally easier to explain even to business users. This is
    because we obtain weights for various features as a result of model training with
    linear algorithms. These weights are direct indicators of how a feature is contributing
    to model prediction. After all, in a linear model, a prediction is the linear
    combination of model weights and features passed through a function. It should
    be noted that interaction between variables in the real world are not essentially
    linear. So, a linear model trying to model the underlying data that has non-linear
    relationships may not have good predictive power. So, while linear models' interpretability
    is great, it comes at the cost of model performance.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型通常更容易解释，即使是对于商业用户来说也是如此。这是因为通过线性算法进行模型训练后，我们获得了各种特征的权重。这些权重是特征对模型预测贡献的直接指标。毕竟，在线性模型中，预测是模型权重和通过函数传递的特征的线性组合。需要注意的是，现实世界中的变量之间的交互并不一定是线性的。因此，试图模拟具有非线性关系的潜在数据的线性模型可能没有很好的预测能力。因此，虽然线性模型的解释性很好，但它是以模型性能为代价的。
- en: On the contrary, non-linear and non-parametric models tend to be very difficult
    to interpret. In most cases, it may not be apparent even to the person building
    the models as to what exactly are the factors driving the prediction and in which
    direction. This is simply because the prediction outcome is a complex non-linear
    combination of variables. It is also known that non-linear models in general are
    better performing models when compared to linear models. Therefore, there is a
    trade-off needed between model interpretability and model performance.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，非线性和非参数模型往往很难解释。在大多数情况下，即使是构建模型的人也可能不清楚是什么因素在驱动预测以及预测的方向。这仅仅是因为预测结果是变量复杂非线性组合的结果。众所周知，与线性模型相比，一般而言非线性模型是性能更好的模型。因此，需要在模型可解释性和模型性能之间进行权衡。
- en: While the goal of model interpretability is difficult to achieve, there is some
    merit in accomplishing this goal. It helps with the retrospection of a model that
    is deemed as being a good performing model and confirming that no noise inadvertently
    existed in the data that is used for model building and testing. It is obvious
    that models with noise as features fail to generalize on unseen data. Model interpretability
    helps with making sure that no noise crept into the models as features. Also,
    it helps build trust with business users that are eventually consumers of the
    model output. After all, there is no point in building a model whose output is
    not going to be consumed!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型可解释性的目标难以实现，但完成这一目标还是有其价值的。它有助于回顾被认为表现良好的模型，并确认用于模型构建和测试的数据中不存在意外噪声。显然，以噪声作为特征模型的泛化能力会失败。模型可解释性有助于确保没有噪声作为特征渗入模型。此外，它还有助于建立与最终将成为模型输出消费者的业务用户的信任。毕竟，构建一个输出不会被消费的模型是没有意义的！
- en: Non-parametric, non-linear models are difficult to interpret, if not impossible.
    Specialized ML methods are now available to aid black box models interpretability.
    **Partial dependency plot** (**PDP**), **Locally interpretable model-agnostic
    explanations** (**LIME**), and **Shapley additive explanations** (**SHAP**) also
    known as Sharpley's are some of the popular methods used by practitioners to decipher
    the internals of a black box model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数、非线性模型难以解释，甚至可能无法解释。现在有专门的机器学习方法可以帮助解释黑盒模型的可解释性。**部分依赖图**（**PDP**）、**局部可解释模型无关解释**（**LIME**）和**Shapley加性解释**（**SHAP**），也称为Sharpley的，是一些从业者用来解析黑盒模型内部结构的流行方法。
- en: Now that there is a good understanding of the various fundamental terms of ML,
    our next journey is to explore the details of the ML project pipeline. This journey
    discussed in the next section helps us understand the process of building an ML
    project, deploying it, and obtaining predictions to use in a business.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经很好地理解了机器学习各种基本术语，我们的下一步是探索机器学习项目流程的细节。下一节中讨论的这次旅行有助于我们了解构建机器学习项目、部署它以及获取用于商业用途的预测的过程。
- en: ML project pipeline
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习项目流程
- en: Most of the content available on ML projects, either through books, blogs, or
    tutorials, explains the mechanics of machine learning in such a way that the dataset
    available is split into training, validation, and test datasets. Models are built
    using training datasets, and model improvements through hyperparameter tuning
    are done iteratively through validation data. Once a model is built and improved
    upon to a point that is acceptable, it is tested for goodness with unseen test
    data and the results of testing are reported out. Most of the public content available,
    ends at this point.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于机器学习项目的现有内容，无论是通过书籍、博客还是教程，都是以这种方式解释机器学习的机制，即将可用的数据集分为训练集、验证集和测试集。模型使用训练集构建，并通过验证数据进行迭代超参数调整来改进模型。一旦构建并改进到可接受的程度，就用未见过的测试数据进行测试，并报告测试结果。大多数公开内容在这一点上就结束了。
- en: In reality, the ML projects in a business situation go beyond this step. We
    may observe that if one stops at testing and reporting a built model performance,
    there is no real use of the model in terms of predicting about data that is coming
    up in future. We also need to realize that the idea of building a model is to
    be able to deploy the model in production and have the predictions based on new
    data so that businesses can take appropriate action.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际情况下，业务环境中的ML项目不仅限于这一步骤。我们可能会观察到，如果只停留在测试和报告构建模型的性能，那么在预测未来数据方面，模型实际上并没有真正的用途。我们还需要意识到，构建模型的想法是能够在生产环境中部署模型，并基于新数据进行预测，以便企业可以采取适当的行动。
- en: In a nutshell, the model needs to be saved and reused. This also means that
    any new data on which predictions need to be made needs to be preprocessed in
    the same way as training data. This ensures that, the new data has the same number
    of columns and also the same types of columns as training data. This part of productionalization
    of the models built in the lab is totally ignored when being taught. This section
    covers an end-to-end pipeline for the models, right from data preprocessing to
    building the models in the lab to productionalization of the models.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，模型需要被保存并重复使用。这也意味着，需要对需要做出预测的新数据进行预处理，其方式与训练数据相同。这确保了新数据具有与训练数据相同数量的列，以及相同的列类型。在实验室中构建的模型的生产化部分在教学中完全被忽视。本节涵盖了从数据预处理到在实验室中构建模型再到模型生产化的端到端流水线。
- en: ML pipelines describe the entire process from raw data acquisition to obtaining
    post processing of the prediction results on unseen data so as to make it available
    for some kind of action by business. It is possible that a pipeline may be depicted
    at a generalized level or described at a very granular level. This current section
    focuses on describing a generic pipeline that may be applied to any ML project.
    Figure 1.8 shows the various components of the ML project pipeline otherwise known
    as the **cross-industry standard process for data mining** (**CRISP-DM**).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ML流水线描述了从原始数据获取到对未见数据进行预测结果后处理的整个流程，以便使其可用于业务采取某种行动。可能的情况是，流水线可以在一般化层面上进行描述，或者以非常细粒度的方式进行描述。本节重点描述一个通用的流水线，该流水线可以应用于任何ML项目。图1.8显示了ML项目流水线的各个组成部分，也称为**跨行业数据挖掘标准流程**（**CRISP-DM**）。
- en: Business understanding
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 业务理解
- en: Once the problem description that is to be solved using ML is clearly articulated,
    the first step in the ML pipeline is to be able to ascertain if the problem is
    of relevance to business and if the goals of the project are laid out without
    any ambiguities. It may also be wise to check if the problem at hand is feasible
    to be solved as an ML problem. These are the various aspects typically covered
    during the business understanding step.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦使用ML解决的问题描述得非常明确，ML流水线的第一步就是确定该问题是否与业务相关，以及项目目标是否明确无误。还应该明智地检查当前的问题是否可以作为一个ML问题来解决。这些是在业务理解步骤中通常涵盖的各个方面。
- en: Understanding and sourcing the data
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解和获取数据
- en: The next step is to identify all the sources of data that are relevant to the
    business problem at hand. Organizations will have one or more systems, such as
    HR management systems, accounting systems, and inventory management systems. Depending
    on the nature of the problem at hand, we may need to fetch the data from multiple
    sources. Also, data that is obtained through the data acquisition step need not
    always be structured as tabular data; it could be unstructured data, such as emails,
    recorded voice files, and images.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是确定所有与当前业务问题相关的数据来源。组织将有一个或多个系统，例如人力资源管理系统、会计系统和库存管理系统。根据问题的性质，我们可能需要从多个来源获取数据。此外，通过数据获取步骤获得的数据不一定总是以表格数据的形式存在；它可能是非结构化数据，例如电子邮件、录音文件和图像。
- en: 'In corporate organizations of reasonable size, it may not be possible for an
    ML professional to work all alone on the task of fetching the data from the diverse
    range of systems. Tight collaboration with other professionals in the organization
    may be required to complete this step of the pipeline successfully:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有一定规模的 corporate organizations 中，ML专业人士可能无法独自完成从各种系统中获取数据的任务。为了成功完成流水线这一步骤，可能需要与其他组织内的专业人士进行紧密合作：
- en: '![](img/d5bb47da-d584-4d81-b670-9fda8dcb87c5.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d5bb47da-d584-4d81-b670-9fda8dcb87c5.png)'
- en: Preparing the data
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: Data preparation enables the creation of input data for ML algorithms to consume.
    Raw data that we get from data sources is often not very clean. Sometimes, the
    data cannot be readily fused into an ML algorithm to create a model. We need to
    ensure that the raw data is cleaned up and it is prepared in a format that is
    acceptable for the ML algorithm to take as input.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备使ML算法能够创建输入数据。我们从数据源获得的数据通常不是很干净。有时，数据不能直接融合到ML算法中以创建模型。我们需要确保原始数据得到清理，并且以ML算法可以接受的格式准备。
- en: 'EDA is a substep in the process of creating the input data. It is a process
    of using visual and quantitative aids to understand the data without getting prejudice
    about the contents of the data. EDA gives us deeper insights into the data available
    at hand. It helps us to understand the required data preparation steps. Some of
    the insights that we could obtain during EDA are the existence of outliers in
    the data, missing values existence in the data, and the duplication of data. All
    of these problems are addressed during data cleansing which is another substep
    in data preparation. Several techniques may be adopted during data cleansing and
    the following mentioned are some of the popular techniques:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: EDA（探索性数据分析）是创建输入数据过程的一个子步骤。它是一个使用视觉和定量辅助工具来理解数据而不带有数据内容偏见的过程。EDA使我们能够更深入地了解手头的数据。它帮助我们理解所需的数据准备步骤。在EDA过程中，我们可以获得的一些见解包括数据中存在异常值、数据中存在缺失值以及数据的重复。所有这些问题都在数据清洗过程中得到解决，这是数据准备过程中的另一个子步骤。在数据清洗过程中可以采用几种技术，以下是一些流行的技术：
- en: Deleting records that are outliers
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除异常值记录。
- en: Deleting redundant columns and irrelevant columns in data
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除数据中的冗余列和不相关列。
- en: Missing values imputation—filling missing values with special value NA or a
    blank or median or mean or mode or with a regressed value
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值插补——用特殊值NA或空白、中位数、平均值或众数或回归值填充缺失值。
- en: Scaling the data
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据进行缩放。
- en: Removing stop words such as *a*, *and*, and *how*, from unstructured text data
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从非结构化文本数据中移除停用词，如*a*、*and*和*how*。
- en: Normalizing words in unstructured text documents with techniques such as stemming,
    and lemmatization
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用诸如词干提取和词形还原等技术对非结构化文本文档中的单词进行归一化。
- en: Eliminating non-dictionary words in text data
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消除文本数据中的非词典词。
- en: Spelling corrections on misspelled words in text documents
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本文档中对拼写错误的单词进行拼写纠正。
- en: Replacing non-recognizable domain-specific acronyms in the text with actual
    word descriptions
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文本中的不可识别的领域特定缩写词替换为实际单词描述。
- en: Rotation, scaling, and translation of image data
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像数据的旋转、缩放和平移。
- en: Representing the unstructured data as vectors, providing labels for the records
    if the problem at hand needs to be dealt with by supervised learning, handling
    class imbalance problems in the data, feature engineering, transforming the data
    through transformation functions such as log transform, min-max transform, square
    root transform, and cube transform, are all part of the data preparation process.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 将非结构化数据表示为向量，如果手头的问题需要通过监督学习来处理，则为记录提供标签，处理数据中的类别不平衡问题，进行特征工程，通过诸如对数变换、最小-最大变换、平方根变换和立方变换等变换函数转换数据，这些都是数据准备过程的一部分。
- en: The output of the data preparation step is tabular data that can be fit readily into
    an ML algorithm as input in order to create models.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备步骤的输出是表格数据，可以轻松地适应ML算法作为输入以创建模型。
- en: An additional substep that is typically done in data preparation is to divide
    the dataset into training data, validation data, and test data. These various
    datasets are used for specific purposes in the model-building step.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备过程中通常执行的一个附加子步骤是将数据集划分为训练数据、验证数据和测试数据。这些不同的数据集在模型构建步骤中用于特定的目的。
- en: Model building and evaluation
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型构建和评估。
- en: Once the data is ready and prior to the creation of the model, we need to pick
    and select the features from the list of features available. This can be accomplished
    through several off-the-shelf feature selection techniques. Some ML algorithms
    (for example, XGBoost) have feature selection inbuilt within the algorithm, therefore
    we need not explicitly perform feature selection prior to carrying out the modeling
    activity.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据准备就绪，在创建模型之前，我们需要从可用的特征列表中选择和选择特征。这可以通过几种现成的特征选择技术来完成。一些ML算法（例如XGBoost）在算法中内置了特征选择，因此在进行建模活动之前，我们不需要显式执行特征选择。
- en: A suite of ML algorithms is available to try and create models on the data.
    Additionally, models may be created through ensembling techniques as well. One
    needs to pick the algorithm(s) and create models using training datasets, then
    tune the hyperparameters of the model using validation datasets. Finally, the
    model that is created can be tested using the test dataset. Issues, such as selecting
    the right metric to evaluate model performance, overfitting, underfitting, and
    acceptable performance thresholds all need to be taken care of in the model-building
    step itself. It may be noted that if we do not obtain acceptable performance on
    the model, it is required to go back to previous steps in order to get more data
    or to create additional features and then repeat the model-building step once
    again to check if the model performance improves. This may be done as many times
    as required until the desired level of performance is achieved by the model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 有一系列机器学习算法可供尝试，并在数据上创建模型。此外，还可以通过集成技术创建模型。需要选择算法（们）并使用训练数据集创建模型，然后使用验证数据集调整模型的超参数。最后，可以使用测试数据集对创建的模型进行测试。在模型构建步骤中，需要处理的问题包括选择合适的指标来评估模型性能、过拟合、欠拟合以及可接受的性能阈值。所有这些问题都需要在模型构建步骤中加以注意。需要注意的是，如果我们没有在模型上获得可接受的性能，就需要回到之前的步骤，获取更多数据或创建额外的特征，然后再次重复模型构建步骤，以检查模型性能是否有所改善。这可能需要多次重复，直到模型达到所需的性能水平。
- en: At the end of the modeling step, we might end up with a suite of models each
    having its own performance measurement on the unseen test data. The model that
    has the best performance can be selected for use in production.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建步骤结束时，我们可能会得到一系列模型，每个模型都有其在未见测试数据上的性能度量。表现最好的模型可以被选中用于生产。
- en: Model deployment
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型部署
- en: The next step is to save the final model that can be used for the future. There
    are several ways to save the model as an object. Once saved, the model can be
    reloaded any time and it can be used for scoring the new data. Saving the model
    as an object is a trivial task and a number of libraries are available in Python
    and R to achieve it. As a result of saving the model, the model object gets persisted
    to the disk as a `.sav` file or a `.pkl` file or a `.pmml` object depending on
    the library used. The object can then be loaded into the memory to perform scoring
    on unseen data.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将最终模型保存下来，以便将来使用。有几种方法可以将模型保存为对象。一旦保存，模型可以在任何时候重新加载，并用于对新数据的评分。将模型保存为对象是一个简单的任务，Python和R中都有许多库可以实现这一点。由于保存了模型，模型对象会持久化到磁盘上，形成`.sav`文件、`.pkl`文件或`.pmml`对象，具体取决于所使用的库。然后可以将对象加载到内存中，以对未见数据进行评分。
- en: 'The final model that is selected for use in production can be deployed to score
    unseen data in the following two modes:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 最终选定的用于生产的模型可以部署到以下两种模式中进行未见数据的评分：
- en: '**Batch mode**: Batch mode scoring is when one accumulates the unseen data
    to be scored in a file, then run a batch job (just another executable script)
    at a predetermined time to perform scoring. The job loads the model object from
    disk to the memory and runs on each of the records in the file that needs to be
    scored. The output is written to another file at a specified location as directed
    in the batch job script. It may be noted that the records to be scored should
    have the same number of columns as in the training data and the type of columns
    should also comply with the training data. It should be ensured that the number
    of levels in factors columns (nominal type data) should also match with that of
    the training data.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量模式**：批量模式评分是指将需要评分的未见数据累积到一个文件中，然后在预定时间运行一个批量作业（这只是另一个可执行脚本）来进行评分。作业将模型对象从磁盘加载到内存中，并对需要评分的文件中的每条记录运行。输出将按照批量作业脚本中的指示写入到指定位置的其他文件中。需要注意的是，要评分的记录应该具有与训练数据相同的列数，列的类型也应该符合训练数据。应确保因素列（名义类型数据）的级别数与训练数据相匹配。'
- en: '**Real-time mode**: There are times where the business needs model scoring
    to happen on the fly. In this case, unlike the batch mode, data is not accumulated
    and we do not wait until the batch job runs for scoring. The expectation is that
    each record of the data, as and when it is available for scoring should be scored
    by the model. The result of the scoring is to be available to business users almost
    instantaneously. In this case, a model needs to be deployed as a web service that
    can serve any requests that come in. The record to be scored can be passed to
    the web service through a simple API call which, in turn, returns the scored result
    that can be consumed by the downstream applications. Again, the unscored data
    record that is passed in the API call should comply with the format of the training
    data records.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时模式**：有时业务需要模型评分即时发生。在这种情况下，与批量模式不同，数据不会累积，我们也不会等待批量作业运行进行评分。预期是，当数据记录可用于评分时，应该由模型进行评分。评分的结果应该几乎瞬间对业务用户可用。在这种情况下，需要将模型部署为一个可以处理任何请求的Web服务。要评分的记录可以通过简单的API调用传递给Web服务，该调用反过来返回可以由下游应用程序消费的评分结果。再次强调，通过API调用传递的未评分数据记录应遵守训练数据记录的格式。'
- en: Yet another way of achieving near real-time results is by running the model
    job on micro batches of data several times a day and at very frequent intervals.
    The data gets accumulated between the intervals until a point where the model
    job kicks off. The model job scores and outputs the results for the data that
    is accumulated similar to batch mode. The business user gets to see the scored
    results as soon as the micro batch job finishes execution. The only difference
    between the micro batches processing versus the batch is that unlike the batch
    mode, business users need not wait until the next business day to get the scored
    results.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种实现近似实时结果的方法是每天多次以非常频繁的间隔运行模型作业在微批次数据上。数据在间隔期间累积，直到模型作业启动。模型作业对累积的数据进行评分并输出结果，类似于批量模式。业务用户可以在微批次作业执行完成后立即看到评分结果。与微批次处理与批量处理相比的唯一区别是，与批量模式不同，业务用户不需要等到下一个工作日才能获得评分结果。
- en: Though, the model building pipeline ends with successfully deploying the ML
    model and making it available for scoring, in real-world business situations,
    the job does not end here. Of course, the success parties flow in but there is
    a need to look again at the models post a certain point in time (maybe in a few
    months post the deployment). A model that is not maintained at regular intervals
    does not get very well used by businesses.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型构建管道以成功部署ML模型并使其可用于评分而结束，但在现实世界的业务场景中，工作并没有在这里结束。当然，成功者会涌入，但需要在某个时间点（可能在部署后的几个月）再次审视模型。如果一个模型没有定期维护，则不太可能被业务使用得很好。
- en: To avoid the models from perishing and not being used by business users, it
    is important to collect feedback on the performance of the model over a period
    of time and capture if any improvements need to be incorporated in the models.
    The unseen data does not come with labels, therefore comparing the model output
    with that of the desired output by business is a manual exercise. Collaborating
    with business users is a strong requirement to get feedback in this situation.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免模型过时且不被业务用户使用，重要的是在一段时间内收集关于模型性能的反馈，并捕捉是否需要将任何改进纳入模型。未见数据没有标签，因此将模型输出与业务期望的输出进行比较是一项手动操作。在这种情况下，与业务用户合作是获取反馈的强烈要求。
- en: If there is a continued business need for the model and if the performance is
    not up to the mark on the unseen data that is scored with existing model, it needs
    to be investigated to identify the root cause(s). It may so happen that several
    things have changed in the data that is scored over a period of time when compared
    to the data on which model was initially trained. In which case, there is a strong
    need to recalibrate the model and it is essentially a jolly good idea to start
    once again!
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对模型有持续的业务需求，并且如果现有模型在评分的未见数据上的性能未达到标准，则需要调查以确定根本原因。可能发生的情况是，与模型最初训练的数据相比，在一段时间内评分的数据中发生了许多变化。在这种情况下，强烈需要重新校准模型，并且从头开始再次开始是一个非常好的主意！
- en: Now that the book has covered all the essentials of ML and the project pipeline,
    the next topic to be covered is the learning paradigm, which will help us learn
    several ML algorithms.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在本书已经涵盖了机器学习的所有基本要素和项目流程，接下来要讨论的主题是学习范式，这将帮助我们学习几个机器学习算法。
- en: Learning paradigm
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习范式
- en: Most learning paradigms that are followed in other books or content about ML
    follow a bottom-up approach. This approach starts from the bottom and works its
    way up. The approach first covers the theoretical elements, such as mathematical
    introductions to the algorithm, the evolution of the algorithm, variations, and
    parameters that the algorithm takes, and then delves into the application of the
    ML algorithm specific to a dataset. This may be a good approach; however, it takes
    longer really to see the results produced by the algorithm. It needs a lot of
    perseverance on the part of the learner to be patient and wait until the practical
    application of the algorithm is covered. In most cases, practitioners and certain
    classes of industry professionals working on ML are really interested in the practical
    aspects and they want to experience the power of the algorithm. For these people,
    the focus is not the theoretical foundations of the algorithm, but it is the practical
    application. The bottom-up approach works counterproductively in this case.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他关于机器学习（ML）的书籍或内容中，大多数遵循的学习范式采用自下而上的方法。这种方法从底层开始，逐步向上推进。首先覆盖理论元素，例如算法的数学介绍、算法的演变、变体以及算法所采用的参数，然后深入到针对特定数据集的机器学习算法的应用。这可能是一个好的方法；然而，真正看到算法产生的结果需要更长的时间。学习者需要极大的耐心和毅力，等待算法的实际应用被涵盖。在大多数情况下，从事机器学习的工作者和某些行业专业人士对实际方面非常感兴趣，他们希望体验算法的力量。对于这些人来说，重点不是算法的理论基础，而是实际应用。在这种情况下，自下而上的方法适得其反。
- en: The learning paradigm followed in this book to teach several ML algorithms is
    opposite to the bottom-up approach. It rather follows a very practical top-down
    approach. The focus of this approach is **learning by coding**.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本书在教授几种机器学习算法时遵循的学习范式与自下而上的方法相反。它更倾向于遵循一个非常实用的自上而下的方法。这种方法的重点是**通过编码学习**。
- en: Each chapter of the book will focus on learning a particular class of ML algorithm.
    To start with, the chapter focuses on how to use the algorithm in various situations
    and how to obtain results from the algorithm in practice. Once the practical application
    of the algorithm is demonstrated using code and a dataset, gradually, the rest
    of the chapter unveils the theoretical details/concepts of the algorithms experienced
    in the chapter thus far. All theoretical details will be ensured to be covered
    only in as much detail as is required to understand the code and to apply the
    algorithm on any new datasets. This ensures that we get to learn the focused application
    areas of the algorithms rather than unwanted theoretical aspects that are of less
    importance applied in the ML world.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 书的每一章都将专注于学习特定类别的机器学习算法。首先，章节将关注如何在各种情况下使用算法，以及如何在实践中从算法中获得结果。一旦使用代码和数据集演示了算法的实际应用，章节的其余部分将逐渐揭示到目前为止章节中体验到的算法的理论细节/概念。所有理论细节都将确保只涵盖理解代码和在任何新的数据集上应用算法所必需的详细程度。这确保了我们能够学习算法的专注应用领域，而不是在机器学习世界中不那么重要的、不希望出现的理论方面。
- en: Datasets
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: Each chapter of the book describes an ML project that solves a business problem
    using an ML algorithm or a set of algorithms that we attempt to learn in that
    specific chapter. The projects considered are from different domains ranging from
    health care, to banking and finance, and to robots. The business problems solved
    in the chapters that follow are carefully selected to demonstrate solving a close-to-real-world
    business use case. The datasets used for the problems are popular open datasets.
    This will help us not only to explore the solutions covered in this book but also
    to examine other solutions that are developed for the problem. The problem solved
    in each of the chapters enriches our experience by applying ML algorithms in various
    domains and helps us get an understanding of how to solve the business problems
    in various domains successfully.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的每一章都描述了一个机器学习项目，该项目使用机器学习算法或一组算法来解决商业问题，这些算法是我们试图在特定章节中学习的。考虑的项目来自不同的领域，从医疗保健到银行和金融，再到机器人。在接下来的章节中解决的商业问题被精心挑选，以展示解决接近现实世界的商业用例。用于问题的数据集是流行的公开数据集。这将帮助我们不仅探索本书中涵盖的解决方案，还可以检查为该问题开发的其它解决方案。每一章中解决的问题通过在各个领域应用机器学习算法来丰富我们的经验，并帮助我们理解如何成功解决各个领域的商业问题。
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Well! We have learned so much together so far, and now we have reached the end
    of this chapter. In this chapter, we covered all that deals with ML, including
    the terminologies and the project pipeline. We also talked about the learning
    paradigm, the datasets, and all the topics and projects that will be covered in
    each chapter.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！到目前为止，我们已经一起学到了很多，现在我们已经到达了这一章的结尾。在这一章中，我们涵盖了所有与机器学习（ML）相关的内容，包括术语和项目流程。我们还讨论了学习范式、数据集以及每一章将要涉及的所有主题和项目。
- en: In the next chapter, we will start to work on ML ensembles to predict employee
    attrition.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始着手处理机器学习集成，以预测员工流失。
