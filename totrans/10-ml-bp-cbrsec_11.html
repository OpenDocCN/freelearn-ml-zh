<html><head></head><body>
		<div id="_idContainer098">
			<h1 id="_idParaDest-159" class="chapter-number"><a id="_idTextAnchor022"/>10</h1>
			<h1 id="_idParaDest-160">Protecting User Privacy with Differential Privacy</h1>
			<p>With the growing prevalence of machine learning, some concerns have been raised about how it could potentially be a risk to user privacy. Prior research has shown that even carefully anonymized datasets can be analyzed by attackers and de-anonymized using pattern analysis or background knowledge. The core idea that privacy is based upon is a user’s right to control the collection, storage, and use of their data. Additionally, privacy regulations mandate that no sensitive information about a user should be leaked, and they also restrict what user information can be used for machine learning tasks such as ad targeting or fraud detection. This has led to concerns about user data being used for machine learning, and privacy is a crucial topic every data scientist needs to <span class="No-Break">know about.</span></p>
			<p>This chapter covers differential privacy, a technique used to perform data analysis while maintaining user privacy at the same time. Differential privacy aims to add noise to data and query results, such that the query accuracy is retained but no user data is leaked. This can help with simple analytical tasks as well as machine learning. We will start by understanding the fundamentals of privacy and what it means for users, and for you as engineers, scientists, and developers. We will then also work our way through privacy by design, and the legal implications of violating privacy regulations. Finally, we will implement differential privacy in machine learning and deep <span class="No-Break">learning models.</span></p>
			<p>In this chapter, we will cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>The basics <span class="No-Break">of privacy</span></li>
				<li><span class="No-Break">Differential privacy</span></li>
				<li>Differentially private <span class="No-Break">machine learning</span></li>
				<li>Differentially private <span class="No-Break">deep learning</span></li>
			</ul>
			<p>By the end of this chapter, you will have a better understanding of why privacy is important and how it can be incorporated into machine <span class="No-Break">learning systems.</span></p>
			<h1 id="_idParaDest-161">Technical requirements</h1>
			<p>You can find the code files for this chapter on GitHub <span class="No-Break">at </span><span class="No-Break">https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%2010</span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-162">The basics of privacy</h1>
			<p>Privacy is the ability<a id="_idIndexMarker732"/> of an individual or a group of individuals to control their personal information and to be able to decide when, how, and to whom that information is shared. It involves the right to be free from unwanted or unwarranted intrusion into their personal life and the right to maintain the confidentiality of <span class="No-Break">personal data.</span></p>
			<p>Privacy is an important aspect of individual autonomy, and it is essential for maintaining personal freedom, dignity, and trust in personal relationships. It can be protected by various means, such as legal safeguards, technological measures, and <span class="No-Break">social norms.</span></p>
			<p>With the increasing use of technology in our daily lives, privacy has become an increasingly important concern, particularly in relation to the collection, use, and sharing of personal data by organizations and governments. As a result, there has been growing interest in developing effective policies and regulations to protect individual privacy. In this section, we will cover the fundamental concepts of privacy, associated legal measures, and the implications to <span class="No-Break">machine learning.</span></p>
			<h2 id="_idParaDest-163">Core elements of data privacy</h2>
			<p>The fundamental<a id="_idIndexMarker733"/> principle behind data privacy is that users should be able to <a id="_idIndexMarker734"/>answer and have control over the <span class="No-Break">following questions:</span></p>
			<ul>
				<li>What data about me is <span class="No-Break">being collected?</span></li>
				<li>What will that data be <span class="No-Break">used for?</span></li>
				<li>Who will have access to <span class="No-Break">the data?</span></li>
				<li>How will the data <span class="No-Break">be protected?</span></li>
			</ul>
			<p>In this section, we will explore <a id="_idIndexMarker735"/>these concepts <span class="No-Break">in detail.</span></p>
			<h3>Data collection</h3>
			<p><strong class="bold">Data collection</strong> refers to the process of gathering personal information or data from individuals. This data can include any information that can identify an individual, such as name, address, phone<a id="_idIndexMarker736"/> number, email, date of birth, social security <a id="_idIndexMarker737"/>number, and so on. Organizations that collect personal data must ensure that the data is collected only for specific, legitimate purposes and that individuals are made aware of what data is being collected and why. In the case of fraud detection or other security applications, data collection may seem overly intrusive (such as private messages being collected for detecting abuse, or computer processes for detecting malware). Additionally, data collection must comply with applicable laws and regulations, and organizations must obtain explicit consent from individuals before collecting <span class="No-Break">their data.</span></p>
			<h3>Data use</h3>
			<p><strong class="bold">Data use</strong> refers to how the <a id="_idIndexMarker738"/>collected data is used by organizations or individuals. Organizations must ensure that they use personal data only for the specific, legitimate purposes for which it was collected and that they do not use the data for any other purposes without the individual’s explicit consent. Additionally, organizations must ensure that they do not use personal <a id="_idIndexMarker739"/>data in a way that discriminates against individuals, such as denying them services or opportunities based on their personal characteristics. Data use also includes using the data for machine learning models as training – some users may not want their data to be used for training <span class="No-Break">or analysis.</span></p>
			<h3>Data access</h3>
			<p><strong class="bold">Data access</strong> refers to the control<a id="_idIndexMarker740"/> that individuals have over their personal data. Individuals <a id="_idIndexMarker741"/>have the right to know what data is being collected about them, who is collecting it, and why it is being collected. They also have the right to access their own personal data and correct any inaccuracies. Additionally, individuals have the right to know who their data is being shared with and for what purposes. This also includes data sharing with other organizations, applications, and services (for example, a shopping website selling your search history with a marketing company). Personal data should only be shared with the individual’s explicit consent and <a id="_idIndexMarker742"/>should only be shared for specific, legitimate purposes. Organizations must ensure that they have appropriate security <a id="_idIndexMarker743"/>measures in place to protect personal data from unauthorized access, disclosure, alteration, <span class="No-Break">or destruction.</span></p>
			<h3>Data protection</h3>
			<p><strong class="bold">Data protection</strong> refers to the <a id="_idIndexMarker744"/>measures taken to protect personal data<a id="_idIndexMarker745"/> from unauthorized access, disclosure, alteration, or destruction. This includes technical, physical, and administrative measures to ensure the security and confidentiality of personal data. Organizations must ensure that they have appropriate security measures in place to protect personal data, such as encryption, access controls, and firewalls. Additionally, organizations must ensure that they have policies and procedures in place to detect and respond to security incidents <span class="No-Break">or breaches.</span></p>
			<h2 id="_idParaDest-164">Privacy and the GDPR</h2>
			<p>While privacy has to do with user consent on data, it is not a purely ethical or moral concern – there are legal requirements and <a id="_idIndexMarker746"/>regulations that organizations must comply with. The <strong class="bold">GDPR</strong> stands for the <strong class="bold">General Data Protection Regulation</strong>. It is a comprehensive data protection law that came into effect on May 25, 2018, in the <strong class="bold">European </strong><span class="No-Break"><strong class="bold">Union</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">EU</strong></span><span class="No-Break">).</span></p>
			<p>The GDPR regulates the <a id="_idIndexMarker747"/>processing of personal data of individuals within the EU, as well as the export of personal data outside the EU. It gives individuals more control over their personal data and requires organizations to be transparent about how they collect, use, and store <span class="No-Break">personal data.</span></p>
			<p>The GDPR sets out<a id="_idIndexMarker748"/> several key principles, including <span class="No-Break">the following:</span></p>
			<ul>
				<li>Lawfulness, fairness, <span class="No-Break">and transparency</span></li>
				<li><span class="No-Break">Purpose limitation</span></li>
				<li><span class="No-Break">Data minimization</span></li>
				<li><span class="No-Break">Accuracy</span></li>
				<li><span class="No-Break">Storage limitation</span></li>
				<li>Integrity and <span class="No-Break">confidentiality (security)</span></li>
				<li><span class="No-Break">Accountability</span></li>
			</ul>
			<p>Under the GDPR, individuals<a id="_idIndexMarker749"/> have the right to access their personal data, correct inaccurate data, have their data erased in certain circumstances, and object to the processing of their data. Organizations that fail to comply with the GDPR can face significant fines and <span class="No-Break">other sanctions.</span></p>
			<p>For example, in January 2019, the CNIL (the French data protection authority) fined Google €50 million for GDPR violations related to the company’s ad personalization practices. The CNIL found that Google had violated the GDPR in two <span class="No-Break">key ways:</span></p>
			<ul>
				<li><strong class="bold">Lack of transparency</strong>: The CNIL found that Google had not provided users with clear and easily accessible information about how their personal data was being used for ad personalization. The information was spread across several different documents, making it difficult for users <span class="No-Break">to understand.</span></li>
				<li><strong class="bold">Lack of valid consent</strong>: The <a id="_idIndexMarker750"/>CNIL found that Google had not obtained valid consent from users for ad personalization. The consent was not sufficiently informed, as users were not clearly told what specific data was being collected and how it was <span class="No-Break">being used.</span></li>
			</ul>
			<p>The CNIL’s investigation was initiated following two <a id="_idIndexMarker751"/>complaints filed by privacy advocacy groups, <strong class="bold">None Of Your Business</strong> (<strong class="bold">NOYB</strong>) and La Quadrature du Net, on May 25, 2018, the same day that the GDPR came <span class="No-Break">into effect.</span></p>
			<p>In addition to the fine, the CNIL ordered Google to make changes to its ad personalization practices, including making it easier for users to access and understand information about how their data is being used and obtaining <span class="No-Break">valid consent.</span></p>
			<p>The Google fine was significant, as it was the largest GDPR fine at the time and demonstrated that regulators were willing to take enforcement action against large tech companies for GDPR violations. The fine also underscored the importance of transparency and valid consent in data processing under <span class="No-Break">the GDPR.</span></p>
			<p>The GDPR has had a significant impact on how organizations handle personal data, not only in the EU but also worldwide, as many companies have had to update their policies and practices to comply<a id="_idIndexMarker752"/> with <span class="No-Break">the regulation.</span></p>
			<h2 id="_idParaDest-165">Privacy by design</h2>
			<p><strong class="bold">Privacy by design</strong> is an approach to <a id="_idIndexMarker753"/>privacy protection that aims to embed privacy and data protection into the design and architecture of systems, products, and services from the outset. The concept was first introduced by the Information and Privacy Commissioner of Ontario, Canada, in the 1990s, and has since been adopted as a best practice by privacy regulators and <span class="No-Break">organizations worldwide.</span></p>
			<p>The privacy-by-design approach involves proactively identifying and addressing privacy risks, rather than trying to retrofit privacy protections after a system or product has been developed. It requires organizations to consider privacy implications at every stage of the design process, from the initial planning and conceptualization phase through to implementation and <span class="No-Break">ongoing operation.</span></p>
			<p>As data scientists and machine learning engineers, if you are designing any system at scale, understanding privacy concerns is important. You should follow the principles of privacy by design while developing any system. There are five key principles that define privacy by design: proactivity, privacy as default, privacy embedded into the design, full functionality, and <span class="No-Break">end-to-end security.</span></p>
			<h3>Proactive not reactive</h3>
			<p>The principle of being proactive not<a id="_idIndexMarker754"/> reactive means that organizations should anticipate potential privacy risks and take steps to mitigate them before they become a problem. This involves conducting <strong class="bold">privacy impact assessments</strong> (<strong class="bold">PIAs</strong>) to identify<a id="_idIndexMarker755"/> and address potential privacy issues at the outset of a project. By taking a proactive approach to privacy, organizations can reduce the likelihood of privacy breaches, protect individual rights, and build trust with <span class="No-Break">their customers.</span></p>
			<h3>Privacy as the default setting</h3>
			<p>The principle of privacy as the default<a id="_idIndexMarker756"/> setting means that individuals should not have to take any action to protect their privacy. This means that privacy protection should be built into systems, products, and services by default and that individuals should not be required to opt out of sharing their data. By making privacy the default setting, individuals are empowered to make informed decisions about their personal information, without having to navigate complex privacy settings <span class="No-Break">or policies.</span></p>
			<h3>Privacy embedded into the design</h3>
			<p>The principle of <a id="_idIndexMarker757"/>embedding privacy into the design means that privacy should be a core consideration in the development of systems, products, and services from the outset. This involves incorporating privacy features and controls into the design of these products and services, such as anonymization, encryption, and data minimization. By building privacy into the design of products and services, organizations can help ensure that privacy is protected by default, rather than as <span class="No-Break">an afterthought.</span></p>
			<h3>Full functionality</h3>
			<p>The principle of full functionality<a id="_idIndexMarker758"/> means that privacy protections should not come at the expense of functionality or usability. This means that privacy protections should be integrated into systems and products without compromising their performance or functionality. By adopting a positive-sum approach to privacy, organizations can build trust with their customers and demonstrate that they take <span class="No-Break">privacy seriously.</span></p>
			<h3>End-to-end security</h3>
			<p>The principle of end-to-end<a id="_idIndexMarker759"/> security means that comprehensive security measures should be implemented throughout the entire life cycle of a product or service, from development to disposal. This involves implementing a range of security measures, such as access controls, encryption, and monitoring, to protect against unauthorized access, use, and disclosure of personal information. By taking a comprehensive approach to security, organizations can help ensure that personal information is protected at every stage of the life cycle and build trust with <span class="No-Break">their customers.</span></p>
			<h2 id="_idParaDest-166">Privacy and machine learning</h2>
			<p>Why did we spend all this<a id="_idIndexMarker760"/> time discussing the concepts behind privacy, the elements of data privacy, and the GDPR? In security areas (such as fraud, abuse, and misinformation), significant types and amounts of user data are collected. Some of it might be deemed obtrusive, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Browsers collecting users’ mouse movements and click patterns to detect click fraud <span class="No-Break">and bots</span></li>
				<li>Security software collecting information on system processes to detect the presence <span class="No-Break">of malware</span></li>
				<li>Social media companies extracting information from private messages and images to detect <span class="No-Break">child pornography</span></li>
			</ul>
			<p>For data scientists in the security domain, the ultimate goal is to provide maximum user security by building a system with the highest precision and recall. However, at the same time, it is important to understand the limitations you will face in data collection and use while designing your systems. For example, if you are building a system for fraud detection, you may not be able to use cookie data in France. Additionally, the GDPR will apply if the data you are collecting is from <span class="No-Break">European users.</span></p>
			<p>Depending on the jurisdiction, you may not be able to collect certain data, or even if it is collected, you may not be able to use it for machine learning models. These factors must be taken into consideration as you design your systems <span class="No-Break">and algorithms.</span></p>
			<p>Furthermore, we know that machine learning is based on identifying trends and patterns from data. Privacy considerations and regulations will severely limit your ability to collect data, extract features, and <span class="No-Break">train models.</span></p>
			<p>Now that you have been introduced to the fundamentals of privacy, we will look at differential privacy, which is considered to be state-of-the-art in privacy and is used by many <span class="No-Break">tech giants.</span></p>
			<h1 id="_idParaDest-167">Differential privacy</h1>
			<p>In this section, we will cover<a id="_idIndexMarker761"/> the basics of differential privacy, including the mathematical definition and a <span class="No-Break">real-world example.</span></p>
			<h2 id="_idParaDest-168">What is differential privacy?</h2>
			<p><strong class="bold">Differential privacy</strong> (<strong class="bold">DP</strong>) is a framework for preserving the privacy of individuals in a dataset when it is used for statistical analysis or machine learning. The goal of DP is to ensure that the output of a computation on a dataset does not reveal sensitive information about any individual in the dataset. This is accomplished by adding controlled noise to the computation in order to mask the contribution of any individual <span class="No-Break">data point.</span></p>
			<p>DP provides a mathematically rigorous definition of privacy protection by quantifying the amount of information that an attacker can learn about an individual by observing the output of a computation. Specifically, DP requires that the probability of observing a particular output from a computation is roughly the same whether a particular individual is included in the dataset <span class="No-Break">or not.</span></p>
			<p>Formally speaking, let <span class="_-----MathTools-_Math_Variable">D</span> and <span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span> be two datasets that differ by, at most, one element, and let <span class="_-----MathTools-_Math_Variable">f</span> be a function that takes a dataset as input and produces an output in some range, <em class="italic">R</em>. Then, the <span class="_-----MathTools-_Math_Variable">f</span> function satisfies ε-differential privacy for any two datasets <span class="_-----MathTools-_Math_Variable">D</span> and <span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span>:</p>
			<p><span class="_-----MathTools-_Math_Function_v-normal">Pr</span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Variable">)</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable">]</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">≤</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">e</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable_v-normal">ε</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator">.</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable_v-normal">P</span><span class="_-----MathTools-_Math_Variable_v-normal">r</span><span class="_-----MathTools-_Math_Base">[</span><span class="_-----MathTools-_Math_Variable">f</span><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator_Extended">∈</span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable">S</span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base">]</span></span></p>
			<p>where <span class="_-----MathTools-_Math_Variable">S</span> is any subset of <em class="italic">R</em> and <em class="italic">δ</em> is a small positive number that accounts for the probability of events that have low probability. In other words, the probability of the output of the <span class="_-----MathTools-_Math_Variable">f</span> function on dataset <span class="_-----MathTools-_Math_Variable">D</span> falling within a set <span class="_-----MathTools-_Math_Variable">S</span> should be very similar to the probability of the output of the <span class="_-----MathTools-_Math_Variable">f</span> function on dataset <span class="_-----MathTools-_Math_Variable">D</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">′</span><span class="_-----MathTools-_Math_Base"> </span> falling within the same set <span class="_-----MathTools-_Math_Variable">S</span>, up to a multiplicative factor of exp(ε). The smaller the value of ε, the stronger the privacy protection, but also the less accurate the results. The <em class="italic">δ</em> parameter is typically set to a very small value, such as 10<span class="superscript">-9</span>, to ensure that the overall privacy guarantee <span class="No-Break">is strong.</span></p>
			<p>The key idea behind DP is to add random noise to the computation in a way that preserves the statistical properties of the data while obscuring the contribution of any individual. The amount of noise added is controlled by a<a id="_idIndexMarker762"/> parameter called the <strong class="bold">privacy budget</strong>, which determines the maximum amount of privacy loss that can occur during <span class="No-Break">the computation.</span></p>
			<p>There are several mechanisms for achieving differential privacy, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Laplace mechanism</strong>: The Laplace mechanism adds random noise to the output of a computation <a id="_idIndexMarker763"/>based on the sensitivity of the computation. The amount of noise added is proportional to the sensitivity of the computation and inversely proportional to the <span class="No-Break">privacy budget.</span></li>
				<li><strong class="bold">Exponential mechanism</strong>: The Exponential mechanism is used to select an output from a set of possible outputs in a way that minimizes the amount of information revealed about any individual. This mechanism selects the output with the highest utility score, where the utility score is a measure of how well the output satisfies the desired properties of <span class="No-Break">the computation.</span></li>
				<li><strong class="bold">Randomized response</strong>: Randomized response is a mechanism used to obtain accurate estimates of binary data while preserving privacy. The mechanism involves flipping the value of the data point with a certain probability, which is determined by the <span class="No-Break">privacy budget.</span></li>
			</ul>
			<p>DP has become increasingly important in recent years due to the widespread use of data in machine learning and statistical analysis. DP can be used to train machine learning models on sensitive data while ensuring that the privacy of individuals in the dataset is preserved. It is also used in other applications, such as census data and <span class="No-Break">medical research.</span></p>
			<h2 id="_idParaDest-169">Differential privacy – a real-world example</h2>
			<p>The concept of differential <a id="_idIndexMarker764"/>privacy can be clarified in detail using a practical example. Suppose a credit card company has a dataset containing information about the transaction amounts and times of its customers, and they want to identify potential cases of fraud. However, the credit card company is concerned about preserving the privacy of its customers and wants to ensure that the analysis cannot be used to identify the transaction amounts or times of any <span class="No-Break">individual customer.</span></p>
			<p>To accomplish this, the credit card company can use differential privacy to add noise to the analysis. Specifically, they can add random noise to the computed statistics in a way that preserves the overall statistical properties of the data but makes it difficult to determine the transaction amounts or times of any <span class="No-Break">individual customer.</span></p>
			<p>For example, the credit <a id="_idIndexMarker765"/>card company could use the Laplace mechanism to add noise to the computed statistics. Let’s say the credit card company wants to compute the total transaction amount for a specific time period, and the sensitivity of the computation is <em class="italic">1</em>, meaning that changing the amount of one transaction can change the computed total by, at most, 1 dollar. The credit card company wants to achieve a privacy budget of <em class="italic">epsilon = 1</em>, meaning that the probability of observing a particular output from the computation should be roughly the same whether a particular customer is included in the dataset <span class="No-Break">or not.</span></p>
			<p>Using the Laplace mechanism with these parameters, the credit card company can add noise drawn from a Laplace distribution with a scale parameter of <em class="italic">1/epsilon = 1</em>. This will add random noise to the computed total in a way that preserves the overall statistical properties of the data but makes it difficult to determine the transaction amounts or times of any <span class="No-Break">individual customer.</span></p>
			<p>For example, the computed total transaction amount might be $10,000, but with the added noise, it might be reported as $10,100. This ensures that the analysis cannot be used to identify the transaction amounts or times of any individual customer with high confidence, while still providing useful information about the overall transaction amounts for the specific <span class="No-Break">time period.</span></p>
			<p>However, suppose the credit card company wants to achieve a higher level of privacy protection and sets the privacy budget to <em class="italic">epsilon = 10</em> instead of <em class="italic">epsilon = 1</em>. This means that the added noise will be larger and the analysis will be more private, but it will also be less accurate. For example, the computed total transaction amount might be reported as $15,000 with <em class="italic">epsilon = 10</em>, which is further from the true value <span class="No-Break">of $10,000.</span></p>
			<p>In summary, differential privacy can be used in the context of fraud detection to protect the privacy of individuals in a dataset while still allowing useful statistical analysis to be performed. However, the choice of privacy budget (epsilon) is important and should be balanced with the level of privacy protection and the desired accuracy of <span class="No-Break">the analysis.</span></p>
			<h2 id="_idParaDest-170">Benefits of differential privacy</h2>
			<p>Why use differential<a id="_idIndexMarker766"/> privacy at all? What benefits will it provide to users? And what benefits, if any, will it provide to engineers, researchers, and scientists? There are several key benefits that differential privacy provides, including strong user privacy guarantees, flexibility in analysis, balance between privacy and utility, robustness, and transparency. These are more important in the cybersecurity domain than others, as we discussed in <a href="B19327_01.xhtml#_idTextAnchor013"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">On Cybersecurity and </em><span class="No-Break"><em class="italic">Machine Learning</em></span><span class="No-Break">.</span></p>
			<h3>User privacy guarantees</h3>
			<p>Differential privacy provides a rigorous mathematical definition of privacy protection that offers strong guarantees of privacy. It ensures that an individual’s personal data cannot be distinguished from the data of others in <span class="No-Break">the dataset.</span></p>
			<p>In a cybersecurity context, differential privacy can be used to protect the privacy of user data in security logs. For example, let’s say a security analyst is examining a log of user login attempts. Differential privacy can be used to protect the privacy of individual users by adding random noise to the log data so that it is impossible to determine whether a specific user attempted to <span class="No-Break">log in.</span></p>
			<h3>Flexibility</h3>
			<p>Differential privacy can be applied to a wide range of data analysis techniques, including queries, machine learning algorithms, and statistical models. In cybersecurity, it can be applied to a variety of security-related data analysis techniques. For example, it can be used to protect the privacy of user data in intrusion detection systems. It can also be applied to the algorithms used by these systems to detect anomalous network activity and to ensure that the privacy of individual users <span class="No-Break">is protected.</span></p>
			<h3>Privacy-utility trade-off</h3>
			<p>Differential privacy provides a way to balance privacy and utility so that accurate statistical analysis can be performed on a dataset while minimizing the risk of exposing sensitive information. It can be used to protect the privacy of sensitive data in cybersecurity applications while still allowing useful insights to be obtained. For example, it can be used to protect the privacy of user data in threat intelligence-sharing systems. It can also be used to protect the privacy of individual users while still allowing organizations to share information about threats <span class="No-Break">and vulnerabilities.</span></p>
			<h3>Robustness</h3>
			<p>Differential privacy is<a id="_idIndexMarker767"/> robust to various types of attacks, including statistical attacks and inference attacks. Differential privacy is designed to protect against a wide range of attacks, including statistical attacks and inference attacks. For example, in a cybersecurity context, differential privacy can be used to protect the privacy of user data in forensic investigations. It can also be used to ensure that sensitive data cannot be inferred from forensic evidence, even if an attacker has access to a large amount of <span class="No-Break">other data.</span></p>
			<h3>Transparency</h3>
			<p>Differential privacy provides a way to quantify the amount of privacy protection provided by a particular technique, which allows individuals and organizations to make informed decisions about the level of privacy they need for <span class="No-Break">their data.</span></p>
			<p>It provides a way to measure the effectiveness of privacy protection techniques, which can be useful in making decisions about data protection in cybersecurity. For example, it can be used to protect the privacy of user data in threat modeling. It can also be used to help organizations understand the level of privacy protection they need to protect against various types of threats and to measure the effectiveness of their existing privacy <span class="No-Break">protection measures.</span></p>
			<p>So far, we have looked at privacy and then differential privacy. Now, let us see how it can be practically applied in the context of <span class="No-Break">machine learning.</span></p>
			<h1 id="_idParaDest-171">Differentially private machine learning</h1>
			<p>In this section, we will look<a id="_idIndexMarker768"/> at how a fraud detection model can incorporate differential privacy. We will first look at the library we use to implement differential privacy, followed by how a credit card fraud detection machine learning model can be made <span class="No-Break">differentially private.</span></p>
			<h2 id="_idParaDest-172">IBM Diffprivlib</h2>
			<p><strong class="source-inline">Diffprivlib</strong> is an open source<a id="_idIndexMarker769"/> Python library that provides a range of differential privacy tools and algorithms for data analysis. The library is designed to help data scientists and developers apply differential privacy techniques to their data in a simple and <span class="No-Break">efficient way.</span></p>
			<p>One of the key features of <strong class="source-inline">Diffprivlib</strong> is its extensive range of differentially private mechanisms. These include mechanisms for adding noise to data, such as the Gaussian, Laplace, and Exponential mechanisms, as well as more advanced mechanisms, such as the hierarchical and subsample mechanisms. The library also includes tools for calculating differential privacy parameters, such as sensitivity and privacy budget (epsilon), and for evaluating the privacy of a <span class="No-Break">given dataset.</span></p>
			<p>Another important feature of <strong class="source-inline">Diffprivlib</strong> is its ease of use. The library provides a simple and intuitive API that allows users to apply differential privacy to their data with just a few lines of code. The API is designed to be compatible with <strong class="source-inline">scikit-learn</strong>, a popular machine learning library for Python, which makes it easy to incorporate differential privacy into existing data <span class="No-Break">analysis workflows.</span></p>
			<p>In addition to its core functionality, <strong class="source-inline">Diffprivlib</strong> includes a number of advanced features and tools that can be used to improve the accuracy and efficiency of differential privacy applications. For example, the library includes tools for generating synthetic datasets that are differentially private, which can be used to test and validate differential privacy mechanisms. It also includes tools for differential private machine learning, which can be used to build models that are both accurate <span class="No-Break">and privacy-preserving.</span></p>
			<p>Overall, <strong class="source-inline">Diffprivlib</strong> provides a powerful set of tools for data privacy that can be used in a wide range of applications, from healthcare and finance to social media and online advertising. Its extensive range of differentially private mechanisms, ease of use, and advanced features make it a valuable resource for anyone looking to improve the privacy and security of their data <span class="No-Break">analysis workflows.</span></p>
			<p>In the following sections, we will use <strong class="source-inline">Diffprivlib</strong> to train and evaluate differentially private machine <span class="No-Break">learning models.</span></p>
			<h2 id="_idParaDest-173">Credit card fraud detection with differential privacy</h2>
			<p>As we know, differential privacy is a framework for preserving the privacy of individuals while allowing statistical <a id="_idIndexMarker770"/>analysis of their data. Many applications today are powered by analysis through machine learning, and hence, the<a id="_idIndexMarker771"/> application of DP in machine learning has been a field of growing interest <span class="No-Break">and importance.</span></p>
			<p>To apply differential privacy to a machine learning technique, we will perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li><strong class="bold">Define the privacy budget</strong>: The first step is to define the privacy budget, which determines the level of privacy protection that will be provided. The privacy budget is typically expressed as ε, which is a small positive number. The smaller the value of ε, the stronger the privacy protection, but also the less accurate <span class="No-Break">the results.</span></li>
				<li><strong class="bold">Add noise to the data</strong>: To ensure differential privacy, noise is added to the data before logistic regression is performed. Specifically, random noise is added to each data point, so that the noise cancels out when the data <span class="No-Break">is aggregated.</span></li>
				<li><strong class="bold">Train the model</strong>: Once the data has been randomized, a machine learning model is trained on the randomized data. This model will be less accurate than a model trained on the original data, but it will still be useful for <span class="No-Break">making predictions.</span></li>
				<li><strong class="bold">Evaluate the model</strong>: Once the model has been trained, it can be used to make predictions on new data. The accuracy of the model will depend on the value of ε that was chosen, as well as the size and complexity of <span class="No-Break">the dataset.</span></li>
			</ol>
			<p>In the following sections, we will look at how this can be applied in practice to two popular classification models: logistic regression and <span class="No-Break">random forests.</span></p>
			<h3>Differentially private logistic regression</h3>
			<p>First, we will import the<a id="_idIndexMarker772"/> <span class="No-Break">required libraries:</span></p>
			<pre class="source-code">
from sklearn.model_selection import train_test_split 
from sklearn.linear_model import LogisticRegression
from diffprivlib.models import LogisticRegression as dpLogisticRegression
import pandas as pd</pre>
			<p>As a simulation, we will be<a id="_idIndexMarker773"/> using the credit card fraud detection dataset from Kaggle. You can use any dataset of your choice. We split the data into<a id="_idIndexMarker774"/> training and test sets, with 2% reserved <span class="No-Break">for testing:</span></p>
			<pre class="source-code">
import pandas as pd
url = "https://raw.githubusercontent.com/nsethi31/Kaggle-Data-Credit-Card-Fraud-Detection/master/creditcard.csv"
df = pd.read_csv(url)</pre>
			<p>To print the columns, you can simply run <span class="No-Break">the following:</span></p>
			<pre class="source-code">
print(df.columns)</pre>
			<p>And you should see <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B19327_10_01.jpg" alt="Figure 1﻿0.1 – Dataset columns"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Dataset columns</p>
			<p>We want to use columns <strong class="source-inline">V1</strong> through <strong class="source-inline">V28</strong> and <strong class="source-inline">Amount</strong> as features, and <strong class="source-inline">Class</strong> as the label. We then want to split the data into training and <span class="No-Break">test sets:</span></p>
			<pre class="source-code">
Y = df['Class'].values
X = df.drop('Time', axis = 1).drop('Class', axis = 1).values
# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,Y,
test_size=0.2,random_state=123)</pre>
			<p>Next, we train a logistic regression model to predict the class of the data. Note that this is the vanilla logistic regression <a id="_idIndexMarker775"/>model from scikit-learn without any differential <span class="No-Break">privacy involved:</span></p>
			<pre class="source-code">
# Train a regular logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)</pre>
			<p>Now, we evaluate the performance of this model on the <span class="No-Break">test set:</span></p>
			<pre class="source-code">
# Evaluate the model on the test set
score = model.score(X_test, y_test)
print("Test set accuracy for regular logistic regression: {:.2f}%".format(score*100))</pre>
			<p>This should print something <span class="No-Break">like this:</span></p>
			<pre class="source-code">
Test set accuracy for regular logistic regression: 99.90%</pre>
			<p>Great! We have nearly 99.9% accuracy on the <span class="No-Break">test set.</span></p>
			<p>Now, we fit a differentially private logistic regression model on the same data. Here, we set the value of the <strong class="source-inline">epsilon</strong> parameter to <strong class="source-inline">1</strong>. You can set this to any value you want, as long as it is not zero (an epsilon of zero indicates no differential privacy, and the model will be equivalent to the <span class="No-Break">vanilla one):</span></p>
			<pre class="source-code">
# Train a differentially private logistic regression model
dp_model = dpLogisticRegression(epsilon=1.0, data_norm=10)
dp_model.fit(X_train, y_train)</pre>
			<p>Then, evaluate it on the test set as we did with the <span class="No-Break">previous model:</span></p>
			<pre class="source-code">
# Evaluate the model on the test set
score = dp_model.score(X_test, y_test)
print("Test set accuracy for differentially private logistic regression: {:.2f}%".format(score*100))</pre>
			<p>You should see an output <span class="No-Break">like this:</span></p>
			<pre class="source-code">
Test set accuracy for differentially private logistic regression: 63.73%</pre>
			<p>Wow – that’s a huge drop! The <a id="_idIndexMarker776"/>accuracy on the test set dropped from 99.9% to about 64%. This is the utility cost associated with <span class="No-Break">increased privacy.</span></p>
			<h3>Differentially private random forest</h3>
			<p>As a fun experiment, let<a id="_idIndexMarker777"/> us try the same with a random forest. The code remains almost the same, except both classifiers are switched to random forests. Here’s the <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
from sklearn.ensemble import RandomForestClassifier
from diffprivlib.models import RandomForestClassifier as dpRandomForestClassifier
# Train a regular logistic regression model
model = RandomForestClassifier()
model.fit(X_train, y_train)
# Evaluate the model on the test set
score = model.score(X_test, y_test)
print("Test set accuracy for regular RF: {:.2f}%".format(score*100))
# Train a differentially private logistic regression model
dp_model = dpRandomForestClassifier(epsilon=1.0, data_norm=10)
dp_model.fit(X_train, y_train)
# Evaluate the model on the test set
score = dp_model.score(X_test, y_test)
print("Test set accuracy for differentially private RF: {:.2f}%".format(score*100))</pre>
			<p>This gives the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker778"/></span><span class="No-Break"> output:</span></p>
			<pre class="source-code">
Test set accuracy for regular RF: 99.95%
Test set accuracy for differentially private RF: 99.80%</pre>
			<p>Interestingly, the drop in accuracy in random forests is much less pronounced and is less than 1%. Therefore, random forests would be a better classifier to use in this scenario if both increased privacy and utility are to <span class="No-Break">be achieved.</span></p>
			<h3>Examining the effect of ϵ</h3>
			<p>Now, we will examine how the<a id="_idIndexMarker779"/> accuracy of the classifier on the test set varies as we change the value of <strong class="source-inline">epsilon</strong>. For multiple values of <strong class="source-inline">epsilon</strong> from <strong class="source-inline">0</strong> to <strong class="source-inline">5</strong>, we will train a differentially private classifier and compute the accuracy on the <span class="No-Break">test set:</span></p>
			<pre class="source-code">
import numpy as np
EPS_MIN = 0.1
EPS_MAX = 10
STEP_SIZE = 0.1
scores = []
epsilons = np.arange(EPS_MIN, EPS_MAX, STEP_SIZE)
for eps in epsilons:
  # Train a differentially private logistic regression model
  dp_model = dpLogisticRegression(epsilon= eps,data_norm=10)
  dp_model.fit(X_train, y_train)
  # Evaluate the model on the test set
  score = dp_model.score(X_test, y_test)
  scores.append(100.0*score)</pre>
			<p>After this block is run, we <a id="_idIndexMarker780"/>can plot the scores against the corresponding <span class="No-Break"><strong class="source-inline">epsilon</strong></span><span class="No-Break"> values:</span></p>
			<pre class="source-code">
import matplotlib.pyplot as plt
plt.plot(epsilons, scores)</pre>
			<p>This should show you a plot <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B19327_10_02.jpg" alt="Figure 1﻿0.2 – Accuracy variation with epsilon for logistic regression"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Accuracy variation with epsilon for logistic regression</p>
			<p>How about the same <a id="_idIndexMarker781"/>evaluation for a random forest? Just replace the model instantiated with a random forest instead of logistic regression. Here is the complete <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
import numpy as np
EPS_MIN = 0.1
EPS_MAX = 10
STEP_SIZE = 0.1
scores = []
epsilons = np.arange(EPS_MIN, EPS_MAX, STEP_SIZE)
for eps in epsilons:
  # Train a differentially private logistic regression model
  dp_model = dpRandomForestClassifier(epsilon= eps,
data_norm=10)
  dp_model.fit(X_train, y_train)
  # Evaluate the model on the test set
  score = dp_model.score(X_test, y_test)
  scores.append(100.0*score)</pre>
			<p>Plotting this gives<a id="_idIndexMarker782"/> you <span class="No-Break">the following:</span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B19327_10_03.jpg" alt="Figure 1﻿0.3 – Accuracy variation with epsilon for random forest"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Accuracy variation with epsilon for random forest</p>
			<p>The graph <em class="italic">appears</em> volatile – but note that the accuracy is always between 99.8% and 99.83%. This means that higher values of <strong class="source-inline">epsilon</strong> do not cause a meaningful difference in accuracy. This model is better suited for differential privacy than the logistic <span class="No-Break">regression model.</span></p>
			<h1 id="_idParaDest-174">Differentially private deep learning</h1>
			<p>In the sections so far, we<a id="_idIndexMarker783"/> covered how differential privacy can be implemented in standard machine learning classifiers. In this section, we will cover how it can be implemented for <span class="No-Break">neural networks.</span></p>
			<h2 id="_idParaDest-175">DP-SGD algorithm</h2>
			<p><strong class="bold">Differentially private stochastic gradient descent</strong> (<strong class="bold">DP-SGD</strong>) is a technique used in machine learning to train <a id="_idIndexMarker784"/>models on sensitive or private data without revealing the data itself. The technique<a id="_idIndexMarker785"/> is based on the concept of differential privacy, which guarantees that an algorithm’s output remains largely unchanged, even if an individual’s data is added <span class="No-Break">or removed.</span></p>
			<p>DP-SGD is a variation <a id="_idIndexMarker786"/>of the <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) algorithm, which is commonly used for training deep neural networks. In SGD, the algorithm updates the model parameters by computing the gradient of the loss function on a small randomly selected subset (or “batch”) of the training data. This is done iteratively until the algorithm converges with a minimum of the <span class="No-Break">loss function.</span></p>
			<p>In DP-SGD, the SGD algorithm is modified to incorporate a privacy mechanism. Specifically, a small amount of random noise is added to the gradients at each iteration, which makes it difficult for an adversary to infer individual data points from the output of <span class="No-Break">the algorithm.</span></p>
			<p>The amount of noise added to the gradients is controlled by a parameter called the privacy budget ε, which determines the maximum amount of information that can be leaked about an individual data point. A smaller value of ε corresponds to a stronger privacy guarantee but also reduces the accuracy of <span class="No-Break">the model.</span></p>
			<p>The amount of noise added to the gradients is calculated using a technique called the Laplace mechanism. The Laplace mechanism adds random noise sampled from the Laplace distribution, which has a probability density function proportional to <em class="italic">exp(-|x|/b)</em>, where <em class="italic">b</em> is the scale parameter. The larger the value of <em class="italic">b</em>, the smaller the amount of <span class="No-Break">noise added.</span></p>
			<p>To ensure that the privacy budget ε is not exceeded over the course of the training process, a technique <a id="_idIndexMarker787"/>called <strong class="bold">moment accountant</strong> is used. Moment accountant estimates the cumulative privacy loss over multiple iterations of the algorithm and ensures that the privacy budget is <span class="No-Break">not exceeded.</span></p>
			<p>DP-SGD differs from a standard SGD only in the gradient calculation step. First, the gradient is calculated for a batch as the partial derivative of the loss with respect to the parameter. Then, the gradients are clipped so that they remain within a fixed window. Finally, random noise is added to the gradients to form the final gradients. This final gradient is used in the parameter update step in <span class="No-Break">gradient descent.</span></p>
			<p>In summary, DP-SGD is a variant of SGD that incorporates a privacy mechanism by adding random noise to the gradients at each iteration. The privacy level is controlled by a parameter called the privacy budget ε, which determines the amount of noise added to the gradients. The Laplace mechanism is used to add the noise, and the moment accountant technique is used to <a id="_idIndexMarker788"/>ensure that the privacy budget is <span class="No-Break">not exceeded.</span></p>
			<p>DP-SGD has several advantages over traditional <span class="No-Break">SGD algorithms:</span></p>
			<ul>
				<li><strong class="bold">Privacy-preserving</strong>: The<a id="_idIndexMarker789"/> primary advantage of DP-SGD is that it preserves the privacy of individual data points. This is particularly important when dealing with sensitive or confidential data, such as medical records or <span class="No-Break">financial data.</span></li>
				<li><strong class="bold">Robustness to re-identification attacks</strong>: DP-SGD provides robustness to re-identification attacks, which attempt to match the output of the algorithm to individual data points. By adding random noise to the gradients, DP-SGD makes it difficult for an attacker to distinguish between individual <span class="No-Break">data points.</span></li>
				<li><strong class="bold">Improved fairness</strong>: DP-SGD can also improve the fairness of machine learning models by ensuring that the model does not rely too heavily on any individual data point. This can help prevent biases in the model and ensure that it performs well across different <span class="No-Break">demographic groups.</span></li>
				<li><strong class="bold">Scalability</strong>: DP-SGD can scale to large datasets and complex models. By using SGD, DP-SGD can train models on large datasets by processing small batches of data at a time. This allows for efficient use of <span class="No-Break">computing resources.</span></li>
				<li><strong class="bold">Accuracy trade-off</strong>: Finally, DP-SGD offers a trade-off between accuracy and privacy. By adjusting the privacy budget ε, the user can control the level of privacy protection while still achieving a reasonable level of accuracy. This makes DP-SGD a flexible and adaptable tool for machine <span class="No-Break">learning applications.</span></li>
			</ul>
			<h2 id="_idParaDest-176">Implementation</h2>
			<p>We will begin, as usual, by<a id="_idIndexMarker790"/> implementing the necessary libraries. Apart from the usual processing and deep learning libraries, we will be using a new one, known as <strong class="source-inline">tensorflow-privacy</strong>. This library provides tools for adding differential privacy to TensorFlow models, including an implementation of the TensorFlow privacy algorithm for training deep learning models with differential privacy. The<a id="_idIndexMarker791"/> library also includes tools for measuring the privacy properties of a model, such as its <strong class="source-inline">epsilon</strong> value, which quantifies the level of privacy protection provided by the differential <span class="No-Break">privacy mechanism:</span></p>
			<pre class="source-code">
import tensorflow as tf
import numpy as np
import tensorflow_privacy
from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy</pre>
			<p>We will now write a function that will load and preprocess our MNIST data. The MNIST dataset is a large collection of handwritten digits that is commonly used as a benchmark dataset for testing machine learning algorithms, particularly those related to image recognition and computer vision. The dataset consists of 60,000 training images and 10,000 testing images, with each image being a grayscale 28x28 pixel image of a handwritten <span class="No-Break">digit (0-9).</span></p>
			<p>Our function will first load the training and test sets from this data. The data is then scaled to 1/255<span class="superscript">th</span> its value, followed by reshaping into the image dimensions. The labels, which are integers from <strong class="source-inline">0</strong> to <strong class="source-inline">9</strong>, are converted into <span class="No-Break">one-hot vectors:</span></p>
			<pre class="source-code">
def load_and_process_MNIST_Data():
    # Define constants
    SCALE_FACTOR = 1/255
    NUM_CLASS = 10
    # Load train and test data
    train, test = tf.keras.datasets.mnist.load_data()
    train_data, train_labels = train
    test_data, test_labels = test
    print("----- Loaded Train and Test Raw Data -----")
    # Scale train and test data
    train_data = np.array(train_data, dtype=np.float32) * SCALE_FACTOR
    test_data = np.array(test_data, dtype=np.float32) * SCALE_FACTOR
    print("----- Scaled Train and Test Data -----")
    # Reshape data for Convolutional NN
    train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)
    test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)
    print("----- Reshaped Train and Test Data -----")
    # Load train and test labels
    train_labels = np.array(train_labels, dtype=np.int32)
    test_labels = np.array(test_labels, dtype=np.int32)
    print("----- Loaded Train and Test Labels -----")
    # One-Hot Encode the labels
    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=NUM_CLASS)
    test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=NUM_CLASS)
    print("----- Categorized Train and Test Labels -----")
    return train_data, train_labels, test_data, test_labels</pre>
			<p>Next, we will define a function that creates our classification model. In this case, we will be using CNNs. We have seen and used CNNs in earlier chapters; however, we will provide <a id="_idIndexMarker792"/>a brief <span class="No-Break">recap here.</span></p>
			<p>A CNN is a type of neural <a id="_idIndexMarker793"/>network that is specifically designed for image recognition and computer vision tasks. CNNs are highly effective at processing and analyzing images due to their ability to detect local patterns and features within an image. At a high level, a CNN consists of a series of layers, including convolutional layers, pooling layers, and fully connected layers. In the convolutional layers, the network learns to detect local features and patterns in the input image by applying a set of filters to the image. The pooling layers then downsample the feature maps obtained from the convolutional layers to reduce the size of the input and make the network more computationally efficient. Finally, the fully connected layers process the output of the convolutional and pooling layers to generate <span class="No-Break">a prediction.</span></p>
			<p>The key innovation of CNNs is the use of convolutional layers, which allow the network to learn spatially invariant features from the input image. This is achieved by sharing weights across different parts of the image, which allows the network to detect the same pattern regardless of its position within the image. CNNs have achieved state-of-the-art performance in a wide range of computer vision tasks, including image classification, object detection, and semantic segmentation. They have been used in many real-world applications, such as self-driving cars, medical image analysis, and facial <span class="No-Break">recognition systems.</span></p>
			<p>Our model creation function initializes an empty list and adds layers to it one by one to build up the <span class="No-Break">CNN structure:</span></p>
			<pre class="source-code">
def MNIST_CNN_Model (num_hidden = 1):
    model_layers = list()
    # Add input layer
    # Convolution
    model_layers.append(tf.keras.layers.Conv2D(16, 8,
                        strides=2,
                        padding='same',
                        activation='relu',
                        input_shape=(28, 28, 1)))
    # Pooling
    model_layers.append(tf.keras.layers.MaxPool2D(2, 1))
    # Add Hidden Layers
    for _ in range(num_hidden):
        # Convolution
        model_layers.append(tf.keras.layers.Conv2D(32, 4,
                           strides=2,
                           padding='valid',
                           activation='relu'))
        # Pooling
        model_layers.append(tf.keras.layers.MaxPool2D(2, 1))
    # Flatten to vector
    model_layers.append(tf.keras.layers.Flatten())
    # Final Dense Layer
    model_layers.append(tf.keras.layers.Dense(32, activation='relu'))
    model_layers.append(tf.keras.layers.Dense(10))
    # Initialize model with these layers
    model = tf.keras.Sequential(model_layers)
    return model</pre>
			<p>All of the core functions<a id="_idIndexMarker794"/> needed have been defined. Now, we use the data loader we implemented earlier to load the training and test data <span class="No-Break">and labels:</span></p>
			<pre class="source-code">
train_data, train_labels, test_data, test_labels = load_and_process_MNIST_Data()</pre>
			<p>Here, we will set some hyperparameters that will be used by <span class="No-Break">the model:</span></p>
			<ul>
				<li><strong class="source-inline">NUM_EPOCHS</strong>: This defines the number of epochs (one epoch is a full pass over the training data) that the model will undergo <span class="No-Break">while training.</span></li>
				<li><strong class="source-inline">BATCH_SIZE</strong>: This defines the number of data instances that will be processed in one batch. Processing here involves running the data through the network, calculating the predicted labels, the loss, and the gradients, and then updating the weights by <span class="No-Break">gradient descent.</span></li>
				<li><strong class="source-inline">MICRO_BATCHES</strong>: The dataset is divided into smaller units called microbatches, with each microbatch <a id="_idIndexMarker795"/>containing a single training example by default. This allows us to clip gradients for each individual example, which reduces the negative impact of clipping on the gradient signal and maximizes the model’s utility. However, increasing the size of microbatches can decrease computational overhead, but it involves clipping the average gradient across multiple examples. It’s important to note that the total number of training examples consumed in a batch remains constant, regardless of the microbatch size. To ensure proper <a id="_idIndexMarker796"/>division, the number of microbatches should evenly divide the <span class="No-Break">batch size.</span></li>
				<li><strong class="source-inline">L2_NORM_CLIP</strong>: This refers to the maximum L2-norm that is allowed for the gradient of the loss function with respect to the model parameters. During training, the gradient computed on a minibatch of data is clipped to ensure that its L2-norm does not exceed the <strong class="source-inline">L2_NORM_CLIP</strong> value. This clipping operation is an essential step in the DP-SGD algorithm because it helps to bind the sensitivity of the gradient with respect to the input data. A higher value can lead to better accuracy but may decrease privacy guarantees, while a lower value can provide stronger privacy guarantees but may result in slower convergence and <span class="No-Break">lower accuracy.</span></li>
				<li><strong class="source-inline">NOISE_MULTIPLIER</strong>: This controls the amount of noise that is added to the gradient updates during training to provide privacy guarantees. In DP-SGD, each gradient update is perturbed by a random noise vector to mask the contribution of individual training examples to the gradient. A higher value increases the amount of noise that is added to the gradient, which in turn provides stronger privacy guarantees but can decrease the accuracy of <span class="No-Break">the model.</span></li>
				<li><strong class="source-inline">LEARN_RATE</strong>: This is the learning rate, and as seen in earlier chapters, controls the degree to which gradients <span class="No-Break">are updated.</span></li>
			</ul>
			<p>Note that the following values we set for these hyperparameters have been derived through experimentation. There is no sure way of knowing what the best parameters are. In fact, you are encouraged to experiment with different values and examine how they affect the privacy and accuracy guarantees of <span class="No-Break">the model:</span></p>
			<pre class="source-code">
NUM_EPOCHS = 3
BATCH_SIZE = 250
MICRO_BATCHES = 250
L2_NORM_CLIP = 1.5
NOISE_MULTIPLIER = 1.3
LEARN_RATE = 0.2</pre>
			<p>We will initialize the<a id="_idIndexMarker797"/> model using the function we defined earlier, and print out a summary to verify <span class="No-Break">the structure:</span></p>
			<pre class="source-code">
model = MNIST_CNN_Model()
model.summary()</pre>
			<p>This will show you something <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B19327_10_04.jpg" alt="Figure 1﻿0.4 – Model structure"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Model structure</p>
			<p>Now, we will define the loss and optimizer used for training. While the loss is categorical cross-entropy (as expected for a multi-class classification problem), we will not use the standard Adam optimizer here but will use a specialized optimizer for <span class="No-Break">differential privacy.</span></p>
			<p><strong class="source-inline">DPKerasSGDOptimizer</strong> is a class in the TensorFlow <strong class="source-inline">Privacy</strong> library that provides an implementation of the SGD optimizer with differential privacy guarantees. It uses the DP-SGD algorithm, which adds<a id="_idIndexMarker798"/> random noise to the gradients computed during each step of the SGD optimization process. The amount of noise added is controlled by two parameters: the noise multiplier and the clipping norm. The noise multiplier determines the amount of noise added to the gradients, while the clipping norm limits the magnitude of the gradients to prevent <span class="No-Break">large updates:</span></p>
			<pre class="source-code">
optimizer = tensorflow_privacy.DPKerasSGDOptimizer(
             l2_norm_clip = L2_NORM_CLIP,
             noise_multiplier = NOISE_MULTIPLIER,
             num_microbatches = MICRO_BATCHES,
             learning_rate = LEARN_RATE)
loss = tf.keras.losses.CategoricalCrossentropy(
       from_logits=True,
       reduction=tf.losses.Reduction.NONE)</pre>
			<p>Finally, we will build the model and start the <span class="No-Break">training process:</span></p>
			<pre class="source-code">
model.compile(optimizer=optimizer,
              loss=loss,
              metrics=['accuracy'])
model.fit(train_data,
          train_labels,
          epochs = NUM_EPOCHS,
          validation_data = (test_data, test_labels),
          batch_size = BATCH_SIZE)</pre>
			<p>This should show you the training loop <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B19327_10_05.jpg" alt="Figure 1﻿0.5 – Training loop"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Training loop</p>
			<p>The model is now trained. The <strong class="source-inline">compute_dp_sgd_privacy</strong> function is useful for analyzing the privacy properties of a differentially private machine learning model trained using the DP-SGD algorithm. By computing the privacy budget, we can ensure that the model satisfies a<a id="_idIndexMarker799"/> desired level of privacy protection and can adjust the parameters of the algorithm accordingly. The function uses the moment accountant method to estimate the privacy budget of the DP-SGD algorithm. This method calculates an upper bound on the privacy budget by analyzing the moments of the privacy <span class="No-Break">loss distribution:</span></p>
			<pre class="source-code">
compute_dp_sgd_privacy.compute_dp_sgd_privacy(
                  n = train_data.shape[0],
                  batch_size = BATCH_SIZE,
                  noise_multiplier = NOISE_MULTIPLIER,
                  epochs = NUM_EPOCHS,
                  delta=1e-5)</pre>
			<p>And this should show you the following <span class="No-Break">privacy measurements:</span></p>
			<pre class="source-code">
DP-SGD with sampling rate = 0.417% and noise_multiplier = 1.3 iterated over 720 steps satisfies differential privacy with eps – 0.563 and delta = 1e-05.
The optimal RDP order is 18.0.
(0.5631726490328062, 18.0)</pre>
			<h2 id="_idParaDest-177">Differential privacy in practice</h2>
			<p>Understanding the<a id="_idIndexMarker800"/> importance and utility of differential privacy, technology giants have started implementing it in their products. Two popular examples are Apple <span class="No-Break">and Microsoft.</span></p>
			<p>Apple routinely collects users’ typing history and behavior locally – this helps power features such as autocorrect and automatic completion of messages. However, it also invites the risk of collecting personal and sensitive information. Users may talk about medical issues, financial details, or other information that they want to protect, and hence using it directly would be a privacy violation. Differential privacy comes to the rescue here. Apple<a id="_idIndexMarker801"/> implements <strong class="bold">local differential privacy</strong>, which guarantees that it is difficult to determine whether a certain user contributed to the computation of an aggregate feature by adding noise to the data before it is shared with Apple for computation <span class="No-Break">and processing.</span></p>
			<p>Another tech giant that has been a forerunner in differential privacy is Microsoft. The Windows operating system needs to collect telemetry in order to understand usage patterns, diagnose faults, and detect malicious software. Microsoft applies differential privacy to the features it collects by adding noise before they are aggregated and sent to Microsoft. Microsoft Office has a <em class="italic">Suggested Replies</em> feature, which enables auto-completion and response suggestions in Outlook and Word. As there might be sensitive data in the emails/documents the model is trained on, Microsoft uses differential privacy in order to ensure that the model doesn’t learn from or leak any <span class="No-Break">such information.</span></p>
			<p>These algorithms often take longer to train and often require tuning for accuracy, but according to Microsoft, this effort can be worth it due to the more rigorous privacy guarantees that differential <span class="No-Break">privacy enables.</span></p>
			<h1 id="_idParaDest-178">Summary</h1>
			<p>In recent years, user privacy has grown as a field of importance. Users are to have full control over their data, including its collection, storage, and use. This can be a hindrance to machine learning, especially in the cybersecurity domain, where increased privacy causing a decreased utility can lead to fraud, network attacks, data theft, <span class="No-Break">or abuse.</span></p>
			<p>This chapter first covered the fundamental aspects of privacy – what it entails, why it is important, the legal requirements surrounding it, and how it can be incorporated into practice through the privacy-by-design framework. We then covered differential privacy, a statistical technique to add noise to data so that analysis can be performed while maintaining user privacy. Finally, we looked at how differential privacy can be applied to machine learning in the domain of credit card fraud detection, as well as deep <span class="No-Break">learning models.</span></p>
			<p>This completes our journey into building machine learning solutions for cybersecurity! Now, it is time to introspect and develop more skills in the domain. The next chapter, which contains a series of interview-related questions and additional blueprints, will help you do <span class="No-Break">just that.</span></p>
		</div>
	</body></html>