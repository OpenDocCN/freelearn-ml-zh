<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Detecting and Recognizing Faces" id="aid-190861"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Detecting and Recognizing Faces</h1></div></div></div><p>Among the many reasons that make computer vision a fascinating subject is the fact that computer vision makes very <span class="emphasis"><em>futuristic</em></span>-sounding tasks a reality. One such feature is face detection. OpenCV has a built-in facility to perform face detection, which has virtually infinite applications in the real world in all sorts of contexts, from security to entertainment.</p><p>This chapter introduces some of OpenCV's face detection functionalities, along with the data files that define particular types of trackable objects. Specifically, we look at Haar cascade classifiers, which analyze contrast between adjacent image regions to determine whether or not a given image or subimage matches a known type. We consider how to combine multiple Haar cascade classifiers in a hierarchy, such that one classifier identifies a parent region (for our purposes, a face) and other classifiers identify child regions (eyes, nose, and mouth).</p><p>We also take a detour into the humble but important subject of rectangles. By drawing, copying, and resizing rectangular image regions, we can perform simple manipulations on image regions that we are tracking.</p><p>By the end of this chapter, we will integrate face tracking and rectangle manipulations into Cameo. Finally, we'll have some face-to-face interaction!</p><div class="section" title="Conceptualizing Haar cascades"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Conceptualizing Haar cascades</h1></div></div></div><p>When <a id="id262" class="indexterm"/>we talk about classifying objects and tracking their location, what exactly are we hoping to pinpoint? What constitutes a recognizable part of an object?</p><p>Photographic images, even from a webcam, may contain a lot of detail for our (human) viewing pleasure. However, image detail tends to be unstable with respect to variations in lighting, viewing angle, viewing distance, camera shake, and digital noise. Moreover, even real differences in physical detail might not interest us for the purpose of classification. I was taught in school that no two snowflakes look alike under a microscope. Fortunately, as a Canadian child, I had already learned how to recognize snowflakes without a microscope, as the similarities are more obvious in bulk.</p><p>Thus, some means of abstracting image detail is useful in producing stable classification and tracking results. The abstractions <a id="id263" class="indexterm"/>are called <span class="strong"><strong>features</strong></span>, which are said to be <span class="strong"><strong>extracted</strong></span> from the image data. There should be far fewer features than pixels, though any pixel might influence multiple features. The level of similarity between two images can be evaluated based on Euclidean distances between the images' corresponding features.</p><p>For example, distance might be defined in terms of spatial coordinates or color coordinates. Haar-like features are one type of feature that is often applied to real-time face tracking. They were first used for this purpose in the paper, <span class="emphasis"><em>Robust Real-Time Face Detection</em></span>, <span class="emphasis"><em>Paul Viola and Michael Jones</em></span>, <span class="emphasis"><em>Kluwer Academic Publishers</em></span>,<span class="emphasis"><em> 2001</em></span> (available at <a class="ulink" href="http://www.vision.caltech.edu/html-files/EE148-2005-Spring/pprs/viola04ijcv.pdf">http://www.vision.caltech.edu/html-files/EE148-2005-Spring/pprs/viola04ijcv.pdf</a>). Each Haar-like feature describes the pattern of contrast among adjacent image regions. For example, edges, vertices, and thin lines each generate distinctive features.</p><p>For any given image, the features may vary depending on the region's size; this may be called the <a id="id264" class="indexterm"/>
<span class="strong"><strong>window size</strong></span>. Two images that differ only in scale should be capable of yielding similar features, albeit for different window sizes. Thus, it is useful to generate features for multiple window sizes. Such a collection of features is called a <span class="strong"><strong>cascade</strong></span>. We may say <a id="id265" class="indexterm"/>a Haar cascade is scale-invariant or, in other words, robust to changes in scale. OpenCV provides a classifier and tracker for scale-invariant Haar cascades that it expects to be in a certain file format.</p><p>Haar cascades, as implemented in OpenCV, are not robust to changes in rotation. For example, an upside-down face is not considered similar to an upright face and a face viewed in profile is not considered similar to a face viewed from the front. A more complex and more resource-intensive implementation could improve Haar cascades' robustness to rotation by considering multiple transformations of images as well as multiple window sizes. However, we will confine ourselves to the implementation in OpenCV.</p></div></div>
<div class="section" title="Getting Haar cascade data" id="aid-19UOO1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Getting Haar cascade data</h1></div></div></div><p>Once you <a id="id266" class="indexterm"/>have a copy of the source code of OpenCV 3, you will find a folder, <code class="literal">data/haarcascades</code>.</p><p>This folder contains all the XML files used by the OpenCV face detection engine to detect faces in still images, videos, and camera feeds.</p><p>Once you find <code class="literal">haarcascades</code>, create a directory for your project; in this folder, create a subfolder called <code class="literal">cascades</code>, and copy the following files from <code class="literal">haarcascades</code> into <code class="literal">cascades</code>:</p><div class="informalexample"><pre class="programlisting">haarcascade_profileface.xml
haarcascade_righteye_2splits.xml
haarcascade_russian_plate_number.xml
haarcascade_smile.xml
haarcascade_upperbody.xml</pre></div><p>As their names suggest, these cascades are for tracking faces, eyes, noses, and mouths. They require a frontal, upright view of the subject. We will use them later when building a face detector. If you are curious about how these data sets are generated, refer to <span class="emphasis"><em>Appendix B</em></span>, <span class="emphasis"><em>Generating Haar Cascades for Custom Targets</em></span>, <span class="emphasis"><em>OpenCV Computer Vision with Python</em></span>. With a lot of patience and a powerful computer, you can make your own cascades and train them for various types of objects.</p></div>
<div class="section" title="Using OpenCV to perform face detection"><div class="titlepage" id="aid-1AT9A2"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Using OpenCV to perform face detection</h1></div></div></div><p>Unlike <a id="id267" class="indexterm"/>what you may think from the outset, performing face detection on a still image or a video feed is an extremely similar operation. The latter is just the sequential version of the former: face detection on videos is simply face detection applied to each frame read into the program from the camera. Naturally, a whole host of concepts are applied to video face detection such as tracking, which does not apply to still images, but it's always good to know that the underlying theory is the same.</p><p>So let's go ahead and detect some faces.</p><div class="section" title="Performing face detection on a still image"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec28"/>Performing face detection on a still image</h2></div></div></div><p>The first and <a id="id268" class="indexterm"/>most basic way to perform face detection is to load an image and detect faces in it. To make the result visually meaningful, we will draw rectangles around faces on the original image.</p><p>Now that you have <code class="literal">haarcascades</code> included in your project, let's go ahead and create a basic script to perform face detection.</p><div class="informalexample"><pre class="programlisting">import cv2

filename = '/path/to/my/pic.jpg'

def detect(filename):
  face_cascade = cv2.CascadeClassifier('./cascades/haarcascade_frontalface_default.xml')
  
  img = cv2.imread(filename)
  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  faces = face_cascade.detectMultiScale(gray, 1.3, 5)
  for (x,y,w,h) in faces:
    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
  cv2.namedWindow('Vikings Detected!!')
  cv2.imshow('Vikings Detected!!', img)
  cv2.imwrite('./vikings.jpg', img)
  cv2.waitKey(0)

detect(filename)</pre></div><p>Let's go through the code. First, we use the obligatory <code class="literal">cv2</code> import (you'll find that every script in this book <a id="id269" class="indexterm"/>will start like this, or almost similar). Secondly, we declare the <code class="literal">detect</code> function.</p><div class="informalexample"><pre class="programlisting">def detect(filename):</pre></div><p>Within this function, we declare a <code class="literal">face_cascade</code> variable, which is a <code class="literal">CascadeClassifier</code> object for faces, and responsible for face detection.</p><div class="informalexample"><pre class="programlisting">  face_cascade =
  cv2.CascadeClassifier('./cascades/haarcascade_frontalface_default.xml')</pre></div><p>We then load our file with <code class="literal">cv2.imread</code>, and convert it to grayscale, because that's the color space in which the face detection happens.</p><p>The next step (<code class="literal">face_cascade.detectMultiScale</code>) is where we operate the actual face detection.</p><div class="informalexample"><pre class="programlisting">  img = cv2.imread(filename)
  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
  faces = face_cascade.detectMultiScale(gray, 1.3, 5)</pre></div><p>The parameters passed are <code class="literal">scaleFactor</code> and <code class="literal">minNeighbors</code>, which determine the percentage reduction of the image at each iteration of the face detection process, and the minimum number of neighbors retained by each face rectangle at each iteration. This may all seem a little complex in the beginning but you can check all the options out in the official documentation.</p><p>The value returned from the detection operation is an array of tuples that represent the face rectangles. The utility method, <code class="literal">cv2.rectangle</code>, allows us to draw rectangles at the specified coordinates (<code class="literal">x</code> and <code class="literal">y</code> represent the left and top coordinates, <code class="literal">w</code> and <code class="literal">h</code> represent the width and height of the face rectangle).</p><p>We will draw blue rectangles around all the faces we find by looping through the faces variable, making sure we use the original image for drawing, not the gray version.</p><div class="informalexample"><pre class="programlisting">for (x,y,w,h) in faces:
    img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)</pre></div><p>Lastly, we create <a id="id270" class="indexterm"/>a <code class="literal">namedWindow</code> instance and display the resulting processed image in it. To prevent the image window from closing automatically, we insert a call to <code class="literal">waitKey</code>, which closes the window down at the press of any key.</p><div class="informalexample"><pre class="programlisting">  cv2.namedWindow('Vikings Detected!!')
  cv2.imshow('Vikings Detected!!', img)
  cv2.waitKey(0)</pre></div><p>And there we go, a whole set of Vikings have been detected in our image, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00213.jpeg" alt="Performing face detection on a still image"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Performing face detection on a video"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec29"/>Performing face detection on a video</h2></div></div></div><p>We now <a id="id271" class="indexterm"/>have a good foundation to understand how to perform face detection on a still image. As mentioned previously, we can repeat the process on the individual frames of a video (be it a camera feed or a video) and perform face detection.</p><p>The script will perform the following tasks: it will open a camera feed, it will read a frame, it will examine that frame for faces, it will scan for eyes within the faces detected, and then it will draw blue rectangles around the faces and green rectangles around the eyes.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's create a file called <code class="literal">face_detection.py</code> and start by importing the necessary module:<div class="informalexample"><pre class="programlisting">import cv2</pre></div></li><li class="listitem">After this, we declare a method, <code class="literal">detect()</code>, which will perform face detection.<div class="informalexample"><pre class="programlisting">     def detect():
  face_cascade = cv2.CascadeClassifier('./cascades/haarcascade_frontalface_default.xml')
  eye_cascade = cv2.CascadeClassifier('./cascades/haarcascade_eye.xml')
  camera = cv2.VideoCapture(0)</pre></div></li><li class="listitem">The first thing we need to do inside the <code class="literal">detect()</code> method is to load the Haar cascade files so that OpenCV can operate face detection. As we copied the cascade files in the local <code class="literal">cascades/</code> folder, we can use a relative path. Then, we open a <code class="literal">VideoCapture</code> object (the camera feed). The <code class="literal">VideoCapture</code> constructor takes a parameter, which indicates the camera to be used; <code class="literal">zero</code> indicates the first camera available.<div class="informalexample"><pre class="programlisting">while (True):
    ret, frame = camera.read()
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</pre></div></li><li class="listitem">Next up, we capture a frame. The <code class="literal">read()</code> method returns two values: a Boolean indicating the success of the frame read operation, and the frame itself. We capture the frame, and then we convert it to grayscale. This is a necessary operation, because face detection in OpenCV happens in the grayscale color space:<div class="informalexample"><pre class="programlisting">    faces = face_cascade.detectMultiScale(gray, 1.3, 5)</pre></div></li><li class="listitem">Much like the single still image example, we call <code class="literal">detectMultiScale</code> on the grayscale version of the frame.<div class="informalexample"><pre class="programlisting">    for (x,y,w,h) in faces:
        img = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)
 
       
        roi_gray = gray[y:y+h, x:x+w]
        
        eyes = eye_cascade.detectMultiScale(roi_gray, 1.03, 5, 0, (40,40))</pre></div><div class="note" title="Note"><h3 class="title"><a id="note17"/>Note</h3><p>There are a few additional parameters in the eye detection. Why? The method <a id="id272" class="indexterm"/>signature for <code class="literal">detectMultiScale</code> takes a number of optional parameters: in the case of detecting a face, the default options were good enough to detect faces. However, eyes are a smaller feature of the face, and self-casting shadows in my beard or my nose and random shadows in the frame were triggering <span class="strong"><strong>false positives</strong></span>.</p><p>By limiting the search for eyes to a minimum size of 40x40 pixels, I was able to discard all false positives. Go ahead and test these parameters until you reach a point at which your application performs as you expected it to (for example, you can try and specify a maximum size for the feature too, or increase the scale factor and number of neighbors).</p></div></li><li class="listitem">Here we have a further step compared to the still image example: we create a region of interest corresponding to the face rectangle, and within this rectangle, we operate "eye detection". This makes sense as you wouldn't want to go looking for eyes outside a face (well, for human beings at least!).<div class="informalexample"><pre class="programlisting">        for (ex,ey,ew,eh) in eyes:
            cv2.rectangle(img,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)</pre></div></li><li class="listitem">Again, we loop through the resulting eye tuples and draw green rectangles around them.<div class="informalexample"><pre class="programlisting">    cv2.imshow("camera", frame)
    if cv2.waitKey(1000 / 12) &amp; 0xff == ord("q"):
      break

  camera.release()
  cv2.destroyAllWindows()

if __name__ == "__main__":
  detect()</pre></div></li><li class="listitem">Finally, we show the resulting frame in the window. All being well, if any face is within the field of view of the camera, you will have a blue rectangle around their <a id="id273" class="indexterm"/>face and a green rectangle around each eye, as shown in this screenshot:<div class="mediaobject"><img src="../Images/image00214.jpeg" alt="Performing face detection on a video"/></div><p style="clear:both; height: 1em;"> </p></li></ol><div style="height:10px; width: 1px"/></div></div><div class="section" title="Performing face recognition"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec30"/>Performing face recognition</h2></div></div></div><p>Detecting <a id="id274" class="indexterm"/>faces is a fantastic feature of OpenCV and one that constitutes the basis for a more advanced operation: face recognition. What is face recognition? It's the ability of a program, given an image or a video feed, to identify a person. One of the ways to achieve this (and the approach adopted by OpenCV) is to "train" the program by feeding it a set of classified pictures (a facial database), and operate the recognition against those pictures.</p><p>This is the process that OpenCV and its face recognition module follow to recognize faces.</p><p>Another important feature of the face recognition module is that each recognition has a confidence score, which allows us to set thresholds in real-life applications to limit the amount of false reads.</p><p>Let's start from the very beginning; to operate face recognition, we need faces to recognize. You can do this in two ways: supply the images yourself or obtain freely available face databases. There are a number of face databases on the Internet:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>The Yale </strong></span><a id="id275" class="indexterm"/><span class="strong"><strong>face database (Yalefaces)</strong></span>: <a class="ulink" href="http://vision.ucsd.edu/content/yale-face-database">http://vision.ucsd.edu/content/yale-face-database</a></li><li class="listitem"><span class="strong"><strong>The </strong></span><a id="id276" class="indexterm"/><span class="strong"><strong>AT&amp;T</strong></span>: <a class="ulink" href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html</a></li><li class="listitem"><span class="strong"><strong>The </strong></span><a id="id277" class="indexterm"/><span class="strong"><strong>Extended Yale or Yale B</strong></span>: <a class="ulink" href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html</a></li></ul></div><p>To operate face <a id="id278" class="indexterm"/>recognition on these samples, you would then have to run face recognition on an image that contains the face of one of the sampled people. That may be an educational process, but I found it to be not as satisfying as providing images of my own. In fact, I probably had the same thought that many people had: I wonder if I could write a program that recognizes my face with a certain degree of confidence.</p><div class="section" title="Generating the data for face recognition"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec09"/>Generating the data for face recognition</h3></div></div></div><p>So let's go <a id="id279" class="indexterm"/>ahead and write a script that will generate those images for us. A few images containing different expressions are all that we need, but we have to make sure the sample images adhere to certain criteria:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Images will be grayscale in the <code class="literal">.pgm</code> format</li><li class="listitem">Square shape</li><li class="listitem">All the same size images (I used 200 x 200; most freely available sets are smaller than that)</li></ul></div><p>Here's the script itself:</p><div class="informalexample"><pre class="programlisting">import cv2

def generate():
  face_cascade = cv2.CascadeClassifier('./cascades/haarcascade_frontalface_default.xml')
  eye_cascade = cv2.CascadeClassifier('./cascades/haarcascade_eye.xml')
  camera = cv2.VideoCapture(0)
  count = 0
  while (True):
    ret, frame = camera.read()
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    faces = face_cascade.detectMultiScale(gray, 1.3, 5)
    
    for (x,y,w,h) in faces:
        img = cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)
        
        f = cv2.resize(gray[y:y+h, x:x+w], (200, 200))

        cv2.imwrite('./data/at/jm/%s.pgm' % str(count), f)
        count += 1

    cv2.imshow("camera", frame)
    if cv2.waitKey(1000 / 12) &amp; 0xff == ord("q"):
      break

  camera.release()
  cv2.destroyAllWindows()

if __name__ == "__main__":
  generate()</pre></div><p>What is quite interesting about this exercise is that we are going to generate sample images building on our newfound knowledge of how to detect a face in a video feed. Effectively, what we are doing is detecting a face, cropping that region of the gray-scaled frame, resizing it to be 200x200 pixels, and saving it with a name in a particular folder (in my case, <code class="literal">jm</code>; you can use your initials) in the <code class="literal">.pgm</code> format.</p><p>I inserted a <a id="id280" class="indexterm"/>variable, <code class="literal">count</code>, because we needed progressive names for the images. Run the script for a few seconds, change expressions a few times, and check the destination folder you specified in the script. You will find a number of images of your face, grayed, resized, and named with the format, <code class="literal">&lt;count&gt;.pgm</code>.</p><p>Let's now move on to try and recognize our face in a video feed. This should be fun!</p></div><div class="section" title="Recognizing faces"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec10"/>Recognizing faces</h3></div></div></div><p>OpenCV 3 comes <a id="id281" class="indexterm"/>with three main methods for recognizing <a id="id282" class="indexterm"/>faces, based on three different algorithms: <span class="strong"><strong>Eigenfaces</strong></span>, <span class="strong"><strong>Fisherfaces</strong></span>, and <a id="id283" class="indexterm"/>
<span class="strong"><strong>Local Binary Pattern Histograms</strong></span> (<span class="strong"><strong>LBPH</strong></span>). It <a id="id284" class="indexterm"/>is beyond the scope of this book to get into the nitty-gritty of the theoretical differences between these methods, but we can give a high-level overview of the concepts.</p><p>I will refer you to the following links for a detailed description of the algorithms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>Principal </strong></span><a id="id285" class="indexterm"/><span class="strong"><strong>Component Analysis (PCA)</strong></span>: A very intuitive introduction by Jonathon Shlens is available at <a class="ulink" href="http://arxiv.org/pdf/1404.1100v1.pdf">http://arxiv.org/pdf/1404.1100v1.pdf</a>. This algorithm was invented in 1901 by K. Pearson, and the original paper, <span class="emphasis"><em>On Lines and Planes of Closest Fit to Systems of Points in Space</em></span>, is available at <a class="ulink" href="http://stat.smmu.edu.cn/history/pearson1901.pdf">http://stat.smmu.edu.cn/history/pearson1901.pdf</a>.</li><li class="listitem"><span class="strong"><strong>Eigenfaces</strong></span>: The <a id="id286" class="indexterm"/>paper, <span class="emphasis"><em>Eigenfaces for Recognition</em></span>, <span class="emphasis"><em>M. Turk and A. Pentland</em></span>, <span class="emphasis"><em>1991</em></span>, is available at <a class="ulink" href="http://www.cs.ucsb.edu/~mturk/Papers/jcn.pdf">http://www.cs.ucsb.edu/~mturk/Papers/jcn.pdf</a>.</li><li class="listitem"><span class="strong"><strong>Fisherfaces</strong></span>: The <a id="id287" class="indexterm"/>seminal paper, <span class="emphasis"><em>THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS</em></span>, <span class="emphasis"><em>R.A. Fisher</em></span>, <span class="emphasis"><em>1936</em></span>, is available at <a class="ulink" href="http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/pdf">http://onlinelibrary.wiley.com/doi/10.1111/j.1469-1809.1936.tb02137.x/pdf</a>.</li><li class="listitem"><span class="strong"><strong>Local </strong></span><a id="id288" class="indexterm"/><span class="strong"><strong>Binary Pattern</strong></span>: The first paper describing this algorithm is <span class="emphasis"><em>Performance evaluation of texture measures with classification based on Kullback discrimination of distributions</em></span>, <span class="emphasis"><em>T. Ojala, M. Pietikainen, D. Harwood</em></span> and is available at <a class="ulink" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=576366&amp;searchWithin%5B%5D=%22Authors%22%3A.QT.Ojala%2C+T..QT.&amp;newsearch=true">http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=576366&amp;searchWithin%5B%5D=%22Authors%22%3A.QT.Ojala%2C+T..QT.&amp;newsearch=true</a>.</li></ul></div><p>First and foremost, all <a id="id289" class="indexterm"/>methods follow a similar process; they all take a set of classified observations (our face database, containing numerous samples per individual), get "trained" on it, perform an analysis of faces detected in an image or video, and determine two elements: whether the subject is identified, and a measure of the confidence of the subject really being identified, which is commonly known as the confidence score.</p><p>Eigenfaces performs a so called PCA, which—of all the mathematical concepts you will hear mentioned in relation to computer vision—is possibly the most descriptive. It basically identifies principal components of a certain set of observations (again, your face database), calculates the <span class="strong"><strong>divergence</strong></span> of the current observation (the faces being detected in an image or frame) compared to the dataset, and it produces a value. The smaller the value, the smaller the difference between face database and detected face; hence, a value of <code class="literal">0</code> is an exact match.</p><p>Fisherfaces derives from PCA and evolves the concept, applying more complex logic. While computationally more intensive, it tends to yield more accurate results than Eigenfaces.</p><p>LBPH instead roughly (again, from a very high level) divides a detected face into small cells and compares each cell to the corresponding cell in the model, producing a histogram of matching values for each area. Because of this flexible approach, LBPH is the only face <a id="id290" class="indexterm"/>recognition algorithm that allows the model sample faces and the detected faces to be of different shape and size. I personally found this to be the most accurate algorithm generally speaking, but each algorithm has its strengths and weaknesses.</p></div><div class="section" title="Preparing the training data"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec11"/>Preparing the training data</h3></div></div></div><p>Now that we <a id="id291" class="indexterm"/>have our data, we need to load these sample pictures into our face recognition algorithms. All face recognition algorithms take two parameters in their <code class="literal">train()</code> method: an array of images and an array of labels. What do these labels represent? They are the IDs of a certain individual/face so that when face recognition is performed, we not only know the person was recognized but also who—among the many people available in our database—the person is.</p><p>To do that, we <a id="id292" class="indexterm"/>need to create a <span class="strong"><strong>comma-separated value</strong></span> (<span class="strong"><strong>CSV</strong></span>) file, which will contain the path to a sample picture followed by the ID of that person. In my case, I have 20 pictures generated with the previous script, in the subfolder, <code class="literal">jm/</code>, of the folder, <code class="literal">data/at/</code>, which contains all the pictures of all the individuals.</p><p>My CSV file therefore looks like this:</p><div class="informalexample"><pre class="programlisting">jm/1.pgm;0
jm/2.pgm;0
jm/3.pgm;0
...
jm/20.pgm;0</pre></div><div class="note" title="Note"><h3 class="title"><a id="note18"/>Note</h3><p>The dots are all the missing numbers. The <code class="literal">jm/</code> instance indicates the subfolder, and the <code class="literal">0</code> value at the end is the ID for my face.</p></div><p>OK, at this stage, we have everything we need to instruct OpenCV to recognize our face.</p></div><div class="section" title="Loading the data and recognizing faces"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec12"/>Loading the data and recognizing faces</h3></div></div></div><p>Next up, we <a id="id293" class="indexterm"/>need to load these two resources (the array of <a id="id294" class="indexterm"/>images and CSV file) into the face recognition algorithm, so it can be trained to recognize our face. To do this, we build a function that reads the CSV file and—for each line of the file—loads the image at the corresponding path into the images array and the ID into the labels array.</p><div class="informalexample"><pre class="programlisting">def read_images(path, sz=None):
    
    c = 0
    X,y = [], []
    for dirname, dirnames, filenames in os.walk(path):
        for subdirname in dirnames:
            subject_path = os.path.join(dirname, subdirname)
            for filename in os.listdir(subject_path):
                try:
                    if (filename == ".directory"):
                        continue
                    filepath = os.path.join(subject_path, filename)
                    im = cv2.imread(os.path.join(subject_path, filename), cv2.IMREAD_GRAYSCALE)

                    # resize to given size (if given)
                    if (sz is not None):
                        im = cv2.resize(im, (200, 200))

                    X.append(np.asarray(im, dtype=np.uint8))
                    y.append(c)
                except IOError, (errno, strerror):
                    print "I/O error({0}): {1}".format(errno, strerror)
                except:
                    print "Unexpected error:", sys.exc_info()[0]
                    raise
            c = c+1
            

    return [X,y]</pre></div></div><div class="section" title="Performing an Eigenfaces recognition"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec13"/>Performing an Eigenfaces recognition</h3></div></div></div><p>We're <a id="id295" class="indexterm"/>ready to test the face recognition <a id="id296" class="indexterm"/>algorithm. Here's the script to perform it:</p><div class="informalexample"><pre class="programlisting">def face_rec():
    names = ['Joe', 'Jane', 'Jack']
    if len(sys.argv) &lt; 2:
        print "USAGE: facerec_demo.py &lt;/path/to/images&gt; [&lt;/path/to/store/images/at&gt;]"
        sys.exit()

    [X,y] = read_images(sys.argv[1])
    y = np.asarray(y, dtype=np.int32)
    
    if len(sys.argv) == 3:
        out_dir = sys.argv[2]
    
    model = cv2.face.createEigenFaceRecognizer()
    model.train(np.asarray(X), np.asarray(y))
    camera = cv2.VideoCapture(0)
    face_cascade = cv2.CascadeClassifier('./cascades/haarcascade_frontalface_default.xml')
    while (True):
      read, img = camera.read()
      faces = face_cascade.detectMultiScale(img, 1.3, 5)
      for (x, y, w, h) in faces:
        img = cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        roi = gray[x:x+w, y:y+h]
        try:
            roi = cv2.resize(roi, (200, 200), interpolation=cv2.INTER_LINEAR)
            params = model.predict(roi)
            print "Label: %s, Confidence: %.2f" % (params[0], params[1])
            cv2.putText(img, names[params[0]], (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, 255, 2)
        except:
            continue
      cv2.imshow("camera", img)
      if cv2.waitKey(1000 / 12) &amp; 0xff == ord("q"):
        break
    cv2.destroyAllWindows()</pre></div><p>There are a few lines that may look a bit mysterious, so let's analyze the script. First of all, there's an array of names declared; those are the actual names of the individual people I stored in <a id="id297" class="indexterm"/>my database of faces. It's great to <a id="id298" class="indexterm"/>identify a person as ID <code class="literal">0</code>, but printing <code class="literal">'Joe'</code> on top of a face that's been correctly detected and recognized is much more dramatic.</p><p>So whenever the script recognizes an ID, we will print the corresponding name in the <code class="literal">names</code> array instead of an ID.</p><p>After this, we load the images as described in the previous function, create the face recognition model with <code class="literal">cv2.createEigenFaceRecognizer()</code>, and train it by passing the two arrays of images and labels (IDs). Note that the Eigenface recognizer takes two important parameters that you can specify: the first one is the number of principal components you want to keep and the second is a float value specifying a confidence threshold.</p><p>Next up, we repeat a similar process to the face detection operation. This time, though, we extend the processing of the frames by also operating face recognition on any face that's been detected.</p><p>This happens in two steps: firstly, we resize the detected face to the expected size (in my case, samples were 200x200 pixels), and then we call the <code class="literal">predict()</code> function on the resized region.</p><div class="note" title="Note"><h3 class="title"><a id="note19"/>Note</h3><p>This is a bit of a simplified process, and it serves the purpose of enabling you to have a basic application running and understand the process of face recognition in OpenCV 3. In reality, you will apply a few more optimizations, such as correctly aligning and rotating detected faces, so the accuracy of the recognition is maximized.</p></div><p>Lastly, we obtain <a id="id299" class="indexterm"/>the results of the recognition <a id="id300" class="indexterm"/>and, just for effect, we draw it in the frame:</p><div class="mediaobject"><img src="../Images/image00215.jpeg" alt="Performing an Eigenfaces recognition"/></div><p style="clear:both; height: 1em;"> </p></div><div class="section" title="Performing face recognition with Fisherfaces"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec14"/>Performing face recognition with Fisherfaces</h3></div></div></div><p>What <a id="id301" class="indexterm"/>about Fisherfaces? The process doesn't change much; we simply need to instantiate a different algorithm. So, the declaration of our model variable would look like so:</p><div class="informalexample"><pre class="programlisting">model = cv2.face.createFisherFaceRecognizer()</pre></div><p>Fisherface <a id="id302" class="indexterm"/>takes the same two arguments as Eigenfaces: the Fisherfaces to keep and the confidence threshold. Faces with confidence above this threshold will be discarded.</p></div><div class="section" title="Performing face recognition with LBPH"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec15"/>Performing face recognition with LBPH</h3></div></div></div><p>Finally, let's <a id="id303" class="indexterm"/>take a quick look at <a id="id304" class="indexterm"/>the LBPH algorithm. Again, the process is very similar. However, the parameters taken by the algorithm factory are a bit more complex as they indicate in order: <code class="literal">radius</code>, <code class="literal">neighbors</code>, <code class="literal">grid_x</code>, <code class="literal">grid_y</code>, and the confidence threshold. If you don't specify these values, they <a id="id305" class="indexterm"/>will automatically be set to 1, 8, 8, 8, and 123.0. The model declaration will look like so:</p><div class="informalexample"><pre class="programlisting">  model = cv2.face.createLBPHFaceRecognizer()</pre></div><div class="note" title="Note"><h3 class="title"><a id="note20"/>Note</h3><p>Note that with LBPH, you won't need to resize images, as the division in grids allows comparing patterns identified in each cell.</p></div></div><div class="section" title="Discarding results with confidence score"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec16"/>Discarding results with confidence score</h3></div></div></div><p>The <code class="literal">predict()</code> method returns a two-element array: the first element is the label of the recognized <a id="id306" class="indexterm"/>individual and the second is the confidence score. All algorithms come with the option of setting a confidence score threshold, which measures the distance of the recognized face from the original model, therefore a score of 0 signifies an exact match.</p><p>There may be cases in which you would rather retain all recognitions, and then apply further processing, so you can come up with your own algorithms to estimate the confidence score of a recognition; for example, if you are trying to identify people in a video, you may want to analyze the confidence score in subsequent frames to establish whether the recognition was successful or not. In this case, you can inspect the confidence score obtained by the algorithm and draw your own conclusions.</p><div class="note" title="Note"><h3 class="title"><a id="note21"/>Note</h3><p>The confidence score value is completely different in Eigenfaces/Fisherfaces and LBPH. Eigenfaces and Fisherfaces will produce values (roughly) in the range 0 to 20,000, with any score below 4-5,000 being quite a confident recognition.</p></div><p>LBPH works similarly; however, the reference value for a good recognition is below 50, and any value above 80 is considered as a low confidence score.</p><p>A normal <a id="id307" class="indexterm"/>custom approach would be to hold-off drawing a rectangle around a recognized face until we have a number of frames with a satisfying arbitrary confidence score, but you have total freedom to use OpenCV's face recognition module to tailor your application to your needs.</p></div></div></div>
<div class="section" title="Summary" id="aid-1BRPS1"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Summary</h1></div></div></div><p>By now, you should have a good understanding of how face detection and face recognition work, and how to implement them in Python and OpenCV 3.</p><p>Face detection and face recognition are constantly evolving branches of computer vision, with algorithms being developed continuously, and they will evolve even faster in the near future with the emphasis posed on robotics and the Internet of things.</p><p>For now, the accuracy of detection and recognition heavily depends on the quality of the training data, so make sure you provide your applications with high-quality face databases and you will be satisfied with the results.</p></div></body></html>