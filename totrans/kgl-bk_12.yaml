- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modeling for Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Computer vision tasks are among the most popular problems in practical applications
    of machine learning; they were the gateway into deep learning for many Kagglers,
    including yours truly (Konrad, that is). Over the last few years, there has been
    tremendous progress in the field and new SOTA libraries continue to be released.
    In this chapter, we will give you an overview of the most popular competition
    types in computer vision:'
  prefs: []
  type: TYPE_NORMAL
- en: Image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will begin with a short section on image augmentation, a group of task-agnostic
    techniques that can be applied to different problems to increase the generalization
    capability of our models.
  prefs: []
  type: TYPE_NORMAL
- en: Augmentation strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While deep learning techniques have been extremely successful in computer vision
    tasks like image recognition, segmentation, or object detection, the underlying
    algorithms are typically extremely data-intensive: they require large amounts
    of data to avoid overfitting. However, not all domains of interest satisfy that
    requirement, which is where **data augmentation**comes in. This is the name for
    a group of image processing techniques that create modified versions of images,
    thus enhancing the size and quality of training datasets, leading to better performance
    of deep learning models. The augmented data will typically represent a more comprehensive
    set of possible data points, thereby minimizing the distance between the training
    and validation set, as well as any future test sets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will review some of the more common augmentation techniques,
    along with choices for their software implementations. The most frequently used
    transformations include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Flipping**: Flipping the image (along the horizontal or vertical axis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rotation**: Rotating the image by a given angle (clockwise or anti-clockwise)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cropping**: A random subsection of the image is selected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Brightness**: Modifying the brightness of the image'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling**: The image is increased or decreased to a higher (outward) or lower
    (inward) size'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Below, we demonstrate how those transformations work in practice using the
    image of an American acting legend and comedian, Betty White:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający osoba, niebieski  Opis wygenerowany automatycznie](img/B17574_10_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1: Betty White image'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can flip the image along the vertical or horizontal axes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający osoba, kobieta, niebieski  Opis wygenerowany automatycznie](img/B17574_10_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2: Betty White image – flipped vertically (left) and horizontally
    (right)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rotations are fairly self-explanatory; notice the automatic padding of the
    image in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający osoba  Opis wygenerowany automatycznie](img/B17574_10_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3: Betty White image – rotated clockwise'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also crop an image to the region of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający osoba, zamknąć, oczy, wpatrywanie się  Opis wygenerowany
    automatycznie](img/B17574_10_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Betty White image – cropped'
  prefs: []
  type: TYPE_NORMAL
- en: 'On a high level, we can say that augmentations can be applied in one of two
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Offline**: These are usually applied for smaller datasets (fewer images or
    smaller sizes, although the definition of “small” depends on the available hardware).
    The idea is to generate modified versions of the original images as a preprocessing
    step for your dataset, and then use those alongside the “original” ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Online**: These are used for bigger datasets. The augmented images are not
    saved on disk; the augmentations are applied in mini-batches and fed to the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next few sections, we will give you an overview of two of the most common
    methods for augmenting your image dataset: the built-in Keras functionality and
    the `albumentations` package. There are several other options available out there
    (`skimage`, OpenCV, `imgaug`, Augmentor, SOLT), but we will focus on the most
    popular ones.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods discussed in this chapter focus on image analysis powered by GPU.
    The use of **tensor processing units** (**TPUs**) is an emerging, but still somewhat
    niche, application. Readers interested in image augmentation in combination with
    TPU-powered analysis are encouraged to check out the excellent work of *Chris
    Deotte* (**@cdeotte**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords](https://www.kaggle.com/cdeotte/triple-stratified-kfold-with-tfrecords)'
  prefs: []
  type: TYPE_NORMAL
- en: Chris is a quadruple Kaggle Grandmaster and a fantastic educator through the
    Notebooks he creates and discussions he participates in; overall, a person definitely
    worth following for any Kaggler, irrespective of your level of experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be using data from the *Cassava Leaf Disease Classification* competition
    ([https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification)).
    As usual, we begin with the groundwork: first, loading the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define some helper functions that will streamline the presentation
    later. We need a way to load images into arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We would like to display multiple images in a gallery style, so we create a
    function that takes as input an array containing the images along with the desired
    number of columns, and outputs the array reshaped into a grid with a given number
    of columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With the boilerplate taken care of, we can load the images for augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s load a single image so we know what our reference is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający podłoże, roślina, zewnętrzne, zielony  Opis wygenerowany
    automatycznie](img/B17574_10_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.5: Reference image'
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will demonstrate how to generate augmented images
    from this reference image using both built-in Keras functionality and the `albumentations`
    library.
  prefs: []
  type: TYPE_NORMAL
- en: Keras built-in augmentations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Keras library has a built-in functionality for augmentations. While not
    as extensive as dedicated packages, it has the advantage of easy integration with
    your code. We do not need a separate code block for defining the augmentation
    transformations but can incorporate them inside `ImageDataGenerator`, a functionality
    we are likely to be using anyway.
  prefs: []
  type: TYPE_NORMAL
- en: The first Keras approach we examine is based upon the `ImageDataGenerator` class.
    As the name suggests, it can be used to generate batches of image data with real-time
    data augmentations.
  prefs: []
  type: TYPE_NORMAL
- en: ImageDataGenerator approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We begin by instantiating an object of class `ImageDataGenerator` in the following
    manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We define the desired augmentations as arguments to `ImageDataGenerator`. The
    official documentation does not seem to address the topic, but practical results
    indicate that the augmentations are applied in the order in which they are defined
    as arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the above example, we utilize only a limited subset of possible options;
    for a full list, the reader is encouraged to consult the official documentation:
    [https://keras.io/api/preprocessing/image/](https://keras.io/api/preprocessing/image/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we iterate through the images with the `.flow` method of the `ImageDataGenerator`
    object. The class provides three different functions to load the image dataset
    in memory and generate batches of augmented data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`flow`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flow_from_directory`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`flow_from_dataframe`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'They all achieve the same objective, but differ in the way the locations of
    the files are specified. In our example, the images are already in memory, so
    we can iterate using the simplest method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can examine the augmented images using the helper functions we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający roślina  Opis wygenerowany automatycznie](img/B17574_10_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: A collection of augmented images'
  prefs: []
  type: TYPE_NORMAL
- en: Augmentations are a very useful tool, but using them efficiently requires a
    judgment call. First, it is obviously a good idea to visualize them to get a feeling
    for the impact on the data. On the one hand, we want to introduce some variation
    in the data to increase the generalization of our model; on the other, if we change
    the images too radically, the input data will be less informative and the model
    performance is likely to suffer. In addition, the choice of which augmentations
    to use can also be problem-specific, as we can see by comparing different competitions.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at *Figure 10.6* above (the reference image from the *Cassava Leaf
    Disease Classification* competition), the leaves on which we are supposed to identify
    the disease can be of different sizes, pointing at different angles, and so on,
    due both to the shapes of the plants and differences in how the images are taken.
    This means transformations such as vertical or horizontal flips, cropping, and
    rotations all make sense in this context.
  prefs: []
  type: TYPE_NORMAL
- en: 'By contrast, we can look at a sample image from the *Severstal: Steel Defect
    Detection* competition ([https://www.kaggle.com/c/severstal-steel-defect-detection](https://www.kaggle.com/c/severstal-steel-defect-detection)).
    In this competition, participants had to localize and classify defects on a steel
    sheet. All the images had the same size and orientation, which means that rotations
    or crops would have produced unrealistic images, adding to the noise and having
    an adverse impact on the generalization capabilities of an algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający tekst, półka, zrzut ekranu  Opis wygenerowany automatycznie](img/B17574_10_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: Sample images from the Severstal competition'
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An alternative approach to data augmentation as a preprocessing step in a native
    Keras manner is to use the `preprocessing` layers API. The functionality is remarkably
    flexible: these pipelines can be used either in combination with Keras models
    or independently, in a manner similar to `ImageDataGenerator`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below we show briefly how a preprocessing layer can be set up. First, the imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We load a pretrained model in the standard Keras manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preprocessing layers can be used in the same way as other layers are used
    inside the `Sequential` constructor; the only requirement is that they need to
    be specified before any others, at the beginning of our model definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: albumentations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `albumentations` package is a fast image augmentation library that is built
    as a wrapper of sorts around other libraries.
  prefs: []
  type: TYPE_NORMAL
- en: The package is the result of intensive coding in quite a few Kaggle competitions
    (see [https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3](https://medium.com/@iglovikov/the-birth-of-albumentations-fe38c1411cb3)),
    and claims among its core developers and contributors quite a few notable Kagglers,
    including *Eugene Khvedchenya* ([https://www.kaggle.com/bloodaxe](https://www.kaggle.com/bloodaxe)),
    *Vladimir Iglovikov* ([https://www.kaggle.com/iglovikov](https://www.kaggle.com/iglovikov)),
    *Alex Parinov* ([https://www.kaggle.com/creafz](https://www.kaggle.com/creafz)),
    and *ZFTurbo* ([https://www.kaggle.com/zfturbo](https://www.kaggle.com/zfturbo)).
  prefs: []
  type: TYPE_NORMAL
- en: The full documentation can be found at [https://albumentations.readthedocs.io/en/latest/](https://albumentations.readthedocs.io/en/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Below we list the important characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: A unified API for different data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for all common computer vision tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration both with TensorFlow and PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using `albumentations` functionality to transform an image is straightforward.
    We begin by initializing the required transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we apply the transformations to our reference image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can access the augmented images with the `''image''` key and visualize the
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are our results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający roślina, zewnętrzne, ogród, trawa  Opis wygenerowany automatycznie](img/B17574_10_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: Image augmented using the albumentations library'
  prefs: []
  type: TYPE_NORMAL
- en: 'Having discussed augmentation as a crucial preprocessing step in approaching
    a computer vision problem, we are now in a position to apply this knowledge in
    the following sections, beginning with a very common task: image classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Chris_Deotte.png)'
  prefs: []
  type: TYPE_IMG
- en: Chris Deotte
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/cdeotte](https://www.kaggle.com/cdeotte)'
  prefs: []
  type: TYPE_NORMAL
- en: Before we proceed, let’s look at a brief conversation we had with Chris Deotte,
    who we’ve mentioned quite a few times in this book (including earlier in this
    chapter), and for good reason. He is a quadruple Kaggle Grandmaster and Senior
    Data Scientist & Researcher at NVIDIA, who joined Kaggle in 2019.
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*I enjoy competitions with fascinating data and competitions that require building
    creative novel models. My specialty is analyzing trained models to determine their
    strengths and weaknesses. Afterward, I enjoy improving the models and/or developing
    post-processing to boost CV LB.*'
  prefs: []
  type: TYPE_NORMAL
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  prefs: []
  type: TYPE_NORMAL
- en: '*I begin each competition by performing EDA (exploratory data analysis), creating
    a local validation, building some simple models, and submitting to Kaggle for
    leaderboard scores. This fosters an intuition of what needs to be done in order
    to build an accurate and competitive model.*'
  prefs: []
  type: TYPE_NORMAL
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle’s Shopee – Price Match Guarantee *was a challenging competition that
    required both image models and natural language models. A key insight was extracting
    embeddings from the two types of models and then determining how to use both image
    and language information together to find product matches.*
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  prefs: []
  type: TYPE_NORMAL
- en: '*Yes. Kaggle helped me become a senior data scientist at NVIDIA by improving
    my skills and boosting my resume’s marketability.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Many employers peruse the work on Kaggle to find employees with specific skills
    to help solve their specific projects. In this way, I have been solicited about
    many job opportunities.*'
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  prefs: []
  type: TYPE_NORMAL
- en: '*In my opinion, inexperienced Kagglers often overlook the importance of local
    validation. Seeing your name on the leaderboard is exciting. And it’s easy to
    focus on improving our leaderboard scores instead of our cross-validation scores.*'
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  prefs: []
  type: TYPE_NORMAL
- en: '*Many times, I have made the mistake of trusting my leaderboard score over
    my cross-validation score and selecting the wrong final submission.*'
  prefs: []
  type: TYPE_NORMAL
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: '*Absolutely. Feature engineering and quick experimentation are important when
    optimizing tabular data models. In order to accelerate the cycle of experimentation
    and validation, using NVIDIA RAPIDS cuDF and cuML on GPU are essential.*'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  prefs: []
  type: TYPE_NORMAL
- en: '*The most important thing is to have fun and learn. Don’t worry about your
    final placement. If you focus on learning and having fun, then over time your
    final placements will become better and better.*'
  prefs: []
  type: TYPE_NORMAL
- en: Do you use other competition platforms? How do they compare to Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*Yes, I have entered competitions outside of Kaggle. Individual companies like
    Booking.com or Twitter.com will occasionally host a competition. These competitions
    are fun and involve high-quality, real-life data.*'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will demonstrate an end-to-end pipeline that can be used
    as a template for handling image classification problems. We will walk through
    the necessary steps, from data preparation, to model setup and estimation, to
    results visualization. Apart from being informative (and cool), this last step
    can also be very useful if you need to examine your code in-depth to get a better
    understanding of the performance.
  prefs: []
  type: TYPE_NORMAL
- en: We will continue using the data from the *Cassava Leaf Disease Classification*
    contest ([https://www.kaggle.com/c/cassava-leaf-disease-classification](https://www.kaggle.com/c/cassava-leaf-disease-classification)).
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we begin by loading the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: It is usually a good idea to define a few helper functions; it makes for code
    that is easier to both read and debug. If you are approaching a general image
    classification problem, a good starting point can be provided by a model from
    the **EfficientNet** family, introduced in 2019 in a paper from the Google Research
    Brain Team ([https://arxiv.org/abs/1905.11946](https://arxiv.org/abs/1905.11946)).
    The basic idea is to balance network depth, width, and resolution to enable more
    efficient scaling across all dimensions and subsequently better performance. For
    our solution, we will use the simplest member of the family, **EfficientNet B0**,
    which is a mobile-sized network with 11 million trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: For a properly detailed explanation of the EfficientNet networks, you are encouraged
    to explore [https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html)
    as a starting point.
  prefs: []
  type: TYPE_NORMAL
- en: 'We construct our model with B0 as the basis, followed by a pooling layer for
    improved translation invariance and a dense layer with an activation function
    suitable for our multiclass classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Some brief remarks on the parameters we pass to the `EfficientNetB0` function:'
  prefs: []
  type: TYPE_NORMAL
- en: The `include_top` parameter allows you to decide whether to include the final
    dense layers. As we want to use the pre-trained model as a feature extractor,
    a default strategy would be to skip them and then define the head ourselves.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`weights` can be set to `None` if we want to train the model from scratch,
    or to `''imagenet''`or `''noisy-student''` if we instead prefer to utilize the
    weights pre-trained on large image collections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The helper function below allows us to visualize the activation layer, so we
    can examine the network performance from a visual angle. This is frequently helpful
    in developing an intuition in a field notorious for its opacity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We generate the activations by creating predictions for a given model based
    on a “restricted” model, in other words, using the entire architecture up until
    the penultimate layer; this is the code up to the `activations` variable. The
    rest of the function ensures we show the right layout of activations, corresponding
    to the shape of the filter in the appropriate convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we process the labels and set up the validation scheme; there is no special
    structure in the data (for example, a time dimension or overlap across classes),
    so we can use a simple random split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: For a refresher on more elaborate validation schemes, refer to *Chapter 6*,
    *Designing Good Validation*.
  prefs: []
  type: TYPE_NORMAL
- en: We are now able to set up the data generators, which are necessary for our TF-based
    algorithm to loop through the image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we instantiate two `ImageDataGenerator` objects; this is when we incorporate
    the image augmentations. For the purpose of this demonstration, we will go with
    the Keras built-in ones. After that, we create the generator using a `flow_from_dataframe()`
    method, which is used to generate batches of tensor image data with real-time
    data augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'With the data structures specified, we can create the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Once our model is created, we can quickly examine a summary. This is mostly
    useful for sanity checks, because unless you have a photographic memory, chances
    are you are not going to remember the layer composition batches of a sophisticated
    model like EffNetB0\. In practice, you can use the summary to check whether the
    dimensions of output filters are correct or whether the parameter counts (trainable
    on non-trainable) are in line with expectations. For the sake of compactness,
    we only demonstrate the first few lines of the output below; inspecting the architecture
    diagram for B0 will give you an idea of how long the complete output would be.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With the above steps taken care of, we can proceed to fitting the model. In
    this step, we can also very conveniently define callbacks. The first one is `ModelCheckpoint`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The checkpoint uses a few parameters worth elaborating on:'
  prefs: []
  type: TYPE_NORMAL
- en: We can preserve the best set of model weights by setting `save_best_only = True`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reduce the size of the model by only keeping the weights, instead of the
    complete set of optimizer state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We decide on which model is optimal by locating a minimum for validation loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we use one of the popular methods for preventing overfitting, **early
    stopping**. We monitor the performance of the model on the holdout set and stop
    the algorithm if the metric stops improving for a given number of epochs, in this
    case `5`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ReduceLROnPlateau` callback monitors the loss on the holdout set and if
    no improvement is seen for a `patience` number of epochs, the learning rate is
    reduced, in this case by a factor of 0.3\. While not a universal solution, it
    can frequently help with convergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to fit the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We will briefly explain the two parameters we have not encountered before:'
  prefs: []
  type: TYPE_NORMAL
- en: The training generator yields `steps_per_epoch` batches per training epoch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the epoch is finished, the validation generator produces `validation_steps`
    batches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example output after calling `model.fit()` is given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Once a model is fitted, we can examine the activations on a sample image using
    the helper function we wrote at the start. While this is not necessary for successful
    model execution, it can help determine what sort of features our model is extracting
    before applying the classification layer at the top:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what we might see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający tekst, warzywo, kolorowy  Opis wygenerowany automatycznie](img/B17574_10_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Sample activations from a fitted model'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can generate the predictions with `model.predict()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: We build the predictions by iterating through the list of images. For each of
    them, we reshape the image to the required dimensions and pick the channel with
    the strongest signal (the model predicts class probabilities, of which we pick
    the largest one with `argmax`). The final predictions are class numbers, in line
    with the metric utilized in the competition.
  prefs: []
  type: TYPE_NORMAL
- en: We have now demonstrated a minimal end-to-end pipeline for image classification.
    Numerous improvements are, of course, possible – for instance, more augmentations,
    bigger architecture, callback customization – but the basic underlying template
    should provide you with a good starting point going forward.
  prefs: []
  type: TYPE_NORMAL
- en: 'We move on now to a second popular problem in computer vision: object detection.'
  prefs: []
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Object detection** is a computer vision/image processing task where we need
    to identify instances of semantic objects of a certain class in an image or video.
    In classification problems like those discussed in the previous section, we simply
    need to assign a class to each image, whereas in object detection tasks, we want
    to draw a **bounding box** around an object of interest to locate it within an
    image.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will use data from the *Global Wheat Detection* competition
    ([https://www.kaggle.com/c/global-wheat-detection](https://www.kaggle.com/c/global-wheat-detection)).
    In this competition, participants had to detect wheat heads, which are spikes
    atop plants containing grain. Detection of these in plant images is used to estimate
    the size and density of wheat heads across crop varieties. We will demonstrate
    how to train a model for solving this using **Yolov5**, a well-established model
    in object detection, and state-of-the-art until late 2021 when it was (based on
    preliminary results) surpassed by the YoloX architecture. Yolov5 gave rise to
    extremely competitive results in the competition and although it was eventually
    disallowed by the organizers due to licensing issues, it is very well suited for
    the purpose of this demonstration.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_10_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Sample image visualizations of detected wheat heads'
  prefs: []
  type: TYPE_NORMAL
- en: An important point worth mentioning before we begin is the different formats
    for bounding box annotations; there are different (but mathematically equivalent)
    ways of describing the coordinates of a rectangle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common types are coco, voc-pascal, and yolo. The differences between
    them are clear from the figure below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_10_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Annotation formats for bounding boxes'
  prefs: []
  type: TYPE_NORMAL
- en: 'One more part we need to define is the grid structure: Yolo detects objects
    by placing a grid over an image and checking for the presence of an object of
    interest (wheat head, in our case) in any of the cells. The bounding boxes are
    reshaped to be offset within the relevant cells of the image and the *(x, y, w,
    h)* parameters are scaled to the unit interval:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_10_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.12: Yolo annotation positioning'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by loading the annotations for our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s inspect a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B17574_10_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.13: Training data with annotations'
  prefs: []
  type: TYPE_NORMAL
- en: 'We extract the actual coordinates of the bounding boxes from the `bbox` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to extract the coordinates in Yolo format into separate columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The implementation from Ultralytics has some requirements on the structure of
    the dataset, specifically, where the annotations are stored and the folders for
    training/validation data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The creation of the folders in the code below is fairly straightforward, but
    a more inquisitive reader is encouraged to consult the official documentation
    ([https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The next thing we do is install the Yolo package itself. If you are running
    this in a Kaggle Notebook or Colab, make sure to double-check GPU is enabled;
    Yolo installation will actually work without it, but you are likely to run into
    all sorts of timeouts and memory issues due to CPU versus GPU performance differences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We omit the output, as it is rather extensive. The last bit of preparation
    needed is the YAML configuration file, where we specify the training and validation
    data locations and the number of classes. We are only interested in detecting
    wheat heads and not distinguishing between different types, so we have one class
    (its name is only provided for notational consistency and can be an arbitrary
    string in this instance):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we can start training our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Unless you are used to launching things from the command line, the incantation
    above is positively cryptic, so let’s discuss its composition in some detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '`train.py` is the workhorse script for training a YoloV5 model, starting from
    pre-trained weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--img 512` means we want the original images (which, as you can see, we did
    not preprocess in any way) to be rescaled to 512x512\. For a competitive result,
    you should use a higher resolution, but this code was executed in a Kaggle Notebook,
    which has certain limitations on resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--batch` refers to the batch size in the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--epochs 3` means we want to train the model for three epochs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--workers 2` specifies the number of workers in the data loader. Increasing
    this number might help performance, but there is a known bug in version 6.0 (the
    most recent one available in the Kaggle Docker image, as of the time of this writing)
    when the number of workers is too high, even on a machine where more might be
    available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--data wheat.yaml` is the file pointing to our data specification YAML file,
    defined above.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--cfg "./yolov5/models/yolov5s.yaml"` specifies the model architecture and
    the corresponding set of weights to be used for initialization. You can use the
    ones provided with the installation (check the official documentation for details),
    or you can customize your own and keep them in the same `.yaml` format.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--name` specifies where the resulting model is to be stored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We break down the output of the training command below. First, the groundwork:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Then comes the model. We see a summary of the architecture, the optimizer setup,
    and the augmentations used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This is followed by the actual training log:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The results from both training and validation stages can be examined; they
    are stored in the `yolov5` folder under `./yolov5/runs/train/yolov5x_fold0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_10_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.14: Validation data with annotations'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have trained the model, we can use the weights from the best performing
    model (Yolov5 has a neat functionality of automatically keeping both the best
    and last epoch model, storing them as `best.pt` and `last.pt`) to generate predictions
    on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We will discuss the parameters that are specific to the inference stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--weights` points to the location of the best weights from our model trained
    above.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--conf 0.1` specifies which candidate bounding boxes generated by the model
    should be kept. As usual, it is a compromise between precision and recall (too
    low a threshold gives a high number of false positives, while moving the threshold
    too high means we might not find any wheat heads at all).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--source` is the location of the test data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The labels created for our test images can be inspected locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what we might see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at an individual prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'It has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This means that in image `2fd875eaa`, our trained model detected two bounding
    boxes (their coordinates are entries 2-5 in the row), with confidence scores above
    0.1 given at the end of the row.
  prefs: []
  type: TYPE_NORMAL
- en: 'How do we go about combining the predictions into a submission in the required
    format? We start by defining a helper function that helps us convert the coordinates
    from the yolo format to coco (as required in this competition): it is a matter
    of simple rearrangement of the order and normalizing to the original range of
    values by multiplying the fractions by the image size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We then proceed to generate a submission file:'
  prefs: []
  type: TYPE_NORMAL
- en: We loop over the files listed above.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each file, all rows are converted into strings in the required format (one
    row represents one bounding box detected).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The rows are then concatenated into a single string corresponding to this file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The generated `submission.csv` file completes our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we have demonstrated how to use a YoloV5 to solve the problem
    of object detection: how to handle annotations in different formats, how to customize
    a model for a specific task, train it, and evaluate the results.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on this knowledge, you should be able to start working with object detection
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now move on to the third popular class of computer vision tasks: semantic
    segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way to think about **segmentation** is that it classifies each pixel
    in an image, assigning it to a corresponding class; combined, those pixels form
    areas of interest, such as regions with disease on an organ in medical images.
    By contrast, object detection (discussed in the previous section) classifies patches
    of an image into different object classes and creates bounding boxes around them.
  prefs: []
  type: TYPE_NORMAL
- en: We will demonstrate the modeling approach using data from the *Sartorius – Cell
    Instance Segmentation* competition ([https://www.kaggle.com/c/sartorius-cell-instance-segmentation](https://www.kaggle.com/c/sartorius-cell-instance-segmentation)).
    In this one, the participants were tasked to train models for instance segmentation
    of neural cells using a set of microscopy images.
  prefs: []
  type: TYPE_NORMAL
- en: Our solution will be built around **Detectron2**, a library created by Facebook
    AI Research that supports multiple detection and segmentation algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Detectron2 is a successor to the original Detectron library ([https://github.com/facebookresearch/Detectron/](https://github.com/facebookresearch/Detectron/))
    and the Mask R-CNN project ([https://github.com/facebookresearch/maskrcnn-benchmark/](https://github.com/facebookresearch/maskrcnn-benchmark/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by installing the extra packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We install `pycocotools` ([https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools](https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools)),
    which we will need to format the annotations, and Detectron2 ([https://github.com/facebookresearch/detectron2](https://github.com/facebookresearch/detectron2)),
    our workhorse in this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can train our model, we need a bit of preparation: the annotations
    need to be converted from the **run-length encoding** (**RLE**) format provided
    by the organizers to the COCO format required as input for Detectron2\. The basic
    idea behind RLE is saving space: creating a segmentation means marking a group
    of pixels in a certain manner. Since an image can be thought of as an array, this
    area can be denoted by a series of straight lines (row- or column-wise).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can encode each of those lines by listing the indices, or by specifying
    a starting position and the length of the subsequent contiguous block. A visual
    example is given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B17574_10_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.15: Visual representation of RLE'
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft’s **Common Objects in Context** (**COCO**) format is a specific JSON
    structure dictating how labels and metadata are saved for an image dataset. Below,
    we demonstrate how to convert RLE to COCO and combine it with a *k*-fold validation
    split, so we get the required train/validation pair of JSON files for each fold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'We need three functions to go from RLE to COCO. First, we need to convert from
    RLE to a binary mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The second one converts a binary mask to RLE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we combine the two in order to produce the COCO output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We split our data into non-overlapping folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now loop over the folds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason why the loop has to be executed in pieces is the size limit of the
    Kaggle environment: the maximum size of Notebook output is limited to 20 GB, and
    5 folds with 2 files (training/validation) for each fold meant a total of 10 JSON
    files, exceeding that limit.'
  prefs: []
  type: TYPE_NORMAL
- en: Such practical considerations are worth keeping in mind when running code in
    a Kaggle Notebook, although for such “preparatory” work, you can, of course, produce
    the results elsewhere, and upload them as Kaggle Datasets afterward.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the splits produced, we can move toward training a Detectron2 model for
    our dataset. As usual, we start by loading the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'While the number of imports from Detectron2 can seem intimidating at first,
    their function will become clear as we progress with the task definition; we start
    by specifying paths to the input data folder, annotations folder, and a YAML file
    defining our preferred model architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'One point worth mentioning here is the iterations parameter (`nof_iters` above).
    Usually, model training is parametrized in terms of the number of epochs, in other
    words, complete passes through the training data. Detectron2 is engineered differently:
    one iteration refers to one mini-batch and different mini-batch sizes are used
    in different parts of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to ensure the results are reproducible, we fix random seeds used by
    different components of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The competition metric was the mean average precision at different **intersection
    over union** (**IoU**) thresholds. As a refresher from *Chapter 5*, *Competition
    Tasks and Metrics*, the IoU of a proposed set of object pixels and a set of true
    object pixels is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_10_001.png)'
  prefs: []
  type: TYPE_IMG
- en: The metric sweeps over a range of IoU thresholds, at each point calculating
    an average precision value. The threshold values range from 0.5 to 0.95, with
    increments of 0.05.
  prefs: []
  type: TYPE_NORMAL
- en: At each threshold value, a precision value is calculated based on the number
    of **true positives** (**TP**), **false negatives** (**FN**), and **false positives**
    (**FP**) resulting from comparing the predicted object with all ground truth objects.
    Lastly, the score returned by the competition metric is the mean taken over the
    individual average precisions of each image in the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, we define the functions necessary to calculate the metric and use it
    directly inside the model as the objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'With the metric defined, we can use it in the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us the basis for creating a `Trainer` object, which is the workhorse
    of our solution built around Detectron2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'We now proceed to load the training/validation data in Detectron2 style:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we instantiate a Detectron2 model, we need to take care of configuring
    it. Most of the values can be left at default values (at least, in a first pass);
    if you decide to tinker a bit more, start with `BATCH_SIZE_PER_IMAGE` (for increased
    generalization performance) and `SCORE_THRESH_TEST` (to limit false negatives):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Training a model is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that the output during training is rich in information about
    the progress of the procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający tekst  Opis wygenerowany automatycznie](img/B17574_10_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.16: Training output from Detectron2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model is trained, we can save the weights and use them for inference
    (potentially in a separate Notebook – see the discussion earlier in this chapter)
    and submission preparation. We start by adding new parameters that allow us to
    regularize the prediction, setting confidence thresholds and minimal mask sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We need a helper function for encoding a single mask into RLE format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Below is the main function for producing all masks per image, filtering out
    the dubious ones (with confidence scores below `THRESHOLDS`) with small areas
    (containing fewer pixels than `MIN_PIXELS`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'We then prepare the lists where image IDs and masks will be stored:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Competitions with large image sets – like the ones discussed in this section
    – often require training models for longer than 9 hours, which is the time limit
    imposed in Code competitions (see [https://www.kaggle.com/docs/competitions](https://www.kaggle.com/docs/competitions)).
    This means that training a model and running inference within the same Notebook
    becomes impossible. A typical workaround is to run a training Notebook/script
    first as a standalone Notebook in Kaggle, Google Colab, GCP, or locally. The output
    of this first Notebook (the trained weights) is used as input to the second one,
    in other words, to define the model used for predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We proceed in that manner by loading the weights of our trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize some of the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B17574_10_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.17: Visualizing a sample prediction from Detectron2 alongside the
    source image'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the helper functions defined above, producing the masks in RLE format
    for submission is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the first few rows of the final submission:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający tekst  Opis wygenerowany automatycznie](img/B17574_10_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.18: Formatted submission from a trained Detectron2 model'
  prefs: []
  type: TYPE_NORMAL
- en: We have reached the end of the section. The pipeline above demonstrates how
    to set up a semantic segmentation model and train it. We have used a small number
    of iterations, but in order to achieve competitive results, longer training is
    necessary.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Laura_Fink.png)'
  prefs: []
  type: TYPE_IMG
- en: Laura Fink
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.kaggle.com/allunia](https://www.kaggle.com/allunia)'
  prefs: []
  type: TYPE_NORMAL
- en: To wrap up this chapter, let’s see what Kaggler Laura Fink has to say about
    her time on the platform. As well as being a Notebooks Grandmaster and producing
    many masterful Notebooks, she is also Head of Data Science at MicroMata.
  prefs: []
  type: TYPE_NORMAL
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  prefs: []
  type: TYPE_NORMAL
- en: '*My favorite competitions are those that want to yield something good to humanity.
    I especially like all healthcare-related challenges. Nonetheless, each competition
    feels like an adventure for me with its own puzzles to be solved. I really enjoy
    learning new skills and exploring new kinds of datasets or problems. Consequently,
    I’m not focused on specific techniques but rather on learning something new. I
    think I’m known for my strengths in exploratory data analysis (EDA).*'
  prefs: []
  type: TYPE_NORMAL
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  prefs: []
  type: TYPE_NORMAL
- en: '*When entering a competition, I start by reading the problem statement and
    the data description. After browsing through the forum and public Notebooks for
    collecting ideas, I usually start by developing my own solutions. In the initial
    phase, I spend some time on EDA to search for hidden groups and get some intuition.
    This helps quite a lot in setting up a proper validation strategy, which I believe
    is the foundation of all remaining steps. Then, I start to iterate through different
    parts of the machine learning pipeline like feature engineering or preprocessing,
    improving the model architecture, asking questions about the data collection,
    searching for leakages, doing more EDA, or building ensembles. I try to improve
    my solution in a greedy fashion. Kaggle competitions are very dynamic and one
    needs to try out diverse ideas and different solutions to survive in the end.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*This is definitely different from my day-to-day work, where the focus is more
    on gaining insights from data and finding simple but effective solutions to improve
    business processes. Here, the task is often more complex than the models used.
    The problem to be solved has to be defined very clearly, which means that one
    has to discuss with experts of different backgrounds which goals should be reached,
    which processes are involved, and how the data needs to be collected or fused.
    Compared to Kaggle competitions, my daily work needs much more communication than
    machine learning skills.*'
  prefs: []
  type: TYPE_NORMAL
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  prefs: []
  type: TYPE_NORMAL
- en: '*The* G2Net Gravitational Wave Detection *competition was one of my favorites.
    The goal was to detect simulated gravitational wave signals that were hidden in
    noise originating from detector components and terrestrial forces. An important
    insight during this competition was that you should have a critical look at standard
    ways to analyze data and try out your own ideas. In the papers I read, the data
    was prepared mainly by using the Fourier or Constant-Q transform after whitening
    the data and applying a bandpass filter.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*It came out very quickly that whitening was not helpful, as it used spline
    interpolation of the Power Spectral Density, which was itself very noisy. Fitting
    polynomials to small subsets of noisy data adds another source of errors because
    of overfitting.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*After dropping the whitening, I tried out different hyperparameters of the
    Constant-Q transform, which turned out to be the leading method in the forum and
    public* *Notebooks for a long time. As there were two sources of gravitational
    waves that can be covered by different ranges of Q-values, I tried out an ensemble
    of models that differed in these hyperparameters. This turned out to be helpful
    in improving my score, but then I reached a limit. The Constant-Q transform applies
    a series of filters to time series and transforms them into the frequency domain.
    I started to ask myself if there was a method that does these filtering tasks
    in a better, more flexible way. It was at the same time that the idea of using
    1 dim CNNs came up in the community, and I loved it. We all know that filters
    of 2 dim CNNs are able to detect edges, lines, and textures given image data.
    The same could be done with “classical” filters like the Laplace or Sobel filter.
    For this reason, I asked myself: can’t we use the 1dCNN to learn the most important
    filters on its own, instead of applying transformations that are already fixed
    somehow?*'
  prefs: []
  type: TYPE_NORMAL
- en: '*I was not able to get my 1 dim CNN solution to work, but it turned out that
    many top teams managed it well. The G2Net competition was one of my favorites
    even though I missed out on the goal of winning a medal. However, the knowledge
    I gained along the way and the lesson I learned about so-called standard approaches
    were very valuable.*'
  prefs: []
  type: TYPE_NORMAL
- en: Has Kaggle helped you in your career? If so, how?
  prefs: []
  type: TYPE_NORMAL
- en: '*I started my first job after university as a Java software developer even
    though I already had my first contact with machine learning during my master’s
    thesis. I was interested in doing more data analytics, but at that time, there
    were almost no data science jobs, or they were not named this way. When I heard
    about Kaggle the first time, I was trapped right from the start. Since then, I
    often found myself on Kaggle during the evenings to have some fun. It was not
    my intent to change my position at that time, but then a research project came
    up that needed machine learning skills. I was able to show that I was a suitable
    candidate for this project because of the knowledge I gained by participating
    on Kaggle. This turned out to be the entry point for my data science career.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Kaggle has always been a great place for me to try out ideas, learn new methods
    and tools, and gain practical experience. The skills I obtained this way have
    been quite helpful for data science projects at work. It’s like a boost of knowledge,
    as Kaggle provides a sandbox for you to try out different ideas and to be creative
    without risk. Failing in a competition means that there was at least one lesson
    to learn, but failing in a project can have a huge negative impact on yourself
    and other people.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Besides taking part in competitions, another great way to build up your portfolio
    is to write Notebooks. In doing so, you can show the world how you approach problems
    and how to communicate insights and conclusions. The latter is very important
    when you have to work with management, clients, and experts from different backgrounds.*'
  prefs: []
  type: TYPE_NORMAL
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  prefs: []
  type: TYPE_NORMAL
- en: '*I think many beginners that enter competitions are seduced by the public leaderboard
    and build their models without having a good validation strategy. While measuring
    their success on the leaderboard, they are likely overfitting to the public test
    data. After the end of the competition, their models are not able to generalize
    to the unseen private test data, and they often fall down hundreds of places.
    I still remember how frustrated I was during the* Mercedes-Benz Greener Manufacturing
    *competition as I was not able to climb up the public leaderboard. But when the
    final standings came out, it was a big surprise how many people were shuffled
    up and down the leaderboard. Since then, I always have in mind that a proper validation
    scheme is very important for managing the challenges of under- and overfitting.*'
  prefs: []
  type: TYPE_NORMAL
- en: What mistakes have you made in competitions in the past?
  prefs: []
  type: TYPE_NORMAL
- en: '*My biggest mistake so far was to spend too much time and effort on the details
    of my solution at the beginning of a competition. Indeed, it’s much better to
    iterate fast through diverse and different ideas after building a proper validation
    strategy. That way, it’s easier and faster to find promising directions for improvements
    and the danger of getting stuck somewhere is much smaller.*'
  prefs: []
  type: TYPE_NORMAL
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  prefs: []
  type: TYPE_NORMAL
- en: '*There are a lot of common tools and libraries you can learn and practice when
    becoming active in the Kaggle community and I can only recommend them all. It’s
    important to stay flexible and to learn about their advantages and disadvantages.
    This way, your solutions don’t depend on your tools, but rather on your ideas
    and creativity.*'
  prefs: []
  type: TYPE_NORMAL
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  prefs: []
  type: TYPE_NORMAL
- en: '*Data science is not about building models, but rather about understanding
    the data and the way it was collected. Many competitions I have entered so far
    showed leakages or had hidden groups in the test data that one could find with
    exploratory data analysis.*'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we gave you an overview of the most important topics related
    to computer vision from a Kaggle competition angle. We introduced augmentations,
    an important class of techniques used for extending the generalization capabilities
    of an algorithm, and followed by demonstrating end-to-end pipelines for three
    of the most frequent problems: image classification, object detection, and semantic
    segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we switch our attention to natural language processing,
    another extremely broad and popular category of problems.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code40480600921811704671.png)'
  prefs: []
  type: TYPE_IMG
