<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Automated Optical Inspection, Object Segmentation, and Detection</h1>
                </header>
            
            <article>
                
<p>In <a href="ceaab6b4-2f4a-45e4-9f5d-2544c75bd405.xhtml">Chapter 4</a>, <em>Delving into Histogram and Filters</em>, we learned about histograms and filters, which allow us to understand image manipulation and create a photo application.</p>
<p>In this chapter, we are going to introduce the basic concepts of object segmentation and detection. This means isolating the objects that appear in an image for future processing and analysis.</p>
<p>This chapter introduces the following topics:</p>
<ul>
<li>Noise removal</li>
<li>Light/background removal basics</li>
<li>Thresholding</li>
<li>Connected components for object segmentation</li>
<li>Finding contours for object segmentation</li>
</ul>
<p><span><span>Many i</span></span>ndustries use complex computer vision systems and hardware. Computer vision tries to detect problems and minimize errors produced in the production process, improving the quality of final products.</p>
<p class="mce-root"/>
<p>In this sector, the name for this computer vision task is <strong>Automated Optical Inspection</strong> (<strong>AOI</strong>). This name appears in the inspection of printed circuit board manufacturers, where one or more cameras scan each circuit to detect critical failures and quality defects. This nomenclature was used in other manufacturing industries so that they could use optical camera systems and computer vision algorithms to increase product quality. Nowadays, optical inspection using different camera types (infrared or 3D cameras), depending on the requirements, and complex algorithms are used in thousands of industries for different purposes such as defect detection, classification, and so on.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>This chapter requires familiarity with the basic C++ programming language. All of the code that's used in this chapter can be downloaded from the following GitHub link: <a href="https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_05">https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_05</a>. The code can be executed on any operating system, though it is only tested on Ubuntu.</p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2DRbMbz">http://bit.ly/2DRbMbz</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Isolating objects in a scene</h1>
                </header>
            
            <article>
                
<p>In this chapter, we are going to introduce the first step in an AOI algorithm and try to isolate different parts or objects in a scene. We are going to take the example of the object detection and classification of three object types (screw, packing ring, and nut) and develop them in this chapter and <a href="83822325-00be-4874-813c-b90097030d85.xhtml">Chapter 6</a>, <em>Learning Object Classification</em>.</p>
<p>Imagine that we are in a company that produces these three objects. All of them are in the same carrier tape. Our objective is to detect each object in the carrier tape and classify each one to allow a robot to put each object on the correct shelf:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/91eceec6-11d9-4b16-820d-cf0197e6f02c.png" style="width:41.92em;height:33.58em;"/></p>
<p><span>In this chapter, we are going to learn how to isolate each object and detect its position in the image in pixels. In the next chapter, we are going to learn how to classify each isolated object to recognize if it is a nut, screw, or a packing ring.</span></p>
<p class="mce-root"/>
<p>In the following screenshot, we show our desired result, where there are a few objects in the left image. In the right image, we have drawn each one in a different color, showing different features such as area, height, width, and contour size:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-955 image-border" src="assets/530371fc-0a24-4292-84d5-06ddbc6015e5.png" style="width:43.83em;height:17.75em;"/></div>
<p>To reach this result, we are going to follow different steps that allow us to understand and organize our algorithms better. We can see these steps in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1050 image-border" src="assets/b95b6820-afcb-47af-8838-0c76b82a8b8a.png" style="width:23.08em;height:27.08em;"/></div>
<p>Our application will be divided into two chapters. In this chapter, we are going to develop and understand the preprocessing and segmentation steps. In <a href="83822325-00be-4874-813c-b90097030d85.xhtml">Chapter 6</a>, <em>Learning Object Classification</em>, we are going to extract the characteristics of each segmented object and train our machine learning system/algorithm on how to recognize each object class.</p>
<p>Our preprocessing steps will be divided into three more subsets:</p>
<ul>
<li><strong>Noise Removal</strong></li>
<li><strong>Light Removal</strong></li>
<li><strong>Binarization</strong></li>
</ul>
<p>In the segmentation step, we are going to use two different algorithms:</p>
<ul>
<li>Contour detection</li>
<li><strong>Connected components</strong> extraction (labeling)</li>
</ul>
<p>We can see these steps and the application flow in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1051 image-border" src="assets/620e2715-7061-4e32-92a5-19d5b9a3f671.png" style="width:30.08em;height:26.75em;"/></div>
<p>Now, it's time to start the preprocessing step so that we can get the best <strong>Binarization</strong> image by removing the noise and lighting effects. This minimizes any possible detection errors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an application for AOI</h1>
                </header>
            
            <article>
                
<p>To create our new application, we require a few input parameters. When a user executes the <span><span>application</span></span>, all of them are optional, excluding the input image to process. The input parameters are as follows:</p>
<ul>
<li>Input image to process</li>
<li>Light image pattern</li>
<li>Light operation, where a user can choose between difference or divide operations</li>
<li>If the user sets <kbd>0</kbd> as a value, the difference operation is applied</li>
<li>If the user set <kbd>1</kbd> as a value, the division operation is applied</li>
<li>Segmentation, where the user can choose between connected components with or without statistics and find contour methods</li>
<li>If the user sets <kbd>1</kbd> as the input value, the connected components method for segment is applied</li>
<li>If the user sets <kbd>2</kbd> as the input value, the connected components method with the statistics area is applied</li>
<li>If the user sets <kbd>3</kbd> as the input value, the find contours method is applied for Segmentation</li>
</ul>
<p>To enable this user selection, we are going to use the <kbd>command line parser</kbd> class with the following keys:</p>
<pre>// OpenCV command line parser functions 
// Keys accepted by command line parser 
const char* keys = 
{ 
  "{help h usage ? | | print this message}" 
   "{@image || Image to process}" 
   "{@lightPattern || Image light pattern to apply to image input}" 
   "{lightMethod | 1 | Method to remove background light, 0 difference, 1 div }" 
   "{segMethod | 1 | Method to segment: 1 connected Components, 2 connected components with stats, 3 find Contours }" 
}; </pre>
<p>We are going to use the <kbd>command line parser</kbd> class <span>in the <kbd>main</kbd> function by</span> checking the parameters. The <kbd>CommandLineParser</kbd> is explained in <a href="37cf2702-b8c6-41ff-a935-fd4030f8ce64.xhtml">Chapter 2</a>, <em>An Introduction to the Basics of OpenCV</em>, in the <em>Reading videos and cameras</em> <span>section:</span></p>
<pre>int main(int argc, const char** argv) 
{ 
  CommandLineParser parser(argc, argv, keys); 
  parser.about("Chapter 5. PhotoTool v1.0.0"); 
  //If requires help show 
  if (parser.has("help")) 
  { 
      parser.printMessage(); 
      return 0; 
  } 
 
  String img_file= parser.get&lt;String&gt;(0); 
  String light_pattern_file= parser.get&lt;String&gt;(1); 
  auto method_light= parser.get&lt;int&gt;("lightMethod"); 
  auto method_seg= parser.get&lt;int&gt;("segMethod"); 
   
  // Check if params are correctly parsed in his variables 
  if (!parser.check()) 
  { 
      parser.printErrors(); 
      return 0; 
  } </pre>
<p>After parsing our command-line user data, we need to check the input image has been loaded correctly. We then load the image and check it has data:</p>
<pre>// Load image to process 
  Mat img= imread(img_file, 0); 
  if(img.data==NULL){ 
    cout &lt;&lt; "Error loading image "&lt;&lt; img_file &lt;&lt; endl; 
    return 0; 
  } </pre>
<p>Now, we are ready to create our AOI process of segmentation. We are going to start with the preprocessing task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preprocessing the input image</h1>
                </header>
            
            <article>
                
<p>This section introduces some of the most common techniques that we can apply for preprocessing images in the context of object segmentation/detection. The preprocessing is the first change we make to a new image before we start working and extracting the information we require from it. Normally, in the preprocessing step, we try to minimize the image noise, light conditions, or image deformation due to a camera lens. These steps minimize errors while detecting objects or segments in our image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Noise removal</h1>
                </header>
            
            <article>
                
<p>If we don't remove the noise, we can detect more objects than we expect because noise is <span>normally</span> <span>represented as small points in the image and can be segmented as an object. The sensor and scanner circuit normally produces this noise. This variation of brightness or color can be represented in different types, such as Gaussian noise, spike noise, and shot noise.</span></p>
<p>There are different techniques that can be used to remove the noise. Here, we are going to use a smooth operation, but depending on the type of noise, some are better than others. A median filter is normally used for removing salt-and-<span>pepper</span> noise; for example, consider the following image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1052 image-border" src="assets/578318f1-5c92-42a9-a75a-fb6c2d6cee45.png" style="width:30.08em;height:20.25em;"/></div>
<p>The preceding image is the original input with salt-and-pepper noise. If we apply a median blur, we get an awesome result in which we lose small details. For example, we lose the borders of the screw, but we maintain perfect edges<span>. See the result in the following image:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1053 image-border" src="assets/ee868769-9c8d-4a81-b863-80c4c3455735.png" style="width:26.58em;height:19.83em;"/></p>
<p><span>If we apply a box filter or Gaussian filter, the noise is not removed but made smooth, and the details of the objects are lost and smoothened too.</span> See the following image for the result:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1054 image-border" src="assets/ddc55313-3189-4ceb-b113-ec491c19868e.png" style="width:26.50em;height:19.83em;"/></p>
<p>OpenCV brings us the <kbd><span class="CodeInTextPACKT">medianBlur</span></kbd> function, which requires three parameters:</p>
<ul>
<li>An input image with the <kbd>1</kbd>, <kbd>3</kbd>, or <kbd>4</kbd> channel's image. When the kernel size is bigger than <kbd>5</kbd>, the image depth can only be <span class="CodeInTextPACKT"><kbd>CV_8U</kbd>.</span></li>
<li>An output image, which is the resulting image on applying median blur with the same type and depth as the input.</li>
</ul>
<ul>
<li>Kernel size, which is an aperture size greater than <kbd>1</kbd> and odd, for example, 3, 5, 7, and so on.</li>
</ul>
<p>The following code is used to remove noise:</p>
<pre>  Mat img_noise; 
  medianBlur(img, img_noise, 3); </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing the background using the light pattern for segmentation</h1>
                </header>
            
            <article>
                
<p>In this section, we are going to develop a basic algorithm that will enable us to remove the background using a light pattern. This preprocessing gives us better segmentation. The input image without noise is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1055 image-border" src="assets/c86fa656-74fa-4826-abe4-7fc224178280.png" style="width:32.75em;height:21.92em;"/></p>
<p><span>If we apply a basic threshold, we will obtain an image</span> <span>result like this:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b3f80307-88d1-41d1-a11b-87e56bf39eef.png" style="width:34.00em;height:22.75em;"/></p>
<p><span>We can see that the top image artifact has a lot of white noise. If we apply a light pattern and background removal technique, we can obtain an awesome</span> <span>result in which we can see that there are no artifacts in the top of image, like the previous threshold operation, and we will obtain better results when we have to segment. We can see the result of background removal and thresholding in the following image:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1056 image-border" src="assets/0e37e0ca-7254-44ac-ac6f-b57b85ad9de5.png" style="width:33.50em;height:22.42em;"/></p>
<div class="packt_figure CDPAlignCenter CDPAlign"/>
<p>Now, how can we remove the light from our image? This is very simple: we only need a picture of our scenario without any objects, taken from exactly the same position and under the same lighting conditions that the other images were taken under; this is a very common technique in AOI because the external conditions are supervised and well-known. The image result for our case is similar to the following image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1057 image-border" src="assets/f3ed20a6-9d23-4926-aba8-d79d7a9a950c.png" style="width:18.17em;height:13.75em;"/></div>
<p>Now, using a simple mathematical operation, we can remove this light pattern. There are two options for removing it:</p>
<ul>
<li>Difference</li>
<li>Division</li>
</ul>
<p>The difference option is the simplest approach. If we have the light pattern <kbd><span class="CodeInTextPACKT">L</span></kbd> and the image picture <span class="CodeInTextPACKT"><kbd>I</kbd>,</span> the resulting removal <kbd><span class="CodeInTextPACKT">R</span></kbd> is the difference between them:</p>
<pre>R= L-I </pre>
<p>This division is a bit more complex, but simple at the same time. If we have the light pattern matrix <kbd><span class="CodeInTextPACKT">L</span></kbd> and the image picture matrix <kbd><span class="CodeInTextPACKT">I</span></kbd>, the result removal <kbd><span class="CodeInTextPACKT">R</span></kbd> is as follows:</p>
<pre>R= 255*(1-(I/L)) </pre>
<p>In this case, we divide the image by the light pattern, and we have the assumption that if our light pattern is white and the objects are darker than the background carrier tape, then the image pixel values are always the same or lower than the light pixel values. The result we obtain from <kbd><span class="CodeInTextPACKT">I/L</span></kbd> is between <kbd>0</kbd> and <kbd>1</kbd>. Finally, we invert the result of this division to get the same color direction range and multiply it by <kbd>255</kbd> to get values within the range of <kbd>0-255</kbd><span>.</span></p>
<p>In our code, we are going to create a new function called <kbd><span class="CodeInTextPACKT">removeLight</span></kbd> with the following parameters:</p>
<ul>
<li>An input image to remove the light/background</li>
<li>A light pattern, <kbd>Mat</kbd></li>
<li>A method, with a <kbd>0</kbd> value for difference and <kbd>1</kbd> for division</li>
</ul>
<p>The result is a new image matrix without light/background. The following code implements the removal of the background through the use of the light pattern:</p>
<pre>Mat removeLight(Mat img, Mat pattern, int method) 
{ 
  Mat aux; 
  // if method is normalization 
  if(method==1) 
  { 
    // Require change our image to 32 float for division 
    Mat img32, pattern32; 
    img.convertTo(img32, CV_32F); 
    pattern.convertTo(pattern32, CV_32F); 
    // Divide the image by the pattern 
    aux= 1-(img32/pattern32); 
    // Convert 8 bits format and scale
    aux.convertTo(aux, CV_8U, 255); 
  }else{ 
    aux= pattern-img; 
  } 
  return aux; 
} </pre>
<p>Let's explore this. After creating the <kbd><span class="CodeInTextPACKT">aux</span></kbd> variable to save the result, we select the method chosen by the user and pass the parameter to the function. If the method that was selected is <kbd>1</kbd>, we apply the division method.</p>
<p>The division method requires a 32-bit float of images to allow us to divide the images and not truncate the numbers into integers. The first step is to convert the image and light pattern mat to floats of 32 bits. To convert images of this format, we can use the <kbd>convertTo</kbd> <span>function</span> <span>of</span> the <kbd>Mat</kbd> <span>class. This function accepts four parameters; the output converted image and the format you wish to convert to the required parameters, but you can define alpha and beta parameters, which allow you to scale and shift the values following the next function, where <em>O</em> is the output image and <em>I</em> the input image:</span></p>
<p><span><span class="mrow"><em><span class="mi">O</span></em><span class="mo">(</span><em><span class="mi">x</span></em><span class="mo">,</span><em><span class="mi">y</span></em><span class="mo">)</span><span class="mo">=<em>cast</em></span><span class="mo">&lt;</span><em><span class="mi">T</span><span class="mi">y</span><span class="mi">p</span><span class="mi">e</span></em><span class="mo">&gt;</span><span class="mo">(</span><span class="mi"><em>α</em> * <em>I</em></span><span class="mo">(</span><em><span class="mi">x</span></em><span class="mo">,</span><em><span class="mi">y</span></em><span class="mo">)</span><span class="mo">+</span><em><span class="mi">β</span></em><span class="mo">)</span></span></span></p>
<p>The following code changes the image to 32-bit float:</p>
<pre>// Required to change our image to 32 float for division 
Mat img32, pattern32; 
img.convertTo(img32, CV_32F); 
pattern.convertTo(pattern32, CV_32F); </pre>
<p>Now, we can carry out the mathematical operations on our matrix as we described, by dividing the image by the pattern and inverting the result:</p>
<pre>// Divide the image by the pattern 
aux= 1-(img32/pattern32); </pre>
<p>Now, we have the result but it is required to return it to an 8-bit depth image, and then use the convert function as we did previously to convert the image's <kbd>mat</kbd> and scale from <kbd>0</kbd> to <kbd>255</kbd> using the alpha parameter:</p>
<pre>// Convert 8 bits format 
aux.convertTo(aux, CV_8U, 255); </pre>
<p>Now, we can return the <kbd><span class="CodeInTextPACKT">aux</span></kbd> variable with the result. For the difference method, the development is very easy because we don't have to convert our images; we only need to apply the difference between the pattern and image and return it. If we don't assume that the pattern is equal to or greater than an image, then we will require a few checks and truncate values that can be less than <kbd>0</kbd> or greater than <kbd>255</kbd>:</p>
<pre>aux= pattern-img; </pre>
<p>The following images are the results of applying the image light pattern to our input image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1058 image-border" src="assets/462bbbc5-adf9-4030-bc5c-58d0e3883a95.png" style="width:36.42em;height:18.33em;"/></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the results that we obtain, we can check how the light gradient <span>and the possible artifacts are removed</span><span>. But what happens when we don't have a light/background pattern? There are a few different techniques to obtain this; we are going to present the most basic one here. Using a filter, we can create one that can be used, but there are better algorithms to learn about the background of images where the pieces appear in different areas. This technique sometimes requires a background estimation image initialization, where our basic approach can play very well. These advanced techniques will be explored in <a href="58a72603-be5a-465f-aa7b-fc8ab1aae596.xhtml">Chapter 8</a>,</span> <em>Video Surveillance, Background Modeling, and Morphological Operations</em><span>. To estimate the background image, we are going to use a blur with a large kernel size applied to our input image. This is a common technique used in</span> <strong>optical character recognition</strong> <em>(</em><strong>OCR</strong><span>), where the letters are thin and small relative to the whole document, allowing us to do an approximation of the light patterns in the image. We can see the light/background pattern reconstruction in the left-hand image and the ground truth in the right-hand:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1059 image-border" src="assets/49a8a85d-84fb-4670-8638-9fe58368fb37.png" style="width:33.17em;height:12.42em;"/></div>
<p>We can see that there are minor differences in the light patterns, but this result is enough to remove the background. We can also see the result in the following image when using different images. In the following image, the result of applying the image difference between the original input image and the estimated background image computed with the previous approach is depicted:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1060 image-border" src="assets/be080517-2a8e-4778-8751-3a22adc85bc5.png" style="width:18.67em;height:11.33em;"/></div>
<p>The <kbd><span class="CodeInTextPACKT">calculateLightPattern</span></kbd> function creates this light pattern or background approximation:</p>
<pre>Mat calculateLightPattern(Mat img) 
{ 
  Mat pattern; 
  // Basic and effective way to calculate the light pattern from one image 
  blur(img, pattern, Size(img.cols/3,img.cols/3)); 
  return pattern; 
} </pre>
<p>This basic function applies a blur to an input image by using a big kernel size relative to the image size. From the code, it is <strong>one-third</strong> of the original width and height.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Thresholding</h1>
                </header>
            
            <article>
                
<p>After removing the background, we only have to binarize the image for future segmentation. We are going to do this with threshold. <kbd>Threshold</kbd> is a simple function that sets each pixel's values to a maximum value (255, for example). If the pixel's value is greater than the <strong>threshold</strong> value or if the pixel's value is lower than the <strong>threshold</strong> value, it will be set to a minimum (0):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d1576ef1-198b-4e9f-ab66-1f109349699f.png" style="width:18.92em;height:3.00em;"/></p>
<p>Now, we are going to apply the <kbd><span class="CodeInTextPACKT">threshold</span></kbd> function using two different <kbd>threshold</kbd> values: we will use a 30 <kbd>threshold</kbd> value when we remove the light/background because all non-interesting regions are black. This is because we apply background removal. We will also a medium value <kbd>threshold</kbd> (140) when we do not use a light removal method, because we have a white background. This last option is used to allow us to check the results with and without background removal:</p>
<pre>  // Binarize image for segment 
  Mat img_thr; 
  if(method_light!=2){ 
   threshold(img_no_light, img_thr, 30, 255, THRESH_BINARY); 
  }else{ 
   threshold(img_no_light, img_thr, 140, 255, THRESH_BINARY_INV); 
  } </pre>
<p>Now, we are going to continue with the most important part of our application: the segmentation. We are going to use two different approaches or algorithms here: connected components and find contours.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Segmenting our input image</h1>
                </header>
            
            <article>
                
<p>Now, we are going to introduce two techniques to segment our threshold image:</p>
<ul>
<li>Connected components</li>
<li>Find contours</li>
</ul>
<p>With these two techniques, we are allowed to extract each <strong>region of interest</strong> (<strong>ROI</strong>) of our image where our targets objects <span>appear. I</span><span>n our case, these are the nut, screw, and ring.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The connected components algorithm</h1>
                </header>
            
            <article>
                
<p>The connected component algorithm is a very common algorithm that's used to segment and identify parts in binary images. The connected component is an iterative algorithm with the purpose of labeling an image using eight or four connectivity pixels. Two pixels are connected if they have the same value and are neighbors. In an image, each pixel has eight neighbor pixels:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1061 image-border" src="assets/b60dde12-2347-4af3-b0a1-063963874bee.png" style="width:13.83em;height:14.00em;"/></div>
<p>Four-connectivity means that only the <strong>2</strong>, <strong>4</strong>, <strong>5</strong>, and <strong>7</strong> neighbors can be connected to the center if they have the same value as the center pixel. With eight-connectivity, the <strong>1</strong>, <strong>2</strong>, <strong>3</strong>, <strong>4</strong>, <strong>5</strong>, <strong>6</strong>, <strong>7</strong>, and <strong>8</strong> neighbors can be connected if they have the same value as the center pixel. <span>We can see the differences in the following example from a four- and eight-connectivity algorithm. We are going to apply each algorithm to the next binarized image. We have used a small <strong>9 x 9</strong> image and zoomed in to show how connected components work and the differences between four- and eight-connectivity:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1062 image-border" src="assets/26a8c0f5-f584-4d8b-b660-12eb8b7261f2.png" style="width:19.25em;height:19.17em;"/></div>
<p>The four-connectivity algorithm detects two objects; we can see this in the left image. The eight-connectivity algorithm detects only one object (the right image) because two diagonal pixels are connected. Eight-connectivity takes care of diagonal connectivity, which is the main difference compared with four-connectivity, since this where only vertical and horizontal pixels are considered. We can see the result in the following image, where each object has a different gray color value:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1063 image-border" src="assets/b40375cf-b26e-4e47-bb95-189154b06334.png" style="width:28.25em;height:15.00em;"/></div>
<p>OpenCV brings us the connected components algorithm with two different functions:</p>
<ul>
<li><kbd>connectedComponents</kbd> (image, labels, connectivity= <kbd>8</kbd>, type= <kbd>CV_32S</kbd>)</li>
<li><kbd>connectedComponentsWithStats</kbd> (image, labels, stats, centroids, connectivity= <kbd>8</kbd>, type= <kbd>CV_32S</kbd>)</li>
</ul>
<p>Both functions return an integer with the number of detected labels, where label <kbd>0</kbd> represents the background. The difference between these two functions is basically the information that is returned. Let's check the parameters of each one. The <kbd><span class="CodeInTextPACKT">connectedComponents</span></kbd> <span>function</span> <span>gives us the following parameters:</span></p>
<ul>
<li><strong>Image</strong>: The input image to be labeled.</li>
<li><strong>Labels</strong>: An output mat that's the same size as the input image, where each pixel has the value of its label, where all OS represents the background, pixels with <kbd>1</kbd> value represent the first connected component object, and so on.</li>
<li><strong>Connectivity</strong>: Two possible values, <kbd>8</kbd> or <kbd>4</kbd>, that represent the connectivity we want to use.</li>
<li><strong>Type</strong>: The type of label image we want to use. Only two types are allowed: <kbd><span class="CodeInTextPACKT">CV32_S</span></kbd> and <kbd><span class="CodeInTextPACKT">CV16_U</span></kbd>. By default, this is <kbd><span class="CodeInTextPACKT">CV32_S</span></kbd><span class="CodeInTextPACKT">.</span></li>
<li>The <kbd><span class="CodeInTextPACKT">connectedComponentsWithStats</span></kbd> function has two more parameters defined. These are <span class="CodeInTextPACKT">stats</span> and <span class="CodeInTextPACKT">centroids</span>:
<ul>
<li><strong>Stats</strong>: This is an output parameter that gives us the following statistical values for each label (background inclusive):
<ul>
<li><kbd><span class="CodeInTextPACKT">CC_STAT_LEFT</span></kbd>: The leftmost <kbd>x</kbd> coordinate of the connected component object</li>
<li><kbd><span class="CodeInTextPACKT">CC_STAT_TOP</span></kbd>: The topmost <kbd>y</kbd> coordinate of the connected component object</li>
<li><kbd><span class="CodeInTextPACKT">CC_STAT_WIDTH</span></kbd>: The width of the connected component object defined by its bounding box</li>
<li><kbd><span class="CodeInTextPACKT">CC_STAT_HEIGHT</span></kbd>: The height of the connected component object defined by its bounding box</li>
<li><kbd><span class="CodeInTextPACKT">CC_STAT_AREA</span></kbd>: The number of pixels (area) of the connected component object</li>
</ul>
</li>
<li><strong>Centroids</strong>: The centroid points to the float type for each label, inclusive of the background that's considered for another connected component.</li>
</ul>
</li>
</ul>
<p>In our example application, we are going to create two functions so that we can apply these two OpenCV algorithms. We will then show the user the obtained result in a new image with colored objects in the basic connected component algorithm. If we select the connected component with the stats method, we are going to draw the respective calculated area that returns this function over each object.</p>
<p>Let's define the basic drawing for the connected component function:</p>
<pre>void ConnectedComponents(Mat img) 
{ 
  // Use connected components to divide our image in multiple connected component objects
     Mat labels; 
     auto num_objects= connectedComponents(img, labels); 
  // Check the number of objects detected 
     if(num_objects &lt; 2 ){ 
        cout &lt;&lt; "No objects detected" &lt;&lt; endl; 
        return; 
      }else{ 
       cout &lt;&lt; "Number of objects detected: " &lt;&lt; num_objects - 1 &lt;&lt; endl; 
      } 
  // Create output image coloring the objects 
     Mat output= Mat::zeros(img.rows,img.cols, CV_8UC3); 
     RNG rng(0xFFFFFFFF); 
     for(auto i=1; i&lt;num_objects; i++){ 
        Mat mask= labels==i; 
        output.setTo(randomColor(rng), mask); 
      } 
     imshow("Result", output); 
} </pre>
<p>First of all, we call the OpenCV <kbd>connectedComponents</kbd> function, which returns the number of objects detected. If the number of objects is less than two, this means that only the background object is detected, and then we don't need to draw anything and we can finish. If the algorithm detects more than one object, we show the number of objects that have been detected on the console:</p>
<pre>  Mat labels; 
  auto num_objects= connectedComponents(img, labels); 
  // Check the number of objects detected 
  if(num_objects &lt; 2){ 
    cout &lt;&lt; "No objects detected" &lt;&lt; endl; 
    return; 
  }else{ 
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; num_objects - 1 &lt;&lt; endl;</pre>
<p class="mce-root"/>
<p>Now, we are going to draw all detected objects in a new image with different colors. After this, we need to create a new black image with the same input size and three channels:</p>
<pre>Mat output= Mat::zeros(img.rows,img.cols, CV_8UC3); </pre>
<p>We will loop over each label, except for the <kbd>0</kbd> value, because this is the background:</p>
<pre>for(int i=1; i&lt;num_objects; i++){ </pre>
<p>To extract each object from the label image, we can create a mask for each <kbd><span class="CodeInTextPACKT">i</span></kbd> <span>label</span> <span>using a comparison and save this in a new image:</span></p>
<pre>    Mat mask= labels==i; </pre>
<p>Finally, we set a pseudo-random color <span>to</span> <span>the output image using the</span> <kbd>mask</kbd><span>:</span></p>
<pre>    output.setTo(randomColor(rng), mask); 
  } </pre>
<p>After looping all of the images, we have all of the detected objects with different colors <span>in our output</span> <span>and we only have to show the output image in a window:</span></p>
<pre>imshow("Result", output); </pre>
<p>This is the result in which each object is painted with different colors or a gray value:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1064 image-border" src="assets/c1ba4e5a-e7ba-4e3e-b50f-1feb158a6dab.png" style="width:24.08em;height:14.25em;"/></div>
<p>Now, we are going to explain how to use the connected components with the <kbd>stats</kbd> OpenCV algorithm and show some more information in the resultant image. The following function implements this functionality:</p>
<pre>void ConnectedComponentsStats(Mat img) 
{ 
  // Use connected components with stats 
  Mat labels, stats, centroids; 
  auto num_objects= connectedComponentsWithStats(img, labels, stats, centroids); 
  // Check the number of objects detected 
  if(num_objects &lt; 2 ){ 
    cout &lt;&lt; "No objects detected" &lt;&lt; endl; 
    return; 
  }else{ 
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; num_objects - 1 &lt;&lt; endl; 
  } 
  // Create output image coloring the objects and show area 
  Mat output= Mat::zeros(img.rows,img.cols, CV_8UC3); 
  RNG rng( 0xFFFFFFFF ); 
  for(auto i=1; i&lt;num_objects; i++){ 
    cout &lt;&lt; "Object "&lt;&lt; i &lt;&lt; " with pos: " &lt;&lt; centroids.at&lt;Point2d&gt;(i) &lt;&lt; " with area " &lt;&lt; stats.at&lt;int&gt;(i, CC_STAT_AREA) &lt;&lt; endl; 
    Mat mask= labels==i; 
    output.setTo(randomColor(rng), mask); 
    // draw text with area 
    stringstream ss; 
    ss &lt;&lt; "area: " &lt;&lt; stats.at&lt;int&gt;(i, CC_STAT_AREA); 
 
    putText(output,  
      ss.str(),  
      centroids.at&lt;Point2d&gt;(i),  
      FONT_HERSHEY_SIMPLEX,  
      0.4,  
      Scalar(255,255,255)); 
  } 
  imshow("Result", output); 
} </pre>
<p>Let's understand this code. As we did in the non-stats function, we call the connected components algorithm, but here, we do this using the <kbd>stats</kbd> function, checking whether we detected more than one object:</p>
<pre>Mat labels, stats, centroids; 
  auto num_objects= connectedComponentsWithStats(img, labels, stats, centroids); 
  // Check the number of objects detected 
  if(num_objects &lt; 2){ 
    cout &lt;&lt; "No objects detected" &lt;&lt; endl; 
    return; 
  }else{ 
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; num_objects - 1 &lt;&lt; endl; 
  }</pre>
<p>Now, we have two more output results: the stats and centroid variables. Then, for each detected <span>label,</span> <span>we are going to show</span> <span>the centroid and area</span> <span>through the command line:</span></p>
<pre>for(auto i=1; i&lt;num_objects; i++){ 
    cout &lt;&lt; "Object "&lt;&lt; i &lt;&lt; " with pos: " &lt;&lt; centroids.at&lt;Point2d&gt;(i) &lt;&lt; " with area " &lt;&lt; stats.at&lt;int&gt;(i, CC_STAT_AREA) &lt;&lt; endl; </pre>
<p>You can check the call to the <span class="CodeInTextPACKT">stats</span> variable to extract the area using the column constant <kbd><span class="CodeInTextPACKT">stats.at&lt;int&gt;(I, CC_STAT_AREA)</span></kbd>. Now, like before, we paint the object labeled with <kbd><span class="CodeInTextPACKT">i</span></kbd> <span>over the output image:</span></p>
<pre>Mat mask= labels==i; 
output.setTo(randomColor(rng), mask); </pre>
<p>Finally, <span>in the centroid position of each segmented object,</span> we want to draw some information (such as the area) <span>on the resultant image</span>. <span>To do this, we use the</span> <span class="CodeInTextPACKT">stats</span> <span>and</span> <span class="CodeInTextPACKT">centroid</span> <span>variables using the</span> <kbd><span class="CodeInTextPACKT">putText</span></kbd> <span>function. First, we have to create a</span> <kbd><span class="CodeInTextPACKT">stringstream</span></kbd> <span>so that we can add the stats area information:</span></p>
<pre>// draw text with area 
stringstream ss; 
ss &lt;&lt; "area: " &lt;&lt; stats.at&lt;int&gt;(i, CC_STAT_AREA); </pre>
<p>Then, we need to use <kbd><span class="CodeInTextPACKT">putText</span></kbd>, using the centroid as the text position:</p>
<pre>putText(output,  
  ss.str(),  
  centroids.at&lt;Point2d&gt;(i),  
  FONT_HERSHEY_SIMPLEX,  
  0.4,  
  Scalar(255,255,255)); </pre>
<p>The result for this function is as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1065 image-border" src="assets/10e79360-47ac-4ae7-83c2-67547df40016.png" style="width:20.50em;height:11.00em;"/></div>
<div class="packt_figure"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The findContours algorithm</h1>
                </header>
            
            <article>
                
<p>The <kbd>findContours</kbd> algorithm is one of the most used OpenCV algorithms in regards to segment objects. This is because this algorithm was included in OpenCV from version 1.0 and gives developers more information and descriptors, including shapes, topological organizations, and so on:</p>
<pre>void findContours(InputOutputArray image, OutputArrayOfArrays contours, OutputArray hierarchy, int mode, int method, Point offset=Point()) </pre>
<p>Let's explain each parameter:</p>
<ul>
<li><strong>Image</strong>: Input binary image.</li>
<li><strong>Contours</strong>: A contour's output where each detected contour is a vector of points.</li>
<li><strong>Hierarchy</strong>: This is the optional output vector where the hierarchy of contours is saved. This is the topology of the image where we can get the relations between each contour. The hierarchy is represented as a vector of four indices, which are (next contour, previous contour, first child, parent contour). Negative indices are given where the given contour has no relationship with other contours. A more detailed explanation can be found at <a href="https://docs.opencv.org/3.4/d9/d8b/tutorial_py_contours_hierarchy.html">https://docs.opencv.org/3.4/d9/d8b/tutorial_py_contours_hierarchy.html</a>.</li>
<li><strong>Mode</strong>: This method is used to retrieve the contours:
<ul>
<li><kbd><span class="CodeInTextPACKT">RETR_EXTERNAL</span></kbd> retrieves only the external contours.</li>
<li><kbd><span class="CodeInTextPACKT">RETR_LIST</span></kbd> retrieves all contours without establishing the hierarchy.</li>
<li><kbd><span class="CodeInTextPACKT">RETR_CCOMP</span></kbd> retrieves all contours with two levels of hierarchy, external and holes. If another object is inside one hole, this is put at the top of the hierarchy.</li>
<li><kbd><span class="CodeInTextPACKT">RETR_TREE</span></kbd> retrieves all contours, creating a full hierarchy between contours.</li>
</ul>
</li>
<li><strong>Method</strong>: This allows us to use the approximation method for retrieving the contour's shapes:
<ul>
<li>If <kbd>CV_CHAIN_APPROX_NONE</kbd> is set, then this does not apply any approximation to the contours and stores the contour's points.</li>
<li><kbd><span class="CodeInTextPACKT">CV_CHAIN_APPROX_SIMPLE</span></kbd> compresses all horizontal, vertical, and diagonal segments, storing only the start and end points.</li>
<li><span class="CodeInTextPACKT"><kbd>CV_CHAIN_APPROX_TC89_L1</kbd> and <kbd>CV_CHAIN_APPROX_TC89_KCOS</kbd></span> apply the <strong>Telchin</strong> <strong>chain</strong> <strong>approximation</strong> algorithm.</li>
</ul>
</li>
<li><strong>Offset</strong>: This is an optional point value to shift all contours. This is very useful when we are working in an ROI and need to retrieve global positions.</li>
</ul>
<div class="packt_infobox">Note: The input image is modified by the <kbd>findContours</kbd> function. Create a copy of your image before sending it to this function if you need it.</div>
<p>Now that we know the parameters of the <kbd>findContours</kbd> function, let's apply this to our example:</p>
<pre>void FindContoursBasic(Mat img) 
{ 
  vector&lt;vector&lt;Point&gt; &gt; contours; 
  findContours(img, contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE); 
  Mat output= Mat::zeros(img.rows,img.cols, CV_8UC3); 
  // Check the number of objects detected 
  if(contours.size() == 0 ){ 
    cout &lt;&lt; "No objects detected" &lt;&lt; endl; 
    return; 
  }else{ 
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; contours.size() &lt;&lt; endl; 
  } 
  RNG rng(0xFFFFFFFF); 
  for(auto i=0; i&lt;contours.size(); i++){ 
    drawContours(output, contours, i, randomColor(rng)); 
    imshow("Result", output); 
  }<br/>} <br/><br/></pre>
<p>Let's explain our implementation, <span>line by line</span><span>.</span></p>
<p>In our case, we don't need any hierarchy, so we are only going to retrieve the external contours of all possible objects. To do this, we can use the <kbd><span class="CodeInTextPACKT">RETR_EXTERNAL</span></kbd> mode and basic contour encoding by using the <kbd><span class="CodeInTextPACKT">CHAIN_APPROX_SIMPLE</span></kbd> method:</p>
<pre>vector&lt;vector&lt;Point&gt; &gt; contours; 
vector&lt;Vec4i&gt; hierarchy; 
findContours(img, contours, RETR_EXTERNAL, CHAIN_APPROX_SIMPLE); </pre>
<p>Like the connected component examples we looked at before, first we check how many contours we have retrieved. If there are none, then we exit our function:</p>
<pre>// Check the number of objects detected 
  if(contours.size() == 0){ 
    cout &lt;&lt; "No objects detected" &lt;&lt; endl; 
    return; 
  }else{ 
    cout &lt;&lt; "Number of objects detected: " &lt;&lt; contours.size() &lt;&lt; endl; 
  }</pre>
<p>Finally, we draw the <span>contour for</span> <span>each detected object. We draw this in our output image with different colors. To do this, OpenCV gives us a function to draw the result of the find contours image:</span></p>
<pre>for(auto i=0; i&lt;contours.size(); i++) 
    drawContours(output, contours, i, randomColor(rng)); 
  imshow("Result", output); 
} </pre>
<p>The <kbd><span class="CodeInTextPACKT">drawContours</span></kbd> function allows the following parameters:</p>
<ul>
<li><strong>Image</strong>: The output image to draw the contours.</li>
<li><strong>Contours</strong>: The vector of contours.</li>
<li><strong>Contour index</strong>: A number indicating the contour to draw. If this is negative, all contours are drawn.</li>
<li><strong>Color</strong>: The color to draw the contour.</li>
<li><strong>Thickness</strong>: If it is negative, the contour is filled with the chosen color.</li>
<li><strong>Line type</strong>: This specifies whether we want to draw with anti-aliasing or another drawing method.</li>
<li><strong>Hierarchy</strong>: This is an optional parameter that is only needed if you want to draw some of the contours.</li>
<li><strong>Max Level</strong>: This is an optional parameter that is only taken into account when the hierarchy parameter is available. If it is set to <kbd>0</kbd>, only the specified contour is drawn. If it is <kbd>1</kbd>, the function draws the current contour and the nested contours too. If it is set to <kbd>2</kbd>, then the algorithm draws all of the specified contour hierarchy.</li>
<li><strong>Offset</strong>: This is an optional parameter for shifting the contours.</li>
</ul>
<p class="CDPAlignLeft CDPAlign">The result of our example can be seen in the following image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1066 image-border" src="assets/eea4536f-0250-4228-8f2f-27ddc44bf863.png" style="width:19.42em;height:14.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we explored the basics of object segmentation in a controlled situation where a camera takes pictures of different objects. Here, we learned how to remove background and light to allow us to binarize our image <span>better, thus</span> minimizing the noise. After binarizing the image, we learned about three different algorithms that we can use to divide and separate each object of one image, allowing us to isolate each object to manipulate or extract features.</p>
<p>We can see this whole process in the following image:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1067 image-border" src="assets/617ededa-c684-4f4c-b460-2d1fe8d99849.png" style="width:73.17em;height:42.42em;"/></div>
<p>Finally, we extracted all of the objects on an image. You will need to do this to continue with the next chapter, where we are going to extract characteristics of each of these objects to train a machine learning system.</p>
<p>In the next chapter, we are going to predict the class of any objects in an image and then call a robot or any other system to pick any of them, or detect an object that is not in the correct carrier tape. We will then look at notifying a person to pick <span>it</span> up.</p>


            </article>

            
        </section>
    </body></html>