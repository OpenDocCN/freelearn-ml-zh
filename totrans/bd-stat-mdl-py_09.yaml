- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Discriminant Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别分析
- en: In the previous chapter, we discussed discrete regression models, including
    classification using logistic regression. In this chapter, we will begin with
    an overview of probability, expanding into conditional and independent probability.
    We then discuss how these two approaches to understanding the laws of probability
    form the basis for Bayes’ Theorem, which is used directly to expand an approach
    called Bayesian statistics. Following this topic, we dive into **Linear Discriminant
    Analysis** (**LDA**) and **Quadratic Discriminant Analysis** (**QDA**), two powerful
    classifiers that model data using the Bayesian approach to probability modeling.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了离散回归模型，包括使用逻辑回归的分类。在本章中，我们将从概率概述开始，扩展到条件概率和独立概率。然后，我们将讨论这两种理解概率定律的方法如何构成贝叶斯定理的基础，贝叶斯定理直接用于扩展称为贝叶斯统计的方法。在此主题之后，我们将深入研究**线性判别分析（LDA**）和**二次判别分析（QDA**），这两种强大的分类器使用贝叶斯概率建模方法来建模数据。
- en: 'In this chapter we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Bayes’ Theorem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: LDA
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA
- en: QDA
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QDA
- en: Bayes’ theorem
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理
- en: In this section, we will discuss **Bayes’ Theorem**, which is used in the classification
    models described later in this chapter. We will start the chapter by discussing
    the basics of probability. Then, we will take a look at dependent events and discuss
    how Bayes’ Theorem is related to dependent events.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论**贝叶斯定理**，该定理用于本章后面描述的分类模型。我们将从讨论概率的基本知识开始本章。然后，我们将探讨相关事件，并讨论贝叶斯定理如何与相关事件相关联。
- en: Probability
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率
- en: 'Probability is a measurement of the likelihood that an event occurs or a certain
    outcome occurs. Generally, we can group events into two types of events: **independent
    events** and **dependent events**. The distinction between the types of events
    is in the name. An independent event is an event that is not affected or influenced
    by the occurrences of other events, while a dependent event is affected or influenced
    by the occurrences of other events.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 概率是衡量事件发生或特定结果发生的可能性的一种度量。通常，我们可以将事件分为两种类型的事件：**独立事件**和**相关事件**。事件类型之间的区别在于名称。独立事件是指不受其他事件发生或影响的事件，而相关事件则受其他事件发生或影响。
- en: 'Let’s think about some examples of these events. For the first example, think
    about a fair coin toss. A coin toss can result in one of two states: heads and
    tails. If the coin is fair, there is a one-in-two chance that the toss will result
    in heads or tails. That means that the probability of heads or tails is 0.5 in
    a fair coin toss. We can calculate probability using the following formula when
    events are countable:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些这些事件的例子。对于第一个例子，考虑一个公平的硬币抛掷。抛掷硬币可以导致两种状态之一：正面和反面。如果硬币是公平的，抛掷硬币得到正面或反面的概率是二分之一。这意味着在公平的硬币抛掷中，正面或反面的概率是0.5。当事件可计数时，我们可以使用以下公式来计算概率：
- en: P(E) =  Count of Desired Outcomes  __________________  Total Number of Outcomes
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: P(E) = 期望结果的数量 / 总结果数量
- en: Here, P(E) is the probability of the desired outcome. In the coin-toss example,
    the desired outcome is either heads or tails, each of which can only occur once
    per coin toss. Thus, for the coin toss, the count of desired events is one. The
    total number of outcomes is two because there are two possible states from a coin
    toss. Putting these numbers in the preceding equation yields 0.5 for heads and
    0.5 for tails. Now, putting your experience to work, does a previous coin toss
    give you any knowledge about the next toss of the same coin? It does not because
    *each coin toss is independent of every other coin toss*; it is an independent
    event.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，P(E)是期望结果发生的概率。在抛硬币的例子中，期望结果是正面或反面，每个结果在每次抛掷中只能发生一次。因此，对于抛硬币，期望事件的数量是一个。总结果的数量是两个，因为抛硬币有两种可能的状态。将这些数字代入前面的方程，得到正面和反面的概率都是0.5。现在，运用你的经验，之前的抛硬币是否给你关于下一次抛掷同一硬币的任何知识？它不会，因为*每次抛掷都是相互独立的*；它是一个独立事件。
- en: 'Now, let’s consider another example. Say we have a bag with three red marbles
    and five blue marbles. Let’s calculate the probability of selecting a red marble
    from the bag: P(Red). There are eight total marbles, which is the total number
    of outcomes in the equation. We want to select a red marble, which means the count
    of desired outcomes is three. This means that the probability of selecting red
    marble from the bag is 3 / 8\. Now, let’s calculate the probability of selecting
    a blue marble from the bag: P(Blue). There are five blue marbles and eight total
    marbles, so the probability of selecting a blue marble from the bag is 5 / 8\.
    Notice that the sum of the two probabilities is one: P(Red) + P(Blue) = 3 / 8
    + 5 / 8 = 1\. This brings up an important property of probabilities: the total
    probability must be equal to one. This, of course, means that once we calculated
    the probability of selecting a red marble, we could have calculated the probability
    of selecting a blue marble by subtracting the probability of selecting a red marble
    from one. The property of probabilities is very useful and is used commonly in
    statistics and **machine** **learning** (**ML**).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑另一个例子。假设我们有一个装有三个红弹珠和五个蓝弹珠的袋子。让我们计算从袋子中抽取红弹珠的概率：P(红)。总共有八个弹珠，这是方程中的总结果数。我们想要抽取一个红弹珠，这意味着期望结果的数量是三个。这意味着从袋子中抽取红弹珠的概率是
    3 / 8。现在，让我们计算从袋子中抽取蓝弹珠的概率：P(蓝)。袋中有五个蓝弹珠和八个总弹珠，所以从袋子中抽取蓝弹珠的概率是 5 / 8。注意，这两个概率的和是一：P(红)
    + P(蓝) = 3 / 8 + 5 / 8 = 1。这提出了概率的一个重要特性：总概率必须等于一。当然，这意味着一旦我们计算了抽取红弹珠的概率，我们就可以通过从一中减去抽取红弹珠的概率来计算抽取蓝弹珠的概率。概率的特性非常有用，并且在统计学和**机器学习**（**ML**）中经常使用。
- en: 'Coming back to the example with red and blue marbles, let’s think about the
    type of event represented by this example. We select a marble from the bag. Now,
    there are two possible options next: (1) we replace the selected marble and select
    a marble from the bag again, and (2) we do not replace the marble and select a
    new marble from the remaining seven marbles in the bag. The first case is called
    **selection with replacement**. Selection with replacement is an independent event
    because the bag is reset before each draw from the bag. The second case is called
    **selection without replacement**. Selection without replacement is where you
    find dependent events. The first time you select a marble, the probability is
    the same as described previously (an independent event). However, if the marble
    is not replaced, the state of the bag has changed, and the probability of the
    next marble selection depends on which one was selected from the bag in the previous
    draw. Then, in selection without replacement, each subsequent marble draw is a
    dependent event. When discussing dependent events, it is useful to describe events
    using **conditional probability**. Let’s discuss conditional probability now.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 回到红蓝弹珠的例子，让我们思考这个例子所代表的事件类型。我们从袋子中抽取一个弹珠。现在，有两个可能的选择：1）我们替换所抽取的弹珠，然后再次从袋子中抽取一个弹珠，2）我们不替换弹珠，然后从袋子中剩余的七个弹珠中抽取一个新的弹珠。第一种情况称为**带替换的选择**。带替换的选择是一个独立事件，因为每次从袋子中抽取之前袋子都会被重置。第二种情况称为**不带替换的选择**。不带替换的选择是相关事件所在的地方。第一次抽取弹珠的概率与之前描述的相同（一个独立事件）。然而，如果弹珠没有被替换，袋子的状态已经改变，下一次弹珠抽取的概率取决于上一次抽取的是哪一个。然后，在不带替换的选择中，每次后续的弹珠抽取都是一个相关事件。在讨论相关事件时，使用**条件概率**来描述事件是有用的。现在，让我们来讨论条件概率。
- en: Conditional probability
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 条件概率
- en: Let’s start with a formal definition. When we have dependent events, we may
    be interested in the probability of Event B given Event A, which means we want
    to know the probability of Event B after Event A occurred. This is called the
    conditional probability of Event B given Event A and is denoted P(B | A). This
    can be thought of as “the probability of Event B on the condition that Event A
    has already occurred.” This may feel a bit vague, so let’s look at conditional
    probability in terms of our last example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从正式的定义开始。当我们有相关事件时，我们可能对在事件A发生后事件B的概率感兴趣，这意味着我们想知道事件A发生后事件B的概率。这被称为事件A给定事件B的条件概率，表示为
    P(B | A)。这可以理解为“在事件A已经发生的情况下事件B的概率。”这可能有点模糊，所以让我们从我们上一个例子来探讨条件概率。
- en: 'Let’s say Event A is drawn a red marble and Event B is drawn a blue marble.
    We want to find the conditional probability of drawing a blue marble given that
    we have already drawn a red marble and can denote this as P(Blue | Red). Let’s
    work through this example. After drawing the first red marble from the bag, there
    are seven marbles remaining: two red marbles and five blue marbles. We calculate
    the probability of selecting a blue marble from the new set using the same equation,
    which gives us the probability of 5 / 7\. This means that the conditional probability
    of drawing a blue marble, given this a red marble was already drawn, is 5 / 7.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设事件 A 是抽出一个红弹珠，事件 B 是抽出一个蓝弹珠。我们想要找到在已经抽出一个红弹珠的条件下抽出一个蓝弹珠的条件概率，我们可以表示为 P(蓝 |
    红)。让我们通过这个例子来解决这个问题。在从袋子里抽出一个红弹珠后，剩下七个弹珠：两个红弹珠和五个蓝弹珠。我们使用相同的方程来计算从新的集合中选择蓝弹珠的概率，这给我们带来了
    5/7 的概率。这意味着在已经抽出一个红弹珠的条件下抽出一个蓝弹珠的条件概率是 5/7。
- en: 'Now that we have discussed conditional probability, let’s look at the probability
    of making two draws from the bag. Formally, we want to calculate the probability
    of Event A and Event B. Continuing to follow our example, let’s calculate the
    probability of drawing a red marble and then drawing a blue marble. We can describe
    the probability of two sequential events with the following equation:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了条件概率，让我们来看看从袋子里抽取两次的概率。正式地说，我们想要计算事件 A 和事件 B 的概率。继续跟随我们的例子，让我们计算先抽出一个红弹珠然后抽出一个蓝弹珠的概率。我们可以用以下方程来描述两个连续事件的概率：
- en: P(A and B) = P(A)* P(B | A)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: P(A 和 B) = P(A) * P(B | A)
- en: 'In words, the probability of A followed by B is the probability of Event A
    times the probability of Event B given Event A. In our example, we have calculated
    P(Red) and P(Blue | Red). We can use those values to calculate the probability
    of drawing a red marble and then a blue marble P(Red and Blue), as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 用话来说，A 后跟 B 的概率是事件 A 的概率乘以在事件 A 发生的条件下事件 B 的概率。在我们的例子中，我们已经计算了 P(红) 和 P(蓝 |
    红)。我们可以使用这些值来计算先抽出一个红弹珠然后抽出一个蓝弹珠的概率 P(红和蓝)，如下所示：
- en: P(Red and Blue) = P(Red)* P(Blue | Red) =  3 _ 8 * 5 _ 7  =  15 _ 56  ≅ 0.27
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: P(红和蓝) = P(红) * P(蓝 | 红) = 3/8 * 5/7 = 15/56 ≅ 0.27
- en: Performing the calculation, we find that P(Red and Blue) is about 0.27.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 进行计算后，我们发现 P(红和蓝) 大约是 0.27。
- en: 'Now, out of interest, let’s make the calculations again, but *exchanging the
    order of the colors*; the probability of drawing a blue marble and then drawing
    a red marble or P(Blue and Red). Following the same logic as shown in the previous
    example, we will find that P(Blue) = 5 / 8 and P(Red | Blue) = 3 / 7\. Thus, the
    equation looks like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，出于好奇，让我们再次进行计算，但*交换颜色的顺序*；先抽出一个蓝弹珠然后抽出一个红弹珠的概率或 P(蓝和红)。遵循与上一个例子中相同的逻辑，我们将发现
    P(蓝) = 5/8 和 P(红 | 蓝) = 3/7。因此，方程看起来是这样的：
- en: P(Blue and Red) = P(Blue)* P(Red | Blue) =  5 _ 8 * 3 _ 7  =  15 _ 56  ≅ 0.27
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: P(蓝和红) = P(蓝) * P(红 | 蓝) = 5/8 * 3/7 = 15/56 ≅ 0.27
- en: Notice that P(Red and Blue) and P(Blue and Red) have the same probability. This
    is no coincidence. This is known as Bayes’ Theorem, which we will discuss next.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 P(红和蓝) 和 P(蓝和红) 具有相同的概率。这并非巧合。这被称为贝叶斯定理，我们将在下一节讨论。
- en: Discussing Bayes’ Theorem
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论贝叶斯定理
- en: In the previous section on conditional probability, we happened across Bayes’
    Theorem in the example of drawing marbles from a bag. Let’s discuss Bayes’ Theorem
    in more detail.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节关于条件概率的部分，我们在从袋子里抽弹珠的例子中偶然遇到了贝叶斯定理。让我们更详细地讨论贝叶斯定理。
- en: 'Bayes’ Theorem is a basis of a field of statistics known as **Bayesian statistics**.
    The central idea of Bayesian statistics is that given a prior probability, we
    have used new information to update probability estimates. While a full description
    of Bayesian statistics is beyond the scope of this book, we will discuss Bayes’
    Theorem as it relates to the concepts discussed in this book. We generally describe
    Bayes’ Theorem with the following equation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理是统计学中一个称为**贝叶斯统计**的领域的基石。贝叶斯统计的核心思想是，给定一个先验概率，我们使用新信息来更新概率估计。虽然贝叶斯统计的完整描述超出了本书的范围，但我们将讨论贝叶斯定理与本书中讨论的概念的相关性。我们通常用以下方程来描述贝叶斯定理：
- en: P(B|A) =  P(A|B)P(B) _ p(A)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: P(B|A) = P(A|B) * P(B) / P(A)
- en: In Bayesian terminology, P(B) is called the **prior** and P(B | A) is called
    the **posterior**. The posterior gives us an update given another event. Let’s
    make use of the Theorem in a real-world example.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在贝叶斯术语中，P(B)被称为**先验**，P(B | A)被称为**后验**。后验给出了给定另一个事件时的更新。让我们用一个现实世界的例子来利用这个定理。
- en: 'Let’s say that we want to determine whether someone has a disease based on
    the results of a test. We would present this in Bayes’ Theorem as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要根据测试结果来确定某人是否患有疾病。我们可以在贝叶斯定理中这样表示：
- en: P(sick|test pos) =  P(test pos|sick)P(sick)  ________________  P(test pos)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: P(sick|test pos) = P(test pos|sick)P(sick) __________________ P(test pos)
- en: Here, P(sick) is the probability of having the disease, P(test pos) is the probability
    of getting a positive test result, and P(test pos|sick) is the conditional probability
    of a positive test result when the patient tested is sick (also known as the accuracy
    of the test).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，P(sick)是患病的概率，P(test pos)是获得阳性测试结果的概率，P(test pos|sick)是当测试的患者患病时的阳性测试结果的条件概率（也称为测试的准确度）。
- en: 'For this example, we will assume the probability of having the disease is 0.2
    and the test has an accuracy of 0.95 and a false positive rate of 0.3\. The prior
    probability in this example is P(sick)=0.2\. We can improve our knowledge of whether
    someone has the disease with the test. We want to find the posterior: P(sick|test pos).'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将假设患病的概率为0.2，测试的准确度为0.95，假阳性率为0.3。在这个例子中，先验概率P(sick)=0.2。我们可以通过测试来提高我们对某人是否患有疾病的认识。我们想要找到后验概率：P(sick|test pos)。
- en: 'Since we know the accuracy of the test is 0.95 and the prior, we already know
    the values of the terms in the nominator of the equation. To find P(test pos),
    we need to consider all cases where we can get a positive test result, which could
    be a true positive or a false positive. We calculate the probability of the positive
    test result as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道测试的准确度为0.95，并且已知先验概率，我们已知方程分子中各项的值。为了找到P(test pos)，我们需要考虑所有可能获得阳性测试结果的情况，这可能是真阳性或假阳性。我们如下计算阳性测试结果的概率：
- en: P(test pos) = P(test pos|sick)P(sick) + P(test pos|not sick)P(not sick)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: P(test pos) = P(test pos|sick)P(sick) + P(test pos|not sick)P(not sick)
- en: 'Recall that the total probability of possible states must equal one: P(not sick)
    = 1 − P(sick) = 0.8\. With this in mind, we can calculate P(sick|test pos):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，可能状态的总概率必须等于1：P(not sick) = 1 − P(sick) = 0.8。考虑到这一点，我们可以计算P(sick|test pos)：
- en: P(sick|test pos) =  0.95(0.2)  ______________  0.95(0.2) + 0.3(0.8)  ≅ 0.44
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: P(sick|test pos) = 0.95(0.2) ______________ 0.95(0.2) + 0.3(0.8) ≅ 0.44
- en: 'Given one test result, we see that the likeliness of the patient being sick
    has increased from 0.2 to 0.44 with the additional information. 0.44 is not a
    particularly convincing result. This is due to the relatively high false positive
    rate of 0.3, even though the test has high accuracy. If we are not convinced,
    we can run the test again using 0.44 as the new prior. Assuming the second test
    is positive, we would get the following result:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个测试结果，我们看到在额外信息的情况下，患者生病的可能性从0.2增加到0.44。0.44不是一个特别令人信服的结果。这主要是因为测试的假阳性率相对较高，为0.3，尽管测试的准确度很高。如果我们还不信服，我们可以再次使用0.44作为新的先验概率来运行测试。假设第二次测试结果为阳性，我们会得到以下结果：
- en: P(sick|test pos) =  0.95(0.441)  ____________________   0.95(0.441) + 0.3(1
    − 0.441)  ≅ 0.71
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: P(sick|test pos) = 0.95(0.441) __________________ 0.95(0.441) + 0.3(1 − 0.441)
    ≅ 0.71
- en: A second positive test provides a more convincing result of 0.71\. We could
    continue to iteratively improve the probability estimation that a patient is sick
    by incorporating the results from additional tests. This incremental improvement
    of probability estimation is the basic idea of Bayesian statistics.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次阳性测试提供了一个更有说服力的结果0.71。我们可以继续迭代地通过结合额外的测试结果来改进患者生病的概率估计。这种概率估计的增量改进是贝叶斯统计的基本思想。
- en: This section contained an introduction to probability. We covered independent
    and dependent events and then provided more details on dependent events, which
    led to the introduction of Bayes’ Theorem. In the next section, we will discuss
    a type of classification model that uses Bayes’ Theorem.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含了对概率的介绍。我们涵盖了独立事件和依赖事件，然后提供了关于依赖事件的更多细节，这导致了贝叶斯定理的引入。在下一节中，我们将讨论一种使用贝叶斯定理的分类模型。
- en: Linear Discriminant Analysis
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性判别分析
- en: In the previous chapter, we discussed logistic regression as a classification
    model leveraging linear regression to model directly the probability of a target
    distribution given an input distribution. One alternative to this approach is
    LDA. LDA models the probability of target distribution class memberships given
    input variable distributions corresponding to each class using decision boundaries
    constructed using Bayes’ Theorem, which we discussed previously. Where we have
    *k* classes, using Bayes’ Theorem, we have the probability density function for
    LDA class membership simply as P(Y = k|X = x) for any discrete random variable,
    *X*. This relies on the posterior probability that an observation *x* in variable
    *X* belongs to the *k*th class.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了逻辑回归作为一种分类模型，它利用线性回归直接对给定输入分布的目标分布概率进行建模。这种方法的另一种选择是LDA。LDA使用贝叶斯定理构建的决策边界来建模给定每个类别的输入变量分布的目标分布类成员概率。在我们有*k*个类别的情况下，使用贝叶斯定理，LDA类成员的概率密度函数简单地为P(Y
    = k|X = x)，对于任何离散随机变量*X*。这依赖于变量*X*中观察值*x*属于第*k*个类别的后验概率。
- en: 'Before proceeding, we must first make note that LDA makes three pertinent assumptions:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们必须首先注意LDA做出三个相关的假设：
- en: Each input variable is normally distributed.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个输入变量都是正态分布的。
- en: Across all target classes, there is **equal covariance** among the predictors.
    In other words, the shared variance of all input variables is uniform. For this
    reason, it is useful to **standardize each input variable** to have a mean of
    0 and **scale to unit variance** (standard deviation of 1).
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有目标类别之间，预测变量之间存在**相同的协方差**。换句话说，所有输入变量的共享方差是均匀的。因此，将每个输入变量**标准化**以具有0的均值和**缩放到单位方差**（标准差为1）是有用的。
- en: Samples are assumed to be independent; **random sampling** is highly important
    to avoid complications resulting from **serial or** **cluster effects**.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设样本是独立的；**随机抽样**对于避免**序列或****集群效应**带来的复杂情况至关重要。
- en: 'With these assumptions met, we can make the assumption that the class probability
    density function is Gaussian (normal), and thus, in a one-dimensional form (one
    input variable), we have the following density function:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在满足这些假设的情况下，我们可以假设类概率密度函数是高斯（正态）分布的，因此，在一维形式（一个输入变量）中，我们有以下密度函数：
- en: 1 _ √ _ 2π  σ k  e  −1 _ 2σ k 2(x−μ k) 2
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 1 _ √ _ 2π  σ k  e  −1 _ 2σ k 2(x−μ k) 2
- en: 'Here, μ k is the *k*th class mean and σ k 2 is the *k*th class variance. In
    the multivariate case, the normal probability density function is this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，μ k 是第*k*个类别的均值，σ k 2 是第*k*个类别的方差。在多变量情况下，正态概率密度函数如下：
- en: 1 _  2π p/2 √ _ |D|   e −1 _ 2 (x−μ) TD −1(x−μ)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 1 _  2π p/2 √ _ |D|   e −1 _ 2 (x−μ) TD −1(x−μ)
- en: 'Here, *D* is the covariance matrix of the input variables, *p* is the number
    of input variables (or parameters), and *π* is the prior probability. The univariate
    calculation of the posterior probability is shown here (where *k* corresponds
    to the *k*th class and *K* corresponds to the number of classes):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*D*是输入变量的协方差矩阵，*p*是输入变量的数量（或参数），*π*是先验概率。这里展示了后验概率的单变量计算（其中*k*对应于第*k*个类别，*K*对应于类别数量）：
- en: π k  1 _ √ _ 2π  σ e  −1 _ 2σ 2(x−μ k) 2  ____________   ∑ i=1 K  π i  1 _ √ _ 2π 
    σ e  −1 _ 2σ 2(x−μ i) 2
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: π k  1 _ √ _ 2π  σ e  −1 _ 2σ 2(x−μ k) 2  ____________   ∑ i=1 K  π i  1 _ √ _ 2π 
    σ e  −1 _ 2σ 2(x−μ i) 2
- en: 'And for the multivariate case, the calculation looks like this:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多变量情况，计算如下：
- en: π k  1 _ √ _ |D|  e −1 _ 2 (x−μ) TD −1(x−μ)   _______________   ∑ i=1 K  π i
     1 _ √ _ |D|  e −1 _ 2 (x−μ) TD −1(x−μ) .
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: π k  1 _ √ _ |D|  e −1 _ 2 (x−μ) TD −1(x−μ)   _______________   ∑ i=1 K  π i
     1 _ √ _ |D|  e −1 _ 2 (x−μ) TD −1(x−μ) .
- en: 'The univariate linear discriminant function, δ k(x), can be written as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 单变量线性判别函数，δ k(x)，可以写成以下形式：
- en: δ k(x) =  x * μ k _ σ 2  −  μ k 2 _ 2 σ 2  + log(π k)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: δ k(x) =  x * μ k _ σ 2  −  μ k 2 _ 2 σ 2  + log(π k)
- en: 'Here, the class with the largest value of δ k(x) has its label assigned to
    the given observation. This uses the Bayesian decision boundary for *K* classes:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，具有δ k(x)最大值的类别将其标签分配给给定的观察值。这使用了*K*个类别的贝叶斯决策边界：
- en: x =  μ 1 2 − μ 2 2 − … − μ K 2   ____________  K(μ 1 − μ 2) .
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: x =  μ 1 2 − μ 2 2 − … − μ K 2   ____________  K(μ 1 − μ 2) .
- en: 'For the multivariate case, the linear discriminant function, which follows
    the same Bayesian decision boundary, looks like this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多变量情况，遵循相同贝叶斯决策边界的线性判别函数看起来如下：
- en: δ k(x) = x T D −1 μ k −  1 _ 2  μ k T D −1 μ k + log(π k).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: δ k(x) = x T D −1 μ k −  1 _ 2  μ k T D −1 μ k + log(π k).
- en: Now that we have identified the probability density functions, calculations
    for class membership posterior probabilities, the Bayesian decision boundary,
    and linear discriminant functions, we can understand how the required assumptions
    for LDA are very important. While transformations can be used to make data fit
    the required assumptions, these transformations must also be adaptable across
    future datasets. Therefore, if the data changes frequently and heavy transformations
    are needed to meet the parametric assumptions required of LDA to produce useful
    results across training and testing, this may not be the proper method of classification
    for the task (something such as **QDA** may be more useful). However, if these
    assumptions can be met at the researcher’s comfort, and with fair subject-matter
    knowledge to confirm this, LDA is an outstanding algorithm that is capable of
    producing highly reliable and stable results across very large datasets, as defined
    by both large feature space and high observation count. LDA also performs well
    on smaller datasets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了概率密度函数，计算类别成员后验概率、贝叶斯决策边界和线性判别函数，我们可以理解为什么 LDA 所需的假设非常重要。虽然可以使用变换来使数据符合所需的假设，但这些变换也必须能够适应未来的数据集。因此，如果数据频繁变化，并且需要大量的变换来满足
    LDA 在训练和测试中产生有用结果所需的参数假设，这可能不是这个任务的正确分类方法（例如，**QDA** 可能更有用）。然而，如果这些假设可以在研究者的舒适范围内得到满足，并且有足够的主题知识来确认这一点，LDA
    是一个出色的算法，能够在非常大的数据集上产生高度可靠和稳定的结果，这由大的特征空间和高观测计数定义。LDA 在较小的数据集上表现也很好。
- en: 'Let’s look at an example in practice. Let’s load the `affairs` dataset from
    `statsmodels` so we can check class imbalance:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个实际例子。让我们从 `statsmodels` 加载 `affairs` 数据集，这样我们可以检查类别不平衡：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here we can see the class imbalance as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到以下类别不平衡：
- en: '`Class 1` `Balance: 32.25%`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`Class 1` `Balance: 32.25%`'
- en: '`Class 2` `Balance: 67.75%`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`Class 2` `Balance: 67.75%`'
- en: 'We can see the class imbalance is 67.75/32.25\. Class imbalance doesn’t often
    become a large concern until it approaches around 90/10, so we’ll leave this as-is
    without any additional work, such as upsampling or downsampling. We’ll recode
    any `affairs` value greater than 0 to 1 to make this a binary classification problem:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到类别不平衡为 67.75/32.25。类别不平衡通常不会成为一个大问题，直到它接近 90/10，所以我们将保持现状，不做任何额外的工作，例如上采样或下采样。我们将任何大于
    0 的 `affairs` 值重新编码为 1，使其成为一个二元分类问题：
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s select the features we want to use:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择我们想要使用的特征：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Recall that input feature standard deviations must be the same across both
    classes for LDA to effectively produce a linear separation boundary. Using the
    method shown next, we can see all input features share the same standard deviation
    across both target classes with the exception of `rate_marriage`. The standard
    deviation here has a difference of about 20%. Considering we have eight input
    features and all features in the model will be scaled, this likely won’t present
    an issue. If the model underperforms, based on this information, we can reasonably
    assume it isn’t the algorithm’s fault as the required assumptions are met. Rather,
    it would more likely be that we don’t have enough features or observations to
    fully explain the target. We exclude `affairs` because that’s the target variable
    and `occupation` and `occupation_husb` because they’re categorical encodings that
    will be one-hot encoded since we are not considering them to be ordinal based
    on our analysis scope:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，对于 LDA 有效地产生线性分离边界，输入特征的标准差必须在两个类别中相同。使用下面显示的方法，我们可以看到所有输入特征在两个目标类别中都共享相同的标准差，除了
    `rate_marriage`。这里的标准差差异大约为 20%。考虑到我们有八个输入特征，并且模型中的所有特征都将进行缩放，这很可能不会成为一个问题。如果模型表现不佳，根据这个信息，我们可以合理地假设这不是算法的错，因为所需的假设已经得到满足。相反，更有可能的是我们没有足够多的特征或观测值来完全解释目标。我们排除了
    `affairs`，因为那是目标变量，以及 `occupation` 和 `occupation_husb`，因为它们是分类编码，由于我们根据分析范围不认为它们是序数，所以将进行独热编码：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Affairs = 0, Feature = rate_marriage, Standard Deviation =` `0.82`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 0, Feature = rate_marriage, Standard Deviation =` `0.82`'
- en: '`Affairs = 1, Feature = rate_marriage, Standard Deviation =` `1.07`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 1, Feature = rate_marriage, Standard Deviation =` `1.07`'
- en: '`Affairs = 0, Feature = age, Standard Deviation =` `6.81`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 0, Feature = age, Standard Deviation =` `6.81`'
- en: '`Affairs = 1, Feature = age, Standard Deviation =` `6.7`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 1, Feature = age, Standard Deviation =` `6.7`'
- en: '`Affairs = 0, Feature = yrs_married, Standard Deviation =` `7.1`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 0, Feature = yrs_married, Standard Deviation =` `7.1`'
- en: '`Affairs = 1, Feature = yrs_married, Standard Deviation =` `7.18`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 1, Feature = yrs_married, Standard Deviation =` `7.18`'
- en: '`Affairs = 0, Feature = children, Standard Deviation =` `1.42`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 0, Feature = children, Standard Deviation =` `1.42`'
- en: '`Affairs = 1, Feature = children, Standard Deviation =` `1.41`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 1, Feature = children, Standard Deviation =` `1.41`'
- en: '`Affairs = 0, Feature = religious, Standard Deviation =` `0.89`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 0, Feature = religious, Standard Deviation =` `0.89`'
- en: '`Affairs = 1, Feature = religious, Standard Deviation =` `0.84`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 1, Feature = religious, Standard Deviation =` `0.84`'
- en: '`Affairs = 0, Feature = educ, Standard Deviation =` `2.21`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 0, Feature = educ, Standard Deviation =` `2.21`'
- en: '`Affairs = 1, Feature = educ, Standard Deviation =` `2.09`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`Affairs = 1, Feature = educ, Standard Deviation =` `2.09`'
- en: 'In the previous chapter, we identified that `occupation` and `occupation_husb`
    do not capture much difference in explaining affairs so we will drop `occupation_husb`
    from the dataset to minimize the volume of one-hot encodes, as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们确定`occupation`和`occupation_husb`在解释婚外情方面没有捕捉到太多差异，因此我们将从数据集中删除`occupation_husb`以最小化one-hot编码的体积，如下所示：
- en: '[PRE4]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s build a train/test split with a test size of 33% of the overall data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个包含33%总体数据的测试集的训练/测试分割：
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We want to retain a copy of the original data, but as noted earlier, we also
    need to center and scale the data for LDA to have an optimal chance of success.
    Therefore, we’ll take copies of the `X` data and scale them. Note, however, we
    do not want to scale the one-hot encoded data since one-hot encodes cannot be
    scaled and retain their meaning. Therefore, we will use scikit-learn’s `ColumnTransformer()`
    pipeline function to apply `StandardScaler()` to all but the one-hot-encoded columns,
    like so:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望保留原始数据的一个副本，但如前所述，我们还需要对数据进行中心化和缩放，以便LDA有最佳成功的机会。因此，我们将复制`X`数据并对其进行缩放。然而，请注意，我们不想缩放one-hot编码的数据，因为one-hot编码不能缩放并保留其意义。因此，我们将使用scikit-learn的`ColumnTransformer()`管道函数将`StandardScaler()`应用于除了one-hot编码列之外的所有列，如下所示：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, let’s fit the LDA model to the training data. We fit the model to the training
    data and then use that to predict the training data to get a performance benchmark.
    Then, we will use the model to predict the testing data to see how well the model
    can generalize on unseen data.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将LDA模型拟合到训练数据上。我们将模型拟合到训练数据上，然后使用它来预测训练数据以获得性能基准。然后，我们将使用该模型来预测测试数据，以查看模型在未见数据上的泛化能力如何。
- en: 'Fit the data, like so:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 按照如下方式拟合数据：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here, we build functions to measure precision and recall. Precision is a measure
    that basically tells you how well a model is at finding only the positive values
    of the target (`affairs = 1`) and none of the others (`affairs = 0`). Recall is
    a measure that practically tells you how well the model performs at finding all
    positive values of the target, regardless of how many of the others it finds.
    The ideal scenario is both of these metrics are very high. However, this may not
    be possible. If neither is very high, the use case should determine which metric
    is more important:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们构建函数来测量精确度和召回率。精确度是一个基本告诉你模型在找到目标（`affairs = 1`）的正值方面做得有多好，并且不包含其他任何值（`affairs
    = 0`）。召回率是一个实际告诉你模型在找到所有目标正值方面做得有多好，不管它找到多少其他值。理想的情况是这两个指标都非常高。然而，这可能并不可能。如果两者都不是非常高的，使用案例应该确定哪个指标更重要：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s now run the model and verify performance:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行模型并验证性能：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here we can see our model results on the training data:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的模型在训练数据上的结果：
- en: '`Precision on Train:` `0.6252`'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`训练集精确度:` `0.6252`'
- en: '`Recall on Train:` `0.3444`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`训练集召回率:` `0.3444`'
- en: 'Here, we can visualize performance using a confusion matrix to compare actual
    target values against the predicted values on the training data:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以使用混淆矩阵可视化性能，以比较训练数据上的实际目标值与预测值：
- en: '![Figure 9.1 – Confusion matrix for LDA training data](img/B18945_09_001.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – LDA训练数据的混淆矩阵](img/B18945_09_001.jpg)'
- en: Figure 9.1 – Confusion matrix for LDA training data
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – LDA训练数据的混淆矩阵
- en: 'Let’s repeat the process on the test data:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在测试数据上重复这个过程：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here we can see our model results on the test data:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的模型在测试数据上的结果：
- en: '`Precision on Test:` `0.6615`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`测试集精确度:` `0.6615`'
- en: '`Recall on Test:` `0.3673`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`测试集召回率:` `0.3673`'
- en: 'Here, we can visualize performance using a confusion matrix to compare actual
    target values against the predicted values on the test data:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以使用混淆矩阵可视化性能，以比较测试数据上的实际目标值与预测值：
- en: '![Figure 9.2 – Confusion matrix for LDA test data](img/B18945_09_002.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – LDA测试数据的混淆矩阵](img/B18945_09_002.jpg)'
- en: Figure 9.2 – Confusion matrix for LDA test data
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – LDA测试数据的混淆矩阵
- en: From these results, we can see the scores are consistent for precision and recall
    across both training and test sets using the model we built on the training data,
    so we can conclude the model generalizes well on unseen data compared to the benchmark
    (training) performance. However, the performance is not great. The issue may be
    that our target encoding is not very useful (perhaps the discretization of affairs
    is not useful and a statistical approach should be taken to discretize the values).
    However, it is also possible the features do not explain enough of the variance
    in the `response` variable to build a model that is of much use (it’s also possible
    affairs cannot be predicted at all and there is too much random noise to model).
    However, for the intent of the chapter, we were able to demonstrate how a model
    using LDA could be constructed and tested in addition to the preprocessing steps
    that must be taken before data can be modeled.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些结果中，我们可以看到使用我们在训练数据上构建的模型，在训练集和测试集上，分数在精确度和召回率方面是一致的，因此我们可以得出结论，与基准（训练）性能相比，该模型在未见过的数据上具有良好的泛化能力。然而，性能并不出色。问题可能在于我们的目标编码并不十分有用（也许事务的离散化并不有用，应该采用统计方法来离散化值）。然而，也有可能特征没有充分解释`response`变量中的方差，以至于无法构建一个非常有用的模型（也可能事务根本无法预测，并且有太多的随机噪声难以建模）。然而，为了本章的目的，我们能够展示如何构建和测试使用LDA的模型，以及在进行数据建模之前必须采取的预处理步骤。
- en: Supervised dimension reduction
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督维度降低
- en: 'Unlike PCA, which can be used to perform unsupervised dimension reduction,
    LDA can be leveraged to perform supervised dimension reduction. That is, upon
    training the model to learn the input variance in relation to the output target,
    a derived set of features can be obtained. The differences are illustrated in
    the following diagram:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与PCA不同，PCA可以用于执行无监督的维度降低，而LDA可以用来执行监督的维度降低。也就是说，在训练模型以学习输入方差与输出目标之间的关系后，可以得到一组派生特征。这些差异在以下图中展示：
- en: '![Figure 9.3 – Dimension reduction: LDA versus PCA](img/B18945_09_003.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 维度降低：LDA与PCA](img/B18945_09_003.jpg)'
- en: 'Figure 9.3 – Dimension reduction: LDA versus PCA'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 维度降低：LDA与PCA的比较
- en: 'Suppose our data produced a good result when using LDA for classification.
    We could then confidently use the same data to build a supervised dimension reduction
    technique. To perform this, we would run the following code using `fit_transform()`
    to transform the input data with respect to the target (rather than the `fit()`
    function we used to fit a classifier earlier). As with the classifier, the data
    should still conform to the assumptions of the LDA model. *Data is reduced according
    to the number of classes in the response variable*. Where the number of classes
    is *C*, the number of reduced features will be *C-1*. Because we have only two
    classes in our target variable, `y`, LDA reduces the input features to a dimension
    of 1:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用LDA进行分类时数据产生了良好的结果。那么，我们可以自信地使用相同的数据来构建一个监督维度降低技术。为了执行这个操作，我们会使用`fit_transform()`方法来转换输入数据，使其与目标相关（而不是我们之前用来拟合分类器的`fit()`函数）。与分类器一样，数据仍然应该符合LDA模型的假设。*数据根据响应变量的类别数量进行降低*。当类别数量为*C*时，降低后的特征数量将是*C-1*。因为我们目标变量`y`中只有两个类别，所以LDA将输入特征降低到一维：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can see the data has now been reduced to a one-dimensional feature:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据现在已经被简化为一维特征：
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Here, we can see our data is reduced from 12 features to 1:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们的数据从12个特征降低到了1个：
- en: '`Input data dimensions: (``4265, 12)`'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`输入数据维度：(``4265, 12`)`'
- en: '`Transformed data dimensions: (``4265, 1)`'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`转换后数据维度：(``4265, 1`)`'
- en: Quadratic Discriminant Analysis
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二次判别分析
- en: 'In the last section, we discussed LDA. The data within each class needs to
    be drawn from a multivariate Gaussian distribution, and the covariance matrix
    is the same across different classes. In this section, we consider another type
    of discriminant analysis called QDA but the assumptions for QDA can be relaxed
    on the covariance matrix assumption. Here, we do not need the covariance matrix
    to be identical across different classes but only for each class to have its own
    covariance matrix. The multivariate Gaussian distribution with a class-specific
    mean vector within each class for observations is still required to conduct QDA.
    We assume that an observation from a k th class satisfies the following formula:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了LDA。每个类中的数据需要来自多变量高斯分布，并且协方差矩阵在不同类之间是相同的。在本节中，我们考虑另一种类型的判别分析，称为QDA，但QDA的假设可以在协方差矩阵假设上放宽。在这里，我们不需要协方差矩阵在不同类之间相同，但每个类都需要有自己的协方差矩阵。对于观察到的每个类，仍然需要具有类特定均值向量的多变量高斯分布来执行QDA。我们假设来自第k 类的观察值满足以下公式：
- en: X~N(μ k, Σ k)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: X~N(μ k, Σ k)
- en: 'We’ll thus consider a generative classifier, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将考虑以下生成分类器：
- en: p(X | y = k, θ) = N(X | μ k, Σ k)
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: p(X | y = k, θ) = N(X | μ k, Σ k)
- en: 'And then, its corresponding class posterior is this:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，其相应的类后验是：
- en: p(y = k | X, θ) ∝ π k N(X | μ k, Σ k).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: p(y = k | X, θ) ∝ π k N(X | μ k, Σ k).
- en: 'Here, π k = p(y = k) is the prior probability of the class k. In literature,
    this is called **Gaussian Discriminant Analysis** (**GDA**). Then, the discriminant
    function is computed by taking the log posterior over k classes, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，π k = p(y = k) 是类k的先验概率。在文献中，这被称为**高斯判别分析**（**GDA**）。然后，通过计算k个类别的对数后验来计算判别函数，如下所示：
- en: logp(y = k | X, θ) = log π k −  log|2π Σ k| _ 2  −  (X − μ k) T Σ k −1(X − μ k)  _______________ 2 
    + C
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: logp(y = k | X, θ) = log π k −  log|2π Σ k| _ 2  −  (X − μ k) T Σ k −1(X − μ k)  _______________ 2 
    + C
- en: Here, C is a constant. Unlike in LDA, the quantity of X appears as a quadratic
    function in the preceding formula and it is known as QDA. The choice between LDA
    or QDA is related to the bias-variance trade-off. If the assumption that shares
    the same covariance matrix over k classes is not good, then a high bias can be
    significant in LDA. In other words, LDA is used when a boundary between classes
    is linear, and QDA is performed better in the case of a nonlinear boundary between
    classes.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，C是一个常数。与LDA不同，X的数量在前一个公式中表现为二次函数，这被称为QDA。LDA或QDA的选择与偏差-方差权衡有关。如果假设k个类共享相同的协方差矩阵并不好，那么在LDA中高偏差可能很重要。换句话说，当类之间的边界是线性的时，使用LDA，而在类之间非线性边界的情况下，QDA表现更好。
- en: 'Now, we consider an example using the same *Iris* dataset that we studied in
    the last chapter. To perform QDA, an option is to use `QuadraticDiscriminantAnalysis`
    in sklearn. As in the last chapter, we use a `train_test_split` function to create
    a training and a test set (80% versus 20%), and then we fit a QDA model on the
    training set and use this model for prediction:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们考虑一个例子，使用我们在上一章研究的相同的*Iris*数据集。要执行QDA，一个选项是使用sklearn中的`QuadraticDiscriminantAnalysis`。与上一章一样，我们使用`train_test_split`函数创建一个训练集和一个测试集（80%对20%），然后在训练集上拟合一个QDA模型，并使用此模型进行预测：
- en: '[PRE13]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The confusion matrix related to the test set is shown here:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这里显示了与测试集相关的混淆矩阵：
- en: '![Figure 9.4 – QDA confusion matrix](img/B18945_09_004.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – QDA混淆矩阵](img/B18945_09_004.jpg)'
- en: Figure 9.4 – QDA confusion matrix
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – QDA混淆矩阵
- en: 'The accuracy is 100% for this dataset. This analysis is based on all variables
    (`sepal_length` (cm), `sepal_width` (cm), `petal_length` (cm), `petal_width` (cm)),
    and for three dependent variables (targets): `setosa`, `versicolor`, and `virginica`.
    For visualization educational purposes, we will only consider the analysis in
    two dimensions using `sepal_length` and `sepal_width` as the independent variables:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，准确率是100%。这个分析基于所有变量（`花瓣长度`（cm），`花瓣宽度`（cm），`花瓣长度`（cm），`花瓣宽度`（cm）），以及三个因变量（目标）：`setosa`，`versicolor`，和`virginica`。为了可视化教育目的，我们将只考虑使用`花瓣长度`和`花瓣宽度`作为自变量的二维分析：
- en: '![Figure 9.5 – sepal_length and sepal_width](img/B18945_09_005.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 花瓣长度和花瓣宽度](img/B18945_09_005.jpg)'
- en: Figure 9.5 – *sepal_length* and *sepal_width*
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – *花瓣长度* 和 *花瓣宽度*
- en: 'The visualization in two dimensions between `sepal_length` and `sepal_width`
    can be reproduced using the following Python code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下Python代码可以重现`花瓣长度`和`花瓣宽度`之间的二维可视化：
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The visualization is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化如下：
- en: '![Figure 9.6 – Iris flower species scatterplot](img/B18945_09_006.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6 – 爱丽丝花种类的散点图](img/B18945_09_006.jpg)'
- en: Figure 9.6 – Iris flower species scatterplot
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – 爱丽丝花种类的散点图
- en: Observing that there is no linear separation between the classes (`setosa`,
    `versicolor`, and `virginica`), a QDA would be a better approach than an LDA.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到类别之间（`setosa`、`versicolor`和`virginica`）没有线性分离，因此QDA（二次判别分析）比LDA（线性判别分析）是一个更好的方法。
- en: 'The `min` and `max` values are `(4.3, 7.9)` for `sepal_length` and `(2.0, 4.4)`
    for `sepal_width`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`min`和`max`值分别为`sepal_length`的`(4.3, 7.9)`和`sepal_width`的`(2.0, 4.4)`：'
- en: '![Figure 9.7 – sepal_length and sepal_width min and max values](img/B18945_09_007.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – `sepal_length`和`sepal_width`的最小和最大值](img/B18945_09_007.jpg)'
- en: Figure 9.7 – sepal_length and sepal_width min and max values
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – `sepal_length`和`sepal_width`的最小和最大值
- en: 'We train a new QDA model using only these two variables, `sepal_length` and
    `sepal_width`, on all observations in the Iris dataset, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用仅包含这两个变量`sepal_length`和`sepal_width`的所有观测值，在爱丽丝数据集上训练一个新的QDA模型，如下所示：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, we create a generic dataset with 500 observations for `sepal_length`
    and `sepal_width` within the min and max range of each variable for a prediction:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个通用的数据集，包含500个观测值，用于预测每个变量的最小和最大范围内的`sepal_length`和`sepal_width`：
- en: '[PRE16]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following Python code is executed for the visualization of boundary separation
    between classes:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码用于可视化类别之间的边界分离：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We get the resulting plot as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下结果图：
- en: '![Figure 9.8 – QDA decision boundaries for Iris dataset](img/B18945_09_008.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – 爱丽丝数据集的QDA决策边界](img/B18945_09_008.jpg)'
- en: Figure 9.8 – QDA decision boundaries for Iris dataset
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 爱丽丝数据集的QDA决策边界
- en: 'There is a good separation between `setosa` and these other two classes but
    not between the `versicolor` and `virginica` classes. The separation between classes
    will be better in higher dimensions, which means that in this example, we consider
    all four independent variables: `sepal_length`, `sepal_width`, `petal_length`,
    and `petal_width`. In other words, this analysis is better conducted in four dimensions,
    but it is not possible to have a four-dimensional visualization for human beings.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`setosa`与其他两个类别之间有很好的分离，但`versicolor`和`virginica`类别之间没有。在更高维度的类别之间分离会更好，这意味着在这个例子中，我们考虑所有四个独立变量：`sepal_length`、`sepal_width`、`petal_length`和`petal_width`。换句话说，这种分析最好在四个维度上进行，但对于人类来说，不可能有一个四维的可视化。'
- en: Summary
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we began with an overview of probability. We covered the differences
    between conditional and independent probability and how Bayes’ Theorem leverages
    these concepts to provide a unique approach to probability modeling. Next, we
    discussed LDA, its assumptions, and how the algorithm can be used to apply Bayesian
    statistics to both perform classification modeling and supervised dimension reduction.
    Finally, we covered QDA, an alternative to LDA when linear decision boundaries
    are not effective.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先概述了概率论。我们讨论了条件概率和独立概率之间的区别，以及贝叶斯定理如何利用这些概念为概率建模提供独特的方法。接下来，我们讨论了LDA，其假设以及如何使用该算法将贝叶斯统计应用于分类建模和监督降维。最后，我们介绍了QDA，当线性决策边界无效时，它是LDA的替代方案。
- en: In the next chapter, we will introduce the fundamentals of time-series analysis,
    including an overview of the depths and limitations of this approach to answering
    statistical questions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍时间序列分析的基础知识，包括对这种方法在回答统计问题方面的深度和局限性的概述。
- en: Part 4:Time Series Models
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4部分：时间序列模型
- en: The objective of this part is to learn how to analyze and create forecasts for
    univariate and multivariate time series data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分的目标是学习如何分析和创建单变量和多变量时间序列数据的预测。
- en: 'It includes the following chapters:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 它包括以下章节：
- en: '[*Chapter 10*](B18945_10.xhtml#_idTextAnchor160), *Introduction to Time Series*'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B18945_10.xhtml#_idTextAnchor160)，*时间序列简介*'
- en: '[*Chapter 11*](B18945_11.xhtml#_idTextAnchor174), *ARIMA Models*'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B18945_11.xhtml#_idTextAnchor174)，*ARIMA模型*'
- en: '[*Chapter 12*](B18945_12.xhtml#_idTextAnchor188), *Multivariate Time Series*'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第12章*](B18945_12.xhtml#_idTextAnchor188)，*多元时间序列*'
