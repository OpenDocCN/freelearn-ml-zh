- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Discriminant Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed discrete regression models, including
    classification using logistic regression. In this chapter, we will begin with
    an overview of probability, expanding into conditional and independent probability.
    We then discuss how these two approaches to understanding the laws of probability
    form the basis for Bayes’ Theorem, which is used directly to expand an approach
    called Bayesian statistics. Following this topic, we dive into **Linear Discriminant
    Analysis** (**LDA**) and **Quadratic Discriminant Analysis** (**QDA**), two powerful
    classifiers that model data using the Bayesian approach to probability modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Bayes’ Theorem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: QDA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayes’ theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss **Bayes’ Theorem**, which is used in the classification
    models described later in this chapter. We will start the chapter by discussing
    the basics of probability. Then, we will take a look at dependent events and discuss
    how Bayes’ Theorem is related to dependent events.
  prefs: []
  type: TYPE_NORMAL
- en: Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Probability is a measurement of the likelihood that an event occurs or a certain
    outcome occurs. Generally, we can group events into two types of events: **independent
    events** and **dependent events**. The distinction between the types of events
    is in the name. An independent event is an event that is not affected or influenced
    by the occurrences of other events, while a dependent event is affected or influenced
    by the occurrences of other events.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s think about some examples of these events. For the first example, think
    about a fair coin toss. A coin toss can result in one of two states: heads and
    tails. If the coin is fair, there is a one-in-two chance that the toss will result
    in heads or tails. That means that the probability of heads or tails is 0.5 in
    a fair coin toss. We can calculate probability using the following formula when
    events are countable:'
  prefs: []
  type: TYPE_NORMAL
- en: P(E) =  Count of Desired Outcomes  __________________  Total Number of Outcomes
  prefs: []
  type: TYPE_NORMAL
- en: Here, P(E) is the probability of the desired outcome. In the coin-toss example,
    the desired outcome is either heads or tails, each of which can only occur once
    per coin toss. Thus, for the coin toss, the count of desired events is one. The
    total number of outcomes is two because there are two possible states from a coin
    toss. Putting these numbers in the preceding equation yields 0.5 for heads and
    0.5 for tails. Now, putting your experience to work, does a previous coin toss
    give you any knowledge about the next toss of the same coin? It does not because
    *each coin toss is independent of every other coin toss*; it is an independent
    event.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s consider another example. Say we have a bag with three red marbles
    and five blue marbles. Let’s calculate the probability of selecting a red marble
    from the bag: P(Red). There are eight total marbles, which is the total number
    of outcomes in the equation. We want to select a red marble, which means the count
    of desired outcomes is three. This means that the probability of selecting red
    marble from the bag is 3 / 8\. Now, let’s calculate the probability of selecting
    a blue marble from the bag: P(Blue). There are five blue marbles and eight total
    marbles, so the probability of selecting a blue marble from the bag is 5 / 8\.
    Notice that the sum of the two probabilities is one: P(Red) + P(Blue) = 3 / 8
    + 5 / 8 = 1\. This brings up an important property of probabilities: the total
    probability must be equal to one. This, of course, means that once we calculated
    the probability of selecting a red marble, we could have calculated the probability
    of selecting a blue marble by subtracting the probability of selecting a red marble
    from one. The property of probabilities is very useful and is used commonly in
    statistics and **machine** **learning** (**ML**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Coming back to the example with red and blue marbles, let’s think about the
    type of event represented by this example. We select a marble from the bag. Now,
    there are two possible options next: (1) we replace the selected marble and select
    a marble from the bag again, and (2) we do not replace the marble and select a
    new marble from the remaining seven marbles in the bag. The first case is called
    **selection with replacement**. Selection with replacement is an independent event
    because the bag is reset before each draw from the bag. The second case is called
    **selection without replacement**. Selection without replacement is where you
    find dependent events. The first time you select a marble, the probability is
    the same as described previously (an independent event). However, if the marble
    is not replaced, the state of the bag has changed, and the probability of the
    next marble selection depends on which one was selected from the bag in the previous
    draw. Then, in selection without replacement, each subsequent marble draw is a
    dependent event. When discussing dependent events, it is useful to describe events
    using **conditional probability**. Let’s discuss conditional probability now.'
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start with a formal definition. When we have dependent events, we may
    be interested in the probability of Event B given Event A, which means we want
    to know the probability of Event B after Event A occurred. This is called the
    conditional probability of Event B given Event A and is denoted P(B | A). This
    can be thought of as “the probability of Event B on the condition that Event A
    has already occurred.” This may feel a bit vague, so let’s look at conditional
    probability in terms of our last example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say Event A is drawn a red marble and Event B is drawn a blue marble.
    We want to find the conditional probability of drawing a blue marble given that
    we have already drawn a red marble and can denote this as P(Blue | Red). Let’s
    work through this example. After drawing the first red marble from the bag, there
    are seven marbles remaining: two red marbles and five blue marbles. We calculate
    the probability of selecting a blue marble from the new set using the same equation,
    which gives us the probability of 5 / 7\. This means that the conditional probability
    of drawing a blue marble, given this a red marble was already drawn, is 5 / 7.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have discussed conditional probability, let’s look at the probability
    of making two draws from the bag. Formally, we want to calculate the probability
    of Event A and Event B. Continuing to follow our example, let’s calculate the
    probability of drawing a red marble and then drawing a blue marble. We can describe
    the probability of two sequential events with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: P(A and B) = P(A)* P(B | A)
  prefs: []
  type: TYPE_NORMAL
- en: 'In words, the probability of A followed by B is the probability of Event A
    times the probability of Event B given Event A. In our example, we have calculated
    P(Red) and P(Blue | Red). We can use those values to calculate the probability
    of drawing a red marble and then a blue marble P(Red and Blue), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(Red and Blue) = P(Red)* P(Blue | Red) =  3 _ 8 * 5 _ 7  =  15 _ 56  ≅ 0.27
  prefs: []
  type: TYPE_NORMAL
- en: Performing the calculation, we find that P(Red and Blue) is about 0.27.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, out of interest, let’s make the calculations again, but *exchanging the
    order of the colors*; the probability of drawing a blue marble and then drawing
    a red marble or P(Blue and Red). Following the same logic as shown in the previous
    example, we will find that P(Blue) = 5 / 8 and P(Red | Blue) = 3 / 7\. Thus, the
    equation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: P(Blue and Red) = P(Blue)* P(Red | Blue) =  5 _ 8 * 3 _ 7  =  15 _ 56  ≅ 0.27
  prefs: []
  type: TYPE_NORMAL
- en: Notice that P(Red and Blue) and P(Blue and Red) have the same probability. This
    is no coincidence. This is known as Bayes’ Theorem, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing Bayes’ Theorem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section on conditional probability, we happened across Bayes’
    Theorem in the example of drawing marbles from a bag. Let’s discuss Bayes’ Theorem
    in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes’ Theorem is a basis of a field of statistics known as **Bayesian statistics**.
    The central idea of Bayesian statistics is that given a prior probability, we
    have used new information to update probability estimates. While a full description
    of Bayesian statistics is beyond the scope of this book, we will discuss Bayes’
    Theorem as it relates to the concepts discussed in this book. We generally describe
    Bayes’ Theorem with the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: P(B|A) =  P(A|B)P(B) _ p(A)
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian terminology, P(B) is called the **prior** and P(B | A) is called
    the **posterior**. The posterior gives us an update given another event. Let’s
    make use of the Theorem in a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that we want to determine whether someone has a disease based on
    the results of a test. We would present this in Bayes’ Theorem as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(sick|test pos) =  P(test pos|sick)P(sick)  ________________  P(test pos)
  prefs: []
  type: TYPE_NORMAL
- en: Here, P(sick) is the probability of having the disease, P(test pos) is the probability
    of getting a positive test result, and P(test pos|sick) is the conditional probability
    of a positive test result when the patient tested is sick (also known as the accuracy
    of the test).
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will assume the probability of having the disease is 0.2
    and the test has an accuracy of 0.95 and a false positive rate of 0.3\. The prior
    probability in this example is P(sick)=0.2\. We can improve our knowledge of whether
    someone has the disease with the test. We want to find the posterior: P(sick|test pos).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we know the accuracy of the test is 0.95 and the prior, we already know
    the values of the terms in the nominator of the equation. To find P(test pos),
    we need to consider all cases where we can get a positive test result, which could
    be a true positive or a false positive. We calculate the probability of the positive
    test result as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(test pos) = P(test pos|sick)P(sick) + P(test pos|not sick)P(not sick)
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that the total probability of possible states must equal one: P(not sick)
    = 1 − P(sick) = 0.8\. With this in mind, we can calculate P(sick|test pos):'
  prefs: []
  type: TYPE_NORMAL
- en: P(sick|test pos) =  0.95(0.2)  ______________  0.95(0.2) + 0.3(0.8)  ≅ 0.44
  prefs: []
  type: TYPE_NORMAL
- en: 'Given one test result, we see that the likeliness of the patient being sick
    has increased from 0.2 to 0.44 with the additional information. 0.44 is not a
    particularly convincing result. This is due to the relatively high false positive
    rate of 0.3, even though the test has high accuracy. If we are not convinced,
    we can run the test again using 0.44 as the new prior. Assuming the second test
    is positive, we would get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: P(sick|test pos) =  0.95(0.441)  ____________________   0.95(0.441) + 0.3(1
    − 0.441)  ≅ 0.71
  prefs: []
  type: TYPE_NORMAL
- en: A second positive test provides a more convincing result of 0.71\. We could
    continue to iteratively improve the probability estimation that a patient is sick
    by incorporating the results from additional tests. This incremental improvement
    of probability estimation is the basic idea of Bayesian statistics.
  prefs: []
  type: TYPE_NORMAL
- en: This section contained an introduction to probability. We covered independent
    and dependent events and then provided more details on dependent events, which
    led to the introduction of Bayes’ Theorem. In the next section, we will discuss
    a type of classification model that uses Bayes’ Theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Linear Discriminant Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed logistic regression as a classification
    model leveraging linear regression to model directly the probability of a target
    distribution given an input distribution. One alternative to this approach is
    LDA. LDA models the probability of target distribution class memberships given
    input variable distributions corresponding to each class using decision boundaries
    constructed using Bayes’ Theorem, which we discussed previously. Where we have
    *k* classes, using Bayes’ Theorem, we have the probability density function for
    LDA class membership simply as P(Y = k|X = x) for any discrete random variable,
    *X*. This relies on the posterior probability that an observation *x* in variable
    *X* belongs to the *k*th class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before proceeding, we must first make note that LDA makes three pertinent assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: Each input variable is normally distributed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Across all target classes, there is **equal covariance** among the predictors.
    In other words, the shared variance of all input variables is uniform. For this
    reason, it is useful to **standardize each input variable** to have a mean of
    0 and **scale to unit variance** (standard deviation of 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Samples are assumed to be independent; **random sampling** is highly important
    to avoid complications resulting from **serial or** **cluster effects**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these assumptions met, we can make the assumption that the class probability
    density function is Gaussian (normal), and thus, in a one-dimensional form (one
    input variable), we have the following density function:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 _ √ _ 2π  σ k  e  −1 _ 2σ k 2(x−μ k) 2
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, μ k is the *k*th class mean and σ k 2 is the *k*th class variance. In
    the multivariate case, the normal probability density function is this:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 _  2π p/2 √ _ |D|   e −1 _ 2 (x−μ) TD −1(x−μ)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *D* is the covariance matrix of the input variables, *p* is the number
    of input variables (or parameters), and *π* is the prior probability. The univariate
    calculation of the posterior probability is shown here (where *k* corresponds
    to the *k*th class and *K* corresponds to the number of classes):'
  prefs: []
  type: TYPE_NORMAL
- en: π k  1 _ √ _ 2π  σ e  −1 _ 2σ 2(x−μ k) 2  ____________   ∑ i=1 K  π i  1 _ √ _ 2π 
    σ e  −1 _ 2σ 2(x−μ i) 2
  prefs: []
  type: TYPE_NORMAL
- en: 'And for the multivariate case, the calculation looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: π k  1 _ √ _ |D|  e −1 _ 2 (x−μ) TD −1(x−μ)   _______________   ∑ i=1 K  π i
     1 _ √ _ |D|  e −1 _ 2 (x−μ) TD −1(x−μ) .
  prefs: []
  type: TYPE_NORMAL
- en: 'The univariate linear discriminant function, δ k(x), can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: δ k(x) =  x * μ k _ σ 2  −  μ k 2 _ 2 σ 2  + log(π k)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the class with the largest value of δ k(x) has its label assigned to
    the given observation. This uses the Bayesian decision boundary for *K* classes:'
  prefs: []
  type: TYPE_NORMAL
- en: x =  μ 1 2 − μ 2 2 − … − μ K 2   ____________  K(μ 1 − μ 2) .
  prefs: []
  type: TYPE_NORMAL
- en: 'For the multivariate case, the linear discriminant function, which follows
    the same Bayesian decision boundary, looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: δ k(x) = x T D −1 μ k −  1 _ 2  μ k T D −1 μ k + log(π k).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have identified the probability density functions, calculations
    for class membership posterior probabilities, the Bayesian decision boundary,
    and linear discriminant functions, we can understand how the required assumptions
    for LDA are very important. While transformations can be used to make data fit
    the required assumptions, these transformations must also be adaptable across
    future datasets. Therefore, if the data changes frequently and heavy transformations
    are needed to meet the parametric assumptions required of LDA to produce useful
    results across training and testing, this may not be the proper method of classification
    for the task (something such as **QDA** may be more useful). However, if these
    assumptions can be met at the researcher’s comfort, and with fair subject-matter
    knowledge to confirm this, LDA is an outstanding algorithm that is capable of
    producing highly reliable and stable results across very large datasets, as defined
    by both large feature space and high observation count. LDA also performs well
    on smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at an example in practice. Let’s load the `affairs` dataset from
    `statsmodels` so we can check class imbalance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see the class imbalance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Class 1` `Balance: 32.25%`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Class 2` `Balance: 67.75%`'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the class imbalance is 67.75/32.25\. Class imbalance doesn’t often
    become a large concern until it approaches around 90/10, so we’ll leave this as-is
    without any additional work, such as upsampling or downsampling. We’ll recode
    any `affairs` value greater than 0 to 1 to make this a binary classification problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s select the features we want to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that input feature standard deviations must be the same across both
    classes for LDA to effectively produce a linear separation boundary. Using the
    method shown next, we can see all input features share the same standard deviation
    across both target classes with the exception of `rate_marriage`. The standard
    deviation here has a difference of about 20%. Considering we have eight input
    features and all features in the model will be scaled, this likely won’t present
    an issue. If the model underperforms, based on this information, we can reasonably
    assume it isn’t the algorithm’s fault as the required assumptions are met. Rather,
    it would more likely be that we don’t have enough features or observations to
    fully explain the target. We exclude `affairs` because that’s the target variable
    and `occupation` and `occupation_husb` because they’re categorical encodings that
    will be one-hot encoded since we are not considering them to be ordinal based
    on our analysis scope:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`Affairs = 0, Feature = rate_marriage, Standard Deviation =` `0.82`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 1, Feature = rate_marriage, Standard Deviation =` `1.07`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 0, Feature = age, Standard Deviation =` `6.81`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 1, Feature = age, Standard Deviation =` `6.7`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 0, Feature = yrs_married, Standard Deviation =` `7.1`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 1, Feature = yrs_married, Standard Deviation =` `7.18`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 0, Feature = children, Standard Deviation =` `1.42`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 1, Feature = children, Standard Deviation =` `1.41`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 0, Feature = religious, Standard Deviation =` `0.89`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 1, Feature = religious, Standard Deviation =` `0.84`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 0, Feature = educ, Standard Deviation =` `2.21`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Affairs = 1, Feature = educ, Standard Deviation =` `2.09`'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, we identified that `occupation` and `occupation_husb`
    do not capture much difference in explaining affairs so we will drop `occupation_husb`
    from the dataset to minimize the volume of one-hot encodes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s build a train/test split with a test size of 33% of the overall data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to retain a copy of the original data, but as noted earlier, we also
    need to center and scale the data for LDA to have an optimal chance of success.
    Therefore, we’ll take copies of the `X` data and scale them. Note, however, we
    do not want to scale the one-hot encoded data since one-hot encodes cannot be
    scaled and retain their meaning. Therefore, we will use scikit-learn’s `ColumnTransformer()`
    pipeline function to apply `StandardScaler()` to all but the one-hot-encoded columns,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s fit the LDA model to the training data. We fit the model to the training
    data and then use that to predict the training data to get a performance benchmark.
    Then, we will use the model to predict the testing data to see how well the model
    can generalize on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fit the data, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we build functions to measure precision and recall. Precision is a measure
    that basically tells you how well a model is at finding only the positive values
    of the target (`affairs = 1`) and none of the others (`affairs = 0`). Recall is
    a measure that practically tells you how well the model performs at finding all
    positive values of the target, regardless of how many of the others it finds.
    The ideal scenario is both of these metrics are very high. However, this may not
    be possible. If neither is very high, the use case should determine which metric
    is more important:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now run the model and verify performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see our model results on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Precision on Train:` `0.6252`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Recall on Train:` `0.3444`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can visualize performance using a confusion matrix to compare actual
    target values against the predicted values on the training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Confusion matrix for LDA training data](img/B18945_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Confusion matrix for LDA training data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s repeat the process on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see our model results on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Precision on Test:` `0.6615`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Recall on Test:` `0.3673`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can visualize performance using a confusion matrix to compare actual
    target values against the predicted values on the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Confusion matrix for LDA test data](img/B18945_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Confusion matrix for LDA test data
  prefs: []
  type: TYPE_NORMAL
- en: From these results, we can see the scores are consistent for precision and recall
    across both training and test sets using the model we built on the training data,
    so we can conclude the model generalizes well on unseen data compared to the benchmark
    (training) performance. However, the performance is not great. The issue may be
    that our target encoding is not very useful (perhaps the discretization of affairs
    is not useful and a statistical approach should be taken to discretize the values).
    However, it is also possible the features do not explain enough of the variance
    in the `response` variable to build a model that is of much use (it’s also possible
    affairs cannot be predicted at all and there is too much random noise to model).
    However, for the intent of the chapter, we were able to demonstrate how a model
    using LDA could be constructed and tested in addition to the preprocessing steps
    that must be taken before data can be modeled.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised dimension reduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike PCA, which can be used to perform unsupervised dimension reduction,
    LDA can be leveraged to perform supervised dimension reduction. That is, upon
    training the model to learn the input variance in relation to the output target,
    a derived set of features can be obtained. The differences are illustrated in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Dimension reduction: LDA versus PCA](img/B18945_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3 – Dimension reduction: LDA versus PCA'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose our data produced a good result when using LDA for classification.
    We could then confidently use the same data to build a supervised dimension reduction
    technique. To perform this, we would run the following code using `fit_transform()`
    to transform the input data with respect to the target (rather than the `fit()`
    function we used to fit a classifier earlier). As with the classifier, the data
    should still conform to the assumptions of the LDA model. *Data is reduced according
    to the number of classes in the response variable*. Where the number of classes
    is *C*, the number of reduced features will be *C-1*. Because we have only two
    classes in our target variable, `y`, LDA reduces the input features to a dimension
    of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the data has now been reduced to a one-dimensional feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can see our data is reduced from 12 features to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Input data dimensions: (``4265, 12)`'
  prefs: []
  type: TYPE_NORMAL
- en: '`Transformed data dimensions: (``4265, 1)`'
  prefs: []
  type: TYPE_NORMAL
- en: Quadratic Discriminant Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the last section, we discussed LDA. The data within each class needs to
    be drawn from a multivariate Gaussian distribution, and the covariance matrix
    is the same across different classes. In this section, we consider another type
    of discriminant analysis called QDA but the assumptions for QDA can be relaxed
    on the covariance matrix assumption. Here, we do not need the covariance matrix
    to be identical across different classes but only for each class to have its own
    covariance matrix. The multivariate Gaussian distribution with a class-specific
    mean vector within each class for observations is still required to conduct QDA.
    We assume that an observation from a k th class satisfies the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: X~N(μ k, Σ k)
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll thus consider a generative classifier, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: p(X | y = k, θ) = N(X | μ k, Σ k)
  prefs: []
  type: TYPE_NORMAL
- en: 'And then, its corresponding class posterior is this:'
  prefs: []
  type: TYPE_NORMAL
- en: p(y = k | X, θ) ∝ π k N(X | μ k, Σ k).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, π k = p(y = k) is the prior probability of the class k. In literature,
    this is called **Gaussian Discriminant Analysis** (**GDA**). Then, the discriminant
    function is computed by taking the log posterior over k classes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: logp(y = k | X, θ) = log π k −  log|2π Σ k| _ 2  −  (X − μ k) T Σ k −1(X − μ k)  _______________ 2 
    + C
  prefs: []
  type: TYPE_NORMAL
- en: Here, C is a constant. Unlike in LDA, the quantity of X appears as a quadratic
    function in the preceding formula and it is known as QDA. The choice between LDA
    or QDA is related to the bias-variance trade-off. If the assumption that shares
    the same covariance matrix over k classes is not good, then a high bias can be
    significant in LDA. In other words, LDA is used when a boundary between classes
    is linear, and QDA is performed better in the case of a nonlinear boundary between
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we consider an example using the same *Iris* dataset that we studied in
    the last chapter. To perform QDA, an option is to use `QuadraticDiscriminantAnalysis`
    in sklearn. As in the last chapter, we use a `train_test_split` function to create
    a training and a test set (80% versus 20%), and then we fit a QDA model on the
    training set and use this model for prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The confusion matrix related to the test set is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – QDA confusion matrix](img/B18945_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – QDA confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'The accuracy is 100% for this dataset. This analysis is based on all variables
    (`sepal_length` (cm), `sepal_width` (cm), `petal_length` (cm), `petal_width` (cm)),
    and for three dependent variables (targets): `setosa`, `versicolor`, and `virginica`.
    For visualization educational purposes, we will only consider the analysis in
    two dimensions using `sepal_length` and `sepal_width` as the independent variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – sepal_length and sepal_width](img/B18945_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – *sepal_length* and *sepal_width*
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualization in two dimensions between `sepal_length` and `sepal_width`
    can be reproduced using the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The visualization is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Iris flower species scatterplot](img/B18945_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Iris flower species scatterplot
  prefs: []
  type: TYPE_NORMAL
- en: Observing that there is no linear separation between the classes (`setosa`,
    `versicolor`, and `virginica`), a QDA would be a better approach than an LDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `min` and `max` values are `(4.3, 7.9)` for `sepal_length` and `(2.0, 4.4)`
    for `sepal_width`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – sepal_length and sepal_width min and max values](img/B18945_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – sepal_length and sepal_width min and max values
  prefs: []
  type: TYPE_NORMAL
- en: 'We train a new QDA model using only these two variables, `sepal_length` and
    `sepal_width`, on all observations in the Iris dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create a generic dataset with 500 observations for `sepal_length`
    and `sepal_width` within the min and max range of each variable for a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following Python code is executed for the visualization of boundary separation
    between classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the resulting plot as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – QDA decision boundaries for Iris dataset](img/B18945_09_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – QDA decision boundaries for Iris dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a good separation between `setosa` and these other two classes but
    not between the `versicolor` and `virginica` classes. The separation between classes
    will be better in higher dimensions, which means that in this example, we consider
    all four independent variables: `sepal_length`, `sepal_width`, `petal_length`,
    and `petal_width`. In other words, this analysis is better conducted in four dimensions,
    but it is not possible to have a four-dimensional visualization for human beings.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we began with an overview of probability. We covered the differences
    between conditional and independent probability and how Bayes’ Theorem leverages
    these concepts to provide a unique approach to probability modeling. Next, we
    discussed LDA, its assumptions, and how the algorithm can be used to apply Bayesian
    statistics to both perform classification modeling and supervised dimension reduction.
    Finally, we covered QDA, an alternative to LDA when linear decision boundaries
    are not effective.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce the fundamentals of time-series analysis,
    including an overview of the depths and limitations of this approach to answering
    statistical questions.
  prefs: []
  type: TYPE_NORMAL
- en: Part 4:Time Series Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of this part is to learn how to analyze and create forecasts for
    univariate and multivariate time series data.
  prefs: []
  type: TYPE_NORMAL
- en: 'It includes the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18945_10.xhtml#_idTextAnchor160), *Introduction to Time Series*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B18945_11.xhtml#_idTextAnchor174), *ARIMA Models*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B18945_12.xhtml#_idTextAnchor188), *Multivariate Time Series*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
