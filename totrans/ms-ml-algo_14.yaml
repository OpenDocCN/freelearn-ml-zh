- en: Introduction to Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to introduce the fundamental concepts of **Reinforcement
    Learning** (**RL**), which is a set of approaches that allows an agent to learn
    how to behave in an unknown environment, thanks to the rewards that are provided
    after each possible action. RL has been studied for decades, but it has reached
    a very high maturity level in the last few years when it became possible to employ
    deep learning models together with standard (and often simple) algorithms in order
    to solve extremely complex problems (such as learning how to play an Atari game
    perfectly).
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we will discuss:'
  prefs: []
  type: TYPE_NORMAL
- en: The concepts of environment, agent, policy, and reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of the **Markov Decision Process** (**MDP**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The policy iteration algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value iteration algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TD(0) algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement Learning fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine that you want to learn to ride a bike and ask a friend for advice.
    They explain how the gears work, how to release the brake and a few other technical
    details. In the end, you ask the secret to keeping balanced. What kind of answer
    do you expect? In an imaginary supervised world, you should be able to perfectly
    quantify your actions and correct the errors by comparing the outcomes with precise
    reference values. In the real world, you have no idea about the quantities underlying
    your actions and, above all, you will never know what the right value is. Increasing
    the level of abstraction, the scenario we''re considering can be described as:
    a generic **agent** performs actions inside an **environment** and receives **feedback**
    that is somehow proportional to the competence of its actions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'According to this **feedback**, the **agent** can correct its actions in order
    to reach a specific goal. This basic schema is represented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5f766b1-6137-4232-830c-af782674037b.png)'
  prefs: []
  type: TYPE_IMG
- en: Basic RL schema
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning to our initial example, when you ride a bike for the first time and
    try to keep your balance, you will notice that the wrong movement causes an increase
    in the slope, which in turn increases the horizontal component of the gravity
    force, pushing the bike laterally. As the vertical component is compensated, the
    result is a rotation that ends when the bike falls down completely. However, as
    you can use your legs to control the balance, when the bike starts falling, thanks
    to Newton''s third law, the force on the leg increases and your brain understands
    that it''s necessary to make a movement in the opposite direction. Even if this
    problem can be easily expressed in terms of physical laws, nobody learns to ride
    a bike by computing forces and momentums. This is one of the main concepts of
    RL: an agent must always make its choices considering a piece of information,
    usually defined as a *reward*, that represents the response, provided by the environment.
    If the action is correct, the reward will be positive, otherwise, it will be negative.
    After receiving a reward, an agent can fine-tune the strategy, called *policy*,
    in order to maximize the expected future reward. For example, after a few rides,
    you will be able to slightly move your body so as to keep the balance while turning,
    but probably, in the beginning, you needed to extend your leg to avoid falling
    down. Hence, your initial policy suggested a wrong action, which received repeated
    negative rewards and so your brain corrected it by increasing the probability
    of choosing another action. The implicit hypothesis that underlies this approach
    is that an agent is always *rational*, meaning that its goal is to maximize the
    expected return of its actions (nobody would like to fall down just to feel a
    different emotion).'
  prefs: []
  type: TYPE_NORMAL
- en: Before discussing the single components of an RL system, it's necessary to add
    a couple of fundamental assumptions. The first one is that an agent can repeat
    the experiences an infinite number of times. In other words, we assume that it's
    possible to learn a valid policy (possibly the optimal one) only if we have enough
    time. Clearly, this is unacceptable in the animal world and we all know that many
    experiences are extremely dangerous; however, this assumption is necessary to
    prove the convergence of some algorithms. Indeed, sub-optimal policies sometimes
    can be learned very quickly, but it's necessary to iterate many times to reach
    the optimal one. In real artificial systems, we always stop the learning process
    after a finite number of iterations, but it's almost impossible to find valid
    solutions if some experiences prevent the agent from continuing to interact with
    the environment. As many tasks have final states (either positive or negative),
    we assume that the agent can play any number of *episodes* (somewhat analogous
    to the epochs of supervised learning), exploiting the experience previously learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second assumption is a little bit more technical and it''s usually known
    as the *Markov property*. When the agent interacts with the environment, it observes
    a sequence of states. Even if it can seem like an oxymoron, we assume that each
    state is stateful. We can explain this concept with a simple example; suppose
    that you''re filling a tank and every five seconds you measure the level. Imagine
    that at *t = 0*, the level *L = 10* and the water is flowing in. What do you expect
    at *t = 1*? Obviously, *L > 10*. In other words, without external unknown causes,
    we assume that a state contains the previous history, so that the sequence, even
    if discretized, represents a continuous evolution where no jumps are allowed.
    When an RL task satisfies this property, it''s called a Markov Decision Process and
    it''s very easy to employ simple algorithms to evaluate the actions. Luckily,
    the majority of natural events can be modeled as MDPs (when you''re walking toward
    a door, every step in the right direction must decrease the distance), but there
    are some games that are implicitly stateless. For example, if you want to employ
    an RL algorithm to learn how to guess the outcome of a probabilistic sequence
    of independent events (such as tossing a coin), the result could be dramatically
    wrong. The reason is clear: any state is independent of the previous ones and
    every attempt to build up a history is a failure. Therefore, if you observe a
    sequence of *0, 0, 0, 0, ...* you are not justified in increasing the value of
    betting on 0 unless, after considering the likelihood of the events, you suppose
    that the coin is loaded. However, if there''s no reason to do so, the process
    isn''t an MDP and every episode (event) is completely independent. All the assumptions
    that we, either implicitly or explicitly, make are based on this fundamental concept,
    so pay attention when evaluating new, unusual scenarios because you may discover
    that the employment of a specific algorithm isn''t theoretically justified.'
  prefs: []
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **environment** is the entity where the **agent** has to reach its goals.
    For our purposes, a generic environment is a system that receives an input action, *a[t]*
    (we use the index *t* because this is a natural time process), and outputs a tuple
    composed by a state, *s[t+1]*, and a reward, *r[t+1]*. These two elements are
    the only pieces of information provided to the agent to make its next decision.
    If we are working with an MDP and the sets of possible actions, A, and states,
    S, are discrete and finite, the problem is a defined finite MDP (in many continuous
    cases, it''s possible to treat the problem as a finite MDP by discretizing the
    spaces). If there are final states, the task is called *episodic* and, in general,
    the goal is to reach a positive final state in the shortest amount of time or
    maximize a score. The schema of the cyclic interaction between agent an environment
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7f8fb93-5d5f-4d1f-9040-e19abef9fd6a.png)'
  prefs: []
  type: TYPE_IMG
- en: Agent-environment interaction schema
  prefs: []
  type: TYPE_NORMAL
- en: 'A very important feature of an environment is its internal nature. It can be
    either *deterministic* or *stochastic*. A deterministic environment is characterized
    by a function that associates each possible action, *a[t]*, in a specific state, *s[t]*,
    to a well-defined successor, *s*[*t+1*], with a precise reward, *r[t+1]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a85f55e3-9b7d-4101-8345-5c39db4ad788.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Conversely, a stochastic environment is characterized by a transition probability
    between the current state, *s[t]*, and a set of possible successors, *s^i[t+1]*,
    given an action, *a[t]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5824d4e-6559-437e-a8d0-724d4a324541.png)'
  prefs: []
  type: TYPE_IMG
- en: If a state, *s[i]*, has a transitional probability, *T(s[i], s[i], a[t]) = 1 ∀
    a[t ]∈ A*, the state is defined as *absorbing*. In general, all ending states
    in episodic tasks are modeled as absorbing ones, to avoid any further transition.
    When an episode is not limited to a fixed number of steps, the only criterion
    to determine its end is to check whether the agent has reached an absorbing state.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we don''t know which state will be the successor, it''s necessary to consider
    the expected value of all possible rewards considering the initial state, *s[t]*,
    and the action, *a[t]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fbf6f10-762b-4510-a62f-9de83b98f0a2.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, it's easier to manage stochastic environments because they can be
    immediately converted into deterministic ones by setting all probabilities to
    zero except the one corresponding to the actual successor (for example, T(•) =
    (0, 0, ..., 1, ..., 0)). In the same way, the expected return can be set equal
    to *r[t+1]*. The knowledge of T(•), as well as *E[r^i[t+1]]*, is necessary to
    employ some specific algorithms, but it can become problematic when finding a
    suitable model for the environment requires an extremely complex analysis. In
    all those cases, model-free methods can be employed and, therefore, the environment
    is considered as a black-box, whose output at time, *t* (subsequent to an action
    performed by the agent, *a[t-1]*), is the only available piece of information
    for the evaluation of a policy.
  prefs: []
  type: TYPE_NORMAL
- en: Rewards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen that rewards (sometimes negative rewards are called *penalties*,
    but it''s preferable to use a standardized notation) are the only feedback provided
    by the environment after each action. However, there are two different approaches
    to the use of rewards. The first one is the strategy of a very short-sighted agent
    and consists in taking into account only the reward just received. The main problem
    with this approach is clearly the inability to consider longer sequences that
    can lead to a very high reward. For example, an agent has to traverse a few states
    with negative reward (for example, -0.1), but after them, they arrive at a state
    with a very positive reward (for example, +5.0). A short-sighted agent couldn''t 
    find out the best policy because it will simply try to avoid the immediate negative
    rewards. On the other side, it''s better to suppose that a single reward contains
    a part of the future rewards that will be obtained following the same policy.
    This concept can be expressed by introducing a *discounted reward*, which is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8473c1dd-1fbc-4804-8cd5-7620a2d93b15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous expression, we are assuming an infinite horizon with a discount
    factor, γ, which is a real number bounded between 0 and 1 (not included). When γ
    = 0, the agent is extremely short-sighted, because of *R[t] = r[t+1]*, but when *γ →*
    1, the current reward takes into account the future contributions discounted in
    a way that is inversely proportional to the time-step. In this way, very close
    rewards will have a higher weight than very distant ones. If the absolute value
    of all rewards is limited by a maximum immediate absolute reward, *|r[i]| ≤ |r[max]|*,
    the previous expression will be always bounded. In fact, considering the properties
    of a geometric series, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2819bbb5-a656-419b-a7a4-0167fe6c9af9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Clearly, the right choice of γ is a crucial factor in many problems and cannot
    be easily generalized. As in many other similar cases, I suggest testing different
    values, picking the one that minimizes the convergence speed while yielding a
    quasi-optimal policy. Of course, if the tasks are episodic with length, *T(e[i])*,
    the discounted reward becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d0ea980-a9a0-4c24-be1c-93ceccf4076a.png)'
  prefs: []
  type: TYPE_IMG
- en: Checkerboard environment in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are going to consider an example based on a checkerboard environment representing
    a tunnel. The goal of the agent is to reach the ending state (lower-right corner),
    avoiding 10 wells that are negative absorbing states. The rewards are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ending state**: +5.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wells**: -5.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**All other states**: -0.1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selecting a small negative reward for all non-terminal states is helpful to
    force the agent to move forward until the maximum (final) reward has been achieved.
    Let''s start modeling an environment that has a 5 × 15 matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The graphical representation of the environment (in terms of rewards) is shown
    in the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3d68b884-208b-479e-8aaf-cbcfd3cf630a.png)'
  prefs: []
  type: TYPE_IMG
- en: Rewards in the tunnel environment
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent is allowed to move in four directions: up, down, left, and right.
    Clearly, in this case, the environment is deterministic because every action moves
    the agent to a predefined cell. We assume that whenever an action is forbidden
    (such as trying to move on the left when the agent is in the first column), the
    successor state is the same one (with the corresponding reward).'
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A *policy* is formally a deterministic or stochastic law that the agent follows
    in order to maximize its return. Conventionally, all policies are denoted with
    the letter *π*. A *deterministic policy* is usually a function of the current
    state that outputs a precise action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/162f2b33-d17f-40f1-bbf9-508873a21c95.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A *stochastic policy*, analogously to environments, outputs the probability
    of each action (in this case, we are assuming we work with a finite MPD):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f34fa72a-a348-41f8-ab78-623ed830cc93.png)'
  prefs: []
  type: TYPE_IMG
- en: However, contrary to the environment, an agent must always pick a specific action,
    transforming any stochastic policy into a deterministic sequence of choices. In
    general, a policy where *π(s, a) > 0 ∀ a ∈ A*, is called *soft* and it's often
    very useful during the training process because it allows a more flexible modeling
    without the premature selection of a suboptimal action. Instead, when *π(s, a[i])
    = 0 ∀ i ≠ j* and *π(s, a[j]) = 1*, the policy is also defined as *hard*. This
    transformation can be performed in many ways, but the most common one is to define
    a policy that is greedy with respect to a value (we're going to discuss this concept
    in the next section). This means that, at every step, the policy will select the
    action that maximizes the value of the successor state. Obviously, this is a very
    rational approach, which could be too pragmatic. In fact, when the values of some
    states don't change, a greedy policy will always force the agent to perform the
    same actions.
  prefs: []
  type: TYPE_NORMAL
- en: Such a problem is known as the *exploration-exploitation dilemma* and arises
    when it would be better to allow the agent to evaluate alternative strategies
    that could appear initially to be suboptimal. In other words, we want the agent
    to explore the environment before starting to exploit the policy, to know whether
    the policy is really the best one or if there are hidden alternatives. To solve
    this problem, it's possible to employ an *ε-greedy policy*, where the value, *ε*,
    is called the *exploration factor* and represents a probability. In this case,
    the policy will pick a random action with probability *ε* and a greedy one with
    probability *1 - ε*. In general, at the beginning of the training process, *ε*
    is kept very close to 1.0 to incentivize the exploration and it's progressively decreased
    when the policy becomes more stable. In many Deep RL applications, this approach
    is fundamental, in particular, when there are no models of the environment. The
    reason is that greedy policies can be initially wrong and it's necessary to allow
    the agent to explore many possible state and action sequences before forcing a
    deterministic decision.
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to analyze a strategy to find an optimal policy
    based on a complete knowledge of the environment (in terms of transition probability
    and expected returns). The first step is to define a method that can be employed
    to build a greedy policy. Let''s suppose we''re working with a finite MDP and
    a generic policy, π; we can define the intrinsic value of a state, *s[t]*, as
    the expected discounted return obtained by the agent starting from *s[t]* and
    following the stochastic policy, *π*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/38a56381-2dab-457c-a2ca-9e2241245ce8.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, we are assuming that, as the agent will follow *π*, state *s[a]*
    is more useful than *s[b]* if the expected return starting from *s[a]* is greater
    than the one obtained starting from s[b]. Unfortunately, trying to directly find
    the value of each state using the previous definition is almost impossible when *γ
    > 0*. However, this a problem that can be solved using Dynamic Programming (for
    further information, please refer to *Dynamic Programming and Markov Process*,
    *Ronald A. Howard*, The MIT Press), which allows us to solve the problem iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In particular, we need to turn the previous formula into a *Bellman equation*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2551adf2-6d67-424c-8d66-f83ed1e7ce83.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first term on the right-hand side can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2c30e3ba-5d6a-43ac-815b-786086908258.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, it is the weighted average of all expected returns considering
    that the agent is state, *s[t]*, and evaluates all possible actions and the consequent
    state transitions. For the second term, we need a small trick. Let''s suppose
    we start from *s[t+1]*, so that the expected value corresponds to *V(s[t+1];π)*;
    however, as the sum starts from *s[t]*, we need to consider all possible transitions
    starting from *s[t]*. In this case, we can rewrite the term as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41fed7cd-c7aa-4832-bf13-e2a3deccaca3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, the first terms take into account all possible transitions starting
    from *s[t]* (and ending in *s[t+1]*), while the second one is the value of each
    ending state. Therefore the complete expression becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9ccdae6-a592-489f-9c0c-01a5be43ee7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For a deterministic policy, instead, the formula is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3aa382e2-5c0e-4915-99d2-261a65c5a260.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous equations are particular cases of a generic discrete *Bellman
    equation* for a finite MDP that can be expressed as a vectorial operator, *L[π]*,
    applied to the value vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a8acd02-9687-4551-8cfa-1288d48e0adc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s easy to prove that there exists a unique fixed point that corresponds
    to *V(s; π)*, so *Lπ V(s; π) = V(s; π)*. However, in order to solve the system,
    we need to consider all equations at the same time because, both on the left-hand
    and on the right-hand side of the *Bellman equation*, there is the *V(•; π)* term.
    Is it possible to transform the problem into an iterative procedure, so that a
    previous computation can be exploited for the following one? The answer is yes
    and it''s the consequence of an important property of *L[π]*. Let''s consider
    the infinity norm of the difference between two value vectors computed at time
    *t* and *t+1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b0670783-6c0d-4f2a-8553-48da529179b2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the discount factor γ ∈ [0, 1[, the Bellman operator, *L[π]*, is a γ-contraction
    that reduces the distance between the arguments by a factor of γ (they get more
    and more similar). The *Banach Fixed-Point Theorem* states that a contraction, *L:
    D → D*, on a metric space, *D*, admits a unique fixed point, *d^* ∈ D*, that can
    be found by repeatedly applying the contraction to any *d^((0)) ∈ D*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, we know about the existence of a unique fixed point, *V(s; π)*, that
    is the goal of our research. If we now consider a generic starting point, *V^((t))*,
    and we compute the norm of the difference with *V(s; π)*, we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf2fbc9e-7282-4cda-b31c-40921fca6251.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Repeating this procedure iteratively until *t = 0*, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f23a51b-189b-4fc7-9fa1-778ced322b11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The term *γ^(t+1) →* *0*, while continuing the iterations over the distance
    between *V^((t))* and *V(s; π)*, gets smaller and smaller, authorizing us to employ
    the iterative approach instead of the one-shot closed method. Hence, the *Bellman
    equation* becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1e54c26-05bf-4af6-b868-05d18dcc5a71.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This formula allows us to find the value for each state (the step is formally
    called *policy evaluation*), but, of course, it requires a policy. At the first
    step, we can randomly select the actions because we don''t have any other piece
    of information, but after a complete evaluation cycle, we can start defining a
    greedy policy with respect to the values. In order to achieve this goal, we need
    to introduce a very important concept in RL, the *Q function*(which must not be
    confused with the *Q function* defined in the EM algorithm), which is defined
    as the expected discounted return obtained by an agent starting from the state, *s[t]*,
    and selecting a specific action, *a[t]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ca507797-275d-4403-a25d-36a984ab908c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The definition is very similar to *V(s; π)*, but, in this case, we include
    the action, a[t], as a variable. Clearly, it''s possible to define a Bellman equation
    for *Q(s, a; π)* by simply removing the policy/action summation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/febf2bce-8a9a-481a-964e-5d0face4df19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Sutton and Barto (in *Reinforcement Learning*, *Sutton R. S.,‎ Barto A. G.*, The
    MIT Press) proved a simple but very important theorem (called the *Policy improvement
    theorem*), which states that given the deterministic policies, *π[1]* and *π[2]*,
    if *Q(s, π[2](s); π[2]) ≥ V(s; π[1]) ∀ s ∈ S*, then *π[2]* is better than or equal
    to *π[1]*. The proof is very compact and can be found in their book, however,
    the result can be understood intuitively. If we consider a sequence of states, *s[1] →
    s[2] → ... → s[n]* and *π[2](s[i]) = π[1](s[i]) ∀ i < m < n*, while *π[2](s[i]) ≥
    π[1](s[i]) ∀ i ≥ m*, the policy, *π[2]*, is at least equal to *π[1]* and it''s
    become better if at least an inequality is strict. Conversely, if *Q(s, π[2](s); π[2]) ≥
    V(s; π[1])*, this means that *π[2](s) ≥ π[1](s)* and, again, *Q(s, π[2](s); π[2])
    > V(s; π[1])* if there''s at least a state, *s[i]*, where *π[2](s[i]) > π[1](s[i])*.
    Hence, after a complete policy evaluation cycle, we are authorized to define a
    new greedy policy as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6f0176d-da62-4f2e-9b0f-1e6677395158.png)'
  prefs: []
  type: TYPE_IMG
- en: This step is called *policy improvement* and its goal is to set the action associated
    with each state as the one that leads to the transition to the successor state
    with the maximum value. It's not difficult to understand that an optimal policy
    will remain stable when *V^((t)) → **V(s; π)*. In fact, when *t → ∞*, the Q function
    will converge to a stable fixed point determined by *V(s; π)* and the argmax(•)
    will always select the same actions. However, if we start with a random policy,
    in general, a single policy evaluation cycle isn't enough to assure the convergence.
    Therefore, after a policy improvement step, it's often necessary to repeat the
    evaluation and continue alternating the two phases until the policy becomes stable
    (that's why the algorithm is called policy iteration). In general, the convergence
    is quite fast, but the actual speed depends on the nature of the problem, the
    number of states and actions, and the consistency of the rewards.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete policy iteration algorithm (as proposed by Sutton and Barto) is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set an initial deterministic random policy *π(s)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial value array *V(s) = 0 ∀ s ∈ S*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a tolerance threshold Thr (for example, Thr = 0.0001)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a maximum number of iterations *N[iter]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a counter *e = 0*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *e < N[iter]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: e += 1
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Do:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *V[old](s) = V(s) ∀ s ∈ S*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a Policy Evaluation step reading the current value from *V[old](s)*
    and updating *V(s)*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: While *Avg(|V(s) - V[old](s)|) > Thr*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *π[old](s) = π(s) ∀ s ∈ S*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a policy improvement step
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If *π[old]**(s) == π(s)*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Break
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Output the final deterministic policy *π(s)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this case, as we have a full knowledge of the environment, there's no need
    for an exploration phase. The policy is always exploited as it's built to be greedy
    to the real value (obtained when t → ∞).
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration in the checkerboard environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We want to apply the policy iteration algorithm in order to find an optimal
    policy for the tunnel environment. Let''s start by defining a random initial policy
    and a value matrix with all values (except the terminal states) equal to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The initial random policy (t=0) is shown in the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5cf3aae5-0698-46d0-b37b-e578de590bcc.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial (t=0) random policy
  prefs: []
  type: TYPE_NORMAL
- en: 'The states denoted with ⊗ represent the wells, while the final positive one
    is represented by the capital letter *E*. Hence, the initial value matrix (t=0)
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d43a363e-87a2-42c2-ba00-4db6a92904f9.png)'
  prefs: []
  type: TYPE_IMG
- en: Initial (t=0) value matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we need to define the functions to perform the policy evaluation
    and improvement steps. As the environment is deterministic, the processes are
    slightly simpler because the generic transition probability, *T(s[i], s[j]; a[k])*,
    is equal to 1 for the only possible successor and 0 otherwise. In the same way,
    the policy is deterministic and only a single action is taken into account. The
    policy evaluation step is performed, freezing the current values and updating
    the whole matrix, *V^((t+1))*, with *V^((t))*; however, it''s also possible to
    use the new values immediately. I invite the reader to test both strategies in
    order to find the fastest way. In this example, we are employing a discount factor,
    γ = 0.9 (it goes without saying that an interesting exercise consists of testing
    different values and comparing the result of the evaluation process and the final
    behavior):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the functions have been defined, we start the policy iteration cycle (with
    a maximum number of epochs, *N[iter]* = 100,000, and a tolerance threshold equal
    to 10^(-5)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'At the end of the process (in this case, the algorithm converged after 182
    iterations, but this value can change with different initial policies), the value
    matrix is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2959dfcb-e800-4921-bc65-68566063961a.png)'
  prefs: []
  type: TYPE_IMG
- en: Final value matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'Analyzing the values, it''s possible to see how the algorithm discovered that
    they are an implicit function of the distance between a cell and the ending state.
    Moreover, the policy always avoids the wells because the maximum value is always
    found in an adjacent state. It''s easy to verify this behavior by plotting the
    final policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a1c460a-f0b9-4ed2-a650-536169c1cf7e.png)'
  prefs: []
  type: TYPE_IMG
- en: Final policy
  prefs: []
  type: TYPE_NORMAL
- en: Picking a random initial state, the agent will always reach the ending one,
    avoiding the wells and confirming the optimality of the policy iteration algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Value iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An alternative approach to policy iteration is provided by the *value iteration*
    algorithm. The main assumption is based on the empirical observation that the
    policy evaluation step converges rather quickly and it''s reasonable to stop the
    process after a fixed number of steps (normally 1). In fact, policy iteration
    can be imagined like a game where the first player tries to find the correct values
    considering a stable policy, while the other one creates a new policy that is
    greedy with respect to the new values. Clearly, the second step compromises the
    validity of the previous evaluation, forcing the first player to repeat the process.
    However, as the Bellman equation uses a single fixed point, the algorithm converges
    to a solution characterized by the fact that the policy doesn''t change anymore
    and, consequently, the evaluation becomes stable. This process can be simplified
    by removing the policy improvement step and continuing the evaluation in a greedy
    fashion. Formally, each step is based on the following update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/186e0f2f-9264-4455-8232-2095e3813a59.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the iteration doesn't consider the policy anymore (assuming implicitly that
    it will be greedy with respect to the values), and selects *V^((t+1))* as the
    maximum possible value among all *V^((t))(a[t])*. In other words, value iteration
    anticipates the choice that is made by the policy improvement step by selecting
    the value that corresponds to the action that is likely (p → 1) to be selected.
    It's not difficult to extend the convergence proof presented in the previous section
    to this case, therefore, *V^((∞)) → V^((opt))*, as well as policy iteration does.
    However, the average number of iterations is normally smaller because we are starting
    with a random policy that can contrast the value iteration process.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the values become stable, the optimal greedy policy is simply obtained
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19a2096e-6fa3-4062-aac7-ba08e39c17a2.png)'
  prefs: []
  type: TYPE_IMG
- en: This step is formally equivalent to a policy improvement iteration, which, however,
    is done only once at the end of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete value iteration algorithm (as proposed by Sutton and Barto) is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set the initial value array, *V(s) = 0 ∀ s ∈ S*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a tolerance threshold, *Thr*, (for example, *Thr = 0.0001*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a maximum number of iteration, *N[iter]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a counter, *e* = 0
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *e < N[iter]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*e += 1*'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Do:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *V[old](s) = V(s) ∀ s ∈ S*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform a value evaluation step reading the current value from *V[old](s)* and
    updating *V(s)*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: While *Avg(|V(s) - V[old](s)|) > Thr*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Output the final deterministic policy *π(s) = argmax[a] Q(s, a)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Value iteration in the checkerboard environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To test this algorithm, we need to set an initial value matrix with all values
    equal to 0 (they can be also randomly chosen but, as we don''t have any prior
    information on the final configuration, every initial choice is probabilistically
    equivalent):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can define the two functions to perform the value evaluation
    and the final policy selection (the function `is_final()` is the one defined in
    the previous example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The main differences are in the `value_evaluation()` function, which now has
    to consider all possible successor states and select the value corresponding to
    the action that leads to the state with the highest value. Instead, the `policy_selection()`
    function is equivalent to `policy_improvement()`, but, as it is invoked only once,
    it outputs directly to the final optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can run a training cycle (assuming the same constants as
    before):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The final value configuration (after 127 iterations) is shown in the following
    chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aad39ec1-85b2-491a-af13-66db44f7841c.png)'
  prefs: []
  type: TYPE_IMG
- en: Final value matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous example, the final value configuration is a function of
    the distance between each state and the ending one, but, in this case, the choice
    of *γ = 0.9* isn''t optimal. In fact, the wells close to the final state aren''t
    considered very dangerous anymore. Plotting the final policy can help us understand
    the behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44dc4df9-0c90-40d7-8bec-eccfd897d1ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Final policy
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, the wells that are far from the target are avoided, but the two
    that are close to the final state are accepted as reasonable penalties. This happens
    because the value iteration algorithm is very greedy with respect to the value
    and the discount factor, *γ < 1.0*; the effect of negative states can be compensated
    for by the final reward. In many scenarios, these states are absorbing, therefore
    their implicit reward is *+∞* or *-∞*, meaning that no other actions can change
    the final value. I invite the reader to repeat the example with different discount
    factors (remember that an agent with *γ → 1* is very short-sighted and will avoid
    any obstacle, even reducing the efficiency of the policy) and change the values
    of the final states. Moreover, the reader should be able to answer the question:
    What is the agent''s behavior when the standard reward (whose default value is
    *-0.1*) is increased or decreased?'
  prefs: []
  type: TYPE_NORMAL
- en: TD(0) algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the problems with Dynamic Programming algorithms is the need for a full
    knowledge of the environment in terms of states and transition probabilities.
    Unfortunately, there are many cases where these pieces of information are unknown
    before the direct experience. In particular, the states can be discovered by letting
    the agent explore the environment, but the transition probabilities require us
    to count the number of transitions to a certain state and this is often impossible.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, an environment with absorbing states can prevent visiting many states
    if the agent has learned a good initial policy. For example, in a game, which
    can be described as an episodic MDP, the agent discovers the environment while
    learning how to move forward without ending in a negative absorbing state.
  prefs: []
  type: TYPE_NORMAL
- en: 'A general solution to these problems is provided by a different evaluation
    strategy, called **Temporal Difference** (**TD**) RL. In this case, we start with
    an empty value matrix and we let the agent follow a greedy policy with respect
    to the value (but the initial one, which is generally random). Once the agent
    observes a transition, *s[i] → s[j]*, due to an action, *a[t]*, with a reward, *r[ij]*,
    it updates the estimation of *V(s[i])*. The process is structured in episodes
    (which is the most natural way) and ends when a maximum number of steps have been
    done or a terminal state is met. In particular, the TD(0) algorithm updates the
    value according to the rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8832ace7-a132-4c8a-96a8-7f7e0b719eb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The constant, *α*, is bound between 0 and 1 and acts as a learning rate. Each
    update considers a variation with respect to the current value, *V^((t))(s[i])*,
    which is proportional to the difference between the actual return and the previous
    estimation. The term *r[ij] + γV^((t))(s[j])* is analogous to the one employed
    in the previous methods and represents the expected value given the current return
    and the discounted value starting from the successor state. However, as *V^((t))(s[j])*
    is an estimation, the process is based on a bootstrap from the previous values.
    In other words, we start from an estimation to determine the next one, which should
    be closer to the stable fixed point. Indeed, TD(0) is the simplest example of
    a family of TD algorithms that are based on a sequence (usually called backup)
    that can be generalized as (considering k steps):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94db5eb2-cbcf-4559-9c1d-d11c3e3dcc18.png)'
  prefs: []
  type: TYPE_IMG
- en: As we're using a single reward to approximate the expected discounted return,
    TD(0) is usually called a one-step TD method (or one-step backup). A more complex
    algorithm can be built considering more subsequent rewards or alternative strategies.
    We're going to analyze a generic variant called TD(*λ*) in [Chapter 15](345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml),
    *Advanced Policy Estimation Algorithms* and explain why this algorithm corresponds
    to a choice of *λ = 0*.
  prefs: []
  type: TYPE_NORMAL
- en: 'TD(0) has been proven to converge, even if the proof (which can be found for
    a model-based approach in *Convergence of Model-Based Temporal Difference Learning
    for Control*, *Van Hasselt H.*, *Wiering M. A.*, *Proceedings of the 2007 IEEE
    Symposium on Approximate Dynamic **Programming and Reinforcement Learning* (ADPRL
    2007)) is more complex because it''s necessary to consider the evolution of the
    Markov Process. In fact, in this case, we are approximating the expected discounted
    return with both a truncated estimation and a bootstrap value, *V(s[j])*, which
    is initially (and for a large number of iterations) unstable. However, assuming
    the convergence for *t → ∞*, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a45126c-3a25-4e27-8279-352de7b6b902.png)'
  prefs: []
  type: TYPE_IMG
- en: The last formula expresses the value of the state, *s[i]*, assuming that the
    greedy optimal policy forces the agent to perform the action that causes the transition
    to *s[j]*. Of course, at this point, it's natural to ask under which conditions
    the algorithm converges. In fact, we are considering episodic tasks and the estimation, *V^((∞))(s[i])*,
    can be correct only if the agent performs a transition to *s[i]* an infinite number
    of times, selecting all possible actions an infinite number of times. Such a condition
    is often expressed by saying that the policy must be **Greedy in the Limit with
    Infinite Explorations** (**GLIE**). In other words, the real greediness is achieved
    only as an asymptotic state when the agent is able to explore the environment
    without limitations for an unlimited number of episodes.
  prefs: []
  type: TYPE_NORMAL
- en: This is probably the most important limitation of TD RL, because, in real-life
    scenarios, some states can be very unlikely and, hence, the estimation can never
    accumulate the experience needed to converge to the actual value. We are going
    to analyze some methods to solve this problem in [Chapter 15](345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml),
    *Advanced Policy Estimation Algorithms*, but, in our example, we employ a random
    start. In other words, as the policy is greedy and could always avoid some states,
    we force the agent to start each episode in a random nonterminal cell. In this
    way, we allow a deep exploration even with a greedy policy. Whenever this approach
    is not feasible (because, for example, the environment dynamics are not controllable),
    the exploration-exploitation dilemma can be solved only by employing an ε-greedy
    policy, which selects a fraction of suboptimal (or even wrong) actions. In this
    way, it's possible to observe a higher number of transitions paying the price
    of a slower convergence.
  prefs: []
  type: TYPE_NORMAL
- en: However, as pointed out by Sutton and Barto, TD(0) converges to the maximum-likelihood
    estimation of the value function determined by the MDP, finding the implicit transition
    probabilities of the model. Therefore, if the number of observations is high enough,
    TD(0) can quickly find an optimal policy, but, at the same time, it's also more
    sensitive to biased estimations if some couple's state-action are never experienced
    (or experienced very seldom). In our example, we don't know which the initial
    state is, hence selecting a fixed starting point yields a policy that is extremely
    rigid and almost completely unable to manage noisy situations. For example, if
    the starting point is changed to an adjacent (but never explored) cell, the algorithm
    could fail to find the optimal path to the positive terminal state. On the other
    hand, if we know that the dynamics are well-defined, TD(0) will force the agent
    to select the actions that are most likely to produce the optimal result given
    the current knowledge of the environment. If the dynamics are partially stochastic,
    the advantage of an ε-greedy policy can be understood considering a sequence of
    episodes where the agent experiences the same transitions and the corresponding
    values are increased proportionally. If, for example, the environment changes
    one transition after many experiences, the agent has to face a brand new experience
    when the policy is already almost stable. The correction requires many episodes
    and, as this random change has a very low probability, it's possible that the
    agent will never learn the correct behavior. Instead, by selecting a few random
    actions, the probability of encountering a similar state (or even the same one)
    increases (think about a game where the state is represented by a screenshot)
    and the algorithm can become more robust with respect to very unlikely transitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete TD(0) algorithm is:'
  prefs: []
  type: TYPE_NORMAL
- en: Set an initial deterministic random policy, *π(s)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the initial value array, *V(s) = 0 ∀ s ∈ S*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the number of episodes, *N[episodes]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a maximum number of steps per episode, *N[max]*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, *α* (for example, *α = 0.1*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, *γ* (for example, *γ** = 0.9*)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a counter, *e = 0*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i = 1* to *N[episodes]*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the initial state, *s[i]*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *s[j]* is non-terminal and *e < N[max]*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*e += 1*'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action, **a[t] = π(s[i])**
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the transition, *(a[t], s[i]) → (s[j], r[ij])*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value function for the state, *s[i]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *s[i] = s[j]*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the policy to be greedy with respect to the value function, *π(s) = argmax[a] Q(s,
    a)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: TD(0) in the checkerboard environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we can test the TD(0) algorithm on the checkerboard environment.
    The first step is to define an initial random policy and a value matrix with all
    elements equal to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As we want to select a random starting point at the beginning of each episode,
    we need to define a helper function that must exclude the terminal states (all
    the constants are the same as previously defined):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can implement the function to evaluate a single episode (setting the
    maximum number of steps equal to 500 and the constant to *α* = 0.25):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The function to determine the greedy policy with respect to the values is the
    same as already implemented in the previous examples; however, we report it to
    guarantee the consistency of the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can start a training cycle with 5,000 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The final value matrix is shown in the following chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c8f833f-bc0f-4df7-9bcf-44c337ab56e0.png)'
  prefs: []
  type: TYPE_IMG
- en: Final value matrix with random starts
  prefs: []
  type: TYPE_NORMAL
- en: 'Like in the previous examples, the final values are inversely proportional
    to the distance from the final positive state. Let''s analyze the resulting policy
    to understand whether the algorithm converged to a consistent solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/daddaf8e-2d83-4d14-8b34-9c36cee7499a.png)'
  prefs: []
  type: TYPE_IMG
- en: Final policy with random starts
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen, the random choice of the starting state is allowed to find
    the best path independently from the initial condition. To better understand the
    advantage of this strategy, let''s plot the final value matrix when the initial
    state is fixed to the cell *(0, 0)*, corresponding to the upper-left corner:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6397023e-d14b-418b-92c8-cdf5f2fd971e.png)'
  prefs: []
  type: TYPE_IMG
- en: Final value matrix with a fixed initial state (0, 0)
  prefs: []
  type: TYPE_NORMAL
- en: 'Without any further analysis, it''s possible to see that many states have never
    been visited or visited only a few times, and the resulting policy is therefore
    extremely greedy with respect to the specific initial state. The blocks containing
    values equal to -1.0 indicate states where the agent often has to pick a random
    action because there''s no difference in the values, hence it can be extremely
    difficult to solve the environment with a different initial state. The resulting
    policy confirms this analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92b1dadf-3967-4406-ab59-56adb216913f.png)'
  prefs: []
  type: TYPE_IMG
- en: Final policy with a fixed initial state (0, 0)
  prefs: []
  type: TYPE_NORMAL
- en: 'As it''s possible to see, the agent is able to reach the final state only when
    the initial point allows us to cross the trajectory starting from *(0, 0)*. In
    all these cases, it''s possible to recover the optimal policy, even if the paths
    longer than the ones obtained in the previous example. Instead, states such as *(0,
    4)* are clearly situations where there''s a *loss of policy*. In other words,
    the agent acts without any knowledge or awareness and the probability of success
    converges to 0\. As an exercise, I invite the reader to test this algorithm with
    different starting points (for example, a set of fixed ones) and higher *α* values.
    The goal is also to answer these questions: Is it possible to speed up the learning
    process? Is it necessary to start from all possible states in order to obtain
    a global optimal policy?'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the most important RL concepts, focusing on the
    mathematical structure of an environment as a Markov Decision Process, and on
    the different kinds of policy and how they can be derived from the expected reward
    obtained by an agent. In particular, we defined the value of a state as the expected
    future reward considering a sequence discounted by a factor, *γ*. In the same
    way, we introduced the concept of the Q function, which is the value of an action
    when the agent is in a specific state.
  prefs: []
  type: TYPE_NORMAL
- en: These concepts directly employed the policy iteration algorithm, which is based
    on a Dynamic Programming approach assuming complete knowledge of the environment.
    The task is split into two stages; during the first one, the agent evaluates all
    the states given the current policy, while in the second one, the policy is updated
    in order to be greedy with respect to the new value function. In this way, the
    agent is forced to always pick the action that leads to a transition that maximizes
    the obtained value.
  prefs: []
  type: TYPE_NORMAL
- en: We also analyzed a variant, called value iteration, that performs a single evaluation
    and selects the policy in a greedy manner. The main difference from the previous
    approach is that now the agent immediately selects the highest value assuming
    that the result of this process is equivalent to a policy iteration. Indeed, it's
    easy to prove that, after infinite transitions, both algorithms converge on the
    optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: The last algorithm is called TD(0) and it's based on a model-free approach.
    In fact, in many cases, it's difficult to know all the transition probabilities
    and, sometimes, even all possible states are unknown. This method is based on
    the Temporal Difference evaluation, which is performed directly while interacting
    with the environment. If the agent can visit all the states an infinite number
    of times (clearly, this is only a theoretical condition), the algorithm has been
    proven to converge to the optimal value function more quickly than other methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 15](345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml), *Advanced
    Policy Estimation Algorithms* we'll continue the discussion of RL algorithms,
    introducing some more advanced methods that can be immediately implemented using
    Deep Convolutional Networks.
  prefs: []
  type: TYPE_NORMAL
