- en: Introduction to Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: In this chapter, we are going to introduce the fundamental concepts of **Reinforcement
    Learning** (**RL**), which is a set of approaches that allows an agent to learn
    how to behave in an unknown environment, thanks to the rewards that are provided
    after each possible action. RL has been studied for decades, but it has reached
    a very high maturity level in the last few years when it became possible to employ
    deep learning models together with standard (and often simple) algorithms in order
    to solve extremely complex problems (such as learning how to play an Atari game
    perfectly).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们将介绍**强化学习**（**RL**）的基本概念，它是一组允许智能体学习如何在未知环境中行为的途径，这得益于在每个可能行动之后提供的奖励。强化学习已经研究了数十年，但在最近几年，它达到了一个非常成熟的水平，因为现在可以结合深度学习模型和标准（通常是简单）算法来解决极其复杂的问题（例如完美学习如何玩Atari游戏）。 '
- en: 'In particular, we will discuss:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是以下内容：
- en: The concepts of environment, agent, policy, and reward
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境、智能体、策略和奖励的概念
- en: The concept of the **Markov Decision Process** (**MDP**)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**（**MDP**）的概念'
- en: The policy iteration algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略迭代算法
- en: The value iteration algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价值迭代算法
- en: The TD(0) algorithm
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD(0)算法
- en: Reinforcement Learning fundamentals
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习基础
- en: 'Imagine that you want to learn to ride a bike and ask a friend for advice.
    They explain how the gears work, how to release the brake and a few other technical
    details. In the end, you ask the secret to keeping balanced. What kind of answer
    do you expect? In an imaginary supervised world, you should be able to perfectly
    quantify your actions and correct the errors by comparing the outcomes with precise
    reference values. In the real world, you have no idea about the quantities underlying
    your actions and, above all, you will never know what the right value is. Increasing
    the level of abstraction, the scenario we''re considering can be described as:
    a generic **agent** performs actions inside an **environment** and receives **feedback**
    that is somehow proportional to the competence of its actions.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你想要学习骑自行车，并向朋友寻求建议。他们解释了齿轮的工作原理，如何释放刹车以及一些其他技术细节。最后，你询问保持平衡的秘诀。你期待什么样的回答？在一个假想的监督世界中，你应该能够完美量化你的行动，并通过将结果与精确的参考值进行比较来纠正错误。在现实世界中，你对行动背后的数量一无所知，更重要的是，你永远不会知道正确的值是什么。提高抽象级别，我们考虑的场景可以描述为：一个通用的**智能体**在**环境**中执行行动，并接收某种程度与行动能力成比例的**反馈**。
- en: 'According to this **feedback**, the **agent** can correct its actions in order
    to reach a specific goal. This basic schema is represented in the following diagram:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个**反馈**，**智能体**可以纠正其行动以达到特定的目标。这个基本架构在以下图中表示：
- en: '![](img/d5f766b1-6137-4232-830c-af782674037b.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d5f766b1-6137-4232-830c-af782674037b.png)'
- en: Basic RL schema
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基本强化学习架构
- en: 'Returning to our initial example, when you ride a bike for the first time and
    try to keep your balance, you will notice that the wrong movement causes an increase
    in the slope, which in turn increases the horizontal component of the gravity
    force, pushing the bike laterally. As the vertical component is compensated, the
    result is a rotation that ends when the bike falls down completely. However, as
    you can use your legs to control the balance, when the bike starts falling, thanks
    to Newton''s third law, the force on the leg increases and your brain understands
    that it''s necessary to make a movement in the opposite direction. Even if this
    problem can be easily expressed in terms of physical laws, nobody learns to ride
    a bike by computing forces and momentums. This is one of the main concepts of
    RL: an agent must always make its choices considering a piece of information,
    usually defined as a *reward*, that represents the response, provided by the environment.
    If the action is correct, the reward will be positive, otherwise, it will be negative.
    After receiving a reward, an agent can fine-tune the strategy, called *policy*,
    in order to maximize the expected future reward. For example, after a few rides,
    you will be able to slightly move your body so as to keep the balance while turning,
    but probably, in the beginning, you needed to extend your leg to avoid falling
    down. Hence, your initial policy suggested a wrong action, which received repeated
    negative rewards and so your brain corrected it by increasing the probability
    of choosing another action. The implicit hypothesis that underlies this approach
    is that an agent is always *rational*, meaning that its goal is to maximize the
    expected return of its actions (nobody would like to fall down just to feel a
    different emotion).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们最初的例子，当你第一次骑自行车并试图保持平衡时，你会注意到错误动作会导致斜率的增加，这反过来又增加了重力力的水平分量，推动自行车向侧面移动。当垂直分量得到补偿时，结果是旋转，直到自行车完全倒下才会停止。然而，由于你可以用腿来控制平衡，当自行车开始倒下时，根据牛顿第三定律，腿上的力会增加，你的大脑会理解有必要做出相反方向的移动。即使这个问题可以用物理定律轻松表达，但没有人通过计算力和动量来学习骑自行车。这是强化学习的主要概念之一：智能体必须始终根据一部分信息做出选择，这通常被定义为*奖励*，它代表了环境提供的响应。如果动作是正确的，奖励将是正的，否则，将是负的。在接收到奖励后，智能体可以微调称为*策略*的策略，以最大化预期的未来奖励。例如，骑了几次车后，你将能够稍微移动身体以在转弯时保持平衡，但可能，在开始时，你需要伸出腿来避免摔倒。因此，你最初的政策建议了一个错误动作，它收到了重复的负面奖励，所以你的大脑通过增加选择另一个动作的概率来纠正它。这个方法背后的隐含假设是，智能体总是*理性的*，这意味着它的目标是最大化其动作的预期回报（没有人愿意为了感受不同的情绪而摔倒）。
- en: Before discussing the single components of an RL system, it's necessary to add
    a couple of fundamental assumptions. The first one is that an agent can repeat
    the experiences an infinite number of times. In other words, we assume that it's
    possible to learn a valid policy (possibly the optimal one) only if we have enough
    time. Clearly, this is unacceptable in the animal world and we all know that many
    experiences are extremely dangerous; however, this assumption is necessary to
    prove the convergence of some algorithms. Indeed, sub-optimal policies sometimes
    can be learned very quickly, but it's necessary to iterate many times to reach
    the optimal one. In real artificial systems, we always stop the learning process
    after a finite number of iterations, but it's almost impossible to find valid
    solutions if some experiences prevent the agent from continuing to interact with
    the environment. As many tasks have final states (either positive or negative),
    we assume that the agent can play any number of *episodes* (somewhat analogous
    to the epochs of supervised learning), exploiting the experience previously learned.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论强化学习系统的单个组件之前，有必要添加几个基本假设。第一个假设是，一个智能体可以无限次地重复其经验。换句话说，我们假设只有当我们有足够的时间时，才可能学习到一个有效的策略（可能是最优策略）。显然，这在动物世界中是不可接受的，我们都知道许多经验是非常危险的；然而，这个假设对于证明某些算法的收敛性是必要的。确实，次优策略有时可以非常快速地学习到，但要达到最优策略，通常需要多次迭代。在现实的人工系统中，我们总是在有限次迭代后停止学习过程，但如果某些经验阻止智能体继续与环境交互，那么几乎不可能找到有效的解决方案。由于许多任务都有最终状态（无论是积极的还是消极的），我们假设智能体可以玩任意数量的*回合*（某种程度上类似于监督学习的时代），利用之前学习到的经验。
- en: 'The second assumption is a little bit more technical and it''s usually known
    as the *Markov property*. When the agent interacts with the environment, it observes
    a sequence of states. Even if it can seem like an oxymoron, we assume that each
    state is stateful. We can explain this concept with a simple example; suppose
    that you''re filling a tank and every five seconds you measure the level. Imagine
    that at *t = 0*, the level *L = 10* and the water is flowing in. What do you expect
    at *t = 1*? Obviously, *L > 10*. In other words, without external unknown causes,
    we assume that a state contains the previous history, so that the sequence, even
    if discretized, represents a continuous evolution where no jumps are allowed.
    When an RL task satisfies this property, it''s called a Markov Decision Process and
    it''s very easy to employ simple algorithms to evaluate the actions. Luckily,
    the majority of natural events can be modeled as MDPs (when you''re walking toward
    a door, every step in the right direction must decrease the distance), but there
    are some games that are implicitly stateless. For example, if you want to employ
    an RL algorithm to learn how to guess the outcome of a probabilistic sequence
    of independent events (such as tossing a coin), the result could be dramatically
    wrong. The reason is clear: any state is independent of the previous ones and
    every attempt to build up a history is a failure. Therefore, if you observe a
    sequence of *0, 0, 0, 0, ...* you are not justified in increasing the value of
    betting on 0 unless, after considering the likelihood of the events, you suppose
    that the coin is loaded. However, if there''s no reason to do so, the process
    isn''t an MDP and every episode (event) is completely independent. All the assumptions
    that we, either implicitly or explicitly, make are based on this fundamental concept,
    so pay attention when evaluating new, unusual scenarios because you may discover
    that the employment of a specific algorithm isn''t theoretically justified.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个假设稍微有点技术性，通常被称为*马尔可夫性质*。当智能体与环境交互时，它会观察到一系列状态。即使这可能看起来有些矛盾，我们假设每个状态都是状态化的。我们可以用一个简单的例子来解释这个概念；假设你在填充一个水箱，每五秒钟你测量一次水位。想象一下，在
    *t = 0* 时，水位 *L = 10*，水正在流入。你在 *t = 1* 时期待什么？显然，*L > 10*。换句话说，在没有外部未知原因的情况下，我们假设一个状态包含其先前历史，因此，即使序列是离散化的，它也代表了一个不允许跳跃的连续演变。当一个RL任务满足这个属性时，它被称为马尔可夫决策过程，并且很容易使用简单的算法来评估动作。幸运的是，大多数自然事件都可以建模为MDP（当你朝着门走时，每一步向右的方向都必须减少距离），但有些游戏是隐式无状态的。例如，如果你想使用RL算法来学习如何猜测一系列独立事件的概率结果（例如抛硬币），结果可能会非常错误。原因很清楚：任何状态都与先前状态无关，并且任何试图建立历史的尝试都是失败的。因此，如果你观察到一系列
    *0, 0, 0, 0, ...*，你无权增加对0下注的价值，除非在考虑事件的可能性后，你假设硬币是作弊的。然而，如果没有理由这样做，这个过程就不是MDP，每个场景（事件）都是完全独立的。我们所做的所有假设，无论是隐式还是显式，都是基于这个基本概念，所以在评估新的、不寻常的场景时要注意，因为你可能会发现使用特定算法的理论依据是不成立的。
- en: Environment
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境
- en: 'The **environment** is the entity where the **agent** has to reach its goals.
    For our purposes, a generic environment is a system that receives an input action, *a[t]*
    (we use the index *t* because this is a natural time process), and outputs a tuple
    composed by a state, *s[t+1]*, and a reward, *r[t+1]*. These two elements are
    the only pieces of information provided to the agent to make its next decision.
    If we are working with an MDP and the sets of possible actions, A, and states,
    S, are discrete and finite, the problem is a defined finite MDP (in many continuous
    cases, it''s possible to treat the problem as a finite MDP by discretizing the
    spaces). If there are final states, the task is called *episodic* and, in general,
    the goal is to reach a positive final state in the shortest amount of time or
    maximize a score. The schema of the cyclic interaction between agent an environment
    is shown in the following diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**环境**是**智能体**必须达到其目标的空间实体。就我们的目的而言，一个通用环境是一个接收输入动作，*a[t]*（我们使用索引 *t* 因为这是一个自然的时间过程），并输出一个由状态，*s[t+1]*，和奖励，*r[t+1]*组成的元组的系统。这两个元素是提供给智能体以做出其下一步决策的唯一信息。如果我们正在处理一个MDP，并且可能动作的集合，A，和状态的集合，S，是离散且有限的，那么问题是一个定义明确的有限MDP（在许多连续情况下，可以通过离散化空间将问题视为有限MDP）。如果有最终状态，该任务被称为*episodic*，通常，目标是在最短的时间内达到一个正的最终状态或最大化分数。智能体和环境之间循环交互的方案如下所示：'
- en: '![](img/e7f8fb93-5d5f-4d1f-9040-e19abef9fd6a.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e7f8fb93-5d5f-4d1f-9040-e19abef9fd6a.png)'
- en: Agent-environment interaction schema
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 代理-环境交互模式
- en: 'A very important feature of an environment is its internal nature. It can be
    either *deterministic* or *stochastic*. A deterministic environment is characterized
    by a function that associates each possible action, *a[t]*, in a specific state, *s[t]*,
    to a well-defined successor, *s*[*t+1*], with a precise reward, *r[t+1]*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 环境的一个非常重要的特征是其内部性质。它可以是有序的或随机的。一个有序的环境由一个函数定义，该函数将每个可能动作，*a[t]*，在特定状态，*s[t]*，关联到一个明确的后继状态，*s*[*t+1*]，以及一个精确的奖励，*r[t+1]*：
- en: '![](img/a85f55e3-9b7d-4101-8345-5c39db4ad788.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a85f55e3-9b7d-4101-8345-5c39db4ad788.png)'
- en: 'Conversely, a stochastic environment is characterized by a transition probability
    between the current state, *s[t]*, and a set of possible successors, *s^i[t+1]*,
    given an action, *a[t]*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，一个随机环境由给定动作，*a[t]*，当前状态，*s[t]*，和一组可能的后继状态，*s^i[t+1]*，之间的转移概率来定义：
- en: '![](img/c5824d4e-6559-437e-a8d0-724d4a324541.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c5824d4e-6559-437e-a8d0-724d4a324541.png)'
- en: If a state, *s[i]*, has a transitional probability, *T(s[i], s[i], a[t]) = 1 ∀
    a[t ]∈ A*, the state is defined as *absorbing*. In general, all ending states
    in episodic tasks are modeled as absorbing ones, to avoid any further transition.
    When an episode is not limited to a fixed number of steps, the only criterion
    to determine its end is to check whether the agent has reached an absorbing state.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个状态，*s[i]*，有一个转移概率，*T(s[i], s[i], a[t]) = 1 ∀ a[t ]∈ A*，则该状态被定义为*吸收状态*。通常，所有在序列任务中的结束状态都被建模为吸收状态，以避免任何进一步的转换。当一个序列不被限制在固定数量的步骤时，确定其结束的唯一标准是检查代理是否达到了吸收状态。
- en: 'As we don''t know which state will be the successor, it''s necessary to consider
    the expected value of all possible rewards considering the initial state, *s[t]*,
    and the action, *a[t]*:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不知道哪个状态将是后续状态，因此有必要考虑在初始状态，*s[t]*，和动作，*a[t]*，下所有可能奖励的期望值：
- en: '![](img/5fbf6f10-762b-4510-a62f-9de83b98f0a2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5fbf6f10-762b-4510-a62f-9de83b98f0a2.png)'
- en: In general, it's easier to manage stochastic environments because they can be
    immediately converted into deterministic ones by setting all probabilities to
    zero except the one corresponding to the actual successor (for example, T(•) =
    (0, 0, ..., 1, ..., 0)). In the same way, the expected return can be set equal
    to *r[t+1]*. The knowledge of T(•), as well as *E[r^i[t+1]]*, is necessary to
    employ some specific algorithms, but it can become problematic when finding a
    suitable model for the environment requires an extremely complex analysis. In
    all those cases, model-free methods can be employed and, therefore, the environment
    is considered as a black-box, whose output at time, *t* (subsequent to an action
    performed by the agent, *a[t-1]*), is the only available piece of information
    for the evaluation of a policy.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，管理随机环境更容易，因为可以通过将所有概率设置为0（除了对应实际后继状态的概率）立即将其转换为有序环境（例如，T(•) = (0, 0, ...,
    1, ..., 0)）。同样，期望回报可以设置为*r[t+1]*。了解T(•)以及*E[r^i[t+1]]*对于应用某些特定算法是必要的，但当需要极其复杂的分析来为环境找到一个合适的模型时，可能会出现问题。在这些所有情况下，可以使用无模型方法，因此环境被视为一个黑盒，其输出在时间，*t*（在代理执行动作，*a[t-1]*之后），是评估策略的唯一可用信息。
- en: Rewards
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励
- en: 'We have seen that rewards (sometimes negative rewards are called *penalties*,
    but it''s preferable to use a standardized notation) are the only feedback provided
    by the environment after each action. However, there are two different approaches
    to the use of rewards. The first one is the strategy of a very short-sighted agent
    and consists in taking into account only the reward just received. The main problem
    with this approach is clearly the inability to consider longer sequences that
    can lead to a very high reward. For example, an agent has to traverse a few states
    with negative reward (for example, -0.1), but after them, they arrive at a state
    with a very positive reward (for example, +5.0). A short-sighted agent couldn''t 
    find out the best policy because it will simply try to avoid the immediate negative
    rewards. On the other side, it''s better to suppose that a single reward contains
    a part of the future rewards that will be obtained following the same policy.
    This concept can be expressed by introducing a *discounted reward*, which is defined
    as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，奖励（有时负奖励被称为*惩罚*，但最好使用标准化的符号）是环境在每次行动后提供的唯一反馈。然而，使用奖励的方法有两种不同的途径。第一种是一个非常短视的代理的策略，只考虑刚刚收到的奖励。这种方法的明显问题是无法考虑可能导致非常高的奖励的更长序列。例如，一个代理必须穿越几个具有负奖励的状态（例如，-0.1），但之后，他们到达一个具有非常积极奖励的状态（例如，+5.0）。短视的代理无法找到最佳策略，因为它只会试图避免立即的负奖励。另一方面，最好假设单个奖励包含未来将获得的奖励的一部分，这些奖励将遵循相同的策略。这个概念可以通过引入*折现奖励*来表示，它被定义为：
- en: '![](img/8473c1dd-1fbc-4804-8cd5-7620a2d93b15.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8473c1dd-1fbc-4804-8cd5-7620a2d93b15.png)'
- en: 'In the previous expression, we are assuming an infinite horizon with a discount
    factor, γ, which is a real number bounded between 0 and 1 (not included). When γ
    = 0, the agent is extremely short-sighted, because of *R[t] = r[t+1]*, but when *γ →*
    1, the current reward takes into account the future contributions discounted in
    a way that is inversely proportional to the time-step. In this way, very close
    rewards will have a higher weight than very distant ones. If the absolute value
    of all rewards is limited by a maximum immediate absolute reward, *|r[i]| ≤ |r[max]|*,
    the previous expression will be always bounded. In fact, considering the properties
    of a geometric series, we get:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，我们假设了一个具有折现因子γ的无穷远视角，γ是一个介于0和1之间的实数（不包括）。当γ = 0时，代理非常短视，因为*R[t] = r[t+1]*，但当*γ*趋近于1时，当前奖励考虑了以时间步长成反比的方式折现的未来贡献。这样，非常接近的奖励将比非常遥远的奖励有更高的权重。如果所有奖励的绝对值都受限于最大即时绝对奖励*|r[i]|
    ≤ |r[max]|*，则前面的表达式将始终有界。事实上，考虑到几何级数的性质，我们得到：
- en: '![](img/2819bbb5-a656-419b-a7a4-0167fe6c9af9.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2819bbb5-a656-419b-a7a4-0167fe6c9af9.png)'
- en: 'Clearly, the right choice of γ is a crucial factor in many problems and cannot
    be easily generalized. As in many other similar cases, I suggest testing different
    values, picking the one that minimizes the convergence speed while yielding a
    quasi-optimal policy. Of course, if the tasks are episodic with length, *T(e[i])*,
    the discounted reward becomes:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，γ的正确选择是许多问题的关键因素，并且不能轻易推广。正如在许多其他类似情况下一样，我建议测试不同的值，选择那个最小化收敛速度同时产生近似最优策略的值。当然，如果任务是具有长度T(e[i])的周期性任务，折现奖励变为：
- en: '![](img/4d0ea980-a9a0-4c24-be1c-93ceccf4076a.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4d0ea980-a9a0-4c24-be1c-93ceccf4076a.png)'
- en: Checkerboard environment in Python
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的棋盘环境
- en: 'We are going to consider an example based on a checkerboard environment representing
    a tunnel. The goal of the agent is to reach the ending state (lower-right corner),
    avoiding 10 wells that are negative absorbing states. The rewards are:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将考虑一个基于代表隧道的棋盘环境的例子。代理的目标是到达结束状态（右下角），避免10个负吸收状态的水井。奖励如下：
- en: '**Ending state**: +5.0'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结束状态**：+5.0'
- en: '**Wells**: -5.0'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**韦尔斯**：-5.0'
- en: '**All other states**: -0.1'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**所有其他状态**：-0.1'
- en: 'Selecting a small negative reward for all non-terminal states is helpful to
    force the agent to move forward until the maximum (final) reward has been achieved.
    Let''s start modeling an environment that has a 5 × 15 matrix:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为所有非终止状态选择一个小负奖励有助于迫使代理前进，直到达到最大（最终）奖励。让我们开始建模一个具有5×15矩阵的环境：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The graphical representation of the environment (in terms of rewards) is shown
    in the following chart:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 环境的图形表示（就奖励而言）如下表所示：
- en: '![](img/3d68b884-208b-479e-8aaf-cbcfd3cf630a.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3d68b884-208b-479e-8aaf-cbcfd3cf630a.png)'
- en: Rewards in the tunnel environment
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 洞穴环境中的奖励
- en: 'The agent is allowed to move in four directions: up, down, left, and right.
    Clearly, in this case, the environment is deterministic because every action moves
    the agent to a predefined cell. We assume that whenever an action is forbidden
    (such as trying to move on the left when the agent is in the first column), the
    successor state is the same one (with the corresponding reward).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 代理允许在四个方向上移动：上、下、左和右。显然，在这种情况下，环境是确定的，因为每个动作都会将代理移动到预定义的单元格。我们假设，每当一个动作被禁止（例如，当代理在第一列时尝试向左移动），后续状态是相同的（带有相应的奖励）。
- en: Policy
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 策略
- en: 'A *policy* is formally a deterministic or stochastic law that the agent follows
    in order to maximize its return. Conventionally, all policies are denoted with
    the letter *π*. A *deterministic policy* is usually a function of the current
    state that outputs a precise action:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *策略* 正式上是代理遵循的确定或随机法则，以最大化其回报。传统上，所有策略都用字母 *π* 表示。一个 *确定策略* 通常是一个当前状态的函数，输出一个精确的动作：
- en: '![](img/162f2b33-d17f-40f1-bbf9-508873a21c95.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/162f2b33-d17f-40f1-bbf9-508873a21c95.png)'
- en: 'A *stochastic policy*, analogously to environments, outputs the probability
    of each action (in this case, we are assuming we work with a finite MPD):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *随机策略*，类似于环境，输出每个动作的概率（在这种情况下，我们假设我们与一个有限的MPD工作）：
- en: '![](img/f34fa72a-a348-41f8-ab78-623ed830cc93.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f34fa72a-a348-41f8-ab78-623ed830cc93.png)'
- en: However, contrary to the environment, an agent must always pick a specific action,
    transforming any stochastic policy into a deterministic sequence of choices. In
    general, a policy where *π(s, a) > 0 ∀ a ∈ A*, is called *soft* and it's often
    very useful during the training process because it allows a more flexible modeling
    without the premature selection of a suboptimal action. Instead, when *π(s, a[i])
    = 0 ∀ i ≠ j* and *π(s, a[j]) = 1*, the policy is also defined as *hard*. This
    transformation can be performed in many ways, but the most common one is to define
    a policy that is greedy with respect to a value (we're going to discuss this concept
    in the next section). This means that, at every step, the policy will select the
    action that maximizes the value of the successor state. Obviously, this is a very
    rational approach, which could be too pragmatic. In fact, when the values of some
    states don't change, a greedy policy will always force the agent to perform the
    same actions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与环境相反，代理必须始终选择一个特定的动作，将任何随机策略转化为一系列确定的选择。一般来说，当 *π(s, a) > 0 ∀ a ∈ A* 的策略被称为
    *软*，它在训练过程中非常有用，因为它允许更灵活的建模，而不会提前选择次优动作。相反，当 *π(s, a[i]) = 0 ∀ i ≠ j* 和 *π(s,
    a[j]) = 1* 时，该策略也被定义为 *硬*。这种转换可以以多种方式执行，但最常见的一种是定义一个相对于价值的贪婪策略（我们将在下一节讨论这个概念）。这意味着在每一步，策略将选择最大化后续状态价值的动作。显然，这是一个非常理性的方法，可能会过于实用。事实上，当某些状态的价值没有变化时，贪婪策略将始终迫使代理执行相同的动作。
- en: Such a problem is known as the *exploration-exploitation dilemma* and arises
    when it would be better to allow the agent to evaluate alternative strategies
    that could appear initially to be suboptimal. In other words, we want the agent
    to explore the environment before starting to exploit the policy, to know whether
    the policy is really the best one or if there are hidden alternatives. To solve
    this problem, it's possible to employ an *ε-greedy policy*, where the value, *ε*,
    is called the *exploration factor* and represents a probability. In this case,
    the policy will pick a random action with probability *ε* and a greedy one with
    probability *1 - ε*. In general, at the beginning of the training process, *ε*
    is kept very close to 1.0 to incentivize the exploration and it's progressively decreased
    when the policy becomes more stable. In many Deep RL applications, this approach
    is fundamental, in particular, when there are no models of the environment. The
    reason is that greedy policies can be initially wrong and it's necessary to allow
    the agent to explore many possible state and action sequences before forcing a
    deterministic decision.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这种问题被称为*探索-利用困境*，当允许智能体评估最初可能看起来次优的替代策略时会出现。换句话说，我们希望智能体在开始利用策略之前探索环境，以确定策略是否真的是最好的，或者是否存在隐藏的替代方案。为了解决这个问题，可以采用*ε-贪婪策略*，其中值*ε*被称为*探索因子*，代表一个概率。在这种情况下，策略将以概率*ε*随机选择一个动作，以概率*1
    - ε*选择贪婪的动作。通常，在训练过程的开始阶段，*ε*保持非常接近1.0，以鼓励探索，并且随着策略变得更加稳定，它逐渐减少。在许多深度强化学习应用中，这种方法是基本的，特别是在没有环境模型的情况下。原因是贪婪策略最初可能是错误的，并且有必要允许智能体在强制做出确定性决策之前探索许多可能的状态和动作序列。
- en: Policy iteration
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 政策迭代
- en: 'In this section, we are going to analyze a strategy to find an optimal policy
    based on a complete knowledge of the environment (in terms of transition probability
    and expected returns). The first step is to define a method that can be employed
    to build a greedy policy. Let''s suppose we''re working with a finite MDP and
    a generic policy, π; we can define the intrinsic value of a state, *s[t]*, as
    the expected discounted return obtained by the agent starting from *s[t]* and
    following the stochastic policy, *π*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将分析一种基于对环境（就转移概率和期望回报而言）的完整知识来寻找最优策略的策略。第一步是定义一种可以用来构建贪婪策略的方法。假设我们正在处理一个有限的MDP和一个通用策略π；我们可以定义状态*s[t]*的内在价值为智能体从状态*s[t]*开始并遵循随机策略π所获得的期望折现回报：
- en: '![](img/38a56381-2dab-457c-a2ca-9e2241245ce8.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/38a56381-2dab-457c-a2ca-9e2241245ce8.png)'
- en: In this case, we are assuming that, as the agent will follow *π*, state *s[a]*
    is more useful than *s[b]* if the expected return starting from *s[a]* is greater
    than the one obtained starting from s[b]. Unfortunately, trying to directly find
    the value of each state using the previous definition is almost impossible when *γ
    > 0*. However, this a problem that can be solved using Dynamic Programming (for
    further information, please refer to *Dynamic Programming and Markov Process*,
    *Ronald A. Howard*, The MIT Press), which allows us to solve the problem iteratively.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们假设，由于智能体将遵循*π*，如果从状态*s[a]*开始获得的期望回报大于从状态s[b]开始获得的回报，那么状态*s[a]*比状态*s[b]*更有用。不幸的是，当*γ
    > 0*时，试图直接使用之前的定义找到每个状态的价值几乎是不可能的。然而，这是一个可以用动态规划（有关更多信息，请参阅*动态规划与马尔可夫过程*，*罗纳德·A·霍华德*，麻省理工学院出版社）解决的问题，它允许我们迭代地解决这个问题。
- en: 'In particular, we need to turn the previous formula into a *Bellman equation*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是我们需要将之前的公式转换成*贝尔曼方程*：
- en: '![](img/2551adf2-6d67-424c-8d66-f83ed1e7ce83.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2551adf2-6d67-424c-8d66-f83ed1e7ce83.png)'
- en: 'The first term on the right-hand side can be expressed as:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 右侧的第一个项可以表示为：
- en: '![](img/2c30e3ba-5d6a-43ac-815b-786086908258.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2c30e3ba-5d6a-43ac-815b-786086908258.png)'
- en: 'In other words, it is the weighted average of all expected returns considering
    that the agent is state, *s[t]*, and evaluates all possible actions and the consequent
    state transitions. For the second term, we need a small trick. Let''s suppose
    we start from *s[t+1]*, so that the expected value corresponds to *V(s[t+1];π)*;
    however, as the sum starts from *s[t]*, we need to consider all possible transitions
    starting from *s[t]*. In this case, we can rewrite the term as:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，它是考虑代理处于状态，*s[t]*，并评估所有可能动作及其后续状态转移的所有预期回报的加权平均值。对于第二项，我们需要一个小技巧。假设我们从*s[t+1]*开始，因此预期值对应于*V(s[t+1];π)*；然而，由于和从*s[t]*开始，我们需要考虑从*s[t]*开始的所有可能的转移。在这种情况下，我们可以将这个项重写为：
- en: '![](img/41fed7cd-c7aa-4832-bf13-e2a3deccaca3.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/41fed7cd-c7aa-4832-bf13-e2a3deccaca3.png)'
- en: 'Again, the first terms take into account all possible transitions starting
    from *s[t]* (and ending in *s[t+1]*), while the second one is the value of each
    ending state. Therefore the complete expression becomes:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，第一项考虑了从*s[t]*（并最终到达*s[t+1]*）开始的所有可能的转移，而第二项是每个结束状态的价值。因此，完整的表达式变为：
- en: '![](img/e9ccdae6-a592-489f-9c0c-01a5be43ee7b.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9ccdae6-a592-489f-9c0c-01a5be43ee7b.png)'
- en: 'For a deterministic policy, instead, the formula is:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于确定性策略，公式是：
- en: '![](img/3aa382e2-5c0e-4915-99d2-261a65c5a260.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3aa382e2-5c0e-4915-99d2-261a65c5a260.png)'
- en: 'The previous equations are particular cases of a generic discrete *Bellman
    equation* for a finite MDP that can be expressed as a vectorial operator, *L[π]*,
    applied to the value vector:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的方程是针对有限马尔可夫决策过程（MDP）的通用离散**贝尔曼方程**的特殊情况，该方程可以表示为一个向量算子，即*L[π]*，作用于价值向量：
- en: '![](img/1a8acd02-9687-4551-8cfa-1288d48e0adc.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1a8acd02-9687-4551-8cfa-1288d48e0adc.png)'
- en: 'It''s easy to prove that there exists a unique fixed point that corresponds
    to *V(s; π)*, so *Lπ V(s; π) = V(s; π)*. However, in order to solve the system,
    we need to consider all equations at the same time because, both on the left-hand
    and on the right-hand side of the *Bellman equation*, there is the *V(•; π)* term.
    Is it possible to transform the problem into an iterative procedure, so that a
    previous computation can be exploited for the following one? The answer is yes
    and it''s the consequence of an important property of *L[π]*. Let''s consider
    the infinity norm of the difference between two value vectors computed at time
    *t* and *t+1*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 证明存在一个唯一的固定点对应于*V(s; π)*是容易的，因此*Lπ V(s; π) = V(s; π)*。然而，为了解这个系统，我们需要同时考虑所有方程，因为在**贝尔曼方程**的左右两边都有*V(•;
    π)*项。能否将问题转化为一个迭代过程，以便利用前一次的计算结果进行下一次计算？答案是肯定的，这是**L[π]**的一个重要属性的后果。让我们考虑在时间*t*和*t+1*计算的两个价值向量之间的无穷范数：
- en: '![](img/b0670783-6c0d-4f2a-8553-48da529179b2.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0670783-6c0d-4f2a-8553-48da529179b2.png)'
- en: 'As the discount factor γ ∈ [0, 1[, the Bellman operator, *L[π]*, is a γ-contraction
    that reduces the distance between the arguments by a factor of γ (they get more
    and more similar). The *Banach Fixed-Point Theorem* states that a contraction, *L:
    D → D*, on a metric space, *D*, admits a unique fixed point, *d^* ∈ D*, that can
    be found by repeatedly applying the contraction to any *d^((0)) ∈ D*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '当折现因子γ ∈ [0, 1]时，贝尔曼算子*L[π]*是一个γ-收缩，通过γ因子减少参数之间的距离（它们变得越来越相似）。**Banach不动点定理**指出，在度量空间*D*上的收缩*L:
    D → D*，在*D*中有一个唯一的固定点*d^* ∈ D*，可以通过反复应用收缩到任何*d^((0)) ∈ D*来找到。'
- en: 'Hence, we know about the existence of a unique fixed point, *V(s; π)*, that
    is the goal of our research. If we now consider a generic starting point, *V^((t))*,
    and we compute the norm of the difference with *V(s; π)*, we obtain:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们知道存在一个唯一的固定点*V(s; π)*，这是我们研究的目标。如果我们现在考虑一个通用的起始点*V^((t))*，并计算与*V(s; π)*之间的范数，我们得到：
- en: '![](img/bf2fbc9e-7282-4cda-b31c-40921fca6251.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bf2fbc9e-7282-4cda-b31c-40921fca6251.png)'
- en: 'Repeating this procedure iteratively until *t = 0*, we get:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 重复此过程，直到*t = 0*，我们得到：
- en: '![](img/1f23a51b-189b-4fc7-9fa1-778ced322b11.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f23a51b-189b-4fc7-9fa1-778ced322b11.png)'
- en: 'The term *γ^(t+1) →* *0*, while continuing the iterations over the distance
    between *V^((t))* and *V(s; π)*, gets smaller and smaller, authorizing us to employ
    the iterative approach instead of the one-shot closed method. Hence, the *Bellman
    equation* becomes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 项*γ^(t+1) →* *0*，同时继续迭代*V^((t))*和*V(s; π)*之间的距离，这个距离越来越小，允许我们使用迭代方法而不是一次性闭合方法。因此，**贝尔曼方程**变为：
- en: '![](img/c1e54c26-05bf-4af6-b868-05d18dcc5a71.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c1e54c26-05bf-4af6-b868-05d18dcc5a71.png)'
- en: 'This formula allows us to find the value for each state (the step is formally
    called *policy evaluation*), but, of course, it requires a policy. At the first
    step, we can randomly select the actions because we don''t have any other piece
    of information, but after a complete evaluation cycle, we can start defining a
    greedy policy with respect to the values. In order to achieve this goal, we need
    to introduce a very important concept in RL, the *Q function*(which must not be
    confused with the *Q function* defined in the EM algorithm), which is defined
    as the expected discounted return obtained by an agent starting from the state, *s[t]*,
    and selecting a specific action, *a[t]*:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式使我们能够找到每个状态（这个步骤正式称为*策略评估*）的值，但当然，它需要一个策略。在第一步，我们可以随机选择动作，因为我们没有其他任何信息，但在完成评估周期后，我们可以开始根据值定义一个贪婪策略。为了实现这个目标，我们需要引入强化学习中的一个非常重要的概念，即*Q函数*（必须与EM算法中定义的*Q函数*区分开来），它被定义为从状态，*s[t]*，开始并选择一个特定动作，*a[t]*，的智能体获得的期望折现回报：
- en: '![](img/ca507797-275d-4403-a25d-36a984ab908c.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ca507797-275d-4403-a25d-36a984ab908c.png)'
- en: 'The definition is very similar to *V(s; π)*, but, in this case, we include
    the action, a[t], as a variable. Clearly, it''s possible to define a Bellman equation
    for *Q(s, a; π)* by simply removing the policy/action summation:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 定义与*V(s; π)*非常相似，但在这个情况下，我们将动作，a[t]，作为一个变量。显然，我们可以通过简单地移除策略/动作求和来定义*Q(s, a;
    π)*的贝尔曼方程：
- en: '![](img/febf2bce-8a9a-481a-964e-5d0face4df19.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/febf2bce-8a9a-481a-964e-5d0face4df19.png)'
- en: 'Sutton and Barto (in *Reinforcement Learning*, *Sutton R. S.,‎ Barto A. G.*, The
    MIT Press) proved a simple but very important theorem (called the *Policy improvement
    theorem*), which states that given the deterministic policies, *π[1]* and *π[2]*,
    if *Q(s, π[2](s); π[2]) ≥ V(s; π[1]) ∀ s ∈ S*, then *π[2]* is better than or equal
    to *π[1]*. The proof is very compact and can be found in their book, however,
    the result can be understood intuitively. If we consider a sequence of states, *s[1] →
    s[2] → ... → s[n]* and *π[2](s[i]) = π[1](s[i]) ∀ i < m < n*, while *π[2](s[i]) ≥
    π[1](s[i]) ∀ i ≥ m*, the policy, *π[2]*, is at least equal to *π[1]* and it''s
    become better if at least an inequality is strict. Conversely, if *Q(s, π[2](s); π[2]) ≥
    V(s; π[1])*, this means that *π[2](s) ≥ π[1](s)* and, again, *Q(s, π[2](s); π[2])
    > V(s; π[1])* if there''s at least a state, *s[i]*, where *π[2](s[i]) > π[1](s[i])*.
    Hence, after a complete policy evaluation cycle, we are authorized to define a
    new greedy policy as:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Sutton和Barto（在*强化学习*，*Sutton R. S.，Barto A. G.*，麻省理工学院出版社）证明了一个简单但非常重要的定理（称为*策略改进定理*），该定理表明，给定确定性策略，*π[1]*和*π[2]*，如果*Q(s,
    π[2](s); π[2]) ≥ V(s; π[1])∀ s ∈ S*，则*π[2]*优于或等于*π[1]*。证明非常紧凑，可以在他们的书中找到，然而，结果可以直观地理解。如果我们考虑一个状态序列，*s[1]
    → s[2] → ... → s[n]*和*π[2](s[i]) = π[1](s[i])∀ i < m < n*，而*π[2](s[i]) ≥ π[1](s[i])∀
    i ≥ m*，策略，*π[2]*，至少等于*π[1]*，并且如果至少有一个不等式是严格的，它就变得更好。相反，如果*Q(s, π[2](s); π[2])
    ≥ V(s; π[1])*，这意味着*π[2](s) ≥ π[1](s)*，并且如果至少有一个状态，*s[i]*，其中*π[2](s[i]) > π[1](s[i])*，那么*Q(s,
    π[2](s); π[2]) > V(s; π[1])*。因此，在完成策略评估周期后，我们有权利定义一个新的贪婪策略：
- en: '![](img/a6f0176d-da62-4f2e-9b0f-1e6677395158.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6f0176d-da62-4f2e-9b0f-1e6677395158.png)'
- en: This step is called *policy improvement* and its goal is to set the action associated
    with each state as the one that leads to the transition to the successor state
    with the maximum value. It's not difficult to understand that an optimal policy
    will remain stable when *V^((t)) → **V(s; π)*. In fact, when *t → ∞*, the Q function
    will converge to a stable fixed point determined by *V(s; π)* and the argmax(•)
    will always select the same actions. However, if we start with a random policy,
    in general, a single policy evaluation cycle isn't enough to assure the convergence.
    Therefore, after a policy improvement step, it's often necessary to repeat the
    evaluation and continue alternating the two phases until the policy becomes stable
    (that's why the algorithm is called policy iteration). In general, the convergence
    is quite fast, but the actual speed depends on the nature of the problem, the
    number of states and actions, and the consistency of the rewards.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步被称为 *策略改进*，其目标是将每个状态关联的动作设置为导致后继状态具有最大值的动作。不难理解，当 *V^((t)) → **V(s; π)* 时，最佳策略将保持稳定。事实上，当
    *t → ∞* 时，Q函数将收敛到由 *V(s; π)* 和 argmax(•) 确定的稳定固定点，并且总是选择相同的动作。然而，如果我们从一个随机策略开始，通常一个策略评估周期不足以保证收敛。因此，在策略改进步骤之后，通常需要重复评估并交替两个阶段，直到策略变得稳定（这就是为什么算法被称为策略迭代）。一般来说，收敛速度相当快，但实际速度取决于问题的性质、状态和动作的数量以及奖励的一致性。
- en: 'The complete policy iteration algorithm (as proposed by Sutton and Barto) is:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的政策迭代算法（如Sutton和Barto所提出）如下：
- en: Set an initial deterministic random policy *π(s)*
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个初始确定性随机策略 *π(s)*
- en: Set the initial value array *V(s) = 0 ∀ s ∈ S*
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始值数组设置为 *V(s) = 0 ∀ s ∈ S*
- en: Set a tolerance threshold Thr (for example, Thr = 0.0001)
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个容差阈值 Thr（例如，Thr = 0.0001）
- en: Set a maximum number of iterations *N[iter]*
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置最大迭代次数 *N[iter]*
- en: Set a counter *e = 0*
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置计数器 *e = 0*
- en: 'While *e < N[iter]*:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *e < N[iter]* 时：
- en: e += 1
  id: totrans-92
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: e += 1
- en: 'Do:'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行：
- en: Set *V[old](s) = V(s) ∀ s ∈ S*
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *V[old](s) = V(s) ∀ s ∈ S*
- en: Perform a Policy Evaluation step reading the current value from *V[old](s)*
    and updating *V(s)*
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行策略评估步骤，从 *V[old](s)* 读取当前值并更新 *V(s)*
- en: While *Avg(|V(s) - V[old](s)|) > Thr*
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *Avg(|V(s) - V[old](s)|) > Thr* 时：
- en: Set *π[old](s) = π(s) ∀ s ∈ S*
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *π[old](s) = π(s) ∀ s ∈ S*
- en: Perform a policy improvement step
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行策略改进步骤
- en: 'If *π[old]**(s) == π(s)*:'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *π[old]**(s) == π(s)*：
- en: Break
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Break
- en: Output the final deterministic policy *π(s)*
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出最终的确定性策略 *π(s)*
- en: In this case, as we have a full knowledge of the environment, there's no need
    for an exploration phase. The policy is always exploited as it's built to be greedy
    to the real value (obtained when t → ∞).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，因为我们完全了解环境，所以不需要探索阶段。策略总是被利用，因为它被设计成对真实值（当t → ∞时获得）贪婪。
- en: Policy iteration in the checkerboard environment
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查棋盘环境中的策略迭代
- en: 'We want to apply the policy iteration algorithm in order to find an optimal
    policy for the tunnel environment. Let''s start by defining a random initial policy
    and a value matrix with all values (except the terminal states) equal to 0:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望应用策略迭代算法以找到隧道环境的最佳策略。让我们首先定义一个随机的初始策略和一个所有值（除了终端状态）都等于0的值矩阵：
- en: '[PRE1]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The initial random policy (t=0) is shown in the following chart:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 初始随机策略（t=0）如下图表所示：
- en: '![](img/5cf3aae5-0698-46d0-b37b-e578de590bcc.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cf3aae5-0698-46d0-b37b-e578de590bcc.png)'
- en: Initial (t=0) random policy
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 初始（t=0）随机策略
- en: 'The states denoted with ⊗ represent the wells, while the final positive one
    is represented by the capital letter *E*. Hence, the initial value matrix (t=0)
    is:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 用 ⊗ 表示的状态表示井，而最终的积极状态由大写字母 *E* 表示。因此，初始值矩阵（t=0）如下：
- en: '![](img/d43a363e-87a2-42c2-ba00-4db6a92904f9.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d43a363e-87a2-42c2-ba00-4db6a92904f9.png)'
- en: Initial (t=0) value matrix
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 初始（t=0）值矩阵
- en: 'At this point, we need to define the functions to perform the policy evaluation
    and improvement steps. As the environment is deterministic, the processes are
    slightly simpler because the generic transition probability, *T(s[i], s[j]; a[k])*,
    is equal to 1 for the only possible successor and 0 otherwise. In the same way,
    the policy is deterministic and only a single action is taken into account. The
    policy evaluation step is performed, freezing the current values and updating
    the whole matrix, *V^((t+1))*, with *V^((t))*; however, it''s also possible to
    use the new values immediately. I invite the reader to test both strategies in
    order to find the fastest way. In this example, we are employing a discount factor,
    γ = 0.9 (it goes without saying that an interesting exercise consists of testing
    different values and comparing the result of the evaluation process and the final
    behavior):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要定义执行政策评估和改进步骤的函数。由于环境是确定性的，因此过程稍微简单一些，因为通用转移概率，*T(s[i]，s[j]; a[k])*，对于唯一可能的后续状态等于1，否则为0。同样，政策也是确定性的，只考虑一个动作。执行政策评估步骤，冻结当前值，并使用*V^((t))*更新整个矩阵，*V^((t+1))*；然而，也可以立即使用新值。我邀请读者测试这两种策略，以找到最快的方法。在这个例子中，我们使用折现因子，γ
    = 0.9（不言而喻，一个有趣的练习是测试不同的值并比较评估过程的结果和最终行为）：
- en: '[PRE2]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the functions have been defined, we start the policy iteration cycle (with
    a maximum number of epochs, *N[iter]* = 100,000, and a tolerance threshold equal
    to 10^(-5)):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好函数后，我们开始政策迭代周期（最大迭代次数，*N[iter]* = 100,000，容差阈值等于10^(-5)）：
- en: '[PRE3]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'At the end of the process (in this case, the algorithm converged after 182
    iterations, but this value can change with different initial policies), the value
    matrix is:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程结束时（在这种情况下，算法在182次迭代后收敛，但这个值可能会因不同的初始政策而变化），值矩阵是：
- en: '![](img/2959dfcb-e800-4921-bc65-68566063961a.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2959dfcb-e800-4921-bc65-68566063961a.png)'
- en: Final value matrix
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最终值矩阵
- en: 'Analyzing the values, it''s possible to see how the algorithm discovered that
    they are an implicit function of the distance between a cell and the ending state.
    Moreover, the policy always avoids the wells because the maximum value is always
    found in an adjacent state. It''s easy to verify this behavior by plotting the
    final policy:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 分析值，可以看到算法是如何发现它们是单元格与结束状态之间距离的隐函数。此外，政策总是避免陷阱，因为最大值总是在相邻状态中找到。通过绘制最终政策可以轻松验证这种行为：
- en: '![](img/9a1c460a-f0b9-4ed2-a650-536169c1cf7e.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9a1c460a-f0b9-4ed2-a650-536169c1cf7e.png)'
- en: Final policy
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最终政策
- en: Picking a random initial state, the agent will always reach the ending one,
    avoiding the wells and confirming the optimality of the policy iteration algorithm.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个随机的初始状态，智能体将始终达到结束状态，避免陷阱并确认政策迭代算法的优化性。
- en: Value iteration
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 值迭代
- en: 'An alternative approach to policy iteration is provided by the *value iteration*
    algorithm. The main assumption is based on the empirical observation that the
    policy evaluation step converges rather quickly and it''s reasonable to stop the
    process after a fixed number of steps (normally 1). In fact, policy iteration
    can be imagined like a game where the first player tries to find the correct values
    considering a stable policy, while the other one creates a new policy that is
    greedy with respect to the new values. Clearly, the second step compromises the
    validity of the previous evaluation, forcing the first player to repeat the process.
    However, as the Bellman equation uses a single fixed point, the algorithm converges
    to a solution characterized by the fact that the policy doesn''t change anymore
    and, consequently, the evaluation becomes stable. This process can be simplified
    by removing the policy improvement step and continuing the evaluation in a greedy
    fashion. Formally, each step is based on the following update rule:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 政策迭代的一个替代方法是**值迭代**算法。主要假设基于经验观察，政策评估步骤收敛得相当快，因此在固定步骤数（通常是1步）后停止过程是合理的。实际上，政策迭代可以想象成一场游戏，其中第一个玩家试图在考虑稳定政策的情况下找到正确的值，而另一个玩家则创建一个对新值具有贪婪性的新政策。显然，第二步会损害先前评估的有效性，迫使第一个玩家重复这个过程。然而，由于贝尔曼方程使用一个固定的单点，算法收敛到一个解决方案，其特征是政策不再改变，因此评估变得稳定。这个过程可以通过移除政策改进步骤并以贪婪方式继续评估来简化。形式上，每一步都是基于以下更新规则：
- en: '![](img/186e0f2f-9264-4455-8232-2095e3813a59.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/186e0f2f-9264-4455-8232-2095e3813a59.png)'
- en: Now the iteration doesn't consider the policy anymore (assuming implicitly that
    it will be greedy with respect to the values), and selects *V^((t+1))* as the
    maximum possible value among all *V^((t))(a[t])*. In other words, value iteration
    anticipates the choice that is made by the policy improvement step by selecting
    the value that corresponds to the action that is likely (p → 1) to be selected.
    It's not difficult to extend the convergence proof presented in the previous section
    to this case, therefore, *V^((∞)) → V^((opt))*, as well as policy iteration does.
    However, the average number of iterations is normally smaller because we are starting
    with a random policy that can contrast the value iteration process.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，迭代不再考虑策略（隐式地假设它将根据值进行贪婪选择），并选择所有 *V^((t))(a[t])* 中最大的可能值 *V^((t+1))*。换句话说，值迭代通过选择对应于最可能（p
    → 1）被选中的动作的值，来预测策略改进步骤所做的选择。将上一节中提出的收敛证明扩展到这个情况并不困难，因此，*V^((∞)) → V^((opt))*，以及策略迭代也是如此。然而，平均迭代次数通常较小，因为我们从一个可以对比值迭代过程的随机策略开始。
- en: 'When the values become stable, the optimal greedy policy is simply obtained
    as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当值变得稳定时，最优贪婪策略简单地获得如下：
- en: '![](img/19a2096e-6fa3-4062-aac7-ba08e39c17a2.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19a2096e-6fa3-4062-aac7-ba08e39c17a2.png)'
- en: This step is formally equivalent to a policy improvement iteration, which, however,
    is done only once at the end of the process.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤在形式上等同于策略改进迭代，然而，它只在过程结束时执行一次。
- en: 'The complete value iteration algorithm (as proposed by Sutton and Barto) is:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Sutton 和 Barto 提出的完整值迭代算法如下：
- en: Set the initial value array, *V(s) = 0 ∀ s ∈ S*
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置初始值数组，*V(s) = 0 ∀ s ∈ S*
- en: Set a tolerance threshold, *Thr*, (for example, *Thr = 0.0001*)
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个容差阈值，*Thr*（例如，*Thr = 0.0001*）
- en: Set a maximum number of iteration, *N[iter]*
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置最大迭代次数，*N[iter]*
- en: Set a counter, *e* = 0
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置计数器，*e* = 0
- en: 'While *e < N[iter]*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'While *e < N[iter]*:'
- en: '*e += 1*'
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*e += 1*'
- en: 'Do:'
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Do:'
- en: Set *V[old](s) = V(s) ∀ s ∈ S*
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Set *V[old](s) = V(s) ∀ s ∈ S*
- en: Perform a value evaluation step reading the current value from *V[old](s)* and
    updating *V(s)*
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行值评估步骤，从 *V[old](s)* 读取当前值并更新 *V(s)*
- en: While *Avg(|V(s) - V[old](s)|) > Thr*
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: While *Avg(|V(s) - V[old](s)|) > Thr*
- en: Output the final deterministic policy *π(s) = argmax[a] Q(s, a)*
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出最终的确定性策略 *π(s) = argmax[a] Q(s, a)*
- en: Value iteration in the checkerboard environment
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 棋盘环境中的值迭代
- en: 'To test this algorithm, we need to set an initial value matrix with all values
    equal to 0 (they can be also randomly chosen but, as we don''t have any prior
    information on the final configuration, every initial choice is probabilistically
    equivalent):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这个算法，我们需要设置一个所有值都等于 0 的初始值矩阵（它们也可以随机选择，但由于我们没有关于最终配置的先验信息，每个初始选择在概率上是等效的）：
- en: '[PRE4]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At this point, we can define the two functions to perform the value evaluation
    and the final policy selection (the function `is_final()` is the one defined in
    the previous example):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以定义两个函数来执行值评估和最终策略选择（函数 `is_final()` 是在先前的例子中定义的）：
- en: '[PRE5]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The main differences are in the `value_evaluation()` function, which now has
    to consider all possible successor states and select the value corresponding to
    the action that leads to the state with the highest value. Instead, the `policy_selection()`
    function is equivalent to `policy_improvement()`, but, as it is invoked only once,
    it outputs directly to the final optimal policy.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于 `value_evaluation()` 函数，它现在必须考虑所有可能的后继状态，并选择导致具有最高值的状态的值。而 `policy_selection()`
    函数与 `policy_improvement()` 相当，但由于它只被调用一次，它直接输出到最终最优策略。
- en: 'At this point, we can run a training cycle (assuming the same constants as
    before):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以运行一个训练周期（假设与之前相同的常数）：
- en: '[PRE6]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The final value configuration (after 127 iterations) is shown in the following
    chart:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最终值配置（经过 127 次迭代）如下图表所示：
- en: '![](img/aad39ec1-85b2-491a-af13-66db44f7841c.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aad39ec1-85b2-491a-af13-66db44f7841c.png)'
- en: Final value matrix
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最终值矩阵
- en: 'As in the previous example, the final value configuration is a function of
    the distance between each state and the ending one, but, in this case, the choice
    of *γ = 0.9* isn''t optimal. In fact, the wells close to the final state aren''t
    considered very dangerous anymore. Plotting the final policy can help us understand
    the behavior:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，最终值配置是每个状态与结束状态之间距离的函数，但在这个情况下，*γ = 0.9* 的选择并不最优。事实上，靠近最终状态的水井不再被认为是非常危险的。绘制最终策略可以帮助我们理解其行为：
- en: '![](img/44dc4df9-0c90-40d7-8bec-eccfd897d1ec.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44dc4df9-0c90-40d7-8bec-eccfd897d1ec.png)'
- en: Final policy
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最终策略
- en: 'As expected, the wells that are far from the target are avoided, but the two
    that are close to the final state are accepted as reasonable penalties. This happens
    because the value iteration algorithm is very greedy with respect to the value
    and the discount factor, *γ < 1.0*; the effect of negative states can be compensated
    for by the final reward. In many scenarios, these states are absorbing, therefore
    their implicit reward is *+∞* or *-∞*, meaning that no other actions can change
    the final value. I invite the reader to repeat the example with different discount
    factors (remember that an agent with *γ → 1* is very short-sighted and will avoid
    any obstacle, even reducing the efficiency of the policy) and change the values
    of the final states. Moreover, the reader should be able to answer the question:
    What is the agent''s behavior when the standard reward (whose default value is
    *-0.1*) is increased or decreased?'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，远离目标的井被避免，但靠近最终状态的两个井被视为合理的惩罚。这是因为价值迭代算法在价值和折扣因子方面非常贪婪，*γ < 1.0*；负状态的影响可以通过最终奖励来补偿。在许多场景中，这些状态是吸收的，因此它们的隐含奖励是*+∞*或*-∞*，这意味着没有其他动作可以改变最终值。我邀请读者用不同的折扣因子（记住，*γ
    → 1*的智能体非常短视，会避开任何障碍，甚至降低策略的效率）重复这个例子，并改变最终状态的价值。此外，读者应该能够回答以下问题：当标准奖励（默认值为*-0.1*）增加或减少时，智能体的行为是什么？
- en: TD(0) algorithm
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD(0)算法
- en: One of the problems with Dynamic Programming algorithms is the need for a full
    knowledge of the environment in terms of states and transition probabilities.
    Unfortunately, there are many cases where these pieces of information are unknown
    before the direct experience. In particular, the states can be discovered by letting
    the agent explore the environment, but the transition probabilities require us
    to count the number of transitions to a certain state and this is often impossible.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 动态规划算法的一个问题是需要完全了解环境的状态和转移概率。不幸的是，在直接经验之前，这些信息在很多情况下都是未知的。特别是，状态可以通过让智能体探索环境来发现，但转移概率需要我们计算到达某个状态的数量，这通常是不可能的。
- en: Moreover, an environment with absorbing states can prevent visiting many states
    if the agent has learned a good initial policy. For example, in a game, which
    can be described as an episodic MDP, the agent discovers the environment while
    learning how to move forward without ending in a negative absorbing state.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果一个环境有吸收状态，那么如果智能体已经学会了一个好的初始策略，它可能会阻止访问许多状态。例如，在一个可以描述为离散马尔可夫决策过程（MDP）的游戏中，智能体在学会如何前进而不陷入负吸收状态的同时，发现环境。
- en: 'A general solution to these problems is provided by a different evaluation
    strategy, called **Temporal Difference** (**TD**) RL. In this case, we start with
    an empty value matrix and we let the agent follow a greedy policy with respect
    to the value (but the initial one, which is generally random). Once the agent
    observes a transition, *s[i] → s[j]*, due to an action, *a[t]*, with a reward, *r[ij]*,
    it updates the estimation of *V(s[i])*. The process is structured in episodes
    (which is the most natural way) and ends when a maximum number of steps have been
    done or a terminal state is met. In particular, the TD(0) algorithm updates the
    value according to the rule:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的通用解决方案是由一种不同的评估策略提供的，称为**时间差分**（**TD**）强化学习。在这种情况下，我们从一个空的价值矩阵开始，并让智能体遵循一个关于价值（但初始的是，通常是随机的）的贪婪策略。一旦智能体观察到由于一个动作*a[t]*导致的转移*s[i] →
    s[j]*，并得到一个奖励*r[ij]*，它就会更新对*V(s[i])*的估计。这个过程以剧集（这是最自然的方式）为结构，并在完成最大步数或遇到终端状态时结束。特别是，TD(0)算法根据以下规则更新价值：
- en: '![](img/8832ace7-a132-4c8a-96a8-7f7e0b719eb1.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8832ace7-a132-4c8a-96a8-7f7e0b719eb1.png)'
- en: 'The constant, *α*, is bound between 0 and 1 and acts as a learning rate. Each
    update considers a variation with respect to the current value, *V^((t))(s[i])*,
    which is proportional to the difference between the actual return and the previous
    estimation. The term *r[ij] + γV^((t))(s[j])* is analogous to the one employed
    in the previous methods and represents the expected value given the current return
    and the discounted value starting from the successor state. However, as *V^((t))(s[j])*
    is an estimation, the process is based on a bootstrap from the previous values.
    In other words, we start from an estimation to determine the next one, which should
    be closer to the stable fixed point. Indeed, TD(0) is the simplest example of
    a family of TD algorithms that are based on a sequence (usually called backup)
    that can be generalized as (considering k steps):'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 常数 *α* 被限制在 0 和 1 之间，并作为学习率。每次更新都会考虑与当前值 *V^((t))(s[i])* 的变化，这个变化与实际回报和先前估计之间的差异成比例。术语
    *r[ij] + γV^((t))(s[j])* 与先前方法中使用的术语类似，并代表给定当前回报和从后续状态开始的折现值所期望的值。然而，由于 *V^((t))(s[j])*
    是一个估计值，这个过程基于从先前值的自举。换句话说，我们从一个估计值开始，以确定下一个值，这个值应该更接近稳定的固定点。确实，TD(0) 是基于序列（通常称为回溯）的
    TD 算法家族中最简单的例子，这个序列可以推广为（考虑 k 步）：
- en: '![](img/94db5eb2-cbcf-4559-9c1d-d11c3e3dcc18.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/94db5eb2-cbcf-4559-9c1d-d11c3e3dcc18.png)'
- en: As we're using a single reward to approximate the expected discounted return,
    TD(0) is usually called a one-step TD method (or one-step backup). A more complex
    algorithm can be built considering more subsequent rewards or alternative strategies.
    We're going to analyze a generic variant called TD(*λ*) in [Chapter 15](345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml),
    *Advanced Policy Estimation Algorithms* and explain why this algorithm corresponds
    to a choice of *λ = 0*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用单个奖励来近似期望的折现回报，TD(0) 通常被称为单步 TD 方法（或单步回溯）。可以通过考虑更多的后续奖励或替代策略来构建更复杂的算法。我们将在第
    15 章 [高级策略估计算法](345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml)中分析一个通用的变体，称为 TD(*λ*)，并解释为什么这个算法对应于
    *λ = 0* 的选择。
- en: 'TD(0) has been proven to converge, even if the proof (which can be found for
    a model-based approach in *Convergence of Model-Based Temporal Difference Learning
    for Control*, *Van Hasselt H.*, *Wiering M. A.*, *Proceedings of the 2007 IEEE
    Symposium on Approximate Dynamic **Programming and Reinforcement Learning* (ADPRL
    2007)) is more complex because it''s necessary to consider the evolution of the
    Markov Process. In fact, in this case, we are approximating the expected discounted
    return with both a truncated estimation and a bootstrap value, *V(s[j])*, which
    is initially (and for a large number of iterations) unstable. However, assuming
    the convergence for *t → ∞*, we get:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: TD(0) 已被证明会收敛，尽管证明（可以在 *基于模型的时序差分学习控制收敛*，*Van Hasselt H.*，*Wiering M. A.*，*2007
    年 IEEE 近似动态规划与强化学习研讨会（ADPRL 2007）论文中找到）更为复杂，因为需要考虑马尔可夫过程的演变。实际上，在这种情况下，我们使用截断估计和自举值
    *V(s[j])* 来近似期望的折现回报，而 *V(s[j])* 在最初（以及大量迭代中）是不稳定的。然而，假设当 *t → ∞* 时收敛，我们得到：
- en: '![](img/5a45126c-3a25-4e27-8279-352de7b6b902.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5a45126c-3a25-4e27-8279-352de7b6b902.png)'
- en: The last formula expresses the value of the state, *s[i]*, assuming that the
    greedy optimal policy forces the agent to perform the action that causes the transition
    to *s[j]*. Of course, at this point, it's natural to ask under which conditions
    the algorithm converges. In fact, we are considering episodic tasks and the estimation, *V^((∞))(s[i])*,
    can be correct only if the agent performs a transition to *s[i]* an infinite number
    of times, selecting all possible actions an infinite number of times. Such a condition
    is often expressed by saying that the policy must be **Greedy in the Limit with
    Infinite Explorations** (**GLIE**). In other words, the real greediness is achieved
    only as an asymptotic state when the agent is able to explore the environment
    without limitations for an unlimited number of episodes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个公式表示在贪婪最优策略迫使智能体执行导致状态从 *s[j]* 转移到 *s[i]* 的动作的情况下，状态 *s[i]* 的值。当然，在这个时候，自然会问在什么条件下算法会收敛。实际上，我们正在考虑的是周期性任务，并且估计
    *V^((∞))(s[i])* 只有在智能体无限次地执行到 *s[i]* 的转移，无限次地选择所有可能动作的情况下才是正确的。这种条件通常通过说策略必须是
    **无限探索下的极限贪婪（GLIE**）来表示。换句话说，真正的贪婪只有在智能体能够无限制地探索环境，并且在不限次数的周期中不受限制时，才作为一个渐近状态实现。
- en: This is probably the most important limitation of TD RL, because, in real-life
    scenarios, some states can be very unlikely and, hence, the estimation can never
    accumulate the experience needed to converge to the actual value. We are going
    to analyze some methods to solve this problem in [Chapter 15](345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml),
    *Advanced Policy Estimation Algorithms*, but, in our example, we employ a random
    start. In other words, as the policy is greedy and could always avoid some states,
    we force the agent to start each episode in a random nonterminal cell. In this
    way, we allow a deep exploration even with a greedy policy. Whenever this approach
    is not feasible (because, for example, the environment dynamics are not controllable),
    the exploration-exploitation dilemma can be solved only by employing an ε-greedy
    policy, which selects a fraction of suboptimal (or even wrong) actions. In this
    way, it's possible to observe a higher number of transitions paying the price
    of a slower convergence.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是TD RL最重要的限制，因为在现实场景中，某些状态可能非常不可能，因此估计永远不会积累足够经验以收敛到实际值。我们将在[第15章](345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml)，“高级策略估计算法”中分析一些解决此问题的方法，但在我们的例子中，我们采用随机起始。换句话说，由于策略是贪婪的，可以始终避免某些状态，我们迫使智能体在每个剧集开始时在一个随机的非终端单元格中。这样，即使在贪婪策略下，我们也允许进行深度探索。当这种方法不可行（例如，因为环境动力学不可控）时，只能通过采用ε-贪婪策略来解决探索-利用困境，该策略选择一部分次优（甚至错误）的动作。这样，可以观察更多的转移，但代价是收敛速度较慢。
- en: However, as pointed out by Sutton and Barto, TD(0) converges to the maximum-likelihood
    estimation of the value function determined by the MDP, finding the implicit transition
    probabilities of the model. Therefore, if the number of observations is high enough,
    TD(0) can quickly find an optimal policy, but, at the same time, it's also more
    sensitive to biased estimations if some couple's state-action are never experienced
    (or experienced very seldom). In our example, we don't know which the initial
    state is, hence selecting a fixed starting point yields a policy that is extremely
    rigid and almost completely unable to manage noisy situations. For example, if
    the starting point is changed to an adjacent (but never explored) cell, the algorithm
    could fail to find the optimal path to the positive terminal state. On the other
    hand, if we know that the dynamics are well-defined, TD(0) will force the agent
    to select the actions that are most likely to produce the optimal result given
    the current knowledge of the environment. If the dynamics are partially stochastic,
    the advantage of an ε-greedy policy can be understood considering a sequence of
    episodes where the agent experiences the same transitions and the corresponding
    values are increased proportionally. If, for example, the environment changes
    one transition after many experiences, the agent has to face a brand new experience
    when the policy is already almost stable. The correction requires many episodes
    and, as this random change has a very low probability, it's possible that the
    agent will never learn the correct behavior. Instead, by selecting a few random
    actions, the probability of encountering a similar state (or even the same one)
    increases (think about a game where the state is represented by a screenshot)
    and the algorithm can become more robust with respect to very unlikely transitions.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如Sutton和Barto所指出的，TD(0)收敛到由MDP确定的值函数的最大似然估计，找到模型的隐式转移概率。因此，如果观察的数量足够高，TD(0)可以快速找到一个最优策略，但与此同时，它对某些状态-动作对从未经历（或很少经历）的偏差估计也更加敏感。在我们的例子中，我们不知道初始状态是哪一个，因此选择一个固定的起始点会导致一个极其僵化的策略，几乎完全无法处理噪声情况。例如，如果起始点改为相邻（但从未探索过）的单元格，算法可能无法找到到达正终端状态的最优路径。另一方面，如果我们知道动力学是明确定义的，TD(0)将迫使智能体选择在当前环境知识下最有可能产生最优结果的动作。如果动力学部分是随机的，ε-贪婪策略的优势可以通过考虑智能体经历相同转移和相应值成比例增加的连续剧集来理解。例如，如果环境在许多经验之后改变一个转移，当策略几乎稳定时，智能体必须面对全新的经验。这种纠正需要许多剧集，由于这种随机变化发生的概率非常低，智能体可能永远无法学会正确的行为。相反，通过选择一些随机动作，遇到类似状态（甚至相同状态）的概率增加（想想一个状态由截图表示的游戏），算法可以相对于非常不可能的转移变得更加鲁棒。
- en: 'The complete TD(0) algorithm is:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的TD(0)算法如下：
- en: Set an initial deterministic random policy, *π(s)*
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个初始的确定性随机策略，*π(s)*
- en: Set the initial value array, *V(s) = 0 ∀ s ∈ S*
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the number of episodes, *N[episodes]*
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a maximum number of steps per episode, *N[max]*
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, *α* (for example, *α = 0.1*)
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a constant, *γ* (for example, *γ** = 0.9*)
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set a counter, *e = 0*
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *i = 1* to *N[episodes]*:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the initial state, *s[i]*
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'While *s[j]* is non-terminal and *e < N[max]*:'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*e += 1*'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the action, **a[t] = π(s[i])**
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the transition, *(a[t], s[i]) → (s[j], r[ij])*
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the value function for the state, *s[i]*
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Set *s[i] = s[j]*
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the policy to be greedy with respect to the value function, *π(s) = argmax[a] Q(s,
    a)*
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: TD(0) in the checkerboard environment
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At this point, we can test the TD(0) algorithm on the checkerboard environment.
    The first step is to define an initial random policy and a value matrix with all
    elements equal to 0:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As we want to select a random starting point at the beginning of each episode,
    we need to define a helper function that must exclude the terminal states (all
    the constants are the same as previously defined):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we can implement the function to evaluate a single episode (setting the
    maximum number of steps equal to 500 and the constant to *α* = 0.25):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The function to determine the greedy policy with respect to the values is the
    same as already implemented in the previous examples; however, we report it to
    guarantee the consistency of the example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'At this point, we can start a training cycle with 5,000 episodes:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The final value matrix is shown in the following chart:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c8f833f-bc0f-4df7-9bcf-44c337ab56e0.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
- en: Final value matrix with random starts
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'Like in the previous examples, the final values are inversely proportional
    to the distance from the final positive state. Let''s analyze the resulting policy
    to understand whether the algorithm converged to a consistent solution:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/daddaf8e-2d83-4d14-8b34-9c36cee7499a.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Final policy with random starts
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen, the random choice of the starting state is allowed to find
    the best path independently from the initial condition. To better understand the
    advantage of this strategy, let''s plot the final value matrix when the initial
    state is fixed to the cell *(0, 0)*, corresponding to the upper-left corner:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6397023e-d14b-418b-92c8-cdf5f2fd971e.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: Final value matrix with a fixed initial state (0, 0)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Without any further analysis, it''s possible to see that many states have never
    been visited or visited only a few times, and the resulting policy is therefore
    extremely greedy with respect to the specific initial state. The blocks containing
    values equal to -1.0 indicate states where the agent often has to pick a random
    action because there''s no difference in the values, hence it can be extremely
    difficult to solve the environment with a different initial state. The resulting
    policy confirms this analysis:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92b1dadf-3967-4406-ab59-56adb216913f.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Final policy with a fixed initial state (0, 0)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'As it''s possible to see, the agent is able to reach the final state only when
    the initial point allows us to cross the trajectory starting from *(0, 0)*. In
    all these cases, it''s possible to recover the optimal policy, even if the paths
    longer than the ones obtained in the previous example. Instead, states such as *(0,
    4)* are clearly situations where there''s a *loss of policy*. In other words,
    the agent acts without any knowledge or awareness and the probability of success
    converges to 0\. As an exercise, I invite the reader to test this algorithm with
    different starting points (for example, a set of fixed ones) and higher *α* values.
    The goal is also to answer these questions: Is it possible to speed up the learning
    process? Is it necessary to start from all possible states in order to obtain
    a global optimal policy?'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如所见，智能体只有在初始点允许我们从*(0, 0)*开始穿越轨迹时才能达到最终状态。在这些所有情况下，即使路径比上一个例子中的路径更长，也有可能恢复最优策略。相反，如*(0,
    4)*这样的状态显然是存在策略损失的情况。换句话说，智能体在没有任何知识或意识的情况下采取行动，成功的概率收敛到0。作为一个练习，我邀请读者用不同的起始点（例如，一组固定的起始点）和更高的*α*值来测试这个算法。目标也是回答这些问题：是否可以加快学习过程？是否需要从所有可能的状态开始，才能获得全局最优策略？
- en: Summary
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced the most important RL concepts, focusing on the
    mathematical structure of an environment as a Markov Decision Process, and on
    the different kinds of policy and how they can be derived from the expected reward
    obtained by an agent. In particular, we defined the value of a state as the expected
    future reward considering a sequence discounted by a factor, *γ*. In the same
    way, we introduced the concept of the Q function, which is the value of an action
    when the agent is in a specific state.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了最重要的强化学习（RL）概念，重点关注环境作为马尔可夫决策过程（MDP）的数学结构，以及不同类型的策略以及它们如何从智能体获得的期望奖励中推导出来。特别是，我们定义了状态的价值为考虑一个按因子*γ*折现的序列的期望未来奖励。同样，我们引入了Q函数的概念，即当智能体处于特定状态时，动作的价值。
- en: These concepts directly employed the policy iteration algorithm, which is based
    on a Dynamic Programming approach assuming complete knowledge of the environment.
    The task is split into two stages; during the first one, the agent evaluates all
    the states given the current policy, while in the second one, the policy is updated
    in order to be greedy with respect to the new value function. In this way, the
    agent is forced to always pick the action that leads to a transition that maximizes
    the obtained value.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概念直接采用了策略迭代算法，该算法基于动态规划方法，假设对环境的完全了解。任务分为两个阶段；在第一阶段，智能体根据当前策略评估所有状态，而在第二阶段，策略被更新以对新价值函数进行贪婪选择。这样，智能体被迫始终选择导致最大化获得价值的转换的动作。
- en: We also analyzed a variant, called value iteration, that performs a single evaluation
    and selects the policy in a greedy manner. The main difference from the previous
    approach is that now the agent immediately selects the highest value assuming
    that the result of this process is equivalent to a policy iteration. Indeed, it's
    easy to prove that, after infinite transitions, both algorithms converge on the
    optimal value function.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还分析了一种变体，称为价值迭代，它执行单一评估并以贪婪的方式选择策略。与前一种方法的主要区别在于，现在智能体立即选择最高价值，假设这个过程的结果与策略迭代等效。实际上，很容易证明，在无限次转换之后，两种算法都会收敛到最优价值函数。
- en: The last algorithm is called TD(0) and it's based on a model-free approach.
    In fact, in many cases, it's difficult to know all the transition probabilities
    and, sometimes, even all possible states are unknown. This method is based on
    the Temporal Difference evaluation, which is performed directly while interacting
    with the environment. If the agent can visit all the states an infinite number
    of times (clearly, this is only a theoretical condition), the algorithm has been
    proven to converge to the optimal value function more quickly than other methods.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个算法称为TD(0)，它基于无模型的方法。实际上，在许多情况下，很难知道所有转换概率，有时甚至所有可能的状态都是未知的。这种方法基于时间差分评估，它在与环境交互时直接执行。如果智能体可以无限次访问所有状态（显然，这只是一个理论条件），则该算法已被证明比其他方法更快地收敛到最优价值函数。
- en: In the next chapter, [Chapter 15](345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml), *Advanced
    Policy Estimation Algorithms* we'll continue the discussion of RL algorithms,
    introducing some more advanced methods that can be immediately implemented using
    Deep Convolutional Networks.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第15章](345db56a-8a2e-4744-9086-b64b016e0a1d.xhtml)，*高级策略估计算法*中，我们将继续讨论强化学习算法，介绍一些可以使用深度卷积网络立即实现的高级方法。
