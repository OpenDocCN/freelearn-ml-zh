<html><head></head><body>
<div id="_idContainer055">
<h1 class="chapter-number" id="_idParaDest-43"><a id="_idTextAnchor043"/><span class="koboSpan" id="kobo.1.1">2</span></h1>
<h1 id="_idParaDest-44"><a id="_idTextAnchor044"/><span class="koboSpan" id="kobo.2.1">Labeling Data for Classification</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In this chapter, we are going to learn how to label tabular data by applying business rules programmatically with Python libraries. </span><span class="koboSpan" id="kobo.3.2">In real-world use cases , not all of our data will have labels. </span><span class="koboSpan" id="kobo.3.3">But we need to prepare labeled data for training</span><a id="_idIndexMarker090"/><span class="koboSpan" id="kobo.4.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">machine learning models</span></strong><span class="koboSpan" id="kobo.6.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.7.1">fine-tuning </span></strong><span class="koboSpan" id="kobo.8.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.9.1">foundation models</span></strong><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">The manual labeling of large sets of data or documents is cumbersome </span><a id="_idIndexMarker091"/><span class="koboSpan" id="kobo.11.1">and expensive. </span><span class="koboSpan" id="kobo.11.2">In case of manual labeling, individual labels are created one by one. </span><span class="koboSpan" id="kobo.11.3">Also, occasionally, sharing private data with a crowd-sourcing team outside the organization is </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">not secure.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">So, programmatically labeling data is required to automate data labeling and quickly label a large-scale dataset. </span><span class="koboSpan" id="kobo.13.2">In case of programmatic labeling, there are mainly three approaches. </span><span class="koboSpan" id="kobo.13.3">In the first approach, users</span><a id="_idIndexMarker092"/><span class="koboSpan" id="kobo.14.1"> create </span><strong class="bold"><span class="koboSpan" id="kobo.15.1">labeling functions</span></strong><span class="koboSpan" id="kobo.16.1"> and apply to vast amounts of unlabeled data to auto label large training datasets. </span><span class="koboSpan" id="kobo.16.2">In the second approach, users </span><a id="_idIndexMarker093"/><span class="koboSpan" id="kobo.17.1">apply </span><strong class="bold"><span class="koboSpan" id="kobo.18.1">semi-supervised learning</span></strong><span class="koboSpan" id="kobo.19.1"> to </span><a id="_idIndexMarker094"/><span class="koboSpan" id="kobo.20.1">create </span><strong class="bold"><span class="koboSpan" id="kobo.21.1">Pseudo-Labels</span></strong><span class="koboSpan" id="kobo.22.1">. </span><strong class="bold"><span class="koboSpan" id="kobo.23.1">K-means clustering</span></strong><span class="koboSpan" id="kobo.24.1"> is another </span><a id="_idIndexMarker095"/><span class="koboSpan" id="kobo.25.1">way to group similar dataset and label those clusters. </span><span class="koboSpan" id="kobo.25.2">We will deep dive into all these three methods in this chapter and label the example tabular dataset. </span><span class="koboSpan" id="kobo.25.3">We will also discover how to utilize </span><strong class="bold"><span class="koboSpan" id="kobo.26.1">large language models</span></strong><span class="koboSpan" id="kobo.27.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.28.1">LLMs</span></strong><span class="koboSpan" id="kobo.29.1">) for predicting labels in tabular data </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">classification tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.31.1">By the end of this chapter, you will be able to create labeling functions and a label model, and finally, you will be able to predict labels using that </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">label model.</span></span></p>
<p><span class="koboSpan" id="kobo.33.1">In this chapter, we are going to cover the following </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">main topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.35.1">Predicting labels with LLMs for </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">tabular data</span></span></li>
<li><span class="koboSpan" id="kobo.37.1">Labeling data using a rule-based </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">generative model</span></span></li>
<li><span class="koboSpan" id="kobo.39.1">Labeling data using </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">semi-supervised learning</span></span></li>
<li><span class="koboSpan" id="kobo.41.1">Labeling data using </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">K-means clustering</span></span></li>
</ul>
<h1 id="_idParaDest-45"><a id="_idTextAnchor045"/><span class="koboSpan" id="kobo.43.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.44.1">We need to install the Snorkel library using the </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">following command:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.46.1">
%pip install snorkel</span></pre> <p><span class="koboSpan" id="kobo.47.1">You can download the dataset and Python notebook from the </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">following link:</span></span></p>
<p><a href="https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/code/Ch02"><span class="No-Break"><span class="koboSpan" id="kobo.49.1">https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/code/Ch02</span></span></a></p>
<p><span class="koboSpan" id="kobo.50.1">OpenAI setup requirements are same as mentioned in </span><a href="B18944_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.51.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.52.1">.</span></span></p>
<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/><span class="koboSpan" id="kobo.53.1">Predicting labels with LLMs for tabular data</span></h1>
<p><span class="koboSpan" id="kobo.54.1">We will explore the process of predicting labels for tabular data classification tasks using </span><strong class="bold"><span class="koboSpan" id="kobo.55.1">large language models</span></strong><span class="koboSpan" id="kobo.56.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.57.1">LLMs</span></strong><span class="koboSpan" id="kobo.58.1">) and </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">few-shot learning.</span></span></p>
<p><span class="koboSpan" id="kobo.60.1">In the case of few-shot learning, we provide a few training data examples in the form of text along with a prompt for the model. </span><span class="koboSpan" id="kobo.60.2">The model adapts to the context and responds to new questions from </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">the user.</span></span></p>
<p><span class="koboSpan" id="kobo.62.1">First, let’s examine how to predict labels using LLMs for </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">tabular data.</span></span></p>
<p><span class="koboSpan" id="kobo.64.1">For tabular data, the initial step involves converting the data into serialized text data using LangChain’s templates. </span><span class="koboSpan" id="kobo.64.2">LangChain templates allow converting rows of data into fluent sentences or paragraphs by mapping columns to text snippets with variables that are filled based on cell values. </span><span class="koboSpan" id="kobo.64.3">Once we have the text data, we can utilize it as few-shot examples, comprising pairs of questions along with their corresponding labels (answers). </span><span class="koboSpan" id="kobo.64.4">Subsequently, we will send this few-shot data to </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">the model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<span class="koboSpan" id="kobo.66.1"><img alt="Figure 2.1 – LLM Few-shot example for ﻿predicting labels" src="image/B18944_02_001.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.67.1">Figure 2.1 – LLM Few-shot example for predicting labels</span></p>
<p><span class="koboSpan" id="kobo.68.1">Now we will use the LangChain template to translate the tabular data to natural </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">language text.</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.70.1">Patient ID</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.71.1">Age</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.72.1">Blood pressure</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.73.1">Diabetes</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.74.1">Death_event</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.75.1">1</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.76.1">70</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.77.1">high</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.78.1">yes</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.79.1">yes</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.80.1">2</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.81.1">45</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.82.1">high</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.83.1">no</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.84.1">no</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.85.1">3</span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.86.1">50</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.87.1">normal</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.88.1">yes</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.89.1">?</span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.90.1">Table 2.1 – Few-shot example data in tabular format</span></p>
<p><span class="koboSpan" id="kobo.91.1">The following is the text after converting the cell values to text </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">using templates:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.93.1">
User message :"The age is 70 and blood pressure is high and diabetes is yes.
</span><span class="koboSpan" id="kobo.93.2">Does this patient had death event? </span><span class="koboSpan" id="kobo.93.3">"
System: Yes</span></pre> <p><span class="koboSpan" id="kobo.94.1">Similarly, we can convert the second row to text and send it as a few-shot example with a prompt to the LLM using </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.95.1">ChatCompletion</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.96.1"> API.</span></span></p>
<p><span class="koboSpan" id="kobo.97.1">These few shot examples are sent along with a prompt to the LLM and then the LLM responds with the predicted label, </span><strong class="source-inline"><span class="koboSpan" id="kobo.98.1">Yes</span></strong><span class="koboSpan" id="kobo.99.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.100.1">No</span></strong><span class="koboSpan" id="kobo.101.1">, for </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">new text:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.103.1">
User message :"The age is 50 and blood pressure is normal and diabetes is yes.
</span><span class="koboSpan" id="kobo.103.2">Does this patient had death event? </span><span class="koboSpan" id="kobo.103.3">"
Then system responds "yes"</span></pre> <p><span class="koboSpan" id="kobo.104.1">We have seen how to leverage few-shot learning for predicting labels for </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">tabular data.</span></span></p>
<p><span class="koboSpan" id="kobo.106.1">The serialized data and the few-shot learning examples are used to provide context to the LLM model so that it can understand the meaning of the data and learn how to classify new tabular datasets. </span><span class="koboSpan" id="kobo.106.2">The LLM model generates a response based on </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">the input.</span></span></p>
<p><span class="koboSpan" id="kobo.108.1">Now, let’s explore a second example of predicting labels using few-shot learning and LLMs for </span><span class="No-Break"><span class="koboSpan" id="kobo.109.1">text data.</span></span></p>
<p><span class="koboSpan" id="kobo.110.1">As mentioned earlier, in the case of few-shot learning, a small set of training data examples is provided along with the prompt to </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">the LLM.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">The following code illustrates few-shot learning for predicting labels using prompts and LLMs. </span><span class="koboSpan" id="kobo.112.2">In this example, a system role is defined to guide the model in understanding the context and desired </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">sentiment labels.</span></span></p>
<p><span class="koboSpan" id="kobo.114.1">The user provides a new message expressing sentiments about a new house. </span><span class="koboSpan" id="kobo.114.2">The code then calls the </span><strong class="source-inline"><span class="koboSpan" id="kobo.115.1">ChatCompletion</span></strong><span class="koboSpan" id="kobo.116.1"> API to generate a response using the specified GPT model deployment. </span><span class="koboSpan" id="kobo.116.2">The system and user messages are structured to guide the model’s understanding of the sentiment. </span><span class="koboSpan" id="kobo.116.3">The sentiment analysis output is obtained from the API response </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">and printed.</span></span></p>
<p><span class="koboSpan" id="kobo.118.1">This approach enables the model to learn from examples provided in the system role and user messages, allowing it to predict sentiment labels for the </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">user’s input:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.120.1">
# Set the system role and user message variables
system_role = """Provide text sentiment in given text:
</span><strong class="bold"><span class="koboSpan" id="kobo.121.1">: I like Arkansas state. </span><span class="koboSpan" id="kobo.121.2">Because it is natural state and beautiful!!!</span></strong><span class="koboSpan" id="kobo.122.1">
positive
: </span><strong class="bold"><span class="koboSpan" id="kobo.123.1">I hate winters. </span><span class="koboSpan" id="kobo.123.2">It's freezing and bad weather</span></strong><span class="koboSpan" id="kobo.124.1">.
</span><span class="koboSpan" id="kobo.124.2">negative"""
user_message = f"""
</span><strong class="bold"><span class="koboSpan" id="kobo.125.1">I love this new house.It's amazing!!!</span></strong><span class="koboSpan" id="kobo.126.1">"""</span></pre> <p><span class="koboSpan" id="kobo.127.1">Let’s call the </span><strong class="source-inline"><span class="koboSpan" id="kobo.128.1">ChatCompletion</span></strong><span class="koboSpan" id="kobo.129.1"> API to send </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">a response:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.131.1">
# Send a completion call to generate the sentiment analysis
response = openai.ChatCompletion.create(
 engine = </span><strong class="source-inline"><span class="koboSpan" id="kobo.132.1">"</span></strong><strong class="bold"><span class="koboSpan" id="kobo.133.1">your gpt model deployment name</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.134.1">"</span></strong><span class="koboSpan" id="kobo.135.1">,
 messages = [
 "role": "system", "content": system_role,
 "role": "user", "content": user_message,
]
)
# Print the sentiment analysis output
print(response['choices'][0]['message']['content'])</span></pre> <p><span class="koboSpan" id="kobo.136.1">When you check the output, you will see </span><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">Positive</span></strong><span class="koboSpan" id="kobo.138.1"> in the response. </span><span class="koboSpan" id="kobo.138.2">We have seen how to send text messages as few-shot examples, along with a prompt, to the LLM using </span><strong class="source-inline"><span class="koboSpan" id="kobo.139.1">ChatCompletion</span></strong><span class="koboSpan" id="kobo.140.1"> API and how the LLM responds with the sentiment based on user input messages. </span><span class="koboSpan" id="kobo.140.2">Next, we’ll discuss labeling data using </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">Snorkel library.</span></span></p>
<h1 id="_idParaDest-47"><a id="_idTextAnchor047"/><span class="koboSpan" id="kobo.142.1">Data labeling using Snorkel</span></h1>
<p><span class="koboSpan" id="kobo.143.1">In this section, we are going to</span><a id="_idIndexMarker096"/><span class="koboSpan" id="kobo.144.1"> learn what Snorkel</span><a id="_idIndexMarker097"/><span class="koboSpan" id="kobo.145.1"> is and how we can use it to label data in </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">Python programmatically.</span></span></p>
<p><span class="koboSpan" id="kobo.147.1">Labeling data is an important step of a data science project and critical for training models to solve specific </span><span class="No-Break"><span class="koboSpan" id="kobo.148.1">business problems.</span></span></p>
<p><span class="koboSpan" id="kobo.149.1">In many real-world cases, training data does not have labels, or very little data with labels is available. </span><span class="koboSpan" id="kobo.149.2">For example, in a housing dataset, in some neighborhoods, historical housing prices may not be available for most of the houses. </span><span class="koboSpan" id="kobo.149.3">Another example, in the case of finance, is all transactions may not have an associated invoice number. </span><span class="koboSpan" id="kobo.149.4">Historical data with labels is </span><a id="_idIndexMarker098"/><span class="koboSpan" id="kobo.150.1">critical for businesses to train models and automate their business processes using </span><strong class="bold"><span class="koboSpan" id="kobo.151.1">machine learning</span></strong><span class="koboSpan" id="kobo.152.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.153.1">ML</span></strong><span class="koboSpan" id="kobo.154.1">) and artificial intelligence. </span><span class="koboSpan" id="kobo.154.2">However, this requires either outsourcing the data labeling to expensive domain experts or the business waiting for a long time to get new training data </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">with labels.</span></span></p>
<p><span class="koboSpan" id="kobo.156.1">This is where Snorkel comes into the picture to help us programmatically build labels and manage the training data at a relatively lower cost than hiring domain experts. </span><span class="koboSpan" id="kobo.156.2">It is also faster than manual labeling. </span><span class="koboSpan" id="kobo.156.3">In some cases, data may be sensitive due to it including personal information such as date of birth and social security number. </span><span class="koboSpan" id="kobo.156.4">In such cases, the organization cannot share data outside of the organization with a third party. </span><span class="koboSpan" id="kobo.156.5">So, organizations prefer to label the data programmatically with a team of in-house developers to protect data privacy and comply with government and consumer </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">data regulations.</span></span></p>
<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/><span class="koboSpan" id="kobo.158.1">What is Snorkel?</span></h2>
<p><span class="koboSpan" id="kobo.159.1">Snorkel</span><a id="_idIndexMarker099"/><span class="koboSpan" id="kobo.160.1"> is a Python open source</span><a id="_idIndexMarker100"/><span class="koboSpan" id="kobo.161.1"> library (</span><a href="https://www.snorkel.org/"><span class="koboSpan" id="kobo.162.1">https://www.snorkel.org/</span></a><span class="koboSpan" id="kobo.163.1">) that is used to create labels based on heuristics for tabular data. </span><span class="koboSpan" id="kobo.163.2">Using this library saves the time and cost of manual labeling. </span><span class="koboSpan" id="kobo.163.3">Snorkel uses a list of defined rules to label the data programmatically. </span><span class="koboSpan" id="kobo.163.4">With Snorkel, business knowledge is applied to generate “weak” labels for the dataset. </span><span class="koboSpan" id="kobo.163.5">This is also called a weak </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">supervision model.</span></span></p>
<p><span class="koboSpan" id="kobo.165.1">The weak generative</span><a id="_idIndexMarker101"/><span class="koboSpan" id="kobo.166.1"> model created by Snorkel generates noisy labels. </span><span class="koboSpan" id="kobo.166.2">These noisy labels are used to train a classifier that generalizes </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">the model.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.168.1"><img alt="Figure 2.2 – Programmatic data labeling (preparing training data)" src="image/B18944_02_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.169.1">Figure 2.2 – Programmatic data labeling (preparing training data)</span></p>
<p><span class="koboSpan" id="kobo.170.1">Here is an overview of the </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">Snorkel system:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.172.1">Domain specific rules are</span><a id="_idIndexMarker102"/><span class="koboSpan" id="kobo.173.1"> extracted from various sources such as </span><strong class="bold"><span class="koboSpan" id="kobo.174.1">subject matter experts</span></strong><span class="koboSpan" id="kobo.175.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.176.1">SMEs</span></strong><span class="koboSpan" id="kobo.177.1">), knowledge bases, legacy rule based </span><a id="_idIndexMarker103"/><span class="koboSpan" id="kobo.178.1">systems, patterns and heuristics using </span><strong class="bold"><span class="koboSpan" id="kobo.179.1">exploratory data </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.180.1">analysis</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.181.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.182.1">EDA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">).</span></span></li>
<li><span class="koboSpan" id="kobo.184.1">Labeling functions are created based on the rules obtained from various sources. </span><span class="koboSpan" id="kobo.184.2">One labeling function is created for each </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">business rule.</span></span></li>
<li><span class="koboSpan" id="kobo.186.1">These labeling functions are applied to the unlabeled dataset to generate the </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">label matrix.</span></span></li>
<li><span class="koboSpan" id="kobo.188.1">Label matrix (</span><strong class="source-inline"><span class="koboSpan" id="kobo.189.1">i*j</span></strong><span class="koboSpan" id="kobo.190.1">) consists of </span><strong class="source-inline"><span class="koboSpan" id="kobo.191.1">i</span></strong><span class="koboSpan" id="kobo.192.1"> rows and </span><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">j</span></strong><span class="koboSpan" id="kobo.194.1"> columns where </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">i</span></strong><span class="koboSpan" id="kobo.196.1"> is the number of observations in the dataset and </span><strong class="source-inline"><span class="koboSpan" id="kobo.197.1">j</span></strong><span class="koboSpan" id="kobo.198.1"> is the number of labeling functions. </span><span class="koboSpan" id="kobo.198.2">Each labeling function creates one label for each row in </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">the dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.200.1">Then, Snorkel’s label model is trained on this label matrix to create a generative model. </span><span class="koboSpan" id="kobo.200.2">This</span><a id="_idIndexMarker104"/><span class="koboSpan" id="kobo.201.1"> generative label model combines the outputs of the various </span><strong class="bold"><span class="koboSpan" id="kobo.202.1">labeling functions</span></strong><span class="koboSpan" id="kobo.203.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.204.1">LFs</span></strong><span class="koboSpan" id="kobo.205.1">) and produces a single, noise-aware probabilistic training </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">label set.</span></span></li>
<li><span class="koboSpan" id="kobo.207.1">Finally, this probabilistic training label set is used to train a downstream discriminative ML model such as </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">logistic regression.</span></span></li>
</ol>
<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/><span class="koboSpan" id="kobo.209.1">Why is Snorkel popular?</span></h2>
<p><span class="koboSpan" id="kobo.210.1">Snorkel is</span><a id="_idIndexMarker105"/><span class="koboSpan" id="kobo.211.1"> used by several large companies, such as Google, Apple, Stanford Medicine, and Tide, and it has proven to solve business problems on a large scale. </span><span class="koboSpan" id="kobo.211.2">For example, as mentioned on the </span><em class="italic"><span class="koboSpan" id="kobo.212.1">snorkel.ai</span></em><span class="koboSpan" id="kobo.213.1"> website, Google used Snorkel to replace 100K+ hand-annotated labels in critical ML pipelines for text classification. </span><span class="koboSpan" id="kobo.213.2">Another example is, researchers at Stanford Medicine used Snorkel to label medical imaging and monitoring datasets, converting years of work of hand-labeling into a several-hour job using Snorkel. </span><span class="koboSpan" id="kobo.213.3">Apple built a solution called Overton that utilizes Snorkel’s framework of weak supervision to overcome cost, privacy, and cold-start issues. </span><span class="koboSpan" id="kobo.213.4">A UK-based fintech company, Tide, used Snorkel to label matching invoices with transactions, which otherwise would have required investing in expensive subject matter experts to hand-label historical data. </span><span class="koboSpan" id="kobo.213.5">Excerpts of these case studies</span><a id="_idIndexMarker106"/><span class="koboSpan" id="kobo.214.1"> and more can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">at </span></span><a href="https://snorkel.ai/case-studies"><span class="No-Break"><span class="koboSpan" id="kobo.216.1">https://snorkel.ai/case-studies</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.217.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.218.1">Snorkel </span><a id="_idIndexMarker107"/><span class="koboSpan" id="kobo.219.1">creates weak labels using business rules and patterns in the absence of labeled data. </span><span class="koboSpan" id="kobo.219.2">Using Snorkel requires relatively less development eﬀort compared to crowdsourcing manual labeling. </span><span class="koboSpan" id="kobo.219.3">The cost of development also comes down due to automation using Python programming instead of hiring expensive business </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">domain experts.</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">The weak labels developed by Snorkel label model can be used to train a machine learning classifier for classification and information extraction tasks. </span><span class="koboSpan" id="kobo.221.2">In the following sections, we are going to see the step-by-step process of generating weak labels using Snorkel. </span><span class="koboSpan" id="kobo.221.3">For that, first, let’s load some unlabeled data from a </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">CSV file.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">In the following example, we have a limited dataset related to adult income </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">with labels.</span></span></p>
<p><span class="koboSpan" id="kobo.225.1">In this adult income dataset, we have the </span><strong class="source-inline"><span class="koboSpan" id="kobo.226.1">income</span></strong><span class="koboSpan" id="kobo.227.1"> label and features such as age, working hours, education, and work class. </span><span class="koboSpan" id="kobo.227.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">income</span></strong><span class="koboSpan" id="kobo.229.1"> label class is available only for a few observations. </span><span class="koboSpan" id="kobo.229.2">The majority of observations are unlabeled without any value for </span><strong class="source-inline"><span class="koboSpan" id="kobo.230.1">income</span></strong><span class="koboSpan" id="kobo.231.1">. </span><span class="koboSpan" id="kobo.231.2">So, our goal is to generate the labels using the Snorkel </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">Python library.</span></span></p>
<p><span class="koboSpan" id="kobo.233.1">We will come up with business rules after analyzing the correlation between each individual feature of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.234.1">income</span></strong><span class="koboSpan" id="kobo.235.1"> label class. </span><span class="koboSpan" id="kobo.235.2">Using those business rules, we will create one labeling function for each business rule with the Snorkel Python library. </span><span class="koboSpan" id="kobo.235.3">Then, we will generate labels for the unlabeled income dataset by applying those </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">labeling functions.</span></span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/><span class="koboSpan" id="kobo.237.1">Loading unlabeled data</span></h2>
<p><span class="koboSpan" id="kobo.238.1">We will first load the adult</span><a id="_idIndexMarker108"/><span class="koboSpan" id="kobo.239.1"> income dataset for predicting weak labels. </span><span class="koboSpan" id="kobo.239.2">Let us load the data using Pandas into </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">a DataFrame:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.241.1">
# loading the data set
#df = pd.read_csv("&lt;YourPath&gt;/adult_income.csv", encoding='latin-1)'</span></pre> <p><span class="koboSpan" id="kobo.242.1">Here, don’t forget to replace </span><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">&lt;YourPath&gt;</span></strong><span class="koboSpan" id="kobo.244.1"> with the path in your system. </span><span class="koboSpan" id="kobo.244.2">The target column name is </span><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">income</span></strong><span class="koboSpan" id="kobo.246.1">. </span><span class="koboSpan" id="kobo.246.2">Let us make sure the data is </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">loaded correctly:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.248.1">
df.head()</span></pre> <p><span class="koboSpan" id="kobo.249.1">Let us model the input </span><span class="No-Break"><span class="koboSpan" id="kobo.250.1">and output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.251.1">
x = df.iloc[:,:-1]
y = df["income"]</span></pre> <p><span class="koboSpan" id="kobo.252.1">In this section, we have loaded the income dataset from a CSV file into a </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">P</span></span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">andas DataFrame.</span></span></p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.255.1">Creating the labeling functions</span></h2>
<p><span class="koboSpan" id="kobo.256.1">Labeling functions implement</span><a id="_idIndexMarker109"/><span class="koboSpan" id="kobo.257.1"> the rules that are used to create the labeling model. </span><span class="koboSpan" id="kobo.257.2">Let us import the following method, which is required to implement the </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">labeling functions:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.259.1">
from snorkel.labeling import labeling_function</span></pre> <p><span class="koboSpan" id="kobo.260.1">We need to create at least four labeling functions to create an accurate labeling generative model. </span><span class="koboSpan" id="kobo.260.2">In order to create labeling functions, let’s first define the labeling rules based on the available small dataset labels. </span><span class="koboSpan" id="kobo.260.3">If no data is available, subject matter expert knowledge is required to define the rules for labeling. </span><span class="koboSpan" id="kobo.260.4">Labeling functions use the following labeling rules for the classification of income range for the given adult </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">income dataset.</span></span></p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.262.1">Labeling rules</span></h2>
<p><span class="koboSpan" id="kobo.263.1">The labeling rules</span><a id="_idIndexMarker110"/><span class="koboSpan" id="kobo.264.1"> from our small-volume income dataset are </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.266.1">Age rule</span></strong><span class="koboSpan" id="kobo.267.1">: Based on exploratory bi-variate analysis of age and income, we come up with a heuristics rule that if the age range is between 28 and 58, then income is greater than $50K; else, it is less </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">than $50K.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.269.1">Education rule</span></strong><span class="koboSpan" id="kobo.270.1">: Based on a bi-variate analysis of education and income, we come up with a heuristic rule that if education is bachelor’s or master’s, then income is &gt; $50K; otherwise, income is &lt; $</span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">50K.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.272.1">Working hours rule</span></strong><span class="koboSpan" id="kobo.273.1">: Based on a bi-variate analysis of working hours and income, we come up with the heuristic rule that if working hours are greater than 40, then income is greater than $50K; otherwise, it is less </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">than $50K</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.275.1">Work class rule</span></strong><span class="koboSpan" id="kobo.276.1">: Based</span><a id="_idIndexMarker111"/><span class="koboSpan" id="kobo.277.1"> on a bi-variate analysis of work class and income, we come up with the heuristic rule that if the work class is </span><strong class="source-inline"><span class="koboSpan" id="kobo.278.1">Self-emp-inc</span></strong><span class="koboSpan" id="kobo.279.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.280.1">Federal-gov</span></strong><span class="koboSpan" id="kobo.281.1">, then income is greater than $50K; otherwise, it is less </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">than $50K.</span></span></li>
</ul>
<h2 id="_idParaDest-53"><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.283.1">Constants</span></h2>
<p><span class="koboSpan" id="kobo.284.1">Let us define various constants</span><a id="_idIndexMarker112"/><span class="koboSpan" id="kobo.285.1"> that are used in our labeling functions</span><a id="_idIndexMarker113"/> <span class="No-Break"><span class="koboSpan" id="kobo.286.1">as follows.</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.287.1">Each labeling function returns either one of the following labels </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">as output:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">income_high</span></strong><span class="koboSpan" id="kobo.290.1"> indicates income &gt; $</span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">50K</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">income_low</span></strong><span class="koboSpan" id="kobo.293.1"> indicates income &lt; $</span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">50K</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.295.1">Let us assign the numerical values to the output labels </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">as follows:</span></span><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">income_high</span></strong><span class="koboSpan" id="kobo.298.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">1</span></strong></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">income_low</span></strong><span class="koboSpan" id="kobo.301.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">0</span></strong></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">Abstain</span></strong><span class="koboSpan" id="kobo.304.1"> = </span><strong class="source-inline"><span class="koboSpan" id="kobo.305.1">-1</span></strong></li></ul><p class="list-inset"><strong class="source-inline"><span class="koboSpan" id="kobo.306.1">Abstain</span></strong><span class="koboSpan" id="kobo.307.1"> indicates that income does not fall within any range for that observation </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">data point:</span></span></p></li>
</ul>
<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.309.1">Labeling functions</span></h2>
<p><span class="koboSpan" id="kobo.310.1">Now, let’s create labeling functions</span><a id="_idIndexMarker114"/><span class="koboSpan" id="kobo.311.1"> using these labeling rules and constants. </span><span class="koboSpan" id="kobo.311.2">We’ll discuss each function one by one </span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">as follows.</span></span></p>
<h3><span class="koboSpan" id="kobo.313.1">Age rule function</span></h3>
<p><span class="koboSpan" id="kobo.314.1">The age rule function is used to check whether the age of the person is greater</span><a id="_idIndexMarker115"/><span class="koboSpan" id="kobo.315.1"> than 28; if so, the income is greater </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">than $50K.</span></span></p>
<p><span class="koboSpan" id="kobo.317.1">We have defined our function with the help of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.318.1">@labeling_function()</span></strong><span class="koboSpan" id="kobo.319.1"> decorator. </span><span class="koboSpan" id="kobo.319.2">This decorator, when applied to a Python function, returns a label. </span><span class="koboSpan" id="kobo.319.3">Let us apply this decorator to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.320.1">age</span></strong><span class="koboSpan" id="kobo.321.1"> function to return the label based </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">on age:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.323.1">
@labeling_function()
def age(record):
    if record['age'] &lt; 28 and record['age'] &gt; 58:
    return income_low
    elif record['age'] &gt;28 and record['age']&lt; 58:
    return income_high
    else:
    return ABSTAIN</span></pre> <p><span class="koboSpan" id="kobo.324.1">Later, we will apply this </span><a id="_idIndexMarker116"/><span class="koboSpan" id="kobo.325.1">age rule labeling function to the unlabeled income dataset using the Pandas LF applier, which returns a label accordingly for each observation in </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">the dataset.</span></span></p>
<h3><span class="koboSpan" id="kobo.327.1">Education rule function</span></h3>
<p><span class="koboSpan" id="kobo.328.1">The education rule function is used to check whether the education of the person is</span><a id="_idIndexMarker117"/><span class="koboSpan" id="kobo.329.1"> bachelor’s or master’s; if so, the income is greater </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">than $50K.</span></span></p>
<p><span class="koboSpan" id="kobo.331.1">We have defined our labeling function for education with the help of the labeling function decorator </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.333.1">
@labeling_function()
def education(record):
    if record['education'] == "Bachelors" or record['education'] == "Masters":
    return income_high
    else:
    return income_low</span></pre> <p><span class="koboSpan" id="kobo.334.1">Later, we are going to apply this labeling function to our unlabeled income dataset using the Pandas LF applier, which returns the label accordingly for each observation in </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">the dataset.</span></span></p>
<h3><span class="koboSpan" id="kobo.336.1">Hours per week rule function</span></h3>
<p><span class="koboSpan" id="kobo.337.1">The hours per week rule function is used to check whether the working</span><a id="_idIndexMarker118"/><span class="koboSpan" id="kobo.338.1"> hours per week of the person is greater than 40; if so, the income is greater </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">than $50K.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">We have defined our labeling function for education with the help of the labeling function decorator </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.342.1">
@labeling_function()
def hours_per_week(record):
    if record['hours.per.week'] &gt; 40 or record['hours.per.week'] &lt; 60:
    return income_high
    else:
    return income_low</span></pre> <p><span class="koboSpan" id="kobo.343.1">Later, we are going to apply this labeling function to our unlabeled income dataset using the Pandas LF applier, which returns the label accordingly for each observation in </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">the dataset.</span></span></p>
<h3><span class="koboSpan" id="kobo.345.1">Work class rule function</span></h3>
<p><span class="koboSpan" id="kobo.346.1">The work class rule function is used to check whether the work class of the</span><a id="_idIndexMarker119"/><span class="koboSpan" id="kobo.347.1"> person is greater than self-employed or federal government; if so, the income is greater </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">than $50K.</span></span></p>
<p><span class="koboSpan" id="kobo.349.1">We have defined our labeling function for the work class with the help of the labeling function decorator </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.351.1">
@labeling_function()
def work_class(record):
    if record['workclass'] == "Self-emp-inc" or record['workclass'] == "Federal-gov":
     return income_high
    else:
     return income_low</span></pre> <h2 id="_idParaDest-55"><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.352.1">Creating a label model</span></h2>
<p><span class="koboSpan" id="kobo.353.1">Now, we are going to apply all the</span><a id="_idIndexMarker120"/><span class="koboSpan" id="kobo.354.1"> labeling functions that we created to the unlabeled income dataset using the Pandas </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">LF applier:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.356.1">
lfs = [age,education,hours_per_week,work_class]
from snorkel.labeling import PandasLFApplier
applier = PandasLFApplier(lfs=lfs)
L_train = applier.apply(df=x)</span></pre> <p><span class="koboSpan" id="kobo.357.1">Here is the output that we got after applying the label functions (</span><strong class="source-inline"><span class="koboSpan" id="kobo.358.1">lfs</span></strong><span class="koboSpan" id="kobo.359.1">) to the input </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">data (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">df</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer050">
<span class="koboSpan" id="kobo.363.1"><img alt="Figure 2.3 – Label matrix" src="image/B18944_02_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.364.1">Figure 2.3 – Label matrix</span></p>
<p><span class="koboSpan" id="kobo.365.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.366.1">L_train</span></strong><span class="koboSpan" id="kobo.367.1"> is the label matrix that is returned after applying the four label functions on the unlabeled dataset. </span><span class="koboSpan" id="kobo.367.2">For each labeling function, one label (</span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">-1</span></strong><span class="koboSpan" id="kobo.369.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.370.1">1</span></strong><span class="koboSpan" id="kobo.371.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">0</span></strong><span class="koboSpan" id="kobo.373.1">) will be returned. </span><span class="koboSpan" id="kobo.373.2">As we have created four labeling functions, four labels will be returned for each observation. </span><span class="koboSpan" id="kobo.373.3">The size of the label matrix will be [n*m], where </span><strong class="source-inline"><span class="koboSpan" id="kobo.374.1">n</span></strong><span class="koboSpan" id="kobo.375.1"> is the number of observations and </span><strong class="source-inline"><span class="koboSpan" id="kobo.376.1">m</span></strong><span class="koboSpan" id="kobo.377.1"> is the number of labeling functions, which is the same as the number of labels for </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">each observation.</span></span></p>
<p><span class="koboSpan" id="kobo.379.1">Next, we will use the </span><a id="_idIndexMarker121"/><span class="koboSpan" id="kobo.380.1">label matrix (</span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">L_train</span></strong><span class="koboSpan" id="kobo.382.1">) that was returned in the </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">previous step:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.384.1">
From snorkel.labeling.model.label_model import LabelModel
label_model = LabelModel(verbose=False)</span></pre> <p><span class="koboSpan" id="kobo.385.1">Train the label model using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.386.1">L_train</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.387.1">label matrix:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.388.1">
label_model.fit(L_train=L_train, n_epochs=1000, seed=100)</span></pre> <p><span class="koboSpan" id="kobo.389.1">Now, let us predict the labels using the trained </span><span class="No-Break"><span class="koboSpan" id="kobo.390.1">label model.</span></span></p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.391.1">Predicting labels</span></h2>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.392.1">L_train</span></strong><span class="koboSpan" id="kobo.393.1"> is passed to the </span><a id="_idIndexMarker122"/><span class="koboSpan" id="kobo.394.1">label model to predict the labels, </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.396.1">
X['Labels'] = label_model.predict(L=L_train)</span></pre> <p><span class="koboSpan" id="kobo.397.1">We get the </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<span class="koboSpan" id="kobo.399.1"><img alt="Figure 2.4 – Probabilistic labels" src="image/B18944_02_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.400.1">Figure 2.4 – Probabilistic labels</span></p>
<p><span class="koboSpan" id="kobo.401.1">As we have seen, we have</span><a id="_idIndexMarker123"/><span class="koboSpan" id="kobo.402.1"> generated a label matrix containing labels after applying label functions to a dataset. </span><span class="koboSpan" id="kobo.402.2">Then, we used that label matrix to train the label model, and then finally used that generative label model to predict the noise-aware probabilistic labels. </span><span class="koboSpan" id="kobo.402.3">Here, the label value </span><strong class="source-inline"><span class="koboSpan" id="kobo.403.1">0</span></strong><span class="koboSpan" id="kobo.404.1"> indicates </span><strong class="source-inline"><span class="koboSpan" id="kobo.405.1">income</span></strong><span class="koboSpan" id="kobo.406.1"> &lt; $50K and the label value </span><strong class="source-inline"><span class="koboSpan" id="kobo.407.1">1</span></strong><span class="koboSpan" id="kobo.408.1"> indicates </span><strong class="source-inline"><span class="koboSpan" id="kobo.409.1">income</span></strong><span class="koboSpan" id="kobo.410.1"> &gt; $50K. </span><span class="koboSpan" id="kobo.410.2">These values are defined using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.411.1">income_high</span></strong><span class="koboSpan" id="kobo.412.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.413.1">income_low</span></strong><span class="koboSpan" id="kobo.414.1"> constants before implementing the </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">labeling functions.</span></span></p>
<p><span class="koboSpan" id="kobo.416.1">We can further use these noise-aware probabilistic labels training set to train the discriminative ML model and then predict </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">accurate labels.</span></span></p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.418.1">Labeling data using the Compose library</span></h1>
<p><span class="koboSpan" id="kobo.419.1">Compose</span><a id="_idIndexMarker124"/><span class="koboSpan" id="kobo.420.1"> is an open source Python library developed to generate the labels for supervised machine learning. </span><span class="koboSpan" id="kobo.420.2">Compose creates labels from historical data </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">using LabelMaker.</span></span></p>
<p><span class="koboSpan" id="kobo.422.1">Subject matter</span><a id="_idIndexMarker125"/><span class="koboSpan" id="kobo.423.1"> experts or end users write labeling functions for the</span><a id="_idIndexMarker126"/><span class="koboSpan" id="kobo.424.1"> outcome of interest. </span><span class="koboSpan" id="kobo.424.2">For example, if the outcome of interest is the amount spent by customers in the last five days, then the labeling function returns the amount spent by taking the last five days of transaction data as input. </span><span class="koboSpan" id="kobo.424.3">We will take a look at this example </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">as follows.</span></span></p>
<p><span class="koboSpan" id="kobo.426.1">Let us first install the </span><strong class="source-inline"><span class="koboSpan" id="kobo.427.1">composeml</span></strong><span class="koboSpan" id="kobo.428.1"> Python package. </span><span class="koboSpan" id="kobo.428.2">It is an open source Python library for </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">prediction engineering:</span></span></p>
<pre class="console"><span class="koboSpan" id="kobo.430.1">
pip install composeml</span></pre> <p><span class="koboSpan" id="kobo.431.1">We will create the label for the total purchase spend amount in the next five days based on the customer’s transactions </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">data history.</span></span></p>
<p><span class="koboSpan" id="kobo.433.1">For this, let us first </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">import </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.435.1">composeml</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.437.1">
import composeml as cp</span></pre> <p><span class="koboSpan" id="kobo.438.1">Then, load the </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">sample data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.440.1">
from demo.next_purchase import load_sample
df = load_sample()</span></pre> <p><span class="koboSpan" id="kobo.441.1">Next, to start using the Compose function, we need to define the </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">labeling function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.443.1">
def amount_spent(df):
total = df['amount'].sum()
return total</span></pre> <p><span class="koboSpan" id="kobo.444.1">Next, we will build </span><strong class="source-inline"><span class="koboSpan" id="kobo.445.1">LabelMaker</span></strong><span class="koboSpan" id="kobo.446.1"> to run a search to automatically extract training examples from historical data. </span><span class="koboSpan" id="kobo.446.2">LabelMaker generates labels using labeling functions. </span><span class="koboSpan" id="kobo.446.3">Now, let us build </span><strong class="source-inline"><span class="koboSpan" id="kobo.447.1">LabelMaker </span></strong><span class="koboSpan" id="kobo.448.1">using the </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">labeling function:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.450.1">
label_maker = cp.LabelMaker(
target_dataframe_name="customer_id",
time_index="transaction_time",
labeling_function=amount_spent,
window_size="5d",
)</span></pre> <p><span class="koboSpan" id="kobo.451.1">Now, let us search and extract the labels using </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">label search:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.453.1">
labels = label_maker.search(
df.sort_values('transaction_time'),
num_examples_per_instance=-1,
ga</span><a id="_idTextAnchor058"/><span class="koboSpan" id="kobo.454.1">p=1,
verbose=True,
)
labels.head()</span></pre> <p><span class="koboSpan" id="kobo.455.1">We can transform the labels by applying a threshold to binary labels </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.457.1">
labels = labels.threshold(300)
labels.head()</span></pre> <p><span class="koboSpan" id="kobo.458.1">Now, the labels </span><a id="_idIndexMarker127"/><span class="koboSpan" id="kobo.459.1">DataFrame contains a label for the amount (</span><strong class="source-inline"><span class="koboSpan" id="kobo.460.1">amount_spent</span></strong><span class="koboSpan" id="kobo.461.1">) forecast to be spent in the next five days. </span><span class="koboSpan" id="kobo.461.2">So, we have seen that Compose is used to </span><a id="_idIndexMarker128"/><span class="koboSpan" id="kobo.462.1">generate labels for the transactions by defining labeling function and LabelMaker. </span><span class="koboSpan" id="kobo.462.2">This labeled data will be</span><a id="_idIndexMarker129"/><span class="koboSpan" id="kobo.463.1"> used for supervised learning for prediction problems. </span><span class="koboSpan" id="kobo.463.2">More documentation about </span><em class="italic"><span class="koboSpan" id="kobo.464.1">Compose</span></em><span class="koboSpan" id="kobo.465.1"> can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">at (</span></span><a href="https://compose.alteryx.com/en/stable/index.html"><span class="No-Break"><span class="koboSpan" id="kobo.467.1">https://compose.alteryx.com/en/stable/index.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.468.1">).</span></span></p>
<h1 id="_idParaDest-58"><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.469.1">Labeling data using semi-supervised learning</span></h1>
<p><span class="koboSpan" id="kobo.470.1">In this section, let us see how to generate labels using </span><span class="No-Break"><span class="koboSpan" id="kobo.471.1">semi-supervised learning.</span></span></p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.472.1">What is semi-supervised learning?</span></h2>
<p><span class="koboSpan" id="kobo.473.1">Semi-supervised learning </span><a id="_idIndexMarker130"/><span class="koboSpan" id="kobo.474.1">falls in between supervised learning and </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">unsupervised learning:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.476.1">In the case of supervised learning, all the training dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">is labeled</span></span></li>
<li><span class="koboSpan" id="kobo.478.1">In the case of unsupervised learning, all the training dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">is unlabeled</span></span></li>
<li><span class="koboSpan" id="kobo.480.1">In the case of semi-supervised learning, a very small set of data is labeled and the majority of the dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">is unlabeled</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.482.1">In this case, first we </span><a id="_idIndexMarker131"/><span class="koboSpan" id="kobo.483.1">will generate the pseudo-labels using a</span><a id="_idIndexMarker132"/><span class="koboSpan" id="kobo.484.1"> small part of the labeled dataset with </span><span class="No-Break"><span class="koboSpan" id="kobo.485.1">supervised learning</span></span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.487.1">In this first step, we use this training dataset to train the supervised model and generate the additional pseudo </span><span class="No-Break"><span class="koboSpan" id="kobo.488.1">labeled dataset:</span></span><p class="list-inset"><em class="italic"><span class="koboSpan" id="kobo.489.1">Training dataset = small set of </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.490.1">labeled dataset</span></em></span></p></li>
<li><span class="koboSpan" id="kobo.491.1">In this second step, we will use the small set of labeled dataset along with the pseudo-labeled dataset generated in the </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">first step:</span></span><p class="list-inset"><em class="italic"><span class="koboSpan" id="kobo.493.1">Training dataset = small set of labeled dataset + </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.494.1">pseudo-labeled dataset</span></em></span></p></li>
</ol>
<h2 id="_idParaDest-60"><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.495.1">What is pseudo-labeling?</span></h2>
<p><span class="koboSpan" id="kobo.496.1">Pseudo-labeling is </span><a id="_idIndexMarker133"/><span class="koboSpan" id="kobo.497.1">a </span><strong class="bold"><span class="koboSpan" id="kobo.498.1">semi-supervised machine learning</span></strong><span class="koboSpan" id="kobo.499.1"> technique that generates labels for unlabeled data by leveraging a model trained on a separate small set of labeled data using </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">supervised learning.</span></span></p>
<p><span class="koboSpan" id="kobo.501.1">For this demonstration, let’s download the </span><em class="italic"><span class="koboSpan" id="kobo.502.1">heart failure dataset</span></em><span class="koboSpan" id="kobo.503.1"> from GitHub repo using the link in the </span><em class="italic"><span class="koboSpan" id="kobo.504.1">Technical </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.505.1">requirements</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.506.1"> section.</span></span></p>
<p><span class="koboSpan" id="kobo.507.1">We know our demo dataset, heart failure dataset, is already labeled. </span><span class="koboSpan" id="kobo.507.2">But we are going to modify the dataset by splitting it into two parts. </span><span class="koboSpan" id="kobo.507.3">One will have labels and the other will be unlabeled. </span><span class="koboSpan" id="kobo.507.4">We generate pseudo-labels for unlabeled data from a labeled dataset. </span><span class="koboSpan" id="kobo.507.5">Then, finally, we will combine this pseudo-labeled dataset with the initial labeled dataset to train the </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">final model.</span></span></p>
<p><span class="koboSpan" id="kobo.509.1">Let’s import the libraries </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.511.1">
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier</span></pre> <p><span class="koboSpan" id="kobo.512.1">Let’s download the</span><a id="_idIndexMarker134"/> <span class="No-Break"><span class="koboSpan" id="kobo.513.1">income dataset:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.514.1">
data = pd.read_csv("&lt;your_path&gt;/heart_failure_dataset.csv ", encoding='latin-1')
 y = training.DEATH_EVENT #Output for each example
 x = training.drop('DEATH_EVENT', axis=1)  #Input</span></pre> <p><span class="koboSpan" id="kobo.515.1">Let’s split the data </span><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.516.1">with and without labels in the 30:70 ratio. </span><span class="koboSpan" id="kobo.516.2">Here, we want 30% of the data to have labels and 70% of the data to be unlabeled to demonstrate a real-world scenario where we have less data with labels and more data </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">without labels:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.518.1">
x_train,x_test,y_train,_ = train_test_split(x,y,test_size=.7)
x_train.shape,y_train.shape,x_test.shape</span></pre> <p><span class="koboSpan" id="kobo.519.1">Now, let’s fit the data to the model and create the model. </span><span class="koboSpan" id="kobo.519.2">Here, we will use the Random Forest classifier. </span><span class="koboSpan" id="kobo.519.3">Think of Random Forest as a group of clever friends in the world of computers. </span><span class="koboSpan" id="kobo.519.4">Each friend knows a little something about the data we give them and can make decisions. </span><span class="koboSpan" id="kobo.519.5">When we put all their ideas together, we get a smart tool that can help us understand and predict things. </span><span class="koboSpan" id="kobo.519.6">It’s like having a team of experts to guide us through the jungle of data. </span><span class="koboSpan" id="kobo.519.7">In this chapter, we’ll explore how this friendly Random Forest can assist us in making sense of data and making </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">better decisions.</span></span></p>
<p><span class="koboSpan" id="kobo.521.1">Random Forest is a versatile ML algorithm that can be used for both regression and classification tasks. </span><span class="koboSpan" id="kobo.521.2">The following subsection provides an explanation </span><span class="No-Break"><span class="koboSpan" id="kobo.522.1">of both.</span></span></p>
<h3><span class="koboSpan" id="kobo.523.1">Random Forest classifier</span></h3>
<p><span class="koboSpan" id="kobo.524.1">In classification tasks, the Random Forest classifier</span><a id="_idIndexMarker135"/><span class="koboSpan" id="kobo.525.1"> is used to predict and assign a class or category to a given input based on a set of input features. </span><span class="koboSpan" id="kobo.525.2">It’s particularly useful for tasks such as spam detection, image classification, and sentiment analysis. </span><span class="koboSpan" id="kobo.525.3">The algorithm works by constructing multiple decision trees during the training process. </span><span class="koboSpan" id="kobo.525.4">Each decision tree is like a “vote” on which class an input should be </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">assigned to.</span></span></p>
<p><span class="koboSpan" id="kobo.527.1">The final prediction is made by taking a majority vote among all the decision trees in the forest. </span><span class="koboSpan" id="kobo.527.2">This ensemble approach often results in robust and </span><span class="No-Break"><span class="koboSpan" id="kobo.528.1">accurate classification:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.529.1">
pseudomodel =  RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)</span></pre> <p><span class="koboSpan" id="kobo.530.1">We are training the pseudo-model with 30% of the labeled </span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">training data:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.532.1">
pseudomodel.fit(x_train,y_train)</span></pre> <p><span class="koboSpan" id="kobo.533.1">Let us calculate the accuracy score for data label </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">model training:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.535.1">
pseudomodel.score(x_train,y_train)
0.95</span></pre> <p><span class="koboSpan" id="kobo.536.1">Now, let us use this pseudo-model to predict the labels for the 70% unlabeled data </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.538.1">x_test</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.540.1">
y_new =  pseudomodel.predict(x_test)
y_new.shape</span></pre> <p><span class="koboSpan" id="kobo.541.1">Let us concatenate</span><a id="_idIndexMarker136"/><span class="koboSpan" id="kobo.542.1"> both the datasets’ features and labels, that is, the original labels </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">and pseudo-labels:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.544.1">
# Take a subset of the test set with pseudo-labels and append it onto
# the training set
from sklearn.utils import shuffle
sampled_pseudo_data = pseudo_data.sample(n=150)
temp_train = pd.concat([x, y], axis=1)
augemented_train = pd.concat([sampled_pseudo_data, temp_train])
shuffle(augemented_train)
x = augemented_train.drop('DEATH_EVENT', axis=1)
y = augemented_train.DEATH_EVENT
x_train_final, x_test_final, y_train_final, y_test_final = train_test_split(x, y, test_size=0.3, random_state=42)</span></pre> <p><span class="koboSpan" id="kobo.545.1">Now, let us </span><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.546.1">create a model for the entire dataset </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.548.1">RandomForestClassifier</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.550.1">
finalmodel =  RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
 #this model is with augmented data
finalmodel.fit(x_train_final,y_train_final)
finalmodel.score(x_train_final,y_train_final)</span></pre> <p><span class="koboSpan" id="kobo.551.1">We get the </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer052">
<span class="koboSpan" id="kobo.553.1"><img alt="Figure 2.5 – final model" src="image/B18944_02_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.554.1">Figure 2.5 – final model</span></p>
<p><span class="koboSpan" id="kobo.555.1">To summarize, we </span><a id="_idIndexMarker137"/><span class="koboSpan" id="kobo.556.1">have imported the </span><em class="italic"><span class="koboSpan" id="kobo.557.1">heart failure dataset</span></em><span class="koboSpan" id="kobo.558.1"> and then changed it to split the dataset into labeled and unlabeled datasets. </span><span class="koboSpan" id="kobo.558.2">Then, we applied semi-supervised learning to generate pseudo-labels. </span><span class="koboSpan" id="kobo.558.3">After that, we combined the original and pseudo-labeled training datasets and again applied the Random Forest regressor to generate the final labels for the unseen dataset using the </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">final model.</span></span></p>
<h1 id="_idParaDest-61"><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.560.1">Labeling data using K-means clustering</span></h1>
<p><span class="koboSpan" id="kobo.561.1">In this section, we are going to</span><a id="_idIndexMarker138"/><span class="koboSpan" id="kobo.562.1"> learn what the K-means clustering </span><a id="_idIndexMarker139"/><span class="koboSpan" id="kobo.563.1">algorithm is and how the K-means clustering algorithm is used to predict labels. </span><span class="koboSpan" id="kobo.563.2">Let us understand what unsupervised </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">learning is.</span></span></p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.565.1">What is unsupervised learning?</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.566.1">Unsupervised learning</span></strong><span class="koboSpan" id="kobo.567.1"> is a </span><a id="_idIndexMarker140"/><span class="koboSpan" id="kobo.568.1">category of machine learning where the algorithm is tasked with discovering patterns, structures, or relationships within a dataset without explicit guidance or labeled outputs. </span><span class="koboSpan" id="kobo.568.2">In other words, the algorithm explores the inherent structure of the data on its own. </span><span class="koboSpan" id="kobo.568.3">The primary goal of unsupervised learning is often to uncover hidden patterns, group similar data points, or reduce the dimensionality of </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">the data.</span></span></p>
<p><span class="koboSpan" id="kobo.570.1">In the case of </span><strong class="bold"><span class="koboSpan" id="kobo.571.1">unsupervised learning</span></strong><span class="koboSpan" id="kobo.572.1">, we have data with no target variable. </span><span class="koboSpan" id="kobo.572.2">We group observations based on similarity into different clusters. </span><span class="koboSpan" id="kobo.572.3">Once we have a limited number of clusters, we can register the labels for </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">those clusters.</span></span></p>
<p><span class="koboSpan" id="kobo.574.1">For example, in the case of customer segmentation, customers with a similar income range are grouped and formed into clusters, and then their spending power is associated with those clusters. </span><span class="koboSpan" id="kobo.574.2">Any new training data without labels can be measured for similarity to identify the cluster, and then the same label can be associated with all observations in the </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">same cluster.</span></span></p>
<p><span class="koboSpan" id="kobo.576.1">K-means clustering is a specific algorithm within the realm of unsupervised learning. </span><span class="koboSpan" id="kobo.576.2">It is a partitioning technique that divides a dataset into distinct, non-overlapping subsets or clusters. </span><span class="koboSpan" id="kobo.576.3">Now, let’s dive deep into </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">K-means clustering.</span></span></p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.578.1">K-means clustering</span></h2>
<p><span class="koboSpan" id="kobo.579.1">Traversing the world of </span><a id="_idIndexMarker141"/><span class="koboSpan" id="kobo.580.1">data is like finding your way through a maze. </span><span class="koboSpan" id="kobo.580.2">Imagine</span><a id="_idIndexMarker142"/><span class="koboSpan" id="kobo.581.1"> you have a bunch of colorful marbles and you want to group similar ones. </span><span class="koboSpan" id="kobo.581.2">K-means clustering is your helper in this quest. </span><span class="koboSpan" id="kobo.581.3">It’s like a sorting magician that organizes your marbles into distinct groups, making it easier to understand them. </span><span class="koboSpan" id="kobo.581.4">K-means clustering is a cool trick that helps you make sense of your data by finding hidden patterns and grouping things that </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">go together.</span></span></p>
<p><span class="koboSpan" id="kobo.583.1">Let us download the </span><em class="italic"><span class="koboSpan" id="kobo.584.1">adult income</span></em><span class="koboSpan" id="kobo.585.1"> dataset, using the link in the </span><em class="italic"><span class="koboSpan" id="kobo.586.1">Technical </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.587.1">requirements</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.588.1"> section.</span></span></p>
<p><span class="koboSpan" id="kobo.589.1">The selected features from this dataset will be used as input to the K-means algorithm, which will then partition the data into clusters based on the patterns in these </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">selected features.</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.591.1">
# Select features for clustering
 selected_features = ['hoursperweek', 'education-num']</span></pre> <p><strong class="source-inline"><span class="koboSpan" id="kobo.592.1">selected_features</span></strong><span class="koboSpan" id="kobo.593.1"> is a list that contains the names of the features (columns) you want to use for clustering. </span><span class="koboSpan" id="kobo.593.2">In this case, the features selected are </span><strong class="source-inline"><span class="koboSpan" id="kobo.594.1">‘hoursperweek’</span></strong><span class="koboSpan" id="kobo.595.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.596.1">‘education-num’</span></strong><span class="koboSpan" id="kobo.597.1">. </span><span class="koboSpan" id="kobo.597.2">These are likely columns from a dataset containing information about individuals, where </span><strong class="source-inline"><span class="koboSpan" id="kobo.598.1">‘hoursperweek’</span></strong><span class="koboSpan" id="kobo.599.1"> represents the number of hours worked per week, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.600.1">‘education-num’</span></strong><span class="koboSpan" id="kobo.601.1"> represents the number of years </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">of education.</span></span></p>
<p><span class="koboSpan" id="kobo.603.1">When performing K-means clustering, it’s common to choose relevant features that you believe might contribute to the formation of meaningful clusters. </span><span class="koboSpan" id="kobo.603.2">The choice of features depends on the nature of your data and the problem you’re trying to solve. </span><span class="koboSpan" id="kobo.603.3">We will identify similar features and then group them into clusters using </span><span class="No-Break"><span class="koboSpan" id="kobo.604.1">K-means clustering.</span></span></p>
<p><span class="koboSpan" id="kobo.605.1">In the case of</span><a id="_idIndexMarker143"/><span class="koboSpan" id="kobo.606.1"> K-means clustering, the following sequence of</span><a id="_idIndexMarker144"/><span class="koboSpan" id="kobo.607.1"> steps needs to </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">be followed:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.609.1">First, identify the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">clusters randomly:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.611.1">
n_clusters = random.randint(2,4)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.612.1">A random number of clusters (between 2 and 4) </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">is chosen.</span></span></p></li> <li><span class="koboSpan" id="kobo.614.1">Initialize K-means with </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">random centroids:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.616.1">
kmeans = KMeans(n_clusters=n_clusters, init='random', random_state=0)
kmeans.fit(X_scaled)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.617.1">The K-means algorithm is initialized with random centroids and fitted to the </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">scaled data.</span></span></p></li> <li><span class="koboSpan" id="kobo.619.1">Perform K-means clustering and update centroids </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">until convergence:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.621.1">
while True:
    # Assign data points to clusters
    cluster_labels = kmeans.predict(X_scaled)
    # Calculate new centroids
    new_centroids = np.array([X_scaled[cluster_labels == i].mean(axis=0) for i in range(n_clusters)])</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.622.1">The code calculates new centroids by computing the mean of the data points in </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">each cluster:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.624.1">    # Check for convergence by comparing old and new centroids
    if np.allclose(kmeans.cluster_centers_, new_centroids):
    break</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.625.1">The loop checks for convergence by comparing the old centroids with the new centroids. </span><span class="koboSpan" id="kobo.625.2">If they are very close, indicating that the centroids have not changed significantly, the loop breaks, and the clustering </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">process stops:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.627.1">    # Update centroids
    kmeans.cluster_centers_ = new_centroids
    # Update inertia
    inertia = kmeans.inertia_</span></pre><p class="list-inset"><strong class="source-inline"><span class="koboSpan" id="kobo.628.1">kmeans.cluster_centers_</span></strong><span class="koboSpan" id="kobo.629.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.630.1">inertia</span></strong><span class="koboSpan" id="kobo.631.1"> are updated within the loop, ensuring that the model parameters are adjusted in </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">each iteration.</span></span></p></li> <li><span class="koboSpan" id="kobo.633.1">Calculate </span><a id="_idIndexMarker145"/><span class="koboSpan" id="kobo.634.1">metrics (Dunn’s Index). </span><span class="koboSpan" id="kobo.634.2">Inter-cluster and</span><a id="_idIndexMarker146"/><span class="koboSpan" id="kobo.635.1"> intra-cluster distances are calculated to compute </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">Dunn’s Index:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.637.1">
intercluster_distances = pairwise_distances(kmeans.cluster_centers_)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.638.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.639.1">pairwise_distances</span></strong><span class="koboSpan" id="kobo.640.1"> from scikit-learn is used to calculate the distances between cluster centers (inter-cluster distances). </span><span class="koboSpan" id="kobo.640.2">The result is a matrix where the element at position (</span><strong class="source-inline"><span class="koboSpan" id="kobo.641.1">i, j</span></strong><span class="koboSpan" id="kobo.642.1">) represents the distance between the </span><strong class="source-inline"><span class="koboSpan" id="kobo.643.1">i</span></strong><span class="koboSpan" id="kobo.644.1">-th and </span><strong class="source-inline"><span class="koboSpan" id="kobo.645.1">j</span></strong><span class="koboSpan" id="kobo.646.1">-th </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">cluster centers.</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.648.1">intracluster_distances = np.array([np.max(pdist(X_scaled[cluster_labels == i])) for i in range(n_clusters)])</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.649.1">For each cluster, </span><strong class="source-inline"><span class="koboSpan" id="kobo.650.1">pdist</span></strong><span class="koboSpan" id="kobo.651.1"> (pairwise distance) is used to calculate the distances between all pairs of data points within the cluster. </span><strong class="source-inline"><span class="koboSpan" id="kobo.652.1">np.max</span></strong><span class="koboSpan" id="kobo.653.1"> then finds the maximum distance within each cluster. </span><span class="koboSpan" id="kobo.653.2">This results in an array where each element represents the maximum intra-cluster distance for a </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">specific cluster:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.655.1">min_intercluster_distances = min(np.min(intercluster_distances[~np.eye(n_clusters, dtype=bool)]), min_intercluster_distances)
max_intracluster_distances = max(np.max(intracluster_distances), max_intracluster_distances)</span></pre><p class="list-inset"><strong class="source-inline"><span class="koboSpan" id="kobo.656.1">min_intercluster_distances</span></strong><span class="koboSpan" id="kobo.657.1"> is </span><a id="_idIndexMarker147"/><span class="koboSpan" id="kobo.658.1">updated by finding the minimum </span><a id="_idIndexMarker148"/><span class="koboSpan" id="kobo.659.1">inter-cluster distance, excluding diagonal elements (distances between a cluster and itself), and comparing it with the </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">current minimum.</span></span></p><p class="list-inset"><strong class="source-inline"><span class="koboSpan" id="kobo.661.1">max_intracluster_distances</span></strong><span class="koboSpan" id="kobo.662.1"> is updated by finding the maximum intra-cluster distance across all clusters and comparing it with the </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">current maximum.</span></span></p><p class="list-inset"><strong class="bold"><span class="koboSpan" id="kobo.664.1">Dunn’s index</span></strong><span class="koboSpan" id="kobo.665.1"> is calculated </span><a id="_idIndexMarker149"/><span class="koboSpan" id="kobo.666.1">by dividing the minimum inter-cluster distance by the maximum </span><span class="No-Break"><span class="koboSpan" id="kobo.667.1">intra-cluster distance:</span></span></p><p class="list-inset"><em class="italic"><span class="koboSpan" id="kobo.668.1">dunn_index = min_intercluster_distances / </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.669.1">max_intracluster_distances</span></em></span></p><p class="list-inset"><span class="koboSpan" id="kobo.670.1">It is a measure of how well-separated clusters are relative to their </span><span class="No-Break"><span class="koboSpan" id="kobo.671.1">internal cohesion:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.672.1"># Print the results
print(f"Number of clusters: {n_clusters}")
print(f"Inertia: {inertia}")
print(f"Dunn's Index: {dunn_index}")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.673.1">We get the </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">following result:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer053">
<span class="koboSpan" id="kobo.675.1"><img alt="Figure 2.6 – K-means clustering metrics" src="image/B18944_02_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.676.1">Figure 2.6 – K-means clustering metrics</span></p>
<p><span class="koboSpan" id="kobo.677.1">Let us understand the metrics Inertia and Dunn's index in </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">the results.</span></span></p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.679.1">Inertia</span></h2>
<p><span class="koboSpan" id="kobo.680.1">Inertia is </span><a id="_idIndexMarker150"/><span class="koboSpan" id="kobo.681.1">nothing but the sum of the distance of all the points from </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">the centroid.</span></span></p>
<p><span class="koboSpan" id="kobo.683.1">If all the points are close to each other, that means they are similar and it is a good cluster. </span><span class="koboSpan" id="kobo.683.2">So, we aim for a small distance for all points from </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">the centroid.</span></span></p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor068"/><span class="koboSpan" id="kobo.685.1">Dunn's index</span></h2>
<p><span class="koboSpan" id="kobo.686.1">If the clusters are</span><a id="_idIndexMarker151"/><span class="koboSpan" id="kobo.687.1"> far from each other, that means the different clusters are clearly separate groups of observations. </span><span class="koboSpan" id="kobo.687.2">So, we measure the distance between the centroids of each cluster, which is called the inter-cluster distance. </span><span class="koboSpan" id="kobo.687.3">A large inter-cluster distance is ideal for </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">good clusters.</span></span></p>
<p><span class="koboSpan" id="kobo.689.1">Now, let us print the cluster labels obtained in </span><span class="No-Break"><span class="koboSpan" id="kobo.690.1">K-means clustering:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.691.1">
# Add the cluster labels as a new column to the original dataset
X['cluster_label'] = kmeans.labels_
# Print the dataset with cluster labels
X.head()</span></pre> <p><span class="koboSpan" id="kobo.692.1">We get the </span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<span class="koboSpan" id="kobo.694.1"><img alt="Figure 2.7 ﻿– Cluster labels" src="image/B18944_02_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.695.1">Figure 2.7 – Cluster labels</span></p>
<p><span class="koboSpan" id="kobo.696.1">We can use the cluster labels as a feature to predict the adult income </span><span class="No-Break"><span class="koboSpan" id="kobo.697.1">using regression.</span></span></p>
<p><span class="koboSpan" id="kobo.698.1">To summarize, we have seen what the K-means clustering algorithm is and how it is used to create clusters for </span><em class="italic"><span class="koboSpan" id="kobo.699.1">adult income</span></em><span class="koboSpan" id="kobo.700.1"> dataset. </span><span class="koboSpan" id="kobo.700.2">The selected features from this dataset will be used as input to the K-means algorithm, which will then partition the data into clusters based on the patterns in these </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">selected features.</span></span></p>
<p><span class="koboSpan" id="kobo.702.1">The complete code notebook is available in this book’s </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">GitHub repository.</span></span></p>
<h1 id="_idParaDest-66"><a id="_idTextAnchor069"/><span class="koboSpan" id="kobo.704.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.705.1">In this chapter, we have seen the implementation of rules using Snorkel labeling functions for predicting the income range and labeling functions using the Compose library to predict the total amount spent by a customer during a given period. </span><span class="koboSpan" id="kobo.705.2">We have learned how semi-supervised learning can be used to generate pseudo-labels and data augmentation. </span><span class="koboSpan" id="kobo.705.3">We also learned how K-means clustering can be used to cluster the income features and then predict the income for each cluster based on </span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">business knowledge.</span></span></p>
<p><span class="koboSpan" id="kobo.707.1">In the next chapter, we are going to learn how we can label data for regression using the Snorkel Python library, semi-supervised learning, and K-means clustering. </span><span class="koboSpan" id="kobo.707.2">Let us explore that in the </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">next chapter.</span></span></p>
</div>
</body></html>