- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Utilizing Qlik AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Qlik AutoML leverages the power of artificial intelligence and automation to
    empower users of all skill levels to build and deploy machine learning models,
    without the need for extensive coding or data science backgrounds. By automating
    repetitive tasks and providing intelligent recommendations, Qlik AutoML streamlines
    the entire machine learning workflow, making it accessible to a broader audience.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will delve into the world of Qlik AutoML, exploring its
    capabilities, benefits, and practical applications. We will provide a comprehensive
    overview of the underlying concepts and techniques that enable Qlik AutoML to
    automate the machine learning process. Moreover, we will guide you through the
    step-by-step implementation of AutoML models within the Qlik ecosystem, highlighting
    its seamless integration with the Qlik Sense analytics platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Features of Qlik AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Qlik AutoML in a cloud environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and monitoring a machine learning model with Qlik AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting Qlik AutoML to an on-premises environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices with Qlik AutoML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Features of Qlik AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Qlik AutoML is a tool within the Qlik Sense analytics platform that automates
    the process of building and deploying machine learning models. It simplifies the
    machine learning workflow and allows users to create predictive models, without
    requiring in-depth knowledge of data science or programming. Some of the key features
    of Qlik AutoML include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automated model selection**: Qlik AutoML automatically selects the best machine
    learning algorithm based on data and the prediction task, saving users from manually
    exploring and comparing different algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameter tuning**: Qlik AutoML optimizes the hyperparameters of the
    selected machine learning model to improve its performance and accuracy. Hyperparameter
    tuning helps fine-tune the model’s behavior and makes it more effective in making
    predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-validation**: Qlik AutoML uses cross-validation techniques to evaluate
    the performance of models. It splits data into multiple subsets and trains and
    tests the models on different combinations, providing more robust performance
    metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model evaluation**: Qlik AutoML provides various performance metrics to evaluate
    models, such as accuracy, precision, recall, and the F1 score. These metrics help
    users assess the model’s predictive power and choose the best-performing model
    for their use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment**: Once the model is built and selected, Qlik AutoML enables
    easy deployment within the Qlik Sense environment. Users can seamlessly integrate
    the predictive models into their existing Qlik apps and dashboards for real-time
    insights and decision-making.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Some of the features (including hyperparameter optimization and the prediction
    API) will require a paid tier of Qlik AutoML. Also, the number of deployments,
    concurrent tasks, and dataset limits are defined by license tier. Specific tier
    limits should be verified by Qlik Sales.
  prefs: []
  type: TYPE_NORMAL
- en: Qlik AutoML aims to democratize machine learning and empower business users
    to leverage advanced analytics capabilities, without extensive technical expertise.
    In [*Chapter 4*](B19863_04.xhtml#_idTextAnchor061), we looked at the general concepts
    in creating a good machine learning solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might remember from that chapter, there are three types of machine learning
    problems that Qlik AutoML can solve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary classification**: Any question that can be answered with a yes or
    no'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class classification**: Questions where there could be multiple outcome
    choices'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression/numeric**: Predicting a number at a future point'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qlik AutoML is available as part of the Qlik Cloud offering. In the following
    section, we will get familiar with the actual process of getting a deployed, production-ready
    model from our training data.
  prefs: []
  type: TYPE_NORMAL
- en: Using Qlik AutoML in a cloud environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several steps when deploying a machine learning model using Qlik
    AutoML. These steps are illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1: The AutoML workflow](img/B19863_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: The AutoML workflow'
  prefs: []
  type: TYPE_NORMAL
- en: As you might remember from our earlier chapters, the first step of every machine
    learning project is to define a business problem and question, followed by the
    steps required for data cleaning, preparation, and modeling. Typically, data cleaning
    and transformation part can take up 80–90% of the time spent on a project.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a machine-learning-ready dataset, we will continue by creating
    a machine learning experiment.
  prefs: []
  type: TYPE_NORMAL
- en: In automated machine learning, the process of training machine learning algorithms
    on a specific dataset and target is automated. When you create an experiment and
    load your dataset, the system automatically examines and prepares data for machine
    learning. It provides you with statistics and insights about each column, aiding
    in the selection of a target variable. Once the training begins, multiple algorithms
    analyze the data, searching for patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Upon completion of the training process, you can assess the performance of the
    generated machine learning models using scores and rankings. By adjusting parameters
    and repeating the training, you can generate multiple versions of the models.
    After carefully evaluating the options, you can choose the model that performs
    best on your dataset. An experiment can have multiple versions, each using one
    or more algorithms, and one experiment can result in several machine learning
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Simply, during the experiment phase, we will fine-tune the model and try to
    achieve the best possible accuracy. Once we are happy with the model, we can deploy
    it into production and start utilizing it in our analysis. We will go through
    each of these steps in our hands-on example in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and monitoring a machine learning model with Qlik AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create an actual implementation using Qlik AutoML.
    We will utilize the famous Iris dataset that we have already used in this book.
    The data preparation part for Iris dataset is already done, so we can jump into
    the model training and experiment part directly.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can find the datasets used in this example in the GitHub repository for
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Only users with Professional entitlement can create experiments.This is a limitation
    at the license level.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume that we have already uploaded the `iris` dataset into our cloud
    tenant. Now, we will start to define a business question. This question defines
    what we would like to achieve from our machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we know, the Iris dataset consists of measurements of four features of three
    different species of Iris flowers. These features are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sepal length**: The length of the sepal, which is the outermost part of the
    flower that protects the petals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sepal width**: The width of the sepal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Petal length**: The length of the petals, which are the colorful leaf-like
    structures inside the flower'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Petal width**: The width of the petals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The dataset contains 150 instances or samples, with 50 samples for each of
    the three Iris species – `setosa`, `versicolor`, and `virginica`. To define a
    machine learning question to predict species in the Iris dataset, we will frame
    it as a multi-class classification problem. Here is a sample question that we
    will form before our investigation:'
  prefs: []
  type: TYPE_NORMAL
- en: Given the measurements of sepal length, sepal width, petal length, and petal
    width, can we accurately classify the species of Iris flowers into setosa, versicolor,
    or virginica?
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the machine learning task involves training a model to learn the
    patterns and relationships between the input features (sepal length, sepal width,
    petal length, and petal width) and the corresponding output classes (`setosa`,
    `versicolor`, and `virginica`). The goal is to develop a predictive model that
    can accurately classify new instances of iris flowers into one of the three species,
    based on their measurements. AutoML will choose the best-performing model for
    us, based on the selected target and variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin the actual model creation by creating a new machine learning
    experiment. To do that, select **+ Add new** � **New ML experiment**, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: A new machine learning experiment](img/B19863_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: A new machine learning experiment'
  prefs: []
  type: TYPE_NORMAL
- en: A new window will open. Insert a name for your new experiment, and select a
    space for it. In my example, I will call the experiment `Iris exp`. Select **Create**
    to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you can select a dataset for training. Select `iris.csv`, which we uploaded
    earlier to our tenant. A preview window will open. In this window, we will define
    our target field. It will also give us important information about the dataset.
    You should see a preview window like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3: Schema view in the machine learning experiment wizard](img/B19863_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Schema view in the machine learning experiment wizard'
  prefs: []
  type: TYPE_NORMAL
- en: You are currently in `String` for the `species` field and `Float (Double)` for
    other fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'All columns have been recognized as `Numeric` or `Categorical` fields. This
    can be changed for each field if needed. We can also see from the `Insights` column
    that our “species” feature has been automatically one-hot encoded. If there are
    any warnings related to some of the features, we can also see these. The following
    information is presented in the `Insights` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Constant**: The column has the same value for all rows. The column can’t
    be used as a target or included feature. This is a pre-set limitation in Qlik
    AutoML to prevent incorrect results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-hot encoded**: The feature type is categorical, and the column has fewer
    than 14 unique values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Impact encoded**: The feature type is categorical, and the column has 14
    or more unique values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High cardinality**: The column has too many unique values and can negatively
    affect model performance if used as a feature. The column can’t be used as a target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`null` values. The column can’t be used as a target or included feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Underrepresented class**: The column has a class with fewer than 10 rows.
    Column can’t be used as a target but can be included as a feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before selecting our target field, we can change our view to **data view**.
    You can do this from the top-right corner of the data preview area. You should
    see the following view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4: Data view in the machine learning experiment wizard](img/B19863_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Data view in the machine learning experiment wizard'
  prefs: []
  type: TYPE_NORMAL
- en: In this view, we can investigate the data content more. We will see a mini-chart
    representing the distribution of data in each numerical field, as well as the
    distribution in categorical fields. We will also get information about the distinct
    and `null` values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now change back to schema view and select our `species` feature as a
    target. To do this, select `species`, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5: Target selection](img/B19863_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: Target selection'
  prefs: []
  type: TYPE_NORMAL
- en: 'All the other features are automatically included in our experiment. In this
    case, we want to keep all features included, but typically, we might want to drop
    some of the fields. On the right side, we can see the summary information about
    the experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6: The experiment summary](img/B19863_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: The experiment summary'
  prefs: []
  type: TYPE_NORMAL
- en: First, we get a summary of our training data. We will see the total amount of
    cells, columns, and rows in a dataset and how many of those have been included
    in the experiment. In our case, all data is included, since we decided to keep
    all the features in our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can see some information about our target. We can also change the target
    before running our experiment. In this case, our target is `species`. The following
    section will give us a summary of the selected features. We will select all the
    features from our data as part of our experiment.
  prefs: []
  type: TYPE_NORMAL
- en: In the algorithm section, we can see that AutoML has identified our model to
    be a multiclass classification, based on our target field. We can decide to exclude
    some algorithms from our experiment if we want. Typically, it is recommended to
    keep all algorithms included.
  prefs: []
  type: TYPE_NORMAL
- en: Under model optimization, we can enable hyperparameter optimization and set
    the maximum time for our experiment to run optimization. Hyperparameter optimization
    will create a series of models from a methodical search for the optimal combination
    of algorithm hyperparameters, maximizing model performance. An experiment can
    take a long time to run if this option is enabled, but results can be more accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ultimately, we will get a reminder of the preprocessing steps that AutoML will
    take care of for us. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`null` values in features that have at least 50% of the values populated. Depending
    on each feature’s data type, AutoML selects `MEAN` or `OTHER` imputation.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Encoding categorical features**: AutoML automatically converts your categorical
    features to numerical values so that algorithms can effectively process and learn
    from your categorical training data. For features with 13 or fewer values, AutoML
    uses one-hot encoding. For features with 14 or more values, AutoML uses impact
    encoding.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature scaling**: AutoML uses feature scaling to normalize the range of
    independent variables in your training data. AutoML calculates the mean and standard
    deviation for each column, and then it calculates the number of standard deviations
    away from the mean for each row.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Automatic holdout of training data**: AutoML extracts 20% of your training
    dataset to be used for final model evaluation. AutoML *holds* that data until
    after model training, when it is used to evaluate the performance of the model.
    The benefit of holdout data is that it is not seen by the model during training
    (unlike cross-validation data), so it is ideal to validate model performance.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Five-fold cross-validation**: After applying the previous preprocessing steps,
    AutoML randomly sorts your remaining training data into five distinct groups called
    “folds” for use in cross-validation. AutoML tests each fold against a model trained
    using the other four folds. In other words, each trained model is tested on a
    piece of data that the model has never seen before.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We are now ready with our experiment setup and can proceed by selecting **Run
    experiment**. The actual model preprocessing and training phase will start; it
    will take a while to finish. After the experiment has finished running the models,
    we will see the **Model metrics** screen, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7: Model metrics](img/B19863_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Model metrics'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the top-performing model was the XGBoost Classification algorithm,
    with an `F1 Macro` score of `0.967`. We also get information about the `F1 Micro`,
    `F1` `Weighted`, and `Accuracy` scores. We covered the meaning of the F1 score
    in the first chapter. The difference between the micro, macro, and weighted F1
    scores is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Macro F1 is the averaged F1 value for each class without weighting (all classes
    are treated equally).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micro F1 is the F1 value calculated across the entire confusion matrix. Calculating
    the micro F1 score is equivalent to calculating the global precision or global
    recall.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted F1 corresponds to the binary classification F1\. It is calculated for
    each class and then combined as a weighted average, considering the number of
    records for each class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you might remember, accuracy measures how often a model makes a correct prediction
    on average. In our case, the accuracy score is 0.967, meaning that our model is
    correct ~97% of cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under `Hyperparameters`, we can also investigate the model parameters. For
    our top-performing model, these look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8: Hyperparameters for XGBoost classification](img/B19863_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Hyperparameters for XGBoost classification'
  prefs: []
  type: TYPE_NORMAL
- en: These are meant to give us more detailed information about the model. Parameters
    are algorithm-specific.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will take a closer look at the **Permutation importance** and **SHAP
    importance** diagrams. We explored the basic concept of both diagrams in [*Chapter
    1*](B19863_01.xhtml#_idTextAnchor014). The following figure shows an example of
    the diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9: The Permutation importance and SHAP importance diagrams](img/B19863_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: The Permutation importance and SHAP importance diagrams'
  prefs: []
  type: TYPE_NORMAL
- en: As you might remember, permutation importance is a measure of how important
    a feature is to the overall prediction of a model. Basically, it describes how
    the model would be affected if you removed its ability to learn from that feature.
    AutoML uses the scikit-learn toolkit to calculate permutation importance.
  prefs: []
  type: TYPE_NORMAL
- en: SHAP importance is a method used to interpret the predictions of machine learning
    models. It provides insights into the contribution of each feature to the prediction
    for a specific instance, or a group of instances. Basically, it represents how
    a feature influences the prediction of a single row, relative to the other features
    in that row and to the average outcome in the dataset. SHAP importance is measured
    at the row level, and AutoML uses various algorithms to calculate the SHAP importance
    score.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the preceding graphs, we can see that **petal_length** is an important
    feature in our prediction, both in terms of permutation and SHAP importance. In
    multiclass problems, we can also investigate the SHAP importance for each class.
    Let’s investigate our SHAP values for each feature by class. Change a graph type
    using the drop-down menu on the SHAP chart, and select **Feature SHAP by class**.
    You should see the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10: SHAP by class](img/B19863_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: SHAP by class'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding graph, we can see that **petal_length** can be used to distinguish
    **setosa** from **versicolor** and **virginica**. Other features are then used
    to determine the species further. We can also see the SHAP importance for each
    specific class if we change the graph type from the drop-down menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to change our experiment, we can select **Configure v2** and modify
    the parameters for it. In this case, we are happy with our model. To deploy our
    model, we can select **Deploy**. We can provide a name for our model if we are
    not happy with the autogenerated one and decide a space for it. AutoML autofills
    some details about the model in the **Description** field. Make sure that the
    **Enable real-time API access** option is enabled, and select **Deploy**. Then,
    select **Open** from the popup, and you should get redirected to our new machine
    learning model. You should then see a view like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11: The deployed machine learning model](img/B19863_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11: The deployed machine learning model'
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side, you can see a schema for the deployed model. The schema
    will tell us what kind of data our model expects when using it for predictions.
    On the right side, we can see details about our model. The other two tabs will
    give us information about predictions that were run manually and the `REST` endpoint
    connectivity. Take a closer look at these, and return to schema view when you
    are done.
  prefs: []
  type: TYPE_NORMAL
- en: We can use our model directly by selecting **Create prediction** and uploading
    a CSV or other data file for our model. This way, we will get our results stored
    as a file in Qlik Cloud. There is also a possibility to schedule predictions and
    apply dynamic naming for result files. However, a more robust way to utilize our
    new model is to use it through a data connector. Let’s take a closer look into
    that next.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our application, we will use another dataset called `iris_test.csv`. To
    begin, upload the file to Qlik Cloud. Create a new Qlik application, and add the
    `iris_test` data to it. Now, we will add the `id` field into our test data. To
    do that, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Our `iris_test.csv` dataset is randomly generated to mimic the characteristics
    of the original `iris` dataset and does not represent the actual data. It should
    be only used for demonstration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will create a connection to our deployed machine learning model. Select
    **Create new connection** under the data connections, and then select **Qlik AutoML**.
    A view like the following will open:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12: A data connection to the machine learning model](img/B19863_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: A data connection to the machine learning model'
  prefs: []
  type: TYPE_NORMAL
- en: Select our deployed model from the `predictions`. Select **Include SHAP** and
    **Include Errors**, since we want our result table to also include these columns.
    SHAP is not available for every algorithm, but it is a good practice to select
    it. If it’s not available, it will not appear in the results table. In our case,
    these values are not available.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `id`. This is a field that ties the generated predictions and our original
    data together. We generated the `id` field in our data earlier. Provide a name
    for your data connection, and click **Save**. You should see a new data connection
    appear in your application; let’s use it. Click **Select data**, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13: Select data in the AutoML connection](img/B19863_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13: Select data in the AutoML connection'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data selection wizard will appear, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14: The data selection wizard](img/B19863_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14: The data selection wizard'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `iris`. This is our original dataset, which we will use to get predictions.
    Select the `predictions` table under the **Tables** section. This is our results
    table. There is no preview available, but you can see the script generated. Select
    **Insert script**. We can now see that our connector generated the following script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Basically, our connector uses the API to send the `iris` table into our machine
    learning model and gets the prediction table back. It takes the connection name
    as a parameter and our data table (`iris`) as an input. We can now load our application
    and investigate the data model viewer. You should see a data model like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15: The data model](img/B19863_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.15: The data model'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, our connector returned a predictions table that is connected
    to our original data table, using `id` as a key. The predictions table contains
    the actual prediction, a possible error message for each row, and the probability
    for every Iris species.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will investigate our prediction results further. Create a new sheet,
    and add the following elements to it:'
  prefs: []
  type: TYPE_NORMAL
- en: A filter pane, with all our features on the top.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A scatter plot, with `Avg(petal_width)` on the *y* axis and `Avg(petal_length)`
    on the *x* axis and `Id` as a bubble. Color by dimension, and select `species_predicted`
    as the coloring dimension.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A table containing all our features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bar chart, with `Count(species_predicted)` as the bar height and `species_predicted`
    as the dimension.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four KPI objects that we will configure later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Four variable inputs, with text and an image container on the left side of each
    of them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You should end up with a layout like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16: The application layout](img/B19863_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16: The application layout'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete application in the GitHub repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the results, we have managed to predict the species with
    our model. If we look at the scatter plot, it seems that our model gives good
    results, keeping in mind that our data is randomly generated. As a last step,
    we will create a simulation that will utilize the API of our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by creating the following variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Make `0` the default value for each variable. Since we have already created
    our variable input and labels, we can assign the variables into inputs and type
    the correct labels into place. You should end up with the following view:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17: The simulation view](img/B19863_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17: The simulation view'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will set up the KPI objects. Select the first KPI, and type `Prediction`
    as a label. Enter the following formula in the **Expression** field:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This syntax might look familiar. We used the same principle in our earlier example
    with R. AutoML connector utilizes advanced analytics integration syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the advanced analytics integration, we have two sets of script functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ScriptEval**: After the hypercube has been aggregated, all rows in the specified
    columns are sent to the connector. The response expected is a single column. If
    multiple columns are returned, the first column that has the same number of rows
    as the input will be picked. The rows in the returned column must be in the same
    order as the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ScriptAggr` is called with many dimensions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For both the preceding sets, there are four different functions based on the
    data types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ScriptEval(Script, Field 1, [Field n])`: The input fields and the response
    must be numeric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ScriptEvalStr(Script, Field 1, [Field n])`: The input fields and the response
    must be a string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ScriptEvalEx(DataTypes, Script, Field 1, [Field n])`: The input fields can
    be either string or numeric, the first parameter is a string of the datatypes,
    and the response must be numeric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ScriptEvalExStr(DataTypes, Script, Field 1, [Field n])`: The input fields
    can be either string or numeric, the first parameter is a string of the datatypes,
    and the response must be string.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We used the `ScriptEvalExStr` function in the preceding example and defined
    the data types of our input fields, since they are numeric but the response is
    a string (`'NNNN'` for numerical fields).
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that our script also contains the details of the connection to be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The connection name refers to the data connector that we created earlier. We
    have also determined the column that we want to get from the model. In our case,
    it is `species_predicted`. Selecting a correct return value is important. You
    can see all the possible fields – for example, from the data manager – if you
    have also used the model during the data load.
  prefs: []
  type: TYPE_NORMAL
- en: In the last part of our script, we will pass the variable values as input to
    our model. The names should match the names of our model schema. That’s why we
    will use the `as` operator to rename the variables.
  prefs: []
  type: TYPE_NORMAL
- en: After configuring the KPI object, you should see `setosa` appear as a value.
    Since all our variables are defined to be `0`, our model will give a prediction
    based on that information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following configurations to the three remaining KPI objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Label**: Setosa'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Script**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Label**: Versicolor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Script**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Label**: Virginica'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Script**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you might have noticed, the script is nearly the same in all our KPIs. We
    define the output by changing the value of the return column from the model. This
    way, we will get the probabilities for each species.
  prefs: []
  type: TYPE_NORMAL
- en: Try to modify the values in input fields, and you should get a prediction and
    probabilities for each of the species in real time.
  prefs: []
  type: TYPE_NORMAL
- en: We have now successfully finished the application and learned how to utilize
    Qlik AutoML in a cloud environment, using both load-time and real-time integration.
    In the following section, we will look at setting up an on-premises environment
    to integrate with Qlik AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting Qlik AutoML to an on-premises environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Qlik AutoML is a cloud tool that integrates tightly with a cloud tenant. However,
    it is possible to utilize the features from an on-premises environment. It is
    important to note that since Qlik AutoML still runs in a cloud environment, all
    data is also passed into the Qlik Cloud tenant. This approach is not suitable
    if the data can’t leave the on-premises environment. The connection is encrypted
    and secure, and Qlik Cloud has all the major security certifications. It is also
    important to note that this approach will require a valid license for Qlik Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'More information about Qlik Cloud security and compliance is available at the
    Qlik Trust site: [https://www.qlik.com/us/trust](https://www.qlik.com/us/trust)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the basic architecture of integrating AutoML with an on-premises
    environment in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18: Qlik AutoML – on-premises and SaaS integration](img/B19863_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.18: Qlik AutoML – on-premises and SaaS integration'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we have our on-premises environment at the bottom.
    We will handle most of the data loads in the on-premises environment in this architecture.
    After preparing a machine-learning-ready dataset, we can utilize Qlik Data Gateway,
    the Qlik CLI, or manually upload to a cloud tenant. This data can be supplemented
    using data coming from other cloud sources. When our training data is in the cloud,
    training the machine learning model will involve the same process from the previous
    section. It is also possible to automate the whole process using tasks, application
    automation, and the Qlik CLI. Once the model is trained, we can then utilize the
    prediction API directly from the on-premises environment.
  prefs: []
  type: TYPE_NORMAL
- en: Specific details about implementing the described environment are different
    in each organization. The preceding diagram can be used as a rough reference,
    but specific implementation should be planned case by case.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this section was to give some ideas about the usage of Qlik AutoML
    in hybrid scenarios. In the following section, we will investigate some of the
    best practices when working with Qlik AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices with Qlik AutoML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are some general guidelines and best practices when working with Qlik
    AutoML. Following these practices and principles will make it easier to get accurate
    results and handle the machine learning project flow. The general principles include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Define the problem**: Clearly define the problem you are trying to solve
    with Qlik AutoML. Identify the variables you want to predict, and understand the
    available data. This is one of the most important best practices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prepare and clean the data**: Ensure that your data is in a format suitable
    for analysis. This may involve cleaning missing values, handling outliers, transforming
    variables, cleaning duplicates, and making sure the data is well formatted. This
    is typically the most time-consuming part of machine learning projects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature engineering**: Explore and create meaningful features from your raw
    data. Qlik AutoML can automate some feature engineering tasks, but it’s still
    important to understand your data and apply domain knowledge to generate relevant
    features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interpretability and explainability**: Understand and interpret the results
    of your models. Qlik AutoML provides tools to interpret generated models, and
    understands the contribution of different features to the predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation and evaluation**: Use proper evaluation metrics to assess the
    performance of your models. Qlik AutoML can provide default metrics, but always
    cross-validate results when possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and maintenance**: Continuously monitor the performance of your
    models in production. Update and retrain the models periodically as new data becomes
    available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative process**: Machine learning is an iterative process, so be prepared
    to refine and improve your models based on feedback and new insights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qlik AutoML is a powerful tool to build machine learning models in an automated
    way, and it can make it easier for end users to understand complex models. When
    utilizing the tool, and keeping in mind the basic principles described previously,
    organizations can get more out of their data. Remember that no machine learning
    tool is a magic box that can solve all the business problems in the world. The
    better you prepare the problem definition and training data, the more accurate
    results you will get from a model.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discovered the usage of Qlik AutoML. We first learned what
    the tool will provide for users and what its key features are. We built our first
    machine learning model with Qlik AutoML using the famous Iris dataset. In this
    section, we discovered how to run experiments and deploy a model from experimentation.
    We also discovered how to utilize the model in a Qlik application, both during
    a data load and in real time. We learned from different metrics how our model
    performed.
  prefs: []
  type: TYPE_NORMAL
- en: In the latter part of this chapter, we took a quick look at an on-premises environment.
    We learned how to utilize Qlik AutoML in hybrid scenarios and how to set up our
    environment in these use cases. We also discovered some of the best practices
    to be used with Qlik AutoML.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, we will dive deep into data visualization. We will
    discover the techniques to visualize machine-learning-related data and investigate
    the use of some of the lesser-used graph types. We will also learn about common
    charts and visualizations, and we will discover some of the settings and configurations
    that will help us get the most out of our data.
  prefs: []
  type: TYPE_NORMAL
