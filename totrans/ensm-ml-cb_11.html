<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Heterogeneous Ensemble for Text Classification Using NLP</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we will cover the following topics:</p>
<ul class="calibre10">
<li class="calibre11">Spam filtering using an ensemble of heterogeneous algorithms</li>
<li class="calibre11">Sentiment analysis of movie reviews using an ensemble model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Text classification is a widely studied area of language processing and text mining. Using text classification mechanisms, we can classify documents into predefined categories based on their content. </span></p>
<p class="calibre2">In this chapter, we'll take a look at how to classify short text messages that get delivered to our mobile phones. While some messages we receive are important, others might represent a serious threat to our privacy. We want to be able to classify the text messages correctly in order to avoid spam and to avoid missing important messages.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spam filtering using an ensemble of heterogeneous algorithms</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">We will use the SMS Spam Collection </span><span class="calibre5">dataset</span><span class="calibre5"> </span><span class="calibre5">from the UCI ML repository to create a spam classifier. Using the spam classifier, we can estimate the polarity of these messages. </span><span class="calibre5">We can use various classifiers to classify the messages either as spam or ham. </span></p>
<p class="calibre2">In this example, we opt for algorithms such as Naive Bayes, random forest, and support vector machines <span class="calibre5">to train our models.</span></p>
<p class="calibre2"><span class="calibre5">We prepare our data using various data-cleaning and preparation mechanisms. To preprocess our data, we will perform the following sequence:</span></p>
<ol class="calibre14">
<li class="calibre11">Convert all text to lowercase</li>
<li class="calibre11">Remove punctuation</li>
<li class="calibre11">Remove stop words</li>
<li class="calibre11">Perform stemming</li>
<li class="calibre11">Tokenize the data</li>
</ol>
<p class="calibre2"><span class="calibre5">We also process our data using <strong class="calibre4">t</strong><strong class="calibre4">erm frequency-inverse data frequency</strong> (<strong class="calibre4">TF-IDF</strong>), </span><span class="calibre5">which tells us how often a word appears in a message or a document. TF is calculated as:</span></p>
<p class="calibre2"><kbd class="calibre12"><span>TF = No. of times a word appears in a document / Total No. of words in the document</span></kbd></p>
<p class="calibre2"><span class="calibre5">TF-IDF numerically scores the importance of a word based on how often the word appears in a document or a collection of documents. </span><span class="calibre5">Simply put, the higher the TF-IDF score, the rarer the term. The lower the score, the more common it is. The mathematical representation of TD-IDF would be as follows:</span></p>
<p class="CDPAlignCenter"><kbd class="calibre12"><span><span><span>t</span><span>f</span><span>i</span><span>d</span><span>f</span><span>(</span><span>w</span><span>,</span><span>d</span><span>,</span><span>D</span><span>)</span><span>= </span><span>t</span><span>f</span><span>(</span><span>t</span><span>,</span><span>d</span><span>) </span><span>× </span><span>i</span><span>d</span><span>f</span><span>(</span><span>t</span><span>,</span><span>D</span><span>)</span></span></span></kbd></p>
<p class="calibre2">where w represents the word, d represents a document and D represents the collection of documents.</p>
<p class="calibre2"><span class="calibre5">In this example, we'll use the SMS spam collection dataset, which has labelled messages that have been gathered for cellphone spam research. This dataset is available in the UCI ML repository and is also provided in the GitHub repository.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">We start by importing the required libraries:</p>
<pre class="calibre15"><span>import</span> <span>os</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
<span>import</span> <span>itertools</span>
<span>import</span> <span>warnings</span>
<span>import</span> <span>string</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>from</span> <span>nltk.corpus</span> <span>import</span> <span>stopwords</span>
<span>from</span> <span>nltk.stem</span> <span>import</span> <span>WordNetLemmatizer</span>
<span>from</span> <span>sklearn.feature_extraction.text</span> <span>import</span> <span>CountVectorizer</span>
<span>from</span> <span>sklearn.feature_extraction.text</span> <span>import</span> <span>TfidfVectorizer</span>
<span>from</span> <span>sklearn.model_selection</span> <span>import</span> <span>train_test_split</span>
<span>from</span> <span>sklearn.naive_bayes</span> <span>import</span> <span>MultinomialNB</span>
<span>from</span> <span>sklearn.metrics</span> <span>import</span> <span>confusion_matrix</span>
<span>from</span> <span>sklearn.model_selection</span> <span>import</span> <span>GridSearchCV</span>
<span>from</span> <span>sklearn.ensemble</span> <span>import</span> <span>RandomForestClassifier</span>
<span>from</span> <span>sklearn.metrics</span> <span>import</span> <span>classification_report</span>
<span>from</span> <span>sklearn.metrics</span> <span>import</span> <span>roc_auc_score</span> <span>as</span> <span>auc</span>
<span>from</span> <span>sklearn.metrics</span> <span>import</span> <span>roc_curve</span>
<span>from</span> <span>sklearn.metrics</span> <span>import</span> <span>accuracy_score</span>
<span>from</span> <span>scipy.stats</span> <span>import</span> <span>mode</span></pre>
<p class="calibre2">Note that for this example we import libraries such as <kbd class="calibre12">nltk</kbd> to prepare our data. We also import the <kbd class="calibre12">CountVectorizer</kbd> and <kbd class="calibre12">TfidVectorizer</kbd> modules from <kbd class="calibre12">sklearn.feature_extraction</kbd>. These modules are used for feature extraction in ML algorithms.</p>
<p class="calibre2">We reuse <kbd class="calibre12">plot_confusion_matrix</kbd> from the scikit-learn website to plot our confusion matrix. This is the same function that we've used in earlier chapters as well:</p>
<pre class="calibre15">def plot_confusion_matrix(cm, classes,<br class="title-page-name"/>                          normalize=False,<br class="title-page-name"/>                          title='Confusion matrix',<br class="title-page-name"/>                          cmap=plt.cm.Blues):<br class="title-page-name"/><br class="title-page-name"/>    plt.imshow(cm, interpolation='nearest', cmap=cmap)<br class="title-page-name"/>    plt.title(title)<br class="title-page-name"/>    plt.colorbar()<br class="title-page-name"/>    tick_marks = np.arange(len(classes))<br class="title-page-name"/>    plt.xticks(tick_marks, classes, rotation=45)<br class="title-page-name"/>    plt.yticks(tick_marks, classes)<br class="title-page-name"/><br class="title-page-name"/>    fmt = '.2f' if normalize else 'd'<br class="title-page-name"/>    thresh = cm.max() / 2.<br class="title-page-name"/>    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):<br class="title-page-name"/>        plt.text(j, i, format(cm[i, j], fmt),<br class="title-page-name"/>                 horizontalalignment="center",<br class="title-page-name"/>                 color="white" if cm[i, j] &gt; thresh else "black")<br class="title-page-name"/><br class="title-page-name"/>    plt.ylabel('True label')<br class="title-page-name"/>    plt.xlabel('Predicted label')<br class="title-page-name"/>    plt.tight_layout()</pre>
<p class="calibre2">We set our working directory and read the dataset:</p>
<pre class="calibre15">os.chdir("/.../Chapter 11/CS - SMS Classification")<br class="title-page-name"/>os.getcwd()<br class="title-page-name"/><br class="title-page-name"/>df_sms = pd.read_csv("sms_labeled_data.csv", encoding = 'utf8')</pre>
<div class="packtinfobox">Note that we use <kbd class="calibre19">encoding='utf8'</kbd>. This is to instruct the <kbd class="calibre19">read_csv()</kbd> method to use UTF encoding to read the file. Python comes with a number of codecs. An exhaustive list is available at <a href="https://docs.python.org/3/library/codecs.html#standard-encodings" class="calibre21">https://docs.python.org/3/library/codecs.html#standard-encodings</a>.</div>
<p class="calibre2">After reading the data, we check whether it has been loaded properly:</p>
<pre class="calibre15">df_sms.head()</pre>
<p class="calibre2">We also check the number of observations and features in the dataset with <kbd class="calibre12">dataframe.shape</kbd>:</p>
<pre class="calibre15">df_sms.shape</pre>
<p class="calibre2">We take a look at the counts of spam and ham messages:</p>
<pre class="calibre15"># Gives the count for ham messages<br class="title-page-name"/>print(df_sms["type"].value_counts()[0])<br class="title-page-name"/>no_of_ham_messages = df_sms["type"].value_counts()[0]<br class="title-page-name"/><br class="title-page-name"/># Gives the count for spam messages<br class="title-page-name"/>print(df_sms["type"].value_counts()[1])<br class="title-page-name"/>no_of_spam_messages = df_sms["type"].value_counts()[1]</pre>
<p class="calibre2">We can also visualize the proportion of spam and ham messages:</p>
<pre class="calibre15">sms_count = pd.value_counts(df_sms["type"], sort= True)<br class="title-page-name"/>ax = sms_count.plot(kind='bar', figsize=(10,10), color= ["green", "orange"], fontsize=13)<br class="title-page-name"/><br class="title-page-name"/>ax.set_alpha(0.8)<br class="title-page-name"/>ax.set_title("Percentage Share of Spam and Ham Messages")<br class="title-page-name"/>ax.set_ylabel("Count of Spam &amp; Ham messages");<br class="title-page-name"/>ax.set_yticks([0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500])<br class="title-page-name"/><br class="title-page-name"/>totals = []<br class="title-page-name"/>for i in ax.patches:<br class="title-page-name"/>totals.append(i.get_height())<br class="title-page-name"/> <br class="title-page-name"/>total = sum(totals)<br class="title-page-name"/><br class="title-page-name"/># set individual bar lables using above list<br class="title-page-name"/>for i in ax.patches:<br class="title-page-name"/>string = str(round((i.get_height()/total)*100, 2))+'%'<br class="title-page-name"/># get_x pulls left or right; get_height pushes up or down<br class="title-page-name"/>ax.text(i.get_x()+0.16, i.get_height(), string, fontsize=13, color='black')</pre>
<p class="calibre2">With the preceding code, we see the following plot:</p>
<p class="CDPAlignCenter"><img class="aligncenter102" src="assets/9d71f67d-1ed3-4372-a211-55d2e492354f.png"/></p>
<p class="calibre2">We also define a function to remove punctuation, convert the text to lowercase, and remove stop words:</p>
<pre class="calibre15">lemmatizer = WordNetLemmatizer()<br class="title-page-name"/><br class="title-page-name"/># Defining a function to remove punctuations, convert text to lowercase and remove stop words<br class="title-page-name"/>def process_text(text):<br class="title-page-name"/>    no_punctuations = [char for char in text if char not in string.punctuation]<br class="title-page-name"/>    no_punctuations = ''.join(no_punctuations)<br class="title-page-name"/><br class="title-page-name"/>    clean_words = [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english')]<br class="title-page-name"/>    clean_words = [lemmatizer.lemmatize(lem) for lem in clean_words]<br class="title-page-name"/>    clean_words = " ".join(clean_words)<br class="title-page-name"/><br class="title-page-name"/>    return clean_words</pre>
<p class="calibre2">We apply the defined <kbd class="calibre12">process_text()</kbd> function to our text variable in the dataset:</p>
<pre class="calibre15">df_sms['text'] = df_sms['text'].apply(text_processing)</pre>
<p class="calibre2">We separate our feature and target variables, and split our data into <kbd class="calibre12">train</kbd> and <kbd class="calibre12">test</kbd> subsets:</p>
<pre class="calibre15">X = df_sms.loc[:,'text']<br class="title-page-name"/>Y = df_sms.loc[:,'type']<br class="title-page-name"/>Y = Y.astype('int')<br class="title-page-name"/><br class="title-page-name"/>X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.3, random_state=1)</pre>
<p class="calibre2">We use the <kbd class="calibre12">CountVectorizer</kbd> module to convert the text into vectors:</p>
<pre class="calibre15">count_vectorizer = CountVectorizer(stop_words='english')<br class="title-page-name"/><br class="title-page-name"/>count_train = count_vectorizer.fit_transform(X_train)<br class="title-page-name"/>count_test = count_vectorizer.transform(X_test)</pre>
<p class="calibre2">We also use the <kbd class="calibre12">TfidfVectorizer</kbd> module to convert the text into TF-IDF vectors:</p>
<pre class="calibre15">tfidf = TfidfVectorizer(stop_words='english')<br class="title-page-name"/><br class="title-page-name"/>tfidf_train = tfidf.fit_transform(X_train)<br class="title-page-name"/>tfidf_test = tfidf.transform(X_test)</pre>
<p class="calibre2">Let's now move on to training our models. We use the following algorithms both on the count data and the TF-IDF data and see how the individual models perform:</p>
<ul class="calibre10">
<li class="calibre11">Naive Bayes</li>
<li class="calibre11">Support vector machine</li>
<li class="calibre11">Random forest</li>
</ul>
<p class="calibre2">We also combine the model predictions to see the result from the ensemble.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2"><span class="calibre5">Let's begin with training our models, and see how they perform in this section:</span></p>
<ol class="calibre14">
<li class="calibre11">Train the model using the Naive Bayes algorithm. Apply this algorithm to both the count data and the TF-IDF data.</li>
</ol>
<p class="calibre20">The following is the code to train the Naive Bayes on the count data:</p>
<pre class="calibre18">from sklearn.naive_bayes import MultinomialNB<br class="title-page-name"/>nb = MultinomialNB()<br class="title-page-name"/><br class="title-page-name"/>nb.fit(count_train, Y_train)<br class="title-page-name"/>nb_pred_train = nb.predict(count_train)<br class="title-page-name"/>nb_pred_test = nb.predict(count_test)<br class="title-page-name"/>nb_pred_train_proba = nb.predict_proba(count_train)<br class="title-page-name"/>nb_pred_test_proba = nb.predict_proba(count_test)<br class="title-page-name"/><br class="title-page-name"/>print('The accuracy for the training data is {}'.format(nb.score(count_train, Y_train)))<br class="title-page-name"/>print('The accuracy for the testing data is {}'.format(nb.score(count_test, Y_test)))</pre>
<p class="calibre20">Take a look at the <kbd class="calibre12">train</kbd> and <kbd class="calibre12">test</kbd> accuracy for the preceding model:</p>
<p class="CDPAlignCenter"><img class="aligncenter103" src="assets/2ffd5dab-de2d-49a5-927c-ab6c7608e50c.png"/></p>
<ol start="2" class="calibre14">
<li class="calibre11">Print the classification report using the <kbd class="calibre12">classification_report()</kbd> method. Pass <kbd class="calibre12">Y_test</kbd> and <kbd class="calibre12">nb_pred_test</kbd> to the <kbd class="calibre12">classification_report()</kbd> method:</li>
</ol>
<pre class="calibre18">print(classification_report(Y_test, nb_pred_test))</pre>
<p class="calibre20">This gives us the following output, which shows the <kbd class="calibre12">precision</kbd>, <kbd class="calibre12">recall</kbd>, <kbd class="calibre12">f1-score</kbd>, and <kbd class="calibre12">support</kbd> for each class in the target variable:</p>
<p class="CDPAlignCenter"><img class="aligncenter104" src="assets/0befb29a-c9bb-40fa-9f71-95dd23122c88.png"/></p>
<ol start="3" class="calibre14">
<li class="calibre11">Pass<span> </span><kbd class="calibre12">Y_test</kbd><span> and </span><kbd class="calibre12">nb_pred_test</kbd><span> to the </span><kbd class="calibre12">plot_confusion_matrix()</kbd> function to plot the confusion matrix, as follows:</li>
</ol>
<pre class="calibre18">target_names = ['Spam','Ham']<br class="title-page-name"/><br class="title-page-name"/># Pass actual &amp; predicted values to the confusion matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, nb_pred_test)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20">The following plot shows us the true negative, false positive, false negative, and true positive values:</p>
<p class="CDPAlignCenter"><img class="aligncenter105" src="assets/c2c7b305-a462-4611-8e53-6bfe868ce3d4.png"/></p>
<div class="packtinfobox">Note that in the <em class="calibre23">Getting ready</em> section earlier, <span>we used the </span><kbd class="calibre19">TfidfVectorizer</kbd><span> module to convert text into TF-IDF vectors.</span></div>
<div class="title-page-name">
<ol start="4" class="calibre60">
<li class="calibre11">Fit the Naive Bayes model to the TF-IDF train data:</li>
</ol>
</div>
<pre class="calibre18">nb.fit(tfidf_train, Y_train)<br class="title-page-name"/>nb_pred_train_tfidf = nb.predict(tfidf_train)<br class="title-page-name"/>nb_pred_test_tfidf = nb.predict(tfidf_test)<br class="title-page-name"/><br class="title-page-name"/>nb_tfidf_pred_train_proba = nb.predict_proba(tfidf_train)<br class="title-page-name"/>nb_tfidf_pred_test_proba = nb.predict_proba(tfidf_test)<br class="title-page-name"/><br class="title-page-name"/>print('The accuracy for the training data is {}'.format(nb.score(count_train, Y_train)))<br class="title-page-name"/>print('The accuracy for the testing data is {}'.format(nb.score(count_test, Y_test)))</pre>
<ol start="5" class="calibre14">
<li class="calibre11">Check the performance statistics of the TF-IDF test data:</li>
</ol>
<pre class="calibre18">print(classification_report(Y_test, nb_pred_test_tfidf))<br class="title-page-name"/><br class="title-page-name"/>target_names = ['Spam','Ham']<br class="title-page-name"/><br class="title-page-name"/># Pass actual &amp; predicted values to the confusion matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, nb_pred_test_tfidf)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/><br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20">In the following screenshot, we can see the output from the preceding code block:</p>
<p class="CDPAlignCenter"><img class="aligncenter106" src="assets/b77d827c-6163-446d-ad09-b4efab591e6d.png"/></p>
<ol start="6" class="calibre14">
<li class="calibre11">Fit the model with the support vector machine classifier with the count data. Use <kbd class="calibre12">GridSearchCV</kbd> to perform a<span> search over the specified parameter values for the estimator:</span></li>
</ol>
<pre class="calibre18">from sklearn.svm import SVC<br class="title-page-name"/><br class="title-page-name"/>svc = SVC(kernel='rbf',probability=True)<br class="title-page-name"/>svc_params = {'C':[0.001, 0.01, 0.1, 1, 10]}<br class="title-page-name"/><br class="title-page-name"/>svc_gcv_rbf_count = GridSearchCV(svc, svc_params, cv=5)<br class="title-page-name"/>svc_gcv_rbf_count.fit(count_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/># We use the grid model to predict the class <br class="title-page-name"/>svc_rbf_train_predicted_values = svc_gcv_rbf_count.predict(count_train)<br class="title-page-name"/>svc_rbf_test_predicted_values = svc_gcv_rbf_count.predict(count_test)<br class="title-page-name"/><br class="title-page-name"/># We use the grid model to predict the class probabilities<br class="title-page-name"/>svc_gcv_train_proba_rbf = svc_gcv_rbf_count.predict_proba(count_train)<br class="title-page-name"/>svc_gcv_test_proba_rbf = svc_gcv_rbf_count.predict_proba(count_test)<br class="title-page-name"/><br class="title-page-name"/>print('The best parameters {}'.format(svc_gcv_rbf_count.best_params_))<br class="title-page-name"/>print('The best score {}'.format(svc_gcv_rbf_count.best_score_))</pre>
<p class="calibre20">The grid-search provides us with the optimum model. We get to see the parameter values and the score of the optimum model:</p>
<p class="CDPAlignCenter"><img class="aligncenter107" src="assets/ecdb5b29-7e5f-410e-94f2-3e40a8426650.png"/></p>
<ol start="7" class="calibre14">
<li class="calibre11">Take a look at the <kbd class="calibre12">test</kbd> accuracy of the count data with the following code:</li>
</ol>
<pre class="calibre18">print(classification_report(Y_test, svc_rbf_test_predicted_values))<br class="title-page-name"/><br class="title-page-name"/>target_names = ['Spam','Ham']<br class="title-page-name"/><br class="title-page-name"/># Pass actual &amp; predicted values to the confusion matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, svc_rbf_test_predicted_values)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm,classes=target_names)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20">Here's the output from <kbd class="calibre12">classification_report()</kbd> and the confusion matrix:</p>
<p class="CDPAlignCenter"><img class="aligncenter108" src="assets/9c07bc83-17ce-42e4-96df-7056ff7fe50d.png"/></p>
<ol start="8" class="calibre14">
<li class="calibre11">Use SVM with the TF-IDF data:</li>
</ol>
<pre class="calibre18">svc = SVC(kernel='rbf',probability=True)<br class="title-page-name"/>svc_params = {'C':[0.001, 0.01, 0.1, 1, 10]}<br class="title-page-name"/><br class="title-page-name"/>svc_gcv = GridSearchCV(svc,svc_params,cv=5)<br class="title-page-name"/>svc_gcv.fit(tfidf_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/># We use the grid model to predict the class <br class="title-page-name"/>svc_tfidf_rbf_train_predicted_values = svc_gcv.predict(tfidf_train)<br class="title-page-name"/>svc_tfidf_rbd_test_predicted_values = svc_gcv.predict(tfidf_test)<br class="title-page-name"/><br class="title-page-name"/># We use the grid model to predict the class probabilities<br class="title-page-name"/>svc_gcv_tfidf_train_proba_rbf = svc_gcv.predict_proba(tfidf_train)<br class="title-page-name"/>svc_gcv_tfidf_test_proba_rbf = svc_gcv.predict_proba(tfidf_test)<br class="title-page-name"/><br class="title-page-name"/>print('The best parameters {}'.format(svc_gcv.best_params_))<br class="title-page-name"/>print('The best score {}'.format(svc_gcv.best_score_))</pre>
<p class="calibre20">The following output shows the best score of the model trained with the SVM and RBF kernel on the TF-IDF data:</p>
<p class="CDPAlignCenter"><img class="aligncenter109" src="assets/ece4a414-3e4f-4767-8574-665c1e7a94fd.png"/></p>
<ol start="9" class="calibre14">
<li class="calibre11">Print the classification report and the confusion matrix for the preceding model:</li>
</ol>
<p class="CDPAlignCenter"><img class="aligncenter110" src="assets/ba4c57e8-fb26-4d52-9d83-62b8d47b5bf9.png"/></p>
<ol start="10" class="calibre14">
<li class="calibre11">Fit the random forest model on the count data with grid search cross-validation, as we did for SVM:</li>
</ol>
<pre class="calibre18"># Set the parameters for grid search<br class="title-page-name"/>rf_params = {"criterion":["gini","entropy"],"min_samples_split":[2,3],"max_depth":[None,2,3],"min_samples_leaf":[1,5],"max_leaf_nodes":[None],"oob_score":[True]}<br class="title-page-name"/><br class="title-page-name"/># Create an instance of the Random Forest Classifier()<br class="title-page-name"/>rf = RandomForestClassifier()<br class="title-page-name"/><br class="title-page-name"/># Use gridsearchCV(), pass the values you have set for grid search<br class="title-page-name"/>rf_gcv = GridSearchCV(rf, rf_params, cv=5)<br class="title-page-name"/><br class="title-page-name"/># Fit the model onto the train data<br class="title-page-name"/>rf_gcv.fit(count_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/># We use the grid model to predict the class <br class="title-page-name"/>rf_train_predicted_values = rf_gcv.predict(count_train)<br class="title-page-name"/>rf_test_predicted_values = rf_gcv.predict(count_test)<br class="title-page-name"/><br class="title-page-name"/># We use the grid model to predict the class probabilities<br class="title-page-name"/>rf_gcv_pred_train_proba = rf_gcv.predict_proba(count_train)<br class="title-page-name"/>rf_gcv_pred_test_proba = rf_gcv.predict_proba(count_test)<br class="title-page-name"/><br class="title-page-name"/>print('The best parameters {}'.format(rf_gcv.best_params_))<br class="title-page-name"/>print('The best score {}'.format(rf_gcv.best_score_))</pre>
<p class="calibre20">A grid search of the random forest with the grid parameters returns the best parameters and the best score, as seen in the following screenshot:</p>
<p class="CDPAlignCenter"><img class="aligncenter111" src="assets/c25f7a64-fd07-4365-92f9-5213bfc47fe8.png"/></p>
<ol start="11" class="calibre14">
<li class="calibre11">Using a classification report and a confusion matrix, take a look at the performance metrics of the random forest model with the count data on our test data:</li>
</ol>
<pre class="calibre18">print(classification_report(Y_test, rf_test_predicted_values))<br class="title-page-name"/><br class="title-page-name"/>target_names = ['Spam','Ham']<br class="title-page-name"/><br class="title-page-name"/># Pass actual &amp; predicted values to the confusion matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, rf_test_predicted_values)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm,classes=target_names)<br class="title-page-name"/>plt.show() </pre>
<p class="calibre20">The report is shown in the following screenshot:</p>
<p class="CDPAlignCenter"><img class="aligncenter112" src="assets/e81e861e-fe2f-4d73-80a0-abc8eae6df88.png"/></p>
<ol start="12" class="calibre14">
<li class="calibre11">Build a model on a random forest with a grid-search on the TF-IDF data:</li>
</ol>
<pre class="calibre18"># Set the parameters for grid search<br class="title-page-name"/>rf_params = {"criterion":["gini","entropy"],"min_samples_split":[2,3],"max_depth":[None,2,3],"min_samples_leaf":[1,5],"max_leaf_nodes":[None],"oob_score":[True]}<br class="title-page-name"/><br class="title-page-name"/># Create an instance of the Random Forest Classifier()<br class="title-page-name"/>rf = RandomForestClassifier()<br class="title-page-name"/><br class="title-page-name"/># Use gridsearchCV(), pass the values you have set for grid search<br class="title-page-name"/>rf_gcv = GridSearchCV(rf, rf_params, cv=5)<br class="title-page-name"/><br class="title-page-name"/>rf_gcv.fit(tfidf_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/>rf_tfidf_train_predicted_values = rf_gcv.predict(tfidf_train)<br class="title-page-name"/>rf_tfidf_test_predicted_values = rf_gcv.predict(tfidf_test)<br class="title-page-name"/><br class="title-page-name"/>rf_gcv_tfidf_pred_train_proba = rf_gcv.predict_proba(tfidf_train)<br class="title-page-name"/>rf_gcv_tfidf_pred_test_proba = rf_gcv.predict_proba(tfidf_test)<br class="title-page-name"/><br class="title-page-name"/>print('The best parameters {}'.format(rf_gcv.best_params_))<br class="title-page-name"/>print('The best score {}'.format(rf_gcv.best_score_))<br class="title-page-name"/><br class="title-page-name"/>print(classification_report(Y_test, rf_tfidf_test_predicted_values))<br class="title-page-name"/><br class="title-page-name"/>target_names = ['Spam','Ham']<br class="title-page-name"/># Pass actual &amp; predicted values to the confusion matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, rf_tfidf_test_predicted_values)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names)<br class="title-page-name"/>plt.show()</pre>
<ol start="13" class="calibre14">
<li class="calibre11">Take the output of the <kbd class="calibre12">predict_proba()</kbd> methods to gather the predicted probabilities from each model to plot the ROC curves. The full code is provided in the code bundle.</li>
</ol>
<p class="calibre20">Here's a sample of the code to plot the ROC curve from the Naive Bayes model on the count data:</p>
<pre class="calibre18">fpr, tpr, thresholds = roc_curve(Y_test, nb_pred_test_proba[:,1])<br class="title-page-name"/>roc_auc = auc(Y_test,nb_pred_test_proba[:,1])<br class="title-page-name"/><br class="title-page-name"/>plt.title('ROC Naive Bayes (Count)')<br class="title-page-name"/>plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)<br class="title-page-name"/>plt.legend(loc='lower right')<br class="title-page-name"/>plt.plot([0,1],[0,1],'r--')<br class="title-page-name"/>plt.xlim([-0.1,1.0])<br class="title-page-name"/>plt.ylim([-0.1,1.01])<br class="title-page-name"/>plt.ylabel('True Positive Rate')<br class="title-page-name"/>plt.xlabel('False Positive Rate')</pre>
<p class="calibre20">With the complete code provided in the code bundle, we can view the ROC plot from all the models and compare them:</p>
<p class="CDPAlignCenter"><img class="aligncenter113" src="assets/385bf4df-cbb8-4f7d-a2f4-552a662169c1.png"/></p>
<ol start="14" class="calibre14">
<li class="calibre11">Average the probabilities from all the models and plot the ROC curves:</li>
</ol>
<pre class="calibre18">plt.subplot(4,3,7)<br class="title-page-name"/><br class="title-page-name"/>### Test Count Data<br class="title-page-name"/>d = (nb_pred_test_proba + svc_gcv_test_proba_rbf + rf_gcv_pred_test_proba)/4<br class="title-page-name"/><br class="title-page-name"/>fpr, tpr, thresholds = roc_curve(Y_test,d[:,1])<br class="title-page-name"/>roc_auc = auc(Y_test,d[:,1])<br class="title-page-name"/><br class="title-page-name"/>plt.title('ROC Ensemble (Count)')<br class="title-page-name"/>plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)<br class="title-page-name"/>plt.legend(loc='lower right')<br class="title-page-name"/>plt.plot([0,1],[0,1],'r--')<br class="title-page-name"/>plt.xlim([-0.1,1.0])<br class="title-page-name"/>plt.ylim([-0.1,1.01])<br class="title-page-name"/>plt.ylabel('True Positive Rate')<br class="title-page-name"/>plt.xlabel('False Positive Rate')<br class="title-page-name"/><br class="title-page-name"/>plt.subplot(4,3,8)<br class="title-page-name"/><br class="title-page-name"/>### Test TF-IDF Data<br class="title-page-name"/>d = (nb_tfidf_pred_test_proba + svc_gcv_tfidf_test_proba_rbf + rf_gcv_tfidf_pred_test_proba)/4<br class="title-page-name"/><br class="title-page-name"/>fpr, tpr, thresholds = roc_curve(Y_test,d[:,1])<br class="title-page-name"/>roc_auc = auc(Y_test,d[:,1])<br class="title-page-name"/><br class="title-page-name"/>plt.title('ROC Ensemble (TF-IDF)')<br class="title-page-name"/>plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)<br class="title-page-name"/>plt.legend(loc='lower right')<br class="title-page-name"/>plt.plot([0,1],[0,1],'r--')<br class="title-page-name"/>plt.xlim([-0.1,1.0])<br class="title-page-name"/>plt.ylim([-0.1,1.01])<br class="title-page-name"/>plt.ylabel('True Positive Rate')<br class="title-page-name"/>plt.xlabel('False Positive Rate')<br class="title-page-name"/>#plt.show()<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout(pad=1,rect=(0, 0, 3.5, 4))<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20"><span class="calibre5">We can see the average result of the ROC and AUC scores in the following screenshot:</span></p>
<p class="CDPAlignCenter"><img class="aligncenter114" src="assets/9c5a656e-c8ec-432c-b6c1-352c8a384032.png"/></p>
<ol start="15" class="calibre14">
<li class="calibre11">Check the accuracy of the ensemble result. Create an array of the predicted results, as follows:</li>
</ol>
<pre class="calibre18">predicted_array = np.array([nb_pred_test_tfidf, svc_tfidf_rbd_test_predicted_values, rf_tfidf_test_predicted_values])<br class="title-page-name"/><br class="title-page-name"/>print("Each array is the prediction of the respective models")<br class="title-page-name"/>print(predicted_array)</pre>
<ol start="16" class="calibre14">
<li class="calibre11">Calculate the mode of the predicted values for the respective observations to perform max-voting in order to get the final predicted result:</li>
</ol>
<pre class="calibre18"># Using mode on the array, we get the max vote for each observation<br class="title-page-name"/>predicted_array = mode(predicted_array)<br class="title-page-name"/><br class="title-page-name"/># Check the array<br class="title-page-name"/>print(predicted_array)<br class="title-page-name"/><br class="title-page-name"/>print("The accuracy for test")<br class="title-page-name"/>accuracy_score(Y_test, predicted_array[0][0])</pre>
<ol start="17" class="calibre14">
<li class="calibre11">Plot the test accuracy for the models trained on the count data and TF-IDF data, respectively:</li>
</ol>
<p class="CDPAlignCenter"><img class="aligncenter115" src="assets/9c36b9ad-7304-436c-9e63-8f6ddd2f907f.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the <em class="calibre13">Getting ready</em> section, we imported all the required libraries and defined the function to plot the confusion matrix. We read our dataset, using UTF8 encoding. We checked the proportion of spam and ham messages in our dataset and used the <span class="calibre5"><kbd class="calibre12">CountVectorizer</kbd> and </span><span class="calibre5"><kbd class="calibre12">TfidfVectorizer</kbd> modules to convert the texts into vectors and TF-IDF vectors, respectively.</span></p>
<p class="calibre2">After that, we built multiple models using various algorithms. We also applied each algorithm on both the count data and the TF-IDF data.</p>
<p class="calibre2">The models need to be built in the following order:</p>
<ol class="calibre14">
<li class="calibre11">Naive Bayes on count data</li>
<li class="calibre11">Naive Bayes on TF-IDF data</li>
<li class="calibre11">SVM with RBF kernel on count data</li>
<li class="calibre11">SVM with RBF kernel on TF-IDF data</li>
<li class="calibre11">Random forest on count data</li>
<li class="calibre11">Random forest on TF-IDF data</li>
</ol>
<p class="calibre2"><span class="calibre5">The Naive</span><span class="calibre5"> Bayes classifier is widely used for text classification in machine learning. The Naive Bayes algorithm is based on the conditional probability of features belonging to a class. </span>In <em class="calibre13">Step 1</em>, we built our first model with the <span class="calibre5">Naive</span><span class="calibre5"> </span>Bayes algorithm on the count data. In <em class="calibre13"><span class="calibre5">S</span>tep 2</em>, we checked the performance metrics using <kbd class="calibre12">classification_report()</kbd> to see the <kbd class="calibre12">precision</kbd>, <kbd class="calibre12">recall</kbd>, <kbd class="calibre12">f1-score</kbd>, and <kbd class="calibre12">support</kbd>. In <em class="calibre13"><span class="calibre5">S</span>tep 3</em>, we called <kbd class="calibre12">plot_confusion_matrix()</kbd> to plot the confusion matrix. </p>
<p class="calibre2">Then, in <em class="calibre13"><span class="calibre5">S</span>tep 4</em>, we built the <span class="calibre5">Naive</span><span class="calibre5"> </span>Bayes model on the TF-IDF data and evaluated the performance in <em class="calibre13"><span class="calibre5">S</span>tep 5</em>. In <em class="calibre13"><span class="calibre5">S</span>tep 6</em> and <em class="calibre13">Step 7</em>, we trained our model using the support vector machine on the count data, evaluated its performance using the output from <kbd class="calibre12">classification_report</kbd>, and plotted the confusion matrix. We trained our SVM model using the RBF kernel. We also showcased an example of using <kbd class="calibre12">GridSearchCV</kbd> to find the best parameters. In <em class="calibre13">Step 8</em> and <em class="calibre13"><span class="calibre5">S</span>tep 9</em>, we repeated what we did in <em class="calibre13"><span class="calibre5">S</span>tep</em> 6 and <em class="calibre13"><span class="calibre5">S</span>tep</em><span class="calibre5"> </span>7, but this time, we trained the SVM on TF-IDF data.</p>
<p class="calibre2">In <em class="calibre13"><span class="calibre5">S</span>tep 10</em>, we trained a random forest model using grid search on the count data. We set <strong class="calibre4">gini</strong> and <strong class="calibre4">entropy</strong> for the <kbd class="calibre12">criterion</kbd> hyperparameter. We also set multiple values for the parameters, such as <kbd class="calibre12">min_samples_split</kbd>, <kbd class="calibre12">max_depth</kbd>,  and <kbd class="calibre12">min_samples_leaf</kbd>. In <em class="calibre13">Step 11</em>, we evaluated the model's performance.</p>
<p class="calibre2">We then trained another random forest model on the TF-IDF data in <em class="calibre13"><span class="calibre5">S</span>tep 12</em>. Using the <kbd class="calibre12">predic_proba()</kbd> function, we got the class probabilities on our test data. We used the same in <em class="calibre13">Step 13</em> to plot the ROC curves with AUC scores annotated on the plots for each of the models. This helps us to compare the performance of the models.</p>
<p class="calibre2">In <em class="calibre13"><span class="calibre5">S</span>tep 14</em>, we averaged the probabilities, which we got from the models for both the count and TF-IDF data. We then plotted the ROC curves for the ensemble results. From <em class="calibre13"><span class="calibre5">S</span>tep 15</em> through to <em class="calibre13"><span class="calibre5">S</span>tep 17</em>, we plotted the test accuracy for each of the models built on the count data as well as the TF-IDF data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sentiment analysis of movie reviews using an ensemble model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Sentiment analysis is another widely studied research area in <strong class="calibre4">natural language processing</strong> (<strong class="calibre4">NLP</strong>). It's a popular task performed on reviews to determine the sentiments of comments provided by reviewers. In this example, we'll focus on analyzing movie review data from the <strong class="calibre4">Internet Movie Database</strong> (<strong class="calibre4">IMDb</strong>) and classifying <span class="calibre5"><span class="calibre5">it </span></span>according to whether it is positive or negative.</p>
<p class="calibre2">We have movie reviews in <kbd class="calibre12">.txt</kbd> files that are separated into two folders: negative and positive. There are 1,000 positive reviews and 1,000 negative reviews. These files can be retrieved from GitHub.</p>
<p class="calibre2">We have divided this case study into two parts:</p>
<ul class="calibre10">
<li class="calibre11">The first part is to prepare the dataset. We'll read the review files that are provided in the <kbd class="calibre12">.txt</kbd> format, append them, label them as positive or negative based on which folder they have been put in, and create a <kbd class="calibre12">.csv</kbd> file that contains the label and text.</li>
<li class="calibre11">In the second part, we'll build multiple base learners on both the count data and on the TF-IDF data. We'll evaluate the performance of the base learners and then evaluate the ensemble of the predictions.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p class="calibre2">We start by importing the required libraries:</p>
<pre class="calibre15">import os<br class="title-page-name"/>import glob<br class="title-page-name"/>import pandas as pd</pre>
<p class="calibre2"> We set our working folder as follows:</p>
<pre class="calibre15">os.chdir("/.../Chapter 11/CS - IMDB Classification")<br class="title-page-name"/>os.getcwd()</pre>
<p class="calibre2">We set our path variable and iterate through the <kbd class="calibre12">.txt</kbd> files in the folders. </p>
<div class="packtinfobox">Note that we have a subfolder, <kbd class="calibre19">/txt_sentoken/pos</kbd>, which holds the TXT files for the positive reviews. Similarly, <span>we have a subfolder, <kbd class="calibre19">/txt_sentoken/neg</kbd>, which holds the TXT files for the negative reviews.</span></div>
<p class="calibre2"><span class="calibre5">The TXT files for the positive reviews are read and the reviews are appended in an array. We use the array to create a DataFrame, </span><kbd class="calibre12">df_pos</kbd><span class="calibre5">.</span></p>
<pre class="calibre15">path="/.../Chapter 11/CS - IMDB Classification/txt_sentoken/pos/*.txt"<br class="title-page-name"/><br class="title-page-name"/>files = glob.glob(path)<br class="title-page-name"/>text_pos = []<br class="title-page-name"/><br class="title-page-name"/>for p in files:<br class="title-page-name"/> file_read = open(p, "r")<br class="title-page-name"/> to_append_pos = file_read.read()<br class="title-page-name"/> text_pos.append(to_append_pos)<br class="title-page-name"/> file_read.close()<br class="title-page-name"/><br class="title-page-name"/>df_pos = pd.DataFrame({'text':text_pos,'label':'positive'})<br class="title-page-name"/>df_pos.head()</pre>
<p class="calibre2">With the <kbd class="calibre12">head()</kbd> method, we take a look at the positive reviews.</p>
<p class="calibre2">We also iterate through the TXT files in the negative folder to read the negative reviews and append them in an array. <span class="calibre5">We use the array to create a DataFrame, <kbd class="calibre12">df_neg</kbd>:</span></p>
<pre class="calibre15">path="/Users/Dippies/CODE PACKT - EML/Chapter 11/CS - IMDB Classification/txt_sentoken/neg/*.txt"<br class="title-page-name"/><br class="title-page-name"/>files = glob.glob(path)<br class="title-page-name"/>text_neg = []<br class="title-page-name"/><br class="title-page-name"/>for n in files:<br class="title-page-name"/>    file_read = open(n, "r")<br class="title-page-name"/>    to_append_neg = file_read.read()<br class="title-page-name"/>    text_neg.append(to_append_neg)<br class="title-page-name"/>    file_read.close()<br class="title-page-name"/><br class="title-page-name"/>df_neg = pd.DataFrame({'text':text_neg,'label':'negative'})<br class="title-page-name"/>df_neg.head()</pre>
<p class="calibre2">Finally, we merge the positive and negative <span class="calibre5">DataFrames in</span>to a single DataFrame using the <kbd class="calibre12">concat()</kbd> method:</p>
<pre class="calibre15">df_moviereviews=pd.concat([df_pos, df_neg])</pre>
<p class="calibre2">We can take a look at the prepared <span class="calibre5">DataFrame</span> with the <kbd class="calibre12">head()</kbd> and <kbd class="calibre12">tail()</kbd> methods:</p>
<pre class="calibre15">print(df_moviereviews.head())<br class="title-page-name"/>print(df_moviereviews.tail())</pre>
<p class="calibre2">The preceding code gives us the following output:</p>
<p class="CDPAlignCenter"><img class="aligncenter116" src="assets/d92dc9a6-754b-4c3f-ba5c-aec37b8ab46d.png"/></p>
<p class="calibre2"><span class="calibre5">From the preceding image, we notice that the positive and negative reviews have been added sequentially. The first half of the DataFrame holds the positive reviews, while the next half holds the negative reviews.</span></p>
<p class="calibre2"><span class="calibre5">Let's shuffle the data so that it doesn't stay in sequential order:</span></p>
<pre class="calibre15">from sklearn.utils import shuffle<br class="title-page-name"/><br class="title-page-name"/>df_moviereviews=shuffle(df_moviereviews)<br class="title-page-name"/>df_moviereviews.head(10)</pre>
<p class="calibre2">We can now see that the data in the <span class="calibre5">DataFrame </span>is shuffled:</p>
<p class="CDPAlignCenter"><img class="aligncenter117" src="assets/5b4555d1-3c8c-4439-bff5-51dc0357469f.png"/></p>
<p class="calibre2">We validate the dimensions of the merged <span class="calibre5">DataFrame</span> to see whether it holds 2,000 observations, which would be the result of combining the 1,000 negative and 1,000 positive reviews:</p>
<pre class="calibre15">df_moviereviews.shape</pre>
<p class="calibre2"><span class="calibre5">From the preceding code, we notice that we have 2,000 observations</span> and 2 columns.</p>
<p class="calibre2">We may also write the resulting <span class="calibre5">DataFrame</span> into another <kbd class="calibre12">.csv</kbd> file in order to avoid recreating the CSV file from the TXT files as we did in the preceding steps:</p>
<pre class="calibre15">df_moviereviews.to_csv("/.../Chapter 11/CS - IMDB Classification/Data_IMDB.csv") </pre>
<p class="calibre2"><span class="calibre5">Next, we'll define the </span><kbd class="calibre12">plot_confusion_matrix()</kbd><span class="calibre5"> </span><span class="calibre5">method that we have used earlier.</span></p>
<p class="calibre2">We can now see the share of the positive and negative reviews in our data. In our case, the proportion is exactly 50:50:</p>
<pre class="calibre15">df_moviereviews["label"].value_counts().plot(kind='pie')<br class="title-page-name"/>plt.tight_layout(pad=1,rect=(0, 0, 0.7, 1))<br class="title-page-name"/><br class="title-page-name"/>plt.text(x=-0.9,y=0.1, \<br class="title-page-name"/>         s=(np.round(((df_moviereviews["label"].\<br class="title-page-name"/>                       value_counts()[0])/(df_moviereviews["label"].value_counts()[0] + \<br class="title-page-name"/>                       df_moviereviews["label"].value_counts()[1])),2)))<br class="title-page-name"/><br class="title-page-name"/>plt.text(x=0.4,y=-0.3, \<br class="title-page-name"/>         s=(np.round(((df_moviereviews["label"].\<br class="title-page-name"/>                       value_counts()[1])/(df_moviereviews["label"].value_counts()[0] + \<br class="title-page-name"/>                       df_moviereviews["label"].value_counts()[1])),2)))<br class="title-page-name"/><br class="title-page-name"/>plt.title("% Share of the Positive and Negative reviews in the dataset")</pre>
<p class="calibre2">The output of the preceding code can be seen in the following screenshot:</p>
<p class="CDPAlignCenter"><img class="aligncenter118" src="assets/b9162d18-91d9-4185-a16e-967abbd75d46.png"/></p>
<p class="calibre2"> <span class="calibre5">We will now replace the "positive" label with "1" and the "negative" label" with "0":</span></p>
<pre class="calibre15">df_moviereviews.loc[df_moviereviews["label"]=='positive',"label",]=1<br class="title-page-name"/>df_moviereviews.loc[df_moviereviews["label"]=='negative',"label",]=0</pre>
<p class="calibre2"><span class="calibre5">We prepare our data using various data-cleaning and preparation mechanisms. We'll follow the same sequence as we followed in the previous recipe to preprocess our data:</span></p>
<ol class="calibre14">
<li class="calibre11">Convert all text to lowercase</li>
<li class="calibre11">Remove punctuation</li>
<li class="calibre11">Remove stop words</li>
<li class="calibre11">Perform stemming</li>
<li class="calibre11">Tokenize the data</li>
</ol>
<p class="calibre2">Next, we'll define a function to perform the preceding clean-up steps:</p>
<pre class="calibre15">lemmatizer = WordNetLemmatizer()<br class="title-page-name"/>def process_text(text):<br class="title-page-name"/>    nopunc = [char for char in text if char not in string.punctuation]<br class="title-page-name"/>    nopunc = ''.join(nopunc)<br class="title-page-name"/><br class="title-page-name"/>    clean_words = [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('english')]<br class="title-page-name"/>    clean_words = [lemmatizer.lemmatize(lem) for lem in clean_words]<br class="title-page-name"/>    clean_words = " ".join(clean_words)<br class="title-page-name"/><br class="title-page-name"/>    return clean_words</pre>
<p class="calibre2">We call the preceding function to process our text data:</p>
<pre class="calibre15">df_moviereviews['text'] = df_moviereviews['text'].apply(process_text)</pre>
<p class="calibre2">We'll now build our base learners and evaluate the ensemble result.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p class="calibre2">We start by importing the remaining libraries we need:</p>
<ol class="calibre14">
<li class="calibre11">Import the required libraries:</li>
</ol>
<pre class="calibre18">import os<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import pandas as pd<br class="title-page-name"/>import itertools<br class="title-page-name"/>import warnings<br class="title-page-name"/>import string<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>from nltk.corpus import stopwords<br class="title-page-name"/>from nltk.stem import WordNetLemmatizer<br class="title-page-name"/>from sklearn.feature_extraction.text import CountVectorizer<br class="title-page-name"/>from sklearn.feature_extraction.text import TfidfVectorizer<br class="title-page-name"/>from sklearn.model_selection import train_test_split<br class="title-page-name"/>from sklearn.naive_bayes import MultinomialNB<br class="title-page-name"/>from sklearn.metrics import confusion_matrix<br class="title-page-name"/>from sklearn.model_selection import GridSearchCV<br class="title-page-name"/>from sklearn.ensemble import RandomForestClassifier<br class="title-page-name"/>from sklearn.metrics import classification_report<br class="title-page-name"/>from sklearn.metrics import roc_auc_score as auc<br class="title-page-name"/>from sklearn.metrics import roc_curve<br class="title-page-name"/>from sklearn.metrics import accuracy_score<br class="title-page-name"/>from scipy.stats import mode</pre>
<ol start="2" class="calibre14">
<li class="calibre11">Separate the target and predictor variables:</li>
</ol>
<pre class="calibre18">X = df_moviereviews.loc[:,'text']<br class="title-page-name"/>Y = df_moviereviews.loc[:,'label']<br class="title-page-name"/>Y = Y.astype('int')</pre>
<ol start="3" class="calibre14">
<li class="calibre11">Perform the train-test split of the data:</li>
</ol>
<pre class="calibre18">X_train,X_test,y_train,y_test = train_test_split(X, Y, test_size=.3, random_state=1)</pre>
<ol start="4" class="calibre14">
<li class="calibre11">Use <kbd class="calibre12">CountVectorizer()</kbd> to convert the text into vectors:</li>
</ol>
<pre class="calibre18">count_vectorizer = CountVectorizer()<br class="title-page-name"/>count_train = count_vectorizer.fit_transform(X_train)<br class="title-page-name"/>count_test = count_vectorizer.transform(X_test)</pre>
<ol start="5" class="calibre14">
<li class="calibre11">Use <kbd class="calibre12">TfidfVectorizer()</kbd> to convert the text into TF-IDF vectors:</li>
</ol>
<pre class="calibre18">tfidf = TfidfVectorizer()<br class="title-page-name"/>tfidf_train = tfidf.fit_transform(X_train)<br class="title-page-name"/>tfidf_test = tfidf.transform(X_test)</pre>
<p class="calibre20">We proceed by training the base learners on the count data and on the <span class="calibre5">TF-IDF </span>data. We train the base learners with random forest models, <span class="calibre5">Naive</span><span class="calibre5"> </span>Bayes models, and the support-vector classifier models. </p>
<ol start="6" class="calibre14">
<li class="calibre11">Train the random forest model using grid-search on the count data:</li>
</ol>
<pre class="calibre18"># Set the parameters for grid search<br class="title-page-name"/>rf_params = {"criterion":["gini","entropy"],\<br class="title-page-name"/>             "min_samples_split":[2,3],\<br class="title-page-name"/>             "max_depth":[None,2,3],\<br class="title-page-name"/>             "min_samples_leaf":[1,5],\<br class="title-page-name"/>             "max_leaf_nodes":[None],\<br class="title-page-name"/>             "oob_score":[True]}<br class="title-page-name"/><br class="title-page-name"/># Create an instance of the RandomForestClassifier()<br class="title-page-name"/>rf = RandomForestClassifier()<br class="title-page-name"/>warnings.filterwarnings("ignore")<br class="title-page-name"/><br class="title-page-name"/># Use gridsearchCV(), pass the values you have set for grid search<br class="title-page-name"/>rf_count = GridSearchCV(rf, rf_params, cv=5)<br class="title-page-name"/><br class="title-page-name"/>rf_count.fit(count_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/># Predict class predictions &amp; class probabilities with test data<br class="title-page-name"/>rf_count_predicted_values = rf_count.predict(count_test)<br class="title-page-name"/>rf_count_probabilities = rf_count.predict_proba(count_test)<br class="title-page-name"/><br class="title-page-name"/>rf_count_train_accuracy = rf_count.score(count_train, Y_train)<br class="title-page-name"/>rf_count_test_accuracy = rf_count.score(count_test, Y_test)<br class="title-page-name"/><br class="title-page-name"/>print('The accuracy for the training data is {}'.\<br class="title-page-name"/>      format(rf_count_train_accuracy))<br class="title-page-name"/><br class="title-page-name"/>print('The accuracy for the testing data is {}'.\<br class="title-page-name"/>      format(rf_count_test_accuracy))</pre>
<ol start="7" class="calibre14">
<li class="calibre11">Evaluate <kbd class="calibre12">precision</kbd>, <kbd class="calibre12">recall</kbd>, <kbd class="calibre12">f1-score</kbd>, <kbd class="calibre12">support</kbd>, and <kbd class="calibre12">accuracy</kbd>:</li>
</ol>
<pre class="calibre18">print(classification_report(Y_test, rf_count_predicted_values))<br class="title-page-name"/><br class="title-page-name"/># Pass actual &amp; predicted values to the confusion_matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, rf_count_predicted_values)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names,normalize=False)<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20">In the following screenshot, we can see the output of the preceding code:</p>
<p class="CDPAlignCenter"><img class="aligncenter119" src="assets/197e72d5-407a-4560-a910-694b77fdc3af.png"/></p>
<ol start="8" class="calibre14">
<li class="calibre11">Train a random forest model on the TF-IDF data using grid-search:</li>
</ol>
<pre class="calibre18"># Set the parameters for grid search<br class="title-page-name"/>rf_params = {"criterion":["gini","entropy"],"min_samples_split":[2,3],"max_depth":[None,2,3],"min_samples_leaf":[1,5],"max_leaf_nodes":[None],"oob_score":[True]}<br class="title-page-name"/><br class="title-page-name"/># Create an instance of the RandomForestClassifier()<br class="title-page-name"/>rf = RandomForestClassifier()<br class="title-page-name"/>warnings.filterwarnings("ignore")<br class="title-page-name"/><br class="title-page-name"/># Use gridsearchCV(), pass the values you have set for grid search<br class="title-page-name"/>rf_tfidf = GridSearchCV(rf, rf_params, cv=5)<br class="title-page-name"/><br class="title-page-name"/>rf_tfidf.fit(tfidf_train, Y_train)</pre>
<ol start="9" class="calibre14">
<li class="calibre11">Evaluate the model's performance:</li>
</ol>
<pre class="calibre18">rf_tfidf_predicted_values = rf_tfidf.predict(tfidf_test)<br class="title-page-name"/>rf_tfidf_probabilities = rf_tfidf.predict_proba(tfidf_test)<br class="title-page-name"/><br class="title-page-name"/>rf_train_accuracy = rf_tfidf.score(tfidf_train, Y_train)<br class="title-page-name"/>rf_test_accuracy = rf_tfidf.score(tfidf_test, Y_test)<br class="title-page-name"/><br class="title-page-name"/>print('The accuracy for the training data is {}'.format(rf_train_accuracy))<br class="title-page-name"/>print('The accuracy for the testing data is {}'.format(rf_test_accuracy))<br class="title-page-name"/><br class="title-page-name"/>print(classification_report(Y_test, rf_tfidf_predicted_values))<br class="title-page-name"/><br class="title-page-name"/># Pass actual &amp; predicted values to the confusion_matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, rf_tfidf_predicted_values)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names,normalize=False)<br class="title-page-name"/>plt.show()</pre>
<ol start="10" class="calibre14">
<li class="calibre11">Train the <span>Naive</span><span> </span>Bayes model on the count data and check the accuracy of the test data:</li>
</ol>
<pre class="calibre18">nb_count = MultinomialNB()<br class="title-page-name"/>nb_count.fit(count_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/>nb_count_predicted_values = nb_count.predict(count_test)<br class="title-page-name"/>nb_count_probabilities = nb_count.predict_proba(count_test)<br class="title-page-name"/><br class="title-page-name"/>nb_train_accuracy = nb_count.score(count_train, Y_train)<br class="title-page-name"/>nb_test_accuracy = nb_count.score(count_test, Y_test)<br class="title-page-name"/><br class="title-page-name"/>print('The accuracy for the training data is {}'.format(nb_train_accuracy))<br class="title-page-name"/>print('The accuracy for the testing data is {}'.format(nb_test_accuracy))</pre>
<ol start="11" class="calibre14">
<li class="calibre11">Evaluate the other model's performance parameters with <kbd class="calibre12">classification_report()</kbd> and the confusion matrix:</li>
</ol>
<pre class="calibre18">print(classification_report(Y_test, nb_predicted_values))<br class="title-page-name"/><br class="title-page-name"/># Pass actual &amp; predicted values to the confusion matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, nb_predicted_values)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names,normalize=False)<br class="title-page-name"/>plt.show()</pre>
<ol start="12" class="calibre14">
<li class="calibre11">Train the Naive Bayes model on the TF-IDF data and evaluate its performance the same way we did for earlier models:</li>
</ol>
<pre class="calibre18">nb_tfidf = MultinomialNB()<br class="title-page-name"/>nb_tfidf.fit(count_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/>nb_tfidf_predicted_values = nb_tfidf.predict(tfidf_test)<br class="title-page-name"/>nb_tfidf_probabilities = nb_tfidf.predict_proba(tfidf_test)<br class="title-page-name"/><br class="title-page-name"/>nb_train_accuracy = nb_tfidf.score(tfidf_train, Y_train)<br class="title-page-name"/>nb_test_accuracy = nb_tfidf.score(tfidf_test, Y_test)<br class="title-page-name"/><br class="title-page-name"/>print('The accuracy for the training data is {}'.format(nb_train_accuracy))<br class="title-page-name"/>print('The accuracy for the testing data is {}'.format(nb_test_accuracy))<br class="title-page-name"/><br class="title-page-name"/>print(classification_report(Y_test, nb_predicted_values))<br class="title-page-name"/><br class="title-page-name"/>#Pass actual &amp; predicted values to the confusion matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, nb_predicted_values)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names,normalize=False)<br class="title-page-name"/>plt.show()</pre>
<ol start="13" class="calibre14">
<li class="calibre11">Train a model with a support vector classifier algorithm with the linear kernel on the count data. We also grid-search the <kbd class="calibre12">C</kbd> parameter for the SVC:</li>
</ol>
<pre class="calibre18">svc_count = SVC(kernel='linear',probability=True)<br class="title-page-name"/>svc_params = {'C':[0.001, 0.01, 0.1, 1, 10]}<br class="title-page-name"/><br class="title-page-name"/>svc_gcv_count = GridSearchCV(svc_count, svc_params, cv=5)<br class="title-page-name"/>svc_gcv_count.fit(count_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/>svc_count_predicted_values = svc_gcv_count.predict(count_test)<br class="title-page-name"/>svc_count_probabilities = svc_gcv_count.predict_proba(count_test)<br class="title-page-name"/><br class="title-page-name"/>svc_count_train_accuracy = svc_gcv_count.score(count_train, Y_train)<br class="title-page-name"/>svc_count_test_accuracy = svc_gcv_count.score(count_test, Y_test)<br class="title-page-name"/><br class="title-page-name"/>print('The accuracy for the training data is {}'.format(svc_gcv_count.score(count_train, Y_train)))<br class="title-page-name"/>print('The accuracy for the testing data is {}'.format(svc_gcv_count.score(count_test, Y_test)))<br class="title-page-name"/><br class="title-page-name"/>print(classification_report(Y_test, svc_count_predicted_values))<br class="title-page-name"/># Pass actual &amp; predicted values to the confusion_matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, svc_count_predicted_values)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names,normalize=False)<br class="title-page-name"/>plt.show()</pre>
<ol start="14" class="calibre14">
<li class="calibre11">Train a model with the support vector classifier algorithm with the linear kernel on the TF-IDF data. We also grid-search the <kbd class="calibre12">C</kbd> parameter for the SVC:</li>
</ol>
<pre class="calibre18">svc_tfidf = SVC(kernel='linear',probability=True)<br class="title-page-name"/>svc_params = {'C':[0.001, 0.01, 0.1, 1, 10]}<br class="title-page-name"/><br class="title-page-name"/>svc_gcv_tfidf = GridSearchCV(svc_tfidf, svc_params, cv=5)<br class="title-page-name"/>svc_gcv_tfidf.fit(tfidf_train, Y_train)<br class="title-page-name"/><br class="title-page-name"/>svc_tfidf_predicted_values = svc_gcv_tfidf.predict(tfidf_test)<br class="title-page-name"/>svc_tfidf_probabilities = svc_gcv_tfidf.predict_proba(tfidf_test)<br class="title-page-name"/><br class="title-page-name"/>svc_tfidf_train_accuracy = svc_gcv_count.score(tfidf_train, Y_train)<br class="title-page-name"/>svc_tfidf_test_accuracy = svc_gcv_count.score(tfidf_test, Y_test)<br class="title-page-name"/><br class="title-page-name"/>print('The accuracy for the training data is {}'.format(svc_gcv_tfidf.score(count_train, Y_train)))<br class="title-page-name"/>print('The accuracy for the testing data is {}'.format(svc_gcv_tfidf.score(count_test, Y_test)))<br class="title-page-name"/><br class="title-page-name"/>print(classification_report(Y_test, svc_tfidf_predicted_values))<br class="title-page-name"/># Pass actual &amp; predicted values to the confusion_matrix()<br class="title-page-name"/>cm = confusion_matrix(Y_test, svc_tfidf_predicted_values)<br class="title-page-name"/>plt.figure()<br class="title-page-name"/>plot_confusion_matrix(cm, classes=target_names)<br class="title-page-name"/>plt.show()</pre>
<ol start="15" class="calibre14">
<li class="calibre11">Plot the ROC curve for each of the models. The code for one of the plots is shown here (the complete code is provided in this book's code bundle):</li>
</ol>
<pre class="calibre18">fpr, tpr, thresholds = roc_curve(Y_test, rf_count_probabilities[:,1])<br class="title-page-name"/>roc_auc = auc(Y_test, rf_count_probabilities[:,1])<br class="title-page-name"/><br class="title-page-name"/>plt.title('ROC Random Forest Count Data')<br class="title-page-name"/>plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)<br class="title-page-name"/>plt.legend(loc='lower right')<br class="title-page-name"/>plt.plot([0,1],[0,1],'r--')<br class="title-page-name"/>plt.xlim([-0.1,1.0])<br class="title-page-name"/>plt.ylim([-0.1,1.01])<br class="title-page-name"/>plt.ylabel('True Positive Rate')<br class="title-page-name"/>plt.xlabel('False Positive Rate')</pre>
<p class="calibre20">In the following screenshot, we can compare the ROC curves of all the models we've trained:</p>
<p class="CDPAlignCenter"><img class="aligncenter120" src="assets/9041a0b7-65de-4953-8d62-d9f33d01c47a.png"/></p>
<ol start="16" class="calibre14">
<li class="calibre11">Plot the ROC curves for the ensemble results on the count and TF-IDF data:</li>
</ol>
<p class="CDPAlignCenter"><img class="aligncenter121" src="assets/fc14287a-bf2c-4069-b5a0-5547aa7d1817.png"/></p>
<ol start="17" class="calibre14">
<li class="calibre11">Calculate the accuracy of the ensemble with max-voting:</li>
</ol>
<pre class="calibre18">predicted_values_count = np.array([rf_count_predicted_values, \<br class="title-page-name"/>                                   nb_count_predicted_values, \<br class="title-page-name"/>                                   svc_count_predicted_values])<br class="title-page-name"/><br class="title-page-name"/>predicted_values_tfidf = np.array([rf_tfidf_predicted_values, \<br class="title-page-name"/>                                   nb_tfidf_predicted_values, \<br class="title-page-name"/>                                   svc_tfidf_predicted_values])<br class="title-page-name"/><br class="title-page-name"/>predicted_values_count = mode(predicted_values_count)<br class="title-page-name"/>predicted_values_tfidf = mode(predicted_values_tfidf)</pre>
<ol start="18" class="calibre14">
<li class="calibre11">Plot the test accuracy for each of the models trained on the count data and the TF-IDF data:</li>
</ol>
<pre class="calibre18">count = np.array([rf_count_test_accuracy,\<br class="title-page-name"/>                  nb_count_test_accuracy,\<br class="title-page-name"/>                  svc_count_test_accuracy,\<br class="title-page-name"/>                  accuracy_score(Y_test, predicted_values_count[0][0])])<br class="title-page-name"/><br class="title-page-name"/>tfidf = np.array([rf_tfidf_test_accuracy,\<br class="title-page-name"/>                  nb_tfidf_test_accuracy,\<br class="title-page-name"/>                  svc_tfidf_test_accuracy,\<br class="title-page-name"/>                  accuracy_score(Y_test, predicted_values_tfidf[0][0])])<br class="title-page-name"/><br class="title-page-name"/>label_list = ["Random Forest", "Naive_Bayes", "SVM_Linear", "Ensemble"] <br class="title-page-name"/>plt.plot(count)<br class="title-page-name"/>plt.plot(tfidf)<br class="title-page-name"/>plt.xticks([0,1,2,3],label_list)<br class="title-page-name"/><br class="title-page-name"/>for i in range(4):<br class="title-page-name"/>    plt.text(x=i,y=(count[i]+0.001), s=np.round(count[i],4))<br class="title-page-name"/><br class="title-page-name"/>for i in range(4):<br class="title-page-name"/>    plt.text(x=i,y=tfidf[i]-0.003, s=np.round(tfidf[i],4))<br class="title-page-name"/><br class="title-page-name"/>    <br class="title-page-name"/>plt.legend(["Count","TFIDF"])<br class="title-page-name"/>plt.title("Test accuracy")<br class="title-page-name"/><br class="title-page-name"/>plt.tight_layout(pad=1,rect=(0, 0, 2.5, 2))<br class="title-page-name"/>plt.show()</pre>
<p class="calibre20">The following plot shows the accuracy comparison between the count data and the TF-IDF data across all models and the ensemble result:</p>
<p class="CDPAlignCenter"><img class="aligncenter122" src="assets/145caddb-b65c-419c-a1d2-a5ab4350fe56.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p class="calibre2">We started by importing the required libraries. In this chapter, we used a module called <kbd class="calibre12">glob</kbd>. The <kbd class="calibre12">glob</kbd> <span class="calibre5">module</span><span class="calibre5"> </span><span class="calibre5">is used to </span><span class="calibre5">define the techniques to match a specified pattern to a path, a directory, and a filename. We used the glob module to look for all the files in a specified path. After that, we used the <kbd class="calibre12">open()</kbd> method to open each file in read mode. We read each file and appended it to form a dataset with all the review comments. We also created a label column to tag each review with a positive or negative tag.</span></p>
<p class="calibre2">However, after we appended all the positive and negative reviews, we noticed that they were added <span class="calibre5">sequentially</span><span class="calibre5">, which means the first half held all the positive reviews and the second half contained the negative reviews. We shuffled the data using the</span> <kbd class="calibre12">shuffle()</kbd> <span class="calibre5">method. </span></p>
<p class="calibre2">We cleaned our data by converting it to lowercase, removing the punctuation and stop words, performing stemming, and tokenizing the texts to create feature vectors.</p>
<p class="calibre2">In the <em class="calibre13">How to do it...</em> section, we started by importing the libraries in <em class="calibre13">Step 1</em>. In <em class="calibre13">Step 2</em>, we separated our target and feature variables into <em class="calibre13">X</em> and <em class="calibre13">Y</em>.</p>
<p class="calibre2">We split our data into train and test subsets in S<em class="calibre13">tep 3</em>. We used <span class="calibre5"><kbd class="calibre12">test_size=.3</kbd> to split the data into train and test subsets.</span></p>
<p class="calibre2">In S<em class="calibre13">tep 4</em> and S<em class="calibre13">tep 5</em>, we used <span class="calibre5"><kbd class="calibre12">CountVectorizer()</kbd> and </span><span class="calibre5"><kbd class="calibre12">TfidfVectorizer()</kbd> to </span><span class="calibre5">convert the text into vectors and the </span><span class="calibre5">text into TF-IDF vectors, respectively. Note that with <kbd class="calibre12">CountVectorizer()</kbd>, we generated the <kbd class="calibre12">count_train</kbd> and <kbd class="calibre12">count_test</kbd> datasets. With <kbd class="calibre12">TfidfVectorizer()</kbd>, we generated the </span><span class="calibre5"><kbd class="calibre12">tfidf_train</kbd> and <kbd class="calibre12">tfidf_test</kbd> datasets.</span></p>
<p class="calibre2">In <em class="calibre13">Step 6</em>, we set our hyperparameters for grid-search to train a random forest model. We trained our random forest model on the count data and checked our train and test accuracy.</p>
<div class="packtinfobox">We used the <kbd class="calibre19">predict()</kbd> and <kbd class="calibre19">predict_proba()</kbd> methods on our test data for all the models we built to predict the class as well as the class probabilities.</div>
<p class="calibre2">In <em class="calibre13">Step 7</em>, we generated the confusion matrix to evaluate the model's performance for the random forest model we built in the preceding step. In <em class="calibre13">Step 8</em> and <em class="calibre13">Step 9</em>, we repeated the training for another random forest model on the TF-IDF data and evaluated the performance. We trained the <span class="calibre5">Naive</span><span class="calibre5"> </span>Bayes model on the count data and the TF-IDF data from <em class="calibre13">Step 10</em> through to <em class="calibre13">Step 12</em>.</p>
<p class="calibre2">In <em class="calibre13">Step 13</em> and <em class="calibre13">Step 14</em>, we trained the support vector classifier algorithm with the linear kernel on the count data and the TF-IDF data, respectively. In <em class="calibre13">Step 15</em>, we plotted the ROC curves with the AUC score for each of the base learners we built. We also plotted the RUC curves for the ensemble in <em class="calibre13">Step 16</em> to compare the performance with the base learners. Finally, in <em class="calibre13">Step 17</em>, we plotted the test accuracy of each of the models on the count and TF-IDF data. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p class="calibre2">In today’s world, the availability and flow of textual information are limitless. This means we need various techniques to deal with these textual matters to extract meaningful information. For example, <strong class="calibre4">p<span class="calibre5">arts-of-speech (POS)</span> tagging</strong> is one of the fundamental tasks in the NLP space. <strong class="calibre4">POS tagging</strong> is used to label words in a text with their respective parts of speech. <span class="calibre5">These tags may then be used </span>with<span class="calibre5"> more complex tasks, such as syntactic and semantic parsing, <strong class="calibre4">machine translation</strong> (<strong class="calibre4">MT</strong>), and question answering.</span></p>
<p class="calibre2">There are eight main parts of speech:</p>
<ul class="calibre10">
<li class="calibre11">Nouns</li>
<li class="calibre11">Pronouns</li>
<li class="calibre11">Adjectives</li>
<li class="calibre11">Verbs</li>
<li class="calibre11">Adverbs</li>
<li class="calibre11">Prepositions</li>
<li class="calibre11">Conjunctions</li>
<li class="calibre11">Interjections:</li>
</ul>
<p class="CDPAlignCenter"><img class="aligncenter123" src="assets/6ab29f62-59f4-4509-99e2-976ee50b21b6.png"/></p>
<p class="calibre2">The NLTK library has functions to get POS tags that can be applied to texts after tokenization. Let's import the required libraries:</p>
<pre class="calibre15"><span>import</span> <span>os</span>
<span>import</span> <span>pandas</span> <span>as</span> <span>pd</span>
<span>import</span> <span>nltk</span>
<span>from</span> <span>nltk.tag</span> <span>import</span> <span>pos_tag</span>
<span>from</span> <span>nltk.corpus</span> <span>import</span> <span>stopwords</span></pre>
<p class="calibre2">We take our previously created DataFrame <kbd class="calibre12">df_moviereviews</kbd>. We convert text into lowercase:</p>
<pre class="calibre15"><span>df_moviereviews</span>['text'] =<span>df_moviereviews</span>['text'].apply(lambda x: " ".join(x.lower() for x in x.split()))<br class="title-page-name"/><span>df_moviereviews</span>['text'].head()</pre>
<p class="calibre2">We preprocess the text by removing stop words, punctuation, lemmatization, and tokenization:</p>
<pre class="calibre15">from nltk.stem.wordnet import WordNetLemmatizer<br class="title-page-name"/>import string<br class="title-page-name"/>stop = set(stopwords.words('english'))<br class="title-page-name"/>exclude = set(string.punctuation) <br class="title-page-name"/>lemma = WordNetLemmatizer()<br class="title-page-name"/>def clean(doc):<br class="title-page-name"/>    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])<br class="title-page-name"/>    stop_free = ''.join(ch for ch in stop_free if ch not in exclude)<br class="title-page-name"/>    normalized = " ".join(lemma.lemmatize(word) for word in stop_free.split())<br class="title-page-name"/>    return normalized<br class="title-page-name"/><br class="title-page-name"/>tokenized_sent = [clean(doc).split() for doc in <span>df_moviereviews</span>["text"]]</pre>
<p class="calibre2">We take a look at the list of the first 10 tokens from the first movie review:</p>
<pre class="calibre15">tokenized_sent[0][0:10]</pre>
<p class="calibre2">This generates the following output:</p>
<p class="CDPAlignCenter"><img src="assets/5c975229-a885-4037-afc2-c6d12b525fbd.png" class="calibre61"/></p>
<p class="calibre2">We perform POS tagging:</p>
<pre class="calibre15">postag=[nltk.pos_tag(token) for token in tokenized_sent]</pre>
<p class="calibre2">We print the first 10 POS tags for the first movie review:</p>
<pre class="calibre15">postag[0][0:10]</pre>
<p class="calibre2">We see the POS tagged words:</p>
<p class="CDPAlignCenter"><img src="assets/089f0a97-2f31-4b4b-8e9a-c4f8d5165df3.png" class="calibre62"/></p>
<p class="calibre2"><strong class="calibre4">Chunking</strong> is another process that can add more structure to POS tagging. <span class="calibre5">Chunking is used for entity detection; it tags multiple tokens to recognize them as meaningful entities. There are various chunkers available; <kbd class="calibre12">NLTK</kbd> provides <kbd class="calibre12">ne_chunk</kbd>, which recognizes people (names), places, and organizations. Other frequently used chunkers include <kbd class="calibre12">OpenNLP</kbd>, <kbd class="calibre12">Yamcha</kbd>, and <kbd class="calibre12">Lingpipe</kbd>. It's also possible to use a combination of chunkers and apply max-voting on the results to improve the classification's performance.</span></p>


            </article>

            
        </section>
    </body></html>