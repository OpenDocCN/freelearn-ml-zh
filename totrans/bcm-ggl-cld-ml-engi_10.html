<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer082">
<h1 class="chapter-number" id="_idParaDest-178"><a id="_idTextAnchor179"/>10 </h1>
<h1 id="_idParaDest-179"><a id="_idTextAnchor180"/>Achieving the GCP ML Certification</h1>
<p>Congratulations! You have gone through all the chapters thus far and built a strong knowledge base and skillset for <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) in Google Cloud. Now, it is time to integrate what you have learned so far and take the GCP ML certification exam – the last part of our learning roadmap.</p>
<p>The Google Professional Machine Learning Engineer certification exam is a very important part of your journey to becoming a Google Cloud Certified Machine Learning Engineer. To prepare for and pass the exam, you must review all the contents in this book and integrate them to deeply understand them and connect all the dots. </p>
<p>We recommend that you take the following steps to prepare for and achieve the Google Professional ML Engineer certification:</p>
<ol>
<li>Read the official Google ML certification exam guide.</li>
<li>Read all the chapters in this book.</li>
<li>Complete all the hands-on labs in this book.</li>
<li>Practice and review all the practice questions in this chapter.</li>
</ol>
<p>To get you prepared, we have provided some practice questions for the ML certification exam, along with the analysis of the questions in this chapter. Make sure you fully understand each question and all the answers to the questions, and why the right answer is right and the wrong answers are wrong. Keep in mind that the questions set here are just examples and we aim to provide a pilot sample for you to follow. You will need to do more research on the internet to reach a comprehensive level for the exam. </p>
<h1 id="_idParaDest-180"><a id="_idTextAnchor181"/>GCP ML exam practice questions</h1>
<p>Please read <a id="_idIndexMarker566"/>each question carefully and thoroughly, and fully understand it. Please also review all the docs that are related to the question at the reference links provided:</p>
<ul>
<li><strong class="bold">Question 1</strong>: Space Y is launching its hundredth satellite to build its StarSphere network. They have designed an accurate orbit (launching speed/time/and so on) for it based on the existing 99 satellite orbits to cover the Earth’s scope. What’s the best solution to forecast the position of the 100 satellites after the hundredth launch?<ol><li>Use ML algorithms and train ML models to forecast</li>
<li>Use neural networks to train the model to forecast</li>
<li>Use physical laws and actual environmental data to model and forecast</li>
<li>Use a linear regression model to forecast</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: This is an ML problem framing question. To decide whether ML is the best method for a problem, we need to see whether traditional science modeling would be very difficult or impossible to solve the problem and whether plenty of data exists. When we start, science modeling will be our first choice since it builds the most accurate model based on science and natural laws. For example, given the initial position and speed of an object, as well as its mass and the forces acting on it, we can precisely predict its position at any time. For this case, the mathematical model works much better than any ML model!</p>
<p>To forecast the hundredth satellite’s orbit, answer C is the best choice here.</p>
<p><strong class="bold">Reference</strong>: Section <em class="italic">Is ML the best solution?</em> in <a href="B18333_03.xhtml#_idTextAnchor072"><em class="italic">Chapter 3</em></a>, <em class="italic">Preparing for ML Development</em></p>
<ul>
<li><strong class="bold">Question 2</strong>: A financial company is building an ML model to detect credit card fraud based on their historical dataset, which contains 20 positives and 4,990 negatives.</li>
</ul>
<p>Due to the imbalanced classes, the model training is not working as desired. What’s the best way to resolve this issue?</p>
<ol>
<li value="1">Data augmentation</li>
<li>Early stopping</li>
<li>Downsampling and upweighting</li>
<li>Regularization</li>
</ol>
<p><strong class="bold">Analysis</strong>: This question is about class imbalance when preparing data for classification problems. When the data is imbalanced, it will be very difficult to train the ML model and get good forecasts. We need to use <em class="italic">downsampling and upweighting</em> to balance the classes, so the answer is C.</p>
<p><strong class="bold">Reference</strong>: Section <em class="italic">Data sampling and balancing</em> in <a href="B18333_03.xhtml#_idTextAnchor072"><em class="italic">Chapter 3</em></a>, <em class="italic">Preparing for ML Development</em></p>
<ul>
<li><strong class="bold">Question 3</strong>: A chemical manufacturer is using a GCP ML pipeline to detect real-time sensor anomalies by queuing the inputs and analyzing and visualizing the data. Which one will you choose for the pipeline? <ol><li value="1">Dataproc | AI Platform | BQ</li>
<li>Dataflow | AutoML | Cloud SQL</li>
<li>Dataflow | AI Platform | BQ</li>
<li>Dataproc | AutoML | Bigtable</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: This is an ML pipeline question. We need to understand the difference between Dataflow and Dataproc, AI Platform and AutoML, as well as the various GCP databases: Cloud SQL, Bigtable, and BQ.</p>
<p>Dataproc and Dataflow are GCP data processing services, and both can process batch or streaming data. However, Dataproc is designed to run on clusters for jobs that are compatible with MapReduce (Apache Hadoop, Hive, and Spark). Dataflow is based on parallel data processing and works better if your data has no implementation with Spark or Hadoop.</p>
<p>AI Platform involves “human-performed” ML training – using your own data and model. AutoML is “automated” ML training with Google’s model and your own data, with no coding.</p>
<p>Out of the GCP database/warehouse products, Cloud SQL is for relational data online transaction processing, Bigtable is more for NoSQL transaction processing, and BQ is great for analyzing and visualizing data (integrating with Data Studio).</p>
<p>Based on this, we will choose C as the answer.</p>
<ul>
<li><strong class="bold">Question 4</strong>: A real estate company, Zeellow, does great business buying and selling properties in the United States. Over the past few years, they have accumulated a big amount of historical data for US houses.</li>
</ul>
<p>Zeellow is using ML training to predict housing prices, and they retrain the models every month by integrating new data. The company does not want to write any code in the ML process. What method best suits their needs?</p>
<ol>
<li value="1">AutoML tables</li>
<li>BigQuery ML</li>
<li>AI Platform </li>
<li>AutoML classification </li>
</ol>
<p><strong class="bold">Analysis</strong>: This question is also about the difference between AutoML and AI Platform, as well as between regression and classification. Since AutoML serves the purpose of no coding during the ML process, and this is a structured data ML problem, the correct answer is A.</p>
<ul>
<li><strong class="bold">Question 5</strong>: The data scientist team is building a deep learning model for a customer <a id="_idIndexMarker567"/>support center of a big <strong class="bold">Enterprise Resource Planning</strong> (<strong class="bold">ERP</strong>) company, which has many ERP products and modules. The DL model will input customers’ chat texts and categorize them into products before routing them to the corresponding team. The company wants to minimize the model development time and data preprocessing time. What strategy/platform should they choose?<ol><li value="1">AI Platform</li>
<li>Auto ML</li>
<li>NLP API</li>
<li>Vertex AI Custom notebooks</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The key point here is that <em class="italic">the company wants to minimize the model development time and data preprocessing time.</em> AutoML is the best choice, so the correct answer is B.</p>
<ul>
<li><strong class="bold">Question 6</strong>: A real estate company, Zeellow, does great business buying and selling properties in the United States. Over the past few years, they have accumulated a big amount of historical data for US houses.</li>
</ul>
<p>Zeellow wants to use ML to forecast future sales by leveraging their historical sales data. The historical data is stored in cloud storage. You want to rapidly experiment with all the available data. How should you build and train your model?</p>
<ol>
<li value="1">Load data into BigQuery and use BigQuery ML</li>
<li>Convert the data into CSV and use AutoML Tables</li>
<li>Convert the data into TFRecords and use TensorFlow</li>
<li>Convert and refactor the data into CSV format and use the built-in XGBoost library</li>
</ol>
<p><strong class="bold">Analysis</strong>: The key point here is that we need to experiment quickly with all the structured datasets stored in cloud storage. BQ and BQML are the best options here since all the others will take a long time to build and train the model. Thus, the correct answer is A.</p>
<ul>
<li><strong class="bold">Question 7</strong>: A real estate company, Zeellow, uses ML to forecast future sales by leveraging their historical data. New data is coming in every week, and Zeellow needs to make sure the model is continually retrained to reflect the marketing trend. What should they do with the historical data and new data?<ol><li value="1">Only use the new data for retraining</li>
<li>Update the datasets weekly with new data</li>
<li>Update the datasets with new data when model evaluation metrics do not meet the required criteria</li>
<li>Update the datasets monthly with new data</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: Model retraining is the key term here. Since data changes over time and causes trained models to become obsolete, model retraining is the norm in the ML process. In this case, when do we need to retrain the model? The answer is when the performance metrics do not meet the requirements. How do we retrain the model? The answer is to use the integrated datasets, including existing and new data. Therefore, the correct answer is C.</p>
<ul>
<li><strong class="bold">Question 8</strong>: A real estate company, Zeellow, uses ML to forecast future sales by leveraging their historical data. Their data science team trained and deployed a DL model in production half a year ago. Recently, the model is suffering from performance issues due to data distribution changes.</li>
</ul>
<p>The team is working on a strategy for model retraining. What is your suggestion?</p>
<ol>
<li value="1">Monitor data skew and retrain the model</li>
<li>Retrain the model with fewer model features</li>
<li>Retrain the model to fix overfitting</li>
<li>Retrain the model with new data coming in every month</li>
</ol>
<p><strong class="bold">Analysis</strong>: Model retraining is based on data value skews, which are significant changes in the statistical properties of data. When data skew is detected, this means that data patterns are changing, and we need to retrain the model to capture these changes. The question did not mention any overfitting issues, nor did it mention feature reduction. The retraining strategy will be monitoring data skew and retraining the model with the new inputs. Thus, the correct answer is A.</p>
<p><strong class="bold">Reference</strong>: <a href="https://developers.google.com/machine-learning/guides/rules-of-ml/#rule_37_measure_trainingserving_skew">https://developers.google.com/machine-learning/guides/rules-of-ml/#rule_37_measure_trainingserving_skew</a>.</p>
<ul>
<li><strong class="bold">Question 9</strong>: Recent research has indicated that when a certain kind of cancer, <em class="italic">X</em>, is developed in a human liver, there are usually other symptoms that can be identified as objects <em class="italic">Y</em> and <em class="italic">Z</em> from CT scan images. A hospital is using this research to train ML models with a label map of (<em class="italic">X</em>, <em class="italic">Y</em>, <em class="italic">Z</em>) on CT images. What cost functions should be used in this case?<ol><li value="1">Binary cross-entropy</li>
<li>Categorical cross-entropy</li>
<li>Sparse categorical cross-entropy</li>
<li>Dense categorical cross-entropy</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The correct answer is B.</p>
<p>In <a href="B18333_05.xhtml#_idTextAnchor116"><em class="italic">Chapter 5</em></a>, <em class="italic">Understanding Neural Networks and Deep Learning</em>, <em class="italic">in The cost function</em> section, we discussed the use cases for different cost functions. Binary cross-entropy is used for binary classification problems. Categorical entropy is better to use when you want to prevent the model from giving more importance to a certain class – the same as the one-hot encoding idea. Sparse categorical entropy is more optimal when your classes are mutually exclusive (for example, when each sample belongs exactly to one class).</p>
<ul>
<li><strong class="bold">Question 10</strong>: The data science team in your company has built a DNN model to forecast the sales value for an automobile company, based on historical data. As a Google ML Engineer, you need to verify that the features selected are good enough for the ML model.<ol><li value="1">Train the model with L1 regularization and verify that the loss is constant</li>
<li>Train the model with no regularization and verify that the loss is constant</li>
<li>Train the model with L2 regularization and verify that the loss is decreasing</li>
<li>Train the model with no regularization and verify that the loss is close to zero</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The correct answer is D. </p>
<p>The loss function is the measurement for model prediction accuracy and is used as an index for the ML training process. To verify that the model that’s been built has enough features, we need to make sure that the loss function is close to zero when no regularizations are used.</p>
<p><strong class="bold">Reference</strong>: Section <em class="italic">Regularization</em> in <a href="B18333_04.xhtml#_idTextAnchor094"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing and Deploying ML Models</em></p>
<ul>
<li><strong class="bold">Question 11</strong>: The data science team in your company has built a DNN model to forecast the sales value for a real estate company, based on historical data. As a Google ML Engineer, you find that the model has over 300 features and that you wish to remove some features that are not contributing to the target. What will you do?<ol><li value="1">Use Explainable AI to understand the feature contributions and reduce the non-contributing ones.</li>
<li>Use L1 regularization to reduce features.</li>
<li>Use L2 regularization to reduce features.</li>
<li>Drop a feature at a time, train the model, and verify that it does not degrade the model. Remove these features.</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The correct answer is A. This question is discussing feature selection, and Explainable AI is one of the ways to understand which features are contributing and which ones are not. It is important to understand that L1 and L2 are methods for resolving model overfitting issues and not feature selection in data engineering.</p>
<ul>
<li><strong class="bold">Question 12</strong>: The data science team in your company has built a DNN model to forecast the sales value for a real estate company, based on historical data. They found that the model fits the training dataset well, but not the validation dataset. What would you do to improve the model?<ol><li value="1">Apply a dropout parameter of 0.3 and decrease the learning rate by a factor of 10</li>
<li>Apply an L2 regularization parameter of 0.3 and decrease the learning rate by a factor of 10</li>
<li>Apply an L1 regularization parameter of 0.3 and increase the learning rate by a factor of 10</li>
<li>Tune the hyperparameters to optimize the L2 regularization and dropout parameters</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The correct answer is D.</p>
<p>This question is discussing techniques to avoid model overfitting. While L1/L2 regularization, dropout parameters, and learning rate are all ways to help, we must tune the hyperparameters and find the optimized values. A hint here is that the correct answer would be fitting to the general case and thus will not have concrete numbers such as 0.3, 10, and so on.</p>
<ul>
<li><strong class="bold">Question 13</strong>: You are building a DL model for a customer service center. The model will input customers’ chat text and analyze their sentiments. What algorithm should be used for the model?<ol><li value="1">MLP</li>
<li>Regression</li>
<li>CNN</li>
<li>RNN</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The correct answer is D.</p>
<p>This question tests the different algorithms used for ML/DL. Since text processing <a id="_idIndexMarker568"/>for sentiment analysis needs to process sequential data (time series), the best option is <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>).</p>
<ul>
<li><strong class="bold">Question 14</strong>: A health insurance company scans customers' hand-filled claim forms and stores them in Google Cloud Storage buckets in real time. They use ML models <a id="_idIndexMarker569"/>to recognize the handwritten texts. Since the claims may contain <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>), company policies require only authorized persons to access the information. What’s the best way to store and process this streaming data?<ol><li value="1">Create two buckets and label them as sensitive and non-sensitive. Store data in the non-sensitive bucket first. Periodically scan it using the DLP API and move the sensitive data to the sensitive bucket.</li>
<li>Create one bucket to store the data. Only allow the ML service account access to it.</li>
<li>Create three buckets – quarantine, sensitive, and non-sensitive. Store all the data in the quarantine bucket first. Then, periodically scan it using the DLP API and move the data to either the sensitive or non-sensitive bucket.</li>
<li>Create three buckets – quarantine, sensitive, and non-sensitive. Store all the data in the quarantine bucket first. Then, once the file has been uploaded, trigger the DLP API to scan it, and move the data to either the sensitive or non-sensitive bucket.</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The correct answer is D.</p>
<p>This is a business use case for PII/private data storage and processing, and a typical solution is to create three buckets and utilize DLP to scan and then move the raw data into different buckets and control their access.</p>
<ul>
<li><strong class="bold">Question 15</strong>: A real estate company, Zeellow, uses ML to forecast future sales by leveraging their historical data. The recent model training was able to achieve the desired forecast accuracy objective, but it took the data science team a long time. They want to decrease the training time without affecting the achieved model accuracy. What hyperparameter should the team adjust?<ol><li value="1">Learning rate</li>
<li>Epochs</li>
<li>Scale tier</li>
<li>Batch size</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The correct answer is C since changing the other three parameters will change the model’s prediction accuracy. </p>
<ul>
<li><strong class="bold">Question 16</strong>: The data science team has built a DNN model to monitor and detect defective products using the images from the assembly line of an automobile manufacturing company. As a Google ML Engineer, you need to measure the performance of the ML model for the test dataset/images. Which of the following would you choose?<ol><li value="1">The AUC value</li>
<li>The recall value</li>
<li>The precision value</li>
<li>The TP value</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The correct answer is A because it measures how well the predictions are ranked rather than their absolute values. It is a classification threshold invariant and thus is the best way to measure the model’s performance.</p>
<ul>
<li><strong class="bold">Question 17</strong>: The data science team has built a DL model to monitor and detect defective products using the images from the assembly line of an automobile manufacturing company. Over time, the team has built multiple model versions in AI Platform. As a Google ML Engineer, how will you compare the model versions?<ol><li value="1">Compare the mean average precision for the model versions</li>
<li>Compare the model loss functions on the training dataset</li>
<li>Compare the model loss functions on the validation dataset</li>
<li>Compare the model loss functions on the testing dataset</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: The correct answer is A because it measures how well the different model versions perform over time: deploy your model as a model version and then create an evaluation job for that version. By comparing the mean average precision across the model versions, you can find the best performer.</p>
<p><strong class="bold">References</strong>: <a href="https://cloud.google.com/ai-platform/prediction/docs/continuous-evaluation/view-metrics#compare_mean_average_precision_across_models">https://cloud.google.com/ai-platform/prediction/docs/continuous-evaluation/view-metrics#compare_mean_average_precision_across_models</a>.</p>
<ul>
<li><strong class="bold">Question 18</strong>: The data science team is building a recommendation engine for an e-commerce website using ML models to increase its business revenue, based on users’ similarities. What model would you choose?<ol><li value="1">Collaborative filtering</li>
<li>Regression</li>
<li>Classification</li>
<li>Content-based filtering</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: For this recommendation engine question, the correct answer is A. </p>
<p>Content-based filtering uses the similarity between items to recommend items that are similar to what the user likes. Collaborative filtering uses similarities between users to provide recommendations. The question specifies “based on users’ similarities.”</p>
<p><strong class="bold">References</strong>: <a href="https://developers.google.com/machine-learning/recommendation/overview/candidate-generation">https://developers.google.com/machine-learning/recommendation/overview/candidate-generation</a>.</p>
<ul>
<li><strong class="bold">Question 19</strong>: The data science team is building a fraud-detection model for a credit card company, whose objective is to detect as much fraud as possible and avoid as many false alarms as possible. What confusion matrix index would you maximize for this model performance evaluation?<ol><li value="1">Precision</li>
<li>Recall</li>
<li>The area under the PR curve</li>
<li>The area under the ROC curve</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: In this fraud-detection problem, it asks you to focus on detecting fraudulent transactions - maximize True Positive rate and minimize False Negative - maximize recall (<em class="italic">Recall = TruePositives / (TruePositives + FalseNegatives)</em>). It also asks you to minimize false alarms (false positives) - maximize precision (<em class="italic">Precision = TruePositives / (TruePositives + FalsePositives)</em>).</p>
<p>So, since you want to maximize both precision and recall, the correct answer is C (maximize the area under the PR curve).</p>
<p><strong class="bold">References</strong>: <a href="https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/">https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/</a>.</p>
<ul>
<li><strong class="bold">Question 20</strong>: The data science team is building a data pipeline for an auto manufacturing company, whose objective is to integrate all the data sources that exist in their on-premise facilities, via a codeless data ETL interface. What GCP service will you use?<ol><li value="1">Dataproc</li>
<li>Dataflow</li>
<li>Dataprep</li>
<li>Data Fusion</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: Since the question asks for data integration with a codeless interface, Data Fusion is the best choice here. Thus, the correct answer is D.</p>
<p><strong class="bold">References</strong>: <a href="https://cloud.google.com/data-fusion/docs/concepts/overview#using_the_code-free_web_ui">https://cloud.google.com/data-fusion/docs/concepts/overview#using_the_code-free_web_ui</a>.</p>
<ul>
<li><strong class="bold">Question 21</strong>: The data science team has built a TensorFlow model in BigQuery for a real estate company, whose objective is to integrate all their data models into the new Google Vertex AI platform. What’s the best strategy?<ol><li value="1">Export the model from BigQuery ML</li>
<li>Register the BQML model to Vertex AI</li>
<li>Import the model into Vertex AI</li>
<li>Use AI Platform as the middle stage </li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: Since the question asks for model integration with Vertex AI, which allows you to register a BQML model in it, the correct answer is B.</p>
<p><strong class="bold">References</strong>: https://cloud.google.com/bigquery-ml/docs/managing-models-vertex.</p>
<ul>
<li><strong class="bold">Question 22</strong>: A real estate company, Zeellow, uses ML to forecast future house sale prices by leveraging their historical data. The data science team needs to build a model to predict US house sale prices based on the house location (US city-specific) and house type. What strategy is the best for feature engineering in this case?<ol><li value="1">One feature cross: [latitude X longitude X housetype]</li>
<li>Two feature crosses: [binned latitude X binned housetype] and [binned longitude X binned housetype]</li>
<li>Three separate binned features: [binned latitude], [binned longitude], [binned housetype]</li>
<li>One feature cross: [binned latitude X binned longitude X binned housetype]</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: Crossing binned latitude with binned longitude enables the model to learn city-specific effects on house types. It prevents a change in latitude from producing the same result as a change in longitude. Depending on the granularity of the bins, this feature cross could learn city-specific housing effects. So, the correct answer is D.</p>
<p><strong class="bold">References</strong>: <a href="https://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding">https://developers.google.com/machine-learning/crash-course/feature-crosses/check-your-understanding</a>.</p>
<ul>
<li><strong class="bold">Question 23</strong>: A health insurance company scans customer’s hand-filled claim forms and stores them in Google Cloud Storage buckets in real time. The data scientist team has developed an AI documentation model to digitize the images. By the end of each day, the submitted forms need to be processed automatically. The model is ready for deployment. What strategy should the team use to process the forms?<ol><li value="1">Vertex AI batch prediction</li>
<li>Vertex AI online prediction</li>
<li>Vertex AI ML pipeline prediction</li>
<li>Cloud Run to trigger prediction</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: As specified in the question, we need to run the process at the end of each day, which implies batch processing using AI Platform or Vertex AI. The correct answer is A.</p>
<ul>
<li><strong class="bold">Question 24</strong>: A real estate company, Zeellow, uses GCP ML to forecast future house sale prices by leveraging their historical data. Their data science team has about 30 members and each member has developed multiple versions of models using Vertex AI customer notebooks. What’s the best strategy to manage these different models and different versions developed by the team members?<ol><li value="1">Set up IAM permissions to allow each member access to their notebooks, models, and versions</li>
<li>Create a GCP project for each member for clean management</li>
<li>Create a map from each member to their GCP resources using BQ</li>
<li>Apply label/tags to the resources when they’re created for scalable inventory/cost/access management</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: Resource tagging/labeling is the best way to manage ML resources for medium/big data science teams. The best answer is D.</p>
<p><strong class="bold">Resource</strong>: https://cloud.google.com/resource-manager/docs/tags/tags-creating-and-managing.</p>
<ul>
<li><strong class="bold">Question 25</strong>: Starbucks is an international coffee shop selling multiple products A, B, C… at different stores (1, 2, 3… using one-hot encoding and location binning). They are building stores and want to leverage ML models to predict product sales based on historical data (A1 is the data for product A sales at store 1). Following the best practices of splitting data into a training subset, validation subset, and testing subset, how should the data be distributed into these subsets?<ol><li value="1">Distribute data randomly across the subsets:<ul><li>Training set: [A1, B2, F1, E2, ...]</li>
<li>Testing set: [A2, C3, D2, F4, ...]</li>
<li>Validation set: [B1, C1, D9, C2...]</li>
</ul></li>
<li>Distribute products randomly across the subsets:<ul><li>Training set: [A1, A2, A3, E1, E2, ...]</li>
<li>Testing set: [B1, B2, C1, C2, C3, ...]</li>
<li>Validation set: [D1, D2, F1, F2, F3, ...]</li>
</ul></li>
<li>Distribute stores randomly across subsets:<ul><li>Training set: [A1, B1, C1, ...]</li>
<li>Testing set: [A2, C2, F2, ...]</li>
<li>Validation set: [D3, A3, C3, ...]</li>
</ul></li>
<li>Aggregate the data groups by the cities where the stores are allocated and distribute cities randomly across subsets</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: This question is about dataset splitting to avoid data leakage. If we distribute the data randomly into the training, validation, and test sets, the model will be able to learn specific qualities about the products. If we divided things up at the product level so that the given products were only in the training subset, the validation subset, or the testing subset, the model would find it more difficult to get high accuracy on the validation since it would need to focus on the product characteristics/qualities. Therefore, the correct answer is B.</p>
<p><strong class="bold">Reference</strong>: https://developers.google.com/machine-learning/crash-course/18th-century-literature.</p>
<ul>
<li><strong class="bold">Question 26</strong>: You are building a DL model with Keras that looks as follows:<p class="source-code">model = tf.keras.sequential</p><p class="source-code">model.add(df.keras.layers.Dense(128,activation='relu',input_shape=(200, )))</p><p class="source-code">model.add(df.keras.layers.Dropout(rate=0.25))</p><p class="source-code">model.add(df.keras.layers.Dense(4,activation='relu'))</p><p class="source-code">model.add(df.keras.layers.Dropout(rate=0.25))</p><p class="source-code">model.add(Dense(2))</p></li>
</ul>
<p>How many trainable weights does this model have?</p>
<ol>
<li value="1">200x128+128x4+4x2</li>
<li>200x128+128x4+2</li>
<li>200x128+129x4+5x2</li>
<li>200x128x0.25+128x4x0.25+4x2</li>
</ol>
<p><strong class="bold">Analysis</strong>: This question is testing the concept of trainable weights in a Keras model. As you can see, the correct answer is D.</p>
<ul>
<li><strong class="bold">Question 27</strong>: The data science team is building a DL model for a customer support center of a big ERP company, which has many ERP products and modules. The company receives over a million customer service calls every day and stores them in GCS. The call data must not leave the region in which the call originated and no PII can be stored/analyzed. The model will analyze calls for customer sentiments. How should you design a data pipeline for call processing, analyzing, and visualizing?<ol><li value="1">GCS -&gt; Speech2Text -&gt; DLP -&gt; BigQuery</li>
<li>GCS -&gt; Pub/Sub -&gt; Speech2Text -&gt; DLP -&gt; Datastore</li>
<li>GCS -&gt; Speech2Text -&gt; DLP -&gt; BigTable</li>
<li>GCS -&gt; Speech2Text -&gt; DLP -&gt; Cloud SQL</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: Since the question asks for a data pipeline to process, analyze, and visualize, the best answer is A. BigQuery is the best tool here to analyze and visualize. </p>
<ul>
<li><strong class="bold">Question 28</strong>: The data science team is building an ML model to monitor and detect defective products using the images from the assembly line of an automobile manufacturing company, which does not have reliable Wi-Fi near the assembly line. As a Google ML Engineer, you need to reduce the amount of time spent by quality control inspectors utilizing the model’s fast defect detection. Your company wants to implement the new ML model as soon as possible. Which model should you use?<ol><li value="1">AutoML Vision </li>
<li>AutoML Vision Edge mobile-versatile-1 </li>
<li>AutoML Vision Edge mobile-low-latency-1 </li>
<li>AutoML Vision Edge mobile-high-accuracy-1 </li>
</ol></li>
</ul>
<p><strong class="bold">Analysis:</strong> Since the question asks for a quick inspection time and prioritizes latency reduction, the correct answer is C. </p>
<p><strong class="bold">Reference</strong>: <a href="https://cloud.google.com/vision/automl/docs/train-edge">https://cloud.google.com/vision/automl/docs/train-edge</a>.</p>
<ul>
<li><strong class="bold">Question 29</strong>: A national hospital is leveraging Google Cloud and a cell phone app to build an ML model to forecast heart attacks based on age, gender, exercise, heart rate, blood pressure, and more. Since the health data is highly sensitive personal information and cannot be stored in cloud databases, how should you train and deploy the ML model?<ol><li value="1">IoT with data encryption</li>
<li>Federated learning</li>
<li>Encrypted BQML</li>
<li>DLP API</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: Federated learning is the best choice here due to the restrictions. With federated learning, all the data is collected, and the model is trained with algorithms across multiple decentralized edge devices such as cell phones or websites, without exchanging them. Therefore, the best answer is B.</p>
<ul>
<li><strong class="bold">Question 30</strong>: You are an ML engineer at a media company. You need to build an ML model to analyze video content frame by frame, identify objects, and alert users if there is inappropriate content. Which Google Cloud products should you use to build this project?<ol><li value="1">Pub/Sub, Cloud Functions, and Cloud Vision API</li>
<li>Pub/Sub, Cloud IoT, Dataflow, Cloud Vision API, and Cloud Logging</li>
<li>Pub/Sub, Cloud Functions, Video Intelligence API, and Cloud Logging</li>
<li>Pub/Sub, Cloud Functions, AutoML Video Intelligence, and Cloud Logging</li>
</ol></li>
</ul>
<p><strong class="bold">Analysis</strong>: Since this question involves video analysis, this will eliminate A and B. AutoML video intelligence is for cases where you wish to customize models with Google’s model and your data. Therefore, C is the correct answer since the Video Intelligence API can be used to meet the requirements.</p>
<h1 id="_idParaDest-181"><a id="_idTextAnchor182"/>Summary</h1>
<p>In this chapter, we discussed the Google Cloud Professional Machine Learning Engineer certification exam and some practice questions. Since GCP ML is a changing domain, many new services have been developed and released by Google while this book was being written. By no means does this book cover all the exam topics in this domain. You will need to refer to the Google certification page for the certification exam guides and updates. </p>
<p>This chapter concludes part four of this book. In part five of this book, in the appendices, we will provide some labs and demos for practicing your hands-on skills. It is recommended that you go through each appendix and practice the labs step-by-step.</p>
</div>
</div>


<div id="sbo-rt-content"><div class="Content" id="_idContainer083">
<h1 id="_idParaDest-182"><a id="_idTextAnchor183"/>Part 5: Appendices</h1>
<p>In this part, we provide hands-on practices for ML in Google Cloud, including the basic GCP services, the Python data science libraries, the scikit-learn library, the GCP Vertex AI suite, and the Google Cloud ML APIs.</p>
<p>This part comprises the following chapters:</p>
<ul>
<li><a href="B18333_11.xhtml#_idTextAnchor184"><em class="italic">Appendix 1</em></a>, Practicing with Basic GCP Services</li>
<li><a href="B18333_12.xhtml#_idTextAnchor195"><em class="italic">Appendix 2</em></a>, Practicing with the Python Data Library</li>
<li><a href="B18333_13.xhtml#_idTextAnchor209"><em class="italic">Appendix 3</em></a>, Practicing with Scikit-Learn</li>
<li><a href="B18333_14.xhtml#_idTextAnchor218"><em class="italic">Appendix 4</em></a>, Practicing with Google Vertex AI</li>
<li><a href="B18333_15.xhtml#_idTextAnchor233"><em class="italic">Appendix 5</em></a>, Practicing with Google Cloud ML API</li>
</ul>
</div>
</div>


<div id="sbo-rt-content"><div id="_idContainer117">
<h1 class="chapter-number" id="_idParaDest-183"><a id="_idTextAnchor184"/>Appendix 1 </h1>
<h1 id="_idParaDest-184"><a id="_idTextAnchor185"/>Practicing with Basic GCP Services</h1>
<p>In this appendix, we will show some GCP resource provisioning examples, using the Google Cloud console and Cloud Shell. We will use the following architecture to practice using the Google Cloud console, as shown in <em class="italic">Figure 11.1</em>:</p>
<ul>
<li>A VPC network, VPC1, and two subnets in it: a public <strong class="source-inline">subnet1</strong> and a private <strong class="source-inline">subnet2</strong></li>
<li>A <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) in the public <strong class="source-inline">subnet1</strong> that has an external IP<a id="_idIndexMarker570"/> address and can be accessed from the internet</li>
<li>A VM in the private <strong class="source-inline">subnet2</strong> that does not have an external IP address and thus can only be accessed from the console browser, or from VMs within the same VPC</li>
<li>Another VPC network, VPC2, and one subnet within VPC2: a private <strong class="source-inline">subnet8</strong></li>
<li>A VM in the private <strong class="source-inline">subnet8</strong></li>
<li>Peering between VPC1 and VPC2</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="" height="833" src="image/B18333_11_1.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – A sample architecture for GCP console practice</p>
<p>In the GCP practice<a id="_idIndexMarker571"/> diagram in <em class="italic">Figure 11.1</em>, <strong class="source-inline">public subnet1</strong> is accessible from the internet. There is a Google Cloud Storage bucket called <strong class="source-inline">B1</strong>. If we want to have VM1, VM2, and VM8 access <strong class="source-inline">B1</strong>, what do we need to do? This is a great question to think about before reading further.</p>
<h1 id="_idParaDest-185"><a id="_idTextAnchor186"/>Practicing using GCP services with the Cloud console</h1>
<p>In GCP, a project<a id="_idIndexMarker572"/> is the basic unit for resource provision. You can<a id="_idIndexMarker573"/> use the following steps to begin a project:</p>
<ol>
<li value="1">After you log<a id="_idIndexMarker574"/> into the GCP console (<a href="https://console.cloud.google.com">https://console.cloud.google.com</a>) from your browser, you will see the following starting page:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer085">
<img alt="" height="581" src="image/B18333_11_2.jpg" width="1219"/>
</div>
</div>
<ol>
<li value="2">You can always create a new project by clicking the drop-down button next to <strong class="bold">My First Project</strong>.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="" height="57" src="image/B18333_11_3.jpg" width="503"/>
</div>
</div>
<p>Within the <strong class="bold">My First Project</strong> project, we will now create the network VPCs, subnets, and VMs.</p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor187"/>Creating network VPCs using the GCP console</h2>
<p>Use the following<a id="_idIndexMarker575"/> steps to create a VPC<a id="_idIndexMarker576"/> in the GCP console:</p>
<ol>
<li value="1">On the upper-left side of the window, there is the navigation drop-down menu that you will be able to use to choose the GCP services.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="" height="620" src="image/B18333_11_4.jpg" width="508"/>
</div>
</div>
<ol>
<li value="2">From the navigation menu on the left side, go to <strong class="bold">VPC network</strong> and select <strong class="bold">VPC networks</strong> from the dropdown. It will prompt you to enable <strong class="bold">Compute Engine API</strong>. Go ahead and enable it.</li>
</ol>
<p>You will be brought to the <strong class="bold">VPC network</strong> page, where you can create a VPC network. </p>
<ol>
<li value="3">Click <strong class="bold">CREATE VPC NETWORK</strong>.<div class="IMG---Figure" id="_idContainer088"><img alt="" height="281" src="image/B18333_11_5.jpg" width="1078"/></div></li>
</ol>
<ol>
<li value="4">Then, fill<a id="_idIndexMarker577"/> in the network <a id="_idIndexMarker578"/>details and create VPC1:<ul><li><strong class="bold">Subnet name</strong>: <strong class="source-inline">subnet1</strong></li>
<li><strong class="bold">IP v4 range</strong>: <strong class="source-inline">10.10.1.0/24</strong></li>
<li><strong class="bold">Region</strong>: <strong class="source-inline">us-east1</strong></li>
<li><strong class="bold">Subnet name</strong>: <strong class="source-inline">subnet2</strong></li>
<li><strong class="bold">IP v4 range</strong>: <strong class="source-inline">10.10.2.0/24</strong></li>
<li><strong class="bold">Region</strong>: <strong class="source-inline">asia-east1</strong></li>
</ul></li>
</ol>
<p>Then, <strong class="source-inline">vpc1</strong> is created with two subnets:</p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="" height="106" src="image/B18333_11_6.jpg" width="1181"/>
</div>
</div>
<ol>
<li value="5">Repeat <em class="italic">Steps 3 </em>and<em class="italic"> 4</em> to create <strong class="source-inline">vpc2</strong>:<ul><li><strong class="bold">Subnet name</strong>: <strong class="source-inline">subnet8</strong></li>
<li><strong class="bold">IP v4 range</strong>: <strong class="source-inline">192.168.1.0/24</strong></li>
<li><strong class="bold">Region</strong>: <strong class="source-inline">europe-central2</strong></li>
</ul></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="" height="191" src="image/B18333_11_7.jpg" width="1229"/>
</div>
</div>
<p>You now<a id="_idIndexMarker579"/> have two VPCs<a id="_idIndexMarker580"/> created: <strong class="source-inline">vpc1</strong> with two subnets and <strong class="source-inline">vpc2</strong> with one subnet.</p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor188"/>Creating a public VM, vm1, within vpc1/subnet1 using the GCP console</h2>
<p>Use the following<a id="_idIndexMarker581"/> steps to create a VM<a id="_idIndexMarker582"/> in the GCP console:</p>
<ol>
<li value="1">From the navigation menu on the left side, go to <strong class="bold">Compute Engine</strong> and then <strong class="bold">VM instances</strong>. Click the <strong class="bold">CREATE INSTANCE</strong> button.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="" height="638" src="image/B18333_11_8.jpg" width="963"/>
</div>
</div>
<p>Then, fill in the VM instance details:</p>
<ul>
<li><strong class="bold">Name</strong>: <strong class="source-inline">vm1</strong></li>
<li><strong class="bold">Region</strong>: <strong class="source-inline">us-east1</strong></li>
<li><strong class="bold">Zone</strong>: <strong class="source-inline">us-east1-b</strong></li>
<li><strong class="bold">Machine configuration</strong>: <strong class="bold">GENERAL-PURPOSE</strong>, <strong class="bold">N1</strong> series </li>
<li><strong class="bold">Machine type</strong>: <strong class="source-inline">f1-micro</strong><strong class="bold"> </strong></li>
<li><strong class="bold">Subnet name</strong>: <strong class="source-inline">subnet1</strong></li>
</ul>
<p>This is shown in<a id="_idIndexMarker583"/> the following<a id="_idIndexMarker584"/> screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="" height="621" src="image/B18333_11_9.jpg" width="1280"/>
</div>
</div>
<ol>
<li value="2">Select the defaults for the other options, and then click on <strong class="bold">NETWORKING, DISKS, SECURITY, MANAGEMENT, SOLE-TENANCY</strong>.<div class="IMG---Figure" id="_idContainer093"><img alt="" height="559" src="image/B18333_11_10.jpg" width="915"/></div></li>
</ol>
<ol>
<li value="3">Expand<a id="_idIndexMarker585"/> the <strong class="bold">Networking</strong> option, then<a id="_idIndexMarker586"/> go to <strong class="bold">Network interfaces</strong>.<div class="IMG---Figure" id="_idContainer094"><img alt="" height="603" src="image/B18333_11_11.jpg" width="551"/></div></li>
</ol>
<ol>
<li value="4">In the <strong class="bold">Edit network interface</strong> section, select <strong class="source-inline">vpc1</strong> for<a id="_idIndexMarker587"/> <strong class="bold">Network</strong><a id="_idIndexMarker588"/> and <strong class="source-inline">subnet1 Ipv4 (10.10.1.0/24)</strong> for <strong class="bold">Subnetwork</strong>, and leave everything else as the default.<div class="IMG---Figure" id="_idContainer095"><img alt="" height="793" src="image/B18333_11_12.jpg" width="530"/></div></li>
</ol>
<ol>
<li value="5">Click <strong class="bold">DONE</strong> and<a id="_idIndexMarker589"/> then<a id="_idIndexMarker590"/> <strong class="bold">CREATE</strong>.<div class="IMG---Figure" id="_idContainer096"><img alt="" height="638" src="image/B18333_11_13.jpg" width="459"/></div></li>
</ol>
<p>At this<a id="_idIndexMarker591"/> time, VM1<a id="_idIndexMarker592"/> is created in <strong class="source-inline">vpc1</strong> and <strong class="source-inline">subnet1</strong> (<strong class="source-inline">10.10.1.0/24</strong>), with the internal IP address of <strong class="source-inline">10.10.1.2</strong> and the external IP address of <strong class="source-inline">34.148.1.115</strong>.</p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="" height="323" src="image/B18333_11_14.jpg" width="903"/>
</div>
</div>
<p>To allow <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) into this Linux VM, you need<a id="_idIndexMarker593"/> to create a firewall rule to allow inbound SSH traffic.</p>
<ol>
<li value="6">Select <strong class="bold">View network details</strong> from<a id="_idIndexMarker594"/> the three dots drop-down<a id="_idIndexMarker595"/> menu.<div class="IMG---Figure" id="_idContainer098"><img alt="" height="431" src="image/B18333_11_15.jpg" width="765"/></div></li>
</ol>
<ol>
<li value="7">Select <strong class="bold">Firewall</strong> and then <strong class="bold">CREATE FIREWALL RULE</strong>.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer099">
<img alt="" height="417" src="image/B18333_11_16.jpg" width="928"/>
</div>
</div>
<ol>
<li value="8">Fill in the firewall rule details:<ul><li><strong class="bold">Name</strong>: <strong class="source-inline">vpc1-firewall-rule2</strong></li>
<li><strong class="bold">Network</strong>: <strong class="source-inline">vpc1</strong></li>
</ul></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer100">
<img alt="" height="494" src="image/B18333_11_17.jpg" width="889"/>
</div>
</div>
<ol>
<li value="9">Select <strong class="bold">All instances in the network</strong> for <strong class="bold">Targets</strong>, enter <strong class="source-inline">0.0.0.0/0</strong> for <strong class="bold">Source IPv4 ranges</strong> (allow all the sources), and<a id="_idIndexMarker596"/> select <strong class="bold">tcp :</strong> and enter <strong class="source-inline">22</strong> as the port <a id="_idIndexMarker597"/>number (SSH uses port <strong class="source-inline">22</strong>). Then, click <strong class="bold">CREATE</strong>.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer101">
<img alt="" height="687" src="image/B18333_11_18.jpg" width="514"/>
</div>
</div>
<ol>
<li value="10">After the firewall rule is created<a id="_idIndexMarker598"/> successfully, go back<a id="_idIndexMarker599"/> to the <strong class="bold">VM instances</strong> page. Select <strong class="bold">Open in browser window</strong> from the <strong class="bold">SSH</strong> drop-down menu (make sure you allow pop-up windows from the browser).<div class="IMG---Figure" id="_idContainer102"><img alt="" height="470" src="image/B18333_11_19.jpg" width="901"/></div></li>
</ol>
<p>Now you are able to SSH into the VM instance.</p>
<div>
<div class="IMG---Figure" id="_idContainer103">
<img alt="" height="670" src="image/B18333_11_20.jpg" width="857"/>
</div>
</div>
<p>You<a id="_idIndexMarker600"/> now have a GCP<a id="_idIndexMarker601"/> VM, called <strong class="source-inline">vm1</strong>, created in <strong class="source-inline">subnet1</strong> of <strong class="source-inline">vpc1</strong>.</p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor189"/>Creating a private VM, vm2, within vpc1/subnet2 using the GCP console</h2>
<ol>
<li value="1">Repeat the steps<a id="_idIndexMarker602"/> in <em class="italic">the previous section</em> to create<a id="_idIndexMarker603"/> a VM in <strong class="source-inline">vpc1/subnet2</strong>. The only changes are as follows:<ul><li>Choose <strong class="source-inline">asia-east1</strong> as the region where <strong class="source-inline">subnet2</strong> sits.</li>
<li>Choose <strong class="source-inline">subnet2</strong> as the subnetwork.</li>
<li>Choose <strong class="bold">None</strong> for <strong class="bold">External IPv4 address</strong> since this is a private VM and no external IP address is assigned.</li>
</ul></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer104">
<img alt="" height="614" src="image/B18333_11_21.jpg" width="564"/>
</div>
</div>
<p>Now, <strong class="source-inline">vm2</strong> is provisioned in <strong class="source-inline">vpc1/subnet2</strong> with<a id="_idIndexMarker604"/> the IP address<a id="_idIndexMarker605"/> of <strong class="source-inline">10.10.2.2</strong>.</p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<img alt="" height="430" src="image/B18333_11_22.jpg" width="917"/>
</div>
</div>
<ol>
<li value="2">Repeat the steps in <em class="italic">Creating a public virtual machine vm1 within the vpc1/subnet1 using GCP console</em> to create a firewall rule to allow <strong class="source-inline">ping</strong> within <strong class="source-inline">vpc1</strong> (<strong class="source-inline">10.10.0.0/16</strong>) so <strong class="source-inline">vm1</strong> and <strong class="source-inline">vm2</strong> can ping each other since they are in the same VPC.<div class="IMG---Figure" id="_idContainer106"><img alt="" height="720" src="image/B18333_11_23.jpg" width="676"/></div></li>
</ol>
<ol>
<li value="3">Ping <strong class="source-inline">vm2</strong> (<strong class="source-inline">10.10.1.2</strong>) from <strong class="source-inline">vm1</strong>.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer107">
<img alt="" height="206" src="image/B18333_11_24.jpg" width="723"/>
</div>
</div>
<p>At this time, you<a id="_idIndexMarker606"/> have a GCP VM, <strong class="source-inline">vm2</strong>, created<a id="_idIndexMarker607"/> in <strong class="source-inline">subnet2</strong> of <strong class="source-inline">vpc1</strong>, and <strong class="source-inline">vm1</strong> can ping <strong class="source-inline">vm2</strong>.</p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor190"/>Creating a private VM, vm8, within vpc2/subnet8 using the GCP console</h2>
<p>Repeat the steps<a id="_idIndexMarker608"/> in <em class="italic">Creating a private virtual machine vm2 within the vpc1/subnet2 using GCP console</em> to create<a id="_idIndexMarker609"/> a VM in <strong class="source-inline">vpc2/subnet8</strong> (<strong class="source-inline">192.168.1.0/24</strong>), with no public IP addresses. The only changes are as follows:</p>
<ul>
<li>Choose <strong class="source-inline">europe-central2</strong> as the region where <strong class="source-inline">subnet3</strong> sits.</li>
<li>Choose <strong class="source-inline">subnet8</strong> as the subnetwork.</li>
</ul>
<p>Notice that <strong class="source-inline">vm1/vm2</strong> cannot ping <strong class="source-inline">vm8</strong> even if you create firewall rules allowing pinging from <strong class="source-inline">vpc1</strong> to <strong class="source-inline">vpc2</strong>, since there are no routes between <strong class="source-inline">vpc1</strong> and <strong class="source-inline">vpc2</strong>. That’s why we need to create peering between <strong class="source-inline">vpc1</strong> and <strong class="source-inline">vpc2</strong>.</p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor191"/>Creating peering between vpc1 and vpc2 using the GCP console</h2>
<p>Use the following<a id="_idIndexMarker610"/> steps to create VPC network peering between <strong class="source-inline">vpc1</strong> and <strong class="source-inline">vpc2</strong>:</p>
<ol>
<li value="1">From the navigation menu, go to <strong class="bold">VPC network</strong> and then <strong class="bold">VPC network peering</strong>.<div class="IMG---Figure" id="_idContainer108"><img alt="" height="691" src="image/B18333_11_25.jpg" width="466"/></div></li>
</ol>
<ol>
<li value="2">Create <strong class="source-inline">vpc12-peering</strong> from <strong class="source-inline">vpc1</strong> to <strong class="source-inline">vpc2</strong>.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer109">
<img alt="" height="820" src="image/B18333_11_26.jpg" width="865"/>
</div>
</div>
<ol>
<li value="3">Do the same<a id="_idIndexMarker611"/> for <strong class="source-inline">vpc2</strong> to <strong class="source-inline">vpc1</strong> peering, so both peerings will now be active.<div class="IMG---Figure" id="_idContainer110"><img alt="" height="193" src="image/B18333_11_27.jpg" width="823"/></div></li>
</ol>
<ol>
<li value="4">Ping <strong class="source-inline">vm8</strong> (<strong class="source-inline">192.168.1.2</strong>) from <strong class="source-inline">vm1</strong>.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer111">
<img alt="" height="118" src="image/B18333_11_28.jpg" width="645"/>
</div>
</div>
<p>You now have<a id="_idIndexMarker612"/> a GCP VM, <strong class="source-inline">vm8</strong>, created in <strong class="source-inline">subnet8</strong> of <strong class="source-inline">vpc2</strong>, and <strong class="source-inline">vm1</strong> can ping <strong class="source-inline">vm8</strong>.</p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor192"/>Creating a GCS bucket from the GCP console</h2>
<p>Use the<a id="_idIndexMarker613"/> following steps to create<a id="_idIndexMarker614"/> a GCS bucket:</p>
<ol>
<li value="1">From the navigation menu, go to <strong class="bold">Cloud Storage</strong> and then <strong class="bold">Buckets</strong>. </li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer112">
<img alt="" height="598" src="image/B18333_11_29.jpg" width="1214"/>
</div>
</div>
<ol>
<li value="2">From the new window, click <strong class="bold">CREATE BUCKET</strong>.</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer113">
<img alt="" height="533" src="image/B18333_11_30.jpg" width="944"/>
</div>
</div>
<ol>
<li value="3">Choose a globally<a id="_idIndexMarker615"/> unique name for the<a id="_idIndexMarker616"/> GCP bucket. Here, we use <strong class="source-inline">bucket-08282022</strong>. Select <strong class="bold">Region</strong> under <strong class="bold">Choose where to store your data</strong>, and select <strong class="bold">us-east1</strong> as the storage bucket region. Click the <strong class="bold">CREATE</strong> button.<div class="IMG---Figure" id="_idContainer114"><img alt="" height="718" src="image/B18333_11_31.jpg" width="866"/></div></li>
</ol>
<p>It will bring you to the bucket page, where you can create a subfolder, upload files, or upload a folder under the previously created bucket.</p>
<div>
<div class="IMG---Figure" id="_idContainer115">
<img alt="" height="430" src="image/B18333_11_32.jpg" width="1159"/>
</div>
</div>
<p>So far, we<a id="_idIndexMarker617"/> have provisioned GCP<a id="_idIndexMarker618"/> resources (VPCs/subnets, VPC peering, VMs, and storage) from the console. All of this provisioning can be done using Cloud Shell. In the next section, we will provide the Cloud Shell commands/scripts for GCP resource provisioning.</p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor193"/>Provisioning GCP resources using Google Cloud Shell</h1>
<p>Instead of using the GCP console, we<a id="_idIndexMarker619"/> can use Google Cloud Shell <a id="_idIndexMarker620"/>to provision all the resources. In the following example, the GCP architecture is shown in <em class="italic">Figure 11.2</em>, and we use the Cloud Shell commands to provision GCP resources, including network VPCs/subnets, VMs, and VPC peering. Please practice using them in Cloud Shell, and make sure you understand each step.</p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<img alt="" height="617" src="image/B18333_11_33.jpg" width="1232"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – A sample architecture for GCP Cloud Shell practice</p>
<ol>
<li value="1">Create<a id="_idIndexMarker621"/> a project,VPC, and<a id="_idIndexMarker622"/> subnet:<p class="source-code">gcloud projects create test10122021 --folder 464105225938</p><p class="source-code">gcloud compute networks create vpc1 --project corvel-032021  --subnet-mode=custom</p><p class="source-code">gcloud compute networks subnets create subnet11 --network=vpc1 --range=10.10.1.0/24 --project corvel-032021 --region us-west1</p><p class="source-code">gcloud compute networks subnets create subnet12 --network=vpc1 --range=10.10.2.0/24 --project corvel-032021 --region us-east1</p><p class="source-code">gcloud compute networks create vpc2 --project corvel-032021  --subnet-mode=custom</p><p class="source-code">gcloud compute networks subnets create subnet2 --network=vpc2 --range=192.168.1.0/24 --project corvel-032021 --region us-central1</p><p class="source-code">gcloud compute networks create vpc3 --project test10122021  --subnet-mode=custom</p><p class="source-code">gcloud compute networks subnets create subnet3 --network=vpc3 --range=172.16.1.0/24 --project test10122021 --region us-central1</p></li>
<li>Create VMs:<p class="source-code">gcloud compute instances create myvm11 --project corvel-032021 --machine-type=f1-micro --zone=us-west1-a  --subnet=subnet11</p><p class="source-code">gcloud compute instances create myvm12 --project corvel-032021 --machine-type=f1-micro --network-interface=subnet=subnet12,no-address  --zone=us-east1-b</p><p class="source-code">gcloud compute instances create myvm2 --project corvel-032021 --machine-type=f1-micro --network-interface=subnet=subnet2,no-address  --zone=us-central1-b</p><p class="source-code">gcloud compute instances create myvm3 --project test10122021 --machine-type=f1-micro --network-interface=subnet=subnet3,no-address --zone=us-central1-b</p></li>
<li>List all the VMs<a id="_idIndexMarker623"/> and write down their IP<a id="_idIndexMarker624"/> addresses:<p class="source-code">gcloud compute instances list --project corvel-032021    </p><p class="source-code">gcloud compute instances list --project test10122021</p></li>
<li>Open the firewall for <strong class="source-inline">VPC1</strong>:<p class="source-code">gcloud compute firewall-rules create fw1 --network vpc1 --allow tcp:22,icmp --source-ranges 0.0.0.0/0 --project corvel-032021</p></li>
<li>SSH from the console to <strong class="source-inline">myvm11</strong>, and you should be able to ping <strong class="source-inline">vm12</strong> from <strong class="source-inline">vm11</strong>.</li>
<li>But how can we ping<a id="_idIndexMarker625"/> from <strong class="source-inline">myvm11</strong> to <strong class="source-inline">myvm2</strong>? You need<a id="_idIndexMarker626"/> to create VPC peering between <strong class="source-inline">VPC1</strong> and <strong class="source-inline">VPC2</strong> (they are in the same project):<p class="source-code">gcloud compute networks peerings create peer12 --project=corvel-032021  --network=vpc1 --peer-project=corvel-032021 --peer-network=vpc2</p><p class="source-code">gcloud compute networks peerings create peer21 --peer-project=corvel-032021  --network=vpc2 --project=corvel-032021 --peer-network=vpc1</p><p class="source-code">gcloud compute networks peerings list --project=corvel-032021</p></li>
<li>Open a firewall for <strong class="source-inline">vpc2</strong>:<p class="source-code">gcloud compute firewall-rules create fw2 --network vpc2 --allow tcp:22,icmp --source-ranges 0.0.0.0/0 --project corvel-032021</p></li>
<li>Now you shall be able to ping <strong class="source-inline">vm2</strong> from <strong class="source-inline">vm11</strong>. But how can we ping from <strong class="source-inline">myvm11</strong> to <strong class="source-inline">myvm3</strong>? You need to create VPC peering between <strong class="source-inline">vpc1</strong> and <strong class="source-inline">vpc3</strong> (they are in different projects):<p class="source-code">gcloud compute networks peerings create peer13 --project=corvel-032021  --network=vpc1 --peer-project=test10122021 --peer-network=vpc3</p><p class="source-code">gcloud compute networks peerings create peer31 --project=test10122021 --network=vpc3 --peer-project=corvel-032021 --peer-network=vpc1</p><p class="source-code">gcloud compute networks peerings list --project=corvel-032021</p></li>
<li>Open a firewall for <strong class="source-inline">vpc3</strong>:<p class="source-code">gcloud compute firewall-rules create fw3 --network vpc3 --allow tcp:22,icmp --source-ranges 10.10.1.0/24 --project test10122021</p></li>
</ol>
<p>Now you<a id="_idIndexMarker627"/> shall be able to<a id="_idIndexMarker628"/> ping <strong class="source-inline">vm3</strong> from <strong class="source-inline">vm11</strong>.</p>
<h1 id="_idParaDest-193"><a id="_idTextAnchor194"/>Summary</h1>
<p>In this appendix, we have provided practice examples to provision GCP services/resources from the GCP console. We have also shown how to create these basic resources using Google Cloud Shell.</p>
</div>
</div>


<div id="sbo-rt-content"><div id="_idContainer132">
<h1 class="chapter-number" id="_idParaDest-194"><a id="_idTextAnchor195"/>Appendix 2 </h1>
<h1 id="_idParaDest-195"><a id="_idTextAnchor196"/>Practicing Using the Python Data Libraries</h1>
<p>In <a href="B18333_02.xhtml#_idTextAnchor054"><em class="italic">Chapter 2</em></a>, <em class="italic">Mastering Python Programming</em>, we covered the Python data libraries, including NumPy, Pandas, Matpotlib, and Seaborn. In this appendix, we will continue learning these libraries by practicing using them on the Google Colab platform (<a href="http://colab.research.google.com">colab.research.google.com</a>). </p>
<p>With a step-by-step approach, we will show how to use these libraries to manage and visualize data. For the NumPy library, we will discuss how to generate and operate NumPy arrays. For the Pandas library, we cover features including Series, DataFrames, missing data handling, GroupBy, and operations. For the Matpotlib and Seaborn libraries, we will show their features by exploring multiple data visualization examples.</p>
<p>Follow these examples and make sure you understand each of them. Practicing each example on Google Colab will yield the best results.</p>
<h1 id="_idParaDest-196"><a id="_idTextAnchor197"/>NumPy</h1>
<p>NumPy is a <a id="_idIndexMarker629"/>library for Python that adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.</p>
<p>In this section, we will go over the following topics:</p>
<ul>
<li>Generating NumPy arrays  </li>
<li>Operating NumPy arrays</li>
</ul>
<p>We will start with how to generate NumPy arrays.</p>
<h2 id="_idParaDest-197"><a id="_idTextAnchor198"/>Generating NumPy arrays</h2>
<p>In this <a id="_idIndexMarker630"/>section, we will demonstrate various ways to create NumPy arrays. Arrays might be one-dimensional or two-dimensional. </p>
<p>Let’s convert a list into a one-dimensional array by using the following code (the first line imports the NumPy library and and gives it the alias of <strong class="source-inline">np</strong>): </p>
<pre class="source-code">import numpy as np
my_list = [1,2,3]
my_list
<strong class="bold">[1, 2, 3]</strong>
import numpy as np
my_list = [1,2,3]
arr = np.array(my_list)
arr
<strong class="bold">array([1, 2, 3])</strong></pre>
<p>Now, let’s <a id="_idIndexMarker631"/>make our list a little complicated with the following code:</p>
<pre class="source-code">import numpy as np
my_mat =[[10,20,30],[40,50,60],[70,80,90]]
np.array(my_mat)
<strong class="bold">array([[10, 20, 30],</strong>
<strong class="bold">       [40, 50, 60],</strong>
<strong class="bold">       [70, 80, 90]])</strong></pre>
<p>Note that there are two sets of brackets that represent the two-dimensional array. </p>
<p>One of the basic functions in NumPy is <strong class="source-inline">arange()</strong>, where you can provide start and stop values. For example, with <strong class="source-inline">0</strong> as the start value and <strong class="source-inline">10</strong> as the stop value, <strong class="source-inline">np.arange()</strong> will generate a one-dimensional array with values from <strong class="source-inline">0</strong> to <strong class="source-inline">10</strong> (the last value provided in the function is not included):</p>
<pre class="source-code">import numpy as np
np.arange(0,10)
<strong class="bold">array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</strong></pre>
<p>Let’s use the same function with some other values/arguments. In the following example, we add an <a id="_idIndexMarker632"/>additional argument called <strong class="bold">step size</strong> to create a one-dimensional array, with even numbers: </p>
<pre class="source-code">import numpy as np
np.arange(0,11,2)
<strong class="bold">array([ 0, 2, 4, 6, 8, 10])</strong></pre>
<p>The first argument in the <strong class="source-inline">arange</strong> function is the <strong class="source-inline">start</strong> value, the second is the <strong class="source-inline">stop</strong> value, and the last one is the step size. </p>
<p>The other <a id="_idIndexMarker633"/>built-in function in NumPy is to generate an array with all zeros. We need to provide the argument with how many zeros we want to generate in the array, as shown in the following snippet: </p>
<pre class="source-code">import numpy as np
np.zeros(5)
<strong class="bold">array([0., 0., 0., 0., 0.])</strong></pre>
<p>You can also provide a tuple as an argument:</p>
<pre class="source-code">import numpy as np
np.zeros((4,4))
<strong class="bold">array([[0., 0., 0., 0.],</strong>
<strong class="bold">       [0., 0., 0., 0.],</strong>
<strong class="bold">       [0., 0., 0., 0.],</strong>
<strong class="bold">       [0., 0., 0., 0.]])</strong></pre>
<p>Similarly, if we need to generate an array with pure ones, we can use the <strong class="source-inline">ones</strong> function and provide a number as an argument, as shown here: </p>
<pre class="source-code">import numpy as np
np.ones((3,4))
<strong class="bold">array([[1., 1., 1., 1.],</strong>
<strong class="bold">       [1., 1., 1., 1.],</strong>
<strong class="bold">       [1., 1., 1., 1.]])</strong></pre>
<p>Another useful built-in function is <strong class="source-inline">linspace</strong>, where we enter arguments as the first number and the last number, evenly spaced between specified intervals. Remember that the <strong class="source-inline">arange</strong> function returns all integers between the start and stop points, but <strong class="source-inline">linspace</strong> takes a third argument, the number of points we want: </p>
<pre class="source-code">import numpy as np
np.linspace(0,5,10)
<strong class="bold">array([0.        , 0.55555556, 1.11111111, 1.66666667, 2.22222222,</strong>
<strong class="bold">       2.77777778, 3.33333333, 3.88888889, 4.44444444, 5.        ])</strong></pre>
<p>In the <a id="_idIndexMarker634"/>previous example, there is a one-dimensional array (specified with a single bracket), with 10 evenly spaced points between <strong class="source-inline">0</strong> and <strong class="source-inline">5</strong>:</p>
<pre class="source-code">import numpy as np
np.linspace(0,5,25)
<strong class="bold">array([0.        , 0.20833333, 0.41666667, 0.625     , 0.83333333,</strong>
<strong class="bold">       1.04166667, 1.25      , 1.45833333, 1.66666667, 1.875     ,</strong>
<strong class="bold">       2.08333333, 2.29166667, 2.5       , 2.70833333, 2.91666667,</strong>
<strong class="bold">       3.125     , 3.33333333, 3.54166667, 3.75      , 3.95833333,</strong>
<strong class="bold">       4.16666667, 4.375     , 4.58333333, 4.79166667, 5.</strong></pre>
<p>Note that, using the same example with different space points (<strong class="source-inline">25</strong>), the array looks like a two-dimensional array, but it is one-dimensional, proven by there only being one bracket in front of the array.</p>
<p>If you are dealing with linear algebra problems, it is basically a two-dimensional square matrix (the same number of rows and columns), where we have a diagonal of ones and everything else is zero. That’s why it takes a single digit as an argument: </p>
<pre class="source-code">import numpy as np
np.eye(5)
<strong class="bold">array([[1., 0., 0., 0., 0.],</strong>
<strong class="bold">       [0., 1., 0., 0., 0.],</strong>
<strong class="bold">       [0., 0., 1., 0., 0.],</strong>
<strong class="bold">       [0., 0., 0., 1., 0.],</strong>
<strong class="bold">       [0., 0., 0., 0., 1.]])</strong></pre>
<p>One of <a id="_idIndexMarker635"/>the most used functions that will be used here generates an array with random numbers, such as the following:</p>
<pre class="source-code">import numpy as np
np.random.rand(4,5)
<strong class="bold">array([[0.44698983, 0.46938684, 0.66609426, 0.95168835, 0.48775195],</strong>
<strong class="bold">       [0.17627195, 0.98549358, 0.69526343, 0.44981183, 0.11574242],</strong>
<strong class="bold">       [0.09377203, 0.35856856, 0.38228733, 0.6129268 , 0.16385609],</strong>
<strong class="bold">       [0.79041234, 0.9281485 , 0.72570369, 0.46438003, 0.3159711 ]])</strong></pre>
<p>If we need to return from the standard normal distribution, instead of <strong class="source-inline">rand</strong>, we can use the <strong class="source-inline">randn</strong> function: </p>
<pre class="source-code">import numpy as np
np.random.randn(4)
<strong class="bold">array([ 1.57461921, -1.47658163, 0.38070033, -1.43224982])</strong></pre>
<p>The other function is <strong class="source-inline">randint</strong>, which returns a random single integer between the lowest and highest integer values, provided as an argument (the lowest inclusive and the highest exclusive): </p>
<pre class="source-code">import numpy as np
np.random.randint(1,100)
<strong class="bold">29</strong></pre>
<p>If we need <a id="_idIndexMarker636"/>a particular number of random integers between the provided interval, we need to provide a third argument: </p>
<pre class="source-code">import numpy as np
np.random.randint(1,100,10)
<strong class="bold">array([29, 7, 33, 85, 83, 34, 5, 50, 53, 39])</strong></pre>
<p>Now, let’s do some conversion. In the following example, we have generated a one-dimensional array with 25 values, saved it in an <strong class="source-inline">array</strong> variable, and then reshaped it into a two-dimensional array:</p>
<pre class="source-code">import numpy as np
array = np.arange(25)
array
<strong class="bold">array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])</strong>
array.reshape(5,5)
<strong class="bold">array([[ 0,  1,  2,  3,  4],</strong>
<strong class="bold">       [ 5,  6,  7,  8,  9],</strong>
<strong class="bold">       [10, 11, 12, 13, 14],</strong>
<strong class="bold">       [15, 16, 17, 18, 19],</strong>
<strong class="bold">       [20, 21, 22, 23, 24]])</strong></pre>
<p>If we want to get the maximum or minimum value in the randomly generated array, we can use the <strong class="source-inline">array.max()</strong> or <strong class="source-inline">array.min()</strong> function respectively: </p>
<pre class="source-code">import numpy as np
array = np.random.randint(0,50,10)
array
<strong class="bold">array([ 2, 8, 31, 2, 25, 34, 49, 8, 49, 42])</strong>
array.max()
<strong class="bold">49</strong>
array.min()
<strong class="bold">2</strong></pre>
<p>If we want <a id="_idIndexMarker637"/>to pick a single value (or a set of values) from the array provided in the preceding example, we can specify this with brackets, as shown here: </p>
<pre class="source-code">import numpy as np
array = np.random.randint(0,50,10)
array
<strong class="bold">array([42, 38, 43, 22, 39, 4, 20, 30, 49, 13])</strong>
array[3]
<strong class="bold">22</strong>
array[0:4]
<strong class="bold">array([42, 38, 43, 22])</strong></pre>
<p>If we want to replace a single value or a set of values in an array, we need to set those values as shown here: </p>
<pre class="source-code">import numpy as np
array = np.random.randint(0,50,100)
array
<strong class="bold">array([42, 10, 14, 34, 45, 18, 21, 11, 33, 32, 22, 13, 11, 42, 16, 20, 10,</strong>
<strong class="bold">        1, 36, 41, 45, 21, 45, 45, 41,  0, 38, 39, 16, 10, 18, 45, 43, 42,</strong>
<strong class="bold">       23, 31, 20, 14,  9, 46, 44, 33, 24, 35,  6,  6, 26, 13, 20, 20, 28,</strong>
<strong class="bold">       23, 46, 40, 15, 43, 17, 31, 15, 48,  9, 17, 46, 28, 48, 41, 30, 28,</strong>
<strong class="bold">       32, 40, 35,  8, 10,  5, 33, 30,  4, 38, 47, 22, 13, 14, 29,  1, 15,</strong>
<strong class="bold">       48, 18, 48, 18, 21, 45,  9,  6,  1, 31, 28,  5, 42,  8, 28])</strong>
array[0:3] = 100
array
<strong class="bold">array([100, 100, 100,  34,  45,  18,  21,  11,  33,  32,  22,  13,  11,</strong>
<strong class="bold">        42,  16,  20,  10,   1,  36,  41,  45,  21,  45,  45,  41,   0,</strong>
<strong class="bold">        38,  39,  16,  10,  18,  45,  43,  42,  23,  31,  20,  14,   9,</strong>
<strong class="bold">        46,  44,  33,  24,  35,   6,   6,  26,  13,  20,  20,  28,  23,</strong>
<strong class="bold">        46,  40,  15,  43,  17,  31,  15,  48,   9,  17,  46,  28,  48,</strong>
<strong class="bold">        41,  30,  28,  32,  40,  35,   8,  10,   5,  33,  30,   4,  38,</strong>
<strong class="bold">        47,  22,  13,  14,  29,   1,  15,  48,  18,  48,  18,  21,  45,</strong>
<strong class="bold">         9,   6,   1,  31,  28,   5,  42,   8,  28])</strong></pre>
<p>As you <a id="_idIndexMarker638"/>can see, values that are in the <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, and <strong class="source-inline">2</strong> indexes are replaced with values of <strong class="source-inline">100</strong>.</p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor199"/>Operating NumPy arrays</h2>
<p>Now, we are <a id="_idIndexMarker639"/>going a little deeper and working with a two-dimensional array. In the following example, we generated an array with 25 random numbers and reshaped it into a two-dimensional array. It then shows a value, which is located on row 1 and column 1:</p>
<pre class="source-code">import numpy as np
array = np.random.randint(0,50,25).reshape(5,5)
array
<strong class="bold">array([[15, 21, 34, 39, 18],</strong>
<strong class="bold">       [42, 41, 28, 24,  2],</strong>
<strong class="bold">       [43, 25, 38, 42, 35],</strong>
<strong class="bold">       [ 3,  4, 27,  2, 49],</strong>
<strong class="bold">       [17,  5, 33, 11, 30]])</strong>
array[0:3]
<strong class="bold">array([[15, 21, 34, 39, 18],</strong>
<strong class="bold">       [42, 41, 28, 24,  2],</strong>
<strong class="bold">       [43, 25, 38, 42, 35]])</strong></pre>
<p>The following is a function to return a Boolean (<strong class="source-inline">True</strong> or <strong class="source-inline">False</strong>) of the array, based on a specific condition:</p>
<pre class="source-code">import numpy as np
array = np.random.randint(0,50,25).reshape(5,5)
array
array([[47, 25,  1, 33,  7],
       [31, 18,  9, 13, 41],
       [28, 33, 34, 19,  2],
       [ 1, 32, 45, 34, 48],
       [27, 34, 38, 18,  9]])
array &gt; 25
<strong class="bold">array([[ True, False, False,  True, False],</strong>
<strong class="bold">       [ True, False, False, False,  True],</strong>
<strong class="bold">       [ True,  True,  True, False, False],</strong>
<strong class="bold">       [False,  True,  True,  True,  True],</strong>
<strong class="bold">       [ True,  True,  True, False, False]])</strong></pre>
<p>We can <a id="_idIndexMarker640"/>also apply some math, such as addition, subtraction, multiplication, and division operations, to an array, as well as applying some functions, such as <strong class="source-inline">sin</strong>, <strong class="source-inline">cos</strong>, and <strong class="source-inline">log</strong>: </p>
<pre class="source-code">import numpy as np
array = np.arange(10)
array
<strong class="bold">array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</strong>
array + array
<strong class="bold">array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18])</strong>
array * 2
<strong class="bold">array([ 0, 2, 4, 6, 8, 10, 12, 14, 16, 18])</strong>
np.sin(array)
<strong class="bold">array([ 0.        ,  0.84147098,  0.90929743,  0.14112001, -0.7568025 ,</strong>
<strong class="bold">       -0.95892427, -0.2794155 ,  0.6569866 ,  0.98935825,  0.41211849])</strong></pre>
<p>We have <a id="_idIndexMarker641"/>shown some examples using the NumPy library; for more information about the library, please refer to <a href="https://numpy.org/">https://numpy.org/</a>.</p>
<h1 id="_idParaDest-199"><a id="_idTextAnchor200"/>Pandas</h1>
<p>Pandas is an <a id="_idIndexMarker642"/>open source library that is built on top of NumPy. Pandas allows for quick data analysis and data preparation. It excels in performance and productivity.  </p>
<p>In this section, we will discuss the following topics:</p>
<ul>
<li>Series </li>
<li>DataFrames</li>
<li>Missing data handling</li>
<li>GroupBy</li>
<li>Operations </li>
</ul>
<p>Depending on the environment, you may need to install Pandas first by going to your command line or terminal and running the following commands:</p>
<p class="source-code">conda install pandas</p>
<p class="source-code">pip install pandas</p>
<p>We will start by looking at the Series data type.   </p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor201"/>Series </h2>
<p>Series is the <a id="_idIndexMarker643"/>first main data type that we will be using with Pandas. Series is <a id="_idIndexMarker644"/>almost the same as the NumPy array. The difference is that with Series, a series of axis labels can be indexed by a label. </p>
<p>We are going to make four different Python objects and form a list:</p>
<pre class="source-code">import numpy as np
import pandas as pd
labels = ['A', 'B','C']
my_data = [100,200,300]
array = np.array(my_data)
d = {'A':100, 'B':200, 'C':300}
pd.Series(data = my_data)
<strong class="bold">A    100</strong>
<strong class="bold">B    200</strong>
<strong class="bold">C    300</strong>
<strong class="bold">dtype: int64</strong></pre>
<p>By default, if we <a id="_idIndexMarker645"/>do not specify the index values, it will assign <strong class="source-inline">0, 1, 2, ….</strong>  Therefore, we can <a id="_idIndexMarker646"/>change those labels into labels we created earlier (<strong class="source-inline">labels</strong>): </p>
<pre class="source-code">pd.Series(my_data, labels)
<strong class="bold">A    100</strong>
<strong class="bold">B    200</strong>
<strong class="bold">C    300</strong>
<strong class="bold">dtype: int64</strong></pre>
<p>If you want to provide a dictionary in a Series, you do not need to provide an index, since the dictionary already has its keys and values: </p>
<pre class="source-code">pd.Series(d)
<strong class="bold">A    100</strong>
<strong class="bold">B    200</strong>
<strong class="bold">C    300</strong>
<strong class="bold">dtype: int64</strong></pre>
<p>A Series can hold any data types, and we can provide labels too:</p>
<pre class="source-code">pd.Series(labels, my_data)
<strong class="bold">100    A</strong>
<strong class="bold">200    B</strong>
<strong class="bold">300    C</strong>
<strong class="bold">dtype: object</strong>
Next we will examine Dataframes.</pre>
<h2 id="_idParaDest-201"><a id="_idTextAnchor202"/>DataFrames</h2>
<p>A DataFrame is a <a id="_idIndexMarker647"/>two-dimensional labeled data structure with columns <a id="_idIndexMarker648"/>of different types. In this section, we will start building our first DataFrame using Pandas. In the following example, we have created a random number generated by NumPy and built a nice view with labels (rows and columns) with Pandas: </p>
<pre class="source-code">import numpy as np
import pandas as pd 
from numpy.random import randn
np.random.seed(101)
df = pd.DataFrame(randn(5,4),['a','b','c','d','e'],['x','y','z','t'])
df
<strong class="bold">          x         y          z         t</strong>
<strong class="bold">a     2.706850  0.628133    0.907969  0.503826</strong>
<strong class="bold">b     0.651118  -0.319318  -0.848077  0.605965</strong>
<strong class="bold">c     -2.018168  0.740122    0.528813 -0.589001</strong>
<strong class="bold">d     0.188695  -0.758872  -0.933237  0.955057</strong>
<strong class="bold">e     0.190794  1.978757    2.605967  0.683509</strong></pre>
<p>If you want to display a specific row of a DataFrame, specify the name of the row in the brackets: </p>
<pre class="source-code">df['x']
<strong class="bold">a    2.706850</strong>
<strong class="bold">b    0.651118</strong>
<strong class="bold">c   -2.018168</strong>
<strong class="bold">d    0.188695</strong>
<strong class="bold">e    0.190794</strong>
<strong class="bold">Name: x, dtype: float64</strong></pre>
<p>Note that <a id="_idIndexMarker649"/>the output is a Series, which we have covered before. If you <a id="_idIndexMarker650"/>want to check the type of the output, use the <strong class="source-inline">type</strong> syntax, as shown here: </p>
<pre class="source-code">type(df['x']) 
<strong class="bold">pandas.core.series.Series</strong></pre>
<p>If you want to display multiple columns from the DataFrame, you can specify them in brackets as a list: </p>
<pre class="source-code">df[['x','y']]
<strong class="bold">          x         y</strong>
<strong class="bold">a     2.706850    0.628133</strong>
<strong class="bold">b     0.651118   -0.319318</strong>
<strong class="bold">c    -2.018168    0.740122</strong>
<strong class="bold">d    0.188695    -0.758872</strong>
<strong class="bold">e    0.190794     1.978757</strong></pre>
<p>Another feature of the DataFrame is being able to add new columns to it. When adding a new column, we need to specify the values of the new column. In the following example, we are creating a new column, <strong class="source-inline">new</strong>, and we will sum all the values from <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> and add them to it:</p>
<pre class="source-code">df['new'] = df['x'] + df['y']
df
<strong class="bold">        x            y         z        t         new</strong>
<strong class="bold">a    2.706850    0.628133  0.907969   0.503826   3.334983</strong>
<strong class="bold">b    0.651118   -0.319318  -0.848077  0.605965   0.331800</strong>
<strong class="bold">c    -2.018168   0.740122  0.528813  -0.589001   -1.278046</strong>
<strong class="bold">d     0.188695  -0.758872  -0.933237  0.955057   -0.570177</strong>
<strong class="bold">e   0.190794    1.978757    2.605967   0.683509  2.169552</strong></pre>
<p>If you <a id="_idIndexMarker651"/>want to delete a particular column or row, you can use the <a id="_idIndexMarker652"/>built-in <strong class="source-inline">drop</strong> function:</p>
<pre class="source-code">df.drop('new', axis=1)
<strong class="bold">         x           y          z        t</strong>
<strong class="bold">a     2.706850   0.628133   0.907969   0.503826</strong>
<strong class="bold">b     0.651118   -0.319318  -0.848077  0.605965</strong>
<strong class="bold">c     -2.018168  0.740122   0.528813   -0.589001</strong>
<strong class="bold">d     0.188695   -0.758872  -0.933237  0.955057</strong>
<strong class="bold">e     0.190794    1.978757   2.605967  0.683509</strong></pre>
<p>If you need to get output from a specific row, you can use the <strong class="source-inline">loc</strong> syntax, which stands for <strong class="source-inline">location</strong>, and you need to specify a row name as an argument: </p>
<pre class="source-code">df
         <strong class="bold">x         y           z          t        new</strong>
<strong class="bold">a    2.706850   0.628133   0.907969   0.503826   3.334983</strong>
<strong class="bold">b    0.651118   -0.319318  -0.848077  0.605965   0.331800</strong>
<strong class="bold">c    -2.018168  0.740122   0.528813  -0.589001   -1.278046</strong>
<strong class="bold">d    0.188695   -0.758872  -0.933237  0.955057   -0.570177</strong>
<strong class="bold">e    0.190794   1.978757   2.605967   0.683509   2.169552</strong>
df.loc['a']
<strong class="bold">x      2.706850</strong>
<strong class="bold">y      0.628133</strong>
<strong class="bold">z      0.907969</strong>
<strong class="bold">t      0.503826</strong>
<strong class="bold">new    3.334983</strong>
<strong class="bold">Name: a, dtype: float64</strong></pre>
<p>Conditional <a id="_idIndexMarker653"/>selection is also a feature of Pandas, where you can call <a id="_idIndexMarker654"/>data as a Boolean (<strong class="source-inline">True</strong> or <strong class="source-inline">False</strong>) in a DataFrame: </p>
<pre class="source-code">df 
<strong class="bold">        x           y         z          t         new</strong>
<strong class="bold">a   2.706850   0.628133    0.907969   0.503826   3.334983</strong>
<strong class="bold">b   0.651118   -0.319318   -0.848077  0.605965   0.331800</strong>
<strong class="bold">c   -2.018168   0.740122   0.528813   0.589001   -1.278046</strong>
<strong class="bold">d   0.188695   -0.758872   -0.933237  0.955057   -0.570177</strong>
<strong class="bold">e   0.190794   1.978757    2.605967   0.683509   2.169552</strong>
df &gt; 0
       <strong class="bold">x      y     z      t     new</strong>
<strong class="bold">a    True   True   True   True   True</strong>
<strong class="bold">b    True   False  False  True   True</strong>
<strong class="bold">c    False  True   True   False  False</strong>
<strong class="bold">d    True   False  False  True   False</strong>
<strong class="bold">e    True   True   True   True   True </strong></pre>
<h2 id="_idParaDest-202"><a id="_idTextAnchor203"/>Missing data handling</h2>
<p>In the <a id="_idIndexMarker655"/>DataFrame we created earlier, if we apply some conditions (such as greater than zero), data that is less than zero will be displayed as <strong class="source-inline">NaN</strong> (null data). If you <a id="_idIndexMarker656"/>need to display only rows and columns that do not have <strong class="source-inline">null</strong> data, use the <strong class="source-inline">dropna</strong> syntax, as shown here: </p>
<pre class="source-code">df
<strong class="bold">         x         y          z          t        new</strong>
<strong class="bold">a    2.706850   0.628133   0.907969   0.503826   3.334983</strong>
<strong class="bold">b    0.651118   -0.319318  -0.848077  0.605965   0.331800</strong>
<strong class="bold">c    -2.018168   0.740122  0.528813  -0.589001   -1.278046</strong>
<strong class="bold">d    0.188695   -0.758872  -0.933237  0.955057   -0.570177</strong>
<strong class="bold">e    0.190794   1.978757   2.605967   0.683509    2.169552</strong>
df[df &gt; 0].dropna()
<strong class="bold">         x           y        z           t         new</strong>
<strong class="bold">a     2.706850   0.628133  0.907969   0.503826   3.334983</strong>
<strong class="bold">e     0.190794   1.978757  2.605967   0.683509   2.169552</strong></pre>
<p>Now, we <a id="_idIndexMarker657"/>are going to apply a condition and save a new DataFrame (<strong class="source-inline">new_df</strong>) with <a id="_idIndexMarker658"/>values that are greater than zero, as shown here: </p>
<pre class="source-code">new_df = df[df &gt; 0]
new_df
<strong class="bold">         x         y           z         t        new</strong>
<strong class="bold">a    2.706850   0.628133   0.907969   0.503826  3.334983</strong>
<strong class="bold">b    0.651118      NaN        NaN     0.605965  0.331800</strong>
<strong class="bold">c       NaN     0.740122   0.528813    NaN         NaN</strong>
<strong class="bold">d    0.188695      NaN       NaN      0.955057     NaN</strong>
<strong class="bold">e    0.190794   1.978757    2.605967  0.683509  2.169552</strong></pre>
<p>For the cells that do not have any value (<strong class="source-inline">NaN</strong>), we are going to replace them with the mean (average) of all values in that column: </p>
<pre class="source-code">new_df['x'].fillna(value=new_df['x'].mean())
<strong class="bold">a    2.706850</strong>
<strong class="bold">b    0.651118</strong>
<strong class="bold">c    0.934364</strong>
<strong class="bold">d    0.188695</strong>
<strong class="bold">e    0.190794</strong>
<strong class="bold">Name: x, dtype: float64</strong></pre>
<p>Note <a id="_idIndexMarker659"/>that the value in the <strong class="source-inline">x</strong> column and <strong class="source-inline">c</strong> row is not null and is <a id="_idIndexMarker660"/>replaced with a value. </p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor204"/>GroupBy</h2>
<p>GroupBy <a id="_idIndexMarker661"/>allows you to group together rows based on columns and perform <a id="_idIndexMarker662"/>an aggregate function on them.</p>
<p>In the next example, we will create a new DataFrame using the dictionary, as shown here:</p>
<pre class="source-code">import numpy as np
import pandas as pd
data = {'Country': ['USA', 'USA', 'France', 'France','Germany','Germany'],
        'Person': ['Sam','Amy','Carhile','Richard','John','Frank'],
        'Sales': [250, 300, 125, 500, 350, 200]}
df = pd.DataFrame(data)
df
<strong class="bold">    Country    Person     Sales</strong>
<strong class="bold">0    USA        Sam       250</strong>
<strong class="bold">1    USA        Amy       300</strong>
<strong class="bold">2    France     Carhile   125</strong>
<strong class="bold">3    France     Richard   500</strong>
<strong class="bold">4    Germany    John      350</strong>
<strong class="bold">5    Germany    Frank     200</strong></pre>
<p>If you <a id="_idIndexMarker663"/>are working with a large DataFrame and want to print the sum of <a id="_idIndexMarker664"/>the sales of each country, use the <strong class="source-inline">groupby</strong> built-in function, as shown here: </p>
<pre class="source-code">df.groupby('Country').sum()
<strong class="bold">Country</strong>    <strong class="bold">Sales</strong>
<strong class="bold">France</strong>     <strong class="bold">625</strong>
<strong class="bold">Germany</strong>    <strong class="bold">550</strong>
<strong class="bold">USA</strong>        <strong class="bold">550</strong></pre>
<h2 id="_idParaDest-204"><a id="_idTextAnchor205"/>Operations</h2>
<p>In this <a id="_idIndexMarker665"/>section, we will show some real-life data operations. In the next <a id="_idIndexMarker666"/>examples, we are going to use a CSV file, <strong class="source-inline">Salaries.csv</strong> (taken from <a href="https://www.kaggle.com/kaggle/sf-salaries?select=Salaries.csv">https://www.kaggle.com/kaggle/sf-salaries?select=Salaries.csv</a>).</p>
<p>After downloading the file to a local computer and uploading it to Google Colab, you can visualize the DataFrame and explore the data using the Pandas library.  </p>
<p>Use the <strong class="source-inline">read_csv</strong> function to read the CSV file, as shown here:</p>
<pre class="source-code">import pandas as pd
df = pd.read_csv('Salaries.csv')
df</pre>
<div>
<div class="IMG---Figure" id="_idContainer118">
<img alt="" height="433" src="image/B18333_12_1.jpg" width="1218"/>
</div>
</div>
<pre class="source-code">df.info()
<strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong>
<strong class="bold">RangeIndex: 148654 entries, 0 to 148653</strong>
<strong class="bold">Data columns (total 13 columns):</strong>
<strong class="bold">#   Column            Non-Null Count   Dtype  </strong>
<strong class="bold">---  ------            --------------   -----  </strong>
<strong class="bold">0   Id                148654 non-null  int64  </strong>
<strong class="bold">1   EmployeeName      148654 non-null  object </strong>
<strong class="bold">2   JobTitle          148654 non-null  object </strong>
<strong class="bold">3   BasePay           148049 non-null  object </strong>
<strong class="bold">4   OvertimePay       148654 non-null  object </strong>
<strong class="bold">5   OtherPay          148654 non-null  object </strong>
<strong class="bold">6   Benefits          112495 non-null  object </strong>
<strong class="bold">7   TotalPay          148654 non-null  float64</strong>
<strong class="bold">8   TotalPayBenefits  148654 non-null  float64</strong>
<strong class="bold">9   Year              148654 non-null  int64  </strong>
<strong class="bold">10  Notes             0 non-null       float64</strong>
<strong class="bold">11  Agency            148654 non-null  object </strong>
<strong class="bold">12  Status            38119 non-null   object </strong>
<strong class="bold">dtypes: float64(3), int64(2), object(8)</strong>
<strong class="bold">memory usage: 14.7+ MB</strong>
df["TotalPay"].mean()
<strong class="bold">74768.32197169267</strong>
df[df["EmployeeName"] == "Joseph Driscoll"]["JobTitle"]
<strong class="bold">36198    Captain, Fire Suppression</strong>
<strong class="bold">Name: JobTitle, dtype: object</strong></pre>
<p>In the <a id="_idIndexMarker667"/>preceding sections, we have shown the basic features of the <a id="_idIndexMarker668"/>Pandas library. For <a id="_idIndexMarker669"/>more information on Pandas features, please refer to <a href="https://pandas.pydata.org">https://pandas.pydata.org</a>.</p>
<h1 id="_idParaDest-205"><a id="_idTextAnchor206"/>Matplotlib</h1>
<p>Matplotlib is <a id="_idIndexMarker670"/>one of the essential libraries of data visualization in Python. It is an excellent two-dimensional and three-dimensional graphics library for generating scientific figures. </p>
<p>Some of the <a id="_idIndexMarker671"/>major pros of Matplotlib are as follows:</p>
<ul>
<li>Generally easy to get started with simple plots</li>
<li>Support for custom labels and text</li>
<li>Great control of every element in a figure </li>
<li>High-quality output in many formats</li>
<li>Very customizable in general</li>
</ul>
<p>We will start with simple data generated by the NumPy library: </p>
<pre class="source-code">import matplotlib.pyplot as plt
import numpy as np
a = np.linspace(1,10,15)
a
<strong class="bold">array([ 1.        ,  1.64285714,  2.28571429,  2.92857143,  3.57142857,</strong>
<strong class="bold">        4.21428571,  4.85714286,  5.5       ,  6.14285714,  6.78571429,</strong>
<strong class="bold">        7.42857143,  8.07142857,  8.71428571,  9.35714286, 10.</strong></pre>
<p>We have <a id="_idIndexMarker672"/>generated data using the <strong class="source-inline">linspace</strong> function in the NumPy library. We can also play with all our data (in our case, it is an array), such as taking each number in an array and squaring it: </p>
<pre class="source-code">b = a ** 2
b
<strong class="bold">array([  1.        ,   2.69897959,   5.2244898 ,   8.57653061,</strong>
<strong class="bold">        12.75510204,  17.76020408,  23.59183673,  30.25      ,</strong>
<strong class="bold">        37.73469388,  46.04591837,  55.18367347,  65.14795918,</strong>
<strong class="bold">        75.93877551,  87.55612245, 100.        ])</strong></pre>
<p>We can plot the data in a graph with a simple function in Matplotlib: </p>
<pre class="source-code">plt.plot(a,b)</pre>
<div>
<div class="IMG---Figure" id="_idContainer119">
<img alt="" height="278" src="image/B18333_12_2.jpg" width="389"/>
</div>
</div>
<p>You can <a id="_idIndexMarker673"/>rearrange the <strong class="source-inline">x</strong> and <strong class="source-inline">y</strong> axes with <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong> values, as shown here: </p>
<pre class="source-code">plt.subplot(1,2,1) 
plt.plot(a,b)
plt.title("Dito training 1")
plt.subplot(1,2,2)
plt.plot(b,a,"r")
plt.title("Dito training 2")</pre>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="" height="264" src="image/B18333_12_3.jpg" width="377"/>
</div>
</div>
<p>So far, we have demonstrated how to plot using the <strong class="source-inline">plot</strong> and <strong class="source-inline">subplot</strong> methods. Now, we are going to dive into object-oriented methods, where we will break down all for <a id="_idIndexMarker674"/>a more formal introduction of the <strong class="source-inline">matplotlib</strong> object-oriented API method. In the following example, we run a built-in <strong class="source-inline">figure</strong> function, which builds an imaginary blank canvas, and later on, we will add a set to this canvas so that it will work more flexibly:</p>
<pre class="source-code">import matplotlib.pyplot as plt
fig = plt.figure()
<strong class="bold">&lt;Figure size 432x288 with 0 Axes&gt;</strong>
import matplotlib.pyplot as plt
fig = plt.figure()
axes = fig.add_axes([0.1,0.1,0.8,0.8])</pre>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="" height="265" src="image/B18333_12_4.jpg" width="390"/>
</div>
</div>
<p>The very first step is to add axes to the canvas using the <strong class="source-inline">add_axes</strong> function. The numbers in the brackets represent the left, the button, the width, and the height of the axes.</p>
<p>Each number <a id="_idIndexMarker675"/>should be between zero and one, representing a percentage, numbers representation: </p>
<ul>
<li>The first number (0.1) represents 10 percent from the left of the canvas. </li>
<li>The second number (0.1) represents 10 percent from the bottom. </li>
<li>The third number (0.8) represents the percentage of axes from the canvas (width).</li>
<li>The fourth number (0.8) represents the percentage of axes from the canvas (height).</li>
</ul>
<p>In the preceding example, we generated <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong> values using NumPy and plotted them in our custom canvas/axes using object-oriented methods, which gives us more control. Using some other functions, we can set names for each axis and title, as shown here:</p>
<pre class="source-code">import matplotlib.pyplot as plt
import numpy as np
a = np.linspace(1,10,15)
b = a ** 2
fig = plt.figure()
axes = fig.add_axes([0.1,0.1,0.8,0.8])
axes.plot(a,b)</pre>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="" height="254" src="image/B18333_12_5.jpg" width="380"/>
</div>
</div>
<p>Now, let’s <a id="_idIndexMarker676"/>put two sets of figures on the same canvas:</p>
<pre class="source-code">import matplotlib.pyplot as plt
import numpy as np
a = np.linspace(1,10,15)
b = a ** 2
fig = plt.figure()
axes1 = fig.add_axes([0.1,0.1,0.8,0.8])
axes2 = fig.add_axes([0.2,0.5,0.4,0.3])
axes1.plot(a,b)
axes2.plot(b,a)
axes1.set_xlabel("X1 Label")
axes1.set_ylabel("Y1 Label")
axes1.set_title("Title 1")
axes2.set_xlabel("X2 Label")
axes2.set_ylabel("Y2 Label")
axes2.set_title("Title 2")</pre>
<div>
<div class="IMG---Figure" id="_idContainer123">
<img alt="" height="291" src="image/B18333_12_6.jpg" width="399"/>
</div>
</div>
<p>In the <a id="_idIndexMarker677"/>next example, we have plotted two sets in the same figure and also added some labels, using the <strong class="source-inline">label</strong> and <strong class="source-inline">legend</strong> methods: </p>
<pre class="source-code">import matplotlib.pyplot as plt
import numpy as np
a = np.linspace(1,10,15)
b = a ** 2
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.plot(a, a**2, label = 'a squared')
ax.plot(a, a**3, label = 'a cubed')
ax.legend()</pre>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="" height="319" src="image/B18333_12_7.jpg" width="478"/>
</div>
</div>
<p>We have <a id="_idIndexMarker678"/>shown some examples using the Matplotlib library; for more <a id="_idIndexMarker679"/>detailed information, please refer to <a href="https://matplotlib.org">https://matplotlib.org</a>.</p>
<h1 id="_idParaDest-206"><a id="_idTextAnchor207"/>Seaborn</h1>
<p>Seaborn is a <a id="_idIndexMarker680"/>statistical library built on top of the Matplotlib library, and all the Matplotlib knowledge that we have learned can be applied to Seaborn. </p>
<p>We will start with a simple importing library. One of the useful features of Seaborn is its built-in datasets. In our case, we are going to use the <strong class="source-inline">tips</strong> dataset:</p>
<pre class="source-code">import seaborn as sns
tips = sns.load_dataset('tips')
tips.head()</pre>
<p>You can also check out some other details (such as the number of columns and rows, and data types) by <a id="_idIndexMarker681"/>using the <strong class="source-inline">.info()</strong> function, as shown here:</p>
<pre class="source-code">    <strong class="bold">total_bill   tip    sex    smoker  day    time    size</strong>
<strong class="bold">0   16.99       1.01    Female   No    Sun   Dinner    2</strong>
<strong class="bold">1   10.34       1.66    Male     No    Sun   Dinner    3</strong>
<strong class="bold">2   21.01       3.50    Male     No    Sun   Dinner    3</strong>
<strong class="bold">3   23.68       3.31    Male     No    Sun   Dinner    2</strong>
<strong class="bold">4   24.59       3.61    Female   No    Sun   Dinner    4</strong>
tips.info()
<strong class="bold">&lt;class 'pandas.core.frame.DataFrame'&gt;</strong>
<strong class="bold">RangeIndex: 244 entries, 0 to 243</strong>
<strong class="bold">Data columns (total 7 columns):</strong>
<strong class="bold">#   Column      Non-Null Count  Dtype</strong>   
---  ------      --------------  -----   
<strong class="bold">0   total_bill  244 non-null    float64 </strong>
<strong class="bold">1   tip         244 non-null    float64 </strong>
<strong class="bold">2   sex         244 non-null    category</strong>
<strong class="bold">3   smoker      244 non-null    category</strong>
<strong class="bold">4   day         244 non-null    category</strong>
<strong class="bold">5   time        244 non-null    category</strong>
<strong class="bold">6   size        244 non-null    int64   </strong>
<strong class="bold">dtypes: category(4), float64(2), int64(1)</strong>
<strong class="bold">memory usage: 7.4 KB</strong></pre>
<p>The example here shows a very basic histogram, where we set the dataset name as <strong class="source-inline">tips</strong> and the <strong class="source-inline">total_bill</strong> data (one of the columns of our dataset):</p>
<pre class="source-code">sns.displot(tips['total_bill']) </pre>
<div>
<div class="IMG---Figure" id="_idContainer125">
<img alt="" height="352" src="image/B18333_12_8.jpg" width="352"/>
</div>
</div>
<pre class="source-code">sns.displot(tips['total_bill'], kde=True, bins = 40)</pre>
<div>
<div class="IMG---Figure" id="_idContainer126">
<img alt="" height="352" src="image/B18333_12_9.jpg" width="352"/>
</div>
</div>
<p>The <strong class="source-inline">displot</strong> function <a id="_idIndexMarker682"/>described previously has different arguments that you can set to modify your graph/histogram. Note that we have entered two arguments, <strong class="source-inline">kde</strong> and <strong class="source-inline">bins</strong>. </p>
<p>Next, we are going to explore different types of graphs using the <strong class="source-inline">jointplot</strong> function. You need to provide the <strong class="source-inline">x</strong> value, the <strong class="source-inline">y</strong> value, and the data name as arguments, as shown here: </p>
<pre class="source-code">sns.jointplot('total_bill', 'tip', data=tips)</pre>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="" height="424" src="image/B18333_12_10.jpg" width="421"/>
</div>
</div>
<p>Note that <a id="_idIndexMarker683"/>the arguments we provided (<strong class="source-inline">total_bill</strong> and <strong class="source-inline">tips</strong>) are from data we imported, and they are the column names. It is a two-distribution plot, and in between, we have a scatter plot. As the bill value increases on the plot, the tip value also increases. <strong class="source-inline">jointplot</strong> also has other arguments that can help you to modify the graph (by default, it is a scatter plot, but you can also pass the hex argument), as shown here: </p>
<pre class="source-code">sns.jointplot('total_bill', 'tip', data=tips, kind = 'hex')</pre>
<div>
<div class="IMG---Figure" id="_idContainer128">
<img alt="" height="424" src="image/B18333_12_11.jpg" width="421"/>
</div>
</div>
<p>The next Seaborn <a id="_idIndexMarker684"/>features/functions we will introduce are box plots and violin plots. These types of plots are used to show the distribution of categorical data. They show the distribution of quantitative data in a way that facilitates comparison between variables:</p>
<pre class="source-code">sns.boxplot(x='day', y='total_bill', data=tips)</pre>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="" height="262" src="image/B18333_12_12.jpg" width="383"/>
</div>
</div>
<p>The violin plot, unlike the box plot, allows you to actually plot all the components that correspond <a id="_idIndexMarker685"/>to actual data points, and it essentially shows the kernel density estimation of the underlying distribution:  </p>
<pre class="source-code">sns.violinplot(x='day', y='total_bill', data=tips)</pre>
<div>
<div class="IMG---Figure" id="_idContainer130">
<img alt="" height="262" src="image/B18333_12_13.jpg" width="383"/>
</div>
</div>
<p>By adding more arguments to the <strong class="source-inline">violinplot</strong> function, you can add different features and details to the graph, as shown here. </p>
<pre class="source-code">sns.violinplot(x='day', y='total_bill', data=tips,
               hue = 'sex', split=True)</pre>
<div>
<div class="IMG---Figure" id="_idContainer131">
<img alt="" height="262" src="image/B18333_12_14.jpg" width="383"/>
</div>
</div>
<p>So far, we have <a id="_idIndexMarker686"/>shown the Seaborn library features via several examples. There are many different visual representations that use the Seaborn library in Python. Check <a id="_idIndexMarker687"/>out the Seaborn website (<a href="https://seaborn.pydata.org/index.xhtml">https://seaborn.pydata.org/index.xhtml</a>) for more details </p>
<h1 id="_idParaDest-207"><a id="_idTextAnchor208"/>Summary</h1>
<p>In this appendix, we practices using the Python data libraries, including NumPy, Pandas, Matpotlib, and Seaborn. Thoroughly understanding these examples will help you to understand the data libraries and master Python programming skills.</p>
</div>
<div>
<div id="_idContainer133">
</div>
</div>
</div>


<div id="sbo-rt-content"><div id="_idContainer144">
<h1 class="chapter-number" id="_idParaDest-208"><a id="_idTextAnchor209"/> </h1>
<h1 id="_idParaDest-209"><a id="_idTextAnchor210"/>Appendix 3: Practicing with Scikit-Learn</h1>
<p>In <a href="B18333_03.xhtml#_idTextAnchor072"><em class="italic">Chapter 3</em></a>, <em class="italic">Preparing for ML Development</em>, and <a href="B18333_04.xhtml#_idTextAnchor094"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing and Deploying ML Models</em>, we discussed the data preparation and ML model development process. In this appendix, we will continue learning about ML modeling skills by practicing using the scikit-learn package on the Google Colaboratory platform (<a href="http://colab.research.google.com">colab.research.google.com</a>). </p>
<p>With a step-by-step approach, we will show you how to develop ML models leveraging the scikit-learn library. We will cover the following practices:</p>
<ul>
<li>Data preparation</li>
<li>Regression</li>
<li>Classification</li>
</ul>
<p>It is a best practice to follow these examples and make sure you understand each of them. Practicing each example on Google Colab will yield the best results. </p>
<h1 id="_idParaDest-210"><a id="_idTextAnchor211"/>Data preparation</h1>
<p>In the previous chapters, we <a id="_idIndexMarker688"/> discussed Python libraries such as NumPy, Pandas, Matplotlib, and Seaborn for processing and visualizing data. Let’s start with simply importing the libraries: </p>
<pre class="source-code">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt</pre>
<p>We will use a simple dataset that has only 4 columns and 10 rows. </p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<img alt="" height="411" src="image/B18333_13_1.jpg" width="662"/>
</div>
</div>
<p>Notice that some of the columns<a id="_idIndexMarker689"/> are categorical and others are numerical, and some of them have missing values that we need to fix. The dataset <strong class="source-inline">.csv</strong> file is uploaded to Google Colab. </p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<img alt="" height="257" src="image/B18333_13_2.jpg" width="610"/>
</div>
</div>
<p>Using the <strong class="source-inline">pandas</strong> library and the <strong class="source-inline">read_csv</strong> function, we read the data and save it to a variable dataset, assign the first three columns (<strong class="bold">Country, Age, and Salary</strong>) as features, assign the feature dataset to <strong class="source-inline">X</strong>, and assign the last column dataset to <strong class="source-inline">y</strong>, as the prediction:</p>
<pre class="source-code">dataset = pd.read_csv('Data.csv')
X = dataset.iloc[:,:-1].values
y = dataset.iloc[:, -1].values
print(X)
<strong class="bold">[['France' 44.0 72000.0]</strong>
<strong class="bold">['Spain' 27.0 48000.0]</strong>
<strong class="bold">['Germany' 30.0 54000.0]</strong>
<strong class="bold">['Spain' 38.0 61000.0]</strong>
<strong class="bold">['Germany' 40.0 nan]</strong>
<strong class="bold">['France' 35.0 58000.0]</strong>
<strong class="bold">['Spain' nan 52000.0]</strong>
<strong class="bold">['France' 48.0 79000.0]</strong>
<strong class="bold">['Germany' 50.0 83000.0]</strong>
<strong class="bold">['France' 37.0 67000.0]]</strong></pre>
<p>Note that it has<a id="_idIndexMarker690"/> some missing values. In the ML training process, we need to minimize the number of missing values, so you can either delete the rows containing missing data cells or replace the missing values with an input value, for example, the average of all values in that column. The following example is the filling of missing values with the mean/average of that particular column: </p>
<pre class="source-code">from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(X[:, 1:3])
X[:, 1:3] = imputer.transform(X[:,1:3])
print(X)
<strong class="bold">[['France' 44.0 72000.0]</strong>
<strong class="bold">['Spain' 27.0 48000.0]</strong>
<strong class="bold">['Germany' 30.0 54000.0]</strong>
<strong class="bold">['Spain' 38.0 61000.0]</strong>
<strong class="bold">['Germany' 40.0 63777.77777777778]</strong>
<strong class="bold">['France' 35.0 58000.0]</strong>
<strong class="bold">['Spain' 38.77777777777778 52000.0]</strong>
<strong class="bold">['France' 48.0 79000.0]</strong>
<strong class="bold">['Germany' 50.0 83000.0]</strong>
<strong class="bold">['France' 37.0 67000.0]]</strong></pre>
<p>The next step is to convert categorical<a id="_idIndexMarker691"/> values to numerical values. In our loaded dataset (<strong class="source-inline">‘data.cvs’</strong>), the name of the column is <strong class="source-inline">Country</strong> and it has three different values (<strong class="source-inline">France</strong>, <strong class="source-inline">Spain</strong>, and <strong class="source-inline">Germany</strong>). We are going to convert it to three columns with binary values using one-hot encoding: </p>
<pre class="source-code">from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])],
remainder = 'passthrough')
X = np.array(ct.fit_transform(X))
print(X)
<strong class="bold">[[1.0 0.0 0.0 44.0 72000.0]</strong>
<strong class="bold">[0.0 0.0 1.0 27.0 48000.0]</strong>
<strong class="bold">[0.0 1.0 0.0 30.0 54000.0]</strong>
<strong class="bold">[0.0 0.0 1.0 38.0 61000.0]</strong>
<strong class="bold">[0.0 1.0 0.0 40.0 63777.77777777778]</strong>
<strong class="bold">[1.0 0.0 0.0 35.0 58000.0]</strong>
<strong class="bold">[0.0 0.0 1.0 38.77777777777778 52000.0]</strong>
<strong class="bold">[1.0 0.0 0.0 48.0 79000.0]</strong>
<strong class="bold">[0.0 1.0 0.0 50.0 83000.0]</strong>
<strong class="bold">[1.0 0.0 0.0 37.0 67000.0]]</strong></pre>
<p>We also need to encode the very last column of our dataset (where values are <strong class="source-inline">Yes</strong> or <strong class="source-inline">No</strong> only) to zeros and ones: </p>
<pre class="source-code">from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
print(y)
<strong class="bold">[0 1 0 0 1 1 0 1 0 1]</strong></pre>
<p>The next step is to split them<a id="_idIndexMarker692"/> into training and testing datasets:</p>
<pre class="source-code">from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state = 1)
print(X_train)
<strong class="bold">[[0.0 0.0 1.0 38.77777777777778 52000.0]</strong>
<strong class="bold">[0.0 1.0 0.0 40.0 63777.77777777778]</strong>
<strong class="bold">[1.0 0.0 0.0 44.0 72000.0]</strong>
<strong class="bold">[0.0 0.0 1.0 38.0 61000.0]</strong>
<strong class="bold">[0.0 0.0 1.0 27.0 48000.0]</strong>
<strong class="bold">[1.0 0.0 0.0 48.0 79000.0]</strong>
<strong class="bold">[0.0 1.0 0.0 50.0 83000.0]</strong>
<strong class="bold">[1.0 0.0 0.0 35.0 58000.0]]</strong>
print(X_test)
<strong class="bold">[[0.0 1.0 0.0 30.0 54000.0]</strong>
<strong class="bold">[1.0 0.0 0.0 37.0 67000.0]]</strong>
print(y_train)
<strong class="bold">[0 1 0 0 1 1 0 1]</strong>
print(y_test)
<strong class="bold">[0 1]</strong></pre>
<p>Now we have practiced the basic data processing skills, let’s get into feature scaling before starting training ML models. There are two types of feature scaling: standardization and normalization. The goal is to have all values<a id="_idIndexMarker693"/> of the features in the same range. Let’s examine the train data:</p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<img alt="" height="191" src="image/B18333_13_3.jpg" width="352"/>
</div>
</div>
<p>We find that the very first three rows were encoded previously, so we will apply feature scaling for rows 4 and 5 only:</p>
<pre class="source-code">from sklearn.preprocessing import StandardScaler
sc  = StandardScaler()
X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])
X_test[:, 3:] = sc.transform(X_test[:, 3:])
print(X_train)
<strong class="bold">[[0.0 0.0 1.0 -0.1915918438457856 -1.0781259408412427]</strong>
<strong class="bold">[0.0 1.0 0.0 -0.014117293757057902 -0.07013167641635401]</strong>
<strong class="bold">[1.0 0.0 0.0 0.5667085065333239 0.6335624327104546]</strong>
<strong class="bold">[0.0 0.0 1.0 -0.3045301939022488 -0.30786617274297895]</strong>
<strong class="bold">[0.0 0.0 1.0 -1.901801144700799 -1.4204636155515822]</strong>
<strong class="bold">[1.0 0.0 0.0 1.1475343068237056 1.2326533634535488]</strong>
<strong class="bold">[0.0 1.0 0.0 1.4379472069688966 1.5749910381638883]</strong>
<strong class="bold">[1.0 0.0 0.0 -0.7401495441200352 -0.5646194287757336]]</strong></pre>
<p>Note that the outcome of <strong class="source-inline">X_train</strong> is between -2 and +2 (avery short range).</p>
<h1 id="_idParaDest-211"><a id="_idTextAnchor212"/>Regression</h1>
<p>Now we have split the datasets and transformed the data, we will show you how to use the scikit-learn library to build up ML models. We will start with regression and show you the following examples:</p>
<ul>
<li>Simple linear regression</li>
<li>Multiple linear regression</li>
<li>Polynomial/non-linear regression</li>
</ul>
<h2 id="_idParaDest-212"><a id="_idTextAnchor213"/>Simple linear regression</h2>
<p>First things<a id="_idIndexMarker694"/> first, we need to prepare<a id="_idIndexMarker695"/> the dataset:</p>
<pre class="source-code">import numpy as pd
import pandas as pd
import matplotlib.pyplot as plt
dataset = pd.read_csv('Salary_Data.csv')
X = dataset.iloc[:,:-1].values
y = dataset.iloc[:, -1].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state = 1)</pre>
<p>Now we can start training our regression model. We need to import a class and feed our training data: </p>
<pre class="source-code">from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)</pre>
<p>Next, we are going to predict the results of the observation in the test set: </p>
<pre class="source-code">y_pred = regressor.predict(X_test)</pre>
<p>Let’s plot our prediction<a id="_idIndexMarker696"/> and real data<a id="_idIndexMarker697"/> to see how close they are: </p>
<pre class="source-code">plt.scatter(X_train, y_train, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color='blue')
plt.title("Salary vs Experiment (Training Set")
plt.xlabel("Years of Experience")
plt.ylabel("Salary")
plt.show()</pre>
<div>
<div class="IMG---Figure" id="_idContainer137">
<img alt="" height="271" src="image/B18333_13_4.jpg" width="400"/>
</div>
</div>
<pre class="source-code">plt.scatter(X_test, y_test, color = 'red')
plt.plot(X_train, regressor.predict(X_train), color='blue')
plt.title("Salary vs Experiment (Training Set")
plt.xlabel("Years of Experience")
plt.ylabel("Salary")
plt.show()</pre>
<div>
<div class="IMG---Figure" id="_idContainer138">
<img alt="" height="273" src="image/B18333_13_5.jpg" width="403"/>
</div>
</div>
<p>As you can see, the<a id="_idIndexMarker698"/> simple linear regression model fits<a id="_idIndexMarker699"/> well with our datasets.</p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor214"/>Multiple linear regression</h2>
<p>In this section, we are going<a id="_idIndexMarker700"/> to use another dataset, which has multiple columns<a id="_idIndexMarker701"/> as data features and one for the predictor:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer139">
<img alt="" height="230" src="image/B18333_13_6.jpg" width="556"/>
</div>
</div>
<p>First, we perform<a id="_idIndexMarker702"/> the data <a id="_idIndexMarker703"/>preparation:</p>
<ol>
<li>Import the necessary libraries and/or classes.</li>
<li>Split the dataset into <strong class="source-inline">X</strong> (features) and <strong class="source-inline">y</strong> (predictors).</li>
<li>Encode column index 3 (state names have been converted to binary values and saved as three additional columns).</li>
<li>Split the data into training and testing: </li>
</ol>
<pre class="source-code">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
dataset = pd.read_csv('50_Startups.csv')
X = dataset.iloc[:,:-1].values
y = dataset.iloc[:, -1].values
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])], remainder='passthrough')
X = np.array(ct.fit_transform(X))
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state=1)</pre>
<p>We can start training our model now:</p>
<pre class="source-code">from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train, y_train)</pre>
<p>And now is the time<a id="_idIndexMarker704"/> to validate/test our model. We cannot visualize<a id="_idIndexMarker705"/> as we did in the simple linear regression, since we have four different features and cannot plot them in a 5-dimensional graph. However, we can display two vectors: vectors of the <em class="italic">real profit</em> in the test set, and the <em class="italic">predicted profit</em>:</p>
<pre class="source-code">y_pred = regressor.predict(X_test)
np.set_printoptions(precision=2)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
<strong class="bold">[[114664.42 105008.31]</strong>
<strong class="bold">[ 90593.16  96479.51]</strong>
<strong class="bold">[ 75692.84  78239.91]</strong>
<strong class="bold">[ 70221.89  81229.06]</strong>
<strong class="bold">[179790.26 191050.39]</strong>
<strong class="bold">[171576.92 182901.99]</strong>
<strong class="bold">[ 49753.59  35673.41]</strong>
<strong class="bold">[102276.66 101004.64]</strong>
<strong class="bold">[ 58649.38  49490.75]</strong>
<strong class="bold">[ 98272.03  97483.56]]</strong></pre>
<p>The left side of the output indicates<a id="_idIndexMarker706"/> the predicted profits, and the right side indicates<a id="_idIndexMarker707"/> the real profits. </p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor215"/>Polynomial/non-linear regression</h2>
<p>In this section, we will show<a id="_idIndexMarker708"/> examples of non-linear regression, where the relationship<a id="_idIndexMarker709"/> between the target <a id="_idIndexMarker710"/>and the feature(s) is not<a id="_idIndexMarker711"/> linear, that is, it is polynomial. We will use a linear model and a non-linear model, and compare how they fit to the real datasets.</p>
<p>We will start with the data preparation:</p>
<pre class="source-code">import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
dataset = pd.read_csv('Position_Salaries.csv')
X = dataset.iloc[:, 1:-1].values
y = dataset.iloc[:, -1].values</pre>
<p>Now, we are going to train two models: linear regression and polynomial regression. The following example shows both regression models: </p>
<pre class="source-code">from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X,y)
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 2)
X_poly = poly_reg.fit_transform(X)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y)</pre>
<p>Next, we<a id="_idIndexMarker712"/> will<a id="_idIndexMarker713"/> visualize<a id="_idIndexMarker714"/> both<a id="_idIndexMarker715"/> regressions. </p>
<p>This is the linear regression:</p>
<pre class="source-code">plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg.predict(X), color = 'blue')
plt.title("Linear Regression")
plt.xlabel("Position Level")
plt.ylabel("Salary")
plt.show()</pre>
<div>
<div class="IMG---Figure" id="_idContainer140">
<img alt="" height="270" src="image/B18333_13_7.jpg" width="379"/>
</div>
</div>
<p>Then, this is the polynomial regression: </p>
<pre class="source-code">plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg_2.predict(X_poly), color = 'blue')
plt.title("Linear Regression")
plt.xlabel("Position Level")
plt.ylabel("Salary")
plt.show()</pre>
<div>
<div class="IMG---Figure" id="_idContainer141">
<img alt="" height="271" src="image/B18333_13_8.jpg" width="380"/>
</div>
</div>
<p>As we can see, polynomial<a id="_idIndexMarker716"/> regression (we used the power of <strong class="source-inline">2</strong>) yields accurate<a id="_idIndexMarker717"/> predictions. If we<a id="_idIndexMarker718"/> go with higher<a id="_idIndexMarker719"/> powers, we will harvest better results. In the following examples, we will change the power to <strong class="source-inline">4</strong> (see line 2), and the result will fit the dataset much better:</p>
<pre class="source-code">from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression()
lin_reg.fit(X,y)
from sklearn.preprocessing import PolynomialFeatures
poly_reg = PolynomialFeatures(degree = 4)
X_poly = poly_reg.fit_transform(X)
lin_reg_2 = LinearRegression()
lin_reg_2.fit(X_poly, y)
plt.scatter(X, y, color = 'red')
plt.plot(X, lin_reg_2.predict(X_poly), color = 'blue')
plt.title("Linear Regression")
plt.xlabel("Position Level")
plt.ylabel("Salary")
plt.show()</pre>
<div>
<div class="IMG---Figure" id="_idContainer142">
<img alt="" height="269" src="image/B18333_13_9.jpg" width="379"/>
</div>
</div>
<p>So far, we have covered<a id="_idIndexMarker720"/> the regression modeling process by practicing <a id="_idIndexMarker721"/>simple linear regression, multiple<a id="_idIndexMarker722"/> linear regression, and<a id="_idIndexMarker723"/> non-linear regression. In the next section, we will discuss classification models.</p>
<h1 id="_idParaDest-215"><a id="_idTextAnchor216"/>Classification</h1>
<p>Unlike regression, where you predict <a id="_idIndexMarker724"/>a continuous number, you use classification to predict a category. We will cover logistic regression here.</p>
<p>We will use a dataset of historical data of iPhone purchases, based on the age and the salary of the buyers, to predict whether a new potential buyer will purchase an iPhone.</p>
<div>
<div class="IMG---Figure" id="_idContainer143">
<img alt="" height="434" src="image/B18333_13_10.jpg" width="331"/>
</div>
</div>
<p>Let’s do the preparation<a id="_idIndexMarker725"/> first:</p>
<pre class="source-code">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
dataset = pd.read_csv('Social_Network_Ads.csv')
X = dataset.iloc[:,:-1].values
y = dataset.iloc[:, -1].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.2, random_state=1)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
print(X_train)
<strong class="bold">[[-0.8  -1.19]</strong>
<strong class="bold">[ 0.76 -1.37]</strong>
<strong class="bold">[ 0.85  1.44]</strong>
<strong class="bold">[-0.51 -1.49]</strong>
<strong class="bold">[-1.49  0.38]</strong>
<strong class="bold">[-1.19  0.55]</strong>
<strong class="bold">[ 1.05 -1.04]</strong>
<strong class="bold">[-0.22 -0.3 ]</strong>
<strong class="bold">[ 0.95 -1.34]</strong>
<strong class="bold">[-1.1  -1.07]</strong>
<strong class="bold">[-0.51  1.97]</strong>
<strong class="bold">[ 2.22 -1.01]</strong>
<strong class="bold">[ 1.44 -1.4 ]</strong>
<strong class="bold">[ 0.07 -0.39]</strong>
<strong class="bold">[-1.19  0.64]</strong>
<strong class="bold">[ 2.02 -0.9 ]</strong>
<strong class="bold">[ 1.15  0.58]</strong>
<strong class="bold">[-0.02  0.29]</strong>
<strong class="bold">[-0.22  0.26]</strong>
<strong class="bold">[-0.32 -0.75]</strong>
<strong class="bold">[-1.68 -0.57]</strong>
<strong class="bold">[ 0.85  0.58]</strong>
<strong class="bold">[-0.61 -1.01]</strong>
<strong class="bold">[ 0.95 -1.13]</strong>
<strong class="bold">[-0.22 -0.54]</strong>
<strong class="bold">[ 0.17  0.82]</strong>
<strong class="bold">[-0.41  1.32]</strong>
<strong class="bold">[ 1.15  0.52]</strong>
<strong class="bold">[ 0.76  0.32]</strong>
<strong class="bold">[ 0.66 -0.87]</strong>
<strong class="bold">[ 0.37 -0.27]</strong>
<strong class="bold">[ 0.46 -0.45]</strong>
<strong class="bold">[-0.22  0.14]</strong>
<strong class="bold">[ 0.37  0.11]</strong>
<strong class="bold">[-1.    0.82]</strong>
<strong class="bold">[-0.71  1.41]</strong>
<strong class="bold">[ 0.37 -0.48]</strong>
<strong class="bold">[ 0.37 -0.48]</strong>
<strong class="bold">[-1.68  0.41]</strong>
<strong class="bold">[ 0.85 -0.81]</strong>
<strong class="bold">[-1.   -1.1 ]</strong>
<strong class="bold">[...]</strong></pre>
<p>Note that all values<a id="_idIndexMarker726"/> are between -3 and +3. </p>
<p>Next, we train the logistic regression model: </p>
<pre class="source-code">from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression()
classifier.fit(X_train, y_train)</pre>
<p>Let’s predict a new result. Before running<a id="_idIndexMarker727"/> prediction scripts, let’s take a look into our original dataset and pick up a random feature set (in our case, the age of <strong class="source-inline">30</strong> and salary of <strong class="source-inline">87000</strong>), and the result is <strong class="source-inline">0</strong>.  </p>
<p>After running the prediction function, the result is also the same: </p>
<pre class="source-code">print(classifier.predict(sc.transform([[30, 87000]])))
<strong class="bold">[0]</strong></pre>
<p>The following example demonstrates the comparison of the actual and predicted results for the testing dataset so we can compare the accuracy/efficiency of the trained model: </p>
<pre class="source-code">y_pred = classifier.predict(X_test)
print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))
<strong class="bold">[[0 0]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[1 0]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[1 1]</strong>
<strong class="bold">[1 0]</strong>
<strong class="bold">[0 1]</strong>
<strong class="bold">[0 0]</strong>
<strong class="bold">[...]]</strong></pre>
<p>The output we have in the preceding <a id="_idIndexMarker728"/>code shows the comparison: the first column is the predicted value and the second column is the real value. To calculate the accuracy/efficiency of the model, we can divide the total of the correct number in the prediction by the total number of actual numbers in the testing dataset, and construct its confusion matrix:</p>
<pre class="source-code">from sklearn.metrics import confusion_matrix, accuracy_score
cm = confusion_matrix(y_test, y_pred)
print(cm)
accuracy_score(y_test, y_pred)
<strong class="bold">[[41  7]</strong>
<strong class="bold">[ 6 26]]</strong>
<strong class="bold">0.8375</strong></pre>
<p>The output shows the following:</p>
<pre class="source-code">TP=41
FP=6
FN=7
TN=26</pre>
<p>The accuracy/efficiency <a id="_idIndexMarker729"/>of the model is 83%.</p>
<h1 id="_idParaDest-216"><a id="_idTextAnchor217"/>Summary</h1>
<p>In this appendix, we have shown examples of data preparation and model development (regression and classification) using the scikit-learn library. Going over these examples and understanding the process will help in your understanding of ML concepts and processes.</p>
</div>
</div>


<div id="sbo-rt-content"><div id="_idContainer204">
<h1 class="chapter-number" id="_idParaDest-217"><a id="_idTextAnchor218"/>Appendix 4 </h1>
<h1 id="_idParaDest-218"><a id="_idTextAnchor219"/>Practicing with Google Vertex AI</h1>
<p>In <a href="B18333_07.xhtml#_idTextAnchor143"><em class="italic">Chapter 7</em></a>, <em class="italic">Exploring Google Cloud Vertex AI</em>, we discussed Google Cloud Vertex AI. This appendix contains some hands-on tutorials for Google Vertex AI in the Google Cloud console, step by step. We will cover the following labs:</p>
<ul>
<li>Vertex AI – enabling its API</li>
<li>Vertex AI – datasets</li>
<li>Vertex AI – labeling tasks</li>
<li>Vertex AI – training</li>
<li>Vertex AI – predictions (Vertex AI Endpoint)</li>
<li>Vertex AI – predictions (Vertex AI Batch Prediction)</li>
<li>Vertex AI – Workbench</li>
<li>Vertex AI – Feature Store</li>
<li>Vertex AI – pipelines and metadata</li>
<li>Vertex AI – model monitoring</li>
</ul>
<p>You are expected to follow these labs to practice Vertex AI and gain implementation skills.  </p>
<h1 id="_idParaDest-219"><a id="_idTextAnchor220"/>Vertex AI – enabling its API</h1>
<p>To start using <a id="_idIndexMarker730"/>Vertex AI in the Google Cloud console, you will need to set up a billing account and create a project. Once you have created a project (<em class="italic">Vertex AI – demo documentation</em>), you will be on the following project home dashboard: </p>
<div>
<div class="IMG---Figure" id="_idContainer145">
<img alt="" height="826" src="image/B18333_14_1.jpg" width="1081"/>
</div>
</div>
<p>Navigate <a id="_idIndexMarker731"/>through the top-left menu to launch Vertex AI: </p>
<div>
<div class="IMG---Figure" id="_idContainer146">
<img alt="" height="720" src="image/B18333_14_2.jpg" width="481"/>
</div>
</div>
<p>To launch Vertex AI for the first time, you need to enable the Vertex AI API. To do so, select a <strong class="bold">Region</strong> and click on the blue <strong class="bold">ENABLE VERTEX AI API</strong> button:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">f</p>
<div>
<div class="IMG---Figure" id="_idContainer147">
<img alt="" height="552" src="image/B18333_14_3.jpg" width="1129"/>
</div>
</div>
<p>After enabling the <a id="_idIndexMarker732"/>Vertex AI API, by default, you will land on the Vertex AI API dashboard.</p>
<h1 id="_idParaDest-220"><a id="_idTextAnchor221"/>Vertex AI – datasets</h1>
<p>The very first tool we <a id="_idIndexMarker733"/>will use in Vertex AI is <strong class="bold">Datasets</strong>. After clicking on <strong class="bold">Datasets</strong>, you will be taken to the respective page. Since we are working on a brand new project, there is no dataset to display. Click on <strong class="bold">CREATE DATASET</strong> to get started:</p>
<div>
<div class="IMG---Figure" id="_idContainer148">
<img alt="" height="646" src="image/B18333_14_4.jpg" width="1025"/>
</div>
</div>
<p>Enter the name of your dataset and select a dataset type to work with from the following <a id="_idIndexMarker734"/>four main categories: </p>
<ul>
<li><strong class="bold">Image</strong>:<ul><li><strong class="bold">Image classification (Single-label)</strong></li>
<li><strong class="bold">Image classification (Multi-label)</strong></li>
<li><strong class="bold">Image object detection</strong></li>
<li><strong class="bold">Image segmentation</strong></li>
</ul></li>
<li><strong class="bold">Tabular</strong>:<ul><li><strong class="bold">Regression/classification</strong></li>
<li><strong class="bold">Forecasting</strong></li>
</ul></li>
<li><strong class="bold">Text</strong>:<ul><li><strong class="bold">Text classification (Single-label)</strong></li>
<li><strong class="bold">Text classification (Multi-label)</strong></li>
<li><strong class="bold">Text entity extraction</strong></li>
<li><strong class="bold">Text sentiment analysis</strong></li>
</ul></li>
<li><strong class="bold">Video</strong>:<ul><li><strong class="bold">Video action recognition</strong></li>
<li><strong class="bold">Video classification</strong></li>
<li><strong class="bold">Video object tracking</strong></li>
</ul></li>
</ul>
<p>After selecting a dataset type, a bucket will be created in Google Cloud Storage as the default dataset <a id="_idIndexMarker735"/>repository. Here, you can specify the region where your bucket will be created: </p>
<div>
<div class="IMG---Figure" id="_idContainer149">
<img alt="" height="664" src="image/B18333_14_5.jpg" width="1081"/>
</div>
</div>
<p>Click on the <strong class="bold">CREATE</strong> button. You will be brought to the next page, where you need to specify the import method. There are three options for importing data:</p>
<ul>
<li><strong class="bold">Upload images from your computer </strong></li>
<li><strong class="bold">Upload import files from your computer</strong></li>
<li><strong class="bold">Select import files from Cloud Storage</strong></li>
</ul>
<p>To select a file(s) from your local computer, click on the <strong class="bold">SELECT FILES</strong> button:</p>
<div>
<div class="IMG---Figure" id="_idContainer150">
<img alt="" height="656" src="image/B18333_14_6.jpg" width="1218"/>
</div>
</div>
<p>On the page that appears, navigate to the folder that contains your pictures on your local computer and <a id="_idIndexMarker736"/>then select one or more pictures that you want to upload to your dataset. Note that you can upload up to 500 images per upload. The pictures that you upload should be in one of the following formats: </p>
<ul>
<li>JPEG</li>
<li>GIF</li>
<li>PNG</li>
<li>BMP</li>
<li>ICO</li>
</ul>
<p>If you need to upload more pictures, you need to click on the <strong class="bold">SELECT FILES</strong> button and specify which Google Cloud bucket you want to use for your dataset.</p>
<p>Once the pictures have been uploaded, you can browse all the images in the dataset and check their status (they will be either <strong class="bold">Labeled</strong> or <strong class="bold">Unlabeled</strong>). As shown in the following screenshot, we have a total of 20 images and all of them are unlabeled (labeling tasks will be covered in the next section):</p>
<div>
<div class="IMG---Figure" id="_idContainer151">
<img alt="" height="1238" src="image/B18333_14_7.jpg" width="1380"/>
</div>
</div>
<p>If you wish to use the <a id="_idIndexMarker737"/>other two options for adding images to a dataset – <strong class="bold">Upload import files from your computer</strong> and <strong class="bold">Select import files from Cloud Storage</strong> – you can simply provide a link to Cloud Storage, as shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="" height="956" src="image/B18333_14_8.jpg" width="1380"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<p>The other three categories (<strong class="bold">Tabular</strong>, <strong class="bold">Text</strong>, and <strong class="bold">Video</strong>) follow the same procedure in that you must create <a id="_idIndexMarker738"/>a dataset and upload files either from your local computer or from Google Cloud Storage. You must also enter a dataset name and select a region from the options provided. </p>
<h1 id="_idParaDest-221"><a id="_idTextAnchor222"/>Vertex AI – labeling tasks</h1>
<p>In this section, you will learn how to label data in Vertex AI. There are several ways to label data within <a id="_idIndexMarker739"/>a created dataset. If you use a small-size dataset, you can label each dataset manually. By default, the dataset only shows 10 images per page. If you want to see all your pictures on the same page, you can select a number from the <strong class="bold">Items per page</strong> option. There are three options – <strong class="bold">10</strong>, <strong class="bold">50</strong>, and <strong class="bold">100</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<img alt="" height="874" src="image/B18333_14_9.jpg" width="1309"/>
</div>
</div>
<p>Since we haven’t created a label yet, we need to define/create a label name. Click on <strong class="bold">ADD NEW LABEL</strong> and enter <a id="_idIndexMarker740"/>the name for a new label in the pop-up that appears. Then, click <strong class="bold">Done</strong>. In this example, we will create two new labels – <strong class="source-inline">Brain</strong> and <strong class="source-inline">Spine</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="" height="795" src="image/B18333_14_10.jpg" width="1070"/>
</div>
</div>
<p>After creating/adding new labels, you can either select every single image and label it or make multiple selections and label them as a group (following each step from <em class="italic">Steps 1</em> to <em class="italic">4</em>): </p>
<div>
<div class="IMG---Figure" id="_idContainer155">
<img alt="" height="821" src="image/B18333_14_11.jpg" width="1380"/>
</div>
</div>
<p>After labeling all the <a id="_idIndexMarker741"/>images, you can check if any of the images have been left unlabeled by going to the summary page: </p>
<div>
<div class="IMG---Figure" id="_idContainer156">
<img alt="" height="707" src="image/B18333_14_12.jpg" width="1380"/>
</div>
</div>
<p>If you have a <a id="_idIndexMarker742"/>very large dataset, you can create a labeling task and assign it to a team to label the dataset. </p>
<h1 id="_idParaDest-222"><a id="_idTextAnchor223"/>Vertex AI – training </h1>
<p>Now that you have a <a id="_idIndexMarker743"/>labeled dataset, it is ready for model training. Vertex AI provides different methods for training your model:</p>
<ul>
<li>AutoML</li>
<li>Custom training (advanced)</li>
</ul>
<p>To start AutoML training, in the Vertex AI console, click on <strong class="bold">Training</strong> and then the <strong class="bold">CREATE</strong> button, which is located at the top of the page (in our case, we are going to perform AutoML training using the dataset of MRI images that we created in the previous section):</p>
<div>
<div class="IMG---Figure" id="_idContainer157">
<img alt="" height="726" src="image/B18333_14_13.jpg" width="1264"/>
</div>
</div>
<p>On the page <a id="_idIndexMarker744"/>that appears, you need to define some specifications for the model you are trying to train:</p>
<ul>
<li><strong class="bold">Select Dataset</strong>: Here, you will be able to see all the datasets you created previously.</li>
<li><strong class="bold">Annotation Set</strong>: Labels are saved in collections called annotations. You can change annotation sets to apply a different group of labels to the same dataset. </li>
</ul>
<p>On the <strong class="bold">Training method</strong> page, select <strong class="bold">AutoML</strong> (this will be selected by default). Then, click <strong class="bold">CONTINUE</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer158">
<img alt="" height="717" src="image/B18333_14_14.jpg" width="1130"/>
</div>
</div>
<p>Since we are working on training a new ML model, we need to select <strong class="bold">Train new model</strong>. But in case you already have a trained model and want to retrain <em class="italic">or</em> train a model as a version of an existing model, select <strong class="bold">Train new version</strong>.</p>
<p>Next, enter the <a id="_idIndexMarker745"/>name of the model and provide a description (the description is optional).</p>
<p>In the <strong class="bold">Advanced options</strong> section, you will be given two options for data splitting: <strong class="bold">Randomly assigned</strong> and <strong class="bold">Manual (Advanced)</strong>. In terms of <strong class="bold">Randomly assigned</strong>, your dataset will be automatically randomized and split into training, validation, and testing sets using the following ratios:</p>
<ul>
<li><strong class="bold">Training</strong>: <strong class="source-inline">80%</strong></li>
<li><strong class="bold">Validation</strong>: <strong class="source-inline">10%</strong></li>
<li><strong class="bold">Testing</strong>: <strong class="source-inline">10%</strong></li>
</ul>
<p>You can change values as needed for your training mode. For more details, check out the Google documentation at <a href="https://cloud.google.com/vertex-ai/docs/general/ml-use?_ga=2.140326960.-2030104523.1635276817&amp;_gac=1.58794719.1650385127.CjwKCAjwu_mSBhAYEiwA5BBmf84zVxwFEpx-VaeJRusJFGq8rVNEovNnLhJ3vLYGMK3Eao6yJhRY5BoCdKgQAvD_BwE">https://cloud.google.com/vertex-ai/docs/general/ml-use?_ga=2.140326960.-2030104523.1635276817&amp;_gac=1.58794719.1650385127.CjwKCAjwu_mSBhAYEiwA5BBmf84zVxwFEpx-VaeJRusJFGq8rVNEovNnLhJ3vLYGMK3Eao6yJhRY5BoCdKgQAvD_BwE</a>: </p>
<div>
<div class="IMG---Figure" id="_idContainer159">
<img alt="" height="1174" src="image/B18333_14_15.jpg" width="1346"/>
</div>
</div>
<p>Click <strong class="bold">CONTINUE</strong>. </p>
<p>On the next and last page, you will be prompted to enter an amount in the <strong class="bold">Budget</strong> section. Here, you <a id="_idIndexMarker746"/>need to specify the maximum amount of time that will be used to train a particular model. Click on the <strong class="bold">START TRAINING</strong> option: </p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<img alt="" height="435" src="image/B18333_14_16.jpg" width="1317"/>
</div>
</div>
<p>When the training is done, you will receive an email confirming this and you will be able to see its status. Now, you can analyze the trained model from the training page (the same page where <a id="_idIndexMarker747"/>we trained our model) or click on <strong class="bold">Models</strong> from the left menu and click on the model you want to analyze:</p>
<div>
<div class="IMG---Figure" id="_idContainer161">
<img alt="" height="646" src="image/B18333_14_17.jpg" width="1323"/>
</div>
</div>
<p>When you click on the model you want to analyze, you will be prompted to select the version of the model. Since we have trained a brand new model, we only have one version. The following screenshot shows a summary of the training model for our tabular dataset: </p>
<div>
<div class="IMG---Figure" id="_idContainer162">
<img alt="" height="670" src="image/B18333_14_18.jpg" width="1185"/>
</div>
</div>
<p>From this chart, you can see the performance of the trained model, as well as the confusion matrix of our classification model.</p>
<p>After model training, it is time <a id="_idIndexMarker748"/>to deploy the model for prediction. There are two ways to deploy the model:</p>
<ul>
<li>Vertex AI Endpoint </li>
<li>Vertex AI Batch Prediction </li>
</ul>
<p>We will discuss these in more detail in the next few sections.</p>
<h1 id="_idParaDest-223"><a id="_idTextAnchor224"/>Vertex AI – predictions (Vertex AI Endpoint)</h1>
<p>In this section, we <a id="_idIndexMarker749"/>are going to deploy our model via Vertex AI Endpoint. There are two ways to deploy a model:</p>
<ul>
<li>From <strong class="bold">Models</strong></li>
<li>From <strong class="bold">Endpoints</strong></li>
</ul>
<p>Let’s look at these options in detail.</p>
<h2 id="_idParaDest-224"><a id="_idTextAnchor225"/>Deploying the model via Models</h2>
<p>Go to the <strong class="bold">Models</strong> section <a id="_idIndexMarker750"/>from the <a id="_idIndexMarker751"/>left menu, select the model you want to deploy, and select the version you want to deploy (remember, we have only one version since we have built/trained a brand-new model). Then, at the top of the page, click on <strong class="bold">DEPLOY &amp; TEST</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer163">
<img alt="" height="563" src="image/B18333_14_19.jpg" width="1380"/>
</div>
</div>
<p>After clicking on <strong class="bold">DEPLOY &amp; TEST</strong>, you will be taken to the next page. Here, click on the blue <strong class="bold">DEPLOY TO ENDPOINT</strong> button:</p>
<div>
<div class="IMG---Figure" id="_idContainer164">
<img alt="" height="656" src="image/B18333_14_20.jpg" width="1235"/>
</div>
</div>
<h2 id="_idParaDest-225"><a id="_idTextAnchor226"/>Deploying the model via Endpoints</h2>
<p>Go to <a id="_idIndexMarker752"/>the <strong class="bold">Endpoints</strong> section from the left menu. By <a id="_idIndexMarker753"/>doing so, you will be navigated to a pop-up page where you need to define your endpoint:</p>
<div>
<div class="IMG---Figure" id="_idContainer165">
<img alt="" height="662" src="image/B18333_14_21.jpg" width="1025"/>
</div>
</div>
<p>On the next page, you need to specify which model you want to deploy to the endpoint and select the version of the model:</p>
<div>
<div class="IMG---Figure" id="_idContainer166">
<img alt="" height="763" src="image/B18333_14_22.jpg" width="1345"/>
</div>
</div>
<p>Specify the <a id="_idIndexMarker754"/>number of compute nodes and leave the <a id="_idIndexMarker755"/>other settings as-is. Then, click <strong class="bold">DONE</strong> and then <strong class="bold">CREATE</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer167">
<img alt="" height="1419" src="image/B18333_14_23.jpg" width="1078"/>
</div>
</div>
<p>Upon <a id="_idIndexMarker756"/>completing the deployment, you will receive an <a id="_idIndexMarker757"/>email specifying the status of the endpoint deployment. Now, we can start making predictions:</p>
<ol>
<li>Go to <strong class="bold">Models</strong>.</li>
<li>Select the model you wish to use.</li>
<li>Select the model’s version.</li>
<li>At the top of the page, click on <strong class="bold">DEPLOY &amp; TEST</strong>.</li>
</ol>
<p>You will land on a page where you can start trying/testing your deployed model. Click on the blue <strong class="bold">UPLOAD IMAGE</strong> button:</p>
<div>
<div class="IMG---Figure" id="_idContainer168">
<img alt="" height="586" src="image/B18333_14_24.jpg" width="1380"/>
</div>
</div>
<p>Choose an image <a id="_idIndexMarker758"/>from your local drive. That image will be <a id="_idIndexMarker759"/>uploaded to the endpoint; on the right-hand side of the page, you will see the predicted result. In our case, we uploaded a random image (<strong class="source-inline">spine MARI</strong>) and the prediction was done with almost 99% accuracy:</p>
<div>
<div class="IMG---Figure" id="_idContainer169">
<img alt="" height="629" src="image/B18333_14_25.jpg" width="1181"/>
</div>
</div>
<p>If you <a id="_idIndexMarker760"/>need to use your endpoint in mobile/web <a id="_idIndexMarker761"/>applications, you can request a sample API. From the same page, click on <strong class="bold">Sample request</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer170">
<img alt="" height="561" src="image/B18333_14_26.jpg" width="1309"/>
</div>
</div>
<p>From the menu that appears, you can copy the script from the <strong class="bold">REST</strong> or <strong class="bold">PYTHON</strong> section based on your needs:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer171">
<img alt="" height="927" src="image/B18333_14_27.jpg" width="694"/>
</div>
</div>
<p>After several <a id="_idIndexMarker762"/>attempts, the system will start generating <a id="_idIndexMarker763"/>graphs based on the logs that have been collected from the endpoint:</p>
<div>
<div class="IMG---Figure" id="_idContainer172">
<img alt="" height="555" src="image/B18333_14_28.jpg" width="1289"/>
</div>
</div>
<p>So far, we <a id="_idIndexMarker764"/>have deployed our model to Vertex AI Endpoint. Now, <a id="_idIndexMarker765"/>let’s learn how to use batch prediction.</p>
<h1 id="_idParaDest-226"><a id="_idTextAnchor227"/>Vertex AI – predictions (Batch Prediction)</h1>
<p>Batch prediction is <a id="_idIndexMarker766"/>used when you don’t require an immediate response and want to get predictions from the accumulated data via a single request. Follow these steps to perform batch prediction for the models we trained earlier: </p>
<ol>
<li value="1">Go to <strong class="bold">Models</strong> from the left menu of the console.</li>
<li>Click on the model you want to work with.</li>
<li>Click on the version of the model you want to work with. </li>
<li>From the top menu, click on <strong class="bold">BATCH PREDICT</strong>.</li>
<li>Click on the blue <strong class="bold">CREATE BATCH PREDICTION</strong> button:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer173">
<img alt="" height="486" src="image/B18333_14_29.jpg" width="986"/>
</div>
</div>
<p>After clicking on <strong class="bold">CREATE BATCH PREDICTION</strong>, you need to define some parameters, such as the batch prediction’s name, source, output, and so on. Let’s analyze each of them: </p>
<ul>
<li><strong class="bold">Batch prediction name</strong>: Enter a name for the batch prediction.</li>
<li><strong class="bold">Select source</strong>: Here, you need to specify the source of the value that will be used in batch prediction. You <a id="_idIndexMarker767"/>can source either the BigQuery table <em class="italic">or</em> the file in Cloud Storage. Remember that since we are using the tabular dataset, the format must be CSV, JSONL, or TFRecord.  </li>
<li><strong class="bold">Batch prediction output</strong>: Select the format of output (BigQuery table, CSV, TFRecord, or JSONL) and provide the path (BigQuery <em class="italic">or</em> Cloud Storage).</li>
<li><strong class="bold">Explainability options</strong>: Optionally, you may check <strong class="bold">Enable feature attributions</strong> for this model to get feature attributions as part of the output.</li>
</ul>
<p>Using a BigQuery dataset and the table we have created, we can create a new batch prediction to predict the <strong class="bold">Risk Level</strong> column:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<img alt="" height="1580" src="image/B18333_14_30.jpg" width="871"/>
</div>
</div>
<p>After defining all the <a id="_idIndexMarker768"/>parameters, click on <strong class="bold">CREATE</strong>; the batch prediction will start processing. It will take some time to complete this process and you will receive an email upon completion.</p>
<h1 id="_idParaDest-227"><a id="_idTextAnchor228"/>Vertex AI – Workbench</h1>
<p>Vertex AI Workbench is a single development environment for the entire data science workflow. You can use <a id="_idIndexMarker769"/>Vertex AI Workbench’s notebook-based environment to query and explore data, develop and train a model, and run your code as <a id="_idIndexMarker770"/>part of a pipeline. Vertex AI workbench offers the following features:</p>
<ul>
<li><strong class="bold">Managed notebooks</strong> are Google-managed environments with integrations and features that help you set up and work in an end-to-end notebook-based production environment.</li>
<li><strong class="bold">User-managed notebooks</strong> are deep learning VM image instances that are heavily customizable and are therefore ideal for users who need a lot of control over their environment.</li>
</ul>
<p>To launch Vertex AI Workbench, navigate to the Vertex AI console and click on <strong class="bold">Workbench</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<img alt="" height="441" src="image/B18333_14_31.jpg" width="1030"/>
</div>
</div>
<p>If you are using Vertex AI Workbench for the first time, you will need to enable Notebook API, which you will be prompted for after you click on <strong class="bold">Workbench</strong>. Click on <strong class="bold">ENABLE</strong>:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<img alt="" height="447" src="image/B18333_14_32.jpg" width="983"/>
</div>
</div>
<p>In this lab, we are going to create a user-managed notebook. Vertex AI provides different types of <a id="_idIndexMarker771"/>Jupyter notebooks with various pre-installed libraries and dependencies. In this example, we will create a simple Python 3 notebook. Follow these steps:</p>
<ol>
<li value="1">From the left menu of the console, click on <strong class="bold">Workbench</strong>.</li>
<li>Click on <strong class="bold">NEW NOTEBOOK</strong> at the top of the page.</li>
<li>From the drop - down menu, click on <strong class="bold">Python 3</strong>:</li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="" height="633" src="image/B18333_14_33.jpg" width="1380"/>
</div>
</div>
<p>From the popup menu, you need to identify the notebook by providing its <strong class="bold">Notebook name</strong>, <strong class="bold">Region</strong>, <strong class="bold">Zone</strong>, and so on. Then, click on <strong class="bold">CREATE</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt="" height="655" src="image/B18333_14_34.jpg" width="954"/>
</div>
</div>
<p>Now, let’s check and <a id="_idIndexMarker772"/>analyze the notebook we created earlier and what features are available in Vertex AI Notebook(s). Vertex AI provides a Jupyter notebook environment. Click on <strong class="bold">OPEN JUPYTERLAB</strong>, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt="" height="616" src="image/B18333_14_35.jpg" width="1380"/>
</div>
</div>
<p>You will be navigated to a new tab containing the JupyterLab environment. On the left-hand side of the page, you will see all your folders. By default, there are two folders, but you can create, upload, or clone a folder from different sources such as GitHub. On the right-hand side <a id="_idIndexMarker773"/>of the page, you have <strong class="bold">Notebook</strong>, <strong class="bold">Console</strong>, and <strong class="bold">Other</strong>, which includes options for <strong class="bold">Terminal</strong>, <strong class="bold">Text File</strong>, and so on. Click on <strong class="bold">Python 3</strong> under Notebook. You will be taken to a blank notebook where you can start programming using Python:</p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="" height="633" src="image/B18333_14_36.jpg" width="931"/>
</div>
</div>
<p>After clicking on <strong class="bold">Python 3</strong>, the Jupyter notebook will create a new <strong class="source-inline">.ipynb</strong> file (on the left) so that you can start inputting your scripts on the right:</p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="" height="329" src="image/B18333_14_37.jpg" width="907"/>
</div>
</div>
<p>With that, you have created the training platform and are ready to start running Jupyter notebooks.</p>
<h1 id="_idParaDest-228"><a id="_idTextAnchor229"/>Vertex AI – Feature Store</h1>
<p>Feature Store in Vertex AI is where you can create entities and features and add values that can be used later <a id="_idIndexMarker774"/>as needed. In this demo, we are going to explore creating a Feature Store with entities and features in the Jupyter notebook by running some Python scripts. Before diving into the notebook, let’s create a Feature Store and entities via the Google Cloud console. From the left menu of the console, click on <strong class="bold">Features</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="" height="860" src="image/B18333_14_38.jpg" width="1380"/>
</div>
</div>
<p>Since we haven’t created any Feature Store(s) yet, this section will be empty. To create a new entity, click on <strong class="bold">CREATE ENTITY TYPE</strong>, which is located at the top of the page: </p>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="" height="698" src="image/B18333_14_39.jpg" width="1380"/>
</div>
</div>
<p>From the popup <a id="_idIndexMarker775"/>menu, enter all the necessary information and click the <strong class="bold">CREATE</strong> button, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="" height="784" src="image/B18333_14_40.jpg" width="650"/>
</div>
</div>
<p>Next, we are going to create a new notebook and clone a repository from GitHub (<a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples">https://github.com/GoogleCloudPlatform/vertex-ai-samples</a>). After creating a notebook and cloning the <a id="_idIndexMarker776"/>aforementioned repository, from the <strong class="source-inline">cloned</strong> folder, go to <strong class="source-inline">Vertex-ai-samples</strong> | <strong class="source-inline">notebooks</strong> | <strong class="source-inline">official</strong> | <strong class="source-inline">feature_store</strong> and click on <strong class="source-inline">gapic-feature-store.ipynb</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="" height="608" src="image/B18333_14_41.jpg" width="1082"/>
</div>
</div>
<p>Install all additional packages and enter your project ID, as shown in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<img alt="" height="486" src="image/B18333_14_42.jpg" width="1081"/>
</div>
</div>
<p>Run all the scripts as you go through (in this demo document, we will not go over every single line, but we will highlight important points related to the Vertex AI Feature Store). </p>
<p>These scripts will do the following: </p>
<ol>
<li value="1">Prepare a dataset for output. It will create a dataset in BigQuery to host the output.</li>
<li>Import libraries and define constants. </li>
<li>Create the feature store, as well as its entities and features.</li>
<li>Import values. </li>
</ol>
<p>After running all the scripts, you can check the created Feature Store (along with its entities and features) in the Google <a id="_idIndexMarker777"/>console. From the console, go to <strong class="bold">Vertex AI</strong> and click on <strong class="bold">Features</strong> from the left menu:</p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<img alt="" height="548" src="image/B18333_14_43.jpg" width="1380"/>
</div>
</div>
<p>You will notice that we have created a Feature Store with two entities (<strong class="source-inline">movies</strong> and <strong class="source-inline">users</strong>). Each entity has three features. If you click on any entity provided, you will see some details about that particular entity (in our case, the entity is <strong class="source-inline">movies</strong>): </p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<img alt="" height="480" src="image/B18333_14_44.jpg" width="1294"/>
</div>
</div>
<p>As a fully managed solution, Vertex AI Feature Store provides a centralized repository for you to organize, store, and serve ML features, and for your team to share, discover, and reuse ML features at <a id="_idIndexMarker778"/>scale. It greatly helps in developing and deploying <a id="_idIndexMarker779"/>new ML applications. For more detailed information about Feature Store, check out <a href="https://cloud.google.com/vertex-ai/docs/featurestore">https://cloud.google.com/vertex-ai/docs/featurestore</a>. </p>
<h1 id="_idParaDest-229"><a id="_idTextAnchor230"/>Vertex AI – pipelines and metadata</h1>
<p>Pipelines help you <a id="_idIndexMarker780"/>automate and reproduce your ML workflow. Vertex AI integrates <a id="_idIndexMarker781"/>its ML offerings across Google Cloud into a seamless development experience. Previously, models trained with AutoML and custom models were accessible via separate services. Vertex AI combines both into a single API, along with other new products. In this demo, we will create and run ML pipelines with Vertex Pipelines. </p>
<p>We are going to use the Vertex AI SDK and create a Jupyter notebook. After creating a notebook, click on <strong class="bold">OPEN JUPYTERLAB</strong>, as shown in the following screenshot:  </p>
<div>
<div class="IMG---Figure" id="_idContainer189">
<img alt="" height="591" src="image/B18333_14_45.jpg" width="1320"/>
</div>
</div>
<p>The new notebook will open in a new tab. Clone the repository (<a href="https://github.com/GoogleCloudPlatform/vertex-ai-samples">https://github.com/GoogleCloudPlatform/vertex-ai-samples</a>). </p>
<p>From the cloned <a id="_idIndexMarker782"/>folder, go to <strong class="source-inline">Vertex-ai-samples</strong> | <strong class="source-inline">notebooks</strong> | <strong class="source-inline">official</strong> | <strong class="source-inline">pipelines</strong>. After clicking on the <strong class="source-inline">automl_tabular_classification_beans.ipynb</strong> file, the notebook will open on <a id="_idIndexMarker783"/>the left-hand side, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<img alt="" height="639" src="image/B18333_14_46.jpg" width="1375"/>
</div>
</div>
<p>From the given notebook, read the overview and look at the dataset, objectives, and the cost of the particular demo. Run all commands as you go (we will not describe every single script but will focus on the main parts that are related to Vertex AI Pipeline). </p>
<p>The scripts you are going to run will do the following:</p>
<ol>
<li value="1">Set the project ID and bucket (where all the data will be stored).</li>
<li>Import the necessary libraries.</li>
<li>Define the constants and create the necessary components.</li>
<li>Create an <a id="_idIndexMarker784"/>end-to-end ML pipeline. This process will take over 2 hours <a id="_idIndexMarker785"/>since it is going to perform the following tasks:<ol><li>Create a dataset in Vertex AI.</li>
<li>Train a tabular classification model with AutoML.</li>
<li>Get evaluation metrics on this model.</li>
<li>Based on the evaluation metrics, it will decide whether to deploy the model using conditional logic in Vertex Pipelines.</li>
<li>Deploy the model to an endpoint using Vertex prediction.</li>
</ol></li>
</ol>
<p>After creating a pipeline, to view and analyze it, from the left menu of Vertex AI, click on <strong class="bold">Pipelines</strong>, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer191">
<img alt="" height="977" src="image/B18333_14_47.jpg" width="1313"/>
</div>
</div>
<p>After clicking on <strong class="bold">Pipelines</strong>, you will <a id="_idIndexMarker786"/>be taken to a page where you can select a pipeline you <a id="_idIndexMarker787"/>want to view. You will see the following diagram of the pipeline:</p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<img alt="" height="921" src="image/B18333_14_48.jpg" width="1132"/>
</div>
</div>
<p>If you click the <strong class="bold">Expand artifacts</strong> button at the top, you’ll be able to see details about the different artifacts <a id="_idIndexMarker788"/>that have been created from your pipeline. For example, if you <a id="_idIndexMarker789"/>click on the <strong class="source-inline">dataset</strong> artifact, you’ll see details about the Vertex AI dataset that was created. You can click the link specified next to <strong class="bold">URI</strong> to go to the page for that dataset:</p>
<div>
<div class="IMG---Figure" id="_idContainer193">
<img alt="" height="446" src="image/B18333_14_49.jpg" width="1510"/>
</div>
</div>
<p>To check the resulting metric visualizations from your custom evaluation component, click on the <strong class="source-inline">metrics</strong> artifact. On <a id="_idIndexMarker790"/>the right-hand side of your dashboard, you’ll be able to <a id="_idIndexMarker791"/>see the confusion matrix for this model:</p>
<div>
<div class="IMG---Figure" id="_idContainer194">
<img alt="" height="1197" src="image/B18333_14_50.jpg" width="1378"/>
</div>
</div>
<p>To check the model and endpoint that was created from this pipeline run, go to the <strong class="bold">Models</strong> section and click on the <strong class="source-inline">automl-beans</strong> model. There, you should see this model deployed to an endpoint:</p>
<div>
<div class="IMG---Figure" id="_idContainer195">
<img alt="" height="572" src="image/B18333_14_51.jpg" width="1348"/>
</div>
</div>
<p>Remember that each <a id="_idIndexMarker792"/>section in the pipeline will produce output that will be used <a id="_idIndexMarker793"/>as input for the next section. Later, if those outputs/inputs need to be modified, click on the <strong class="bold">Metadata</strong> button from the left menu: </p>
<div>
<div class="IMG---Figure" id="_idContainer196">
<img alt="" height="673" src="image/B18333_14_52.jpg" width="1380"/>
</div>
</div>
<p>With that, we have <a id="_idIndexMarker794"/>covered Vertex AI Pipelines and metadata. In the next <a id="_idIndexMarker795"/>section, we will discuss model monitoring for Vertex AI.</p>
<h1 id="_idParaDest-230"><a id="_idTextAnchor231"/>Vertex AI – model monitoring</h1>
<p>After model deployment, we <a id="_idIndexMarker796"/>need to monitor it since the data and environment may change and cause <a id="_idIndexMarker797"/>the model to deteriorate over time. Two concepts of <a id="_idIndexMarker798"/>monitoring should be considered: <strong class="bold">feature skew</strong> and <strong class="bold">drift detection</strong>. </p>
<p>In our demo documentation, we are going to build a brand-new tabular dataset and train the model. In this example, we will be using the <em class="italic">Women’s International Football Results</em> (<a href="https://www.kaggle.com/datasets/martj42/womens-international-football-results">https://www.kaggle.com/datasets/martj42/womens-international-football-results</a>) dataset. </p>
<p>We have created a tabular dataset where we have uploaded a CSV file that’s been downloaded from Kaggle. The following screenshot shows a summary of the dataset: </p>
<div>
<div class="IMG---Figure" id="_idContainer197">
<img alt="" height="769" src="image/B18333_14_53.jpg" width="1380"/>
</div>
</div>
<p>We have also trained <a id="_idIndexMarker799"/>a model using the AutoML method, and as the target, we have used the <strong class="source-inline">neutral</strong> column, which has two values (either <strong class="source-inline">False</strong> or <strong class="source-inline">True</strong>). The following screenshot shows the summary of the trained model: </p>
<div>
<div class="IMG---Figure" id="_idContainer198">
<img alt="" height="706" src="image/B18333_14_54.jpg" width="1380"/>
</div>
</div>
<p>With Explainable AI, we <a id="_idIndexMarker800"/>can see that the <strong class="source-inline">tournament</strong> column has the most impact on our model:</p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<img alt="" height="648" src="image/B18333_14_55.jpg" width="611"/>
</div>
</div>
<p>Next, we need to deploy our model to Endpoint. Click on the <strong class="bold">DEPLOY TO ENDPOINT</strong> button, as shown here:</p>
<div>
<div class="IMG---Figure" id="_idContainer200">
<img alt="" height="576" src="image/B18333_14_56.jpg" width="1280"/>
</div>
</div>
<p>In the popup menu that appears (on the right-hand side), fill out all the fields:</p>
<ul>
<li><strong class="bold">Monitoring job display name</strong>: The name of the monitoring job.</li>
<li><strong class="bold">Monitoring windows length</strong>: How many hours the model will be monitored for.</li>
<li><strong class="bold">Alert emails</strong>: Enter <a id="_idIndexMarker801"/>at least 1 email that is going to receive an alert (you can enter multiple email addresses).</li>
<li><strong class="bold">Sampling rate</strong>: The percentage of the sampling. </li>
</ul>
<p>Leave the rest of the fields as-is and click <strong class="bold">CONTINUE</strong>: </p>
<div>
<div class="IMG---Figure" id="_idContainer201">
<img alt="" height="1232" src="image/B18333_14_57.jpg" width="833"/>
</div>
</div>
<p>In the next and last section, you need to specify the monitoring objective (skew detection or drift detection). If you select the <strong class="bold">Training-serving skew detection</strong> option, you need to specify <a id="_idIndexMarker802"/>the training data source and target column. However, if you select the <strong class="bold">Prediction drift detection</strong> option, you need to specify alert thresholds. In our case, we will select <strong class="bold">Prediction drift detection</strong>. Next, click on the <strong class="bold">DEPLOY</strong> button:</p>
<div>
<div class="IMG---Figure" id="_idContainer202">
<img alt="" height="847" src="image/B18333_14_58.jpg" width="1115"/>
</div>
</div>
<p>It will take a while <a id="_idIndexMarker803"/>to process the deployment to the endpoint. Once the deployment has finished, you will receive emails about the <em class="italic">notification and status of the deployment</em>, and the <em class="italic">monitoring job being created</em> (in two separate emails): </p>
<div>
<div class="IMG---Figure" id="_idContainer203">
<img alt="" height="525" src="image/B18333_14_59.jpg" width="803"/>
</div>
</div>
<p>The preceding screenshot shows the model monitoring job request email notification. Note that the request <a id="_idIndexMarker804"/>has been submitted and is based on the incoming prediction request. It will be sampled and logged for analysis. </p>
<h1 id="_idParaDest-231"><a id="_idTextAnchor232"/>Summary</h1>
<p>In this appendix, we looked at examples based on the Google Cloud Vertex AI suite, which provides end-to-end services for data scientists. We covered Vertex AI datasets, labeling tasks, training, prediction, Workbench, Feature Store, pipelines, metadata, and model monitoring.</p>
<p>In the next appendix, we will discuss how to use various Google Cloud ML APIs, including the Vision API, NLP API, Speech-to-Text API, Text-to-Speech API, Translation API, and Dialogflow API.</p>
</div>
</div>


<div id="sbo-rt-content"><div id="_idContainer265">
<h1 class="chapter-number" id="_idParaDest-232"><a id="_idTextAnchor233"/>Appendix 5  </h1>
<h1 id="_idParaDest-233"><a id="_idTextAnchor234"/>Practicing with Google Cloud ML API</h1>
<p>In <a href="B18333_08.xhtml#_idTextAnchor159"><em class="italic">Chapter 8</em></a>, <em class="italic">Discovering Google Cloud ML API</em>, we explored the Google Cloud ML API, which is the API interface provided by Google, based on pre-trained models. The Google Cloud ML API includes the following APIs, all of which will be covered as topics in this appendix:</p>
<ul>
<li>Google Cloud Vision API</li>
<li>Google Cloud NLP API</li>
<li>Google Cloud Speech-to-Text API</li>
<li>Google Cloud Text-to-Speech API</li>
<li>Google Cloud Translation API</li>
<li>Google Cloud Dialogflow API</li>
</ul>
<p>In this appendix, we will provide implementation examples for each of these APIs. Let’s get started.</p>
<h1 id="_idParaDest-234"><a id="_idTextAnchor235"/>Google Cloud Vision API</h1>
<p>In this appendix, we <a id="_idIndexMarker805"/>will show you how to use the Vision API via Google Cloud Shell and the Python SDK. </p>
<p>Before we can start using the Vision API, we need to enable the Vision API from the Google console. From the left menu of the console, navigate to <strong class="bold">APIs and Services</strong> | <strong class="bold">Library</strong> and search for <strong class="source-inline">Vision API</strong>. After clicking on <strong class="bold">Vision API</strong>, you will be prompted to enable the API. In our case, the API is already enabled, as shown here:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer205">
<img alt="" height="323" src="image/B18333_15_1.jpg" width="858"/>
</div>
</div>
<p>We will start by using the Vision API via Google Cloud Shell. From the Google console, from the top-right corner, click on the Cloud Shell icon, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<img alt="" height="481" src="image/B18333_15_2.jpg" width="1142"/>
</div>
</div>
<p>After clicking the <a id="_idIndexMarker806"/>Cloud Shell icon, the Shell Terminal will appear at the bottom of the console, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer207">
<img alt="" height="207" src="image/B18333_15_3.jpg" width="1178"/>
</div>
</div>
<p>Now, let’s look at some examples to show how the Vision API works in Google Cloud Shell:</p>
<ul>
<li><strong class="bold">Example 1</strong> is an image of a tree: </li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer208">
<img alt="" height="578" src="image/B18333_15_4.jpg" width="772"/>
</div>
</div>
<p>To upload a file <a id="_idIndexMarker807"/>to Google Cloud Shell, click on the three dots icon highlighted in the following screenshot and click <strong class="bold">Upload</strong>: </p>
<div>
<div class="IMG---Figure" id="_idContainer209">
<img alt="" height="407" src="image/B18333_15_5.jpg" width="376"/>
</div>
</div>
<p>On the page that appears, you will be prompted to either upload a file or a folder. Navigate to your browser and select the image that you want to upload – in our case, <strong class="source-inline">tree.jpeg</strong>. Run the following command in Google Cloud Shell:</p>
<p class="source-code">gcloud ml vision detect-objects tree.jpeg </p>
<div>
<div class="IMG---Figure" id="_idContainer210">
<img alt="" height="833" src="image/B18333_15_6.jpg" width="1100"/>
</div>
</div>
<p>In the preceding screenshot, we can see <em class="italic">X</em> and <em class="italic">Y</em> values. These represent the corners of the <a id="_idIndexMarker808"/>object, and the object was detected as a flower with a 78% confidence level. </p>
<ul>
<li><strong class="bold">Example 2</strong> is an image of the Google logo: </li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer211">
<img alt="" height="208" src="image/B18333_15_7.jpg" width="208"/>
</div>
</div>
<p>Now, we will try another option that’s available from the Vision API and detect a logo. Upload the image of the Google logo, and type the following command in Google Cloud Shell:</p>
<p class="source-code">gcloud ml vision detect-logos glogo.jpeg</p>
<div>
<div class="IMG---Figure" id="_idContainer212">
<img alt="" height="538" src="image/B18333_15_8.jpg" width="850"/>
</div>
</div>
<p>You can also try using other options, such as detecting text on the <em class="italic">same</em> image. It will display <a id="_idIndexMarker809"/>the result, such as each character and the location of each character on the image: </p>
<p class="source-code">gcloud ml vision detect-text glogo.jpeg</p>
<div>
<div class="IMG---Figure" id="_idContainer213">
<img alt="" height="353" src="image/B18333_15_9.jpg" width="758"/>
</div>
</div>
<p>Note that the preceding screenshot only displays part of the result since the output is long. </p>
<p>Now, let’s interact with the Python SDK and the Vision API:</p>
<ol>
<li>First, you must install Python’s Google Cloud Vision API. Use the following command in Google Cloud Shell:<p class="source-code">pip3 install - -upgrade google-cloud-vision</p></li>
<li>Next, we will need some Python code. The following is a simple Python script where it detects the Google logo in the image:<p class="source-code">import io  </p><p class="source-code">import os</p><p class="source-code"># import the Google client library</p><p class="source-code">from google.cloud import vision</p><p class="source-code"># instantiates a client</p><p class="source-code">client = vision.ImageAnnotatorClient()</p><p class="source-code"># provide the name of the image file to annotate</p><p class="source-code">file_name = os.path.abspath('glogo.jpeg')</p><p class="source-code"># load the image into memoryview</p><p class="source-code">with io.open(file_name, 'rb') as image_file:</p><p class="source-code">     content = image_file.read()</p><p class="source-code">image = vision.Image(content = content)</p><p class="source-code"># Performs label detection on the image file</p><p class="source-code">response = client.logo_detection(image=image)</p><p class="source-code">logos = response.logo_annotations</p><p class="source-code">for logo in logos:</p><p class="source-code">     print(logo.description + " : " + str(logo.score))</p></li>
</ol>
<p>Here, we have imported <a id="_idIndexMarker810"/>all the necessary libraries, then defined an image name (the same logo image we used earlier), detected the logo using the Vision API, and printed the result and the score. </p>
<p>Now, we need to upload the Python script to Cloud Shell. After uploading the Python script, we can execute it, as follows:</p>
<pre class="source-code">python3 main.py
<strong class="bold">Google: 0.979978402153015 </strong></pre>
<p>The result <a id="_idIndexMarker811"/>indicates that the logo is <strong class="bold">Google</strong> and the score is 0.98, which means it has a confidence level of about 98%.</p>
<h1 id="_idParaDest-235"><a id="_idTextAnchor236"/>Google Cloud NLP API</h1>
<p>The Google Cloud NLP API uses <a id="_idIndexMarker812"/>models to analyze text. There are several ways to use the NLP API. Here, we will show you how to use the NLP API via Google Cloud Shell and the Python SDK. </p>
<p>Before you can use the NLP API, you will need to enable it. Simply navigate to the Natural Language API from the left menu of the console <em class="italic">or</em> type <strong class="source-inline">Natural Language API</strong> in the search bar at the top of the page. After landing on the NLP API web page, you will be asked to <strong class="bold">Enable API</strong> (if you have already done this, you can skip this). </p>
<p>Now, let’s start using the NLP API with Google Cloud Shell. Click on the Cloud Shell icon to activate it and type the following command: </p>
<pre class="source-code">gcloud ml language classify-text --content =
'If Roe is overturne, legislatures in 26 states have pending laws indicating that they intent to ban abortions, according to the Guttmacher Institure, a research organization that supports abortion rights. That could leave many women in need of abortion services hunders or thousands of miles away from access to procedure - unaffordable for many.
Corporate America is increasingly being drawn from the political sidelines on the abortion issue in response to pressure from inverstors, customer and employees. Companies are also struggling to attract and retain talent and worry about the impact these states anti-abortion laws could have on their workers.'</pre>
<p>As output, you will see the result of the classification based on the text provided: </p>
<div>
<div class="IMG---Figure" id="_idContainer214">
<img alt="" height="491" src="image/B18333_15_10.jpg" width="1413"/>
</div>
</div>
<p>The preceding output shows that it successfully identified the text in <strong class="source-inline">People &amp; Society / Social Issues &amp; Advocacy</strong> with 98% confidence and <strong class="source-inline">Sensitive Subjects</strong> with 89% confidence. </p>
<p>Let’s try another option – that is, <strong class="source-inline">analyze-sentiment</strong>. Here, we can provide some text to analyze the <a id="_idIndexMarker813"/>sentiment. For this example, we will use a restaurant review from Google Maps:</p>
<div>
<div class="IMG---Figure" id="_idContainer215">
<img alt="" height="455" src="image/B18333_15_11.jpg" width="1039"/>
</div>
</div>
<p>Enter the following command in Google Cloud Shell:</p>
<pre class="source-code">gcloud ml language analyze-sentiment - - content =
"From the minute I walked into the door, the family atmosphere hit me like a wave. The people that manage this place of the highest quality and the food matches it. I had a stromboli which I usually avoid because they turn out gummy and nasty. This place was a complete opposite. The bite of fresh garlic in the crust. The salty nuttiness of the mozzarella, the quality of the pepperoni and thin sliced sausage. Everything deserves the chefs kiss. This restaurant is an hour and half away from my home Greensville but well worth it several time over. I will definitely be back."</pre>
<p>The analysis will be displayed. The following screenshot shows part of the result: </p>
<div>
<div class="IMG---Figure" id="_idContainer216">
<img alt="" height="440" src="image/B18333_15_12.jpg" width="990"/>
</div>
</div>
<p>You can scroll <a id="_idIndexMarker814"/>through the result to view its details. There are two main concepts we need to understand: magnitude (the overall strength of the emotion – that is, either positive or negative) and score (the overall emotional leaning of the text).</p>
<p>Now, let’s investigate the NLP API with the Python SDK. There are a few other SDKs available as well, such as Java, PHP, Go, and others: </p>
<div>
<div class="IMG---Figure" id="_idContainer217">
<img alt="" height="155" src="image/B18333_15_13.jpg" width="786"/>
</div>
</div>
<p>First, we need to install the SDK. Type the following command: </p>
<pre class="source-code">pip install google-cloud-language</pre>
<p>You need to have a credential to interact with Google Cloud. Therefore, create a service account, generate and upload the key, and activate it. We will skip this step here.</p>
<p>Next, we will need some Python code. The following is a simple Python script that analyzes sentiment from the given text:</p>
<pre class="source-code"># Import ghe Google Cloud client library
from google.com import language_v1
#set a client variable
client = language_v1.LanguageServiceClient()
# The text to analyze
text = "We heard great review and thought we finally found a Authentic Classy Italiant Restaurant... WRONG!"
document = language_v1.Document(content=text, type_ = language_v1.Document.Type.PLAIN_TEXT)
#Detects the sentiment of the text
sentiment = client.analyze_sentiment(request={'document':document}).document_sentiment
print("Text: {}".format(text))
print("Sentiment:{}".format(sentiment.score, sentiment.magnitude))</pre>
<p>Save the script on your local computer and upload it to Google Cloud Shell with a filename of <strong class="source-inline">analyze_sentiment.py</strong>. When all the files have been uploaded, you can run the <strong class="source-inline">.py</strong> file to execute the script and start the analysis process:</p>
<pre class="source-code">python3 analyze_sentiment.py</pre>
<p>Using the <strong class="source-inline">analyze-sentiment</strong> method, we can retrieve the value of the sentiment and magnitude. We will let <a id="_idIndexMarker815"/>you execute it and get the results.</p>
<p>The NLP API can also be used to integrate with other APIs, such as the <strong class="bold">Speech-To-Text</strong> (<strong class="bold">STT</strong>) API.  By using <a id="_idIndexMarker816"/>the STT API, we can convert a sound/voice file into text and apply the NLP API, as well as various methods, to analyze the sentiment/entity/syntax of the text. </p>
<h1 id="_idParaDest-236"><a id="_idTextAnchor237"/>Google Cloud Speech-to-Text API</h1>
<p>The Google Cloud Speech-to-Text API is used to convert audio into text. The service is based on deep <a id="_idIndexMarker817"/>learning technology and supports 120 languages and variants. The service can be used to transcribe audio files as well as support voice-activated interfaces. Cloud Speech-to-Text automatically detects the language being spoken. </p>
<p>First, we need to enable the Speech-to-Text API. From the left menu, scroll down and click on <strong class="bold">Speech-to-Text</strong> or type <strong class="source-inline">speech to text</strong> in the search bar at the top of the page. </p>
<p>You will be taken to a page where you will be asked to enable the API. If you enabled it previously, you will see the following page: </p>
<div>
<div class="IMG---Figure" id="_idContainer218">
<img alt="" height="281" src="image/B18333_15_14.jpg" width="944"/>
</div>
</div>
<p>Since we are going to use <strong class="source-inline">curl</strong> to send a request to the Speech-To-Text API, we will need to generate an API key to pass in our request URL. </p>
<p>Create a request file named <strong class="source-inline">request.json</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer219">
<img alt="" height="277" src="image/B18333_15_15.jpg" width="1017"/>
</div>
</div>
<p>Then, run the following command (note that we skipped the key creation step):</p>
<pre class="source-code">curl -s -X POST -H "Content-Type: application/json" -- data-binay @request.json \ "http://speech.googleapis.com/v1/speech:recognize?key=${API_KEY} &gt; result.json</pre>
<p>The preceding command calls the Speech-to-Text API, gets all the variables from the <strong class="source-inline">request.json</strong> file, uses the key we have generated, and sends/saves all the results to the <strong class="source-inline">result.json</strong> file. To view the <strong class="source-inline">result.json</strong> file, type the following command: </p>
<pre class="source-code">cat result.json</pre>
<div>
<div class="IMG---Figure" id="_idContainer220">
<img alt="" height="313" src="image/B18333_15_16.jpg" width="956"/>
</div>
</div>
<p>The preceding <a id="_idIndexMarker818"/>screenshot shows the converted text from the given audio file, the confidence level (which is 98%), the time processed, and the language of the text. </p>
<p>Speech-to-Text supports <a id="_idIndexMarker819"/>different languages as well. Check out <a href="https://cloud.google.com/speech-to-text/docs/languages">https://cloud.google.com/speech-to-text/docs/languages</a> for more details about supported languages. </p>
<h1 id="_idParaDest-237"><a id="_idTextAnchor238"/>Google Cloud Text-To-Speech API</h1>
<p>The Google Cloud <a id="_idIndexMarker820"/>Text-to-Speech API maps natural language texts to human-like speech. The initial step is to enable the Text-to-Speech API:</p>
<div>
<div class="IMG---Figure" id="_idContainer221">
<img alt="" height="316" src="image/B18333_15_17.jpg" width="805"/>
</div>
</div>
<p>Go to Cloud Shell by clicking on the Shell icon. Before diving into the API, check the list of the supported <a id="_idIndexMarker821"/>voices and languages. You can check the available languages and voices via Google Cloud Shell using the following command: </p>
<pre class="source-code">curl -H "Authorization: Bearer "$(gcloud auth application-default print-access-token) \
     -H "Content-Type: application/json; charset=utf-8" \
     "https://texttospeech.googleapis.com/v1/voices"</pre>
<p>The preceding command will list <em class="italic">all</em> possible languages and voices, along with their corresponding code. The following screenshot shows just a small part of the list: </p>
<div>
<div class="IMG---Figure" id="_idContainer222">
<img alt="" height="676" src="image/B18333_15_18.jpg" width="765"/>
</div>
</div>
<p>Next, we need to create a JSON file named <strong class="source-inline">synthesize-text.json</strong>, where we will specify the language and voice codes and provide some full text that we want to convert into audio:</p>
<pre class="source-code">{
     'input':{
     'text':'This is a demo documentation for the Cloud Text-to-Speech API. In this demo documentation we are using the United States English language. The code of the language is: "en-US-Standard-A". Thank you.'
     },
     'voice':{
     'languageCode':'en-us',
     'name':'en-US-Standard-A',
     'ssmlGender':'MALE'
     },
     'audioConfig':{
     'audioEncoding':'MP3'
     }
}</pre>
<p>Use the following <a id="_idIndexMarker822"/>code to call the Text-to-Speech API using the <strong class="source-inline">curl</strong> command:</p>
<pre class="source-code">curl -H "Authorization: Bearer "$(gcloud auth application-default print-access-token) \
  -H "Content-Type: application/json; charset=utf-8" \
  -d @synthesize-text.json "https://texttospeech.googleapis.com/v1/text:synthesize" \
  &gt; synthesize-text.txt</pre>
<p>After running the preceding command, the result will be saved to a file called <strong class="source-inline">synthesize-text.txt</strong>.</p>
<p>Open the <strong class="source-inline">synthesize-text.txt</strong> file. You’ll notice that the Text-to-Speech API provides the audio output in base64-encoded text and has been assigned to the <strong class="source-inline">audioContent</strong> field, as shown here:</p>
<pre class="source-code">{
  "audioContent": "//NExAASGoHwABhGudEACdzqFXfRE4EY3AACkD/zX4ADf/6J/[...]"
}</pre>
<p>Now, create <a id="_idIndexMarker823"/>a Python file named <strong class="source-inline">tts_decode.py</strong>:</p>
<pre class="source-code">import argparse
from base64 import decodebytes
import json
"""
Usage:
     python tts_decode.py --input "synthesize-text.txt" \
     --output "synthesize-text-audio.mp3"
"""
def decode_tts_output(input_file, output_file):
     """ Decode output from Cloud Text-to-Speech.
     input_file: the response from Cloud Text-to-Speech
     output_file: the name of the audio file to create
     """
     with open(input_file) as input:
     response = json.load(input)
     audio_data = response['audioContent']
     with open(output_file, "wb") as new_file:
     new_file.write(decodebytes(audio_data.encode('utf-8')))
if __name__ == '__main__':
     parser = argparse.ArgumentParser(
     description="Decode output from Cloud Text-to-Speech",
     formatter_class=argparse.RawDescriptionHelpFormatter)
     parser.add_argument('--input',
                     help='The response from the Text-to-Speech API.',
                     required=True)
    parser.add_argument('--output',
                     help='The name of the audio file to create',
                     required=True)
     args = parser.parse_args()
     decode_tts_output(args.input, args.output)</pre>
<p>Finally, run the <a id="_idIndexMarker824"/>following command from Cloud Shell:</p>
<pre class="source-code">python tts_decode.py --input "synthesize-text.txt" --output "synthesize-text-audio.mp3"</pre>
<p>Now, our MP3 file is ready.</p>
<h1 id="_idParaDest-238"><a id="_idTextAnchor239"/>Google Cloud Translation API</h1>
<p>Google’s Cloud Translation <a id="_idIndexMarker825"/>API allows you to translate text that’s in more than 100 languages. There are two ways to use the Translation API. From the main menu of the Google Cloud console, click on <strong class="bold">Translation</strong> (as shown in the following screenshot) or type <strong class="source-inline">Translation API</strong> into the search bar (at the top of the page):</p>
<div>
<div class="IMG---Figure" id="_idContainer223">
<img alt="" height="1416" src="image/B18333_15_19.jpg" width="794"/>
</div>
</div>
<p>After clicking <a id="_idIndexMarker826"/>on the dashboard, you will be prompted to select one of three options: </p>
<ul>
<li><strong class="bold">AutoML Translation</strong></li>
<li><strong class="bold">Cloud Translation API</strong></li>
<li><strong class="bold">Translation Hub</strong></li>
</ul>
<p>Since we are going to use <strong class="source-inline">curl</strong> to send a request to the Translation API, we will need to generate an API key to pass in our request URL.   </p>
<p>First, let’s define our text. Type <strong class="source-inline">TEXT=</strong> and type any sentence. Remember that the space between words should <em class="italic">not</em> be left blank; instead, in each space, type <strong class="source-inline">%20</strong>. In our example, we will type <strong class="source-inline">This is a demo documentation</strong>, as shown here: </p>
<pre class="source-code">TEXT = "This%20is%20a%20demo%20documentation"</pre>
<p>Now that we’ve defined our <strong class="source-inline">TEXT</strong> variable, type the following command, which is underlined in red: </p>
<pre class="source-code">curl "https://translation.googleapis.com/language/translate/v2?target=ru&amp;key=${API_KEY}&amp;q=${TEXT}"</pre>
<p>That is a <strong class="source-inline">curl</strong> call <a id="_idIndexMarker827"/>for the API where we specified a <a id="_idIndexMarker828"/>target language (in our case, it is <strong class="source-inline">ru</strong>, which stands for Russian). Check out <a href="https://cloud.google.com/translate/docs/languages">https://cloud.google.com/translate/docs/languages</a> to view all the supported languages and ISO-639-1 codes. </p>
<p>After running the command provided, you will see the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer224">
<img alt="" height="450" src="image/B18333_15_20.jpg" width="1485"/>
</div>
</div>
<p>The Google Cloud Translation API also has a feature that detects a language and translates it into <em class="italic">any</em> supported language. </p>
<p>In this example, we will provide two different texts in different languages, as shown here: </p>
<pre class="source-code">TEXT_ONE = "Merhaba%20Dostlar"
TEXT_TWO = "привет%20друзья"</pre>
<p>Instead of using a space between words, we need to type <strong class="source-inline">%20</strong>.  </p>
<p>After defining <strong class="source-inline">TEXT_ONE</strong> and <strong class="source-inline">TEXT_TWO</strong>, run the following command to call the Translation API to detect the language(s): </p>
<pre class="source-code">Curl "https://translation.googleapis.com/language/translate/v2/detect?key=${API_KEY}&amp;q=$"TEXT_TWO}"</pre>
<p>This will return the language that was detected with a confidence level between zero and one (where zero stands for 0% and one stands for 100%):</p>
<div>
<div class="IMG---Figure" id="_idContainer225">
<img alt="" height="692" src="image/B18333_15_21.jpg" width="626"/>
</div>
</div>
<p>As we can see, the <a id="_idIndexMarker829"/>language of <strong class="source-inline">TEXT_ONE</strong> is Turkish (<strong class="source-inline">tr</strong>) with a 100% confidence level and the language of <strong class="source-inline">TEXT_TWO</strong> is Russian (<strong class="source-inline">ru</strong>) with a 100% confidence level as well. </p>
<h1 id="_idParaDest-239"><a id="_idTextAnchor240"/>Google Cloud Dialogflow API</h1>
<p>The Dialogflow API is <a id="_idIndexMarker830"/>used for chatbots, <strong class="bold">interactive voice response</strong> (<strong class="bold">IVR</strong>), and <a id="_idIndexMarker831"/>other dialog-based interactions with human speech. First, we need to enable the Dialogflow API:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer226">
<img alt="" height="403" src="image/B18333_15_22.jpg" width="640"/>
</div>
</div>
<p>To start using the <a id="_idIndexMarker832"/>platform, open a new tab and type <a href="https://dialogflow.cloud.google.com/#/logindialogflow.cloud.google.com">https://dialogflow.cloud.google.com/#/logindialogflow.cloud.google.com</a> into your browser. You might be asked to sign in with your Google account.</p>
<p>You will be taken to <a id="_idIndexMarker833"/>the Dialogflow platform. This is the Dialogflow ES (essential) version. Here, we will mainly be using its <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>). There is <a id="_idIndexMarker834"/>a section where you can type scripts (such as Google Functions), which will be covered in this demo documentation. </p>
<p>Now, let’s become familiar with the Dialogflow UI. The following screenshot shows the page you’ll see when you first start the Dialogflow platform: </p>
<div>
<div class="IMG---Figure" id="_idContainer227">
<img alt="" height="698" src="image/B18333_15_23.jpg" width="1082"/>
</div>
</div>
<p>In Dialogflow, you <a id="_idIndexMarker835"/>can have multiple agents. If you have already created agents before, you can check them by clicking on the down arrow shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer228">
<img alt="" height="489" src="image/B18333_15_24.jpg" width="498"/>
</div>
</div>
<p>Here, you can view a list of all created agents and/or create a new agent. To create a new agent, scroll down to the bottom and click on <strong class="bold">Create new agent</strong>, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer229">
<img alt="" height="133" src="image/B18333_15_25.jpg" width="292"/>
</div>
</div>
<p>You will be taken to the following page: </p>
<div>
<div class="IMG---Figure" id="_idContainer230">
<img alt="" height="487" src="image/B18333_15_26.jpg" width="912"/>
</div>
</div>
<p>In our case, we <a id="_idIndexMarker836"/>will name our new agent <strong class="source-inline">Test-Chatbot</strong> and leave all the other settings as-is. Now, click <strong class="bold">CREATE</strong>: </p>
<div>
<div class="IMG---Figure" id="_idContainer231">
<img alt="" height="567" src="image/B18333_15_27.jpg" width="804"/>
</div>
</div>
<p>Once your agent <a id="_idIndexMarker837"/>has been created, you will see <strong class="bold">Intents</strong> on the left:</p>
<div>
<div class="IMG---Figure" id="_idContainer232">
<img alt="" height="850" src="image/B18333_15_28.jpg" width="457"/>
</div>
</div>
<p>This option helps you understand the intent of the user. Then, we have <strong class="bold">Entities</strong>, which allows you to grab useful information from users. For example, when someone says “I want a veggie pizza,” the chatbot can understand that they want a vegetarian pizza instead of a normal pizza.  </p>
<p>At the time of writing, <strong class="bold">Knowledge</strong> is a beta feature. You can use this feature to create a knowledge base inside Dialogflow.</p>
<p><strong class="bold">Fulfillment</strong> is where <a id="_idIndexMarker838"/>you can integrate Dialogflow with other systems such as your customer management system. The following screenshot shows an example script powered by Google Functions:</p>
<div>
<div class="IMG---Figure" id="_idContainer233">
<img alt="" height="776" src="image/B18333_15_29.jpg" width="963"/>
</div>
</div>
<p>Now, let’s learn how to use intents in Dialogflow. An intent is an action a user wants to perform or a question a <a id="_idIndexMarker839"/>user has. For example, let’s say that they want to order a pizza, book an appointment, or want more information about your company. In Dialogflow, we can create an agent that can understand the intent of the user and automatically reply to it. </p>
<p>When you create <a id="_idIndexMarker840"/>a new agent (as we did here), Dialogflow <a id="_idIndexMarker841"/>creates two intents by default:</p>
<ul>
<li><strong class="bold">Default Welcome Intent</strong> </li>
<li><strong class="bold">Default Fallback Intent</strong></li>
</ul>
<p>These can be seen in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer234">
<img alt="" height="452" src="image/B18333_15_30.jpg" width="1189"/>
</div>
</div>
<p>Let’s see what’s <a id="_idIndexMarker842"/>inside <strong class="bold">Default Welcome Intent</strong>. To do so, click <a id="_idIndexMarker843"/>on <strong class="bold">Default Welcome Intent</strong>. This intent is for understanding greetings such as Hello, Hi, and others: </p>
<div>
<div class="IMG---Figure" id="_idContainer235">
<img alt="" height="645" src="image/B18333_15_31.jpg" width="562"/>
</div>
</div>
<p>Each intent consists of two main parts: </p>
<ul>
<li>Training phases </li>
<li>Response phases</li>
</ul>
<p>Training phases help <a id="_idIndexMarker844"/>Dialogflow understand the intent of the user, whereas the <a id="_idIndexMarker845"/>response phase involves Dialogflow understanding the greeting. If it does, it will respond with some text, which will be provided in the <strong class="bold">Responses</strong> section: </p>
<div>
<div class="IMG---Figure" id="_idContainer236">
<img alt="" height="874" src="image/B18333_15_32.jpg" width="853"/>
</div>
</div>
<p>The other default intent provided by Dialogflow is <strong class="bold">Default Fallback Intent</strong>. Here, when Dialogflow does not understand the user, it will respond with one of the pieces of text provided as a text response in <strong class="bold">Default Fallback Intent</strong>: </p>
<div>
<div class="IMG---Figure" id="_idContainer237">
<img alt="" height="851" src="image/B18333_15_33.jpg" width="566"/>
</div>
</div>
<p>Now, let’s dive <a id="_idIndexMarker846"/>into creating an intent (as mentioned previously, we will build <a id="_idIndexMarker847"/>a chatbot where the user will order a pizza). </p>
<p>To create a new intent, click <strong class="bold">CREATE INTENT</strong>, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer238">
<img alt="" height="348" src="image/B18333_15_34.jpg" width="897"/>
</div>
</div>
<p>We will name our first intent <strong class="source-inline">opening_times</strong>, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer239">
<img alt="" height="367" src="image/B18333_15_35.jpg" width="863"/>
</div>
</div>
<p>Click on <strong class="bold">ADD TRAINING PHRASES</strong>. This is where we will provide examples of how users can express their intent:</p>
<div>
<div class="IMG---Figure" id="_idContainer240">
<img alt="" height="263" src="image/B18333_15_36.jpg" width="887"/>
</div>
</div>
<p>Let’s add a <a id="_idIndexMarker848"/>couple of training phrases, as <a id="_idIndexMarker849"/>shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer241">
<img alt="" height="415" src="image/B18333_15_37.jpg" width="755"/>
</div>
</div>
<p>Next, we need to add some responses. Click on <strong class="bold">ADD RESPONSE</strong>, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer242">
<img alt="" height="411" src="image/B18333_15_38.jpg" width="1274"/>
</div>
</div>
<p>We will add just one response here, but remember you can add multiple responses for a particular intent:</p>
<div>
<div class="IMG---Figure" id="_idContainer243">
<img alt="" height="472" src="image/B18333_15_39.jpg" width="870"/>
</div>
</div>
<p>When you’ve finished with the training phrases and responses, save them; they will automatically start training the agent. Dialogflow needs to train the agent to respond to the question. This will take a <a id="_idIndexMarker850"/>couple of seconds (or minutes if you have a long input); you <a id="_idIndexMarker851"/>will be notified when training has been completed. </p>
<p>Now, let’s try our trained agent. On the right-hand side of the page, you will see a small section where you can try your trained agent.</p>
<p>Let’s type a question for our agent. Let’s ask if they are open today and see what response we get:</p>
<div>
<div class="IMG---Figure" id="_idContainer244">
<img alt="" height="1352" src="image/B18333_15_40.jpg" width="652"/>
</div>
</div>
<p>In this agent, we can create multiple intents. Dialogflow will understand which intents to use and respond to the user’s question. </p>
<p>Now, let’s create a new intent <a id="_idIndexMarker852"/>for ordering pizza. Click on <strong class="bold">Intents</strong> from the <a id="_idIndexMarker853"/>left menu and click on <strong class="bold">CREATE INTENT</strong>. The following screenshot shows some expressions you can use: </p>
<div>
<div class="IMG---Figure" id="_idContainer245">
<img alt="" height="515" src="image/B18333_15_41.jpg" width="853"/>
</div>
</div>
<p>Now, provide a single response to the question (you can add more responses if you wish): </p>
<div>
<div class="IMG---Figure" id="_idContainer246">
<img alt="" height="584" src="image/B18333_15_42.jpg" width="1290"/>
</div>
</div>
<p>Upon saving <a id="_idIndexMarker854"/>your intent, your agent will be <a id="_idIndexMarker855"/>retrained. Let’s test our agent: </p>
<div>
<div class="IMG---Figure" id="_idContainer247">
<img alt="" height="474" src="image/B18333_15_43.jpg" width="628"/>
</div>
</div>
<p>Note that the user’s phrase was different than it was in the training phases, but Dialogflow still understands the intent and gives the correct response. </p>
<p>If the user needs to provide more details, such as different toppings for the pizza, then Dialogflow <a id="_idIndexMarker856"/>will need to use entities that haven’t been created yet. So, we will need to create an entity. </p>
<p>From the left menu, click on <strong class="bold">Entities</strong> and then <strong class="bold">CREATE ENTITY</strong>, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer248">
<img alt="" height="406" src="image/B18333_15_44.jpg" width="1185"/>
</div>
</div>
<p>We will use <strong class="source-inline">topping</strong> as the entity’s name. Check the <strong class="bold">Define synonyms</strong> box; this will help us find synonyms for each word. The following screenshot shows some examples of synonyms for <strong class="source-inline">topping</strong>: </p>
<div>
<div class="IMG---Figure" id="_idContainer249">
<img alt="" height="871" src="image/B18333_15_45.jpg" width="1199"/>
</div>
</div>
<p>Click <strong class="bold">SAVE</strong>.</p>
<p>Let’s go back to <a id="_idIndexMarker857"/>our intent to order pizza and create a new training phrase with some details of ordering such as “I want to order cheese pizza.” </p>
<p>Here are some additional expressions for ordering a pizza: </p>
<div>
<div class="IMG---Figure" id="_idContainer250">
<img alt="" height="661" src="image/B18333_15_46.jpg" width="995"/>
</div>
</div>
<p>Note that when we type <strong class="source-inline">two</strong>, it automatically detects the entity: </p>
<div>
<div class="IMG---Figure" id="_idContainer251">
<img alt="" height="394" src="image/B18333_15_47.jpg" width="1111"/>
</div>
</div>
<p><strong class="source-inline">@sys.number</strong> is a built-in entity <a id="_idIndexMarker858"/>in Dialogflow that captures/recognizes the <a id="_idIndexMarker859"/>numbers in the dialog. There are also other built-in entities that can <a id="_idIndexMarker860"/>recognize emails, addresses, phone numbers, and other details. Check out <a href="https://cloud.google.com/dialogflow/es/docs/reference/system-entities">https://cloud.google.com/dialogflow/es/docs/reference/system-entities</a> for more built-in entities. Here are some examples: </p>
<div>
<div class="IMG---Figure" id="_idContainer252">
<img alt="" height="411" src="image/B18333_15_48.jpg" width="894"/>
</div>
</div>
<p>In our case, we are building entities, so we will need to map some words to an entity. This will allow us to see which part of the sentence is related to which entity. Simply select a word or a phrase from the expression that we used in the <strong class="bold">Intents</strong> section and assign it to the entity that’s been created, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer253">
<img alt="" height="407" src="image/B18333_15_49.jpg" width="1123"/>
</div>
</div>
<p>After selecting the <a id="_idIndexMarker861"/>entity, you can select other words or phrases from the training expression and map them to that entity:</p>
<div>
<div class="IMG---Figure" id="_idContainer254">
<img alt="" height="729" src="image/B18333_15_50.jpg" width="967"/>
</div>
</div>
<p>In this example, we have two different entities – <strong class="source-inline">@sys.number</strong> and <strong class="source-inline">@topping</strong>:  </p>
<div>
<div class="IMG---Figure" id="_idContainer255">
<img alt="" height="547" src="image/B18333_15_51.jpg" width="1195"/>
</div>
</div>
<p>We can also <a id="_idIndexMarker862"/>change or modify the response with more dynamic answers:</p>
<div>
<div class="IMG---Figure" id="_idContainer256">
<img alt="" height="358" src="image/B18333_15_52.jpg" width="973"/>
</div>
</div>
<p>Click the <strong class="bold">SAVE</strong> button to train the agent and test/try the chatbot: </p>
<div>
<div class="IMG---Figure" id="_idContainer257">
<img alt="" height="483" src="image/B18333_15_53.jpg" width="626"/>
</div>
</div>
<p>If the user provides information about the topping for the pizza, we need to reinforce the extra <a id="_idIndexMarker863"/>question to gather that particular information. Click the checkbox next to <strong class="bold">topping</strong> in the <strong class="bold">REQUIRED</strong> section and click on <strong class="bold">Define prompts</strong>, as shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer258">
<img alt="" height="463" src="image/B18333_15_54.jpg" width="992"/>
</div>
</div>
<p>Add a question that the user will be asked, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer259">
<img alt="" height="368" src="image/B18333_15_55.jpg" width="850"/>
</div>
</div>
<p>Click <strong class="bold">Close</strong> and save the agent. Note that you can add more prompts as well. Now, let’s test the agent, which doesn’t know anything about the topping: </p>
<div>
<div class="IMG---Figure" id="_idContainer260">
<img alt="" height="295" src="image/B18333_15_56.jpg" width="633"/>
</div>
</div>
<p>With that, we have a <a id="_idIndexMarker864"/>very simple chatbot with basic functionality. But in real life, we would want to keep asking the user if they have any other questions <em class="italic">or</em> if they would like to add some other items to their current order. In our case, we have a chatbot where users order a pizza. In addition to asking about the pizza and its toppings, we can ask them if they <a id="_idIndexMarker865"/>want another pizza <em class="italic">or</em> if they want to add some drinks. This feature is called a <strong class="bold">follow-up intent</strong>. To enable this feature, click on <strong class="bold">Add follow-up intent</strong> next to the <strong class="bold">order_pizza</strong> intent, as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer261">
<img alt="" height="422" src="image/B18333_15_57.jpg" width="960"/>
</div>
</div>
<p>So, if a user <a id="_idIndexMarker866"/>wants to continue to add another pizza, they can simply answer <strong class="bold">YES</strong>. Click <strong class="bold">YES</strong> to add the follow-up intent: </p>
<div>
<div class="IMG---Figure" id="_idContainer262">
<img alt="" height="154" src="image/B18333_15_58.jpg" width="506"/>
</div>
</div>
<p>Click on the <strong class="bold">order_pizza - yes</strong> section; you will be taken to a page where you can modify the section’s content. </p>
<p>In our case, we will change the name of the intent, leave all the training phrases as-is (they are good enough to use in our case), and add a response that states <strong class="source-inline">Great! What topping do you want on your pizza?</strong>. Then, click <strong class="bold">SAVE</strong>: </p>
<div>
<div class="IMG---Figure" id="_idContainer263">
<img alt="" height="1330" src="image/B18333_15_59.jpg" width="771"/>
</div>
</div>
<p>But what happens if the user answers <strong class="bold">NO</strong> to this follow-up question (Do you want to add more pizza?)? In this case, we would like to ask if they want to add a drink to their order. As we did <a id="_idIndexMarker867"/>previously, from the follow-up intent, select <strong class="bold">NO</strong>, click on the sub-intent and change its title (optional), leave all the training phrases as-is, and type the answer, as shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer264">
<img alt="" height="1098" src="image/B18333_15_60.jpg" width="690"/>
</div>
</div>
<p>After saving the <a id="_idIndexMarker868"/>agent, you can try to chat with the bot. You can keep adding follow-up questions or new intents based on your designed conversation architecture. </p>
<h1 id="_idParaDest-240"><a id="_idTextAnchor241"/>Summary</h1>
<p>In this appendix, we provided examples of how to use various Google Cloud ML APIs, including the Vision API, NLP API, Speech-To-Text API, Text-To-Speech API, Translation API, and Dialogflow API.</p>
</div>
</div>
</body></html>