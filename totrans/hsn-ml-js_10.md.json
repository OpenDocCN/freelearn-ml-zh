["```py\n{\n  \"name\": \"Ch10-NLP\",\n  \"version\": \"1.0.0\",\n  \"description\": \"ML in JS Example for Chapter 10 - NLP\",\n  \"main\": \"src/index.js\",\n  \"author\": \"Burak Kanber\",\n  \"license\": \"MIT\",\n  \"scripts\": {\n    \"start\": \"node src/index.js\"\n  },\n  \"dependencies\": {\n    \"compromise\": \"^11.7.0\",\n    \"natural\": \"^0.5.6\",\n    \"wordnet-db\": \"^3.1.6\"\n  }\n}\n```", "```py\nconst compromise = require('compromise');\nconst natural = require('natural');\n```", "```py\n[\n    ['plate', 'laser'],\n    ['parachute', 'parasail'],\n    ['parachute', 'panoply']\n]\n    .forEach(function(pair) {\n        console.log(\"Levenshtein distance between '\"+pair[0]+\"' and '\"+pair[1]+\"': \"\n            + natural.LevenshteinDistance.apply(null, pair)\n        );\n    });\n```", "```py\nLevenshtein distance between 'plate' and 'laser': 3\nLevenshtein distance between 'parachute' and 'parasail': 5\nLevenshtein distance between 'parachute' and 'panoply': 7\n```", "```py\nconst fulltextSearch = (query, documents) => {\n    const db = new natural.TfIdf();\n    documents.forEach(document => db.addDocument(document));\n    db.tfidfs(query, (docId, score) => {\n        console.log(\"DocID \" + docId + \" has score: \" + score);\n    });\n};\n\nfulltextSearch(\"fashion style\", [\n    \"i love cooking, it really relaxes me and makes me feel at home\",\n    \"food and restaurants are basically my favorite things\",\n    \"i'm not really a fashionable person\",\n    \"that new fashion blogger has a really great style\",\n    \"i don't love the cinematic style of that movie\"\n]);\n```", "```py\nDocID 0 has score: 0\nDocID 1 has score: 0\nDocID 2 has score: 0\nDocID 3 has score: 3.4271163556401456\nDocID 4 has score: 1.5108256237659907\n```", "```py\nconst stemmedFulltextSearch = (query, documents) => {\n    const db = new natural.TfIdf();\n    const tokenizer = new natural.WordTokenizer();\n    const stemmer = natural.PorterStemmer.stem;\n    const stemAndTokenize = text => tokenizer.tokenize(text).map(token => stemmer(token));\n\n    documents.forEach(document => db.addDocument(stemAndTokenize(document)));\n    db.tfidfs(stemAndTokenize(query), (docId, score) => {\n        console.log(\"DocID \" + docId + \" has score: \" + score);\n    });\n};\n\nstemmedFulltextSearch(\"fashion style\", [\n    \"i love cooking, it really relaxes me and makes me feel at home\",\n    \"food and restaurants are basically my favorite things\",\n    \"i'm not really a fashionable person\",\n    \"that new fashion blogger has a really great style\",\n    \"i don't love the cinematic style of that movie\"\n]);\n```", "```py\nDocID 0 has score: 0\nDocID 1 has score: 0\nDocID 2 has score: 1.5108256237659907\nDocID 3 has score: 3.0216512475319814\nDocID 4 has score: 1.5108256237659907\n```", "```py\nconst summarize = (article, maxSentences = 3) => {\n    const sentences = compromise(article).sentences().out('array');\n    const db = new natural.TfIdf();\n    const tokenizer = new natural.WordTokenizer();\n    const stemmer = natural.PorterStemmer.stem;\n    const stemAndTokenize = text => tokenizer.tokenize(text).map(token => stemmer(token));\n    const scoresMap = {};\n\n    // Add each sentence to the document\n    sentences.forEach(sentence => db.addDocument(stemAndTokenize(sentence)));\n\n    // Loop over all words in the document and add that word's score to an overall score for each sentence\n    stemAndTokenize(article).forEach(token => {\n        db.tfidfs(token, (sentenceId, score) => {\n            if (!scoresMap[sentenceId]) scoresMap[sentenceId] = 0;\n            scoresMap[sentenceId] += score;\n        });\n    });\n\n    // Convert our scoresMap into an array so that we can easily sort it\n    let scoresArray = Object.entries(scoresMap).map(item => ({score: item[1], sentenceId: item[0]}));\n    // Sort the array by descending score\n    scoresArray.sort((a, b) => a.score < b.score ? 1 : -1);\n    // Pick the top maxSentences sentences\n    scoresArray = scoresArray.slice(0, maxSentences);\n    // Re-sort by ascending sentenceId\n    scoresArray.sort((a, b) => parseInt(a.sentenceId) < parseInt(b.sentenceId) ? -1 : 1);\n    // Return sentences\n    return scoresArray\n        .map(item => sentences[item.sentenceId])\n        .join('. ');\n\n};\n```", "```py\nconst summarizableArticle = \"One of the most popular metrics used in search relevance, text mining, and information retrieval is the term frequency - inverse document frequency score, or tf-idf for short. In essence, tf-idf measures how significant a word is to a particular document. The tf-idf metric therefore only makes sense in the context of a word in a document that's part of a larger corpus of documents. Imagine you have a corpus of documents, like blog posts on varying topics, that you want to make searchable. The end user of your application runs a search query for fashion style. How do you then find matching documents and rank them by relevance? The tf-idf score is made of two separate but related components. The first is term frequency, or the relative frequency of a specific term in a given document. If a 100-word blog post contains the word fashion four times, then the term frequency of the word fashion is 4% for that one document. Note that term frequency only requires a single term and a single document as parameters; the full corpus of documents is not required for the term frequency component of tf-idf. Term frequency by itself is not sufficient to determine relevance, however. Words like this and the appear very frequently in most text and will have high term frequencies, but those words are not typically relevant to any search.\";\n\nconsole.log(\"3-sentence summary:\");\nconsole.log(summarize(summarizableArticle, 3));\nconsole.log(\"5-sentence summary:\");\nconsole.log(summarize(summarizableArticle, 5));\n```", "```py\n3-sentence summary:\n the tf idf metric therefore only makes sense in the context of a word in a document that's part of a larger corpus of documents. if a 100-word blog post contains the word fashion four times then the term frequency of the word fashion is 4% for that one document. note that term frequency only requires a single term and a single document as parameters the full corpus of documents is not required for the term frequency component of tf idf\n\n 5-sentence summary:\n one of the most popular metrics used in search relevance text mining and information retrieval is the term frequency inverse document frequency score or tf idf for short. the tf idf metric therefore only makes sense in the context of a word in a document that's part of a larger corpus of documents. the first is term frequency or the relative frequency of a specific term in a given document. if a 100-word blog post contains the word fashion four times then the term frequency of the word fashion is 4% for that one document. note that term frequency only requires a single term and a single document as parameters the full corpus of documents is not required for the term frequency component of tf idf\n```", "```py\nconst tokenizablePhrase = \"I've not yet seen 'THOR: RAGNAROK'; I've heard it's a great movie though. What'd you think of it?\";\n\nconst simpleTokenizer = (text) =>\n    text.toLowerCase()\n        .replace(/(\\w)'(\\w)/g, '$1$2')\n        .replace(/\\W/g, ' ')\n        .split(' ')\n        .filter(token => token.length > 2);\n\nconsole.log(simpleTokenizer(tokenizablePhrase));\n```", "```py\n[ 'ive', 'not', 'yet', 'seen', 'thor',\n 'ragnarok', 'ive', 'heard', 'its',\n 'great', 'movie', 'though',\n 'whatd', 'you', 'think' ]\n```", "```py\nconsole.log(\"Natural.js Word Tokenizer:\");\nconsole.log((new natural.WordTokenizer()).tokenize(tokenizablePhrase));\n```", "```py\nNatural.js Word Tokenizer:\n [ 'I', 've', 'not', 'yet', 'seen',\n 'THOR', 'RAGNAROK', 'I', 've',\n 'heard', 'it', 's', 'a', 'great', 'movie',\n 'though', 'What', 'd', 'you', 'think',\n 'of', 'it' ]\n```", "```py\nconsole.log(\"Natural.js WordPunct Tokenizer:\");\nconsole.log((new natural.WordPunctTokenizer()).tokenize(tokenizablePhrase));\n```", "```py\nNatural.js WordPunct Tokenizer:\n [ 'I', '\\'', 've', 'not', 'yet', 'seen',\n '\\'', 'THOR', ': ', 'RAGNAROK', '\\'', '; ',\n 'I', '\\'', 've', 'heard', 'it', '\\'', 's',\n 'a', 'great', 'movie', 'though', '.', 'What',\n '\\'', 'd', 'you', 'think', 'of',\n 'it', '?' ]\n```", "```py\nconsole.log(\"Compromise.js Words:\");\nconsole.log(compromise(tokenizablePhrase).words().out('array'));\nconsole.log(\"Compromise.js Adjectives:\");\nconsole.log(compromise(tokenizablePhrase).adjectives().out('array'));\nconsole.log(\"Compromise.js Nouns:\");\nconsole.log(compromise(tokenizablePhrase).nouns().out('array'));\nconsole.log(\"Compromise.js Questions:\");\nconsole.log(compromise(tokenizablePhrase).questions().out('array'));\nconsole.log(\"Compromise.js Contractions:\");\nconsole.log(compromise(tokenizablePhrase).contractions().out('array'));\nconsole.log(\"Compromise.js Contractions, Expanded:\");\nconsole.log(compromise(tokenizablePhrase).contractions().expand().out('array'));\n```", "```py\nCompromise.js Words:\n [ 'i\\'ve', '', 'not', 'yet', 'seen',\n 'thor', 'ragnarok', 'i\\'ve', '', 'heard',\n 'it\\'s', '', 'a', 'great', 'movie', 'though',\n 'what\\'d', '', 'you', 'think', 'of', 'it' ]\n Compromise.js Adjectives:\n [ 'great' ]\n Compromise.js Nouns:\n [ 'thor', 'ragnarok', 'movie' ]\n Compromise.js Questions:\n [ 'what\\'d you think of it' ]\n Compromise.js Contractions:\n [ 'i\\'ve', 'i\\'ve', 'it\\'s', 'what\\'d' ]\n Compromise.js Contractions, Expanded:\n [ 'i have', 'i have', 'it is', 'what did' ]\n```", "```py\n['_SPF:PASS',\n '_DKIM:FAIL',\n '_SUBJ:buy',\n '_SUBJ:pharmaceuticals',\n '_SUBJ:online',\n '_LINK:pay',\n '_LINK:bitcoin',\n 'are',\n 'you',\n 'interested',\n 'buying',\n 'medicine',\n 'online']\n```", "```py\nconsole.log(\"Natural.js bigrams:\");\nconsole.log(natural.NGrams.bigrams(tokenizablePhrase));\n```", "```py\nNatural.js bigrams:\n [ [ 'I', 've' ],\n [ 've', 'not' ],\n [ 'not', 'yet' ],\n [ 'yet', 'seen' ],\n [ 'seen', 'THOR' ],\n [ 'THOR', 'RAGNAROK' ],\n [ 'RAGNAROK', 'I' ],\n [ 'I', 've' ],\n [ 've', 'heard' ],\n [ 'heard', 'it' ],\n [ 'it', 's' ],\n [ 's', 'a' ],\n [ 'a', 'great' ],\n [ 'great', 'movie' ],\n [ 'movie', 'though' ],\n [ 'though', 'What' ],\n [ 'What', 'd' ],\n [ 'd', 'you' ],\n [ 'you', 'think' ],\n [ 'think', 'of' ],\n [ 'of', 'it' ] ]\n```", "```py\nconsole.log(\"Tokenized and stemmed:\");\nconsole.log(\n    (new natural.WordTokenizer())\n        .tokenize(\n            \"Writing and write, lucky and luckies, part parts and parted\"\n        )\n        .map(natural.PorterStemmer.stem)\n```", "```py\nTokenized and stemmed:\n [ 'write', 'and', 'write',\n 'lucki', 'and', 'lucki',\n 'part', 'part', 'and', 'part' ]\n```", "```py\nconsole.log(\n    (new natural.WordTokenizer())\n        .tokenize(\"Francis Bacon and France is Bacon\")\n        .map(t => natural.Metaphone.process(t))\n);\n```", "```py\n[ 'FRNSS', 'BKN', 'ANT', 'FRNS', 'IS', 'BKN' ]\n```", "```py\nconsole.log(natural.Metaphone.compare(\"praise\", \"preys\"));\nconsole.log(natural.Metaphone.compare(\"praise\", \"frays\"));\n```", "```py\nconst siriCommand = \"Hey Siri, order me a pizza from John's pizzeria\";\nconst siriCommandObject = compromise(siriCommand);\n\nconsole.log(siriCommandObject.verbs().out('array'));\nconsole.log(siriCommandObject.nouns().out('array'));\n```", "```py\n[ 'order' ]\n[ 'siri', 'pizza', 'john\\'s pizzeria' ]\n```", "```py\nconsole.log(\n    compromise(\"Hey Siri, order me a pizza from John's pizzeria\")\n        .match(\"#Noun [#Verb me a #Noun+ *+ #Noun+]\").out('text')\n);\n\nconsole.log(\n    compromise(\"OK Google, write me a letter to the congressman\")\n        .match(\"#Noun [#Verb me a #Noun+ *+ #Noun+]\").out('text')\n);\n```", "```py\norder me a pizza from John's\nwrite me a letter to the congressman\n```"]