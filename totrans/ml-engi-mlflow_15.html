<html><head></head><body>
		<div id="_idContainer118">
			<h1 id="_idParaDest-138"><em class="italic"><a id="_idTextAnchor161"/>Chapter 11</em>: Performance Monitoring</h1>
			<p>In this chapter, you will learn about the important and relevant area of <strong class="bold">Machine Learning (ML)</strong> operations and how to ensure a smooth ride in the production systems developed so far in this book using best practices in the area and known operational patterns. We will understand the concept of operations in ML, and look at metrics for monitoring data quality in ML systems.</p>
			<p>Specifically, we will look at the following sections in this chapter: </p>
			<ul>
				<li>Overview of performance monitoring for ML models</li>
				<li>Monitoring data drift and model performance</li>
				<li>Monitoring target drift</li>
				<li>Infrastructure monitoring and alerting </li>
			</ul>
			<p>We will address some practical reference tools for performance and reliability monitoring of ML systems.</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor162"/>Technical requirements</h1>
			<p>For this chapter, you will need the following prerequisites: </p>
			<ul>
				<li>The latest version of Docker installed on your machine. If you don't already have it installed, please follow the instructions at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
				<li>The latest version of <strong class="source-inline">docker-compose</strong> installed. To do this, please follow the instructions at https://docs.docker.com/compose/install/.</li>
				<li>Access to Git in the command line, which can be installed as described at <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>.</li>
				<li>Access to a Bash terminal (Linux or Windows). </li>
				<li>Access to a browser.</li>
				<li>Python 3.8+ installed.</li>
				<li>The latest version of your ML platform installed locally as described in <a href="B16783_03_Final_SB_epub.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a><em class="italic">, Your Data Science Workbench</em>.</li>
				<li>An AWS account configured to run the MLflow model.</li>
			</ul>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor163"/>Overview of performance monitoring for machine learning models</h1>
			<p>Monitoring is at the cornerstone of reliable ML systems able to consistently unlock the value of data and provide critical feedback for improvement.</p>
			<p>On the monitoring side of ML models<a id="_idIndexMarker388"/>, there are multiple interested parties, and we should take the requirements for monitoring<a id="_idIndexMarker389"/> from the different stakeholders involved. One example of a typical set of stakeholders<a id="_idIndexMarker390"/> is the following:</p>
			<ul>
				<li><strong class="bold">Data scientists</strong>: Their focus regarding monitoring is evaluating model performance and data<a id="_idIndexMarker391"/> drift that might negatively affect that performance.</li>
				<li><strong class="bold">Software engineers</strong>: These stakeholders<a id="_idIndexMarker392"/> want to ensure that they have metrics that assess whether their products have reliable and correct access to the APIs that are serving models.</li>
				<li><strong class="bold">Data engineers</strong>: They want to ensure that the data<a id="_idIndexMarker393"/> pipelines are reliable and pushing data reliably, at the right velocity, and in line with the correct schemas.</li>
				<li><strong class="bold">Business/product stakeholders</strong>: These stakeholders are interested in the core impact of the overall solution on their customer<a id="_idIndexMarker394"/> base. For instance, in a trading platform, they might be most concerned with the profit-to-risk ratio that the overall solution brings to the company. A circuit breaker might be added to the algorithm if the market is in a day of very high volatility or in an atypical situation.</li>
			</ul>
			<p>The most widely used dimensions<a id="_idIndexMarker395"/> of monitoring in the ML industry are the following:</p>
			<ul>
				<li><strong class="bold">Data drift</strong>: This corresponds to significant changes in the input data used either for training or inference<a id="_idIndexMarker396"/> in a model. It might indicate a change of the modeled premise in the real world, which will require the model to be retrained, redeveloped, or even archived if it's no longer suitable. This can be easily detected by monitoring the distributions<a id="_idIndexMarker397"/> of data used for training the model versus the data used for scoring or inference over time.</li>
				<li><strong class="bold">Target drift</strong>: In line with the change of regimens in input data, we often see the same change in the distribution<a id="_idIndexMarker398"/> of outcomes of the model over a period of time. The common periods are months, weeks, or days, and might indicate a significant change in the environment that would require model redevelopment and tweaking.</li>
				<li><strong class="bold">Performance drift</strong>: This involves looking<a id="_idIndexMarker399"/> at whether the performance metrics<a id="_idIndexMarker400"/> such as accuracy for classification problems, or root mean square error, start suffering a gradually worsening over time. This is an indication of an issue with the model requiring investigation and action from the model developer or maintainer.</li>
				<li><strong class="bold">Platform and infrastructure metrics</strong>: This type of metrics<a id="_idIndexMarker401"/> is not directly related to modeling, but with the systems infrastructure that encloses the model. It implies abnormal CPU, memory, network, or disk usage that will certainly affect the ability of the model to deliver value to the business.</li>
				<li><strong class="bold">Business metrics</strong>: Very critical business metrics<a id="_idIndexMarker402"/>, such as the profitability of the models, in some circumstances should be added to the model operations in order<a id="_idIndexMarker403"/> to ensure that the team responsible for the model can monitor the<a id="_idIndexMarker404"/> ability of the model to deliver on its business<a id="_idIndexMarker405"/> premise.</li>
			</ul>
			<p>In the next section, we will look at using a tool that we can integrate with <strong class="bold">MLflow</strong> to monitor<a id="_idIndexMarker406"/> for data drift and check the performance of models.</p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor164"/>Monitoring data drift and model performance </h1>
			<p>In this section, we will run through an example that you can follow in the notebook available in the <strong class="bold">GitHub</strong> repository (at https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter11/model_performance_drifts) of the code of the package. We will run through the process of calculating different types of drift and exploring its integration with MLflow.</p>
			<p>One emergent open source tool in the space of monitoring<a id="_idIndexMarker407"/> model performance<a id="_idIndexMarker408"/> is called <strong class="bold">Evidently</strong> (<a href="https://evidentlyai.com/">https://evidentlyai.com/</a>). Evidently aids<a id="_idIndexMarker409"/> us in analyzing ML models during the production and validation phases. It generates handy reports integrated with <strong class="source-inline">pandas</strong>, JSON, and CSV. It allows us to monitor multiple drifts in ML models and their performance. The GitHub repository for Evidently is available at <a href="https://github.com/evidentlyai/evidently/">https://github.com/evidentlyai/evidently/</a>.</p>
			<p>In this section, we will explore the combination of Evidently with MLflow, in order to monitor data drift and model performances in the next section.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor165"/>Monitoring data drift</h2>
			<p>In this subsection, we will set up <strong class="bold">Evidently</strong> in our environment and understand how to integrate<a id="_idIndexMarker410"/> it. Follow these steps in the GitHub repository (refer to the <em class="italic">Technical requirements</em> section for more details): </p>
			<ol>
				<li>Install <strong class="source-inline">evidently</strong>:<p class="source-code">pip install evidently==0.1.17.dev0</p></li>
				<li>Import the relevant libraries:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn import datasets</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from evidently.dashboard import Dashboard</p><p class="source-code">from evidently.tabs import DataDriftTab, NumTargetDriftTab,CatTargetDriftTab</p></li>
				<li>Get a reference dataset, basically a training dataset. We will add a set of features<a id="_idIndexMarker411"/> to the <strong class="source-inline">pandas</strong> DataFrame so <strong class="source-inline">evidently</strong> will be able to use the feature names in the drift reports:<p class="source-code">reference_data = \</p><p class="source-code">pd.read_csv("training_data.csv", header=None,</p><p class="source-code">            names=[ "day{}".format(i) for i in \</p><p class="source-code">                    range(0,14) ]+["target"] )</p><p>The following <em class="italic">Figure 11.1</em> represents the data structure of the training data that we will be using as the reference dataset:</p><div id="_idContainer110" class="IMG---Figure"><img src="image/image0017.jpg" alt="Figure 11.1 – Sample of the dataset to be used"/></div><p class="figure-caption">Figure 11.1 – Sample of the dataset to be used </p></li>
				<li>In this step, we load the <strong class="source-inline">to_score_input_data.csv</strong> file. This is the file to be scored. Our intention later in this exercise is to calculate the distribution difference between the data in the reference training set and the data to be scored:<p class="source-code">latest_input_data = \</p><p class="source-code">pd.read_csv("to_score_input_data.csv", header=None,</p><p class="source-code">             names=[ "day{}".format(i) for i in \</p><p class="source-code">                     range(0,14) ] )</p></li>
				<li>Execute the data drift<a id="_idIndexMarker412"/> report generation and log into an MLflow run. Basically, what happens in the following code excerpt is the generation of an Evidently dashboard with the reference data and the latest input data. A drift report is calculated and loaded into an MLflow run so it can be actioned and reviewed in further steps:<p class="source-code">EXPERIMENT_NAME="./reports_data_drift"</p><p class="source-code">mlflow.set_experiment(EXPERIMENT_NAME)</p><p class="source-code">with mlflow.start_run():</p><p class="source-code">    drift_dashboard = Dashboard(tabs=[DataDriftTab])</p><p class="source-code">    drift_dashboard.calculate(reference_data,</p><p class="source-code">                              latest_input_data)</p><p class="source-code">    drift_dashboard.save(EXPERIMENT_NAME+"/input_data_drift.html")</p><p class="source-code">    drift_dashboard._save_to_json(EXPERIMENT_NAME+"/input_data_drift.json")</p><p class="source-code">    mlflow.log_artifacts(EXPERIMENT_NAME)</p></li>
				<li>You can run now the notebook code (on the <strong class="source-inline">monitoring_data_drift_performance.ipynb</strong> file) of the previous cells and explore your data drift reports in the MLflow UI over the Artifacts component of the MLflow<a id="_idIndexMarker413"/> run. <em class="italic">Figure 11.2</em> shows that the tool didn't detect any drift among the 14 features, and the distributions are presented accordingly:</li>
			</ol>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/image0028.jpg" alt="Figure 11.2 – Sample of the dataset to be used"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – Sample of the dataset to be used </p>
			<p>In a similar fashion to data drift, we will now look in the next subsection at target drift to uncover other possible issues in our model.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor166"/>Monitoring target drift</h2>
			<p>We will now compare the scored output with the reference training<a id="_idIndexMarker414"/> output to look for possible target drift:</p>
			<ol>
				<li value="1">Get the recently scored dataset:<p class="source-code">production_scored_data = \</p><p class="source-code">pd.read_csv("scored_data.csv", header=None,</p><p class="source-code">            names=[ "day{}".format(i) for i in \</p><p class="source-code">                    range(0,14) ]+["target"] )</p><p class="source-code">bcancer_data_and_target_drift = \</p><p class="source-code">Dashboard(reference_data, production_scored_data,</p><p class="source-code">          tabs=[ CatTargetDriftTab])</p><p class="source-code">bcancer_data_and_target_drift.save('reports/target_drift.html')</p></li>
				<li>Execute the data drift report generation and log the results in MLflow:<p class="source-code">EXPERIMENT_NAME="./reports_target_drift"</p><p class="source-code">mlflow.set_experiment(EXPERIMENT_NAME)</p><p class="source-code">with mlflow.start_run():</p><p class="source-code">    model_target_drift = \</p><p class="source-code">    Dashboard(reference_data, production_scored_data,</p><p class="source-code">              tabs=[CatTargetDriftTab])</p><p class="source-code">    model_target_drift.save(EXPERIMENT_NAME+"/target_drift.html")</p><p class="source-code">    drift_dashboard._save_to_json(EXPERIMENT_NAME+"/target_drift.json")</p><p class="source-code">    mlflow.log_artifacts(EXPERIMENT_NAME)</p></li>
				<li>Explore the target<a id="_idIndexMarker415"/> drift reports on your target. As can be seen in <em class="italic">Figure 11.3</em>, no statistically significant figure on this run was found for target drift. In detecting drift, Evidently does statistical tests using the probability of the data<a id="_idIndexMarker416"/> being from a different distribution represented by the <strong class="bold">p-value</strong> (more details<a id="_idIndexMarker417"/> on this can be found at <a href="https://en.wikipedia.org/wiki/P-value">https://en.wikipedia.org/wiki/P-value</a>). It compares the results between the reference and the current data:<div id="_idContainer112" class="IMG---Figure"><img src="image/image0037.jpg" alt=""/></div><p class="figure-caption">Figure 11.3 – Target data drift for target</p></li>
				<li>As shown in <em class="italic">Figure 11.4</em>, you can drill down further into target drift on a specific feature; in this case, a specific<a id="_idIndexMarker418"/> previous <strong class="bold">day8</strong> to predict the stock price:</li>
			</ol>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/image0048.jpg" alt="Figure 11.4 – Target data drift for our target"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – Target data drift for our target</p>
			<p>After having learned how<a id="_idIndexMarker419"/> to detect drift in the input data, we will now look at how to use Evidently to monitor drift in models.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor167"/>Monitoring model drift</h2>
			<p>Monitoring model drift<a id="_idIndexMarker420"/> is extremely important to ensure that your model is still delivering at its optimal performance level. From this analysis, you can make a decision on whether to retrain your model or even develop a new one from scratch.</p>
			<p>We will now monitor model drift. To do this, you need to execute the following steps:</p>
			<ol>
				<li value="1">Import the relevant libraries:<p class="source-code">import xgboost as xgb</p><p class="source-code">import mlflow</p><p class="source-code">from evidently.tabs import ClassificationPerformanceTab</p></li>
				<li>Get a reference dataset:<p class="source-code">X=reference_data.iloc[:,:-1]</p><p class="source-code">Y=reference_data.iloc[:,-1]</p><p class="source-code">reference, production, y_train, y_test = \</p><p class="source-code">train_test_split(X, Y, test_size=0.33,</p><p class="source-code">                 random_state=4284, stratify=Y)</p><p class="source-code">reference_train = xgb.DMatrix(reference,label=y_train)</p><p class="source-code">dproduction= xgb.DMatrix(production)</p><p class="source-code">dreference=xgb.DMatrix(reference)</p></li>
				<li> Train your model:<p class="source-code">mlflow.xgboost.autolog()</p><p class="source-code">EXPERIMENT_NAME="reports_model_performance"</p><p class="source-code">mlflow.set_experiment(EXPERIMENT_NAME)</p><p class="source-code">with mlflow.start_run() as run:</p><p class="source-code">    model=xgb.train(dtrain=reference_train,params={})</p></li>
				<li>Create a reference prediction<a id="_idIndexMarker421"/> and training predictions:<p class="source-code">    train_proba_predict = model.predict(dreference)</p><p class="source-code">    test_proba_predict = model.predict(dproduction)</p><p class="source-code">    test_predictions = [1. if y_cont &gt; threshold else 0. for y_cont in test_proba_predict]</p><p class="source-code">    train_predictions = [1. if y_cont &gt; threshold else 0. for y_cont in train_proba_predict]</p><p class="source-code">    reference['target'] = y_train</p><p class="source-code">    reference['prediction'] = train_predictions</p><p class="source-code">    production['target'] = y_test</p><p class="source-code">    production['prediction'] = test_predictions</p></li>
				<li>Generate and attach the performance reports to your execution:<p class="source-code">    classification_performance = Dashboard( </p><p class="source-code">                  tabs=[ClassificationPerformanceTab])</p><p class="source-code">    classification_performance.calculate(reference,</p><p class="source-code">                                         production)</p><p class="source-code">    classification_performance.save('.reports/'+EXPERIMENT_NAME+'.html')</p><p class="source-code">    mlflow.log_artifact('.reports/'+EXPERIMENT_NAME+'.html')</p></li>
				<li>Explore your MLflow performance metrics report. By looking at the reports generated, you can check on the <strong class="bold">Reference</strong> metrics that <strong class="bold">Accuracy</strong>, <strong class="bold">Precision</strong>, <strong class="bold">Recall</strong>, and <strong class="bold">F1 metrics</strong>, which are considered the reference metrics based on the training data, have maximum values of <strong class="bold">1</strong>. The current status on the row below is definitely degraded when we test the subset of testing data. This can help you make the call on whether it is sensible for the model to still be in production<a id="_idIndexMarker422"/> with the current <strong class="bold">F1</strong> value:</li>
			</ol>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/image0056.jpg" alt="Figure 11.5 – Target data drift for target"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – Target data drift for target</p>
			<p>After having delved into the details of data drift, target drift, and model performance monitoring, along with how to integrate these functionalities with MLflow, we will now look at the basic principles of monitoring infrastructure, including monitoring and alerting.</p>
			<h1 id="_idParaDest-145">Infrastruc<a id="_idTextAnchor168"/>ture monitoring and alerting </h1>
			<p>The main dimensions of monitoring in ML systems from an infrastructure perspective do not differ from those in traditional software systems. </p>
			<p>In order to illustrate this exact issue, we will leverage the monitoring and alerting tools available<a id="_idIndexMarker423"/> in <strong class="bold">AWS CloudWatch</strong> and <strong class="bold">SageMaker</strong> to illustrate<a id="_idIndexMarker424"/> an example of setting up monitoring and alerting infrastructure. This same mechanism can be set up with tools such as Grafana/Prometheus for on-premises and cloud deployments alike. These monitoring tools achieve similar goals and provide comparable features, so you should choose the most appropriate depending on your environment and cloud provider.</p>
			<p><strong class="bold">AWS CloudWatch</strong> provides a monitoring<a id="_idIndexMarker425"/> and observability solution. It allows you to monitor your applications, respond to system-wide performance changes, optimize resource use, and receive a single view of operational health.</p>
			<p>At a higher level, we can split the infrastructure<a id="_idIndexMarker426"/> monitoring and alerting components into the following three items:</p>
			<ul>
				<li><strong class="bold">Resource metrics</strong>: This refers to metrics<a id="_idIndexMarker427"/> regarding the hardware infrastructure where the system is deployed. The main metrics in this case would be the following:<p>a. <strong class="bold">CPU utilization</strong>: This is basically a unit of utilization<a id="_idIndexMarker428"/> of your processor as a percentage value. This is the general metric available and should be monitored.</p><p>b. <strong class="bold">Memory utilization</strong>: The percentage of memory<a id="_idIndexMarker429"/> in use at the moment by your computing system.</p><p>c. <strong class="bold">Network data transfer</strong>: Network data transfer<a id="_idIndexMarker430"/> refers to the amount of traffic in and out of a specific compute node. It is generally measured in Mb/s. An anomaly might mean that you need to add more nodes to your system or increase capacity.</p><p>d. <strong class="bold">Disk I/O</strong>: This is measured<a id="_idIndexMarker431"/> in the throughput of writes and reads from the disk; it might point to a system under stress that needs to be either scaled or have its performance investigated:</p></li>
			</ul>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/image0065.jpg" alt="Figure 11.6 – SageMaker infrastructure metric examples&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – SageMaker infrastructure metric examples</p>
			<ul>
				<li><strong class="bold">System metrics</strong>: The second pillar of infrastructure monitoring and alerting components refers<a id="_idIndexMarker432"/> to metrics<a id="_idIndexMarker433"/> regarding the system infrastructure where the system is deployed. The main metrics in this case would be the following:<p>a. <strong class="bold">Request throughput</strong>: The number of predictions<a id="_idIndexMarker434"/> served over a second</p><p>b. <strong class="bold">Error rate</strong>: The number of errors<a id="_idIndexMarker435"/> per prediction</p><p>c. <strong class="bold">Request latencies</strong>: The end-to-end time<a id="_idIndexMarker436"/> taken to serve a prediction</p><p>d. <strong class="bold">Validation metrics</strong>: Error metrics<a id="_idIndexMarker437"/> on input data for the request</p><p>A production system such as SageMaker pushes system metrics into AWS CloudWatch to provide real-time system metrics monitoring. AWS CloudWatch has a complete feature set of features to manage, store, and monitor metrics and dashboards:</p></li>
			</ul>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/image0074.jpg" alt="Figure 11.7 – Specify an alarm in AWS CloudWatch"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Specify an alarm in AWS CloudWatch</p>
			<ul>
				<li><strong class="bold">Alerting</strong>: For alerting, we use any of the metrics calculated in the previous section and set<a id="_idIndexMarker438"/> up a threshold that we consider acceptable. The AWS CloudWatch interface allows you to easily set up alerts on the default service metrics and custom metrics. The team responsible for reliability is alerted by CloudWatch sending messages to a corporate chat/Slack, <a id="_idTextAnchor169"/>email address, or mobile phone to allow the team to address or mitigate the incident:</li>
			</ul>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/image0084.jpg" alt="Figure 11.8 – Specify an alarm in AWS CloudWatch"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Specify an alarm in AWS CloudWatch</p>
			<p>You can use the same monitoring<a id="_idIndexMarker439"/> tools to log and monitor all the other metrics that are interrelated with your ML systems. For instance, having an alert for the weekly profit of a ML model is a business metric that should be deployed alongside the core systems metrics of your system.</p>
			<p>After being exposed to an overview of AWS CloudWatch as an example of a tool to implement metrics monitoring and alerting for your ML systems in production, we will explore advanced concepts of MLflow in the last chapter of the book.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor170"/>Summary</h1>
			<p>In this chapter, we introduced the concepts of data drift and target drift, and examined different approaches to performance monitoring in ML systems.</p>
			<p>We started by introducing important concepts in the realm of performance and monitoring, different types of drift and business metrics to monitor, and the use of AWS CloudWatch as a tool to implement monitoring and alerting in real-time systems. </p>
			<p>Performance and monitoring is an important component of our architecture, and it will allow us to conclude an important layer of our ML system's architecture. Now let's delve into the next chapter on advanced topics in MLflow.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor171"/>Further reading</h1>
			<p>In order to further your knowledge, you can consult the documentation at the following links: </p>
			<ul>
				<li><a href="https://www.mlflow.org/docs/latest/projects.html">https://www.mlflow.org/docs/latest/projects.html</a></li>
				<li><a href="https://evidentlyai.com/">https://evidentlyai.com/</a></li>
				<li><a href="https://aws.amazon.com/cloudwatch/">https://aws.amazon.com/cloudwatch/</a></li>
			</ul>
		</div>
	</body></html>