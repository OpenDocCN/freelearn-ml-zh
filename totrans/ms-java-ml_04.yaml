- en: Chapter 4. Semi-Supervised and Active Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章. 半监督学习和主动学习
- en: In [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised
    Learning"), *Practical Approach to Real-World Supervised Learning* and [Chapter
    3](ch03.html "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised
    Machine Learning Techniques*, we discussed two major groups of machine learning
    techniques which apply to opposite situations when it comes to the availability
    of labeled data—one where all target values are known and the other where none
    are. In contrast, the techniques in this chapter address the situation when we
    must analyze and learn from data that is a mix of a small portion with labels
    and a large number of unlabeled instances.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.html "第二章. 实际应用中的监督学习")《实际应用中的监督学习》和[第三章](ch03.html "第三章. 无监督机器学习技术")《无监督机器学习技术》中，我们讨论了两组主要的机器学习技术，这些技术适用于不同情况下标签数据的可用性——一种情况是所有目标值都是已知的，另一种情况是没有任何目标值。相比之下，本章中的技术处理的是我们必须分析并从包含一小部分带标签数据和大量未标记实例的数据中学习的情况。
- en: In speech and image recognition, a vast quantity of data is available, and in
    various forms. However, the cost of labeling or classifying all that data is costly
    and therefore, in practice, the proportion of speech or images that are classified
    to those that are not classified is very small. Similarly, in web text or document
    classification, there are an enormous number of documents on the World Wide Web
    but classifying them based on either topics or contexts requires domain experts—this
    makes the process complex and expensive. In this chapter, we will discuss two
    broad topics that cover the area of "learning from unlabeled data", namely **Semi-Supervised
    Learning** (**SSL**) and Active Learning. We will introduce each of the topics
    and discuss the taxonomy and algorithms associated with each as we did in previous
    chapters. Since the book emphasizes the practical approach, we will discuss tools
    and libraries available for each type of learning. We will then consider real-world
    case studies and demonstrate the techniques that are useful when applying the
    tools in practical situations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在语音和图像识别中，有大量的数据可用，并且以各种形式存在。然而，对所有这些数据进行标记或分类的成本很高，因此，在实践中，被分类的语音或图像的比例与未被分类的比例非常小。同样，在网页文本或文档分类中，互联网上有大量的文档，但根据主题或上下文进行分类需要领域专家——这使得过程复杂且昂贵。在本章中，我们将讨论两个广泛的主题，涵盖“从未标记数据中学习”的领域，即**半监督学习**（**SSL**）和主动学习。我们将介绍每个主题，并像前几章一样讨论与每个主题相关的分类法和算法。由于本书强调实用方法，我们将讨论每种学习类型可用的工具和库。然后，我们将考虑现实世界的案例研究，并展示在实际情况中应用工具时有用的技术。
- en: 'Here is the list of topics that are covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是本章涵盖的主题列表：
- en: 'Semi-Supervised Learning:'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习：
- en: Representation, notation, and assumptions
  id: totrans-5
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示法、符号和假设
- en: 'Semi-Supervised Learning techniques:'
  id: totrans-6
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习技术：
- en: Self-training SSL
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自训练SSL
- en: Co-training SSL
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Co-training SSL
- en: Cluster and label SSL
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类和标签SSL
- en: Transductive graph label propagation
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递式图标签传播
- en: Transductive SVM
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传递式SVM
- en: Case study in Semi-Supervised Learning
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半监督学习案例研究
- en: 'Active Learning:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主动学习：
- en: Representation and notation
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示法和符号
- en: Active Learning scenarios
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主动学习场景
- en: 'Active Learning approaches:'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主动学习方法：
- en: Uncertainty sampling
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不确定性采样
- en: Least confident sampling
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最不自信采样
- en: Smallest margin sampling
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小边缘采样
- en: Label entropy sampling
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签熵采样
- en: 'Version space sampling:'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本空间采样：
- en: Query by disagreement
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意见分歧查询
- en: Query by committee
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 委员会查询
- en: 'Data distribution sampling:'
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分布采样：
- en: Expected model change
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预期模型变化
- en: Expected error reduction
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预期误差减少
- en: Variance reduction
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方差减少
- en: Density weighted methods
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密度加权方法
- en: Case study in Active Learning
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主动学习案例研究
- en: Semi-supervised learning
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 半监督学习
- en: 'The idea behind semi-supervised learning is to learn from labeled and unlabeled
    data to improve the predictive power of the models. The notion is explained with
    a simple illustration, *Figure 1*, which shows that when a large amount of unlabeled
    data is available, for example, HTML documents on the web, the expert can classify
    a few of them into known categories such as sports, news, entertainment, and so
    on. This small set of labeled data together with the large unlabeled dataset can
    then be used by semi-supervised learning techniques to learn models. Thus, using
    the knowledge of both labeled and unlabeled data, the model can classify unseen
    documents in the future. In contrast, supervised learning uses labeled data only:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习背后的思想是从标记和无标签数据中学习，以提高模型的预测能力。这一概念通过一个简单的插图，*图1*，进行了说明，它显示当有大量无标签数据可用时，例如网络上的HTML文档，专家可以将其中一些分类到已知的类别，如体育、新闻、娱乐等。这个小的标记数据集与大量的无标签数据集一起，可以用于半监督学习技术来学习模型。因此，利用标记和无标签数据的知识，模型可以分类未来未见过的文档。相比之下，监督学习仅使用标记数据：
- en: '![Semi-supervised learning](img/B05137_04_003.jpg)![Semi-supervised learning](img/B05137_04_005.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![半监督学习](img/B05137_04_003.jpg)![半监督学习](img/B05137_04_005.jpg)'
- en: Figure 1\. Semi-Supervised Learning process (bottom) contrasted with Supervised
    Learning (top) using classification of web documents as an example. The main difference
    is the amount of labeled data available for learning, highlighted by the qualifier
    "small" in the semi-supervised case.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 使用网络文档分类作为示例，展示了半监督学习过程（底部）与监督学习（顶部）的对比。主要区别在于可用于学习的标记数据量，在半监督学习中通过“少量”这一限定词进行了强调。
- en: Representation, notation, and assumptions
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示法、符号和假设
- en: As before, we will introduce the notation we use in this chapter. The dataset
    *D* consists of individual data instances represented as **x**, which is also
    represented as a set {**x**[1], **x**[2],…**x**[n]}, the set of data instances
    without labels. The labels associated with these data instances are {*y*[1], *y*[2],
    … *y*[n]}. The entire labeled dataset can be represented as paired elements in
    a set, as given by *D* = {(**x**[1], *y*[1]), (**x**2,*y*[2]), … (**x**[n], *y*[n])}
    where **x**[i] ∈ ℝ^d. In semi-supervised learning, we divide the dataset *D* further
    into two sets *U* and *L* for unlabeled and labeled data respectively.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们将介绍本章中使用的符号。数据集 *D* 由单个数据实例组成，表示为 **x**，它也以集合 {**x**[1], **x**[2],…**x**[n]}
    的形式表示，这是没有标签的数据实例集合。与这些数据实例相关联的标签是 {*y*[1], *y*[2], … *y*[n]}。整个标记数据集可以表示为集合中的配对元素，如
    *D* = {(**x**[1], *y*[1]), (**x**2,*y*[2]), … (**x**[n], *y*[n])}，其中 **x**[i]
    ∈ ℝ^d。在半监督学习中，我们将数据集 *D* 进一步分为两个集合 *U* 和 *L*，分别用于无标签数据和标记数据。
- en: The labeled data ![Representation, notation, and assumptions](img/B05137_04_015.jpg)
    consists of all labeled data with known outcomes {y[1], y[2], .. y[l]}. The unlabeled
    data ![Representation, notation, and assumptions](img/B05137_04_017.jpg) is the
    dataset where the outcomes are not known. |*U*| > |*L*| .
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 标记数据 ![表示法、符号和假设](img/B05137_04_015.jpg) 由所有已知结果 {y[1], y[2], .. y[l]} 的标记数据组成。无标签数据
    ![表示法、符号和假设](img/B05137_04_017.jpg) 是结果未知的数据集。|*U*| > |*L*|。
- en: Inductive semi-supervised learning consists of a set of techniques, which, given
    the training set *D* with labeled data ![Representation, notation, and assumptions](img/B05137_04_015.jpg)
    and unlabeled data ![Representation, notation, and assumptions](img/B05137_04_017.jpg),
    learns a model represented as ![Representation, notation, and assumptions](img/B05137_04_021.jpg)
    so that the model *f* can be a good predictor on unseen data beyond the training
    unlabeled data *U*. It "induces" a model that can be used just like supervised
    learning algorithms to predict on unseen instances.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳半监督学习包括一系列技术，这些技术给定带有标记数据 ![表示法、符号和假设](img/B05137_04_015.jpg) 和无标签数据 ![表示法、符号和假设](img/B05137_04_017.jpg)
    的训练集 *D*，学习一个表示为 ![表示法、符号和假设](img/B05137_04_021.jpg) 的模型，使得模型 *f* 可以在训练无标签数据 *U*
    之外的未见数据上成为一个好的预测器。它“归纳”出一个模型，可以像监督学习算法一样用于预测未见实例。
- en: Transductive semi-supervised learning consists of a set of techniques, which,
    given the training set *D*, learns a model ![Representation, notation, and assumptions](img/B05137_04_024.jpg)
    that makes predictions on unlabeled data alone. It is not required to perform
    on unseen future instances and hence is a simpler form of SSL than inductive based
    learning.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 传导式半监督学习由一系列技术组成，给定训练集 *D*，它学习一个模型 ![表示、符号和假设](img/B05137_04_024.jpg)，该模型仅对未标记数据进行预测。它不需要在未见未来的实例上执行，因此是比基于归纳学习更简单的SSL形式。
- en: 'Some of the assumptions made in the semi-supervised learning algorithms that
    should hold true for these types of learning to be successful are noted in the
    following list. For SSL to work, one or more of these assumptions must be true:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在半监督学习算法中，以下列出的某些假设对于这些类型的学习成功至关重要。为了使SSL（半监督学习）工作，以下假设中必须有一个或多个是真实的：
- en: '**Semi-supervised smoothness**: In simple terms, if two points are "close"
    in terms of density or distance, then their labels agree. Conversely, if two points
    are separated and in different density regions, then their labels need not agree.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督平滑性**：简单来说，如果两个点在密度或距离方面“接近”，则它们的标签是一致的。相反，如果两个点分离且位于不同的密度区域，则它们的标签不需要一致。'
- en: '**Cluster togetherness**: If the data instances of classes tend to form a cluster,
    then the unlabeled data can aid the clustering algorithm to find better clusters.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类共存性**：如果类别的数据实例倾向于形成簇，那么未标记数据可以帮助聚类算法找到更好的簇。'
- en: '**Manifold togetherness**: In many real-world datasets, the high-dimensional
    data lies in a low-dimensional manifold, enabling learning algorithms to overcome
    the curse of dimensionality. If this is true in the given dataset, the unlabeled
    data also maps to the manifold and can improve the learning.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流形共存性**：在许多现实世界的数据集中，高维数据位于低维流形中，这使得学习算法能够克服维度灾难。如果给定数据集中存在这种情况，未标记的数据也会映射到流形上，从而可以改善学习。'
- en: Semi-supervised learning techniques
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半监督学习技术
- en: 'In this section, we will describe different SSL techniques, and some accompanying
    algorithms. We will use the same structure as in previous chapters and describe
    each method in three subsections: *Inputs and outputs*, *How does it work?*, and
    *Advantages and limitations*.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将描述不同的SSL（半监督学习）技术以及一些伴随的算法。我们将使用与之前章节相同的结构，并在三个子节中描述每种方法：*输入和输出*、*如何工作*和*优势和局限性*。
- en: Self-training SSL
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自训练SSL
- en: 'Self-training is the simplest form of SSL, where we perform a simple iterative
    process of imputing the data from the unlabeled set by applying the model learned
    from the labeled set (*References* [1]):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 自训练是SSL（半监督学习）最简单的一种形式，其中我们通过应用从标记集学习到的模型对未标记集的数据进行简单迭代填充过程（*参考文献* [1]）：
- en: '![Self-training SSL](img/B05137_04_025.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![自训练SSL](img/B05137_04_025.jpg)'
- en: Figure 2\. Self-training SSL in binary classification with some labeled data
    shown with blue rectangles and yellow circles. After various iterations, the unlabeled
    data gets mapped to the respective classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 使用一些用蓝色矩形和黄色圆圈表示的标记数据在二元分类中进行自训练SSL。经过多次迭代后，未标记数据被映射到相应的类别。
- en: Inputs and outputs
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: The input is training data with a small amount of labeled and a large amount
    of unlabeled data. A base classifier, either linear or non-linear, such as Naïve
    Bayes, KNN, Decision Tree, or other, is provided along with the hyper-parameters
    needed for each of the algorithms. The constraints on data types will be similar
    to the base learner. Stopping conditions such as *maximum iterations reached*
    or *unlabeled data exhausted* are choices that must be made as well. Often, we
    use base learners which give probabilities or ranks to the outputs. As output,
    this technique generates models that can be used for performing predictions on
    unseen datasets other than the unlabeled data provided.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是带有少量标记和大量未标记数据的训练数据。提供一个基分类器，无论是线性的还是非线性的，例如朴素贝叶斯、KNN、决策树或其他，以及每个算法所需的超参数。数据类型上的约束将与基学习器相似。还需要做出停止条件的选择，例如“达到最大迭代次数”或“未标记数据耗尽”。通常，我们使用基学习器，这些学习器会对输出给出概率或排名。作为输出，这种技术生成可以用于对未标记数据集以外的未见数据集进行预测的模型。
- en: How does it work?
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'The entire algorithm can be summarized as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 整个算法可以总结如下：
- en: 'While stopping criteria not reached:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在停止标准未达到的情况下：
- en: Train the classifier model ![How does it work?](img/B05137_04_021.jpg) with
    labeled data *L*
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标记数据 *L* 训练分类器模型 ![如何工作？](img/B05137_04_021.jpg)
- en: Apply the classifier model *f* on unlabeled data *U*
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在未标记数据 *U* 上应用分类器模型 *f*
- en: Choose *k* most confident predictions from *U* as set *L*[u]
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *U* 中选择 *k* 个最自信的预测作为集合 *L*[u]
- en: Augment the labeled data with the k data points *L = L* *∪* *L*[u]
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 k 个数据点 *L = L* *∪* *L*[u] 增强标记数据
- en: Repeat all the steps under 2.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 2 下的所有步骤重复。
- en: In the abstract, self-training can be seen as an expectation maximization process
    applied to a semi-supervised setting. The process of training the classifier model
    is finding the parameter θ using MLE or MAP. Computing the labels using the learned
    model is similar to the *EXPECTATION* step where ![How does it work?](img/B05137_04_031.jpg)
    is estimating the label from *U* given the parameter θ. The iterative next step
    of learning the model with augmented labels is akin to the *MAXIMIZATION* step
    where the new parameter is tuned to *θ'*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在概述中，自训练可以看作是将期望最大化过程应用于半监督设置。训练分类器模型的过程是使用最大似然估计（MLE）或最大后验概率估计（MAP）找到参数 θ。使用学习到的模型计算标签类似于
    *期望* 步骤，其中 ![如何工作？](img/B05137_04_031.jpg) 是根据参数 θ 从 *U* 中估计标签。学习带有增强标签的模型的迭代下一步类似于
    *最大化* 步骤，其中新参数被调整到 *θ'*。
- en: Advantages and limitations
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: Simple, works with most supervised learning techniques.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单，与大多数监督学习技术兼容。
- en: Outliers and noise can cause mistakes in predictions to be reinforced and the
    technique to degrade.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值和噪声可能导致预测错误被强化，技术退化。
- en: Co-training SSL or multi-view SSL
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 协同训练 SSL 或多视图 SSL
- en: 'Co-training based SSL involves learning from two different "views" of the same
    data. It is a special case of multi-view SS (References [2]). Each view can be
    considered as a feature set of the point capturing some domain knowledge and is
    orthogonal to the other view. For example, a web documents dataset can be considered
    to have two views: one view is features representing the text and the other view
    is features representing hyperlinks to other documents. The assumption is that
    there is enough data for each view and learning from each view improves the overall
    labeling process. In datasets where such partitions of features are not possible,
    splitting features randomly into disjoint sets forms the views.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于协同训练的 SSL 涉及从相同数据的不同“视图”中进行学习。它是多视图 SS（参考文献 [2]）的一个特例。每个视图都可以被认为是一个捕获某些领域知识的点特征集，并且与其他视图正交。例如，一个网页文档数据集可以被认为有两个视图：一个视图是代表文本的特征，另一个视图是代表指向其他文档的超链接的特征。假设每个视图都有足够的数据，并且从每个视图中进行学习可以提高整体标记过程。在无法进行此类特征划分的数据集中，将特征随机分割成不相交的集合形成视图。
- en: Inputs and outputs
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Input is training data with a few labeled and a large number of unlabeled data.
    In addition to providing the data points, there are feature sets corresponding
    to each view and the assumption is that these feature sets are not overlapping
    and solve different classification problems. A base classifier, linear or non-linear,
    such as Naïve Bayes, KNN, Decision Tree, or any other, is selected along with
    the hyper-parameters needed for each of the algorithms. As output, this method
    generates models that can be used for performing predictions on unseen datasets
    other than the unlabeled data provided.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是带有少量标记和大量未标记数据的训练数据。除了提供数据点外，还有对应于每个视图的特征集，并且假设这些特征集不重叠且解决不同的分类问题。选择一个基分类器，如朴素贝叶斯、KNN、决策树或任何其他，以及每个算法所需的超参数。作为输出，此方法生成可用于对未标记数据以外的未见数据集进行预测的模型。
- en: How does it work?
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'We will demonstrate the algorithm using two views of the data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用数据的两个视图来演示该算法：
- en: Initialize the data as ![How does it work?](img/B05137_04_015.jpg) labeled and
    ![How does it work?](img/B05137_04_017.jpg) unlabeled. Each data point has two
    views **x = [x****¹****,x****²****]** and *L = [L**¹**,L**2**]*.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据初始化为 ![如何工作？](img/B05137_04_015.jpg) 标记和 ![如何工作？](img/B05137_04_017.jpg)
    未标记。每个数据点有两个视图 **x = [x****¹****,x****²****]** 和 *L = [L**¹**,L**2**]*。
- en: 'While stopping criteria not reached:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当未达到停止标准时：
- en: Train the classifier models ![How does it work?](img/B05137_04_035.jpg) and
    ![How does it work?](img/B05137_04_036.jpg) with labeled data *L*1 and *L*2 respectively.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标记数据 *L*1 和 *L*2 分别训练分类器模型 ![如何工作？](img/B05137_04_035.jpg) 和 ![如何工作？](img/B05137_04_036.jpg)。
- en: Apply the classifier models *f*¹ and *f*² on unlabeled data *U* using their
    own features.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用它们自己的特征在未标记数据 *U* 上应用分类器模型 *f*¹ 和 *f*²。
- en: Choose *k* the most confident predictions from *U*, applying *f*¹ and *f*² as
    set *L*[u]¹ and *L*[u]² respectively.
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *U* 中选择 *k* 个最自信的预测，应用 *f*¹ 和 *f*² 作为集合 *L*[u]¹ 和 *L*[u]² 分别。
- en: Augment the labeled data with the *k* data points *L*¹ = *L*¹ ∪ *L*[u]¹ and
    *L*² = *L*² ∪ *L*[u]²
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *k* 个数据点 *L*¹ = *L*¹ ∪ *L*[u]¹ 和 *L*² = *L*² ∪ *L*[u]² 增强标记数据。
- en: Repeat all the steps under 2.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 2. 下重复所有步骤。
- en: Advantages and limitations
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: When the features have different aspects or a mix of different domains, co-training
    becomes more beneficial than simple self-training
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征具有不同的方面或不同领域的混合时，协同训练比简单的自训练更有利。
- en: The necessary and sufficient condition of having orthogonal views and ability
    to learn from them poses challenges for the generality of the technique
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有正交视图并从中学习的能力的必要和充分条件对技术的通用性提出了挑战。
- en: Cluster and label SSL
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类和标签 SSL
- en: This technique, like self-training, is quiet generic and applicable to domains
    and datasets where the clustering supposition mentioned in the assumptions section
    holds true (References [3]).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术与自训练类似，相当通用，适用于假设部分中提到的聚类假设成立（参考文献 [3]）的领域和数据集。
- en: Inputs and outputs
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Input is training data with a few labeled and a large number of unlabeled instances.
    A clustering algorithm and its parameters along with a classification algorithm
    with its parameters constitute additional inputs. The technique generates a classification
    model that can help predict the classes of unseen data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是带有少量标记实例和大量未标记实例的训练数据。聚类算法及其参数以及分类算法及其参数是额外的输入。该技术生成一个分类模型，可以帮助预测未见数据的类别。
- en: How does it work?
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'The abstract algorithm can be given as:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象算法可以表示为：
- en: Initialize data as ![How does it work?](img/B05137_04_015.jpg) labeled and ![How
    does it work?](img/B05137_04_017.jpg) unlabeled.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据初始化为 ![如何工作？](img/B05137_04_015.jpg) 标记的和 ![如何工作？](img/B05137_04_017.jpg)
    未标记的。
- en: Cluster the entire data, both labeled and unlabeled using the clustering algorithm.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用聚类算法对整个数据集进行聚类，包括标记的和未标记的数据。
- en: For each cluster let *S* be the set of labeled instances drawn from set *L*.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个簇，让 *S* 是从集合 *L* 中抽取的标记实例的集合。
- en: Learn a supervised model from *S*, *f*[s] = *L*[s].
  id: totrans-90
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *S* 中学习一个监督模型，*f*[s] = *L*[s]。
- en: Apply the model *f*[s] and classify unlabeled instances for each cluster using
    the preceding model.
  id: totrans-91
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用模型 *f*[s] 并使用前面的模型对每个簇中的未标记实例进行分类。
- en: Since all the unlabeled instances ![How does it work?](img/B05137_04_017.jpg)
    get assigned a label by the preceding process, a supervised classification model
    is run on the entire set.![How does it work?](img/B05137_04_048.jpg)
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于所有未标记的实例 ![如何工作？](img/B05137_04_017.jpg) 都通过前面的过程分配了标签，因此在整个集合上运行监督分类模型。[![如何工作？](img/B05137_04_048.jpg)]
- en: Figure 3\. Clustering and label SSL – clustering followed by classification
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 3\. 聚类和标签 SSL – 聚类后进行分类
- en: Advantages and limitations
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: Works very well when the cluster assumption holds true and the choice of clustering
    algorithm and parameters are correct
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当聚类假设成立且聚类算法和参数选择正确时，效果非常好。
- en: Large number of parameters and choices make this an unwieldy technique in many
    real-world problems
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大量的参数和选择使得这在许多现实世界问题中成为一个难以驾驭的技术。
- en: Transductive graph label propagation
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传递性图标签传播
- en: The key idea behind graph-based methods is to represent every instance in the
    dataset, labeled and unlabeled, as a node and compute the edges as some form of
    "similarity" between them. Known labels are used to propagate labels in the unlabeled
    data using the basic concepts of label smoothness as discussed in the assumptions
    section, that is, similar data points will lie "close" o each other graphically
    (*References* [4]).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图的方法背后的关键思想是将数据集中的每个实例（标记的和未标记的）表示为一个节点，并计算它们之间的“相似性”作为某种形式的“相似性”。使用假设部分中讨论的基本概念（即，相似的数据点在图形上会“靠近”）的已知标签来传播未标记数据中的标签（参考文献
    [4]）。
- en: Figure 4 shows how the similarity indicated by the thickness of the arrow from
    the first data point to the last varies when the handwritten digit pattern changes.
    Knowing the first label, the label propagation can effectively label the next
    three digits due to the similarity in features while the last digit, though labeled
    the same, has a lower similarity as compared to the first three.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4 展示了当手写数字模式变化时，从第一个数据点到最后一个数据点的箭头粗细所表示的相似性如何变化。知道第一个标签后，由于特征相似性，标签传播可以有效地标记接下来的三个数字，而最后一个数字，尽管被标记为相同，但与前面三个相比，相似性较低。
- en: '![Transductive graph label propagation](img/B05137_04_050.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![传输图标签传播](img/B05137_04_050.jpg)'
- en: Figure 4\. Transductive graph label propagation – classification of hand-written
    digits. Leftmost and rightmost images are labeled, others are unlabeled. Arrow
    thickness is a visual measure of similarity to labeled digit "2" on the left.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 传输图标签传播 – 手写数字的分类。最左端和最右端的图像已标记，其他图像未标记。箭头粗细是相对于左侧标记数字 "2" 的相似性的视觉度量。
- en: Inputs and outputs
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Input is training data with a few labeled and a large number of unlabeled data.
    The graph weighting or similarity computing method, such as k-nearest weighting,
    Gaussian decaying distance, or ϵ-radius method is chosen. Output is the labeled
    set for the entire data; it generally doesn't build inductive models like the
    algorithms seen previously.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是带有少量标记和大量未标记数据的训练数据。选择图加权或相似性计算方法，如 k-最近邻加权、高斯衰减距离或 ϵ-半径方法。输出是整个数据的标记集；它通常不构建如先前算法中看到的归纳模型。
- en: How does it work?
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'The general label propagation method follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通用标签传播方法如下：
- en: 'Build a graph *g = (V,E)* where:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个图 *g = (V,E)*，其中：
- en: Vertices *V = {1, 2…n}* correspond to data belonging to both labeled set *L*
    and unlabeled set *U*.
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶点 *V = {1, 2…n}* 对应于既属于标记集 *L* 又属于未标记集 *U* 的数据。
- en: Edges *E* are weight matrices **W**, such that **W**[i,j] represents similarity
    in some form between two data points **x**[i], **x**[j].
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边 *E* 是权重矩阵 **W**，其中 **W**[i,j] 表示两个数据点 **x**[i]，**x**[j] 之间的某种形式的相似性。
- en: Compute the diagonal degree matrix **D** by ![How does it work?](img/B05137_04_059.jpg).
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 ![如何工作？](img/B05137_04_059.jpg) 计算对角度矩阵 **D**。
- en: Assume the labeled set is binary and has ![How does it work?](img/B05137_04_060.jpg).
    Initialize the labels of all unlabeled data to be 0\. ![How does it work?](img/B05137_04_061.jpg)
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设标记集是二元的，并且有 ![如何工作？](img/B05137_04_060.jpg)。将所有未标记数据的标签初始化为 0\. ![如何工作？](img/B05137_04_061.jpg)
- en: 'Iterate at *t* = 0:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *t* = 0 时迭代：
- en: '![How does it work?](img/B05137_04_063.jpg)'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_04_063.jpg)'
- en: '![How does it work?](img/B05137_04_064.jpg) (reset the labels of labeled instances
    back to the original)'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_04_064.jpg)（将标记实例的标签重置为原始值）'
- en: Go back to step 4, until convergence ![How does it work?](img/B05137_04_065.jpg)
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回步骤 4，直到收敛 ![如何工作？](img/B05137_04_065.jpg)
- en: Label the unlabeled points ![How does it work?](img/B05137_04_017.jpg) using
    the convergence labels ![How does it work?](img/B05137_04_065.jpg).
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用收敛标签 ![如何工作？](img/B05137_04_017.jpg) 标记未标记的点 ![如何工作？](img/B05137_04_065.jpg)。
- en: There are many variations based on similarity, optimization selected in iterations,
    and so on.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 基于相似性、迭代中选择的优化等有许多变体。
- en: Advantages and limitations
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: The graph-based semi-supervised learning methods are costly in terms of computations—generally
    O(n³) where *n* is the number of instances. Though speeding and caching techniques
    help, the computational cost over large data makes it infeasible in many real-world
    data situations.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于图的半监督学习方法在计算上代价高昂——通常为 O(n³)，其中 *n* 是实例的数量。尽管加速和缓存技术有所帮助，但在大量数据上的计算成本使得在许多实际数据情况下不可行。
- en: The transductive nature makes it difficult for practical purposes where models
    need to be induced for unseen data. There are extensions such as Harmonic Mixtures,
    and so on, which address these concerns.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传输性质使得它在需要为未见数据诱导模型的实际应用中难以使用。有如谐波混合等扩展，可以解决这些问题。
- en: Transductive SVM (TSVM)
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 传输支持向量机（TSVM）
- en: Transductive SVM is one of the oldest and most popular transductive semi-supervised
    learnig methods, introduced by Vapnik (*References* [5]). The key principle is
    that unlabeled data along with labeled data can help find the decision boundary
    using concepts of large margins. The underlying principle is that the decision
    boundaries normally don't lie in high density regions!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Transductive SVM是其中最古老且最受欢迎的归纳半监督学习方法之一，由Vapnik提出（*参考文献* [5]）。其关键原则是，未标记数据与标记数据一起可以帮助使用大间隔的概念找到决策边界。其基本原理是，决策边界通常不位于高密度区域！
- en: Inputs and outputs
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Input is training data with few labeled and a large number of unlabeled data.
    Input has to be numeric features for TSVM computations. The choice of kernels,
    kernel parameters, and cost factors, which are all SVM-based parameters, are also
    input variables. The output is labels for the unlabeled dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是带有少量标记和大量未标记数据的训练数据。对于TSVM计算，输入必须是数值特征。核的选择、核参数和成本因子，这些都是基于SVM的参数，也是输入变量。输出是对未标记数据集的标签。
- en: How does it work?
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: Generally, SVM works as an optimization problem in the labeled hard boundary
    SVM formulated in terms of weight vector **w** and the bias *b* ![How does it
    work?](img/B05137_04_070.jpg) subject to ![How does it work?](img/B05137_04_072.jpg)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，SVM作为权重向量**w**和偏置*b*![如何工作？](img/B05137_04_070.jpg)的优化问题，在标记的硬边界SVM中用这些术语表述，受![如何工作？](img/B05137_04_072.jpg)约束。
- en: Initialize the data as ![How does it work?](img/B05137_04_015.jpg) labeled and
    ![How does it work?](img/B05137_04_017.jpg) unlabeled.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据初始化为![如何工作？](img/B05137_04_015.jpg)标记的和![如何工作？](img/B05137_04_017.jpg)未标记的。
- en: In TSVM, the equation is modified as follows:![How does it work?](img/B05137_04_073.jpg)
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在TSVM中，方程被修改如下：![如何工作？](img/B05137_04_073.jpg)
- en: 'This is subject to the following condition:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于以下条件：
- en: '![How does it work?](img/B05137_04_074.jpg)![How does it work?](img/B05137_04_075.jpg)![How
    does it work?](img/B05137_04_076.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_04_074.jpg)![如何工作？](img/B05137_04_075.jpg)![如何工作？](img/B05137_04_076.jpg)'
- en: This is exactly like inductive SVM but using only labeled data. When we constrain
    the unlabeled data to conform to the side of the hyperplane of labeled data in
    order to maximize the margin, it results in unlabeled data being labeled with
    maximum margin separation! By adding the penalty factor to the constraints or
    replacing the dot product in the input space with kernels as in inductive SVM,
    complex non-linear noisy datasets can be labeled from unlabeled data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这与归纳SVM非常相似，但仅使用标记数据。当我们约束未标记数据以符合标记数据的超平面一侧，以最大化间隔时，它会导致未标记数据以最大间隔分离被标记！通过向约束中添加惩罚因子或用核替换输入空间中的点积，就像归纳SVM中那样，可以从未标记数据中标记复杂的非线性噪声数据集。
- en: '*Figure 5* illustrates the concept of TSVM in comparison with inductive SVM
    run on labeled data only and why TSVM can find better decision boundaries using
    the unlabeled datasets. The unlabeled datasets on either side of hyperplane are
    closer to their respective classes, thus helping find better margin separators.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5*说明了TSVM的概念，与仅对标记数据进行归纳SVM运行的概念进行比较，以及为什么TSVM可以使用未标记数据集找到更好的决策边界。超平面两侧的未标记数据集更接近其各自的类别，从而有助于找到更好的间隔分离器。'
- en: '![How does it work?](img/B05137_04_077.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_04_077.jpg)'
- en: Figure 5\. Transductive SVM
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图5. Transductive SVM
- en: Advantages and limitations
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性：
- en: TSVMs can work very well in linear or non-linear datasets given noiseless labeled
    data.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定无噪声标记数据的情况下，TSVM可以在线性或非线性数据集上非常有效地工作。
- en: TSVMs have the same issues in finding hyper-parameters and tuning them to get
    the best results as inductive SVMs.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TSVM在寻找超参数并调整它们以获得最佳结果方面与归纳SVM有相同的问题。
- en: Case study in semi-supervised learning
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 半监督学习案例研究
- en: For this case study, we use another well-studied dataset from the UCI Repository,
    the Wisconsin Breast Cancer dataset. In the first part of the experiment, we demonstrate
    how to apply the Transductive SVM technique of semi-supervised learning using
    the open-source library called `JKernelMachines`. We choose the SVMLight algorithm
    and a Gaussian kernel for this technique.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个案例研究，我们使用UCI存储库中另一个经过充分研究的数据集，即威斯康星州乳腺癌数据集。在实验的第一部分，我们展示了如何使用开源库`JKernelMachines`应用半监督学习的Transductive
    SVM技术。我们选择SVMLight算法和Gaussian核来使用这项技术。
- en: In the second part, we use KEEL, a GUI-based framework and compare results from
    several evolutionary learning based algorithms using the UCI Breast Cancer dataset.
    The tools, methodology, and evaluation measures are described in the following
    subsections.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们使用基于 GUI 的框架 KEEL，并使用 UCI 乳腺癌数据集比较了基于进化学习算法的结果。工具、方法和评估措施将在以下子节中描述。
- en: Tools and software
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工具和软件
- en: The two open source Java tools used in the semi-supervised learning case study
    are `JKernelMachines`, a Transductive SVM, and KEEL, a GUI-based tool that uses
    evolutionary algorithms for learning.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在半监督学习案例研究中使用的两个开源 Java 工具是 `JKernelMachines`，一个 Transductive SVM，以及 KEEL，一个基于
    GUI 的工具，它使用进化算法进行学习。
- en: Note
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**JKernelMachines (Transductive SVM)**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**JKernelMachines (Transductive SVM**) '
- en: '`JKernelMachines` is a pure Java library that provides an efficient framework
    for using and rapidly developing specialized kernels. Kernels are similarity functions
    used in SVMs. `JKernelMachines` provides kernel implementations defined on structured
    data in addition to standard kernels on vector data such as Linear and Gaussian.
    In particular, it offers a combination of kernels, kernels defined over lists,
    and kernels with various caching strategies. The library also contains SVM optimization
    algorithm implementations including LaSVM and One-Class SVM using SMO. The creators
    of the library report that the results of JKernelMachines on some common UCI repository
    datasets are comparable to or better than the Weka library.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`JKernelMachines` 是一个纯 Java 库，它提供了一个高效的平台用于使用和快速开发专门的核函数。核函数是 SVM 中使用的相似性函数。`JKernelMachines`
    除了提供标准核函数（如线性核和高斯核）外，还提供了在结构化数据上定义的核函数实现。特别地，它提供了一系列核函数的组合，列表上的核函数，以及具有各种缓存策略的核函数。该库还包含了
    SVM 优化算法的实现，包括 LaSVM 和 One-Class SVM 使用 SMO。库的制作者报告称，JKernelMachines 在一些常见的 UCI
    仓库数据集上的结果与 Weka 库相当或更好。'
- en: 'The example of loading data and running Transductive SVM using `JKernelMachines`
    is given here:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出了使用 `JKernelMachines` 加载数据和运行 Transductive SVM 的示例：
- en: '[PRE0]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the second approach, we use KEEL with the same dataset.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种方法中，我们使用与相同数据集的 KEEL。
- en: Note
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**KEEL**'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**KEEL**'
- en: '**KEEL** (**Knowledge Extraction based on Evolutionary Learning**) is a non-commercial
    (GPLv3) Java tool with GUI which enables users to analyze the behavior of evolutionary
    learning for a variety of data mining problems, including regression, classification,
    and unsupervised learning. It relieves users from the burden of programming sophisticated
    evolutionary algorithms and allows them to focus on new learning models created
    using the toolkit. KEEL is intended to meet the needs of researchers as well as
    students.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**KEEL** （**基于进化学习进行知识提取**）是一个非商业（GPLv3）的 Java 工具，具有 GUI，它使用户能够分析各种数据挖掘问题的进化学习行为，包括回归、分类和无监督学习。它减轻了用户编写复杂的进化算法的负担，并允许他们专注于使用工具包创建的新学习模型。KEEL旨在满足研究人员和学生的需求。'
- en: KEEL contains algorithms for data preprocessing and post-processing as well
    as statistical libraries, and a Knowledge Extraction Algorithms Library which
    incorporates multiple evolutionary learning algorithms with classical learning
    techniques.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: KEEL 包含数据预处理和后处理的算法，以及统计库，还有一个知识提取算法库，该库结合了多种进化学习算法和经典学习技术。
- en: 'The GUI wizard included in the tool offers different functional components
    for each stage of the pipeline, including:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 工具中包含的 GUI 向导为管道的每个阶段提供不同的功能组件，包括：
- en: 'Data management: Import, export of data, data transformation, visualization,
    and so on'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管理：数据的导入、导出、数据转换、可视化等
- en: 'Experiment design: Selection of classifier, estimator, unsupervised techniques,
    validation method, and so on'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验设计：选择分类器、估计器、无监督技术、验证方法等
- en: 'SSL experiments: Transductive and inductive classification (see image of off-line
    method for SSL experiment design in this section)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSL 实验：归纳和演绎分类（参见本节中 SSL 实验设计离线方法的图像）
- en: 'Statistical analysis: This provides tests for pair-wise and multiple comparisons,
    parametric, and non-parametric procedures.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计分析：这提供了成对和多重比较的测试，参数和非参数程序。
- en: For more info, visit [http://sci2s.ugr.es/keel/](http://sci2s.ugr.es/keel/)
    and [http://sci2s.ugr.es/keel/pdf/keel/articulo/Alcalaetal-SoftComputing-Keel1.0.pdf](http://sci2s.ugr.es/keel/pdf/keel/articulo/Alcalaetal-SoftComputing-Keel1.0.pdf).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 想要了解更多信息，请访问 [http://sci2s.ugr.es/keel/](http://sci2s.ugr.es/keel/) 和 [http://sci2s.ugr.es/keel/pdf/keel/articulo/Alcalaetal-SoftComputing-Keel1.0.pdf](http://sci2s.ugr.es/keel/pdf/keel/articulo/Alcalaetal-SoftComputing-Keel1.0.pdf)。
- en: '![Tools and software](img/B05137_04_078.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![工具和软件](img/B05137_04_078.jpg)'
- en: 'Figure 6: KEEL – wizard-based graphical interface'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：KEEL – 基于向导的图形界面
- en: Business problem
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 商业问题
- en: Breast cancer is the top cancer in women worldwide and is increasing particularly
    in developing countries where the majority of cases are diagnosed in late stages.
    Examination of tumor mass using a non-surgical procedure is an inexpensive and
    preventative measure for early detection of the disease.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 乳腺癌是全球女性最常见的癌症，尤其是在发展中国家，病例诊断往往在晚期。使用非手术程序检查肿瘤质量是早期发现疾病的一种经济且预防性的措施。
- en: In this case-study, a marked dataset from such a procedure is used and the goal
    is to classify the breast cancer data into Malignant and Benign using multiple
    SSL techniques.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，使用了一个从该过程明显标记的数据集，目标是使用多种SSL技术将乳腺癌数据分类为Malignant和Benign。
- en: Machine learning mapping
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习映射
- en: To illustrate the techniques learned in this chapter so far, we will use SSL
    to classify. Whereas the dataset contains labels for all the examples, in order
    to treat this as a problem where we can apply SSL, we will consider a fraction
    of the data to be unlabeled. In fact, we run multiple experiments using different
    fractions of unlabeled data for comparison. The different base learners used are
    classification algorithms familiar to us from previous chapters.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明本章迄今为止学到的技术，我们将使用半监督学习（SSL）进行分类。虽然数据集包含所有示例的标签，但为了将其视为可以应用SSL的问题，我们将考虑数据的一部分为未标记。实际上，我们进行了多次实验，使用不同比例的未标记数据进行比较。所使用的不同基学习器是我们从先前章节中熟悉的分类算法。
- en: Data collection
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据收集
- en: This dataset was collected by the University of Wisconsin Hospitals, Madison.
    The dataset is available in Weka AARF format. The data is not partitioned into
    training, validation and test.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集由威斯康星大学麦迪逊分校收集。数据集以Weka AARF格式提供。数据未划分为训练、验证和测试集。
- en: Data quality analysis
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据质量分析
- en: The examples in the data contain no unique identifier. There are 16 examples
    for which the Bare Nuclei attribute has missing values. The target Class is the
    only categorical attribute and has two values. All other attributes are continuous
    and in the range [1, 10].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的示例不包含唯一标识符。有16个示例的“裸核”属性有缺失值。目标类别是唯一的分类属性，有两个值。所有其他属性都是连续的，范围在[1, 10]之间。
- en: Data sampling and transformation
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据采样和转换
- en: In the experiments, we present results for 10-fold cross-validation. For comparison,
    four runs were performed each using a different fraction of labeled data—10%,
    20%, 30%, and 40%.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们展示了10折交叉验证的结果。为了比较，每个运行都使用了不同比例的标记数据——10%、20%、30%和40%。
- en: A numeric sample code number was added to each example as a unique identifier.
    The categorical values Malignant and Benign, for the class attribute, were replaced
    by the numeric values 4 and 2 respectively.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个示例添加了一个数字样本代码作为唯一标识符。对于类别属性，将Malignant和Benign的类别值分别替换为数字4和2。
- en: Datasets and analysis
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集和分析
- en: 'The Breast Cancer Dataset Wisconsin (Original) is available from the UCI Machine
    Learning Repository at: [https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星大学乳腺癌数据集（原始）可在UCI机器学习仓库中找到：[https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original)](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Original))。
- en: This database was originally obtained from the University of Wisconsin Hospitals,
    Madison from Dr. William H. Wolberg. The dataset was created by Dr. Wolberg for
    the diagnosis and prognosis of breast tumors. The data is based exclusively on
    measurements involving the **Fine Needle Aspirate** (**FNA**) test. In this test,
    fluid from a breast mass is extracted using a small-gauge needle and then visually
    examined under a microscope.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据库最初是从威斯康星大学麦迪逊分校的威廉·H·沃尔伯格博士那里获得的。该数据集是由沃尔伯格博士为乳腺癌的诊断和预后而创建的。数据仅基于涉及**细针穿刺吸液**（**FNA**）测试的测量。在这个测试中，使用小号针从乳腺肿块中提取液体，然后在显微镜下进行视觉检查。
- en: 'A total of 699 instances with nine numeric attributes and a binary class (malignant/benign)
    constitute the dataset. The percentage of missing values is 0.2%. There are 65.5%
    malignant and 34.5% benign cases in the dataset. The feature names and range of
    valid values are listed in the following table:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '| Num. | Feature Name | Domain |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: '| 1 | Sample code number | id number |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
- en: '| 2 | Clump Thickness | 1 - 10 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| 3 | Uniformity of Cell Size | 1 - 10 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| 4 | Uniformity of Cell Shape | 1 - 10 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| 5 | Marginal Adhesion | 1 - 10 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| 6 | Single Epithelial Cell Size | 1 - 10 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| 7 | Bare Nuclei | 1 - 10 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| 8 | Bland Chromatin | 1 - 10 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| 9 | Normal Nucleoli | 1 - 10 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| 10 | Mitoses | 1 - 10 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| 11 | Class | 2 for benign, 4 for malignant |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: Feature analysis results
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Summary statistics by feature appear in Table 1.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Clump Thickness | Cell Size Unifor-mity | Cell Shape Unifor-mity | Marginal
    Adhesion | Single Epi Cell Size | Bare Nuclei | Bland Chromatin | Normal Nucleoli
    | Mitoses |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| mean | 4.418 | 3.134 | 3.207 | 2.807 | 3.216 | 3.545 | 3.438 | 2.867 | 1.589
    |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| std | 2.816 | 3.051 | 2.972 | 2.855 | 2.214 | 3.644 | 2.438 | 3.054 | 1.715
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| min | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| 25% | 2 | 1 | 1 | 1 | 2 |   | 2 | 1 | 1 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| 50% | 4 | 1 | 1 | 1 | 2 |   | 3 | 1 | 1 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| 75% | 6 | 5 | 5 | 4 | 4 |   | 5 | 4 | 1 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| max | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 | 10 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '*Table 1\. Features summary*'
  id: totrans-203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Experiments and results
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two SSL algorithms were selected for the experiments—self-training and co-training.
    In addition, four classification methods were chosen as base learners—Naïve Bayes,
    C4.5, K-NN, and SMO. Further, each experiment was run using four different partitions
    of labeled and unlabeled data (10%, 20%, 30%, and 40% labeled).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: The hyper-parameters for the algorithms and base classifiers are given in Table
    2\. You can see the accuracy across the different runs corresponding to four partitions
    of labeled and unlabeled data for the two SSL algorithms.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we give the performance results for each experiment for the case of
    40% labeled. Performance metrics provided are Accuracy and the Kappa statistic
    with standard deviations.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Parameters |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
- en: '| Self-Training | MAX_ITER = 40 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: '| Co-Training | MAX_ITER = 40, Initial Unlabeled Pool=75 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
- en: '| KNN | K = 3, Euclidean distance |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: '| C4.5 | pruned tree, confidence = 0.25, 2 examples per leaf |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| NB | No parameters specified |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| SMO | C = 1.0, Tolerance Parameter = 0.001, Epsilon= 1.0E-12, Kernel Type
    = Polynomial, Polynomial degree = 1, Fit logistic models = true |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '*Table 2\. Base classifier hyper-parameters for self-training and co-training*'
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| SSL Algorithm | 10% | 20% | 30% | 40% |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| Self-Training C 4.5 | 0.9 | 0.93 | 0.94 | 0.947 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| Co-Training SMO | 0.959 | 0.949 | 0.962 | 0.959 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '*Table 3\. Model accuracy for samples with varying fraction of labeled examples*'
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '| Algorithm | Accuracy (no unlabeled) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| C4.5 10-fold CV | 0.947 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| C4.5 10折交叉验证 | 0.947 |'
- en: '| SMO 10 fold CV | 0.967 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| SMO 10折交叉验证 | 0.967 |'
- en: '|   |   | 10 fold CV Wisconsin 40% Labeled Data |   |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|   |   | 10折交叉验证威斯康星40%标记数据 |   |'
- en: '| Self-Training (kNN) | Accuracy | 0.9623 (1) | Kappa | 0.9170 (2) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 自训练（kNN） | 准确率 | 0.9623 (1) | Kappa | 0.9170 (2) |'
- en: '|   | Std Dev | 0.0329 | Std Dev | 0.0714 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|   | 标准差 | 0.0329 | 标准差 | 0.0714 |'
- en: '| Self-Training (C45) | Accuracy | 0.9606 (3) | Kappa | 0.9144 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 自训练（C45） | 准确率 | 0.9606 (3) | Kappa | 0.9144 |'
- en: '|   | Std Dev | 0.0241 | Std Dev | 0.0511 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|   | 标准差 | 0.0241 | 标准差 | 0.0511 |'
- en: '| Self-Training (NB) | Accuracy | 0.9547 | Kappa | 0.9036 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 自训练（NB） | 准确率 | 0.9547 | Kappa | 0.9036 |'
- en: '|   | Std Dev | 0.0252 | Std Dev | 0.0533 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|   | 标准差 | 0.0252 | 标准差 | 0.0533 |'
- en: '| Self-Training (SMO) | Accuracy | 0.9547 | Kappa | 0.9035 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 自训练（SMO） | 准确率 | 0.9547 | Kappa | 0.9035 |'
- en: '|   | Std Dev | 0.0208 | Std Dev | 0.0435 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|   | 标准差 | 0.0208 | 标准差 | 0.0435 |'
- en: '| Co-Training (NN) | Accuracy | 0.9492 | Kappa | 0.8869 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 协同训练（NN） | 准确率 | 0.9492 | Kappa | 0.8869 |'
- en: '|   | Std Dev | 0.0403 | Std Dev | 0.0893 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|   | 标准差 | 0.0403 | 标准差 | 0.0893 |'
- en: '| Co-Training (C45) | Accuracy | 0.9417 | Kappa | 0.8733 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 协同训练（C45） | 准确率 | 0.9417 | Kappa | 0.8733 |'
- en: '|   | Std Dev | 0.0230 | Std Dev | 0.0480 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|   | 标准差 | 0.0230 | 标准差 | 0.0480 |'
- en: '| Co-Training (NB) | Accuracy | 0.9622 (2) | Kappa | 0.9193 (1) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 协同训练（NB） | 准确率 | 0.9622 (2) | Kappa | 0.9193 (1) |'
- en: '|   | Std Dev | 0.0290 | Std Dev | 0.0614 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|   | 标准差 | 0.0290 | 标准差 | 0.0614 |'
- en: '| Co-Training (SMO) | Accuracy | 0.9592 | Kappa | 0.9128 (3) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 协同训练（SMO） | 准确率 | 0.9592 | Kappa | 0.9128 (3) |'
- en: '|   | Std Dev | 0.0274 | Std Dev | 0.0580 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|   | 标准差 | 0.0274 | 标准差 | 0.0580 |'
- en: '*Table 4\. Model performance comparison using 40% labeled examples. The top
    ranking performers in each category are shown in parentheses.*'
  id: totrans-242
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表4. 使用40%标记示例的模型性能比较。每个类别的顶级表现者用括号表示。*'
- en: Analysis of semi-supervised learning
  id: totrans-243
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 半监督学习分析
- en: With 40% of labeled data, semi-supervised self-training with C4.5 achieves the
    same result as 100% of labeled data with just C4.5\. This shows the strength of
    semi-supervised learning when the data is sparsely labeled.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在40%的标记数据下，使用C4.5的半监督自训练达到了与100%标记数据相同的成果。这显示了半监督学习在数据稀疏标记时的强大能力。
- en: SMO with polynomial kernel, with 30-40% data comes close to the 100% data but
    not as good as C4.5.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多项式核的SMO，在30-40%的数据下接近100%的数据，但不如C4.5好。
- en: Self-training and co-training with four classifiers on 40% labeled training
    data shows
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在40%标记的训练数据上，使用四个分类器的自训练和协同训练显示
- en: KNN as base classifier and self-training has the highest accuracy (0.9623) which
    indicates the non-linear boundary of the data. Co-training with Naïve Bayes comes
    very close.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以KNN作为基分类器和自训练具有最高的准确率（0.9623），这表明数据的非线性边界。与朴素贝叶斯协同训练非常接近。
- en: 'Self-training with classifiers such as linear Naïve Bayes, non-linear C4.5
    and highly non-linear KNN shows steady improvements in accuracy: 0.9547, 0.9606,
    0.9623, which again shows that using self-training but choosing the right underlying
    classifier for the problem is very important.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用如线性朴素贝叶斯、非线性C4.5和高度非线性的KNN等分类器的自训练显示了准确率的稳步提升：0.9547、0.9606、0.9623，这再次表明使用自训练但选择正确的底层分类器对于问题非常重要。
- en: Co-training with Naive Bayes has highest Kappa statistic (0.9193) and almost
    similar accuracy as KNN with self-training. The independence relationship between
    features—hence breaking the feature sets into orthogonal feature sets and using
    them for classifiers—improves the learning.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与朴素贝叶斯协同训练具有最高的Kappa统计量（0.9193）和与KNN自训练几乎相同的准确率。特征之间的独立性关系——因此将特征集分解为正交特征集并用于分类器——提高了学习效果。
- en: Active learning
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主动学习
- en: Although active learning has many similarities with semi-supervised learning,
    it has its own distinctive approach to modeling with datasets containing labeled
    and unlabeled data. It has roots in the basic human psychology that asking more
    questions often tends to solve problems.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管主动学习与半监督学习有许多相似之处，但它对包含标记和无标记数据的集合进行建模的方法具有自己独特的途径。它源于基本的人类心理学，即提出更多问题通常有助于解决问题。
- en: 'The main idea behind active learning is that if the learner gets to pick the
    instances to learn from rather than being handed labeled data, it can learn more
    effectively with less data (*Reference* [6]). With very small amount of labeled
    data, it can carefully pick instances from unlabeled data to get label information
    and use that to iteratively improve learning. This basic approach of querying
    for unlabeled data to get labels from a so-called oracle—an expert in the domain—distinguishes
    active learning from semi-supervised or passive learning. The following figure
    illustrates the difference and the iterative process involved:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习背后的主要思想是，如果学习者可以自己选择要学习的实例，而不是被动地获得标记数据，那么它可以用更少的数据更有效地学习（*参考文献* [6]）。在非常少量的标记数据的情况下，它可以仔细地从未标记数据中挑选实例以获取标签信息，并使用这些信息迭代地改进学习。这种从所谓的专家（领域专家）那里查询未标记数据以获取标签的基本方法，将主动学习与半监督学习或被动学习区分开来。以下图示说明了差异和涉及的迭代过程：
- en: '![Active learning](img/B05137_04_079.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![主动学习](img/B05137_04_079.jpg)'
- en: Figure 7\. Active Machine Learning process contrasted with Supervised and Semi-Supervised
    Learning process.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 与监督学习和半监督学习过程相比的主动机器学习过程。
- en: Representation and notation
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 表示和符号
- en: The dataset *D*, which represents all the data instances and their labels, is
    given by *D* = {(**x**[1], *y*[2]), (**x**[2], *y*[2]), … (**x**[n], *y*[n])}
    where ![Representation and notation](img/B05137_04_012.jpg) are the individual
    instances of data and {*y*[1], *y*[2], … *y*[n]} is the set of associated labels.
    *D* is composed of two sets *U*, labeled data and *L*, unlabeled data. **x** is
    the set {**x**[1], **x**[2],… **x**[n]} of data instances without labels.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 *D*，它表示所有数据实例及其标签，由 *D* = {(**x**[1]，*y*[2])，(**x**[2]，*y*[2])，… (**x**[n]，*y*[n])}
    给出，其中 ![表示和符号](img/B05137_04_012.jpg) 是数据个体的实例，而 {*y*[1]，*y*[2]，… *y*[n]} 是相关标签的集合。*D*
    由两个集合 *U*（标记数据）和 *L*（未标记数据）组成。**x** 是无标签数据实例的集合 {**x**[1]，**x**[2]，… **x**[n]}。
- en: The dataset ![Representation and notation](img/B05137_04_015.jpg) consists of
    all labeled data with known outcomes {*y*[1], *y*[2], … *y*[l]} whereas ![Representation
    and notation](img/B05137_04_017.jpg) is the dataset where the outcomes are not
    known. As before, |*U*|>> |*L*|.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集![表示和符号](img/B05137_04_015.jpg)包含所有已知结果的有标签数据 {*y*[1]，*y*[2]，… *y*[l]}，而![表示和符号](img/B05137_04_017.jpg)是结果未知的数据集。与之前一样，|*U*|>>
    |*L*|。
- en: Active learning scenarios
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主动学习场景
- en: 'Active learning scenarios can be broadly classified as:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习场景可以广泛地分为：
- en: '**Stream-based active learning**: In this approach, an instance or example
    is picked only from the unlabeled dataset and a decision is made whether to ignore
    the data or pass it on to the oracle to get its label (*Referee*[10,11]).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于流的主动学习**：在此方法中，实例或示例仅从未标记数据集中选择，并决定是否忽略数据或将数据传递给专家以获取其标签（*参考文献*[10,11]）。'
- en: '**Pool-based active learning**: In this approach, the instances are queried
    from the unlabeled dataset and then ranked on the basis of informativeness and
    a set from these is sent to the Oracle to get the labels (*Referees* [12]).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于池的主动学习**：在此方法中，从未标记数据集中查询实例，然后根据信息性进行排序，并从这些实例中选取一组发送给Oracle以获取标签（*参考文献*[12]）。'
- en: '**Query synthesis**: In this method, the learner has only information about
    input space (features) and synthesizes queries from the unlabeled set for the
    membership. This is not used in practical applications, as most often it doesn''t
    consider the data generating distribution and hence often the queries are arbitrary
    or meaningless.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询合成**：在此方法中，学习者只有关于输入空间（特征）的信息，并从未标记集中合成查询以确定成员资格。这种方法在实用应用中很少使用，因为它通常不考虑数据生成分布，因此查询往往是任意或无意义的。'
- en: Active learning approaches
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主动学习方法
- en: Regardless of the scenario involved, every active learning approach includes
    selecting a query strategy or sampling method which establishes the mechanism
    for picking the queries in each iteration. Each method reveals a distinct way
    of seeking out unlabeled examples with the best information content for improving
    the learning process. In the following subsections, we describe the major query
    strategy frameworks, how they work, their merits and limitations, and the different
    strategies within each framework.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 无论涉及何种场景，每个主动学习方法都包括选择查询策略或采样方法，这为每个迭代中查询的选择建立了机制。每种方法都揭示了一种寻找具有最佳信息内容的未标记示例的独特方式，以改善学习过程。在以下小节中，我们描述了主要的查询策略框架，它们的工作原理，优点和局限性，以及每个框架中的不同策略。
- en: Uncertainty sampling
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不确定性采样
- en: The key idea behind this form of sampling is to select instances from the unlabeled
    pool that the current model is most uncertain about. The learner can then avoid
    the instances the model is more certain or confident in classifying (*Reerences*
    [8]).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这种采样形式背后的关键思想是从未标记池中选择当前模型最不确定的实例。学习者可以避免模型更有信心或自信地分类的实例（*参考文献* [8]）。
- en: Probabilistic based models (Naïve Bayes, Logistic Regression, and so on) are
    the most natural choices for such methods as they give confidence measures on
    the given model, say *θ* for the data **x**, for a class *y*[i] *i* ϵ classes,
    and the probability ![Uncertainty sampling](img/B05137_04_086.jpg) as the posterior
    probability.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 基于概率的模型（如朴素贝叶斯、逻辑回归等）是此类方法的最自然选择，因为它们给出了对给定模型（例如，对于数据 **x** 的 *θ*，对于类别 *y*[i]
    *i* ϵ classes，以及作为后验概率的 ![不确定性采样](img/B05137_04_086.jpg)）的置信度度量。
- en: How does it work?
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'The general process for all uncertainty-based algorithms is outlined as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 所有基于不确定性的算法的一般过程概述如下：
- en: Initialize the data as labeled, ![How does it work?](img/B05137_04_015.jpg)
    and unlabeled, ![How does it work?](img/B05137_04_017.jpg).
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据初始化为已标记的，![如何工作？](img/B05137_04_015.jpg)和未标记的，![如何工作？](img/B05137_04_017.jpg)。
- en: 'While there is still unlabeled data:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当仍有未标记数据时：
- en: Train the classifier model ![How does it work?](img/B05137_04_021.jpg) with
    the labeled data *L*.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标签数据 *L* 训练分类器模型 ![如何工作？](img/B05137_04_021.jpg)。
- en: Apply the classifier model *f* on the unlabeled data *U* to assess informativeness
    *J* using one of the sampling mechanisms (see next section)
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分类器模型 *f* 应用于未标记数据 *U*，使用采样机制（见下一节）之一来评估信息性 *J*。
- en: Choose *k* most informative data from *U* as set *L*[u] to get labels from the
    oracle.
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *U* 中选择 *k* 个最有信息性的数据作为集合 *L*[u]，从先知那里获取标签。
- en: 'Augment the labeled data with the *k* new labeled data points obtained in the
    previous step: *L* = *L* ∪ *L*[u].'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一步中获取的 *k* 个新标签数据点来增强标签数据：*L* = *L* ∪ *L*[u]。
- en: Repeat all the steps under 2.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2 下的所有步骤。
- en: Some of the most common query synthesis algorithms to sample the informative
    instances from the data are given next.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 下面给出了一些最常用的查询合成算法，用于从数据中采样信息实例。
- en: Least confident sampling
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最不自信采样
- en: In this technique, the data instances are sorted based on their confidence in
    reverse, and the instances most likely to be queried or selected are the ones
    the model is least confident about. The idea behind this is the least confident
    ones are the ones near the margin or separating hyperplane and getting their labels
    will be the best way to learn the boundaries effectively.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项技术中，数据实例根据它们的置信度按逆序排序，最有可能被查询或选择的实例是模型最不自信的实例。背后的想法是最不自信的实例靠近边缘或分离超平面，获取它们的标签将是有效学习边界的最佳方式。
- en: This can be formulated as ![Least confident sampling](img/B05137_04_088.jpg).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以表示为 ![最不自信采样](img/B05137_04_088.jpg)。
- en: The disadvantage of this method is that it effectively considers information
    of the best; information on the rest of the posterior distribution is not used.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点在于它实际上只考虑了最佳信息；后验分布其余部分的信息没有被使用。
- en: Smallest margin sampling
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最小边界采样
- en: This is margin-based sampling, where the instances with smaller margins have
    more ambiguity than instances with larger margins.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于边界的采样，其中具有较小边界的实例比具有较大边界的实例具有更多的歧义。
- en: This can be formulated as ![Smallest margin sampling](img/B05137_04_089.jpg)
    where ![Smallest margin sampling](img/B05137_04_090.jpg) and ![Smallest margin
    sampling](img/B05137_04_091.jpg) are two labels for the instance **x**.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以表示为 ![最小边界采样](img/B05137_04_089.jpg)，其中 ![最小边界采样](img/B05137_04_090.jpg)
    和 ![最小边界采样](img/B05137_04_091.jpg) 是实例 **x** 的两个标签。
- en: Label entropy sampling
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 标签熵采样
- en: 'Entropy, which is the measure of average information content in the data and
    is the impurity measure, can be used to sample the instances. This can be formulated
    as:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 熵，它是数据平均信息内容的度量，也是杂质度量，可以用来采样实例。这可以表示为：
- en: '![Label entropy sampling](img/B05137_04_092.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![标签熵采样](img/B05137_04_092.jpg)'
- en: Advantages and limitations
  id: totrans-288
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性：
- en: Label entropy sampling is the simplest approach and can work with any probabilistic
    classifiers—this is the biggest advantage
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签熵采样是最简单的方法，可以与任何概率分类器一起工作——这是最大的优势
- en: Presence of outliers or wrong feedback can go unnoticed and models can degrade
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值或错误反馈的存在可能被忽视，模型可能会退化
- en: Version space sampling
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 版本空间采样
- en: Hypothesis *H* is the set of all the particular models that generalize or explain
    the training data; for example, all possible sets of weights that separate two
    linearly separable classes. Version spaces *V* are subsets of hypothesis *H*,
    which are consistent with the training data as defined by Tom Mitchell (*References*
    [15]) such that ![Version space sampling](img/B05137_04_095.jpg).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 *H* 是所有特定模型集合，这些模型可以泛化或解释训练数据；例如，所有可能的权重集合，可以分离两个线性可分类别。版本空间 *V* 是假设 *H*
    的子集，根据汤姆·米切尔（*参考文献* [15]）的定义与训练数据一致，如下所示 ![版本空间采样](img/B05137_04_095.jpg)。
- en: The idea behind this sampling is to query instances from the unlabeled dataset
    that reduce the size of the version space or minimize |*V*|.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这种采样的背后思想是从未标记的数据集中查询实例，以减少版本空间的大小或最小化 |*V*|。
- en: Query by disagreement (QBD)
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过不一致性查询（QBD）
- en: QBD is one of the earliest algorithms which works on maintaining a version space
    *V*—when two hypotheses disagree on the label of new incoming data, that instance
    is selected for getting labels from the oracle or expert.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: QBD 是最早维护版本空间 *V* 的算法之一——当两个假设对新到达数据的标签不一致时，该实例被选中以从预言者或专家那里获取标签。
- en: How does it work?
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'The entire algorithm can be summarized as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 整个算法可以总结如下：
- en: Initialize ![How does it work?](img/B05137_04_095.jpg) as the set of all legal
    hypotheses.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 ![如何工作？](img/B05137_04_095.jpg) 初始化为所有合法假设的集合。
- en: Initialize the data as ![How does it work?](img/B05137_04_015.jpg) labeled and
    ![How does it work?](img/B05137_04_017.jpg) unlabeled.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据初始化为 ![如何工作？](img/B05137_04_015.jpg) 标记和 ![如何工作？](img/B05137_04_017.jpg)
    未标记。
- en: 'While data **x***['']* is in *U*:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当数据 **x***[']* 在 *U* 中时：
- en: 'If ![How does it work?](img/B05137_04_099.jpg) for any *h*[2] ∈ *V*:'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 ![如何工作？](img/B05137_04_099.jpg) 对于任何 *h*[2] ∈ *V*：
- en: Query the label of **x***[']* and get *y[']*.
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询 **x***[']* 的标签并获取 *y[']*。
- en: '*V* = {h: h(**x**['']) = *y['']* for all points.'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*V* = {h: h(**x**['']) = *y['']* 对于所有点。'
- en: 'Otherwise:'
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 否则：
- en: Ignore **x***[']*.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 忽略 **x***[']*。
- en: Query by Committee (QBC)
  id: totrans-307
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 委员会查询（QBC）
- en: Query by Committee overcomes the limitation of Query by Disagreement related
    to maintaining all possible version spaces by creating a committee of classifiers
    and using their votes as the mechanism to capture the disagreement (*References*
    [7]).
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通过委员会查询克服了与维护所有可能的版本空间相关的查询不一致性的限制，通过创建一个分类器委员会并使用它们的投票作为捕捉不一致性的机制（*参考文献* [7]）。
- en: How does it work?
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'For this algorithm:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此算法：
- en: Initialize the data as ![How does it work?](img/B05137_04_015.jpg) labeled and
    ![How does it work?](img/B05137_04_017.jpg) unlabeled.
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据初始化为 ![如何工作？](img/B05137_04_015.jpg) 标记和 ![如何工作？](img/B05137_04_017.jpg)
    未标记。
- en: Train the committee of models *C* = {*θ*¹*θ*², ... *θ*^c} on the labeled data
    *w* (see the following).
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在标记数据 *w* 上训练模型委员会 *C* = {*θ*¹*θ*², ... *θ*^c}（见下文）。
- en: 'For all data **x***^''* in the *U*:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有数据 **x***^'* 在 *U* 中：
- en: Vote for predictions on **x***'* as {![How does it work?](img/B05137_04_107.jpg).
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对 **x***'* 的预测进行投票，作为 {![如何工作？](img/B05137_04_107.jpg)}。
- en: Rank the instances based on maximum disagreement (see the following).
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据最大不一致性对实例进行排序（见下文）。
- en: Choose *k* most informative data from *U* as set *L*[u] to get labels from the
    oracle.
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *U* 中选择 *k* 个最有信息量的数据作为集合 *L*[u]，以从预言者那里获取标签。
- en: Augment the labeled data with the *k* new labeled data points *L* = *L* ∪ *L*[u].
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *k* 个新的标记数据点 *L* = *L* ∪ *L*[u] 添加到标记数据中。
- en: Retrain the models {*θ*[1], *θ*[2], ... *θ*[c]} with new *L*.
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的 *L* 重新训练模型 {*θ*[1], *θ*[2], ... *θ*[c]}。
- en: With the two tasks of training the committee of learners and choosing the disagreement
    methods, each has various choices.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练学习者的委员会和选择不一致方法这两个任务中，每个都有各种选择。
- en: Training different models can be either done using different samples from *L*
    or they can be trained using ensemble methods such as boosting and bagging.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 训练不同的模型可以通过从 *L* 中选择不同的样本来完成，或者可以使用提升和袋装等集成方法进行训练。
- en: 'Vote entropy is one of the methods chosen as the disagreement metric to rank.
    The mathematical way of representing it is:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 投票熵是作为不一致性度量方法之一选择的。其数学表示方式如下：
- en: '![How does it work?](img/B05137_04_109.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_04_109.jpg)'
- en: Here, *V(y[i])* is number of votes given to the label *y*[i] from all possible
    labels and |*C*| is size of the committee.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*V(y[i])* 是从所有可能的标签中给出的标签 *y*[i] 的投票数，而 |*C*| 是委员会的大小。
- en: '**Kullback-Leibler** (**KL**) divergence is an information theoretic measure
    of divergence between two probability distributions. The disagreement is quantified
    as the average divergence of each committee''s prediction from that of the consensus
    in the committee *C*:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '**库尔巴克-莱布勒**（**KL**）散度是两个概率分布之间差异的信息论度量。不一致性被量化为每个委员会预测与委员会 *C* 中共识的平均差异：'
- en: '![How does it work?](img/B05137_04_114.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_04_114.jpg)'
- en: Advantages and limitations
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are the following:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: Simplicity and the fact that it can work with any supervised algorithm gives
    it a great advantage.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单性以及它可以与任何监督算法一起工作的事实给它带来了巨大的优势。
- en: There are theoretical guarantees of minimizing errors and generalizing in some
    conditions.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些条件下，有理论保证可以最小化误差和泛化。
- en: Query by Disagreement suffers from maintaining a large number of valid hypotheses.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过不一致性查询（Query by Disagreement）受到维护大量有效假设的困扰。
- en: These methods still suffer from the issue of wrong feedback going unnoticed
    and models potentially degrading.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法仍然存在错误反馈被忽视和模型可能退化的问题。
- en: Data distribution sampling
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分布采样
- en: The previous methods selected the best instances from the unlabeled set based
    either on the uncertainty posed by the samples on the models or by reducing the
    hypothesis space size. Neither of these methods worked on what is best for the
    model itself. The idea behind data distribution sampling is that adding samples
    that help reduce the errors to the model serves to improve predictions on unseen
    instances using expected values (*References* [13 and 14]).
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的方法基于样本对模型的不确定性或通过减少假设空间大小来选择未标记集中的最佳实例。这些方法都没有针对模型本身的最佳选择。数据分布采样的想法是，添加有助于减少模型误差的样本，有助于通过预期值（*参考文献*
    [13 和 14]）提高对未见实例的预测。
- en: How does it work?
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: There are different ways to find what is the best sample for the given model
    and here we will describe each one in some detail.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的方法来找到给定模型的最佳样本，我们将详细描述每一种方法。
- en: Expected model change
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预期模型变化
- en: 'The idea behind this is to select examples from the unlabeled set that that
    will bring maximum change in the model:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这种想法的背后的目的是选择未标记集中将带来模型最大变化的示例：
- en: '![Expected model change](img/B05137_04_115.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![预期模型变化](img/B05137_04_115.jpg)'
- en: Here, P[θ] (*y*|**x**) = expectation over labels of **x**, ![Expected model
    change](img/B05137_04_117.jpg) is the sum over unlabeled instances of the entropy
    of including **x** *'* after retraining with **x**.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，P[θ] (*y*|**x**) = **x** 标签的期望，![预期模型变化](img/B05137_04_117.jpg) 是在重新训练 **x**
    后，包括 **x** *'* 的熵在未标记实例上的总和。
- en: Expected error reduction
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预期误差减少
- en: 'Here, the approach is to select examples from the unlabeled set that reduce
    the model''s generalized error the most. The generalized error is measured using
    the unlabeled set with expected labels:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，方法是选择未标记集中最能减少模型泛化误差的示例。泛化误差使用带有预期标签的未标记集进行衡量：
- en: '![Expected error reduction](img/B05137_04_119.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![预期误差减少](img/B05137_04_119.jpg)'
- en: Here, Pθ (*y*|**x**) = expectation over labels of **x**, ![Expected error reduction](img/B05137_04_120.jpg)
    is the sum over unlabeled instances of the entropy of including x*^'* after retraining
    with **x**.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Pθ (*y*|**x**) = **x** 标签的期望，![预期误差减少](img/B05137_04_120.jpg) 是在重新训练 **x**
    后，包括 x*^'* 的熵在未标记实例上的总和。
- en: Variance reduction
  id: totrans-344
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方差减少
- en: 'The general equation of estimation on an out-of-sample error in terms of noise-bias-variance
    is given by the following:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 以噪声-偏差-方差为参数，对样本外误差的估计的一般方程如下：
- en: '![Variance reduction](img/B05137_04_121.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![方差减少](img/B05137_04_121.jpg)'
- en: 'Here, **G**(**x**) is the model''s prediction given the label *y*. In variance
    reduction, we select examples from the unlabeled set that most reduce the variance
    in the model:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**G**(**x**) 是给定标签 *y* 的模型预测。在方差减少中，我们选择未标记集中最能减少模型方差的示例：
- en: '![Variance reduction](img/B05137_04_124.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![方差减少](img/B05137_04_124.jpg)'
- en: Here, *θ* + represents the model after it has been retrained with the new point
    **x** *^'* and its label *y^'*.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*θ* + 表示使用新点 **x** *^'* 和其标签 *y^'* 重新训练后的模型。
- en: Density weighted methods
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 密度加权方法
- en: In this approach, we select examples from the unlabeled set that have average
    similarity to the labeled set.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们从未标记集中选择与标记集平均相似度较高的示例。
- en: 'This can be represented as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以表示如下：
- en: '![Density weighted methods](img/B05137_04_127.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![密度加权方法](img/B05137_04_127.jpg)'
- en: Here, *sim*(**x**, **x** *^'*) is the density term or the similarity term where
    H[θ](*y*|**x**) is the base utility measure.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*sim*(**x**, **x** *^'*)是密度项或相似度项，其中H[θ](*y*|**x**)是基本效用度量。
- en: Advantages and limitations
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: 'The advantages and limitations are as follows:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 优点和局限性如下：
- en: The biggest advantage is that they work directly on the model as an optimization
    objective rather than implicit or indirect methods described before
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大的优势是它们直接在模型上作为优化目标工作，而不是之前描述的隐式或间接方法。
- en: These methods can work on pool- or stream-based scenarios
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法可以在基于池或流的场景中工作
- en: These methods have some theoretical guarantees on the bounds and generalizations
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法在界限和泛化方面有一些理论保证。
- en: The biggest disadvantage of these methods is computational cost and difficulty
    in implementation
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法的最大缺点是计算成本高，实现困难。
- en: Case study in active learning
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主动学习案例研究
- en: This case study uses another well-known publicly available dataset to demonstrate
    active learning techniques using open source Java libraries. As before, we begin
    with defining the business problem, what tools and frameworks are used, how the
    principles of machine learning are realized in the solution, and what the data
    analysis steps reveal. Next, we describe the experiments that were conducted,
    evaluate the performance of the various models, and provide an analysis of the
    results.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究使用另一个众所周知的公开数据集来展示使用开源Java库的主动学习技术。和之前一样，我们首先定义商业问题，使用的工具和框架，如何在解决方案中实现机器学习原理，以及数据分析步骤揭示了什么。接下来，我们描述了进行的实验，评估了各种模型的表现，并提供了结果分析。
- en: Tools and software
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具和软件
- en: For the experiments in Active Learning, JCLAL was the tool used. JCLAL is a
    Java framework for Active Learning, supporting single-label and multi-label learning.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在主动学习实验中，我们使用了JCLAL工具。JCLAL是一个Java框架，用于主动学习，支持单标签和多标签学习。
- en: Note
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'JCLAL is open source and is distributed under the GNU general public license:
    [https://sourceforge.net/p/jclal/git/ci/master/tree/](https://sourceforge.net/p/jclal/git/ci/master/tree/).'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: JCLAL是开源的，并遵循GNU通用公共许可证分发：[https://sourceforge.net/p/jclal/git/ci/master/tree/](https://sourceforge.net/p/jclal/git/ci/master/tree/)。
- en: Business problem
  id: totrans-367
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业问题
- en: The abalone dataset, which is used in these experiments, contains data on various
    physical and anatomical characteristics of abalone—commonly known as sea snails.
    The goal is to predict the number of rings in the shell, which is indicative of
    the age of the specimen.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些实验中使用的鲍鱼数据集包含了鲍鱼的各种物理和解剖特征数据——通常被称为海蜗牛。目标是预测壳中的环数，这可以指示样本的年龄。
- en: Machine learning mapping
  id: totrans-369
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习映射
- en: As we have seen, active learning is characterized by starting with a small set
    of labeled data accompanied by techniques of querying the unlabeled data such
    that we incrementally add instances to the labeled set. This is performed over
    multiple iterations, a batch at a time. The number of iterations and batch size
    are hyper-parameters for these techniques. The querying strategy and the choice
    of supervised learning method used to train on the growing number of labeled instances
    are additional inputs.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，主动学习的特点是从小数据集开始，该数据集包含标签数据，并伴随查询未标记数据的技巧，以便我们逐步向标签集添加实例。这是分批进行的，每次迭代一批。迭代的次数和批量大小是这些技术的超参数。查询策略和用于在不断增加的标签实例上训练的监督学习方法的选择是额外的输入。
- en: Data Collection
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: As before, we will use an existing dataset available from the UCI repository
    ([https://archive.ics.uci.edu/ml/datasets/Abalone](https://archive.ics.uci.edu/ml/datasets/Abalone)).
    The original owners of the database are the Department of Primary Industry and
    Fisheries in Tasmania, Australia.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将使用来自UCI存储库的现有数据集（[https://archive.ics.uci.edu/ml/datasets/Abalone](https://archive.ics.uci.edu/ml/datasets/Abalone)）。数据库的原始所有者是澳大利亚塔斯马尼亚州初级工业和渔业部。
- en: 'The data types and descriptions of the attributes accompany the data and are
    reproduced in *Table 5*. The class attribute, Rings, has 29 distinct classes:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 数据类型和属性描述与数据一起提供，并在*表5*中重现。类别属性“环数”有29个不同的类别：
- en: '| Name | Data type | Measurement units | Description |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 数据类型 | 测量单位 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Sex | nominal | M, F, and I (infant) | sex of specimen |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 性别 | 名义 | M, F, 和 I (婴儿) | 样本的性别 |'
- en: '| Length | continuous | mm | longest shell measurement |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 长度 | 连续 | 毫米 | 最长壳的测量值 |'
- en: '| Diameter | continuous | mm | perpendicular to length |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| Height | continuous | mm | with meat in shell |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| Whole weight | continuous | grams | whole abalone |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '| Shucked weight | continuous | grams | weight of meat |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| Viscera weight | continuous | grams | gut weight (after bleeding) |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '| Shell weight | continuous | grams | after being dried |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| Rings | integer | count | +1.5 gives the age in years |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '*Table 5\. Abalone dataset features*'
  id: totrans-385
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Data sampling and transformation
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this experiment, we treated a randomly selected 4,155 records as unlabeled
    and kept the remaining 17 as labeled. There is no transformation of the data.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Feature analysis and dimensionality reduction
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With only eight features, there is no need for dimensionality reduction. The
    dataset comes with some statistics on the features, reproduced in *Table 6*:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '|   | **Length** | **Diameter** | **Height** | **Whole** | **Shucked** | **Viscera**
    | **Shell** | **Rings** |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '| Min | 0.075 | 0.055 | 0 | 0.002 | 0.001 | 0.001 | 0.002 | 1 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| Max | 0.815 | 0.65 | 1.13 | 2.826 | 1.488 | 0.76 | 1.005 | 29 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '| Mean | 0.524 | 0.408 | 0.14 | 0.829 | 0.359 | 0.181 | 0.239 | 9.934 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| SD | 0.12 | 0.099 | 0.042 | 0.49 | 0.222 | 0.11 | 0.139 | 3.224 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '| Correl | 0.557 | 0.575 | 0.557 | 0.54 | 0.421 | 0.504 | 0.628 | 1 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: '*Table 6\. Summary statistics by feature*'
  id: totrans-396
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Models, results, and evaluation
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We conducted two sets of experiments. The first used pool-based scenarios and
    the second, stream-based. In each set, we used entropy sampling, least confident
    sampling, margin sampling, and vote entropy sampling. The classifiers used were
    Naïve Bayes, Logistic Regression, and J48 (implementation of C4.5). For every
    experiment, 100 iterations were run, with batch sizes of 1 and 10\. In *Table
    7*, we present a subset of these results, specifically, pool-based and stream-based
    scenarios for each sampling method using Naïve Bayes, Simple Logistic, and C4.5
    classifiers with a batch size of 10.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The full set of results can be seen at [https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter4](https://github.com/mjmlbook/mastering-java-machine-learning/tree/master/Chapter4).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'The JCLAL library requires an XML configuration file to specify which scenario
    to use, the query strategy selected, batch size, max iterations, and base classifier.
    The following is an example configuration:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The tools itself is invoked via the following:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Pool-based scenarios
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the following three tables, we compare results using pool-based scenarios
    when using Naïve Bayes, Simple Logistic, and C4.5 classifiers.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '**Naïve Bayes:**'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Area Under ROC | F Measure | False Positive Rate | Precision
    | Recall |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-EntropySampling-NaiveBayes-b10 | 0.6021 | 0.1032 | 0.0556(1) |
    0.1805 | 0.1304 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-KLDivergence-NaiveBayes-b10 | 0.6639(1) | 0.1441(1) | 0.0563 |
    0.1765 | 0.1504 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-LeastConfidentSampling-NaiveBayes-b10 | 0.6406 | 0.1300 | 0.0827
    | 0.1835(1) | 0.1810(1) |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-VoteEntropy-NaiveBayes-b10 | 0.6639(1) | 0.1441(1) | 0.0563 | 0.1765
    | 0.1504 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '*Table 7\. Performance of pool-based scenario using Naïve Bayes classifier*'
  id: totrans-414
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Logistic Regression**:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Area Under ROC | F Measure | False Positive Rate | Precision
    | Recall |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-EntropySampling-SimpleLogistic-b10 | 0.6831 | 0.1571 | 0.1157 |
    0.1651 | 0.2185(1) |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-KLDivergence-SimpleLogistic-b10 | 0.7175(1) | 0.1616 | 0.1049 |
    0.2117(1) | 0.2065 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-LeastConfidentSampling-SimpleLogistic-b10 | 0.6629 | 0.1392 | 0.1181(1)
    | 0.1751 | 0.1961 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-VoteEntropy-SimpleLogistic-b10 | 0.6959 | 0.1634(1) | 0.0895 |
    0.2307 | 0.1880 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
- en: '*Table 8\. Performance of pool-based scenario using Logistic Regression classifier*'
  id: totrans-422
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**C4.5**:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Area Under ROC | F Measure | False Positive Rate | Precision
    | Recall |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-EntropySampling-J48-b10 | 0.6730(1) | 0.3286(1) | 0.0737 | 0.3432(1)
    | 0.32780(1) |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-KLDivergence-J48-b10 | 0.6686 | 0.2979 | 0.0705(1) | 0.3153 | 0.2955
    |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-LeastConfidentSampling-J48-b10 | 0.6591 | 0.3094 | 0.0843 | 0.3124
    | 0.3227 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
- en: '| PoolBased-VoteEntropy-J48-b10 | 0.6686 | 0.2979 | 0.0706 | 0.3153 | 0.2955
    |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
- en: '*Table 9\. Performance of pool-based scenario using C4.5 classifier*'
  id: totrans-430
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Stream-based scenarios
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the following three tables, we have results for experiments on stream-based
    scenarios using Naïve Bayes, Logistic Regression, and C4.5 classifiers with four
    different sampling methods.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: '**Naïve Bayes**:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Area Under ROC | F Measure | False Positive Rate | Precision
    | Recall |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| StreamBased-EntropySampling-NaiveBayes-b10 | 0.6673(1) | 0.1432(1) | 0.0563
    | 0.1842(1) | 0.1480 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| StreamBased-LeastConfidentSampling-NaiveBayes-b10 | 0.5585 | 0.0923 | 0.1415
    | 0.1610 | 0.1807(1) |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| StreamBased-MarginSampling-NaiveBayes-b10 | 0.6736(1) | 0.1282 | 0.0548(1)
    | 0.1806 | 0.1475 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| StreamBased-VoteEntropyQuery-NaiveBayes-b10 | 0.5585 | 0.0923 | 0.1415 |
    0.1610 | 0.1807(1) |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '*Table 10\. Performance of stream-based scenario using Naïve Bayes classifier*'
  id: totrans-440
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**Logistic Regression**:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Area Under ROC | F Measure | False Positive Rate | Precision
    | Recall |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: '| StreamBased-EntropySampling-SimpleLogistic-b10 | 0.7343(1) | 0.1994(1) |
    0.0871 | 0.2154 | 0.2185(1) |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
- en: '| StreamBased-LeastConfidentSampling-SimpleLogistic-b10 | 0.7068 | 0.1750 |
    0.0906 | 0.2324(1) | 0.2019 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
- en: '| StreamBased-MarginSampling-SimpleLogistic-b10 | 0.7311 | 0.1994(1) | 0.0861
    | 0.2177 | 0.214 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
- en: '| StreamBased-VoteEntropy-SimpleLogistic-b10 | 0.5506 | 0.0963 | 0.0667(1)
    | 0.1093 | 0.1117 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
- en: '*Table 11\. Performance of stream-based scenario using Logistic Regression
    classifier*'
  id: totrans-448
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '**C4.5**:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: '| Experiment | Area Under ROC | F Measure | False Positive Rate | Precision
    | Recall |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| 实验 | ROC曲线下面积 | F度量 | 假正率 | 精确率 | 召回率 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| StreamBased-EntropySampling-J48-b10 | 0.6648 | 0.3053 | 0.0756 | 0.3189(1)
    | 0.3032 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| StreamBased-EntropySampling-J48-b10 | 0.6648 | 0.3053 | 0.0756 | 0.3189(1)
    | 0.3032 |'
- en: '| StreamBased-LeastConfidentSampling-J48-b10 | 0.6748(1) | 0.3064(1) | 0.0832
    | 0.3128 | 0.3189(1) |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| StreamBased-LeastConfidentSampling-J48-b10 | 0.6748(1) | 0.3064(1) | 0.0832
    | 0.3128 | 0.3189(1) |'
- en: '| StreamBased-MarginSampling-J48-b10 | 0.6660 | 0.2998 | 0.0728(1) | 0.3163
    | 0.2967 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| StreamBased-MarginSampling-J48-b10 | 0.6660 | 0.2998 | 0.0728(1) | 0.3163
    | 0.2967 |'
- en: '| StreamBased-VoteEntropy-J48-b10 | 0.4966 | 0.0627 | 0.0742 | 0.1096 | 0.0758
    |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| StreamBased-VoteEntropy-J48-b10 | 0.4966 | 0.0627 | 0.0742 | 0.1096 | 0.0758
    |'
- en: '*Table 12\. Performance of stream-based scenario using C4.5 classifier*'
  id: totrans-456
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表12. 使用C4.5分类器的基于流的场景性能*'
- en: Analysis of active learning results
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主动学习结果分析
- en: It is quite interesting to see that pool-based, Query By Committee—an ensemble
    method—using KL-Divergence sampling does really well across most classifiers.
    As discussed in the section, these methods have been proven to have a theoretical
    guarantee on reducing the errors by keeping a large hypothesis space, and this
    experimental result supports that empirically.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 看到基于池的、基于委员会的查询——一种集成方法——使用KL散度采样在大多数分类器上表现良好，这确实很有趣。正如章节中讨论的，这些方法已经被证明通过保持一个大的假设空间来减少错误，并且这个实验结果从实证上支持了这一点。
- en: Pool-based, entropy-based sampling using C4.5 as a classifier has the highest
    Precision, Recall, FPR and F-Measure. Also with stream-based, entropy sampling
    with C4.5, the metrics are similarly high. With different sampling techniques
    and C4.5 using pool-based as in KL-Divergence, LeastConfident or vote entropy,
    the metrics are significantly higher. Thus, this can be attributed more strongly
    to the underlying classifier C4.5 in finding non-linear patterns.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 基于池的、基于熵的采样使用C4.5作为分类器具有最高的精确率、召回率、假正率和F度量。此外，在基于流的熵采样中使用C4.5，指标也相似地很高。使用不同的采样技术和基于池的C4.5，如KL散度、最不自信或投票熵，指标显著更高。因此，这可以更强烈地归因于底层分类器C4.5在寻找非线性模式方面的能力。
- en: The Logistic Regression algorithm performs very well in both stream-based and
    pool-based when considering AUC. This may be completely due to the fact that LR
    has a good probabilistic approach in confidence mapping, which is an important
    factor for giving good AUC scores.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到AUC，逻辑回归算法在基于流和基于池的情况下都表现非常好。这可能完全是因为LR在置信映射方面有一个很好的概率方法，这是获得良好AUC分数的重要因素。
- en: Summary
  id: totrans-461
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: After a tour of supervised and unsupervised machine learning techniques and
    their application to real-world datasets in the previous chapters, this chapter
    introduces the concepts, techniques, and tools of **Semi-Supervised Learning**
    (**SSL**) and **Active Learning** (**AL**).
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们游览了监督学习和无监督学习技术及其在现实世界数据集上的应用，本章介绍了**半监督学习**（**SSL**）和**主动学习**（**AL**）的概念、技术和工具。
- en: In SSL, we are given a few labeled examples and many unlabeled ones—the goal
    is either to simply train on the labeled ones in order to classify the unlabeled
    ones (transductive SSL), or use the unlabeled and labeled examples to train models
    to correctly classify new, unseen data (inductive SSL). All techniques in SSL
    are based on one or more of the assumptions related to semi-supervised smoothness,
    cluster togetherness, and manifold togetherness.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 在SSL中，我们被给予一些标记的示例和许多未标记的示例——目标是要么简单地训练标记的示例以对未标记的示例进行分类（归纳SSL），要么使用未标记和标记的示例来训练模型以正确分类新的、未见过的数据（归纳SSL）。SSL中的所有技术都基于与半监督平滑性、聚类一致性和流形一致性相关的假设之一或多个。
- en: Different SSL techniques are applicable to different situations. The simple
    self-training SSL is straightforward and works with most supervised learning algorithms;
    when the data is from more than just one domain, the co-training SSL is a suitable
    method. When the cluster togetherness assumption holds, the cluster and label
    SSL technique can be used; a "closeness" measure is exploited by transductive
    graph label propagation, which can be computationally expensive. Transductive
    SVM performs well with linear or non-linear data and we see an example of training
    a TSVM with a Gaussian kernel on the UCI Breast Cancer dataset using the `JKernelMachines`
    library. We present experiments comparing SSL models using the graphical Java
    tool KEEL in the concluding part of the SSL portion of the chapter.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的半监督学习（SSL）技术适用于不同的场景。简单的自训练SSL方法直接且与大多数监督学习算法兼容；当数据来自多个领域时，协同训练SSL是一个合适的方法。当簇内聚假设成立时，可以使用簇和标签SSL技术；通过传递图标签传播利用“接近度”度量，这可能计算成本较高。传递SVM在线性或非线性数据上表现良好，我们通过使用`JKernelMachines`库在UCI乳腺癌数据集上训练具有高斯核的TSVM的例子来展示。在章节SSL部分的结尾部分，我们展示了使用图形Java工具KEEL比较SSL模型的实验。
- en: We introduced active learning (AL) in the second half of the chapter. In this
    type of learning, various strategies are used to query the unlabeled portion of
    the dataset in order to present the expert with examples that will prove most
    effective in learning from the entire dataset. As the expert, or oracle, provides
    the labels to the selected instances, the learner steadily improves its ability
    to generalize. The techniques of AL are characterized by the choice of classifier,
    or committee of classifiers, and importantly, on the querying strategy chosen.
    These strategies include uncertainty sampling, where the instances with the least
    confidence are queries, version sampling, where a subset of the hypotheses that
    explain the training data are selected, and data distribution sampling, which
    involves improving the model by selections that would decrease the generalization
    error. We presented a case study using the UCI abalone dataset to demonstrate
    active learning in practice. The tool used here is the JCLAL Java framework for
    active learning.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在章节的后半部分介绍了主动学习（AL）。在这种学习中，使用各种策略查询数据集的无标签部分，以便向专家展示将证明从整个数据集中学习最有效的示例。作为专家或先知，为所选实例提供标签后，学习者稳步提高其泛化能力。AL技术以分类器或分类器委员会的选择以及重要的查询策略选择为特征。这些策略包括不确定性采样，其中查询最没有信心的实例，版本采样，其中选择解释训练数据的假设子集，以及数据分布采样，这涉及通过选择会减少泛化错误的选项来改进模型。我们通过使用UCI鲍鱼数据集的案例研究来展示主动学习的实际应用。这里使用的工具是用于主动学习的JCLAL
    Java框架。
- en: References
  id: totrans-466
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Yarowsky, D (1995). *Unsupervised word sense disambiguation rivaling supervised
    methods*. Proceedings of the 33rd Annual Meeting of the Association for Computational
    Linguistics (pp. 189–196)
  id: totrans-467
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yarowsky, D (1995). *无监督词义消歧与监督方法相媲美*。第33届计算语言学协会年会论文集（第189–196页）
- en: 'Blum, A., and Mitchell, T (1998). *Combining labeled and unlabeled data with
    co-training*. COLT: Proceedings of the Workshop on Computational Learning Theory.'
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Blum, A.，and Mitchell, T (1998). *结合有标签和无标签数据与协同训练*。COLT：计算学习理论研讨会论文集。
- en: Demiriz, A., Bennett, K., and Embrechts, M (1999). *Semi-supervised clustering
    using genetic algorithms*. Proceedings of Artificial Neural Networks in Engineering.
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Demiriz, A.，Bennett, K.，and Embrechts, M (1999). *使用遗传算法进行半监督聚类*。人工神经网络在工程中的应用研讨会论文集。
- en: Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux (2006). *Label Propagation
    and Quadratic Criterion*. In Semi-Supervised Learning, pp. 193-216
  id: totrans-470
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux (2006). *标签传播和二次准则*。在半监督学习，第193-216页
- en: T. Joachims (1998). *Transductive Inference for Text Classiﬁcation using Support
    Vector Machines*, ICML.
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T. Joachims (1998). *基于支持向量机的文本分类的归纳推理*，ICML.
- en: 'B. Settles (2008). *Curious Machines: Active Learning with Structured Instances*.
    PhD thesis, University of Wisconsin–Madison.'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B. Settles (2008). *好奇的机器：具有结构实例的主动学习*。威斯康星大学麦迪逊分校博士论文。
- en: D. Angluin (1988). *Queries and concept learning*. Machine Learning, 2:319–342.
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Angluin (1988). *查询和概念学习*。机器学习，2:319–342.
- en: D. Lewis and W. Gale (1994). *A sequential algorithm for training text classifiers*.
    In Proceedings of the ACM SIGIR Conference on Research and Development in Information
    Retrieval, pages 3–12\. ACM/Springer.
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Lewis 和 W. Gale (1994). *训练文本分类器的顺序算法*. 见《ACM SIGIR信息检索研究与发展会议论文集》（ACM SIGIR
    Conference on Research and Development in Information Retrieval），第3–12页。ACM/Springer。
- en: H.S. Seung, M. Opper, and H. Sompolinsky (1992). *Query by committee*. In Proceedings
    of the ACM Workshop on Computational Learning Theory, pages 287–294.
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: H.S. Seung, M. Opper, 和 H. Sompolinsky (1992). *委员会查询*. 见《计算学习理论研讨会论文集》（ACM
    Workshop on Computational Learning Theory），第287–294页。
- en: D. Cohn, L. Atlas, R. Ladner, M. El-Sharkawi, R. Marks II, M. Aggoune, and D.
    Park (1992). *Training connectionist networks with queries and selective sampling*.
    In Advances in Neural Information Processing Systems (NIPS). Morgan Kaufmann.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Cohn, L. Atlas, R. Ladner, M. El-Sharkawi, R. Marks II, M. Aggoune, and D.
    Park (1992). *使用查询和选择性采样训练连接主义网络*. 见《神经网络信息处理系统进展》（NIPS），摩根考夫曼出版社。
- en: D. Cohn, L. Atlas, and R. Ladner (1994). *Improving generalization with active
    learning*. Machine Learning, 15(2):201–221.
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Cohn, L. Atlas, 和 R. Ladner (1994). *通过主动学习提高泛化能力*. 机器学习，15(2):201–221。
- en: D. Lewis and J. Catlett (1994). *Heterogeneous uncertainty sampling for supervised
    learning*. In Proceedings of the International Conference on Machine Learning
    (ICML), pages 148–156\. Morgan Kaufmann.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Lewis 和 J. Catlett (1994). *用于监督学习的异质不确定性采样*. 见《国际机器学习会议论文集》（ICML），第148–156页。摩根考夫曼出版社。
- en: S. Dasgupta, A. Kalai, and C. Monteleoni (2005). *Analysis of perceptron-based
    active learning*. In Proceedings of the Conference on Learning Theory (COLT),
    pages 249–263\. Springer.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S. Dasgupta, A. Kalai, 和 C. Monteleoni (2005). *基于感知器的主动学习分析*. 见《学习理论会议论文集》（COLT），第249–263页。斯普林格出版社。
- en: S. Dasgupta, D. Hsu, and C. Monteleoni (2008). *A general agnostic active learning
    algorithm*. In Advances in Neural Information Processing Systems (NIPS), volume
    20, pages 353–360\. MIT Press.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S. Dasgupta, D. Hsu, 和 C. Monteleoni (2008). *一个通用的无监督主动学习算法*. 见《神经网络信息处理系统进展》（NIPS），第20卷，第353–360页。麻省理工学院出版社。
- en: T. Mitchell (1982). *Generalization as search*. Artificial Intelligence, 18:203–226.
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T. Mitchell (1982). *泛化作为搜索*. 人工智能，18:203–226。
