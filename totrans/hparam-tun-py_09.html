<html><head></head><body>
		<div id="_idContainer308">
			<h1 id="_idParaDest-57"><em class="italic"><a id="_idTextAnchor062"/>Chapter 7</em><span class="superscript">: Hyperparameter Tuning via Scikit</span></h1>
			<p><strong class="source-inline">scikit-learn</strong> is one of the Python packages that is used the most by data scientists. This package provides a range of <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>)-related modules that are ready to be used with minimum effort, including for the task of hyperparameter tuning. One of the main selling points of this package is its consistent interface across many implemented classes, which almost every data scientist loves! Apart from <strong class="source-inline">scikit-learn</strong>, there are also other packages for the hyperparameter tuning task that are built on top of <strong class="source-inline">scikit-learn</strong> or mimic the interface of <strong class="source-inline">scikit-learn</strong>, such as <strong class="source-inline">scikit-optimize</strong> and <strong class="source-inline">scikit-hyperband</strong>.</p>
			<p>In this chapter, we’ll learn about all of the important things to do with <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">scikit-optimize</strong>, and <strong class="source-inline">scikit-hyperband</strong>, along with how to utilize them to implement the hyperparameter tuning methods that we learned about in the previous chapters. We’ll start by walking through how to install each of the packages. Then, we’ll learn not only how to utilize those packages with their default configurations but also discuss the available configurations along with their usage. Additionally, we’ll discuss how the implementation of the hyperparameter tuning methods is related to the theory that we learned in previous chapters, as there might be some minor differences or adjustments made in the implementation.</p>
			<p>Finally, equipped with the knowledge from previous chapters, you will also be able to understand what’s happening if there are errors or unexpected results and understand how to set up the method configuration to match your specific problem.</p>
			<p>In this chapter, we’ll be covering the following main topics:</p>
			<ul>
				<li>Introducing scikit</li>
				<li>Implementing Grid Search</li>
				<li>Implementing Random Search</li>
				<li>Implementing Coarse-to-Fine Search</li>
				<li>Implementing Successive Halving</li>
				<li>Implementing Hyper Band </li>
				<li>Implementing Bayesian Optimization Gaussian Process</li>
				<li>Implementing Bayesian Optimization Random Forest</li>
				<li>Implementing Bayesian Optimization Gradient Boosted Trees</li>
			</ul>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor063"/>Technical requirements</h1>
			<p>We will learn how to implement various hyperparameter tuning methods with <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">scikit-optimize</strong>, and <strong class="source-inline">scikit-hyperband</strong>. To ensure that you can reproduce the code examples in this chapter, you will require the following:</p>
			<ul>
				<li>Python 3 (version 3.7 or above)</li>
				<li>An installed <strong class="source-inline">Pandas</strong> package (version 1.3.4 or above)</li>
				<li>An installed <strong class="source-inline">NumPy</strong> package (version 1.21.2 or above)</li>
				<li>An installed <strong class="source-inline">Scipy</strong> package (version 1.7.3 or above)</li>
				<li>An installed <strong class="source-inline">Matplotlib</strong> package (version 3.5.0 or above)</li>
				<li>An installed <strong class="source-inline">scikit-learn</strong> package (version 1.0.1 or above)</li>
				<li>An installed <strong class="source-inline">scikit-optimize</strong> package (version 0.9.0 or above)</li>
				<li>An installed <strong class="source-inline">scikit-hyperband</strong> package (directly cloned from the GitHub repository)</li>
			</ul>
			<p>All of the code examples for this chapter can be found on GitHub at <a href="https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python">https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python</a>.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor064"/>Introducing Scikit</h1>
			<p><strong class="source-inline">scikit-learn</strong>, which is <a id="_idIndexMarker328"/>commonly called <strong class="bold">Sklearn</strong>, is <a id="_idIndexMarker329"/>a very popular open source package in Python that is widely used for ML-related tasks, starting from data preprocessing, model training and evaluation, model selection, hyperparameter tuning, and more. One of the main selling points of the <strong class="source-inline">sklearn</strong> package is the consistency of its interface across many implemented classes. </p>
			<p>For example, all of the implemented ML models, or <strong class="bold">estimators</strong>, in <strong class="source-inline">sklearn</strong> have the same <strong class="source-inline">fit()</strong> and <strong class="source-inline">predict()</strong> methods for fitting the model on the training data and evaluating the fitted model on the test data, respectively. When working with data preprocessors, or <strong class="bold">transformers</strong>, in <strong class="source-inline">sklearn</strong>, the typical method that every preprocessor has is the <strong class="source-inline">fit()</strong>, <strong class="source-inline">transform()</strong>, and <strong class="source-inline">fit_transform()</strong> methods for fitting the preprocessor, transforming new data with the fitted preprocessor, and fitting and directly transforming the data that is used to fit the preprocessor, respectively. </p>
			<p>In <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Evaluating Machine Learning Models</em>, we learned how <strong class="source-inline">sklearn</strong> can be utilized to evaluate the performance of ML models through the concept of cross-validation, where the <a id="_idIndexMarker330"/>full data is split into several parts, such as train, validation, and test data. In <em class="italic">Chapters 3–6</em>, we always used the cross-validation score as our objective function. While we can manually perform hyperparameter tuning and calculate the cross-validation score based on the split data, <strong class="source-inline">sklearn</strong> provides dedicated classes for hyperparameter tuning that use the cross-validation score as the objective function during the tuning process. There are several hyperparameter tuning classes implemented in <strong class="source-inline">sklearn</strong>, such as <strong class="source-inline">GridSearchCV</strong>, <strong class="source-inline">RandomizedSearchCV</strong>, <strong class="source-inline">HalvingGridSearchCV</strong>, and <strong class="source-inline">HalvingRandomSearchCV</strong>.</p>
			<p>Also, all of the hyperparameter tuning classes implemented in <strong class="source-inline">sklearn</strong> have a consistent interface. We can use the <strong class="source-inline">fit()</strong> method to perform hyperparameter tuning on the given data where the cross-validation score is used as the objective function. Then, we can use the <strong class="source-inline">best_params_</strong> attribute to get the best set of hyperparameters, the <strong class="source-inline">best_score_</strong> attribute to get the average cross-validated score from the best set of hyperparameters, and the <strong class="source-inline">cv_results_</strong> attribute to get the details of the hyperparameter tuning process, including but not limited to the objective function score for each tested set of hyperparameters in each of the folds.</p>
			<p>To prevent data leakage when performing the data preprocessing steps (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Evaluating Machine Learning Models</em>), <strong class="source-inline">sklearn</strong> also provides a <strong class="source-inline">Pipeline</strong> object that can be used along with the hyperparameter tuning classes. This <strong class="source-inline">Pipeline</strong> object will ensure that any data preprocessing steps are only fitted based on the train set during the cross-validation. Essentially, this object is just <em class="italic">a chain of several </em><strong class="source-inline">sklearn</strong><em class="italic"> transformers and estimators</em>, which has the same <strong class="source-inline">fit()</strong> and <strong class="source-inline">predict()</strong> method, just like a usual <strong class="source-inline">sklearn</strong> estimator.</p>
			<p>While <strong class="source-inline">sklearn</strong> can be utilized for many ML-related tasks, <strong class="source-inline">scikit-optimize</strong>, which is <a id="_idIndexMarker331"/>commonly called <strong class="bold">skopt</strong>, is a package built on top of <strong class="source-inline">sklearn</strong> and can be utilized for implementing the <strong class="bold">Sequential Model-Based Optimization</strong> (<strong class="bold">SMBO</strong>) methods (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>). <strong class="source-inline">skopt</strong> has a very similar interface to <strong class="source-inline">sklearn</strong>, so it will be very easy for you to get familiar with <strong class="source-inline">skopt</strong> once you are already familiar with <strong class="source-inline">sklearn</strong> itself. The main <a id="_idIndexMarker332"/>hyperparameter tuning class implemented in <strong class="source-inline">skopt</strong> is the <strong class="source-inline">BayesSearchCV</strong> class. </p>
			<p><strong class="source-inline">skopt</strong> provides <a id="_idIndexMarker333"/>four implementations for the optimizer within <a id="_idIndexMarker334"/>the <strong class="source-inline">BayesSearchCV</strong> class, namely <strong class="bold">Gaussian Process</strong> (<strong class="bold">GP</strong>), <strong class="bold">Random Forest</strong> (<strong class="bold">RF</strong>), <strong class="bold">Gradient Boosted Regression Trees</strong> (<strong class="bold">GBRT</strong>), and <strong class="bold">Extra Trees</strong> (<strong class="bold">ET</strong>). Furthermore, you <a id="_idIndexMarker335"/>can also <a id="_idIndexMarker336"/>use any other regressors from <strong class="source-inline">sklearn</strong> to be utilized <a id="_idIndexMarker337"/>as the optimizer. Note that, here, the optimizer refers to the <strong class="bold">surrogate model</strong> that we learned in <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">,Exploring Bayesian Optimization</em>. Additionally, <strong class="source-inline">skopt</strong> provides <a id="_idIndexMarker338"/>various <a id="_idIndexMarker339"/>implementations <a id="_idIndexMarker340"/>of the <strong class="bold">acquisition function</strong>, namely the <strong class="bold">Expected Improvement</strong> (<strong class="bold">EI</strong>), <strong class="bold">Probability of Improvement</strong> (<strong class="bold">PI</strong>), and <strong class="bold">Lower Confidence Bound</strong> (<strong class="bold">LCB</strong>) functions.</p>
			<p>Last but <a id="_idIndexMarker341"/>not least, the <strong class="source-inline">scikit-hyperband</strong> package. Additionally, this package is built on top of <strong class="source-inline">sklearn</strong> and is specifically designed for the HB implementation. The hyperparameter tuning class implemented in this package is <strong class="source-inline">HyperbandSearchCV</strong>. It also has a very similar interface to <strong class="source-inline">sklearn</strong>.</p>
			<p>As for <strong class="source-inline">sklearn</strong> and <strong class="source-inline">skopt</strong>, you can easily install them via <strong class="source-inline">pip install</strong>, just like you usually install other packages. As for <strong class="source-inline">scikit-hyperband</strong>, the author of the package didn’t put this on <strong class="bold">PyPI</strong>, which means you have to install the package directly from the GitHub repository. Furthermore, at the time of writing, the last update of the GitHub repo (<a href="https://github.com/thuijskens/scikit-hyperband">https://github.com/thuijskens/scikit-hyperband</a>) was in 2020. There are several blocks of code that are no longer compatible with the newer version of <strong class="source-inline">sklearn</strong>. Luckily, there’s a forked version (<a href="https://github.com/louisowen6/scikit-hyperband">https://github.com/louisowen6/scikit-hyperband</a>) of the original repo that works nicely with the newer version of <strong class="source-inline">sklearn </strong>(<strong class="source-inline">1.0.1</strong> or above). To install <strong class="source-inline">scikit-hyperband</strong>, please follow the following steps:</p>
			<ol>
				<li>Clone <a href="https://github.com/louisowen6/scikit-hyperband">https://github.com/louisowen6/scikit-hyperband</a> to your loca machinel:<p class="source-code">git clone https://github.com/louisowen6/scikit-hyperband</p></li>
				<li>Open the cloned repository:<p class="source-code">cd scikit-hyperband</p></li>
				<li>Move the <strong class="source-inline">hyperband</strong> folder to your working directory:<p class="source-code">mv hyperband "path/to/your/working/directory"</p></li>
			</ol>
			<p>Now that you <a id="_idIndexMarker342"/>are aware of the <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">scikit-optimize</strong>, and <strong class="source-inline">scikit-hyperband</strong> packages, in the following sections, we will learn how to utilize them to implement various hyperparameter tuning methods.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor065"/>Implementing Grid Search</h1>
			<p>To implement <strong class="bold">Grid Search</strong> (see <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a><em class="italic">, Exploring Exhaustive Search</em>), we can actually write our own code from scratch since <a id="_idIndexMarker343"/>it is just a simple nested <strong class="source-inline">for loop</strong> that tests all of the possible hyperparameter values in the search space. However, by using <strong class="source-inline">sklearn</strong>’s implementation of Grid Search, <strong class="source-inline">GridSearchCV</strong>, we can have a cleaner code since we just need to call a single line of code to instantiate the class. </p>
			<p>Let’s walk through an example of how we can utilize <strong class="source-inline">GridSearchCV</strong> to perform Grid Search. Note that, in this example, we are performing hyperparameter tuning on an RF model. We will utilize <strong class="source-inline">sklearn’s</strong> implementation of RF, <strong class="source-inline">RandomForestClassifier</strong>. The dataset <a id="_idIndexMarker344"/>used in this example is the <em class="italic">Banking Dataset – Marketing Targets</em> provided on Kaggle (<a href="https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets">https://www.kaggle.com/datasets/prakharrathi25/banking-dataset-marketing-targets</a>). </p>
			<p class="callout-heading">Original Data Source</p>
			<p class="callout">This data was first published in <em class="italic">A Data-Driven Approach to Predict the Success of Bank Telemarketing</em>, by Sérgio Moro, Paulo Cortez, and Paulo Rita, Decision Support Systems, Elsevier, 62:22–31, June 2014 (<a href="https://doi.org/10.1016/j.dss.2014.03.001">https://doi.org/10.1016/j.dss.2014.03.001</a>).</p>
			<p>This is a binary classification dataset with 16 features related to the marketing campaigns conducted by a bank institution. The target variable consists of two classes, <em class="italic">yes</em> or <em class="italic">no</em>, indicating whether the client of the bank has subscribed to a term deposit or not. Hence, the goal of training an ML model on this dataset is to identify whether a customer is potentially wanting to subscribe to the term deposit or not. For more details, you can <a id="_idIndexMarker345"/>refer to the description on the Kaggle page: </p>
			<ol>
				<li value="1">There are two datasets provided, namely the <strong class="source-inline">train.csv</strong> dataset and the <strong class="source-inline">test.csv</strong> dataset. However, we will not use the provided <strong class="source-inline">test.csv</strong> dataset since it is sampled directly from the train data. We will manually split <strong class="source-inline">train.csv</strong> into two subsets, namely the train set and the test set, using the help of the <strong class="source-inline">train_test_split</strong> function from <strong class="source-inline">sklearn</strong> (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Evaluating Machine Learning Models</em>). We will set the <strong class="source-inline">test_size</strong> parameter to <strong class="source-inline">0.1</strong>, meaning we will have <strong class="source-inline">40,689</strong> and <strong class="source-inline">4,522</strong> rows for the train set and the test set, respectively. The following code shows you how to load the data and perform the train set and the test set splitting:<p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">df = pd.read_csv("train.csv",sep=";")</p><p class="source-code">df_train, df_test = train_test_split(df, test_size=0.1, random_state=0)</p></li>
			</ol>
			<p>Out of the 16 features provided in the data, there are 7 numerical features and 9 categorical features. As for the target class distribution, 12% of them are <em class="italic">yes</em> and 88% of them are <em class="italic">no</em>, for both train and test datasets. This means that we can’t use accuracy as our metric since we have an imbalanced class problem—a situation where we have a very skewed distribution of the target classes. Instead, in this example, we will use the F1-score.</p>
			<ol>
				<li value="2">Before performing Grid Search, let’s see how <em class="italic">RandomForestClassifier</em> with the default hyperparameter values work. Furthermore, let’s also try to train our model on only those seven numerical features for now. The following code shows you how to get only numerical features, train the model on those features in the train set, and finally, evaluate the model on the test set:<p class="source-code">import numpy as np</p><p class="source-code">from sklearn.ensemble import RandomForestClassifier</p><p class="source-code">from sklearn.metrics import f1_score</p></li>
			</ol>
			<p>The <strong class="source-inline">X_train_numerical</strong> variable only stores numerical features from the train data:</p>
			<p class="source-code">X_train_numerical = df_train.select_dtypes(include=np.number).drop(columns=['y'])</p>
			<p class="source-code">y_train = df_train['y']</p>
			<p>The <strong class="source-inline">X_test_numerical</strong> variable <a id="_idIndexMarker346"/>only stores numerical features from the test data:</p>
			<p class="source-code">X_test_numerical = df_test.select_dtypes(include=np.number).drop(columns=['y'])</p>
			<p class="source-code">y_test = df_test['y']</p>
			<p>Fit the model on train data:</p>
			<p class="source-code">model = RandomForestClassifier(random_state=0)</p>
			<p class="source-code">model.fit(X_train_numerical,y_train)</p>
			<p>Evaluate the model on the test data:</p>
			<p class="source-code">y_pred = model.predict(X_test_numerical)</p>
			<p class="source-code">print(f1_score(y_test, y_pred))</p>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.436</strong> for the F1-Score when testing our trained RF model on the test set. Remember that this is the result of only using numerical features and the default hyperparameters of the <strong class="source-inline">RandomForestClassifier</strong>.</p>
			<ol>
				<li value="3">Before performing Grid Search, we have to define the hyperparameter space in a dictionary of list format, where the keys refer to the name of the hyperparameters and the lists consist of all the values we want to test for each hyperparameter. Let’s say we define the hyperparameter space for <strong class="source-inline">RandomForestClassifier</strong> as follows:<p class="source-code">hyperparameter_space = {</p><p class="source-code">"n_estimators": [25,50,100,150,200],</p><p class="source-code">"criterion": ["gini", "entropy"],</p><p class="source-code">"max_depth": [3, 5, 10, 15, 20, None],</p><p class="source-code">"class_weight": ["balanced","balanced_subsample"],</p><p class="source-code">"min_samples_split": [0.01,0.1,0.25,0.5,0.75,1.0],</p><p class="source-code">}</p></li>
				<li>Once we have defined the hyperparameter space, we can apply the <strong class="source-inline">GridSearchCV</strong> class to the train data, use the best set of hyperparameters to train a new model <a id="_idIndexMarker347"/>on the full train data, and then evaluate that final trained model on the test data, just as we learned in <em class="italic">Chapters 3–6</em>. The following code shows you how to do that:<p class="source-code">from sklearn.model_selection import GridSearchCV</p></li>
			</ol>
			<p>Initiate the model:</p>
			<p class="source-code">model = RandomForestClassifier(random_state=0)</p>
			<p>Initiate the <strong class="source-inline">GridSearchCV</strong> class:</p>
			<p class="source-code">clf = GridSearchCV(model, hyperparameter_space, </p>
			<p class="source-code">                   scoring='f1', cv=5, </p>
			<p class="source-code">                   n_jobs=-1, refit = True)</p>
			<p>Run the <strong class="source-inline">GridSearchCV</strong> class:</p>
			<p class="source-code">clf.fit(X_train_numerical, y_train)</p>
			<p>Print the best set of hyperparameters:</p>
			<p class="source-code">print(clf.best_params_,clf.best_score_)</p>
			<p>Evaluate the final trained model on the test data:</p>
			<p class="source-code">print(clf.score(X_test_numerical,y_test))</p>
			<p>Look how clean our code is by utilizing <strong class="source-inline">sklearn</strong>’s implementation of Grid Search instead of writing our code from scratch! Notice that we just need to pass <strong class="source-inline">sklearn</strong>’s estimator and the hyperparameter space dictionary to the <strong class="source-inline">GridSearchCV</strong> class, and the rest will be handled by <strong class="source-inline">sklearn</strong>. In this example, we also pass several additional parameters to the class, such as <strong class="source-inline">scoring=’f1’</strong>, <strong class="source-inline">cv=5</strong>, <strong class="source-inline">n_jobs=-1</strong>, and <strong class="source-inline">refit=True</strong>. </p>
			<p>As its name suggests, the <strong class="source-inline">scoring</strong> parameter governs the scoring strategy that we want to use <a id="_idIndexMarker348"/>to evaluate our model during the cross-validation. While our objective function will always be the cross-validation score, this parameter controls what type of score we want to use as our metric. In this example, we are using the F1-score as our metric. However, you can also pass a custom callable function as the scoring strategy. </p>
			<p class="callout-heading">Available Scoring Strategies in Sklearn</p>
			<p class="callout">You can <a id="_idIndexMarker349"/>refer to <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter">https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter</a> for all of the implemented scoring strategies by <strong class="source-inline">sklearn</strong>, and refer to <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#scoring">https://scikit-learn.org/stable/modules/model_evaluation.html#scoring</a> if you want to implement your own custom scoring strategy.</p>
			<p>The <strong class="source-inline">cv</strong> parameter indicates how many folds of cross-validation you want to perform. The <strong class="source-inline">n_jobs</strong> parameter controls how many jobs you want to run in parallel. If you decide to use all of the processors, you can simply set <strong class="source-inline">n_jobs=–1</strong>, just as we did in the example. </p>
			<p>Last but not least, we have the <strong class="source-inline">refit</strong> parameter. This Boolean parameter is responsible for deciding whether at the end of the hyperparameter tuning process we want to refit our model on the full train set using the best set of hyperparameters or not. In this example, we set <strong class="source-inline">refit=True</strong>, meaning that <strong class="source-inline">sklearn</strong> will automatically refit our RF model on the full train set using the best set of hyperparameters. It is very important to retrain our model on the full train set after performing hyperparameter tuning since we only utilize subsets of the train set during the hyperparameter tuning process. There are several other parameters that you can control when initiating a <strong class="source-inline">GridSearchCV</strong> class. For more <a id="_idIndexMarker350"/>details, you can refer to the official page of <strong class="source-inline">sklearn</strong> (<a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html</a>).</p>
			<p>Let’s go back to our example. By performing Grid Search in the predefined hyperparameter space, we are able to get an F1-score of <strong class="source-inline">0.495</strong> when evaluated on the test set. The best <a id="_idIndexMarker351"/>set of hyperparameters is <strong class="source-inline">{‘class_weight’: ‘balanced’, ‘criterion’: ‘entropy’, ‘min_samples_split’: 0.01, ‘n_estimators’: 150}</strong> with an objective function score of <strong class="source-inline">0.493</strong>. Note that we can get the best set of hyperparameters along with its objective function score via the <strong class="source-inline">best_params_</strong> and <strong class="source-inline">best_score_</strong> attributes, respectively. Not bad! We get around <strong class="source-inline">0.06</strong> of improvement in the F1-score. However, note that we are still only using numerical features.</p>
			<p>Next, we will try to utilize not only numerical features but also categorical features from our data. To be able to utilize those categorical features, we need to perform the <strong class="bold">categorical encoding</strong> preprocessing <a id="_idIndexMarker352"/>step. Why? Because ML models are not able to understand non-numerical features. Therefore, we need to convert those non-numerical features into numerical ones so that the ML model is able to utilize those features.</p>
			<p>Remember that when we want to perform any data preprocessing steps, we have to be very careful with it to prevent any data leakage problem where we might introduce part of our test data into the train data (see <a href="B18753_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a><em class="italic">, Evaluating Machine Learning Models</em>). To prevent this problem, we can utilize the <strong class="source-inline">Pipeline</strong> object from <strong class="source-inline">sklearn</strong>. So, instead of passing an estimator to the <strong class="source-inline">GridSearchCV</strong> class, we can also pass a <strong class="source-inline">Pipeline</strong> object that consists of a chain of data preprocessors and an estimator:</p>
			<ol>
				<li value="1">Since, in this example, not all of our features are categorical and we only want to perform categorical encoding on those non-numerical features, we can utilize the <strong class="source-inline">ColumnTransformer</strong> class to specify which features we want to apply the categorical encoding step. Let’s say we also want to perform a normalization preprocessing step on the numerical features. We can also pass those numerical features to the <strong class="source-inline">ColumnTransformer</strong> class along with the normalization transformer. Then, it will automatically apply the normalization step to only those numerical features. The following code shows you how to create such a <strong class="source-inline">Pipeline</strong> object with <strong class="source-inline">ColumnTransformer</strong>, where we use <strong class="source-inline">StandardScaler</strong> for the normalization step and <strong class="source-inline">OneHotEncoder</strong> for the categorical encoding step:<p class="source-code">from sklearn.preprocessing import StandardScaler, OneHotEncoder</p><p class="source-code">from sklearn.compose import ColumnTransformer</p><p class="source-code">from sklearn.pipeline import Pipeline</p></li>
			</ol>
			<p>Get list of <a id="_idIndexMarker353"/>numerical features and categorical features:</p>
			<p class="source-code">numerical_feats = list(df_train.drop(columns='y').select_dtypes(include=np.number).columns)</p>
			<p class="source-code">categorical_feats = list(df_train.drop(columns='y').select_dtypes(exclude=np.number).columns)</p>
			<p>Initiate the preprocessor for numerical features and categorical features:</p>
			<p class="source-code">numeric_preprocessor = StandardScaler()</p>
			<p class="source-code">categorical_preprocessor = OneHotEncoder(handle_unknown="ignore")</p>
			<p>Delegate each preprocessor to the corresponding features:</p>
			<p class="source-code">preprocessor = ColumnTransformer(</p>
			<p class="source-code">    transformers=[</p>
			<p class="source-code">        ("num", numeric_preprocessor, numerical_feats),</p>
			<p class="source-code">        ("cat", categorical_preprocessor, categorical_feats),</p>
			<p class="source-code">    ])</p>
			<p>Create a pipeline of preprocessors and models. In this example, we named our pr-processing steps as <em class="italic">“preprocessor”</em> and the modeling step as <em class="italic">“model”</em>:</p>
			<p class="source-code">pipe = Pipeline(</p>
			<p class="source-code">    steps=[("preprocessor", preprocessor), </p>
			<p class="source-code">           ("model", RandomForestClassifier(random_state=0))])</p>
			<p>As you can see in the previous code blocks, the <strong class="source-inline">ColumnTransformer</strong> class is responsible <a id="_idIndexMarker354"/>for delegating each preprocessor to the corresponding features. Then, we can just reuse it for all of our preprocessing steps through a single preprocessor variable. Finally, we can create a pipeline consisting of the preprocessor variable and <strong class="source-inline">RandomForestClassifier</strong>. Note that within the <strong class="source-inline">ColumnTransformer</strong> class and the <strong class="source-inline">Pipeline</strong> class, we also have to provide the name of each preprocessor and step in the pipeline, respectively. </p>
			<ol>
				<li value="2">Now that we have defined the pipeline, we can see how our model performs on the test set (without hyperparameter tuning) by utilizing all of the features and preprocessors defined in the pipeline. The following code shows how we can directly use the pipeline to perform the same <strong class="source-inline">fit()</strong> and <strong class="source-inline">predict()</strong> methods as we did earlier: <p class="source-code"><strong class="bold">pipe</strong>.fit(<strong class="bold">X_train_full</strong>,y_train)</p><p class="source-code">y_pred = <strong class="bold">pipe</strong>.predict(<strong class="bold">X_test_full</strong>)</p><p class="source-code">print(f1_score(y_test, y_pred))</p></li>
			</ol>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.516</strong> for the F1-score when testing our trained pipeline on the test set. </p>
			<ol>
				<li value="3">Next, we can start performing Grid Search over the pipeline, too. However, before we can do that, we need to redefine the hyperparameter space. We need to change the keys in the dictionary with the format of <strong class="source-inline">&lt;estimator_name_in_pipeline&gt;__&lt;hyperparameter_name&gt;</strong>. The following is the redefined version of our hyperparameter space:<p class="source-code">hyperparameter_space = { </p><p class="source-code">"model__n_estimators": [25,50,100,150,200], </p><p class="source-code">"model__criterion": ["gini", "entropy"], </p><p class="source-code">"model__class_weight": ["balanced", "balanced_subsample"],</p><p class="source-code">"model__min_samples_split": [0.01,0.1,0.25,0.5,0.75,1.0], </p><p class="source-code">}</p></li>
				<li>The following code shows you how to perform Grid Search over the pipeline instead of <a id="_idIndexMarker355"/>the estimator itself. Essentially, the code is the same as the previous version. The only difference is that we are performing the Grid Search over the pipeline and on <em class="italic">all</em> of the features in the data, not just the numerical features.</li>
			</ol>
			<p>Initiate the <strong class="source-inline">GridSearchCV</strong> class:</p>
			<p class="source-code">clf = GridSearchCV(<strong class="bold">pipe</strong>, hyperparameter_space, </p>
			<p class="source-code">                   scoring = 'f1', cv=5, </p>
			<p class="source-code">                   n_jobs=-1, refit = True</p>
			<p>Run the <strong class="source-inline">GridSearchCV</strong> class:</p>
			<p class="source-code">clf.fit(<strong class="bold">X_train_full</strong>, y_train)</p>
			<p>Print the best set of hyperparameters:</p>
			<p class="source-code">print(clf.best_params_, clf.best_score_)</p>
			<p>Evaluate the final trained model on the test data:</p>
			<p class="source-code">print(clf.score(<strong class="bold">X_test_full</strong>, y_test))</p>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.549</strong> for the F1-Score when testing our final trained RF model with the best set of hyperparameters on the test set. The best set of hyperparameters is <strong class="source-inline">{‘model__class_weight’: ‘balanced_subsample’, ‘model__criterion’: ‘gini’, ‘model__min_samples_split’: 0.01, ‘model__n_estimators’: 100}</strong> with an objective function score of <strong class="source-inline">0.549</strong>.</p>
			<p>It is worth noting that we can also <em class="italic">create a pipeline within a pipeline</em>. For example, we can create a pipeline for <strong class="source-inline">numeric_preprocessor</strong> that consists of a chain of missing value imputation <a id="_idIndexMarker356"/>and normalization modules. The following code shows how we can create such a pipeline. The <strong class="source-inline">SimpleImputer</strong> class is the missing value imputation transformer from <strong class="source-inline">sklearn</strong> that can help us to perform mean, median, mode, or constant imputation strategies if there are any missing values:</p>
			<pre class="source-code">from sklearn.impute import SimpleImputer</pre>
			<pre class="source-code">numeric_preprocessor = Pipeline(</pre>
			<pre class="source-code">steps=[("missing_value_imputation", SimpleImputer(strategy="mean")),     ("normalization", StandardScaler())]</pre>
			<pre class="source-code">)</pre>
			<p>In this section, we have learned how to implement Grid Search in <strong class="source-inline">sklearn</strong> through the <strong class="source-inline">GridSearchCV</strong> class, starting from defining the hyperparameter space, setting each important parameter of the <strong class="source-inline">GridSearchCV</strong> class, learning how to utilize the <strong class="source-inline">Pipeline</strong> and <strong class="source-inline">ColumnTransformer</strong> classes to prevent data leakage issues, and learning how to create a pipeline within the pipeline. </p>
			<p>In the next section, we will learn how to implement Random Search in <strong class="source-inline">sklearn</strong> via <strong class="source-inline">RandomizedSearchCV</strong>.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor066"/>Implementing Random Search</h1>
			<p>Implementing <strong class="bold">Random Search</strong> (see <a href="B18753_03_ePub.xhtml#_idTextAnchor031"><em class="italic">Chapter 3</em></a><em class="italic">, Exploring Exhaustive Search</em>) in <strong class="source-inline">sklearn</strong> is very similar to implementing Grid Search. The main difference is that we have to provide the number of trials <a id="_idIndexMarker357"/>or iterations since Random Search will not try all of the possible combinations in the hyperparameter space. Additionally, we have to provide the accompanying distribution for each of the hyperparameters when defining the search space. In <strong class="source-inline">sklearn</strong>, Random Search is implemented in the <strong class="source-inline">RandomizedSearchCV</strong> class.</p>
			<p>To understand how we can implement Random Search in <strong class="source-inline">sklearn</strong>, let’s use the same example from the <em class="italic">Implementing Grid Search</em> section. Let’s directly try using all of the features available in the dataset. All of the pipeline creation processes are exactly the same, so we will directly jump into the process of how to define the hyperparameter space and the <strong class="source-inline">RandomizedSearchCV</strong> class. The following code shows you how to define the accompanying distribution for each of the hyperparameters in the space: </p>
			<pre class="source-code">from scipy.stats import randint, truncnorm</pre>
			<pre class="source-code">hyperparameter_space = { </pre>
			<pre class="source-code">"model__n_estimators": randint(5, 200), </pre>
			<pre class="source-code">"model__criterion": ["gini", "entropy"],</pre>
			<pre class="source-code">"model__class_weight": ["balanced","balanced_subsample"],</pre>
			<pre class="source-code">"model__min_samples_split": truncnorm(a=0,b=0.5,loc=0.005, scale=0.01),</pre>
			<pre class="source-code">}</pre>
			<p>As you can see, the hyperparameter space is quite different from the one that we defined previously in the <em class="italic">Implementing Grid Search</em> section. Here, we are also specifying the distribution for each of the hyperparameters, where <strong class="source-inline">randint</strong> and <strong class="source-inline">truncnorm</strong> are utilized for the <strong class="source-inline">n_estimators</strong> and <strong class="source-inline">min_samples_split</strong> hyperparameters. As for <strong class="source-inline">criterion</strong> and <strong class="source-inline">class_weight</strong>, we are still using the same configuration as the previous search space. Note that <em class="italic">by not specifying any distribution</em> means we are <em class="italic">applying uniform distribution</em> to the hyperparameter, where all of the values will have the same probability to be tested. </p>
			<p>Essentially, the <strong class="source-inline">randint</strong> distribution is just a uniform distribution for discrete variables, while <strong class="source-inline">truncnorm</strong> stands for truncated normal distribution, which, as its name suggests, is a modified normal distribution bounded on a particular range. In this example, the range is bounded on a range from <strong class="source-inline">a=0</strong> and <strong class="source-inline">b=0.5</strong>, with a mean of <strong class="source-inline">loc=0.005</strong> and a standard deviation of <strong class="source-inline">scale=0.01</strong>. </p>
			<p class="callout-heading">Distribution for Hyperparameters</p>
			<p class="callout">There are many other available distributions that you can utilize. <strong class="source-inline">sklearn</strong> accepts all distributions that have the <strong class="source-inline">rvs</strong> method, as in the distribution implementation from <strong class="source-inline">Scipy</strong>. Essentially, this method is just a method to sample a value from the specified distribution. For more details, please <a id="_idIndexMarker358"/>refer to the official documentation page of <strong class="source-inline">Scipy</strong> (<a href="https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions">https://docs.scipy.org/doc/scipy/reference/stats.html#probability-distributions</a>).</p>
			<p>When initiating the <strong class="source-inline">RandomizedSearchCV</strong> class, we also have to define the <strong class="source-inline">n_iter</strong> and <strong class="source-inline">random_state</strong> parameters, which refer to the number of iterations and the random seed, respectively. The following code shows you how to perform Random Search over the <a id="_idIndexMarker359"/>same pipeline defined in the <em class="italic">Implementing Grid Search</em> section. In contrast with the example in the <em class="italic">Implementing Grid Search</em> section, which only performs <strong class="source-inline">120</strong> iterations of Grid Search, here, we perform <strong class="source-inline">200</strong> iterations of random search since we set <strong class="source-inline">n_iter=200</strong>. Additionally, we have a bigger hyperparameter space since we increase the granularity of the <strong class="source-inline">n_estimators</strong> and <strong class="source-inline">min_samples_split</strong> hyperparameter values:</p>
			<pre class="source-code">from sklearn.model_selection import RandomizedSearchCV</pre>
			<p>Initiate the  <strong class="source-inline">RandomizedSearchCV</strong> class:</p>
			<pre class="source-code">clf = <strong class="bold">RandomizedSearchCV</strong>(pipe, hyperparameter_space, </pre>
			<pre class="source-code">                         <strong class="bold">n_iter = 200</strong>, <strong class="bold">random_state = 0</strong>,</pre>
			<pre class="source-code">                         scoring = 'f1', cv=5, </pre>
			<pre class="source-code">                         n_jobs=-1, refit = True)</pre>
			<p>Run the <strong class="source-inline">RandomizedSearchCV</strong> class:</p>
			<pre class="source-code">clf.fit(X_train_full, y_train)</pre>
			<p>Print the best set of hyperparameters:</p>
			<pre class="source-code">print(clf.best_params_, clf.best_score_)</pre>
			<p>Evaluate the final trained model on the test data:</p>
			<pre class="source-code">print(clf.score(X_test_full, y_test))</pre>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.563</strong> for the F1-score when testing our final trained <a id="_idIndexMarker360"/>RF model with the best set of hyperparameters on the test set. The best set of hyperparameters is <strong class="source-inline">{‘model__class_weight’: ‘balanced_subsample’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.005155815445940717, ‘model__n_estimators’: 187}</strong> with an objective function score of <strong class="source-inline">0.562</strong>.</p>
			<p>In this section, we have learned how to implement Random Search in <strong class="source-inline">sklearn</strong> through the <strong class="source-inline">RandomizedSearchCV</strong> class, starting from defining the hyperparameter space to setting each important parameter of the <strong class="source-inline">RandomizedSearchCV</strong> class. In the next section, we will learn how to perform CFS with <strong class="source-inline">sklearn</strong>.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor067"/>Implementing Coarse-to-Fine Search</h1>
			<p><strong class="bold">Coarse-to-Fine Search (CFS)</strong> is part of the Multi-Fidelity Optimization group that utilizes Grid <a id="_idIndexMarker361"/>Search and/or Random Search during the hyperparameter tuning process (see <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a><em class="italic">, Exploring Multi-Fidelity Optimization</em>). Although CFS is not implemented directly in the <strong class="source-inline">sklearn</strong> package, you can find the implemented custom class, <strong class="source-inline">CoarseToFineSearchCV</strong>, in the repo mentioned in the <em class="italic">Technical Requirements</em> section. </p>
			<p>Let’s use the same example and hyperparameter space as in the <em class="italic">Implementing Random Search</em> section, to see how <strong class="source-inline">CoarseToFineSearchCV</strong> works in practice. Note that this implementation of CFS only utilizes Random Search and uses the top <em class="italic">N</em> percentiles scheme to define the promising subspace in each iteration, similar to the example shown in <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>. However, <em class="italic">you can edit the code based on your own preference</em> since CFS is a very simple method with customizable modules.</p>
			<p>The following code shows you how to perform CFS with the <strong class="source-inline">CoarseToFineSearchCV</strong> class. It is worth noting that this class has very similar parameters to the <strong class="source-inline">RandomizedSearchCV</strong> class, with several additional parameters. The <strong class="source-inline">random_iters</strong> parameter controls the number of iterations for each random search trial, <strong class="source-inline">top_n_percentile</strong> controls the <em class="italic">N</em> value within the top N percentiles promising subspace definition (see <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>), <strong class="source-inline">n_iter</strong> defines the number of CFS iterations to be performed, and <strong class="source-inline">continuous_hyperparams</strong> stores the list of continuous hyperparameters in the predefined space.</p>
			<p>Initiate the <strong class="source-inline">CoarseToFineSearchCV</strong> class:</p>
			<pre class="source-code">clf = <strong class="bold">CoarseToFineSearchCV</strong>(pipe, hyperparameter_space,</pre>
			<pre class="source-code"><strong class="bold">random_iters=25</strong>, <strong class="bold">top_n_percentile=50</strong>, <strong class="bold">n_iter=10</strong>, </pre>
			<pre class="source-code"><strong class="bold">continuous_hyperparams=['model__min_samples_split']</strong>,</pre>
			<pre class="source-code">random_state=0, scoring='f1', cv=5, </pre>
			<pre class="source-code">n_jobs=-1, refit=True)</pre>
			<p>Run the <strong class="source-inline">CoarseToFineSearchCV</strong> class:</p>
			<pre class="source-code">clf.fit(X_train_full, y_train)</pre>
			<p>Print the <a id="_idIndexMarker362"/>best set of hyperparameters:</p>
			<pre class="source-code">print(clf.best_params_, clf.best_score_)</pre>
			<p>Evaluate the final trained model on the test data:</p>
			<pre class="source-code"><strong class="bold">y_pred = clf.predict(X_test_full)</strong></pre>
			<pre class="source-code">print(f1_score(y_test, y_pred))</pre>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.561</strong> for the F1-score when testing our final trained RF model with the best set of hyperparameters on the test set. The best set of hyperparameters is <strong class="source-inline">{‘model__class_weight’: ‘balanced_subsample’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.005867409821769845, ‘model__n_estimators’: 106}</strong> with an objective function score of <strong class="source-inline">0.560</strong>.</p>
			<p>In this section, we have learned how to implement CFS using a custom class on top of <strong class="source-inline">sklearn</strong> through the <strong class="source-inline">CoarseToFineSearchCV</strong> class. In the next section, we will learn how to perform SH with <strong class="source-inline">sklearn</strong>.</p>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor068"/>Implementing Successive Halving</h1>
			<p>Similar to CFS, <strong class="bold">Successive Halving (SH)</strong> is also part of the Multi-Fidelity Optimization <a id="_idIndexMarker363"/>group (see <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>). There are two implementations of SH in <strong class="source-inline">sklearn</strong>, namely <strong class="source-inline">HalvingGridSearchCV</strong> and <strong class="source-inline">HalvingRandomSearchCV</strong>. As their names suggest, the former class is an implementation of SH that utilizes Grid Search in each of the SH iterations, while the latter utilizes Random Search. </p>
			<p>By default, SH implementations in <strong class="source-inline">sklearn</strong> use the number of samples, or <em class="italic">n_samples</em>, as the definition of the budget or resource in SH. However, it is also possible to define a budget with other definitions. For example, we can use <strong class="source-inline">n_estimators</strong> in RF as the budget, instead of using the number of samples. It is worth noting that we cannot use <strong class="source-inline">n_estimators</strong>, or any other hyperparameters, to define the budget if it is part of the hyperparameter space.</p>
			<p>Both <strong class="source-inline">HalvingGridSearchCV</strong> and <strong class="source-inline">HalvingRandomSearchCV</strong> have similar standard SH parameters to control how the SH iterations will work, such as the <strong class="source-inline">factor</strong> parameter, which refers to the multiplier factor for SH, <strong class="source-inline">resource</strong>, which refers to what definition of budget we want to use, <strong class="source-inline">max_resources</strong> refers to the maximum budget or resource, and <strong class="source-inline">min_resources</strong>, which refers to the minimum number of resources to be used at the first iteration. By default, the <strong class="source-inline">max_resources</strong> parameter is set to <em class="italic">auto</em>, meaning it will use the total number of samples that we have when <strong class="source-inline">resource=’n_samples’</strong>. On the other hand, <strong class="source-inline">sklearn</strong> implemented a heuristic to define the default value for the <strong class="source-inline">min_resources</strong> parameter, referred to as <em class="italic">smallest</em>. This heuristic will ensure that we have a small value of <strong class="source-inline">min_resources</strong>.</p>
			<p>Specific for <strong class="source-inline">HalvingRandomSearchCV</strong>, there is also the <strong class="source-inline">n_candidates</strong> parameter that refers to the initial number of candidates to be evaluated at the first iteration. Note that this parameter is not available in <strong class="source-inline">HalvingGridSearchCV</strong> since it will automatically evaluate all of the hyperparameter candidates in the predefined space. It is worth noting that <strong class="source-inline">sklearn</strong> implemented a strategy, called <em class="italic">exhaust</em>, to define the default value of the <strong class="source-inline">n_candidates</strong> parameter. This strategy ensures that we evaluate enough candidates at the first iteration so that we can utilize as many resources as possible at the last SH iteration. </p>
			<p>Besides those standard SH parameters, both of the classes also have the <strong class="source-inline">aggressive_elimination</strong> parameter, which can be utilized when we have a low number of resources. If this Boolean parameter is set to <strong class="source-inline">True</strong>, <strong class="source-inline">sklearn</strong> will automatically rerun the first SH iteration several times until the number of candidates is small enough. The goal of this parameter is to ensure that we only evaluate a maximum of <strong class="source-inline">factor</strong> candidates in the last SH iteration. Note that this parameter is only implemented in <strong class="source-inline">sklearn</strong>, the original SH doesn’t introduce this strategy as part of the tuning method (see <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>).</p>
			<p>Similar to <strong class="source-inline">GridSearchCV</strong> and <strong class="source-inline">RandomizedSearchCV</strong>, <strong class="source-inline">HalvingGridSearchCV</strong> and <strong class="source-inline">HalvingRandomSearchCV</strong> also have the usual default <strong class="source-inline">sklearn</strong> parameters for hyperparameter tuning, such as <strong class="source-inline">cv</strong>, <strong class="source-inline">scoring</strong>, <strong class="source-inline">refit</strong>, <strong class="source-inline">random_state</strong>, and <strong class="source-inline">n_jobs</strong>. </p>
			<p class="callout-heading">Experimental Features of SH in sklearn</p>
			<p class="callout">It is worth noting that as per <strong class="source-inline">version 1.0.2</strong> of <strong class="source-inline">sklearn</strong>, the SH implementations are still in the experimental phase. This means that there might be changes in the implementation or interface of the classes without any depreciation cycle.</p>
			<p>The following <a id="_idIndexMarker364"/>code shows how <strong class="source-inline">HalvingRandomSearchCV</strong> works with its default SH parameters. Note that we still use the same example and hyperparameter space as in the <em class="italic">Implementing Random Search</em> section. It is also worth noting that we only use the <strong class="source-inline">HalvingRandomSearchCV</strong> class in this example since <strong class="source-inline">HalvingGridSearchCV</strong> has a very similar interface:  </p>
			<pre class="source-code">from <strong class="bold">sklearn.experimental</strong> import enable_halving_search_cv</pre>
			<pre class="source-code">from sklearn.model_selection import HalvingRandomSearchCV</pre>
			<p>Initiate the <strong class="source-inline">HalvingRandomSearchCV</strong> class:</p>
			<pre class="source-code">clf = <strong class="bold">HalvingRandomSearchCV</strong>(pipe, hyperparameter_space, </pre>
			<pre class="source-code">                            <strong class="bold">factor=3</strong>,</pre>
			<pre class="source-code"> <strong class="bold">aggressive_elimination=False</strong>,</pre>
			<pre class="source-code">                            random_state = 0,</pre>
			<pre class="source-code">                            scoring = 'f1', cv=5, </pre>
			<pre class="source-code">                            n_jobs=-1, refit = True)</pre>
			<p>Run the <strong class="source-inline">HalvingRandomSearchCV</strong> class:</p>
			<pre class="source-code">clf.fit(X_train_full, y_train)</pre>
			<p>Print the best set of hyperparameters:</p>
			<pre class="source-code">print(clf.best_params_, clf.best_score_)</pre>
			<p>Evaluate the final trained model on the test data:</p>
			<pre class="source-code">print(clf.score(X_test_full, y_test))</pre>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.556</strong> for the F1-score when testing our final trained <a id="_idIndexMarker365"/>RF model with the best set of hyperparameters on the test set. The best set of hyperparameters is <strong class="source-inline">{‘model__class_weight’: ‘balanced_subsample’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.007286406330027324, ‘model__n_estimators’: 42}</strong> with an objective function score of <strong class="source-inline">0.565</strong>.</p>
			<ol>
				<li value="1">The following code shows you how to generate a figure that shows the tuning process in each SH iteration: <p class="source-code">import matplotlib.pyplot as plt</p></li>
			</ol>
			<p>Get the fitting history of each trial:</p>
			<p class="source-code">results = pd.DataFrame(clf.cv_results_)</p>
			<p class="source-code">results["params_str"] = results.params.apply(str)</p>
			<p class="source-code">results.drop_duplicates(subset=("params_str", "iter"), inplace=True)</p>
			<p class="source-code">mean_scores = results.pivot(</p>
			<p class="source-code">index="iter", columns="params_str", values="mean_test_score")</p>
			<p>Plot the fitting history for each trial:</p>
			<p class="source-code">fig, ax = plt.subplots(figsize=(16,16))</p>
			<p class="source-code">ax = mean_scores.plot(legend=False, alpha=0.6, ax=ax)</p>
			<p class="source-code">labels = [</p>
			<p class="source-code">    f"Iteration {i+1}\nn_samples={clf.n_resources_[i]}\nn_candidates={clf.n_candidates_[i]}"</p>
			<p class="source-code">    for i in range(clf.n_iterations_)]</p>
			<p class="source-code">ax.set_xticks(range(clf.n_iterations_))</p>
			<p class="source-code">ax.set_xticklabels(labels, rotation=0, multialignment="left",size=16)</p>
			<p class="source-code">ax.set_title("F1-Score of Candidates over Iterations",size=20)</p>
			<p class="source-code">ax.set_ylabel("5-Folds Cross Validation F1-Score", fontsize=18)</p>
			<p class="source-code">ax.set_xlabel("")</p>
			<p class="source-code">plt.tight_layout()</p>
			<p class="source-code">plt.show()</p>
			<ol>
				<li value="2">Based <a id="_idIndexMarker366"/>on the preceding code, we get the following figure:</li>
			</ol>
			<div>
				<div id="_idContainer307" class="IMG---Figure">
					<img src="image/B18753_07_001.jpg" alt="Figure 7.1 – The SH hyperparameter tuning process&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – The SH hyperparameter tuning process</p>
			<p>Based on <em class="italic">Figure 7.1</em>, we can see that we only utilized around 14,000 samples in the last iteration <a id="_idIndexMarker367"/>while we have around 40,000 samples in our training data. Indeed, this is not an ideal case since there are too many samples not being utilized in the last SH iteration. We can change the default value of the SH parameters set by <strong class="source-inline">sklearn</strong> to ensure that we utilize as many resources as possible at the last iteration, through the <strong class="source-inline">min_resources</strong> and <strong class="source-inline">n_candidates</strong> parameters. </p>
			<p>In this <a id="_idIndexMarker368"/>section, we have learned how to implement SH in <strong class="source-inline">sklearn</strong> through the <strong class="source-inline">HalvingRandomSearchCV</strong> and <strong class="source-inline">HalvingGridSearchCV</strong> classes. We have also learned all of the important parameters available for both classes. In the next section, we will learn how to perform HB with <strong class="source-inline">scikit-hyperband</strong>.</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor069"/>Implementing Hyper Band</h1>
			<p>The extension <a id="_idIndexMarker369"/>of Successive Halving, the <strong class="bold">Hyper Band (HB)</strong> method (see <a href="B18753_06_ePub.xhtml#_idTextAnchor054"><em class="italic">Chapter 6</em></a>), is implemented in the <strong class="source-inline">scikit-hyperband</strong> package. This package is built on top of <strong class="source-inline">sklearn</strong>, which means it also provides a very similar interface for <strong class="source-inline">GridSearchCV</strong>, <strong class="source-inline">RandomizedSearchCV</strong>, <strong class="source-inline">HalvingGridSearchCV</strong>, and<strong class="source-inline"> HalvingRandomSearchCV</strong>. </p>
			<p>In contrast with the default SH budget definition in the <strong class="source-inline">sklearn</strong> implementation, <em class="italic">Scikit-Hyperband defines the budget</em> as the number of estimators, <em class="italic">n_estimators</em>, in an ensemble of trees, or the number of iterations for estimators trained with stochastic gradient descent, such as the XGBoost algorithm. Additionally, we can use any other hyperparameters that exist in the estimator as the budget definition. However, <strong class="source-inline">scikit-hyperband</strong> <em class="italic">doesn’t allow us to use the number of samples as the budget definition</em>.</p>
			<p>Let’s use the same example as in the <em class="italic">Implementing Successive Halving</em> section, but with a different hyperparameter space. Here, we use the number of estimators, <em class="italic">n_estimators</em>, as the resource, which means we have to take out this hyperparameter from our search space. Note that you also have to remove any other hyperparameters from the space when you use it as the resource definition, just like in the <strong class="source-inline">sklearn</strong> implementation of SH. </p>
			<p>The following code shows you how <strong class="source-inline">HyperbandSearchCV</strong> works. The <strong class="source-inline">resource_param</strong> parameter refers to the hyperparameter that you want to use as the budget definition. The <strong class="source-inline">eta</strong> parameter is actually the same as the factor parameter in the <strong class="source-inline">HalvingRandomSearchCV</strong> or <strong class="source-inline">HalvingGridSearchCV</strong> classes, which refers to the multiplier factor for each SH run. The <strong class="source-inline">min_iter</strong> and <strong class="source-inline">max_iter</strong> parameters refer to the minimum and maximum resources for all brackets. Note that there’s no automatic strategy like in the <strong class="source-inline">sklearn</strong> implementation of SH for setting the value of the <strong class="source-inline">min_iter</strong> and <strong class="source-inline">max_iter</strong> parameters. </p>
			<p>The remaining <strong class="source-inline">HyperbandSearchCV</strong> parameters are similar to any other <strong class="source-inline">sklearn</strong> implementation of the hyperparameter tuning methods. It is worth noting that the HB implementation <a id="_idIndexMarker370"/>used in this book is the modified version of the <strong class="source-inline">scikit-hyperband </strong>package. Please check the following folder in the book’s GitHub repo (<a href="https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/tree/main/hyperband">https://github.com/PacktPublishing/Hyperparameter-Tuning-with-Python/tree/main/hyperband</a>):</p>
			<pre class="source-code">from hyperband import HyperbandSearchCV</pre>
			<p>Initiate the <strong class="source-inline">HyperbandSearchCV</strong> class:</p>
			<pre class="source-code">clf = HyperbandSearchCV(pipe, hyperparameter_space,</pre>
			<pre class="source-code">                        <strong class="bold">resource_param='model__n_estimators'</strong>,</pre>
			<pre class="source-code">                        <strong class="bold">eta=3</strong>, <strong class="bold">min_iter=1</strong>, <strong class="bold">max_iter=100</strong>,</pre>
			<pre class="source-code">                        random_state = 0,</pre>
			<pre class="source-code">                        scoring = 'f1', cv=5, </pre>
			<pre class="source-code">                        n_jobs=-1, refit = True)</pre>
			<pre class="source-code"> </pre>
			<p>Run the <strong class="source-inline">HyperbandSearchCV</strong> class:</p>
			<pre class="source-code">clf.fit(X_train_full, y_train)</pre>
			<p>Print the best set of hyperparameters:</p>
			<pre class="source-code">print(clf.best_params_, clf.best_score_)</pre>
			<p>Evaluate the final trained model on the test data:</p>
			<pre class="source-code">print(clf.score(X_test_full, y_test))</pre>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.569</strong> in F1-score when testing our final trained RF model with the best set of hyperparameters on the test set. The best set of hyperparameters is <strong class="source-inline">{‘model__class_weight’: ‘balanced’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.0055643644642829684, ‘model__n_estimators’: 33}</strong> with an objective function score of <strong class="source-inline">0.560</strong>. Note that although we remove <strong class="source-inline">model__n_estimators</strong> from the search space, <strong class="source-inline">HyperbandSearchCV</strong> still outputs the best value for this hyperparameter by choosing from the best bracket.</p>
			<p>In this section, we have learned how to implement HB using the help of the <strong class="source-inline">scikit-hyperband</strong> package <a id="_idIndexMarker371"/>along with all of the important parameters available for the <strong class="source-inline">HyperbandSearchCV</strong> class. In the next section, we will learn how to perform Bayesian Optimization with <strong class="source-inline">scikit-optimize</strong>.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor070"/>Implementing Bayesian Optimization Gaussian Process</h1>
			<p><strong class="bold">Bayesian Optimization Gaussian Process (BOGP)</strong> is one of the variants of the Bayesian Optimization hyperparameter tuning group (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>). To implement BOGP, we <a id="_idIndexMarker372"/>can utilize the <strong class="source-inline">skopt</strong> package. Similar to <strong class="source-inline">scikit-hyperband</strong>, this package is also built on top of the <strong class="source-inline">sklearn</strong> package, which means the interface for the implemented Bayesian Optimization tuning class, <strong class="source-inline">BayesSearchCV</strong>, is very similar to <strong class="source-inline">GridSearchCV</strong>, <strong class="source-inline">RandomizedSearchCV</strong>, <strong class="source-inline">HalvingGridSearchCV</strong>,  <strong class="source-inline">HalvingRandomSearchCV</strong>, and <strong class="source-inline">HyperbandSearchCV</strong>.</p>
			<p>However, unlike <strong class="source-inline">sklearn</strong> or <strong class="source-inline">scikit-hyperband</strong>, which works well directly with the distribution implemented in <strong class="source-inline">scipy</strong>, in <strong class="source-inline">skopt</strong>, we can only use the wrapper provided by the package when defining the hyperparameter space. The wrappers are defined within the <strong class="source-inline">skopt.space.Dimension</strong> instances and consist of three types of dimensions, such as <strong class="source-inline">Real</strong>, <strong class="source-inline">Integer</strong>, and <strong class="source-inline">Categorical</strong>. Within each of these dimension wrappers, <strong class="source-inline">skopt</strong> actually uses the same distribution from the <strong class="source-inline">scipy</strong> package. </p>
			<p>By default, the <strong class="source-inline">Real</strong> dimension only supports the <strong class="source-inline">uniform</strong> and <strong class="source-inline">log-uniform</strong> distributions and can take any real/numerical value as the input. As for the <strong class="source-inline">Categorical</strong> dimension, this wrapper can only take categorical values as the input, as implied by its name. It will automatically convert categorical values into integers or even real values, which means we can also utilize categorical hyperparameters for BOGP! Although we can do this, remember that BOGP only works best for the actual real variables (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>). Finally, we have the <strong class="source-inline">Integer</strong> dimension wrapper. By default, this wrapper only supports <strong class="source-inline">uniform</strong> and <strong class="source-inline">log-uniform</strong> distributions for integer formatting. The <strong class="source-inline">uniform</strong> distribution will utilize the <strong class="source-inline">randint</strong> distribution from <strong class="source-inline">scipy</strong>, while the <strong class="source-inline">log-uniform</strong> distribution is exactly the same as the one that is used in the Real wrapper. </p>
			<p>It is worth noting that we can write our own wrapper for other distributions too; for example, the <strong class="source-inline">truncnorm</strong> distribution that we use in all of our earlier examples. In fact, you can find the custom <strong class="source-inline">Real</strong> wrapper that consists of the <strong class="source-inline">truncnorm</strong>, <strong class="source-inline">uniform</strong>, and <strong class="source-inline">log-uniform</strong> distributions in the repo mentioned in the <em class="italic">Technical Requirements</em> section. The following code shows you how we can define the hyperparameter space for <strong class="source-inline">BayesSearchCV</strong>. Note that we are still using the same example <a id="_idIndexMarker373"/>and hyperparameter space as the <em class="italic">Implementing Random Search</em> section. Here, <strong class="source-inline">Integer</strong> and <strong class="source-inline">Categorical</strong> are the original wrappers provided by <strong class="source-inline">skopt</strong>, while the <strong class="source-inline">Real</strong> wrapper is the custom wrapper that consists of the <strong class="source-inline">truncnorm</strong> distribution, too:</p>
			<pre class="source-code">from skopt.space import *</pre>
			<pre class="source-code">hyperparameter_space = {</pre>
			<pre class="source-code">"model__n_estimators": <strong class="bold">Integer</strong>(low=5, high=200),</pre>
			<pre class="source-code">"model__criterion": <strong class="bold">Categorical</strong>(["gini", "entropy"]),</pre>
			<pre class="source-code">"model__class_weight": <strong class="bold">Categorical</strong>(["balanced","balanced_subsample"]),</pre>
			<pre class="source-code">"model__min_samples_split": <strong class="bold">Real</strong>(low=0,high=0.5,prior="<strong class="bold">truncnorm</strong>",</pre>
			<pre class="source-code">                                 **{"loc":0.005,"scale":0.01})</pre>
			<pre class="source-code">}</pre>
			<p>All of the parameters of the <strong class="source-inline">BayesSearchCV</strong> class are very similar to the <strong class="source-inline">GridSearchCV</strong>, <strong class="source-inline">RandomizedSearchCV</strong>, <strong class="source-inline">HalvingGridSearchCV</strong>, <strong class="source-inline">HalvingRandomSearchCV</strong>, or <strong class="source-inline">HyperbandSearchCV</strong>. The only specific parameters for <strong class="source-inline">BayesSearchCV</strong> are the <strong class="source-inline">n_iter</strong> and <strong class="source-inline">optimizer_kwargs</strong> which refer to the total number of trials to be performed and the parameter that consists of all related parameters for the <strong class="source-inline">Optimizer</strong>, respectively. Here, the <strong class="source-inline">Optimizer</strong> is a class that represents each of the Bayesian Optimization steps, starting from initializing the initial points, fitting the surrogate model, sampling the next set of hyperparameters using the help of the acquisition function, and optimizing the acquisition function (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>). </p>
			<p>There are several parameters available that we can pass to the <strong class="source-inline">optimizer_kwargs</strong> dictionary. The <strong class="source-inline">base_estimator</strong> parameter refers to the type of surrogate model to be used. <strong class="source-inline">skopt</strong> has prepared several surrogate models with default setups, including the Gaussian Process or <em class="italic">GP</em>.  The <strong class="source-inline">n_initial_points</strong> parameter refers to the number of random initial points before the actual Bayesian Optimization steps begin. The <strong class="source-inline">initial_point_generator</strong> parameter refers to the initialization method to be used. By default, <strong class="source-inline">skopt</strong> will initialize them randomly. However, you can also change the initialization method to <em class="italic">lhs</em>, <em class="italic">sobol</em>, <em class="italic">halton</em>, <em class="italic">hammersly</em>, or <em class="italic">grid</em>. </p>
			<p>As for the type of acquisition function to be used, by default, <strong class="source-inline">skopt</strong> will use <em class="italic">gp_hedge</em>, which is <a id="_idIndexMarker374"/>an acquisition function that will <a id="_idIndexMarker375"/>automatically choose either one of the <strong class="bold">Lower Confidence Bound</strong> (<strong class="bold">LCB</strong>), <strong class="bold">Expected Improvement</strong> (<strong class="bold">EI</strong>), or <strong class="bold">Probability of Improvement</strong> (<strong class="bold">PI</strong>) based <a id="_idIndexMarker376"/>on the probability. However, we can also choose to use each of those acquisition functions independently, by setting the <strong class="source-inline">acq_func</strong> parameter to <em class="italic">LCB</em>, <em class="italic">EI</em>, and <em class="italic">PI</em>, respectively. As explained in <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>, besides choosing what acquisition function needs to be used, we also have to define <a id="_idIndexMarker377"/>what kind of optimizer to be utilized for the acquisition function itself. There are two options for the acquisition function’s optimizer provided by <strong class="source-inline">skopt</strong>, namely random sampling (<em class="italic">sampling</em>) and <em class="italic">lbfgs</em>, or the type of second-order optimization strategy mentioned in <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>. By default, <strong class="source-inline">skopt</strong> sets the <strong class="source-inline">acq_optimizer</strong> parameter to <em class="italic">auto</em>, which will choose automatically when to use the <em class="italic">sampling</em> or <em class="italic">lbfgs</em> optimization methods.</p>
			<p>Finally, we can also pass the <strong class="source-inline">acq_func_kwargs</strong> parameter within the <strong class="source-inline">optimizer_kwargs</strong> parameter. We can pass all parameters related to the acquisition function to this <strong class="source-inline">acq_func_kwargs</strong> parameter; for example, the <strong class="source-inline">xi</strong> parameter that controls the exploration and exploitation behavior of the BOGP, as explained in <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a>. While the <strong class="source-inline">xi</strong> parameter is responsible for controlling the exploration versus exploitation trade-off for EI and PI acquisition functions, there is also another parameter called <strong class="source-inline">kappa</strong>, which is responsible for the same task as the LCB acquisition function. The higher the value of <strong class="source-inline">xi</strong> or <strong class="source-inline">kappa</strong> means that we are favoring exploration over exploitation, and vice versa. For more information about all of the parameters that are available in the <strong class="source-inline">BayesSearchCV</strong> class, you can refer to the official API <a id="_idIndexMarker378"/>reference of the <strong class="source-inline">skopt</strong> package (<a href="https://scikit-optimize.github.io/stable/modules/classes.html">https://scikit-optimize.github.io/stable/modules/classes.html</a>).</p>
			<p>The following code shows how we can utilize <strong class="source-inline">BayesSearchCV</strong> to perform BOGP on the <a id="_idIndexMarker379"/>same example as the <em class="italic">Implementing Random Search</em> section:</p>
			<pre class="source-code">from skopt import BayesSearchCV</pre>
			<p>Initiate the <strong class="source-inline">BayesSearchCV</strong> class:</p>
			<pre class="source-code">clf = <strong class="bold">BayesSearchCV</strong>(pipe, hyperparameter_space, <strong class="bold">n_iter=50</strong>,</pre>
			<pre class="source-code"><strong class="bold">optimizer_kwargs</strong>={"<strong class="bold">base_estimator</strong>":<strong class="bold">"GP"</strong>,</pre>
			<pre class="source-code">                  "<strong class="bold">n_initial_points</strong>":10,</pre>
			<pre class="source-code">                  "<strong class="bold">initial_point_generator</strong>":"random",</pre>
			<pre class="source-code">                  "<strong class="bold">acq_func</strong>":"EI",</pre>
			<pre class="source-code">                  "<strong class="bold">acq_optimizer</strong>":"auto",</pre>
			<pre class="source-code">                  "<strong class="bold">n_jobs</strong>":-1,</pre>
			<pre class="source-code">                  "<strong class="bold">random_state</strong>":0,</pre>
			<pre class="source-code">                  "<strong class="bold">acq_func_kwargs</strong>": {"<strong class="bold">xi</strong>":0.01}</pre>
			<pre class="source-code">                  },</pre>
			<pre class="source-code">random_state = 0,</pre>
			<pre class="source-code">scoring = 'f1', cv=5, </pre>
			<pre class="source-code">n_jobs=-1, refit = True)</pre>
			<p>Run the <strong class="source-inline">BayesSearchCV</strong> class:</p>
			<pre class="source-code">clf.fit(X_train_full, y_train)</pre>
			<p>Print the best set of hyperparameters:</p>
			<pre class="source-code">print(clf.best_params_, clf.best_score_)</pre>
			<p>Evaluate the final trained model on the test data:</p>
			<pre class="source-code">print(clf.score(X_test_full, y_test))</pre>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.539</strong> for the F1-Score when testing our final trained RF model with the best set of hyperparameters on the test set. The best set of hyperparameters is <strong class="source-inline">{‘model__class_weight’: ‘balanced’, ‘model__criterion’: ‘entropy’, ‘model__min_samples_split’: 0.02363008892366518, ‘model__n_estimators’: 94}</strong> with an objective function score of <strong class="source-inline">0.530</strong>.</p>
			<p>In <a id="_idIndexMarker380"/>this section, we have learned how to implement BOGP in <strong class="source-inline">skopt</strong> along with all of the important parameters available for the <strong class="source-inline">BayesSearchCV</strong> class. It is worth noting that <strong class="source-inline">skopt</strong> also has experiment tracking modules that include several native supports for plotting the result. We will learn more about those modules in <a href="B18753_13_ePub.xhtml#_idTextAnchor125"><em class="italic">Chapter 13</em></a>, <em class="italic">Tracking Hyperparameter Tuning Experiments</em>. In the next section, we will learn how to perform another variant of Bayesian Optimization that utilizes RF as its surrogate model with <strong class="source-inline">skopt</strong>.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor071"/>Implementing Bayesian Optimization Random Forest</h1>
			<p><strong class="bold">Bayesian Optimization Random Forest (BORF)</strong> is another variant of Bayesian Optimization <a id="_idIndexMarker381"/>hyperparameter tuning methods that utilize RF as the surrogate model. Note that this variant <a id="_idIndexMarker382"/>is different from <strong class="bold">Sequential Model Algorithm Configuration</strong> (<strong class="bold">SMAC</strong>) although both of them utilize RF as the surrogate model (see <a href="B18753_04_ePub.xhtml#_idTextAnchor036"><em class="italic">Chapter 4</em></a><em class="italic">, Exploring Bayesian Optimization</em>).</p>
			<p>Implementing BORF with <strong class="source-inline">skopt</strong> is actually very similar to implementing BOGP as discussed in the previous section. We just need to change the <strong class="source-inline">base_estimator</strong> parameter within <strong class="source-inline">optimizer_kwargs</strong> to <em class="italic">RF</em>. Let’s use the same example as in the <em class="italic">Implementing Bayesian Optimization Gaussian Process</em> section, but change the acquisition function from <em class="italic">EI</em> to <em class="italic">LCB</em>.  Additionally, let’s change the <strong class="source-inline">xi</strong> parameter in the <strong class="source-inline">acq_func_kwargs</strong> to <em class="italic">kappa</em> since we are using <em class="italic">LCB</em> as our acquisition function. Note that we can also still use the same acquisition function. The changes made here just to show how you can interact with the interface of the <strong class="source-inline">BayesSearchCV</strong> class:</p>
			<pre class="source-code">from skopt import BayesSearchCV</pre>
			<p>Initiate the <strong class="source-inline">BayesSearchCV</strong> class:</p>
			<pre class="source-code">clf = BayesSearchCV(pipe, hyperparameter_space, n_iter=50,</pre>
			<pre class="source-code">optimizer_kwargs={"base_estimator":<strong class="bold">"RF"</strong>,</pre>
			<pre class="source-code">                  "n_initial_points":10,</pre>
			<pre class="source-code">                  "initial_point_generator":"random",</pre>
			<pre class="source-code">                  "acq_func":<strong class="bold">"LCB"</strong>,</pre>
			<pre class="source-code">                  "acq_optimizer":"auto",</pre>
			<pre class="source-code">                  "n_jobs":-1,</pre>
			<pre class="source-code">                  "random_state":0,</pre>
			<pre class="source-code">                  "acq_func_kwargs": {<strong class="bold">"kappa":1.96</strong>}</pre>
			<pre class="source-code">                  },</pre>
			<pre class="source-code">random_state = 0,</pre>
			<pre class="source-code">scoring = 'f1', cv=5, </pre>
			<pre class="source-code">n_jobs=-1, refit = True)</pre>
			<p>Run the <strong class="source-inline">BayesSearchCV</strong> class:</p>
			<pre class="source-code">clf.fit(X_train_full, y_train)</pre>
			<p>Print <a id="_idIndexMarker383"/>the best set of hyperparameters:</p>
			<pre class="source-code">print(clf.best_params_, clf.best_score_)</pre>
			<p>Evaluate the final trained model on the test data.</p>
			<pre class="source-code">print(clf.score(X_test_full, y_test))</pre>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.617</strong> for the F1-score when testing our final trained RF model with the best set of hyperparameters on the test set. The best set of hyperparameters is <strong class="source-inline">{‘model__class_weight’: ‘balanced_subsample’, ‘model__criterion’: ‘gini’, ‘model__min_samples_split’: 0.00043534042560206855, ‘model__n_estimators’: 85}</strong> with an objective function score of <strong class="source-inline">0.616</strong>.</p>
			<p>In this section, we have learned how to implement BORF in <strong class="source-inline">skopt</strong> through the <strong class="source-inline">BayesSearchCV</strong> class. In the next section, we will learn how to perform another variant of Bayesian Optimization, which utilizes Gradient Boosted Trees as its surrogate model with <strong class="source-inline">skopt</strong>.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor072"/>Implementing Bayesian Optimization Gradient Boosted Trees</h1>
			<p><strong class="bold">Bayesian Optimization Gradient Boosted Trees</strong> (<strong class="bold">BOGBRT</strong>) is another variant of Bayesian <a id="_idIndexMarker384"/>Optimization that utilizes Gradient Boosted Trees as a surrogate model. Note that there will be endless variants of Bayesian Optimization that we can implement in <strong class="source-inline">skopt</strong> since we can just pass any other regressors from <strong class="source-inline">sklearn</strong> to be utilized as the <strong class="source-inline">base_estimator</strong> parameter. However, <em class="italic">GBRT</em> is part of the default surrogate model with predefined default hyperparameter values from the <strong class="source-inline">skopt</strong> package.</p>
			<p>Similar to the <em class="italic">Implementing Bayesian Optimization Random Forest</em> section, we can just change the <strong class="source-inline">base_estimator</strong> parameter within <strong class="source-inline">optimizer_kwargs</strong> to <em class="italic">GBRT</em>. The following code shows you how to implement BOGBRT in <strong class="source-inline">skopt</strong>:</p>
			<pre class="source-code">from skopt import BayesSearchCV</pre>
			<p>Initiate the <strong class="source-inline">BayesSearchCV</strong> class:</p>
			<pre class="source-code">clf = BayesSearchCV(pipe, hyperparameter_space, n_iter=50,</pre>
			<pre class="source-code">optimizer_kwargs={"base_estimator":<strong class="bold">"GBRT"</strong>,</pre>
			<pre class="source-code">                  "n_initial_points":10,</pre>
			<pre class="source-code">                  "initial_point_generator":"random",</pre>
			<pre class="source-code">                  "acq_func":"LCB",</pre>
			<pre class="source-code">                  "acq_optimizer":"auto",</pre>
			<pre class="source-code">                  "n_jobs":-1,</pre>
			<pre class="source-code">                  "random_state":0,</pre>
			<pre class="source-code">                  "acq_func_kwargs": {"kappa":1.96}</pre>
			<pre class="source-code">                  },</pre>
			<pre class="source-code">random_state = 0,</pre>
			<pre class="source-code">scoring = 'f1', cv=5, </pre>
			<pre class="source-code">n_jobs=-1, refit = True)</pre>
			<p>Run the <strong class="source-inline">BayesSearchCV</strong> class:</p>
			<pre class="source-code">clf.fit(X_train_full, y_train)</pre>
			<p>Print <a id="_idIndexMarker385"/>the best set of hyperparameters:</p>
			<pre class="source-code">print(clf.best_params_, clf.best_score_)</pre>
			<p>Evaluate the final trained model on the test data:</p>
			<pre class="source-code">print(clf.score(X_test_full, y_test))</pre>
			<p>Based on the preceding code, we get around <strong class="source-inline">0.611</strong> for the F1-Score when testing our final trained RF model with the best set of hyperparameters on the test set. The best set of hyperparameters is <strong class="source-inline">{‘model__class_weight’: ‘balanced_subsample’, ‘model__criterion’: ‘gini’, ‘model__min_samples_split’: 0.0005745541104096049, ‘model__n_estimators’: 143}</strong> with an objective function score of <strong class="source-inline">0.618</strong>.</p>
			<p>In this section, we have learned how to implement BOGBRT in <strong class="source-inline">skopt</strong> through the <strong class="source-inline">BayesSearchCV</strong> class by using the same example as in the <em class="italic">Implementing Bayesian Optimization Random Forest</em> section.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor073"/>Summary</h1>
			<p>In this chapter, we have learned all the important things about the <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">scikit-optimize</strong>, and <strong class="source-inline">scikit-hyperband</strong> packages for hyperparameter tuning purposes. Additionally, we have learned how to implement various hyperparameter tuning methods using the help of those packages, along with understanding each of the important parameters of the classes and how are they related to the theory that we have learned in the previous chapters. From now on, you should be able to utilize these packages to implement your chosen hyperparameter tuning method and, ultimately, boost the performance of your ML model. Equipped with the knowledge from <em class="italic">Chapters 3–6</em>, you will also be able to understand what’s happening if there are errors or unexpected results and how to set up the method configuration to match your specific problem.</p>
			<p>In the next chapter, we will learn about the Hyperopt package and how to utilize it to perform various hyperparameter tuning methods. The goal of the next chapter is similar to this chapter, that is, to be able to utilize the package for hyperparameter tuning purposes and understand each of the parameters of the implemented classes.</p>
		</div>
		<div>
			<div id="_idContainer309">
			</div>
		</div>
	</body></html>