- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interpretability and Explainability in Machine Learning Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The majority of the machine learning models we use or develop are complex and
    require the use of explainability techniques to identify opportunities for improving
    their performance, reducing their bias, and increasing their reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will look at the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable versus black-box machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainability methods in machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practicing machine learning explainability in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing why having explainability is not enough
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about the importance of explainability
    in machine learning modeling and practiced using some of the explainability techniques
    in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements should be considered for this chapter as they help
    you better understand the mentioned concepts, use them in your projects, and practice
    with the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` >= 1.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` >= 1.22.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` >= 3.7.1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter06](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter06).
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable versus black-box machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interpretable and simple models such as linear regression make it easy to assess
    the possibility of improving them, finding issues with them such as biases that
    need to be detected and removed, and building trust in using such models. However,
    to achieve higher performance, we usually don’t stop with these simple models
    and rely on complex or so-called black-box models. In this section, we will review
    some of the interpretable models and then introduce techniques you can use to
    explain your black-box models.
  prefs: []
  type: TYPE_NORMAL
- en: Interpretable machine learning models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear models such as linear and logistic regression, shallow decision trees,
    and Naive Bayes classifiers are examples of simple and interpretable methods (*Figure
    6**.1*). We can easily extract the contribution of features in predictions of
    outputs for these models and identify opportunities for improving their performance,
    such as by adding or removing features or changing feature normalization. We can
    also easily identify if there are biases in our models – for example, for a specific
    race or gender group. However, these models are very simple, and having access
    to large datasets of thousands or millions of samples allows us to train high-performance
    but complex models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Examples of interpretable classification methods](img/B16369_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Examples of interpretable classification methods
  prefs: []
  type: TYPE_NORMAL
- en: Complex models, such as random forest models with many deep decision trees or
    deep neural networks, help us in achieving higher performance, although they work
    almost like black-box systems. To be able to understand these models and explain
    how they come up with their predictions, and to build trust in their utility,
    we can use machine learning explainability techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability for complex models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Explainability techniques work like bridges between complex machine learning
    models and users. They are supposed to provide explainabilities that are faithful
    to how the models work. And on the other side, they are supposed to provide explanations
    that are useful and understandable for the users. These explanations can be used
    to identify opportunities for improving model performance, reducing the sensitivity
    of models to small feature value changes, increasing data efficiency in model
    training, trying to help in proper reasoning in the model and avoid spurious correlations,
    and helping in achieving fairness (Weber et al., 2022; *Figure 6**.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Effects of using explainability on machine learning models](img/B16369_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Effects of using explainability on machine learning models
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a better understanding of the importance of explainability
    in machine learning modeling, we are ready to get into the details of explainability
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Explainability methods in machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to keep the following considerations in mind when using or developing
    explainability techniques for machine learning modeling (Ribeiro et al., 2016):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interpretability**: The explanations need to be understandable to users.
    One of the main objectives of machine learning explanation is to make complex
    models understandable for users and, if possible, provide actionable information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Local fidelity (faithfulness)**: Capturing the complexity of models so that
    they are completely faithful and meet global faithfulness criteria can’t be achieved
    by all techniques. However, an explanation should be at least locally faithful
    to the model. In other words, an explanation needs to properly explain how the
    model behaves in the close neighborhood of the data point under investigation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Being model-agnostic**: Although there are techniques that are designed for
    specific machine learning methods, such as random forest, they are supposed to
    be agnostic to models that are built with different hyperparameters or for different
    datasets. An explainability technique needs to consider the model as a black box
    and provide explanations for the model either globally or locally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainability techniques can be categorized into **local explainability** and
    **global explainability** methods. Local explainability methods aim to meet the
    previously listed criteria, while global explainability techniques try to go beyond
    local explainability and provide global explanations to the models.
  prefs: []
  type: TYPE_NORMAL
- en: Local explainability techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Local explainability helps us understand the behavior of a model close to a
    data point in a feature space. Although these models meet local fidelity criteria,
    features identified to be locally important might not be globally important, and
    vice versa (Ribeiro et al., 2016). This means that we cannot infer local explanations
    from global explanations, and vice versa, easily. In this section, we will discuss
    five local explanation techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactuals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sample-based explainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule-based explainability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saliency maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also go through a few global explainability techniques after.
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the primary approaches to local explainability is explaining the contribution
    of each feature *locally* in predicting the outcome of the target data points
    in a neighborhood. Widely used examples of such methods include **SHapley Additive
    exPlanations** (**SHAP**) (Lundberg et al., 2017) and **Local Interpretable Model-agnostic
    Explanations** (**LIME**) (Ribeiro et al., 2016). Let’s briefly discuss the theory
    behind these two methods and practice them in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Local explanation using SHAP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SHAP is a Python framework that was introduced by Scott Lundberg and Su-In Lee
    (Lundberg and Lee, 2017). The idea of this framework is based on using the Shapely
    value, a known concept named after Lloyd Shapley, an American game theorist and
    Nobel Prize winner (Winter, 2022). SHAP can determine the contribution of each
    feature to a model’s prediction. As features work cooperatively in determining
    the decision boundaries of classification models and eventually affecting model
    predictions, SHAP tries to first identify the marginal contribution of each feature
    and then provide Shapely values as an estimate of the contribution of each feature
    in cooperation with the whole feature set regarding the predictions of a model.
    From a theoretical perspective, these marginal contributions can be calculated
    by removing features individually and in different combinations, calculating the
    effect of each feature set removal, and then normalizing the contributions. This
    process can’t be repeated for all possible feature combinations as the number
    of possible combinations could grow exponentially to billions, even for a model
    with 40 features. Instead, this process is used a limited number of times to come
    up with an approximation of Shapely values. Also, since removing features is not
    possible in most machine learning models, feature values get replaced by alternative
    values either from a random distribution or from a background set of meaningful
    and possible values for each feature. We don’t want to get into the theoretical
    details of this process but we will practice using this approach in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Local explanation using LIME
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'LIME is an alternative to SHAP for local explainability that explains the predictions
    of any classifier or regressor, in a model-agnostic way, by approximating a model
    locally with an interpretable model (*Figure 6**.3*; Ribeiro et al., 2016):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Schematic representation of local interpretable modeling in
    LIME](img/B16369_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Schematic representation of local interpretable modeling in LIME
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the advantages of this technique, which were also mentioned in the
    original paper by Ribeiro et al., 2016, include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The theory and the provided explanations are intuitive and easy to understand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse explanations are provided to increase interpretability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Works with different types of structured and unstructured data, such as texts
    and images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counterfactuals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Counterfactual examples, or explanations, help us identify what needs to be
    changed in an instance to change the outcome of a classification model. These
    counterfactuals could help in identifying actionable paths in many applications,
    such as finance, retail, marketing, recruiting, and healthcare. One example is
    when suggesting to a bank customer how they can change the rejection to their
    loan application (Guidotti, 2022). Counterfactuals could also help in identifying
    biases in models that help us improve model performance or eliminate fairness
    issues in our models. We need to keep the following considerations in mind while
    generating and using counterfactual explanations (Guidotti, 2022):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Validity**: A counterfactual example is valid if and only if its classification
    outcome would be different from the original sample.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Similarity**: A counterfactual example should be as similar as possible to
    the original data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Diversity**: Although counterfactual examples should be similar to the original
    samples they are derived from, they need to be diverse among each other to provide
    different options (that is, different possible feature changes).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actionability**: Not all the feature value changes are actionable. The actionability
    of the counterfactuals that are suggested by a counterfactual method is an important
    factor in benefitting from them in practice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plausibility**: The feature values of a counterfactual example should be
    plausible. The plausibility of the counterfactuals increases trust in deriving
    explanations from them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also have to note that counterfactual explainers need to be efficient and
    fast enough in generating the counterfactuals and stable in generating counterfactuals
    for similar data points (Guidotti, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: Sample-based explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach to explainability is to rely on the feature values and results
    of real or synthetic data points to help in local model explainability. In this
    category of explainability techniques, we aim to find out which samples are misclassified
    and what feature sets result in an increasing chance of misclassification to help
    us explain our models. We can also assess which training data points result in
    a change in the decision boundary so that we can predict the output of test or
    production data points. There are statistical methods such as the **Influence
    function** (Koh and Liang 2017), a classical approach for assessing the influence
    of samples on model parameters, that we can use to identify the sample’s contribution
    to the decision-making process of models.
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based explainability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rule-based methods such as **Anchor explanations** aim to find the conditions
    of feature values that result in a high probability of getting the same output
    (Ribeiro et al., 2018). For example, in the case of predicting the salary of individuals
    in a dataset to be less than or equal to 50k or above 50k, “Education <= high
    school to result in <=50k salary” could be considered a rule in rule-based explanation.
    These explanations need to be locally faithful.
  prefs: []
  type: TYPE_NORMAL
- en: Saliency maps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The objective of saliency maps is to explain which features contribute more
    or less to the predicted outputs for a data point. These methods are commonly
    used in machine learning or deep learning models that have been trained on image
    data (Simonyan et al., 2013). For example, we can use saliency maps to figure
    out if a classification model uses a background forest to identify if it is an
    image of a bear rather than a teddy bear or uses the components of the bear’s
    body for it.
  prefs: []
  type: TYPE_NORMAL
- en: Global explanation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the difficulty in achieving a reliable global explanation for machine
    learning models, it could increase trust in them (Ribeiro et al., 2016). Performance
    is not the only aspect of building trust when developing and deploying machine
    learning models. And local explanations, although very helpful in investigating
    individual samples and providing actionable information, might not be enough for
    this trust building. Here, we will discuss three approaches for going beyond local
    explanation, including collecting local explanations, knowledge distillation,
    and summaries of counterfactuals.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting local explanations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Submodular pick LIME** (**SP-LIME**) is a global explanation technique that
    uses local explanations of LIME to come up with a global perspective of a model’s
    behavior (Riberio et al., 2016). As it might not be feasible to use the local
    explanations of all data points, SP-LIME picks a representative diverse set of
    samples capable of representing the global behavior of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The idea of **knowledge distillation** is to approximate the behavior of complex
    models, which was initially proposed for neural network models, using simpler
    interpretable models such as decision trees (Hinton et al., 2015; Frosst and Hinton,
    2017). In other words, we aim to build simpler models, such as decision trees,
    that approximate the predictions of complex models for a given set of samples.
  prefs: []
  type: TYPE_NORMAL
- en: Summaries of counterfactuals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can use a summary of counterfactuals that’s been generated for multiple data
    points with correct and incorrect predicted outcomes to figure out the contribution
    of features in output prediction and the sensitivity of a prediction to feature
    perturbation. We will practice using counterfactuals later in this chapter, where
    you will see that not all counterfactuals are acceptable and they need to be chosen
    according to the meaning behind features and their values.
  prefs: []
  type: TYPE_NORMAL
- en: Practicing machine learning explainability in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are several Python libraries you can use to extract local and global
    explanations for your machine learning models (*Table 6.1*). Here, we want to
    practice with a few of the ones that focus on local model explainability:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Library** | **Library Name for Importing** **and Installation** | **URL**
    |'
  prefs: []
  type: TYPE_TB
- en: '| SHAP | `Shap` | [https://pypi.org/project/shap/](https://pypi.org/project/shap/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| LIME | `Lime` | [https://pypi.org/project/lime/](https://pypi.org/project/lime/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Shapash | `shapash` | [https://pypi.org/project/shapash/](https://pypi.org/project/shapash/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ELI5 | `eli5` | [https://pypi.org/project/eli5/](https://pypi.org/project/eli5/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Explainer dashboard | `explainer dashboard` | [https://pypi.org/project/explainerdashboard/](https://pypi.org/project/explainerdashboard/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dalex | `dalex` | [https://pypi.org/project/dalex/](https://pypi.org/project/dalex/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| OmniXAI | `omnixai` | [https://pypi.org/project/omnixai/](https://pypi.org/project/omnixai/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| CARLA | `carla` | [https://carla-counterfactual-and-recourse-library.readthedocs.io/en/latest/](https://carla-counterfactual-and-recourse-library.readthedocs.io/en/latest/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Diverse Counterfactual** **Explanations** (**DiCE**) | `dice-ml` | [https://pypi.org/project/dice-ml/](https://pypi.org/project/dice-ml/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Machine Learning Library Extensions | `mlxtend` | [https://pypi.org/project/mlxtend/](https://pypi.org/project/mlxtend/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Anchor | `anchor` | [https://github.com/marcotcr/anchor](https://github.com/marcotcr/anchor)
    |'
  prefs: []
  type: TYPE_TB
- en: Table 6.1 – Python libraries or repositories with available functionalities
    for machine learning model explainability
  prefs: []
  type: TYPE_NORMAL
- en: First, we will practice with SHAP, a widely used technique for machine learning
    explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Explanations in SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ll first look at performing local explanation with SHAP, followed by global
    explanation later.
  prefs: []
  type: TYPE_NORMAL
- en: Local explanation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will practice with SHAP to extract feature importance from
    our machine learning models. We will use the **University of California Irvine**
    (**UCI**) adult dataset to predict if people made over $50k in the 90s; this is
    also available as the adult income dataset as part of the SHAP library. You can
    read about the definition of the features and other information about this dataset
    at [https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to build a supervised machine learning model using this dataset
    before using any explainability method. We will use **XGBoost** as a high-performance
    machine learning method for tabular data to practice with SHAP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'There are different methods to approximate feature importance that are available
    in the SHAP library, such as `shap.LinearExplainer()`, `shap.KernelExplainer()`,
    `shap.TreeExplainer()`, and `shap.DeepExplainer()`. You can use `shap.TreeExplainer()`
    in the case of tree-based methods such as random forest and XGBoost. Let’s build
    an explainer object using the trained model and then extract Shapely values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'There are multiple plotting functions in the SHAP library to provide us with
    visual illustrations of feature importance using Shapely values. For example,
    we can use `shap.dependence_plot()` to identify the Shapely value for the *Education-Num*
    feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following dependence plot clearly shows that a higher *Education-Num* value
    results in a higher Shapely value or a greater contribution in predicting a positive
    outcome (that is, >50k salary):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – SHAP values for the Education-Num feature in the test set of
    the adult income dataset](img/B16369_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – SHAP values for the Education-Num feature in the test set of the
    adult income dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'We can repeat this process with other features, such as *Age*, which results
    in a similar explanation as *Education-Num*. The only difference in using `shap.dependence_plot()`
    for *Education-Num* and *Age* is `interaction_index`, which is specified as `None`
    for *Age*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 6.5 – SHAP values for the Age feature in the test set of the adult
    income dataset](img/B16369_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – SHAP values for the Age feature in the test set of the adult income
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'If we need to extract an explanation of our model on a specific subset of our
    dataset, we can use the same functions but use the subset of data we want to investigate
    instead of the whole dataset. We can also use training and test sets to identify
    explanations in the data that’s used for model training and unseen data we want
    to use to evaluate the performance of our model. To showcase this, we will investigate
    the importance of *Age* on a subset of the test set that’s been misclassified
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the SHAP values have similar trends for misclassified data
    points (*Figure 6**.6*) and the whole dataset (*Figure 6**.5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – SHAP values for the Age feature for the misclassified data points
    in the test set of the adult income dataset](img/B16369_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – SHAP values for the Age feature for the misclassified data points
    in the test set of the adult income dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to extracting Shapely values across a series of data points, we
    also need to investigate how features contributed to the correct or wrong prediction
    for a data point. Here, we chose two samples: `sample 12`, with the actual label
    being False or 0 (that is, low income) and the predicted label being True or 1
    (that is, high income), and `sample 24`, with the actual and predicted labels
    of True and False, respectively. Here, we can use `shap.plots._waterfall.waterfall_legacy()`
    and extract the expected values of the input features, as shown in *Figure 6**.7*.
    In this kind of plotting in SHAP, for each feature, *X*, *f(X)* is the predicted
    value given *X*, and *E[f(X)]* is the expected value of the target variable (that
    is, the mean of all predictions, *mean(model.predict(X))*). This plot shows us
    how much a single feature affected the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 6**.7*, which is for `sample 12`, shows us that *Relationship* and
    *Education-Num* are the features with the most effect, and *Race* and *Country*
    are the ones with the least effect on the outcome of this sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – SHAP waterfall plot of sample 12 in the adult income dataset](img/B16369_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – SHAP waterfall plot of sample 12 in the adult income dataset
  prefs: []
  type: TYPE_NORMAL
- en: '*Relationship* and *Education-Num* are also the features with the most effect
    for `sample 24` (*Figure 6**.8*). However, the third most contribution in `sample
    12` is from *Hours per week*, which has a low effect on the outcome of `sample
    24`. This is the type of analysis we can do to compare some of the incorrect predictions
    and identify potentially actionable suggestions for improving model performance.
    Alternatively, we can extract actionable suggestions to improve the future income
    of individuals in this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – SHAP waterfall plot of sample 24 in the adult income dataset](img/B16369_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – SHAP waterfall plot of sample 24 in the adult income dataset
  prefs: []
  type: TYPE_NORMAL
- en: Despite the easy-to-understand insights provided by SHAP, we need to make sure
    feature dependencies in our models don’t lead to confusion when we interpret Shapely
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Global explanation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although `shap.dependence_plot()` might seem to provide a global explanation,
    as it shows the effect of a feature across all or a subset of data points, we
    need explanations across model features and data points to build trust for our
    models. `shap.summary_plot()` is an example of such a global explanation that
    summarizes the Shapely values of features across the specified set of data points.
    These kinds of summary plots and results are important for identifying the most
    effective features and understanding if there are biases, such as concerning race
    or sex, in our model. With the following summary plot (*Figure 6**.9*), we can
    easily see that *Sex* and *Race* are not among the features with the most effect,
    although their effect is not necessarily negligible and might need further investigation.
    We will talk about model bias and fairness in the next chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – SHAP summary plot for the adult income dataset](img/B16369_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – SHAP summary plot for the adult income dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the code to generate the previous summary plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Explanations using LIME
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having learned how to perform explanations using SHAP, we will now turn our
    attention to LIME. We will start with local explanation first.
  prefs: []
  type: TYPE_NORMAL
- en: Local explanation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LIME is another way to get an easy-to-understand local explanation for individual
    data points. We can use the `lime` Python library to build an explainer object
    and then use it to identify local explanations for samples of interest. Here,
    once again, we will use the XGBoost model we trained for SHAP and generate explanations
    for `sample 12` and `sample 24` to show that their outcomes were incorrectly predicted.
  prefs: []
  type: TYPE_NORMAL
- en: By default, `lime` uses *ridge regression* as the interpretable model for generating
    local explanations. We can change this method in the `lime.lime_tabular.LimeTabularExplainer()`
    class by changing `feature_selection` to `none` for linear modeling without any
    feature selection, or `lasso_path`, which uses `lasso_path()` from `scikit-learn`,
    as another form of supervised linear modeling with regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can interpret the middle plot in *Figure 6**.10* for `sample 12` as the
    local contribution of features in predicting the outcome as *Higher income* or
    *Lower income*. Similar to SHAP, the *Education-Num* and *Relationship* features
    contribute the most to the sample being incorrectly predicted as *Higher income*.
    On the other hand, *Capital Gain* and *Capital Loss* have the maximum contribution
    in pushing the prediction of the sample’s output as the other class. But we also
    have to pay attention to feature values as both *Capital Gain* and *Capital Loss*
    are zero for this sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – LIME local explanation for sample 12 in the adult income dataset](img/B16369_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – LIME local explanation for sample 12 in the adult income dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can investigate the result of LIME for `sample 24`, as shown
    in *Figure 6**.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – LIME local explanation for sample 24 in the adult income dataset](img/B16369_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – LIME local explanation for sample 24 in the adult income dataset
  prefs: []
  type: TYPE_NORMAL
- en: '*Capital Gain*, *Education-Num*, and *Hours per week* contribute the most to
    predicting the output in positive or negative directions. However, *Capital Gain*
    doesn’t affect this specific data point as its value is zero.'
  prefs: []
  type: TYPE_NORMAL
- en: Global explanation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`lime.submodular_pick.SubmodularPick()` to pick these samples. Here are the
    parameters of this class that could help you explain your global regression or
    classification models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`predict_fn` (prediction function): For `ScikitClassifiers`, this is `classifier.predict_proba()`,
    while for `ScikitRegressors`, this is `regressor.predict()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sample_size`: The number of data points to explain if `method == ''sample''`
    is chosen'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_exps_desired`: The number of explanation objects returned'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_features`: The maximum number of features present in the explanation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 6**.12* shows the three data points that were picked by SP-LIME:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Data points selected by SPI-LIME for global explainability](img/B16369_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Data points selected by SPI-LIME for global explainability
  prefs: []
  type: TYPE_NORMAL
- en: But instead of visualizing the picked instances, you can use the `as_map()`
    parameter instead of `show_in_notebook()` for each explanation object, as part
    of the explanation objects in `sp_obj.explanations`, and then summarize the information
    for a bigger set of data points instead of investigating a handful of samples.
    For such analysis, you can use a small percentage of data points, such as 1% or
    lower in the case of very large datasets with tens of thousands of data points
    or more.
  prefs: []
  type: TYPE_NORMAL
- en: Counterfactual generation using Diverse Counterfactual Explanations (DiCE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use the `dice_ml` Python library (Mothilal et al., 2020) to generate
    counterfactuals and understand how a model can switch from one prediction to another,
    as explained earlier in this chapter. First, we must train a model and then make
    an explanation object using the `dice_ml.Dice()` Python class, after installing
    and importing the `dice_ml` library, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use the generated explanation object to generate counterfactuals
    for one or multiple samples. Here, we are generating 10 counterfactuals for `sample
    1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 6**.13* shows both the feature values of the target sample and 10 corresponding
    counterfactuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – A selected data point and the generated counterfactuals from
    the adult income dataset](img/B16369_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – A selected data point and the generated counterfactuals from the
    adult income dataset
  prefs: []
  type: TYPE_NORMAL
- en: Although all counterfactuals meet the objective of switching the outcome of
    the target sample (that is, `sample 1`), not all counterfactuals are feasible
    according to the definition and meaning of each feature. For example, if we want
    to suggest to a 29-year-old individual that they change their outcome from low
    to high salary, suggesting that they will earn a high salary when they are 80
    years old is not an effective and actionable suggestion. Also, suggesting a change
    of *hours_per_week* of work from 38 to >90 is not feasible. You need to use such
    considerations in rejecting counterfactuals so that you can identify opportunities
    for model performance and provide actionable suggestions to users. Also, you can
    switch between different techniques to generate more meaningful counterfactuals
    for your models and applications.
  prefs: []
  type: TYPE_NORMAL
- en: There are more recent Python libraries such as `Dalex` (Baniecki et al., 2021)
    and `OmniXA` (Yang et al., 2022) that you can use for model explainability. We
    will also discuss how these methods and Python libraries can be used to decrease
    bias and help us move toward fairness in developing new or revising our already
    trained machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing why having explainability is not enough
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explainability helps us build trust for the users of our models. As you learned
    in this chapter, you can use explainability techniques to understand how your
    models generate the outputs for one or multiple instances in a dataset. These
    explanations could help in improving our models from a performance and fairness
    perspective. However, we cannot achieve such improvements by simply using these
    techniques blindly and generating some results in Python. For example, as we discussed
    in the *Counterfactual generation using Diverse Counterfactual Explanations (DiCE)*
    section, some of the generated counterfactuals might not be reasonable and meaningful
    and we cannot rely on them. Or, when generating local explanations for one or
    multiple data points using SHAP or LIME, we need to pay attention to the meaning
    of features, the range of values for each feature and the meaning behind them,
    and the characteristics of each data point we investigate. One aspect of decision-making
    using explainability is to distinguish the issues with the model and the specific
    data points in training, testing, or production that we are investigating. A data
    point could be an outlier that makes our model less reliable for it but doesn’t
    necessarily make our model less reliable as a whole. In the next chapter, [*Chapter
    7*](B16369_07.xhtml#_idTextAnchor218), *Decreasing Bias and Achieving Fairness*,
    we will discuss that bias detection is not simply about identifying if there are
    features such as *age*, *race*, or *skin color* that our models rely on.
  prefs: []
  type: TYPE_NORMAL
- en: Altogether, these considerations tell us that running a few Python classes to
    use explainability for our models is not enough to achieve trust and generate
    meaningful explanations. There is more to it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about interpretable machine learning models and
    how explainability techniques could help you in improving the performance and
    reliability of your models. You learned about different local and global explainability
    techniques, such as SHAP and LIME, and practiced with them in Python. You also
    had the chance to practice with the provided Python code to learn how to use machine
    learning explainability techniques in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about the approaches to detect and decrease
    biases in your models and how you can use the available functionalities in Python
    to meet the necessary fairness criteria when developing machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How could explainability help you improve your model’s performance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between local and global explainability?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it better to use linear models because of their interpretability?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does explainability analysis make a machine learning model more reliable?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could you explain the difference between SHAP and LIME for machine learning
    explainability?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could you benefit from counterfactuals in developing machine learning models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assume a machine learning model is used for loan approval in a bank. Are all
    suggested counterfactuals useful in suggesting ways a person could improve their
    chance of getting approval?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Weber, Leander, et al. *Beyond explaining: Opportunities and challenges of
    XAI-based model improvement*. Information Fusion (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linardatos, Pantelis, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. *Explainable
    AI: A review of machine learning interpretability methods*. Entropy 23.1 (2020):
    18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gilpin, Leilani H., et al. *Explaining explanations: An overview of interpretability
    of machine learning*. 2018 IEEE 5th International Conference on data science and
    advanced analytics (DSAA). IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carvalho, Diogo V., Eduardo M. Pereira, and Jaime S. Cardoso. *Machine learning
    interpretability: A survey on methods and metrics*. Electronics 8.8 (2019): 832.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Winter, Eyal. *The Shapley value*. Handbook of game theory with economic applications
    3 (2002): 2025-2054.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Guide to Explainable AI Using* *Python*: [https://www.thepythoncode.com/article/explainable-ai-model-python](https://www.thepythoncode.com/article/explainable-ai-model-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Burkart, Nadia, and Marco F. Huber. *A survey on the explainability of supervised
    machine learning*. Journal of Artificial Intelligence Research 70 (2021): 245-317.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guidotti, Riccardo. *Counterfactual explanations and how to find them: literature
    review and benchmarking*. Data Mining and Knowledge Discovery (2022): 1-55.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. *Anchors: High-precision
    model-agnostic explanations*. Proceedings of the AAAI conference on artificial
    intelligence. Vol. 32\. No. 1\. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. *Distilling the knowledge in
    a neural network*. arXiv preprint arXiv:1503.02531 (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. *Deep inside convolutional
    networks: Visualising image classification models and saliency maps*. arXiv preprint
    arXiv:1312.6034 (2013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frosst, Nicholas, and Geoffrey Hinton. *Distilling a neural network into a soft
    decision tree*. arXiv preprint arXiv:1711.09784 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lundberg, Scott M., and Su-In Lee. *A unified approach to interpreting model
    predictions*. Advances in neural information processing systems 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. *“Why should I trust
    you?” Explaining the predictions of any classifier*. Proceedings of the 22nd ACM
    SIGKDD international conference on knowledge discovery and data mining. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baniecki, Hubert, et al. *Dalex: responsible machine learning with interactive
    explainability and fairness in Python*. The Journal of Machine Learning Research
    22.1 (2021): 9759-9765.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang, Wenzhuo, et al. *OmniXAI: A Library for Explainable AI*. arXiv preprint
    arXiv:2206.01612 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hima Lakkaraju, Julius Adebayo, Sameer Singh, *AAAI 2021 Tutorial on Explaining
    Machine* *Learning Predictions*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mothilal, Ramaravind K., Amit Sharma, and Chenhao Tan. *Explaining machine learning
    classifiers through diverse counterfactual explanations*. Proceedings of the 2020
    conference on fairness, accountability, and transparency. 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
