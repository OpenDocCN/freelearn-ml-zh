- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Interpretability and Explainability in Machine Learning Modeling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习建模中的可解释性和可解释性
- en: The majority of the machine learning models we use or develop are complex and
    require the use of explainability techniques to identify opportunities for improving
    their performance, reducing their bias, and increasing their reliability.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用或开发的绝大多数机器学习模型都是复杂的，需要使用可解释性技术来识别改进它们性能、减少偏差和增加可靠性的机会。
- en: 'We will look at the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: Interpretable versus black-box machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可解释性机器学习与黑盒机器学习
- en: Explainability methods in machine learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习中的可解释性方法
- en: Practicing machine learning explainability in Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中练习机器学习可解释性
- en: Reviewing why having explainability is not enough
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审视为什么可解释性并不足够
- en: By the end of this chapter, you will have learned about the importance of explainability
    in machine learning modeling and practiced using some of the explainability techniques
    in Python.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解机器学习建模中可解释性的重要性，并练习使用Python中的某些可解释性技术。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following requirements should be considered for this chapter as they help
    you better understand the mentioned concepts, use them in your projects, and practice
    with the provided code:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，以下要求应予以考虑，因为它们有助于您更好地理解所提到的概念，在您的项目中使用它们，并使用提供的代码进行实践：
- en: 'Python library requirements:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库要求：
- en: '`sklearn` >= 1.2.2'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` >= 1.2.2'
- en: '`numpy` >= 1.22.4'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` >= 1.22.4'
- en: '`matplotlib` >= 3.7.1'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` >= 3.7.1'
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter06](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter06).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到本章的代码文件：[https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter06](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter06)。
- en: Interpretable versus black-box machine learning
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性机器学习与黑盒机器学习
- en: Interpretable and simple models such as linear regression make it easy to assess
    the possibility of improving them, finding issues with them such as biases that
    need to be detected and removed, and building trust in using such models. However,
    to achieve higher performance, we usually don’t stop with these simple models
    and rely on complex or so-called black-box models. In this section, we will review
    some of the interpretable models and then introduce techniques you can use to
    explain your black-box models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如线性回归这样的可解释和简单模型使得评估改进它们的可能性、发现它们的问题（如需要检测和消除的偏差）以及建立对使用此类模型的信任变得容易。然而，为了实现更高的性能，我们通常不会止步于这些简单模型，而是依赖于复杂或所谓的黑盒模型。在本节中，我们将回顾一些可解释模型，然后介绍您可以用来解释您的黑盒模型的技术。
- en: Interpretable machine learning models
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释的机器学习模型
- en: 'Linear models such as linear and logistic regression, shallow decision trees,
    and Naive Bayes classifiers are examples of simple and interpretable methods (*Figure
    6**.1*). We can easily extract the contribution of features in predictions of
    outputs for these models and identify opportunities for improving their performance,
    such as by adding or removing features or changing feature normalization. We can
    also easily identify if there are biases in our models – for example, for a specific
    race or gender group. However, these models are very simple, and having access
    to large datasets of thousands or millions of samples allows us to train high-performance
    but complex models:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型，如线性回归和逻辑回归、浅层决策树和朴素贝叶斯分类器，是简单且可解释的方法的例子（*图6.1*）。我们可以轻松地提取这些模型在预测输出中对特征贡献的部分，并识别改进它们性能的机会，例如通过添加或删除特征或改变特征归一化。我们还可以轻松地识别模型中是否存在偏差——例如，对于特定的种族或性别群体。然而，这些模型非常简单，能够访问成千上万或数百万样本的大型数据集使我们能够训练高性能但复杂的模型：
- en: '![Figure 6.1 – Examples of interpretable classification methods](img/B16369_06_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 可解释分类方法的示例](img/B16369_06_01.jpg)'
- en: Figure 6.1 – Examples of interpretable classification methods
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 可解释分类方法的示例
- en: Complex models, such as random forest models with many deep decision trees or
    deep neural networks, help us in achieving higher performance, although they work
    almost like black-box systems. To be able to understand these models and explain
    how they come up with their predictions, and to build trust in their utility,
    we can use machine learning explainability techniques.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂模型，例如具有许多深度决策树或深度神经网络的随机森林模型，虽然它们几乎像黑盒系统一样工作，但有助于我们实现更高的性能。为了能够理解这些模型并解释它们是如何得出预测的，以及建立对其有用性的信任，我们可以使用机器学习可解释性技术。
- en: Explainability for complex models
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 复杂模型的可解释性
- en: 'Explainability techniques work like bridges between complex machine learning
    models and users. They are supposed to provide explainabilities that are faithful
    to how the models work. And on the other side, they are supposed to provide explanations
    that are useful and understandable for the users. These explanations can be used
    to identify opportunities for improving model performance, reducing the sensitivity
    of models to small feature value changes, increasing data efficiency in model
    training, trying to help in proper reasoning in the model and avoid spurious correlations,
    and helping in achieving fairness (Weber et al., 2022; *Figure 6**.2*):'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性技术就像复杂机器学习模型和用户之间的桥梁。它们应该提供忠实于模型工作方式的解释。另一方面，它们应该提供对用户有用且易于理解的解释。这些解释可用于识别改进模型性能的机会、减少模型对特征值变化敏感性的影响、提高模型训练中的数据效率、帮助在模型中进行适当的推理、避免虚假相关性，并帮助实现公平性（Weber等人，2022年；*图6.2*）：
- en: '![Figure 6.2 – Effects of using explainability on machine learning models](img/B16369_06_02.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 使用可解释性对机器学习模型的影响](img/B16369_06_02.jpg)'
- en: Figure 6.2 – Effects of using explainability on machine learning models
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 使用可解释性对机器学习模型的影响
- en: Now that you have a better understanding of the importance of explainability
    in machine learning modeling, we are ready to get into the details of explainability
    techniques.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经更好地理解了可解释性在机器学习建模中的重要性，我们准备深入了解可解释性技术的细节。
- en: Explainability methods in machine learning
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的可解释性方法
- en: 'We need to keep the following considerations in mind when using or developing
    explainability techniques for machine learning modeling (Ribeiro et al., 2016):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用或开发机器学习建模的可解释性技术时，我们需要牢记以下考虑因素（Ribeiro等人，2016年）：
- en: '**Interpretability**: The explanations need to be understandable to users.
    One of the main objectives of machine learning explanation is to make complex
    models understandable for users and, if possible, provide actionable information.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：解释需要用户能够理解。机器学习解释的主要目标之一是使复杂模型对用户可理解，并在可能的情况下提供可操作的信息。'
- en: '**Local fidelity (faithfulness)**: Capturing the complexity of models so that
    they are completely faithful and meet global faithfulness criteria can’t be achieved
    by all techniques. However, an explanation should be at least locally faithful
    to the model. In other words, an explanation needs to properly explain how the
    model behaves in the close neighborhood of the data point under investigation.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**局部忠实度（忠实性）**：捕捉模型的复杂性，以便它们完全忠实并满足全局忠实度标准，并非所有技术都能实现。然而，解释至少应该对模型局部忠实。换句话说，解释需要恰当地解释模型在研究数据点的邻近区域是如何表现的。'
- en: '**Being model-agnostic**: Although there are techniques that are designed for
    specific machine learning methods, such as random forest, they are supposed to
    be agnostic to models that are built with different hyperparameters or for different
    datasets. An explainability technique needs to consider the model as a black box
    and provide explanations for the model either globally or locally.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型无关性**：尽管有一些技术是为特定的机器学习方法设计的，例如随机森林，但它们应该对使用不同超参数或针对不同数据集构建的模型保持无关。可解释性技术需要将模型视为黑盒，并为模型提供全局或局部的解释。'
- en: Explainability techniques can be categorized into **local explainability** and
    **global explainability** methods. Local explainability methods aim to meet the
    previously listed criteria, while global explainability techniques try to go beyond
    local explainability and provide global explanations to the models.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性技术可以分为**局部可解释性**和**全局可解释性**方法。局部可解释性方法旨在满足之前列出的标准，而全局可解释性技术试图超越局部可解释性，并为模型提供全局解释。
- en: Local explainability techniques
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局部可解释性技术
- en: 'Local explainability helps us understand the behavior of a model close to a
    data point in a feature space. Although these models meet local fidelity criteria,
    features identified to be locally important might not be globally important, and
    vice versa (Ribeiro et al., 2016). This means that we cannot infer local explanations
    from global explanations, and vice versa, easily. In this section, we will discuss
    five local explanation techniques:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 局部可解释性帮助我们理解模型在特征空间中接近数据点的行为。尽管这些模型满足局部保真度标准，但被识别为局部重要的特征可能不是全局重要的，反之亦然（Ribeiro
    等人，2016）。这意味着我们无法轻易地从全局解释推断出局部解释，反之亦然。在本节中，我们将讨论五种局部解释技术：
- en: Feature importance
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征重要性
- en: Counterfactuals
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反事实
- en: Sample-based explainability
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于样本的可解释性
- en: Rule-based explainability
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于规则的解释性
- en: Saliency maps
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显著性图
- en: We will also go through a few global explainability techniques after.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们还将介绍一些全局可解释性技术。
- en: Feature importance
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征重要性
- en: One of the primary approaches to local explainability is explaining the contribution
    of each feature *locally* in predicting the outcome of the target data points
    in a neighborhood. Widely used examples of such methods include **SHapley Additive
    exPlanations** (**SHAP**) (Lundberg et al., 2017) and **Local Interpretable Model-agnostic
    Explanations** (**LIME**) (Ribeiro et al., 2016). Let’s briefly discuss the theory
    behind these two methods and practice them in Python.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 局部可解释性的主要方法之一是解释每个特征在预测目标数据点在邻域中的结果时的局部贡献。这类方法的广泛使用例子包括 **SHapley Additive exPlanations**（SHAP）（Lundberg
    等人，2017）和 **Local Interpretable Model-agnostic Explanations**（LIME）（Ribeiro 等人，2016）。让我们简要讨论这两种方法背后的理论，并在
    Python 中实践它们。
- en: Local explanation using SHAP
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 SHAP 的局部解释
- en: SHAP is a Python framework that was introduced by Scott Lundberg and Su-In Lee
    (Lundberg and Lee, 2017). The idea of this framework is based on using the Shapely
    value, a known concept named after Lloyd Shapley, an American game theorist and
    Nobel Prize winner (Winter, 2022). SHAP can determine the contribution of each
    feature to a model’s prediction. As features work cooperatively in determining
    the decision boundaries of classification models and eventually affecting model
    predictions, SHAP tries to first identify the marginal contribution of each feature
    and then provide Shapely values as an estimate of the contribution of each feature
    in cooperation with the whole feature set regarding the predictions of a model.
    From a theoretical perspective, these marginal contributions can be calculated
    by removing features individually and in different combinations, calculating the
    effect of each feature set removal, and then normalizing the contributions. This
    process can’t be repeated for all possible feature combinations as the number
    of possible combinations could grow exponentially to billions, even for a model
    with 40 features. Instead, this process is used a limited number of times to come
    up with an approximation of Shapely values. Also, since removing features is not
    possible in most machine learning models, feature values get replaced by alternative
    values either from a random distribution or from a background set of meaningful
    and possible values for each feature. We don’t want to get into the theoretical
    details of this process but we will practice using this approach in the next section.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 是由 Scott Lundberg 和 Su-In Lee（Lundberg 和 Lee，2017）引入的 Python 框架。这个框架的想法是基于使用
    Shapely 值，这是一个以美国博弈论家、诺贝尔奖获得者 Lloyd Shapley 命名的已知概念（Winter，2022）。SHAP 可以确定每个特征对模型预测的贡献。由于特征在确定分类模型的决策边界并最终影响模型预测方面是协同工作的，SHAP
    尝试首先识别每个特征的边际贡献，然后提供 Shapely 值作为每个特征在整个特征集合作下对模型预测贡献的估计。从理论角度来看，这些边际贡献可以通过单独移除特征以及在不同的组合中移除特征来计算，计算每个特征集移除的效果，然后对贡献进行归一化。由于可能的组合数量可能呈指数增长到数十亿，即使对于具有
    40 个特征的模型，这个过程也不能对所有可能的特征组合重复进行。相反，这个过程只使用有限次数来得到 Shapely 值的近似。此外，由于在大多数机器学习模型中无法移除特征，特征值要么由随机分布中的替代值替换，要么由每个特征的背景集合中的有意义且可能的值替换。我们不想深入探讨这个过程的细节，但将在下一节中练习使用这种方法。
- en: Local explanation using LIME
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 LIME 的局部解释
- en: 'LIME is an alternative to SHAP for local explainability that explains the predictions
    of any classifier or regressor, in a model-agnostic way, by approximating a model
    locally with an interpretable model (*Figure 6**.3*; Ribeiro et al., 2016):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 是 SHAP 的替代品，用于局部可解释性，它以模型无关的方式通过局部近似可解释模型来解释任何分类器或回归器的预测（*图 6**.3*；Ribeiro
    等人，2016）：
- en: '![Figure 6.3 – Schematic representation of local interpretable modeling in
    LIME](img/B16369_06_03.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – LIME 中局部可解释建模的示意图](img/B16369_06_03.jpg)'
- en: Figure 6.3 – Schematic representation of local interpretable modeling in LIME
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – LIME 中局部可解释建模的示意图
- en: 'Some of the advantages of this technique, which were also mentioned in the
    original paper by Ribeiro et al., 2016, include the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 该技术的某些优点，如 Ribeiro 等人（2016）的原论文中提到的，包括以下内容：
- en: The theory and the provided explanations are intuitive and easy to understand
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论和提供的解释直观且易于理解
- en: Sparse explanations are provided to increase interpretability
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供稀疏解释以增加可解释性
- en: Works with different types of structured and unstructured data, such as texts
    and images
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与不同类型的结构化和非结构化数据一起工作，例如文本和图像
- en: Counterfactuals
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反事实
- en: 'Counterfactual examples, or explanations, help us identify what needs to be
    changed in an instance to change the outcome of a classification model. These
    counterfactuals could help in identifying actionable paths in many applications,
    such as finance, retail, marketing, recruiting, and healthcare. One example is
    when suggesting to a bank customer how they can change the rejection to their
    loan application (Guidotti, 2022). Counterfactuals could also help in identifying
    biases in models that help us improve model performance or eliminate fairness
    issues in our models. We need to keep the following considerations in mind while
    generating and using counterfactual explanations (Guidotti, 2022):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 反事实示例或解释有助于我们识别在实例中需要改变什么才能改变分类模型的输出。这些反事实可以帮助在许多应用中识别可操作路径，例如金融、零售、营销、招聘和医疗保健。一个例子是向银行客户建议他们如何改变其贷款申请被拒绝的情况（Guidotti,
    2022）。反事实还可以帮助识别模型中的偏差，这有助于我们提高模型性能或消除模型中的公平性问题。在生成和使用反事实解释时，我们需要考虑以下因素（Guidotti,
    2022）：
- en: '**Validity**: A counterfactual example is valid if and only if its classification
    outcome would be different from the original sample.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有效性**：只有当反事实示例的分类结果与原始样本不同时，反事实示例才是有效的。'
- en: '**Similarity**: A counterfactual example should be as similar as possible to
    the original data point.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相似性**：反事实示例应尽可能与原始数据点相似。'
- en: '**Diversity**: Although counterfactual examples should be similar to the original
    samples they are derived from, they need to be diverse among each other to provide
    different options (that is, different possible feature changes).'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：尽管反事实示例应与它们所派生的原始样本相似，但它们之间需要具有多样性，以提供不同的选项（即不同的可能特征变化）。'
- en: '**Actionability**: Not all the feature value changes are actionable. The actionability
    of the counterfactuals that are suggested by a counterfactual method is an important
    factor in benefitting from them in practice.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可操作性**：并非所有特征值的变化都具有可操作性。由反事实方法建议的反事实的可操作性是实际受益的重要因素。'
- en: '**Plausibility**: The feature values of a counterfactual example should be
    plausible. The plausibility of the counterfactuals increases trust in deriving
    explanations from them.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合理性**：反事实示例的特征值应该是合理的。反事实的合理性增加了从它们中推导出解释的信任度。'
- en: We also have to note that counterfactual explainers need to be efficient and
    fast enough in generating the counterfactuals and stable in generating counterfactuals
    for similar data points (Guidotti, 2022).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还必须注意，反事实解释器需要高效且足够快地生成反事实，并且在生成与相似数据点相关的反事实时保持稳定（Guidotti, 2022）。
- en: Sample-based explainability
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于样本的可解释性
- en: Another approach to explainability is to rely on the feature values and results
    of real or synthetic data points to help in local model explainability. In this
    category of explainability techniques, we aim to find out which samples are misclassified
    and what feature sets result in an increasing chance of misclassification to help
    us explain our models. We can also assess which training data points result in
    a change in the decision boundary so that we can predict the output of test or
    production data points. There are statistical methods such as the **Influence
    function** (Koh and Liang 2017), a classical approach for assessing the influence
    of samples on model parameters, that we can use to identify the sample’s contribution
    to the decision-making process of models.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解释性的方法是依靠真实或合成数据点的特征值和结果来帮助局部模型解释性。在这个解释性技术类别中，我们的目标是找出哪些样本被错误分类，以及哪些特征集导致错误分类的概率增加，以帮助我们解释我们的模型。我们还可以评估哪些训练数据点导致决策边界的改变，以便我们可以预测测试或生产数据点的输出。有一些统计方法，如**影响函数**（Koh和Liang
    2017），这是一种评估样本对模型参数影响的经典方法，我们可以用它来识别样本对模型决策过程的贡献。
- en: Rule-based explainability
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于规则的解释性
- en: Rule-based methods such as **Anchor explanations** aim to find the conditions
    of feature values that result in a high probability of getting the same output
    (Ribeiro et al., 2018). For example, in the case of predicting the salary of individuals
    in a dataset to be less than or equal to 50k or above 50k, “Education <= high
    school to result in <=50k salary” could be considered a rule in rule-based explanation.
    These explanations need to be locally faithful.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的解释方法，如**锚点解释**，旨在找出导致高概率得到相同输出的特征值条件（Ribeiro等，2018）。例如，在预测数据集中个人的薪水低于或等于50k或高于50k的情况下，“教育程度≤高中导致薪水≤50k”可以被视为基于规则的解释中的一个规则。这些解释需要局部忠实。
- en: Saliency maps
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 显著性图
- en: The objective of saliency maps is to explain which features contribute more
    or less to the predicted outputs for a data point. These methods are commonly
    used in machine learning or deep learning models that have been trained on image
    data (Simonyan et al., 2013). For example, we can use saliency maps to figure
    out if a classification model uses a background forest to identify if it is an
    image of a bear rather than a teddy bear or uses the components of the bear’s
    body for it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 显著性图的目标是解释哪些特征对数据点的预测输出贡献更多或更少。这些方法通常用于在图像数据上训练的机器学习或深度学习模型（Simonyan等，2013）。例如，我们可以使用显著性图来确定分类模型是否使用背景森林来识别它是一张熊的图片而不是泰迪熊，或者是否使用熊的身体部件来识别它。
- en: Global explanation
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全局解释
- en: Despite the difficulty in achieving a reliable global explanation for machine
    learning models, it could increase trust in them (Ribeiro et al., 2016). Performance
    is not the only aspect of building trust when developing and deploying machine
    learning models. And local explanations, although very helpful in investigating
    individual samples and providing actionable information, might not be enough for
    this trust building. Here, we will discuss three approaches for going beyond local
    explanation, including collecting local explanations, knowledge distillation,
    and summaries of counterfactuals.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管为机器学习模型实现可靠的全球解释很困难，但它可以增加对它们的信任（Ribeiro等，2016）。在开发和部署机器学习模型时，性能并不是建立信任的唯一方面。虽然局部解释在调查单个样本和提供可操作信息方面非常有帮助，但可能不足以建立这种信任。在这里，我们将讨论三种超越局部解释的方法，包括收集局部解释、知识蒸馏和反事实的摘要。
- en: Collecting local explanations
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 收集局部解释
- en: '**Submodular pick LIME** (**SP-LIME**) is a global explanation technique that
    uses local explanations of LIME to come up with a global perspective of a model’s
    behavior (Riberio et al., 2016). As it might not be feasible to use the local
    explanations of all data points, SP-LIME picks a representative diverse set of
    samples capable of representing the global behavior of the model.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**子模可加选择LIME**（**SP-LIME**）是一种全局解释技术，它使用LIME的局部解释来提供一个模型行为的全局视角（Riberio等，2016）。由于可能无法使用所有数据点的局部解释，SP-LIME选择了一组代表性的多样化样本，这些样本能够代表模型的全局行为。'
- en: Knowledge distillation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 知识蒸馏
- en: The idea of **knowledge distillation** is to approximate the behavior of complex
    models, which was initially proposed for neural network models, using simpler
    interpretable models such as decision trees (Hinton et al., 2015; Frosst and Hinton,
    2017). In other words, we aim to build simpler models, such as decision trees,
    that approximate the predictions of complex models for a given set of samples.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**知识蒸馏**的思想是使用简单的可解释模型（如决策树）来近似复杂模型的行为，这一概念最初是为神经网络模型提出的（Hinton等人，2015年；Frosst和Hinton，2017年）。换句话说，我们的目标是构建简单的模型，如决策树，以近似给定样本集的复杂模型的预测。'
- en: Summaries of counterfactuals
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反事实的摘要
- en: We can use a summary of counterfactuals that’s been generated for multiple data
    points with correct and incorrect predicted outcomes to figure out the contribution
    of features in output prediction and the sensitivity of a prediction to feature
    perturbation. We will practice using counterfactuals later in this chapter, where
    you will see that not all counterfactuals are acceptable and they need to be chosen
    according to the meaning behind features and their values.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用为多个数据点生成的反事实摘要（包括正确和错误的预测结果）来了解特征在输出预测中的贡献以及预测对特征扰动的敏感性。我们将在本章后面练习使用反事实，你将看到并非所有反事实都是可接受的，并且它们需要根据特征及其值的含义来选择。
- en: Practicing machine learning explainability in Python
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中实践机器学习可解释性
- en: 'There are several Python libraries you can use to extract local and global
    explanations for your machine learning models (*Table 6.1*). Here, we want to
    practice with a few of the ones that focus on local model explainability:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个Python库你可以用来提取你的机器学习模型的局部和全局解释（*表6.1*）。在这里，我们想练习一些专注于局部模型可解释性的库：
- en: '| **Library** | **Library Name for Importing** **and Installation** | **URL**
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **库** | **导入和安装库名称** | **URL** |'
- en: '| SHAP | `Shap` | [https://pypi.org/project/shap/](https://pypi.org/project/shap/)
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SHAP | `Shap` | [https://pypi.org/project/shap/](https://pypi.org/project/shap/)
    |'
- en: '| LIME | `Lime` | [https://pypi.org/project/lime/](https://pypi.org/project/lime/)
    |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LIME | `Lime` | [https://pypi.org/project/lime/](https://pypi.org/project/lime/)
    |'
- en: '| Shapash | `shapash` | [https://pypi.org/project/shapash/](https://pypi.org/project/shapash/)
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Shapash | `shapash` | [https://pypi.org/project/shapash/](https://pypi.org/project/shapash/)
    |'
- en: '| ELI5 | `eli5` | [https://pypi.org/project/eli5/](https://pypi.org/project/eli5/)
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| ELI5 | `eli5` | [https://pypi.org/project/eli5/](https://pypi.org/project/eli5/)
    |'
- en: '| Explainer dashboard | `explainer dashboard` | [https://pypi.org/project/explainerdashboard/](https://pypi.org/project/explainerdashboard/)
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 解释仪表板 | `explainer dashboard` | [https://pypi.org/project/explainerdashboard/](https://pypi.org/project/explainerdashboard/)
    |'
- en: '| Dalex | `dalex` | [https://pypi.org/project/dalex/](https://pypi.org/project/dalex/)
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| Dalex | `dalex` | [https://pypi.org/project/dalex/](https://pypi.org/project/dalex/)
    |'
- en: '| OmniXAI | `omnixai` | [https://pypi.org/project/omnixai/](https://pypi.org/project/omnixai/)
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| OmniXAI | `omnixai` | [https://pypi.org/project/omnixai/](https://pypi.org/project/omnixai/)
    |'
- en: '| CARLA | `carla` | [https://carla-counterfactual-and-recourse-library.readthedocs.io/en/latest/](https://carla-counterfactual-and-recourse-library.readthedocs.io/en/latest/)
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| CARLA | `carla` | [https://carla-counterfactual-and-recourse-library.readthedocs.io/en/latest/](https://carla-counterfactual-and-recourse-library.readthedocs.io/en/latest/)
    |'
- en: '| **Diverse Counterfactual** **Explanations** (**DiCE**) | `dice-ml` | [https://pypi.org/project/dice-ml/](https://pypi.org/project/dice-ml/)
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **Diverse Counterfactual Explanations (DiCE**) | `dice-ml` | [https://pypi.org/project/dice-ml/](https://pypi.org/project/dice-ml/)
    |'
- en: '| Machine Learning Library Extensions | `mlxtend` | [https://pypi.org/project/mlxtend/](https://pypi.org/project/mlxtend/)
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 机器学习库扩展 | `mlxtend` | [https://pypi.org/project/mlxtend/](https://pypi.org/project/mlxtend/)
    |'
- en: '| Anchor | `anchor` | [https://github.com/marcotcr/anchor](https://github.com/marcotcr/anchor)
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 锚点 | `anchor` | [https://github.com/marcotcr/anchor](https://github.com/marcotcr/anchor)
    |'
- en: Table 6.1 – Python libraries or repositories with available functionalities
    for machine learning model explainability
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – 具有机器学习模型可解释性功能的Python库或存储库
- en: First, we will practice with SHAP, a widely used technique for machine learning
    explainability.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用SHAP进行练习，这是一种广泛用于机器学习可解释性的技术。
- en: Explanations in SHAP
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP中的解释
- en: We’ll first look at performing local explanation with SHAP, followed by global
    explanation later.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探讨使用SHAP进行局部解释，随后再讨论全局解释。
- en: Local explanation
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部解释
- en: In this section, we will practice with SHAP to extract feature importance from
    our machine learning models. We will use the **University of California Irvine**
    (**UCI**) adult dataset to predict if people made over $50k in the 90s; this is
    also available as the adult income dataset as part of the SHAP library. You can
    read about the definition of the features and other information about this dataset
    at [https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用SHAP从我们的机器学习模型中提取特征重要性。我们将使用**加州大学欧文分校**（**UCI**）的成年人数据集来预测90年代人们是否年收入超过50k；这也可以作为SHAP库的一部分的成年人收入数据集。您可以在[https://archive.ics.uci.edu/ml/datasets/adult](https://archive.ics.uci.edu/ml/datasets/adult)上阅读有关特征定义和其他有关此数据集的信息。
- en: 'First, we need to build a supervised machine learning model using this dataset
    before using any explainability method. We will use **XGBoost** as a high-performance
    machine learning method for tabular data to practice with SHAP:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在使用任何可解释性方法之前，我们需要使用此数据集构建一个监督机器学习模型。我们将使用**XGBoost**作为表格数据的高性能机器学习方法来练习SHAP：
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are different methods to approximate feature importance that are available
    in the SHAP library, such as `shap.LinearExplainer()`, `shap.KernelExplainer()`,
    `shap.TreeExplainer()`, and `shap.DeepExplainer()`. You can use `shap.TreeExplainer()`
    in the case of tree-based methods such as random forest and XGBoost. Let’s build
    an explainer object using the trained model and then extract Shapely values:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP库中提供了不同的方法来近似特征重要性，例如`shap.LinearExplainer()`、`shap.KernelExplainer()`、`shap.TreeExplainer()`和`shap.DeepExplainer()`。在基于树的方法（如随机森林和XGBoost）的情况下，您可以使用`shap.TreeExplainer()`。让我们使用训练好的模型构建一个解释器对象，然后提取Shapely值：
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There are multiple plotting functions in the SHAP library to provide us with
    visual illustrations of feature importance using Shapely values. For example,
    we can use `shap.dependence_plot()` to identify the Shapely value for the *Education-Num*
    feature:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP库中有多个绘图函数，可以提供使用Shapely值的特征重要性的视觉说明。例如，我们可以使用`shap.dependence_plot()`来识别*教育程度数*特征的Shapely值：
- en: '[PRE2]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following dependence plot clearly shows that a higher *Education-Num* value
    results in a higher Shapely value or a greater contribution in predicting a positive
    outcome (that is, >50k salary):'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的依赖性图清楚地表明，更高的*教育程度数*值会导致更高的Shapely值或更大的预测积极结果（即，>50k薪水）的贡献：
- en: '![Figure 6.4 – SHAP values for the Education-Num feature in the test set of
    the adult income dataset](img/B16369_06_04.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 成年人收入数据集测试集中教育程度数特征的SHAP值](img/B16369_06_04.jpg)'
- en: Figure 6.4 – SHAP values for the Education-Num feature in the test set of the
    adult income dataset
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 成年人收入数据集测试集中教育程度数特征的SHAP值
- en: 'We can repeat this process with other features, such as *Age*, which results
    in a similar explanation as *Education-Num*. The only difference in using `shap.dependence_plot()`
    for *Education-Num* and *Age* is `interaction_index`, which is specified as `None`
    for *Age*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用其他特征重复此过程，例如*年龄*，这将产生与*教育程度数*相似的解释。使用`shap.dependence_plot()`对*教育程度数*和*年龄*的唯一区别在于`interaction_index`，对于*年龄*被指定为`None`：
- en: '[PRE3]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![Figure 6.5 – SHAP values for the Age feature in the test set of the adult
    income dataset](img/B16369_06_05.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 成年人收入数据集测试集中年龄特征的SHAP值](img/B16369_06_05.jpg)'
- en: Figure 6.5 – SHAP values for the Age feature in the test set of the adult income
    dataset
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 成年人收入数据集测试集中年龄特征的SHAP值
- en: 'If we need to extract an explanation of our model on a specific subset of our
    dataset, we can use the same functions but use the subset of data we want to investigate
    instead of the whole dataset. We can also use training and test sets to identify
    explanations in the data that’s used for model training and unseen data we want
    to use to evaluate the performance of our model. To showcase this, we will investigate
    the importance of *Age* on a subset of the test set that’s been misclassified
    using the following code:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要从数据集的特定子集中提取模型解释，我们可以使用相同的函数，但使用我们想要调查的数据子集而不是整个数据集。我们还可以使用训练集和测试集来识别用于模型训练的数据中的解释以及我们想要用于评估模型性能的未见数据。为了展示这一点，我们将使用以下代码调查测试集中被错误分类的子集中*年龄*的重要性：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see, the SHAP values have similar trends for misclassified data
    points (*Figure 6**.6*) and the whole dataset (*Figure 6**.5*):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，SHAP值对于错误分类的数据点（*图6.6*）和整个数据集（*图6.5*）具有相似的趋势：
- en: '![Figure 6.6 – SHAP values for the Age feature for the misclassified data points
    in the test set of the adult income dataset](img/B16369_06_06.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – SHAP values for the Age feature for the misclassified data points
    in the test set of the adult income dataset
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to extracting Shapely values across a series of data points, we
    also need to investigate how features contributed to the correct or wrong prediction
    for a data point. Here, we chose two samples: `sample 12`, with the actual label
    being False or 0 (that is, low income) and the predicted label being True or 1
    (that is, high income), and `sample 24`, with the actual and predicted labels
    of True and False, respectively. Here, we can use `shap.plots._waterfall.waterfall_legacy()`
    and extract the expected values of the input features, as shown in *Figure 6**.7*.
    In this kind of plotting in SHAP, for each feature, *X*, *f(X)* is the predicted
    value given *X*, and *E[f(X)]* is the expected value of the target variable (that
    is, the mean of all predictions, *mean(model.predict(X))*). This plot shows us
    how much a single feature affected the prediction:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Figure 6**.7*, which is for `sample 12`, shows us that *Relationship* and
    *Education-Num* are the features with the most effect, and *Race* and *Country*
    are the ones with the least effect on the outcome of this sample:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – SHAP waterfall plot of sample 12 in the adult income dataset](img/B16369_06_07.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – SHAP waterfall plot of sample 12 in the adult income dataset
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '*Relationship* and *Education-Num* are also the features with the most effect
    for `sample 24` (*Figure 6**.8*). However, the third most contribution in `sample
    12` is from *Hours per week*, which has a low effect on the outcome of `sample
    24`. This is the type of analysis we can do to compare some of the incorrect predictions
    and identify potentially actionable suggestions for improving model performance.
    Alternatively, we can extract actionable suggestions to improve the future income
    of individuals in this dataset:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – SHAP waterfall plot of sample 24 in the adult income dataset](img/B16369_06_08.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – SHAP waterfall plot of sample 24 in the adult income dataset
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Despite the easy-to-understand insights provided by SHAP, we need to make sure
    feature dependencies in our models don’t lead to confusion when we interpret Shapely
    values.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Global explanation
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although `shap.dependence_plot()` might seem to provide a global explanation,
    as it shows the effect of a feature across all or a subset of data points, we
    need explanations across model features and data points to build trust for our
    models. `shap.summary_plot()` is an example of such a global explanation that
    summarizes the Shapely values of features across the specified set of data points.
    These kinds of summary plots and results are important for identifying the most
    effective features and understanding if there are biases, such as concerning race
    or sex, in our model. With the following summary plot (*Figure 6**.9*), we can
    easily see that *Sex* and *Race* are not among the features with the most effect,
    although their effect is not necessarily negligible and might need further investigation.
    We will talk about model bias and fairness in the next chapter:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `shap.dependence_plot()` 可能看起来提供了全局解释，因为它显示了特征对所有或数据点的子集的影响，但我们仍需要模型特征和数据点的解释来建立对模型的信任。`shap.summary_plot()`
    是此类全局解释的一个例子，它总结了指定数据点集中特征的全局 Shapely 值。这类摘要图和结果对于识别最有效的特征以及了解模型中是否存在诸如种族或性别之类的偏见非常重要。通过以下摘要图（图
    6.9），我们可以轻松地看到 *性别* 和 *种族* 并不是影响最大的特征之一，尽管它们的影响可能并不一定可以忽略，可能需要进一步调查。我们将在下一章讨论模型偏差和公平性：
- en: '![Figure 6.9 – SHAP summary plot for the adult income dataset](img/B16369_06_09.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 成人收入数据集的 SHAP 摘要图](img/B16369_06_09.jpg)'
- en: Figure 6.9 – SHAP summary plot for the adult income dataset
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 成人收入数据集的 SHAP 摘要图
- en: 'Here is the code to generate the previous summary plot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是生成之前摘要图的代码：
- en: '[PRE6]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Explanations using LIME
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LIME 的解释
- en: Having learned how to perform explanations using SHAP, we will now turn our
    attention to LIME. We will start with local explanation first.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习了如何使用 SHAP 进行解释后，我们现在将注意力转向 LIME。我们将首先从局部解释开始。
- en: Local explanation
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部解释
- en: LIME is another way to get an easy-to-understand local explanation for individual
    data points. We can use the `lime` Python library to build an explainer object
    and then use it to identify local explanations for samples of interest. Here,
    once again, we will use the XGBoost model we trained for SHAP and generate explanations
    for `sample 12` and `sample 24` to show that their outcomes were incorrectly predicted.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: LIME 是获取单个数据点易于理解的局部解释的另一种方法。我们可以使用 `lime` Python 库来构建解释器对象，然后使用它来识别感兴趣样本的局部解释。在这里，我们再次将使用为
    SHAP 训练的 XGBoost 模型，并为 `sample 12` 和 `sample 24` 生成解释，以表明它们的预测结果是不正确的。
- en: By default, `lime` uses *ridge regression* as the interpretable model for generating
    local explanations. We can change this method in the `lime.lime_tabular.LimeTabularExplainer()`
    class by changing `feature_selection` to `none` for linear modeling without any
    feature selection, or `lasso_path`, which uses `lasso_path()` from `scikit-learn`,
    as another form of supervised linear modeling with regularization.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`lime` 使用 *岭回归* 作为生成局部解释的可解释模型。我们可以在 `lime.lime_tabular.LimeTabularExplainer()`
    类中通过将 `feature_selection` 改为 `none` 来更改此方法，以进行无特征选择的线性建模，或者使用 `lasso_path`，它使用
    `scikit-learn` 的 `lasso_path()`，作为另一种带有正则化的监督线性建模形式。
- en: Note
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can interpret the middle plot in *Figure 6**.10* for `sample 12` as the
    local contribution of features in predicting the outcome as *Higher income* or
    *Lower income*. Similar to SHAP, the *Education-Num* and *Relationship* features
    contribute the most to the sample being incorrectly predicted as *Higher income*.
    On the other hand, *Capital Gain* and *Capital Loss* have the maximum contribution
    in pushing the prediction of the sample’s output as the other class. But we also
    have to pay attention to feature values as both *Capital Gain* and *Capital Loss*
    are zero for this sample:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将图 6.10 中中间的图解释为 `sample 12` 预测结果为 *高收入* 或 *低收入* 的局部特征贡献。与 SHAP 类似，*教育程度*
    和 *关系* 特征对样本被错误预测为 *高收入* 的贡献最大。另一方面，*资本收益* 和 *资本损失* 对推动样本输出预测为另一类别的贡献最大。但我们也必须注意特征值，因为对于这个样本，*资本收益*
    和 *资本损失* 都是零：
- en: '![Figure 6.10 – LIME local explanation for sample 12 in the adult income dataset](img/B16369_06_10.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 成人收入数据集中样本 12 的 LIME 局部解释](img/B16369_06_10.jpg)'
- en: Figure 6.10 – LIME local explanation for sample 12 in the adult income dataset
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 成人收入数据集中样本 12 的 LIME 局部解释
- en: 'Similarly, we can investigate the result of LIME for `sample 24`, as shown
    in *Figure 6**.11*:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以调查 `sample 24` 的 LIME 结果，如图 6.11 所示：
- en: '![Figure 6.11 – LIME local explanation for sample 24 in the adult income dataset](img/B16369_06_11.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – 成年人收入数据集中样本24的LIME局部解释](img/B16369_06_11.jpg)'
- en: Figure 6.11 – LIME local explanation for sample 24 in the adult income dataset
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 成年人收入数据集中样本24的LIME局部解释
- en: '*Capital Gain*, *Education-Num*, and *Hours per week* contribute the most to
    predicting the output in positive or negative directions. However, *Capital Gain*
    doesn’t affect this specific data point as its value is zero.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*资本收益*、*教育年限*和*每周工作时间*对预测输出在正负方向上的贡献最大。然而，*资本收益*不影响这个特定的数据点，因为其值为零。'
- en: Global explanation
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全局解释
- en: '`lime.submodular_pick.SubmodularPick()` to pick these samples. Here are the
    parameters of this class that could help you explain your global regression or
    classification models:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `lime.submodular_pick.SubmodularPick()` 来选择这些样本。以下是此类的参数，这些参数可以帮助您解释全局回归或分类模型：
- en: '`predict_fn` (prediction function): For `ScikitClassifiers`, this is `classifier.predict_proba()`,
    while for `ScikitRegressors`, this is `regressor.predict()`'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predict_fn`（预测函数）：对于 `ScikitClassifiers`，这是 `classifier.predict_proba()`，而对于
    `ScikitRegressors`，这是 `regressor.predict()`'
- en: '`sample_size`: The number of data points to explain if `method == ''sample''`
    is chosen'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_size`：如果选择 `method == ''sample''`，则要解释的数据点的数量'
- en: '`num_exps_desired`: The number of explanation objects returned'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_exps_desired`：返回的解释对象的数量'
- en: '`num_features`: The maximum number of features present in the explanation:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_features`：解释中存在的最大特征数：'
- en: '[PRE9]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '*Figure 6**.12* shows the three data points that were picked by SP-LIME:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6**.12* 展示了SP-LIME选出的三个数据点：'
- en: '![Figure 6.12 – Data points selected by SPI-LIME for global explainability](img/B16369_06_12.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – SPI-LIME为全局可解释性选择的数据点](img/B16369_06_12.jpg)'
- en: Figure 6.12 – Data points selected by SPI-LIME for global explainability
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – SPI-LIME为全局可解释性选择的数据点
- en: But instead of visualizing the picked instances, you can use the `as_map()`
    parameter instead of `show_in_notebook()` for each explanation object, as part
    of the explanation objects in `sp_obj.explanations`, and then summarize the information
    for a bigger set of data points instead of investigating a handful of samples.
    For such analysis, you can use a small percentage of data points, such as 1% or
    lower in the case of very large datasets with tens of thousands of data points
    or more.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但您可以选择不可视化选定的实例，而是为每个解释对象使用 `as_map()` 参数而不是 `show_in_notebook()`，作为 `sp_obj.explanations`
    中的解释对象的一部分，然后为更大的数据点集总结信息，而不是调查少量样本。对于此类分析，您可以使用少量数据点，例如在具有数万个数据点的非常大的数据集中，使用1%或更低的百分比。
- en: Counterfactual generation using Diverse Counterfactual Explanations (DiCE)
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多样化的反事实解释（DiCE）生成反事实
- en: 'You can use the `dice_ml` Python library (Mothilal et al., 2020) to generate
    counterfactuals and understand how a model can switch from one prediction to another,
    as explained earlier in this chapter. First, we must train a model and then make
    an explanation object using the `dice_ml.Dice()` Python class, after installing
    and importing the `dice_ml` library, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `dice_ml` Python 库（Mothilal 等人，2020年）来生成反事实，并了解模型如何从一个预测切换到另一个预测，正如本章前面所解释的。首先，我们必须训练一个模型，然后使用
    `dice_ml.Dice()` Python 类创建一个解释对象，在安装并导入 `dice_ml` 库之后，如下所示：
- en: '[PRE10]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we can use the generated explanation object to generate counterfactuals
    for one or multiple samples. Here, we are generating 10 counterfactuals for `sample
    1`:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用生成的解释对象为单个或多个样本生成反事实。在这里，我们为 `sample 1` 生成10个反事实：
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '*Figure 6**.13* shows both the feature values of the target sample and 10 corresponding
    counterfactuals:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6**.13* 展示了目标样本的特征值以及10个相应的反事实：'
- en: '![Figure 6.13 – A selected data point and the generated counterfactuals from
    the adult income dataset](img/B16369_06_13.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – 成年人收入数据集中选定的数据点和生成的反事实](img/B16369_06_13.jpg)'
- en: Figure 6.13 – A selected data point and the generated counterfactuals from the
    adult income dataset
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 成年人收入数据集中选定的数据点和生成的反事实
- en: Although all counterfactuals meet the objective of switching the outcome of
    the target sample (that is, `sample 1`), not all counterfactuals are feasible
    according to the definition and meaning of each feature. For example, if we want
    to suggest to a 29-year-old individual that they change their outcome from low
    to high salary, suggesting that they will earn a high salary when they are 80
    years old is not an effective and actionable suggestion. Also, suggesting a change
    of *hours_per_week* of work from 38 to >90 is not feasible. You need to use such
    considerations in rejecting counterfactuals so that you can identify opportunities
    for model performance and provide actionable suggestions to users. Also, you can
    switch between different techniques to generate more meaningful counterfactuals
    for your models and applications.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管所有反事实都符合切换目标样本结果的目标（即，`样本1`），但并非所有反事实都符合每个特征的定义和意义，是可行的。例如，如果我们想建议一个29岁的人将他们的结果从低薪改为高薪，建议他们在80岁时会赚高薪并不是一个有效和可行的建议。同样，建议将*每周工作小时数*从38小时增加到>90小时也是不可行的。你需要使用这样的考虑来拒绝反事实，以便你可以识别模型性能的机会，并为用户提供可操作的建议。此外，你可以切换到不同的技术来为你的模型和应用生成更有意义反事实。
- en: There are more recent Python libraries such as `Dalex` (Baniecki et al., 2021)
    and `OmniXA` (Yang et al., 2022) that you can use for model explainability. We
    will also discuss how these methods and Python libraries can be used to decrease
    bias and help us move toward fairness in developing new or revising our already
    trained machine learning models.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有更多最近的Python库，如`Dalex`（Baniecki等人，2021年）和`OmniXA`（杨等人，2022年），你可以用于模型可解释性。我们还将讨论如何使用这些方法和Python库来减少偏差，并帮助我们朝着在新开发或修改我们已训练的机器学习模型时实现公平性迈进。
- en: Reviewing why having explainability is not enough
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾为什么仅仅拥有可解释性是不够的
- en: Explainability helps us build trust for the users of our models. As you learned
    in this chapter, you can use explainability techniques to understand how your
    models generate the outputs for one or multiple instances in a dataset. These
    explanations could help in improving our models from a performance and fairness
    perspective. However, we cannot achieve such improvements by simply using these
    techniques blindly and generating some results in Python. For example, as we discussed
    in the *Counterfactual generation using Diverse Counterfactual Explanations (DiCE)*
    section, some of the generated counterfactuals might not be reasonable and meaningful
    and we cannot rely on them. Or, when generating local explanations for one or
    multiple data points using SHAP or LIME, we need to pay attention to the meaning
    of features, the range of values for each feature and the meaning behind them,
    and the characteristics of each data point we investigate. One aspect of decision-making
    using explainability is to distinguish the issues with the model and the specific
    data points in training, testing, or production that we are investigating. A data
    point could be an outlier that makes our model less reliable for it but doesn’t
    necessarily make our model less reliable as a whole. In the next chapter, [*Chapter
    7*](B16369_07.xhtml#_idTextAnchor218), *Decreasing Bias and Achieving Fairness*,
    we will discuss that bias detection is not simply about identifying if there are
    features such as *age*, *race*, or *skin color* that our models rely on.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性帮助我们为模型用户建立信任。正如你在本章所学，你可以使用可解释性技术来理解你的模型是如何生成数据集中一个或多个实例的输出的。这些解释有助于从性能和公平性的角度改进我们的模型。然而，我们并不能仅仅通过盲目地使用这些技术并在Python中生成一些结果来实现这样的改进。例如，正如我们在*使用多样化的反事实解释（DiCE）生成反事实*部分所讨论的，一些生成的反事实可能并不合理和有意义，我们不能依赖它们。或者，当使用SHAP或LIME为单个或多个数据点生成局部解释时，我们需要注意特征的意义、每个特征值的范围及其背后的意义，以及我们调查的每个数据点的特征。使用可解释性进行决策的一个方面是区分模型和我们在训练、测试或生产中调查的特定数据点的问题。一个数据点可能是一个异常值，它使我们的模型对它来说不那么可靠，但并不一定使我们的模型整体上不那么可靠。在下一章，[*第7章*](B16369_07.xhtml#_idTextAnchor218)，*减少偏差和实现公平性*中，我们将讨论偏差检测并不仅仅是识别我们的模型依赖于诸如*年龄*、*种族*或*肤色*等特征。
- en: Altogether, these considerations tell us that running a few Python classes to
    use explainability for our models is not enough to achieve trust and generate
    meaningful explanations. There is more to it.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，这些考虑告诉我们，仅仅运行几个Python类来为我们的模型使用可解释性是不够的，以达到信任并生成有意义的解释。这还远不止于此。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about interpretable machine learning models and
    how explainability techniques could help you in improving the performance and
    reliability of your models. You learned about different local and global explainability
    techniques, such as SHAP and LIME, and practiced with them in Python. You also
    had the chance to practice with the provided Python code to learn how to use machine
    learning explainability techniques in your projects.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about the approaches to detect and decrease
    biases in your models and how you can use the available functionalities in Python
    to meet the necessary fairness criteria when developing machine learning models.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How could explainability help you improve your model’s performance?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between local and global explainability?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is it better to use linear models because of their interpretability?
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does explainability analysis make a machine learning model more reliable?
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could you explain the difference between SHAP and LIME for machine learning
    explainability?
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could you benefit from counterfactuals in developing machine learning models?
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assume a machine learning model is used for loan approval in a bank. Are all
    suggested counterfactuals useful in suggesting ways a person could improve their
    chance of getting approval?
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Weber, Leander, et al. *Beyond explaining: Opportunities and challenges of
    XAI-based model improvement*. Information Fusion (2022).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linardatos, Pantelis, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. *Explainable
    AI: A review of machine learning interpretability methods*. Entropy 23.1 (2020):
    18.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gilpin, Leilani H., et al. *Explaining explanations: An overview of interpretability
    of machine learning*. 2018 IEEE 5th International Conference on data science and
    advanced analytics (DSAA). IEEE, 2018.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carvalho, Diogo V., Eduardo M. Pereira, and Jaime S. Cardoso. *Machine learning
    interpretability: A survey on methods and metrics*. Electronics 8.8 (2019): 832.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Winter, Eyal. *The Shapley value*. Handbook of game theory with economic applications
    3 (2002): 2025-2054.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Guide to Explainable AI Using* *Python*: [https://www.thepythoncode.com/article/explainable-ai-model-python](https://www.thepythoncode.com/article/explainable-ai-model-python)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Burkart, Nadia, and Marco F. Huber. *A survey on the explainability of supervised
    machine learning*. Journal of Artificial Intelligence Research 70 (2021): 245-317.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guidotti, Riccardo. *Counterfactual explanations and how to find them: literature
    review and benchmarking*. Data Mining and Knowledge Discovery (2022): 1-55.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. *Anchors: High-precision
    model-agnostic explanations*. Proceedings of the AAAI conference on artificial
    intelligence. Vol. 32\. No. 1\. 2018.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. *Distilling the knowledge in
    a neural network*. arXiv preprint arXiv:1503.02531 (2015).
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. *Deep inside convolutional
    networks: Visualising image classification models and saliency maps*. arXiv preprint
    arXiv:1312.6034 (2013).'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman. *卷积网络内部深探：可视化图像分类模型和显著性图*.
    arXiv预印本 arXiv:1312.6034 (2013)。
- en: Frosst, Nicholas, and Geoffrey Hinton. *Distilling a neural network into a soft
    decision tree*. arXiv preprint arXiv:1711.09784 (2017).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frosst, Nicholas, and Geoffrey Hinton. *将神经网络蒸馏成软决策树*. arXiv预印本 arXiv:1711.09784
    (2017)。
- en: Lundberg, Scott M., and Su-In Lee. *A unified approach to interpreting model
    predictions*. Advances in neural information processing systems 30 (2017).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lundberg, Scott M., and Su-In Lee. *解释模型预测的统一方法*. 神经信息处理系统进展第30卷 (2017)。
- en: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. *“Why should I trust
    you?” Explaining the predictions of any classifier*. Proceedings of the 22nd ACM
    SIGKDD international conference on knowledge discovery and data mining. 2016.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. *“为什么我应该相信你？”解释任何分类器的预测*.
    第22届ACM SIGKDD国际知识发现和数据挖掘会议论文集，2016年。
- en: 'Baniecki, Hubert, et al. *Dalex: responsible machine learning with interactive
    explainability and fairness in Python*. The Journal of Machine Learning Research
    22.1 (2021): 9759-9765.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Baniecki, Hubert, et al. *Dalex: 在Python中实现具有交互式可解释性和公平性的负责任机器学习*. 机器学习研究杂志第22卷第1期
    (2021): 9759-9765。'
- en: 'Yang, Wenzhuo, et al. *OmniXAI: A Library for Explainable AI*. arXiv preprint
    arXiv:2206.01612 (2022).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang, Wenzhuo, et al. *OmniXAI: 一个可解释人工智能库*. arXiv预印本 arXiv:2206.01612 (2022)。'
- en: Hima Lakkaraju, Julius Adebayo, Sameer Singh, *AAAI 2021 Tutorial on Explaining
    Machine* *Learning Predictions*.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hima Lakkaraju, Julius Adebayo, Sameer Singh, *AAAI 2021教程：解释机器学习预测*。
- en: Mothilal, Ramaravind K., Amit Sharma, and Chenhao Tan. *Explaining machine learning
    classifiers through diverse counterfactual explanations*. Proceedings of the 2020
    conference on fairness, accountability, and transparency. 2020.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mothilal, Ramaravind K., Amit Sharma, and Chenhao Tan. *通过多样化的反事实解释解释机器学习分类器*.
    2020年公平、问责和透明度会议论文集。
