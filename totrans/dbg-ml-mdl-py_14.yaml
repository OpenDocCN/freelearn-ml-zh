- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Recent Advancements in Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Supervised learning was the focus of the majority of successful applications
    of machine learning across different industries and application domains until
    2020\. However, other techniques, such as generative modeling, later caught the
    attention of developers and users of machine learning. So, an understanding of
    such techniques will help you to broaden your understanding of machine learning
    capabilities beyond supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Generative modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about the meaning, widely
    used techniques, and benefits of generative modeling, **reinforcement learning**
    (**RL**), and **self-supervised learning** (**SSL**). You will also practice some
    of these techniques using Python and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements are applicable to this chapter as they will help
    you better understand the concepts, be able to use them in your projects, and
    practice with the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch` >= 2.0.0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torchvision` >= 0.15.1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` >= 3.7.1'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter14](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter14).
  prefs: []
  type: TYPE_NORMAL
- en: Generative modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative modeling, or more generally Generative AI, provides you with the
    opportunity to generate data that is close to an expected or reference set of
    data points or distributions, commonly referred to as realistic data. One of the
    most successful applications of generative modeling has been in language modeling.
    The success story of **Generative Pre-trained Transformer** (**GPT**)-4 and ChatGPT
    ([https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)), a chatbot
    built on top of GPT-4 and GPT-3.5, and similar tools such as Perplexity ([https://www.perplexity.ai/](https://www.perplexity.ai/)),
    resulted in the rise in interest among engineers, scientists, people in different
    businesses such as finance and healthcare, and many other job roles in generative
    modeling. When using Chat-GPT or GPT-4, you can ask a question or provide the
    description of an ask, called a prompt, and then these tools generate a series
    of statements or data to provide you with the answer, information, or text you
    asked for.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the successful application of generative modeling in text generation,
    many other applications of generative modeling can help you in your work or studies.
    For example, GPT-4 and its previous versions or other similar models, such as
    LLaMA (Touvron et al., 2023), can be used for code generation and completion ([https://github.com/features/copilot/](https://github.com/features/copilot/)
    and [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)).
    You can write the code you are interested in generating and it generates the corresponding
    code for you. Although the generated code might not work as expected all the time,
    it is usually close to what is expected, at least after a couple of trials.
  prefs: []
  type: TYPE_NORMAL
- en: There have also been many other successful applications of generative modeling,
    such as in image generation ([https://openai.com/product/dall-e-2](https://openai.com/product/dall-e-2)),
    drug discovery (Cheng et al., 2021), fashion design (Davis et al., 2023), manufacturing
    (Zhao et al., 2023), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Beginning in 2023, many traditional commercial tools and services started integrating
    Generative AI capabilities. For example, you can now edit photos using Generative
    AI in Adobe Photoshop simply by explaining what you need in plain English ([https://www.adobe.com/ca/products/photoshop/generative-fill.html](https://www.adobe.com/ca/products/photoshop/generative-fill.html)).
    WolframAlpha also combined its power of symbolic computation with Generative AI,
    which you can use to ask for specific symbolic processes in plain English ([https://www.wolframalpha.com/input?i=Generative+Adversarial+Networks](https://www.wolframalpha.com/input?i=Generative+Adversarial+Networks)).
    Khan Academy ([https://www.khanacademy.org/](https://www.khanacademy.org/)) designed
    a strategy to help teachers and students benefit from Generative AI, specifically
    ChatGPT, instead of it being harmful to the education of students.
  prefs: []
  type: TYPE_NORMAL
- en: These success stories have been achieved by relying on different deep learning
    techniques designed for generative modeling, which we will briefly review next.
  prefs: []
  type: TYPE_NORMAL
- en: Generative deep learning techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are multiple generative modeling approaches with available the APIs available
    in PyTorch or other deep learning frameworks, such as TensorFlow. Here, we will
    review some of them to help you start learning more about how they work and how
    you can use them in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer-based text generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You already learned that transformers, introduced in 2017 (Vaswani et al., 2017),
    are used to generate the most successful recent language models in [*Chapter 13*](B16369_13.xhtml#_idTextAnchor342),
    *Advanced Deep Learning Techniques*. However, these models are not useful only
    for tasks such as translation, which is traditional in natural language processing,
    but can be used in generative modeling to help us generate meaningful text, for
    example, in response to a question we ask. This is the approach behind GPT models,
    Chat-GPT, and many other generative language models. The process of providing
    a short text, as an ask or a question, is also called prompting, in which we need
    to provide a good prompt to get a good answer. We will talk about optimal prompting
    in the *Prompt engineering for text-based generative* *models* section.
  prefs: []
  type: TYPE_NORMAL
- en: Variational autoencoders (VAEs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Autoencoders are techniques with which you can reduce the number of features
    to an information-rich set of embeddings, which you can consider a more complicated
    version of **principal component analysis** (**PCA**) to better understand it.
    It does that by first attempting to encode the original space to the new embedding
    (called encoding), then decode the embeddings, and regenerate the original features
    for each data point (called decoding). In a VAE (Kingma and Welling, 2013), instead
    of one set of features (embeddings), it generates a distribution for each new
    feature. For example, instead of reducing the original 1,000 features to 100 features,
    each having one float value, you get 100 new variables, each being a normal (or
    Gaussian) distribution. The beauty of this process is that then you can select
    different values from these distributions for each variable and generate a new
    set of 100 embeddings. In the process of decoding them, these embeddings get decoded
    and a new set of features with the original size (1,000) gets generated. This
    process can be used for different types of data such as images (Vahdat et al.,
    2020) and graphs (Simonovsky et al., 2018; Wengong et al., 2018). You can find
    a collection of VAEs implemented in PyTorch at [https://github.com/AntixK/PyTorch-VAE](https://github.com/AntixK/PyTorch-VAE).
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks (GANs)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this technique introduced in 2014 (Goodfellow et al., 2020), a discriminator
    that works like a supervised classification model and a generator work alongside
    each other. The generator, which could be a neural network architecture for generating
    the desired data types, such as images, generates images aiming to fool the discriminator
    into recognizing the generated data as real data. The discriminator learns to
    remain good at distinguishing generated data from real data. The generated data
    in some cases is called fake data, as in technologies and models such as deepfakes
    ([https://www.businessinsider.com/guides/tech/what-is-deepfake](https://www.businessinsider.com/guides/tech/what-is-deepfake)).
    However, the generated data can be used as opportunities for new data points to
    be used in different applications, such as drug discovery (Prykhodko et al., 2019).
    You can use `torchgan` to implement GANs ([https://torchgan.readthedocs.io/en/latest/](https://torchgan.readthedocs.io/en/latest/)).
  prefs: []
  type: TYPE_NORMAL
- en: As there has been an emerging generation of prompt-based technologies built
    on top of generative models, we will provide a better understanding of how to
    optimally design prompts next.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering for text-based generative models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt engineering is not only a recent topic in machine learning but has also
    become a highly paid job title. In prompt engineering, we aim to provide optimal
    prompts to generate the best possible result (for example, text, code, and images)
    and identify issues with the generative models as opportunities for improving
    them. A basic understanding of large language and generative models, your language
    proficiency, and domain knowledge for domain-specific data generation can help
    you in better prompting. There are free resources that you can use to learn about
    prompt engineering, such as a course by Andrew Ng and OpenAI ([https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/))
    and some introductory content about prompt engineering released by Microsoft ([https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/prompt-engineering](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/concepts/prompt-engineering)).
    However, we will not leave you to learn this topic from scratch by yourself. We
    will provide you with some guidance for optimal prompting here that will help
    you improve your prompting skills.
  prefs: []
  type: TYPE_NORMAL
- en: Targeted prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our daily conversations, either at work, university, or home, there are
    ways we try to make sure the person across from us better understands what we
    mean, and as a result, we get a better response. For example, if you tell your
    friend, “Give me that” instead of “Give me that bottle of water on the desk,”
    there is a chance that your friend won’t give you the bottle of water or get confused
    about what exactly you are referring to. In prompting, you can get better responses
    and data generated, such as images, if you clearly explain what you want for a
    very specific task. Here are a few techniques to use for better prompting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Be specific about the ask**: You can provide specific information such as
    the format of the data you would like to be generated, such as bullet points or
    code, and the task you are referring to, such as writing an email versus a business
    plan.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specify who the data is getting generated for**: You can even specify an
    expertise or job title for whom the data is getting generated, such as generating
    a piece of text for a machine learning engineer, business manager, or software
    developer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specify time**: You can specify whether you want information about the date
    when technology got released, the first time something was announced, the chronological
    order of events, the change in something such as the net worth of a famous rich
    person such as Elon Musk over time, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Simplify the concepts**: You can provide a simplified version of what you
    ask to make sure the model doesn’t get confused by the complexity of your prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although these techniques will help you in better prompting, there is still
    a chance of getting false answers with high confidence if you ask for a text response
    or unrelated data generation. This is what is usually referred to as a hallucination.
    One of the ways to decrease the chance of irrelevant or wrong responses or data
    generation is to provide tests for the model to use. When we write functions and
    classes in Python, we can design unit tests to make sure their output meets the
    expectation, as discussed in [*Chapter 8*](B16369_08.xhtml#_idTextAnchor243),
    *Controlling Risks Using* *Test-Driven Development*.
  prefs: []
  type: TYPE_NORMAL
- en: Generative modeling using PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can develop generative models based on different techniques discussed earlier
    in this chapter using PyTorch. We want to practice with VAEs here. With VAEs,
    the aim is to identify a probability distribution for a lower-dimensional representation
    of data. For example, the model learns about the mean and variance (or log variance)
    for the representations of the input parameters, assuming normal or Gaussian distribution
    for the latent space (that is, the space of the latent variables or representations).
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import the required libraries and modules and load the `Flowers102`
    dataset from PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define a class for the VAE as follows in which two linear layers are
    defined to encode the input pixels of images. Then, the mean and variance of the
    probability distribution of latent space are also defined by two linear layers
    for decoding the latent variables back to the original number of inputs to generate
    images similar to the input data. The learned mean and variance of the distribution
    in latent space will be then used to generate new latent variables and potentially
    generate new data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We now initialize the defined `VAE` class and determine the `Adam` optimizer
    as the optimization algorithm with `0.002` as the learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define a loss function using `binary_cross_entropy` as follows to compare
    the regenerated pixels with the input pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to train the model using the `Flowers102` dataset we loaded
    before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We can then use this trained model to generate images that almost look like
    flowers (see *Figure 14**.1*). Upon hyperparameter optimization, such as changing
    the model's architecture, you can achieve better results. You can review hyperparameter
    optimization in deep learning in [*Chapter 12*](B16369_12.xhtml#_idTextAnchor320),
    *Going Beyond ML Debugging with* *Deep Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Example images generated by the simple VAE we developed earlier](img/B16369_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Example images generated by the simple VAE we developed earlier
  prefs: []
  type: TYPE_NORMAL
- en: This was a simple example of generative modeling using PyTorch. In spite of
    the success of generative modeling, part of the recent success of tools developed
    using generative models, such as Chat-GPT, is due to the smart use of reinforcement
    learning, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) is not a new idea or technique. The initial
    idea dates back to the 1950s, when it was introduced by Richard Bellman with the
    concept of the Bellman equation (Sutton and Barto, 2018). However, its recent
    combination with human feedback, which we will explain in the next section, provided
    a new opportunity for its utility in developing machine learning technologies.
    The general idea of RL is to learn by experience, or interaction with a specified
    environment, instead of using a collected set of data points for training, as
    in supervised learning. An agent is considered in RL, which learns how to improve
    actions to get a greater reward (Kaelbling et al., 1996). The agent learns to
    improve its approach to taking action, or policy in more technical terminology,
    iteratively after receiving the reward of the action taken in the previous step.'
  prefs: []
  type: TYPE_NORMAL
- en: In the history of RL, two important developments and utilities resulted in an
    increase in its popularity including the development of Q-learning (Watkins, 1989)
    and combining RL and deep learning (Mnih et al., 2013) using Q-learning. In spite
    of the success stories behind RL and the intuition that it mimics learning by
    experience as humans do, it has been shown that deep reinforcement learning is
    not data efficient and requires large amounts of data or iterative experience,
    which makes it fundamentally different from human learning (Botvinick et al.,
    2019).
  prefs: []
  type: TYPE_NORMAL
- en: More recently, **reinforcement learning with human feedback** (**RLHF**) was
    used as a successful application of reinforcement learning to improve the results
    of generative models, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning with human feedback (RLHF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With reinforcement learning with human feedback, the reward is calculated based
    on the feedback of humans, either experts or non-experts, depending on the problem.
    However, the reward is not like a predefined mathematical formula considering
    the complexity of the problems such as language modeling. The feedback provided
    by humans results in improving the model step by step. For example, the training
    process of a RLHF language model can be summarized as follows ([https://huggingface.co/blog/rlhf](https://huggingface.co/blog/rlhf)):'
  prefs: []
  type: TYPE_NORMAL
- en: Training a language model, which is referred to as pretraining.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data collection and training the reward model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning the language model with reinforcement learning using the reward
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, learning how to use PyTorch to design RLHF-based models could be helpful
    to better understand this concept.
  prefs: []
  type: TYPE_NORMAL
- en: RLHF with PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the major challenges in benefitting from RLHF is designing an infrastructure
    for human feedback collection and curation, then providing them to calculate the
    reward, and then improving the main pre-trained model. Here, we don’t want to
    get into that aspect of RLHF but rather go through a simple code example to understand
    how such feedback can be incorporated into a machine learning model. There are
    good resources, such as [https://github.com/lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch),
    that can help you to improve your understanding of RLHF and how to implement it
    using Python and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will use GPT-2 ([https://huggingface.co/transformers/v1.2.0/_modules/pytorch_transformers/modeling_gpt2.html](https://huggingface.co/transformers/v1.2.0/_modules/pytorch_transformers/modeling_gpt2.html))
    as the pre-trained model. First, we import the necessary libraries and modules
    and initialize the model, tokenizer, and optimizer, which is chosen to be `Adam`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, assuming we collected the human feedback and formatted it properly, we
    can use it to create a DataLoader from PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to design a reward model, for which we use a two-layer fully
    connected neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then initialize the reward model using the previously defined class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to improve our pre-trained model using the collected human
    feedback and the reward model. If you pay attention to the following code, the
    main difference between this simple loop over epochs and batches for model training
    compared to neural networks without a reward model is the reward calculation and
    then using it for loss calculation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This was a very simple example of designing RLHF-based model improvement, used
    to help you better understand the concept. Resources such as [https://github.com/lucidrains/PaLM-rlhf-pytorch](https://github.com/lucidrains/PaLM-rlhf-pytorch)
    will help you to implement more complex ways of incorporating such human feedback
    for improving your models.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s go through another interesting topic in machine learning, called
    self-supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning (SSL)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Self-supervised learning** (**SSL**) is not a new concept. It''s similar
    to RL, but it gained attention after its combination with deep learning due to
    its effectiveness in learning data representations. Examples of such models are
    Word2vec for language modeling (Mikolov et al., 2013) and Meta’s RoBERTa models
    trained using SSL, which achieved state-of-the-art performance on several language
    modeling tasks. The idea of SSL is to define an objective for the machine learning
    model that doesn’t rely on pre-labeling or the quantification of data points –
    for example, predicting the positions of objects or people in videos for each
    time step using previous time steps, masking parts of images or sequence data,
    and aiming to refill those masked sections. One of the widely used applications
    of such models is in RL to learn representations of images and text, and then
    use those representations in other contexts, for example, in supervised modeling
    of smaller datasets with data labels (Kolesnikov et al., 2019, Wang et al., 2020).'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple techniques under the umbrella of SSL, three of which are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contrastive learning**: The idea of contrastive learning is to learn representations
    that result in similar data points being closer to each other compared to dissimilar
    data points (Jaiswal et al., 2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoregressive models**: In autoregressive modeling, the model aims to predict
    the next data points, either based on time or a specific sequence order, given
    the previous ones. This is a very popular technique in language modeling, where
    models such as GPT predict the next word in a sentence (Radford et al., 2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-supervision via inpainting**: In this approach, we mask parts of the
    data and train the models to fill in the missing parts. For example, a portion
    of an image might be masked, and the model is trained to predict the masked portion.
    Masked autoencoder is an example of such a technique in which the masked portions
    of images are refilled in the decoding process of the autoencoder (Zhang et al.,
    2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will practice with a simple example of self-supervised modeling using
    Python and PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a programming perspective, the main difference between deep learning for
    SSL compared to supervised learning is in defining the objectives and data for
    training and testing. Here, we want to practice with `Flowers102` dataset we used
    to practice with RLHF.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first define the neural network class using two encoding and decoding `torch.nn.Conv2d()`
    layers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We then initialize the model, specify `torch.nn.MSELoss()` as the criterion
    for comparison of predicted and true images, and `torch.optim.Adam()` as the optimizer
    with a learning rate of `0.001`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following function helps us to implement masking on random 8x8 portions
    of each image, which then the autoencoder learns to fill:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we train the model for 200 epochs as follows. As you can see in *Figure
    14**.2*, the images first get masked, and then in the decoding step, the autoencoder
    attempts to rebuild the full image, including the masked portions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the examples of the resulting refilled images shown in *Figure
    14**.2*, the model could find the patterns correctly. However, with proper hyperparameter
    optimization and designing models with better neural network architectures, you
    can achieve higher performance and better models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Example images (first row), their masked versions (second row),
    and regenerated versions (third row) using the convolutional autoencoder model](img/B16369_14_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – Example images (first row), their masked versions (second row),
    and regenerated versions (third row) using the convolutional autoencoder model
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about SSL and the other techniques provided in this chapter
    using the provided resources and references to better understand these concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you gained a high-level understanding of recent advancements
    in machine learning modeling beyond supervised learning, including generative
    modeling, reinforcement learning, and self-supervised learning. You also learned
    about optimal prompting and prompt engineering to benefit from tools and applications
    built on top of generative models that accept text prompts as input from users.
    You were provided with the relevant code repositories and functionalities available
    in Python and PyTorch that will help you to start learning more about these advanced
    techniques. This knowledge helps you not only better understand how they work
    if you come across them but also start building models of your own using these
    advanced techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about the benefits of identifying causal
    relationships in machine learning modeling and practice with Python libraries
    that help you in implementing causal modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are examples of generative deep learning techniques?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are examples of generative text models that use transformers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are generators and discriminators in GANs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the techniques you can use for better prompting?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Could you explain how RL could be helpful in importing the results of generative
    models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Briefly explain contrastive learning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Cheng, Yu, et al. “*Molecular design in drug discovery: a comprehensive review
    of deep generative models*.” *Briefings in bioinformatics* 22.6 (2021): bbab344.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Davis, Richard Lee, et al. “*Fashioning the Future: Unlocking the Creative
    Potential of Deep Generative Models for Design Space Exploration*.” *Extended
    Abstracts of the 2023 CHI Conference on Human Factors in Computing* *Systems*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao, Yaoyao Fiona, et al., eds. “*Design for Advanced Manufacturing*.” *Journal
    of Mechanical Design* 145.1 (2023): 010301.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron, Hugo, et al. “*Llama: Open and efficient foundation language models*.”
    arXiv preprint arXiv:2302.13971 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani, Ashish, et al. “*Attention is all you need*.” *Advances in neural information
    processing systems* 30 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma, Diederik P., and Max Welling. “*Auto-encoding variational bayes*.” arXiv
    preprint arXiv:1312.6114 (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vahdat, Arash, and Jan Kautz. “*NVAE: A deep hierarchical variational autoencoder*.”
    *Advances in neural information processing systems* 33 (2020): 19667-19679.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonovsky, Martin, and Nikos Komodakis. “*Graphvae: Towards generation of
    small graphs using variational autoencoders*.” *Artificial Neural Networks and
    Machine Learning–ICANN 2018: 27th International Conference on Artificial Neural
    Networks*, Rhodes, Greece, October 4-7, 2018, Proceedings, Part I 27\. Springer
    International Publishing (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jin, Wengong, Regina Barzilay, and Tommi Jaakkola. “*Junction tree variational
    autoencoder for molecular graph generation*.” *International conference on machine
    learning*. PMLR (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow, Ian, et al. “*Generative adversarial networks*.” *Communications
    of the ACM* 63.11 (2020): 139-144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras, Tero, Samuli Laine, and Timo Aila. “*A style-based generator architecture
    for generative adversarial networks*.” *Proceedings of the IEEE/CVF conference
    on computer vision and pattern* *recognition* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prykhodko, Oleksii, et al. “*A de novo molecular generation method using latent
    vector based generative adversarial network*.” *Journal of Cheminformatics* 11.1
    (2019): 1-13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton, Richard S., and Andrew G. Barto. *Reinforcement learning: An introduction*.
    MIT Press (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kaelbling, Leslie Pack, Michael L. Littman, and Andrew W. Moore. “*Reinforcement
    learning: A survey*.” *Journal of artificial intelligence research* 4 (1996):
    237-285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watkins, Christopher John Cornish Hellaby. *Learning from delayed* *rewards*.
    (1989).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih, Volodymyr, et al. “*Playing atari with deep reinforcement learning*.”
    arXiv preprint arXiv:1312.5602 (2013).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Botvinick, Matthew, et al. “*Reinforcement learning, fast and slow*.” *Trends
    in cognitive sciences* 23.5 (2019): 408-422.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kolesnikov, Alexander, Xiaohua Zhai, and Lucas Beyer. “*Revisiting self-supervised
    visual representation learning*.” *Proceedings of the IEEE/CVF conference on computer
    vision and pattern* *recognition* (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang, Jiangliu, Jianbo Jiao, and Yun-Hui Liu. “*Self-supervised video representation
    learning by pace prediction*.” *Computer Vision–ECCV 2020: 16th European Conference*,
    Glasgow, UK, August 23–28, 2020, Proceedings, Part XVII 16\. Springer International
    Publishing (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaiswal, Ashish, et al. “*A survey on contrastive self-supervised learning*.”
    *Technologies* 9.1 (2020): 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Radford, Alec, et al. “*Language models are unsupervised multitask learners*.”
    OpenAI blog 1.8 (2019): 9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang, Chaoning, et al. “*A survey on masked autoencoder for self-supervised
    learning in vision and beyond*.” arXiv preprint arXiv:2208.00173 (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 5:Advanced Topics in Model Debugging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the concluding part of this book, we will address some of the most pivotal
    topics in machine learning. We will begin by explaining differences between correlation
    and causality, shedding light on their distinct implications in model development.
    Transitioning to the topic of security and privacy, we will discuss the pressing
    concerns, challenges, and techniques that ensure our models are both robust and
    respectful of user data. We will wrap up the book with an explanation of human-in-the
    -loop machine learning, emphasizing the synergy between human expertise and automated
    systems, and how this collaboration paves the way for more effective solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B16369_15.xhtml#_idTextAnchor406), *Correlation versus Causality*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B16369_16.xhtml#_idTextAnchor429), *Security and Privacy in
    Machine Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B16369_17.xhtml#_idTextAnchor447), *Human-in-the-Loop Machine
    Learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
