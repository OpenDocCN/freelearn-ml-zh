- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Diving Deeper – Preparing and Processing Data for AI/ML Workloads on Google
    Cloud
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨 – 在Google Cloud上为AI/ML工作负载准备和处理数据
- en: 'In the previous chapter, we did some very rudimentary data exploration by looking
    at a few details relating to our dataset, using functions such as `pandas.DataFrame.info()`
    and `pandas.DataFrame.head()`. In this chapter, we will dive deeper into the realm
    of data exploration and preparation for data science workloads, as represented
    by the section highlighted in blue in the data science life-cycle diagram shown
    in *Figure 6**.1*:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们通过查看与我们的数据集相关的一些细节，使用诸如`pandas.DataFrame.info()`和`pandas.DataFrame.head()`等函数，进行了一些非常基础的数据探索。在本章中，我们将更深入地探讨数据探索和准备领域，这在本章中由数据科学生命周期图中用蓝色突出显示的部分所代表。1*：
- en: '![Figure 6.1: Data exploration and processing](img/B18143_06_1.jpg)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1：数据探索和处理](img/B18143_06_1.jpg)'
- en: 'Figure 6.1: Data exploration and processing'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：数据探索和处理
- en: In the early stages of a typical data science project, you would likely perform
    many of the data exploration and preparation steps in Jupyter notebooks, which,
    as we have seen, are useful for experimenting with small datasets. When you bring
    your workload into production, however, you are likely to use much larger datasets,
    in which case you would usually need to use different tools for processing your
    data. Google is seen as an industry leader when it comes to large-scale data processing,
    analytics, and AI/ML. For example, Google Cloud BigQuery is well established as
    one of the most popular data warehouse services in the industry, and Google Cloud
    has many additional industry-leading services for data processing and analytics
    workloads.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型数据科学项目的早期阶段，您可能会在Jupyter笔记本中执行许多数据探索和准备步骤，正如我们所看到的，这对于实验小数据集是有用的。然而，当您将工作负载投入生产时，您可能会使用更大的数据集，在这种情况下，您通常会需要使用不同的工具来处理您的数据。在大型规模数据处理、分析和AI/ML方面，Google被视为行业领导者。例如，Google
    Cloud BigQuery已成为行业中最受欢迎的数据仓库服务之一，Google Cloud还拥有许多其他行业领先的数据处理和分析工作负载服务。
- en: In this chapter, we will learn how to explore, visualize, and prepare data for
    ML use cases with tools such as Vertex AI, BigQuery, Dataproc, and Cloud Composer.
    Furthermore, we will dive into processing streaming data on the fly with Dataflow,
    and introduce the fundamentals of data pipelines. By the end of this chapter,
    you will be able to create, build, and run data pipelines on Google Cloud, equipping
    you with the necessary skills to take on complex data processing tasks in today’s
    fast-paced, data-driven world.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用Vertex AI、BigQuery、Dataproc和Cloud Composer等工具探索、可视化和准备数据以供ML用例使用。此外，我们将深入了解实时处理流数据的Dataflow，并介绍数据管道的基本原理。到本章结束时，您将能够创建、构建和运行Google
    Cloud上的数据管道，为您在当今快节奏、数据驱动的世界中承担复杂数据处理任务提供必要的技能。
- en: 'This chapter covers the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了以下主题：
- en: Prerequisites and basic concepts
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 先决条件和基本概念
- en: Ingesting data into Google Cloud
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据导入Google Cloud
- en: Exploring and visualizing data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和可视化数据
- en: Cleaning and preparing the data for ML workloads
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗和准备数据以供ML工作负载使用
- en: An introduction to data pipelines
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管道简介
- en: Processing batch and streaming data
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理批量和流数据
- en: Building and running data pipelines on Google Cloud
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Google Cloud上构建和运行数据管道
- en: Let’s begin by discussing the prerequisites for this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来讨论本章的先决条件。
- en: Prerequisites for this chapter
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章的先决条件
- en: The activities in this section need to be completed before we can start to perform
    the primary activities in this chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以开始执行本章的主要活动之前，本节中的活动需要完成。
- en: Enabling APIs
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用API
- en: 'In addition to the methods we discussed in earlier chapters to enable Google
    Cloud APIs, such as the Google Cloud Shell or being prompted in the Google Cloud
    console, you can also proactively search for an API in order to enable it in the
    console. To do that, you would perform the following steps, in which *[service/API
    name]* is the name of the Service/API you wish to enable:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在前几章中讨论的用于启用Google Cloud API的方法，例如Google Cloud Shell或Google Cloud控制台中被提示，您还可以主动搜索一个API以在控制台中启用它。为此，您需要执行以下步骤，其中*[服务/API名称]*是您希望启用的服务/API的名称：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **APIs & Services** → **Library**.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单 → **APIs & Services** → **Library**。
- en: Search for *[service name]* in the search box.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索框中搜索*[服务名称]*。
- en: Select the API from the list of results.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从结果列表中选择API。
- en: On the page that displays information about the API, click **Enable**.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在显示API信息的页面上，点击**启用**。
- en: 'Perform the preceding steps for each of the following service/API names:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为以下每个服务/API名称执行前面的步骤：
- en: Compute Engine API
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Compute Engine API
- en: Cloud Scheduler API
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Scheduler API
- en: Dataflow API
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dataflow API
- en: Data Pipelines API
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据管道API
- en: Cloud Dataproc API
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Dataproc API
- en: Cloud Composer API
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Composer API
- en: Cloud Pub/Sub API
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Pub/Sub API
- en: After you have enabled the required APIs, we’re ready to move on to the next
    section.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在您启用所需的API之后，我们就可以继续下一节了。
- en: IAM permissions
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IAM权限
- en: In this section, we will set up the **identity and access management** (**IAM**)
    permissions required to enable the activities that come later in this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将设置启用本章后续活动所需的**身份和访问管理**（**IAM**）权限。
- en: Service accounts
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务帐户
- en: In previous chapters, I mentioned that there are multiple ways to authenticate
    with Google Cloud services and that the API keys you used in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146)
    were the simplest authentication methods. Now that we’re advancing to more complex
    use cases, we will begin to use a more advanced form of authentication, referred
    to as service accounts.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我提到过有多种方式可以与Google Cloud服务进行身份验证，您在[*第4章*](B18143_04.xhtml#_idTextAnchor146)中使用的API密钥是最简单的身份验证方法。现在，随着我们向更复杂的使用案例迈进，我们将开始使用一种更高级的身份验证形式，称为服务帐户。
- en: Google Cloud service accounts are special accounts used by Google Cloud services,
    applications, and **virtual machines** (**VMs**) to interact with and authenticate
    to other Google Cloud resources. They are an interesting concept because, in addition
    to being resources, they are also considered to be identities or principals, just
    like people, and just like people, they have their own email addresses and permissions
    associated with them. However, these email addresses and permissions apply to
    the machines and applications that use them, rather than to people. Service accounts
    provide a way for machines and applications to have an identity when those systems
    want to perform an activity that requires authentication with Google Cloud APIs
    and resources.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud服务帐户是Google Cloud服务、应用程序和**虚拟机**（**VM**）用于与其他Google Cloud资源交互和验证的特殊帐户。它们是一个有趣的概念，因为除了是资源外，它们还被认为是有身份或主体，就像人一样，就像人一样，它们有自己的电子邮件地址和与它们相关的权限。然而，这些电子邮件地址和权限适用于使用它们的机器和应用程序，而不是人。服务帐户提供了一种方式，让机器和应用程序在系统想要执行需要通过Google
    Cloud API和资源进行身份验证的活动时拥有身份。
- en: We’re going to create a service account with the required permissions for the
    activities we will perform in this chapter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个服务帐户，用于在本章中执行的活动所需的权限。
- en: Data processing service account
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据处理服务帐户
- en: Considering that we’re going to use multiple Google Cloud services to process
    data in various ways in this chapter, we will create a service account with permissions
    mainly related to Google Cloud data processing services, such as BigQuery, Cloud
    Composer, Dataflow, Dataproc, **Google Cloud Storage** (**GCS**), and Pub/Sub.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到在本章中我们将使用多个Google Cloud服务以不同的方式处理数据，我们将创建一个具有主要与Google Cloud数据处理服务相关的权限的服务帐户，例如BigQuery、Cloud
    Composer、Dataflow、Dataproc、**Google Cloud Storage**（**GCS**）和Pub/Sub。
- en: 'Perform the following steps to create the required service account:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以创建所需的服务帐户：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **IAM & Admin** → **Service accounts**.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单→ **IAM & Admin** → **服务帐户**。
- en: Select **Create** **service account**.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**创建** **服务帐户**。
- en: For the service account name, enter `data-processing-sa`.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于服务帐户名称，输入`data-processing-sa`。
- en: 'In the section titled **Grant this service account access to project**, add
    the roles shown in *Figure 6**.2*:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在标题为**授予此服务帐户访问项目**的章节中，添加*图6**.2中显示的角色：
- en: '![Figure 6.2: Dataflow worker service account permissions](img/B18143_06_2.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2：Dataflow工作服务帐户权限](img/B18143_06_2.jpg)'
- en: 'Figure 6.2: Dataflow worker service account permissions'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2：Dataflow工作服务帐户权限
- en: Select **Done**.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**完成**。
- en: Our service account is now ready to be used later in this chapter.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的服务帐户现在已准备好在本章的后续部分使用。
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'In the service account we just created, we also added the **Service Account
    User** permission. This is because, in some of the use cases we will implement,
    our service account will also need to temporarily impersonate or “act as” other
    service accounts or identities. For more information on the concept of impersonating
    service accounts, see the following documentation: [https://cloud.google.com/iam/docs/service-account-permissions#directly-impersonate](https://cloud.google.com/iam/docs/service-account-permissions#directly-impersonate).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚创建的服务账户中，我们还添加了**服务账户用户**权限。这是因为，在我们将要实施的某些用例中，我们的服务账户也需要暂时代表或“充当”其他服务账户或身份。有关代表服务账户的概念的更多信息，请参阅以下文档：[https://cloud.google.com/iam/docs/service-account-permissions#directly-impersonate](https://cloud.google.com/iam/docs/service-account-permissions#directly-impersonate)。
- en: Cloud Storage bucket folders
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云存储桶文件夹
- en: 'We will use a Cloud Storage bucket to store data for the activities later in
    this chapter. We already created a bucket in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146),
    so we can simply add some folders to the bucket for storing our data. Perform
    the following steps to create the folders:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用云存储桶来存储本章后面活动所需的数据。我们已经在[*第4章*](B18143_04.xhtml#_idTextAnchor146)中创建了一个桶，因此我们可以简单地向桶中添加一些文件夹来存储我们的数据。执行以下步骤以创建文件夹：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Cloud Storage** → **Buckets**.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单→**云存储**→**桶**。
- en: Click on the name of the bucket you created in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击[*第4章*](B18143_04.xhtml#_idTextAnchor146)中创建的桶的名称。
- en: Select **Create folder**.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**创建文件夹**。
- en: Name it `data`.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其命名为`data`。
- en: Select **Create**.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**创建**。
- en: 'Repeat the preceding steps with the following additional folder names:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 重复前面的步骤，使用以下额外的文件夹名称：
- en: '`code`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`code`'
- en: '`dataflow`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dataflow`'
- en: '`pyspark-airbnb`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pyspark-airbnb`'
- en: Our folders are now ready to store our data.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据文件夹现在已准备好存储数据。
- en: Uploading data
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上传数据
- en: We will use a file named `AB_NYC_2019.csv` as a dataset for some of the activities
    in this chapter. In the clone of our GitHub repository that you created on your
    local machine in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146), you will find
    that file in a directory named `data`, which exists within the directory named
    `Chapter-06`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用名为`AB_NYC_2019.csv`的文件作为本章某些活动的数据集。在[*第4章*](B18143_04.xhtml#_idTextAnchor146)中您在本地机器上创建的GitHub仓库克隆副本中，您将在名为`data`的目录中找到该文件，该目录位于名为`Chapter-06`的目录内。
- en: 'Therefore, you should find the file at the following path on your local machine
    (the slashes will be reversed if you’re using Microsoft Windows):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您应该在您的本地机器上找到以下路径处的文件（如果您使用的是Microsoft Windows，斜杠将被反转）：
- en: '`[Location in which you cloned our` `GitHub repository]/``Chapter-06``/data/AB_NYC_2019.csv`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`[您克隆我们的GitHub仓库的位置]/``Chapter-06``/data/AB_NYC_2019.csv`'
- en: 'In order to upload this file, perform the following steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了上传此文件，请执行以下步骤：
- en: Navigate into the `data` folder you created in GCS in the previous section.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到上一节中在GCS中创建的`data`文件夹。
- en: Click **Upload files**.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**上传文件**。
- en: Now, select the `AB_NYC_2019.csv` file by navigating to the `Chapter-06``/data`
    directory in the clone of our GitHub repository that you created on your local
    machine.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，通过导航到您在本地机器上创建的GitHub仓库克隆副本中的`Chapter-06/data`目录，选择`AB_NYC_2019.csv`文件。
- en: Now that we’ve completed the prerequisites, let’s discuss some important industry
    concepts that we will dive into in this chapter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了先决条件，让我们讨论一些重要的行业概念，我们将在本章中深入探讨。
- en: Fundamental concepts in this chapter
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章的基本概念
- en: In this book, we aim to provide you with knowledge not only regarding how to
    use the relevant Google Cloud services for various types of workloads but also
    regarding important industry concepts that relate to each of the relevant technologies.
    In this section, we briefly cover concepts that provide additional context for
    this chapter’s learning activities.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们的目标是为您提供有关如何使用相关Google Cloud服务来处理各种工作负载的知识，同时也提供与每个相关技术相关的重要行业概念。在本节中，我们简要介绍了为本章的学习活动提供额外背景的概念。
- en: Ingesting data into Google Cloud
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据摄入Google Cloud
- en: In previous chapters, you already performed steps to upload data to GCS and
    Google Cloud BigQuery. In addition to performing bulk uploads to GCS and BigQuery,
    it’s also possible to stream data into those services. You will see this in action
    in this chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，您已经执行了上传数据到GCS和谷歌云大数据查询（BigQuery）的步骤。除了对GCS和BigQuery执行批量上传外，还可以将这些服务中的数据流式传输。您将在本章中看到这一过程的具体操作。
- en: This section provides a more holistic overview of Google Cloud’s data ingestion
    options. We will only cover Google Cloud’s services here, but there are also countless
    options of third-party database and data management services that you can run
    on Google Cloud, such as MongoDB, Cassandra, Neo4j, and many others that are available
    through the Google Cloud Marketplace.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了对谷歌云数据摄取选项的更全面概述。在这里，我们只涵盖谷歌云的服务，但您还可以在谷歌云上运行无数第三方数据库和数据管理服务，例如MongoDB、Cassandra、Neo4j以及通过谷歌云市场提供的许多其他服务。
- en: For streaming data use cases, which we will describe in more detail later in
    this chapter, you can use Cloud Pub/Sub to ingest data from sources such as IoT
    devices or website clickstream feeds. Dataflow can also be used to ingest data
    from various sources, transform it, and write it to other Google Cloud services
    such as BigQuery, Bigtable, or Cloud Storage. As we will discuss later in this
    chapter, Dataflow also supports batch data processing use cases.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对于流式数据处理用例，我们将在本章后面更详细地描述，您可以使用Cloud Pub/Sub从物联网设备或网站点击流源摄取数据。Dataflow也可以用于从各种来源摄取数据，对其进行转换，并将其写入其他谷歌云服务，如BigQuery、Bigtable或云存储。正如我们将在本章后面讨论的，Dataflow还支持批量数据处理用例。
- en: Google Cloud Dataproc can be used to ingest data from **Hadoop Distributed File
    System** (**HDFS**)-compatible sources or other distributed storage systems, and
    Google Cloud Data Fusion can also be used to ingest data from various sources,
    transform it, and write it to most Google Cloud storage and database services.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌云Dataproc可用于从兼容**Hadoop分布式文件系统**（**HDFS**）的源或其他分布式存储系统摄取数据，谷歌云数据融合也可以用于从各种来源摄取数据，对其进行转换，并将其写入大多数谷歌云存储和数据库服务。
- en: For relational database data, you can use the Google Cloud **Database Migration
    Service** (**DMS**) to ingest data into Google Cloud SQL, which is a fully managed
    relational database service for MySQL, PostgreSQL, and SQL Server. You can also
    use standard SQL clients, import/export tools, and third-party database migration
    tools to ingest data into Cloud SQL instances.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于关系型数据库数据，您可以使用谷歌云**数据库迁移服务**（**DMS**）将数据摄取到谷歌云SQL中，这是一个为MySQL、PostgreSQL和SQL
    Server提供的完全托管的关系型数据库服务。您还可以使用标准SQL客户端、导入/导出工具和第三方数据库迁移工具将数据摄取到云SQL实例中。
- en: 'For non-relational database data, there are multiple options, including the
    following:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非关系型数据库数据，有多种选择，包括以下内容：
- en: Google Cloud Bigtable, which is a fully managed, scalable NoSQL database for
    large-scale, low-latency workloads. You can use the Bigtable HBase API or the
    Cloud Bigtable client libraries to ingest data into Bigtable.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌云Bigtable，这是一个为大规模、低延迟工作负载提供的完全托管、可扩展的NoSQL数据库。您可以使用Bigtable HBase API或Cloud
    Bigtable客户端库将数据摄取到Bigtable中。
- en: Google Cloud Firestore, which is a fully managed, serverless NoSQL document
    database for web and mobile applications. Firestore provides client libraries,
    REST APIs, or gRPC APIs to ingest data.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌云Firestore，这是一个为Web和移动应用提供的完全托管、无服务器的NoSQL文档数据库。Firestore提供了客户端库、REST API或gRPC
    API来摄取数据。
- en: 'Some of you may also be familiar with Google Cloud Datastore, which was a separate
    Google Cloud non-relational database offering but has been somewhat merged with
    Google Cloud Firestore. To see more details regarding how these database options
    relate to each other, please see the Google Cloud documentation at the following
    link: [https://cloud.google.com/datastore/docs/firestore-or-datastore](https://cloud.google.com/datastore/docs/firestore-or-datastore).'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些您可能也熟悉谷歌云数据存储（Google Cloud Datastore），它曾经是独立的谷歌云非关系型数据库服务，但已经与谷歌云Firestore有所合并。要了解更多关于这些数据库选项之间如何相互关联的详细信息，请参阅以下链接的谷歌云文档：[https://cloud.google.com/datastore/docs/firestore-or-datastore](https://cloud.google.com/datastore/docs/firestore-or-datastore)。
- en: Now that we’ve covered many of the options for ingesting data into Google Cloud,
    let’s discuss what kinds of things we may want to do with that data after ingesting
    it, starting with data transformation approaches, such as **Extract, Transform,
    Load** (**ETL**) and **Extract, Load,** **Transform** (**ELT**).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了将数据导入Google Cloud的许多选项，让我们讨论在导入数据后我们可能想要对数据进行哪些操作，从数据转换方法开始，例如**提取、转换、加载**（**ETL**）和**提取、加载、转换**（**ELT**）。
- en: ETL and ELT
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ETL和ELT
- en: ETL and ELT are two approaches for data integration, processing, and storage.
    In ETL, data is first extracted from source systems and then transformed into
    another format. Such transformations could include cleansing, enrichment, aggregation,
    or deduplication. These transformations usually take place within an intermediary
    processing engine or staging area. Once the data is transformed, it is loaded
    into the target system, which is typically a data warehouse or a data lake. ETL
    is a traditional approach that is well suited for environments where data consistency
    and quality are critical. However, it can be time-consuming and resource-intensive
    due to the processing steps happening before the data is loaded.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ETL和ELT是数据集成、处理和存储的两种方法。在ETL中，数据首先从源系统中提取出来，然后转换成另一种格式。这些转换可能包括清洗、丰富、聚合或去重。这些转换通常在一个中间处理引擎或暂存区域中进行。一旦数据被转换，它就被加载到目标系统中，这通常是数据仓库或数据湖。ETL是一种传统方法，非常适合数据一致性和质量至关重要的环境。然而，由于在数据加载之前发生处理步骤，它可能既耗时又消耗资源。
- en: On the other hand, ELT reverses the order of the last two steps. Data is first
    extracted from source systems and then directly loaded into the target system,
    such as a modern data warehouse or data lake. The transformation step is performed
    within the target system itself, using the processing capabilities of the destination
    system. ELT has gained popularity in recent years due to the increased scalability
    and processing power of modern cloud-based data warehouses, as it enables faster
    loading and allows users to perform complex transformations on demand.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，ELT颠倒了最后两个步骤的顺序。数据首先从源系统中提取出来，然后直接加载到目标系统中，例如现代数据仓库或数据湖。转换步骤在目标系统内部执行，使用目标系统的处理能力。近年来，由于现代基于云的数据仓库的可扩展性和处理能力的提高，ELT因其能够更快地加载并允许用户按需执行复杂转换而越来越受欢迎。
- en: Choosing between ETL and ELT depends on the specific requirements of the data
    processing environment, data quality needs, and the target system’s capabilities.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在ETL和ELT之间进行选择取决于数据处理环境的具体要求、数据质量需求以及目标系统的能力。
- en: Batch and streaming data processing
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量和流数据处理
- en: 'At a high level, there are two main ways in which we can process data: batch
    and streaming. In this section, we explain each of these approaches, which will
    equip us with the knowledge we need for the activities that follow in this chapter.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，我们可以以两种主要方式处理数据：批量处理和流处理。在本节中，我们将解释这些方法，这将为我们提供本章后续活动中所需的知识。
- en: Batch data processing
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量数据处理
- en: 'Batch data processing usually refers to performing a series of transformations
    on large datasets in a bulk manner. These kinds of pipelines are suitable for
    use cases in which you need to process large amounts of data in parallel over
    a period of hours or even days. As an example, imagine that your company runs
    an online retail business that has separate deployments in different geographical
    regions, such as a *www.example.com* website in North America, a *www.example.co.uk*
    website in the UK, and a *www.example.cn* website in China. Every night, you may
    want to run a workload that takes all of the items purchased by all customers
    in each region that day, merges that data across regions, and then feeds that
    data into an ML model. This would be an example of a batch workload. Other examples
    of batch data processing use cases include the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 批量数据处理通常指的是以批量方式对大量数据集进行一系列转换。这类管道适用于需要在一小时或甚至几天内并行处理大量数据的用例。例如，想象一下您的公司在不同的地理区域运行在线零售业务，例如在北美有一个*www.example.com*网站，在英国有一个*www.example.co.uk*网站，在中国有一个*www.example.cn*网站。每个晚上，您可能希望运行一个工作负载，该工作负载将当天每个区域所有客户购买的所有商品汇总起来，将数据跨区域合并，然后将这些数据输入到机器学习模型中。这将是批量工作负载的一个例子。其他批量数据处理用例的例子包括以下内容：
- en: Processing daily sales transactions to generate reports for business decision-making
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理每日销售交易以生成业务决策报告
- en: Analyzing log files from a web server to understand user behavior over a specific
    time window (for example, a day or week)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析来自Web服务器的日志文件以了解特定时间窗口内的用户行为（例如，一天或一周）
- en: Running large-scale data transformation jobs, such as converting raw data into
    a structured format for downstream data systems
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行大规模数据转换作业，例如将原始数据转换为下游数据系统所需的结构化格式
- en: In addition to data processing tasks that only need to be performed periodically,
    companies may also need to process data in real time or near real time, which
    brings us to the topic of streaming data processing.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了需要定期执行的数据处理任务之外，公司可能还需要实时或接近实时地处理数据，这使我们来到了流数据处理的主题。
- en: Streaming data processing
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流数据处理
- en: 'Streaming data processing, as the name implies, involves working on a stream
    of data that continuously flows into a system, usually on an ongoing basis. Rather
    than processing an enormous dataset as a single job, the data in a streaming processing
    use case usually consists of small pieces of data that are processed in flight.
    Examples of streaming data processing include the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 流数据处理，正如其名所示，涉及对持续流入系统的数据流进行工作，通常是在持续的基础上。而不是将庞大的数据集作为一个单独的作业进行处理，流数据处理用例中的数据通常由在飞行中的小数据块组成。流数据处理的例子包括以下内容：
- en: Analyzing social media feeds to identify trends or detect events (for example,
    **sentiment analysis** (**SA**), hashtag tracking)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析社交媒体流以识别趋势或检测事件（例如，**情感分析**（SA），标签跟踪）
- en: Monitoring and analyzing IoT sensor data in real time to detect anomalies, trigger
    alerts, or optimize processes
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时监控和分析物联网传感器数据以检测异常、触发警报或优化流程
- en: Processing financial transactions in real time for fraud detection and prevention
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时处理金融交易以进行欺诈检测和预防
- en: Now that we’ve discussed the two main high-level categories of data processing
    use cases, let’s start diving into how we actually implement these use cases.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了数据处理的两个主要高级用例类别，让我们开始深入了解我们如何实际实现这些用例。
- en: Data pipelines
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道
- en: Data pipelines are how we automate the concepts we just described, such as large-scale
    ETL/ELT or streaming data transformations. Data pipelines are essential for organizations
    that handle large volumes of data or require complex data processing workloads,
    as they help to streamline data management at scale. Without such automation pipelines,
    employees would spend a lot of time performing mundane and repetitive but complex
    and error-prone data processing activities. There are multiple Google Cloud services
    that can be used for batch and streaming data pipelines, which you will learn
    to use in this chapter.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道是我们自动化的概念，例如大规模ETL/ELT或流数据转换。对于处理大量数据或需要复杂数据处理工作负载的组织来说，数据管道是必不可少的，因为它们有助于在规模上简化数据管理。如果没有这样的自动化管道，员工将花费大量时间进行枯燥且重复但复杂且易出错的数据处理活动。Google
    Cloud提供了多种服务，可用于批处理和流数据管道，你将在本章中学习如何使用这些服务。
- en: Now that we’ve covered some of the fundamental concepts, it’s time to start
    diving into some practical data processing activities.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一些基本概念，是时候开始深入了解一些实际的数据处理活动了。
- en: Before we start to build automated data processing pipelines, however, we will
    first need to explore our data so that we can understand what kinds of transformations
    we would want to implement in our pipelines.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们开始构建自动化的数据处理管道之前，我们首先需要探索我们的数据，以便我们了解我们希望在管道中实施哪些类型的转换。
- en: Note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'While, in previous chapters, we included the code directly in the pages of
    this book, we are now advancing to more complex use cases that require a lot of
    code that would not be suitable to include directly in the pages of this book.
    Please review the code artifacts in the GitHub repository related to this chapter
    to understand the code we are using to implement these steps: [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们将代码直接包含在本书的页面中，但现在我们正在转向更复杂的使用案例，这些使用案例需要大量不适合直接包含在本书页面中的代码。请查阅GitHub仓库中与本章节相关的代码工件，以了解我们用于实现这些步骤的代码：[https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects)。
- en: Throughout the rest of the book, I will continue to include code directly where
    it makes sense.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我将继续在适当的地方直接包含代码。
- en: Exploring, visualizing, and preparing data
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索、可视化和准备数据
- en: '*Use case*: We’re planning a trip to New York City, and we want to get an idea
    of what the best accommodation options would be. Rather than perusing through
    and evaluating lots of individual Airbnb postings, we’re going to download lots
    of the reviews and do some bulk data analysis and data processing to get some
    insights.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '*用例*：我们计划去纽约市旅行，并想了解最佳的住宿选项。我们不会逐个浏览和评估大量的Airbnb帖子，而是会下载大量的评论，并进行批量数据分析和数据处理，以获得一些见解。'
- en: We can use the Vertex AI Workbench notebook that we created in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left side of the screen, navigate to the `Chapter-6` directory
    and open the`Chapter-6-Airbnb.ipynb` notebook. You can choose **Python (Local)**
    as the kernel. As you did in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168),
    run each cell in the notebook by selecting the cell and pressing *Shift* + *Enter*
    on your keyboard.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在[*第五章*](B18143_05.xhtml#_idTextAnchor168)中创建的Vertex AI Workbench笔记本来完成这个目的。请打开该笔记本实例上的JupyterLab。在屏幕左侧的目录浏览器中，导航到`Chapter-6`目录并打开`Chapter-6-Airbnb.ipynb`笔记本。您可以选择**Python
    (Local)**作为内核。正如您在[*第五章*](B18143_05.xhtml#_idTextAnchor168)中所做的那样，通过选择单元格并在键盘上按*Shift*
    + *Enter*来运行笔记本中的每个单元格。
- en: 'In the notebook, we use markdown cells to describe each step in detail so that
    you can understand each step in the process. We use libraries such as `pandas`,
    `matplotlib`, and `seaborn` to summarize and visualize the contents of our dataset,
    and then we perform data cleaning and preparation activities such as filling in
    missing values, removing outliers, and removing features that are not likely to
    be useful for training a regression model to predict accommodation prices. *Figure
    6**.3* shows an example of one of our data visualization graphs, in which we view
    the range and distribution of prices for the listings in our dataset:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在笔记本中，我们使用Markdown单元格详细描述每个步骤，以便您理解过程中的每个步骤。我们使用`pandas`、`matplotlib`和`seaborn`等库来总结和可视化数据集的内容，然后我们执行数据清理和准备活动，例如填充缺失值、移除异常值以及移除对训练预测住宿价格的回归模型可能不实用的特征。*图6**.3*展示了我们数据可视化图表的一个示例，其中我们查看数据集中列表价格的范围和分布：
- en: '![Figure 6.3: Distribution of prices in the dataset](img/B18143_06_3.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3：数据集中价格分布](img/B18143_06_3.jpg)'
- en: 'Figure 6.3: Distribution of prices in the dataset'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：数据集中价格分布
- en: As we can see, the majority of the accommodation options cost less than $200
    per night, but there are some data points (although not many) between $600 and
    $1,000 per night. Those are either very expensive accommodation options or they
    could be potential outliers/errors in the data. You can see additional data visualization
    graphs in the notebook.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，大多数住宿选项每晚的费用低于200美元，但也有一些数据点（尽管不多）每晚在600至1000美元之间。这些可能是非常昂贵的住宿选项，或者它们可能是数据中的潜在异常值/错误。您可以在笔记本中查看更多的数据可视化图表。
- en: 'Regarding data cleanup activities, to clean up potential pricing outliers,
    for example, we use the following piece of code to set a limit of $800 (although
    still high) and remove any listings above that nightly rate:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据清理活动，例如，为了清理潜在的价格异常值，我们使用以下代码片段来设置800美元的限制（尽管仍然很高）并移除任何超过该每晚价格的列表：
- en: '[PRE0]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To remove features that are not likely to be useful for training a regression
    model to predict accommodation prices, we use the following piece of code:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了移除对训练预测住宿价格的回归模型可能不实用的特征，我们使用以下代码片段：
- en: '[PRE1]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: These are just a couple of examples of the data preparation steps we perform
    in the notebook.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是我们在笔记本中执行的数据准备步骤的几个示例。
- en: When you have completed executing all of the activities in the notebook, we
    will move on to see how we can turn those activities into an automated pipeline
    in production. We will start by implementing a batch data pipeline.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成笔记本中所有活动的执行后，我们将继续探讨如何将这些活动转化为生产环境中的自动化流程。我们将首先实现批量数据处理流程。
- en: Batch data pipelines
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量数据处理流程
- en: Now that we’ve used our Jupyter notebook to explore our data and figure out
    what kinds of transformations we want to perform on our dataset, let’s imagine
    that we want to turn this into a production workload that can run automatically
    on very large files, without needing any further human effort. As I mentioned
    earlier, this is essential in any company that implements large-scale data analytics
    and AI/ML workloads. It’s simply not feasible to get somebody to manually perform
    those transformations every time, and for very large data volumes, the transformations
    could not be performed on a notebook instance. For example, imagine we get thousands
    of new postings every day, and we want to automatically prepare that data for
    an ML model. We can do this by creating an automated pipeline to perform data
    transformations every night (or however often we wish).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用我们的 Jupyter 笔记本来探索我们的数据，并确定我们想在数据集上执行哪些类型的转换，让我们设想一下，我们想要将这个转换成一个可以自动在非常大的文件上运行的生产工作负载，而不需要任何进一步的人工努力。正如我之前提到的，这对于任何实施大规模数据分析和
    AI/ML 工作负载的公司来说都是至关重要的。让某个人每次都手动执行这些转换是不切实际的，而对于非常大的数据量，这些转换无法在笔记本实例上执行。例如，想象一下，我们每天都会收到数千条新的帖子，我们想要自动为
    ML 模型准备这些数据。我们可以通过创建一个自动化的管道来每晚（或我们希望的时间间隔）执行数据转换来实现这一点。
- en: Batch data pipeline concepts and tools
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量数据处理管道的概念和工具
- en: Before we start diving in and building our batch data processing pipeline, let’s
    first cover some important concepts and tools in this domain.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始深入构建我们的批量数据处理管道之前，让我们首先介绍这个领域的一些重要概念和工具。
- en: Apache Spark
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Spark
- en: Apache Spark is a highly popular development and execution framework that can
    be used to implement very large-scale data processing workloads and other types
    of large-scale computing use cases such as ML. Its power lies both in its in-memory
    processing capabilities and its ability to implement multiple large computing
    and data processing tasks in parallel.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个非常流行的开发和执行框架，可以用于实现非常大的数据处理工作负载以及其他类型的大规模计算用例，如 ML。它的力量既在于其内存处理能力，也在于其能够并行实现多个大型计算和数据处理任务的能力。
- en: While Spark can be used for both batch and streaming (or micro-batching) data
    processing workloads, we’re going to use it to perform our batch data transformations
    in this chapter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Spark 可以用于批处理和流式（或微批处理）数据处理工作负载，但我们在本章中将使用它来执行我们的批量数据转换。
- en: Google Cloud Dataproc
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud Dataproc
- en: As we discussed in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), Google Cloud
    Dataproc is a fully managed, fast, and easy-to-use service for running Apache
    Spark and Apache Hadoop clusters on GCP. In this chapter, we will use it to execute
    our Spark processing jobs.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [*第 3 章*](B18143_03.xhtml#_idTextAnchor059) 中讨论的那样，Google Cloud Dataproc
    是一个完全托管、快速且易于使用的服务，可以在 GCP 上运行 Apache Spark 和 Apache Hadoop 集群。在本章中，我们将使用它来执行我们的
    Spark 处理作业。
- en: Apache Airflow
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Airflow
- en: Apache Airflow is an open source platform used for orchestrating complex data
    workflows. It was created by Airbnb and later contributed to the **Apache Software
    Foundation** (**ASF**). Airflow is designed to help developers and data engineers
    create, schedule, monitor, and manage workflows, making it easier to handle tasks
    that depend on one another. It is commonly used in data engineering and data science
    projects for tasks such as ETL, ML pipelines, and data analytics, and it is widely
    used by organizations across various industries, making it a popular choice for
    managing complex data workflows.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Airflow 是一个开源平台，用于编排复杂的数据工作流。它由 Airbnb 创建，后来贡献给了 **Apache 软件基金会**（**ASF**）。Airflow
    设计用于帮助开发人员和数据工程师创建、安排、监控和管理工作流，使得处理相互依赖的任务变得更加容易。它通常用于数据工程和数据科学项目中的任务，如 ETL、ML
    管道和数据分析，并且被各个行业的组织广泛使用，使其成为管理复杂数据工作流的热门选择。
- en: Directed acyclic graphs
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 有向无环图
- en: 'Airflow represents workflows as **directed acyclic graphs** (**DAGs**), which
    consist of tasks and their dependencies. Each task in the workflow is represented
    as a node, and the dependencies between tasks are represented as directed edges.
    This structure ensures that tasks are executed in a specific order without creating
    loops, as depicted in *Figure 6**.4*:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow 将工作流表示为 **有向无环图**（**DAGs**），它由任务及其依赖关系组成。工作流中的每个任务都表示为一个节点，任务之间的依赖关系由有向边表示。这种结构确保了任务按照特定的顺序执行，不会创建循环，如图
    *6**.4* 所示：
- en: '![Figure 6.4: A simple DAG  (excerpted from source:  https://www.flickr.com/photos/dullhunk/4647369097)](img/B18143_06_4.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4：一个简单的DAG（来源：https://www.flickr.com/photos/dullhunk/4647369097）](img/B18143_06_4.jpg)'
- en: 'Figure 6.4: A simple DAG (excerpted from source: https://www.flickr.com/photos/dullhunk/4647369097)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：一个简单的DAG（来源：https://www.flickr.com/photos/dullhunk/4647369097）
- en: In *Figure 6**.4*, we can see that tasks *b*, *c*, *d*, and *e* all depend on
    task *a*. Similarly, task *d* also depends on tasks *b* and *c*, and task *e*
    also depends on tasks *c* and *d*.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.4*中，我们可以看到任务*b*、*c*、*d*和*e*都依赖于任务*a*。同样，任务*d*也依赖于任务*b*和*c*，任务*e*也依赖于任务*c*和*d*。
- en: In Airflow, a DAG is defined in a Python script, which represents tasks and
    their dependencies as code.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在Airflow中，DAG通过Python脚本定义，它将任务及其依赖关系表示为代码。
- en: Google Cloud Composer
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud Composer
- en: '**Google Cloud Composer** (**GCC**) is a fully managed workflow orchestration
    service built on Apache Airflow. It allows you to create, schedule, and monitor
    data workflows across various Google Cloud services, as well as on-premises or
    multi-cloud environments.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '**Google Cloud Composer**（**GCC**）是基于Apache Airflow构建的完全托管的工作流程编排服务。它允许您在多个Google
    Cloud服务、本地或多云环境中创建、安排和监控数据工作流。'
- en: Cloud Composer simplifies the process of setting up and managing Apache Airflow
    by providing an easy-to-use interface and automating the infrastructure management.
    This allows you to focus on creating and maintaining your workflows while Google
    takes care of the underlying infrastructure, scaling, and updates.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 云作曲家通过提供易于使用的界面并自动化基础设施管理来简化设置和管理Apache Airflow的过程。这允许您专注于创建和维护您的流程，而Google则负责底层基础设施、扩展和更新。
- en: Now that we’ve covered the important concepts for implementing batch data pipelines,
    let’s start building our pipeline.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了实现批量数据管道的重要概念，让我们开始构建我们的管道。
- en: Building our batch data pipeline
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建我们的批量数据管道
- en: 'In this section, we will create our Spark job and run it on Google Cloud Dataproc
    and will use GCC to orchestrate our job. What this means is that we can get GCC
    to automatically run our job every day. Each time it runs our job, it will create
    a Dataproc cluster, execute our Spark job, and then delete the Dataproc cluster
    when our job completes. This is a standard best practice that companies use to
    save money because you should not have computing resources running when you are
    not using them. The architecture of our pipeline on Google Cloud is shown in *Figure
    6**.5*:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建我们的Spark作业并在Google Cloud Dataproc上运行它，并将使用GCC来编排我们的作业。这意味着我们可以让GCC每天自动运行我们的作业。每次运行作业时，它将创建一个Dataproc集群，执行我们的Spark作业，然后在我们的作业完成后删除Dataproc集群。这是公司用来节省资金的标准最佳实践，因为当你不使用计算资源时，你不应该有它们在运行。我们Google
    Cloud上的管道架构在*图6.5*中显示：
- en: '![Figure 6.5: Batch data pipeline architecture](img/B18143_06_5.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5：批量数据管道架构](img/B18143_06_5.jpg)'
- en: 'Figure 6.5: Batch data pipeline architecture'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：批量数据管道架构
- en: Let’s begin by setting up Cloud Composer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从设置云作曲家开始。
- en: Cloud Composer
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 云作曲家
- en: In this section, we will set up Cloud Composer to schedule and run our batch
    data processing pipeline.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将设置云作曲家以安排和运行我们的批量数据处理管道。
- en: Cloud Composer environment
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云作曲家环境
- en: 'Everything we do in Cloud Composer happens within a Cloud Composer environment.
    To set up our Cloud Composer environment, perform the following steps:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在云作曲家中，我们做的所有事情都在云作曲家环境中进行。要设置我们的云作曲家环境，请执行以下步骤：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Composer** → **Environments**.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单→**作曲家**→**环境**。
- en: Select **Create Environment**.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**创建环境**。
- en: If prompted, select **Composer 2**.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要，选择**作曲家2**。
- en: 'On the screen that appears, enter a name for your Composer environment. See
    *Figure 6**.6* for reference:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在出现的屏幕上，输入您的Composer环境名称。参见*图6.6*以获取参考：
- en: '![Figure 6.6: Creating a Composer environment](img/B18143_06_6.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6：创建Composer环境](img/B18143_06_6.jpg)'
- en: 'Figure 6.6: Creating a Composer environment'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：创建Composer环境
- en: Select your preferred region. (Remember that it’s better if you use the same
    region for each activity throughout this book, if possible.)
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您首选的区域。（记住，如果可能的话，在整个本书中为每个活动使用相同的区域会更好。）
- en: Select the latest image version. See *Figure 6**.6* for reference.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择最新的镜像版本。参见*图6.6*以获取参考。
- en: '*IMPORTANT*: Select the service account you created earlier in this chapter
    (if you used the suggested name, then it will include `data-processing-sa` in
    the name).'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*重要提示*：选择您在本章中创建的服务帐户（如果您使用了建议的名称，那么名称中将包含 `data-processing-sa`）。'
- en: In the **Environment resources** section, select a small environment.
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**环境资源**部分，选择一个小型环境。
- en: Leave all other options at their default values and select **Create**.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有其他选项保留在默认值并选择**创建**。
- en: The environment will take up to 25 minutes to spin up.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 环境启动可能需要 25 分钟。
- en: While waiting for the environment to spin up, let’s move on to the next section.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在等待环境启动的同时，让我们继续下一节。
- en: Cloud Composer Python code
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Cloud Composer Python 代码
- en: 'In this section, we will review and prepare the Python code for our Cloud Composer
    Spark workload. There are two code resources that we will use with Cloud Composer,
    which can be found in our GitHub repository ([https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/tree/main/](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/tree/main/Chapter-6)Chapter-06):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将审查和准备用于我们的 Cloud Composer Spark 工作负载的 Python 代码。我们将使用两个代码资源与 Cloud Composer
    一起，这些资源可以在我们的 GitHub 仓库中找到（[https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/tree/main/](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/tree/main/Chapter-6)Chapter-06）：
- en: '`composer-dag.py`, which contains the Python code that defines our Cloud Composer
    Airflow DAG'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`composer-dag.py`，其中包含定义我们的 Cloud Composer Airflow DAG 的 Python 代码'
- en: '`chapter-6-pyspark.py`, which contains the PySpark code that defines our Spark
    job'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chapter-6-pyspark.py`，其中包含定义我们的 Spark 作业的 PySpark 代码'
- en: 'Perform the following steps to start preparing those files for use with Cloud
    Composer:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以开始准备这些文件用于与 Cloud Composer 一起使用：
- en: Locate those files in the clone of our GitHub repository that you created on
    your local machine, and open them for editing.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您在本地机器上创建的我们的 GitHub 仓库的克隆中找到这些文件，并打开它们进行编辑。
- en: In the `chapter-6-pyspark.py` file, you just need to update the storage locations
    for the source and destination datasets. To do that, search for the `GCS-BUCKET-NAME`
    string in the file and replace it with your own GCS bucket name that you created
    earlier.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `chapter-6-pyspark.py` 文件中，您只需更新源和目标数据集的存储位置。为此，在文件中搜索 `GCS-BUCKET-NAME` 字符串，并将其替换为您之前创建的自己的
    GCS 存储桶名称。
- en: IMPORTANT
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 重要
- en: The **GCS-BUCKET-NAME** string exists in two locations in the file (once near
    the beginning, and again near the end). One location specifies the source dataset,
    and the other location specifies the destination where our Spark job will save
    the processed data. Replace both occurrences of the string with your own GCS bucket
    name.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '**GCS-BUCKET-NAME** 字符串在文件中有两个位置（一次在开头附近，一次在结尾附近）。一个位置指定源数据集，另一个位置指定 Spark
    作业将保存处理后的数据的目标位置。将字符串的所有出现替换为您自己的 GCS 存储桶名称。'
- en: 'In the `composer-dag.py` file, you will see the following block of variables
    near the beginning of the file:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `composer-dag.py` 文件中，您将在文件开头附近看到以下变量块：
- en: '[PRE2]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: All of those variables need to be updated with specific values from your GCP
    project. The comments in the file provide additional details on the replacements.
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有这些变量都需要使用您 GCP 项目中的特定值进行更新。文件中的注释提供了关于替换的额外细节。
- en: In addition to making the aforementioned changes, review the contents of the
    code to get an understanding of how Cloud Composer will execute our jobs. The
    code contains comments to describe what it’s doing in each section so that you
    can understand how it works.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除了做出上述更改外，请审查代码内容，以了解 Cloud Composer 将如何执行我们的作业。代码中包含注释，描述了每个部分正在做什么，以便您了解其工作原理。
- en: When you have completed the preceding steps, we’re ready to upload the resources
    to be used by Cloud Composer.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您完成前面的步骤后，我们就可以上传 Cloud Composer 将要使用的资源了。
- en: 'Cloud Composer requires the preceding code resources to be stored in GCS, but
    in two separate locations:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cloud Composer 需要将前面的代码资源存储在 GCS 中，但需要在两个不同的位置：
- en: 'For `chapter-6-pyspark.py`: Upload this file in the `code` folder you created
    earlier in GCS. To do that, navigate into the `code` folder you created, select
    `chapter-6-pyspark.py``composer-dag.py`: This file will be uploaded to a special
    folder that Cloud Composer will create. When your Cloud Composer environment is
    fully created, click on the name of your newly created environment in the Cloud
    Composer console, and your environment details screen will open. Near the top
    of the screen, select `composer-dag.py` file from the clone of our GitHub repository
    that you created on your local machine (that is, the file you edited in the previous
    steps).'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`chapter-6-pyspark.py`：将此文件上传到您之前在GCS中创建的`code`文件夹中。为此，导航到您创建的`code`文件夹，选择`chapter-6-pyspark.py`。`composer-dag.py`：此文件将被上传到Cloud
    Composer为您创建的特定文件夹。当您的Cloud Composer环境完全创建后，在Cloud Composer控制台中点击您新创建的环境名称，您的环境详细信息屏幕将打开。在屏幕顶部附近，从您在本地机器上创建的GitHub仓库克隆中（即您在之前步骤中编辑的文件）选择`composer-dag.py`文件。
- en: That’s it! As soon as the `composer-dag.py` file is uploaded, Cloud Composer
    will completely automate everything for you. It will take a few minutes, but Cloud
    Composer will create your DAG, and you will see it appearing in the Cloud Composer
    console. It will then execute the DAG, meaning that it will create a Dataproc
    cluster, execute a Spark job to perform the data transformations we specified,
    and delete the Dataproc cluster when the job completes.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！一旦`composer-dag.py`文件上传完毕，Cloud Composer将为您完全自动化一切。这可能需要几分钟，但Cloud Composer将创建您的DAG，您将在Cloud
    Composer控制台中看到它出现。然后它将执行DAG，这意味着它将创建一个Dataproc集群，执行Spark作业以执行我们指定的数据转换，并在作业完成后删除Dataproc集群。
- en: You can see the various tasks happening in the Composer and Dataproc consoles
    (give it some time in each case), and the final test will be to verify that the
    processed data appears in the GCS destination you specified in the PySpark code.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Composer和Dataproc控制台中看到正在进行的各种任务（每种情况都给一些时间），最终的测试将是验证处理后的数据是否出现在PySpark代码中指定的GCS目标中。
- en: When you have completed all of the aforementioned steps and you no longer need
    your Composer environment, you can delete the environment.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成所有上述步骤并且不再需要您的Composer环境时，您可以删除该环境。
- en: Deleting the Cloud Composer environment
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 删除Cloud Composer环境
- en: 'Perform the following steps to delete the Composer environment:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以删除Composer环境：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Composer** → **Environments**.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单→**Composer**→**环境**。
- en: Select the checkbox next to the name of your environment.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您环境名称旁边的复选框。
- en: 'Select **DELETE** at the top of the screen. See *Figure 6**.7* for reference:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕顶部选择**删除**。参见*图6.7*以获取参考：
- en: '![Figure 6.7: Deleting the Composer environment](img/B18143_06_7.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7：删除Composer环境](img/B18143_06_7.jpg)'
- en: 'Figure 6.7: Deleting the Composer environment'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7：删除Composer环境
- en: Select **DELETE** in the confirmation screen that appears.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在出现的确认屏幕中选择**删除**。
- en: It will take a few minutes for the environment to delete, after which it will
    disappear from your list of environments.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 环境删除可能需要几分钟，之后它将从您的环境列表中消失。
- en: Awesome work! You have officially created your first data processing pipeline
    on Google Cloud!
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！您已经在Google Cloud上正式创建了您的第一个数据处理管道！
- en: Note that the method we used in this chapter is an example of one (very popular)
    pattern for using multiple Google Cloud services to implement a data pipeline.
    Google Cloud also provides other products that could be used to implement similar
    outcomes, such as Google Cloud Data Fusion, which enables you to create pipelines
    using a visual user interface. We will explore other services in later chapters
    of this book, and one other important offering that was launched by Google Cloud
    in 2023 is Serverless Spark, which we’ll briefly discuss next.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，本章中我们使用的方法是使用多个Google Cloud服务实现数据管道（一个非常流行的）模式的一个示例。Google Cloud还提供其他产品，可用于实现类似的结果，例如Google
    Cloud Data Fusion，它允许您使用可视化用户界面创建管道。我们将在本书的后续章节中探索其他服务，而Google Cloud在2023年推出的另一个重要服务是无服务器Spark，我们将在下一节中简要讨论。
- en: Google Cloud Serverless Spark
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud无服务器Spark
- en: Google Cloud Serverless Spark is a fully managed, serverless Apache Spark service
    that makes it easy to run Spark jobs without having to provision or manage any
    infrastructure. It also automatically scales your jobs up or down based on demand,
    so you only pay for the resources that you use, which makes it a cost-effective
    way to run Spark jobs, even for short-lived or intermittent workloads.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 无服务器 Spark 是一个完全托管的无服务器 Apache Spark 服务，它使得在不配置或管理任何基础设施的情况下运行
    Spark 作业变得简单。它还会根据需求自动扩展您的作业，因此您只需为使用的资源付费，这使得它是一种成本效益高的运行 Spark 作业的方式，即使是对于短期或间歇性工作负载也是如此。
- en: It’s also integrated with other Google Cloud services, such as BigQuery, Dataflow,
    Dataproc, and Vertex AI, and popular open-source tools such as Zeppelin and Jupyter
    Notebook, making it easy to explore and analyze data and build and run **end-to-end**
    (**E2E**) data pipelines directly with those services.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 它还与 Google Cloud 的其他服务集成，例如 BigQuery、Dataflow、Dataproc 和 Vertex AI，以及流行的开源工具如
    Zeppelin 和 Jupyter Notebook，这使得使用这些服务直接探索和分析数据以及构建和运行 **端到端**（**E2E**）数据管道变得容易。
- en: 'With Serverless Spark on Dataproc, for example, you can select from pre-made
    templates to easily perform common tasks such as moving and transforming data
    between **Java Database Connectivity** (**JDBC**) or Apache Hive data stores and
    GCS or BigQuery, or you can build your own Docker containers that Serverless Spark
    will run in order to implement custom data processing workloads. For more information
    on how to develop such custom containers, see the following Google Cloud documentation:
    [https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers](https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 Dataproc 上的无服务器 Spark 中，您可以从预制的模板中选择，轻松执行常见任务，如将数据在 **Java 数据库连接**（**JDBC**）或
    Apache Hive 数据存储与 GCS 或 BigQuery 之间移动和转换，或者您可以为无服务器 Spark 构建自己的 Docker 容器，以便实现自定义数据处理工作负载。有关如何开发此类自定义容器的更多信息，请参阅以下
    Google Cloud 文档：[https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers](https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers)。
- en: Now that we’ve learned how to build a batch data processing pipeline, we will
    move on to implementing streaming data pipelines.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何构建批量数据处理管道，我们将继续实施流式数据处理管道。
- en: Streaming data pipelines
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式数据处理管道
- en: In this section, we will deal with a different kind of data source and will
    learn about the differences in processing data in real time versus the batch-oriented
    methods we used in the previous sections.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将处理不同类型的数据源，并了解实时数据处理与我们在前几节中使用的面向批量的方法的差异。
- en: Streaming data pipeline concepts and tools
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式数据处理管道的概念和工具
- en: Again, before we start to build a streaming data processing pipeline, there
    are some important concepts and tools that we need to introduce and understand.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在我们开始构建流式数据处理管道之前，有一些重要的概念和工具我们需要介绍和理解。
- en: Apache Beam
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache Beam
- en: Apache Beam is an open source, unified programming model for processing and
    analyzing large-scale data in batch and streaming modes. It was initially developed
    by Google as a part of its internal data processing tools, and later, it was donated
    to the ASF. Beam provides a unified way to write data processing pipelines that
    can be executed on various distributed processing backends such as Apache Flink,
    Apache Samza, Apache Spark, Google Cloud Dataflow, and others. It supports multiple
    programming languages, including Java, Python, and Go, and it allows developers
    to write both batch and streaming data processing pipelines using a single API,
    which simplifies the development process and enables seamless switching between
    batch and streaming modes. It also provides a rich set of built-in I/O connectors
    for various data sources and sinks, including Kafka, Hadoop, Google Cloud Pub/Sub,
    BigQuery, and others. Additionally, developers can build their own custom connectors
    if needed. In this chapter, we will use Apache Beam on Google Cloud Dataflow to
    create a pipeline to process data in real time.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Beam 是一个开源的、统一的编程模型，用于批处理和流处理模式下的大规模数据处理。它最初由 Google 开发，作为其内部数据处理工具的一部分，后来捐赠给了
    ASF。Beam 提供了一种统一的方式来编写可以在各种分布式处理后端上执行的数据处理管道，例如 Apache Flink、Apache Samza、Apache
    Spark、Google Cloud Dataflow 等。它支持多种编程语言，包括 Java、Python 和 Go，并允许开发者使用单个 API 编写批处理和流数据处理管道，从而简化了开发过程并实现了批处理和流处理模式之间的无缝切换。它还提供了一套丰富的内置
    I/O 连接器，用于各种数据源和接收器，包括 Kafka、Hadoop、Google Cloud Pub/Sub、BigQuery 等。此外，如果需要，开发者可以构建自己的自定义连接器。在本章中，我们将使用
    Apache Beam 在 Google Cloud Dataflow 上创建一个管道，以实时处理数据。
- en: Apache Beam concepts
  id: totrans-208
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Apache Beam 概念
- en: 'In this section, we discuss some basic concepts of the Apache Beam programming
    model, which include the following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了 Apache Beam 编程模型的一些基本概念，包括以下内容：
- en: '**Pipelines**: Just like the concept of a pipeline in Apache Airflow, which
    we used earlier in this chapter, an Apache Beam pipeline is a DAG representing
    a sequence of data processing steps or the overall data processing workflow in
    Apache Beam workloads.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pipelines**：就像我们在本章前面使用的 Apache Airflow 中的管道概念一样，Apache Beam 管道是一个 DAG，它表示
    Apache Beam 工作负载中的数据处理步骤序列或整体数据处理工作流程。'
- en: '**PCollections**: A **Parallel Collection** (**PCollection**) is an immutable
    distributed dataset representing a collection of data elements. It is the primary
    data structure used in Apache Beam pipelines to hold and manipulate data.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PCollections**：**并行集合**（**PCollection**）是一个不可变的分布式数据集，表示数据元素集合。它是 Apache
    Beam 管道中用于存储和操作数据的主要数据结构。'
- en: '**PTransforms**: A **Parallel Transform** (**PTransform**) is a user-defined
    operation that takes one or more PCollections as input, processes the data, and
    produces one or more PCollections as output. PTransforms are the building blocks
    of a pipeline and define the data processing logic.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PTransforms**：**并行转换**（**PTransform**）是一个用户定义的操作，它接受一个或多个 PCollections 作为输入，处理数据，并产生一个或多个
    PCollections 作为输出。PTransforms 是管道的构建块，并定义了数据处理逻辑。'
- en: '**Windowing**: Windowing is a mechanism that allows grouping data elements
    in a PCollection based on timestamps or other criteria. This concept is particularly
    useful for processing unbounded datasets in streaming applications, where data
    elements need to be processed in finite windows.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Windowing**：窗口化是一种机制，允许根据时间戳或其他标准对 PCollection 中的数据元素进行分组。这个概念对于处理流应用程序中的无界数据集特别有用，在这些应用程序中，数据元素需要以有限窗口进行处理。'
- en: '**Watermarks**: Watermarks are a way to estimate the progress of time in a
    streaming pipeline, and they help to determine when it’s safe to emit results
    for a particular window.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Watermarks**：水印是估计流处理管道中时间进度的一种方式，并有助于确定何时可以安全地发出特定窗口的结果。'
- en: '**Triggers**: A trigger determines when to aggregate the results of each window,
    based on factors such as the arrival of a certain number of data elements, the
    passage of a certain amount of time, or the advancement of watermarks.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**触发器**：触发器根据某些因素（如到达一定数量的数据元素、经过一定的时间或水印的推进）确定何时聚合每个窗口的结果。'
- en: '**Runners**: Runners are the components responsible for executing a Beam pipeline
    on a specific execution engine or distributed processing platform.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Runners**：运行器是负责在特定执行引擎或分布式处理平台上执行 Beam 管道的组件。'
- en: The Beam model decouples the pipeline definition from the underlying execution
    engine, allowing users to choose the most suitable platform for their use case.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Beam 模型将管道定义与底层执行引擎解耦，使用户能够为他们的用例选择最合适的平台。
- en: Google Cloud Dataflow
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud Dataflow
- en: We introduced Dataflow in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), and
    now we’ll dive deeper into it. Dataflow is a Google Cloud service on which you
    can run Apache Beam. In other words, it provides one of the execution environments
    or runners on which you can run Apache Beam workloads.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [*第 3 章*](B18143_03.xhtml#_idTextAnchor059) 中介绍了 Dataflow，现在我们将更深入地探讨它。Dataflow
    是一个 Google Cloud 服务，你可以在其上运行 Apache Beam。换句话说，它提供了 Apache Beam 工作负载可以运行的执行环境或运行器之一。
- en: Dataflow provides multiple features for various types of use cases, and in this
    section, we’ll briefly discuss the various options and their applications.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Dataflow 为各种类型的用例提供了多个功能，在本节中，我们将简要讨论各种选项及其应用。
- en: Dataflow Data Pipelines and Dataflow Jobs
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dataflow 数据管道和 Dataflow 作业
- en: People often get confused about the difference between Dataflow Data Pipelines
    and Dataflow Jobs. The best way to look at this is that Dataflow Data Pipelines
    refer to the definition of a data pipeline, which could be executed on a recurring
    basis, whereas Dataflow Jobs refer to a single execution of a data pipeline. The
    reason this can be confusing is that you can create a new pipeline definition
    either in the Dataflow Jobs console or in the Dataflow Data Pipelines console.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常对 Dataflow 数据管道和 Dataflow 作业之间的区别感到困惑。最好的看待方式是，Dataflow 数据管道指的是数据管道的定义，这可以定期执行，而
    Dataflow 作业则指单个数据管道的执行。这种混淆的原因是，你可以在 Dataflow 作业控制台或 Dataflow 数据管道控制台中创建新的管道定义。
- en: In either case, when creating a pipeline definition, we have the option to use
    predefined templates that cover common types of tasks that people often want to
    do with Dataflow, such as transferring data from BigQuery to Bigtable, or from
    Cloud Spanner to Pub/Sub, and there are lots of different templates to choose
    from, covering a wide variety of data sources and destinations. These templates
    make it very easy for us to implement a data transfer workload without requiring
    much or any development effort on our part. Alternatively, if we have more complex
    data processing needs that are not included in one of the standard templates,
    then we can create our own custom pipeline definitions. We will look at both options
    later in this chapter.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，当创建管道定义时，我们都有使用预定义模板的选项，这些模板涵盖了人们经常希望使用 Dataflow 完成的常见任务类型，例如将数据从 BigQuery
    转移到 Bigtable，或从 Cloud Spanner 转移到 Pub/Sub，而且有大量的不同模板可供选择，覆盖了广泛的数据源和目的地。这些模板使我们能够非常容易地实现数据传输工作负载，而无需我们进行很多或任何开发工作。或者，如果我们有更复杂的数据处理需求，而这些需求不包括在任何标准模板中，那么我们可以创建自己的自定义管道定义。我们将在本章后面探讨这两种选项。
- en: Dataflow Workbench notebooks
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dataflow Workbench 笔记本
- en: One way in which we can develop custom data processing pipelines is by using
    Dataflow Workbench notebooks. This may sound somewhat familiar, because you may
    remember creating and using a Vertex AI Workbench notebook in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168).
    In the Dataflow Workbench console, we can create notebooks that come with Apache
    Beam already installed. We will create a notebook in the Dataflow Workbench console
    later in this chapter.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用 Dataflow Workbench 笔本来开发自定义数据处理管道。这可能听起来有些熟悉，因为你可能还记得在 [*第 5 章*](B18143_05.xhtml#_idTextAnchor168)
    中创建和使用 Vertex AI Workbench 笔记本。在 Dataflow Workbench 控制台中，我们可以创建已经预装 Apache Beam
    的笔记本。我们将在本章后面创建一个笔记本。
- en: Dataflow snapshots
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dataflow 快照
- en: Dataflow snapshots save the state of a streaming pipeline, which allows you
    to start a new version of your Dataflow job without losing state. This is useful
    for backup and recovery, testing, and rolling back updates to streaming pipelines.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Dataflow 快照保存了流管道的状态，这使得你可以在不丢失状态的情况下启动 Dataflow 作业的新版本。这对于备份和恢复、测试以及回滚流管道的更新非常有用。
- en: SQL Workspace
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SQL 工作区
- en: The Dataflow console also includes a built-in SQL Workspace that enables you
    to run SQL queries directly from the console and send the results to BigQuery
    or Pub/Sub. This can be useful for ad hoc use cases in which you want to simply
    run a SQL query to fetch information from a given source and store the results
    in one of the supported destinations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Dataflow 控制台还包括一个内置的 SQL 工作区，它允许你直接从控制台运行 SQL 查询，并将结果发送到 BigQuery 或 Pub/Sub。这对于你只想简单地运行一个
    SQL 查询以从给定源获取信息并将结果存储在支持的某个目的地的情况非常有用。
- en: Now that we’ve covered the important concepts for implementing streaming data
    pipelines, let’s start building our pipeline.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了实现流数据管道的重要概念，让我们开始构建我们的管道。
- en: Building our streaming data pipeline
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建我们的流数据管道
- en: Sticking with the theme of planning our trip to New York City, the activities
    in the previous sections of this chapter gave us some good insights into what
    kinds of accommodation options are available to us, and now we want to assess
    our transportation options; specifically, how much it’s likely to cost us to travel
    around in taxis while we’re there.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 坚持我们计划去纽约市的主题，本章前几节的活动让我们对可用的住宿选项有了很好的了解，现在我们想要评估我们的交通选项；具体来说，我们想知道在纽约市乘坐出租车旅行的费用可能多少。
- en: 'Our streaming data pipeline will take input data from Google Cloud Pub/Sub,
    perform some processing in Dataflow, and place the outputs into BigQuery for analysis.
    The architecture of our pipeline on Google Cloud is shown in *Figure 6**.8*:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的流数据管道将从 Google Cloud Pub/Sub 获取输入数据，在 Dataflow 中进行一些处理，并将输出放入 BigQuery 以进行分析。我们的管道在
    Google Cloud 上的架构如 *图 6.8* 所示：
- en: '![Figure 6.8: Streaming data pipeline](img/B18143_06_8.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8：流数据管道](img/B18143_06_8.jpg)'
- en: 'Figure 6.8: Streaming data pipeline'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8：流数据管道
- en: Google Cloud provides a public stream of data that can be used to test these
    kinds of stream processing workloads, which contains information relating to New
    York City taxi rides, and which we will use in our example in this section. Let’s
    start by creating a destination for our streamed data, also referred to as a **sink**
    for our pipeline.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 提供了一个公共数据流，可以用于测试这些类型的流处理工作负载，其中包含与纽约市出租车行程相关的信息，我们将在本节的示例中使用它。让我们首先为我们的流数据创建一个目的地，也称为我们的管道的
    **汇**。
- en: Creating a BigQuery dataset
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 BigQuery 数据集
- en: 'We will use Google Cloud BigQuery as a storage system for our data. To get
    started, we first need to define a dataset in BigQuery that we will use as our
    pipeline destination. To do this, perform the following steps:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Google Cloud BigQuery 作为我们的数据存储系统。要开始，我们首先需要在 BigQuery 中定义一个数据集，我们将将其用作我们的管道目的地。为此，执行以下步骤：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **BigQuery**.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Google Cloud 控制台中，导航到 **Google Cloud 服务** 菜单→ **BigQuery**。
- en: 'In the top-left corner of the screen, you will see your project name. Click
    on the symbol of three vertical dots to the right of your project name (see *Figure
    6**.9* for reference):'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕的左上角，你会看到你的项目名称。点击项目名称右侧的三个垂直点符号（参见 *图 6.9* 以获取参考）：
- en: '![Figure 6.9: BigQuery project menu](img/B18143_06_9.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9：BigQuery 项目菜单](img/B18143_06_9.jpg)'
- en: 'Figure 6.9: BigQuery project menu'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9：BigQuery 项目菜单
- en: In the menu that gets displayed, select **Create dataset**.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在显示的菜单中，选择 **创建数据集**。
- en: 'Give your dataset a name: `taxirides`.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给你的数据集起个名字：`taxirides`。
- en: Select your preferred region, and select **Create dataset**.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你首选的区域，并选择 **创建数据集**。
- en: Now that we’ve created our dataset, we will need to create a table within that
    dataset.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了数据集，我们需要在该数据集中创建一个表。
- en: Creating a BigQuery table
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 BigQuery 表
- en: 'A BigQuery dataset generally contains one or more tables that contain the actual
    data. Let’s create a table into which our data will be streamed. To do this, perform
    the following steps:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 BigQuery 数据集通常包含一个或多个包含实际数据的表。让我们创建一个我们将数据流到其中的表。为此，执行以下步骤：
- en: In the BigQuery editor, click on the `taxirides` dataset you just created and
    select **Create table**.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 BigQuery 编辑器中，点击你刚刚创建的 `taxirides` 数据集，并选择 **创建表**。
- en: In the `realtime` as the table name.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将表名设置为 `realtime`。
- en: 'In the **Schema** section, click the plus sign (**+**) to add a new field.
    Our first field has the following properties (leave all other options for each
    field at their default values):'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **模式** 部分中，点击加号（**+**）添加一个新字段。我们的第一个字段具有以下属性（将每个字段的其余选项保留为其默认值）：
- en: '| **Field name** | **Type** | **Mode** |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| **字段名称** | **类型** | **模式** |'
- en: '| `ride_id` | `STRING` | `NULLABLE` |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `ride_id` | `STRING` | `NULLABLE` |'
- en: 'Table 6.1: Properties for the first field in the BigQuery table'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1：BigQuery表中第一个字段的属性
- en: '4. Repeat *step 3* to add more fields, until the schema looks like that shown
    in *Figure 6**.10*:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 重复*步骤3*以添加更多字段，直到架构看起来像*图6.10*中所示：
- en: '![Figure 6.10: Table schema](img/B18143_06_10.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10：表架构](img/B18143_06_10.jpg)'
- en: 'Figure 6.10: Table schema'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10：表架构
- en: 5. Select **Create table**.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 选择**创建表**。
- en: Now our table is ready for us to stream data into it, let’s move on to creating
    our data streaming pipeline.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的表已经准备好接收数据流，让我们继续创建我们的数据流管道。
- en: Creating a Dataflow job from a template
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从模板创建Dataflow作业
- en: 'We’re going to use a Dataflow template to create our first data streaming pipeline.
    To do this, perform the following steps:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Dataflow模板来创建我们的第一个数据流管道。为此，请执行以下步骤：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Dataflow** → **Jobs**.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单→**Dataflow**→**作业**。
- en: Select **Create job** **from template**.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**从模板创建作业**。
- en: For `taxi-data-raw`.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于`taxi-data-raw`。
- en: Select your preferred region.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您首选的区域。
- en: In the drop-down menu for **Dataflow template**, select the **Pub/Sub to** **BigQuery**
    template.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**Dataflow模板**下拉菜单中，选择**Pub/Sub到****BigQuery**模板。
- en: Note
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Pub/Sub has also released a direct integration with BigQuery, but considering
    that we want to illustrate how to use a Dataflow template, we’re using the Dataflow
    connection method in this chapter.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Pub/Sub还发布了与BigQuery的直接集成，但考虑到我们想要说明如何使用Dataflow模板，我们本章使用Dataflow连接方法。
- en: Next, in the **Input Pub/Sub topic** field, select **Enter** **topic manually**.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在**输入Pub/Sub主题**字段中，选择**手动输入主题**。
- en: 'Enter the following topic:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入以下主题：
- en: '[PRE3]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the `realtime` table you created in the previous section.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在上一节中创建的`realtime`表中。
- en: Click **SELECT** at the bottom of the screen.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕底部单击**选择**。
- en: 'In the **Temporary location** field, enter your desired storage location path
    in the following format (replace [BUCKET-NAME] with your bucket name): gs://[BUCKET-NAME]/dataflow.'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**临时位置**字段中，输入以下格式的所需存储位置路径（将[BUCKET-NAME]替换为您的桶名称）：gs://[BUCKET-NAME]/dataflow。
- en: Expand the **Optional Parameters** section and scroll down until you find the
    **Service account** **email** field.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 展开**可选参数**部分，并向下滚动，直到找到**服务帐户****电子邮件**字段。
- en: In that field, select the service account you created earlier in this chapter
    (it will contain `data-processing-sa` in the name if you used the suggested name).
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在该字段中，选择您在本章中较早创建的服务帐户（如果您使用了建议的名称，名称中将包含`data-processing-sa`）。
- en: Leave all other options at their default values.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有其他选项保留在默认值。
- en: Select **RUN JOB**.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**运行作业**。
- en: 'After a few minutes, you will see the job details appearing, and a graph that
    looks similar to the one shown in *Figure 6**.11*:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 几分钟后，您将看到作业详情出现，一个类似于*图6.11*所示的图形：
- en: '![Figure 6.11: Dataflow execution graph](img/B18143_06_11.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11：Dataflow执行图](img/B18143_06_11.jpg)'
- en: 'Figure 6.11: Dataflow execution graph'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11：Dataflow执行图
- en: Try clicking around on each of the steps and sub-steps in the graph to get a
    better understanding of what each step is doing.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在图中每个步骤和子步骤上点击，以更好地了解每个步骤的作用。
- en: After some time, you can head over to the BigQuery console and verify that the
    data is streaming into the table. Move on to the next section in order to do that.
    However, don’t close the Dataflow console yet because you will come back here
    after verifying the data in BigQuery.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一段时间后，您可以前往BigQuery控制台并验证数据是否正在流入表中。继续下一节以执行此操作。但是，请不要关闭Dataflow控制台，因为您将在验证BigQuery中的数据后返回这里。
- en: Verifying data in BigQuery
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证BigQuery中的数据
- en: 'To verify the data in BigQuery, perform the following steps:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证BigQuery中的数据，请执行以下步骤：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **BigQuery**.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单→**BigQuery**。
- en: In the top-left corner of the screen, you will see your project name. Click
    on the arrow symbol to the left of your project name to expand it (see *Figure
    6**.12* for reference).
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕的左上角，您将看到您的项目名称。单击项目名称左侧的箭头符号以展开它（参见*图6.12*以供参考）。
- en: Then, click on the arrow symbol to the left of your dataset name (`taxirides`)
    to expand it.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，单击数据集名称（`taxirides`）左侧的箭头符号以展开它。
- en: Select your `realtime` table.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您的`realtime`表。
- en: Select the **PREVIEW** tab.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**预览**选项卡。
- en: 'You should then see a screen that looks like the one shown in *Figure 6**.12*:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你应该会看到一个类似于*图6.12*所示的屏幕：
- en: '![Figure 6.12: BigQuery data](img/B18143_06_12.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12：BigQuery数据](img/B18143_06_12.jpg)'
- en: 'Figure 6.12: BigQuery data'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12：BigQuery数据
- en: When you have verified the data in BigQuery, you can go back to the Dataflow
    console and stop the Dataflow job by clicking **STOP** at the top of the screen.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当您在BigQuery中验证了数据后，您可以返回Dataflow控制台，通过点击屏幕顶部的**停止**来停止Dataflow作业。
- en: Now that you’ve seen how easy it is to set up a Dataflow job from a template,
    let’s move on to more complex Dataflow use cases.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了从模板设置Dataflow作业是多么容易，让我们继续探讨更复杂的Dataflow使用案例。
- en: Creating a Dataflow notebook
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据流笔记本
- en: 'Because we want to use a customized Apache Beam notebook, we will create a
    notebook in the Dataflow Workbench console. Perform the following steps to create
    the notebook:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们想使用定制的Apache Beam笔记本，所以我们将创建一个在Dataflow Workbench控制台中的笔记本。按照以下步骤创建笔记本：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Dataflow** → **Workbench**.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单→**Dataflow**→**Workbench**。
- en: At the top of the screen, select the **Instances** tab.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕顶部，选择**实例**标签。
- en: Now, at the top of the screen , select **Create New**.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在屏幕顶部，选择**创建新**。
- en: 'In the screen that appears (see *Figure 6**.13* for reference), you can either
    accept the default notebook name or create a name of your preference:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在出现的屏幕中（参见*图6.13*以获取参考），您可以选择接受默认的笔记本名称或创建您偏好的名称：
- en: '![Figure 6.13: Creating a user-managed notebook](img/B18143_06_13.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13：创建用户管理的笔记本](img/B18143_06_13.jpg)'
- en: 'Figure 6.13: Creating a user-managed notebook'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13：创建用户管理的笔记本
- en: Select your preferred region and zone. The zone selection doesn’t really matter
    in this case, but I recommend selecting the same region you have been using in
    previous activities in this book.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您首选的区域和区域。在这种情况下，区域选择并不重要，但我建议选择您在此书之前活动中一直使用的相同区域。
- en: Select **Continue**, and then **Continue** again.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**继续**，然后再次**继续**。
- en: Select E2 as the compute instance type.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择E2作为计算实例类型。
- en: You can also configure an idle timeout period after which the machine will automatically
    shut down if it is idle for that amount of time. This helps to save costs.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您还可以配置一个空闲超时周期，在此周期后，如果机器空闲这么长时间，它将自动关闭。这有助于节省成本。
- en: Select **Continue** multiple times until you reach the **IAM** and **security**
    screen
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**继续**多次，直到达到**IAM**和**安全**屏幕
- en: 'In the **IAM and security** screen, select the **Single user** option. See
    *Figure 6**.14* for reference:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**IAM和安全**屏幕中，选择**单用户**选项。参见*图6.14*以获取参考：
- en: '![Figure 6.14: Dataflow notebook – IAM and security](img/B18143_06_14.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图6.14：数据流笔记本 – IAM和安全](img/B18143_06_14.jpg)'
- en: 'Figure 6.14: Dataflow notebook – IAM and security'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14：数据流笔记本 – IAM和安全
- en: Note
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You may remember performing a similar step (that is, selecting the **Single
    user authentication** option) when we created our managed notebook in [*Chapter
    5*](B18143_05.xhtml#_idTextAnchor168). This allows us to directly use the notebook
    without needing to authenticate it as a service account beforehand.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能记得在[*第5章*](B18143_05.xhtml#_idTextAnchor168)中创建我们的托管笔记本时执行了类似的步骤（即选择**单用户身份验证**选项）。这允许我们直接使用笔记本，而无需事先将其作为服务账户进行身份验证。
- en: The **User email** box that appears should automatically be populated with your
    login email address. If not, enter the email address that you use for logging
    in to the Google Cloud console.
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 出现的**用户电子邮件**框应自动填充您的登录电子邮件地址。如果不是，请输入您用于登录Google Cloud控制台的电子邮件地址。
- en: Also, we want our notebook instance to use the service account we created earlier
    in this chapter, so **uncheck** the option that says **Use default Compute Engine
    service account on the VM to call Google** **Cloud APIs**.
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们希望我们的笔记本实例使用我们在本章早期创建的服务账户，所以**取消选中**表示**在VM上使用默认Compute Engine服务账户调用Google
    Cloud API**的选项。
- en: In the `data`, and you should see the name of the service account you created
    earlier in this chapter appearing in the list of available service accounts (assuming
    that you named it `data-processing-sa`, as recommended in that section).
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`data`中，你应该会在可用的服务账户列表中看到您在本章早期创建的服务账户的名称（假设您将其命名为`data-processing-sa`，如该部分中建议的）。
- en: Select that service account in the list.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列表中选择该服务账户。
- en: Also, take note of the **Pricing summary** option in the top-right corner of
    the screen. This is an estimate of how much it would cost if you were to leave
    the notebook running all month. Fortunately, you will only use it for a short
    time in this chapter. If you did not configure an idle shutdown period when creating
    the notebook, remember to shut it down when you’re finished using it.
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，请注意屏幕右上角的**定价摘要**选项。这是如果您整个月都保持笔记本运行，它将花费的估计金额。幸运的是，您在本章中只会使用很短的时间。如果您在创建笔记本时没有配置空闲关闭时间段，请记住在您完成使用后关闭它。
- en: You can leave all other options at their default values and select **Create**
    at the bottom of the screen.
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以将所有其他选项保留在默认值，并在屏幕底部选择**创建**。
- en: It will take a few minutes for the notebook instance to be created. When the
    instance creation has completed, you will see it in the list of user-managed notebooks,
    and an option to open JupyterLab will appear.
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建笔记本实例需要几分钟。当实例创建完成后，您将在用户管理的笔记本列表中看到它，并且将出现一个打开 JupyterLab 的选项。
- en: Select **Open Jupyterlab**.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**打开 Jupyterlab**。
- en: When the JupyterLab screen opens, it’s time to clone our repository into your
    notebook. This is similar to the process you performed in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168),
    but you have created a separate notebook instance in this chapter, so we need
    to clone the repository into this instance. The steps, again, to clone the repository,
    are as follows.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 JupyterLab 屏幕打开时，是时候将我们的仓库克隆到您的笔记本中了。这个过程与您在[*第 5 章*](B18143_05.xhtml#_idTextAnchor168)中执行的过程类似，但您在本章中创建了一个单独的笔记本实例，因此我们需要将仓库克隆到这个实例中。克隆仓库的步骤如下。
- en: 'Click on the **Git** symbol in the menu on the left of the screen. The symbol
    will look like the one shown in *Figure 6**.15*:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击屏幕左侧菜单中的**Git**符号。该符号看起来就像*图 6**.15*中所示的那样：
- en: '![Figure 6.15: Git symbol](img/B18143_06_15.jpg)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15：Git 符号](img/B18143_06_15.jpg)'
- en: 'Figure 6.15: Git symbol'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15：Git 符号
- en: Select **Clone Repository**.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**克隆仓库**。
- en: 'Enter our repository URL: [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects).'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入我们的仓库 URL：[https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects).
- en: If any options are displayed, leave them at their default values.
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果显示任何选项，请保留它们的默认值。
- en: Select **Clone**.
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**克隆**。
- en: You should see a new folder appear in your notebook, named `Google-Machine-Learning-for-Solutions-Architects`.
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您应该会在您的笔记本中看到一个名为 `Google-Machine-Learning-for-Solutions-Architects` 的新文件夹。
- en: Double-click on that folder, double-click on the `Chapter-06` folder within
    it, and then double-click on the `Streaming_NYC_Taxi_Data.ipynb` file to open
    it.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击该文件夹，然后双击其中的 `Chapter-06` 文件夹，最后双击 `Streaming_NYC_Taxi_Data.ipynb` 文件以打开它。
- en: 'In the **Select Kernel** screen that appears, select the latest version of
    Apache Beam. At the time of writing this, the latest available option in the launcher
    is Apache Beam 2.4.6 (see *Figure 6**.16* for reference):'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在出现的**选择内核**屏幕中，选择 Apache Beam 的最新版本。在撰写本文时，启动器中可用的最新选项是 Apache Beam 2.4.6（参见*图
    6**.16*以获取参考）：
- en: '![Figure 6.16: Selecting a notebook kernel](img/B18143_06_16.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.16：选择笔记本内核](img/B18143_06_16.jpg)'
- en: 'Figure 6.16: Selecting a notebook kernel'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16：选择笔记本内核
- en: The notebook we have opened contains a lot of Apache Beam Python code that we
    can use to process the data that’s streaming in from the public Pub/Sub topic.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打开的笔记本包含大量的 Apache Beam Python 代码，我们可以使用这些代码来处理从公共 Pub/Sub 主题流进的数据。
- en: Run each of the cells in the notebook, and read the explanations in the markdown
    and comments to understand what we’re doing. We’re using Apache Beam to define
    a streaming data processing pipeline that we will run in Google Cloud Dataflow.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行笔记本中的每个单元格，并阅读 markdown 和注释中的说明，以了解我们在做什么。我们正在使用 Apache Beam 定义一个将在 Google
    Cloud Dataflow 中运行的数据流处理管道。
- en: When you have completed running the cells in the notebook, you can go to the
    Dataflow Jobs console, and you will see your new pipeline running there. Give
    it a few minutes for the pipeline to start up and for data to flow through the
    pipeline. Just like before, I recommend that you click on various parts of the
    pipeline execution graph in order to get a better understanding of how the pipeline
    is structured. Next, let’s verify this new pipeline data in BigQuery, but again,
    don’t close the Dataflow console yet, because you will come back here after verifying
    the data in BigQuery.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成笔记本中单元格的运行后，你可以转到Dataflow作业控制台，你将看到你的新管道在那里运行。给管道启动和数据处理一些时间，就像之前一样，我建议你点击管道执行图的各种部分，以便更好地理解管道的结构。接下来，让我们在BigQuery中验证这个新管道的数据，但同样，不要关闭Dataflow控制台，因为你在验证BigQuery中的数据后将会回到这里。
- en: Verifying data in BigQuery
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证BigQuery中的数据
- en: To verify the data in BigQuery, open the BigQuery console, and under the `taxirides`
    dataset, you will see a new table that has been created by our custom Dataflow
    pipeline, called `run_rates`. Click on the `run_rates` values that were computed
    by our pipeline.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证BigQuery中的数据，请打开BigQuery控制台，在`taxirides`数据集下，你会看到一个由我们的自定义Dataflow管道创建的新表，称为`run_rates`。点击由我们的管道计算出的`run_rates`值。
- en: When you have verified the data in BigQuery, you can go back to the Dataflow
    console and stop the Dataflow job by clicking **STOP** at the top of the screen.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在BigQuery中验证了数据后，你可以回到Dataflow控制台，通过点击屏幕顶部的**停止**来停止Dataflow作业。
- en: 'Now that we have completed the activities in this section, you can shut down
    your user-managed notebook by performing the following steps:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了本节的活动，你可以通过以下步骤关闭你的用户管理的笔记本：
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **Dataflow** → **Workbench**.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Google Cloud控制台中，导航到**Google Cloud服务**菜单 → **Dataflow** → **工作台**。
- en: At the top of the screen, select the **User-managed** **notebooks** tab.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在屏幕顶部，选择**用户管理**的**笔记本**标签。
- en: Select the checkbox next to your notebook name, and click **STOP** at the top
    of the screen (just above the **User-managed** **notebooks** tab).
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择你的笔记本名称旁边的复选框，并在屏幕顶部（在**用户管理**的**笔记本**标签上方）点击**停止**。
- en: The notebook will shut down after a few minutes.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本将在几分钟后关闭。
- en: If everything worked as expected, you have now successfully created a custom
    pipeline that processes and transforms streaming data in flight!
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切按预期进行，你现在已经成功创建了一个自定义管道，该管道可以处理和转换飞行中的流数据！
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to ingest data into Google Cloud from various
    sources, and you discovered important concepts on how to process data in Google
    Cloud.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何从各种来源将数据导入Google Cloud，并且你发现了在Google Cloud中处理数据的重要概念。
- en: You then learned about exploring and visualizing data using Vertex AI and BigQuery.
    Next, you learned how to clean and prepare data for ML workloads using Jupyter
    notebooks, and then how to create an automated data pipeline to perform the same
    transformations at a production scale in a batch method using Apache Spark on
    Google Cloud Dataproc, as well as how to automatically orchestrate that entire
    process using Apache Airflow in GCC.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 你随后学习了如何使用Vertex AI和BigQuery探索和可视化数据。接下来，你学习了如何使用Jupyter笔记本清理和准备数据以供ML工作负载使用，然后是如何在Google
    Cloud Dataproc上使用Apache Spark以批量方法创建自动数据管道，以在生产规模上执行相同的转换，以及如何使用Apache Airflow在GCC中自动编排整个流程。
- en: We then covered important concepts and tools related to processing streaming
    data, and you finally built your own streaming data processing pipelines using
    Apache Beam on Google Cloud Dataflow.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后介绍了与处理流数据相关的重要概念和工具，而你最终使用Apache Beam在Google Cloud Dataflow上构建了自己的流数据处理管道。
- en: In the next chapter, we will spend additional time on data processing and preparation,
    with a specific focus on the concept of feature engineering.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将花更多的时间在数据处理和准备上，特别关注特征工程的概念。
