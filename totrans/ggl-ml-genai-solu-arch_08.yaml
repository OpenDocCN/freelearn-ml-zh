- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diving Deeper – Preparing and Processing Data for AI/ML Workloads on Google
    Cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we did some very rudimentary data exploration by looking
    at a few details relating to our dataset, using functions such as `pandas.DataFrame.info()`
    and `pandas.DataFrame.head()`. In this chapter, we will dive deeper into the realm
    of data exploration and preparation for data science workloads, as represented
    by the section highlighted in blue in the data science life-cycle diagram shown
    in *Figure 6**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: Data exploration and processing](img/B18143_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: Data exploration and processing'
  prefs: []
  type: TYPE_NORMAL
- en: In the early stages of a typical data science project, you would likely perform
    many of the data exploration and preparation steps in Jupyter notebooks, which,
    as we have seen, are useful for experimenting with small datasets. When you bring
    your workload into production, however, you are likely to use much larger datasets,
    in which case you would usually need to use different tools for processing your
    data. Google is seen as an industry leader when it comes to large-scale data processing,
    analytics, and AI/ML. For example, Google Cloud BigQuery is well established as
    one of the most popular data warehouse services in the industry, and Google Cloud
    has many additional industry-leading services for data processing and analytics
    workloads.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to explore, visualize, and prepare data for
    ML use cases with tools such as Vertex AI, BigQuery, Dataproc, and Cloud Composer.
    Furthermore, we will dive into processing streaming data on the fly with Dataflow,
    and introduce the fundamentals of data pipelines. By the end of this chapter,
    you will be able to create, build, and run data pipelines on Google Cloud, equipping
    you with the necessary skills to take on complex data processing tasks in today’s
    fast-paced, data-driven world.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites and basic concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting data into Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring and visualizing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning and preparing the data for ML workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to data pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing batch and streaming data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and running data pipelines on Google Cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by discussing the prerequisites for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for this chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The activities in this section need to be completed before we can start to perform
    the primary activities in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the methods we discussed in earlier chapters to enable Google
    Cloud APIs, such as the Google Cloud Shell or being prompted in the Google Cloud
    console, you can also proactively search for an API in order to enable it in the
    console. To do that, you would perform the following steps, in which *[service/API
    name]* is the name of the Service/API you wish to enable:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **APIs & Services** → **Library**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search for *[service name]* in the search box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the API from the list of results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the page that displays information about the API, click **Enable**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Perform the preceding steps for each of the following service/API names:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute Engine API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Scheduler API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataflow API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Pipelines API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Dataproc API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Composer API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Pub/Sub API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After you have enabled the required APIs, we’re ready to move on to the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: IAM permissions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will set up the **identity and access management** (**IAM**)
    permissions required to enable the activities that come later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Service accounts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In previous chapters, I mentioned that there are multiple ways to authenticate
    with Google Cloud services and that the API keys you used in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146)
    were the simplest authentication methods. Now that we’re advancing to more complex
    use cases, we will begin to use a more advanced form of authentication, referred
    to as service accounts.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud service accounts are special accounts used by Google Cloud services,
    applications, and **virtual machines** (**VMs**) to interact with and authenticate
    to other Google Cloud resources. They are an interesting concept because, in addition
    to being resources, they are also considered to be identities or principals, just
    like people, and just like people, they have their own email addresses and permissions
    associated with them. However, these email addresses and permissions apply to
    the machines and applications that use them, rather than to people. Service accounts
    provide a way for machines and applications to have an identity when those systems
    want to perform an activity that requires authentication with Google Cloud APIs
    and resources.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to create a service account with the required permissions for the
    activities we will perform in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Data processing service account
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Considering that we’re going to use multiple Google Cloud services to process
    data in various ways in this chapter, we will create a service account with permissions
    mainly related to Google Cloud data processing services, such as BigQuery, Cloud
    Composer, Dataflow, Dataproc, **Google Cloud Storage** (**GCS**), and Pub/Sub.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to create the required service account:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **IAM & Admin** → **Service accounts**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create** **service account**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the service account name, enter `data-processing-sa`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the section titled **Grant this service account access to project**, add
    the roles shown in *Figure 6**.2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.2: Dataflow worker service account permissions](img/B18143_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Dataflow worker service account permissions'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Done**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our service account is now ready to be used later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'In the service account we just created, we also added the **Service Account
    User** permission. This is because, in some of the use cases we will implement,
    our service account will also need to temporarily impersonate or “act as” other
    service accounts or identities. For more information on the concept of impersonating
    service accounts, see the following documentation: [https://cloud.google.com/iam/docs/service-account-permissions#directly-impersonate](https://cloud.google.com/iam/docs/service-account-permissions#directly-impersonate).'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Storage bucket folders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use a Cloud Storage bucket to store data for the activities later in
    this chapter. We already created a bucket in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146),
    so we can simply add some folders to the bucket for storing our data. Perform
    the following steps to create the folders:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Cloud Storage** → **Buckets**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on the name of the bucket you created in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create folder**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name it `data`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Repeat the preceding steps with the following additional folder names:'
  prefs: []
  type: TYPE_NORMAL
- en: '`code`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dataflow`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pyspark-airbnb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our folders are now ready to store our data.
  prefs: []
  type: TYPE_NORMAL
- en: Uploading data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use a file named `AB_NYC_2019.csv` as a dataset for some of the activities
    in this chapter. In the clone of our GitHub repository that you created on your
    local machine in [*Chapter 4*](B18143_04.xhtml#_idTextAnchor146), you will find
    that file in a directory named `data`, which exists within the directory named
    `Chapter-06`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, you should find the file at the following path on your local machine
    (the slashes will be reversed if you’re using Microsoft Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: '`[Location in which you cloned our` `GitHub repository]/``Chapter-06``/data/AB_NYC_2019.csv`'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to upload this file, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Navigate into the `data` folder you created in GCS in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Upload files**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, select the `AB_NYC_2019.csv` file by navigating to the `Chapter-06``/data`
    directory in the clone of our GitHub repository that you created on your local
    machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we’ve completed the prerequisites, let’s discuss some important industry
    concepts that we will dive into in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental concepts in this chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we aim to provide you with knowledge not only regarding how to
    use the relevant Google Cloud services for various types of workloads but also
    regarding important industry concepts that relate to each of the relevant technologies.
    In this section, we briefly cover concepts that provide additional context for
    this chapter’s learning activities.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting data into Google Cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In previous chapters, you already performed steps to upload data to GCS and
    Google Cloud BigQuery. In addition to performing bulk uploads to GCS and BigQuery,
    it’s also possible to stream data into those services. You will see this in action
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This section provides a more holistic overview of Google Cloud’s data ingestion
    options. We will only cover Google Cloud’s services here, but there are also countless
    options of third-party database and data management services that you can run
    on Google Cloud, such as MongoDB, Cassandra, Neo4j, and many others that are available
    through the Google Cloud Marketplace.
  prefs: []
  type: TYPE_NORMAL
- en: For streaming data use cases, which we will describe in more detail later in
    this chapter, you can use Cloud Pub/Sub to ingest data from sources such as IoT
    devices or website clickstream feeds. Dataflow can also be used to ingest data
    from various sources, transform it, and write it to other Google Cloud services
    such as BigQuery, Bigtable, or Cloud Storage. As we will discuss later in this
    chapter, Dataflow also supports batch data processing use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Dataproc can be used to ingest data from **Hadoop Distributed File
    System** (**HDFS**)-compatible sources or other distributed storage systems, and
    Google Cloud Data Fusion can also be used to ingest data from various sources,
    transform it, and write it to most Google Cloud storage and database services.
  prefs: []
  type: TYPE_NORMAL
- en: For relational database data, you can use the Google Cloud **Database Migration
    Service** (**DMS**) to ingest data into Google Cloud SQL, which is a fully managed
    relational database service for MySQL, PostgreSQL, and SQL Server. You can also
    use standard SQL clients, import/export tools, and third-party database migration
    tools to ingest data into Cloud SQL instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'For non-relational database data, there are multiple options, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Bigtable, which is a fully managed, scalable NoSQL database for
    large-scale, low-latency workloads. You can use the Bigtable HBase API or the
    Cloud Bigtable client libraries to ingest data into Bigtable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud Firestore, which is a fully managed, serverless NoSQL document
    database for web and mobile applications. Firestore provides client libraries,
    REST APIs, or gRPC APIs to ingest data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Some of you may also be familiar with Google Cloud Datastore, which was a separate
    Google Cloud non-relational database offering but has been somewhat merged with
    Google Cloud Firestore. To see more details regarding how these database options
    relate to each other, please see the Google Cloud documentation at the following
    link: [https://cloud.google.com/datastore/docs/firestore-or-datastore](https://cloud.google.com/datastore/docs/firestore-or-datastore).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve covered many of the options for ingesting data into Google Cloud,
    let’s discuss what kinds of things we may want to do with that data after ingesting
    it, starting with data transformation approaches, such as **Extract, Transform,
    Load** (**ETL**) and **Extract, Load,** **Transform** (**ELT**).
  prefs: []
  type: TYPE_NORMAL
- en: ETL and ELT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ETL and ELT are two approaches for data integration, processing, and storage.
    In ETL, data is first extracted from source systems and then transformed into
    another format. Such transformations could include cleansing, enrichment, aggregation,
    or deduplication. These transformations usually take place within an intermediary
    processing engine or staging area. Once the data is transformed, it is loaded
    into the target system, which is typically a data warehouse or a data lake. ETL
    is a traditional approach that is well suited for environments where data consistency
    and quality are critical. However, it can be time-consuming and resource-intensive
    due to the processing steps happening before the data is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, ELT reverses the order of the last two steps. Data is first
    extracted from source systems and then directly loaded into the target system,
    such as a modern data warehouse or data lake. The transformation step is performed
    within the target system itself, using the processing capabilities of the destination
    system. ELT has gained popularity in recent years due to the increased scalability
    and processing power of modern cloud-based data warehouses, as it enables faster
    loading and allows users to perform complex transformations on demand.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between ETL and ELT depends on the specific requirements of the data
    processing environment, data quality needs, and the target system’s capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Batch and streaming data processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At a high level, there are two main ways in which we can process data: batch
    and streaming. In this section, we explain each of these approaches, which will
    equip us with the knowledge we need for the activities that follow in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Batch data processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Batch data processing usually refers to performing a series of transformations
    on large datasets in a bulk manner. These kinds of pipelines are suitable for
    use cases in which you need to process large amounts of data in parallel over
    a period of hours or even days. As an example, imagine that your company runs
    an online retail business that has separate deployments in different geographical
    regions, such as a *www.example.com* website in North America, a *www.example.co.uk*
    website in the UK, and a *www.example.cn* website in China. Every night, you may
    want to run a workload that takes all of the items purchased by all customers
    in each region that day, merges that data across regions, and then feeds that
    data into an ML model. This would be an example of a batch workload. Other examples
    of batch data processing use cases include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Processing daily sales transactions to generate reports for business decision-making
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing log files from a web server to understand user behavior over a specific
    time window (for example, a day or week)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running large-scale data transformation jobs, such as converting raw data into
    a structured format for downstream data systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to data processing tasks that only need to be performed periodically,
    companies may also need to process data in real time or near real time, which
    brings us to the topic of streaming data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Streaming data processing, as the name implies, involves working on a stream
    of data that continuously flows into a system, usually on an ongoing basis. Rather
    than processing an enormous dataset as a single job, the data in a streaming processing
    use case usually consists of small pieces of data that are processed in flight.
    Examples of streaming data processing include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing social media feeds to identify trends or detect events (for example,
    **sentiment analysis** (**SA**), hashtag tracking)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and analyzing IoT sensor data in real time to detect anomalies, trigger
    alerts, or optimize processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing financial transactions in real time for fraud detection and prevention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve discussed the two main high-level categories of data processing
    use cases, let’s start diving into how we actually implement these use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Data pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data pipelines are how we automate the concepts we just described, such as large-scale
    ETL/ELT or streaming data transformations. Data pipelines are essential for organizations
    that handle large volumes of data or require complex data processing workloads,
    as they help to streamline data management at scale. Without such automation pipelines,
    employees would spend a lot of time performing mundane and repetitive but complex
    and error-prone data processing activities. There are multiple Google Cloud services
    that can be used for batch and streaming data pipelines, which you will learn
    to use in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered some of the fundamental concepts, it’s time to start
    diving into some practical data processing activities.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start to build automated data processing pipelines, however, we will
    first need to explore our data so that we can understand what kinds of transformations
    we would want to implement in our pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'While, in previous chapters, we included the code directly in the pages of
    this book, we are now advancing to more complex use cases that require a lot of
    code that would not be suitable to include directly in the pages of this book.
    Please review the code artifacts in the GitHub repository related to this chapter
    to understand the code we are using to implement these steps: [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects).'
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the rest of the book, I will continue to include code directly where
    it makes sense.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring, visualizing, and preparing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Use case*: We’re planning a trip to New York City, and we want to get an idea
    of what the best accommodation options would be. Rather than perusing through
    and evaluating lots of individual Airbnb postings, we’re going to download lots
    of the reviews and do some bulk data analysis and data processing to get some
    insights.'
  prefs: []
  type: TYPE_NORMAL
- en: We can use the Vertex AI Workbench notebook that we created in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168)
    for this purpose. Please open JupyterLab on that notebook instance. In the directory
    explorer on the left side of the screen, navigate to the `Chapter-6` directory
    and open the`Chapter-6-Airbnb.ipynb` notebook. You can choose **Python (Local)**
    as the kernel. As you did in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168),
    run each cell in the notebook by selecting the cell and pressing *Shift* + *Enter*
    on your keyboard.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the notebook, we use markdown cells to describe each step in detail so that
    you can understand each step in the process. We use libraries such as `pandas`,
    `matplotlib`, and `seaborn` to summarize and visualize the contents of our dataset,
    and then we perform data cleaning and preparation activities such as filling in
    missing values, removing outliers, and removing features that are not likely to
    be useful for training a regression model to predict accommodation prices. *Figure
    6**.3* shows an example of one of our data visualization graphs, in which we view
    the range and distribution of prices for the listings in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Distribution of prices in the dataset](img/B18143_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Distribution of prices in the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the majority of the accommodation options cost less than $200
    per night, but there are some data points (although not many) between $600 and
    $1,000 per night. Those are either very expensive accommodation options or they
    could be potential outliers/errors in the data. You can see additional data visualization
    graphs in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regarding data cleanup activities, to clean up potential pricing outliers,
    for example, we use the following piece of code to set a limit of $800 (although
    still high) and remove any listings above that nightly rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To remove features that are not likely to be useful for training a regression
    model to predict accommodation prices, we use the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: These are just a couple of examples of the data preparation steps we perform
    in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: When you have completed executing all of the activities in the notebook, we
    will move on to see how we can turn those activities into an automated pipeline
    in production. We will start by implementing a batch data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Batch data pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve used our Jupyter notebook to explore our data and figure out
    what kinds of transformations we want to perform on our dataset, let’s imagine
    that we want to turn this into a production workload that can run automatically
    on very large files, without needing any further human effort. As I mentioned
    earlier, this is essential in any company that implements large-scale data analytics
    and AI/ML workloads. It’s simply not feasible to get somebody to manually perform
    those transformations every time, and for very large data volumes, the transformations
    could not be performed on a notebook instance. For example, imagine we get thousands
    of new postings every day, and we want to automatically prepare that data for
    an ML model. We can do this by creating an automated pipeline to perform data
    transformations every night (or however often we wish).
  prefs: []
  type: TYPE_NORMAL
- en: Batch data pipeline concepts and tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we start diving in and building our batch data processing pipeline, let’s
    first cover some important concepts and tools in this domain.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Spark is a highly popular development and execution framework that can
    be used to implement very large-scale data processing workloads and other types
    of large-scale computing use cases such as ML. Its power lies both in its in-memory
    processing capabilities and its ability to implement multiple large computing
    and data processing tasks in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: While Spark can be used for both batch and streaming (or micro-batching) data
    processing workloads, we’re going to use it to perform our batch data transformations
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Dataproc
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), Google Cloud
    Dataproc is a fully managed, fast, and easy-to-use service for running Apache
    Spark and Apache Hadoop clusters on GCP. In this chapter, we will use it to execute
    our Spark processing jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Airflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Airflow is an open source platform used for orchestrating complex data
    workflows. It was created by Airbnb and later contributed to the **Apache Software
    Foundation** (**ASF**). Airflow is designed to help developers and data engineers
    create, schedule, monitor, and manage workflows, making it easier to handle tasks
    that depend on one another. It is commonly used in data engineering and data science
    projects for tasks such as ETL, ML pipelines, and data analytics, and it is widely
    used by organizations across various industries, making it a popular choice for
    managing complex data workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Directed acyclic graphs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Airflow represents workflows as **directed acyclic graphs** (**DAGs**), which
    consist of tasks and their dependencies. Each task in the workflow is represented
    as a node, and the dependencies between tasks are represented as directed edges.
    This structure ensures that tasks are executed in a specific order without creating
    loops, as depicted in *Figure 6**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: A simple DAG  (excerpted from source:  https://www.flickr.com/photos/dullhunk/4647369097)](img/B18143_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: A simple DAG (excerpted from source: https://www.flickr.com/photos/dullhunk/4647369097)'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.4*, we can see that tasks *b*, *c*, *d*, and *e* all depend on
    task *a*. Similarly, task *d* also depends on tasks *b* and *c*, and task *e*
    also depends on tasks *c* and *d*.
  prefs: []
  type: TYPE_NORMAL
- en: In Airflow, a DAG is defined in a Python script, which represents tasks and
    their dependencies as code.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Composer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Google Cloud Composer** (**GCC**) is a fully managed workflow orchestration
    service built on Apache Airflow. It allows you to create, schedule, and monitor
    data workflows across various Google Cloud services, as well as on-premises or
    multi-cloud environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Composer simplifies the process of setting up and managing Apache Airflow
    by providing an easy-to-use interface and automating the infrastructure management.
    This allows you to focus on creating and maintaining your workflows while Google
    takes care of the underlying infrastructure, scaling, and updates.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the important concepts for implementing batch data pipelines,
    let’s start building our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Building our batch data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will create our Spark job and run it on Google Cloud Dataproc
    and will use GCC to orchestrate our job. What this means is that we can get GCC
    to automatically run our job every day. Each time it runs our job, it will create
    a Dataproc cluster, execute our Spark job, and then delete the Dataproc cluster
    when our job completes. This is a standard best practice that companies use to
    save money because you should not have computing resources running when you are
    not using them. The architecture of our pipeline on Google Cloud is shown in *Figure
    6**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: Batch data pipeline architecture](img/B18143_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Batch data pipeline architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin by setting up Cloud Composer.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Composer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will set up Cloud Composer to schedule and run our batch
    data processing pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Composer environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Everything we do in Cloud Composer happens within a Cloud Composer environment.
    To set up our Cloud Composer environment, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Composer** → **Environments**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create Environment**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If prompted, select **Composer 2**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the screen that appears, enter a name for your Composer environment. See
    *Figure 6**.6* for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.6: Creating a Composer environment](img/B18143_06_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Creating a Composer environment'
  prefs: []
  type: TYPE_NORMAL
- en: Select your preferred region. (Remember that it’s better if you use the same
    region for each activity throughout this book, if possible.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the latest image version. See *Figure 6**.6* for reference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*IMPORTANT*: Select the service account you created earlier in this chapter
    (if you used the suggested name, then it will include `data-processing-sa` in
    the name).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Environment resources** section, select a small environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave all other options at their default values and select **Create**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The environment will take up to 25 minutes to spin up.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While waiting for the environment to spin up, let’s move on to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Composer Python code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will review and prepare the Python code for our Cloud Composer
    Spark workload. There are two code resources that we will use with Cloud Composer,
    which can be found in our GitHub repository ([https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/tree/main/](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/tree/main/Chapter-6)Chapter-06):'
  prefs: []
  type: TYPE_NORMAL
- en: '`composer-dag.py`, which contains the Python code that defines our Cloud Composer
    Airflow DAG'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`chapter-6-pyspark.py`, which contains the PySpark code that defines our Spark
    job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform the following steps to start preparing those files for use with Cloud
    Composer:'
  prefs: []
  type: TYPE_NORMAL
- en: Locate those files in the clone of our GitHub repository that you created on
    your local machine, and open them for editing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `chapter-6-pyspark.py` file, you just need to update the storage locations
    for the source and destination datasets. To do that, search for the `GCS-BUCKET-NAME`
    string in the file and replace it with your own GCS bucket name that you created
    earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IMPORTANT
  prefs: []
  type: TYPE_NORMAL
- en: The **GCS-BUCKET-NAME** string exists in two locations in the file (once near
    the beginning, and again near the end). One location specifies the source dataset,
    and the other location specifies the destination where our Spark job will save
    the processed data. Replace both occurrences of the string with your own GCS bucket
    name.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `composer-dag.py` file, you will see the following block of variables
    near the beginning of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: All of those variables need to be updated with specific values from your GCP
    project. The comments in the file provide additional details on the replacements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition to making the aforementioned changes, review the contents of the
    code to get an understanding of how Cloud Composer will execute our jobs. The
    code contains comments to describe what it’s doing in each section so that you
    can understand how it works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you have completed the preceding steps, we’re ready to upload the resources
    to be used by Cloud Composer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Cloud Composer requires the preceding code resources to be stored in GCS, but
    in two separate locations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For `chapter-6-pyspark.py`: Upload this file in the `code` folder you created
    earlier in GCS. To do that, navigate into the `code` folder you created, select
    `chapter-6-pyspark.py``composer-dag.py`: This file will be uploaded to a special
    folder that Cloud Composer will create. When your Cloud Composer environment is
    fully created, click on the name of your newly created environment in the Cloud
    Composer console, and your environment details screen will open. Near the top
    of the screen, select `composer-dag.py` file from the clone of our GitHub repository
    that you created on your local machine (that is, the file you edited in the previous
    steps).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s it! As soon as the `composer-dag.py` file is uploaded, Cloud Composer
    will completely automate everything for you. It will take a few minutes, but Cloud
    Composer will create your DAG, and you will see it appearing in the Cloud Composer
    console. It will then execute the DAG, meaning that it will create a Dataproc
    cluster, execute a Spark job to perform the data transformations we specified,
    and delete the Dataproc cluster when the job completes.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the various tasks happening in the Composer and Dataproc consoles
    (give it some time in each case), and the final test will be to verify that the
    processed data appears in the GCS destination you specified in the PySpark code.
  prefs: []
  type: TYPE_NORMAL
- en: When you have completed all of the aforementioned steps and you no longer need
    your Composer environment, you can delete the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting the Cloud Composer environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Perform the following steps to delete the Composer environment:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Composer** → **Environments**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the checkbox next to the name of your environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select **DELETE** at the top of the screen. See *Figure 6**.7* for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.7: Deleting the Composer environment](img/B18143_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Deleting the Composer environment'
  prefs: []
  type: TYPE_NORMAL
- en: Select **DELETE** in the confirmation screen that appears.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will take a few minutes for the environment to delete, after which it will
    disappear from your list of environments.
  prefs: []
  type: TYPE_NORMAL
- en: Awesome work! You have officially created your first data processing pipeline
    on Google Cloud!
  prefs: []
  type: TYPE_NORMAL
- en: Note that the method we used in this chapter is an example of one (very popular)
    pattern for using multiple Google Cloud services to implement a data pipeline.
    Google Cloud also provides other products that could be used to implement similar
    outcomes, such as Google Cloud Data Fusion, which enables you to create pipelines
    using a visual user interface. We will explore other services in later chapters
    of this book, and one other important offering that was launched by Google Cloud
    in 2023 is Serverless Spark, which we’ll briefly discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Serverless Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google Cloud Serverless Spark is a fully managed, serverless Apache Spark service
    that makes it easy to run Spark jobs without having to provision or manage any
    infrastructure. It also automatically scales your jobs up or down based on demand,
    so you only pay for the resources that you use, which makes it a cost-effective
    way to run Spark jobs, even for short-lived or intermittent workloads.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also integrated with other Google Cloud services, such as BigQuery, Dataflow,
    Dataproc, and Vertex AI, and popular open-source tools such as Zeppelin and Jupyter
    Notebook, making it easy to explore and analyze data and build and run **end-to-end**
    (**E2E**) data pipelines directly with those services.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Serverless Spark on Dataproc, for example, you can select from pre-made
    templates to easily perform common tasks such as moving and transforming data
    between **Java Database Connectivity** (**JDBC**) or Apache Hive data stores and
    GCS or BigQuery, or you can build your own Docker containers that Serverless Spark
    will run in order to implement custom data processing workloads. For more information
    on how to develop such custom containers, see the following Google Cloud documentation:
    [https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers](https://cloud.google.com/dataproc-serverless/docs/guides/custom-containers).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned how to build a batch data processing pipeline, we will
    move on to implementing streaming data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will deal with a different kind of data source and will
    learn about the differences in processing data in real time versus the batch-oriented
    methods we used in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data pipeline concepts and tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Again, before we start to build a streaming data processing pipeline, there
    are some important concepts and tools that we need to introduce and understand.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Beam
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Beam is an open source, unified programming model for processing and
    analyzing large-scale data in batch and streaming modes. It was initially developed
    by Google as a part of its internal data processing tools, and later, it was donated
    to the ASF. Beam provides a unified way to write data processing pipelines that
    can be executed on various distributed processing backends such as Apache Flink,
    Apache Samza, Apache Spark, Google Cloud Dataflow, and others. It supports multiple
    programming languages, including Java, Python, and Go, and it allows developers
    to write both batch and streaming data processing pipelines using a single API,
    which simplifies the development process and enables seamless switching between
    batch and streaming modes. It also provides a rich set of built-in I/O connectors
    for various data sources and sinks, including Kafka, Hadoop, Google Cloud Pub/Sub,
    BigQuery, and others. Additionally, developers can build their own custom connectors
    if needed. In this chapter, we will use Apache Beam on Google Cloud Dataflow to
    create a pipeline to process data in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Beam concepts
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we discuss some basic concepts of the Apache Beam programming
    model, which include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pipelines**: Just like the concept of a pipeline in Apache Airflow, which
    we used earlier in this chapter, an Apache Beam pipeline is a DAG representing
    a sequence of data processing steps or the overall data processing workflow in
    Apache Beam workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PCollections**: A **Parallel Collection** (**PCollection**) is an immutable
    distributed dataset representing a collection of data elements. It is the primary
    data structure used in Apache Beam pipelines to hold and manipulate data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PTransforms**: A **Parallel Transform** (**PTransform**) is a user-defined
    operation that takes one or more PCollections as input, processes the data, and
    produces one or more PCollections as output. PTransforms are the building blocks
    of a pipeline and define the data processing logic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Windowing**: Windowing is a mechanism that allows grouping data elements
    in a PCollection based on timestamps or other criteria. This concept is particularly
    useful for processing unbounded datasets in streaming applications, where data
    elements need to be processed in finite windows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Watermarks**: Watermarks are a way to estimate the progress of time in a
    streaming pipeline, and they help to determine when it’s safe to emit results
    for a particular window.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Triggers**: A trigger determines when to aggregate the results of each window,
    based on factors such as the arrival of a certain number of data elements, the
    passage of a certain amount of time, or the advancement of watermarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runners**: Runners are the components responsible for executing a Beam pipeline
    on a specific execution engine or distributed processing platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Beam model decouples the pipeline definition from the underlying execution
    engine, allowing users to choose the most suitable platform for their use case.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Dataflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We introduced Dataflow in [*Chapter 3*](B18143_03.xhtml#_idTextAnchor059), and
    now we’ll dive deeper into it. Dataflow is a Google Cloud service on which you
    can run Apache Beam. In other words, it provides one of the execution environments
    or runners on which you can run Apache Beam workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Dataflow provides multiple features for various types of use cases, and in this
    section, we’ll briefly discuss the various options and their applications.
  prefs: []
  type: TYPE_NORMAL
- en: Dataflow Data Pipelines and Dataflow Jobs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: People often get confused about the difference between Dataflow Data Pipelines
    and Dataflow Jobs. The best way to look at this is that Dataflow Data Pipelines
    refer to the definition of a data pipeline, which could be executed on a recurring
    basis, whereas Dataflow Jobs refer to a single execution of a data pipeline. The
    reason this can be confusing is that you can create a new pipeline definition
    either in the Dataflow Jobs console or in the Dataflow Data Pipelines console.
  prefs: []
  type: TYPE_NORMAL
- en: In either case, when creating a pipeline definition, we have the option to use
    predefined templates that cover common types of tasks that people often want to
    do with Dataflow, such as transferring data from BigQuery to Bigtable, or from
    Cloud Spanner to Pub/Sub, and there are lots of different templates to choose
    from, covering a wide variety of data sources and destinations. These templates
    make it very easy for us to implement a data transfer workload without requiring
    much or any development effort on our part. Alternatively, if we have more complex
    data processing needs that are not included in one of the standard templates,
    then we can create our own custom pipeline definitions. We will look at both options
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Dataflow Workbench notebooks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One way in which we can develop custom data processing pipelines is by using
    Dataflow Workbench notebooks. This may sound somewhat familiar, because you may
    remember creating and using a Vertex AI Workbench notebook in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168).
    In the Dataflow Workbench console, we can create notebooks that come with Apache
    Beam already installed. We will create a notebook in the Dataflow Workbench console
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Dataflow snapshots
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dataflow snapshots save the state of a streaming pipeline, which allows you
    to start a new version of your Dataflow job without losing state. This is useful
    for backup and recovery, testing, and rolling back updates to streaming pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: SQL Workspace
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Dataflow console also includes a built-in SQL Workspace that enables you
    to run SQL queries directly from the console and send the results to BigQuery
    or Pub/Sub. This can be useful for ad hoc use cases in which you want to simply
    run a SQL query to fetch information from a given source and store the results
    in one of the supported destinations.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered the important concepts for implementing streaming data
    pipelines, let’s start building our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Building our streaming data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sticking with the theme of planning our trip to New York City, the activities
    in the previous sections of this chapter gave us some good insights into what
    kinds of accommodation options are available to us, and now we want to assess
    our transportation options; specifically, how much it’s likely to cost us to travel
    around in taxis while we’re there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our streaming data pipeline will take input data from Google Cloud Pub/Sub,
    perform some processing in Dataflow, and place the outputs into BigQuery for analysis.
    The architecture of our pipeline on Google Cloud is shown in *Figure 6**.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8: Streaming data pipeline](img/B18143_06_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Streaming data pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud provides a public stream of data that can be used to test these
    kinds of stream processing workloads, which contains information relating to New
    York City taxi rides, and which we will use in our example in this section. Let’s
    start by creating a destination for our streamed data, also referred to as a **sink**
    for our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a BigQuery dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use Google Cloud BigQuery as a storage system for our data. To get
    started, we first need to define a dataset in BigQuery that we will use as our
    pipeline destination. To do this, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **BigQuery**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the top-left corner of the screen, you will see your project name. Click
    on the symbol of three vertical dots to the right of your project name (see *Figure
    6**.9* for reference):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.9: BigQuery project menu](img/B18143_06_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: BigQuery project menu'
  prefs: []
  type: TYPE_NORMAL
- en: In the menu that gets displayed, select **Create dataset**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Give your dataset a name: `taxirides`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your preferred region, and select **Create dataset**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we’ve created our dataset, we will need to create a table within that
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a BigQuery table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A BigQuery dataset generally contains one or more tables that contain the actual
    data. Let’s create a table into which our data will be streamed. To do this, perform
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the BigQuery editor, click on the `taxirides` dataset you just created and
    select **Create table**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `realtime` as the table name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Schema** section, click the plus sign (**+**) to add a new field.
    Our first field has the following properties (leave all other options for each
    field at their default values):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '| **Field name** | **Type** | **Mode** |'
  prefs: []
  type: TYPE_TB
- en: '| `ride_id` | `STRING` | `NULLABLE` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6.1: Properties for the first field in the BigQuery table'
  prefs: []
  type: TYPE_NORMAL
- en: '4. Repeat *step 3* to add more fields, until the schema looks like that shown
    in *Figure 6**.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10: Table schema](img/B18143_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: Table schema'
  prefs: []
  type: TYPE_NORMAL
- en: 5. Select **Create table**.
  prefs: []
  type: TYPE_NORMAL
- en: Now our table is ready for us to stream data into it, let’s move on to creating
    our data streaming pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Dataflow job from a template
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’re going to use a Dataflow template to create our first data streaming pipeline.
    To do this, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Dataflow** → **Jobs**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Create job** **from template**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For `taxi-data-raw`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your preferred region.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the drop-down menu for **Dataflow template**, select the **Pub/Sub to** **BigQuery**
    template.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Pub/Sub has also released a direct integration with BigQuery, but considering
    that we want to illustrate how to use a Dataflow template, we’re using the Dataflow
    connection method in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, in the **Input Pub/Sub topic** field, select **Enter** **topic manually**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the following topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the `realtime` table you created in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **SELECT** at the bottom of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Temporary location** field, enter your desired storage location path
    in the following format (replace [BUCKET-NAME] with your bucket name): gs://[BUCKET-NAME]/dataflow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the **Optional Parameters** section and scroll down until you find the
    **Service account** **email** field.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In that field, select the service account you created earlier in this chapter
    (it will contain `data-processing-sa` in the name if you used the suggested name).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Leave all other options at their default values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **RUN JOB**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After a few minutes, you will see the job details appearing, and a graph that
    looks similar to the one shown in *Figure 6**.11*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.11: Dataflow execution graph](img/B18143_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Dataflow execution graph'
  prefs: []
  type: TYPE_NORMAL
- en: Try clicking around on each of the steps and sub-steps in the graph to get a
    better understanding of what each step is doing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After some time, you can head over to the BigQuery console and verify that the
    data is streaming into the table. Move on to the next section in order to do that.
    However, don’t close the Dataflow console yet because you will come back here
    after verifying the data in BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying data in BigQuery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To verify the data in BigQuery, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **BigQuery**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the top-left corner of the screen, you will see your project name. Click
    on the arrow symbol to the left of your project name to expand it (see *Figure
    6**.12* for reference).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, click on the arrow symbol to the left of your dataset name (`taxirides`)
    to expand it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select your `realtime` table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **PREVIEW** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You should then see a screen that looks like the one shown in *Figure 6**.12*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.12: BigQuery data](img/B18143_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: BigQuery data'
  prefs: []
  type: TYPE_NORMAL
- en: When you have verified the data in BigQuery, you can go back to the Dataflow
    console and stop the Dataflow job by clicking **STOP** at the top of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that you’ve seen how easy it is to set up a Dataflow job from a template,
    let’s move on to more complex Dataflow use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Dataflow notebook
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because we want to use a customized Apache Beam notebook, we will create a
    notebook in the Dataflow Workbench console. Perform the following steps to create
    the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu→
    **Dataflow** → **Workbench**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the top of the screen, select the **Instances** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, at the top of the screen , select **Create New**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the screen that appears (see *Figure 6**.13* for reference), you can either
    accept the default notebook name or create a name of your preference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.13: Creating a user-managed notebook](img/B18143_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Creating a user-managed notebook'
  prefs: []
  type: TYPE_NORMAL
- en: Select your preferred region and zone. The zone selection doesn’t really matter
    in this case, but I recommend selecting the same region you have been using in
    previous activities in this book.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Continue**, and then **Continue** again.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select E2 as the compute instance type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can also configure an idle timeout period after which the machine will automatically
    shut down if it is idle for that amount of time. This helps to save costs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Continue** multiple times until you reach the **IAM** and **security**
    screen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **IAM and security** screen, select the **Single user** option. See
    *Figure 6**.14* for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.14: Dataflow notebook – IAM and security](img/B18143_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Dataflow notebook – IAM and security'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may remember performing a similar step (that is, selecting the **Single
    user authentication** option) when we created our managed notebook in [*Chapter
    5*](B18143_05.xhtml#_idTextAnchor168). This allows us to directly use the notebook
    without needing to authenticate it as a service account beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: The **User email** box that appears should automatically be populated with your
    login email address. If not, enter the email address that you use for logging
    in to the Google Cloud console.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also, we want our notebook instance to use the service account we created earlier
    in this chapter, so **uncheck** the option that says **Use default Compute Engine
    service account on the VM to call Google** **Cloud APIs**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the `data`, and you should see the name of the service account you created
    earlier in this chapter appearing in the list of available service accounts (assuming
    that you named it `data-processing-sa`, as recommended in that section).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select that service account in the list.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also, take note of the **Pricing summary** option in the top-right corner of
    the screen. This is an estimate of how much it would cost if you were to leave
    the notebook running all month. Fortunately, you will only use it for a short
    time in this chapter. If you did not configure an idle shutdown period when creating
    the notebook, remember to shut it down when you’re finished using it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can leave all other options at their default values and select **Create**
    at the bottom of the screen.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will take a few minutes for the notebook instance to be created. When the
    instance creation has completed, you will see it in the list of user-managed notebooks,
    and an option to open JupyterLab will appear.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Open Jupyterlab**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When the JupyterLab screen opens, it’s time to clone our repository into your
    notebook. This is similar to the process you performed in [*Chapter 5*](B18143_05.xhtml#_idTextAnchor168),
    but you have created a separate notebook instance in this chapter, so we need
    to clone the repository into this instance. The steps, again, to clone the repository,
    are as follows.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click on the **Git** symbol in the menu on the left of the screen. The symbol
    will look like the one shown in *Figure 6**.15*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.15: Git symbol](img/B18143_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Git symbol'
  prefs: []
  type: TYPE_NORMAL
- en: Select **Clone Repository**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter our repository URL: [https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects](https://github.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If any options are displayed, leave them at their default values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Clone**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You should see a new folder appear in your notebook, named `Google-Machine-Learning-for-Solutions-Architects`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Double-click on that folder, double-click on the `Chapter-06` folder within
    it, and then double-click on the `Streaming_NYC_Taxi_Data.ipynb` file to open
    it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Select Kernel** screen that appears, select the latest version of
    Apache Beam. At the time of writing this, the latest available option in the launcher
    is Apache Beam 2.4.6 (see *Figure 6**.16* for reference):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 6.16: Selecting a notebook kernel](img/B18143_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16: Selecting a notebook kernel'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook we have opened contains a lot of Apache Beam Python code that we
    can use to process the data that’s streaming in from the public Pub/Sub topic.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run each of the cells in the notebook, and read the explanations in the markdown
    and comments to understand what we’re doing. We’re using Apache Beam to define
    a streaming data processing pipeline that we will run in Google Cloud Dataflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When you have completed running the cells in the notebook, you can go to the
    Dataflow Jobs console, and you will see your new pipeline running there. Give
    it a few minutes for the pipeline to start up and for data to flow through the
    pipeline. Just like before, I recommend that you click on various parts of the
    pipeline execution graph in order to get a better understanding of how the pipeline
    is structured. Next, let’s verify this new pipeline data in BigQuery, but again,
    don’t close the Dataflow console yet, because you will come back here after verifying
    the data in BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: Verifying data in BigQuery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To verify the data in BigQuery, open the BigQuery console, and under the `taxirides`
    dataset, you will see a new table that has been created by our custom Dataflow
    pipeline, called `run_rates`. Click on the `run_rates` values that were computed
    by our pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: When you have verified the data in BigQuery, you can go back to the Dataflow
    console and stop the Dataflow job by clicking **STOP** at the top of the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have completed the activities in this section, you can shut down
    your user-managed notebook by performing the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In the Google Cloud console, navigate to the **Google Cloud services** menu
    → **Dataflow** → **Workbench**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the top of the screen, select the **User-managed** **notebooks** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the checkbox next to your notebook name, and click **STOP** at the top
    of the screen (just above the **User-managed** **notebooks** tab).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The notebook will shut down after a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: If everything worked as expected, you have now successfully created a custom
    pipeline that processes and transforms streaming data in flight!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to ingest data into Google Cloud from various
    sources, and you discovered important concepts on how to process data in Google
    Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: You then learned about exploring and visualizing data using Vertex AI and BigQuery.
    Next, you learned how to clean and prepare data for ML workloads using Jupyter
    notebooks, and then how to create an automated data pipeline to perform the same
    transformations at a production scale in a batch method using Apache Spark on
    Google Cloud Dataproc, as well as how to automatically orchestrate that entire
    process using Apache Airflow in GCC.
  prefs: []
  type: TYPE_NORMAL
- en: We then covered important concepts and tools related to processing streaming
    data, and you finally built your own streaming data processing pipelines using
    Apache Beam on Google Cloud Dataflow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will spend additional time on data processing and preparation,
    with a specific focus on the concept of feature engineering.
  prefs: []
  type: TYPE_NORMAL
