<html><head></head><body>
<div id="sbo-rt-content" class="calibre1"><div id="_idContainer190" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-286"><a id="_idTextAnchor348" class="calibre6 pcalibre pcalibre1"/>14</h1>
<h1 id="_idParaDest-287" class="calibre5"><a id="_idTextAnchor349" class="calibre6 pcalibre pcalibre1"/>Additional AI/ML Tools, Frameworks, and Considerations</h1>
<p class="calibre3">At this point, we have <a id="_idIndexMarker1563" class="calibre6 pcalibre pcalibre1"/>covered all of the major steps and considerations in a typical <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) project. Considering that AI/ML is one of the fastest-developing areas of research in the technology industry, new tools, methodologies, and frameworks emerge <span>every day.</span></p>
<p class="calibre3">In this chapter, we will discuss additional tools and frameworks that are popular in the data science <a id="_idIndexMarker1564" class="calibre6 pcalibre pcalibre1"/>industry that we haven’t covered so far. This includes important topics such as <strong class="bold">BigQuery ML</strong> (<strong class="bold">BQML</strong>), various types of hardware that we can use for AI/ML workloads, and the use of open source libraries and frameworks such as PyTorch, Ray, and Spark MLlib. We will also discuss some tips on how to implement large-scale distributed training on <span>Google Cloud.</span></p>
<p class="calibre3">At the end of this chapter, I will provide some additional context to help transition the focus of the remainder of this book to Generative AI. This will include diving a bit deeper into some of the commonly used neural network architectures that I described at a high level earlier in <span>this book.</span></p>
<p class="calibre3">For example, in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>, we covered the basics of neural networks and introduced common <a id="_idIndexMarker1565" class="calibre6 pcalibre pcalibre1"/>types of neural network architectures, such <a id="_idIndexMarker1566" class="calibre6 pcalibre pcalibre1"/>as <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>), <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>), and transformers. In this chapter, we will dive into those use cases in more detail to build some foundational knowledge for discussing Generative AI in the remaining chapters. Specifically, this chapter includes the following <span>main topics:</span></p>
<ul class="calibre16">
<li class="calibre8">Custom <span>Jupyter kernels</span></li>
<li class="calibre8"><span>BQML</span></li>
<li class="calibre8">Hardware considerations for <span>AI/ML workloads</span></li>
<li class="calibre8">Additional popular open source tools and frameworks – Spark MLlib, Ray, and PyTorch on <span>Google Cloud</span></li>
<li class="calibre8">Large-scale distributed <span>model training</span></li>
<li class="calibre8">Transitioning to <span>Generative AI</span></li>
</ul>
<p class="calibre3">The first topic in the list is also associated with some prerequisite steps that we need to cover to set up our environment for the practical activities in this chapter. These will be described in the <span>next section.</span></p>
<h1 id="_idParaDest-288" class="calibre5"><a id="_idTextAnchor350" class="calibre6 pcalibre pcalibre1"/>Prerequisite topics and steps</h1>
<p class="calibre3">This section <a id="_idIndexMarker1567" class="calibre6 pcalibre pcalibre1"/>describes the prerequisite topics and steps for setting up our Vertex AI <span>Workbench environment.</span></p>
<h3 class="calibre11">Custom Jupyter kernels and package dependency management</h3>
<p class="calibre3">When we run our code in a Jupyter Notebook (such as a Vertex AI Workbench Notebook), the environment in which our code executes is referred to as a kernel. Vertex AI Workbench instances come with various kernels already installed for popular tools <a id="_idIndexMarker1568" class="calibre6 pcalibre pcalibre1"/>and frameworks such <a id="_idIndexMarker1569" class="calibre6 pcalibre pcalibre1"/>as TensorFlow and PyTorch, which we will cover in more depth in <span>this chapter.</span></p>
<p class="calibre3">However, we can also create custom kernels if we want to define isolated environments with specific packages installed. This is a good practice to follow when using packages that are in preview mode, for example, as they may have very specific dependency requirements. We will <a id="_idIndexMarker1570" class="calibre6 pcalibre pcalibre1"/>use one such library, called <strong class="source-inline">bigframes</strong>, which I will describe in detail in this chapter. As a prerequisite, I will outline how to create a custom Jupyter kernel and explain some important concepts related to that process. Let’s begin with the concept of <span>virtual environments.</span></p>
<h4 class="calibre20">Virtual environments</h4>
<p class="calibre3">When our code executes, it runs within an environment, and this environment contains all of <a id="_idIndexMarker1571" class="calibre6 pcalibre pcalibre1"/>the dependencies that our code requires, which are usually other software packages. Managing the dependencies for various packages can be complicated, especially if we have two or more pieces of software that depend on different versions of a particular package. For example, imagine the <span>following scenario:</span></p>
<ul class="calibre16">
<li class="calibre8">Software package X depends on version 1.0.3 of software <span>package A</span></li>
<li class="calibre8">Software package Y depends on version 2.2.1 of software <span>package A</span></li>
</ul>
<p class="calibre3">If we installed software package Y and all of its dependencies in our environment, then our environment would contain version 2.2.1 of software <span>package A.</span></p>
<p class="calibre3">Now, if we try to run software package X in our environment, it may fail because it specifically requires a different version (that is, version 1.0.3) of software package A to be installed <a id="_idIndexMarker1572" class="calibre6 pcalibre pcalibre1"/>in the environment. This problem is referred to as a <span><strong class="bold">dependency conflict</strong></span><span>.</span></p>
<p class="calibre3">Virtual environments can help us avoid this problem because, as the name suggests, they provide a virtual execution environment in which we can run our code. When we create a virtual environment, it’s almost like creating a dedicated machine on which to execute our code because that environment, and everything in it, is isolated from other execution environments, but the isolation is virtual because it is simply a logical separation from other environments that can run on the <span>same machine.</span></p>
<p class="calibre3">When we use Vertex AI Notebook instances, there are two main types of virtual environments that we <span>can create:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Python virtual environments</strong>, which we can create by using the Python <strong class="source-inline">venv</strong> module, and <a id="_idIndexMarker1573" class="calibre6 pcalibre pcalibre1"/>which are <a id="_idIndexMarker1574" class="calibre6 pcalibre pcalibre1"/>therefore specific to Python packages. This option uses <strong class="source-inline">pip</strong> for <span>package management.</span></li>
<li class="calibre8"><strong class="bold">Conda environments</strong>, which use the Conda package and environment management <a id="_idIndexMarker1575" class="calibre6 pcalibre pcalibre1"/>system that goes beyond <a id="_idIndexMarker1576" class="calibre6 pcalibre pcalibre1"/>Python and can also manage packages for various other languages, such as R, Ruby, <span>and others.</span></li>
</ul>
<p class="calibre3">When determining which option to use, bear in mind that Python virtual environments are simpler and more lightweight, but Conda offers more features and handles more complex scenarios (as a result, Conda environments can be larger and slower to set up than Python virtual environments). We will use Conda for our use case. I will describe <span>this next.</span></p>
<h4 class="calibre20">Creating a Conda virtual environment and a custom Jupyter kernel</h4>
<p class="calibre3">Perform <a id="_idIndexMarker1577" class="calibre6 pcalibre pcalibre1"/>the following steps to create the Python <a id="_idIndexMarker1578" class="calibre6 pcalibre pcalibre1"/>virtual environment and custom Jupyter kernel that we will use later in <span>this chapter:</span></p>
<ol class="calibre7">
<li class="calibre8">Open JupyterLab on the Vertex AI Notebook instance you created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a><span>.</span></li>
<li class="calibre8">Select <strong class="bold">File</strong> | <strong class="bold">New</strong> | <strong class="bold">Terminal</strong> and perform the following steps on the <span>terminal screen.</span></li>
<li class="calibre8">Create a <span>Conda environment:</span><pre class="source-code">
conda create --name bigframes-env -y</pre></li> <li class="calibre8">Activate <span>the environment:</span><pre class="source-code">
conda activate bigframes-env</pre></li> <li class="calibre8">Install <strong class="source-inline">bigframes</strong> (we could also install any other packages we want, but we’ll keep it simple <span>for now):</span><pre class="source-code">
conda install bigframes -y</pre></li> <li class="calibre8">Install JupyterLab in the new environment (this will make our new Conda environment available as a kernel in JupyterLab via JupyterLab’s <span>autodiscovery feature):</span><pre class="source-code">
conda install jupyterlab -y</pre></li> <li class="calibre8">Change the display name of the kernel (it can take a few minutes for this update to appear in the Vertex AI Workbench <span>JupyterLab interface):</span><pre class="source-code">
sed -i 's/"display_name": "Python 3 (ipykernel)"/"display_name": "Python 3 (bigframes)"/' /opt/conda/envs/bigframes-env/share/jupyter/kernels/python3/kernel.json</pre></li> </ol>
<p class="calibre3">Now, our Conda <a id="_idIndexMarker1579" class="calibre6 pcalibre pcalibre1"/>environment and custom Jupyter <a id="_idIndexMarker1580" class="calibre6 pcalibre pcalibre1"/>kernel are ready to be used in the hands-on exercises in this chapter. We just have one more prerequisite step to perform before we dive into the remaining topics in this chapter, at that is to stage the required files for our serverless Spark <span>MLlib activities.</span></p>
<h2 id="_idParaDest-289" class="calibre9"><a id="_idTextAnchor351" class="calibre6 pcalibre pcalibre1"/>Staging files for serverless Spark MLlib activities</h2>
<p class="calibre3">In this <a id="_idIndexMarker1581" class="calibre6 pcalibre pcalibre1"/>section, we will stage some files in Google Cloud Storage to be used in the serverless Spark MLlib activities later in this chapter. To do this, perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">Go to the Google Cloud console and open Cloud Shell by clicking the <strong class="bold">Cloud Shell</strong> icon, as shown in <span><em class="italic">Figure 14</em></span>. This icon looks like a “greater than” symbol, followed by an underscore – that <span>is, </span><span><strong class="bold">&gt;_</strong></span><span>:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer179">
<img alt="Figure 14.1: Activating Cloud Shell" src="image/B18143_14_002.jpg" class="calibre172"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.1: Activating Cloud Shell</p>
<ol class="calibre7">
<li value="2" class="calibre8">Run the <a id="_idIndexMarker1582" class="calibre6 pcalibre pcalibre1"/>following commands in Cloud Shell to download the required files from our <span>GitHub repository:</span><pre class="source-code">
wget https://raw.githubusercontent.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/main/Chapter-14/data/data_processed_titanic_part.snappy.parquet
wget https://raw.githubusercontent.com/PacktPublishing/Google-Machine-Learning-for-Solutions-Architects/main/Chapter-14/pyspark-ml.py</pre></li> <li class="calibre8">Run the following commands in Cloud Shell to upload the required files to Google Cloud Storage (<strong class="bold">important: replace [YOUR-BUCKET-NAME] with the name of a bucket you created in </strong><span><strong class="bold">earlier chapters</strong></span><span>):</span><pre class="source-code">
gsutil cp pyspark-ml.py gs://[YOUR-BUCKET-NAME] /code/additional-use-cases-chapter/pyspark-ml.py
gsutil cp *.parquet gs://[YOUR-BUCKET-NAME]/data/processed/mlops-titanic</pre></li> </ol>
<p class="calibre3">Now, everything <a id="_idIndexMarker1583" class="calibre6 pcalibre pcalibre1"/>is ready for our hands-on exercises later in this chapter. Next, we’ll dive into the important topic <span>of BQML.</span></p>
<h1 id="_idParaDest-290" class="calibre5"><a id="_idTextAnchor352" class="calibre6 pcalibre pcalibre1"/>BQML</h1>
<p class="calibre3">We first introduced BigQuery in <a href="B18143_03.xhtml#_idTextAnchor059" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 3</em></span></a>, and we’ve used it in various chapters of this book for its <a id="_idIndexMarker1584" class="calibre6 pcalibre pcalibre1"/>data management and processing functionality. However, given the close relationship between large-scale data processing and ML, Google Cloud has built ML functionality directly into BigQuery, as well as native integrations with Google Cloud Vertex AI. This functionality is referred to as BQML, and this section explores this service <span>in detail.</span></p>
<p class="calibre3">BQML enables us to create and execute ML models using standard SQL queries in BigQuery. Considering that many companies already store large amounts of data in BigQuery, BQML makes it easy for data scientists in those companies to train models on large datasets and make predictions directly in the database system, without needing to move the data around between different <span>storage systems.</span></p>
<p class="calibre3">It supports a variety of ML algorithms, including linear regression, logistic regression, k-means clustering, and deep neural networks, as well as forecasting use cases based on time series data stored <span>in BigQuery.</span></p>
<p class="calibre3">In addition to training and prediction, we can use BQML to perform many of the steps in the model development life cycle, such as feature engineering, model performance evaluation, and hyperparameter tuning. Considering that all of this can be done using standard SQL, data scientists can easily start using BQML without needing to go through a steep learning curve in terms of tooling. This is referred to as lowering the barrier of entry, where the barrier of entry represents how easy or difficult it is for people to start performing an activity. For instance, an activity with a high barrier of entry presents a lot of initial difficulty. An example of this would be a system that requires extensive training or complex prerequisites, such as provisioning infrastructure, before somebody could start using that system. Some technology systems require months of training and effort before somebody can start using them effectively. BQML, on the other hand, can easily be used by anybody who understands standard SQL syntax, and it does not require any complex infrastructure provisioning. Just like other managed services provided by Google Cloud, BigQuery (and, by extension, BQML) manages the infrastructure for us, and it can <a id="_idIndexMarker1585" class="calibre6 pcalibre pcalibre1"/>automatically scale up and down based <span>on demand.</span></p>
<p class="calibre3">With this in mind, let’s take a look at how we can use BQML to develop and use <span>ML models.</span></p>
<h2 id="_idParaDest-291" class="calibre9"><a id="_idTextAnchor353" class="calibre6 pcalibre pcalibre1"/>Using BQML</h2>
<p class="calibre3">While the Jupyter Notebook that accompanies this chapter provides hands-on instructions <a id="_idIndexMarker1586" class="calibre6 pcalibre pcalibre1"/>on how to use BQML for various ML tasks, I will summarize some of the main features here. For context, let’s recall our ML model development life cycle, as depicted in <span><em class="italic">Figure 14</em></span><span><em class="italic">.3</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer180">
<img alt="Figure 14.2: ML model development life cycle" src="image/B18143_14_003.jpg" class="calibre173"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.2: ML model development life cycle</p>
<p class="calibre3">The following subsections dive into each step in <span>more detail.</span></p>
<h3 class="calibre11">Data preparation</h3>
<p class="calibre3">We <a id="_idIndexMarker1587" class="calibre6 pcalibre pcalibre1"/>will combine the following steps shown <a id="_idIndexMarker1588" class="calibre6 pcalibre pcalibre1"/>in <span><em class="italic">Figure 14</em></span><em class="italic">.3</em> into <span>this section:</span></p>
<ul class="calibre16">
<li class="calibre8"><span>Ingest data</span></li>
<li class="calibre8">Store <span>input data</span></li>
<li class="calibre8"><span>Explore data</span></li>
<li class="calibre8"><span>Process/transform data</span></li>
<li class="calibre8">Store <span>processed data</span></li>
</ul>
<p class="calibre3">Starting with data ingestion and storage, there are many ways in which we can get our data into BigQuery. For example, we can directly upload files via the BigQuery console UI or the CLI. For larger datasets, we can stage them in Google Cloud Storage and import them to BigQuery from there. We can also use the BigQuery Data Transfer Service to automate data movement into BigQuery, either as a one-off transfer or on a scheduled cadence. We can stream data into BigQuery by integrating with other services, such as Google Cloud Pub/Sub and Dataflow. There are also third-party ETL tools that can be used to transfer data <span>to BigQuery.</span></p>
<p class="calibre3">Once we <a id="_idIndexMarker1589" class="calibre6 pcalibre pcalibre1"/>have stored our data in BigQuery, we can use various tools to <a id="_idIndexMarker1590" class="calibre6 pcalibre pcalibre1"/>explore and transform the data, in addition to the BigQuery console and standard SQL. We introduced and used the <strong class="source-inline">pandas</strong> library in previous chapters, and we know that it’s a very widely used library in data science, particularly for data exploration and manipulation. For this reason, many data scientists like to use pandas directly with their data stored in BigQuery. Fortunately, there are additional libraries that make it easy to <span>do this:</span></p>
<ul class="calibre16">
<li class="calibre8">The BigQuery DataFrames <span>Python API</span></li>
<li class="calibre8">The <span><strong class="source-inline">pandas_gbq</strong></span><span> library</span></li>
</ul>
<p class="calibre3">Let’s take a look at each of these in <span>more detail.</span></p>
<h4 class="calibre20">The BigQuery DataFrames Python API</h4>
<p class="calibre3">The BigQuery DataFrames Python API enables us to use Python to analyze and manipulate <a id="_idIndexMarker1591" class="calibre6 pcalibre pcalibre1"/>data in BigQuery and <a id="_idIndexMarker1592" class="calibre6 pcalibre pcalibre1"/>perform various ML tasks. It’s a relatively new, open source option that was launched and is maintained by Google Cloud for using DataFrames to interact with BigQuery (at the time of writing this in December 2023, it is currently in Preview status). We can access it by using the <strong class="source-inline">bigframes</strong> Python library, which consists of two <span>main parts:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="source-inline">bigframes.pandas</strong>, which implements a pandas-like API on top <span>of BigQuery</span></li>
<li class="calibre8"><strong class="source-inline">bigframes.ml</strong>, which implements a <strong class="source-inline">scikit-learn</strong>-like API on top of <span>BigQuery ML</span></li>
</ul>
<p class="calibre3">The Jupyter Notebook that accompanies this chapter provides instructions on how to use <strong class="source-inline">bigframes.pandas</strong> in more<a id="_idIndexMarker1593" class="calibre6 pcalibre pcalibre1"/> detail. We will<a id="_idIndexMarker1594" class="calibre6 pcalibre pcalibre1"/> dive into those steps shortly, but first, I’ll briefly <span>cover </span><span><strong class="source-inline">pandas_gbq</strong></span><span>.</span></p>
<h4 class="calibre20">pandas_gbq</h4>
<p class="calibre3">If I were writing this chapter a few months ago, the <strong class="source-inline">pandas_gbq</strong> library would have been the <a id="_idIndexMarker1595" class="calibre6 pcalibre pcalibre1"/>main or only option that I would include in this section because, at the time of writing, it has been the primary option available for using pandas <a id="_idIndexMarker1596" class="calibre6 pcalibre pcalibre1"/>to interact with BigQuery. It’s an open source library that is maintained by PyData and volunteer contributors and has been around for quite some time (since 2017), so it has become broadly used in <span>the industry.</span></p>
<p class="calibre3">Essentially, it’s a thin wrapper around the BigQuery client library (<strong class="source-inline">google-cloud-bigquery</strong>) that provides a simple interface for running SQL queries and uploading pandas DataFrames to BigQuery. The results from these queries are parsed into a <strong class="source-inline">pandas.DataFrame</strong> object in which the shape and data types are derived from the <span>source table.</span></p>
<p class="calibre3">The Jupyter Notebook that accompanies this chapter also provides instructions on how to use this library in more detail. Now would be a good time to dive into those steps. Open JupyterLab on the Vertex AI Workbench instance you created in <a href="B18143_05.xhtml#_idTextAnchor168" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 5</em></span></a> and perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the navigation panel on the left-hand side of the screen, navigate to the <strong class="source-inline">Chapter-14</strong> directory within the <span><strong class="source-inline">Google-Machine-Learning-for-Solutions-Architects</strong></span><span> folder.</span></li>
<li class="calibre8">Double-click on the <strong class="source-inline">pandas-gbq.ipynb</strong> notebook file to open it. When prompted to select a kernel, you can use the default <strong class="bold">Python 3 (</strong><span><strong class="bold">ipykernel)</strong></span><span> kernel.</span></li>
<li class="calibre8">Double-click on the <strong class="source-inline">bigframes.ipynb</strong> notebook file to open it. When prompted to select a kernel, you can use the default <strong class="bold">Python 3 (bigframes)</strong> kernel that we created in the <em class="italic">Prerequisite topics and steps</em> section of <span>this chapter.</span></li>
<li class="calibre8">In each of the notebooks you’ve opened, press <em class="italic">Shift</em> + <em class="italic">Enter</em> to execute <span>each cell.</span><p class="calibre3">The notebooks contain comments and Markdown text that describe what the code in each cell <span>is doing.</span></p></li>
</ol>
<p class="calibre3">As you can see, there are multiple options for interacting with BigQuery data in <span>Vertex AI.</span></p>
<p class="calibre3">Given the broad adoption of the <strong class="source-inline">pandas_gbq</strong> library in the industry, and now that Google has <a id="_idIndexMarker1597" class="calibre6 pcalibre pcalibre1"/>launched the official BigQuery DataFrames Python API described previously, it’s likely that both options will continue to be popular <a id="_idIndexMarker1598" class="calibre6 pcalibre pcalibre1"/>among data scientists. A key thing to bear in mind is that <strong class="source-inline">pandas-gbq</strong> downloads data to your local environment, whereas the BigQuery DataFrames Python API is used to run your data operations on Google Cloud <span>distributed infrastructure.</span></p>
<p class="calibre3">Next, let’s discuss how to create, use, and manage ML models with BQML. The Jupyter Notebook file that accompanies this chapter can be used to implement the following steps, but let’s discuss them before diving into the <span>notebook file.</span></p>
<h3 class="calibre11">Creating ML models</h3>
<p class="calibre3">We can use the <strong class="source-inline">CREATE MODEL</strong> statement in BigQuery to define and train a model. For example, the <a id="_idIndexMarker1599" class="calibre6 pcalibre pcalibre1"/>following code excerpt <a id="_idIndexMarker1600" class="calibre6 pcalibre pcalibre1"/>creates a linear regression model named <strong class="source-inline">my_model_name</strong>, which is trained on the data in <strong class="source-inline">my_dataset.my_table</strong>, using the values in <strong class="source-inline">target_colum</strong> as <span>the labels:</span></p>
<pre class="source-code">
CREATE OR REPLACE MODEL `my_dataset.my_model_name`
OPTIONS(model_type = 'linear_reg', input_label_cols=['target_column']) AS
SELECT * FROM `my_dataset.my_table`;</pre> <p class="calibre3">As we’ve already discussed, BQML supports many types of commonly used algorithms. We can also import and export our models from/to Google Cloud Storage to interact with other tools and frameworks, and we can even perform hyperparameter tuning during the model <span>development process.</span></p>
<h3 class="calibre11">Evaluating ML models</h3>
<p class="calibre3">Once our <a id="_idIndexMarker1601" class="calibre6 pcalibre pcalibre1"/>model has been trained, we can evaluate its performance <a id="_idIndexMarker1602" class="calibre6 pcalibre pcalibre1"/>using SQL code such as <span>the following:</span></p>
<pre class="source-code">
SELECT * FROM ML.EVALUATE(MODEL `my_dataset.my_model_name`, 
TABLE `my_dataset.my_evaluation_table`);</pre> <p class="calibre3">This returns evaluation metrics such as accuracy, precision, recall, and more, depending on the <span>model type.</span></p>
<h3 class="calibre11">Generating predictions</h3>
<p class="calibre3">Next, we can <a id="_idIndexMarker1603" class="calibre6 pcalibre pcalibre1"/>use the model to make predictions using SQL code such as <span>the following:</span></p>
<pre class="source-code">
SELECT * FROM ML.PREDICT(MODEL `my_dataset.my_model_name`, 
TABLE `my_dataset.my_input_table`);</pre> <p class="calibre3">This statement will feed the data from <strong class="source-inline">my_dataset.my_input_table</strong> into our trained model to <span>generate predictions.</span></p>
<p class="calibre3">Now, we can use the Jupyter Notebook file that accompanies this chapter to implement these steps. To do that, open JupyterLab on the Vertex AI Workbench instance you created in <span>Chapter 5</span> and perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the navigation panel on the left-hand side of the screen, navigate to the <strong class="source-inline">Chapter-14</strong> directory within the <span><strong class="source-inline">Google-Machine-Learning-for-Solutions-Architects</strong></span><span> folder.</span></li>
<li class="calibre8">Double-click <a id="_idIndexMarker1604" class="calibre6 pcalibre pcalibre1"/>on the <strong class="source-inline">BQML.ipynb</strong> notebook file to open it. When prompted, select the default <strong class="bold">Python 3 (</strong><span><strong class="bold">ipykernel)</strong></span><span> kernel.</span></li>
<li class="calibre8">Press <em class="italic">Shift</em> + <em class="italic">Enter</em> to execute <span>each cell.</span><p class="calibre3">The notebook contains comments and Markdown text that describe what the code in each cell <span>is doing.</span></p></li>
</ol>
<p class="calibre3">Next, we’ll discuss how to set up model monitoring and <span>continuous training.</span></p>
<h3 class="calibre11">Model monitoring and continuous training</h3>
<p class="calibre3">To implement ongoing model monitoring and continuous training, we could set up scheduled <a id="_idIndexMarker1605" class="calibre6 pcalibre pcalibre1"/>jobs to execute the evaluation and training queries. This is a solution that we would need to architect using <a id="_idIndexMarker1606" class="calibre6 pcalibre pcalibre1"/>additional Google Cloud services, such as Google Cloud Scheduler and Google Cloud Functions, in which we could use Google Cloud Scheduler to periodically invoke Google Cloud Functions to run the training and evaluation queries. Having said that, for complex MLOps pipelines, Vertex AI offers more functionality for customization. This point brings us to our next topic, which is determining when to use BQML versus other tools for AI/ML <span>use cases.</span></p>
<h2 id="_idParaDest-292" class="calibre9"><a id="_idTextAnchor354" class="calibre6 pcalibre pcalibre1"/>When to use BQML versus other tools for AI/ML use cases</h2>
<p class="calibre3">We’ve <a id="_idIndexMarker1607" class="calibre6 pcalibre pcalibre1"/>already covered that BQML is a great fit for data analysts <a id="_idIndexMarker1608" class="calibre6 pcalibre pcalibre1"/>and data scientists who want to use familiar SQL syntax to implement ML use cases on data stored in BigQuery, and how simplicity is one of its main benefits in that context. Bear in mind that there can often be a tradeoff between simplicity and customization. If you need to build highly advanced and customized models, then you may find that the simplicity of SQL does not provide the same level of customization that can be achieved when using specialized AI/ML frameworks such as TensorFlow, PyTorch, Ray, and <span>Spark MLlib.</span></p>
<p class="calibre3">You can also have “the best of both worlds” by interacting with models hosted in Vertex AI from <a id="_idIndexMarker1609" class="calibre6 pcalibre pcalibre1"/>BigQuery by using <strong class="bold">BigQuery remote functions</strong>, which provide integrations with Cloud Functions and Cloud Run. With this approach, you can <a id="_idIndexMarker1610" class="calibre6 pcalibre pcalibre1"/>write code that will be executed on Cloud Functions or Cloud Run, and you can invoke that code from a query in BigQuery. The code <a id="_idIndexMarker1611" class="calibre6 pcalibre pcalibre1"/>could send an inference request to a model that has been trained and hosted in Vertex AI, and then also send the response back to BigQuery. You could even implement transformations in both the request and the response, if needed, to ensure compatibility between your query’s expected data types and your model’s expected data types. You can find out more about BigQuery remote functions in the Google Cloud documentation at the following link, which describes known limitations and best <span>practices: </span><a href="https://cloud.google.com/bigquery/docs/remote-functions" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/bigquery/docs/remote-functions</span></a><span>.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">While BigQuery is designed for large-scale analytics workloads, Google Cloud Spanner is another highly scalable database service in Google Cloud. Spanner is designed for distributed and strongly consistent transactional use cases, and it also supports SQL syntax. It recently added integrations with Vertex AI, providing somewhat similar functionality as BQML (that is, the ability to access ML models hosted on Vertex AI through a SQL interface), but intended for transactional rather than <span>analytical workloads.</span></p>
<p class="calibre3">Next, I will end this <a id="_idIndexMarker1612" class="calibre6 pcalibre pcalibre1"/>section with a brief discussion of BigQuery Studio, which is <a id="_idIndexMarker1613" class="calibre6 pcalibre pcalibre1"/>a convenient way to manage all of our BigQuery tasks <span>and workloads.</span></p>
<h2 id="_idParaDest-293" class="calibre9"><a id="_idTextAnchor355" class="calibre6 pcalibre pcalibre1"/>BigQuery Studio</h2>
<p class="calibre3">BigQuery Studio, as <a id="_idIndexMarker1614" class="calibre6 pcalibre pcalibre1"/>the name suggests, provides a single-pane-of-glass experience that allows us to perform many different kinds of activities in BigQuery. It includes a SQL editor interface that enables us to write and run queries directly in the Google Cloud console, with intelligent code development assistance via integration with Duet, a Google Cloud Generative AI real-time code assistant that we will cover in more detail later in this book. BigQuery Studio also enables us to schedule SQL queries to run on a periodic cadence (for example, every day) for use cases that require repeated query executions, such as generating a <span>daily report.</span></p>
<p class="calibre3">Via BigQuery Studio, we can access Dataform to define and run data processing workflows within BigQuery and utilize BigQuery Studio’s integrations with Dataplex for data discovery, data profiling, and data quality management. BigQuery Studio also integrates with <a id="_idIndexMarker1615" class="calibre6 pcalibre pcalibre1"/>Colab Enterprise, enabling us to use Jupyter Notebooks directly within the <span>BigQuery console.</span></p>
<p class="calibre3">I encourage you to visit the BigQuery Studio interface within your Google Cloud console and explore its various capabilities <span>and integrations.</span></p>
<p class="calibre3">We will revisit BigQuery in later chapters, but for now, let’s discuss some hardware considerations for <span>AI/ML workloads.</span></p>
<h1 id="_idParaDest-294" class="calibre5"><a id="_idTextAnchor356" class="calibre6 pcalibre pcalibre1"/>Hardware considerations for AI/ML workloads</h1>
<p class="calibre3">The majority of the topics in this book focus on the software and service-level functionalities that are available on Google Cloud. Advanced practitioners will also be interested <a id="_idIndexMarker1616" class="calibre6 pcalibre pcalibre1"/>in what kind of hardware capabilities exist. If your use cases require extreme performance, then selecting the right hardware components on which to run your workloads is an important decision. The <a id="_idIndexMarker1617" class="calibre6 pcalibre pcalibre1"/>selection and efficient usage of underlying hardware also affect the costs, which are, of course, another important factor in your solution <a id="_idIndexMarker1618" class="calibre6 pcalibre pcalibre1"/>architecture. In this section, we’ll shift the discussion so that it focuses on some of the hardware considerations for running AI/ML <a id="_idIndexMarker1619" class="calibre6 pcalibre pcalibre1"/>workloads in Google Cloud, beginning with an overview of <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>), <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>), and <strong class="bold">tensor processing unit</strong> (<span><strong class="bold">TPU</strong></span><span>) capabilities.</span></p>
<h2 id="_idParaDest-295" class="calibre9"><a id="_idTextAnchor357" class="calibre6 pcalibre pcalibre1"/>CPUs, GPUs, and TPUs</h2>
<p class="calibre3">You will probably already be familiar with CPUs and GPUs, but TPUs are more specific to Google Cloud. As a brief overview, CPUs are what power the vast majority of consumer devices, such as laptops and mobile phones, as well as general-purpose servers in data centers. They are effective at multi-tasking across a broad range of tasks, but the processing they perform is <span>somewhat sequential.</span></p>
<p class="calibre3">GPUs, on the other hand, are built with an architecture that optimizes parallel (rather than sequential) processing. If you can take a process and break it down into similar tasks that can run in parallel, then you’ll be able to complete that process much more quickly on a GPU than on a CPU. Although, as the name suggests, GPUs were originally designed <a id="_idIndexMarker1620" class="calibre6 pcalibre pcalibre1"/>for handling graphics, which involves processing large blocks of pixels and vertices in parallel, it turns out that the kinds of matrix <a id="_idIndexMarker1621" class="calibre6 pcalibre pcalibre1"/>manipulation tasks that are inherent to many AI/ML workloads can also be sped up by the parallel architecture <span>of GPUs.</span></p>
<p class="calibre3">TPUs were <a id="_idIndexMarker1622" class="calibre6 pcalibre pcalibre1"/>designed by Google to accelerate TensorFlow operations and they are optimized for both training and running models more efficiently than CPUs and GPUs for some types of workloads. Although they were created specifically for TensorFlow, they can now also be used with other frameworks, such as <a id="_idIndexMarker1623" class="calibre6 pcalibre pcalibre1"/>PyTorch, by using libraries such as PyTorch/XLA, which is a Python package that uses the <strong class="bold">Accelerated Linear Algebra </strong>(<strong class="bold">XLA</strong>) deep learning compiler to enable PyTorch to connect to TPUs and use TPU cores <span>as devices.</span></p>
<p class="calibre3">Google Cloud provides many different types of hardware servers that we can choose for running our ML workloads, and these servers provide various amounts of CPU, GPU, and <span>TPU power.</span></p>
<p class="calibre3">At the time <a id="_idIndexMarker1624" class="calibre6 pcalibre pcalibre1"/>of writing this in December 2023, Google Cloud has recently <a id="_idIndexMarker1625" class="calibre6 pcalibre pcalibre1"/>launched its most powerful TPU (<strong class="bold">Cloud TPU v5p</strong>) in conjunction with their new <strong class="bold">AI Hypercomputer</strong> offering, which provides highly optimized resources (that is, storage, compute, networking, and more) for <span>AI/ML workloads.</span></p>
<p class="calibre3">Given the broad variety of computing options on Google Cloud, I will not list all of them here. Google Cloud is constantly launching additional options, so I encourage you to review the Google Cloud documentation to find the latest <span>details: </span><a href="https://cloud.google.com/compute/docs/machine-resource" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/compute/docs/machine-resource</span></a><span>.</span></p>
<p class="calibre3">Next, let’s switch the discussion back to the software level and explore some popular open source tools and frameworks in the ML and data science industry that are supported in <span>Google Cloud.</span></p>
<h1 id="_idParaDest-296" class="calibre5"><a id="_idTextAnchor358" class="calibre6 pcalibre pcalibre1"/>Additional open source tools and frameworks –Spark MLlib, Ray, and PyTorch on Google Cloud</h1>
<p class="calibre3">In this <a id="_idIndexMarker1626" class="calibre6 pcalibre pcalibre1"/>section, I’ll introduce additional open source <a id="_idIndexMarker1627" class="calibre6 pcalibre pcalibre1"/>tools and frameworks, such as PyTorch, Ray, and Spark <strong class="bold">Machine Learning Library</strong> (<strong class="bold">MLlib</strong>), and demonstrate how they can be used to implement AI/ML workloads on <span>Google Cloud.</span></p>
<h2 id="_idParaDest-297" class="calibre9"><a id="_idTextAnchor359" class="calibre6 pcalibre pcalibre1"/>Spark MLlib</h2>
<p class="calibre3">We introduced Apache Spark in previous chapters and used it for data processing to perform <a id="_idIndexMarker1628" class="calibre6 pcalibre pcalibre1"/>feature engineering in <a href="B18143_06.xhtml#_idTextAnchor187" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 6</em></span></a>. Apache Spark MLlib is a component of Apache Spark that provides ML tools and algorithms <a id="_idIndexMarker1629" class="calibre6 pcalibre pcalibre1"/>that are optimized for parallel processing with large datasets. In addition to feature engineering, we can use the tools in MLlib to implement various stages in our ML model development life cycle, such as model training, model evaluation, hyperparameter tuning, and prediction, as well as assembling stages into a pipeline that can be executed end <span>to end.</span></p>
<p class="calibre3">Just as we discussed in the context of data processing, one of the main advantages of Apache Spark (including MLlib) is its ability to execute large-scale computing workloads. While libraries such as scikit-learn work well for performing ML workloads on single machines, MLlib can distribute the computation work across many machines, which enables us to handle larger datasets <span>more efficiently.</span></p>
<p class="calibre3">In the hands-on activities that accompany this chapter, we will use Spark MLlib to train a model on Google Cloud Dataproc by using the Serverless Spark features within Vertex AI. This is important to understand from a solution architecture perspective: we will perform the steps in Vertex AI, but it will run the Spark job on Dataproc in the background. There are multiple ways in which we can implement this workload, and we will cover the following two methods in our <span>hands-on activities:</span></p>
<ol class="calibre7">
<li class="calibre8">Using an MLOps pipeline, similar to the activities we performed in <a href="B18143_11.xhtml#_idTextAnchor288" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 11</em></span></a><span>.</span></li>
<li class="calibre8">Using the Serverless Spark user interface in <span>Vertex AI.</span></li>
</ol>
<p class="calibre3">Let’s begin with the first method: using an <span>MLOps pipeline.</span></p>
<h3 class="calibre11">Serverless Spark via Kubeflow Pipelines on Vertex AI</h3>
<p class="calibre3">In <a href="B18143_11.xhtml#_idTextAnchor288" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 11</em></span></a>, we used the <strong class="source-inline">DataprocPySparkBatchOp</strong> operator in Kubeflow Pipelines <a id="_idIndexMarker1630" class="calibre6 pcalibre pcalibre1"/>to automate the execution of a serverless Spark job in an MLOps pipeline. We will use the same <a id="_idIndexMarker1631" class="calibre6 pcalibre pcalibre1"/>operator again in this chapter, but this time, we will use it to run a model training and evaluation job using Spark MLlib. To do so, open JupyterLab on the Vertex AI Workbench instance you created in <span>Chapter 5</span> and perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the navigation panel on the left-hand side of the screen, navigate to the <strong class="source-inline">Chapter-14</strong> directory within the <span><strong class="source-inline">Google-Machine-Learning-for-Solutions-Architects</strong></span><span> folder.</span></li>
<li class="calibre8">Double-click on the <strong class="source-inline">spark-ml.ipynb</strong> notebook file to open it. When you’re prompted to select a kernel, select the <strong class="bold">Python 3 (</strong><span><strong class="bold">ipykernel)</strong></span><span> kernel.</span></li>
<li class="calibre8">In each of the notebooks you’ve opened, press <em class="italic">Shift</em> + <em class="italic">Enter</em> to execute <span>each cell.</span><p class="calibre3">The notebooks contain comments and Markdown text that describe what the code in each cell <span>is doing.</span></p></li>
</ol>
<p class="calibre3">As we can see, it’s quite easy to extend the activities we performed in <a href="B18143_11.xhtml#_idTextAnchor288" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 11</em></span></a> to use Kubeflow Pipelines to automate a Serverless Spark job for model training purposes with Spark MLlib. Let’s take a look at another option for running our Serverless Spark job directly via the Serverless Spark user interface in <span>Vertex AI.</span></p>
<h3 class="calibre11">The Serverless Spark user interface in Vertex AI</h3>
<p class="calibre3">To <a id="_idIndexMarker1632" class="calibre6 pcalibre pcalibre1"/>access the Serverless Spark user interface <a id="_idIndexMarker1633" class="calibre6 pcalibre pcalibre1"/>in Vertex AI, open JupyterLab on the Vertex AI Workbench instance you created in <span>Chapter 5</span> and perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">From the <strong class="bold">File</strong> menu, select <span><strong class="bold">New Launcher</strong></span><span>.</span></li>
<li class="calibre8">Scroll down to the <strong class="bold">Dataproc Jobs and Sessions</strong> section and <span>select </span><span><strong class="bold">Serverless</strong></span><span>.</span></li>
<li class="calibre8">Select <span><strong class="bold">Create Batch</strong></span><span>.</span></li>
<li class="calibre8">On the <a id="_idIndexMarker1634" class="calibre6 pcalibre pcalibre1"/>screen that <a id="_idIndexMarker1635" class="calibre6 pcalibre pcalibre1"/>appears (see <span><em class="italic">Figure 14</em></span><em class="italic">.4</em> for reference), enter the <span>following details:</span><ol class="calibre77"><li class="upper-roman"><strong class="bold">Main jar URI</strong> (you need to select the radio button for this option to display the entry field): Enter the GCS location where you saved the <strong class="source-inline">pyspark-ml.py</strong> file. It should be in the following format <strong class="bold">(important: replace YOUR-BUCKET-NAME with the name of the bucket you used in the prerequisites section of this </strong><span><strong class="bold">chapter</strong></span><span>): </span><span><strong class="source-inline">gs://YOUR-BUCKET-NAME/code/additional-use-cases-chapter/pyspark-ml.py</strong></span><span>.</span></li><li class="upper-roman"><strong class="bold">Arguments</strong> (press <em class="italic">Enter</em> after specifying each of the following lines): <strong class="bold">Important: replace YOUR-BUCKET-NAME with the name of the bucket you used in the prerequisites section of </strong><span><strong class="bold">this chapter</strong></span><span>:</span><ol class="calibre77"><li class="lower-roman"><strong class="source-inline">--</strong><span><strong class="source-inline">processed_data_path=gs://YOUR-BUCKET-NAME/data/processed/mlops-titanic</strong></span></li><li class="lower-roman"><strong class="source-inline">--</strong><span><strong class="source-inline">model_path=gs://YOUR-BUCKET-NAME/models/additional-use-cases-chapter/</strong></span></li></ol></li></ol></li>
<li class="calibre8">Leave all other fields at their default values and click <strong class="bold">Submit</strong> at the bottom of <span>the screen:</span></li>
</ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer181">
<img alt="Figure 14.3: The Serverless Spark user interface in Vertex AI" src="image/B18143_14_004.jpg" class="calibre174"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.3: The Serverless Spark user interface in Vertex AI</p>
<p class="calibre3">After <a id="_idIndexMarker1636" class="calibre6 pcalibre pcalibre1"/>clicking the <strong class="bold">Submit</strong> button, a screen will appear showing a list of serverless Spark jobs; your <a id="_idIndexMarker1637" class="calibre6 pcalibre pcalibre1"/>newly submitted job will appear at the top of the list (you may need to refresh your browser page to update the list). Wait until the status of your job <span>says </span><span><strong class="bold">Succeeded</strong></span><span>.</span></p>
<ol class="calibre7">
<li value="6" class="calibre8">If the job fails for any reason, click on the name of the job to display its details, and then click <strong class="bold">View Cloud Logs</strong> at the top of <span>the screen.</span></li>
<li class="calibre8">It may take some time for the logs to populate. You can click the <strong class="bold">Run query</strong> button periodically to refresh <span>the logs.</span></li>
<li class="calibre8">When the job has finished executing, the model artifacts will be saved in two directories named <strong class="source-inline">metadata</strong> and <strong class="source-inline">stages</strong> at the location you specified for the <strong class="source-inline">--model-path</strong> argument. Verify that those directories have been created and populated by performing the following steps in the Google <span>Cloud console:</span><ol class="calibre77"><li class="lower-roman">Go to the Google Cloud services menu and choose <strong class="bold">Google </strong><span><strong class="bold">Cloud Storage</strong></span><span>.</span></li><li class="lower-roman">Browse to the path you specified for the <strong class="source-inline">--model-path</strong> argument by clicking on each successive component of the path – for <span>example, </span><span><strong class="source-inline">YOUR-BUCKET-NAME/models/additional-use-cases-chapter/</strong></span><span>.</span></li></ol></li>
</ol>
<p class="calibre3">Great job! You <a id="_idIndexMarker1638" class="calibre6 pcalibre pcalibre1"/>have just used <a id="_idIndexMarker1639" class="calibre6 pcalibre pcalibre1"/>Spark MLlib to implement a serverless Spark workload to train a model on Dataproc via Vertex AI. Next, we’ll briefly discuss another distributed computing framework that has become increasingly broadly used in the data science <span>industry: Ray.</span></p>
<h2 id="_idParaDest-298" class="calibre9"><a id="_idTextAnchor360" class="calibre6 pcalibre pcalibre1"/>Ray</h2>
<p class="calibre3">I won’t spend <a id="_idIndexMarker1640" class="calibre6 pcalibre pcalibre1"/>much time on Ray in this book, but I want to mention it for completeness. The developers of Ray describe it as an “<em class="italic">open source unified compute framework that makes it easy to scale AI and Python workloads.</em>” Ray is another type <a id="_idIndexMarker1641" class="calibre6 pcalibre pcalibre1"/>of distributed execution framework that has been gaining popularity in recent years, especially in <span>AI/ML applications.</span></p>
<p class="calibre3">Like Spark, Ray enables parallelization of code and distribution of tasks across a cluster of machines. It also includes components that can help with specific model development steps, such as Ray Tune for hyperparameter tuning, and Ray RLlib for reinforcement learning. Google Cloud Vertex AI now directly supports Ray by enabling us to create <span>Ray clusters.</span></p>
<h2 id="_idParaDest-299" class="calibre9"><a id="_idTextAnchor361" class="calibre6 pcalibre pcalibre1"/>PyTorch</h2>
<p class="calibre3">Like TensorFlow and Spark MLlib, PyTorch is an open source framework that includes a set of tools <a id="_idIndexMarker1642" class="calibre6 pcalibre pcalibre1"/>and libraries for ML and deep learning use cases. It was originally developed by Facebook’s AI Research lab, and it evolved <a id="_idIndexMarker1643" class="calibre6 pcalibre pcalibre1"/>from another AI/ML framework named Torch, which is based on a programming language named Lua. PyTorch, as the name suggests, has a Pythonic interface, and it has gained popularity in recent years for its ease of use and the flexibility provided by its various components, such as TorchVision for computer vision, TorchAudio for audio processing, and TorchText for natural <span>language processing.</span></p>
<p class="calibre3">PyTorch <a id="_idIndexMarker1644" class="calibre6 pcalibre pcalibre1"/>uses a dynamic (or “imperative”) computational graph, referred to as a <strong class="bold">define-by-run</strong> approach, where the graph is built on the fly as operations are performed, which enables more intuitive and flexible model development compared to static graphs used in some <span>other frameworks.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">TensorFlow used to only provide the option of using a static computation graph, but in recent years, it has introduced an option named “Eager mode,” which offers dynamic <span>graph functionality.</span></p>
<p class="calibre3">Also like TensorFlow, PyTorch supports GPU acceleration (via NVIDIA’s CUDA framework) for tensor computations, which significantly speeds up model training <span>and inference.</span></p>
<p class="calibre3">On the topic of speeding up model training, it’s a common practice in computing to get more work done faster by performing operations in parallel across multiple devices. In the next section, we will discuss the practice of achieving this goal via <span>distributed training.</span></p>
<h1 id="_idParaDest-300" class="calibre5"><a id="_idTextAnchor362" class="calibre6 pcalibre pcalibre1"/>Large-scale distributed model training</h1>
<p class="calibre3">Think <a id="_idIndexMarker1645" class="calibre6 pcalibre pcalibre1"/>back to the discussion we had in the <em class="italic">AI/ML and cloud computing</em> section of <a href="B18143_01.xhtml#_idTextAnchor015" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 1</em></span></a>, in which I described the process of scaling our models to larger sizes, starting with small models that we could train on our laptop, progressing to larger models trained on a powerful, high-end server, and eventually getting to the scale at which a single computer (even the most powerful server on the market) couldn’t handle either the size of the model or the dataset on which the model is trained. In this section, we’ll look at what it means to train such large-scale models in <span>more detail.</span></p>
<p class="calibre3">We’ve covered the model training process in great detail throughout this book, but I’ll briefly summarize the process here as a knowledge refresher because these concepts are important when discussing large-scale distributed model training. For this discussion, I will focus on supervised training of <span>neural networks.</span></p>
<p class="calibre3">The <a id="_idIndexMarker1646" class="calibre6 pcalibre pcalibre1"/>supervised training process, at a high level, works <span>as follows:</span></p>
<ol class="calibre7">
<li class="calibre8">Instances from our training dataset are fed into <span>the model.</span></li>
<li class="calibre8">The model algorithm or network processes the training instances, and for each instance, it tries to predict the target variable. In the case of neural networks, this is referred to as <span>forward propagation.</span></li>
<li class="calibre8">After making <a id="_idIndexMarker1647" class="calibre6 pcalibre pcalibre1"/>a prediction, the model calculates the error or loss using a loss function (for example, <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) for regression or cross-entropy for classification), which measures the difference between the model’s prediction and the actual label (the <span>ground truth).</span></li>
<li class="calibre8">The backpropagation process then begins, which applies the chain rule from calculus to compute gradients of the loss function concerning each weight in <span>the network.</span></li>
<li class="calibre8">An optimization algorithm such as gradient descent makes use of the gradients that were calculated during backpropagation to update the weights in <span>the network.</span></li>
</ol>
<p class="calibre3">Training is usually performed by looping through the preceding process multiple times in a series of epochs, where an epoch is one complete pass through the entire training dataset. However, when using large datasets, the training process can divide the overall dataset into smaller batches, and each pass through a batch is considered a training step. Also, some frameworks, such as TensorFlow, allow us to specify the number of training steps per epoch. Over time, if the model is learning effectively, its predictions will become more accurate after each epoch, and the loss will decrease (hopefully to an <span>acceptable threshold).</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The term <strong class="bold">processor</strong> in these <a id="_idIndexMarker1648" class="calibre6 pcalibre pcalibre1"/>discussions refers to CPUs, GPUs, and TPUs. Also, the concepts we will discuss in this section can apply to training workloads that are distributed across multiple processors in a single machine, or across multiple processors on <span>multiple machines.</span></p>
<p class="callout">In distributed training use cases, some communication and data sharing is required among the processors. When using a single machine, the processors usually share the same system memory (<strong class="bold">RAM</strong>), which simplifies data sharing, but when using multiple machines, communication needs to happen over a network (including routers, switches, and more), which can introduce latency and <span>additional complexity.</span></p>
<p class="calibre3">We’ve <a id="_idIndexMarker1649" class="calibre6 pcalibre pcalibre1"/>implemented the training process many times previously in this book. Next, we’ll discuss how this works when we’re using <span>multiple processors.</span></p>
<h2 id="_idParaDest-301" class="calibre9"><a id="_idTextAnchor363" class="calibre6 pcalibre pcalibre1"/>Data parallelism and model parallelism</h2>
<p class="calibre3">There <a id="_idIndexMarker1650" class="calibre6 pcalibre pcalibre1"/>are generally two main reasons <a id="_idIndexMarker1651" class="calibre6 pcalibre pcalibre1"/>we would need to implement distributed <span>training workloads:</span></p>
<ul class="calibre16">
<li class="calibre8">We want to use a very <span>large dataset</span></li>
<li class="calibre8">We want to train a very <span>large model</span></li>
</ul>
<p class="calibre3">Note <a id="_idIndexMarker1652" class="calibre6 pcalibre pcalibre1"/>that both scenarios may exist <a id="_idIndexMarker1653" class="calibre6 pcalibre pcalibre1"/>at the same time (that is, we may need to train a very large model on a very large dataset). In this section, I’ll explain each of these scenarios in detail, starting with the case of using <span>large datasets.</span></p>
<h4 class="calibre20">Data parallelism</h4>
<p class="calibre3">Sometimes, we need to train our models on huge datasets that would take a long time to process on just one processor. To make the training process more efficient, we can split the <a id="_idIndexMarker1654" class="calibre6 pcalibre pcalibre1"/>dataset into smaller subsets or batches and process them in parallel across multiple processors. In this case, we can have a copy of our model running on each processor, which works through the subset of data that is loaded onto that processor. A simple example would be if we had a dataset that contained 10,000 data points and we ran 10 processors that each worked on 1,000 data points from our dataset. This approach is referred to as <strong class="bold">data parallelism</strong>, and it is depicted in <span><em class="italic">Figure 14</em></span><span><em class="italic">.5</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer182">
<img alt="Figure 14.4: Data parallelism" src="image/B18143_14_005.jpg" class="calibre175"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.4: Data parallelism</p>
<p class="calibre3">In <span><em class="italic">Figure 14</em></span><em class="italic">.5</em>, each purple box represents a batch of our data, and each batch is sent to a separate processor in parallel. Each batch in the diagram contains only a few data points, but in reality, the batches would be much larger (this diagram intends to illustrate the concept at a high level). Next, we’ll discuss the other main scenario in which we may need to implement a distributed training workload. This is referred to as <span>model parallelism.</span></p>
<h4 class="calibre20">Model parallelism</h4>
<p class="calibre3">Sometimes, the model itself is so large that it cannot fit in memory on a single processor. In these cases, we need to spread the model across multiple processors, where each <a id="_idIndexMarker1655" class="calibre6 pcalibre pcalibre1"/>processor handles a different portion or segment of the model. A segment could be a layer in the network, or pieces of layers in the case of some very complex models. <span><em class="italic">Figure 14</em></span><em class="italic">.6</em> shows a simple example of model parallelism, in which each layer of our neural network runs on a <span>separate processor:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer183">
<img alt="Figure 14.5: Model parallelism" src="image/B18143_14_006.jpg" class="calibre176"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.5: Model parallelism</p>
<p class="calibre3">There are different methods we can use to break our model down into segments, and we generally want to choose an approach that minimizes communication overhead between the processors. This is because that part of the process can introduce the most latency and complexity, especially when we’re using multiple machines where communication needs to happen over a <span>physical network.</span></p>
<p class="calibre3">In addition <a id="_idIndexMarker1656" class="calibre6 pcalibre pcalibre1"/>to optimizing communication between the processors, we also need to design our training procedure in a way that maximizes the utilization of each processor. Remember that the outputs from one layer in our neural network become inputs to other layers in our network, so there are <strong class="bold">sequential dependencies</strong> among the layers. When our layers are distributed across different processors, some processors could be idle while they wait for inputs to be propagated from the previous layer in the network. This, of course, would be an inefficient use of our processing <a id="_idIndexMarker1657" class="calibre6 pcalibre pcalibre1"/>resources, and we can use an approach called <strong class="bold">pipelining</strong> to improve the training efficiency by breaking the input data up into micro-batches. In that case, each segment or stage of the model processes its micro-batch and passes the outputs to the next stage. Then, while the next stage begins to process the outputs, the previous stage can start processing the next micro-batch, and so on. In this way, data flows through the network in a more streamlined manner, rather than processors for later layers being idle while waiting for an earlier layer to process an entire <span>large dataset.</span></p>
<p class="calibre3">So far, I’ve been mainly talking about how to parallelize the training procedure in terms of how the models process the inputs (that is, the forward pass in the training process). Remember that the training procedure also includes a feedback loop to check the outputs of the model, compute the loss against the ground truth, then compute the gradients of the loss <a id="_idIndexMarker1658" class="calibre6 pcalibre pcalibre1"/>and update the weights. Next, we will dive into how those steps can be implemented in a <span>distributed manner.</span></p>
<h2 id="_idParaDest-302" class="calibre9"><a id="_idTextAnchor364" class="calibre6 pcalibre pcalibre1"/>Distributed training update process</h2>
<p class="calibre3">In the case of model parallelism, data is still processed through the network sequentially – that is, each layer (or segment) in the network produces outputs, and those outputs become <a id="_idIndexMarker1659" class="calibre6 pcalibre pcalibre1"/>the inputs for the next layer (even though the layers or segments are being processed on different processors). Because of this, the backpropagation process can also happen sequentially, where <a id="_idIndexMarker1660" class="calibre6 pcalibre pcalibre1"/>gradients are computed in each segment starting from the last to the first, similar to how it’s done in a non-distributed setting. Each segment independently updates its portion of the model weights based on the <span>computed gradients.</span></p>
<p class="calibre3">In the case of data parallelism, however, subsets of the training data are processed in parallel across different processors, so the overall workflow is not sequential. Each processor has an identical copy of the model, but each copy works on a different subset of the data. For this reason, the individual replicas of the model on each processor are unaware of what’s being done on the other processors. Therefore, some additional coordination is needed when computing the loss, the gradients, and <span>the weights.</span></p>
<p class="calibre3">We can implement this coordination in various ways, but we should generally pick one of the following <span>two options:</span></p>
<ul class="calibre16">
<li class="calibre8">Use a <a id="_idIndexMarker1661" class="calibre6 pcalibre pcalibre1"/>centralized <strong class="bold">parameter server</strong> to do <span>the coordination</span></li>
<li class="calibre8">Implement a protocol that allows the individual training servers to act as a community (I will refer to this as “<span>collaborative computing”)</span></li>
</ul>
<p class="calibre3">Let’s <a id="_idIndexMarker1662" class="calibre6 pcalibre pcalibre1"/>discuss each of these approaches in more <a id="_idIndexMarker1663" class="calibre6 pcalibre pcalibre1"/>detail, starting with the parameter <span>server approach.</span></p>
<h3 class="calibre11">The parameter server approach</h3>
<p class="calibre3">In the <a id="_idIndexMarker1664" class="calibre6 pcalibre pcalibre1"/>parameter server approach, one or more nodes in the distributed system are designated as parameter <a id="_idIndexMarker1665" class="calibre6 pcalibre pcalibre1"/>servers, and these nodes hold the global model parameters. This means that they have a global view of the gradients and parameters for all of the model replicas across all nodes in the distributed training system <span>or cluster.</span></p>
<p class="calibre3">In this case, the individual training nodes (let’s call them “worker nodes”) process different subsets of data and compute their respective gradients, and then each worker node sends its gradients to the centralized parameter server(s). The parameter server(s) then update the global model parameters based on these gradients and send the updated parameters back to the worker nodes. A simplified architecture diagram for this approach is shown in <span><em class="italic">Figure 14</em></span><span><em class="italic">.7</em></span><span>:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer184">
<img alt="Figure 14.6: The parameter server approach" src="image/B18143_14_007.jpg" class="calibre177"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.6: The parameter server approach</p>
<p class="calibre3">In <span><em class="italic">Figure 14</em></span><em class="italic">.7</em>, the arrows represent the interchange of gradients and weight updates between the workers and the parameters server(s). Dashed lines and borders represent that there could be one or more parameter servers because a single parameter server could become a bottleneck in the solution. There also needs to be a coordinating entity in <a id="_idIndexMarker1666" class="calibre6 pcalibre pcalibre1"/>this architecture to manage all of the resources and their communication. This could be co-located <a id="_idIndexMarker1667" class="calibre6 pcalibre pcalibre1"/>in the parameter server or could be implemented as a <span>separate server.</span></p>
<p class="calibre3">Next, we’ll discuss the other option for implementing model training with data parallelism where, instead of using centralized servers for performing the coordination steps, all of the workers collaborate directly with <span>each other.</span></p>
<h3 class="calibre11">The collaborative computing approach using Ring All-Reduce</h3>
<p class="calibre3">In the collaborative computing approach, there is no central server or set of servers that are <a id="_idIndexMarker1668" class="calibre6 pcalibre pcalibre1"/>dedicated to the purpose of performing coordination among the independent worker nodes in the training cluster. Instead, a distributed computing protocol <a id="_idIndexMarker1669" class="calibre6 pcalibre pcalibre1"/>is used for coordination <a id="_idIndexMarker1670" class="calibre6 pcalibre pcalibre1"/>among the nodes. This means that the nodes get updates (in this case, the computed gradients) from each other. Numerous communication protocols exist for coordination across distributed computing systems, and a pretty popular one is<a id="_idIndexMarker1671" class="calibre6 pcalibre pcalibre1"/> known as <strong class="bold">All-Reduce</strong>. These kinds of protocols are generally designed to aggregate data across all nodes in a distributed system and then share the result back to all nodes. This means that, even though each node may process a different subset of our dataset, and the model instance on each node may therefore compute different gradients and weights, each node eventually ends up with an aggregated representation of the gradients and weights that is consistent across <span>all nodes.</span></p>
<p class="calibre3">When using All-Reduce, we can connect the processors in various ways, such as by implementing a fully connected mesh, as depicted in <em class="italic">14.8</em>, a ring architecture, as depicted in <span><em class="italic">Figure 14</em></span><em class="italic">.9</em>, or other approaches, such as tree and butterfly architectures, which we will not <span>cover here:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer185">
<img alt="Figure 14.7: Fully connected mesh" src="image/B18143_14_008.jpg" class="calibre178"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.7: Fully connected mesh</p>
<p class="calibre3">As shown in <span><em class="italic">Figure 14</em></span><em class="italic">.8</em>, the fully connected mesh architecture, in which all nodes are connected <a id="_idIndexMarker1672" class="calibre6 pcalibre pcalibre1"/>to all other nodes, would require a lot of complex coordination among <a id="_idIndexMarker1673" class="calibre6 pcalibre pcalibre1"/>all of the nodes. For <a id="_idIndexMarker1674" class="calibre6 pcalibre pcalibre1"/>this reason, the simpler and more effective ring architecture is <span>often chosen:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer186">
<img alt="Figure 14.8: Ring architecture" src="image/B18143_14_009.jpg" class="calibre179"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.8: Ring architecture</p>
<p class="calibre3">In the ring architecture, as the name implies, the nodes are logically arranged in a ring. One of <a id="_idIndexMarker1675" class="calibre6 pcalibre pcalibre1"/>the important aspects of this architecture, referred to as <strong class="bold">Ring All-Reduce</strong>, is that each node only needs to send its computed gradients to one of its neighbors. The neighbor then combines (aggregates) the received gradients with its own gradients, passes <a id="_idIndexMarker1676" class="calibre6 pcalibre pcalibre1"/>the aggregated gradients onto the next node in the ring, and so on. This aggregation is often as simple as adding the gradients together. After <em class="italic">N-1</em> steps (where <em class="italic">N</em> is the number of nodes), each node will have a copy of the aggregated gradients for all nodes. They can then simply perform a division operation to calculate the average gradients, and use those values in the <span>optimization step.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">To be more specific, the Ring All-Reduce algorithm is usually broken down into two phases, referred to as the “reduce-scatter” phase and the “all-gather” phase, but that’s a level of detail that’s not necessary for understanding how the process works at a high level, as described in <span>this section.</span></p>
<p class="calibre3">At this point, you might be wondering which approach is better – that is, using a centralized parameter server or using a collaborative approach such as Ring All-Reduce. The answer, as is <a id="_idIndexMarker1677" class="calibre6 pcalibre pcalibre1"/>the case in many<a id="_idIndexMarker1678" class="calibre6 pcalibre pcalibre1"/> solution architecture decisions, is. “It depends.” Let’s discuss some additional factors that play into this decision, such as whether we want to implement synchronous versus asynchronous <span>data parallelism.</span></p>
<h3 class="calibre11">Synchronous versus asynchronous data parallelism</h3>
<p class="calibre3">In <a id="_idIndexMarker1679" class="calibre6 pcalibre pcalibre1"/>the case of synchronous data parallelism, all the nodes need to update their parameters at the <a id="_idIndexMarker1680" class="calibre6 pcalibre pcalibre1"/>same time, or <a id="_idIndexMarker1681" class="calibre6 pcalibre pcalibre1"/>at least in a coordinated way. A benefit of this approach is that it provides consistency in parameter updates across all of the nodes. However, it can result in bottlenecks that slow down the overall training process, because all nodes must wait for the <span>slowest one.</span></p>
<p class="calibre3">In the case of asynchronous data parallelism, on the other hand, the nodes update their parameters independently without waiting for others, which can lead to faster training. However, this can sometimes cause problems with model convergence and stability because the nodes can go out of sync if the procedure is not <span>implemented correctly.</span></p>
<p class="calibre3">Asynchronous implementations can also be more fault tolerant because the nodes are not dependent on each other, so if a node goes out of service for some reason, the rest of <a id="_idIndexMarker1682" class="calibre6 pcalibre pcalibre1"/>the cluster <a id="_idIndexMarker1683" class="calibre6 pcalibre pcalibre1"/>can continue <a id="_idIndexMarker1684" class="calibre6 pcalibre pcalibre1"/>to function, so long as we have set up the environment in a <span>fault-tolerant manner.</span></p>
<p class="calibre3">All-Reduce is generally implemented synchronously because the nodes need to share their gradients, whereas the parameter server approach can be implemented either synchronously or asynchronously, with asynchronous usually being the more <span>common choice.</span></p>
<p class="calibre3">Next, we’ll learn about how Google Cloud Vertex AI provides an optimized All-Reduce mechanism via the Vertex AI <span>Reduction Server.</span></p>
<h3 class="calibre11">The Vertex AI Reduction Server</h3>
<p class="calibre3">While the Ring All-Reduce approach is pretty well established and popular in the industry, it also has some limitations. For example, in practice, it is found that latency tends to <a id="_idIndexMarker1685" class="calibre6 pcalibre pcalibre1"/>scale linearly with the <a id="_idIndexMarker1686" class="calibre6 pcalibre pcalibre1"/>number of workers in the ring (that is, the more workers we add, the more latency we get). Also, since each worker needs to wait on all other workers in the ring, a single slow worker can slow down the ring overall. Additionally, the ring architecture can have a single point of failure, whereby if one node fails, it can break the <span>entire ring.</span></p>
<p class="calibre3">For these reasons, Google created a product called the Reduction Server, which provides a faster All-Reduce algorithm. In addition to the worker nodes, the Vertex AI Reduction <a id="_idIndexMarker1687" class="calibre6 pcalibre pcalibre1"/>Server also provides <strong class="bold">reducer nodes</strong>. The workers host and execute the model replicas, compute the gradients, and apply the optimization steps, while the reducers have a relatively simple job, which is just to aggregate the gradients from workers. <span><em class="italic">Figure 14</em></span><em class="italic">.10</em> shows an example of the Reduction <span>Server architecture:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer187">
<img alt="Figure 14.9: Reduction Server implementation" src="image/B18143_14_010.jpg" class="calibre180"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.9: Reduction Server implementation</p>
<p class="calibre3">In <span><em class="italic">Figure 14</em></span><em class="italic">.10</em>, you’ll notice a similarity to the parameter server architecture, but bear in mind that the reducers perform much fewer tasks than parameter servers, and this simplicity provides some benefits. For example, the reducers are just lightweight instances <a id="_idIndexMarker1688" class="calibre6 pcalibre pcalibre1"/>that can use CPUs, which, in this case, can be significantly cheaper than GPUs. Also, unlike the ring architecture, the latency does not scale linearly as we add more reducer nodes, and there is no <a id="_idIndexMarker1689" class="calibre6 pcalibre pcalibre1"/>single point of failure that can break the <span>overall architecture.</span></p>
<p class="calibre3">Based on the improvements provided by the Vertex AI Reduction Server architecture, I recommend using this approach when performing distributed training on <span>Vertex AI.</span></p>
<p class="calibre3">Distributed training jobs generally involve using a lot of resources, which could incur expenses, so we will not perform a hands-on activity for this topic in this chapter. If you would like to learn more about how to implement an ML project using distributed training on Vertex AI (optionally, also including the Reduction Server), I recommend referencing the documentation <span>at </span><a href="https://cloud.google.com/vertex-ai/docs/training/distributed-training" class="calibre6 pcalibre pcalibre1"><span>https://cloud.google.com/vertex-ai/docs/training/distributed-training</span></a><span>.</span></p>
<p class="calibre3">Next, we’ll look at some other important factors for <span>distributed training.</span></p>
<h3 class="calibre11">Other important factors for distributed training</h3>
<p class="calibre3">In distributed training, the most complex and error-prone part of the process is usually managing communication among the nodes. If this is not implemented efficiently, then it can cause <a id="_idIndexMarker1690" class="calibre6 pcalibre pcalibre1"/>bottlenecks and impact training performance. We’ve already discussed how we generally want to minimize how much data needs to be sent between the nodes, and fortunately, there are some tricks we can use to help in this regard, such as compressing the gradients before <span>transmitting them.</span></p>
<p class="calibre3">Also, considering that distributed training jobs usually need to process a lot of data, they can often run for long periods, even when the workload is being parallelized across multiple nodes. For this reason, we should put mechanisms in place to help us recover from failures that could occur during the training process. For example, if our job has been running for many hours, we wouldn’t want to have to start it from the beginning again if something fails before the job finishes. A common and important mechanism to implement for <a id="_idIndexMarker1691" class="calibre6 pcalibre pcalibre1"/>this purpose is <strong class="bold">checkpointing</strong>, in which we regularly save the state of the model during training so that the process can continue from a recent state if failure occurs. This is just like periodically saving our work while we’re working on a document just in case our laptop <span>crashes unexpectedly.</span></p>
<p class="calibre3">All of the distributed training concepts we’ve discussed in this section so far assume that we have all of the required data stored somewhere in our environment, and we simply need to distribute that data (or the models) across multiple nodes. However, there’s another popular type of distributed model training referred to as <strong class="bold">federated learning</strong>, which I’ll briefly <span>describe next.</span></p>
<h2 id="_idParaDest-303" class="calibre9"><a id="_idTextAnchor365" class="calibre6 pcalibre pcalibre1"/>Federated learning</h2>
<p class="calibre3">In previous chapters, we discussed deploying ML models to edge devices such as mobile phones or <a id="_idIndexMarker1692" class="calibre6 pcalibre pcalibre1"/>IoT devices, usually to provide low-latency inference <a id="_idIndexMarker1693" class="calibre6 pcalibre pcalibre1"/>on those devices. We also discussed how those edge devices generally have much less computing power and data storage resources than servers have. Therefore, our ability to train models on those devices <span>is limited.</span></p>
<p class="calibre3">Let’s imagine that the models that are deployed to the devices can be improved by being updated with data that becomes available to the devices from their local environment, such as from sensors. If we want to provide the best possible experience for users of those devices, then we will want to ensure that the models are periodically updated to account for new data that becomes available. However, in some cases, it may be inefficient or otherwise undesirable to constantly send data from the edge devices to a central location for larger model <span>training jobs.</span></p>
<p class="calibre3">Federated learning is a technique that enables us to periodically update the models without needing to send data from those devices back to a centralized location. Instead, copies of the model on each device are trained locally by the data that becomes available to the device. The model training process on each device goes through the familiar steps of calculating the loss and the gradients, and then updating the model parameters or weights accordingly. Let’s imagine that this kind of training is being performed <a id="_idIndexMarker1694" class="calibre6 pcalibre pcalibre1"/>on millions of devices, where each device is making updates to the model based on small pieces of data that <a id="_idIndexMarker1695" class="calibre6 pcalibre pcalibre1"/>become available to it. Given the limited training capabilities on each device, this process alone will not lead to a lot of model improvements in the long term. However, in the case of federated learning, the updated weights can be sent back to a central location to be combined to create a more powerful model that can learn from the small training processes on millions of devices. This means that all of the weights from the millions of devices can be averaged (that is, aggregated) to form a more advanced model. This updated model (or an optimized version of it) can then be sent out to each device, and the process can be repeated on an ongoing basis to keep improving the models <span>over time.</span></p>
<p class="calibre3">The effective thing about this process is that only the model weights need to be communicated back to the centralized infrastructure, and none of the actual data on the devices ever needs to be transmitted. This is important for two <span>main reasons:</span></p>
<ul class="calibre16">
<li class="calibre8">The weights are much smaller (in terms of data size) than the training data, so sending only the weights is much more efficient than sending training data from the devices to the <span>central infrastructure</span></li>
<li class="calibre8">Since only the weights are being transmitted (and no actual user data), this helps keep user data safe and <span>preserves privacy</span></li>
</ul>
<p class="calibre3">There are, of course, other methods for implementing distributed model training, but we’ve covered many of the more popular <span>ones here.</span></p>
<p class="calibre3">The remainder of this book will focus on Generative AI, so this is a good point for us to revisit some important topics that we covered at a high level earlier in this book that will help us understand <a id="_idIndexMarker1696" class="calibre6 pcalibre pcalibre1"/>various<a id="_idIndexMarker1697" class="calibre6 pcalibre pcalibre1"/> developments that led to the Generative <span>AI era.</span></p>
<h1 id="_idParaDest-304" class="calibre5"><a id="_idTextAnchor366" class="calibre6 pcalibre pcalibre1"/>Transitioning to Generative AI</h1>
<p class="calibre3">Before we start diving into Generative AI, I’ll provide additional details on some of the important <a id="_idIndexMarker1698" class="calibre6 pcalibre pcalibre1"/>neural network architectures we discussed earlier in this book. For example, in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>, we briefly introduced some common neural <a id="_idIndexMarker1699" class="calibre6 pcalibre pcalibre1"/>network architectures, such as CNNs, RNNs, <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>), and transformers. In this section, we will dive deeper into how some of these notable architectures work, for <span>two reasons:</span></p>
<ul class="calibre16">
<li class="calibre8">The practical exercises accompanying this chapter include building a CNN for a computer vision use case, so it’s important to describe the inner workings of CNNs in <span>more detail</span></li>
<li class="calibre8">The rest of the neural network architectures mentioned here can be seen as milestones in the journey toward developing Generative <span>AI technologies</span></li>
</ul>
<p class="calibre3">Let’s begin with a deeper dive into CNNs and <span>computer vision.</span></p>
<h2 id="_idParaDest-305" class="calibre9"><a id="_idTextAnchor367" class="calibre6 pcalibre pcalibre1"/>CNNs and computer vision</h2>
<p class="calibre3">As we discussed in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>, CNNs are often used in computer vision use cases such as object <a id="_idIndexMarker1700" class="calibre6 pcalibre pcalibre1"/>recognition and visual categorization, and their trick is to break <a id="_idIndexMarker1701" class="calibre6 pcalibre pcalibre1"/>pictures down into smaller components, or “features,” and learn <a id="_idIndexMarker1702" class="calibre6 pcalibre pcalibre1"/>each component separately. In <a id="_idIndexMarker1703" class="calibre6 pcalibre pcalibre1"/>this way, the network learns smaller details in the picture and then combines them to identify larger shapes or <span>objects hierarchically.</span></p>
<h3 class="calibre11">The architecture of CNNs</h3>
<p class="calibre3">We could <a id="_idIndexMarker1704" class="calibre6 pcalibre pcalibre1"/>probably write an entire book on CNN architecture, but that level of detail is not required in this book. I’ll cover some of the important concepts at a high level, and the Jupyter Notebook that accompanies this chapter provides code examples on how to build a relatively simple CNN for a computer vision <span>use case.</span></p>
<p class="calibre3">As a brief refresher of what we discussed in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>, the most basic form of neural network, as depicted in <span><em class="italic">Figure 14</em></span><em class="italic">.11</em>, is referred to as a <strong class="bold">feed-forward neural network</strong> (<strong class="bold">FFNN</strong>), in <a id="_idIndexMarker1705" class="calibre6 pcalibre pcalibre1"/>which the information that is being fed into the network follows a simple forward direction as it passes through <span>the network:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer188">
<img alt="Figure 14.10: A simple neural network" src="image/B18143_14_011.jpg" class="calibre181"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.10: A simple neural network</p>
<p class="calibre3">A CNN, on <a id="_idIndexMarker1706" class="calibre6 pcalibre pcalibre1"/>the other hand, adds some specific types of layers into the architecture that help with implementing the hierarchical processing we mentioned in the introduction to this section. These types of layers will be explained at a high level in the <span>following sub-sections.</span></p>
<h4 class="calibre20">Convolutional layers</h4>
<p class="calibre3">These <a id="_idIndexMarker1707" class="calibre6 pcalibre pcalibre1"/>layers <a id="_idIndexMarker1708" class="calibre6 pcalibre pcalibre1"/>perform a mathematical operation called <strong class="bold">convolution</strong>, which involves sliding a filter (or kernel) over the input image to produce a feature map. The filter is a small matrix that’s used to detect specific features, such as edges, corners, or textures, where each filter is <strong class="bold">convolved</strong> across the input image, computing <a id="_idIndexMarker1709" class="calibre6 pcalibre pcalibre1"/>the dot product between the filter and input, resulting in a <span>feature map.</span></p>
<p class="calibre3">The <a id="_idIndexMarker1710" class="calibre6 pcalibre pcalibre1"/>output of a convolutional layer, therefore, is a set of feature maps that represent specific features that are detected by the filters at different locations in the <span>input image.</span></p>
<h4 class="calibre20">Pooling layers</h4>
<p class="calibre3">Pooling layers are mainly used for dimensionality reduction by downsampling the feature <a id="_idIndexMarker1711" class="calibre6 pcalibre pcalibre1"/>maps and reducing the number of parameters. We covered the concept of dimensionality reduction in detail in <a href="B18143_07.xhtml#_idTextAnchor215" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 7</em></span></a> and discussed how it is <a id="_idIndexMarker1712" class="calibre6 pcalibre pcalibre1"/>often necessary to implement dimensionality reduction to reduce the amount of computation required during training and serving ML models, as well as to reduce the likelihood of overfitting the training data. Different pooling approaches can be used, but the most common approach is called “max pooling,” which takes the maximum value from a set of pixels in a region (defined by the pool size). Another approach, called “average pooling,” simply takes the average of the values from a set of pixels in <span>a region.</span></p>
<h4 class="calibre20">Fully connected layers</h4>
<p class="calibre3">Combining the convolutional and pooling layers results in the features being extracted from <a id="_idIndexMarker1713" class="calibre6 pcalibre pcalibre1"/>the input images. After the input data is passed through the convolutional and pooling layers, the <a id="_idIndexMarker1714" class="calibre6 pcalibre pcalibre1"/>high-level reasoning in the network is then performed by using fully connected layers. These are the kinds of layers we covered when we discussed FFNNs, where the neurons in each layer have connections to all activations in the previous layer. Just as in the case of FFNNs, the fully connected layers are often used for the final classification or regression task that the network <span>is performing.</span></p>
<p class="calibre3">The final output layer often uses a softmax activation function to output a probability distribution over the classes that the model is trying to identify. Alternatively, for binary classification, a sigmoid function could <span>be used.</span></p>
<p class="callout-heading">Residual networks</p>
<p class="callout">Residual networks, or ResNets, are a type of CNN that was developed to address the problem <a id="_idIndexMarker1715" class="calibre6 pcalibre pcalibre1"/>of vanishing and exploding gradients, something we discussed in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>. They do this by introducing “skip connections,” which are shortcuts that “skip” over layers within the network, and directly connect the output of one layer to the input of a <span>later layer.</span></p>
<p class="calibre3">That’s <a id="_idIndexMarker1716" class="calibre6 pcalibre pcalibre1"/>about as much detail as we’re going to cover regarding the inner workings of CNNs. Many research papers go into great detail on how CNNs <a id="_idIndexMarker1717" class="calibre6 pcalibre pcalibre1"/>work, but that’s a level of detail beyond the scope of this book. Next, we’ll dive in and work on some code examples to build our very first CNN for a computer vision <span>use case!</span></p>
<p class="calibre3">In the Jupyter Notebooks that accompany this chapter, we will use Keras to build a simple CNN. This is where you will also start learning how to use PyTorch, which we’ll then use to perform the <span>same task.</span></p>
<h3 class="calibre11">Building a CNN</h3>
<p class="calibre3">To build <a id="_idIndexMarker1718" class="calibre6 pcalibre pcalibre1"/>our CNNs, open JupyterLab on the Vertex AI Workbench instance you created in <span>Chapter 5</span> and perform the <span>following steps:</span></p>
<ol class="calibre7">
<li class="calibre8">In the navigation panel on the left-hand side of the screen, navigate to the <strong class="source-inline">Chapter-14</strong> directory within the <span><strong class="source-inline">Google-Machine-Learning-for-Solutions-Architects</strong></span><span> folder.</span></li>
<li class="calibre8">Double-click on the <strong class="source-inline">Keras-CV.ipynb</strong> notebook file to open it. When you’re prompted to select a kernel, select the <span><strong class="bold">TensorFlow</strong></span><span> kernel.</span></li>
<li class="calibre8">Double-click on the <strong class="source-inline">PyTorch-CV.ipynb</strong> notebook file to open it. When you’re prompted to select a kernel, select <span><strong class="bold">PyTorch</strong></span><span> kernel.</span></li>
<li class="calibre8">In each of the notebooks you’ve opened, press <em class="italic">Shift</em> + <em class="italic">Enter</em> to execute <span>each cell.</span><p class="calibre3">The notebooks contain comments and Markdown text that describe what the code in each cell <span>is doing.</span></p></li>
</ol>
<p class="calibre3">Take a moment to savor the fact that you have just used both Keras and PyTorch to build <a id="_idIndexMarker1719" class="calibre6 pcalibre pcalibre1"/>CNNs for a computer vision use case on Google Cloud Vertex AI. This is some pretty <span>advanced stuff!</span></p>
<p class="calibre3">Next, we’ll <a id="_idIndexMarker1720" class="calibre6 pcalibre pcalibre1"/>take a look at some types of neural network architectures that are typically useful for sequential data and use cases such as <strong class="bold">natural language </strong><span><strong class="bold">processing</strong></span><span> (</span><span><strong class="bold">NLP</strong></span><span>).</span></p>
<h2 id="_idParaDest-306" class="calibre9"><a id="_idTextAnchor368" class="calibre6 pcalibre pcalibre1"/>RNNs, LSTMs, and transformers</h2>
<p class="calibre3">Again, we already introduced these concepts at a high level in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>. In this section, we will dive a little bit deeper into these types of architectures. We won’t build these <a id="_idIndexMarker1721" class="calibre6 pcalibre pcalibre1"/>networks in this chapter, nor go into excessive detail; I’m covering these topics here mainly for historical context to pave the way for <a id="_idIndexMarker1722" class="calibre6 pcalibre pcalibre1"/>concepts and activities that I’ll introduce in the Generative AI chapters in this book. This is because all of the neural network <a id="_idIndexMarker1723" class="calibre6 pcalibre pcalibre1"/>architectures in this section can be seen as stepping stones toward the development of the Generative AI models we use today, in the order shown in <span><em class="italic">Table 14.1:</em></span></p>
<table class="no-table-style" id="table001-3">
<colgroup class="calibre12">
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
</colgroup>
<tbody class="calibre14">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold">Development</strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold">Timeframe</strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold">References</strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span>RNN</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span>1990</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Elman, J. L. (1990). <em class="italic">Finding structure in time</em>. Cognitive Science, <span>14(2), 179-211.</span></p>
<p class="calibre3"><span>https://doi.org/10.1207/s15516709cog1402_1.</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span>LSTM</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span>1997</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Hochreiter, Sepp &amp; Schmidhuber, Jürgen. (1997). <em class="italic">Long Short-term Memory</em>. Neural computation. 9. <span>1735-80. 10.1162/neco.1997.9.8.1735.</span></p>
<p class="calibre3"><span>https://doi.org/10.1162/neco.1997.9.8.1735.</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span>Transformer</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span>2017</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. <em class="italic">Attention Is All You Need</em>. [2017, June 12]. arXiv preprint <span>arXiv:1706.03762. http://arxiv.org/abs/1706.03762.</span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 14.1: The stepping stones in sequential model development</p>
<p class="calibre3">Let’s begin by diving <span>into RNNs.</span></p>
<h3 class="calibre11">RNNs</h3>
<p class="calibre3">As you read <a id="_idIndexMarker1724" class="calibre6 pcalibre pcalibre1"/>each word in this sentence, you need to <a id="_idIndexMarker1725" class="calibre6 pcalibre pcalibre1"/>remember the words that came before it so that you can build an understanding of what the overall sentence is saying. This is an application of <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>), which is a sub-field of NLP. In a more generic sense, the sentence represents sequential data because the data points (in this case, words) in the sentence relate to other words in the sentence and the order in which they are processed matters. <span><em class="italic">Figure 14</em></span><em class="italic">.12</em> shows an example of sequence data being fed into a <span>neural network:</span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer189">
<img alt="Figure 14.11: Sequence data being fed into a neural network" src="image/B18143_14_012.jpg" class="calibre182"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 14.11: Sequence data being fed into a neural network</p>
<p class="calibre3">In <span><em class="italic">Figure 14</em></span><em class="italic">.12</em>, we can see that data point 1 will be processed first by the neural network, and then data point 2 will be processed, and so on, where each data point is processed one at a time by <span>the network.</span></p>
<p class="calibre3">In simple FFNNs, each data point is passed independently through the network, so the network does not associate data points with previous data points. As we briefly mentioned in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>, RNNs introduce a type of looping mechanism into the network architecture, and this looping mechanism is what enables the network to “remember” previous data points. Let’s look at this in a bit more detail to understand what <span>this means.</span></p>
<p class="calibre3">In RNNs, the concept of each data point being processed by the network is referred to as a <strong class="bold">time step</strong>. The important thing to note is that, with RNNs, the outputs from neurons in each time step can be saved for future reference. Let’s call this the <strong class="bold">current state</strong> of our network; this is often referred to as the <strong class="bold">hidden state</strong> of the network because it is generally not <span>exposed externally.</span></p>
<p class="calibre3">At each time step, neurons in the network receive two sets <span>of inputs:</span></p>
<ul class="calibre16">
<li class="calibre8">The output from neurons in the previous layer of the network (or inputs from the input layer if we’re referring to the first hidden layer in <span>the network).</span></li>
<li class="calibre8">The output from the same neuron (that is, itself) that was saved from the previous time step. This is the <span>hidden state.</span></li>
</ul>
<p class="calibre3">Looking at <a id="_idIndexMarker1726" class="calibre6 pcalibre pcalibre1"/>our diagram in <span><em class="italic">Figure 14</em></span><em class="italic">.12</em>, in the first time step, when the first data point in the sequence (that is, data point 1) passes through the network, the process will be much like a standard FFNN, as we described in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>, because there is no previous state <span>to remember.</span></p>
<p class="calibre3">However, when data point 2 is being processed through the network, each neuron can combine this new data with the “current state” of the network (which is the state that was created after processing data point 1). This process repeats for each new data point that passes through the network, and by doing this, the network maintains a kind of memory of the data points it <span>processed previously.</span></p>
<h4 class="calibre20">Backpropagation through time (BPTT)</h4>
<p class="calibre3">We discussed the backpropagation process in neural network learning in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>. Considering that RNNs use the concept of time steps and combine the outputs of neurons <a id="_idIndexMarker1727" class="calibre6 pcalibre pcalibre1"/>over time, the backpropagation process in RNNs needs to be modified accordingly. This modified <a id="_idIndexMarker1728" class="calibre6 pcalibre pcalibre1"/>version of backpropagation is <span>called </span><span><strong class="bold">BPTT</strong></span><span>.</span></p>
<p class="calibre3">It involves “unrolling” the RNN in time, meaning that the time steps are treated as separate layers of a deep neural network. Once the network has been unrolled, the standard backpropagation algorithm is applied – that is, the network’s error is calculated at each time step, the gradients are computed, and then these gradients are used to update the weights in <span>the network.</span></p>
<h4 class="calibre20">Limitations of RNNs – gradients and short-term memory</h4>
<p class="calibre3">In an RNN, the unrolling that is performed as part of the BPTT process can lead to very deep networks for long sequences. Remember that vanishing and exploding gradients happen <a id="_idIndexMarker1729" class="calibre6 pcalibre pcalibre1"/>because of the chain rule in calculus. As the gradients are backpropagated through the deep networks associated with long sequences, this can make the problem of vanishing and exploding gradients <span>more pronounced.</span></p>
<p class="calibre3">This also <a id="_idIndexMarker1730" class="calibre6 pcalibre pcalibre1"/>affects the “memory” of the RNN. As sequences get longer, the ability for backpropagation to update the gradients of earlier time steps gets smaller and smaller (in the case of vanishing gradients). This has the effect of making the network “forget” earlier time steps in longer sequences, which means that RNNs are generally limited to <span>short sequences.</span></p>
<p class="calibre3">The vanishing <a id="_idIndexMarker1731" class="calibre6 pcalibre pcalibre1"/>and exploding gradient problems and short-term memory limitations <a id="_idIndexMarker1732" class="calibre6 pcalibre pcalibre1"/>have led to the development of more advanced RNN architectures such as LSTMs, which can address these issues. We’ll discuss this in more <span>detail next.</span></p>
<h3 class="calibre11">LSTMs</h3>
<p class="calibre3">LSTM is <a id="_idIndexMarker1733" class="calibre6 pcalibre pcalibre1"/>a type of RNN that was designed specifically to overcome the limitations of traditional RNNs, especially the vanishing and exploding gradient problems when processing long sequences, as we discussed in the <span>previous section.</span></p>
<p class="calibre3">In addition to <a id="_idIndexMarker1734" class="calibre6 pcalibre pcalibre1"/>the hidden state mechanism we described in the <em class="italic">RNNs</em> section, LSTMs add a concept called <strong class="bold">cell state</strong>. This is often likened to a kind of conveyor belt that runs straight through the entire chain of an LSTM network, which allows information to be easily transported across many steps in the process. The hidden state still acts as the network’s short-term memory, and the cell state acts as a <span>longer-term memory.</span></p>
<p class="calibre3">LSTMs also <a id="_idIndexMarker1735" class="calibre6 pcalibre pcalibre1"/>introduce the concepts of <strong class="bold">gates</strong>, which can be seen as checkpoints along the conveyor belt that decide what information should be maintained, dropped, or added. In this way, the gates regulate the flow of information through the network. There are generally three types <span>of gates:</span></p>
<ul class="calibre16">
<li class="calibre8"><strong class="bold">Input gates</strong>, which <a id="_idIndexMarker1736" class="calibre6 pcalibre pcalibre1"/>update the <a id="_idIndexMarker1737" class="calibre6 pcalibre pcalibre1"/>cell state with <span>new information</span></li>
<li class="calibre8"><strong class="bold">Output gates</strong>, which <a id="_idIndexMarker1738" class="calibre6 pcalibre pcalibre1"/>use the cell state to determine what the <a id="_idIndexMarker1739" class="calibre6 pcalibre pcalibre1"/>next hidden state <span>should be</span></li>
<li class="calibre8"><strong class="bold">Forget gates</strong>, which <a id="_idIndexMarker1740" class="calibre6 pcalibre pcalibre1"/>decide what information to discard <a id="_idIndexMarker1741" class="calibre6 pcalibre pcalibre1"/>from the <span>cell state</span></li>
</ul>
<p class="calibre3">If you’d like to learn more about how these gates are used in combination with the hidden state and the cell state, I recommend reading the original paper (Hochreiter and Schmidhuber, 1997) <span>at </span><a href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory" class="calibre6 pcalibre pcalibre1"><span>https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory</span></a><span>.</span></p>
<p class="calibre3">Speaking of gates, a variation <a id="_idIndexMarker1742" class="calibre6 pcalibre pcalibre1"/>on LSTMs called <strong class="bold">gated recurrent unit</strong> (<strong class="bold">GRU</strong>) was more recently introduced (Cho et al., 2014). It combines the forget and input <a id="_idIndexMarker1743" class="calibre6 pcalibre pcalibre1"/>gates into a single <strong class="bold">update gate</strong> and merges the cell state and hidden state, which results in a simpler and more <span>efficient model.</span></p>
<p class="calibre3">Next, we’ll discuss one of the most important breakthroughs in the AI/ML industry in recent years: the <span>transformer architecture.</span></p>
<h3 class="calibre11">Transformers</h3>
<p class="calibre3">As is the case in most fields of research, the pace of advancement generally proceeds at a somewhat linear rate, with occasional transformative breakthroughs along the timeline. The linear <a id="_idIndexMarker1744" class="calibre6 pcalibre pcalibre1"/>advancement in any field represents the step-by-step progress that occurs on an ongoing basis, and the transformative breakthroughs represent the sudden, giant leaps forward that completely change everything. When Google invented the transformer architecture (Vaswani, A., et al., 2017), this was one such giant leap forward in AI/ML research, and it has formed the basis of the Generative AI revolution that we are currently experiencing in <span>the world.</span></p>
<p class="calibre3">The progression from RNNs to LSTMs is what I would consider to be a linear development. However, the progression from LSTMs to the transformer architecture was an enormous jump forward in the development of <span>AI/ML technologies.</span></p>
<p class="calibre3">Firstly, unlike RNNs and LSTMs, which process sequences step by step, transformers can handle entire sequences simultaneously (in parallel), and this parallel processing allows them to manage long-range dependencies in our data much more efficiently. This also allows them to achieve larger scales than previous model architectures, resulting in what are now the largest and most powerful language models in the industry. However, parallel processing is not the most significant feature of the transformer architecture. As suggested in the title of the paper that first introduced the transformer architecture to the world – <em class="italic">Attention Is All You Need</em> (arXiv:1706.03762 [cs.CL]) – the main innovation in transformers is the self-attention mechanism, which allows the model to weigh the importance of different parts of the input data. For example, if we feed a sentence (or sequence) into a <a id="_idIndexMarker1745" class="calibre6 pcalibre pcalibre1"/>transformer model, the self-attention mechanism allows the model to understand the entire sentence and its internal relationships (for example, the relationships between <span>the words).</span></p>
<p class="calibre3">Transformers <a id="_idIndexMarker1746" class="calibre6 pcalibre pcalibre1"/>run multiple self-attention <strong class="bold">heads</strong> in parallel on the sequence, something that’s referred to as <strong class="bold">multi-head attention</strong> in the paper. Hopefully, you already read through the paper when we linked it in <a href="B18143_09.xhtml#_idTextAnchor245" class="calibre6 pcalibre pcalibre1"><span><em class="italic">Chapter 9</em></span></a>, but if not, I highly recommend reading the original paper to dive into the intricate details of how transformers work, given their pivotal importance in the AI/ML and Generative AI industry. Transformers are the basis of the current state-of-the-art Generative <span>AI models.</span></p>
<p class="calibre3">Now that we’ve recapped<a id="_idIndexMarker1747" class="calibre6 pcalibre pcalibre1"/> the evolutionary steps that led to their development, we are ready to move on to the next chapter. But first, let’s take a moment to reflect on what <span>we’ve learned.</span></p>
<h1 id="_idParaDest-307" class="calibre5"><a id="_idTextAnchor369" class="calibre6 pcalibre pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we covered important AI/ML concepts that are typically used by more advanced practitioners who have specific needs or preferences in terms of how they want to implement their ML workloads, such as by using specific frameworks, or by parallelizing their workloads across <span>multiple processors.</span></p>
<p class="calibre3">We started by discussing BQML, focusing on the close relationship between data processing, data analytics, and ML. We learned how to train and evaluate a model using BQML, and how to get predictions from that model, all by using SQL <span>query syntax.</span></p>
<p class="calibre3">Then, we discussed different types of hardware that we can use for our AI/ML workloads, and popular tools and frameworks that we had not previously covered in this book, such as Spark MLlib, Ray, <span>and PyTorch.</span></p>
<p class="calibre3">Next, we dived into CNNs and their use in computer vision before moving on to discuss neural network architectures that are particularly useful for sequential data and use cases such as NLP, such as RNNs, LSTMs, and transformers. We dived into the inner workings of these architectures, mainly to understand the evolutionary steps that have led to the development of the Generative AI technologies that we will outline in the remaining chapters of <span>this book.</span></p>
<p class="calibre3">Finally, we discussed distributed model training in AI/ML, including why it’s needed, and some of the mechanisms we can use to implement it, such as data parallelism and model parallelism. We also dived into different approaches for implementing data parallelism, such as using centralized parameter servers, or distributed coordination mechanisms such as Ring All-Reduce. We also explored the topic of federated learning, and how it can help to preserve privacy by avoiding the process of transmitting data <span>from devices.</span></p>
<p class="calibre3">Next, we will embark on an exciting journey as we dive into the world of Generative AI. Join me in the next chapter to <span>get started!</span></p>
</div>
</div></body></html>