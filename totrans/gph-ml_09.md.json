["```py\nJupyter==1.0.0\nnetworkx==2.5\nscikit-learn==0.24.0 \nnumpy==1.19.2 \nnode2vec==0.3.3 \ntensorflow==2.4.1 \nstellargraph==1.2.1\ncommunities==2.2.0 \ngit+https://github.com/palash1992/GEM.git\n```", "```py\nG = nx.read_edgelist(\"facebook_combined.txt\", create_using=nx.Graph(), nodetype=int)\n```", "```py\nprint(nx.info(G))\n```", "```py\nName: \nType: Graph\nNumber of nodes: 4039\nNumber of edges: 88234\nAverage degree:  43.6910\n```", "```py\nnx.draw_networkx(G, pos=spring_pos, with_labels=False, node_size=35)\n```", "```py\nego_nodes = set([int(name.split('.')[0]) for name in os.listdir(\"facebook/\")])\n```", "```py\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n```", "```py\nassortativity = nx.degree_pearson_correlation_coefficient(G)\n```", "```py\n0.06357722918564912\n```", "```py\nt = nx.transitivity(G)\n```", "```py\n0.5191742775433075\n```", "```py\naC = nx.average_clustering(G)\n```", "```py\n0.6055467186200876\n```", "```py\nbC = nx.betweenness_centrality(G)\n np.mean(list(bC.values()))\n```", "```py\n0.0006669573568730229\n```", "```py\ndef draw_metric(G, dct, spring_pos):\n  top = 10\n  max_nodes =  sorted(dct.items(), key=lambda v: -v[1])[:top]\n  max_keys = [key for key,_ in max_nodes]\n  max_vals = [val*300 for _, val in max_nodes]\n  plt.axis(\"off\")\n  nx.draw_networkx(G,\n                   pos=spring_pos,\n                   cmap='Blues',\n                   edge_color=default_edge_color,\n                   node_color=default_node_color,\n                   node_size=3,\n                   alpha=0.4,\n                   with_labels=False)\n  nx.draw_networkx_nodes(G,\n                         pos=spring_pos,\n                         nodelist=max_keys,\n                         node_color=enhanced_edge_color,\n                         node_size=max_vals)\n```", "```py\ndraw_metric(G,bC,spring_pos)\n```", "```py\ndeg_C = nx.degree_centrality(G)\n np.mean(list(deg_C.values()))\ndraw_metric(G,deg_C,spring_pos)\n```", "```py\n0.010819963503439287\n```", "```py\nclos_C = nx.closeness_centrality(G)\n np.mean(list(clos_C.values()))\ndraw_metric(G,clos_C,spring_pos)\n```", "```py\n0.2761677635668376\n```", "```py\nimport community\nparts = community.best_partition(G)\n values = [parts.get(node) for node in G.nodes()]\nn_sizes = [5]*len(G.nodes())\nplt.axis(\"off\")\nnx.draw_networkx(G, pos=spring_pos, cmap=plt.get_cmap(\"Blues\"), edge_color=default_edge_color, node_color=values, node_size=n_sizes, with_labels=False)\n```", "```py\nfor node in ego_nodes:\n   n_sizes[node] = 250\nnodes = nx.draw_networkx_nodes(G,spring_pos,ego_nodes,node_color=[parts.get(node) for node in ego_nodes])\n nodes.set_edgecolor(enhanced_node_color)\n```", "```py\nfrom sklearn.model_selection import train_test_split\nfrom stellargraph.data import EdgeSplitter\nfrom stellargraph import StellarGraph\nedgeSplitter = EdgeSplitter(G) \ngraph_test, samples_test, labels_test = edgeSplitter.train_test_split(p=0.1, method=\"global\", seed=24)\nedgeSplitter = EdgeSplitter(graph_test, G)\n graph_train, samples_train, labels_train = edgeSplitter.train_test_split(p=0.1, method=\"global\", seed=24)\n```", "```py\n    from node2vec import Node2Vec\n    node2vec = Node2Vec(graph_train) \n    model = node2vec.fit()\n    ```", "```py\n    from node2vec.edges import HadamardEmbedder\n    edges_embs = HadamardEmbedder(keyed_vectors=model.wv)\n     train_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_train]\n    ```", "```py\n    from sklearn.ensemble import RandomForestClassifier \n    from sklearn import metrics \n    rf = RandomForestClassifier(n_estimators=10)\n     rf.fit(train_embeddings, labels_train);\n    ```", "```py\n    edges_embs = HadamardEmbedder(keyed_vectors=model.wv) test_embeddings = [edges_embs[str(x[0]),str(x[1])] for x in samples_test]\n    ```", "```py\n    y_pred = rf.predict(test_embeddings) \n    print('Precision:', metrics.precision_score(labels_test, y_pred)) \n    print('Recall:', metrics.recall_score(labels_test, y_pred)) \n    print('F1-Score:', metrics.f1_score(labels_test, y_pred)) \n    ```", "```py\n    Precision: 0.9701333333333333\n    Recall: 0.9162573983125551\n    F1-Score: 0.9424260086781945\n    ```", "```py\neye = np.eye(graph_train.number_of_nodes())\nfake_features = {n:eye[n] for n in G.nodes()}\nnx.set_node_attributes(graph_train, fake_features, \"fake\")\neye = np.eye(graph_test.number_of_nodes())\nfake_features = {n:eye[n] for n in G.nodes()}\nnx.set_node_attributes(graph_test, fake_features, \"fake\")\n```", "```py\nfrom stellargraph.mapper import GraphSAGELinkGenerator\nbatch_size = 64\nnum_samples = [4, 4]\n# convert graph_train and graph_test for stellargraph\nsg_graph_train = StellarGraph.from_networkx(graph_train, node_features=\"fake\")\nsg_graph_test = StellarGraph.from_networkx(graph_test, node_features=\"fake\")\ntrain_gen = GraphSAGELinkGenerator(sg_graph_train, batch_size, num_samples)\n train_flow = train_gen.flow(samples_train, labels_train, shuffle=True, seed=24)\ntest_gen = GraphSAGELinkGenerator(sg_graph_test, batch_size, num_samples)\n test_flow = test_gen.flow(samples_test, labels_test, seed=24)\n```", "```py\nfrom stellargraph.layer import GraphSAGE, link_classification\nfrom tensorflow import keras\nlayer_sizes = [20, 20]\ngraphsage = GraphSAGE(layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3)\nx_inp, x_out = graphsage.in_out_tensors()\n# define the link classifier\nprediction = link_classification(output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\")(x_out)\nmodel = keras.Model(inputs=x_inp, outputs=prediction)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr=1e-3),\n    loss=keras.losses.mse,\n    metrics=[\"acc\"],\n)\n```", "```py\nepochs = 10\nhistory = model.fit(train_flow, epochs=epochs, validation_data=test_flow)\n```", "```py\nEpoch 18/20\nloss: 0.4921 - acc: 0.8476 - val_loss: 0.5251 - val_acc: 0.7884\nEpoch 19/20\nloss: 0.4935 - acc: 0.8446 - val_loss: 0.5247 - val_acc: 0.7922\nEpoch 20/20\nloss: 0.4922 - acc: 0.8476 - val_loss: 0.5242 - val_acc: 0.7913\n```", "```py\nfrom sklearn import metrics \ny_pred = np.round(model.predict(train_flow)).flatten()\nprint('Precision:', metrics.precision_score(labels_train, y_pred)) \nprint('Recall:', metrics.recall_score(labels_train, y_pred))  print('F1-Score:', metrics.f1_score(labels_train, y_pred)) \n```", "```py\nPrecision: 0.7156476303969199\nRecall: 0.983125550938169\nF1-Score: 0.8283289124668435\n```", "```py\nload_features()\nparse_nodes(G, ego_nodes)\n```", "```py\nprint(G.nodes[0])\n```", "```py\n{'features': array([1., 1., 1., ..., 0., 0., 0.])}\n```", "```py\nsg_graph_train = StellarGraph.from_networkx(graph_train, node_features=\"features\")\nsg_graph_test = StellarGraph.from_networkx(graph_test, node_features=\"features\")\n```", "```py\ntrain_gen = GraphSAGELinkGenerator(sg_graph_train, batch_size, num_samples)\ntrain_flow = train_gen.flow(samples_train, labels_train, shuffle=True, seed=24)\ntest_gen = GraphSAGELinkGenerator(sg_graph_test, batch_size, num_samples)\ntest_flow = test_gen.flow(samples_test, labels_test, seed=24)\nlayer_sizes = [20, 20]\ngraphsage = GraphSAGE(layer_sizes=layer_sizes, generator=train_gen, bias=True, dropout=0.3)\nx_inp, x_out = graphsage.in_out_tensors()\nprediction = link_classification(output_dim=1, output_act=\"sigmoid\", edge_embedding_method=\"ip\")(x_out)\nmodel = keras.Model(inputs=x_inp, outputs=prediction)\nmodel.compile(\n    optimizer=keras.optimizers.Adam(lr=1e-3),\n    loss=keras.losses.mse,\n    metrics=[\"acc\"],\n)\nepochs = 10\nhistory = model.fit(train_flow, epochs=epochs, validation_data=test_flow)\n```", "```py\nEpoch 18/20\nloss: 0.1337 - acc: 0.9564 - val_loss: 0.1872 - val_acc: 0.9387\nEpoch 19/20\nloss: 0.1324 - acc: 0.9560 - val_loss: 0.1880 - val_acc: 0.9340\nEpoch 20/20\nloss: 0.1310 - acc: 0.9585 - val_loss: 0.1869 - val_acc: 0.9365\n```", "```py\nfrom sklearn import metrics \ny_pred = np.round(model.predict(train_flow)).flatten()\nprint('Precision:', metrics.precision_score(labels_train, y_pred)) \nprint('Recall:', metrics.recall_score(labels_train, y_pred)) \nprint('F1-Score:', metrics.f1_score(labels_train, y_pred))\n```", "```py\nPrecision: 0.7895418326693228\nRecall: 0.9982369978592117\nF1-Score: 0.8817084700517213\n```", "```py\nfeat_train = get_hc_features(graph_train, samples_train, labels_train)\nfeat_test = get_hc_features(graph_test, samples_test, labels_test)\n```", "```py\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \nrf = RandomForestClassifier(n_estimators=10) \nrf.fit(feat_train, labels_train); \n```", "```py\ny_pred = rf.predict(feat_test)\nprint('Precision:', metrics.precision_score(labels_test, y_pred))\n print('Recall:', metrics.recall_score(labels_test, y_pred)) print('F1-Score:', metrics.f1_score(labels_test, y_pred)) \n```", "```py\nPrecision: 0.9636952636282395\nRecall: 0.9777853337866939\nF1-Score: 0.9706891701828411\n```"]