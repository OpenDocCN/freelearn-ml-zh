<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Text Mining and Natural Language Processing"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Text Mining and Natural Language Processing</h1></div></div></div><p>
<span class="strong"><strong>Natural language processing</strong></span> (<span class="strong"><strong>NLP</strong></span>) is ubiquitous today in various applications such as mobile <a id="id1470" class="indexterm"/>apps, ecommerce websites, emails, news websites, and more. Detecting spam in e-mails, characterizing e-mails, speech synthesis, categorizing news, searching and recommending products, performing sentiment analysis on social media brands—these are all different aspects of NLP and mining text for information.</p><p>There has been an exponential increase in digital information that is textual in content—in the form of web pages, e-books, SMS messages, documents of various formats, e-mails, social media messages such as tweets and Facebook posts, now ranges in exabytes (an exabyte is 1,018 bytes). Historically, the earliest foundational work relying on automata and probabilistic modeling began in the 1950s. The 1970s saw changes such as stochastic modeling, Markov modeling, and syntactic parsing, but their progress was limited during the 'AI Winter' years. The 1990s saw the emergence of text mining and a statistical revolution that included ideas of corpus statistics, supervised Machine Learning, and human annotation of text data. From the year 2000 onwards, with great progress in computing and Big Data, as well as the introduction of sophisticated Machine Learning algorithms in supervised and unsupervised learning, the area has received rekindled interest and is now among the hottest topics in research, both in academia and the R&amp;D departments of commercial enterprises. In this chapter, we will discuss some aspects of NLP and text mining that are essential in Machine Learning.</p><p>The chapter begins with an introduction to the key areas within NLP, and it then explains the important processing and transformation steps that make the documents more suitable for Machine Learning, whether supervised or unsupervised. The concept of topic modeling, clustering, and named entity recognition follow, with brief descriptions of two Java toolkits that offer powerful text processing capabilities. The case study for this chapter uses another widely-known dataset to demonstrate several techniques described here through experiments using the tools KNIME and Mallet.</p><p>The chapter is organized as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">NLP, subfields, and tasks:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Text categorization</li><li class="listitem" style="list-style-type: disc">POS tagging</li><li class="listitem" style="list-style-type: disc">Text clustering</li><li class="listitem" style="list-style-type: disc">Information extraction and named entity recognition</li><li class="listitem" style="list-style-type: disc">Sentiment analysis</li><li class="listitem" style="list-style-type: disc">Coreference resolution</li><li class="listitem" style="list-style-type: disc">Word-sense disambiguation</li><li class="listitem" style="list-style-type: disc">Machine translation</li><li class="listitem" style="list-style-type: disc">Semantic reasoning and inferencing</li><li class="listitem" style="list-style-type: disc">Summarization</li><li class="listitem" style="list-style-type: disc">Questions and answers</li><li class="listitem" style="list-style-type: disc">Issues with mining and unstructured data</li></ul></div></li><li class="listitem" style="list-style-type: disc">Text processing components and transformations:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Document collection and standardization</li><li class="listitem" style="list-style-type: disc">Tokenization</li><li class="listitem" style="list-style-type: disc">Stop words removal</li><li class="listitem" style="list-style-type: disc">Stemming/Lemmatization</li><li class="listitem" style="list-style-type: disc">Local/Global dictionary</li><li class="listitem" style="list-style-type: disc">Feature extraction/generation</li><li class="listitem" style="list-style-type: disc">Feature representation and similarity</li><li class="listitem" style="list-style-type: disc">Feature selection and dimensionality reduction</li></ul></div></li><li class="listitem" style="list-style-type: disc">Topics in text mining:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Topic modeling</li><li class="listitem" style="list-style-type: disc">Text clustering</li><li class="listitem" style="list-style-type: disc">Named entity recognition</li><li class="listitem" style="list-style-type: disc">Deep learning and NLP</li></ul></div></li><li class="listitem" style="list-style-type: disc">Tools and usage:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Mallet</li><li class="listitem" style="list-style-type: disc">KNIME</li></ul></div></li><li class="listitem" style="list-style-type: disc">Case study</li></ul></div><div class="section" title="NLP, subfields, and tasks"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec66"/>NLP, subfields, and tasks</h1></div></div></div><p>Information <a id="id1471" class="indexterm"/>about the real world exists in the <a id="id1472" class="indexterm"/>form of structured data, typically generated by automated processes, or <a id="id1473" class="indexterm"/>unstructured data, which, in the case of text, is created by direct human agency in the form of the written or spoken word. The process of observing real-world situations and using either automated processes or having humans perceive and convert that information into understandable data is very similar in both structured and unstructured data. The transformation of the observed world into unstructured data involves complexities such as the language of the text, the format in which it exists, variances among different observers in interpreting the same data, and so on. Furthermore, the ambiguity caused by the syntax and semantics of the chosen language, subtlety in expression, the context in the data, and so on, make the task of mining text data very difficult.</p><p>Next, we will discuss some high-level subfields and tasks that involve NLP and text mining. The subject of NLP is quite vast, and the following topics is in no way comprehensive.</p><div class="section" title="Text categorization"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec117"/>Text categorization</h2></div></div></div><p>This <a id="id1474" class="indexterm"/>field is one of the most well-established, and in its basic <a id="id1475" class="indexterm"/>form classifies documents with unstructured text data into predefined categories. This can be viewed as a direct extension of supervised Machine Learning in the unstructured text world, learning from historic documents to predict categories of unseen documents in the future. Basic methods in spam detection in e-mails or news categorization are among some of the most prominent applications of this task.</p><div class="mediaobject"><img src="graphics/B05137_08_001.jpg" alt="Text categorization"/><div class="caption"><p>Figure 1: Text Categorization showing classification into different categories</p></div></div></div><div class="section" title="Part-of-speech tagging (POS tagging)"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec118"/>Part-of-speech tagging (POS tagging)</h2></div></div></div><p>Another <a id="id1476" class="indexterm"/>subtask in NLP that has seen a lot of success is associating parts-of-speech of the language—such as nouns, adjectives, verbs—to words in a text, based on context and relationship <a id="id1477" class="indexterm"/>to adjacent words. Today, instead of manual POS tagging, automated and sophisticated POS taggers perform the job.</p><div class="mediaobject"><img src="graphics/B05137_08_002.jpg" alt="Part-of-speech tagging (POS tagging)"/><div class="caption"><p>Figure 2: POS Tags associated with segment of text</p></div></div></div><div class="section" title="Text clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec119"/>Text clustering </h2></div></div></div><p>Clustering <a id="id1478" class="indexterm"/>unstructured data for organization, retrieval, and groupings <a id="id1479" class="indexterm"/>based on similarity is the subfield of text clustering. This field is also well-developed with advancements in different clustering and text representations suited for learning.</p><div class="mediaobject"><img src="graphics/B05137_08_003.jpg" alt="Text clustering"/><div class="caption"><p>Figure 3: Clustering of OS news documents to various OS specific clusters</p></div></div></div><div class="section" title="Information extraction and named entity recognition"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec120"/>Information extraction and named entity recognition</h2></div></div></div><p>The <a id="id1480" class="indexterm"/>task of extracting specific <a id="id1481" class="indexterm"/>elements, such <a id="id1482" class="indexterm"/>as time, location, organization, entities, and so on, comes <a id="id1483" class="indexterm"/>under the topic of information extraction. Named entity recognition is a sub-field that has wide applications in different domains, from reviews of historical documents to bioinformatics with gene and drug information.</p><div class="mediaobject"><img src="graphics/B05137_08_004.jpg" alt="Information extraction and named entity recognition"/><div class="caption"><p>Figure 4: Named Entity Recognition in a sentence</p></div></div></div><div class="section" title="Sentiment analysis and opinion mining"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec121"/>Sentiment analysis and opinion mining</h2></div></div></div><p>Another <a id="id1484" class="indexterm"/>sub-field in the area <a id="id1485" class="indexterm"/>of NLP involves inferring <a id="id1486" class="indexterm"/>the sentiments of observers in order to categorize them with <a id="id1487" class="indexterm"/>an understandable metric or to give insights into their opinions. This area is not as advanced as some of the ones mentioned previously, but much research is being done in this direction.</p><div class="mediaobject"><img src="graphics/B05137_08_005.jpg" alt="Sentiment analysis and opinion mining"/><div class="caption"><p>Figure 5: Sentiment Analysis showing positive and negative sentiments for sentences</p></div></div></div><div class="section" title="Coreference resolution"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec122"/>Coreference resolution</h2></div></div></div><p>Understanding <a id="id1488" class="indexterm"/>references to multiple entities existing in the <a id="id1489" class="indexterm"/>text and disambiguating that reference is another popular area of NLP. This is considered as a stepping stone in doing more complex tasks such as question answering and summarization, which will be discussed later.</p><div class="mediaobject"><img src="graphics/B05137_08_006.jpg" alt="Coreference resolution"/><div class="caption"><p>Figure 6: Coreference resolution showing how pronouns get disambiguated</p></div></div></div><div class="section" title="Word sense disambiguation"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec123"/>Word sense disambiguation</h2></div></div></div><p>In a <a id="id1490" class="indexterm"/>language such <a id="id1491" class="indexterm"/>as English, since the same word can have multiple meanings based on the context, deciphering this automatically is an important part of NLP, and the focus of <span class="strong"><strong>word sense disambiguation</strong></span> (<span class="strong"><strong>WSD</strong></span>).</p><div class="mediaobject"><img src="graphics/B05137_08_007.jpg" alt="Word sense disambiguation"/><div class="caption"><p>Figure 7: Showing how word "mouse" is associated with right word using the context</p></div></div></div><div class="section" title="Machine translation"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec124"/>Machine translation</h2></div></div></div><p>Translating <a id="id1492" class="indexterm"/>text from one language to another or from speech <a id="id1493" class="indexterm"/>to text in different languages is broadly covered in the area of <span class="strong"><strong>machine translation</strong></span> (<span class="strong"><strong>MT</strong></span>). This field has made significant progress in the last few years, with the usage of Machine Learning algorithms in supervised, unsupervised, and semi-supervised learning. Deep learning with techniques such as LSTM has been proved to be the most effective technique in this area, and is widely used by Google for its translation.</p><div class="mediaobject"><img src="graphics/B05137_08_008.jpg" alt="Machine translation"/><div class="caption"><p>Figure 8: Machine Translation showing English to Chinese conversion</p></div></div></div><div class="section" title="Semantic reasoning and inferencing"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec125"/>Semantic reasoning and inferencing </h2></div></div></div><p>Reasoning, deriving <a id="id1494" class="indexterm"/>logic, and <a id="id1495" class="indexterm"/>inferencing from unstructured <a id="id1496" class="indexterm"/>text is the <a id="id1497" class="indexterm"/>next level of advancement in NLP.</p><div class="mediaobject"><img src="graphics/B05137_08_009.jpg" alt="Semantic reasoning and inferencing"/><div class="caption"><p>Figure 9: Semantic Inferencing answering complex questions</p></div></div></div><div class="section" title="Text summarization"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec126"/>Text summarization</h2></div></div></div><p>A <a id="id1498" class="indexterm"/>subfield that is growing <a id="id1499" class="indexterm"/>in popularity in NLP is the automated summarization of large documents or passages of text to a small representative text that can be easily understood. This is one of the budding research areas in NLP. Search engines' usage of summaries, multi-document summarizations for experts, and so on, are some of the applications that are benefiting from this field.</p></div><div class="section" title="Automating question and answers"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec127"/>Automating question and answers</h2></div></div></div><p>Answering <a id="id1500" class="indexterm"/>questions posed <a id="id1501" class="indexterm"/>by humans in natural language, ranging from questions specific to a certain domain to generic, open-ended questions is another emerging field in the area of NLP.</p></div></div></div>
<div class="section" title="Issues with mining unstructured data"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec67"/>Issues with mining unstructured data</h1></div></div></div><p>Humans <a id="id1502" class="indexterm"/>can read, parse, and understand unstructured text/documents more easily than computer-based programs. Some of the reasons why text mining is more complicated than general supervised or unsupervised learning are given here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Ambiguity <a id="id1503" class="indexterm"/>in terms and phrases. The word <span class="emphasis"><em>bank</em></span> has multiple meanings, which a human reader can correctly associate based on context, yet this requires preprocessing steps such as POS tagging and word sense disambiguation, as we have seen. According to the Oxford English Dictionary, the word <span class="emphasis"><em>run</em></span> has no fewer than 645 different uses in the verb form alone and we can see that such words can indeed present problems in resolving the meaning intended (between them, the words run, put, set, and take have more than a thousand meanings).</li><li class="listitem" style="list-style-type: disc">Context and background knowledge associated with the text. Consider a sentence that uses a neologism with the suffix <span class="emphasis"><em>gate</em></span> to signify a political scandal, as in, <span class="emphasis"><em>With cries for impeachment and popularity ratings in a nosedive, Russiagate finally dealt a deathblow to his presidency</em></span>. A human reader can surmise what is being referred to by the coinage <span class="emphasis"><em>Russiagate</em></span> as something that recalls the sense of high-profile intrigue, by association via an affix, of another momentous scandal in US political history, <span class="emphasis"><em>Watergate</em></span>. This is particularly difficult for a machine to make sense of.</li><li class="listitem" style="list-style-type: disc">Reasoning, that is, inferencing from documents is very difficult as mapping unstructured information to knowledge bases is itself a big hurdle.</li><li class="listitem" style="list-style-type: disc">Ability to perform supervised learning needs labeled training documents and based on the domain, performing labeling on the documents can be time consuming and costly.</li></ul></div></div>
<div class="section" title="Text processing components and transformations"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec68"/>Text processing components and transformations</h1></div></div></div><p>In <a id="id1504" class="indexterm"/>this section, we will discuss some common preprocessing <a id="id1505" class="indexterm"/>and transformation steps that are done in most text mining processes. The general concept is to convert the documents into structured datasets with features or attributes that most Machine Learning algorithms can use to perform different kinds of learning.</p><p>We will briefly describe some of the most used techniques in the next section. Different applications of text mining might use different pieces or variations of the components shown in the following figure:</p><div class="mediaobject"><img src="graphics/B05137_08_010.jpg" alt="Text processing components and transformations"/><div class="caption"><p>Figure 10: Text Processing components and the flow</p></div></div><div class="section" title="Document collection and standardization"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec128"/>Document collection and standardization</h2></div></div></div><p>One <a id="id1506" class="indexterm"/>of the first steps in most text <a id="id1507" class="indexterm"/>mining applications is the collection <a id="id1508" class="indexterm"/>of data in the form of a body of documents—often referred to as a <span class="emphasis"><em>corpus</em></span> in the text mining world. These documents can have <a id="id1509" class="indexterm"/>predefined categorization associated with them or it can simply be an unlabeled corpus. The documents can be of heterogeneous formats or standardized into one format for the next process of tokenization. Having multiple formats such as text, HTML, DOCs, PDGs, and so on, can lead to many complexities <a id="id1510" class="indexterm"/>and hence one format, such as XML or <span class="strong"><strong>JavaScript Object Notation</strong></span> (<span class="strong"><strong>JSON</strong></span>) is normally preferred in most applications.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec141"/>Inputs and outputs</h3></div></div></div><p>Inputs <a id="id1511" class="indexterm"/>are vast collections of homogeneous or heterogeneous <a id="id1512" class="indexterm"/>sources <a id="id1513" class="indexterm"/>and outputs are <a id="id1514" class="indexterm"/>a collection of documents standardized into one format, such as XML.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec142"/>How does it work?</h3></div></div></div><p>Standardization <a id="id1515" class="indexterm"/>involves ensuring tools and formats are <a id="id1516" class="indexterm"/>agreed upon based on the needs of the application:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Agree to a standard format, such as XML, with predefined tags that provide information on meta-attributes of documents <code class="literal">(&lt;author&gt;</code>, <code class="literal">&lt;title&gt;</code>, <code class="literal">&lt;date&gt;</code>, and so on) and actual content, such as <code class="literal">&lt;document&gt;</code>.</li><li class="listitem">Most document processors can either be transformed into XML or transformation <a id="id1517" class="indexterm"/>code can be written to perform <a id="id1518" class="indexterm"/>this.</li></ol></div></div></div><div class="section" title="Tokenization"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec129"/>Tokenization</h2></div></div></div><p>The <a id="id1519" class="indexterm"/>task of tokenization is to extract words <a id="id1520" class="indexterm"/>or meaningful characters from the text containing a stream of these words. For example, the text <span class="emphasis"><em>The boy stood up. He then ran after the dog</em></span> can be tokenized into tokens such as <span class="emphasis"><em>{the, boy, stood, up, he, ran, after, the, dog}</em></span>.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec143"/>Inputs and outputs</h3></div></div></div><p>An input <a id="id1521" class="indexterm"/>is a collection of documents in a well-known format as described <a id="id1522" class="indexterm"/>in the last section and an output is a document with tokens of words or characters as needed in the application.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec144"/>How does it work?</h3></div></div></div><p>Any <a id="id1523" class="indexterm"/>automated system for tokenization must address the particular challenges presented by the language(s) it is expected to handle:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In languages such as English, tokenization is relatively simple due to the presence of white space, tabs, and newline for separating the words.</li><li class="listitem" style="list-style-type: disc">There are different challenges in each language—even in English, abbreviations such as <span class="emphasis"><em>Dr.</em></span>, alphanumeric characters (<span class="emphasis"><em>B12</em></span>), different naming schemes (<span class="emphasis"><em>O'Reilly</em></span>), and so on, must be tokenized appropriately.</li><li class="listitem" style="list-style-type: disc">Language-specific rules in the form of if-then instructions are written to extract tokens from the documents.</li></ul></div></div></div><div class="section" title="Stop words removal"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec130"/>Stop words removal</h2></div></div></div><p>This <a id="id1524" class="indexterm"/>involves removing high frequency <a id="id1525" class="indexterm"/>words that have no discriminatory or predictive value. If every word can be viewed as a feature, this process reduces the dimension of the feature vector by a significant number. Prepositions, articles, and pronouns are some of the examples that form the stop words that are removed without affecting <a id="id1526" class="indexterm"/>the performance <a id="id1527" class="indexterm"/>of text mining in many applications.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec145"/>Inputs and outputs</h3></div></div></div><p>An input <a id="id1528" class="indexterm"/>is a collection of documents with the tokens extracted and <a id="id1529" class="indexterm"/>an output is a collection of documents with tokens reduced by removing stop words.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec146"/>How does it work?</h3></div></div></div><p>There <a id="id1530" class="indexterm"/>are various techniques that have evolved in the last few years ranging from manually precompiled lists to statistical elimination using either term-based or mutual information.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The most commonly used technique for many languages is a manually precompiled list of stop words, including prepositions (in, for, on), articles (a, an, the), pronouns (his, her, they, their), and so on.</li><li class="listitem" style="list-style-type: disc">Many tools use Zipf's law (<span class="emphasis"><em>References</em></span> [3]), where high frequency words, singletons, and unique terms are removed. Luhn's early work (<span class="emphasis"><em>References</em></span> [4]), as represented in the following figure 11, shows thresholds of the upper bound and lower bound of word frequency, which give us the significant words that can be used for modeling:<div class="mediaobject"><img src="graphics/B05137_08_012.jpg" alt="How does it work?"/><div class="caption"><p>Figure 11: Word Frequency distribution, showing how frequently used, significant and rare words exist in corpus</p></div></div></li></ul></div></div></div><div class="section" title="Stemming or lemmatization"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec131"/>Stemming or lemmatization</h2></div></div></div><p>The <a id="id1531" class="indexterm"/>idea of normalizing tokens of similar words into one is known <a id="id1532" class="indexterm"/>as stemming or lemmatization. Thus, reducing all occurrences of "talking", "talks", "talked", and so on in a document to one root word "talk" in <a id="id1533" class="indexterm"/>the document is an example of stemming.</p><div class="section" title="Inputs and outputs"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec147"/>Inputs and outputs</h3></div></div></div><p>An input <a id="id1534" class="indexterm"/>is documents with tokens and an output is documents with <a id="id1535" class="indexterm"/>reduced tokens normalized to their stem or root words.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec148"/>How does it work?</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">There <a id="id1536" class="indexterm"/>are basically two types of stemming: inflectional stemming and stemming to the root.</li><li class="listitem">Inflectional stemming generally involves removing affixes, normalizing the verb tenses and removing plurals. Thus, "ships" to "ship", "is", "are" and "am" to "be" in English.</li><li class="listitem">Stemming to the root is generally a more aggressive form than inflectional stemming, where words are normalized to their roots. An example of this is "applications", "applied", "reapply", and so on, all reduced to the root word "apply".</li><li class="listitem">Lovin's stemmer was one of the first stemming algorithms (<span class="emphasis"><em>References</em></span> [1]). Porter's stemming, which evolved in the 1980s with around 60 rules in 6 steps, is still the most widely used form of stemming (<span class="emphasis"><em>References</em></span> [2]).</li><li class="listitem">Present-day applications drive a wide range of statistical techniques based on stemming, including those using n-grams (a contiguous sequence of n items, either letters or <a id="id1537" class="indexterm"/>words from a given sequence of text), <span class="strong"><strong>hidden Markov models</strong></span> (<span class="strong"><strong>HMM</strong></span>), and context-sensitive stemming. </li></ol></div></div></div><div class="section" title="Local/global dictionary or vocabulary?"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec132"/>Local/global dictionary or vocabulary?</h2></div></div></div><p>Once <a id="id1538" class="indexterm"/>the preprocessing task of <a id="id1539" class="indexterm"/>converting documents into tokens is performed, the next step is the creation of a corpus or vocabulary as a single dictionary, using all the tokens from all documents. Alternatively, several dictionaries are created based on category, using specific tokens from fewer documents.</p><p>Many applications in topic modeling and text categorization perform well when dictionaries are created per topic/category, which is known as a local dictionary. On the other hand, many applications in document clustering and information extraction perform well when one single global dictionary is created from all the document tokens. The choice of creating one or many specific dictionaries depends on the core NLP task, as well as on computational and storage requirements. </p></div><div class="section" title="Feature extraction/generation"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec133"/>Feature extraction/generation</h2></div></div></div><p>A key <a id="id1540" class="indexterm"/>step in converting <a id="id1541" class="indexterm"/>the document(s) with unstructured text is to transform them into datasets with structured features, similar to what we have seen so far in Machine Learning datasets. Extracting features from text so that it can be used in Machine Learning tasks such as supervised, unsupervised, and semi-supervised learning depends on many factors, such as the goals of the applications, domain-specific requirements, and feasibility. There are a wide variety of features, such as words, phrases, sentences, POS-tagged words, typographical elements, and so on, that can be extracted from any document. We will give a broad range of features that are commonly used in different Machine Learning applications. </p><div class="section" title="Lexical features"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec149"/>Lexical features</h3></div></div></div><p>Lexical <a id="id1542" class="indexterm"/>features are the most frequently used <a id="id1543" class="indexterm"/>features in text mining applications. Lexical features form the basis for the next level of features. They are the simple character- or word- level features constructed without trying to capture information about intent or the various meanings associated with the text. Lexical features can be further broken down into character-based features, word-based features, part-of-speech features, and taxonomies, for example. In the next section, we will describe some of them in greater detail.</p><div class="section" title="Character-based features"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec216"/>Character-based features</h4></div></div></div><p>Individual <a id="id1544" class="indexterm"/>characters (unigram) or a sequence of characters (n-gram) are the simplest forms of features that can be constructed from the text document. The bag of characters or unigram characters have no positional information, while higher order n-grams capture some amount of context and positional information. These features can be encoded or given numeric values in different ways, such as binary 0/1 values, or counts, for example, as discussed later in the next section.</p><p>Let us consider the memorable Dr. Seuss rhyme as the text content—"the Cat in the Hat steps onto the mat". While the bag-of-characters (1-gram or unigram features) will generate unique characters {"t","h", "e", "c","a","i","n","s","p","o","n","m"} as features, the 3-gram features are { "\sCa" ,"\sHa", "\sin" , "\sma", "\son", "\sst", "\sth", "Cat", "Hat", "at\s", "e\sC", "e\sH", "e\sm", "eps", "he\s", "in\s ", "mat", "n\st", "nto", "o\st", "ont", "ps\s", "s\so" , "ste"," t\si"," t\ss" , "tep", "the", "to\s "}. As can be seen, as "n" increases, the number of features increases exponentially and soon becomes unwieldy. The advantage of n-grams is that at the cost of increasing the total number of features, the assembled features often seem to capture combinations of characters that are more interesting <a id="id1545" class="indexterm"/>than the individual characters themselves.</p></div><div class="section" title="Word-based features"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec217"/>Word-based features</h4></div></div></div><p>Instead <a id="id1546" class="indexterm"/>of generating features from characters, features can similarly be constructed from words in a unigram and n-gram manner. These are the most popular feature generation techniques. The unigram or 1-word token is also known as the bag of words model. So the example of "the Cat in the Hat steps onto the mat" when considered as unigram features is {"the", "Cat", "in", "Hat", "steps", "onto", "mat"}. Similarly, bigram features on the same text would result in {"the Cat", "Cat in", "in the", "the Hat", "Hat step", "steps onto", "onto the", "the mat"}. As in the case of character-based features, by going to higher "n" in the n-grams, the number of features increases, but so does the ability to capture word sense via context. </p></div><div class="section" title="Part-of-speech tagging features"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec218"/>Part-of-speech tagging features</h4></div></div></div><p>The <a id="id1547" class="indexterm"/>input is text with words and the output is text where every word is associated with the grammatical tag. In many applications, part-of-speech gives a context and is useful in identification of named entities, phrases, entity disambiguation, and so on. In the example "the Cat in the Hat steps onto the mat" , the output is {"the\Det", "Cat\Noun", "in\Prep", "the\Det", "Hat\Noun", "steps\Verb", "onto\Prep", "the\Det", "mat\Noun"}. Language specific rule-based taggers or Markov chain–based probabilistic taggers are often used in this process. </p></div><div class="section" title="Taxonomy features"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec219"/>Taxonomy features</h4></div></div></div><p>Creating <a id="id1548" class="indexterm"/>taxonomies from the text data and using it to understand relationships between words is also useful in different contexts. Various taxonomical features such as hypernyms, hyponyms, is-member, member-of, is-part, part-of, antonyms, synonyms, acronyms, and so on, give lexical context that proves useful in searches, retrieval, and matching in many text mining scenarios. </p></div></div><div class="section" title="Syntactic features"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec150"/>Syntactic features</h3></div></div></div><p>The <a id="id1549" class="indexterm"/>next level of features that are higher <a id="id1550" class="indexterm"/>than just characters or words in text documents are the syntax-based features. Syntactical representation of sentences in text is generally in the form of syntax trees. Syntax trees capture nodes as terms used in sentences, and relationships between the nodes are captured as links. Syntactic features can also capture more complex features about sentences and usage—such as aggregates—that can be used for Machine Learning. It can also capture statistics about syntax trees—such as sentences being left-heavy, right-heavy, or balanced—that can be used to understand signatures of different content or writers. </p><p>Two sentences can have the same characters and words in the lexical analysis, but their syntax <a id="id1551" class="indexterm"/>trees or intent could be completely different. Breaking the sentences in the text into different phrases—<span class="strong"><strong>Noun Phrase</strong></span> (<span class="strong"><strong>NP</strong></span>), <span class="strong"><strong>Prepositional Phrase</strong></span> (<span class="strong"><strong>PP</strong></span>), <span class="strong"><strong>Verbal</strong></span> (or Gerund) <span class="strong"><strong>Phrase</strong></span> (<span class="strong"><strong>VP</strong></span>), and so on—and capturing <a id="id1552" class="indexterm"/>phrase structure trees for the sentences are part <a id="id1553" class="indexterm"/>of this <a id="id1554" class="indexterm"/>processing <a id="id1555" class="indexterm"/>task. The following is the syntactic parse tree for our example sentence:</p><div class="informalexample"><pre class="programlisting">(S (NP (NP the cat)
       (PP in
           (NP the hat)))
   (VP steps
       (PP onto
           (NP the mat))))</pre></div><p>
<span class="strong"><strong>Syntactic Language Models</strong></span> (<span class="strong"><strong>SLM</strong></span>) are about determining the probability of a sequence of <a id="id1556" class="indexterm"/>terms. Language model features are used in machine translation, spelling correction, speech translation, summarization, and so on, to name a few of the applications. Language models can additionally use parse trees and syntax trees in their computation as well. </p><p>The chain rule is applied to compute the joint probability of terms in the sentence:</p><div class="mediaobject"><img src="graphics/B05137_08_013.jpg" alt="Syntactic features"/></div><p>In the example "the cat in the hat steps onto the mat":</p><div class="mediaobject"><img src="graphics/B05137_08_014.jpg" alt="Syntactic features"/></div><p>Generally, the estimation of the probability of long sentences based on counts using any corpus is difficult due to the need for many examples of such sentences. Most language models use the Markov assumption of independence and n-grams (2-5 words) in practical implementations (<span class="emphasis"><em>References</em></span> [8]). </p></div><div class="section" title="Semantic features"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec151"/>Semantic features</h3></div></div></div><p>Semantic <a id="id1557" class="indexterm"/>features attempt to capture <a id="id1558" class="indexterm"/>the "meaning" of the text, which is then used for different applications of text mining. One of the simplest forms of semantic features is the process of adding annotations to the documents. These annotations, or metadata, can have additional information that describes or captures the intent of the text or documents. Adding tags using collaborative tagging to capture tags as keywords describing the text is a common semantic feature generation process. </p><p>Another form of semantic feature generation is the process of ontological representation of the text. Generic and domain specific ontologies that capture different relationships between objects are available in knowledge bases and have well-known specifications, such as Semantic Web 2.0. These ontological features help in deriving complex inferencing, summarization, classification, and clustering tasks in text mining. The terms in the text or documents can be mapped to "concepts" in ontologies and stored in knowledge bases. These concepts in ontologies have semantic properties, and are related to other concepts in a number of ways, such as generalization/specialization, member-of/isAMember, association, and so on, to name a few. These attributes or properties of concepts and relationships can be further used in search, retrieval, and even in predictive modeling. Many semantic features use the lexical and syntactic processes as pre-cursors to the semantic process and use the outputs, such as nouns, to map to concepts in ontologies, for example. Adding the concepts to an existing ontology or annotating it with more concepts makes the structure more suitable for learning. For example, in the "the cat in the .." sentence, "cat" has properties such as {age, eats, ...} and has different relationships, such as { "isA Mammal", "hasChild", "hasParent", and so on}. </p></div></div><div class="section" title="Feature representation and similarity"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec134"/>Feature representation and similarity</h2></div></div></div><p>Lexical, syntactic, and semantic features, described in the last section, often have representations <a id="id1559" class="indexterm"/>that are completely <a id="id1560" class="indexterm"/>different from each other. Representations of the same feature type, that is, lexical, syntactic, or semantic, can differ based on the computation or mining task for which they are employed. In this section, we will describe the most common lexical feature-based representation known as vector space models.</p><div class="section" title="Vector space model"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec152"/>Vector space model</h3></div></div></div><p>The <span class="strong"><strong>vector space model</strong></span> (<span class="strong"><strong>VSM</strong></span>) is a transformation of the unstructured document to a numeric <a id="id1561" class="indexterm"/>vector representation where terms in the corpus form the dimensions of the vector and we use some numeric way of associating value with these dimensions. </p><p>As discussed in the section on dictionaries, a corpus is formed out of unique words and phrases from the entire collection of documents in a domain or within local sub-categories of one. Each of the elements of such a dictionary are the dimensions of the vector. The terms—which can be single words or phrases, as in n-grams—form the dimensions and can have different values associated with them in a given text/document. The goal is to capture the values in the dimensions in a way that reflects the relevancy of the term(s) in the entire corpus (<span class="emphasis"><em>References</em></span> [11]). Thus, each document or file is represented as a high-dimensional numeric vector. Due to the sparsity of terms, the numeric vector representation has a sparse representation in numeric space. Next, we will give some well-known ways of associating values to these terms.</p><div class="section" title="Binary"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec220"/>Binary</h4></div></div></div><p>This <a id="id1562" class="indexterm"/>is the simplest form of associating value to the terms, or dimensions. In binary form, each term in the corpus is given a 0 or 1 value based on the presence or absence of the term in the document. For example, consider the following three documents:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Document 1: "The Cat in the Hat steps onto the mat"</li><li class="listitem" style="list-style-type: disc">Document 2: "The Cat sat on the Hat"</li><li class="listitem" style="list-style-type: disc">Document 3: "The Cat loves to step on the mat"</li></ul></div><p>After preprocessing by removing stop words {on, the, in, onto} and stemming {love/loves, steps/step} using a unigram or bag of words, {cat, hat, step, mat, sat, love} are the features of the corpus. Each document is now represented in a binary vector space model as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Terms</p>
</th><th style="text-align: left" valign="bottom">
<p>cat</p>
</th><th style="text-align: left" valign="bottom">
<p>hat</p>
</th><th style="text-align: left" valign="bottom">
<p>step</p>
</th><th style="text-align: left" valign="bottom">
<p>mat</p>
</th><th style="text-align: left" valign="bottom">
<p>sat</p>
</th><th style="text-align: left" valign="bottom">
<p>love</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Document 1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Document 2</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Document 3</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr></tbody></table></div></div><div class="section" title="Term frequency (TF)"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec221"/>Term frequency (TF)</h4></div></div></div><p>In <span class="strong"><strong>term frequency</strong></span> (<span class="strong"><strong>TF</strong></span>), as the name suggests, the frequency of terms in the entire document <a id="id1563" class="indexterm"/>forms the numeric value of <a id="id1564" class="indexterm"/>the feature. The basic assumption is that the higher the frequency of the term, the greater the relevance of that term for the document. Counts of terms or normalized counts of terms are used as values in each column of terms: </p><p>
<span class="emphasis"><em>tf(t) = count(D, t)</em></span>
</p><p>The following table gives term frequencies for the three documents in our example: </p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>TF / Terms</p>
</th><th style="text-align: left" valign="bottom">
<p>cat</p>
</th><th style="text-align: left" valign="bottom">
<p>hat</p>
</th><th style="text-align: left" valign="bottom">
<p>step</p>
</th><th style="text-align: left" valign="bottom">
<p>mat</p>
</th><th style="text-align: left" valign="bottom">
<p>sat</p>
</th><th style="text-align: left" valign="bottom">
<p>love</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Document 1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Document 2</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Document 3</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr></tbody></table></div></div><div class="section" title="Inverse document frequency (IDF)"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec222"/>Inverse document frequency (IDF)</h4></div></div></div><p>
<span class="strong"><strong>Inverse document frequency</strong></span> (<span class="strong"><strong>IDF</strong></span>) has various <a id="id1565" class="indexterm"/>flavors, but the most common way of computing <a id="id1566" class="indexterm"/>it is using the following:</p><div class="mediaobject"><img src="graphics/B05137_08_016.jpg" alt="Inverse document frequency (IDF)"/></div><p>Here, <span class="inlinemediaobject"><img src="graphics/B05137_08_017.jpg" alt="Inverse document frequency (IDF)"/></span> <span class="inlinemediaobject"><img src="graphics/B05137_08_018.jpg" alt="Inverse document frequency (IDF)"/></span> IDF favors mostly those terms that occur relatively infrequently in the documents. Some empirically motivated improvements to IDF have also been proposed in the research (<span class="emphasis"><em>References</em></span> [7]).</p><p>TF for our example corpus:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Terms</p>
</th><th style="text-align: left" valign="bottom">
<p>cat</p>
</th><th style="text-align: left" valign="bottom">
<p>hat</p>
</th><th style="text-align: left" valign="bottom">
<p>step</p>
</th><th style="text-align: left" valign="bottom">
<p>mat</p>
</th><th style="text-align: left" valign="bottom">
<p>sat</p>
</th><th style="text-align: left" valign="bottom">
<p>love</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>N/nj</p>
</td><td style="text-align: left" valign="top">
<p>3/3</p>
</td><td style="text-align: left" valign="top">
<p>3/2</p>
</td><td style="text-align: left" valign="top">
<p>3/2</p>
</td><td style="text-align: left" valign="top">
<p>3/2</p>
</td><td style="text-align: left" valign="top">
<p>3/1</p>
</td><td style="text-align: left" valign="top">
<p>3/1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>IDF</p>
</td><td style="text-align: left" valign="top">
<p>0.0</p>
</td><td style="text-align: left" valign="top">
<p>0.40</p>
</td><td style="text-align: left" valign="top">
<p>0.40</p>
</td><td style="text-align: left" valign="top">
<p>0.40</p>
</td><td style="text-align: left" valign="top">
<p>1.10</p>
</td><td style="text-align: left" valign="top">
<p>1.10</p>
</td></tr></tbody></table></div></div><div class="section" title="Term frequency-inverse document frequency (TF-IDF)"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec223"/>Term frequency-inverse document frequency (TF-IDF)</h4></div></div></div><p>Combining <a id="id1567" class="indexterm"/>both term frequencies <a id="id1568" class="indexterm"/>and inverse document frequencies in one metric, we get the term frequency-inverse document frequency values. The idea is to value those terms that are relatively uncommon in the corpus (high IDF), but are reasonably relevant for the document (high TF). TF-IDF is the most common form of value association in many text mining processes:</p><div class="mediaobject"><img src="graphics/B05137_08_019.jpg" alt="Term frequency-inverse document frequency (TF-IDF)"/></div><p>This gives us the TF-IDF for all the terms in each of the documents:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>TF-IDF/Terms</p>
</th><th style="text-align: left" valign="bottom">
<p>cat</p>
</th><th style="text-align: left" valign="bottom">
<p>hat</p>
</th><th style="text-align: left" valign="bottom">
<p>step</p>
</th><th style="text-align: left" valign="bottom">
<p>mat</p>
</th><th style="text-align: left" valign="bottom">
<p>sat</p>
</th><th style="text-align: left" valign="bottom">
<p>love</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Document 1</p>
</td><td style="text-align: left" valign="top">
<p>0.0</p>
</td><td style="text-align: left" valign="top">
<p>0.40</p>
</td><td style="text-align: left" valign="top">
<p>0.40</p>
</td><td style="text-align: left" valign="top">
<p>0.40</p>
</td><td style="text-align: left" valign="top">
<p>1.10</p>
</td><td style="text-align: left" valign="top">
<p>1.10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Document 2</p>
</td><td style="text-align: left" valign="top">
<p>0.0</p>
</td><td style="text-align: left" valign="top">
<p>0.40</p>
</td><td style="text-align: left" valign="top">
<p>0.0</p>
</td><td style="text-align: left" valign="top">
<p>0.0</p>
</td><td style="text-align: left" valign="top">
<p>1.10</p>
</td><td style="text-align: left" valign="top">
<p>0.0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Document 3</p>
</td><td style="text-align: left" valign="top">
<p>0.0</p>
</td><td style="text-align: left" valign="top">
<p>0.0</p>
</td><td style="text-align: left" valign="top">
<p>0.40</p>
</td><td style="text-align: left" valign="top">
<p>0.40</p>
</td><td style="text-align: left" valign="top">
<p>0.0</p>
</td><td style="text-align: left" valign="top">
<p>1.10</p>
</td></tr></tbody></table></div></div></div><div class="section" title="Similarity measures"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec153"/>Similarity measures</h3></div></div></div><p>Many <a id="id1569" class="indexterm"/>techniques in supervised, unsupervised, and semi-supervised learning use "similarity" measures in their underlying algorithms to find similar patterns or to separate different patterns. Similarity measures are tied closely to the representation of the data. In the VSM representation of documents, the vectors are very high dimensional and sparse. This poses a serious issue in most traditional similarity measures for classification, clustering, or information retrieval. Angle-based similarity measures, such as cosine distances or Jaccard coefficients, are more often used in practice. Consider two vectors represented by <span class="strong"><strong>t</strong></span><sub>1</sub> and <span class="strong"><strong>t</strong></span><sub>2</sub> corresponding to two text documents.</p><div class="section" title="Euclidean distance"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec224"/>Euclidean distance</h4></div></div></div><p>This <a id="id1570" class="indexterm"/>is the L2 norm in the feature space of the <a id="id1571" class="indexterm"/>documents:</p><div class="mediaobject"><img src="graphics/B05137_08_023.jpg" alt="Euclidean distance"/></div></div><div class="section" title="Cosine distance"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec225"/>Cosine distance</h4></div></div></div><p>This <a id="id1572" class="indexterm"/>angle-based similarity measure considers <a id="id1573" class="indexterm"/>orientation between vectors only and not their lengths. It is equal to the cosine of the angle between the vectors.  Since the vector space model is a positive space, cosine distance varies from 0 (orthogonal, no common terms) to 1 (all terms are common to both, but not necessarily with the same term frequency):</p><div class="mediaobject"><img src="graphics/B05137_08_024.jpg" alt="Cosine distance"/></div></div><div class="section" title="Pairwise-adaptive similarity"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec226"/>Pairwise-adaptive similarity</h4></div></div></div><p>This <a id="id1574" class="indexterm"/>measure the distance in a reduced <a id="id1575" class="indexterm"/>feature space by only considering the features that are most important in the two documents:</p><div class="mediaobject"><img src="graphics/B05137_08_025.jpg" alt="Pairwise-adaptive similarity"/></div><p>Here, <span class="strong"><strong>t</strong></span><sub>i,k</sub> is a vector formed from a subset of the features of <span class="strong"><strong>t</strong></span><sub>i</sub> (<span class="emphasis"><em>i</em></span> = 1, 2) containing the union of the <span class="emphasis"><em>K</em></span> largest features appearing in <span class="strong"><strong>t</strong></span><sub>1</sub> and <span class="strong"><strong>t</strong></span><sub>2</sub>.</p></div><div class="section" title="Extended Jaccard coefficient"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec227"/>Extended Jaccard coefficient</h4></div></div></div><p>This <a id="id1576" class="indexterm"/>measure is computed as a ratio of the shared terms <a id="id1577" class="indexterm"/>to the union of the terms between the documents:</p><div class="mediaobject"><img src="graphics/B05137_08_031.jpg" alt="Extended Jaccard coefficient"/></div></div><div class="section" title="Dice coefficient"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec228"/>Dice coefficient</h4></div></div></div><p>The <a id="id1578" class="indexterm"/>Dice coefficient <a id="id1579" class="indexterm"/>is given by the following:</p><div class="mediaobject"><img src="graphics/B05137_08_032.jpg" alt="Dice coefficient"/></div></div></div></div><div class="section" title="Feature selection and dimensionality reduction"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec135"/>Feature selection and dimensionality reduction</h2></div></div></div><p>The <a id="id1580" class="indexterm"/>goal is the same as in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span> and <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>. The problem of the curse of dimensionality becomes even more <a id="id1581" class="indexterm"/>pronounced with text mining and high dimensional features. </p><div class="section" title="Feature selection"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec154"/>Feature selection</h3></div></div></div><p>Most <a id="id1582" class="indexterm"/>feature selection techniques are supervised techniques <a id="id1583" class="indexterm"/>that depend on the labels or the outcomes for scoring the features. In the majority of cases, we perform filter-based rather than wrapper-based feature selection, due to the lower performance cost. Even among filter-based methods, some, such as those involving multivariate techniques such as <span class="strong"><strong>Correlation based Feature selection</strong></span> (<span class="strong"><strong>CFS</strong></span>), as described in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>, can be quite costly or result in suboptimal performance due to high dimensionality (<span class="emphasis"><em>References</em></span> [9]). </p><div class="section" title="Information theoretic techniques"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec229"/>Information theoretic techniques</h4></div></div></div><p>As <a id="id1584" class="indexterm"/>shown in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>, filter-based univariate feature selection <a id="id1585" class="indexterm"/>methods, such as <span class="strong"><strong>Information gain</strong></span> (<span class="strong"><strong>IG</strong></span>) and <span class="strong"><strong>Gain Ratio</strong></span> (<span class="strong"><strong>GR</strong></span>), are most <a id="id1586" class="indexterm"/>commonly used once preprocessing and feature extraction is done. </p><p>In their research, Yang and Pederson (<span class="emphasis"><em>References</em></span> [10]) clearly showed the benefits of feature selection and reduction using IG to remove close to 98% of terms and yet improve the predictive capability of the classifiers. </p><p>Many of the <a id="id1587" class="indexterm"/>information theoretic or entropy-based methods have a stronger influence resulting from the marginal probabilities of the tokens. This can be an issue when the terms have equal conditional probability P(t|class), the rarer terms may have better scores than the common terms. </p></div><div class="section" title="Statistical-based techniques"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec230"/>Statistical-based techniques</h4></div></div></div><p>?<sup>2</sup> feature <a id="id1588" class="indexterm"/>selection is one of the most common statistical-based techniques employed to perform feature selection in text mining. ?<sup>2</sup> statistics, as shown in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>, give the independence relationship between the tokens in the text and the classes. </p><p>It has been shown that ?<sup>2</sup> statistics for feature selection may not be effective when there are low-frequency terms (<span class="emphasis"><em>References</em></span> [19]).</p></div><div class="section" title="Frequency-based techniques"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec231"/>Frequency-based techniques</h4></div></div></div><p>Using <a id="id1589" class="indexterm"/>the term frequency or the document frequency described in the section on feature representation, a threshold can be manually set, and only terms above or below a certain threshold can be allowed used for modeling in either classification or <a id="id1590" class="indexterm"/>clustering tasks. Note that <span class="strong"><strong>term frequency</strong></span> (<span class="strong"><strong>TF</strong></span>) and <span class="strong"><strong>document frequency</strong></span> (<span class="strong"><strong>DF</strong></span>) methods are biased towards common words while some of <a id="id1591" class="indexterm"/>the information theoretic or statistical-based methods are biased towards less frequent words. The choice of selection of features depends on the domain, the particular application of predictive learning, and more importantly, on how models using these features are evaluated, especially on the unseen dataset.</p></div></div><div class="section" title="Dimensionality reduction"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec155"/>Dimensionality reduction</h3></div></div></div><p>Another approach that we saw in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, was <a id="id1592" class="indexterm"/>to use unsupervised techniques to reduce the features using some form of transformation to decide their usefulness. </p><p>
<span class="strong"><strong>Principal component analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>) computes a covariance or correlation matrix from the <a id="id1593" class="indexterm"/>document-term matrix. It transforms the data into linear combinations of terms in the inputs in such a way that the transformed combination of features or terms has higher discriminating power than the input terms. PCA with cut-off or thresholding on the transformed features, as shown in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, can bring down the dimensionality substantially and even improve or give comparable performance to the high dimensional input space. The only issue with using PCA is that the transformed features are not interpretable, and for domains where understanding which terms or combinations yield better predictive models, this technique has some limitations. </p><p>
<span class="strong"><strong>Latent semantic analysis</strong></span> (<span class="strong"><strong>LSA</strong></span>) is another way of using the input matrix constructed from <a id="id1594" class="indexterm"/>terms and documents and transforming it into lower <a id="id1595" class="indexterm"/>dimensions with latent concepts discovered through combinations of terms used in documents (<span class="emphasis"><em>References</em></span> [5]). The following figure captures the process using the <span class="strong"><strong>singular value decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>) method for factorizing the input document-term matrix: </p><div class="mediaobject"><img src="graphics/B05137_08_035.jpg" alt="Dimensionality reduction"/><div class="caption"><p>Figure 12: SVD factorization of input document-terms  into LSA document vectors and LSA term vectors</p></div></div><p>LSA has been shown to be a very effective way of reducing the dimensions and also of improving the predictive performance in models. The disadvantage of LSA is that storage of both vectors U and V is needed for performing retrievals or queries. Determining the lower dimension k is hard and needs some heuristics similar to k-means discussed in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>. </p></div></div></div>
<div class="section" title="Topics in text mining"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec69"/>Topics in text mining</h1></div></div></div><p>As <a id="id1596" class="indexterm"/>we saw in the first section, the area of text mining and performing Machine Learning on text spans a wide range of topics. Each topic discussed has some customizations to the mainstream algorithms, or there are specific algorithms that have been developed to perform the task called for in that area. We have chosen four broad topics, namely, text categorization, topic modeling, text clustering, and named entity recognition, and will discuss each in some detail. </p><div class="section" title="Text categorization/classification"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec136"/>Text categorization/classification</h2></div></div></div><p>The <a id="id1597" class="indexterm"/>text classification problem manifests itself in different applications, such as document filtering and organization, information retrieval, opinion and sentiment mining, e-mail spam filtering, and so on. Similar to the classification problem discussed in <a class="link" href="ch02.html" title="Chapter 2. Practical Approach to Real-World Supervised Learning">Chapter 2</a>, <span class="emphasis"><em>Practical Approach to Real-World Supervised Learning</em></span>, the general idea is to train on the training data with labels and to predict the labels of unseen documents.</p><p>As discussed in the previous section, the preprocessing steps help to transform the unstructured document collection into well-known numeric or categorical/binary structured data arranged in terms of a document-term matrix. The choice of performing some preprocessing <a id="id1598" class="indexterm"/>steps, such as stemming or customizing stop words, depends on the data and applications. The feature choice is generally basic lexical features, n-grams of words as terms, and only in certain cases do we use the entire text as a string without breaking it into terms or tokens. It is common to use binary feature representation or frequency-based representation for document-term structured data. Once this transformation is complete, we do feature selection using univariate analysis, such as information gain or chi-square, to choose discriminating features above certain thresholds of scores. One may also perform feature transformation and dimensionality reduction such as PCA or LSA in many applications.</p><p>There is a wide range in the choice of classifiers once we get structured data from the preceding process. In the research as well as in commercial applications, we see the use of most of the common modeling techniques, including linear (linear regression, logistic regression, and so on), non-linear (SVM, neural networks, KNN), generative (naïve bayes, bayesian networks), interpretable (decision trees, rules), and ensemble-based (bagging, boosting, random forest) classifiers. Many algorithms use similarity or distance metrics, of which cosine distance is the most popular choice. In certain classifiers, such as SVM, the string representation of the document can be used as is, with the right choice of string kernels and similarity-based metrics on strings to compute the dot products. </p><p>Validation and evaluation methods are similar to supervised classification methodologies—splitting the data into train/validation/test, training on training data, tuning parameters of algorithm(s) on validation data, and estimating the performance of the models on hold-out or test data. </p><p>Since most of text classification involves a large number of documents, and the target classes are rare, the metrics used for evaluation, tuning, or choosing algorithms are in most cases precision, recall, and F-score measure, as follows: </p><div class="mediaobject"><img src="graphics/B05137_08_039.jpg" alt="Text categorization/classification"/></div><div class="mediaobject"><img src="graphics/B05137_08_040.jpg" alt="Text categorization/classification"/></div><div class="mediaobject"><img src="graphics/B05137_08_041.jpg" alt="Text categorization/classification"/></div></div><div class="section" title="Topic modeling"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec137"/>Topic modeling</h2></div></div></div><p>A <a id="id1599" class="indexterm"/>topic is a distribution over a fixed vocabulary. Topic <a id="id1600" class="indexterm"/>modeling can be defined as an ability to capture different core ideas or themes in various documents. This has a wide range of applications, such as the summarization of documents, understanding reasons for sentiments, trends, the news, and many others. The following figure shows how topic modeling can discern a user-specified number <span class="emphasis"><em>k</em></span> of topics from a corpus and then, for every document, assign proportions representing how much of each topic is found in the document:</p><div class="mediaobject"><img src="graphics/B05137_08_043.jpg" alt="Topic modeling"/><div class="caption"><p>Figure 13: Probabilistic topic weights assignment for documents</p></div></div><p>There are quite a few techniques for performing topic modeling using supervised and unsupervised learning in the literature (<span class="emphasis"><em>References</em></span> [13]). We will discuss the most common <a id="id1601" class="indexterm"/>technique known as <span class="strong"><strong>probabilistic latent semantic index</strong></span> (<span class="strong"><strong>PLSI</strong></span>). </p><div class="section" title="Probabilistic latent semantic analysis (PLSA)"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec156"/>Probabilistic latent semantic analysis (PLSA)</h3></div></div></div><p>The idea of PLSA, as in the LSA for feature reduction, is to find latent concepts hidden in the <a id="id1602" class="indexterm"/>corpus by discovering <a id="id1603" class="indexterm"/>the association between co-occurring terms and treating the documents as mixtures of these concepts. This is an unsupervised technique, similar to dimensionality reduction, but the idea is to use it to model the mixture of topics or latent concepts in the document (<span class="emphasis"><em>References</em></span> [12]). </p><p>As shown in the following figure, the model may associate terms occurring together often in the corpus with a latent concept, and each document can then be said to exhibit that topic to a smaller or larger extent:</p><div class="mediaobject"><img src="graphics/B05137_08_044.jpg" alt="Probabilistic latent semantic analysis (PLSA)"/><div class="caption"><p>Figure 14: Latent concept of Baseball capturing the association between documents and related terms</p></div></div><div class="section" title="Input and output"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec232"/>Input and output</h4></div></div></div><p>The inputs are: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A collection <a id="id1604" class="indexterm"/>of documents <a id="id1605" class="indexterm"/>following a certain format and structure. We will give the notation:<div class="mediaobject"><img src="graphics/B05137_08_045.jpg" alt="Input and output"/></div></li><li class="listitem" style="list-style-type: disc">The number of topics that need to be modeled or discovered as <span class="emphasis"><em>k</em></span>.</li></ul></div><p>The output is: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>k</em></span> topics identified T = {T<sub>1</sub>, T<sub>2</sub>,…T<sub>k</sub>}.</li><li class="listitem" style="list-style-type: disc">For each document, coverage of the topic given in the document <span class="emphasis"><em>d</em></span><sub>i</sub> can be written as = {<span class="emphasis"><em>p</em></span><sub>i1</sub>, <span class="emphasis"><em>p</em></span><sub>i2</sub>, …<span class="emphasis"><em>p</em></span><sub>ik</sub>}, where <span class="emphasis"><em>p</em></span><sub>ij</sub>is the probability of the document <span class="emphasis"><em>d</em></span>i covering the topic T<sub>j</sub>.</li></ul></div></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec233"/>How does it work?</h4></div></div></div><p>Implementations <a id="id1606" class="indexterm"/>of PLSA generally follow the steps described here: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Basic preprocessing steps as discussed previously, such as tokenization, stop words removal, stemming, dictionary of words formation, feature extraction (unigrams or n-grams, and so on), and feature selection (unsupervised techniques) are carried out, if necessary.</li><li class="listitem">The problem can be reduced to estimating the distribution of terms in a document, and, given the distribution, choosing the topic based on the maximum terms corresponding to the topic. </li><li class="listitem">Introducing a "latent variable" <span class="emphasis"><em>z</em></span> helps us to select whether the term belongs to a topic. Note that <span class="emphasis"><em>z</em></span> is not "observed", but we assume that it is related to picking the term from the topic. Thus, the probability of the term <span class="emphasis"><em>t</em></span> given the document <span class="emphasis"><em>t</em></span> can be expressed in terms of this latent variable as:<div class="mediaobject"><img src="graphics/B05137_08_053.jpg" alt="How does it work?"/></div></li><li class="listitem">By using two sets of variables (?, p) the equation can be written as: <div class="mediaobject"><img src="graphics/B05137_08_055.jpg" alt="How does it work?"/></div><p>Here, p(t|z; ?) is the probability of latent concepts in terms and p(z|d; p) is the probability of latent concepts in document-specific mixtures. </p></li><li class="listitem">Using log-likelihood to estimate the parameters to maximize: <div class="mediaobject"><img src="graphics/B05137_08_058.jpg" alt="How does it work?"/></div></li><li class="listitem">Since <a id="id1607" class="indexterm"/>this equation involves nonconvex optimization, the EM algorithm is often used to find the parameters iteratively until convergence is reached or the total number of iterations are completed (<span class="emphasis"><em>References</em></span> [6]):<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The E-step of the EM algorithm is used to determine the posterior probability of the latent concepts. The probability of term t occurring in the document d, can be explained by the latent concept z as: <div class="mediaobject"><img src="graphics/B05137_08_062.jpg" alt="How does it work?"/></div></li><li class="listitem">The M-step of the EM algorithm uses the values obtained from the E-step, that is, p(z|d, t) and does parameter estimation as: <div class="mediaobject"><img src="graphics/B05137_08_064.jpg" alt="How does it work?"/></div></li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_08_065.jpg" alt="How does it work?"/></span> = how often the term <span class="emphasis"><em>t</em></span> is associated with concept <span class="emphasis"><em>z</em></span>:
<div class="mediaobject"><img src="graphics/B05137_08_067.jpg" alt="How does it work?"/></div></li><li class="listitem"><span class="inlinemediaobject"><img src="graphics/B05137_08_068.jpg" alt="How does it work?"/></span>= how often <a id="id1608" class="indexterm"/>document <span class="emphasis"><em>d</em></span> is associated with concept <span class="emphasis"><em>z</em></span>.
</li></ol></div></li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec234"/>Advantages and limitations</h4></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Though widely <a id="id1609" class="indexterm"/>used, PLSA has some <a id="id1610" class="indexterm"/>drawbacks that have been overcome by more recent techniques.</li><li class="listitem" style="list-style-type: disc">The unsupervised nature of the algorithm and its general applicability allows it to be used in a wide variety of similar text mining applications, such as clustering documents, associating topics related to authors/time, and so on. </li><li class="listitem" style="list-style-type: disc">PLSA with EM algorithms, as discussed in previous chapters, face the problem of getting "stuck in local optima", unlike other global algorithms, such as evolutionary algorithms.</li><li class="listitem" style="list-style-type: disc">PLSA algorithms can only do topic identification in known documents, but cannot do any predictive <a id="id1611" class="indexterm"/>modeling. PLSA has been generalized and is known as <span class="strong"><strong>latent dirichlet allocation</strong></span> (<span class="strong"><strong>LDA</strong></span>) to overcome this (<span class="emphasis"><em>References</em></span> [14]).</li></ul></div></div></div></div><div class="section" title="Text clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec138"/>Text clustering</h2></div></div></div><p>The <a id="id1612" class="indexterm"/>goal of clustering, as seen in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, is to find groups of data, text, or documents that are similar to <a id="id1613" class="indexterm"/>one another within the group. The granularity of unstructured data can vary from small phrases or sentences, paragraphs, and passages of text to a collection of documents. Text clustering finds its application in many domains, such as information retrieval, summarization, topic modeling, and document classification in unsupervised situations, to name a few. Traditional techniques in clustering can be employed once the unstructured text data is transformed into structured data via preprocessing. The difficulty with <a id="id1614" class="indexterm"/>traditional clustering techniques is the high-dimensional and sparse nature of the dataset obtained using the transformed document-term matrix representation. Many traditional clustering algorithms work only on numeric values of features. Because of this constraint, categorical <a id="id1615" class="indexterm"/>or binary representation of terms cannot be used and often TF or TF-IDF are used for representation of the document-term matrix. </p><p>In this section, we will discuss some of the basic processes and techniques in clustering. We will start with pre-processing and transformations and then discuss some techniques that are widely used and the modifications made to them.</p><div class="section" title="Feature transformation, selection, and reduction"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec157"/>Feature transformation, selection, and reduction</h3></div></div></div><p>Most <a id="id1616" class="indexterm"/>of the pre-processing steps discussed in <a id="id1617" class="indexterm"/>this section are normally used to get either unigram or n-gram <a id="id1618" class="indexterm"/>representation of terms in documents. Dimensionality reduction techniques, such as LSA, are often employed to transform the features into smaller latent space.</p></div><div class="section" title="Clustering techniques"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec158"/>Clustering techniques</h3></div></div></div><p>The <a id="id1619" class="indexterm"/>techniques for clustering in text include probabilistic <a id="id1620" class="indexterm"/>models, as well as those that use distance-based methods, which are familiar to us from when we learned about structured data. We will <a id="id1621" class="indexterm"/>also discuss <span class="strong"><strong>Non-negative Matrix Factorization</strong></span> (<span class="strong"><strong>NNMF</strong></span>) as an effective technique with good performance and interpretability.</p><div class="section" title="Generative probabilistic models"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec235"/>Generative probabilistic models</h4></div></div></div><p>There <a id="id1622" class="indexterm"/>is commonality between <a id="id1623" class="indexterm"/>topic modeling and text clustering in generative methods. As shown in the following figure, clustering associates a document with a single cluster (generally), compared to topic modeling where each document can have a probability of coverage in multiple topics. Every word in topic modeling can be generated by multiple topics in an independent manner, whereas in clustering all the words are generated from the same cluster:</p><div class="mediaobject"><img src="graphics/B05137_08_069.jpg" alt="Generative probabilistic models"/><div class="caption"><p>Figure 15: Exclusive mapping of documents to K-Clusters</p></div></div><p>Mathematically, this <a id="id1624" class="indexterm"/>can be explained using two topics T = {T<sub>1</sub>, T<sub>2</sub>} and two clusters c = {c<sub>1</sub>, c<sub>2</sub>}.</p><p>In clustering, the <a id="id1625" class="indexterm"/>likelihood of the document can be given as: </p><div class="mediaobject"><img src="graphics/B05137_08_072.jpg" alt="Generative probabilistic models"/></div><p>If the document has, say, L terms, this can be further expanded as: </p><div class="mediaobject"><img src="graphics/B05137_08_074.jpg" alt="Generative probabilistic models"/></div><p>Thus, once you "assume" a cluster, all the words come from that cluster. The product of all terms is computed, followed by the summation across all clusters. </p><p>In topic modeling, the likelihood of the document can be given as: </p><div class="mediaobject"><img src="graphics/B05137_08_075.jpg" alt="Generative probabilistic models"/></div><p>Thus, each term ti can be picked independently from topics and hence summation is done inside and the product is done outside. </p><div class="section" title="Input and output"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl5sec88"/>Input and output</h5></div></div></div><p>The inputs are: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A collection <a id="id1626" class="indexterm"/>of documents following <a id="id1627" class="indexterm"/>a certain format and structure expressed with the following notation:<p>D = {<span class="emphasis"><em>d</em></span>1, <span class="emphasis"><em>d</em></span>2, … <span class="emphasis"><em>d</em></span>n}</p></li><li class="listitem" style="list-style-type: disc">The number of clusters that need to be modeled or discovered as <span class="emphasis"><em>k</em></span>.</li></ul></div><p>The output is: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>k</em></span> clusters identified c = {c<sub>1</sub>, c<sub>2</sub>, … c<sub>k</sub>}.</li><li class="listitem" style="list-style-type: disc">For each document, <span class="emphasis"><em>p(d</em></span><sub>i</sub><span class="emphasis"><em>)</em></span> is mapped to one of the clusters <span class="emphasis"><em>k</em></span>.</li></ul></div></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl5sec89"/>How does it work?</h5></div></div></div><p>Here are the steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">
Basic <a id="id1628" class="indexterm"/>preprocessing steps as discussed previously, such as tokenization, stop words removal, stemming, dictionary of words formation, feature extraction (unigrams or n-grams, and so on) of terms, feature transformations (LSA), and even feature selection. Let <span class="emphasis"><em>t</em></span> be the terms in the final feature set; they correspond to the dictionary or vocabulary <span class="inlinemediaobject"><img src="graphics/B05137_08_154.jpg" alt="How does it work?"/></span>.
</li><li class="listitem">Similar to PLSA, we introduce a "latent variable", <span class="emphasis"><em>z</em></span>, which helps us to select whether the document belonging to the cluster falls in the range of <span class="emphasis"><em>z</em></span> ={1, 2, … <span class="emphasis"><em>k</em></span>} corresponding to the clusters. Let the <span class="emphasis"><em>?</em></span> parameter be the parameter we estimate for each latent variable such that <span class="emphasis"><em>p(?</em></span><sub>i</sub><span class="emphasis"><em>)</em></span> corresponds to the probability of cluster <span class="emphasis"><em>z = i</em></span>.</li><li class="listitem">The probability of a document belonging to a cluster is given by <span class="emphasis"><em>p(?</em></span><sub>i</sub><span class="emphasis"><em>)</em></span>, and every term in the document generated from that cluster is given by <span class="emphasis"><em>p(t|?</em></span><sub>i</sub><span class="emphasis"><em>)</em></span>. The likelihood equation can be written as:<div class="mediaobject"><img src="graphics/B05137_08_087.jpg" alt="How does it work?"/></div><p>Note that instead of going through the documents, it is rewritten with terms <span class="emphasis"><em>t</em></span> in the vocabulary <span class="inlinemediaobject"><img src="graphics/B05137_08_154.jpg" alt="How does it work?"/></span> raised to the number of times that term appears in the document. </p></li><li class="listitem">Perform the EM <a id="id1629" class="indexterm"/>algorithm in a similar way to the method we used previously to estimate the parameters as follows:<div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The E-step of the EM algorithm is used to infer the cluster from which the document was generated:<div class="mediaobject"><img src="graphics/B05137_08_090.jpg" alt="How does it work?"/></div></li><li class="listitem">The M-step of the EM algorithm is used to re-estimate the parameters using the result of the E-step, as shown here:</li></ol></div><div class="mediaobject"><img src="graphics/B05137_08_091.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_08_093.jpg" alt="How does it work?"/></div></li><li class="listitem">
  
The final probability estimate for each document can be done using either the maximum <a id="id1630" class="indexterm"/>likelihood or by using a Bayesian algorithm with prior probabilities, as shown here: 
<span class="inlinemediaobject"><img src="graphics/B05137_08_094.jpg" alt="How does it work?"/></span>
or <span class="inlinemediaobject"><img src="graphics/B05137_08_095.jpg" alt="How does it work?"/></span></li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl5sec90"/>Advantages and limitations</h5></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Generative-based <a id="id1631" class="indexterm"/>models have similar <a id="id1632" class="indexterm"/>advantages to LSA and PLSA, where we get a probabilistic score for documents in clusters. By applying domain knowledge or priors using cluster size, the assignments can be further fine-tuned.</li><li class="listitem" style="list-style-type: disc">The disadvantages of the EM algorithm having to do with getting stuck in local optima and being sensitive to the starting point are still true here.</li></ul></div></div></div><div class="section" title="Distance-based text clustering"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec236"/>Distance-based text clustering</h4></div></div></div><p>Most <a id="id1633" class="indexterm"/>distance-based clustering algorithms rely on the similarity or the distance measure used to determine how far apart instances are from each other in feature space. Normally in datasets with numeric values, Euclidean distance or its variations work very well. In text mining, even after converting unstructured text to structured features of terms with numeric values, it has been found that the cosine and Jaccard similarity functions perform better.</p><p>Often, Agglomerative or Hierarchical clustering, discussed in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, are used, which can merge documents based on similarity, as discussed previously. Merging the documents or groups is often done using single linkage, group average linkage, and complete linkage techniques. Agglomerative clustering also results in a structure that can be used for information retrieval and the searching of documents. </p><p>The partition-based clustering techniques k-means and k-medoids accompanied by <span class="emphasis"><em>h</em></span> a suitable similarity or distance method are also employed. The issue with k-means, as indicated in the discussion on clustering techniques, is the sensitivity to starting conditions along with computation space and time. k-medoids are sensitive to the sparse data structure and also have computation space and time constraints. </p></div><div class="section" title="Non-negative matrix factorization (NMF)"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl4sec237"/>Non-negative matrix factorization (NMF)</h4></div></div></div><p>Non-negative <a id="id1634" class="indexterm"/>matrix factorization is another technique used to factorize a large data-feature matrix into two <a id="id1635" class="indexterm"/>non-negative matrices, which not only perform the dimensionality reduction, but are also easier to inspect. NMF has gained popularity for document clustering, and many variants of NMF with different optimization functions have now been shown to be very effective in clustering text (<span class="emphasis"><em>References</em></span> [15]). </p><div class="section" title="Input and output"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl5sec91"/>Input and output</h5></div></div></div><p>The inputs are: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A collection <a id="id1636" class="indexterm"/>of documents <a id="id1637" class="indexterm"/>following a certain format and structure given by the notation:<p>D = {d<sub>1</sub>, d<sub>2</sub>, … d<sub>n</sub>}</p></li><li class="listitem" style="list-style-type: disc">Number of clusters that need to be modeled or discovered as <span class="emphasis"><em>k</em></span>.</li></ul></div><p>The output is:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">k clusters identified c = {c<sub>1</sub>, c<sub>2</sub>, … c<sub>k</sub>} with documents assigned to the clusters.</li></ul></div></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl5sec92"/>How does it work?</h5></div></div></div><p>The <a id="id1638" class="indexterm"/>mathematical details and interpretation of NMF are given in the following: </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The basic idea behind NMF is to factorize the input matrix using low-rank approximation, as follows: <div class="mediaobject"><img src="graphics/B05137_08_098.jpg" alt="How does it work?"/></div></li><li class="listitem">A non-linear optimization function is used as: <div class="mediaobject"><img src="graphics/B05137_08_099.jpg" alt="How does it work?"/></div><p>This is convex in W or H, but not in both, resulting in no guarantees of a global minimum. Various algorithms that use constrained least squares, such as mean-square error and gradient descent, are used to solve the optimization function. </p></li><li class="listitem">The interpretation of NMF, especially in understanding the latent topics based on<a id="id1639" class="indexterm"/> terms, makes it very useful. The input A<sub>m x n</sub> of terms and documents, can be represented in low rank approximation as W<sub>m x k</sub> H<sub>k x n</sub> matrices, where W<sub>m x k</sub> is the term-topic representation whose columns are NMF basis vectors. The non zero elements of column 1 of W, given by W<sub>1</sub>, correspond to particular terms. Thus, the w<sub>ij</sub> can be interpreted as a basis vector W<sub>i</sub> about the terms j. The H<sub>i1</sub> can be interpreted as how much the document given by doc 1 has affinity towards the direction of the topic vector W<sub>i</sub>. </li><li class="listitem">From the paper (<span class="emphasis"><em>References</em></span> [18]) it was clearly shown how the basis vectors obtained for the medical abstracts, known as the Medlars dataset, creates highly interpretable basis vectors. The highest weighted terms in these basis vectors directly correspond to the concept, for example, W<sub>1</sub> corresponds to the topic related to "heart" and W<sub>5</sub> is related to "developmental disability". <div class="mediaobject"><img src="graphics/B05137_08_112.jpg" alt="How does it work?"/><div class="caption"><p>Figure 16: From Langville et al (2006) showing some basis vectors for medical datasets for interpretability. </p></div></div></li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h5 class="title"><a id="ch08lvl5sec93"/>Advantages and limitations</h5></div></div></div><p>NMF <a id="id1640" class="indexterm"/>has been shown to be almost <a id="id1641" class="indexterm"/>equal in performance with top algorithms, such as LSI, for information retrieval and queries:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Scalability, computation, and storage is better in NMF than in LSA or LSI using SVD.</li><li class="listitem" style="list-style-type: disc">NMF has a problem with optimization not being global and getting stuck in local minima. </li><li class="listitem" style="list-style-type: disc">NMF generation of factors depends on the algorithms for optimization and the parameters chosen, and is not unique.</li></ul></div></div></div></div><div class="section" title="Evaluation of text clustering"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec159"/>Evaluation of text clustering</h3></div></div></div><p>In the case of labeled datasets, all the external measures discussed in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques,</em></span> such as F-measure and Rand Index are useful in <a id="id1642" class="indexterm"/>evaluating the clustering techniques. When the dataset doesn't have labels, some of the techniques described as the internal measures, such as the Davies–Bouldin Index, R-Squared, and Silhouette Index, can be used. </p><p>The general good practice is to adapt and make sure similarity between the documents, as discussed in this section, is used for measuring closeness, remoteness, and spread of the cluster when applied to text mining data. Similarly usage depends on the algorithm and some relevance to the problem too. In distance-based partition algorithms, the similarity of the document can be computed with the mean vector or the centroid. In hierarchical algorithms, similarity can be computed with most similar or dissimilar documents in the group. </p></div></div><div class="section" title="Named entity recognition"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec139"/>Named entity recognition</h2></div></div></div><p>
<span class="strong"><strong>Named entity recognition</strong></span> (<span class="strong"><strong>NER</strong></span>) is one of the most important topics in information retrieval <a id="id1643" class="indexterm"/>for text mining. Many complex <a id="id1644" class="indexterm"/>mining tasks, such as the identification of relations, annotations of events, and correlation between entities, use NER as the initial component or basic preprocessing step. </p><p>Historically, manual rules-based and regular expression-based techniques were used for entity recognition. These manual rules relied on basic pre processing, using POS tags as features, along with hand-engineered features, such as the presence of capital words, usage of punctuations prior to words, and so on. </p><p>Statistical learning-based techniques are now used more for NER and its variants. NER can be mapped to sequence labeling and prediction problems in Machine Learning. BIO notation, where each entity type T has two labels B-T and I-T corresponding to beginning and intermediate, respectively, is labeled, and learning involves finding the pattern and predicting it in unseen data. The O represents an outside or unrelated entity in the sequence of text. The entity type T is further classified into Person, Organization, Data, and location in the most basic form. </p><p>In <a id="id1645" class="indexterm"/>this section, we will discuss the two most <a id="id1646" class="indexterm"/>common algorithms used: generative-based hidden Markov models and discriminative-based maximum entropy models. </p><p>Though we are discussing these algorithms in the context of Named Entity Recognition, the same algorithms and processes can be used for other NLP tasks such as POS Tagging, where tags are associated with a sequence rather than associating the NER classes. </p><div class="section" title="Hidden Markov models for NER"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec160"/>Hidden Markov models for NER</h3></div></div></div><p>Hidden <a id="id1647" class="indexterm"/>Markov models, as explained in <a class="link" href="ch06.html" title="Chapter 6. Probabilistic Graph Modeling">Chapter 6</a>, <span class="emphasis"><em>Probabilistic Graph Modeling</em></span>, are the sequence-based <a id="id1648" class="indexterm"/>generative models that assume an underlying distribution that generates the sequences. The training data obtained by labeling sequences with the right NER classes can be used to learn the distribution and parameters, so that for unseen future sequences, effective predictions can be performed.</p><div class="section" title="Input and output"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl5sec94"/>Input and output</h4></div></div></div><p>The <a id="id1649" class="indexterm"/>training data consists of text sequences x ={x<sub>1</sub>, x<sub>2</sub>, ... x<sub>n</sub>} where each xi is a word in the text sequence and labels for each word are available as y ={y<sub>1</sub>, y<sub>2</sub>, ... y<sub>n</sub>}. The algorithm generates a model so that on testing on unseen <a id="id1650" class="indexterm"/>data, the labels for new sequences can be generated.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl5sec95"/>How does it work?</h4></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In the <a id="id1651" class="indexterm"/>simplest form, a Markov assumption is made, which is that the hidden states and labels of the sequences are only dependent on the previous state. An adaptation to the sequence of words with labels is shown in the following figure:<div class="mediaobject"><img src="graphics/B05137_08_116.jpg" alt="How does it work?"/><div class="caption"><p>Figure 17: Text sequence and labels corresponding to NER in Hidden Markov Chain</p></div></div></li><li class="listitem">The HMM formulation of the sequence classification helps in estimating the joint probability maximized on training data:<div class="mediaobject"><img src="graphics/B05137_08_117.jpg" alt="How does it work?"/></div></li><li class="listitem">Each <span class="emphasis"><em>y</em></span><sub>i</sub> is assumed to be generated based on <span class="emphasis"><em>y</em></span><sub>i–1</sub> and <span class="emphasis"><em>x</em></span>i. The first word in the entity is generated conditioned on current and previous labels, that is, <span class="emphasis"><em>y</em></span><sub>i</sub> and <span class="emphasis"><em>y</em></span><sub>i–1</sub>. If the instance is already a Named entity, then conditioning is only on previous <a id="id1652" class="indexterm"/>instances, that is, <span class="emphasis"><em>x</em></span><sub>i–1</sub>. Outside words such as "visited" and "in" are considered "not a name class". </li><li class="listitem">The HMM formulation with the forward-backward algorithm can be used to determine the likelihood of a sequence of observations with parameters learned from the training data.</li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl5sec96"/>Advantages and limitations</h4></div></div></div><p>The advantages and limitations are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">HMMs <a id="id1653" class="indexterm"/>are good for short sequences, as shown, with<a id="id1654" class="indexterm"/> one word or term and independence assumption. For sequences with entities that have a longer span, the results will violate these assumptions.</li><li class="listitem" style="list-style-type: disc">The HMM needs a large set of training data to estimate the parameters.</li></ul></div></div></div><div class="section" title="Maximum entropy Markov models for NER"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec161"/>Maximum entropy Markov models for NER</h3></div></div></div><p>
<span class="strong"><strong>Maximum entropy Markov model</strong></span> (<span class="strong"><strong>MEMM</strong></span>) is a popular NER technique that uses the concept <a id="id1655" class="indexterm"/>of Markov chains and maximum <a id="id1656" class="indexterm"/>entropy models to learn and predict the named entities (<span class="emphasis"><em>References</em></span> [16] and [17]). </p><div class="section" title="Input and output"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl5sec97"/>Input and output</h4></div></div></div><p>The <a id="id1657" class="indexterm"/>training data consists of text <a id="id1658" class="indexterm"/>sequences  x ={x1, x2, ... xn} where each xi is a word in the text sequence and labels for each word are available as y ={y1, y2, ... yn}. The algorithm generates models so that, on testing on unseen data, the labels for new sequences can be generated.</p></div><div class="section" title="How does it work?"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl5sec98"/>How does it work?</h4></div></div></div><p>The <a id="id1659" class="indexterm"/>following illustrates how the MEMM method is used for learning named entities. </p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The features in MEMM can be word features or other types of features, such as "isWordCapitalized", and so on, which gives it a bit more context and improves performance compared to HMM, where it is only word-based. </li><li class="listitem">Next, let us look at a maximum entropy model known as a MaxEnt model, which is an exponential probabilistic model, but which can also be seen as a multinomial logistic regression model. In basic MaxEnt models, given the features {f<sub>1</sub>, f<sub>2</sub> … f<sub>N</sub>} and classes c<sub>1</sub>, c<sub>2</sub> … c<sub>C</sub>, weights for these features are learned {w<sub>c1</sub>, w<sub>c2</sub> … w<sub>cN</sub>} per class using optimization methods from the training data, and the probability of a particular class can be estimated as:<div class="mediaobject"><img src="graphics/B05137_08_126.jpg" alt="How does it work?"/></div></li><li class="listitem">The feature fi is formally written as f<sub>i</sub>(c, x), which means the feature f<sub>i</sub> for class c and observation x. The f<sub>i</sub>(c, x) is generally binary with values 1/0 in most NER models. Thus, it can be written as: <div class="mediaobject"><img src="graphics/B05137_08_131.jpg" alt="How does it work?"/></div><p>Maximum likelihood based on the probability of prediction across class can be used to select a single class: </p><div class="mediaobject"><img src="graphics/B05137_08_132.jpg" alt="How does it work?"/></div></li><li class="listitem">For every word, we use the current word, the features from "nearby" words, and the predictions on the nearby words to create a joint probability model. This is <a id="id1660" class="indexterm"/>also called local learning as the chunks of test and distribution are learned around local features corresponding to the word. <p>Mathematically, we see how a discriminative model is created from current word and last prediction as: </p><div class="mediaobject"><img src="graphics/B05137_08_133.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_08_134.jpg" alt="How does it work?"/></div><p>Generalizing for the k features:</p><div class="mediaobject"><img src="graphics/B05137_08_135.jpg" alt="How does it work?"/></div></li><li class="listitem">Thus, in MEMM we compute the probability of the state, which is the class in NER, and even though we condition on prediction of nearby words given by y<sub>i–1</sub>, in general we can use more features, and that is the advantage over the HMM model discussed previously:<div class="mediaobject"><img src="graphics/B05137_08_137.jpg" alt="How does it work?"/></div><div class="mediaobject"><img src="graphics/B05137_08_138.jpg" alt="How does it work?"/><div class="caption"><p>Figure 18 : Text Sequences and observation probabilities with labels </p></div></div></li><li class="listitem">The Viterbi algorithm is used to perform the estimation of class for the word or <a id="id1661" class="indexterm"/>decoding/inferencing in HMM, that is, to get estimates for p(y<sub>i</sub>|y<sub>i–1</sub>, X<sub>i</sub>)</li><li class="listitem">Finally, the MaxEnt model is used to estimate the weights as before using the optimization methods for state changes in general:<div class="mediaobject"><img src="graphics/B05137_08_140.jpg" alt="How does it work?"/></div></li></ol></div></div><div class="section" title="Advantages and limitations"><div class="titlepage"><div><div><h4 class="title"><a id="ch08lvl5sec99"/>Advantages and limitations</h4></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">MEMM <a id="id1662" class="indexterm"/>has more flexibility <a id="id1663" class="indexterm"/>in using features that are not just word-based or even human-engineered, giving it more richness and enabling its models to be more predictive.</li><li class="listitem" style="list-style-type: disc">MEMM can have a range of more than just close words, giving it an advantage of detection over larger spans compared to HMM.</li></ul></div></div></div></div><div class="section" title="Deep learning and NLP"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec140"/>Deep learning and NLP</h2></div></div></div><p>In the <a id="id1664" class="indexterm"/>last few years, Deep Learning and its application <a id="id1665" class="indexterm"/>to various areas of NLP has shown huge success and is <a id="id1666" class="indexterm"/>considered the cutting edge of technology <a id="id1667" class="indexterm"/>these days. The main advantage of using Deep Learning lies in a small subset of tools and methods, which are useful in a wide variety of NLP problems. It solves the basic issue of feature engineering and carefully created manual representations by automatically learning them, and thus solves the issue of having a large number of language experts dealing with a wide range of problems, such as text classification, sentiment analysis, POS tagging, and machine translation, to name a few. In this section, we will try to cover important concepts and research in the area of Deep Learning and NLP.</p><p>In his seminal paper, Bengio introduced one of the most important building blocks for deep learning known as word embedding or word vector (<span class="emphasis"><em>References</em></span> [20]). Word embedding can be defined as a parameterized function that maps words to a high dimensional vector (usually 25 to 500 dimensions based on the application). </p><p>Formally, this can be written as <span class="inlinemediaobject"><img src="graphics/B05137_08_141.jpg" alt="Deep learning and NLP"/></span>.</p><p>For example, <span class="inlinemediaobject"><img src="graphics/B05137_08_142.jpg" alt="Deep learning and NLP"/></span> and <span class="inlinemediaobject"><img src="graphics/B05137_08_143.jpg" alt="Deep learning and NLP"/></span>, and so on.</p><p>A neural network (<span class="emphasis"><em>R</em></span>) whose inputs are the words from sentences or n-grams of sentences with binary classification, such as whether the sequence of words in n-grams are valid or not, is used to train and learn the <span class="emphasis"><em>W</em></span> and <span class="emphasis"><em>R</em></span>:</p><div class="mediaobject"><img src="graphics/B05137_08_146.jpg" alt="Deep learning and NLP"/><div class="caption"><p>Figure 19: A modular neural network learning 5-gram words for valid–invalid classification</p></div></div><p>For example: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>R(W(cat),W(sat ),W(on),W(the),W(mat)) = 1(valid)</em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>R(W(cat),W(sat),W(on),W(the),W(mat)) = 0 (Invalid)</em></span></li></ul></div><p>The idea of training these sentences or n-grams is not only to learn the correct structure of the phrases, but also the right parameters for <span class="emphasis"><em>W</em></span> and <span class="emphasis"><em>R</em></span>. The word embeddings can also be projected <a id="id1668" class="indexterm"/>on to a lower dimensional space, such as a 2D space, using <a id="id1669" class="indexterm"/>various linear and non linear feature reduction/visualization <a id="id1670" class="indexterm"/>techniques introduced in <a class="link" href="ch03.html" title="Chapter 3. Unsupervised Machine Learning Techniques">Chapter 3</a>, <span class="emphasis"><em>Unsupervised Machine Learning Techniques</em></span>, which humans can easily visualize. This <a id="id1671" class="indexterm"/>visualization of word embeddings in two dimensions using techniques such as t-SNE discovers important information about the closeness of words based on semantic meaning and even clustering of words in the area, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B05137_08_149.jpg" alt="Deep learning and NLP"/><div class="caption"><p>Figure 20: t-SNE representation of a small section of the entire word mappings. Numbers in Roman numerals and words are shown clustered together on the left, while semantically close words are clustered together on the right.</p></div></div><p>Further extending the concepts, both Collobert and Mikolov showed that the side effects of learning the word embeddings can be very useful in a variety of NLP tasks, such as similar phrase learning (for example, <span class="emphasis"><em>W("the color is red")) </em></span>
<span class="emphasis"><em>?</em></span>
<span class="emphasis"><em> W("the color is yellow"))</em></span>, finding synonyms (for example, <span class="emphasis"><em>W("nailed")) </em></span>
<span class="emphasis"><em>?</em></span>
<span class="emphasis"><em> W("smashed"))</em></span>, analogy mapping (for example, <span class="emphasis"><em>W("man")?W("woman") then W("king")?W("queen"))</em></span>, and even complex relationship mapping (for example, <span class="emphasis"><em>W("Paris")?W("France") then W("Tokyo")?W("Japan"))</em></span> (<span class="emphasis"><em>References</em></span> [21 and 22]).</p><p>The extension of word embedding concepts towards a generalized representation that helps us reuse <a id="id1672" class="indexterm"/>the representation with various NLP problems <a id="id1673" class="indexterm"/>(with minor extensions) has been the main reason for many recent successes of Deep Learning in NLP. Socher, in his research, extended the word embeddings concept <a id="id1674" class="indexterm"/>to produce bilingual word <a id="id1675" class="indexterm"/>embeddings, that is, embed words from two different languages, such as Chinese (Mandarin) and English into a shared space (<span class="emphasis"><em>References</em></span> [23]). By learning two language word embeddings independently of each other and then projecting them in a same space, his work gives us interesting insights into word similarities across languages that can be extended for Machine Translation. Socher also did interesting work on projecting the images learned from CNNs with the word embedding in to the same space for associating words with images as a basic classification problem (<span class="emphasis"><em>References</em></span> [24]). Google, around the same time, has also been working on similar concepts, but at a larger scale for word-image matching and learning (<span class="emphasis"><em>References</em></span> [26]).</p><p>Extending the word embedding concept to have combiners or association modules that can help combine <a id="id1676" class="indexterm"/>the words, words-phrases, phrases-phrases <a id="id1677" class="indexterm"/>in all combinations to learn complex sentences has been the idea of Recursive Neural Networks. The following figure shows how <a id="id1678" class="indexterm"/>complex association <span class="emphasis"><em>((the cat)(sat(on (the mat))))</em></span> can be <a id="id1679" class="indexterm"/>learned using Recursive Neural Networks. It also removes the constraint of a "fixed" number of inputs in neural networks because of the ability to recursively combine:</p><div class="mediaobject"><img src="graphics/B05137_08_155.jpg" alt="Deep learning and NLP"/><div class="caption"><p>Figure 21: Recursive Neural Network showing how complex phrases can be learned.</p></div></div><p>Recursive Neural Networks have been showing great promise in NLP tasks, such as sentiment analysis, where association of one negative word at the start of many positive words has an overall negative impact on sentences, as shown in the following figure: </p><div class="mediaobject"><img src="graphics/B05137_08_156.jpg" alt="Deep learning and NLP"/><div class="caption"><p>Figure 22: A complex sentence showing words with negative (as red circle), positive (green circle), and neutral (empty with 0) connected through RNN with overall negative sentiment.</p></div></div><p>The <a id="id1680" class="indexterm"/>concept of recursive neural networks is now extended using the <a id="id1681" class="indexterm"/>building blocks of encoders and decoders to learn reversible <a id="id1682" class="indexterm"/>sentence representation—that is, reconstructing <a id="id1683" class="indexterm"/>the original sentence with roughly the same meaning from the input sentence (<span class="emphasis"><em>References</em></span> [27]). This has become the central core theme behind Neural Machine Translation. Modeling conversations using the encoder-decoder framework of RNNs has also made huge breakthroughs (<span class="emphasis"><em>References</em></span> [28]). </p></div></div>
<div class="section" title="Tools and usage"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec70"/>Tools and usage</h1></div></div></div><p>We <a id="id1684" class="indexterm"/>will now <a id="id1685" class="indexterm"/>discuss some of the most well-known tools and libraries in Java that are used in various NLP and text mining applications.</p><div class="section" title="Mallet"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl3sec162"/>Mallet </h2></div></div></div><p>Mallet <a id="id1686" class="indexterm"/>is a Machine Learning toolkit for text written in Java, which comes with <a id="id1687" class="indexterm"/>several natural language processing libraries, including those some for document classification, sequence tagging, and topic modeling, as well as various Machine Learning algorithms. It is open source, released under CPL. Mallet exposes an extensive API (see the following screenshots) to create and configure sequences of "pipes" for pre-processing, vectorizing, feature selection, and so on, as well as to extend implementations of classification and clustering algorithms, plus a host of other text analytics and Machine Learning capabilities.</p><div class="mediaobject"><img src="graphics/B05137_08_157.jpg" alt="Mallet"/></div><div class="mediaobject"><img src="graphics/B05137_08_158.jpg" alt="Mallet"/></div></div><div class="section" title="KNIME"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl3sec163"/>KNIME</h2></div></div></div><p>KNIME is an open <a id="id1688" class="indexterm"/>platform for analytics with Open GL licensing with a number <a id="id1689" class="indexterm"/>of powerful tools for conducting all aspects of data science. The <a id="id1690" class="indexterm"/>Text Processing module is available for separate download from KNIME Labs. KNIME has an intuitive drag and drop UI with downloadable examples available from their workflow server.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>KNIME: <a class="ulink" href="https://www.knime.org/">https://www.knime.org/</a>
</p><p>KNIME Labs: <a class="ulink" href="https://tech.knime.org/knime-text-processing">https://tech.knime.org/knime-text-processing</a>
</p></div></div><p>The platform includes a Node repository that contains all the necessary tools to compose your workflow with a convenient nesting of nodes that can easily be reused by copying and pasting. The execution of the workflows is simple. Debugging errors can take some getting used to, so our recommendation is to take the text mining example, use a different dataset as input, and make the workflow execute without errors. This is the quickest way to get familiar with the platform.</p></div><div class="section" title="Topic modeling with mallet"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec142"/>Topic modeling with mallet</h2></div></div></div><p>We will now illustrate the usage of API and Java code <a id="id1691" class="indexterm"/>to implement Topic Modeling to give the user an illustration <a id="id1692" class="indexterm"/>on how to build a text learning pipeline for a problem in Java:</p><div class="informalexample"><pre class="programlisting">//create pipeline
ArrayList&lt;Pipe&gt; pipeList = new ArrayList&lt;Pipe&gt;();
  // Pipes: lowercase, tokenize, remove stopwords, map to features
pipeList.add( new CharSequenceLowercase() );
pipeList.add( new CharSequence2TokenSequence(Pattern.compile("\\p{L}[\\p{L}\\p{P}]+\\p{L}")) );
pipeList.add( new TokenSequenceRemoveStopwords(new File("stopReuters/en.txt"), "UTF-8", false, false, false) );
//add all 
pipeList.add( new TokenSequence2FeatureSequence() );
InstanceList instances = new InstanceList (new SerialPipes(pipeList));
//read the file
Reader fileReader = new InputStreamReader(new FileInputStream(new File(reutersFile)), "UTF-8");
instances.addThruPipe(new CsvIterator (fileReader, Pattern.compile("^(\\S*)[\\s,]*(\\S*)[\\s,]*(.*)$"),
3, 2, 1)); // name fields, data, label</pre></div><p>ParallelTopicModel in Mallet has an API with parameters such as the number of topics, alpha, and beta that control the underlying parameter for tuning the LDA using Dirichlet distribution. Parallelization is very well supported, as seen by the increased number of threads available in the system:</p><div class="informalexample"><pre class="programlisting">ParallelTopicModel model = new ParallelTopicModel(10, 1.0, 0.01);//10 topics using LDA method
model.addInstances(instances);//add instances
model.setNumThreads(3);//parallelize with threading
model.setNumIterations(1000);//gibbs sampling iterations
model.estimate();//perform estimation of probability</pre></div><p>Topic and term association is shown in the following screenshot as the result of running the ParallelTopicModel in Mallet. Clearly, the top terms and association of the topics are very well discovered in many cases, such as the classes of exec, acq, wheat, crude, corn, and earning:</p><div class="mediaobject"><img src="graphics/B05137_08_159.jpg" alt="Topic modeling with mallet"/></div></div><div class="section" title="Business problem"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec143"/>Business problem</h2></div></div></div><p>The <a id="id1693" class="indexterm"/>Reuters corpus labels each document with one of 10 categories. The aim of the experiments in this case study is to employ the techniques of text processing learned in this chapter to give structure to these documents using vector space modeling. This is done in three different ways, and four classification algorithms are used to train and make predictions using the transformed dataset in each of the three cases. The open source Java analytics platform KNIME was used for text processing and learning.</p></div><div class="section" title="Machine Learning mapping"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec144"/>Machine Learning mapping</h2></div></div></div><p>Among the <a id="id1694" class="indexterm"/>learning techniques for unstructured data, such as text or images, classification of the data into different categories given a training set with labels is a supervised learning problem. However, since the data is unstructured, some statistical or information theoretic means are necessary to extract learnable features from the data. In the design of this study, we performed feature representation and selection on the documents before using linear, non-linear, and ensemble methods for classification.</p></div><div class="section" title="Data collection"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec145"/>Data collection</h2></div></div></div><p>The <a id="id1695" class="indexterm"/>dataset used in the experiments is a version of the Reuters-21578 Distribution 1.0 Text Categorization Dataset available from the UCI Machine Learning Repository:</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>Reuters-21578 dataset: <a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection">https://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection</a>
</p></div></div><p>This <a id="id1696" class="indexterm"/>dataset is a Modified-Apte split containing 9,981 documents, each with a class label indicating the category of the document. There are 10 distinct categories in the dataset.</p></div><div class="section" title="Data sampling and transformation"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec146"/>Data sampling and transformation</h2></div></div></div><p>After<a id="id1697" class="indexterm"/> importing the data file, we performed a series <a id="id1698" class="indexterm"/>of pre-processing steps in order to enrich and transform the data before training any models on the documents. These steps can be seen in the screenshot of the workflow created in KNIME. They include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Punctuation erasure</li><li class="listitem" style="list-style-type: disc">N char filtering (removes tokens less than four characters in length)</li><li class="listitem" style="list-style-type: disc">Number filtering</li><li class="listitem" style="list-style-type: disc">Case conversion – convert all to lower case</li><li class="listitem" style="list-style-type: disc">Stop word filtering</li><li class="listitem" style="list-style-type: disc">Stemming</li></ul></div><p>Prior to the learning step, we sampled the data randomly into a 70-30 split for training and testing, respectively. We used five-fold cross-validation in each experiment.</p><div class="mediaobject"><img src="graphics/B05137_08_160.jpg" alt="Data sampling and transformation"/></div><p>The <a id="id1699" class="indexterm"/>preceding screenshot shows the workflow for the first <a id="id1700" class="indexterm"/>experiment set, which uses a binary vector of features. Data import is followed by a series of pre processing nodes, after which the dataset is transformed into a document vector. After adding back the target vector, the workflow branches out into four classification tasks, each using a five-fold cross-validation setup. Results are gathered in the Scorer node.</p></div><div class="section" title="Feature analysis and dimensionality reduction"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec147"/>Feature analysis and dimensionality reduction</h2></div></div></div><p>We <a id="id1701" class="indexterm"/>conducted three sets of experiments in total. In <a id="id1702" class="indexterm"/>the first set, after pre processing, we used binary vectorization of the terms, which adds a representation indicating whether or not a term appeared in the document:</p><div class="mediaobject"><img src="graphics/B05137_1.jpg" alt="Feature analysis and dimensionality reduction"/></div><p>In the second experiment, we used the values for relative <span class="strong"><strong>Term Frequency</strong></span> (<span class="strong"><strong>TF</strong></span>) for each term, resulting in a value between 0 and 1. </p><div class="mediaobject"><img src="graphics/B05137_08_163.jpg" alt="Feature analysis and dimensionality reduction"/></div><p>In <a id="id1703" class="indexterm"/>the third, we performed feature selection by filtering <a id="id1704" class="indexterm"/>out terms that had a relative TF score of less than 0.01.</p></div><div class="section" title="Models, results, and evaluation"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec148"/>Models, results, and evaluation</h2></div></div></div><p>For <a id="id1705" class="indexterm"/>each of the three sets of experiments, we used two linear <a id="id1706" class="indexterm"/>classifiers (naïve Bayes and SVM using linear kernel) and <a id="id1707" class="indexterm"/>two non-linear classifiers (Decision Tree and AdaBoost with Naïve Bayes as base learner). In text mining classification, precision/recall metrics are generally chosen as the evaluation metric over accuracy, which is more common in traditional, balanced classification problems.</p><p>The results from the three sets of experiments are given in the tables. Scores are averages over all the classes:</p><p>Binary Term Vector: </p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Classifier</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Sensitivity</p>
</th><th style="text-align: left" valign="bottom">
<p>Specificity</p>
</th><th style="text-align: left" valign="bottom">
<p>F-measure</p>
</th><th style="text-align: left" valign="bottom">
<p>Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>Cohen's kappa</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Naïve Bayes</p>
</td><td style="text-align: left" valign="top">
<p>0.5079</p>
</td><td style="text-align: left" valign="top">
<p>0.5281</p>
</td><td style="text-align: left" valign="top">
<p>0.5079</p>
</td><td style="text-align: left" valign="top">
<p>0.9634</p>
</td><td style="text-align: left" valign="top">
<p>0.5087</p>
</td><td style="text-align: left" valign="top">
<p>0.7063</p>
</td><td style="text-align: left" valign="top">
<p>0.6122</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Decision Tree</p>
</td><td style="text-align: left" valign="top">
<p>0.4989</p>
</td><td style="text-align: left" valign="top">
<p>0.5042</p>
</td><td style="text-align: left" valign="top">
<p>0.4989</p>
</td><td style="text-align: left" valign="top">
<p>0.9518</p>
</td><td style="text-align: left" valign="top">
<p>0.5013</p>
</td><td style="text-align: left" valign="top">
<p>0.7427(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.6637(2)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>AdaBoost(NB)</p>
</td><td style="text-align: left" valign="top">
<p>0.5118(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.5444(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.5118(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.9665(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.5219(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.7285</p>
</td><td style="text-align: left" valign="top">
<p>0.6425</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LibSVM</p>
</td><td style="text-align: left" valign="top">
<p>0.6032(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.5633(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.6032(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.9808(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.5768(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.8290(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.7766(1)</p>
</td></tr></tbody></table></div><p>Relative TF vector:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Classifier</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Sensitivity</p>
</th><th style="text-align: left" valign="bottom">
<p>Specificity</p>
</th><th style="text-align: left" valign="bottom">
<p>F-measure</p>
</th><th style="text-align: left" valign="bottom">
<p>Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>Cohen's kappa</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Naïve Bayes</p>
</td><td style="text-align: left" valign="top">
<p>0.4853</p>
</td><td style="text-align: left" valign="top">
<p>0.5480(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.4853</p>
</td><td style="text-align: left" valign="top">
<p>0.9641</p>
</td><td style="text-align: left" valign="top">
<p>0.5113(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.7248</p>
</td><td style="text-align: left" valign="top">
<p>0.6292</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Decision Tree</p>
</td><td style="text-align: left" valign="top">
<p>0.4947(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.4954</p>
</td><td style="text-align: left" valign="top">
<p>0.4947(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.9703(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.4950</p>
</td><td style="text-align: left" valign="top">
<p>0.7403(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.6612(2)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>AdaBoost(NB)</p>
</td><td style="text-align: left" valign="top">
<p>0.4668</p>
</td><td style="text-align: left" valign="top">
<p>0.5326</p>
</td><td style="text-align: left" valign="top">
<p>0.4668</p>
</td><td style="text-align: left" valign="top">
<p>0.9669</p>
</td><td style="text-align: left" valign="top">
<p>0.4842</p>
</td><td style="text-align: left" valign="top">
<p>0.6963</p>
</td><td style="text-align: left" valign="top">
<p>0.6125</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LibSVM</p>
</td><td style="text-align: left" valign="top">
<p>0.6559(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.6651(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.6559(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.9824(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.6224(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.8433(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.7962(1)</p>
</td></tr></tbody></table></div><p>Relative TF vector with threshold filtering (rel TF &gt; 0.01):</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Classifier</p>
</th><th style="text-align: left" valign="bottom">
<p>Recall</p>
</th><th style="text-align: left" valign="bottom">
<p>Precision</p>
</th><th style="text-align: left" valign="bottom">
<p>Sensitivity</p>
</th><th style="text-align: left" valign="bottom">
<p>Specificity</p>
</th><th style="text-align: left" valign="bottom">
<p>F-measure</p>
</th><th style="text-align: left" valign="bottom">
<p>Accuracy</p>
</th><th style="text-align: left" valign="bottom">
<p>Cohen's kappa</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Naïve Bayes</p>
</td><td style="text-align: left" valign="top">
<p>0.4689</p>
</td><td style="text-align: left" valign="top">
<p>0.5456(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.4689</p>
</td><td style="text-align: left" valign="top">
<p>0.9622</p>
</td><td style="text-align: left" valign="top">
<p>0.4988</p>
</td><td style="text-align: left" valign="top">
<p>0.7133</p>
</td><td style="text-align: left" valign="top">
<p>0.6117</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Decision Tree</p>
</td><td style="text-align: left" valign="top">
<p>0.5008(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.5042</p>
</td><td style="text-align: left" valign="top">
<p>0.5008(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.9706(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.5022(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.7439(2)</p>
</td><td style="text-align: left" valign="top">
<p>0.6657(2)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>AdaBoost(NB)</p>
</td><td style="text-align: left" valign="top">
<p>0.4435</p>
</td><td style="text-align: left" valign="top">
<p>0.4992</p>
</td><td style="text-align: left" valign="top">
<p>0.4435</p>
</td><td style="text-align: left" valign="top">
<p>0.9617</p>
</td><td style="text-align: left" valign="top">
<p>0.4598</p>
</td><td style="text-align: left" valign="top">
<p>0.6870</p>
</td><td style="text-align: left" valign="top">
<p>0.5874</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>LibSVM</p>
</td><td style="text-align: left" valign="top">
<p>0.6438(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.6326(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.6438(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.9810(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.6118(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.8313(1)</p>
</td><td style="text-align: left" valign="top">
<p>0.7806(1)</p>
</td></tr></tbody></table></div></div><div class="section" title="Analysis of text processing results"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec149"/>Analysis of text processing results</h2></div></div></div><p>The <a id="id1708" class="indexterm"/>analysis of results obtained from our experiments on the Reuters dataset is presented here with some key observations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">As seen in the first table, with the binary representation of terms, Naïve Bayes scores around 0.7, which indicates that the features generated have good discriminating power. AdaBoost on the same configuration of Naïve Bayes further improves all the metrics, such as precision, recall, F1-measure, and accuracy, by about 2%, indicating the advantage of boosting and meta-learning. </li><li class="listitem" style="list-style-type: disc">As seen in the first table, non-linear classifiers, such as Decision Tree, do only marginally better than linear Naïve Bayes in most metrics. SVM with a linear classifier increases accuracy by 17% over linear Naïve Bayes and has better metrics similarly in almost all measures. SVM and kernels, which have no issues with higher dimensional data, the curse of text classification, are thus one of the better algorithms for modeling, and the results confirm this.</li><li class="listitem" style="list-style-type: disc">Changing the representation from binary to TF improves many measures, such as accuracy, for linear Naïve Bayes (from 0.70 to 0.72) and SVM (0.82 to 0.84). This indeed confirms that TF-based representation in many numeric-based algorithms, such as SVM. AdaBoost performance with Naïve Bayes drops in most metrics when the underlying classifier Bayes gets stronger in performance, as shown in many theoretical and empirical results. </li><li class="listitem" style="list-style-type: disc">Finally, by reducing features using threshold TF &gt; 0.01, as used here, we get almost similar or somewhat reduced performance in most classifiers, indicating that although certain terms seem rare, they have discriminating power, and reducing them has a negative impact. </li></ul></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec71"/>Summary</h1></div></div></div><p>A large proportion of information in the digital world is textual. Text mining and NLP are areas concerned with extracting information from this unstructured form of data. Several important sub areas in the field are active topics of research today and an understanding of these areas is essential for data scientists.</p><p>Text categorization is concerned with classifying documents into pre-determined categories. Text may be enriched by annotating words, as with POS tagging, in order to give it more structure for subsequent processing tasks to act on. Unsupervised techniques such as clustering can be applied to documents as well. Information extraction and named entity recognition help identify information-rich specifics such as location, person or organization name, and so on. Summarization is another important application for producing concise abstracts of larger documents or sets of documents. Various ambiguities of language and semantics such as context, word sense, and reasoning make the tasks of NLP challenging. </p><p>Transformations of the contents of text include tokenization, stop words removal, and word stemming, all of which prepare the corpus by standardizing the content so Machine Learning techniques can be applied productively. Next, lexical, semantic, and syntactic features are extracted so numerical values can represent the document structure more conventionally with a vector space model. Similarity and distance measures can then be applied to effectively compare documents for sameness. Dimensionality reduction is key due to the large number of features that are typically present. The details of the techniques for topic modeling, PLSA and text clustering, and named entity recognition are described in this chapter. Finally, the recent techniques employing deep learning in various fields of NLP are introduced to the readers.</p><p>Mallet and KNIME are two open source Java-based tools that provide powerful NLP and Machine Learning capabilities. The case study examines performance of different classifiers on the Reuters corpus using KNIME.</p></div>
<div class="section" title="References"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec72"/>References </h1></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">J. B. Lovins (1968). <span class="emphasis"><em>Development of a stemming algorithm</em></span>, Mechanical Translation and Computer Linguistic, vol.11, no.1/2, pp. 22-31. </li><li class="listitem">Porter M.F, (1980). <span class="emphasis"><em>An algorithm for suffix stripping</em></span>, Program; 14, 130-137.</li><li class="listitem">ZIPF, H.P., (1949). <span class="emphasis"><em>Human Behaviour and the Principle of Least Effort</em></span>, Addison-Wesley, Cambridge, Massachusetts.</li><li class="listitem">LUHN, H.P., (1958). <span class="emphasis"><em>The automatic creation of literature abstracts</em></span>', IBM Journal of Research and Development, 2, 159-165.</li><li class="listitem">Deerwester, S., Dumais, S., Furnas, G., &amp; Landauer, T. (1990), <span class="emphasis"><em>Indexing by latent semantic analysis</em></span>, Journal of the American Society for Information Sciences, 41, 391–407. </li><li class="listitem">Dempster, A. P., Laird, N. M., &amp; Rubin, D. B. (1977), <span class="emphasis"><em>Maximum likelihood from incomplete data via the EM algorithm</em></span>. Journal of the Royal Statistic Society, Series B, 39(1), 1–38.</li><li class="listitem">Greiff, W. R. (1998). <span class="emphasis"><em>A theory of term weighting based on exploratory data analysis</em></span>. In 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, New York, NY. ACM. </li><li class="listitem">P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra, and J/ C. Lai (1992), <span class="emphasis"><em>Class-based n-gram models of natural language</em></span>, Computational Linguistics, 18, 4, 467-479.</li><li class="listitem">T. Liu, S. Lin, Z. Chen, W.-Y. Ma (2003), <span class="emphasis"><em>An Evaluation on Feature Selection for Text Clustering</em></span>, ICML Conference.</li><li class="listitem">Y. Yang, J. O. Pederson (1995). <span class="emphasis"><em>A comparative study on feature selection in text categorization</em></span>, ACM SIGIR Conference.</li><li class="listitem">Salton, G. &amp; Buckley, C. (1998). <span class="emphasis"><em>Term weighting approaches in automatic text retrieval</em></span>. Information Processing &amp; Management, 24(5), 513–523. </li><li class="listitem">Hofmann, T. (2001). <span class="emphasis"><em>Unsupervised learning by probabilistic latent semantic analysis</em></span>. Machine Learning Journal, 41(1), 177–196. </li><li class="listitem">D. Blei, J. Lafferty (2006). <span class="emphasis"><em>Dynamic topic models</em></span>. ICML Conference.</li><li class="listitem">D. Blei, A. Ng, M. Jordan (2003). <span class="emphasis"><em>Latent Dirichlet allocation</em></span>, Journal of Machine Learning Research, 3: pp. 993–1022.</li><li class="listitem">W. Xu, X. Liu, and Y. Gong (2003). <span class="emphasis"><em>Document-Clustering based on Non-negative Matrix Factorization</em></span>. Proceedings of SIGIR'03, Toronto, CA, pp. 267-273, </li><li class="listitem">Dud´ik M. and Schapire (2006). R. E. <span class="emphasis"><em>Maximum entropy distribution estimation with generalized regularization</em></span>. In Lugosi, G. and Simon, H. (Eds.), COLT, Berlin, pp. 123– 138, Springer-Verlag,.</li><li class="listitem">McCallum, A., Freitag, D., and Pereira, F. C. N. (2000). <span class="emphasis"><em>Maximum Entropy Markov Models for Information Extraction and Segmentation</em></span>. In ICML, pp. 591–598..</li><li class="listitem">Langville, A. N, Meyer, C. D., Albright, R. (2006). <span class="emphasis"><em>Initializations for the Nonnegative Factorization</em></span>. KDD, Philadelphia, USA</li><li class="listitem">Dunning, T. (1993). <span class="emphasis"><em>Accurate Methods for the Statistics of Surprise and Coincidence. Computational Linguistics</em></span>, 19, 1, pp. 61-74.</li><li class="listitem">Y. Bengio, R. Ducharme, P. Vincent and C. Jauvin (2003). <span class="emphasis"><em>A Neural Probabilistic Language Model</em></span>. Journal of Machine Learning Research.</li><li class="listitem">R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. (2011). <span class="emphasis"><em>Natural language processing (almost) from scratch</em></span>. Journal of Machine Learning Research, 12:2493–2537.</li><li class="listitem">T. Mikolov, K. Chen, G. Corrado and J. Dean (2013). <span class="emphasis"><em>Efficient Estimation of Word Representations in Vector Space</em></span>. arXiv:1301.3781v1.</li><li class="listitem">R. Socher, Christopher Manning, and Andrew Ng. (2010). <span class="emphasis"><em>Learning continuous phrase representations and syntactic parsing with recursive neural networks</em></span>. In NIPS 2010 Workshop on Deep Learning and Unsupervised Feature Learning.</li><li class="listitem">R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning. (2011). <span class="emphasis"><em>Semi-supervised recursive autoencoders for predicting sentiment distributions</em></span>. In EMNLP.</li><li class="listitem">M. Luong, R. Socher and C. Manning (2013). <span class="emphasis"><em>Better word representations with recursive neural networks for morphology</em></span>. CONLL.</li><li class="listitem">A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al (2013). <span class="emphasis"><em>Devise: A deep visual-semantic embedding model</em></span>. In NIPS Proceedings.</li><li class="listitem">Léon Bottou (2011). From Machine Learning to Machine Reasoning. <a class="ulink" href="https://arxiv.org/pdf/1102.1808v3.pdf">https://arxiv.org/pdf/1102.1808v3.pdf</a>.</li><li class="listitem">Cho, Kyunghyun, et al (2014). <span class="emphasis"><em>Learning phrase representations using rnn encoder-decoder for statistical machine translation</em></span>. arXiv preprint arXiv:1406.1078.</li></ol></div></div></body></html>