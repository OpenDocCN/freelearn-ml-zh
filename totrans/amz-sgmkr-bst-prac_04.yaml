- en: 'Chapter 3: Data Labeling with Amazon SageMaker Ground Truth'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the biggest barriers to ML projects in most companies is access to labeled
    training data. At one company we worked with, we were trying to identify consumer-impacting
    outages. The customer had a lot of data from each layer of their application stack,
    but they couldn't agree on how to define an outage. Is an outage when a load balancer
    is down? Probably not – we have redundancy in the infrastructure layer. Is an
    outage when a customer can't access the service for over 10 minutes? That's probably
    too granular; a single customer might have problems due to local network connectivity
    issues. So, what exactly do we mean by an outage? How can we automatically label
    our training data as *outage* or *not an outage*?
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll review labeling data using SageMaker Ground Truth. We'll
    cover common challenges associated with large datasets and potentially biased
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with labeling data at scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing unique labeling requirements with custom labeling workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using active learning to reduce labeling time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security and permissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will need an AWS account to run the examples included in this chapter. If
    you have not set up the data science environment yet, please refer to [*Chapter
    2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Data Science Environments*,
    which provides a walk-through of the setup process.
  prefs: []
  type: TYPE_NORMAL
- en: Code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter03](https://github.com/PacktPublishing/Amazon-SageMaker-Best-Practices/tree/main/Chapter03).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter is in the `CH03` folder of the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with labeling data at scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Besides the conceptual challenges with agreeing on how to label data, we need
    to consider the logistics. **SageMaker Ground Truth** lets you assign data labeling
    jobs to a human workforce. But you may face additional challenges such as the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unique labeling logic**: If our labeling case requires a custom workflow,
    we need to model that in Ground Truth.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Annotation quality**: The labels applied by workers may be subject to implicit
    bias that affects the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost and time**: Labeling data requires people for a period of time. If you
    have a very large dataset, you''ll consume a lot of person-hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Given that your data may be sensitive, you need to make sure
    that access to the data is restricted to an authorized workforce.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional information
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you need an introduction to Ground Truth, please review [*Chapter 2*](B17249_02_Final_JM_ePub.xhtml#_idTextAnchor039)
    of *Learn Amazon SageMaker,* written by Julien Simon.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To put these concerns into focus, let's consider our weather data introduced
    in the previous chapter. Ground Truth doesn't have a built-in workflow that lets
    us prompt workers to label weather data according to our logic for describing
    air as *good* or *bad*. The dataset for the entire time span is approximately
    499 GB; labeling each entry by hand as *good* or *bad* weather quality will take
    some time. Finally, our workers may have their own implicit or unconscious bias.
  prefs: []
  type: TYPE_NORMAL
- en: A worker who grew up in a city with severe smog may have a much different perception
    of air quality than someone who grew up in a rural area with very clean air. In
    the following sections, we'll discuss how to address these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing unique labeling requirements with custom labeling workflows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's get started with a labeling job for our weather data. We want to label
    each weather report as *good* or *bad*. In order to help our workers do that,
    we'll make a nice frontend that shows the location of the weather station on a
    map and displays the reading from the weather station. We need a custom workflow
    because this scenario doesn't fall neatly into any of the existing Ground Truth
    templates.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will have to set up the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A private workforce backed by a Cognito user pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A manifest file that lists the items we want to label
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A custom Ground Truth labeling workflow, consisting of two Lambda functions
    and a UI template
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The notebook `LabelData.ipynb` in the `CH02` folder of our repository walks
    through these steps.
  prefs: []
  type: TYPE_NORMAL
- en: A private labeling workforce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although you can use a public workforce, most companies will want to use a private
    workforce to label their own data. Setting up a private workforce starts by defining
    a Cognito user pool, which, for real use cases, could link to another identity
    provider such as Active Directory.
  prefs: []
  type: TYPE_NORMAL
- en: We'll create a user group in Cognito; you could use groups to create teams for
    different types of labeling jobs. Finally, we'll define a SageMaker work team
    linked to the Cognito user group. Note that SageMaker creates a labeling domain
    that we have to set as the callback URL in the Cognito user pool client.
  prefs: []
  type: TYPE_NORMAL
- en: Once the work team is set up, the notebook will add an example worker.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Create a private workforce** part of the notebook executes all of these
    steps for you:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Cognito user pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Cognito client for the user pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an identity pool for the client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a user group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assigning a domain to the user pool
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a SageMaker work team that uses the Cognito user pool and group
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a sample user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once you execute the **Create a private workforce** part of the notebook, you
    should see a private workforce defined, along with the login URL that the workers
    would use. If you scroll further down this part of the console, you''ll also see
    information about the work team and any workers assigned to the team, as shown
    in *Figure 3.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Labeling workforce shown in the SageMaker console'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Labeling workforce shown in the SageMaker console
  prefs: []
  type: TYPE_NORMAL
- en: Listing the data to label
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to create a manifest file that tells Ground Truth how to find the data
    we want to label. In the manifest, we can list references to files in S3 or we
    can provide text data directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall that our source data is in JSON format. Each source file contains multiple
    entries that look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We cannot pass in links to individual files, as each file contains multiple
    records to label. Rather, we will summarize each record directly in the manifest
    file. Each line in the manifest will contain the air quality metric and location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `Create a manifest file` notebook section will write out a manifest for
    a set of records. Since you are the only worker you have, we limit the number
    of records to 20 by default (more on this in the next section).
  prefs: []
  type: TYPE_NORMAL
- en: Creating the workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to create a custom workflow, we need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A *Lambda function that can take one entry from the manifest and inject variables
    into the UI*. In this case, we will simply map the items in the manifest text
    entries into a metric label to display along with a geolocation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A *UI template that displays the data sensibly for a worker*. In this case,
    we have a simple UI template that presents the metric along with a map showing
    the location where the metric was collected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the purposes of this book, we are using map tiles from OpenStreetMap. Do
    not use these tiles for production use cases. Instead, use a commercial provider
    such as Google Maps or Here.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A *Lambda function that consolidates annotations from multiple workers*. We
    simply do a pass-through here since we only have one worker in our sample workforce.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The notebook section `Create a custom workflow` walks you through these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining IAM roles for the workflow and the Lambda function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uploading the user interface template and the Lambda processing code to S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating the pre- and post-processing Lambda functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the labeling job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the labeling job is created, you can log in to the labeling portal URL
    (see *Figure 1.1*), using the username and password you specified in the notebook.
    Once you open the job, you''ll see a UI like *Figure 3.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Labeling UI showing the location of a weather station. The locations
    are shown in the local language](img/B17249_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Labeling UI showing the location of a weather station. The locations
    are shown in the local language
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll see a map showing the location of the measurement and the actual measurement.
    You can pick *good* or *bad* to specify whether you think the measurement represents
    a good or bad air quality day. After you have labeled all of the metrics, your
    job will show as complete, and you''ll see the label for each data point, as shown
    in *Figure 3.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Completed labeling job'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Completed labeling job
  prefs: []
  type: TYPE_NORMAL
- en: We'll describe how to use the labeling output in the next chapter. You'll see
    examples of the labeling output in the notebook that goes with this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Improving labeling quality using multiple workers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Relying on a single opinion for a subjective evaluation is risky. In some cases,
    labeling seems straightforward; telling a car from an airplane when labeling transportation
    pictures is pretty simple. But let's go back to our weather data. If we're labeling
    air quality as good or bad based on a measurement that's not intuitive, such as
    the level of particulate matter (PM25), we may find that a worker's opinion depends
    greatly on the advice we give them and their preconceptions. If a worker believes
    that a certain city or country has *dirty air*, they are likely to favor a *bad*
    label in ambiguous cases. And these biases have real consequences – some governments
    are very sensitive to the idea that their air quality is bad!
  prefs: []
  type: TYPE_NORMAL
- en: One way to combat this problem is to use multiple workers to label each item
    and somehow combine the scores. In the notebook section called `Add another worker`,
    we'll add a second worker to our private workforce. Then in the `Launch labeling
    job for multiple workers` section, we'll create a new labeling job. Once the new
    job is ready, log in as both workers and label the small set of data we've selected.
  prefs: []
  type: TYPE_NORMAL
- en: What happens now? We'll need to adjust our post-processing Lambda to consolidate
    the annotations. We could use a variety of strategies for the consolidation. For
    example, we could use a majority voting scheme, with ties being assigned to a
    *mixed* category. In this chapter, we'll simply use the latest annotation as the
    winner since we only have two workers.
  prefs: []
  type: TYPE_NORMAL
- en: Using active learning to reduce labeling time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve set up a labeling workflow, we need to think about scale. If
    our dataset has more than 5,000 records, it''s likely that Ground Truth can learn
    how to label for us. (You need at least 1,250 labeled records for automatic labeling,
    but at least 5,000 is a good rule of thumb.) This happens in an iterative process,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Auto-labeling workflow'
  prefs: []
  type: TYPE_NORMAL
- en: ytt
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Auto-labeling workflow
  prefs: []
  type: TYPE_NORMAL
- en: When you create a labeling job using automatic labeling, Ground Truth will select
    a random sample of input data for manual labeling. If at least 90% of these items
    are labeled without error, Ground Truth will split the labeled data into a training
    and validation set. It will train a model and compute a confidence score, then
    attempt to label the remaining data. If the automatically generated labels are
    beneath the confidence threshold, it will refer them to workers for human review.
    This process repeats until the entire dataset is labeled. While this process is
    difficult to simulate, it provides an iterative method to improve automatic labeling
    with human input.
  prefs: []
  type: TYPE_NORMAL
- en: As a concluding note to this section, you may wonder what the difference is
    between a model that can automatically label data and a more general-purpose ML
    model. There's a fine line here. Keep in mind that the data we use for Ground
    Truth may not be completely representative of the data we see in production. Our
    goal for a generic ML model is a model that can produce accurate inferences without
    any human input.
  prefs: []
  type: TYPE_NORMAL
- en: Security and permissions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While some data is not sensitive, most companies would not want to expose their
    data to the public during the labeling process. In this section, we'll cover data
    access control, encryption, and workforce management for data labeling.
  prefs: []
  type: TYPE_NORMAL
- en: You should follow the principle of least-privileged access when using Ground
    Truth (or any other cloud service). Restrict the users who are allowed to create
    labeling jobs, and restrict users allowed to create labeling jobs using non-private
    workforces. In a custom labeling job, explicitly provide invoke permissions to
    your Lambda functions. Restrict labeling job access to only the appropriate S3
    buckets and prefixes.
  prefs: []
  type: TYPE_NORMAL
- en: When you run a labeling job, Ground Truth will always encrypt the output in
    S3\. You can use the S3-managed key or provide your own KMS key. For non-sensitive
    data, the default S3 managed key is adequate. If you have sensitive data, consider
    using separate KMS keys for different datasets, as that provides another layer
    of security. You can also use a KMS key to encrypt the storage volumes on instances
    used for automatic labeling.
  prefs: []
  type: TYPE_NORMAL
- en: When managing your workforce, you should restrict access to a known-good IP
    address range (CIDR block). You should also use the worker tracking features to
    log which workers are accessing data. When using Cognito for authentication, make
    use of strong password policies and multi-factor authentication. In most cases,
    large companies will prefer to use their own identity provider for workforce management.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, note that you'll need to add **CORS** (**cross-origin resource sharing**)
    configuration to your S3 buckets involved in labeling jobs, as described in the
    documentation ([https://docs.aws.amazon.com/sagemaker/latest/dg/sms-cors-update.html](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-cors-update.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we head toward the summary, do have a look at the following table as
    it summarizes some of the best practices for data labeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Summary of data labeling best practices'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B17249_03_Table_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Summary of data labeling best practices
  prefs: []
  type: TYPE_NORMAL
- en: With this, we now come to the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started digging into our weather dataset, focusing on the
    problem of data labeling. We learned how to use SageMaker Ground Truth to label
    large datasets using a combination of human review and automation, how to use
    custom workflows to aid the labeling process, and how to fight labeling bias by
    using multiple opinions. We ended with some advice on making sure that the labeling
    process is secure.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll explore data preparation. We'll run a feature engineering
    processing job on the full dataset.
  prefs: []
  type: TYPE_NORMAL
