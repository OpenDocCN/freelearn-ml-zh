- en: Chapter 4. Web Mining Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Web data mining techniques are used to explore the data available online and
    then extract the relevant information from the Internet. Searching on the web
    is a complex process that requires different algorithms, and they will be the
    main focus of this chapter. Given a search query, the relevant pages are obtained
    using the data available on each web page, which is usually divided in the page
    content and the page hyperlinks to other pages. Usually, a search engine has multiple
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: A web crawler or spider for collecting web pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A parser that extracts content and preprocesses web pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An indexer to organize the web pages in data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A retrieval information system to score the most important documents related
    to a query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A ranking algorithm to order the web pages in a meaningful manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These parts can be divided into web structure mining techniques and web content
    mining techniques.
  prefs: []
  type: TYPE_NORMAL
- en: The web crawler, indexer, and ranking procedures refer to the web structure
    (the network of hyperlinks). The other parts (parser and retrieval system) of
    a search engine are web content analysis methods because the text information
    on web pages is used to perform such operations.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the content of a collection of web pages can be further analyzed
    using some natural language processing techniques, such as **latent Dirichlet
    allocation** opinion mining or sentiment analysis tools. These techniques are
    especially important for extracting subjective information about web users, and
    so they are widely found in many commercial applications, from marketing to consultancy.
    These sentiment analysis techniques will be discussed at the end of the chapter.
    Now we will start discussing the web structure mining category.
  prefs: []
  type: TYPE_NORMAL
- en: Web structure mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This field of web mining focuses on the discovery of the relationships among
    web pages and how to use this link structure to find the relevance of web pages.
    For the first task, usually a spider is employed, and the links and the collected
    web pages are stored in a indexer. For the the last task, the web page ranking
    evaluates the importance of the web pages.
  prefs: []
  type: TYPE_NORMAL
- en: Web crawlers (or spiders)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A spider starts from a set of URLs (seed pages) and then extracts the URL inside
    them to fetch more pages. New links are then extracted from the new pages and
    the process continues until some criteria are matched. The unvisited URLs are
    stored in a list called **frontier** , and depending on how the list is used,
    we can have different crawler algorithms, such as breadth-first and preferential
    spiders. In the **breadth-first** algorithm, the next URL to crawl comes from
    the head of the frontier while the new URLs are appended to the frontier tail.
    Preferential spider instead employs a certain importance estimate on the list
    of unvisited URLs to determine which page to crawl first. Note that the extraction
    of links from a page is performed using a parser, and this operation is discussed
    in more detail in the related paragraph of the web content mining section.
  prefs: []
  type: TYPE_NORMAL
- en: A web crawler is essentially a graph search algorithm in which the structure
    of the neighborhood of the starting pages is retrieved, following certain criteria
    such as the maximum number of links to follow (depth of the graph), maximum number
    of pages to crawl, or time limit. A spider can then extract a portion of the Web
    that has interesting structures, such as hubs and authorities. A hub is a web
    page that contains a large number of links, while an authority is defined as,
    a page, with a large number of times that its URL occurs on other web pages (it
    is a measure of the page's popularity). A popular Python implementation of the
    crawler is given by the Scrapy library, which also employs concurrency methods
    (asynchronous programming using Twisted) to speed up operations. A tutorial on
    this module is given in [Chapter 7](text00050.html#page "Chapter 7. Movie Recommendation
    System Web Application") , *Movie Recommendation System Web Application* when
    the crawler will be used to extract information about movie reviews.
  prefs: []
  type: TYPE_NORMAL
- en: Indexer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An indexer is a way to store web pages found by the crawler in a structured
    database to allow subsequent fast retrieval on a given search query. The simplest
    indexing approach is to directly store all the pages and, at query time, just
    scan for all the documents that contain the keywords in the query. However, this
    method is not feasible if the number of pages is large (which in practice, it
    is) due to high computational costs. The most common method to speed up the retrieval
    is called **inverted index scheme** , which is used by the most popular search
    engines.
  prefs: []
  type: TYPE_NORMAL
- en: Given a set of web pages *p[1] , …, p[k]* and a vocabulary *V* containing all
    the words ![Indexer](img/Image00368.jpg) in the pages, the inverted index database
    is obtained by storing lists such as ![Indexer](img/Image00369.jpg) , …, ![Indexer](img/Image00370.jpg)
    ,
  prefs: []
  type: TYPE_NORMAL
- en: Here, ![Indexer](img/Image00371.jpg) is the ID of the web page *j* . Extra information
    can be stored for each word, for example, the frequency count of the word or its
    position on each page. The implementation of the indexer is beyond the scope of
    this book, but the general concepts have been described in this paragraph for
    completeness.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a search query with a list of words will retrieve all the inverted
    lists related to each word and then merge the lists. The order of the final lists
    will be chosen using the ranking algorithm together with an information retrieval
    system to measure the relevance of the documents to the query.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking – PageRank algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A ranking algorithm is important because the usual number of web pages that
    a single information retrieval query can return may be huge, so there is a problem
    of how to choose the most relevant pages. Furthermore, the information retrieval
    model can easily be spammed by just inserting many keywords into the page to make
    the page relevant to a large number of queries. So, the problem to evaluate the
    importance (that is, ranking score) of a web page on the Internet has been addressed
    considering the fact that the web has a graph in which the hyperlinks—links from
    a page to another—are the primary source of information to estimate the relevance
    of web pages. The hyperlinks can be divided as:'
  prefs: []
  type: TYPE_NORMAL
- en: 'in-links of page *i* : hyperlinks that point to page *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'out-links of page *i* : hyperlinks that point to other pages from page *i*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Intuitively, the more in-links a web page has, the more important the page
    should be. The study of this hyperlink structure is part of social network analysis,
    and many algorithms have been used and proposed. But for historical reasons, we
    will explain the most well known algorithm, called **PageRank** , which was presented
    by Sergey Brin and Larry Page (the founders of Google) in 1998\. The whole idea
    is to calculate the prestige of a page as the sum of the prestiges of the pages
    that point to it. If the prestige of a page *j* is *P(j)* it is equally distributed
    to all the pages *N[j]* that it points to so that each out-link receives a portion
    of prestige equal to *P(j)|N[j]* . Formally, the prestige or page rank score of
    a page *i* can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ranking – PageRank algorithm](img/Image00372.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![Ranking – PageRank algorithm](img/Image00373.jpg) if page *j* points
    to page *i* ; otherwise it is equal to *0* . *A[ij] * is called adjacency matrix
    and it represents the portion of prestige that propagates from node *j* to node
    *i* . Considering *N* total nodes in the graph, the preceding equation can be
    rewritten in matrix form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ranking – PageRank algorithm](img/Image00374.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that this equation is equivalent to an eigensystem with eigenvalue ![Ranking
    – PageRank algorithm](img/Image00375.jpg) if the adjacency matrix satisfies certain
    conditions. Another way to interpret the preceding equation is to use the Markov
    chain terminology—the entry *A[ij] * becomes the transition probability from node
    *j* to node *i* and the prestige of node *i* , *p(i)* , is the probability to
    visit node *i* . In this scenario, it may happen that two nodes (or more) point
    to each other but do not point to other pages. Once one of these two nodes has
    been visited, a loop will occur and the user will be trapped in it. This situation
    is called **rank sink** , (the matrix *A* is called **periodic** ) and the solution
    is to add a transition matrix term that allows jumping from each page to another
    page at random without following the Markov chain described by *A* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ranking – PageRank algorithm](img/Image00376.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *E=ee^T* is a matrix of one entry of dimensions *N´N* ( *e* is a unit
    vector), and *d* (also called the **damping factor** ) is the probability to follow
    the transition given by the transition matrix *A* . *(1-d)* is the probability
    to visit a page randomly. In this final form, all the nodes are linked to each
    other so that even if the adjacency matrix has a row with many *0* entries for
    a particular node *s* , *A[sj]* , there is always a small probability equal to
    ![Ranking – PageRank algorithm](img/Image00377.jpg) that *s* is visited from all
    the *N* nodes in the graph. Note that *A* has to be stochastic, which means each
    row has to sum to *1* ; ![Ranking – PageRank algorithm](img/Image00378.jpg) (at
    least one entry per row different from *0* or at least one out-link per page).
    The equation can be simplified by normalizing the *P* vector as *e^T P=N* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Ranking – PageRank algorithm](img/Image00379.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This can be solved using the power iteration method. This algorithm will be
    used in [Chapter 8](text00059.html#ch08 "Chapter 8. Sentiment Analyser Application
    for Movie Reviews") , *Sentiment Analyser Application on Movie Reviews* to implement
    an example of a movie review sentiment analysis system. The main advantages of
    this algorithm is that it does not depend on the query (so the PageRank scores
    can be computed offline and retrieved at query time), and it is very robust to
    spamming since it is not feasible for a spammer to insert in-links to their page
    on influential pages.
  prefs: []
  type: TYPE_NORMAL
- en: Web content mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This type of mining focuses on extracting information from the content of web
    pages. Each page is usually gathered and organized (using a parsing technique),
    processed to remove the unimportant parts from the text (natural language processing),
    and then analyzed using an information retrieval system to match the relevant
    documents to a given query. These three components are discussed in the following
    paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A web page is written in HTML format, so the first operation is to extract
    the relevant pieces of information. An HTML parser builds a tree of tags from
    which the content can be extracted. Nowadays, there are many parsers available,
    but as an example, we use the Scrapy library see [Chapter 7](text00050.html#page
    "Chapter 7. Movie Recommendation System Web Application") , *Movie Recommendation
    System Web Application* which provides a command-line parser. Let''s say we want
    to parse the main page of Wikipedia, [https://en.wikipedia.org/wiki/Main_Page](https://en.wikipedia.org/wiki/Main_Page)
    . We simply type this in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'A prompt will be ready to parse the page using the `response` object and the
    `xpath` language. For example we want to obtain the title''s page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we want to extract all the embedded links in page (this operation is needed
    for the crawler to work), which are usually put on `<a>` , and the URL value is
    on an `href` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that a more robust way to parse content can be used since the web pages
    are usually written by non-programmers, so the HTML may contain syntax errors
    that browsers typically repair. Note also that web pages may contain a large amount
    of data due to advertisements, making the parsing of relevant information complicated.
    Different algorithms have been proposed (for instance, tree matching) to identify
    the main content of a page but no Python libraries are available at the moment,
    so we have decided not to discuss this topic further. However, note that a nice
    parsing implementation for extracting the body of a web article can be found in
    the newspaper library and it will also be used in [Chapter 7](text00050.html#page
    "Chapter 7. Movie Recommendation System Web Application") , *Movie Recommendation
    System Web Application* .
  prefs: []
  type: TYPE_NORMAL
- en: Natural language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the text content of a web page has been extracted, the text data is usually
    preprocessed to remove parts that do not bring any relevant information. A text
    is tokenized, that is, transformed into a list of words (tokens), and all the
    punctuation marks are removed. Another usual operation is to remove all the *stopwords*
    , that is, all the words used to construct the syntax of a sentence but not containing
    text information (such as conjunctions, articles, and prepositions) such as *a*
    , *about* , *an* , *are* , *as* , *at* , *be* , *by* , *for* , *from* , *how*
    , *in* , *is* , *of* , *on* , *or* , *that* , *the* , *these* , *this* , *to*
    , *was* , *what* , *when* , *where* , *who* , *will* , *with* , and many others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many words in English (or any language) share the same root but have different
    suffixes or prefixes. For example, the words *think* , *thinking* , and *thinker*
    all share the same root— *think* indicating that the meaning is the same—but the
    role in a sentence is different (verb, noun, and so on). The procedure to reduce
    all the words in a set to its roots it is called **stemming** , and many algorithms
    have been invented to do so (Porter, Snowball, and Lancaster). All of these techniques
    are parts of a broader range of algorithms called **natural language processing**
    , and they are implemented in Python on the `nltk` library (installed as usual
    through `sudo pip install nltk` ). As an example, the following code preprocesses
    a sample text using the techniques described previously (using the Python interface
    terminal):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Natural language processing](img/Image00380.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that the `stopwords` list has been downloaded using the `nltk dowloader
    nltk.download('stopwords')` .
  prefs: []
  type: TYPE_NORMAL
- en: Information retrieval models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The information retrieval methods are needed to find the most relevant documents
    to a given query. The words contained in the web pages can be modeled using different
    approaches such as Boolean models, vector space models, and probabilistic models,
    and in this book, we have decided to discuss the vector space models and how to
    implement them. Formally, given a vocabulary of *V* words, each web page *d[i]*
    (or document) in a collection of *N* pages, can be thought of as a vector of words,
    ![Information retrieval models](img/Image00381.jpg) , where each word *j* belonging
    to the document *i* is represented by *w[ij] * , which can be either a number
    (weight) or a vector depending on the chosen algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Term frequency-inverse document frequency (TF-IDF), *w[ij]* , is a real number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latent Semantic Analysis** ( **LSA** ), *w[ij]* , is a real number (representation
    independent of the document *i* )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doc2Vec (or word2vec), *w[ij]* , is a vector of real numbers (representation
    independent of the document *i* )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since the query can also be represented by a vector of words, ![Information
    retrieval models](img/Image00382.jpg) , the web pages most similar to the vector
    *q* are found by calculating a similarity measure between the query vector and
    each document. The most used similarity measure is called cosine similarity, for
    any document *i* given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Information retrieval models](img/Image00383.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note that there are other measures used in literature (okapi and pivoted normalization
    weighting), but for the purpose of this book, they are not necessary.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections will give some details about the three methods before
    being applied in a text case in the final paragraph of the section.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This method calculates *w[ij]* , taking into account the fact that a word that
    appears many times and in a large number of pages is likely to be less important
    than a word that occurs many times but only in a subset of documents. It is given
    by the multiplication of two factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![TF-IDF](img/Image00384.jpg) where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![TF-IDF](img/Image00385.jpg) is the normalized frequency of the word *j* in
    the document *I*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![TF-IDF](img/Image00386.jpg) is the inverse document frequency and *df[j]
    * is the number of web pages that contain the word *j*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent Semantic Analysis (LSA)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The name of this algorithm comes from the idea that there is a latent space
    in which each word (and each document) can be efficiently described, assuming
    that words with similar meanings also occur in similar text positions. Projection
    on this subspace is performed using the (truncated) SVD method already discussed
    in [Chapter 2](text00020.html#ch02 "Chapter 2. Unsupervised Machine Learning")
    , *Machine Learning Techniques – Unsupervised Learning* . We contextualize the
    method for LSA as follows: the web pages are collected together in matrix *X (V
    ´N* ), in which each column is a document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Latent Semantic Analysis (LSA)](img/Image00387.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *U[t]* ( *V ´d* ) is the matrix of the words projected in the new latent
    space with *d* dimensions, ![Latent Semantic Analysis (LSA)](img/Image00388.jpg)
    ( *d* ´ *N* ) is the transpose matrix of the documents transformed into the subspace,
    and ![Latent Semantic Analysis (LSA)](img/Image00389.jpg) ( *d* ´ *d* ) is the
    diagonal matrix with singular values. The query vector itself is projected into
    the latent space by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Latent Semantic Analysis (LSA)](img/Image00390.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now, each document represented by each row of *V[t]* can be compared with *q[t]*
    using the cosine similarity. Note that the true mathematical representation of
    the documents on the latent space is given by ![Latent Semantic Analysis (LSA)](img/Image00391.jpg)
    (not *V[t] * ) because the singular values are the scaling factors of the space
    axis components and they must be taken into account. Therefore, this matrix should
    be compared with ![Latent Semantic Analysis (LSA)](img/Image00392.jpg) . Nevertheless,
    it usually computes the similarity between *V[t] * and *q[t] * , and in practice,
    it is still unknown which method returns the best results.
  prefs: []
  type: TYPE_NORMAL
- en: Doc2Vec (word2vec)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This method represents each word *j* , *w[j]* , as a vector ![Doc2Vec (word2vec)](img/Image00393.jpg)
    but independent of the document *d[i] * it occurs in. Doc2Vec is an extension
    of the word2vec algorithm originally proposed by Mikolov and others, and it employs
    neuron networks and backpropagation to generate the word (and document) vectors.
    Due to the increasing importance of neuron networks (especially deep learning)
    in many machine learning applications, we decided to include the main concepts
    and formulas of this quite advanced method here to give you an introduction to
    a subject that will become extremely important in the future of machine learning
    in various fields. The following description is based on the paper Rong (2014)
    and Le and Mikolov (2014), and the notation also reflects the name currently used
    in literature.
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec – continuous bag of words and skip-gram architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Each word *j* in the vocabulary *V* is represented by a vector of length *|V|*
    , with binary entries *x[j] =(x[1j] , …, x[Vj] )* , where only *x[jj] =1* ; otherwise,
    *0* . The word2vec method trains a single (hidden) layer of *N* neurons (weights),
    choosing between two different network architectures (shown in the following figure).
    Note that both architectures have only one layer of *N* neurons or weights, *h*
    . This means that the method has to be considered *shallow* learning and not deep,
    which typically refers to networks with many hidden layers. The **Continuous Bag
    Of Words** ( **CBOW** ) method (displayed to the right in the following figure)
    trains the model using a set of *C* words as an input called *context* trying
    to predict the word (target) that occurs adjacent to the input text. The reverse
    approach is called **Skip-gram** , in which the input is the target word and the
    network is trained to predict the context set (displayed to the left in the following
    figure ). Note that *C* is called the window parameter and it sets how far from
    the target word the context words are selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Word2vec – continuous bag of words and skip-gram architectures](img/Image00394.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Skip-gram (left) and CBOW (right) architectures of the word2vec algorithm; figures
    taken from word2vec Parameter Learning Explained by X Rong (2015)
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, the matrix *W* transforms the input vectors into the hidden layer
    and *W'* transforms from the hidden layer to the output layer *y* , where the
    target (or context) is evaluated. In the training phase, the error from the true
    target (or context) is computed and used to calculate a stochastic gradient descent
    to update both the matrices *W* and *W'* . We will give a more mathematical description
    of the CBOW method in the following section. Note that the Skip-gram equations
    are similar and we will refer to the Rong (2015) paper for further details.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical description of the CBOW model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Starting from the input layer, the hidden layer *h* can be obtained by computing,
    ![Mathematical description of the CBOW model](img/Image00395.jpg) , where ![Mathematical
    description of the CBOW model](img/Image00396.jpg) is a vector of length *N* that
    represents the word *w[i] * on the hidden layer and *w[C] * is the average of
    the *C* context vectors ![Mathematical description of the CBOW model](img/Image00396.jpg)
    . Choosing a target word *w[j] * , the score at the output layer *u[j] * is obtained
    by multiplying the vector ![Mathematical description of the CBOW model](img/Image00397.jpg)
    (the j-th column of *W''* ) by *h* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mathematical description of the CBOW model](img/Image00398.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is not the final value on the output layer *y[j]* because we want to evaluate
    the posterior conditional probability to have the target word *w[j]* given the
    context *C* expressed by the **softmax** formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mathematical description of the CBOW model](img/Image00399.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now the training objective is to maximize this probability for all the words
    in the vocabulary, which is equivalent to ![Mathematical description of the CBOW
    model](img/Image00400.jpg) , where ![Mathematical description of the CBOW model](img/Image00401.jpg)
    and the index *j^M * represents the vector of *W'* in which the product is maximum,
    that is, the most probable target word.
  prefs: []
  type: TYPE_NORMAL
- en: 'The stochastic gradient descent equations are then obtained by calculating
    the derivatives of *E* with respect to the entries of *W (w[ij]* ) and *W'' (w''[ij'']*
    ). The final equations for each output target word *w[j]* are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Mathematical description of the CBOW model](img/Image00402.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '![Mathematical description of the CBOW model](img/Image00403.jpg) where ![Mathematical
    description of the CBOW model](img/Image00404.jpg) and *a* is the learning rate
    of the gradient descent. The derivative ![Mathematical description of the CBOW
    model](img/Image00405.jpg) represents the error of the network with respect to
    the true target word so that the error is back propagated on the system, which
    can learn iteratively. Note that the vectors ![Mathematical description of the
    CBOW model](img/Image00406.jpg) are the usual word representations used to perform
    the semantic operations.'
  prefs: []
  type: TYPE_NORMAL
- en: Further details can be found in the Rong (2015) paper.
  prefs: []
  type: TYPE_NORMAL
- en: Doc2Vec extension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As explained in Le and Mikolov (2014), Doc2Vec is a natural extension of the
    word2vec method in which a document is considered as an additional word vector.
    So in the case of the CBOW architecture, the hidden layer vector *h* is just the
    average of the context vectors and the document vector *d[i]* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doc2Vec extension](img/Image00407.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This architecture is shown in the following figure, and it is called the **distributed
    memory model** ( **DM** ) because the document *d[i]* vector just remembers the
    information of the document not represented by the context words. The vector ![Doc2Vec
    extension](img/Image00408.jpg) is shared with all the context words sampled from
    the document *d[i] * but the matrix *W* (and *W''* ) is the same for all the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Doc2Vec extension](img/Image00409.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A distributed memory model example with a context of three words (window=3);
    figure taken from Distributed Representations of Sentences and Documents by Le
    and Mikolov (2014)
  prefs: []
  type: TYPE_NORMAL
- en: The other proposed architecture is called **distributed bag of words** ( **DBOW**
    ), which considers only a document vector in the input layer and a set of context
    words sampled from the document in the output layer. It has been shown that the
    DM architecture performs better than DBOW, and it is therefore the default model
    in the `gensim` library implementation. The reader is advised to read the paper
    of Le and Mikolov (2014) for further details.
  prefs: []
  type: TYPE_NORMAL
- en: Movie review query example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To show in action the three information retrieval methods discussed previously,
    we use the IMBD movie reviews in the *polarity dataset v2.0* and *Pool of 27886
    unprocessed html files* at [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)
    , provided by Bo Pang and Lillian Lee (the dataset and the code are also stored
    in the GitHub account of the author at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/)
    . Download and unzip the `movie.zip` file from the website (called `polarity_html.zip`
    ), which creates the `movie` folder with all the web page movie reviews (about
    2000 files). First of all, we need to prepare the data from the files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00410.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This time we use **BeautifulSoup** to parse the title of the movie from each
    HTML web page and create a dictionary, `moviedict` . The `polarity dataset v2.0.tar.gz`
    contains a folder, `review_polarity` , which is inside the `txt_sentoken/` folder
    that split the positive and negative reviews into two separate subfolders (pros
    and cons). These files are preprocessed using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00411.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now all the 2,000 reviews are stored in the `tot_textreviews` list and the
    corresponding titles in `tot_titles` . The TF-IDF model can be trained using `sklearn`
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00412.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'After the `PreprocessTfidf` function, apply all the preprocessing techniques
    (removing stop words, tokenizing, and stemming) to each document. In the same
    way, we can train the LSA model using the `gensim` library, specifying 10 latent
    dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00413.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the `GenSimCorpus` function just preprocesses the documents with
    the usual techniques and transforms them into a format that the gensim LSA implementation
    can read. From the `lsi` object, it is possible to obtain the matrices *U* , *V*
    , and S that are needed to transform the query into the latent space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00414.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Also the indexed dictionary of words, `dict_words` , has been calculated to
    transform a query word into the corresponding index word in `dict_corpus` .
  prefs: []
  type: TYPE_NORMAL
- en: 'The last model to train is Doc2Vec. First, we prepare the data in a format
    that the gensim Doc2Vec implementation can handle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00415.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Each review has been placed in a `namedtuple` object, which contains the words
    preprocessed by the `PreprocessDoc2Vec` function (stopwords removed and tokenization
    performed) and the tag that is the name of the file. Note that we chose not to
    apply a stemmer because the results are generally better without it (the reader
    can test the results by applying the stemmer, setting the Boolean flag `doc2vecstem`
    to `True` ). The Doc2Vec training is finally performed by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00416.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We set the DM architecture (`dm` =1), the hidden layer with 500 dimensions (`size`
    ), a window size of 10 words, and all the words that occur at least once have
    been taken into account by the model (`min_count` =1).The other parameters are
    related to the efficiency optimization method (`negative` for negative sampling
    and `hs` for hierarchical softmax). The training lasted for `20` epochs, with
    a learning rate equal to `0.99` .
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now verify which results each method returns, defining a query to retrieve
    all the web documents related to sci-fi movies, that is, movies usually described
    by this list of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00417.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The TF-IDF method returns the five most similar web pages using the following
    script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00418.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the model uses a sparse matrix format to store data, so the `cosine_similarity`
    function converts the vectors into regular vectors. Then it computes the similarity.
    In a similar way, the query is converted in a *q[k]* in LSA terminology and the
    five most similar web pages are printed out:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00419.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the `doc2vec` model transforms the query list into a vector using
    the `infer_vector` function, and the most similar reviews are returned by the
    `most_similar` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Movie review query example](img/Image00420.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that the `random` parameter of the model needs to be set up to a fixed
    value to return deterministic results whenever an optimization approach is used
    (negative sampling or hierarchical softmax). The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TF-IDF** :![Movie review query example](img/Image00421.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LSA** :![Movie review query example](img/Image00422.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doc2vec:![Movie review query example](img/Image00423.jpg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All three methods show movies related to the query. Interestingly, TF-IDF performs
    better than the more advanced LSA and Doc2Vec algorithms because *In the Heat
    of the Night* , *Pokemon* , *Rocky Horror Picture Show* , and *Wild Things* are
    not related to the query compared with the TF-IDF results which show only one
    movie ( *No Telling* ) as unrelated. The movies *Charlie's Angels* and *Batman
    & Robin* are action movies, so they are mostly related to the single query word
    *action* . Doc2Vec returns the worst results mostly because the training dataset
    is too small to learn good vector representations (as an example, Google released
    a word2vec trained dataset based on billions of documents, or more). The website
    [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)
    provides a larger dataset, so the reader can try to train Doc2Vec with more data
    as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Postprocessing information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the web pages are collected from the Web, there are natural language processing
    algorithms that are able to extract relevant information for different commercial
    purposes apart from building a web search engine. We will discuss here algorithms
    that are able to extract the main topics on the collection of documents (latent
    Dirichlet analysis) and to extract the sentiment or opinion of each web page (opinion
    mining techniques).
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Latent Dirichlet allocation** ( **LDA** ) is a natural language processing
    algorithm that belongs to the generative model category. The technique is based
    on the observations of some variables that can be explained by other underlined
    unobserved variables, which are the reasons the observed data is similar or different.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider text documents in which words are the observations. Each
    document can be the result of a mixture of topics (unobserved variables) and each
    word refers to a specific topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, consider the two following documents describing two companies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**doc1** : Changing how people search for fashion items and, share and buy
    fashion via visual recognition, TRUELIFE is going to become the best approach
    to search the ultimate trends …'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**doc2** : Cinema4you enabling any venue to be a cinema is a new digital filmed
    media distribution company currently in the testing phase. It applies technologies
    used in Video on Demand and broadcasting to ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LDA is a way of automatically discovering latent topics that these documents
    contain. For example, given these documents and asked for two topics, LDA might
    return the following words associated with each topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '**topic 1** : people Video fashion media…'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**topic 2** : Cinema technologies recognition broadcasting…'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, the second topic can be labeled as *technology* while the first as
    *business* .
  prefs: []
  type: TYPE_NORMAL
- en: 'Documents are then represented as mixtures of topics that spit out words with
    certain probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '**doc1** : topic 1 42%,topic 2 64%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**doc2** : topic 1 21%, topic 2 79%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This representation of the documents can be useful in various applications such
    as clustering of pages in different groups, or to extract the main common subjects
    of a collection of pages. The mathematical model behind this algorithm is explained
    in the next paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Documents are represented as random mixtures over latent topics, where each
    topic is characterized by a distribution over words. LDA assumes the following
    process for a corpus consisting of *M* documents, *d=(d[1] , …, d[M] )* , with
    each *i* containing *N[i]* words. If *V* is the length of the vocabulary, a word
    of document *i* is represented by a vector *w[i]* of length *V* , where only an
    element *w[i] [v] =1* and the others are *0* :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model](img/Image00424.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The number of latent dimensions (topics) is *K* , and for each document, ![Model](img/Image00425.jpg)
    is the vector of topics associated with each word *w[i] * , where *z[i] * is a
    vector of *0's* of length *K* except for the element *j, z[i] ^j =1* , that represents
    the topic *w[i] * has been drawn from.
  prefs: []
  type: TYPE_NORMAL
- en: 'b indicates the *K* ´ *V* matrix, where *b[ij]* represents the probability
    that each word *j* in the vocabulary is drawn from topic *i* : ![Model](img/Image00426.jpg)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, each row *i* of b is the word''s distribution of topic *i* , while each
    column *j* is the topic''s distribution of word *j* . Using these definitions,
    the process is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: From a chosen distribution (usually Poisson), draw the length of each document
    *N[i]* .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each document *d[i]* , draw the topic distribution q *[i]* , as a Dirichlet
    distribution *Dir(a)* , where ![Model](img/Image00427.jpg) and *a* is a parameter
    vector of length *K* such that ![Model](img/Image00428.jpg) .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each document *d[i]* , for each word *n* , draw a topic from the multinomial
    ![Model](img/Image00429.jpg) .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each document *d[i]* , for each word *n* , and for each topic *z[n]* , draw
    a word *w[n]* from a multinomial given by the row *z[n]* of b, ![Model](img/Image00430.jpg)
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The objective of the algorithm is to maximize the posterior probability for
    each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model](img/Image00431.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Applying the conditional probability definition, the numerator becomes the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model](img/Image00432.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So, the probability that the document *i* is given by topic vector *z* and
    word probability matrix b can be expressed as a multiplication of the single word
    probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model](img/Image00433.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering that *z[n]* is a vector with only one component *j* different from
    *0* , *z^j [n] =1* , then ![Model](img/Image00434.jpg) . Substituting these expressions
    on (2):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model](img/Image00435.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The denominator of (1) is obtained simply by integration over q *[i]* and summation
    over *z* . The final values of the topic distribution *q[i]* and the words per
    topic distribution (rows of *b* ) are obtained by calculating this probability
    by approximated inference techniques; those are beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter *a* is called the concentration parameter, and it indicates how
    much the distribution is spread over the possible values. A concentration parameter
    of *1* (or *k* , the dimension of the Dirichlet distribution, by the definition
    used in topic modeling literature) results in all sets of probabilities being
    equally probable. Meanwhile, in the limit as the concentration parameter tends
    towards zero, only distributions with nearly the entire mass concentrated on one
    of their components are likely (the words are less shared among different topics
    and they concentrate on a few topics).
  prefs: []
  type: TYPE_NORMAL
- en: As an example, a 100,000-dimension categorical distribution has a vocabulary
    of 100,000 words even though a topic may be represented by a couple of hundred
    words. As a consequence, typical values for the concentration parameter are between
    0.01 and 0.001, or lower if the vocabulary's size is millions of words or higher.
  prefs: []
  type: TYPE_NORMAL
- en: According to L. Li and Y. Zhang's paper *An empirical study of text classification
    using Latent Dirichlet Allocation* , LDA can be used as an effective dimension
    reduction method for text modeling. However, even though the method has performed
    well in various applications, there are certain issues to consider. The initialization
    of the model is random, which means it can lead to different results in each run.
    Also, the choice of concentration parameters is important, but there is no standard
    method to choose them.
  prefs: []
  type: TYPE_NORMAL
- en: Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Consider again the movie reviews'' web pages, `textreviews` , already preprocessed
    in the *Movie review query example* section, and LDA is applied to test whether
    it is possible to gather reviews on different topics. As usual, the following
    code is available in `postprocessing.ipynb` at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/)
    :'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example](img/Image00436.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As usual we have transformed each document in tokens (a different tokenizer
    has been used) and the stop words have been removed. To achieve better results,
    we filter out the most frequent words (such as `movie` and `film` ) that do not
    add any information to the pages. We ignore all the words with more than 1,000
    occurrences or observed less than three times:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example](img/Image00437.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can train the LDA model with 10 topics (`passes` is the number of training
    passes through the corpus):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example](img/Image00438.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The code returns the following 10 most probable words associated with each
    topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example](img/Image00439.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Although not all the topics have an easy interpretation, we can definitely
    see that topic 2 is associated with the words `disney` , `mulan` (a Disney movie),
    `love` , and `life` is a topic about animation movies, topic 6 is associated with
    the words `action` , `alien` , `bad` , and `planet` is related to fantasy sci-fi
    movies. In fact, we can query all the movies with most probable topic equal to
    6 like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example](img/Image00440.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This will return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Most of these titles are clearly sci-fi and fantasy movies, so the LDA algorithm
    clusters them correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Note that with the documents' representations in the topic space (`lda_lfq[corpus]`
    ), it would be possible to apply a cluster algorithm (see [Chapter 2](text00020.html#ch02
    "Chapter 2. Unsupervised Machine Learning") , *Machine Learning Techniques – Unsupervised
    Learning* ) but this is left to the reader as an exercise. Note also that each
    time the LDA algorithm is run, it may lead to different results due to the random
    initialization of the model (that is, it's normal if your results are different
    from what it is shown in this paragraph).
  prefs: []
  type: TYPE_NORMAL
- en: Opinion mining (sentiment analysis)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Opinion mining or sentiment analysis is the field of study of text to extract
    the opinion of the writer, which can usually be positive or negative (or neutral).
    This analysis is particularly useful especially in marketing to find the public
    opinion on products or services. The standard approach is to consider the sentiment
    (or polarity), negative or positive, as the target of a classification problem.
    A dataset of documents will have as many features as the number of different words
    contained in the vocabulary, and classification algorithms such as SVM and Naive
    Bayes are typically used. As an example, we consider the 2,000 movie reviews already
    used for testing LDA and information retrieval models that are already labeled
    (positive or negative). All of the code discussed in this paragraph is available
    on the `postprocessing.ipynb` IPython notebook at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/)
    . As before, we import the data and preprocess:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00441.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The data is then split into a training set (80%) and a test set (20%) in a
    way the `nltk` library can process (a list of tuples each or those with a dictionary
    containing the document words and the label):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00442.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can train and test a `NaiveBayesClassifier` (multinomial) using the
    `nltk` library and check the error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00443.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The code returns an error of 28.25%, but it is possible to improve the result
    by computing the best bigrams in each document. A bigram is defined as a pair
    of consecutive words, and the *X²* test is used to find bigrams that do not occur
    by chance but with a larger frequency. These particular bigrams contain relevant
    information for the text and are called collocations in natural language processing
    terminology. For example, given a bigram of two words, **w1** and **w2** , in
    our corpus with a total number of N possible bigrams, under the null hypothesis
    that **w1** and **w2** occur independently to each other, we can fill a two-dimensional
    matrix *O* by collecting the occurrences of the bigram ( **w1** , **w2** ) and
    the rest of the possible bigrams, such as these:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | w1 | Not w1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **w2** | 10 | 901 |'
  prefs: []
  type: TYPE_TB
- en: '| **Not w2** | 345 | 1,111,111 |'
  prefs: []
  type: TYPE_TB
- en: 'The *X²* measure is then given by ![Opinion mining (sentiment analysis)](img/Image00444.jpg)
    , where *O[ij] * is the number of occurrences of the bigram given by the words
    *(i, j)* (so that *O[00] =10* and so on) and *E[ij] * is the expected frequency
    of the bigram *(i, j)* (for example, ![Opinion mining (sentiment analysis)](img/Image00445.jpg)
    ). Intuitively, *X² * is higher the more the observed frequency *O[ij] * differs
    from the expected mean *E[ij] * , so the null hypothesis is likely to be rejected.
    The bigram is a good collocation and it contains *more information* than a bigram
    that follows the expected means. It can be shown that the *X² * can be calculated
    as the f test (also called **mean square contingency coefficient** ) multiplied
    by the total number of bigram occurrences *N* , as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00446.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'More information about the collocations and the *X²* methods can be found in
    *Foundations of Statistical Natural Language Processing* by C. D. Manning and
    H. Schuetze (1999). Note also that the *X²* , as the information gain measure
    (not discussed here), can be thought of as a feature selection method as defined
    in [Chapter 3](text00024.html#page "Chapter 3. Supervised Machine Learning") ,
    *Supervised Machine Learning* . Using the `nltk` library, we can use the *X²*
    measure to select the 500 best bigrams per document and then train a Naive Bayes
    classifier again, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00447.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This time the error rate is 20%, which is lower than in the normal method.
    The *X²* test can also be used to extract the most informative words from the
    whole corpus. We can measure how much the single word frequency differs from the
    frequency of the positive (or negative) documents to score its importance (for
    example, if the word `great` has a high *X²* value on positive reviews but low
    on negative reviews, it means that the word gives information that the review
    is positive). The 10,000 most significant words of the corpus can be extracted
    by calculating for each of them, the the overall frequency on the entire corpus
    and the frequencies over the positive and negative subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00448.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we can simply train a Naive Bayes classifier again using only the words
    in the `bestwords` set for each document:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00449.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The error rate is 12.75%, which is remarkably low considering the relatively
    small dataset. Note that to have a more reliable result, a cross-validation method
    (see [Chapter 3](text00024.html#page "Chapter 3. Supervised Machine Learning")
    , *Supervised Machine Learning* ) should be applied, but this is given to the
    reader as an exercise. Also note that the Doc2Vec vectors (compute in the *Movie
    review query example* section) can be used to train a classifier. Assuming that
    the Doc2Vec vectors have already been trained and stored in the `model_d2v.doc2vec`
    object, as usual we split the data into a training dataset (80%) and a test set
    (20%):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00450.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then we can train an SVM classifier ( **radial basis function kernel** ( **RBF**
    ) kernel) or a logistic regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00451.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logistic regression and SVM give very low accuracies, of `0.5172` and `0.5225`
    respectively. This is mostly due to the small size of the training dataset, which
    does not allow us to train algorithms that have a large number of parameters to
    train, such as neuron networks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter both the most common and advanced algorithms used to manage
    web data were discussed and implemented using a series of Python libraries. Now
    you should have a clear understanding of the challenges faced in the web mining
    area and should be able to handle some of these issues with Python. In the next
    chapter, we will discuss the most important recommendation systems algorithms
    used to date in the commercial environment.
  prefs: []
  type: TYPE_NORMAL
- en: 读累了记得休息一会哦~
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**公众号：古德猫宁李**'
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书单分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书友学习交流
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**网站：**[沉金书屋 https://www.chenjin5.com](https://www.chenjin5.com)'
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 电子书打包资源分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 学习资源分享
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
