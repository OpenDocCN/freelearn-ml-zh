- en: Chapter 4. Web Mining Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 网络挖掘技术
- en: 'Web data mining techniques are used to explore the data available online and
    then extract the relevant information from the Internet. Searching on the web
    is a complex process that requires different algorithms, and they will be the
    main focus of this chapter. Given a search query, the relevant pages are obtained
    using the data available on each web page, which is usually divided in the page
    content and the page hyperlinks to other pages. Usually, a search engine has multiple
    components:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 网络数据挖掘技术用于探索在线可用的数据，然后从互联网中提取相关信息。在网络上搜索是一个复杂的过程，需要不同的算法，这些算法将是本章的主要内容。给定一个搜索查询，使用每个网页上的数据获取相关页面，这些数据通常分为页面内容和指向其他页面的页面超链接。通常，搜索引擎具有多个组件：
- en: A web crawler or spider for collecting web pages
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于收集网页的网络爬虫或蜘蛛
- en: A parser that extracts content and preprocesses web pages
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个解析器，用于提取内容和预处理网页
- en: An indexer to organize the web pages in data structures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个索引器，用于在数据结构中组织网页
- en: A retrieval information system to score the most important documents related
    to a query
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个检索信息系统，对与查询相关的最重要文档进行评分
- en: A ranking algorithm to order the web pages in a meaningful manner
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个排名算法，以有意义的方式对网页进行排序
- en: These parts can be divided into web structure mining techniques and web content
    mining techniques.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这些部分可以分为网络结构挖掘技术和网络内容挖掘技术。
- en: The web crawler, indexer, and ranking procedures refer to the web structure
    (the network of hyperlinks). The other parts (parser and retrieval system) of
    a search engine are web content analysis methods because the text information
    on web pages is used to perform such operations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫、索引器和排名过程指的是网络结构（超链接的网络）。搜索引擎的其他部分（解析器和检索系统）是网络内容分析方法，因为网页上的文本信息用于执行这些操作。
- en: Furthermore, the content of a collection of web pages can be further analyzed
    using some natural language processing techniques, such as **latent Dirichlet
    allocation** opinion mining or sentiment analysis tools. These techniques are
    especially important for extracting subjective information about web users, and
    so they are widely found in many commercial applications, from marketing to consultancy.
    These sentiment analysis techniques will be discussed at the end of the chapter.
    Now we will start discussing the web structure mining category.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，可以使用一些自然语言处理技术进一步分析一组网页的内容，例如**潜在狄利克雷分配**意见挖掘或情感分析工具。这些技术在提取关于网络用户的主观信息方面尤为重要，因此它们在许多商业应用中广泛存在，从市场营销到咨询服务。这些情感分析技术将在本章末进行讨论。现在我们将开始讨论网络结构挖掘类别。
- en: Web structure mining
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络结构挖掘
- en: This field of web mining focuses on the discovery of the relationships among
    web pages and how to use this link structure to find the relevance of web pages.
    For the first task, usually a spider is employed, and the links and the collected
    web pages are stored in a indexer. For the the last task, the web page ranking
    evaluates the importance of the web pages.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个网络挖掘领域专注于发现网页之间的关系以及如何使用这种链接结构来找到网页的相关性。对于第一个任务，通常使用蜘蛛，并将链接和收集到的网页存储在索引器中。对于最后一个任务，网页排名评估网页的重要性。
- en: Web crawlers (or spiders)
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络爬虫（或蜘蛛）
- en: A spider starts from a set of URLs (seed pages) and then extracts the URL inside
    them to fetch more pages. New links are then extracted from the new pages and
    the process continues until some criteria are matched. The unvisited URLs are
    stored in a list called **frontier** , and depending on how the list is used,
    we can have different crawler algorithms, such as breadth-first and preferential
    spiders. In the **breadth-first** algorithm, the next URL to crawl comes from
    the head of the frontier while the new URLs are appended to the frontier tail.
    Preferential spider instead employs a certain importance estimate on the list
    of unvisited URLs to determine which page to crawl first. Note that the extraction
    of links from a page is performed using a parser, and this operation is discussed
    in more detail in the related paragraph of the web content mining section.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 蜘蛛从一组URL（种子页面）开始，然后从中提取URL以获取更多页面。然后从新页面中提取新链接，这个过程会一直持续到满足某些条件。未访问的URL存储在一个称为**frontier**的列表中，根据如何使用这个列表，我们可以有不同的爬虫算法，例如广度优先和优先级爬虫。在**广度优先**算法中，下一个要爬取的URL来自frontier的头部，而新URL被追加到frontier的尾部。优先级爬虫则使用对未访问URL列表的某种重要性估计来确定先爬取哪个页面。请注意，从页面中提取链接的操作是通过解析器完成的，这个操作在网页内容挖掘部分的相应段落中进行了更详细的讨论。
- en: A web crawler is essentially a graph search algorithm in which the structure
    of the neighborhood of the starting pages is retrieved, following certain criteria
    such as the maximum number of links to follow (depth of the graph), maximum number
    of pages to crawl, or time limit. A spider can then extract a portion of the Web
    that has interesting structures, such as hubs and authorities. A hub is a web
    page that contains a large number of links, while an authority is defined as,
    a page, with a large number of times that its URL occurs on other web pages (it
    is a measure of the page's popularity). A popular Python implementation of the
    crawler is given by the Scrapy library, which also employs concurrency methods
    (asynchronous programming using Twisted) to speed up operations. A tutorial on
    this module is given in [Chapter 7](text00050.html#page "Chapter 7. Movie Recommendation
    System Web Application") , *Movie Recommendation System Web Application* when
    the crawler will be used to extract information about movie reviews.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 网络爬虫本质上是一种图搜索算法，它检索起始页面的邻域结构，遵循某些标准，如最大链接数（图的深度）、最大爬取页面数或时间限制。然后，蜘蛛可以提取具有有趣结构的Web部分，例如枢纽和权威。枢纽是一个包含大量链接的网页，而权威被定义为，一个在其URL在其它网页上出现次数较多的页面（它是页面流行度的一个度量）。一个流行的Python爬虫实现是由Scrapy库提供的，它还采用了并发方法（使用Twisted的异步编程）来加速操作。在[第7章](text00050.html#page
    "第7章。电影推荐系统Web应用")，*电影推荐系统Web应用*中给出了这个模块的教程，当爬虫将用于提取电影评论信息时。
- en: Indexer
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 索引器
- en: An indexer is a way to store web pages found by the crawler in a structured
    database to allow subsequent fast retrieval on a given search query. The simplest
    indexing approach is to directly store all the pages and, at query time, just
    scan for all the documents that contain the keywords in the query. However, this
    method is not feasible if the number of pages is large (which in practice, it
    is) due to high computational costs. The most common method to speed up the retrieval
    is called **inverted index scheme** , which is used by the most popular search
    engines.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 索引器是一种将爬虫找到的网页存储在结构化数据库中的方法，以便在给定搜索查询时能够快速检索。最简单的索引方法是将所有页面直接存储，在查询时，只需扫描包含查询中关键词的所有文档。然而，如果页面数量很大（在实践中确实如此），由于计算成本高，这种方法是不可行的。最常见的方法是使用称为**倒排索引方案**的方法来加速检索，这是大多数流行搜索引擎所使用的。
- en: Given a set of web pages *p[1] , …, p[k]* and a vocabulary *V* containing all
    the words ![Indexer](img/Image00368.jpg) in the pages, the inverted index database
    is obtained by storing lists such as ![Indexer](img/Image00369.jpg) , …, ![Indexer](img/Image00370.jpg)
    ,
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组网页 *p[1] , …, p[k]* 和一个包含页面中所有单词的词汇表 *V*，通过存储如下列表 ![Indexer](img/Image00368.jpg)
    ，…， ![Indexer](img/Image00369.jpg) ，…， ![Indexer](img/Image00370.jpg) ，获得倒排索引数据库。
- en: Here, ![Indexer](img/Image00371.jpg) is the ID of the web page *j* . Extra information
    can be stored for each word, for example, the frequency count of the word or its
    position on each page. The implementation of the indexer is beyond the scope of
    this book, but the general concepts have been described in this paragraph for
    completeness.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![索引器](img/Image00371.jpg)是网页 *j* 的 ID。可以为每个单词存储额外信息，例如单词的频率计数或它在每个页面上的位置。索引器的实现超出了本书的范围，但为了完整性，本段描述了这些一般概念。
- en: Therefore, a search query with a list of words will retrieve all the inverted
    lists related to each word and then merge the lists. The order of the final lists
    will be chosen using the ranking algorithm together with an information retrieval
    system to measure the relevance of the documents to the query.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个包含单词列表的搜索查询将检索与每个单词相关的所有倒排列表，然后合并这些列表。最终列表的顺序将使用排名算法以及信息检索系统来衡量文档与查询的相关性来选择。
- en: Ranking – PageRank algorithm
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 排名 – PageRank 算法
- en: 'A ranking algorithm is important because the usual number of web pages that
    a single information retrieval query can return may be huge, so there is a problem
    of how to choose the most relevant pages. Furthermore, the information retrieval
    model can easily be spammed by just inserting many keywords into the page to make
    the page relevant to a large number of queries. So, the problem to evaluate the
    importance (that is, ranking score) of a web page on the Internet has been addressed
    considering the fact that the web has a graph in which the hyperlinks—links from
    a page to another—are the primary source of information to estimate the relevance
    of web pages. The hyperlinks can be divided as:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 排名算法很重要，因为单个信息检索查询可以返回的网页数量可能非常大，因此存在如何选择最相关网页的问题。此外，信息检索模型很容易被通过在页面上插入许多关键词来垃圾邮件化，使得页面与大量查询相关。因此，考虑到网络具有一个超链接——从一个页面指向另一个页面的链接——是估计网页相关性的主要信息来源的事实，已经解决了评估网页在互联网上的重要性（即排名分数）的问题。超链接可以分为：
- en: 'in-links of page *i* : hyperlinks that point to page *i*'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面 *i* 的入链：指向页面 *i* 的超链接
- en: 'out-links of page *i* : hyperlinks that point to other pages from page *i*'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面 *i* 的出链：从页面 *i* 指向其他页面的超链接
- en: 'Intuitively, the more in-links a web page has, the more important the page
    should be. The study of this hyperlink structure is part of social network analysis,
    and many algorithms have been used and proposed. But for historical reasons, we
    will explain the most well known algorithm, called **PageRank** , which was presented
    by Sergey Brin and Larry Page (the founders of Google) in 1998\. The whole idea
    is to calculate the prestige of a page as the sum of the prestiges of the pages
    that point to it. If the prestige of a page *j* is *P(j)* it is equally distributed
    to all the pages *N[j]* that it points to so that each out-link receives a portion
    of prestige equal to *P(j)|N[j]* . Formally, the prestige or page rank score of
    a page *i* can be defined as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，一个网页拥有的入链越多，该网页应该越重要。对这种超链接结构的研究是社会网络分析的一部分，已经使用和提出了许多算法。但出于历史原因，我们将解释最著名的算法，称为
    **PageRank**，由 Sergey Brin 和 Larry Page（Google 的创始人）在 1998 年提出。整个想法是将页面的声望计算为指向它的页面的声望之和。如果页面
    *j* 的声望为 *P(j)*，它将平均分配给它指向的所有页面 *N[j]*，使得每个出链获得与 *P(j)|N[j]* 相等的声望部分。正式地，页面 *i*
    的声望或页面排名分数可以定义为：
- en: '![Ranking – PageRank algorithm](img/Image00372.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![排名 – PageRank 算法](img/Image00372.jpg)'
- en: 'Here, ![Ranking – PageRank algorithm](img/Image00373.jpg) if page *j* points
    to page *i* ; otherwise it is equal to *0* . *A[ij] * is called adjacency matrix
    and it represents the portion of prestige that propagates from node *j* to node
    *i* . Considering *N* total nodes in the graph, the preceding equation can be
    rewritten in matrix form:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![排名 – PageRank 算法](img/Image00373.jpg)如果页面 *j* 指向页面 *i*；否则它等于 *0*。*A[ij]*
    被称为邻接矩阵，它表示从节点 *j* 到节点 *i* 传播的声望部分。考虑到图中总共有 *N* 个节点，前面的方程可以重写为矩阵形式：
- en: '![Ranking – PageRank algorithm](img/Image00374.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![排名 – PageRank 算法](img/Image00374.jpg)'
- en: 'Note that this equation is equivalent to an eigensystem with eigenvalue ![Ranking
    – PageRank algorithm](img/Image00375.jpg) if the adjacency matrix satisfies certain
    conditions. Another way to interpret the preceding equation is to use the Markov
    chain terminology—the entry *A[ij] * becomes the transition probability from node
    *j* to node *i* and the prestige of node *i* , *p(i)* , is the probability to
    visit node *i* . In this scenario, it may happen that two nodes (or more) point
    to each other but do not point to other pages. Once one of these two nodes has
    been visited, a loop will occur and the user will be trapped in it. This situation
    is called **rank sink** , (the matrix *A* is called **periodic** ) and the solution
    is to add a transition matrix term that allows jumping from each page to another
    page at random without following the Markov chain described by *A* :'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果邻接矩阵满足某些条件，此方程等价于具有特征值 ![排名 – PageRank 算法](img/Image00375.jpg) 的特征系统。另一种解释前述方程的方法是使用马尔可夫链术语——项
    *A[ij]* 变成从节点 *j* 到节点 *i* 的转移概率，节点 *i* 的声望 *p(i)* 是访问节点 *i* 的概率。在这种情况下，可能发生两个节点（或更多）相互指向但不对其他页面进行指向。一旦访问了这两个节点之一，就会发生循环，用户将被困在其中。这种情况被称为**排名陷阱**，（矩阵
    *A* 被称为**周期性**），解决方案是添加一个转移矩阵项，允许在不遵循由 *A* 描述的马尔可夫链的情况下随机从一个页面跳转到另一个页面：
- en: '![Ranking – PageRank algorithm](img/Image00376.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![排名 – PageRank 算法](img/Image00376.jpg)'
- en: 'Here, *E=ee^T* is a matrix of one entry of dimensions *N´N* ( *e* is a unit
    vector), and *d* (also called the **damping factor** ) is the probability to follow
    the transition given by the transition matrix *A* . *(1-d)* is the probability
    to visit a page randomly. In this final form, all the nodes are linked to each
    other so that even if the adjacency matrix has a row with many *0* entries for
    a particular node *s* , *A[sj]* , there is always a small probability equal to
    ![Ranking – PageRank algorithm](img/Image00377.jpg) that *s* is visited from all
    the *N* nodes in the graph. Note that *A* has to be stochastic, which means each
    row has to sum to *1* ; ![Ranking – PageRank algorithm](img/Image00378.jpg) (at
    least one entry per row different from *0* or at least one out-link per page).
    The equation can be simplified by normalizing the *P* vector as *e^T P=N* :'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*E=ee^T* 是一个 *N×N* 维度的矩阵的一个条目（*e* 是一个单位向量），*d*（也称为**阻尼因子**）是遵循由转移矩阵 *A*
    给出的转移的概率。*(1-d)* 是随机访问页面的概率。在这种最终形式中，所有节点都相互连接，即使对于特定节点 *s* 的邻接矩阵有多个 *0* 条目，*A[sj]*，从图中的所有
    *N* 个节点访问 *s* 的概率始终有一个等于 ![排名 – PageRank 算法](img/Image00377.jpg) 的小概率。注意，*A* 必须是随机的，这意味着每一行都必须求和为
    *1*；![排名 – PageRank 算法](img/Image00378.jpg)（至少每行有一个不同于 *0* 的条目或至少每页有一个出链）。可以通过将
    *P* 向量标准化为 *e^T P=N* 来简化方程：
- en: '![Ranking – PageRank algorithm](img/Image00379.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![排名 – PageRank 算法](img/Image00379.jpg)'
- en: This can be solved using the power iteration method. This algorithm will be
    used in [Chapter 8](text00059.html#ch08 "Chapter 8. Sentiment Analyser Application
    for Movie Reviews") , *Sentiment Analyser Application on Movie Reviews* to implement
    an example of a movie review sentiment analysis system. The main advantages of
    this algorithm is that it does not depend on the query (so the PageRank scores
    can be computed offline and retrieved at query time), and it is very robust to
    spamming since it is not feasible for a spammer to insert in-links to their page
    on influential pages.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过幂迭代法来解决。此算法将在第 8 章 [情感分析器应用](text00059.html#ch08 "第 8 章。电影评论情感分析器应用")，*电影评论情感分析器应用*中用于实现一个电影评论情感分析系统的示例。此算法的主要优点是它不依赖于查询（因此，可以在查询时离线计算
    PageRank 分数并检索），并且它对垃圾邮件非常稳健，因为垃圾邮件发送者不可能在影响力较大的页面上插入指向他们页面的入链。
- en: Web content mining
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络内容挖掘
- en: This type of mining focuses on extracting information from the content of web
    pages. Each page is usually gathered and organized (using a parsing technique),
    processed to remove the unimportant parts from the text (natural language processing),
    and then analyzed using an information retrieval system to match the relevant
    documents to a given query. These three components are discussed in the following
    paragraphs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的挖掘侧重于从网页内容中提取信息。每个页面通常会被收集和组织（使用解析技术），处理以从文本中去除不重要的部分（自然语言处理），然后使用信息检索系统进行分析，以将相关文档与给定的查询相匹配。以下段落将讨论这三个组件。
- en: Parsing
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析
- en: 'A web page is written in HTML format, so the first operation is to extract
    the relevant pieces of information. An HTML parser builds a tree of tags from
    which the content can be extracted. Nowadays, there are many parsers available,
    but as an example, we use the Scrapy library see [Chapter 7](text00050.html#page
    "Chapter 7. Movie Recommendation System Web Application") , *Movie Recommendation
    System Web Application* which provides a command-line parser. Let''s say we want
    to parse the main page of Wikipedia, [https://en.wikipedia.org/wiki/Main_Page](https://en.wikipedia.org/wiki/Main_Page)
    . We simply type this in a terminal:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一个网页是用HTML格式编写的，因此第一步是提取相关信息。HTML解析器从标签中构建一个树形结构，从而可以从中提取内容。如今，有许多解析器可供选择，但作为一个例子，我们使用Scrapy库，见[第7章](text00050.html#page
    "第7章. 电影推荐系统Web应用")，*电影推荐系统Web应用*，它提供了一个命令行解析器。假设我们想要解析维基百科的主页，[https://en.wikipedia.org/wiki/Main_Page](https://en.wikipedia.org/wiki/Main_Page)
    。我们只需在终端中输入以下内容：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A prompt will be ready to parse the page using the `response` object and the
    `xpath` language. For example we want to obtain the title''s page:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将准备好使用`response`对象和`xpath`语言来解析页面。例如，我们想要获取页面的标题：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Or we want to extract all the embedded links in page (this operation is needed
    for the crawler to work), which are usually put on `<a>` , and the URL value is
    on an `href` attribute:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们想要提取页面中所有嵌入的链接（这项操作对于爬虫的工作是必需的），这些链接通常放在`<a>`标签上，URL值位于`href`属性中：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note that a more robust way to parse content can be used since the web pages
    are usually written by non-programmers, so the HTML may contain syntax errors
    that browsers typically repair. Note also that web pages may contain a large amount
    of data due to advertisements, making the parsing of relevant information complicated.
    Different algorithms have been proposed (for instance, tree matching) to identify
    the main content of a page but no Python libraries are available at the moment,
    so we have decided not to discuss this topic further. However, note that a nice
    parsing implementation for extracting the body of a web article can be found in
    the newspaper library and it will also be used in [Chapter 7](text00050.html#page
    "Chapter 7. Movie Recommendation System Web Application") , *Movie Recommendation
    System Web Application* .
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，可以使用更健壮的方式来解析内容，因为网页通常是由非程序员编写的，所以HTML可能包含语法错误，浏览器通常会修复这些错误。还要注意的是，由于广告等原因，网页可能包含大量数据，这使得解析相关信息变得复杂。已经提出了不同的算法（例如，树匹配）来识别页面的主要内容，但目前没有Python库可用，所以我们决定不再进一步讨论这个话题。然而，请注意，在newspaper库中可以找到一个很好的解析实现，用于提取网络文章的主体，它也将在[第7章](text00050.html#page
    "第7章. 电影推荐系统Web应用")，*电影推荐系统Web应用*中使用。
- en: Natural language processing
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Once the text content of a web page has been extracted, the text data is usually
    preprocessed to remove parts that do not bring any relevant information. A text
    is tokenized, that is, transformed into a list of words (tokens), and all the
    punctuation marks are removed. Another usual operation is to remove all the *stopwords*
    , that is, all the words used to construct the syntax of a sentence but not containing
    text information (such as conjunctions, articles, and prepositions) such as *a*
    , *about* , *an* , *are* , *as* , *at* , *be* , *by* , *for* , *from* , *how*
    , *in* , *is* , *of* , *on* , *or* , *that* , *the* , *these* , *this* , *to*
    , *was* , *what* , *when* , *where* , *who* , *will* , *with* , and many others.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提取了网页的文本内容，通常会对文本数据进行预处理，以去除不包含任何相关信息的部分。文本被标记化，即转换成一个单词列表（标记），并且所有标点符号都被移除。另一个常见的操作是移除所有*停用词*，即那些用于构建句子语法但不包含文本信息的单词（如连词、冠词和介词）等，例如*a*、*about*、*an*、*are*、*as*、*at*、*be*、*by*、*for*、*from*、*how*、*in*、*is*、*of*、*on*、*or*、*that*、*the*、*these*、*this*、*to*、*was*、*what*、*when*、*where*、*who*、*will*、*with*，以及许多其他单词。
- en: 'Many words in English (or any language) share the same root but have different
    suffixes or prefixes. For example, the words *think* , *thinking* , and *thinker*
    all share the same root— *think* indicating that the meaning is the same—but the
    role in a sentence is different (verb, noun, and so on). The procedure to reduce
    all the words in a set to its roots it is called **stemming** , and many algorithms
    have been invented to do so (Porter, Snowball, and Lancaster). All of these techniques
    are parts of a broader range of algorithms called **natural language processing**
    , and they are implemented in Python on the `nltk` library (installed as usual
    through `sudo pip install nltk` ). As an example, the following code preprocesses
    a sample text using the techniques described previously (using the Python interface
    terminal):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 英语（或任何语言）中的许多单词具有相同的词根，但有不同的后缀或前缀。例如，单词 *think*、*thinking* 和 *thinker* 都有相同的词根—
    *think* 表示它们的意义相同，但在句子中的作用不同（动词、名词等）。将一组中的所有单词还原到其词根的过程称为 **词干提取**，为此已经发明了许多算法（Porter、Snowball
    和 Lancaster）。所有这些技术都是更广泛算法范围的一部分，称为 **自然语言处理**，并且它们在 Python 的 `nltk` 库中实现（通常通过
    `sudo pip install nltk` 安装）。例如，以下代码使用之前描述的技术（使用 Python 接口终端）预处理一个示例文本：
- en: '![Natural language processing](img/Image00380.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![自然语言处理](img/Image00380.jpg)'
- en: Note that the `stopwords` list has been downloaded using the `nltk dowloader
    nltk.download('stopwords')` .
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`stopwords` 列表已通过 `nltk dowloader nltk.download('stopwords')` 下载。
- en: Information retrieval models
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信息检索模型
- en: 'The information retrieval methods are needed to find the most relevant documents
    to a given query. The words contained in the web pages can be modeled using different
    approaches such as Boolean models, vector space models, and probabilistic models,
    and in this book, we have decided to discuss the vector space models and how to
    implement them. Formally, given a vocabulary of *V* words, each web page *d[i]*
    (or document) in a collection of *N* pages, can be thought of as a vector of words,
    ![Information retrieval models](img/Image00381.jpg) , where each word *j* belonging
    to the document *i* is represented by *w[ij] * , which can be either a number
    (weight) or a vector depending on the chosen algorithm:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索方法需要找到与给定查询最相关的文档。网页中的单词可以使用不同的方法进行建模，例如布尔模型、向量空间模型和概率模型，本书中我们决定讨论向量空间模型及其实现方法。形式上，给定一个包含
    *V* 个单词的词汇表，一个包含 *N* 页面的集合中的每个网页 *d[i]*（或文档）可以被视为一个单词向量，![信息检索模型](img/Image00381.jpg)，其中属于文档
    *i* 的每个单词 *j* 由 *w[ij]* 表示，这可以是根据所选算法的数字（权重）或向量：
- en: Term frequency-inverse document frequency (TF-IDF), *w[ij]* , is a real number
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词语频率-逆文档频率（TF-IDF），*w[ij]*，是一个实数
- en: '**Latent Semantic Analysis** ( **LSA** ), *w[ij]* , is a real number (representation
    independent of the document *i* )'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在语义分析**（**LSA**），*w[ij]*，是一个实数（与文档 *i* 的表示无关）'
- en: Doc2Vec (or word2vec), *w[ij]* , is a vector of real numbers (representation
    independent of the document *i* )
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doc2Vec（或 word2vec），*w[ij]*，是一个实数向量（与文档 *i* 的表示无关）
- en: 'Since the query can also be represented by a vector of words, ![Information
    retrieval models](img/Image00382.jpg) , the web pages most similar to the vector
    *q* are found by calculating a similarity measure between the query vector and
    each document. The most used similarity measure is called cosine similarity, for
    any document *i* given by:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于查询也可以表示为一个单词向量，![信息检索模型](img/Image00382.jpg)，因此通过计算查询向量与每个文档之间的相似度度量来找到与向量
    *q* 最相似的网页。最常用的相似度度量称为余弦相似度，对于任何给定的文档 *i*：
- en: '![Information retrieval models](img/Image00383.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![信息检索模型](img/Image00383.jpg)'
- en: Note that there are other measures used in literature (okapi and pivoted normalization
    weighting), but for the purpose of this book, they are not necessary.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，文献中还有其他使用的度量方法（okapi 和 pivoted normalization weighting），但为了本书的目的，它们不是必要的。
- en: The following sections will give some details about the three methods before
    being applied in a text case in the final paragraph of the section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下几节将在本节最后一段的文本案例中应用之前将详细介绍这三种方法。
- en: TF-IDF
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF-IDF
- en: 'This method calculates *w[ij]* , taking into account the fact that a word that
    appears many times and in a large number of pages is likely to be less important
    than a word that occurs many times but only in a subset of documents. It is given
    by the multiplication of two factors:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法计算 *w[ij]* ，考虑到一个在大量页面中多次出现的单词可能不如在文档子集内多次出现但出现次数较少的单词重要。它由两个因素的乘积给出：
- en: '![TF-IDF](img/Image00384.jpg) where:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![TF-IDF](img/Image00384.jpg) 其中：'
- en: '![TF-IDF](img/Image00385.jpg) is the normalized frequency of the word *j* in
    the document *I*'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![TF-IDF](img/Image00385.jpg) 是单词 *j* 在文档 *I* 中的标准化频率'
- en: '![TF-IDF](img/Image00386.jpg) is the inverse document frequency and *df[j]
    * is the number of web pages that contain the word *j*'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![TF-IDF](img/Image00386.jpg) 是逆文档频率，*df[j] * 是包含单词 *j* 的网页数量'
- en: Latent Semantic Analysis (LSA)
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 潜在语义分析 (LSA)
- en: 'The name of this algorithm comes from the idea that there is a latent space
    in which each word (and each document) can be efficiently described, assuming
    that words with similar meanings also occur in similar text positions. Projection
    on this subspace is performed using the (truncated) SVD method already discussed
    in [Chapter 2](text00020.html#ch02 "Chapter 2. Unsupervised Machine Learning")
    , *Machine Learning Techniques – Unsupervised Learning* . We contextualize the
    method for LSA as follows: the web pages are collected together in matrix *X (V
    ´N* ), in which each column is a document:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此算法的名称来源于以下观点：存在一个潜在空间，其中每个单词（以及每个文档）都可以被有效地描述，假设具有相似意义的单词也出现在相似文本位置。在此子空间上的投影是通过已讨论的（截断）奇异值分解（SVD）方法实现的，参见[第2章](text00020.html#ch02
    "第2章。无监督机器学习")，*机器学习技术 – 无监督学习*。我们将此方法应用于LSA的上下文中如下：网页被收集到矩阵 *X (V ´N* ) 中，其中每一列是一个文档：
- en: '![Latent Semantic Analysis (LSA)](img/Image00387.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![潜在语义分析 (LSA)](img/Image00387.jpg)'
- en: 'Here, *U[t]* ( *V ´d* ) is the matrix of the words projected in the new latent
    space with *d* dimensions, ![Latent Semantic Analysis (LSA)](img/Image00388.jpg)
    ( *d* ´ *N* ) is the transpose matrix of the documents transformed into the subspace,
    and ![Latent Semantic Analysis (LSA)](img/Image00389.jpg) ( *d* ´ *d* ) is the
    diagonal matrix with singular values. The query vector itself is projected into
    the latent space by:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*U[t]* ( *V ´d* ) 是在具有 *d* 维度的新潜在空间中投影的单词矩阵，![潜在语义分析 (LSA)](img/Image00388.jpg)
    ( *d* ´ *N* ) 是将文档转换到子空间后的转置矩阵，而![潜在语义分析 (LSA)](img/Image00389.jpg) ( *d* ´ *d*
    ) 是具有奇异值的对角矩阵。查询向量本身通过以下方式投影到潜在空间中：
- en: '![Latent Semantic Analysis (LSA)](img/Image00390.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![潜在语义分析 (LSA)](img/Image00390.jpg)'
- en: Now, each document represented by each row of *V[t]* can be compared with *q[t]*
    using the cosine similarity. Note that the true mathematical representation of
    the documents on the latent space is given by ![Latent Semantic Analysis (LSA)](img/Image00391.jpg)
    (not *V[t] * ) because the singular values are the scaling factors of the space
    axis components and they must be taken into account. Therefore, this matrix should
    be compared with ![Latent Semantic Analysis (LSA)](img/Image00392.jpg) . Nevertheless,
    it usually computes the similarity between *V[t] * and *q[t] * , and in practice,
    it is still unknown which method returns the best results.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个由 *V[t]* 的每一行表示的文档可以通过余弦相似度与 *q[t]* 进行比较。请注意，文档在潜在空间中的真实数学表示由![潜在语义分析 (LSA)](img/Image00391.jpg)
    (而不是 *V[t] * ) 给出，因为奇异值是空间轴组件的缩放因子，必须考虑。因此，这个矩阵应该与![潜在语义分析 (LSA)](img/Image00392.jpg)
    进行比较。尽管如此，它通常计算 *V[t] * 和 *q[t] * 之间的相似度，而在实践中，哪种方法返回最佳结果仍然未知。
- en: Doc2Vec (word2vec)
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Doc2Vec (word2vec)
- en: This method represents each word *j* , *w[j]* , as a vector ![Doc2Vec (word2vec)](img/Image00393.jpg)
    but independent of the document *d[i] * it occurs in. Doc2Vec is an extension
    of the word2vec algorithm originally proposed by Mikolov and others, and it employs
    neuron networks and backpropagation to generate the word (and document) vectors.
    Due to the increasing importance of neuron networks (especially deep learning)
    in many machine learning applications, we decided to include the main concepts
    and formulas of this quite advanced method here to give you an introduction to
    a subject that will become extremely important in the future of machine learning
    in various fields. The following description is based on the paper Rong (2014)
    and Le and Mikolov (2014), and the notation also reflects the name currently used
    in literature.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将每个词 *j* ， *w[j]* ，表示为一个向量 ![Doc2Vec (word2vec)](img/Image00393.jpg) ，但与其出现的文档
    *d[i]* 无关。Doc2Vec是Mikolov等人最初提出的word2vec算法的扩展，它使用神经网络和反向传播来生成词（和文档）向量。由于神经网络（尤其是深度学习）在许多机器学习应用中的重要性日益增加，我们决定在此介绍这种相当高级的方法的主要概念和公式，以便为您介绍一个在未来机器学习的各个领域中都将变得极其重要的主题。以下描述基于Rong（2014）和Le和Mikolov（2014）的论文，并且符号也反映了文献中目前使用的名称。
- en: Word2vec – continuous bag of words and skip-gram architectures
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2vec – 连续词袋和skip-gram架构
- en: 'Each word *j* in the vocabulary *V* is represented by a vector of length *|V|*
    , with binary entries *x[j] =(x[1j] , …, x[Vj] )* , where only *x[jj] =1* ; otherwise,
    *0* . The word2vec method trains a single (hidden) layer of *N* neurons (weights),
    choosing between two different network architectures (shown in the following figure).
    Note that both architectures have only one layer of *N* neurons or weights, *h*
    . This means that the method has to be considered *shallow* learning and not deep,
    which typically refers to networks with many hidden layers. The **Continuous Bag
    Of Words** ( **CBOW** ) method (displayed to the right in the following figure)
    trains the model using a set of *C* words as an input called *context* trying
    to predict the word (target) that occurs adjacent to the input text. The reverse
    approach is called **Skip-gram** , in which the input is the target word and the
    network is trained to predict the context set (displayed to the left in the following
    figure ). Note that *C* is called the window parameter and it sets how far from
    the target word the context words are selected:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表 *V* 中的每个词 *j* 都用一个长度为 *|V|* 的向量表示，具有二进制条目 *x[j] =(x[1j] , …, x[Vj] )* ，其中只有
    *x[jj] =1* ，否则为 *0* 。word2vec方法训练一个（隐藏）层 *N* 个神经元（权重），在两种不同的网络架构（以下图中所示）之间进行选择。请注意，这两种架构都只有一层
    *N* 个神经元或权重， *h* 。这意味着该方法必须被视为浅层学习而不是深层学习，后者通常指的是具有许多隐藏层的网络。**连续词袋**（ **CBOW**
    ）方法（以下图中右侧所示）使用一组 *C* 个词作为输入，称为上下文，试图预测输入文本旁边出现的词（目标）。相反的方法称为**Skip-gram**，其中输入是目标词，网络被训练来预测上下文集（以下图中左侧所示）。请注意，*C*
    被称为窗口参数，它设置上下文词选择距离目标词有多远：
- en: '![Word2vec – continuous bag of words and skip-gram architectures](img/Image00394.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Word2vec – 连续词袋和skip-gram架构](img/Image00394.jpg)'
- en: Skip-gram (left) and CBOW (right) architectures of the word2vec algorithm; figures
    taken from word2vec Parameter Learning Explained by X Rong (2015)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec算法的Skip-gram（左侧）和CBOW（右侧）架构；图取自X Rong（2015）的《word2vec参数学习解释》
- en: In both cases, the matrix *W* transforms the input vectors into the hidden layer
    and *W'* transforms from the hidden layer to the output layer *y* , where the
    target (or context) is evaluated. In the training phase, the error from the true
    target (or context) is computed and used to calculate a stochastic gradient descent
    to update both the matrices *W* and *W'* . We will give a more mathematical description
    of the CBOW method in the following section. Note that the Skip-gram equations
    are similar and we will refer to the Rong (2015) paper for further details.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，矩阵 *W* 将输入向量转换为隐藏层，而 *W'* 将从隐藏层转换到输出层 *y* ，其中评估目标（或上下文）。在训练阶段，计算真实目标（或上下文）的错误，并用于计算随机梯度下降以更新矩阵
    *W* 和 *W'* 。我们将在下一节中给出CBOW方法的更数学描述。请注意，Skip-gram方程式类似，我们将参考Rong（2015）的论文以获取更多详细信息。
- en: Mathematical description of the CBOW model
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CBOW模型的数学描述
- en: 'Starting from the input layer, the hidden layer *h* can be obtained by computing,
    ![Mathematical description of the CBOW model](img/Image00395.jpg) , where ![Mathematical
    description of the CBOW model](img/Image00396.jpg) is a vector of length *N* that
    represents the word *w[i] * on the hidden layer and *w[C] * is the average of
    the *C* context vectors ![Mathematical description of the CBOW model](img/Image00396.jpg)
    . Choosing a target word *w[j] * , the score at the output layer *u[j] * is obtained
    by multiplying the vector ![Mathematical description of the CBOW model](img/Image00397.jpg)
    (the j-th column of *W''* ) by *h* :'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入层开始，隐藏层*h*可以通过计算获得，![CBOW模型的数学描述](img/Image00395.jpg)，其中![CBOW模型的数学描述](img/Image00396.jpg)是一个长度为*N*的向量，代表隐藏层上的词*w[i]*和*w[C]*是上下文向量![CBOW模型的数学描述](img/Image00396.jpg)的平均值。选择目标词*w[j]*，输出层的得分*u[j]*是通过将向量![CBOW模型的数学描述](img/Image00397.jpg)（*W'*的j列）与*h*相乘得到的：
- en: '![Mathematical description of the CBOW model](img/Image00398.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![CBOW模型的数学描述](img/Image00398.jpg)'
- en: 'This is not the final value on the output layer *y[j]* because we want to evaluate
    the posterior conditional probability to have the target word *w[j]* given the
    context *C* expressed by the **softmax** formula:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是输出层*y[j]*上的最终值，因为我们想评估在给定上下文*C*的情况下，目标词*w[j]*的后验条件概率，这可以通过**softmax**公式来表示：
- en: '![Mathematical description of the CBOW model](img/Image00399.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![CBOW模型的数学描述](img/Image00399.jpg)'
- en: Now the training objective is to maximize this probability for all the words
    in the vocabulary, which is equivalent to ![Mathematical description of the CBOW
    model](img/Image00400.jpg) , where ![Mathematical description of the CBOW model](img/Image00401.jpg)
    and the index *j^M * represents the vector of *W'* in which the product is maximum,
    that is, the most probable target word.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的训练目标是最大化词汇表中所有词的概率，这等价于![CBOW模型的数学描述](img/Image00400.jpg)，其中![CBOW模型的数学描述](img/Image00401.jpg)和索引*j^M*代表*W'*中乘积最大的向量，即最可能的目标词。
- en: 'The stochastic gradient descent equations are then obtained by calculating
    the derivatives of *E* with respect to the entries of *W (w[ij]* ) and *W'' (w''[ij'']*
    ). The final equations for each output target word *w[j]* are:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过计算*E*相对于*W (w[ij]* )和*W' (w'[ij']* )的条目的导数，得到随机梯度下降方程。每个输出目标词*w[j]*的最终方程是：
- en: '![Mathematical description of the CBOW model](img/Image00402.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![CBOW模型的数学描述](img/Image00402.jpg)'
- en: '![Mathematical description of the CBOW model](img/Image00403.jpg) where ![Mathematical
    description of the CBOW model](img/Image00404.jpg) and *a* is the learning rate
    of the gradient descent. The derivative ![Mathematical description of the CBOW
    model](img/Image00405.jpg) represents the error of the network with respect to
    the true target word so that the error is back propagated on the system, which
    can learn iteratively. Note that the vectors ![Mathematical description of the
    CBOW model](img/Image00406.jpg) are the usual word representations used to perform
    the semantic operations.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![CBOW模型的数学描述](img/Image00403.jpg)其中![CBOW模型的数学描述](img/Image00404.jpg)和*a*是梯度下降的学习率。导数![CBOW模型的数学描述](img/Image00405.jpg)表示网络相对于真实目标词的误差，以便误差可以反向传播到系统中，系统可以迭代学习。注意，向量![CBOW模型的数学描述](img/Image00406.jpg)是执行语义操作时使用的常用词向量表示。'
- en: Further details can be found in the Rong (2015) paper.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节可以在Rong (2015)的论文中找到。
- en: Doc2Vec extension
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Doc2Vec扩展
- en: 'As explained in Le and Mikolov (2014), Doc2Vec is a natural extension of the
    word2vec method in which a document is considered as an additional word vector.
    So in the case of the CBOW architecture, the hidden layer vector *h* is just the
    average of the context vectors and the document vector *d[i]* :'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如Le和Mikolov (2014)所述，Doc2Vec是word2vec方法的自然扩展，其中将文档视为一个额外的词向量。因此，在CBOW架构的情况下，隐藏层向量*h*是上下文向量平均值和文档向量*d[i]*：
- en: '![Doc2Vec extension](img/Image00407.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![Doc2Vec扩展](img/Image00407.jpg)'
- en: 'This architecture is shown in the following figure, and it is called the **distributed
    memory model** ( **DM** ) because the document *d[i]* vector just remembers the
    information of the document not represented by the context words. The vector ![Doc2Vec
    extension](img/Image00408.jpg) is shared with all the context words sampled from
    the document *d[i] * but the matrix *W* (and *W''* ) is the same for all the documents:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构如图所示，被称为**分布式内存模型**（**DM**），因为文档*d[i]*向量只记住由上下文词未表示的文档信息。向量![Doc2Vec扩展](img/Image00408.jpg)与从文档*d[i]*中采样的所有上下文词共享，但矩阵*W*（和*W'*）对所有文档都是相同的：
- en: '![Doc2Vec extension](img/Image00409.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![Doc2Vec扩展](img/Image00409.jpg)'
- en: A distributed memory model example with a context of three words (window=3);
    figure taken from Distributed Representations of Sentences and Documents by Le
    and Mikolov (2014)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有三个单词上下文的分布式内存模型示例（window=3）；图来自Le和Mikolov的《Sentences and Documents的分布式表示》（2014年）
- en: The other proposed architecture is called **distributed bag of words** ( **DBOW**
    ), which considers only a document vector in the input layer and a set of context
    words sampled from the document in the output layer. It has been shown that the
    DM architecture performs better than DBOW, and it is therefore the default model
    in the `gensim` library implementation. The reader is advised to read the paper
    of Le and Mikolov (2014) for further details.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个提出的架构称为**分布式词袋**（**DBOW**），它只考虑输入层中的一个文档向量以及从文档中采样的上下文词集合。已经证明DM架构的性能优于DBOW，因此它是`gensim`库实现中的默认模型。建议读者阅读Le和Mikolov（2014）的论文以获取更多详细信息。
- en: Movie review query example
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 电影评论查询示例
- en: 'To show in action the three information retrieval methods discussed previously,
    we use the IMBD movie reviews in the *polarity dataset v2.0* and *Pool of 27886
    unprocessed html files* at [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)
    , provided by Bo Pang and Lillian Lee (the dataset and the code are also stored
    in the GitHub account of the author at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/)
    . Download and unzip the `movie.zip` file from the website (called `polarity_html.zip`
    ), which creates the `movie` folder with all the web page movie reviews (about
    2000 files). First of all, we need to prepare the data from the files:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示之前讨论的三种信息检索方法，我们使用了来自 *polarity dataset v2.0* 和 *27886个未处理的html文件集合* 的IMBD电影评论，这些数据由Bo
    Pang和Lillian Lee提供，网址为[http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)。数据集和代码也存储在作者的GitHub账户[https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/)中。从网站上下载并解压`movie.zip`文件（称为`polarity_html.zip`），这将创建一个包含所有网页电影评论的`movie`文件夹（约2000个文件）。首先，我们需要从文件中准备数据：
- en: '![Movie review query example](img/Image00410.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00410.jpg)'
- en: 'This time we use **BeautifulSoup** to parse the title of the movie from each
    HTML web page and create a dictionary, `moviedict` . The `polarity dataset v2.0.tar.gz`
    contains a folder, `review_polarity` , which is inside the `txt_sentoken/` folder
    that split the positive and negative reviews into two separate subfolders (pros
    and cons). These files are preprocessed using the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们使用**BeautifulSoup**解析每个HTML网页的电影标题并创建一个字典，`moviedict`。`polarity dataset
    v2.0.tar.gz`包含一个名为`review_polarity`的文件夹，它位于`txt_sentoken/`文件夹内，该文件夹将正面和负面评论分割成两个单独的子文件夹（优点和缺点）。这些文件使用以下代码进行预处理：
- en: '![Movie review query example](img/Image00411.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00411.jpg)'
- en: 'Now all the 2,000 reviews are stored in the `tot_textreviews` list and the
    corresponding titles in `tot_titles` . The TF-IDF model can be trained using `sklearn`
    :'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有2000条评论都存储在`tot_textreviews`列表中，相应的标题在`tot_titles`中。可以使用`sklearn`训练TF-IDF模型：
- en: '![Movie review query example](img/Image00412.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00412.jpg)'
- en: 'After the `PreprocessTfidf` function, apply all the preprocessing techniques
    (removing stop words, tokenizing, and stemming) to each document. In the same
    way, we can train the LSA model using the `gensim` library, specifying 10 latent
    dimensions:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在`PreprocessTfidf`函数之后，将所有预处理技术（去除停用词、分词和词干提取）应用于每个文档。同样，我们可以使用`gensim`库训练LSA模型，指定10个潜在维度：
- en: '![Movie review query example](img/Image00413.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00413.jpg)'
- en: 'Note that the `GenSimCorpus` function just preprocesses the documents with
    the usual techniques and transforms them into a format that the gensim LSA implementation
    can read. From the `lsi` object, it is possible to obtain the matrices *U* , *V*
    , and S that are needed to transform the query into the latent space:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`GenSimCorpus`函数只是使用常规技术预处理文档，并将它们转换为gensim LSA实现可以读取的格式。从`lsi`对象中，可以获取到将查询转换为潜在空间所需的矩阵*U*、*V*和S：
- en: '![Movie review query example](img/Image00414.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00414.jpg)'
- en: Also the indexed dictionary of words, `dict_words` , has been calculated to
    transform a query word into the corresponding index word in `dict_corpus` .
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还计算了单词索引字典`dict_words`，将查询词转换为`dict_corpus`中的对应索引词。
- en: 'The last model to train is Doc2Vec. First, we prepare the data in a format
    that the gensim Doc2Vec implementation can handle:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后要训练的模型是Doc2Vec。首先，我们将数据准备成gensim Doc2Vec实现可以处理的形式：
- en: '![Movie review query example](img/Image00415.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00415.jpg)'
- en: 'Each review has been placed in a `namedtuple` object, which contains the words
    preprocessed by the `PreprocessDoc2Vec` function (stopwords removed and tokenization
    performed) and the tag that is the name of the file. Note that we chose not to
    apply a stemmer because the results are generally better without it (the reader
    can test the results by applying the stemmer, setting the Boolean flag `doc2vecstem`
    to `True` ). The Doc2Vec training is finally performed by the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每条评论都放置在一个`namedtuple`对象中，该对象包含由`PreprocessDoc2Vec`函数预处理过的单词（移除了停用词并执行了分词）以及代表文件名的标签。请注意，我们没有选择应用词干提取器，因为不使用它时结果通常更好（读者可以通过应用词干提取器来测试结果，将布尔标志`doc2vecstem`设置为`True`）。Doc2Vec训练最终通过以下代码完成：
- en: '![Movie review query example](img/Image00416.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00416.jpg)'
- en: We set the DM architecture (`dm` =1), the hidden layer with 500 dimensions (`size`
    ), a window size of 10 words, and all the words that occur at least once have
    been taken into account by the model (`min_count` =1).The other parameters are
    related to the efficiency optimization method (`negative` for negative sampling
    and `hs` for hierarchical softmax). The training lasted for `20` epochs, with
    a learning rate equal to `0.99` .
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了DM架构（`dm` =1），具有500维度的隐藏层（`size`），窗口大小为10个单词，并且模型考虑了至少出现一次的所有单词（`min_count`
    =1）。其他参数与效率优化方法相关（`negative`用于负采样和`hs`用于层次 softmax）。训练持续了`20`个epoch，学习率等于`0.99`。
- en: 'We can now verify which results each method returns, defining a query to retrieve
    all the web documents related to sci-fi movies, that is, movies usually described
    by this list of words:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以验证每种方法返回的结果，定义一个查询以检索所有与科幻电影相关的网络文档，即通常用以下单词列表描述的电影：
- en: '![Movie review query example](img/Image00417.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00417.jpg)'
- en: 'The TF-IDF method returns the five most similar web pages using the following
    script:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF方法使用以下脚本返回最相似的五个网页：
- en: '![Movie review query example](img/Image00418.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00418.jpg)'
- en: 'Note that the model uses a sparse matrix format to store data, so the `cosine_similarity`
    function converts the vectors into regular vectors. Then it computes the similarity.
    In a similar way, the query is converted in a *q[k]* in LSA terminology and the
    five most similar web pages are printed out:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，该模型使用稀疏矩阵格式来存储数据，因此`cosine_similarity`函数将向量转换为常规向量。然后它计算相似度。以类似的方式，查询在LSA术语中被转换为*q[k]*，并打印出最相似的五个网页：
- en: '![Movie review query example](img/Image00419.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00419.jpg)'
- en: 'Finally, the `doc2vec` model transforms the query list into a vector using
    the `infer_vector` function, and the most similar reviews are returned by the
    `most_similar` function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`doc2vec`模型使用`infer_vector`函数将查询列表转换为向量，并通过`most_similar`函数返回最相似的评论：
- en: '![Movie review query example](img/Image00420.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![电影评论查询示例](img/Image00420.jpg)'
- en: 'Note that the `random` parameter of the model needs to be set up to a fixed
    value to return deterministic results whenever an optimization approach is used
    (negative sampling or hierarchical softmax). The results are as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型的`random`参数需要设置为一个固定值，以便在每次使用优化方法（负采样或层次 softmax）时返回确定性的结果。结果如下：
- en: '**TF-IDF** :![Movie review query example](img/Image00421.jpg)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF-IDF** :![电影评论查询示例](img/Image00421.jpg)'
- en: '**LSA** :![Movie review query example](img/Image00422.jpg)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSA** :![电影评论查询示例](img/Image00422.jpg)'
- en: Doc2vec:![Movie review query example](img/Image00423.jpg)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doc2vec:![电影评论查询示例](img/Image00423.jpg)
- en: All three methods show movies related to the query. Interestingly, TF-IDF performs
    better than the more advanced LSA and Doc2Vec algorithms because *In the Heat
    of the Night* , *Pokemon* , *Rocky Horror Picture Show* , and *Wild Things* are
    not related to the query compared with the TF-IDF results which show only one
    movie ( *No Telling* ) as unrelated. The movies *Charlie's Angels* and *Batman
    & Robin* are action movies, so they are mostly related to the single query word
    *action* . Doc2Vec returns the worst results mostly because the training dataset
    is too small to learn good vector representations (as an example, Google released
    a word2vec trained dataset based on billions of documents, or more). The website
    [http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)
    provides a larger dataset, so the reader can try to train Doc2Vec with more data
    as an exercise.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种方法都显示与查询相关的电影。有趣的是，TF-IDF的性能优于更先进的LSA和Doc2Vec算法，因为在*夜幕下的热*、*宝可梦*、*摇滚恐怖电影秀*和*野性之物*与查询不相关，而TF-IDF的结果只显示一部电影（*无可奉告*）作为不相关。电影*查理的安吉拉*和*蝙蝠侠与罗宾*是动作电影，所以它们大部分与单个查询词*动作*相关。Doc2Vec返回最差的结果，主要是因为训练数据集太小，无法学习良好的向量表示（例如，谷歌发布了一个基于数十亿文档训练的word2vec数据集，或者更多）。网站[http://www.cs.cornell.edu/people/pabo/movie-review-data/](http://www.cs.cornell.edu/people/pabo/movie-review-data/)提供了一个更大的数据集，因此读者可以尝试使用更多数据训练Doc2Vec作为练习。
- en: Postprocessing information
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 后处理信息
- en: Once the web pages are collected from the Web, there are natural language processing
    algorithms that are able to extract relevant information for different commercial
    purposes apart from building a web search engine. We will discuss here algorithms
    that are able to extract the main topics on the collection of documents (latent
    Dirichlet analysis) and to extract the sentiment or opinion of each web page (opinion
    mining techniques).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从网络中收集了网页，除了构建网络搜索引擎之外，还有一些自然语言处理算法能够提取不同商业目的的相关信息。我们将在这里讨论能够从文档集合中提取主要主题（潜在狄利克雷分析）和提取每个网页的情感或意见（意见挖掘技术）的算法。
- en: Latent Dirichlet allocation
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: '**Latent Dirichlet allocation** ( **LDA** ) is a natural language processing
    algorithm that belongs to the generative model category. The technique is based
    on the observations of some variables that can be explained by other underlined
    unobserved variables, which are the reasons the observed data is similar or different.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）是一种属于生成模型类别的自然语言处理算法。该技术基于一些变量的观察，这些变量可以通过其他下划线的未观察到的变量来解释，这些变量是观察到的数据相似或不同的原因。'
- en: For example, consider text documents in which words are the observations. Each
    document can be the result of a mixture of topics (unobserved variables) and each
    word refers to a specific topic.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑文本文档，其中单词是观察结果。每个文档可以是多个主题（未观察到的变量）混合的结果，每个单词都指代一个特定的主题。
- en: 'For instance, consider the two following documents describing two companies:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下两个描述两家公司的文档：
- en: '**doc1** : Changing how people search for fashion items and, share and buy
    fashion via visual recognition, TRUELIFE is going to become the best approach
    to search the ultimate trends …'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文档1**：改变人们搜索时尚商品的方式，通过视觉识别分享和购买时尚，TRUELIFE将成为搜索终极趋势的最佳方法 …'
- en: '**doc2** : Cinema4you enabling any venue to be a cinema is a new digital filmed
    media distribution company currently in the testing phase. It applies technologies
    used in Video on Demand and broadcasting to ...'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**doc2**：Cinema4you使任何场所都能成为电影院，是一家目前处于测试阶段的新的数字电影媒体发行公司。它应用了视频点播和广播中使用的科技
    ...'
- en: 'LDA is a way of automatically discovering latent topics that these documents
    contain. For example, given these documents and asked for two topics, LDA might
    return the following words associated with each topic:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是一种自动发现这些文档包含的潜在主题的方法。例如，给定这些文档并要求两个主题，LDA可能会返回与每个主题相关的以下单词：
- en: '**topic 1** : people Video fashion media…'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题1**：人物、视频、媒体…'
- en: '**topic 2** : Cinema technologies recognition broadcasting…'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题2**：电影技术、识别、广播…'
- en: Therefore, the second topic can be labeled as *technology* while the first as
    *business* .
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第二个主题可以标记为*技术*，而第一个主题为*商业*。
- en: 'Documents are then represented as mixtures of topics that spit out words with
    certain probabilities:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 文档随后被表示为以一定概率吐出单词的主题混合：
- en: '**doc1** : topic 1 42%,topic 2 64%'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**doc1** : 主题 1 42%，主题 2 64%'
- en: '**doc2** : topic 1 21%, topic 2 79%'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**doc2** : 主题 1 21%，主题 2 79%'
- en: This representation of the documents can be useful in various applications such
    as clustering of pages in different groups, or to extract the main common subjects
    of a collection of pages. The mathematical model behind this algorithm is explained
    in the next paragraph.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这种文档表示在诸如不同组页面聚类或提取页面集合的主要共同主题等应用中可能很有用。该算法背后的数学模型将在下一段中解释。
- en: Model
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型
- en: 'Documents are represented as random mixtures over latent topics, where each
    topic is characterized by a distribution over words. LDA assumes the following
    process for a corpus consisting of *M* documents, *d=(d[1] , …, d[M] )* , with
    each *i* containing *N[i]* words. If *V* is the length of the vocabulary, a word
    of document *i* is represented by a vector *w[i]* of length *V* , where only an
    element *w[i] [v] =1* and the others are *0* :'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '文档被表示为潜在主题上的随机混合，其中每个主题由单词上的分布来表征。LDA 假设对于由 *M* 个文档组成的语料库，*d=(d[1] , …, d[M]
    )* ，每个 *i* 包含 *N[i]* 个单词。如果 *V* 是词汇表长度，文档 *i* 中的一个单词由长度为 *V* 的向量 *w[i]* 来表示，其中只有一个元素
    *w[i] [v] =1* ，其余都是 *0* :'
- en: '![Model](img/Image00424.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![模型](img/Image00424.jpg)'
- en: The number of latent dimensions (topics) is *K* , and for each document, ![Model](img/Image00425.jpg)
    is the vector of topics associated with each word *w[i] * , where *z[i] * is a
    vector of *0's* of length *K* except for the element *j, z[i] ^j =1* , that represents
    the topic *w[i] * has been drawn from.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 潜在维度（主题）的数量是 *K* ，对于每份文档，![模型](img/Image00425.jpg) 是与每个单词 *w[i]* 相关的主题向量，其中
    *z[i]* 是长度为 *K* 的 *0* 向量，除了元素 *j* ，*z[i]^j =1* ，它表示主题 *w[i]* 已经被抽取。
- en: 'b indicates the *K* ´ *V* matrix, where *b[ij]* represents the probability
    that each word *j* in the vocabulary is drawn from topic *i* : ![Model](img/Image00426.jpg)
    .'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: b 表示 *K* ´ *V* 矩阵，其中 *b[ij]* 代表词汇表中的每个单词 *j* 被从主题 *i* 抽取的概率：![模型](img/Image00426.jpg)
    。
- en: 'So, each row *i* of b is the word''s distribution of topic *i* , while each
    column *j* is the topic''s distribution of word *j* . Using these definitions,
    the process is described as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，b 的每一行 *i* 是主题 *i* 的单词分布，而每一列 *j* 是单词 *j* 的主题分布。使用这些定义，过程描述如下：
- en: From a chosen distribution (usually Poisson), draw the length of each document
    *N[i]* .
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从选定的分布（通常是泊松分布）中抽取每份文档的长度 *N[i]* 。
- en: For each document *d[i]* , draw the topic distribution q *[i]* , as a Dirichlet
    distribution *Dir(a)* , where ![Model](img/Image00427.jpg) and *a* is a parameter
    vector of length *K* such that ![Model](img/Image00428.jpg) .
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每份文档 *d[i]* ，抽取主题分布 q *[i]* ，作为一个狄利克雷分布 *Dir(a)* ，其中 ![模型](img/Image00427.jpg)
    和 *a* 是长度为 *K* 的参数向量，使得 ![模型](img/Image00428.jpg) 。
- en: For each document *d[i]* , for each word *n* , draw a topic from the multinomial
    ![Model](img/Image00429.jpg) .
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每份文档 *d[i]* ，对于每个单词 *n* ，从![模型](img/Image00429.jpg) 的多项式分布中抽取一个主题。
- en: For each document *d[i]* , for each word *n* , and for each topic *z[n]* , draw
    a word *w[n]* from a multinomial given by the row *z[n]* of b, ![Model](img/Image00430.jpg)
    .
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每份文档 *d[i]* ，对于每个单词 *n* ，以及对于每个主题 *z[n]* ，从由 b 的第 *z[n]* 行给出的多项式分布中抽取一个单词
    *w[n]* ，![模型](img/Image00430.jpg) 。
- en: 'The objective of the algorithm is to maximize the posterior probability for
    each document:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的目的是最大化每个文档的后验概率：
- en: '![Model](img/Image00431.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![模型](img/Image00431.jpg)'
- en: 'Applying the conditional probability definition, the numerator becomes the
    following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 应用条件概率定义，分子变为以下内容：
- en: '![Model](img/Image00432.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![模型](img/Image00432.jpg)'
- en: 'So, the probability that the document *i* is given by topic vector *z* and
    word probability matrix b can be expressed as a multiplication of the single word
    probabilities:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，文档 *i* 在主题向量 *z* 和单词概率矩阵 b 给定下的概率可以表示为单个单词概率的乘积：
- en: '![Model](img/Image00433.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![模型](img/Image00433.jpg)'
- en: 'Considering that *z[n]* is a vector with only one component *j* different from
    *0* , *z^j [n] =1* , then ![Model](img/Image00434.jpg) . Substituting these expressions
    on (2):'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 *z[n]* 是一个只有一个分量 *j* 与 *0* 不同的向量，*z^j [n] =1* ，那么 ![模型](img/Image00434.jpg)
    。将这些表达式代入 (2)：
- en: '![Model](img/Image00435.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![模型](img/Image00435.jpg)'
- en: The denominator of (1) is obtained simply by integration over q *[i]* and summation
    over *z* . The final values of the topic distribution *q[i]* and the words per
    topic distribution (rows of *b* ) are obtained by calculating this probability
    by approximated inference techniques; those are beyond the scope of this book.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: (1)中的分母是通过在q *[i]*上积分和在*z*上求和得到的。主题分布*q[i]*和每个主题的单词分布（*b*的行）的最终值是通过通过近似推理技术计算这个概率得到的；这些内容超出了本书的范围。
- en: The parameter *a* is called the concentration parameter, and it indicates how
    much the distribution is spread over the possible values. A concentration parameter
    of *1* (or *k* , the dimension of the Dirichlet distribution, by the definition
    used in topic modeling literature) results in all sets of probabilities being
    equally probable. Meanwhile, in the limit as the concentration parameter tends
    towards zero, only distributions with nearly the entire mass concentrated on one
    of their components are likely (the words are less shared among different topics
    and they concentrate on a few topics).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 参数*a*被称为浓度参数，它表示分布如何在可能的值上分散。浓度参数为*1*（或*k*，根据主题建模文献中使用的定义，狄利克雷分布的维度）导致所有概率集合具有相等的可能性。同时，当浓度参数趋向于零时，只有几乎全部质量集中在它们的一个组成部分上的分布可能是可能的（单词在不同主题之间共享较少，并且集中在少数几个主题上）。
- en: As an example, a 100,000-dimension categorical distribution has a vocabulary
    of 100,000 words even though a topic may be represented by a couple of hundred
    words. As a consequence, typical values for the concentration parameter are between
    0.01 and 0.001, or lower if the vocabulary's size is millions of words or higher.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个100,000维度的分类分布有100,000个单词的词汇量，尽管一个主题可能只由几百个单词表示。因此，典型的浓度参数值在0.01到0.001之间，或者如果词汇量的大小是数百万个单词或更高，则更低。
- en: According to L. Li and Y. Zhang's paper *An empirical study of text classification
    using Latent Dirichlet Allocation* , LDA can be used as an effective dimension
    reduction method for text modeling. However, even though the method has performed
    well in various applications, there are certain issues to consider. The initialization
    of the model is random, which means it can lead to different results in each run.
    Also, the choice of concentration parameters is important, but there is no standard
    method to choose them.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 根据L. Li和Y. Zhang的论文*使用潜在狄利克雷分配进行文本分类的经验研究*，LDA可以作为文本建模的有效降维方法。然而，尽管该方法在各种应用中表现良好，但仍有一些问题需要考虑。模型的初始化是随机的，这意味着它可能导致每次运行的结果不同。此外，浓度参数的选择很重要，但没有标准的方法来选择它们。
- en: Example
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例
- en: 'Consider again the movie reviews'' web pages, `textreviews` , already preprocessed
    in the *Movie review query example* section, and LDA is applied to test whether
    it is possible to gather reviews on different topics. As usual, the following
    code is available in `postprocessing.ipynb` at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/)
    :'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑电影评论网页，`textreviews`，在*电影评论查询示例*部分已经预处理过，并应用LDA来测试是否可以收集不同主题的评论。像往常一样，以下代码可在`postprocessing.ipynb`中找到，位于[https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/)：
- en: '![Example](img/Image00436.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/Image00436.jpg)'
- en: 'As usual we have transformed each document in tokens (a different tokenizer
    has been used) and the stop words have been removed. To achieve better results,
    we filter out the most frequent words (such as `movie` and `film` ) that do not
    add any information to the pages. We ignore all the words with more than 1,000
    occurrences or observed less than three times:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们将每个文档转换成了标记（使用了不同的标记化器）并去除了停用词。为了获得更好的结果，我们过滤掉了那些对页面不提供任何信息的最频繁出现的单词（例如`movie`和`film`）。我们忽略了所有出现次数超过1,000次或观察次数少于三次的单词：
- en: '![Example](img/Image00437.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/Image00437.jpg)'
- en: 'Now we can train the LDA model with 10 topics (`passes` is the number of training
    passes through the corpus):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以用10个主题来训练LDA模型（`passes`是训练过程中通过语料库的遍历次数）：
- en: '![Example](img/Image00438.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/Image00438.jpg)'
- en: 'The code returns the following 10 most probable words associated with each
    topic:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 代码返回与每个主题相关联的以下10个最可能的单词：
- en: '![Example](img/Image00439.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/Image00439.jpg)'
- en: 'Although not all the topics have an easy interpretation, we can definitely
    see that topic 2 is associated with the words `disney` , `mulan` (a Disney movie),
    `love` , and `life` is a topic about animation movies, topic 6 is associated with
    the words `action` , `alien` , `bad` , and `planet` is related to fantasy sci-fi
    movies. In fact, we can query all the movies with most probable topic equal to
    6 like this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然并非所有主题都有一个简单的解释，但我们确实可以看到主题2与单词`disney`、`mulan`（一部迪士尼电影）、`love`相关联，而`life`是一个关于动画电影的主题，主题6与单词`action`、`alien`、`bad`相关联，而`planet`与科幻电影相关。实际上，我们可以查询所有最可能的主题等于6的电影，如下所示：
- en: '![Example](img/Image00440.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![示例](img/Image00440.jpg)'
- en: 'This will return:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回：
- en: '[PRE3]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Most of these titles are clearly sci-fi and fantasy movies, so the LDA algorithm
    clusters them correctly.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些标题显然是科幻和奇幻电影，所以LDA算法正确地将它们聚类在一起。
- en: Note that with the documents' representations in the topic space (`lda_lfq[corpus]`
    ), it would be possible to apply a cluster algorithm (see [Chapter 2](text00020.html#ch02
    "Chapter 2. Unsupervised Machine Learning") , *Machine Learning Techniques – Unsupervised
    Learning* ) but this is left to the reader as an exercise. Note also that each
    time the LDA algorithm is run, it may lead to different results due to the random
    initialization of the model (that is, it's normal if your results are different
    from what it is shown in this paragraph).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于文档在主题空间中的表示（`lda_lfq[corpus]`），可以应用聚类算法（见[第2章](text00020.html#ch02 "第2章。无监督机器学习")，*机器学习技术
    - 无监督学习*），但这留作读者的练习。还要注意，每次运行LDA算法时，由于模型的随机初始化，可能会导致不同的结果（也就是说，如果你的结果与本段中显示的不同是正常的）。
- en: Opinion mining (sentiment analysis)
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 观点挖掘（情感分析）
- en: 'Opinion mining or sentiment analysis is the field of study of text to extract
    the opinion of the writer, which can usually be positive or negative (or neutral).
    This analysis is particularly useful especially in marketing to find the public
    opinion on products or services. The standard approach is to consider the sentiment
    (or polarity), negative or positive, as the target of a classification problem.
    A dataset of documents will have as many features as the number of different words
    contained in the vocabulary, and classification algorithms such as SVM and Naive
    Bayes are typically used. As an example, we consider the 2,000 movie reviews already
    used for testing LDA and information retrieval models that are already labeled
    (positive or negative). All of the code discussed in this paragraph is available
    on the `postprocessing.ipynb` IPython notebook at [https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/)
    . As before, we import the data and preprocess:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 观点挖掘或情感分析是研究文本以提取作者观点的领域，这通常可以是积极的或消极的（或中性的）。这种分析在市场营销中尤其有用，可以找到公众对产品或服务的意见。标准方法是将情感（或极性），即积极或消极，视为分类问题的目标。一个文档数据集将具有与词汇表中不同单词数量一样多的特征，并且通常使用SVM和朴素贝叶斯等分类算法。作为一个例子，我们考虑了已经用于测试LDA和信息检索模型的2,000条电影评论，这些评论已经标记（正面或负面）。本段中讨论的所有代码都可在`postprocessing.ipynb`
    IPython笔记本中找到，网址为[https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/](https://github.com/ai2010/machine_learning_for_the_web/tree/master/chapter_4/)。与之前一样，我们导入数据并进行预处理：
- en: '![Opinion mining (sentiment analysis)](img/Image00441.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![观点挖掘（情感分析）](img/Image00441.jpg)'
- en: 'The data is then split into a training set (80%) and a test set (20%) in a
    way the `nltk` library can process (a list of tuples each or those with a dictionary
    containing the document words and the label):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 数据随后被分成一个训练集（80%）和一个测试集（20%），以`nltk`库可以处理的方式（每个元组或包含文档单词和标签的字典列表）：
- en: '![Opinion mining (sentiment analysis)](img/Image00442.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![观点挖掘（情感分析）](img/Image00442.jpg)'
- en: 'Now we can train and test a `NaiveBayesClassifier` (multinomial) using the
    `nltk` library and check the error:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`nltk`库训练和测试一个`NaiveBayesClassifier`（多项式）并检查错误：
- en: '![Opinion mining (sentiment analysis)](img/Image00443.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![观点挖掘（情感分析）](img/Image00443.jpg)'
- en: 'The code returns an error of 28.25%, but it is possible to improve the result
    by computing the best bigrams in each document. A bigram is defined as a pair
    of consecutive words, and the *X²* test is used to find bigrams that do not occur
    by chance but with a larger frequency. These particular bigrams contain relevant
    information for the text and are called collocations in natural language processing
    terminology. For example, given a bigram of two words, **w1** and **w2** , in
    our corpus with a total number of N possible bigrams, under the null hypothesis
    that **w1** and **w2** occur independently to each other, we can fill a two-dimensional
    matrix *O* by collecting the occurrences of the bigram ( **w1** , **w2** ) and
    the rest of the possible bigrams, such as these:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '|   | w1 | Not w1 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| **w2** | 10 | 901 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| **Not w2** | 345 | 1,111,111 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: 'The *X²* measure is then given by ![Opinion mining (sentiment analysis)](img/Image00444.jpg)
    , where *O[ij] * is the number of occurrences of the bigram given by the words
    *(i, j)* (so that *O[00] =10* and so on) and *E[ij] * is the expected frequency
    of the bigram *(i, j)* (for example, ![Opinion mining (sentiment analysis)](img/Image00445.jpg)
    ). Intuitively, *X² * is higher the more the observed frequency *O[ij] * differs
    from the expected mean *E[ij] * , so the null hypothesis is likely to be rejected.
    The bigram is a good collocation and it contains *more information* than a bigram
    that follows the expected means. It can be shown that the *X² * can be calculated
    as the f test (also called **mean square contingency coefficient** ) multiplied
    by the total number of bigram occurrences *N* , as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00446.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: 'More information about the collocations and the *X²* methods can be found in
    *Foundations of Statistical Natural Language Processing* by C. D. Manning and
    H. Schuetze (1999). Note also that the *X²* , as the information gain measure
    (not discussed here), can be thought of as a feature selection method as defined
    in [Chapter 3](text00024.html#page "Chapter 3. Supervised Machine Learning") ,
    *Supervised Machine Learning* . Using the `nltk` library, we can use the *X²*
    measure to select the 500 best bigrams per document and then train a Naive Bayes
    classifier again, as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00447.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'This time the error rate is 20%, which is lower than in the normal method.
    The *X²* test can also be used to extract the most informative words from the
    whole corpus. We can measure how much the single word frequency differs from the
    frequency of the positive (or negative) documents to score its importance (for
    example, if the word `great` has a high *X²* value on positive reviews but low
    on negative reviews, it means that the word gives information that the review
    is positive). The 10,000 most significant words of the corpus can be extracted
    by calculating for each of them, the the overall frequency on the entire corpus
    and the frequencies over the positive and negative subsets:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00448.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'Now we can simply train a Naive Bayes classifier again using only the words
    in the `bestwords` set for each document:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00449.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'The error rate is 12.75%, which is remarkably low considering the relatively
    small dataset. Note that to have a more reliable result, a cross-validation method
    (see [Chapter 3](text00024.html#page "Chapter 3. Supervised Machine Learning")
    , *Supervised Machine Learning* ) should be applied, but this is given to the
    reader as an exercise. Also note that the Doc2Vec vectors (compute in the *Movie
    review query example* section) can be used to train a classifier. Assuming that
    the Doc2Vec vectors have already been trained and stored in the `model_d2v.doc2vec`
    object, as usual we split the data into a training dataset (80%) and a test set
    (20%):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00450.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
- en: 'Then we can train an SVM classifier ( **radial basis function kernel** ( **RBF**
    ) kernel) or a logistic regression model:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Opinion mining (sentiment analysis)](img/Image00451.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Logistic regression and SVM give very low accuracies, of `0.5172` and `0.5225`
    respectively. This is mostly due to the small size of the training dataset, which
    does not allow us to train algorithms that have a large number of parameters to
    train, such as neuron networks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter both the most common and advanced algorithms used to manage
    web data were discussed and implemented using a series of Python libraries. Now
    you should have a clear understanding of the challenges faced in the web mining
    area and should be able to handle some of these issues with Python. In the next
    chapter, we will discuss the most important recommendation systems algorithms
    used to date in the commercial environment.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 读累了记得休息一会哦~
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**公众号：古德猫宁李**'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书单分享
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 书友学习交流
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**网站：**[沉金书屋 https://www.chenjin5.com](https://www.chenjin5.com)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 电子书搜索下载
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 电子书打包资源分享
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 学习资源分享
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
