<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Machine Learning Tools, Libraries, and Frameworks"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Machine Learning Tools, Libraries, and Frameworks</h1></div></div></div><p>In the previous chapter, we covered the Machine learning solution architecture and the implementation aspects of a technology platform—Hadoop. In this chapter, we will look at some of the highly adopted and upcoming Machine learning tools, libraries, and frameworks. This chapter is a primer for the following chapters as it covers how to implement a specific Machine learning algorithm using out-of-box functions of an identified Machine learning framework.</p><p>We will first cover the landscape of open source and commercial Machine learning libraries or tools that are available in the market, and pick the top five open source options. For each of the identified options, starting from installation steps, learning the syntax, implementing a complex Machine learning algorithm, to plotting graphs, we will cover it all. This chapter is mandatory for the readers in the order of occurrence as it is a foundation for all the example implementations in the chapters that follow.</p><p>Each of the identified frameworks can operate as standalone libraries and can run on Hadoop as well. In addition to learning how to program and implement a Machine learning algorithm, we will also cover how each of the identified frameworks integrate and run on Hadoop; this is what differentiates these tutorials from the mainstream ones found on the web.</p><p>The topics listed here are covered in depth in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A brief list of commercial and open source Machine learning libraries.</li><li class="listitem" style="list-style-type: disc">Top libraries or frameworks covered are R, Mahout, Julia, Python (Machine learning libraries, in particular), and Spark.</li><li class="listitem" style="list-style-type: disc">Apache Mahout<a id="id491" class="indexterm"/> is a framework used for running Machine learning algorithms built over Hadoop and is a Java-based open source Machine learning option. This framework can also work standalone. It is known for running Machine learning algorithms to heavy volumes of data. This framework is a part of Hadoop ecosystem components and has its distribution.</li><li class="listitem" style="list-style-type: disc">R is <a id="id492" class="indexterm"/>an open source Machine learning and a data mining tool that is adopted very widely in the Machine learning community. This framework library can either work standalone or can be run on Hadoop using the Hadoop runtime R extensions.</li><li class="listitem" style="list-style-type: disc">Julia is<a id="id493" class="indexterm"/> an open source high-performance programming language that supports running numeric and statistical computing functions in a distributed and parallel way.</li><li class="listitem" style="list-style-type: disc">Python is an<a id="id494" class="indexterm"/> interpreted, high-level programming language that is designed to try out different things and it is something that does not fall into the traditional waterfall way of development. We will explore the basic Python libraries—<span class="strong"><strong>NumPy</strong></span> <a id="id495" class="indexterm"/>and <span class="strong"><strong>SciPy</strong></span><a id="id496" class="indexterm"/> and use scikit-learn to execute our first Machine learning program. Also, we will explore how to write a Hadoop MapReduce program in Python.</li><li class="listitem" style="list-style-type: disc">Apache Spark <a id="id497" class="indexterm"/>and its Machine learning core libraries: Spark is a cluster computing system with API for Java, Python, and Scala. We will explore the <span class="strong"><strong>MLlib API</strong></span> <a id="id498" class="indexterm"/>for Machine learning and use a version for Apache Hadoop. The focus will be to explore the Spark Java APIs.</li><li class="listitem" style="list-style-type: disc">A brief introduction to Spring XD<a id="id499" class="indexterm"/> and the related Machine learning libraries.</li><li class="listitem" style="list-style-type: disc">For each of the identified Machine learning frameworks, integration with Hadoop will be a primary focus.</li></ul></div><div class="section" title="Machine learning tools – A landscape"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec23"/>Machine learning tools – A landscape</h1></div></div></div><p>There are several open source and <a id="id500" class="indexterm"/>commercial Machine learning frameworks and tools in the market that have evolved over the last few decades. While the field of Machine learning itself is evolving in building powerful algorithms for diverse requirements across domains, we now see a surge of open source options for large-scale Machine learning that have reached a significant level of maturity and are being widely adopted by the data science and Machine learning communities.</p><p>The model has changed significantly in the recent past, and researchers are encouraged to publish their software under an open source model. Since there are problems that authors face while publishing their work in using algorithmic implementations for Machine learning, any work that is reviewed and improvised through usage by the data science community is considered to be of more value.</p><p>The following figure shows a concept model of some important commercial and open source Machine learning frameworks and tools in the market. The highlighted ones will be covered in depth in this <a id="id501" class="indexterm"/>chapter.</p><div class="mediaobject"><img src="graphics/B03980_04_01.jpg" alt="Machine learning tools – A landscape"/></div><p>Some of these libraries are around specific programming languages such as Java, Python, C++, Scala, and so on. Some of these libraries, like Julia, Spark, and Mahout already support distributed, and parallel processing and others such as R and Python can run as MapReduce functions on Hadoop.</p><p>In the following sections, for <a id="id502" class="indexterm"/>each of the highlighted Machine learning libraries, the following will be covered:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">An overview of the library or tool with the details of out-of-box Machine learning functions supported</li><li class="listitem" style="list-style-type: disc">Installation, setup, and configuration guide</li><li class="listitem" style="list-style-type: disc">Introduction to syntax and basic data processing functions, and then the Advanced Machine learning functions example implementations</li><li class="listitem" style="list-style-type: disc">Samples for visualizations and plotting (wherever applicable)</li><li class="listitem" style="list-style-type: disc">Integration and execution on the Hadoop platform</li></ul></div></div></div>
<div class="section" title="Apache Mahout"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec24"/>Apache Mahout</h1></div></div></div><p>Apache Mahout is a <a id="id503" class="indexterm"/>Machine learning library that comes packaged with Apache Hadoop and forms an important part of the Hadoop ecosystem.</p><p>Mahout came into existence in 2008 as a subproject of Apache Lucene (an open source search engine). Lucene is an API that has an implementation of search, text mining, and information-retrieval techniques. Most of these search and text analytics internally apply Machine learning techniques. The recommendation engines that were built for the search engines started off under a new subproject called Mahout. Mahout means the <span class="emphasis"><em>rider of an elephant</em></span>, signifying the running of Machine learning algorithms over Hadoop. It is a scalable Machine learning implementation that can run in a standalone mode (does not tightly integrate with Hadoop) as well.</p><div class="mediaobject"><img src="graphics/B03980_04_02.jpg" alt="Apache Mahout"/></div><p>Mahout is a set of some basic Machine learning Java libraries used for classification, clustering, pattern mining, and so on. Though Mahout today provides support for a subset of Machine learning algorithms, it still ranks among the most adopted frameworks as it inherently supports analytics on large datasets to the degree of hundreds of millions of rows, which can be unstructured in nature as well.</p><div class="section" title="How does Mahout work?"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec54"/>How does Mahout work?</h2></div></div></div><p>Mahout implements <a id="id504" class="indexterm"/>Hadoop MapReduce, and the most important aspect is that it works on top of Hadoop and applies a distributed computing paradigm.</p><div class="mediaobject"><img src="graphics/B03980_04_03.jpg" alt="How does Mahout work?"/></div><p>Following are some of the specific Machine learning tasks that Mahout currently implements:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Collaborative Filtering / Recommendation</strong></span>: This<a id="id505" class="indexterm"/> takes a user input and finds items that the users might like</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Clustering</strong></span>: This takes a <a id="id506" class="indexterm"/>bunch of documents as input and groups them based on the topics they refer/belong to</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Classification</strong></span>: This takes a <a id="id507" class="indexterm"/>bunch of documents and, based on the existing categorization of the documents, learns what category a given document might belong to, and maps the document to that category</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Frequent itemset mining</strong></span>: This takes <a id="id508" class="indexterm"/>a bunch of items as input and, based on the learning from the real occurrences, identifies which items occur or appear together</li></ul></div><p>There are certain<a id="id509" class="indexterm"/> algorithms, for example, logistic regression and SVM (more about these algorithms will be covered in the chapters to follow), which cannot be parallelized and run in a standalone mode.</p></div><div class="section" title="Installing and setting up Apache Mahout"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec55"/>Installing and setting up Apache Mahout</h2></div></div></div><p>In this chapter, we will<a id="id510" class="indexterm"/> look at how to run Mahout in a standalone mode and on<a id="id511" class="indexterm"/> Hadoop. Though there was the new 1.0 version of Apache Mahout available at the time of writing this book, we will use version 0.9 (the latest stable version) in all the examples. The operating system used is Ubuntu 12.04 desktop 32-bit version.</p><p>Following are the dependencies and key requirements for installing Apache Mahout:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">JDK (1.6 or above; we will use 1.7 u9 version for the examples throughout this book)</li><li class="listitem" style="list-style-type: disc">Maven (2.2 or above; we will use 3.0.4 for the examples throughout this book)</li><li class="listitem" style="list-style-type: disc">Apache Hadoop (2.0; not mandatory as Mahout can be run locally)</li><li class="listitem" style="list-style-type: disc">Apache Mahout (0.9 distribution)</li><li class="listitem" style="list-style-type: disc">Development environment—Eclipse IDE (Luna)</li></ul></div><p>In <a class="link" href="ch03.html" title="Chapter 3. An Introduction to Hadoop's Architecture and Ecosystem">Chapter 3</a>, <span class="emphasis"><em>An Introduction to Hadoop's Architecture and Ecosystem</em></span>, we have seen how Apache Hadoop 2.0 single node installation is done along with the required prerequisites like Java.</p><p>In this chapter, we will cover <a id="id512" class="indexterm"/>the setting up of Maven, Eclipse for the development environment, and configuring Apache Mahout to run on and off Hadoop. As the considered platform and related frameworks are open sources, we will use the VirtualBox machine emulator<a id="id513" class="indexterm"/> hosted by the Windows 7 Professional edition.</p><p>As you may recollect, Hadoop cannot run as a root user, and hence we have a user created for this purpose—<code class="literal">practical-ml</code> to install and run everything.</p><div class="section" title="Setting up Maven"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec49"/>Setting up Maven</h3></div></div></div><p>It is recommended that <a id="id514" class="indexterm"/>Maven is used to get the required Mahout jars, and it gets easy to switch to any newer versions easily with Mahout. In the absence of Maven, downloading the dependencies will get more complicated. For more details on specific features of Maven and its utility in application development, refer to <a class="ulink" href="https://www.packtpub.com/application-development/apache-maven-3-cookbook">https://www.packtpub.com/application-development/apache-maven-3-cookbook</a>.</p><p>Maven version 3.0.4 can be downloaded from one of the mirrors of the Apache website. The following command can be used for this purpose:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>wget http://it.apache.contactlab.it/maven/maven-3/3.0.4/binaries/apachemaven-3.0.4-bin.tar.gz</strong></span>
</pre></div><p>To manually install Maven, perform the following instructions:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Extract the distribution archive that is, <code class="literal">apache-maven-3.0.4-bin.tar.gz</code> to the directory you wish to install Maven 3.0.4.</li><li class="listitem">With these instructions, the <code class="literal">/usr/local/apache-maven</code> path will be chosen. An <code class="literal">apache-maven-3.0.4</code> subdirectory will be created from the archive.</li><li class="listitem">The following lines need to be appended to the <code class="literal">.bashrc</code> file:<div class="informalexample"><pre class="programlisting">export M2_HOME=/usr/local/apache-maven-3.0.4
export M2=$M2_HOME/bin
export PATH=$M2:$PATH
export JAVA_HOME=$HOME/programs/jdk</pre></div></li></ol></div><p>
<code class="literal">JAVA_HOME</code> should point to a location where the JDK is installed. For example, export <code class="literal">JAVA_HOME=/usr/java/jdk1.7. $JAVA_HOME/bin</code> is in your <code class="literal">PATH</code> environment variable. The <code class="literal">PATH</code> variable is set during the Java installation. This should be verified.</p><p>We can now check for the successful installation of Maven by running the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mvn –version</strong></span>
</pre></div><p>In case there are any <a id="id515" class="indexterm"/>proxy settings, we will have to explicitly update the proxy settings in the <code class="literal">settings.xml</code> file, which is in the <code class="literal">conf</code> folder of Maven installation.</p></div><div class="section" title="Setting-up Apache Mahout using Eclipse IDE"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec50"/>Setting-up Apache Mahout using Eclipse IDE</h3></div></div></div><p>The procedure detailed next <a id="id516" class="indexterm"/>covers the steps to set up the<a id="id517" class="indexterm"/> Mahout environment, code base, accessing of examples, running, debugging, and testing them using Eclipse IDE. This is the recommended way to set up and is the simplest way to set up Apache Mahout for the development teams.</p><p>Execute the following steps to get the Apache Mahout tar, untar it and navigate to the installation.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Set up Eclipse IDE.<p>The latest version of Eclipse<a id="id518" class="indexterm"/> can be downloaded from the following link:</p><p>
<a class="ulink" href="https://www.eclipse.org/downloads/">https://www.eclipse.org/downloads/</a>
</p></li><li class="listitem">Download Mahout Distribution from the direct link using the command here:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget -c http://archive.apache.org/dist/mahout/0.9/mahout-distribution-0.9.tar.gz</strong></span>
</pre></div></li><li class="listitem">Extract the archive from it using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tar zxf mahout-distribution-0.9.tar.gz</strong></span>
</pre></div></li><li class="listitem">Convert the project into an Eclipse project:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd mahout-distribution-0.9</strong></span>
<span class="strong"><strong>$ mvn eclipse: eclipse</strong></span>
</pre></div><p>The earlier command builds the Eclipse project.</p></li><li class="listitem">Set the <code class="literal">M2_REPO</code> classpath variable to point to the local repository path. The following command adds all the Maven jars to the Eclipse classpath:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mvn -Declipse.workspace= eclipse:add-maven-repo</strong></span>
</pre></div></li><li class="listitem">Now, let's <a id="id519" class="indexterm"/>import the Eclipse Mahout <a id="id520" class="indexterm"/>projects.<p>Navigate from the menu, <span class="strong"><strong>File</strong></span> | <span class="strong"><strong>Import</strong></span> | <span class="strong"><strong>General</strong></span> | <span class="strong"><strong>Existing Projects</strong></span> into <span class="strong"><strong>Workspace</strong></span>.</p><div class="mediaobject"><img src="graphics/B03980_04_04.jpg" alt="Setting-up Apache Mahout using Eclipse IDE"/></div></li></ol></div></div><div class="section" title="Setting up Apache Mahout without Eclipse"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec51"/>Setting up Apache Mahout without Eclipse</h3></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download <a id="id521" class="indexterm"/>Mahout Distribution from the direct link using the command here:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget -c http://archive.apache.org/dist/mahout/0.9/mahout-distribution-0.9.tar.gz</strong></span>
</pre></div></li><li class="listitem">Extract the Mahout distribution to the <code class="literal">/usr/local</code> folder:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd /usr/local</strong></span>
<span class="strong"><strong>$ sudo tar xzf mahout-distribution-0.9.tar.gz</strong></span>
<span class="strong"><strong>$ sudo mv mahout-distribution-0.9.tar.gz mahout</strong></span>
<span class="strong"><strong>$ sudo chown –R practical-ml:hadoop mahout</strong></span>
</pre></div></li><li class="listitem">Set the Java, Maven, and Mahout paths in the <code class="literal">.bashrc</code> file.<p>Open the <code class="literal">.bashrc</code> file using the command here:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gedit ~/.bashrc</strong></span>
</pre></div><p>Add the following content to the file:</p><div class="informalexample"><pre class="programlisting">export MAHOUT_HOME = /usr/local/mahout
path=$path:$MAHOUT_HOME/bin
export M2_HOME=/usr/local/maven
export PATH=$M2:$PATH
export M2=$M2_HOME/bin
PATH=$PATH:$JAVA_HOME/bin;$M2_HOME/bin</pre></div></li><li class="listitem">To run Mahout in the local mode (this means in the standalone mode where there is no need for Hadoop, and the algorithms will not run in parallel or MapReduce mode).<p>Set the local mode to true using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$MAHOUT_LOCAL=true</strong></span>
</pre></div><p>This will force Mahout to not look for the Hadoop configurations in <code class="literal">$HADOOP_CONF_DIR</code>.</p><p>
<code class="literal">MAHOUT_LOCAL</code> is set, so we don't add <code class="literal">HADOOP_CONF_DIR</code> to the classpath.</p></li></ol></div><p>There is an alternative to run<a id="id522" class="indexterm"/> Mahout on Hadoop. Firstly, ensure Hadoop 2.x is installed and configured successfully. Then, follow these instructions:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Set <code class="literal">$HADOOP_HOME</code>, <code class="literal">$HADOOP_CONF_DIR</code> are set and added to <code class="literal">$PATH</code>.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export HADOOP_CONF_DIR=$HADOOP_HOME/conf</strong></span>
</pre></div><p>The above sets the mode in which Hadoop is run (for example, in <code class="literal">core-site.xml</code>, <code class="literal">hdfs-site.xml</code>, <code class="literal">mapred-site.xml</code>, and so on.)</p></li><li class="listitem">Now, launch the Hadoop instance using the command here:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$HADOOP_HOME/bin/start-all.sh</strong></span>
</pre></div></li><li class="listitem">Check <code class="literal">http://localhost:50030</code>, and <code class="literal">http://localhost:50070</code> URLs to confirm whether <a id="id523" class="indexterm"/>Hadoop is up and running.</li><li class="listitem">Build Apache Mahout using Maven by running the following Maven command from the Mahout directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>/usr/local/mahout$ mvn install</strong></span>
</pre></div></li></ol></div><p>The following output is seen on a successful install:</p><div class="mediaobject"><img src="graphics/B03980_04_05.jpg" alt="Setting up Apache Mahout without Eclipse"/></div></div></div><div class="section" title="Mahout Packages"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec56"/>Mahout Packages</h2></div></div></div><p>The following figure depicts<a id="id524" class="indexterm"/> different packages in Mahout that provide some out-of-box support for several Machine learning algorithms. At the core, the modules are the utilities, math vectors, collections, and Hadoop with MapReduce for the parallel processing and the file system for distributed storage.</p><p>Moreover, over the core modules are the <a id="id525" class="indexterm"/>Machine learning packages as listed here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Classification</li><li class="listitem" style="list-style-type: disc">Clustering</li><li class="listitem" style="list-style-type: disc">Evolutionary Algorithms</li><li class="listitem" style="list-style-type: disc">Recommenders</li><li class="listitem" style="list-style-type: disc">Regression</li><li class="listitem" style="list-style-type: disc">FPM</li><li class="listitem" style="list-style-type: disc">Dimension Reduction<div class="mediaobject"><img src="graphics/B03980_04_06.jpg" alt="Mahout Packages"/></div></li></ul></div><p>More details are <a id="id526" class="indexterm"/>covered on the previous packages in detail in the chapters to follow, with example implementations using each of the packages for an identified problem.</p></div><div class="section" title="Implementing vectors in Mahout"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec57"/>Implementing vectors in Mahout</h2></div></div></div><p>As we understand, to <a id="id527" class="indexterm"/>demonstrate most of the Machine learning<a id="id528" class="indexterm"/> algorithm implementations in Mahout, we need the data in classic Mahout dataset format. At the core, the code for this is primary to use some Mahout ready-to-use scripts with some minor changes in the settings. Given below is the standard process:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create sequence files from the raw text files.<p>
<span class="strong"><strong>Sequence files</strong></span> <a id="id529" class="indexterm"/>are predominantly a binary encoding of the key/value pair representation of data. The attributes given next are the key header elements that represent metadata details:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Version</li><li class="listitem" style="list-style-type: disc">Key name</li><li class="listitem" style="list-style-type: disc">Value name</li><li class="listitem" style="list-style-type: disc">Compression</li></ul></div></li><li class="listitem">Generate vectors from the sequence files. More on the actual commands to generate sequence files is covered in the following chapters while demonstrating the implementation for each of the identified Machine learning algorithms.</li><li class="listitem">Running functions on these working vectors</li></ol></div><p>There are different types of vector implementations in Mahout, and the definitions hold good in general as well.</p><div class="mediaobject"><img src="graphics/B03980_04_07.jpg" alt="Implementing vectors in Mahout"/></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Dense Vectors</strong></span>: These <a id="id530" class="indexterm"/>vectors are usually an array of doubles, and the size of this vector is the same as the number of features in the dataset. Since all the entries are preallocated irrespective of a zero value, these vectors are called dense vectors.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Sparse Vectors</strong></span>: These<a id="id531" class="indexterm"/> vectors are arrays of vectors and are represented only with non-zero or null values. With sparse vectors, there are two subcategories: the random-access and sequential-access sparse vectors.<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Random Access Sparse Vectors</strong></span>: Random access sparse vectors are the HashMap<a id="id532" class="indexterm"/> representations <a id="id533" class="indexterm"/>where the key is an integer value, and the value is a double value. At any given point in time, a value can be accessed by passing in the given key.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Sequential Access Sparse Vectors</strong></span>: These vectors are nothing but a set of two arrays <a id="id534" class="indexterm"/>where the first array<a id="id535" class="indexterm"/> is the array of keys (the integers), and the second array is an array of values (the doubles). These vectors are optimized for linear reads, unlike the random access sparse vectors. Again, the storage is done for only non-zero values.</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>For a detailed understanding of working with Apache Mahout, refer to the Packt Publication for Apache Mahout titled <span class="emphasis"><em>Apache Mahout Cookbook</em></span>.</p></div></div></li></ul></div><p>While this section covers a framework that is built to work with Hadoop with small configuration changes, in the next section, we cover the powerful and highly adopted option in the market—R. Hadoop provides explicit adapters to have the R programs work in the MapReduce model, which is covered next.</p></div></div>
<div class="section" title="R"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec25"/>R</h1></div></div></div><p>R is a<a id="id536" class="indexterm"/> language for data analysis and is used as an environment that is a primary driver in the field of Machine learning, statistical computing, and data mining and provides a comprehensive platform for basic and advanced visualizations or graphics. Today, R is a basic skill that almost all data scientists or would-be data scientists have or <span class="emphasis"><em>must</em></span> learn.</p><p>R is primarily a GNU project known to be similar to the S language that was initially developed at Bell Laboratories (formerly known as AT&amp;T and now, Lucent Technologies) by John Chambers and team. The initial goal for S was to support all statistical functions and was widely used by hard-core statisticians.</p><p>R comes with a wide range of open source packages that can be downloaded and configured free of cost, and are installed or loaded on a need basis into the R environment. These packages provide out-of-box support for a wide variety of statistical techniques that include linear and non-linear modeling, time-series analysis, classification, clustering, and so on.</p><p>Along with these, the highly extensible graphical functions are available. The support for these advanced graphical functions has been a primary differentiator for R as the output is known for its publication quality plots. In addition to these, R also supports many open source graphical libraries and visualization tools that are both open source and commercial in nature.</p><p>Though, at the core, R is not meant to work in a distributed environment or run the algorithms in a parallel mode, there are several extensions available (both open source and commercial) that make R more scalable and support large dataset. In this chapter, we will cover how R can be integrated with Apache Hadoop, and thus can run and leverage the MapReduce capabilities.</p><p>Most importantly, R is free software that is widely adopted and has many committers and support groups constantly working on retaining its high relevance in the field of data science.</p><p>Some of the key <a id="id537" class="indexterm"/>capabilities that R supports today are listed here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The ability to effectively manage and store data that the models operate on</li><li class="listitem" style="list-style-type: disc">Facilitating some core suite of functions for calculations on arrays, vectors, and matrices among others</li><li class="listitem" style="list-style-type: disc">Several out-of-box Machine learning functions that can be loaded on demand and help implement data science projects with ease</li><li class="listitem" style="list-style-type: disc">Advanced and sophisticated graphical functions that can be used with ease and help to produce valuable dashboards for business owners</li><li class="listitem" style="list-style-type: disc">A wide and active community of adopters and committers that has developed rapidly with extensions via a large collection of packages</li><li class="listitem" style="list-style-type: disc">R is considered as a platform that supports newly developing methods of interactive data analysis</li></ul></div><div class="section" title="Installing and setting up R"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec58"/>Installing and setting up R</h2></div></div></div><p>For all the examples<a id="id538" class="indexterm"/> in this book, we will use the stable version 2.15.1 of R and the <a id="id539" class="indexterm"/>CRAN references for all<a id="id540" class="indexterm"/> the latest R packages.</p><p>Refer to the <a class="ulink" href="https://cran.r-project.org/bin/windows/base/old/2.15.1/">https://cran.r-project.org/bin/windows/base/old/2.15.1/</a> link to download R for Windows.</p><p>A detailed installation process is covered at <a class="ulink" href="https://cran.r-project.org/doc/manuals/R-admin.html#Top">https://cran.r-project.org/doc/manuals/R-admin.html#Top</a>.</p><p>We can use R with the R GUI or the IDE RStudio. Following are the screenshots of the R interface that the users can see post a successful installation of the R GUI and R IDE, and the RStudio.</p><div class="mediaobject"><img src="graphics/B03980_04_08.jpg" alt="Installing and setting up R"/></div><p>We will need to set the CRAN mirror path to be able to access and load the required R packages by navigating from the menu path <span class="strong"><strong>Packages</strong></span> | <span class="strong"><strong>Set CRAN mirror</strong></span>
</p><div class="mediaobject"><img src="graphics/B03980_04_09.jpg" alt="Installing and setting up R"/></div><p>The following screenshot<a id="id541" class="indexterm"/> shows a list of mirror sites from which the <a id="id542" class="indexterm"/>developer can choose the most appropriate one:</p><div class="mediaobject"><img src="graphics/B03980_04_10.jpg" alt="Installing and setting up R"/></div><p>The R Editor can be used<a id="id543" class="indexterm"/> to write any advanced operations, and the results can be seen on the <a id="id544" class="indexterm"/>console as shown here:</p><div class="mediaobject"><img src="graphics/B03980_04_11.jpg" alt="Installing and setting up R"/></div><p>Following is a screenshot of a graphical plot:</p><div class="mediaobject"><img src="graphics/B03980_04_12.jpg" alt="Installing and setting up R"/></div></div><div class="section" title="Integrating R with Apache Hadoop"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec59"/>Integrating R with Apache Hadoop</h2></div></div></div><p>So far, we have seen <a id="id545" class="indexterm"/>Apache Hadoop and its core components, HDFS and YARN (MapReduce 2.0), and R. There are three different ways in which we can look at integrating R with Hadoop, and hence the support for large-scale Machine learning.</p><div class="section" title="Approach 1 – Using R and Streaming APIs in Hadoop"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec52"/>Approach 1 – Using R and Streaming APIs in Hadoop</h3></div></div></div><p>To integrate an R<a id="id546" class="indexterm"/> function with Hadoop and see it running in a MapReduce mode, Hadoop supports Streaming APIs for R. These Streaming APIs primarily help in running any script that can access and operate with standard I/O in a MapReduce mode. So in the case of R, there wouldn't be any explicit client side integration done with R. The following is an example of R and streaming:</p><div class="informalexample"><pre class="programlisting">$ ${HADOOP_HOME}/bin/Hadoop jar
${HADOOP_HOME}/contrib/streaming/*.jar \
-inputformat
org.apache.hadoop.mapred.TextInputFormat \
-input input_data.txt \
-output \
-mapper /home/tst/src/map.R \
-reducer /home/tst/src/reduce.R \
-file /home/tst/src/map.R \
-file /home/tst/src/reduce.R</pre></div></div><div class="section" title="Approach 2 – Using the Rhipe package of R"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec53"/>Approach 2 – Using the Rhipe package of R</h3></div></div></div><p>There is a package in<a id="id547" class="indexterm"/> R called Rhipe that allows running a MapReduce job within R. To use this way of implementing R on Hadoop; there are some prerequisites:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">R needs to be installed on each DataNode in the Hadoop Cluster</li><li class="listitem" style="list-style-type: disc">Protocol Buffers will be installed and available on each DataNode (for more information on Protocol Buffers<a id="id548" class="indexterm"/> refer to <a class="ulink" href="http://wiki.apache.org/hadoop/ProtocolBuffers">http://wiki.apache.org/hadoop/ProtocolBuffers</a>)</li><li class="listitem" style="list-style-type: disc">Rhipe should be available on each data node</li></ul></div><p>The following is a <a id="id549" class="indexterm"/>sample format for using the <code class="literal">Rhipe</code> library in R to implement MapReduce:</p><div class="informalexample"><pre class="programlisting">library(Rhipe)
rhinit(TRUE, TRUE);
map&lt;-expression ( {lapply (map.values, function(mapper)…)})
reduce&lt;-expression(
pre = {…},
reduce = {…},
post = {…},
)
x &lt;- rhmr(map=map, reduce=reduce,
 ifolder=inputPath,
 ofolder=outputPath,
 inout=c('text', 'text'),
 jobname='test name'))
rhex(x)</pre></div></div><div class="section" title="Approach 3 – Using RHadoop"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec54"/>Approach 3 – Using RHadoop</h3></div></div></div><p>RHadoop, very similar to<a id="id550" class="indexterm"/> Rhipe, facilitates running R functions in a MapReduce mode. It is an open source library built by Revolution Analytics. Following are some packages, which are a part of the RHadoop library:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>plyrmr</strong></span>: This is a package<a id="id551" class="indexterm"/> that provides functions for common data manipulation requirements for large datasets running on Hadoop</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>rmr</strong></span>: This is a package<a id="id552" class="indexterm"/> that has a collection of functions that integrate R and Hadoop</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>rdfs</strong></span>: This is a package <a id="id553" class="indexterm"/>with functions that help interface R and HDFS</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>rhbase</strong></span>: This is a<a id="id554" class="indexterm"/> package with functions that help interface R and HBase</li></ul></div><p>The following is an example that uses the rmr package and demonstrates steps to integrate R and Hadoop using the functions from this package:</p><div class="informalexample"><pre class="programlisting">library(rmr)
maplogic&lt;-function(k,v) { …}
reducelogic&lt;-function(k,vv) { …}
mapreduce( input ="data.txt",
output="output",
textinputformat =rawtextinputformat,
map = maplogic,
reduce=reducelogic
)</pre></div></div><div class="section" title="Summary of R/Hadoop integration approaches"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec55"/>Summary of R/Hadoop integration approaches</h3></div></div></div><p>In summary, all of the<a id="id555" class="indexterm"/> previous three approaches yield results <a id="id556" class="indexterm"/>and facilitate integrating R and Hadoop. They help in scaling R to operate on the large-scale data that will help with HDFS. Each of these approaches has pros and cons. Here is a summary of conclusions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Hadoop Streaming API is the simplest of all the approaches as there are no complications regarding installation and setup requirements</li><li class="listitem" style="list-style-type: disc">Both Rhipe and RHadoop require some effort to setup R and related packages on the Hadoop cluster</li><li class="listitem" style="list-style-type: disc">Regarding implementation approach, Streaming API is more of a command line map, and reduce functions are inputs to the function, whereas both Rhipe and RHadoop allow developers to define and call custom MapReduce functions within R</li><li class="listitem" style="list-style-type: disc">In case of Hadoop Streaming API, there is no client side integration required, whereas both Rhipe and RHadoop require the client side integration</li><li class="listitem" style="list-style-type: disc">The alternatives to scaling Machine learning are Apache Mahout, Apache Hive, and some commercial versions of R from Revolution Analytics, Segue framework, and others</li></ul></div></div><div class="section" title="Implementing in R (using examples)"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec56"/>Implementing in R (using examples)</h3></div></div></div><p>In this section, we will briefly cover some implementation aspects of R, and focus on learning the syntax and understanding some core functions and its usage.</p><div class="section" title="R Expressions"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec08"/>R Expressions</h4></div></div></div><p>R can be used as a simple<a id="id557" class="indexterm"/> math calculator; here are some basic ways of using it. Here is a trace of what is seen on the R console:</p><div class="informalexample"><pre class="programlisting">&gt; 1+1
[1] 2
&gt; "Welcome to R!"
[1] "Welcome to R!"
&gt; 6*7
[1] 42
&gt; 10&lt;22
[1] TRUE
&gt; 2+7==5
[1] FALSE</pre></div><div class="section" title="Assignments"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl5sec04"/>Assignments</h5></div></div></div><p>This is used to assign value to a <a id="id558" class="indexterm"/>variable and apply some operations to this variable:</p><p>Case 1: Assigning a numeric value:</p><div class="informalexample"><pre class="programlisting">&gt; x&lt;-24
&gt; x/2
[1] 12</pre></div><p>Case 2: Assigning a string literal:</p><div class="informalexample"><pre class="programlisting">&gt; x &lt;- "Try R!"
[1] "Try R!"
&gt; x
[1] " Try R!"</pre></div><p>Case 3: Assigning a logical value:</p><div class="informalexample"><pre class="programlisting">&gt; x &lt;- TRUE
[1] TRUE</pre></div></div><div class="section" title="Functions"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl5sec05"/>Functions</h5></div></div></div><p>There are many <a id="id559" class="indexterm"/>out-of-box functions and to invoke a function in R, we should provide the function name and pass required arguments. Here are some examples of functions and the results, as seen in the R Console:</p><div class="informalexample"><pre class="programlisting">&gt; sum(4,3,5,7)
[1] 19
&gt; rep("Fun!", times=3)
[1] " Fun!" "Fun!" "Fun!"
&gt; sqrt(81)
[1] 9</pre></div><p>Here is the command to get help for a function in R:</p><div class="informalexample"><pre class="programlisting">&gt; help(sum)
sum package: base R Documentation

Sum of Vector Elements

Description:

     'sum' returns the sum of all the values present in its arguments.

Usage:

     sum(..., na.rm = FALSE)</pre></div></div></div><div class="section" title="R Vectors"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec09"/>R Vectors</h4></div></div></div><p>A vector is a simple list of <a id="id560" class="indexterm"/>values by definition that forms the core of R data types. Many of the Machine learning functions leverage these.</p><p>Here are some key functions with their usage context:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Function/Syntax</p>
</th><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th><th style="text-align: left" valign="bottom">
<p>Example</p>
</th><th style="text-align: left" valign="bottom">
<p>Output on R Console</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">m:n</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Outputs numbers from <code class="literal">m</code> to <code class="literal">n</code> increment by 1</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal"> &gt; 5:9</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">[1] 5 6 7 8 9</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">seq(m,n)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Outputs numbers from <code class="literal">m</code> to <code class="literal">n</code> increment by 1</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal"> &gt; seq(5,9)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">[1] 5 6 7 8 9</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<code class="literal">seq(m,n, i)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>Outputs numbers from <code class="literal">m</code> to <code class="literal">n</code> increment by <code class="literal">i</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal"> &gt; seq(1,3,0.5)</code>
</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">[1] 1 1.5 2 2.5 3</code>
</p>
</td></tr></tbody></table></div><div class="section" title="Assigning, accessing, and manipulating vectors"><div class="titlepage"><div><div><h5 class="title"><a id="ch04lvl5sec06"/>Assigning, accessing, and manipulating vectors</h5></div></div></div><p>The following <a id="id561" class="indexterm"/>table has<a id="id562" class="indexterm"/> examples for creating, accessing, and <a id="id563" class="indexterm"/>manipulating matrices in R:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th><th style="text-align: left" valign="bottom">
<p>Example</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Creating a vector of literals</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; sentence &lt;- c('practical', 'machine', 'learning')</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Accessing the third value of the vectors</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; sentence[3]</code>
</p>
<p>
<code class="literal">[1] "learning."</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Updating a value in the vector</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; sentence[1] &lt;- "implementing"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Adding a new value to the vector</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; sentence[4] &lt;- "algorithms"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Getting values for the given indices</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; sentence[c(1,3)]</code>
</p>
<p>
<code class="literal">[1] "implementing" "learning"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Getting values for range of indices</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; sentence[2:4]</code>
</p>
<p>
<code class="literal">[1] "machine" "learning" "algorithms"</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Adding a range of new values</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; sentence[5:7] &lt;- c('for','large','datasets')</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Incrementing <a id="id564" class="indexterm"/>vector values by 1</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; a &lt;- c(1, 2, 3)</code>
</p>
<p>
<code class="literal">&gt; a + 1</code>
</p>
<p>
<code class="literal">[1] 2 3 4</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Dividing each value in vector by a value</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; a / 2</code>
</p>
<p>
<code class="literal">[1] 0.5 1.0 1.5</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Multiplying each <a id="id565" class="indexterm"/>value of the vector by a value</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; a*2</code>
</p>
<p>
<code class="literal">[1] 2 4 6</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Adding two<a id="id566" class="indexterm"/> vectors</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; b &lt;- c(4, 5, 6)</code>
</p>
<p>
<code class="literal">&gt; a + b</code>
</p>
<p>
<code class="literal">[1] 5 7 9</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Comparing two vectors</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; a == c(1, 99, 3)</code>
</p>
<p>
<code class="literal">[1]  TRUE FALSE  TRUE</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Applying a function on each value of the vector</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; sqrt(a)</code>
</p>
<p>
<code class="literal">[1] 1.000000 1.414214 1.732051</code>
</p>
</td></tr></tbody></table></div></div></div><div class="section" title="R Matrices"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec10"/>R Matrices</h4></div></div></div><p>Matrices are two-dimensional<a id="id567" class="indexterm"/> vectors that have rows and columns. The following table has examples for creating, accessing, and manipulating matrices in R:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th><th style="text-align: left" valign="bottom">
<p>Example</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Creating a 3 X 4 matrix with values defaulted to zero</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; matrix(0, 3, 4)</code>
</p>
<p>
<code class="literal">     [,1] [,2] [,3] [,4]</code>
</p>
<p>
<code class="literal">[1,]    0    0    0    0</code>
</p>
<p>
<code class="literal">[2,]    0    0    0    0</code>
</p>
<p>
<code class="literal">[3,]    0    0    0    0</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Initializing a matrix with a range of values</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; a &lt;- 1:12</code>
</p>
<p>
<code class="literal">&gt; m &lt;- matrix(a, 3, 4)</code>
</p>
<p>
<code class="literal">     [,1] [,2] [,3] [,4]</code>
</p>
<p>
<code class="literal">[1,]    1    4    7   10</code>
</p>
<p>
<code class="literal">[2,]    2    5    8   11</code>
</p>
<p>
<code class="literal">[3,]    3    6    9   12</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Accessing a value from the matrix</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; m[2, 3]</code>
</p>
<p>
<code class="literal">[1] 8</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Assigning a value to a position of choice in a matrix</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; m[1, 4] &lt;- 0</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Retrieving an array of the entire row or a column of choice</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; m[2,]</code>
</p>
<p>
<code class="literal">[1] 2 5 8 11</code>
</p>
<p>
<code class="literal">&gt; m[3,]</code>
</p>
<p>
<code class="literal">[1] 7 8 9</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Retrieving a<a id="id568" class="indexterm"/> subset of the bigger matrix</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; m[, 2:4]</code>
</p>
<p>
<code class="literal">     [,1] [,2] [,3]</code>
</p>
<p>
<code class="literal">[1,]    4    7    10</code>
</p>
<p>
<code class="literal">[2,]    5    8    11</code>
</p>
</td></tr></tbody></table></div></div><div class="section" title="R Factors"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec11"/>R Factors</h4></div></div></div><p>In data analytics and Machine <a id="id569" class="indexterm"/>learning, it is common to group or categorize data. For example, a good or a bad customer. R's <code class="literal">factor</code> data type is used to track the categorized data. All that needs to be done is defining a vector of categories and passing it as a parameter to the <code class="literal">factor</code> function.</p><p>The following example demonstrates creation and assignment of categories using <code class="literal">factors</code>:</p><div class="informalexample"><pre class="programlisting">&gt; ornaments &lt;- c('ring', 'chain', 'bangle', 'anklet', 'nosepin', 'earring', 'ring', 'anklet')
&gt; ornamenttypes &lt;- factor(ornaments)
&gt; print(ornamenttypes)
[1] ring chain bangle anklet nosepin earring
Levels: anklet bangle chain earring nosepin ring</pre></div><p>Each of the defined categories usually has an integer value associated with the literal. Passing the <code class="literal">factor</code> to <a id="id570" class="indexterm"/>the <code class="literal">as.integer</code> function will give the integer equivalents, as shown here:</p><div class="informalexample"><pre class="programlisting">&gt; as.integer(ornamenttypes)
[1] 6 3 2 1 5 4 6 1</pre></div></div><div class="section" title="R Data Frames"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec12"/>R Data Frames</h4></div></div></div><p>Data frames relate to the concept of <a id="id571" class="indexterm"/>database tables. This data type is very powerful in R, and it helps tie different related attributes of a dataset together. For example, the number of items purchased has a relationship with the total bill value and the overall applicable discount. There should be a way to link these attributes, and data frames help to do so:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Purpose</p>
</th><th style="text-align: left" valign="bottom">
<p>Example</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Creating a data frame and checking the values</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; purchase &lt;- data.frame(totalbill, noitems, discount</code>
</p>
<p>
<code class="literal">&gt; print(purchase)</code>
</p>
<p>
<code class="literal">  totalbill noitems discount</code>
</p>
<p>
<code class="literal">1     300    5      10</code>
</p>
<p>
<code class="literal">2     200    3       7.5</code>
</p>
<p>
<code class="literal">3     100    1       5</code>
</p>
<p>
<code class="literal">)</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Accessing the data of the data frame using indexes or labels</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; purchase[[2]]</code>
</p>
<p>
<code class="literal">[1]  5 3 1</code>
</p>
<p>
<code class="literal">&gt; purchase[["totalbill"]]</code>
</p>
<p>
<code class="literal">[1] 300 200 100</code>
</p>
<p>
<code class="literal">&gt; purchase$discount</code>
</p>
<p>
<code class="literal">[1]  10 7.5 5</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Loading data <a id="id572" class="indexterm"/>frames with the data from CSV files</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; list.files()</code>
</p>
<p>
<code class="literal">[1] "monthlypurchases.csv"</code>
</p>
<p>
<code class="literal">&gt; read.csv("monthlypurchases.csv")</code>
</p>
<p>
<code class="literal">         Amount    Items Discount</code>
</p>
<p>
<code class="literal">1 2500             35    15</code>
</p>
<p>
<code class="literal">2 5464             42    25</code>
</p>
<p>
<code class="literal">3 1245             8     6</code>
</p>
</td></tr></tbody></table></div></div><div class="section" title="R Statistical frameworks"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec13"/>R Statistical frameworks</h4></div></div></div><p>R supports a bunch of statistical <a id="id573" class="indexterm"/>out-of-box functions that help statisticians explain the data. Some of the functions with examples are shown in the following table:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Function</p>
</th><th style="text-align: left" valign="bottom">
<p>Example</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Mean</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">limbs &lt;- c(4, 3, 4, 3, 2, 4, 4, 4)</code>
</p>
<p>
<code class="literal">names(limbs) &lt;- c('One-Eye', 'Peg-Leg', 'Smitty', 'Hook', 'Scooter', 'Dan', 'Mikey', 'Blackbeard')</code>
</p>
<p>
<code class="literal">&gt; mean(limbs)</code>
</p>
<p>
<code class="literal">[1] 3.5</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Median</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; median(limbs)</code>
</p>
<p>
<code class="literal">[1] 4</code>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Standard deviation</p>
</td><td style="text-align: left" valign="top">
<p>
<code class="literal">&gt; pounds &lt;- c(45000, 50000, 35000, 40000, 35000, 45000, 10000, 15000)</code>
</p>
<p>
<code class="literal">&gt; deviation &lt;- sd(pounds)</code>
</p>
</td></tr></tbody></table></div><p>Each piece of the contained R code is saved for a run in a file with the <code class="literal">.R</code> extension.</p><p>In this section, we have seen how R can be set up and how some basic functions and data types can be used. There are many Machine learning specific packages that we will be exploring in the following chapters.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>For a detailed understanding of working with R for Machine learning, refer to the Packt Publication for R titled <span class="emphasis"><em>Machine learning with R</em></span>.</p></div></div></div></div></div></div>
<div class="section" title="Julia"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec26"/>Julia</h1></div></div></div><p>Julia, in<a id="id574" class="indexterm"/> recent times, has gained much popularity and adoption in the Machine learning and data science fields as a high-performance alternative to Python. Julia is a dynamic programming language that is built to support distributed and parallel computing, thus known to be convenient and fast.</p><p>Performance in Julia is a result of the JIT compiler and type interfacing feature. Also, unlike other numeric programming languages, Julia does not enforce vectorization of values. Similar to R, MATLAB, and Python, Julia provides ease and expressiveness for high-level numerical computing.</p><p>Following are some key<a id="id575" class="indexterm"/> characteristics of Julia:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The core APIs and mathematical primitive operations are written in Julia</li><li class="listitem" style="list-style-type: disc">It consists rich types for constructing and describing objects</li><li class="listitem" style="list-style-type: disc">Julia supports for multiple dispatch that enable using functions across many combinations of arguments</li><li class="listitem" style="list-style-type: disc">It facilitates the automation of specialized code generation for different argument types</li><li class="listitem" style="list-style-type: disc">Proven performance is on par with statically compiled languages like C</li><li class="listitem" style="list-style-type: disc">It is a free and open source programming language (MIT licensed)</li><li class="listitem" style="list-style-type: disc">User-defined types are as fast and compact as built-ins</li><li class="listitem" style="list-style-type: disc">It does not enforce or require vectorization code for performance</li><li class="listitem" style="list-style-type: disc">It is designed for distributed and parallel computation</li><li class="listitem" style="list-style-type: disc">Julia comes with co-routines and lightweight threading</li><li class="listitem" style="list-style-type: disc">Julia supports the ability to invoke the C functions directly</li><li class="listitem" style="list-style-type: disc">Shell-like capabilities for managing processes</li><li class="listitem" style="list-style-type: disc">It provides <a id="id576" class="indexterm"/>Lisp-like macros</li></ul></div><div class="section" title="Installing and setting up Julia"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec60"/>Installing and setting up Julia</h2></div></div></div><p>We will be using <a id="id577" class="indexterm"/>Julia's latest version<a id="id578" class="indexterm"/> that was available at the time of writing this book—v 0.3.4.</p><p>Julia programs can be built and executed by:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Using Julia command line</li><li class="listitem" style="list-style-type: disc">Using Juno—an IDE for Julia</li><li class="listitem" style="list-style-type: disc">Using a <a id="id579" class="indexterm"/>ready-to-use environment at <a class="ulink" href="https://juliabox.org/">https://juliabox.org/</a>, where the Julia environment can be accessed using a browser</li></ul></div><div class="section" title="Downloading and using the command line version of Julia"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec57"/>Downloading and using the command line version of Julia</h3></div></div></div><p>Use the link <a class="ulink" href="http://julialang.org/downloads/">http://julialang.org/downloads/</a> to download<a id="id580" class="indexterm"/> the<a id="id581" class="indexterm"/> required<a id="id582" class="indexterm"/> Julia version.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download the <a id="id583" class="indexterm"/>appropriate executable and run it.<div class="mediaobject"><img src="graphics/B03980_04_13.jpg" alt="Downloading and using the command line version of Julia"/></div></li><li class="listitem">After the successful installation, open the Julia console and Julia is ready to use.<div class="mediaobject"><img src="graphics/B03980_04_14.jpg" alt="Downloading and using the command line version of Julia"/></div></li></ol></div></div><div class="section" title="Using Juno IDE for running Julia"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec58"/>Using Juno IDE for running Julia</h3></div></div></div><p>Juno IDE<a id="id584" class="indexterm"/> makes developing Julia code easy. Download the latest Juno IDE <a id="id585" class="indexterm"/>version from <a class="ulink" href="http://junolab.org/docs/install.html">http://junolab.org/docs/install.html</a>.</p><p>Juno has Julia's core APIs and functions that help in simplifying the development process. Following is a screenshot of how Juno can be used:</p><div class="mediaobject"><img src="graphics/B03980_04_15.jpg" alt="Using Juno IDE for running Julia"/></div></div><div class="section" title="Using Julia via the browser"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec59"/>Using Julia via the browser</h3></div></div></div><p>Using this option does not <a id="id586" class="indexterm"/>require any installation of Julia. Follow these steps to access the<a id="id587" class="indexterm"/> Julia environment online:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Access <a class="ulink" href="https://juliabox.org/">https://juliabox.org/</a> from the browser<div class="mediaobject"><img src="graphics/B03980_04_16.jpg" alt="Using Julia via the browser"/></div></li><li class="listitem">Log in using the Google account. This will create a unique instance of Julia for the logged-in user. This will give access to the Julia console and the IJulia instances.</li></ol></div><p>With one of the three<a id="id588" class="indexterm"/> approaches that we have seen previously, we have access to the<a id="id589" class="indexterm"/> Julia console from where the Julia code can be executed. Each piece of the contained Julia code is built in a file with a <code class="literal">.jl</code> extension.</p></div></div><div class="section" title="Running the Julia code from the command line"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec61"/>Running the Julia code from the command line</h2></div></div></div><p>Julia compiles the code at<a id="id590" class="indexterm"/> runtime and translates each method into a <a id="id591" class="indexterm"/>machine code using <span class="strong"><strong>just-in-time</strong></span> (<span class="strong"><strong>JIT</strong></span>) <a id="id592" class="indexterm"/>compilers. Internally, it utilizes <a id="id593" class="indexterm"/>
<span class="strong"><strong>Low-Level Virtual Machine</strong></span> (<span class="strong"><strong>LLVM</strong></span>) for optimization and code generation. LLVM is a full-fledged project that is a collection of standard compiler technologies. This is used as a part of iOS.</p><p>From the shell of choice, run the following:</p><div class="informalexample"><pre class="programlisting">&lt;&lt;/path/to/Julia&gt;&gt;/myjuliascript.jl</pre></div><p>Alternatively, open the Julia console from the Julia command line installation and run the following command:</p><div class="informalexample"><pre class="programlisting">julia&gt; include("&lt;&lt;path/to/juliascript&gt;&gt;/myjuliascript.jl")</pre></div></div><div class="section" title="Implementing in Julia (with examples)"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec62"/>Implementing in Julia (with examples)</h2></div></div></div><p>In this section, we will cover some basic topics under coding Julia and understanding the syntax. At the end of this section, readers should be able to easily write their Julia script and run the same. Regarding syntax, Julia programming language is very similar to MATLAB.</p></div><div class="section" title="Using variables and assignments"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec63"/>Using variables and assignments</h2></div></div></div><p>Variables in Julia, like any other <a id="id594" class="indexterm"/>programming language, are used for storing and manipulating<a id="id595" class="indexterm"/> data. Following is an example of defining, assigning, and manipulating variables and values:</p><div class="informalexample"><pre class="programlisting"># Assign a numeric value to a variable
julia&gt; x = 10
10

# Perform a simple mathematical manipulation of variables
julia&gt; x + 1
11

# Assigning or reassigning values to variables.
julia&gt; x = 1 + 1
2

# Assigning a string literal to a variable
julia&gt; x = "Hello World!"
"Hello, World!"</pre></div><p>Julia, being a mathematical programming language, provides several fundamental constants. Here is an example that can be directly used in the code. Additionally, we can define our constants and reassign values:</p><div class="informalexample"><pre class="programlisting">julia&gt; pi
π = 3.1415926535897...</pre></div><div class="section" title="Numeric primitives"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec60"/>Numeric primitives</h3></div></div></div><p>For any mathematical programming <a id="id596" class="indexterm"/>language that supports numeric-based computing, Integers and floating-point values form the basic building blocks and are called numeric primitives.</p><p>Julia comes with a support for large set numeric primitives that are extensive and very well-complimented mathematical functions.</p></div><div class="section" title="Data structures"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec61"/>Data structures</h3></div></div></div><p>Julia supports several data<a id="id597" class="indexterm"/> structures in addition to all the primitive data types such as Vectors, Matrices, Tuples, Dictionaries, Sets and so on. Following are some example representations with the usage:</p><div class="informalexample"><pre class="programlisting"># Vector
b = [4, 5, 6]
b[1] # =&gt; 4
b[end] # =&gt; 6

# Matrix
matrix = [1 2; 3 4]

# Tuple
tup = (1, 2, 3)
tup[1] # =&gt; 1
tup[1] = 3 # =&gt; ERROR #since tuples are immutable, assigning a value results in an error

# Dictionary
dict = ["one"=&gt; 1, "two"=&gt; 2, "three"=&gt; 3]
dict["one"] # =&gt; 1

# Set
filled_set = Set(1,2,2,3,4)</pre></div></div><div class="section" title="Working with Strings and String manipulations"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec62"/>Working with Strings and String manipulations</h3></div></div></div><p>Here are some examples <a id="id598" class="indexterm"/>of operating <a id="id599" class="indexterm"/>with Strings in Julia:</p><div class="informalexample"><pre class="programlisting">split("I love learning Julia ! ")
# =&gt; 5-element Array{SubString{ASCIIString},1}:
"I"
"love."
"learning."
"Julia"
"!"

join(["It seems to be interesting", "to see",
"how it works"], ", ")
# =&gt; "It seems interesting, to see, how it works."</pre></div></div><div class="section" title="Packages"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec63"/>Packages</h3></div></div></div><p>Julia comes with several packages<a id="id600" class="indexterm"/> that have inbuilt functions and support many out-of-box features for implementing Machine learning algorithms as well. Following is the list:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Images.jl</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Graphs.jl</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">DataFrames.jl</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">DimensionalityReduction.jl</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Distributions.jl</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">NLOpt.jl</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">ArgParse.jl</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Logging.jl</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">FactCheck.jl</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">METADATA.jl</code></li></ul></div><p>More details on Julia packages<a id="id601" class="indexterm"/> can be accessed at <a class="ulink" href="https://github.com/JuliaLang/">https://github.com/JuliaLang/</a>.</p></div><div class="section" title="Interoperability"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec64"/>Interoperability</h3></div></div></div><p>This following section covers the integration aspects<a id="id602" class="indexterm"/> of Julia with various other programming languages.</p><div class="section" title="Integrating with C"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec14"/>Integrating with C</h4></div></div></div><p>Julia is flexible and without <a id="id603" class="indexterm"/>any wrappers, supports invoking C functions directly. Following is an example that demonstrates how this is done:</p><div class="informalexample"><pre class="programlisting">julia&gt; ccall(:clock, Int32, ())
2292761
julia&gt; ccall(:getenv, Ptr{Uint8int8}, (Ptr{Uint8},), "SHELL")
Ptr{Uint8} @0x00007fff5fbffc45
julia&gt; bytestring(ans)
"/bin/bash"</pre></div></div><div class="section" title="Integrating with Python"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec15"/>Integrating with Python</h4></div></div></div><p>Similar to the C <a id="id604" class="indexterm"/>function calls, Julia supports invoking Python functions directly. It is important that we have the <code class="literal">PyCall</code> package installed to be able to do so. <code class="literal">PyCall.jl</code> offers automatic type conversion between Julia and Python. For example, Julia arrays are converted to NumPy arrays.</p><p>Following is an example that demonstrates invoking Python functions from the Julia code:</p><div class="informalexample"><pre class="programlisting">julia&gt; using PyCall # Installed with Pkg.add("PyCall")
julia&gt; @pyimport math
julia&gt; math.sin(math.pi / 4) - sin(pi / 4)
0.0
julia&gt; @pyimport pylab
julia&gt; x = linspace(0,2*pi,1000); y = sin(3*x + 4*cos(2*x));
julia&gt; pylab.plot(x, y; color="red", linewidth=2.0, linestyle="--")
julia&gt; pylab.show()</pre></div></div><div class="section" title="Integrating with MATLAB"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec16"/>Integrating with MATLAB</h4></div></div></div><p>Following example <a id="id605" class="indexterm"/>demonstrates integrating Julia to invoke MATLAB functions:</p><div class="informalexample"><pre class="programlisting">using MATLAB

function sampleFunction(bmap::BitMatrix)
@mput bmap
@matlab bmapthin = bwmorph(bmap, "thin", inf)
convert(BitArray, @mget bmapthin)
end</pre></div></div></div><div class="section" title="Graphics and plotting"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec65"/>Graphics and plotting</h3></div></div></div><p>Julia has several packages that help produce<a id="id606" class="indexterm"/> graphs and plots. Some of them are<a id="id607" class="indexterm"/> listed here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Gadfly.jl</code>: This is very similar to ggplot2</li><li class="listitem" style="list-style-type: disc"><code class="literal">Winston.jl</code>: This is very similar to Matplotlib</li><li class="listitem" style="list-style-type: disc"><code class="literal">Gaston.jl</code>: This interfaces with gnuplot</li></ul></div><p>The example here demonstrates using <code class="literal">PyPlot</code>:</p><div class="informalexample"><pre class="programlisting">using PyPlot
x = linspace(-2pi, 2pi)
y = sin(x)
plot(x, y, "--b")</pre></div><div class="mediaobject"><img src="graphics/B03980_04_17.jpg" alt="Graphics and plotting"/></div></div></div><div class="section" title="Benefits of adopting Julia"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec64"/>Benefits of adopting Julia</h2></div></div></div><p>Here are some of the direct <a id="id608" class="indexterm"/>benefits that one can look forward to for adopting Julia in the Machine learning implementations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Julia facilitates fast prototyping without compromising on performance</li><li class="listitem" style="list-style-type: disc">It inherently supports the parallelization of code</li><li class="listitem" style="list-style-type: disc">It provides a simpler way of expressing algorithms with special Julia types</li><li class="listitem" style="list-style-type: disc">Julia can easily invoke or integrate with C, Python, MATLAB, and C++</li><li class="listitem" style="list-style-type: disc">Julia is facilitated by an enthusiastic, friendly, and supportive community</li><li class="listitem" style="list-style-type: disc">It works with<a id="id609" class="indexterm"/> Hadoop and leverages Hive-based querying</li></ul></div></div><div class="section" title="Integrating Julia and Hadoop"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec65"/>Integrating Julia and Hadoop</h2></div></div></div><p>Integrating any <a id="id610" class="indexterm"/>programming language with Hadoop typically means the data stored in Hadoop should be accessible, and the program should be able to execute a specific logic on the data. This can happen either by retrieving the data from Hadoop and bringing it closer to the program or by moving the program to the data and to execute in a MapReduce or parallel processing mode. Obviously, in the first case where the data is fetched from Hadoop and brought to the code for executing the logic, there needs to be sufficient RAM to be able to hold and process this data in the memory, and this could restrict the ability to run on really large volumes. In the second case, where the code is taken to the data that is distributed across the data nodes, the logic should be parallelizable, and the Map and Reduce logics should be built.</p><p>The Julia integration with the Hadoop platform is slightly in its initial stages, and the current approach that is detailed is the first approach described previously where the connection to Hadoop/HDFS is made from the Julia code using a standard ODBC connectivity. The data is fetched into the RAM for further processing. Now, this code can run directly on the DataNode and can update the HDFS data.</p><p>We will use <code class="literal">ODBC.jl</code> that can be <a id="id611" class="indexterm"/>obtained from GitHub using the following link:</p><p>
<a class="ulink" href="https://github.com/quinnj/ODBC.jl">https://github.com/quinnj/ODBC.jl</a>
</p><p>This is a simple low-level ODBC interface for Julia. It can be installed through the Julia package manager using the following commands:</p><p>Following command creates a Julia package repository (only runs once for all packages)</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>julia&gt; Pkg.init()</strong></span>
</pre></div><p>Following command creates the <code class="literal">ODBC repo</code> folder and downloads the <code class="literal">ODBC</code> package and dependency (if needed)</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>julia&gt; Pkg.add("ODBC")</strong></span>
</pre></div><p>Following command loads the <a id="id612" class="indexterm"/>ODBC module for use (needs to be run with each new Julia instance)</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>julia&gt; using ODBC</strong></span>
</pre></div><p>Following are some important functions that can be used to work with Hadoop/HDFS:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To connect using an ODBC datasource, user and password use—<code class="literal">co = ODBC.connect("mydatasource",usr="johndoe",pwd="12345")</code>.</li><li class="listitem" style="list-style-type: disc">To disconnect use <code class="literal">disconnect(connection::Connection=conn)</code>.</li><li class="listitem" style="list-style-type: disc">To connect using a connection string use <code class="literal">advancedconnect(conn_string::String)</code>.</li><li class="listitem" style="list-style-type: disc">To ask a query and fetch a subset of data on the datasource, this query string is a Hive query that will be run on HDFS—<code class="literal">query(connecti on Connection=conn, querystring; fi le=: DataFrame,delim='\t')</code>.</li></ul></div><p>An example implementation is given here:</p><p>Use following command to load ODBC module:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>using ODBC</strong></span>
</pre></div><p>To connect to Hadoop cluster via Hive use this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hiveconn = ODBC.connect("servername"; usr="your-user-name", pwd="your-password-here")</strong></span>
</pre></div><p>To write a Hive query and store it as a Julia string, use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hive_query_string = "select …;"</strong></span>
</pre></div><p>To run a query, save results directly to file use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>query(hive_query_string, hiveconn;output="C:\\sample.csv",delim=',')</strong></span>
</pre></div><p>The Julia program can now access the data from this file to execute Machine learning algorithms.</p></div></div>
<div class="section" title="Python"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec27"/>Python</h1></div></div></div><p>Python is one of the <a id="id613" class="indexterm"/>highly adopted programming or scripting languages in the field of Machine learning and data science. Python is always known for its ease of learning, implementation, and maintenance. Python is highly portable and can run on the Unix-based, Windows and Mac platforms. With the availability of libraries such as Pydoop and SciPy, its relevance in the world of big data analytics has tremendously increased. Some of the key reasons for the popularity of Python in solving Machine learning problems are listed here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Python is known to be well suited for data analysis</li><li class="listitem" style="list-style-type: disc">It is a versatile scripting language that can be used for writing some basic quick and dirty scripts for testing some basic functions, or it can be used in real-time applications leveraging its full-featured toolkits</li><li class="listitem" style="list-style-type: disc">Python comes with complete Machine learning packages (refer to <a class="ulink" href="http://mloss.org/software/">http://mloss.org/software/</a>) and can be used in a plug-and-play manner</li></ul></div><div class="section" title="Toolkit options in Python"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec66"/>Toolkit options in Python</h2></div></div></div><p>Before we go deeper<a id="id614" class="indexterm"/> into what toolkit options we have in Python, let's first understand the toolkit options trade-offs that should be considered before choosing one.</p><p>Some of the questions that we should evaluate for the appropriate toolkit can be as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What are my performance priorities? Do I need offline or real-time processing implementations?</li><li class="listitem" style="list-style-type: disc">How transparent are the toolkits? Can I customize the library myself?</li><li class="listitem" style="list-style-type: disc">What is the community status? How fast are bugs fixed and how is the community support and expert communication availability?</li></ul></div><p>There are three options in Python:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Use Python external bindings. These are the interfaces to popular packages in markets such as Matlab, R, Octave, and so on. This option will work well, in case we already have some implementations existing in the previously mentioned frameworks that we are looking at seamlessly migrating into Python.</li><li class="listitem" style="list-style-type: disc">Use <a id="id615" class="indexterm"/>Python-based toolkits. There are some toolkits written in Python that come with a bunch algorithms. Some of the Python toolkits will be covered in the next section.</li><li class="listitem" style="list-style-type: disc">Write your logic/toolkit.</li></ul></div></div><div class="section" title="Implementation of Python (using examples)"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec67"/>Implementation of Python (using examples)</h2></div></div></div><p>Python has two core toolkits, which<a id="id616" class="indexterm"/> are more of building blocks and almost all the specialized toolkits that are listed here use these core toolkits. These are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NumPy</strong></span>: NumPy<a id="id617" class="indexterm"/> is about fast and efficient arrays built in Python</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>SciPy</strong></span>: This is a <a id="id618" class="indexterm"/>bunch of algorithms for standard operations built in NumPy</li></ul></div><p>There are a bunch of C/C++ based implementations such as LIBLINEAR, LIBSVM, OpenCV, and others</p><p>Let's now see some of the popular Python toolkits and also those that have been updated within a span of a year of writing this book:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>NLTK</strong></span>: This stands for <a id="id619" class="indexterm"/>natural language toolkit. This focuses on the <a id="id620" class="indexterm"/><span class="strong"><strong>Natural language processing</strong></span> (<span class="strong"><strong>NLP</strong></span>).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>mlpy</strong></span>: This is Machine learning<a id="id621" class="indexterm"/> algorithms toolkit that comes with support for some key Machine learning algorithms such as classifications, regression, and clustering among others.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>PyML</strong></span>: This <a id="id622" class="indexterm"/>toolkit focuses on <a id="id623" class="indexterm"/><span class="strong"><strong>Support Vector Machine</strong></span> (<span class="strong"><strong>SVM</strong></span>). We will cover more on this in the coming chapters.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>PyBrain</strong></span>: This toolkit<a id="id624" class="indexterm"/> focuses on Neural networks and related functions.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>mdp-toolkit</strong></span>: The focus of <a id="id625" class="indexterm"/>this toolkit is data processing and it supports scheduling and parallelizing the processing.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>scikit-learn</strong></span>: This is <a id="id626" class="indexterm"/>one of the most popular toolkits and is being highly adopted by data scientists in the recent past. It has support for supervised, and unsupervised learning, some special support for feature selection, and visualizations as well. There is a large team that is actively building this toolkit and is known for its excellent documentation.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Pydoop</strong></span>: This<a id="id627" class="indexterm"/> is the Python integration with the Hadoop platform.</li></ul></div><p>
<span class="strong"><strong>Pydoop</strong></span> and <span class="strong"><strong>SciPy</strong></span>
<a id="id628" class="indexterm"/> are heavily deployed in big data analytics.</p><p>In this chapter, we will explore the scikit-learn toolkit, and demonstrate all our examples in the upcoming chapters using this toolkit.</p><p>For a Python programmer, using scikit-learn can help bring Machine learning into a production system very easily.</p><div class="section" title="Installing Python and setting up scikit-learn"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec66"/>Installing Python and setting up scikit-learn</h3></div></div></div><p>Following are the core <a id="id629" class="indexterm"/>Python <a id="id630" class="indexterm"/>toolkit versions and dependencies for installing Python and scikit-learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Python (&gt;= 2.6 or &gt;= 3.3)</li><li class="listitem" style="list-style-type: disc">NumPy (&gt;= 1.6.1)</li><li class="listitem" style="list-style-type: disc">SciPy (&gt;= 0.9).</li><li class="listitem" style="list-style-type: disc">A working C++ compiler</li></ul></div><p>We will be using the wheel packages (<code class="literal">.whl</code> files) for scikit-learn from PyPI, and install it using the pip utility.</p><p>To install in your home directory, use the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python setup.py install --home</strong></span>
</pre></div><p>For using the git<a id="id631" class="indexterm"/> repo <a id="id632" class="indexterm"/>directly from the GitHub to install scikit-learn on the local disk, use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>% git clone git://github.com/scikit-learn/scikit-learn/</strong></span>
<span class="strong"><strong>% cd scikit-learn</strong></span>
</pre></div><div class="section" title="Loading data"><div class="titlepage"><div><div><h4 class="title"><a id="ch04lvl4sec17"/>Loading data</h4></div></div></div><p>Scikit-learn comes with a few standard datasets, for instance, the <code class="literal">iris</code> and <code class="literal">digits</code> datasets that can be used for building and running Machine learning algorithms.</p><p>Here are some steps to follow to load the standard datasets shipped with scikit-learn:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from sklearn import datasets
&gt;&gt;&gt; iris = datasets.load_iris()
&gt;&gt;&gt; digits = datasets.load_digits()
&gt;&gt;&gt; print digits.data
[[ 0. 0. 5. ..., 0. 0. 0.]
[ 0. 0. 0. ..., 10. 0. 0.]
[ 0. 0. 0. ..., 16. 9. 0.]
...,
[ 0. 0. 1. ..., 6. 0. 0.]
[ 0. 0. 2. ..., 12. 0. 0.]
[ 0. 0. 10. ..., 12. 1. 0.]]
&gt;&gt;&gt; digits.target
array([0, 1, 2, ..., 8, 9, 8])</pre></div></div></div></div></div>
<div class="section" title="Apache Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Apache Spark</h1></div></div></div><p>Apache Spark<a id="id633" class="indexterm"/> is an open-source framework for fast, big data or large-scale processing with the support for streaming, SQL, Machine learning, and graph processing. This framework is implemented in Scala and supports programming languages such as Java, Scala, and Python. The magnitude of performance is up to 10X to 20X is the traditional Hadoop stack. Spark is a general purpose framework and allows interactive programming along with the support for streaming. Spark can work with Hadoop supporting Hadoop formats like SequenceFiles or InputFormats in a standalone mode. It includes local file systems, Hive, HBase, Cassandra, and Amazon S3 among others.</p><p>We will use Spark 1.2.0 for all the examples throughout this book.</p><p>The following figure depicts the<a id="id634" class="indexterm"/> core modules of Apache Spark:</p><div class="mediaobject"><img src="graphics/B03980_04_24.jpg" alt="Apache Spark"/></div><p>Some of the basic functions of Spark framework include task scheduling, interaction with storage systems, fault tolerance, and memory management. Spark follows a programming paradigm called<a id="id635" class="indexterm"/> <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>). This is primarily related to managing distributed data storage and parallel computing.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spark SQL</strong></span> is Spark's <a id="id636" class="indexterm"/>package for querying and processing structured and unstructured data. The core functions of this package are:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">To facilitate loading the data from varied structured sources such as Hive, JSON, and others</li><li class="listitem" style="list-style-type: disc">To provide integration between SQL and regular Python or Java or Scala code, and provide the capability to build custom functions that can execute on distributed data and in parallel</li><li class="listitem" style="list-style-type: disc">To support the SQL-based querying from external tools through standard database<a id="id637" class="indexterm"/> connections (JDBC/ODBC) including <span class="strong"><strong>Tableau</strong></span></li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spark Streaming</strong></span><a id="id638" class="indexterm"/> module is used for processing real-time, large-scale streams of data. This API is different from the Streaming I/O API of Hadoop.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>MLib</strong></span> module <a id="id639" class="indexterm"/>provides out-of-box Machine learning algorithm functions that are scalable and can run on a cluster.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>GraphX</strong></span> module <a id="id640" class="indexterm"/>provides functions for graph manipulations.</li></ul></div><p>In this chapter, we will learn how to use Spark in conjunction with the Scala programming language. Let's now have a quick overview of Scala and learn how to code in Scala.</p><div class="section" title="Scala"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec68"/>Scala</h2></div></div></div><p>Scala <a id="id641" class="indexterm"/>is a strongly typed programming language that requires <a id="id642" class="indexterm"/>
<span class="strong"><strong>JVM</strong></span> (<span class="strong"><strong>Java Virtual Machine</strong></span>) to run. It is an independent platform and can leverage Java APIs. We will use interpretive prompt to run Scala with Spark. The command prompt here shows how Scala can be run with Spark using the interpretive prompt.</p><div class="mediaobject"><img src="graphics/B03980_04_18.jpg" alt="Scala"/></div><p>Let's look at some <a id="id643" class="indexterm"/>Scala examples.</p><p>The following code can be pasted directly into the command prompt:</p><div class="informalexample"><pre class="programlisting">//Default variables are assigned to any expressions
scala&gt;8 * 5 + 2
Res0: Int = 42
Scala&gt;0.5 * res0
Res1= Double = 21.0
//All simple data types are objects
scala&gt;"Hello, " + res0
Res2: java.lang.String = Hello, 42
scala&gt;10.toString()
Res2: String = 10
scala&gt;a.+(b)
Res1: Int = 200            //So you can consider , the operator as a method
A method b as a shorthand for a.method(b)
scala&gt;val myVal: String = "Foo"
keyword "val" this means that a variable cannot change value  (immutable variable)
scala&gt;var myVar:String = "Foo"
the keyword var means that it is a variable that can be changed (mutable variable)
scala&gt; def cube(a: Int): Int = a * a * a
cube: (a: Int)Int
scala&gt; myNumbers.map(x =&gt; cube(x))
res8: List[Int] = List(1, 8, 27, 64, 125, 64, 27)
scala&gt; myNumbers.map(x =&gt; x * x * x)
res9: List[Int] = List(1, 8, 27, 64, 125, 64, 27)
scala&gt; val myNumbers = List(1,2,3,4,5,4,3)
myNumbers: List[Int] = List(1, 2, 3, 4, 5, 4, 3)
scala&gt; def factorial(n:Int):Int = if (n==0) 1 else n * factorial(n-1)
factorial: (n: Int)Int
scala&gt; myNumbers.map(factorial)
res18: List[Int] = List(1, 2, 6, 24, 120, 24, 6)
scala&gt; myNumbers.map(factorial).sum
res19: Int = 183
scala&gt; var factor = 3
factor: Int = 3
scala&gt; val multiplier = (i:Int) =&gt; i * factor
multiplier: Int =&gt; Int = &lt;function1&gt;
scala&gt; val l1 = List(1,2,3,4,5) map multiplier
l1: List[Int] = List(3, 6, 9, 12, 15)
scala&gt; factor = 5
factor: Int = 5</pre></div></div><div class="section" title="Programming with Resilient Distributed Datasets (RDD)"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec69"/>Programming with Resilient Distributed Datasets (RDD)</h2></div></div></div><p>RDDs are Spark's<a id="id644" class="indexterm"/> core abstraction for working with data. They are immutable distributed collections of elements. All functions in Spark only work on RDDs.</p><p>Spark automatically distributes the data contained in RDDs across the nodes within a cluster as partitions and supports parallel processing to be performed on them. RDDs can be created by importing from external datasets or distributing collections in the driver program. The following command demonstrates this function:</p><div class="informalexample"><pre class="programlisting">scala&gt; val c = file.filter(line =&gt; line.contains("and"))</pre></div><p>The <code class="literal">collect()</code> method will write the output to the console:</p><div class="informalexample"><pre class="programlisting">scala&gt;c.collect()</pre></div><p>The output of the results is usually saved to the external storage system. The <code class="literal">count()</code> function gives the number of output lines. The following will print out the lines:</p><div class="informalexample"><pre class="programlisting">scala&gt;println("input had " + c.count() + " lines")</pre></div><p>The <code class="literal">take()</code> function will fetch <span class="emphasis"><em>n</em></span> records from the result:</p><div class="informalexample"><pre class="programlisting">scala&gt;c.take(10).foreach(println)</pre></div><p>RDDs process in a lazy manner by Spark to bring in the efficiency while handling large datasets.</p><p>To reuse RDD in multiple actions, you can ask Spark to persist it using <code class="literal">RDD.persist()</code>.</p><p>We can ask Spark to persist our data in some different places. After computing it the first time, Spark will store the RDD contents in the memory (partitioned across the machines in your cluster) and reuse them for future actions.</p><p>Hence, following are the basic steps to process RDDs:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create input RDDs from external data.</li><li class="listitem">Transforming them to define new RDDs using transformations, for example <code class="literal">filter()</code>.</li><li class="listitem">Storing intermediate RDDs for reuse using <code class="literal">persist()</code>.</li><li class="listitem">Invoking any required function (for example, <code class="literal">count()</code>) to start a parallel computation process.</li></ol></div><p>Following is an <a id="id645" class="indexterm"/>example of RDD using Pi Estimation with Scala:</p><div class="informalexample"><pre class="programlisting">scala&gt;var NUM_SAMPLES=5
scala&gt; val count = sc.parallelize(1 to NUM_SAMPLES).map{i =&gt;
     | val x = Math.random()
     | val y = Math.random()
     |  if (x*x + y*y &lt; 1) 1 else 0
     | }.reduce(_ + _)
scala&gt;println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)</pre></div></div></div>
<div class="section" title="Spring XD"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Spring XD</h1></div></div></div><p>Though this book does not include <a id="id646" class="indexterm"/>Spring XD framework to demonstrate the Machine learning algorithm, a small introduction is given here as this is found to be fast emerging for adoption in the Machine learning world.</p><p>XD stands for eXtreme Data. This open source framework is built by the Pivotal team (earlier the SpringSource) as the one-stop-shop for developing and deploying big data applications.</p><p>Spring XD is a distributed and extensible framework that unifies data ingestion, analytics functions in real-time, batch, and supports data export. Spring XD is built on Spring Integration and Spring Batch frameworks.</p><p>Following are some<a id="id647" class="indexterm"/> key features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Spring XD is a unified platform for batch and stream workloads. It is an open and extensible runtime.</li><li class="listitem" style="list-style-type: disc">Scalable and high-performance, it is a distributed data ingestion framework that can ingest data from a variety of sources that include HDFS, NOSQL, or Splunk.</li><li class="listitem" style="list-style-type: disc">It supports for real-time analytics at ingestion time, for example, gathering metrics and counting values.</li><li class="listitem" style="list-style-type: disc">It has workflow management through batch jobs that include interactions with standard RDBMS and Hadoop systems.</li><li class="listitem" style="list-style-type: disc">It is a <a id="id648" class="indexterm"/>scalable and high-performance data export, for example, from HDFS to an RDBMS or NoSQL database.</li></ul></div><p>Spring XD is known to implement Lambda Architecture that in theory is defined to support both batch and real-time processing. More information on evolutionary architectures such as Lambda Architecture is covered in <a class="link" href="ch14.html" title="Chapter 14. New generation data architectures for Machine learning">Chapter 14</a>, <span class="emphasis"><em>New generation data architectures for Machine learning</em></span>.</p><p>Spring XD architecture primarily <a id="id649" class="indexterm"/>has three architecture layers to help facilitate the previous features:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="strong"><strong>Speed Layer</strong></span>: This is <a id="id650" class="indexterm"/>about accessing and processing data in real time. This process keeps the system more up-to-date.<div class="mediaobject"><img src="graphics/B03980_04_19.jpg" alt="Spring XD"/></div></li><li class="listitem"><span class="strong"><strong>Batch Layer</strong></span>: The Batch<a id="id651" class="indexterm"/> layer has access to the complete master dataset also called the data lake meaning <span class="emphasis"><em>source of truth</em></span>.<div class="mediaobject"><img src="graphics/B03980_04_20.jpg" alt="Spring XD"/></div></li><li class="listitem"><span class="strong"><strong>Serving Layer</strong></span>: The Service<a id="id652" class="indexterm"/> layer is more of a query layer that is responsible for exposing the data post processing to an unsubscribed consumer. This layer makes batch data queryable and is usually known for high throughput driven responses.<div class="mediaobject"><img src="graphics/B03980_04_23.jpg" alt="Spring XD"/></div></li></ol></div><p>Spring XD Runtime architecture is shown here (source Pivotal):</p><div class="mediaobject"><img src="graphics/B03980_04_22.jpg" alt="Spring XD"/></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Summary</h1></div></div></div><p>In this chapter, we learned about the open source options for implementing Machine learning, and covered installation, implementation, and execution of libraries, tools, and frameworks such as Apache Mahout, Python, R, Julia, and Apache Spark's MLib. Importantly, we covered the integration of these frameworks with the big data platform—Apache Hadoop. This chapter is more of a foundation for the coming chapters where we will learn how to use these frameworks in implementing specific Machine learning algorithms.</p></div></body></html>