<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;Building Scalable Recommendation Engines with Mahout" id="aid-1S2JE1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Building Scalable Recommendation Engines with Mahout</h1></div></div></div><p>Imagine that you have just launched an online e-commerce website to sell clothes designed by you and you are lucky enough to make your business kick-start well and make it a successful venture. With more web traffic coming to your site, the most obvious choice is to implement a recommendation engine on your website with features such as people who visited something also visited something else, items similar to the current item, and so on. Since your website is new and successful, you have implemented a recommendation engine using popular tools, such as R and Python. The recommendation functionality is deployed and works well, adding more value to the success of the business. Now with more business coming in and with an increase in your user base, the most likely problem you might face with the website is that your customers start complaining that your website is becoming slow.</p><p>Upon analyzing the root cause, the obvious reason would be that the recommender features that are added to the website are slowing down the site. This is bound to happen because of the limitation of collaborative filtering algorithms used to cater for recommendations. Every time we calculate the similarity between users, the entire user base will be loaded into the memory and the similarity values would be calculated. This operation will be fast with a small user base. Assume that with a large use base, such as one million users, the collaborative filtering model will be thrown out of the memory exception. By increasing the RAM capability, we might address this to some extent, but it still won't help us. Increasing the RAM would be bad idea as it shoots up the infrastructure cost.</p><p>The best way is to redesign the recommender engine on a distributed platform, such as Hadoop. This is where Apache Mahout will come in handy as it is an open source machine learning library built for the distributed platform, Apache Hadoop.</p><p>In this chapter, we will be covering the following sections:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Mahout general introduction</li><li class="listitem">Setting up Mahout standalone and distributed mode</li><li class="listitem">Core building blocks of Mahout</li><li class="listitem">Building and evaluating recommendation engines with Mahout such as user-based collaborative filtering, item-based collaborative filtering, SVD recommendation engines, and ALS recommendation engines.<div class="mediaobject"><img src="../Images/image00459.jpeg" alt="Building Scalable Recommendation Engines with Mahout"/></div><p style="clear:both; height: 1em;"> </p></li></ul></div><div class="section" title="Mahout - a general introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec56"/>Mahout - a general introduction</h1></div></div></div><p><span class="strong"><strong>Apache Mahout</strong></span> is an open source java library built on top of Apache Hadoop, which provides large-scale machine learning algorithms. Though this library was originally started with the MapReduce paradigm, the framework currently offers bindings to Apache Spark, H2O, and Apache Flink. The latest version of Mahout supports collaborative filtering recommendation engines, clustering, classification, dimensionality reduction, H2O, and spark bindings.</p><p>The major features of Mahout 0.12.2 are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">An extensible programming environment and framework for the building of scalable algorithms</li><li class="listitem">Support for Apache Spark, Apache Flink, and H2O algorithms</li><li class="listitem">Samsara, a vector Math environment similar to the R programming language</li></ul></div><p>As mentioned in the previous section, though many things are possible with Mahout, we will be limiting our discussion to building recommendation engines using Mahout. Mahout provides support for both the standalone mode, where the recommendation model or application can be deployed on a single server, and the distributed mode, where the recommendation model can be deployed on a distributed platform.</p></div></div>
<div class="section" title="Setting up Mahout"><div class="titlepage" id="aid-1T1402"><div><div><h1 class="title"><a id="ch09lvl1sec57"/>Setting up Mahout</h1></div></div></div><p>In this section, we shall look at setting up Mahout in standalone and distributed mode.</p><div class="section" title="The standalone mode - using Mahout as a library"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec104"/>The standalone mode - using Mahout as a library</h2></div></div></div><p>The standalone mode of Mahout usually involves two steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Adding Mahout libraries to the Java application that wants to use the Mahout capabilities</li><li class="listitem">Calling Mahout recommendation engine functions to build the recommender application</li></ul></div><p>Running an application that uses Mahout requires the following dependencies to be added to the <code class="literal">pom.xml</code> file of your Java Maven project:</p><div class="mediaobject"><img src="../Images/image00460.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p><p>The preceding dependencies will download all the required jars or libraries required to run the Mahout functionalities, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00461.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p><p>Another step is to go to the official Apache Mahout website and download the required Mahout jar files, as shown here:</p><p>The latest Mahout library can be downloaded from the Apache Mahout official website at <a class="ulink" href="http://mahout.apache.org/general/downloads.html">http://mahout.apache.org/general/downloads.html</a>.</p><p>The following image shows the screenshot of the above mentioned URL:</p><div class="mediaobject"><img src="../Images/image00462.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p><p>Download the tar file(tar files are just executable) instead of source files, as we just need the jar files of Mahout to build recommendation engines:</p><div class="mediaobject"><img src="../Images/image00463.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p><p>After downloading the tar file, just extract all the files and add the required jars to the Java application:</p><div class="mediaobject"><img src="../Images/image00464.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p><p>With this minimal setup, let's build a very basic recommendation engine using Java Eclipse.</p><p>The minimal setup just requires the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a Java Maven project in Eclipse with the following attribute selection:<p>The following image shows the screenshot of creating a new Maven project setup step 1:</p><div class="mediaobject"><img src="../Images/image00465.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p><p>In the following image, add the <span class="strong"><strong>Artifact Id</strong></span> "<code class="literal">recommendations</code>":</p><div class="mediaobject"><img src="../Images/image00466.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">A Maven project will be created with <code class="literal">app.java</code> as the default class. We can make changes in this class to build our standalone recommendation engine:<div class="mediaobject"><img src="../Images/image00467.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">Set Java runtime as 1.7 or higher, as shown in the next screenshot:<div class="mediaobject"><img src="../Images/image00468.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">Set the required Maven dependencies listed as <span class="strong"><strong>mahout-mr</strong></span>, <span class="strong"><strong>mahout-math</strong></span>, <span class="strong"><strong>slf4j-log4j</strong></span>, <span class="strong"><strong>commons-math3</strong></span>, and <span class="strong"><strong>guava</strong></span>; this will download the required jars for the application to run, as shown in the following screenshot:<div class="mediaobject"><img src="../Images/image00469.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">These dependencies can be seen in the following screenshot:<div class="mediaobject"><img src="../Images/image00470.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">Create a folder called <code class="literal">data</code> in the project and create a sample dataset, as shown in the following screenshot:<div class="mediaobject"><img src="../Images/image00471.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">Now rename <code class="literal">app.java</code> to the <code class="literal">UserbasedRecommender.java</code> file. Write the code in the java class to build the basic user-based recommender system:<pre class="programlisting">package com.packtpub.mahout.recommenders; &#13;
 &#13;
import java.io.File; &#13;
import java.io.IOException; &#13;
import java.util.List; &#13;
 &#13;
import org.apache.mahout.cf.taste.common.TasteException; &#13;
import org.apache.mahout.cf.taste.impl.model.file.FileDataModel; &#13;
import org.apache.mahout.cf.taste.impl.neighborhood.ThresholdUserNeighborhood; &#13;
import org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender; &#13;
import org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarity; &#13;
import org.apache.mahout.cf.taste.model.DataModel; &#13;
import org.apache.mahout.cf.taste.neighborhood.UserNeighborhood; &#13;
import org.apache.mahout.cf.taste.recommender.RecommendedItem; &#13;
import org.apache.mahout.cf.taste.recommender.UserBasedRecommender; &#13;
import org.apache.mahout.cf.taste.similarity.UserSimilarity; &#13;
 &#13;
//class for generating User Based Recommendation &#13;
public class UserbasedRecommender  &#13;
{ &#13;
    public static void main( String[] args ) throws TasteException, IOException &#13;
    { &#13;
    //creating data model &#13;
         DataModel model = new FileDataModel(new File("data/dataset.csv"));     &#13;
    // creating pearson similarity between users  &#13;
    UserSimilarity similarity = new PearsonCorrelationSimilarity(model); &#13;
     &#13;
         //creating user neighborhood &#13;
           UserNeighborhood neighborhood = new ThresholdUserNeighborhood(0.1,                                             similarity, model); &#13;
     &#13;
      // creating recommender model &#13;
            UserBasedRecommender recommender = new       GenericUserBasedRecommender(model, neighborhood, similarity); &#13;
     &#13;
        //generating 3 recommendations for user 2 &#13;
    List&lt;RecommendedItem&gt; recommendations = recommender.recommend(2, 3); &#13;
    for (RecommendedItem recommendation : recommendations) { &#13;
      System.out.println(recommendation); &#13;
    } &#13;
    } &#13;
} &#13;
</pre><p>Running the preceding code will generate the recommendations for user 2, as shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00472.jpeg" alt="The standalone mode - using Mahout as a library"/></div><p style="clear:both; height: 1em;"> </p></li></ol><div style="height:10px; width: 1px"/></div><p>Boom! We have created our first user-based recommendation engine. Don't worry about what we have done or what's happening; everything will become clearer in the next few sections. For now, just try to understand how the Mahout library can be used in the standalone mode to build recommendation engines.</p></div><div class="section" title="Setting Mahout for the distributed mode"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec105"/>Setting Mahout for the distributed mode</h2></div></div></div><p>We have seen how to use Mahout libraries in the standalone mode. In this section, let's see how to setup Mahout on a distributed platform, such as HDFS. The following are the requirements in order to set up Mahout:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Java 7 and higher</li><li class="listitem">Apache Hadoop</li><li class="listitem">Apache Mahout</li></ul></div><p>Setting up Java 7 and installing Hadoop is out of the scope of the is book. We can find very good resources online on how to set up Hadoop. Assuming Hadoop is already set up, follow these steps to set up Mahout:</p><p>Download and extract the latest Mahout distribution from Apache Mahout website, as explained earlier.</p><p>Let's set up environment values:</p><pre class="programlisting">Export JAVA_HOME = path/to/java7 or more &#13;
export MAHOUT_HOME = /home/softwares/ apache-mahout-distribution-0.12.2 &#13;
export MAHOUT_LOCAL = true #for standalone mode &#13;
export PATH = $MAHOUT_HOME/bin &#13;
export CLASSPATH = $MAHOUT_HOME/lib:$CLASSPATH &#13;
</pre><div class="note" title="Note"><h3 class="title"><a id="tip24"/>Tip</h3><p>Unset <code class="literal">MAHOUT_LOCAL</code> in order to run it on the Hadoop cluster.</p></div><p>Once the environment variables are set up, use the following commands in the command line to run a recommendation engine on the distributed platform.</p><p>Using the following code, we are generating item-based recommendations using the log likelihood similarity:</p><pre class="programlisting">
<span class="strong"><strong>mahout recommenditembased -s SIMILARITY_LOGLIKELIHOOD -i mahout/data.txt -o mahout/output1 --numRecommendations 25 &#13;
 &#13;
[cloudera@quickstart ~]$ mahout recommenditembased -s SIMILARITY_LOGLIKELIHOOD -i mahout/data.txt -o mahout/output1 --numRecommendations 25 &#13;
MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath. &#13;
Running on hadoop, using /usr/lib/hadoop/bin/hadoop and HADOOP_CONF_DIR=/etc/hadoop/conf &#13;
MAHOUT-JOB: /usr/lib/mahout/mahout-examples-0.9-cdh5.4.0-job.jar &#13;
16/11/10 11:05:09 INFO common.AbstractJob: Command line arguments: {--booleanData=[false], --endPhase=[2147483647], --input=[mahout/data.txt], --maxPrefsInItemSimilarity=[500], --maxPrefsPerUser=[10], --maxSimilaritiesPerItem=[100], --minPrefsPerUser=[1], --numRecommendations=[25], --output=[mahout/output1], --similarityClassname=[SIMILARITY_LOGLIKELIHOOD], --startPhase=[0], --tempDir=[temp]} &#13;
16/11/10 11:05:09 INFO common.AbstractJob: Command line arguments: {--booleanData=[false], --endPhase=[2147483647], --input=[mahout/data.txt], --minPrefsPerUser=[1], --output=[temp/preparePreferenceMatrix], --ratingShift=[0.0], --startPhase=[0], --tempDir=[temp]} &#13;
16/11/10 11:05:10 INFO Configuration.deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir &#13;
16/11/10 11:05:10 INFO Configuration.deprecation: mapred.compress.map.output is deprecated. Instead, use mapreduce.map.output.compress &#13;
16/11/10 11:05:10 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir &#13;
16/11/10 11:05:11 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032 &#13;
16/11/10 11:05:20 INFO input.FileInputFormat: Total input paths to process : 1 &#13;
16/11/10 11:05:22 INFO mapreduce.JobSubmitter: number of splits:1 &#13;
16/11/10 11:05:24 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1478802142793_0003 &#13;
16/11/10 11:05:42 INFO impl.YarnClientImpl: Submitted application application_1478802142793_0003 &#13;
16/11/10 11:05:52 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1478802142793_0003/ &#13;
16/11/10 11:05:52 INFO mapreduce.Job: Running job: job_1478802142793_0003 &#13;
16/11/10 11:16:45 INFO mapreduce.Job: Job job_1478802142793_0011 running in uber mode : false &#13;
16/11/10 11:16:45 INFO mapreduce.Job:  map 0% reduce 0% &#13;
16/11/10 11:16:58 INFO mapreduce.Job:  map 100% reduce 0% &#13;
16/11/10 11:17:19 INFO mapreduce.Job:  map 100% reduce 100% &#13;
16/11/10 11:17:20 INFO mapreduce.Job: Job job_1478802142793_0011 completed successfully &#13;
16/11/10 11:17:21 INFO mapreduce.Job: Counters: 49 &#13;
File System Counters &#13;
------------------------------- &#13;
------------------------------- &#13;
Bytes Written=28 &#13;
16/11/10 11:17:21 INFO driver.MahoutDriver: Program took 732329 ms (Minutes: 12.205483333333333) &#13;
</strong></span>
</pre><p>The output is as follows:</p><div class="mediaobject"><img src="../Images/image00473.jpeg" alt="Setting Mahout for the distributed mode"/></div><p style="clear:both; height: 1em;"> </p></div></div>
<div class="section" title="Core building blocks of Mahout"><div class="titlepage" id="aid-1TVKI2"><div><div><h1 class="title"><a id="ch09lvl1sec58"/>Core building blocks of Mahout</h1></div></div></div><p>Like any other recommendation engine framework, Mahout also provides a rich set of components to build customized recommender systems that are enterprise-ready, scalable, flexible, and that perform well.</p><p>The key components of Mahout are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">DataModel<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Similarity: UserSimilarity, ItemSimilarity</li></ul></div></li><li class="listitem">User neighborhood</li><li class="listitem">Recommender</li><li class="listitem">Recommender evaluator</li></ul></div><div class="section" title="Components of a user-based collaborative recommendation engine"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec106"/>Components of a user-based collaborative recommendation engine</h2></div></div></div><p>In this section, we shall cover the components required for building a user-based collaborative filtering system.</p><div class="mediaobject"><img src="../Images/image00474.jpeg" alt="Components of a user-based collaborative recommendation engine"/></div><p style="clear:both; height: 1em;"> </p><p>The components of a user-based collaborative recommendation engine are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>DataModel</strong></span>: A DataModel implementation allows us to store and provide access to the user, item, and preference data required for computation. The DataModel component allows us to pull data from the data source. Mahout provides <span class="strong"><strong>MySQLJDBCDataModel</strong></span>, which allows us to pull data from the database via JDBC and MySQL. For the purpose of our example, we use the <span class="strong"><strong>FileDataModel</strong></span> interface to access data from files that Mahout exposes.<p>Some other DataModels exposed by Mahout are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>HBaseDataModel</strong></span>: (<a class="ulink" href="http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.html">http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.html</a>)</li><li class="listitem"><span class="strong"><strong>GenericJDBCDataModel</strong></span>: (<a class="ulink" href="http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/jdbc/GenericJDBCDataModel.html">http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/jdbc/GenericJDBCDataModel.html</a>)</li><li class="listitem"><span class="strong"><strong>PostgreSQLJDBCDataModel</strong></span>: (<a class="ulink" href="http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/jdbc/PostgreSQLJDBCDataModel.html">http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/jdbc/PostgreSQLJDBCDataModel.html</a>)</li><li class="listitem"><span class="strong"><strong>MongoDBDataModel</strong></span>: (<a class="ulink" href="http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.html">http://apache.github.io/mahout/0.10.1/docs/mahout-integration/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.html</a>)</li></ul></div><p>Mahout expects the user data to be in the format of a userID, itemID, preference triplet. The preference values can be either continuous or Boolean. Mahout has support for both continuous and Boolean preference values. Each input triplet, containing userID, itemID, and preference, which we supply to the DataModel, will be represented in a memory-efficient <span class="strong"><strong>Preference object</strong></span> or a <span class="strong"><strong>PreferenceArray</strong></span> object.</p></li><li class="listitem"><span class="strong"><strong>UserSimilarity</strong></span>: The UserSimilarity interface calculates the similarity between two users. The implementations of UserSimilarity return values in the range of -1.0 to 1.0 usually, with 1.0 being the perfect similarity. In previous chapters, we saw the multiple ways in which we can calculate the similarity between users, such as Euclidean Distance, Pearson Coefficient, cosine distance, and so on. There are many implementations of the UserSimilarity interface to calculate the User Similarity, which are listed as follows:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">CachingUserSimilarity</li><li class="listitem">CityBlockSimilarity</li><li class="listitem">EuclideanDistanceSimilarity</li><li class="listitem">GenericUserSimilarity</li><li class="listitem">LogLikelihoodSimilarity</li><li class="listitem">PearsonCorrelationSimilarity</li><li class="listitem">SpearmanCorrelationSimilarity</li><li class="listitem">TanimotoCoefficientSimilarity</li><li class="listitem">UncenteredCosineSimilarity</li></ul></div></li><li class="listitem"><span class="strong"><strong>ItemSimilarity</strong></span>: Similar to UserSimilarity, Mahout also provides the ItemSimilarity interface, analogous to UserSimilarity, which can be used to calculate the similarity between items. The implementations of UserSimilarity return values in the range of -1.0 to 1.0 usually, with 1.0 being the perfect similarity:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">AbstractItemSimilarity</li><li class="listitem">AbstractJDBCItemSimilarity</li><li class="listitem">CachingItemSimilarity</li><li class="listitem">CityBlockSimilarity</li><li class="listitem">EuclideanDistanceSimilarity</li><li class="listitem">FileItemSimilarity</li><li class="listitem">GenericItemSimilarity</li><li class="listitem">LogLikelihoodSimilarity</li><li class="listitem">MySQLJDBCInMemoryItemSimilarity</li><li class="listitem">MySQLJDBCItemSimilarity</li><li class="listitem">PearsonCorrelationSimilarity</li><li class="listitem">SQL92JDBCInMemoryItemSimilarity</li><li class="listitem">SQL92JDBCItemSimilarity</li><li class="listitem">TanimotoCoefficientSimilarity</li><li class="listitem">UncenteredCosineSimilarity</li></ul></div></li><li class="listitem"><span class="strong"><strong>UserNeighborhood</strong></span>: In a user-based recommender, recommendations generated for the active user are produced by finding a neighborhood of similar users. UserNeighborhood usually refers to a way to determine the neighborhood for a given active user, for example, the ten nearest neighbors to take into account while generating recommendations.<p>These neighborhood classes implement the UserSimilarity interface for their operation.
The following are the implementations of the neighborhood interface:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">CachingUserNeighborhood</li><li class="listitem">NearestNUserNeighborhood</li><li class="listitem">ThresholdUserNeighborhood</li></ul></div></li><li class="listitem"><span class="strong"><strong>Recommender</strong></span>: A recommender is the core abstraction in Mahout. Given the <code class="literal">DataModel</code> object as the input, it produces recommendations for items to users. The implementations of the recommender interface are as follows:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">AbstractRecommender</li><li class="listitem">CachingRecommender</li><li class="listitem">GenericBooleanPrefItemBasedRecommender</li><li class="listitem">GenericBooleanPrefUserBasedRecommender</li><li class="listitem">GenericItemBasedRecommender</li><li class="listitem">GenericUserBasedRecommender</li><li class="listitem">ItemAverageRecommender</li><li class="listitem">ItemUserAverageRecommender</li><li class="listitem">RandomRecommender</li><li class="listitem">RecommenderWrapper</li><li class="listitem">SVDRecommender</li></ul></div></li></ul></div></div><div class="section" title="Building recommendation engines using Mahout"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec107"/>Building recommendation engines using Mahout</h2></div></div></div><p>Now that we have covered the core building blocks of the Mahout recommendation engine framework, let's start building recommendations. In this section, we will look at a series of different recommendation engines implemented using the standalone mode. The recommendation engine capabilities are using implementations of the <code class="literal">org.apache.mahout.cf.taste.impl</code> package.</p><p>The recommendation engines we see in this section are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">User-based collaborative filtering</li><li class="listitem">Item-based collaborative filtering</li><li class="listitem">SVD recommenders</li></ul></div></div><div class="section" title="Dataset description"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec108"/>Dataset description</h2></div></div></div><p>Before we get into recommender implementations, let's look at the dataset we use in this section. For this section, we use the restaurant and consumer data dataset available from the UCI machine learning dataset repository from the following URL:</p><p><a class="ulink" href="https://archive.ics.uci.edu/ml/datasets/Restaurant+%26+consumer+data">https://archive.ics.uci.edu/ml/datasets/Restaurant+%26+consumer+data</a></p><p>This dataset can be used to build collaborative filtering applications using consumer preference information. The dataset, the file downloaded from the previous link, contains nine files listed in the following figure. Of all the files in this exercise, we use the <code class="literal">rating_final.csv</code> file, which contains attributes such as userID, placeID, rating, food_rating, and service_rating. But for our use cases, we only use userID, placeID, and rating. We can think of the data as a preference value given to Place by a given user.</p><div class="note" title="Note"><h3 class="title"><a id="note25"/>Note</h3><p>We will have to make use of the previously created project in the setup session.</p><p>Add the input <code class="literal">ratings_final.csv</code> file to the <span class="emphasis"><em>data</em></span> folder to the current project structure.</p></div><p>So first, let's preprocess the original raw data into the required format of the userID, placeID, and rating triplet. Here's the raw dataset used for this exercise:</p><div class="mediaobject"><img src="../Images/image00475.jpeg" alt="Dataset description"/></div><p style="clear:both; height: 1em;"> </p><p>The following program will prepare the required triplet dataset, implemented as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Read each line from the <code class="literal">ratings_final.csv</code> file</li><li class="listitem">Extract the first three columns</li><li class="listitem">Write the extracted columns from the previous step to a new <code class="literal">recoDataset.csv</code> file</li></ul></div><p>The following java program implements the previously explained steps:</p><pre class="programlisting">package com.packtpub.mahout.recommenders; &#13;
 &#13;
import java.io.FileReader; &#13;
import java.io.FileWriter; &#13;
import java.io.IOException; &#13;
import java.util.ArrayList; &#13;
import java.util.List; &#13;
 &#13;
import au.com.bytecode.opencsv.CSVReader; &#13;
import au.com.bytecode.opencsv.CSVWriter; &#13;
 &#13;
public class Preprocessdata  { &#13;
 &#13;
public static void main(String[] args) throws IOException { &#13;
String fileName = "data/rating_final.csv"; &#13;
String csv = "data/recoDataset.csv";          &#13;
CSVReader csvReader = new CSVReader(new FileReader(fileName)); &#13;
String[] row = null; &#13;
List&lt;String[]&gt; data = new ArrayList&lt;String[]&gt;(); &#13;
CSVWriter writer = new CSVWriter(new FileWriter(csv), &#13;
CSVWriter.DEFAULT_SEPARATOR, &#13;
CSVWriter.NO_QUOTE_CHARACTER); &#13;
while((row = csvReader.readNext()) != null) { &#13;
if(!row[0].contains("userID")){ &#13;
data.add(new String[] {row[0].substring(1), row[1],row[2]}); &#13;
} &#13;
} &#13;
writer.writeAll(data); &#13;
writer.close(); &#13;
csvReader.close(); &#13;
} &#13;
 &#13;
} &#13;
</pre><p>Upon running the preceding java program, the final dataset that we use to build recommendation engines will be created under the <span class="emphasis"><em>data</em></span> folder as the <code class="literal">recoDataset.csv</code> file. The following is a sample dataset:</p><div class="mediaobject"><img src="../Images/image00476.jpeg" alt="Dataset description"/></div><p style="clear:both; height: 1em;"> </p><p>Now that we have preprocessed the required data, let's start building our recommendation engines with the Mahout framework.</p></div><div class="section" title="User-based collaborative filtering"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec109"/>User-based collaborative filtering</h2></div></div></div><p>Just for the sake of a refresher: the user-based recommender system generates recommendations based on the UserSimilarity calculation between users and then uses UserNeighborhood to choose top-N users and then generate recommendations.</p><p>Let's first execute the following code and then we shall look at the code line by line. We will use the Euclidean Distance similarity and Nearest Neighborhood methods to generate recommendations:</p><pre class="programlisting">package com.packtpub.mahout.recommenders; &#13;
 &#13;
import java.io.File; &#13;
import java.io.IOException; &#13;
import java.util.List; &#13;
 &#13;
import org.apache.mahout.cf.taste.common.TasteException; &#13;
import org.apache.mahout.cf.taste.impl.model.file.FileDataModel; &#13;
import org.apache.mahout.cf.taste.impl.neighborhood.NearestNUserNeighborhood; &#13;
import org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender; &#13;
import org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarity; &#13;
import org.apache.mahout.cf.taste.model.DataModel; &#13;
import org.apache.mahout.cf.taste.neighborhood.UserNeighborhood; &#13;
import org.apache.mahout.cf.taste.recommender.RecommendedItem; &#13;
import org.apache.mahout.cf.taste.recommender.UserBasedRecommender; &#13;
import org.apache.mahout.cf.taste.similarity.UserSimilarity; &#13;
 &#13;
//class for generating User Based Recommendation &#13;
public class UserbasedRecommendations &#13;
{ &#13;
    public static void main( String[] args ) throws TasteException, IOException &#13;
    { &#13;
    //creating data model &#13;
    DataModel model = new FileDataModel(new File("data/recoDataset.csv"));      &#13;
    // creating Euclidean distance similarity between users  &#13;
    UserSimilarity similarity = new EuclideanDistanceSimilarity(model); &#13;
    //creating user neighborhood &#13;
    UserNeighborhood neighborhood = new NearestNUserNeighborhood(10, similarity, model); &#13;
    // creating recommender model &#13;
    UserBasedRecommender recommender = new GenericUserBasedRecommender(model, neighborhood, similarity); &#13;
    //generating 3 recommendations for user 1068 &#13;
    List&lt;RecommendedItem&gt; recommendations = recommender.recommend(1068, 3); &#13;
    for (RecommendedItem recommendation : recommendations) { &#13;
      System.out.println(recommendation); &#13;
    } &#13;
    } &#13;
} &#13;
</pre><p>Running this program generates recommendations shown in the following figures. We are generating the top three user-based item recommendations to <code class="literal">UserId - 1068</code>:</p><p>From the result, we can conclude that for <code class="literal">UserId - 1068</code>, the top three recommended places along with similarity values are as follows:</p><div class="mediaobject"><img src="../Images/image00477.jpeg" alt="User-based collaborative filtering"/></div><p style="clear:both; height: 1em;"> </p><p>Let's now look at the code line by line; just recall the core building blocks of the Mahout recommendations section. We need DataModel, Similarity calculation, UserNeighborhood, recommender, and generating recommendations. This order is used in the previous code:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The code in the <code class="literal">UserbasedRecommender.main</code> method creates a data source from the <code class="literal">data/recoDataset.csv</code> CSV file using <code class="literal">org.apache.mahout.cf.taste.impl.model.file.FileDataModel.FileDataModel class</code>. This class constructor gets the <code class="literal">Java.io.File</code> instance containing the preferences data and creates the <code class="literal">DataModel</code> class instance model:<pre class="programlisting">        //creating data model &#13;
        DataModel model = new FileDataModel(new &#13;
          File("data/recoDataset.csv")); &#13;
</pre></li><li class="listitem">In this step, we create the UserSimilarity instance: the similarity calculation between all users using <code class="literal">org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarity class</code>, which takes the <code class="literal">FileDataModel</code> instance created in the previous step as the constructor parameter:<pre class="programlisting">        // creating Euclidean distance similarity between users  &#13;
        UserSimilarity similarity = new &#13;
          EuclideanDistanceSimilarity(model); &#13;
     &#13;
</pre></li><li class="listitem">In this step, we create the UserNeighborhood instance: the neighborhood using <code class="literal">org.apache.mahout.cf.taste.impl.neighborhood.NearestNUserNeighborhood </code>class, and it takes three parameters: the number of nearest neighbors to be considered, the UserSimilarity instance-similarity, the DataModel instance which is the model created in the previous steps as inputs:<pre class="programlisting">        //creating user neighborhood &#13;
        UserNeighborhood neighborhood = new &#13;
          NearestNUserNeighborhood(10, similarity, model); &#13;
</pre></li><li class="listitem">The next step is to generate a recommender model. This is achieved using the <code class="literal">org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender class</code> instance. A <code class="literal">GenericUserBasedRecommender</code> instance-the recommender is created by passing the DataModel instance model, the UserNeighborhood instance neighborhood, the UserSimilarity instance similarity as inputs to the constructer while creating the recommender object.<pre class="programlisting">        // creating recommender model &#13;
        UserBasedRecommender recommender = new   &#13;
          GenericUserBasedRecommender(model, neighborhood, similarity); &#13;
   &#13;
</pre></li><li class="listitem">Kudos! We have created our user-based recommender system using the Euclidean Distance similarity and the <code class="literal">NearestNNeighborhhood</code> method to create a recommender model. Now the next step would be to generate recommendations; for this, we call the <code class="literal">recommend()</code> method available in the recommender object, which takes <code class="literal">UserId</code> for which the recommendations and the number of recommendations have to be generated:<pre class="programlisting">        //generating 3 recommendations for user 1068 &#13;
        List&lt;RecommendedItem&gt; recommendations = &#13;
          recommender.recommend(1068, 3); &#13;
</pre></li></ol><div style="height:10px; width: 1px"/></div><p>This step has generated three item recommendations to the <code class="literal">UserId</code> 1068 along with the strength of the preference.</p><p>In our case, we generated the following recommendations:</p><pre class="programlisting">item:132613, value:1.2205102 &#13;
item:132667, value:1.0 &#13;
item:132584, value:0.98069793 &#13;
</pre></div></div>
<div class="section" title="Item-based collaborative filtering" id="aid-1UU541"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec59"/>Item-based collaborative filtering</h1></div></div></div><p>Item-based recommenders recommend similar items to users by considering the similarity between items instead of the similarity of users, as shown in the previous section.</p><p>The following is the given java program to build item-based collaborative filtering. We have used <code class="literal">LogLikelihoodSimilarity</code> to calculate <code class="literal">ItemSimilarity</code>, and then we used the <code class="literal">GenericItemBasedRecommender</code> class to recommend items to users. In addition, we can see how to check similar items for a given item using the <code class="literal">mostSimilarItems</code> method present in <code class="literal">GenericItemBasedRecommender</code>:</p><pre class="programlisting">package com.packpub.mahout.recommendationengines; &#13;
 &#13;
import java.io.File; &#13;
import java.io.IOException; &#13;
import java.util.List; &#13;
 &#13;
import org.apache.mahout.cf.taste.common.TasteException; &#13;
import org.apache.mahout.cf.taste.impl.model.file.FileDataModel; &#13;
import org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender; &#13;
import org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarity; &#13;
import org.apache.mahout.cf.taste.model.DataModel; &#13;
import org.apache.mahout.cf.taste.recommender.RecommendedItem; &#13;
import org.apache.mahout.cf.taste.similarity.ItemSimilarity; &#13;
 &#13;
 &#13;
public class ItembasedRecommendations { &#13;
 &#13;
public static void main(String[] args) throws TasteException, IOException { &#13;
DataModel model = new FileDataModel(new File("data/recoDataset.csv")); &#13;
    ItemSimilarity similarity = new LogLikelihoodSimilarity(model); &#13;
    GenericItemBasedRecommender recommender = new GenericItemBasedRecommender(model, similarity); &#13;
    System.out.println("*********Recommend Items to Users********"); &#13;
    List&lt;RecommendedItem&gt; recommendations = recommender.recommend(1068, 3); &#13;
    for (RecommendedItem recommendation : recommendations) { &#13;
      System.out.println(recommendation); &#13;
    } &#13;
     System.out.println("*********Most Similar Items********"); &#13;
    List&lt;RecommendedItem&gt; similarItems = recommender.mostSimilarItems(135104, 3); &#13;
    for (RecommendedItem similarItem : similarItems) { &#13;
      System.out.println(similarItem); &#13;
    } &#13;
} &#13;
 &#13;
} &#13;
</pre><p>Running the previous program will generate three items most similar to the input item, in our case, for the placeID <code class="literal">135104</code>, the most similar placeID attributes along with the strength of the similarity is shown in the following screenshot:</p><div class="mediaobject"><img src="../Images/image00478.jpeg" alt="Item-based collaborative filtering"/></div><p style="clear:both; height: 1em;"> </p><p>Let's look at each step of the preceding program in order to understand what's happening in the preceding implementation:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step, like in the previous section, is to create the DataModel instance using the <code class="literal">org.apache.mahout.cf.taste.impl.model.file.FileDataModel</code> class:<pre class="programlisting">        //we create DataModel instance - model  &#13;
        DataModel model = new FileDataModel(new &#13;
          File("data/recoDataset.csv")); &#13;
</pre></li><li class="listitem">In this step, we create the ItemSimilarity instance, the similarity calculation between all users using the <code class="literal">org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarity</code> class, which takes the <code class="literal">FileDataModel</code> instance created in the previous step as the constructor parameter:<pre class="programlisting">        // creating LogLikelihood distance similarity between users  &#13;
        ItemSimilarity similarity = new LogLikelihoodSimilarity &#13;
          (model);  &#13;
</pre></li><li class="listitem">The next step is to generate a recommender model. This is achieved using the <code class="literal">org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender</code> class instance. A <code class="literal">GenericItemBasedRecommender</code> instance recommender is created passing the DataModel instance which is the model ItemSimilarity instance-similarity as inputs to the constructer while creating the recommender object.<pre class="programlisting">        // creating recommender model &#13;
        GenericItemBasedRecommender recommender = new    &#13;
          GenericItemBasedRecommender(model, similarity); &#13;
   &#13;
</pre><div class="note" title="Note"><h3 class="title"><a id="note26"/>Note</h3><p>The choice of the similarity metric is left to you; it is set as per your requirement.</p></div></li><li class="listitem">Kudos! We have created our item-based recommender system using the <code class="literal">LogLikelihood</code> similarity to create a recommender model. Now the next step would be to generate recommendations, and for this, we call the <code class="literal">recommend()</code> method available in the recommender object, which takes <code class="literal">UserId</code> for which the recommendations and the number of recommendations have to be generated:<pre class="programlisting">        //generating 3 recommendations for user 1068 &#13;
        List&lt;RecommendedItem&gt; recommendations = &#13;
          recommender.recommend(1068, 3); &#13;
</pre><p>This step has generated three item recommendations to the UserID 1068 along with the strength of the preference.</p><p>In our case, we generated the following recommendations:</p><pre class="programlisting">        item:132613, value:1.2205102 &#13;
        item:132667, value:1.0 &#13;
        item:132584, value:0.98069793 &#13;
</pre></li><li class="listitem">Imagine that we want to see items similar to a particular item; recommender interfaces, such as the <code class="literal">GenericItemBasedRecommender</code> class in our example, provide the <code class="literal">mostSimilarItems()</code> method, which takes <code class="literal">UserId</code>, the number of items to be displayed as inputs, and extracts <code class="literal">similarItems</code> for a given item:</li></ol><div style="height:10px; width: 1px"/></div><pre class="programlisting">        List&lt;RecommendedItem&gt; similarItems =    &#13;
          recommender.mostSimilarItems(135104, 3); &#13;
</pre><p>In our example, the three places most similar to <code class="literal">PlaceId</code> 135104 are shown as follows:</p><pre class="programlisting">item:132667, value:0.96383345 &#13;
item:132732, value:0.9602005 &#13;
item:132733, value:0.9543598 &#13;
</pre><p>In the following section, let's evaluate the recommendations we have created so far.</p></div>
<div class="section" title="Evaluating collaborative filtering" id="aid-1VSLM1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec60"/>Evaluating collaborative filtering</h1></div></div></div><p>We have seen how to build recommendations using collaborative filtering approaches. But the key thing is to build efficient recommendations. Evaluating the accuracy of the recommender models - what we built - is a very crucial step in building recommendation engines. In this section, we will look at how to evaluate both user-based recommenders and item-based recommenders.</p><p>Mahout provides components that enable us to evaluate the accuracy of the recommendation models we have built so far. We can evaluate how closely our recommendation engine estimates the preferences against the actual preference values. We can instruct Mahout to use part of the original training data to set aside and use this test dataset in order to calculate the accuracy of the model.</p><p>We can use any of the following listed recommender evaluators provided by Mahout as per our requirement:</p><div class="mediaobject"><img src="../Images/image00479.jpeg" alt="Evaluating collaborative filtering"/></div><p style="clear:both; height: 1em;"> </p><p>Recommender evaluation using Mahout usually requires two steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">Creating an instance of the <code class="literal">org.apache.mahout.cf.taste.impl.eval.RMSRecommenderEvaluator</code> class available from the preceding list, which will create the accuracy score</li><li class="listitem">Implementing the inner interface for <code class="literal">org.apache.mahout.cf.taste.eval.RecommenderBuilder</code> so as to create the recommender that the <code class="literal">RecommenderEvaluator</code> class instance from the previous step can use to produce the accuracy score</li></ul></div><p>The listing shows the java implementation for user-based recommender model evaluation. For this exercise, we have used the root mean squared error evaluation technique.</p></div>
<div class="section" title="Evaluating user-based recommenders" id="aid-20R681"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec61"/>Evaluating user-based recommenders</h1></div></div></div><p>In this section, we shall see the code for evaluating the user-based recommendations we built in the previous section:</p><pre class="programlisting">package com.packtpub.mahout.recommenders; 
 
import java.io.File; 
import java.io.IOException; 
 
import org.apache.mahout.cf.taste.common.TasteException; 
import org.apache.mahout.cf.taste.eval.RecommenderBuilder; 
import org.apache.mahout.cf.taste.eval.RecommenderEvaluator; 
import org.apache.mahout.cf.taste.impl.eval.RMSRecommenderEvaluator; 
import org.apache.mahout.cf.taste.impl.model.file.FileDataModel; 
import org.apache.mahout.cf.taste.impl.neighborhood.NearestNUserNeighborhood; 
import org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommender; 
import org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarity; 
import org.apache.mahout.cf.taste.model.DataModel; 
import org.apache.mahout.cf.taste.neighborhood.UserNeighborhood; 
import org.apache.mahout.cf.taste.recommender.Recommender; 
import org.apache.mahout.cf.taste.similarity.UserSimilarity; 
 
public class EvaluateUBCFRecommender { 
 
public static void main(String[] args) throws IOException, TasteException { 
 
DataModel model = new FileDataModel(new File("data/recoDataset.csv")); 
RecommenderEvaluator evaluator = new RMSRecommenderEvaluator(); 
RecommenderBuilder builder = new RecommenderBuilder() { 
public Recommender buildRecommender(DataModel model) 
throws TasteException { 
UserSimilarity similarity = new EuclideanDistanceSimilarity(model); 
UserNeighborhood neighborhood = 
new NearestNUserNeighborhood (10, similarity, model); 
return 
new GenericUserBasedRecommender (model, neighborhood, similarity); 
} 
}; 
double score = evaluator.evaluate( 
builder, null, model, 0.8, 1.0); 
System.out.println(score); 
} 
 
} 
</pre><p>Executing the preceding program will give us the model accuracy: <code class="literal">0.692216091226208</code>.</p></div>
<div class="section" title="Evaluating item-based recommenders" id="aid-21PMQ1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec62"/>Evaluating item-based recommenders</h1></div></div></div><p>Below code snippet will be used in evaluating the item-based recommendations:</p><pre class="programlisting">package com.packtpub.mahout.recommenders; 
 
import java.io.File; 
import java.io.IOException; 
 
import org.apache.mahout.cf.taste.common.TasteException; 
import org.apache.mahout.cf.taste.eval.RecommenderBuilder; 
import org.apache.mahout.cf.taste.eval.RecommenderEvaluator; 
import org.apache.mahout.cf.taste.impl.eval.RMSRecommenderEvaluator; 
import org.apache.mahout.cf.taste.impl.model.file.FileDataModel; 
import org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender; 
import org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarity; 
import org.apache.mahout.cf.taste.model.DataModel; 
import org.apache.mahout.cf.taste.recommender.Recommender; 
import org.apache.mahout.cf.taste.similarity.ItemSimilarity; 
 
public class EvaluateIBCFRecommender { 
 
public static void main(String[] args) throws IOException, TasteException { 
 
DataModel model = new FileDataModel(new File("data/recoDataset.csv")); 
//RMS Recommender Evaluator 
RecommenderEvaluator evaluator = new RMSRecommenderEvaluator(); 
RecommenderBuilder builder = new RecommenderBuilder() { 
public Recommender buildRecommender(DataModel model) 
throws TasteException { 
ItemSimilarity similarity = new LogLikelihoodSimilarity(model); 
return 
new GenericItemBasedRecommender(model, similarity); 
} 
}; 
double score = evaluator.evaluate(builder, null, model, 0.7, 1.0); 
System.out.println(score); 
 
} 
 
} 
</pre><p>Executing the previous program will give us the model accuracy: <code class="literal">0.6041129199039021</code>.</p><p>Now let's look at this evaluation implementation step by step:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The first step is to create a DataModel instance model using the <code class="literal">org.apache.mahout.cf.taste.impl.model.file.FileDataModel </code>class:<pre class="programlisting">        DataModel model = new FileDataModel(new 
          File("data/recoDataset.csv")); 
</pre></li><li class="listitem">In this step, we create the <code class="literal">org.apache.mahout.cf.taste.impl.eval.RMSRecommenderEvaluator</code> instance evaluator, which will calculate the recommendation engine accuracy:<pre class="programlisting">        // Recommendation engine model evaluator engine  
        RecommenderEvaluator evaluator = new RMSRecommenderEvaluator();     
</pre></li><li class="listitem">In this step, we implement the <code class="literal">org.apache.mahout.cf.taste.eval.RecommenderBuilder</code> interface to create the recommender of our choice.<p>Let's use the same recommender models we used for both user-based and item-based recommenders in the previous section:</p><pre class="programlisting">        // User based recommenders 
        public Recommender buildRecommender(DataModel model) 
        throws TasteException { 
        UserSimilarity similarity = new  
          EuclideanDistanceSimilarity(model); 
        UserNeighborhood neighborhood = 
        new NearestNUserNeighborhood (2, similarity, model); 
        return 
        new GenericUserBasedRecommender (model, neighborhood, 
          similarity); 
        } 
        }; 
 
        //Item based recommenders 
        public Recommender buildRecommender(DataModel model) 
        throws TasteException { 
        ItemSimilarity similarity = new LogLikelihoodSimilarity(model); 
        return 
        new GenericItemBasedRecommender(model, similarity); 
        } 
        }; 
</pre></li><li class="listitem">Now we are ready to calculate the recommendation accuracy. For this, we use the <code class="literal">evaluate()</code> method from the evaluator instance. The <code class="literal">evaluate()</code> method does not accept a recommender instance--which we created in user-based/item-based recommenders directly--but it accepts RecommenderBuilder, created in step 3 of our examples/index, which can build the recommender to test the accuracy on top of a given DataModel.<p>The <code class="literal">evaluate()</code> method takes four parameters: the recommender builder created in step 3, the DataModel object created in step 1, the DataModel builder object that we don't need for our example, the training percentage--in our case, we used 0.7 % as the training dataset and 0.3 as the test dataset--, evaluation percentage, the percentage of users to be used in evaluation.</p><p>The <code class="literal">evaluate()</code> method returns the accuracy score of the model, which is how well the recommender-predicted preferences match the real values. Lower values indicate a better match, with 0 being the perfect match:</p><pre class="programlisting">        //generating 3 recommendations for user 1068 
        double score = evaluator.evaluate(builder, null, model, 0.7, 
          1.0);   
</pre></li></ol><div style="height:10px; width: 1px"/></div></div>
<div class="section" title="SVD recommenders" id="aid-22O7C1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec63"/>SVD recommenders</h1></div></div></div><p>Similar to the item-based and user-based recommender systems explained earlier, we can also use model-based recommender implementations in Mahout, such as <code class="literal">SVDRecommender</code>, which uses matrix factorization methods to generate recommendations.</p><p>The steps are similar to previous implementations. Two important steps that need to be understood here are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The<code class="literal"> org.apache.mahout.cf.taste.impl.recommender.svd.ALSWRFactorizer</code> class, which factorizes the user rating matrix using <span class="emphasis"><em>Alternating-Least-Squares with Weighted-λ-Regularization</em></span>. The <code class="literal">ALSWRFactorizer</code> class constructor takes parameters such as DataModel, the number of features, the regularization parameter, and the number of iterations as inputs. This <code class="literal">ALSWRFactorizer</code> class instance is passed as the input parameter to the recommender object: the <code class="literal">SVDRecommender</code> class.</li><li class="listitem">The<code class="literal"> org.apache.mahout.cf.taste.impl.recommender.svd.SVDRecommender </code>class generates the recommendation model by taking <code class="literal">DataModel</code> and <code class="literal">ALSWRFactorizer</code> objects.</li></ul></div><p>The rest of the other steps are very similar to what we saw in the previous examples:</p><p>The following code snippet shows how to build SVD recommender systems:</p><pre class="programlisting">package com.packpub.mahout.recommendationengines; &#13;
 &#13;
import java.io.File; &#13;
import java.io.IOException; &#13;
import java.util.List; &#13;
 &#13;
import org.apache.mahout.cf.taste.common.TasteException; &#13;
import org.apache.mahout.cf.taste.impl.model.file.FileDataModel; &#13;
import org.apache.mahout.cf.taste.impl.recommender.svd.ALSWRFactorizer; &#13;
import org.apache.mahout.cf.taste.impl.recommender.svd.SVDRecommender; &#13;
import org.apache.mahout.cf.taste.model.DataModel; &#13;
import org.apache.mahout.cf.taste.recommender.RecommendedItem; &#13;
 &#13;
 &#13;
public class UserBasedSVDRecommender { &#13;
 &#13;
public static void main(String[] args) throws TasteException, IOException { &#13;
//MF recommender model &#13;
    DataModel model = new FileDataModel(new File("data/dataset.csv"));    &#13;
    ALSWRFactorizer factorizer = new ALSWRFactorizer(model, 50, 0.065, 15); &#13;
    SVDRecommender recommender = new SVDRecommender(model, factorizer);     &#13;
    List&lt;RecommendedItem&gt; recommendations = recommender.recommend(2, 3); &#13;
    for (RecommendedItem recommendation : recommendations) { &#13;
      System.out.println(recommendation); &#13;
    } &#13;
 &#13;
} &#13;
 &#13;
} &#13;
</pre></div>
<div class="section" title="Distributed recommendations using Mahout"><div class="titlepage" id="aid-23MNU2"><div><div><h1 class="title"><a id="ch09lvl1sec64"/>Distributed recommendations using Mahout</h1></div></div></div><p>Up to now, we have seen how to build recommendation engines in the standalone mode. In most cases, the standalone implementations are very handy and they work quite efficiently in handling a million records provided we supply the dataset format, such as the userID, itemID, and preference triplet.</p><p>When the size of the data increases, the standalone mode might not be able to address the requirements. We need to look for ways to handle the enormous amount of data and be able to process the data to build recommendations. One approach is to port our standalone solution to the distributed mode, an example of which is Hadoop platforms.</p><p>The porting of the recommender solution to Hadoop is not straight forward, as the data will be distributed across the nodes. The memory-based models, such as neighbourhood recommenders, or model-based recommenders, such as Alternating Least Squares, requires the entire data to be available while generating the model, which will not be available on a distributed platform. Hence we need an entirely new design to build recommender systems.</p><p>Luckily, Mahout has removed the headaches in designing recommender implementations that can be distributed. These Mahout-distributed recommendation engine implementations are provided as jobs that internally run a series of map-reduce phases.</p><p>For example, Mahout-distributed recommendations using Alternating Least Squares consists of two jobs:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">A parallel matrix factorization job</li><li class="listitem">A recommendation job</li></ul></div><p>The matrix factorization job takes the user-item-rating file as the input and creates the user latent matrix that is a user feature matrix and an item feature matrix.</p><p>The recommendation job uses the latent feature matrices created using the matrix factorization job and computes Top-N recommendations.</p><p>The two jobs are executed sequentially, the input data is read from HDFS, and final recommendations are written to HDFS.</p><p>In this section, we shall look at how to generate recommendations using the item-based recommendation engine and Alternating Least Squares methods using Hadoop. Let's begin.</p><div class="section" title="ALS recommendation on Hadoop"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec110"/>ALS recommendation on Hadoop</h2></div></div></div><p>To build the recommendation using ALS implementations, the following are the steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Load data to the Hadoop platform. The ALS implementation of Mahout expects the input to be a triplet: userID, itemID, and preference value (explicit rating/implicit rating).</li><li class="listitem">Execute the ALS recommendation engine implementation job; this job will create user and item latent matrices by taking the input dataset from step 1.</li><li class="listitem">Execute the recommender job that takes the user-item latent feature matrices created in step 2 and generate Top-N recommendations.</li></ol><div style="height:10px; width: 1px"/></div><p>Let's execute all the steps one by one.</p><div class="note" title="Note"><h3 class="title"><a id="note27"/>Note</h3><p>For the following exercise, we are using CDH 5 and Centos 6.
This is assuming <code class="literal">JAVA_HOME</code> is set and Mahout is installed properly.</p></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Load data to the Hadoop platform as follows:<pre class="programlisting">
<span class="strong"><strong>#create a directory to store the input data using mkdir command</strong></span>
<span class="strong"><strong>[cloudera@quickstart ~]$ hadoop fs -mkdir mahout</strong></span>
</pre><p>Let's check whether we have created the directory properly using the <code class="literal">ls </code>command:</p><pre class="programlisting">
<span class="strong"><strong>[cloudera@quickstart ~]$ hadoop fs -ls</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>drwxr-xr-x   - cloudera cloudera          0 2016-11-14 18:31 mahout</strong></span>
</pre><p>Now let's load data to the HDFS using the <code class="literal">copyFromLocal</code> command:</p><pre class="programlisting">
<span class="strong"><strong>hadoop fs -copyFromLocal /home/cloudera/datasets/u.data mahout</strong></span>
</pre><div class="note" title="Note"><h3 class="title"><a id="note28"/>Note</h3><p>The input data is the MovieLens dataset that consists of one million rating data.</p></div><p>Let's verify that the data is loaded properly using the <code class="literal">ls</code> command:</p><pre class="programlisting">
<span class="strong"><strong>[cloudera@quickstart ~]$ hadoop fs -ls mahout</strong></span>
<span class="strong"><strong>Found 1 items</strong></span>
<span class="strong"><strong>-rw-r--r--   1 cloudera cloudera    1979173 2016-11-14 18:32 mahout/u.data</strong></span>
</pre><p>Now that we have seen that the data is loaded properly, let's look at the first few records of the input data:</p><div class="mediaobject"><img src="../Images/image00480.jpeg" alt="ALS recommendation on Hadoop"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">Create <span class="strong"><strong>User</strong></span> and <span class="strong"><strong>Item latent</strong></span> matrices. To create the latent feature matrices, we need to run the following commands from the command line:<pre class="programlisting">
<span class="strong"><strong>$MAHOUT_HOME\bin\mahout parallelALS \</strong></span>
<span class="strong"><strong>    --input mahout \</strong></span>
<span class="strong"><strong>    --output output \</strong></span>
<span class="strong"><strong>    --lambda 0.1 \</strong></span>
<span class="strong"><strong>    --implicitFeedback false \</strong></span>
<span class="strong"><strong>    --numFeatures 10 \</strong></span>
<span class="strong"><strong>    --numIterations 1  \</strong></span>
<span class="strong"><strong>    --tempDir tmp</strong></span>
</pre><p>Let's look at each of the command parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>$MAHOUT_HOME\bin\mahout</strong></span>: This is the executable file that runs the underlying matrix factorization job.</li><li class="listitem"><span class="strong"><strong>parallelALS</strong></span>: This is the name of the algorithm to be applied on the input dataset. The <code class="literal">parallelALS</code> command invokes the underlying <code class="literal">ParallelALSFactorizationJob class object</code>, which is a map-reduce implementation of the factorization algorithms described in <span class="emphasis"><em>Large-scale Parallel Collaborative Filtering for the Netflix Prize</em></span>.</li><li class="listitem"><span class="strong"><strong>--input</strong></span>: This is the HDFS input path of the input ratings data.</li><li class="listitem"><span class="strong"><strong>--output</strong></span>: This is the path where the output latent matrices for the user and item will be generated.</li><li class="listitem"><span class="strong"><strong>--lambda</strong></span>: This is the regularization parameter given in order to avoid overfitting.</li><li class="listitem"><span class="strong"><strong>--alpha</strong></span>: This is the confidence parameter used for implicit feedback only.</li><li class="listitem"><span class="strong"><strong>--implicitFeatures</strong></span>: This is the Boolean value to state whether the preference values are true or false. In our case, they are false.</li><li class="listitem"><span class="strong"><strong>--numIterations</strong></span>: This is the total number of times the model gets recomputed by applying the learnings from the previous model to the new model.</li><li class="listitem"><span class="strong"><strong>--tempDir</strong></span>: This is the path to the temporary directory where the intermediate results are written.</li></ul></div><p>On executing the command we saw, three datasets are created in the <span class="emphasis"><em>output</em></span> directory:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>U</strong></span>: This contains the user latent feature matrix</li><li class="listitem"><span class="strong"><strong>M</strong></span>: The contains the item latent feature matrix</li><li class="listitem"><span class="strong"><strong>userRatings: </strong></span>All the outputs are of a sequence file format.</li></ul></div></li><li class="listitem">Generate recommendations for all the users. This step takes the <span class="emphasis"><em>output</em></span> results stored to HDFS from the previous step as the input, generates recommendations, and writes the final recommendations to the <span class="emphasis"><em>recommendations output</em></span> directory on HDFS.<p>The following command will invoke the <code class="literal">org.apache.mahout.cf.taste.hadoop.als.RecommenderJob</code> recommender job, which internally calls an <code class="literal">org.apache.mahout.cf.taste.hadoop.als.PredictionMapper</code> class to generate recommendations:</p><pre class="programlisting">
<span class="strong"><strong>$MAHOUT_HOME\bin\mahout recommendfactorized \</strong></span>
<span class="strong"><strong>       --input output/userRatings/  \</strong></span>
<span class="strong"><strong>       --userFeatures output/U/ \</strong></span>
<span class="strong"><strong>       --itemFeatures output/M/ \</strong></span>
<span class="strong"><strong>       --numRecommendations 15 \</strong></span>
<span class="strong"><strong>       --output recommendations/topNrecommendations \</strong></span>
<span class="strong"><strong>       --maxRating 5</strong></span>
</pre><p>Let's look at each of the parameters in detail:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong>-- input</strong></span>: This is the HDFS path containing the list of the userID file to be used to generate recommendations in the sequence file format. In our example, the <span class="emphasis"><em>output/userRatings</em></span> directory contains all the userID to be used to generate recommendations in the sequence file format; this file is the output of step 2.</li><li class="listitem"><span class="strong"><strong>--userFeatures</strong></span>: This is the HDFS path containing user latent features generated as the output in step 2.</li><li class="listitem"><span class="strong"><strong>--itemFeatures</strong></span>: This is the HDFS path containing item latent features generated as the output in step 2.</li><li class="listitem"><span class="strong"><strong>--numRecommendations</strong></span>: The number of recommendations to be generated per user.</li><li class="listitem"><span class="strong"><strong>--output recommendations</strong></span>: This is the HDFS path where the final recommendations have to be generated.</li><li class="listitem"><span class="strong"><strong>--maxRating</strong></span>: This is the maximum rating that the generated recommendations should contain.</li></ul></div></li></ol><div style="height:10px; width: 1px"/></div><p>Upon running the previous commands in the command line, recommendations are generated into the recommendations folder on HDFS, as follows:</p><div class="mediaobject"><img src="../Images/image00481.jpeg" alt="ALS recommendation on Hadoop"/></div><p style="clear:both; height: 1em;"> </p><p>In the earlier result, we can see the first ten user recommendations in order. Each user vector contains itemID and the rating that the algorithm has predicted. While serving recommendations, we can just send recommendations as is.</p><p>Now you may get a question like this: what if I want to generate recommendations to specific users? Mahout supports such scenarios as well. Remember the input parameter in step 3? Just provide the HDFS path containing the userID which we need for recommendations. But make sure that the input path containing the userID are in a sequence file format.</p></div></div>
<div class="section" title="The architecture for a scalable system" id="aid-24L8G1"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec65"/>The architecture for a scalable system</h1></div></div></div><p>Taking the recommendation engine system to production is the same as any other system. The previous figure shows a very simple recommendation engine deployed on a production system:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">The production system is Centos 6 with Java 8 and the Apache Tomcat server installed on the system. CDH 5 and the Mahout 0.12 version is also installed on it so that the recommender jobs we have built so far can be deployed:<div class="mediaobject"><img src="../Images/image00482.jpeg" alt="The architecture for a scalable system"/></div><p style="clear:both; height: 1em;"> </p></li><li class="listitem">The Java code we have written so far can be made as jar files and deployed on the production system. Schedule all the jobs at a regular interval as per our requirements.</li><li class="listitem">At a defined scheduled time, the recommender jobs start executing and the data is pulled from data sources, computes recommendation models, and generates recommendations.</li><li class="listitem">The data for the recommendation module will be read and written back to the HDFS file system.</li><li class="listitem">The frontend applications will read the final outputs from HDFS.</li></ul></div></div>
<div class="section" title="Summary" id="aid-25JP21"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec66"/>Summary</h1></div></div></div><p>In this chapter, we saw how to build recommendations using Apache Mahout. We looked at how we can leverage Mahout for both the standalone and the distributed mode. We have written Java code for user-based, item-based, and SVD-based recommendation engines in the standalone mode and Alternating Least Squares recommendations in the distributed mode. We also saw how we can evaluate the recommendation engine models. In the final section, we explored a very basic system of how to take Mahout to production.</p><p>In the final chapter, we shall cover the future of recommendation engines, where the recommendation engines are heading, and the promising use cases to lookout for.</p></div></body></html>