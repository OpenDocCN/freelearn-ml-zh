- en: '*Chapter 10*: Logistic Regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：逻辑回归'
- en: In this and the next few chapters, we will explore models for classification.
    These involve targets with two or several class values, such as whether a student
    will pass a class or not or whether a customer will choose chicken, beef, or tofu
    at a restaurant with only these three choices. There are several machine learning
    algorithms for these kinds of classification problems. We will take a look at
    some of the most popular ones in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和接下来的几章中，我们将探讨分类模型。这些模型涉及具有两个或多个类别值的目标，例如学生是否会通过一门课程，或者顾客在只有鸡肉、牛肉和豆腐三种选择的情况下会选择哪一种。对于这类分类问题，存在几种机器学习算法。在本章中，我们将查看其中一些最受欢迎的算法。
- en: Logistic regression has been used to build models with binary targets for decades.
    Traditionally, it has been used to generate estimates of the impact of an independent
    variable or variables on the odds of a dichotomous outcome. Since our focus is
    on prediction, rather than the effect of each feature, we will also explore regularization
    techniques, such as lasso regression. These techniques can improve the accuracy
    of our classification predictions. We will also examine strategies for predicting
    a multiclass target (when there are more than two possible target values).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归已经用于构建具有二元目标的模型数十年了。传统上，它被用来生成独立变量或变量对二元结果概率影响估计。由于我们的重点是预测，而不是每个特征的影响，我们还将探讨正则化技术，如lasso回归。这些技术可以提高我们的分类预测准确性。我们还将检查预测多类别目标（当存在超过两个可能的目标值时）的策略。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Key concepts of logistic regression
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归的关键概念
- en: Binary classification with logistic regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行二元分类
- en: Regularization with logistic regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行正则化
- en: Multinomial logistic regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式逻辑回归
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will stick to the libraries that are available in most
    scientific distributions of Python: pandas, NumPy, and scikit-learn. All the code
    in this chapter will run fine with scikit-learn versions 0.24.2 and 1.0.2.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将坚持使用在大多数Python科学发行版中可用的库：pandas、NumPy和scikit-learn。本章中的所有代码都可以在scikit-learn版本0.24.2和1.0.2上正常运行。
- en: Key concepts of logistic regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归的关键概念
- en: If you are familiar with linear regression, or read [*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091),
    *Linear Regression Models*, of this book, you have probably anticipated some of
    the issues we will discuss in this chapter – regularization, linearity among regressors,
    and normally distributed residuals. If you have built supervised machine learning
    models in the past or worked through the last few chapters of this book, then
    you have also likely anticipated that we will spend some time discussing the bias-variance
    tradeoff and how that influences our choice of model.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉线性回归，或者阅读了本书的*第7章*，*线性回归模型*，你可能会预见到我们将在本章讨论的一些问题——正则化、回归器的线性关系和正态分布的残差。如果你过去构建过监督机器学习模型，或者阅读过本书的最后一章，那么你很可能预见到我们将花一些时间讨论偏差-方差权衡以及它如何影响我们选择模型。
- en: I remember being introduced to logistic regression 35 years ago in a college
    course. It is often presented in undergraduate texts almost as a special case
    of linear regression; that is, linear regression with a binary dependent variable
    coupled with some transformation to keep predictions between 0 and 1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我记得35年前在一门大学课程中第一次接触到逻辑回归。在本科教科书中，它通常几乎被呈现为线性回归的一个特例；也就是说，具有二元因变量的线性回归，并伴随一些变换以保持预测值在0到1之间。
- en: It does share many similarities with linear regression of a numeric target variable.
    Logistic regression is relatively easy to train and interpret. Optimization techniques
    for both linear and logistic regression are efficient and can generate low bias
    predictors.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 它确实与数值目标变量的线性回归有许多相似之处。逻辑回归相对容易训练和解释。线性回归和逻辑回归的优化技术都是高效的，可以生成低偏差的预测器。
- en: 'Also like linear regression, logistic regression predicts a target based on
    weights assigned to each feature. But to constrain the predicted probability to
    between 0 and 1, we use the sigmoid function. This function takes any value and
    maps it to a value between 0 and 1:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，与线性回归一样，逻辑回归也是基于分配给每个特征的权重来预测目标。但为了将预测概率约束在0到1之间，我们使用sigmoid函数。这个函数将任何值映射到0到1之间的值：
- en: '![](img/B17978_10_001.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_10_001.jpg)'
- en: As *x* approaches infinity, ![](img/B17978_10_002.png) gets closer to 1\. As
    *x* approaches negative infinity, ![](img/B17978_10_003.png) gets closer to 0.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当*x*趋近于无穷大时，![](img/B17978_10_002.png)趋近于1。当*x*趋近于负无穷大时，![](img/B17978_10_003.png)趋近于0。
- en: 'The following plot illustrates a sigmoid function:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示说明了sigmoid函数：
- en: '![Figure 10.1 – Sigmoid function ](img/B17978_10_0011.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – Sigmoid函数](img/B17978_10_0011.jpg)'
- en: Figure 10.1 – Sigmoid function
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – Sigmoid函数
- en: 'We can plug the familiar equation for linear regression, ![](img/B17978_10_004.png),
    into the sigmoid function to predict the probability of class membership:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将线性回归的熟悉方程![](img/B17978_10_004.png)代入sigmoid函数来预测类成员的概率：
- en: '![](img/B17978_10_005.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_10_005.jpg)'
- en: 'Here ![](img/B17978_10_006.png) is the predicted probability of class membership
    in the binary case. The coefficients (the betas) can be converted into odds ratios
    for interpretation, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17978_10_006.png)是二元情况下类成员的预测概率。系数（β）可以转换为优势比以进行解释，如下所示：
- en: '![](img/B17978_10_007.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_10_007.jpg)'
- en: Here, *r* is the odds ratio and β is the coefficient. A 1-unit increase in the
    value of a feature multiplies the odds of class membership by ![](img/B17978_10_008.png).
    Similarly, for a binary feature, a true value has ![](img/B17978_10_009.png) times
    the odds of class membership as does a false value for that feature, all else
    being equal.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*r*是优势比，β是系数。特征值增加1个单位会乘以类成员的优势比![](img/B17978_10_008.png)。同样，对于二元特征，一个真值有![](img/B17978_10_009.png)倍类成员的优势比，而一个假值也有相同特征的优势比，其他条件相同。
- en: Logistic regression has several advantages as an algorithm for classification
    problems. Features can be dichotomous, categorical, or numeric, and do not need
    to be normally distributed. The target variable can have more than two possible
    values, as we will discuss later, and it can be nominal or ordinal. Another key
    advantage is that the relationship between features and the target is not assumed
    to be linear.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归作为分类问题的算法具有几个优点。特征可以是二元的、分类的或数值的，且不需要服从正态分布。目标变量可以具有超过两个的可能值，正如我们稍后将要讨论的，它可以是无序的或有序的。另一个关键优点是，特征与目标之间的关系不假设是线性的。
- en: The nomenclature here is a tad confusing. Why are we using a regression algorithm
    for a classification problem? Well, logistic regression predicts the probability
    of class membership. We apply a decision rule to those probabilities to predict
    membership. The default threshold is often 0.5 with binary targets. Instances
    with predicted probabilities greater than or equal to 0.5 get a positive class
    or 1 or True; those less than 0.5 are assigned 0 or False.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的命名有点令人困惑。为什么我们使用回归算法来解决分类问题？好吧，逻辑回归预测类成员的概率。我们应用决策规则来预测这些概率。对于二元目标，默认阈值通常是0.5；预测概率大于或等于0.5的实例被赋予正类或1或True；那些小于0.5的实例被分配0或False。
- en: Logistic regression extensions
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归的扩展
- en: We will consider two key extensions of logistic regression in this chapter.
    We will explore multiclass models – that is, those where the target has more than
    two values. We will also examine the regularization of logistic models to improve
    (lessen) variance.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将考虑逻辑回归的两个关键扩展。我们将探讨多类模型——即目标值超过两个的模型。我们还将检查逻辑模型的正则化以改善（减少）方差。
- en: 'A popular choice when constructing multiclass models is **multinomial logistic
    regression** (**MLR**). With MLR, the prediction probability distribution is a
    multinomial probability distribution. We can replace the equation we used for
    the binary classifier with a softmax function:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建多类模型时，**多项式逻辑回归**（**MLR**）是一个流行的选择。使用MLR，预测概率分布是一个多项式概率分布。我们可以用softmax函数替换我们用于二元分类器的方程：
- en: '![](img/B17978_10_010.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_10_010.jpg)'
- en: Here, ![](img/B17978_10_011.png). This calculates a probability for each class
    label, *j*, where *k* is the number of classes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B17978_10_011.png)。这为每个类别标签*j*计算一个概率，其中*k*是类别的数量。
- en: An alternative to multinomial logistic regression when we have more than two
    classes is **one-versus-rest** (**OVR**) logistic regression. This extension to
    logistic regression turns the multiclass problem into a binary problem, estimating
    the probability of class membership versus membership in all of the other classes.
    The key assumption here is that membership in each class is independent. We will
    use MLR in an example in this chapter. One advantage it has over OVR is that the
    predicted probabilities are more reliable.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们拥有超过两个类别时，**一对余**（**OVR**）逻辑回归是多项式逻辑回归的一个替代方案。这种逻辑回归的扩展将多类别问题转化为二分类问题，估计类别成员资格相对于所有其他类别的成员资格的概率。这里的关键假设是每个类别的成员资格是独立的。在本章的例子中，我们将使用MLR。它相较于OVR的一个优点是预测概率更加可靠。
- en: As mentioned previously, logistic regression has some of the same challenges
    as linear regression, including that the low bias of our predictions comes with
    high variance. This is more likely to be a problem when several features are highly
    correlated. Fortunately, we can deal with this with regularization, just as we
    saw in [*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091), *Linear Regression
    Models*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，逻辑回归与线性回归有一些相同的挑战，包括我们的预测的低偏差伴随着高方差。当几个特征高度相关时，这更有可能成为一个问题。幸运的是，我们可以通过正则化来解决这个问题，就像我们在[*第7章*](B17978_07_ePub.xhtml#_idTextAnchor091)，“线性回归模型”中看到的那样。
- en: 'Regularization adds a penalty to the loss function. We still seek to minimize
    the error, but also constrain the size of our parameters. **L1** regularization,
    also referred to as lasso regression, penalizes the absolute value of the weights
    (or coefficients):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化会给损失函数添加一个惩罚项。我们仍然寻求最小化误差，但同时也约束参数的大小。**L1**正则化，也称为lasso回归，惩罚权重（或系数）的绝对值：
- en: '![](img/B17978_10_012.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_10_012.jpg)'
- en: 'Here, *p* is the number of features and λ determines the strength of the regularization.
    **L2** regularization, also referred to as ridge regression, penalizes the squared
    values of the weights:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*p*是特征的数量，λ决定了正则化的强度。**L2**正则化，也称为岭回归，惩罚权重（或系数）的平方值：
- en: '![](img/B17978_10_013.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_10_013.jpg)'
- en: 'Both L1 and L2 regularization push the weights toward 0, though L1 regularization
    is more likely to lead to sparse models. In scikit-learn, we use the *C* parameter
    to adjust the value of λ, where *C* is just the inverse of λ:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化都将权重推向0，尽管L1正则化更有可能导致稀疏模型。在scikit-learn中，我们使用*C*参数来调整λ的值，其中*C*只是λ的倒数：
- en: '![](img/B17978_10_014.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_10_014.jpg)'
- en: We can get a balance between L1 and L2 with elastic net regression. With elastic
    net regression, we adjust the L1 ratio. A value of 0.5 uses L1 and L2 equally.
    We can use hyperparameter tuning to choose the best value for the L1 ratio.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过弹性网络回归在L1和L2之间取得平衡。在弹性网络回归中，我们调整L1比率。0.5的值表示L1和L2同等使用。我们可以使用超参数调整来选择L1比率的最佳值。
- en: Regularization can result in a model with lower variance, which is a good tradeoff
    when we are less concerned about our coefficients than we are with our predictions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化可能导致具有更低方差模型的产生，当我们对预测的关注超过对系数的关注时，这是一个很好的权衡。
- en: Before building a model with regularization, we will construct a fairly straightforward
    logistic model with a binary target. We will also spend a good amount of time
    evaluating that model. This will be the first classification model we will build
    in this book and model evaluation looks very different for those models than it
    does for regression models.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建具有正则化的模型之前，我们将构建一个相当简单的具有二元目标的逻辑模型。我们还将花大量时间评估该模型。这将是本书中我们将构建的第一个分类模型，并且模型评估对于这些模型与回归模型看起来非常不同。
- en: Binary classification with logistic regression
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归的二分类
- en: Logistic regression is often used to model health outcomes when the target is
    binary, such as whether the person gets a disease or not. We will go through an
    example of that in this section. We will build a model to predict if an individual
    will have heart disease based on personal characteristics such as smoking and
    alcohol drinking habits; health features, including BMI, asthma, diabetes, and
    skin cancer; and age.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当目标为二元时，逻辑回归常用于建模健康结果，例如，一个人是否患有疾病。在本节中，我们将通过一个例子来展示这一点。我们将构建一个模型，根据个人的吸烟和饮酒习惯、健康特征（包括BMI、哮喘、糖尿病和皮肤癌）以及年龄等个人特征来预测一个人是否会患有心脏病。
- en: Note
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this chapter, we will work exclusively with data on heart disease that’s
    available for public download at [https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease).
    This dataset is derived from the United States Center for Disease Control data
    on more than 400,000 individuals from 2020\. Data columns include whether respondents
    ever had heart disease, body mass index, ever smoked, heavy alcohol drinking,
    age, diabetes, and kidney disease. We will work with a 30,000 individual sample
    in this section to speed up the processing, but the full dataset is available
    in the same folder in this book’s GitHub repository.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专门使用可在[https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease](https://www.kaggle.com/datasets/kamilpytlak/personal-key-indicators-of-heart-disease)公开下载的心脏病数据。这个数据集来源于2020年美国疾病控制与预防中心超过40万个人的数据。数据列包括受访者是否曾经患有心脏病、体重指数、是否吸烟、大量饮酒、年龄、糖尿病和肾病。在本节中，我们将使用30,000个个体样本以加快处理速度，但完整的数据集可以在本书GitHub仓库的同一文件夹中找到。
- en: 'We will also do a little more preprocessing in this chapter than we have in
    previous chapters. We will integrate much of this work with our pipeline. This
    will make it easier to reuse this code in the future and lessens the likelihood
    of data leakage. Follow these steps:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将比以前章节进行更多的预处理。我们将把大部分工作整合到我们的管道中。这将使将来重用此代码更容易，并减少数据泄露的可能性。请按照以下步骤操作：
- en: 'We will start by importing the same libraries we have worked with in the last
    few chapters. We will also import the `LogisticRegression` and `metrics` modules.
    We will use the `metrics` module from scikit-learn to evaluate each of our classification
    models in this part of this book. In addition to `matplotlib` for visualizations,
    we will also use `seaborn`:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先导入我们在过去几章中使用的相同库。我们还将导入 `LogisticRegression` 和 `metrics` 模块。我们将使用scikit-learn的
    `metrics` 模块来评估本书这一部分中的每个分类模型。除了 `matplotlib` 用于可视化外，我们还将使用 `seaborn`：
- en: '[PRE0]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We are also going to need several custom classes to handle the preprocessing.
    We have already seen the `OutlierTrans` class. Here, we have added a couple of
    new classes – `MakeOrdinal` and `ReplaceVals`:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要几个自定义类来处理预处理。我们已经看到了 `OutlierTrans` 类。在这里，我们添加了两个新的类——`MakeOrdinal` 和 `ReplaceVals`：
- en: '[PRE1]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `MakeOrdinal` class takes a character feature and assigns numeric values
    based on an alphanumeric sort. For example, a feature that has three possible
    values – not well, okay, and well – would be transformed into an ordinal feature
    with values of 0, 1, and 2, respectively.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`MakeOrdinal` 类接受一个字符特征并根据字母数字排序分配数值。例如，一个有三个可能值（不好、一般、好）的特征将被转换为一个序数特征，其值分别为0、1和2。'
- en: Recall that scikit-learn pipeline transformers must have `fit` and `transform`
    methods, and must inherit from `BaseEstimator`. They often also inherit from `TransformerMixin`,
    though there are other options.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，scikit-learn管道转换器必须具有 `fit` 和 `transform` 方法，并且必须继承自 `BaseEstimator`。它们通常也继承自
    `TransformerMixin`，尽管还有其他选项。
- en: 'All the action in the `MakeOrdinal` class happens in the `transform` method.
    We loop over all of the columns that are passed to it by the column transformer.
    For each column, we find all the unique values and sort them alphanumerically,
    storing the unique values in a NumPy array that we name `cats`. Then, we use a
    lambda function and NumPy’s `where` method to find the index of `cats` associated
    with each feature value:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`MakeOrdinal` 类中的所有操作都在 `transform` 方法中完成。我们遍历通过列转换器传递给它的所有列。对于每一列，我们找到所有唯一的值并按字母数字顺序排序，将唯一的值存储在我们命名为
    `cats` 的NumPy数组中。然后，我们使用lambda函数和NumPy的 `where` 方法来找到与每个特征值关联的 `cats` 的索引：'
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`MakeOrdinal` will work fine when the alphanumeric order matches a meaningful
    order, as with the previous example. When that is not true, we can use `ReplaceVals`
    to assign appropriate ordinal values. This class replaces values in any feature
    with alternative values based on a dictionary passed to it.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当字母数字顺序与一个有意义的顺序相匹配时，`MakeOrdinal` 将正常工作，就像前面的例子一样。当这不是真的时，我们可以使用 `ReplaceVals`
    来分配适当的序数值。这个类根据传递给它的字典替换任何特征中的值。
- en: 'We could have just used the pandas `replace` method without putting it in a
    pipeline, but this way, it is easier to integrate our recoding with other pipeline
    steps, such as feature scaling:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用pandas的 `replace` 方法而不将其放入管道中，但这样更容易将我们的重新编码与其他管道步骤（如特征缩放）集成：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Do not worry if you do not fully understand how we will use these classes yet.
    It will be clearer when we add them to our column transformations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在不完全理解我们将如何使用这些类，请不要担心。当我们将它们添加到列转换中时，一切都会变得清晰。
- en: Next, we will load the heart disease data and take a look at a few rows. Several
    string features are conceptually binary, such as `alcoholdrinkingheavy`, which
    is `Yes` when the person is a heavy drinker and `No` otherwise. We will need to
    encode these features before running a model.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载心脏病数据并查看几行。一些字符串特征在概念上是二元的，例如`alcoholdrinkingheavy`，当一个人是重度饮酒者时为`Yes`，否则为`No`。在运行模型之前，我们需要对这些特征进行编码。
- en: 'The `agecategory` feature is character data that represents the age interval.
    We will need to convert that feature into numeric:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`agecategory`特征是表示年龄区间的字符数据。我们需要将这个特征转换为数值：'
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s look at the size of the DataFrame and how many missing values we have.
    There are 30,000 instances, but there are no missings for any of the 18 data columns.
    That’s great. We won’t have to worry about that when we construct our pipeline:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看DataFrame的大小以及有多少缺失值。有30,000个实例，但18个数据列中没有任何缺失值。这太好了。当我们构建管道时，我们不必担心这一点：
- en: '[PRE5]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s change the `heartdisease` variable, which will be our target, into a
    `0` and `1` variable. This will give us one less thing to worry about later. One
    thing to notice right away is that the target’s values are quite imbalanced. Less
    than 10% of our observations have heart disease. That, of course, is good news,
    but it presents some challenges for modeling that we will need to handle:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将`heartdisease`变量，也就是我们的目标变量，转换为`0`和`1`变量。这将减少我们以后需要担心的事情。立即要注意的一件事是，目标变量的值非常不平衡。我们观察结果中不到10%的人患有心脏病。这当然是好消息，但也给我们的建模带来了一些挑战，我们需要处理这些挑战：
- en: '[PRE6]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We should organize our features by the preprocessing we will be doing with them.
    We will be scaling the numeric features and doing one-hot encoding with the categorical
    features. We want to make the `agecategory` and `genhealth` features, which are
    currently strings, into ordinal features.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该根据我们将要对它们进行的预处理来组织我们的特征。我们将对数值特征进行缩放，并对分类特征进行独热编码。我们希望将当前为字符串的`agecategory`和`genhealth`特征转换为有序特征。
- en: 'We need to do a specific cleanup of the `diabetic` feature. Some individuals
    indicate no, but that they were borderline. For our purposes, we will consider
    them a *no*. Some individuals had diabetes during their pregnancies only. We will
    consider them a *yes*. For both `genhealth` and `diabetic`, we will set up a dictionary
    that will indicate how feature values should be replaced. We will use that dictionary
    in the `ReplaceVals` transformer of our pipeline:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对`diabetic`特征进行特定的清理。有些人表示没有，但他们处于边缘状态。为了我们的目的，我们将它们视为`no`。有些人怀孕期间只有糖尿病。我们将它们视为`yes`。对于`genhealth`和`diabetic`，我们将设置一个字典，以指示特征值应该如何替换。我们将在管道的`ReplaceVals`转换器中使用该字典：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We should take a look at some frequencies for the binary features, as well
    as other categorical features. A large percentage of the individuals (42%) report
    that they have been smokers. 14% report that they have difficulty walking:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该查看一些二元特征以及其他分类特征的频率。很大比例的个人（42%）报告说他们是吸烟者。14%的人报告说他们走路有困难：
- en: '[PRE8]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s also look at frequencies for the other categorical features. There are
    nearly equal numbers of men and women. Most people report excellent or very good
    health:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看其他分类特征的频率。男性和女性的数量几乎相等。大多数人报告他们的健康状况非常好或非常好：
- en: '[PRE9]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This produces the following output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We should also look at some descriptive statistics for the numerical features. The
    median value for both bad physical health and mental health days is 0; that is,
    at least half of the observations report no bad physical health days, and at least
    half report no bad mental health days over the previous month:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该查看一些数值特征的描述性统计。对于不良的身体健康和心理健康天数的中位数都是0；也就是说，至少一半的观察结果报告没有不良的身体健康天数，至少一半报告没有不良的心理健康天数：
- en: '[PRE11]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We will need to do some scaling. We will also need to do some encoding of the
    categorical features. There are also some extreme values for the numerical features.
    A `sleeptimenightly` value of 24 seems unlikely! It is probably a good idea to
    deal with them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要进行一些缩放。我们还需要对分类特征进行编码。数值特征也有一些极端值。`sleeptimenightly`的值为24似乎不太可能！处理它们可能是个好主意。
- en: 'Now, we are ready to build our pipeline. Let’s create the training and testing
    DataFrames:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好构建我们的流水线。让我们创建训练和测试的DataFrame：
- en: '[PRE12]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, we will set up the column transformations. We will create a one-hot encoder
    instance that we will use for all of the categorical features. For the numeric
    columns, we will remove extreme values using the `OutlierTrans` object and then
    impute the median.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将设置列转换。我们将创建一个one-hot编码器实例，我们将使用它来处理所有分类特征。对于数值列，我们将使用`OutlierTrans`对象去除极端值，然后填充中位数。
- en: We will convert the `agecategory` feature into an ordinal one using the `MakeOrdinal`
    transformer and code the `genhealth` and `diabetic` features using the `ReplaceVals`
    transformer.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`MakeOrdinal`转换器将`agecategory`特征转换为有序特征，并使用`ReplaceVals`转换器对`genhealth`和`diabetic`特征进行编码。
- en: 'We will add the column transformation to our pipeline in the next step:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一步将列转换添加到我们的流水线中：
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Now, we are ready to set up and fit our pipeline. First, we will instantiate
    logistic regression and stratified k-fold objects, which we will use with recursive
    feature elimination. Recall that recursive feature elimination needs an estimator.
    We use stratified k-fold to get approximately the same target value distribution
    in each fold.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好设置和调整我们的流水线。首先，我们将实例化逻辑回归和分层k折对象，我们将使用递归特征消除。回想一下，递归特征消除需要一个估计器。我们使用分层k折来确保每个折叠中目标值的分布大致相同。
- en: 'Now, we must create another logistic regression instance for our model. We
    will set the `class_weight` parameter to `balanced`. This should improve the model’s
    ability to deal with the class imbalance. Then, we will add the column transformation,
    recursive feature elimination, and logistic regression instance to our pipeline,
    and then fit it:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须为我们的模型创建另一个逻辑回归实例。我们将`class_weight`参数设置为`balanced`。这应该会提高模型处理类别不平衡的能力。然后，我们将列转换、递归特征消除和逻辑回归实例添加到我们的流水线中，然后对其进行调整：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We need to do a little work to recover the column names from the pipeline after
    the fit. We can use the `get_feature_names` method of the one-hot encoder for
    the `bin` transformer and the `cat` transformer for this. This gives us the column
    names for the binary and categorical features after the encoding. The names of
    the numerical features remain unchanged. We will use the feature names later:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在调整后，我们需要做一些工作来恢复流水线中的列名。我们可以使用`bin`转换器的one-hot编码器的`get_feature_names`方法和`cat`转换器的`get_feature_names`方法。这为我们提供了编码后的二元和分类特征的列名。数值特征的名称保持不变。我们将在后面使用这些特征名：
- en: '[PRE15]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, let’s look at the results from the recursive feature elimination. We can
    use the `ranking_` attribute of the `rfecv` object to get the ranking of each
    feature. Those with a *1* for ranking will be selected for our model.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看递归特征消除的结果。我们可以使用`rfecv`对象的`ranking_`属性来获取每个特征的排名。那些排名为*1*的特征将被选入我们的模型。
- en: 'If we use the `get_support` method or the `support_` attribute of the `rfecv`
    object instead of the `ranking_` attribute, we get just those features that will
    be used in our model – that is, those with a ranking of 1\. We will do that in
    the next step:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`rfecv`对象的`get_support`方法或`support_`属性代替`ranking_`属性，我们只会得到那些将在我们的模型中使用的特点——即那些排名为1的特点。我们将在下一步做这个：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can get the odds ratios from the coefficients from the logistic regression. Recall
    that the odds ratio is the exponentiated coefficient. There are 13 coefficients,
    which makes sense because we learned in the previous step that 13 features got
    a ranking of 1.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以从逻辑回归的系数中获取优势比。回想一下，优势比是指数化的系数。有13个系数，这是有意义的，因为我们之前学习到有13个特征得到了1的排名。
- en: We will use the `get_support` method of the `rfecv` step to get the names of
    the selected features and create a NumPy array with those names and the odds ratios,
    `oddswithlabs`. We then create a pandas DataFrame and sort by the odds ratio in
    descending order.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`rfecv`步骤的`get_support`方法来获取所选特征的名称，并创建一个包含这些名称和优势比（`oddswithlabs`）的NumPy数组。然后我们创建一个pandas
    DataFrame，并按优势比降序排序。
- en: 'Not surprisingly, those who had a stroke and older individuals are substantially
    more likely to have heart disease. If the individual had a stroke, they had three
    times the odds of having heart disease, controlling for everything else. The odds
    of having heart disease increase by 2.88 times for each increase in age category.
    On the other hand, the odds of having heart disease decline by about half (57%)
    for every increase in general health; from, say, fair to good. Surprisingly, heavy
    alcohol drinking is associated with lower odds of heart disease, controlling for
    everything else:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，那些曾经中风的人和老年人患心脏病的可能性要大得多。如果个人曾经中风，他们在其他条件相同的情况下患心脏病的几率是三倍。另一方面，随着年龄类别的增加，患心脏病的几率增加2.88倍。另一方面，随着总体健康状况的提高，患心脏病的几率大约减少一半（57%）；比如说，从“一般”到“良好”。令人惊讶的是，在控制其他条件的情况下，大量饮酒与心脏病几率降低有关：
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now that we have fit our logistic regression model, we are ready to evaluate
    it. In the next section, we will spend some time looking into various performance
    measures, including accuracy and sensitivity. We will use many of the concepts
    that we introduced in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing
    for Model Evaluation*.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拟合了逻辑回归模型，我们准备对其进行评估。在下一节中，我们将花时间探讨各种性能指标，包括准确率和灵敏度。我们将使用我们在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，“准备模型评估”中介绍的一些概念。
- en: Evaluating a logistic regression model
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估逻辑回归模型
- en: The most intuitive measure of a classification model’s performance is its accuracy
    – that is, how often our predictions are correct. In some cases, however, we might
    be at least as concerned about sensitivity – the percent of positive cases that
    we predict correctly – as accuracy; we may even be willing to lose a little accuracy
    to improve sensitivity. Predictive models of diseases often fall into that category.
    But whenever there is a class imbalance, measures such as accuracy and sensitivity
    can give us very different estimates of the performance of our model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分类模型性能的最直观的衡量标准是其准确率——也就是说，我们的预测有多正确。然而，在某些情况下，我们可能至少和准确率一样关心灵敏度——即我们正确预测的阳性案例的百分比；我们甚至可能愿意牺牲一点准确率来提高灵敏度。疾病预测模型通常属于这一类。但是，每当存在类别不平衡时，准确率和灵敏度等指标可能会给我们提供关于模型性能的非常不同的估计。
- en: In addition to being concerned about accuracy or sensitivity, we might be worried
    about our model’s **specificity** or **precision**. We may want a model that can
    identify negative cases with high reliability, even if that means it does not
    do as good a job of identifying positives. Specificity is a measure of the percentage
    of all negatives identified by the model.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了关注准确率或灵敏度之外，我们还可能担心我们的模型的**特异性**或**精确度**。我们可能希望有一个模型能够以高可靠性识别出负面案例，即使这意味着它不能很好地识别正面案例。特异性是模型识别出的所有负面案例所占的百分比。
- en: Precision, which is the percentage of predicted positives that are positives,
    is another important measure. For some applications, it is important to limit
    false positives, even if we have to tolerate lower sensitivity. An apple grower,
    using image recognition to identify bad apples, may prefer a high-precision model
    to a more sensitive one, not wanting to discard apples unnecessarily.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度，即预测为正面的案例中实际为正面的比例，是另一个重要的衡量指标。对于某些应用来说，限制误报非常重要，即使这意味着我们必须容忍较低的灵敏度。一个使用图像识别来识别坏苹果的苹果种植者可能更倾向于选择一个高精确度的模型，而不是一个更灵敏的模型，不希望不必要地丢弃苹果。
- en: 'This can be made more clear by looking at a confusion matrix:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看混淆矩阵可以使这一点更加清晰：
- en: '![Figure 10.2 – Confusion matrix of actual by predicted values for a binary
    target ](img/B17978_10_002.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – 二元目标按预测值预测的实际值混淆矩阵](img/B17978_10_002.jpg)'
- en: Figure 10.2 – Confusion matrix of actual by predicted values for a binary target
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – 二元目标按预测值预测的实际值混淆矩阵
- en: 'The confusion matrix helps us conceptualize accuracy, sensitivity, specificity,
    and precision. Accuracy is the percentage of observations for which our prediction
    was correct. This can be stated more precisely as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵帮助我们理解准确率、灵敏度、特异性和精确度。准确率是我们预测正确的观察值的百分比。这可以更精确地表述如下：
- en: '![](img/B17978_10_015.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17978_10_015.jpg)'
- en: 'Sensitivity is the number of times we predicted positives correctly divided
    by the number of positives. It might be helpful to glance at the confusion matrix
    again and confirm that actual positive values can either be predicted positives
    (TP) or predicted negatives (FN). Sensitivity is also referred to as **recall**
    or the **true positive rate**:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 敏感性是指我们正确预测正值的次数除以正值的总数。再次查看混淆矩阵可能会有所帮助，以确认实际正值可以是预测正值（TP）或预测负值（FN）。敏感性也被称为**召回率**或**真正阳性率**：
- en: '![](img/B17978_10_016.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_10_016.jpg)'
- en: 'Specificity is the number of times we correctly predicted a negative value
    (TN) divided by the number of actual negative values (TN + FP). Specificity is
    also known as the **true negative rate**:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 特异性是指我们正确预测负值的次数（TN）除以实际负值总数（TN + FP）。特异性也被称为**真正阴性率**：
- en: '![](img/B17978_10_017.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_10_017.jpg)'
- en: 'Precision is the number of times we correctly predicted a positive value (TP)
    divided by the number of positive values predicted:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度是指我们正确预测正值的次数（TP）除以预测的正值总数：
- en: '![](img/B17978_10_018.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_10_018.jpg)'
- en: 'We went over these concepts in more detail in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078),
    *Preparing for* *Model Evaluation*. In this section, we will examine the accuracy,
    sensitivity, specificity, and precision of our logistic regression model of heart
    disease:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)中更详细地介绍了这些概念，*准备模型评估*。在本节中，我们将检查心脏病逻辑回归模型的准确性、敏感性、特异性和精确度：
- en: 'We can use the `predict` method of the pipeline we fitted in the previous section
    to generate predictions from our logistic regression. Then, we can generate a
    confusion matrix:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用上一节中拟合的管道的`predict`方法来从我们的逻辑回归中生成预测。然后，我们可以生成一个混淆矩阵：
- en: '[PRE18]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This produces the following plot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 10.3 – A confusion matrix for heart disease prediction ](img/B17978_10_003.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – 心脏病预测混淆矩阵](img/B17978_10_003.jpg)'
- en: Figure 10.3 – A confusion matrix for heart disease prediction
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 心脏病预测混淆矩阵
- en: The first thing to notice here is that most of the action is in the top-left
    quadrant, where we correctly predict actual negative values in the testing data.
    That is going to help our accuracy a fair bit. Nonetheless, we have a fair number
    of false positives. We predict heart disease 1,430 times (out of 5,506 negative
    instances) when there is no heart disease. We do seem to do an okay job of identifying
    positive heart disease instances, correctly classifying 392 instances (out of
    494) that were positive.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这里首先要注意的是，大部分动作都在左上角，我们在测试数据中正确预测了实际负值。这将大大有助于我们的准确性。尽管如此，我们还是有相当数量的假阳性。当没有心脏病时，我们预测心脏病1,430次（在5,506个负实例中）。我们似乎在识别正性心脏病实例方面做得还不错，正确分类了392个实例（在494个正实例中）。
- en: 'Let’s calculate the accuracy, sensitivity, specificity, and precision. The
    overall accuracy is not great, at 74%. Sensitivity is pretty decent though, at
    79%. (Of course, how *decent* the sensitivity is depends on the domain and judgment.
    For something such as heart disease, we likely want it to be higher.) This can
    be seen in the following code:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们计算准确性、敏感性、特异性和精确度。总体准确性并不高，为74%。敏感性相当不错，为79%。（当然，敏感性的好坏取决于领域和判断。对于像心脏病这样的领域，我们可能希望它更高。）这可以在以下代码中看到：
- en: '[PRE19]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can do these calculations in a more straightforward way using the `metrics`
    module (I chose a more roundabout approach in the previous step to illustrate
    how the calculations are done):'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`metrics`模块以更直接的方式来进行这些计算（我在上一步中选择了更迂回的方法来展示计算过程）：
- en: '[PRE20]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The biggest problem with our model is the very low level of precision – that
    is, 22%. This is due to the large number of false positives. The majority of the
    time that our model predicts positive, it is wrong.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型的最大问题是精确度非常低——即22%。这是由于大量的假阳性。我们的模型预测正性的大多数情况下都是错误的。
- en: 'In addition to the four measures that we have already calculated, it can also
    be helpful to get the false positive rate. The false positive rate is the propensity
    of our model to predict positive when the actual value is negative:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们已经计算出的四个指标之外，获取假阳性率也可能很有帮助。假阳性率是指我们的模型在实际值为负时预测为正的倾向：
- en: '![](img/B17978_10_019.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_10_019.jpg)'
- en: 'Let’s calculate the false positive rate:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们计算假阳性率：
- en: '[PRE21]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: So, 26% of the time that a person does not have heart disease, we predicted
    that they do. While we certainly want to limit the number of false positives,
    this often means sacrificing some sensitivity. We will demonstrate why this is
    true later in this section.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，26%的时间，当一个人没有心脏病时，我们预测他有心脏病。虽然我们当然希望限制假阳性的数量，但这通常意味着牺牲一些敏感性。我们将在本节后面演示为什么这是真的。
- en: 'We should take a closer look at the prediction probabilities generated by our
    model. Here, the threshold for a positive class prediction is 0.5, which is often
    the default with logistic regression. (Recall that logistic regression predicts
    a probability of class membership. We need an accompanying decision rule, such
    as the 0.5 threshold, to predict the class.) This can be seen in the following
    code:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该仔细查看模型生成的预测概率。在这里，正类预测的阈值是0.5，这在逻辑回归中通常是默认值。（回想一下，逻辑回归预测的是类成员的概率。我们需要一个伴随的决策规则，比如0.5阈值，来预测类别。）这可以在以下代码中看到：
- en: '[PRE22]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can use a **kernel density estimate** (**KDE**) plot to visualize these
    probabilities. We can also see how a different decision rule may impact our predictions.
    For example, we could move the threshold from 0.5 to 0.25\. At a glance, that
    has some advantages. The area between the two possible thresholds has somewhat
    more heart disease cases than no heart disease cases. We would be getting the
    brown area between the dashed lines right, predicting heart disease correctly
    where we would not have with the 0.5 threshold. That is a larger area than the
    green area between the lines, where we turn some of the true negative predictions
    at the 0.5 threshold into false positives at the 0.25 threshold:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用**核密度估计**（**KDE**）图来可视化这些概率。我们还可以看到不同的决策规则可能如何影响我们的预测。例如，我们可以将阈值从0.5移动到0.25。乍一看，这有一些优点。两个可能阈值之间的区域比没有心脏病病例的心脏病病例要多一些。我们将得到虚线之间的棕色区域，正确预测心脏病，而不会在0.5阈值下这样做。这比线之间的绿色区域更大，在0.5阈值下，我们将一些真正的阴性预测变成了0.25阈值下的假阳性：
- en: '[PRE23]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This generates the following plot:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 10.4 – Heart disease predicted probability distribution ](img/B17978_10_004.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – 心脏病预测概率分布](img/B17978_10_004.jpg)'
- en: Figure 10.4 – Heart disease predicted probability distribution
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – 预测的心脏病概率分布
- en: Let’s consider the tradeoff between precision and sensitivity a little more
    carefully than we have so far. Remember that precision is the rate at which we
    are right when we predict a positive class value. Sensitivity, also referred to
    as recall or the true positive rate, is the rate at which we identify an actual
    positive as positive.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比之前更仔细地考虑精确度和敏感性之间的权衡。记住，精确度是我们预测正类值时正确率的比率。敏感性，也称为召回率或真正阳性率，是我们识别实际正例为正例的比率。
- en: 'We can plot precision and sensitivity curves as follows:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以如下绘制精确度和敏感性曲线：
- en: '[PRE24]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This generates the following plot:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 10.5 – Precision and sensitivity at threshold values ](img/B17978_10_0051.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5 – 阈值值下的精确度和敏感性](img/B17978_10_0051.jpg)'
- en: Figure 10.5 – Precision and sensitivity at threshold values
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – 阈值值下的精确度和敏感性
- en: As the threshold increases beyond 0.2, there is a sharper decrease in sensitivity
    than there is an increase in precision.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当阈值超过0.2时，敏感性的下降比精确度的增加更为明显。
- en: 'It is often also helpful to look at the false positive rate with the sensitivity
    rate. The false positive rate is the propensity of our model to predict positive
    when the actual value is negative. One way to see that relationship is with a
    ROC curve:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通常，查看假阳性率与敏感性率也是很有帮助的。假阳性率是我们模型在实际值为负时预测为正的倾向。了解这种关系的一种方法是通过ROC曲线：
- en: '[PRE25]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This produces the following plot:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 10.6 – ROC curve ](img/B17978_10_006.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图10.6 – ROC曲线](img/B17978_10_006.jpg)'
- en: Figure 10.6 – ROC curve
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 – ROC曲线
- en: Here, we can see that increasing the false positive rate buys us less increase
    in sensitivity the higher the false positive rate is. Beyond a false positive
    rate of 0.5, there is not much payoff at all.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到，随着假阳性率的增加，我们获得的敏感性增加越来越少。超过0.5的假阳性率，几乎没有回报。
- en: 'It may also be helpful to just plot the false positive rate and sensitivity
    by threshold:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可能还有助于仅通过阈值绘制假阳性率和敏感性：
- en: '[PRE26]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This produces the following plot:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 10.7 – Sensitivity and false positive rate ](img/B17978_10_0071.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图10.7 – 灵敏度和假阳性率](img/B17978_10_0071.jpg)'
- en: Figure 10.7 – Sensitivity and false positive rate
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – 灵敏度和假阳性率
- en: Here, we can see that as we lower the threshold below 0.25, the false positive
    rate increases more rapidly than sensitivity.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们可以看到，当我们把阈值降低到0.25以下时，假阳性率比灵敏度增加得更快。
- en: These last two visualizations hint at the possibility of finding an optimal
    threshold value – that is, one with the best tradeoff between sensitivity and
    the false positive rate; at least mathematically, ignoring domain knowledge.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这最后两个可视化暗示了找到最佳阈值值——即在灵敏度和假阳性率之间有最佳权衡的值；至少在数学上，忽略领域知识。
- en: 'We will calculate the `argmax` function. We want the value of the threshold
    at that index. The optimal threshold according to this calculation is 0.46, which
    isn’t very different from the default:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将计算`argmax`函数。我们想要这个索引处的阈值值。根据这个计算，最佳阈值是0.46，这与默认值并不太不同：
- en: '[PRE27]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can redo the confusion matrix based on this alternative threshold:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以根据这个替代阈值重新做混淆矩阵：
- en: '[PRE28]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This produces the following plot:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 10.8 – Confusion matrix of heart disease prediction  ](img/B17978_10_008.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图10.8 – 心脏病预测混淆矩阵](img/B17978_10_008.jpg)'
- en: Figure 10.8 – Confusion matrix of heart disease prediction
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 – 心脏病预测混淆矩阵
- en: 'This gives us a small improvement in sensitivity:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这给我们带来了灵敏度的微小提升：
- en: '[PRE29]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The point here is not that we should change thresholds willy-nilly. This is
    often a bad idea. But we should keep two points in mind. First, when we have a
    highly imbalanced class, a 0.5 threshold may not make sense. Second, this is an
    important place to lean on domain knowledge. For some classification problems,
    a false positive is substantially less important than a false negative.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这里要说明的不是我们应该随意更改阈值。这通常是一个坏主意。但我们应该记住两点。首先，当我们有一个高度不平衡的类别时，0.5的阈值可能没有意义。其次，这是依赖领域知识的一个重要地方。对于某些分类问题，假阳性远不如假阴性重要。
- en: In this section, we focused on sensitivity, precision, and false positive rate
    as measures of model performance. That is partly because of space limitations,
    but also because of the issues with this particular target – imbalance classes
    and the likely preference for sensitivity. We will be emphasizing other measures,
    such as accuracy and specificity, in other models that we will be building in
    the next few chapters. In the rest of this chapter, we will look at a couple of
    extensions of logistic regression, regularization and multinomial logistic regression.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们关注了灵敏度、精确度和假阳性率作为模型性能的度量。这部分是因为空间限制，也因为这个特定目标的问题——不平衡的类别和可能对灵敏度的偏好。在接下来的几章中，我们将强调其他度量，如准确性和特异性，在其他我们将构建的模型中。在本章的其余部分，我们将探讨逻辑回归的几个扩展，包括正则化和多项式逻辑回归。
- en: Regularization with logistic regression
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归进行正则化
- en: 'If you have already worked your way through [*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091),
    *Linear Regression Models*, and read the first section of this chapter, you already
    have a good idea of how regularization works. We add a penalty to the estimator
    that minimizes our parameter estimates. The size of that penalty is typically
    tuned based on a measure of model performance. We will work through that in this
    section. Follow these steps:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经阅读了[*第7章*](B17978_07_ePub.xhtml#_idTextAnchor091)《线性回归模型》并阅读了本章的第一节，你对正则化的工作原理已经有了很好的了解。我们向估计器添加一个惩罚，以最小化我们的参数估计。这个惩罚的大小通常基于模型性能的度量来调整。我们将在本节中探讨这一点。按照以下步骤进行：
- en: 'We will load the same modules that we worked with in the previous section,
    plus the modules we will need for the necessary hyperparameter tuning. We will
    use `RandomizedSearchCV` and `uniform` to find the best value for our penalty
    strength:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载与上一节中使用的相同模块，以及我们需要进行必要超参数调整的模块。我们将使用`RandomizedSearchCV`和`uniform`来找到我们惩罚强度的最佳值：
- en: '[PRE30]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we will load the heart disease data and do a little processing:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载心脏病数据并进行一些处理：
- en: '[PRE31]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we will organize our features to facilitate the column transformation
    we will do in a couple of steps:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将组织我们的特征，以便于我们在接下来的几个步骤中进行的列转换：
- en: '[PRE32]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, we must create testing and training DataFrames:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们必须创建测试和训练数据框：
- en: '[PRE33]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, we must set up the column transformations:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们必须设置列转换：
- en: '[PRE34]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now, we are ready to run our model. We will instantiate logistic regression
    and repeated stratified k-fold objects. Then, we will create a pipeline with our
    column transformation from the previous step and the logistic regression.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行我们的模型了。我们将实例化逻辑回归和重复分层k折对象。然后，我们将创建一个包含之前步骤中的列转换和逻辑回归的管道。
- en: After that, we will create a list of dictionaries for our hyperparameters, rather
    than just one dictionary, as we have done previously in this book. This is because
    not all hyperparameters work together. For example, we cannot use an L1 penalty
    with a `newton-cg` solver. The `logisticregression__` (note the double underscore)
    prefix to the dictionary key names indicates that we want the values to be passed
    to the logistic regression step of our pipeline.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将为超参数创建一个字典列表，而不是像在这本书中之前所做的那样只创建一个字典。这是因为并非所有超参数都能一起工作。例如，我们不能使用`newton-cg`求解器与L1惩罚一起使用。字典键名前缀的`logisticregression__`（注意双下划线）表示我们希望将这些值传递到管道的逻辑回归步骤。
- en: 'We will set the `n_iter` parameter to `20` for our randomized grid search to
    get it to sample hyperparameters 20 times. Each of those times, the grid search
    will select from the hyperparameters listed in one of the dictionaries. We will
    indicate that we want the grid search scoring to be based on the area under the
    ROC curve:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设置随机网格搜索的`n_iter`参数为`20`，以便它采样超参数20次。每次，网格搜索都将从列出的一个字典中选择超参数。我们将指示我们希望网格搜索的评分基于ROC曲线下的面积：
- en: '[PRE35]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After fitting the search, the `best_params` attribute gives us the parameters
    associated with the highest score. Elastic net regression, with an L1 ratio closer
    to L1 than to L2, performs the best:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在拟合搜索后，`best_params`属性给出了与最高分数相关的参数。弹性网络回归，其L1比率更接近L1而不是L2，表现最佳：
- en: '[PRE36]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Let’s look at some of the other top scores from the grid search. The best three
    models have pretty much the same score. One uses elastic net regression, another
    L1, and another L2.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看网格搜索的一些其他高分。最好的三个模型得分几乎相同。一个使用弹性网络回归，另一个使用L1，另一个使用L2。
- en: 'The `cv_results_` dictionary of the grid search provides us with lots of information
    about the 20 models that were tried. The `params` list in that dictionary has
    a somewhat complicated structure because some keys are not present for some iterations,
    such as `L1_ratio`. We can use `json_normalize` to flatten the structure:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索的`cv_results_`字典为我们提供了关于尝试过的20个模型的大量信息。该字典中的`params`列表结构有些复杂，因为某些键在某些迭代中不存在，例如`L1_ratio`。我们可以使用`json_normalize`来简化结构：
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s take a look at the confusion matrix:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看混淆矩阵：
- en: '[PRE38]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This generates the following plot:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下图表：
- en: '![Figure 10.9 – Confusion matrix of heart disease prediction ](img/B17978_10_009.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图10.9 – 心脏病预测混淆矩阵](img/B17978_10_009.jpg)'
- en: Figure 10.9 – Confusion matrix of heart disease prediction
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – 心脏病预测混淆矩阵
- en: 'Let’s also look at some metrics. Our scores are largely unchanged from our
    model without regularization:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也看看一些度量指标。我们的分数与没有正则化的模型基本没有变化：
- en: '[PRE39]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Even though regularization provided no obvious improvement in the performance
    of our model, there are many times when it does. It is also not as necessary to
    worry about feature selection when using L1 regularization, as the weights for
    less important features will be 0.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管正则化并没有明显提高我们模型的表现，但很多时候它确实做到了。在使用L1正则化时，也不必过于担心特征选择，因为不太重要的特征的权重将会是0。
- en: We still haven’t dealt with how to handle models where the target has more than
    two possible values, though almost all the discussion in the last two sections
    applies to multiclass models as well. In the next section, we will learn how to
    use multinomial logistic regression to model multiclass targets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们还没有解决如何处理目标值超过两个的可能值的模型，尽管上一两节的几乎所有讨论也适用于多类模型。在下一节中，我们将学习如何使用多项式逻辑回归来建模多类目标。
- en: Multinomial logistic regression
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式逻辑回归
- en: Logistic regression would not be as useful if it only worked for binary classification
    problems. Fortunately, we can use multinomial logistic regression when our target
    has more than two values.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果逻辑回归只适用于二元分类问题，那么它就不会那么有用。幸运的是，当我们的目标值超过两个时，我们可以使用多项式逻辑回归。
- en: In this section, we will work with data on machine failures as a function of
    air and process temperature, torque, and rotational speed.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将处理关于空气和工艺温度、扭矩和旋转速度作为机器故障函数的数据。
- en: Note
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This dataset on machine failure is available for public use at [https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification).
    There are 10,000 observations, 12 features, and two possible targets. One is binary
    – that is, the machine failed or didn’t. The other has types of failure. The instances
    in this dataset are synthetic, generated by a process designed to mimic machine
    failure rates and causes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这个关于机器故障的数据集可在[https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification](https://www.kaggle.com/datasets/shivamb/machine-predictive-maintenance-classification)公开使用。有10,000个观测值，12个特征，以及两个可能的目标。一个是二元的——也就是说，机器故障或未故障。另一个是故障类型。这个数据集中的实例是合成的，由一个旨在模仿机器故障率和原因的过程生成。
- en: 'Let’s learn how to use multinomial logistic regression to model machine failure:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们学习如何使用多项式逻辑回归来建模机器故障：
- en: 'First, we will import the now-familiar libraries. We will also import `cross_validate`,
    which we first used in [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing
    for* *Model Evaluation*, to help us evaluate our model:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将导入现在熟悉的库。我们还将导入`cross_validate`，我们首次在第6章*准备模型评估*中使用它，以帮助我们评估我们的模型：
- en: '[PRE40]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We will load the machine failure data and take a look at its structure. We
    do not have any missing data. That’s great news:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将加载机器故障数据并查看其结构。我们没有缺失数据。这是个好消息：
- en: '[PRE41]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s look at a few rows. `machinetype` has values of `L`, `M`, and `H`. These
    values are proxies for machines of low, medium, and high quality, respectively:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看几行。`machinetype`的值为`L`、`M`和`H`。这些值分别是低、中、高质机器的代理：
- en: '[PRE42]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We should also generate some frequencies:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该生成一些频率：
- en: '[PRE43]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let’s collapse the `failtype` values and create numeric code for them. We will
    combine random failures and tool wear failures since the counts are so low for
    random failures:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将`failtype`值合并，并为它们创建数字代码。由于随机故障的计数很低，我们将随机故障和工具磨损故障合并：
- en: '[PRE44]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We should confirm that `failtypecode` does what we intended:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该确认`failtypecode`是否按我们的意图工作：
- en: '[PRE45]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s also get some descriptive statistics:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们也获取一些描述性统计信息：
- en: '[PRE46]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now, let’s create the testing and training DataFrames. We will also set up
    the column transformations:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建测试和训练数据框。我们还将设置列转换：
- en: '[PRE47]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, let’s set up a pipeline with our column transformations and our multinomial
    logistic regression model We just need to set the `multi_class` attribute to multinomial
    when we instantiate the logistic regression:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们设置一个包含我们的列转换和多项逻辑回归模型的管道。当我们实例化逻辑回归时，只需将`multi_class`属性设置为多项式即可：
- en: '[PRE48]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, we can generate a confusion matrix:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以生成一个混淆矩阵：
- en: '[PRE49]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This produces the following plot:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 10.10 – Confusion matrix of predicted machine failure types ](img/B17978_10_0101.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![图10.10 – 预测机器故障类型的混淆矩阵](img/B17978_10_0101.jpg)'
- en: Figure 10.10 – Confusion matrix of predicted machine failure types
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10 – 预测机器故障类型的混淆矩阵
- en: The confusion matrix shows that our model does not do a good job of predicting
    the failure type when there is a failure, particularly with power failures or
    other failures.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵显示，当存在故障时，我们的模型在预测故障类型方面做得并不好，尤其是电力故障或其他故障。
- en: We can use `cross_validate` to evaluate this model. We mainly get excellent
    scores for accuracy, precision, and sensitivity (recall). However, this is misleading.
    The weighted scores when the classes are so imbalanced (almost all of the instances
    have `no failure`) are very heavily influenced by the class that contains almost
    all of the values. Our model gets `no failure` correct reliably.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`cross_validate`来评估这个模型。我们主要得到准确率、精确率和敏感度（召回率）的优异成绩。然而，这是误导性的。当类别如此不平衡（几乎所有实例都是`无故障`）时，加权分数会受到包含几乎所有值的类别的严重影响。我们的模型能够可靠地正确预测`无故障`。
- en: If we look at the `f1_macro` score (recall from [*Chapter 6*](B17978_06_ePub.xhtml#_idTextAnchor078),
    *Preparing for* *Model Evaluation*, that `f1` is the harmonic mean of precision
    and sensitivity), we will see that our model does not do very well for classes
    other than the `no failure` class. (The `macro` score is just a simple average.)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`f1_macro`分数（回忆一下[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，*准备模型评估*，其中`f1`是精确率和敏感性的调和平均值），我们会看到，除了`无故障`类别之外，我们的模型在其它类别上表现并不好。（`macro`分数只是一个简单的平均值。）
- en: 'We could have just used a classification report here, as we did in [*Chapter
    6*](B17978_06_ePub.xhtml#_idTextAnchor078), *Preparing for* *Model Evaluation*,
    but I sometimes find it helpful to generate the stats I need:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用分类报告在这里，就像我们在[*第6章*](B17978_06_ePub.xhtml#_idTextAnchor078)，“准备模型评估”中做的那样，但我有时发现生成我需要的统计数据很有帮助：
- en: '[PRE50]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: In this section, we explored how to construct a multinomial logistic regression
    model. This approach works regardless of whether the target is nominal or ordinal.
    In this case, it was nominal. We also saw how we can extend the model evaluation
    approaches we used for logistic regression with a binary target. We reviewed how
    to interpret the confusion matrix and scoring metrics when we have more than two
    classes.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了如何构建多项逻辑回归模型。这种方法无论目标变量是名义变量还是有序变量都适用。在这种情况下，它是名义变量。我们还看到了如何将用于具有二元目标的逻辑回归模型中的模型评估方法进行扩展。我们回顾了当我们有超过两个类别时如何解释混淆矩阵和评分指标。
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Logistic regression has been a go-to tool for me for many, many years when I
    have needed to predict a categorical target. It is an efficient algorithm with
    low bias. Some of its disadvantages, such as high variance and difficulty handling
    highly correlated predictors, can be addressed with regularization and feature
    selection. We went over examples of doing that in this chapter. We also examined
    how to handle imbalanced classes in terms of what such targets mean for modeling
    and interpretation of results.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归多年来一直是我预测分类目标的首选工具。它是一个高效且偏差低的算法。一些其缺点，如高方差和难以处理高度相关的预测因子，可以通过正则化和特征选择来解决。我们在本章中探讨了如何做到这一点。我们还考察了如何处理不平衡类别，以及这些目标对建模和结果解释的含义。
- en: In the next chapter, we will look at a very popular alternative to logistic
    regression for classification – decision trees. We will see that decision trees
    have many advantages that make them a particularly good option if we need to model
    complexity, without having to worry as much about how our features are specified
    as we do with logistic regression.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨分类中一个非常流行的逻辑回归替代方案——决策树。我们会看到决策树具有许多优点，使得它们在需要建模复杂性时成为一个特别好的选择，而不必像使用逻辑回归那样过多地担心我们的特征是如何指定的。
