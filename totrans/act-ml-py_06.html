<html><head></head><body>
<div id="book-content" class="calibre2">
<div id="sbo-rt-content" class="calibre3"><div id="_idContainer064" class="calibre4">
			<h1 id="_idParaDest-55" class="calibre8"><a id="_idTextAnchor056" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>4</h1>
			<h1 id="_idParaDest-56" class="calibre8"><a id="_idTextAnchor057" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Applying Active Learning to Computer Vision</h1>
			<p class="calibre6">In this chapter, we will dive into using active learning techniques for computer vision tasks. Computer vision involves analyzing visual data such as images and videos to extract useful information. It relies heavily on machine learning models such as convolutional neural networks. However, these models require large labeled training sets, which can be expensive and time-consuming to obtain. Active ML provides a solution by interactively querying the user to label only the most informative examples. This chapter demonstrates how to implement uncertainty sampling for diverse computer vision tasks. By the end, you will have the tools to efficiently train computer vision models with optimized labeling effort. The active ML methods presented open up new possibilities for building robust vision systems with fewer <span>data requirements.</span></p>
			<p class="calibre6">By the end of this chapter, you will be able to do <span>the following:</span></p>
			<ul class="calibre16">
				<li class="calibre20">Implementing active ML for an image <span>classification project</span></li>
				<li class="calibre20">Applying active ML to an object <span>detection project</span></li>
				<li class="calibre20">Using active ML for an instance <span>segmentation project</span></li>
			</ul>
			<h1 id="_idParaDest-57" class="calibre8"><a id="_idTextAnchor058" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Technical requirements</h1>
			<p class="calibre6">In this chapter, you will need to install the Ultralytics, PyYAML, and <span>Roboflow packages.</span></p>
			<p class="calibre6">Ultralytics is a popular open source Python library for building high-performance computer vision and deep learning models. It provides implementations of state-of-the-art object detection and image segmentation models including YOLO that can be trained on <span>custom datasets.</span></p>
			<p class="calibre6">PyYAML is a Python library used for reading and writing YAML files. YAML is a human-readable data serialization format. PyYAML allows loading YAML data from files or strings into Python data types such as dictionaries and lists. It can also dump Python objects back into <span>YAML strings.</span></p>
			<p class="calibre6">Roboflow, as presented in earlier chapters, is a platform that helps with preparing and managing datasets for computer vision models. It provides tools to annotate images, create training/test splits, and export labeled datasets in formats that are usable by deep learning frameworks such as PyTorch. Roboflow also integrates with libraries such as Ultralytics to streamline training pipelines. The main goal is to simplify the dataset management aspects of developing <span>CV models.</span></p>
			<p class="calibre6">To install these packages, we can run the <span>following code:</span></p>
			<pre class="source-code">
pip install ultralytics &amp;&amp; pip install pyyaml &amp;&amp; pip install roboflow</pre>			<p class="calibre6">You will also need the <span>following imports:</span></p>
			<pre class="source-code">
import torch
from torch.utils.data import DataLoader, Subset
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms
import torchvision
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
from roboflow import Roboflow
import glob
import os
import yaml
import cv2</pre>			<p class="calibre6">Additionally, you will need a Roboflow account in order to get a Roboflow API key. You can create an account <span>here: </span><a href="https://app.roboflow.com/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span>https://app.roboflow.com/</span></a><span>.</span></p>
			<h1 id="_idParaDest-58" class="calibre8"><a id="_idTextAnchor059" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Implementing active ML for an image classification project</h1>
			<p class="calibre6">In this section, we <a id="_idIndexMarker166" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>will guide you through <a id="_idIndexMarker167" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>the implementation of active ML techniques for an image classification project. Image classification has various applications in computer vision, ranging from identifying products for an e-commerce website to detecting patterns of deforestation on geospatial tiles. However, creating accurate image classifiers requires extensive datasets of labeled images, which can be expensive and time-consuming to gather, as mentioned in <a href="B21789_01.xhtml#_idTextAnchor015" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span><em class="italic">Chapter 1</em></span></a>, <em class="italic">Introducing Active Machine Learning</em>. Active ML offers a solution to this labeling bottleneck by interactively requesting the oracle to label only the most <span>informative examples.</span></p>
			<p class="calibre6">We will build an image classification model that will be capable of accurately classifying various images obtained from the CIFAR-10 dataset. This dataset is widely recognized in the field of computer vision and contains a diverse collection of 60,000 images, each belonging to one of 10 different classes. We will start with a small <em class="italic">labeled</em> set of only 2,000 images from CIFAR-10, then employ an active ML strategy to select the best images to present to an oracle for labeling. Specifically, we will use uncertainty sampling to query the examples the model is least certain about. We use uncertainty sampling here as it is simpler and less computationally expensive than other methods we have discussed previously. For instance, query-by-committee requires training multiple models, which is <span>computationally expensive.</span></p>
			<p class="calibre6">As more labels are acquired, model accuracy improves with fewer training examples. This demonstrates how active learning can create high-performing computer vision models with significantly lower <span>data requirements.</span></p>
			<h2 id="_idParaDest-59" class="calibre9"><a id="_idTextAnchor060" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Building a CNN for the CIFAR dataset</h2>
			<p class="calibre6">The implementation<a id="_idIndexMarker168" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> will <a id="_idIndexMarker169" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>cover<a id="_idIndexMarker170" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> initializing a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>) classifier, training our model with a small labeled set, selecting unlabeled images for the next labeling step using active ML, acquiring new labels, retraining the model, and tracking <span>model performance.</span></p>
			<p class="callout-heading">Quick reminder</p>
			<p class="callout">A CNN classifier takes an image as input, extracts feature maps using convolutions, integrates the features in fully connected layers, and outputs predicted class probabilities based on what it learned during training. The convolutions allow it to automatically learn relevant visual patterns, making CNNs very effective for image classification tasks. You can find the PyTorch official tutorial on building a neural network model <span>at </span><a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span>https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py</span></a><span>.</span></p>
			<p class="calibre6">Let’s create a <a id="_idIndexMarker171" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>simple<a id="_idIndexMarker172" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> image <a id="_idIndexMarker173" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/><span>classification model:</span></p>
			<pre class="source-code">
class Net(nn.Module):
    def __init__(sel<a id="_idTextAnchor061" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>f):
        super().__init__()
# First convolutional layer with 6 output channels, 5x5 kernel
        self.conv1 = nn.Conv2d(3, 6, 5)
# Max pooling layer with 2x2 window and default stride
        self.pool = nn.MaxPool2d(2, 2)
# Second convolutional layer with 16 output channels, 5x5 kernel
        self.conv2 = nn.Conv2d(6, 16, 5)
# First fully connected layer
# Flattened input size determined by conv2 output shape
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
# Second fully connected layer with 84 nodes
        self.fc2 = nn.Linear(120, 84)
# Final fully connected output layer
# 10 nodes for 10 image classes
        self.fc3 = nn.Linear(84, 10)
    def forward(self, x):
# Pass input through first conv and activation
        x = self.pool(F.relu(self.conv1(x)))
# Second conv and activation, then pool
        x = self.pool(F.relu(self.conv2(x)))
# Flatten input for first fully connected layer
        x = torch.flatten(x, 1)
# Pass through all fully connected layers and activations
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x</pre>			<p class="calibre6">Note that we<a id="_idIndexMarker174" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> use this <a id="_idIndexMarker175" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>model<a id="_idIndexMarker176" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> because it is a small CNN that runs quickly and efficiently. This is helpful for running simple proofs of concept. However, next, we could use one of the<a id="_idIndexMarker177" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> pretrained models (such as <strong class="bold">ResNet</strong> at <a href="https://paperswithcode.com/method/resnet" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://paperswithcode.com/method/resnet</a> or <strong class="bold">MobileNet</strong> at <a href="https://paperswithcode.com/method/mobilenetv2" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://paperswithcode.com/method/mobilenetv2</a>) from <strong class="source-inline">torchvision</strong>, <span>as </span><span><a id="_idIndexMarker178" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/></span><span>follows:</span></p>
			<pre class="source-code">
from torchvision import models
model = models.resnet18(pretrained=True)
model = models.mobilenet_v2(pretrained=True)</pre>			<p class="calibre6">You can find all the <strong class="source-inline">torchvision</strong> pretrained models on the library’s model <span>page: </span><a href="https://pytorch.org/vision/stable/models.html" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span>https://pytorch.org/vision/stable/models.html</span></a><span>.</span></p>
			<p class="calibre6">Now, we load the CIFAR-10 dataset with the appropriate transform function. The transform function defines a series of data processing and augmentation operations that are automatically applied when fetching samples from a PyTorch dataset. In the following code, we convert the images to tensors and <span>normalize them:</span></p>
			<pre class="source-code">
transform = transforms.Compose(
[transforms.ToTensor(),
transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
full_dataset = torchvision.datasets.CIFAR10(
    root='cifar10', train=True, download=True, transform=transform)
print(len(full_dataset))</pre>			<p class="calibre6">This print shows<a id="_idIndexMarker179" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> us that <a id="_idIndexMarker180" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>the<a id="_idIndexMarker181" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> length of the full training dataset is 50,000 images. We are using the CIFAR-10 train dataset because we set the Boolean value of <strong class="source-inline">train=True</strong>. Later on, we will use the test set from CIFAR-10 and will then <span>set </span><span><strong class="source-inline">train=False</strong></span><span>.</span></p>
			<p class="calibre6">Now, we will create a small dataset of 2,000 labeled images. The purpose here is to simulate the existence of a small labeled set of images, while the remaining images are unlabeled. Our objective is to identify and select the most informative images for labeling next with <span>active ML:</span></p>
			<pre class="source-code">
init_indices = list(range(2000)) # indices for initial our "labeled" set
labeled_set = Subset(full_dataset, init_indices)</pre>			<p class="calibre6">So, we have created a small labeled dataset and now need to initialize our training PyTorch data loader. A <strong class="bold">PyTorch data loader</strong> is used to load and iterate over datasets for training neural <a id="_idIndexMarker182" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>networks. It takes the dataset that contains the actual images and labels and is responsible for batching up these samples and feeding them to the model. The data loader allows you to specify a batch size, which determines how many samples are batched together – this is usually set to something like 64 or 128. Additionally, the data loader will shuffle the data by default if you are using it for a training set. This randomization of the order of samples helps the model generalize better <span>during training:</span></p>
			<pre class="source-code">
# Data loaders
labeled_loader = DataLoader(labeled_set, batch_size=64, shuffle=True)</pre>			<p class="calibre6">The next step is to initialize our model. We know that CIFAR-10 has <span>10 classes:</span></p>
			<pre class="source-code">
classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 
    'horse', 'ship', 'truck')
model = Net(n_classes=len(classes))</pre>			<p class="calibre6">A good practice<a id="_idIndexMarker183" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> is to <a id="_idIndexMarker184" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>visualize <a id="_idIndexMarker185" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>the data with which we <span>are working:</span></p>
			<pre class="source-code">
def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    plt.show()
# get some random training images
dataiter = iter(labeled_loader)
images, labels = next(dataiter)
# show images
imshow(torchvision.utils.make_grid(images))</pre>			<p class="calibre6"><span><em class="italic">Figure 4</em></span><em class="italic">.1</em> depicts a sample of CIFAR-10 <span>dataset images.</span></p>
			<div class="calibre18">
				<div id="_idContainer057" class="img---figure">
					<img src="image/B21789_04_1.jpg" alt="Figure 4.1 – A random visualization of some CIFAR-10 dataset images" class="calibre66"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 4.1 – A random visualization of some CIFAR-10 dataset images</p>
			<p class="calibre6">It is also good <a id="_idIndexMarker186" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>to take a<a id="_idIndexMarker187" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> look <a id="_idIndexMarker188" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>at the labels, so let’s print the first <span>five labels:</span></p>
			<pre class="source-code">
print(' '.join(f'{classes[labels[j]]:5s}' for j in range(5)))</pre>			<p class="calibre6">The preceding code returns the following list of labels as <span>the output:</span></p>
			<pre class="source-code">
frog  truck truck deer  car</pre>			<p class="calibre6">We can see that this is correct when cross-referencing these labels with the first five images in <span><em class="italic">Figure 4</em></span><span><em class="italic">.1</em></span><span>.</span></p>
			<p class="callout-heading">What is unnormalizing?</p>
			<p class="callout"><strong class="bold">Unnormalizing</strong> an image<a id="_idIndexMarker189" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> means reversing any normalization that was previously applied to the image pixel values in order to restore the original pixel value distribution (from the 0–1 range to the original <span>0–255 range).</span></p>
			<p class="calibre6">Now that we have our data loader, we can start the training loop; we first define our loss and optimizer. The <strong class="bold">loss function</strong> measures<a id="_idIndexMarker190" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> how well the model’s predictions match the true labels for a batch of images. It calculates the error between the predicted and true labels. Common loss functions for classification include cross-entropy loss and negative log-likelihood loss. These loss functions will output a high number if the model predicts incorrect labels, and a low number if the predictions are accurate. The goal during training is to minimize the loss by updating the model parameters. A good resource for learning about the loss functions available for use in PyTorch can be found <span>here: </span><a href="https://pytorch.org/docs/stable/nn.html#loss-functions" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span>https://pytorch.org/docs/stable/nn.html#loss-functions</span></a><span>.</span></p>
			<p class="calibre6">The <strong class="bold">optimizer</strong> is responsible<a id="_idIndexMarker191" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> for this parameter updating. It uses the loss value to perform backpropagation and update the model’s weights and biases to reduce the loss. <strong class="bold">Stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) is a popular optimization algorithm, where the<a id="_idIndexMarker192" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> parameters are updated proportionally to the gradient of the loss function. The learning rate controls the size of the updates. Other optimizers <a id="_idIndexMarker193" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>such as <strong class="bold">Adam</strong> and <strong class="bold">RMSProp</strong> are <a id="_idIndexMarker194" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>also commonly used <a id="_idIndexMarker195" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>for <a id="_idIndexMarker196" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>deep learning models (to learn<a id="_idIndexMarker197" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> about the optimizer functions available for use in PyTorch, you can visit this <span>link: </span><a href="https://pytorch.org/docs/stable/optim.html" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span>https://pytorch.org/docs/stable/optim.html</span></a><span>):</span></p>
			<pre class="source-code">
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)</pre>			<p class="calibre6">We will train our model for 100 epochs. Epochs represent the number of passes through the full training dataset during the training of the model. We define a <strong class="source-inline">train</strong> function as follows to run <span>our training:</span></p>
			<pre class="source-code">
def train(model, data_loader, epochs = 100):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    for epoch in range(epochs):  # loop over the dataset multiple times
        running_loss = 0.0
        for i, data in enumerate(data_loader, 0):
# get the inputs; data is a list of [inputs, labels]
            inputs, labels = data
# zero the parameter gradients
            optimizer.zero_grad()
# forward + backward + optimize
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
# print statistics
            running_loss += loss.item()
            if i % 10 == 9:    # print every 10 mini-batches
                print(f'[{epoch + 1}, {i + 1:5d}] loss: {
                    running_loss / 2000:.3f}')
                running_loss = 0.0
    print('Finished Training')
    return model</pre>			<p class="calibre6">Then, we run <span>the training:</span></p>
			<pre class="source-code">
model = train(model, labeled_loader)</pre>			<p class="calibre6">We have now an initial<a id="_idIndexMarker198" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> trained<a id="_idIndexMarker199" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> model on our small dataset and we want<a id="_idIndexMarker200" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> to use it to select the next images to label. But first, let’s evaluate this model on the CIFAR-10 test set. We define an <span>evaluation function:</span></p>
			<pre class="source-code">
def evaluate(model, test_dataset, batch_size=1):
    # Testing
    model.eval()
    test_loader = torch.utils.data.DataLoader(test_dataset, 
        batch_size)
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in tqdm(test_loader):
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    print('\nAccuracy of the model on the test images: {} 
        %'.format(100 * correct / total))</pre>			<p class="calibre6">We can then use this <a id="_idIndexMarker201" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>function <a id="_idIndexMarker202" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>with our trained model once we define our test set <span>as follows:</span></p>
			<pre class="source-code">
test_set = torchvision.datasets.CIFAR10(
    root='data', train=False, transform=transform, download=True)
print(len(test_set))</pre>			<p class="calibre6">The test set’s length <span>is 10,000.</span></p>
			<p class="calibre6">Let’s use our evaluation function with this <span>test set:</span></p>
			<pre class="source-code">
evaluate(model, test_set)</pre>			<p class="calibre6">This gives us the <span>following result:</span></p>
			<pre class="source-code">
Accuracy of the model on the test images: 40.08 %</pre>			<p class="calibre6">So we have now tested our first trained model, which was trained on the 2,000 images of our initial small labeled set. The<a id="_idIndexMarker203" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> model’s <a id="_idIndexMarker204" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>accuracy on the test set is 40.08%. We aim to<a id="_idIndexMarker205" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> improve this accuracy by labeling more images. This is where our active ML selection strategy comes <span>into play.</span></p>
			<h2 id="_idParaDest-60" class="calibre9"><a id="_idTextAnchor062" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Applying uncertainty sampling to improve classification performance</h2>
			<p class="calibre6">We will choose the <a id="_idIndexMarker206" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>most informative images to label next from our dataset – namely, the frames where the <strong class="bold">model is least confident</strong>, a method discussed in <a href="B21789_02.xhtml#_idTextAnchor027" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span><em class="italic">Chapter 2</em></span></a>, <em class="italic">Designing Query </em><span><em class="italic">Strategy Frameworks</em></span><span>.</span></p>
			<p class="calibre6">We first define a function to get the model’s <span>uncertainty scores:</span></p>
			<pre class="source-code">
def least_confident_score(predicted_probs):
    return 1 - predicted_probs[np.argmax(predicted_probs)]</pre>			<p class="calibre6">Then, we define our data loader for the unlabeled set. We will use a batch size of 1 as we will loop through all the images to get the <span>uncertainty scores:</span></p>
			<pre class="source-code">
unlabeled_loader = DataLoader(full_dataset, batch_size=1)</pre>			<p class="calibre6">We collect the confidence scores for our set of <span><strong class="bold">unlabeled</strong></span><span> images:</span></p>
			<pre class="source-code">
least_confident_scores = []
for image, label in unlabeled_loader:
    probs = F.softmax(model(image), dim=1)
    score = least_confident_score(probs.detach().numpy()[0])
    least_confident_scores.append(score)
print(least_confident_scores)</pre>			<p class="calibre6">This returns <span>the following:</span></p>
			<pre class="source-code">
[0.637821763753891, 0.4338147044181824, 0.18698161840438843, 0.6028554439544678, 0.35655343532562256, 0.3845849633216858, 0.4887065887451172, ...]</pre>			<p class="calibre6">These values<a id="_idIndexMarker207" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> represent the <strong class="bold">least confidence scores</strong> of the model’s predictions. The higher the scores, the less confident the model is. Therefore, next, we want to know the indices of the images where the scores are highest. We decide that we want to select 200 <span>images (queries):</span></p>
			<pre class="source-code">
num_queries = 200</pre>			<p class="calibre6">Then, we sort <span>by uncertainty:</span></p>
			<pre class="source-code">
sorted_uncertainties, indices = torch.sort(
    torch.tensor(least_confident_scores))</pre>			<p class="calibre6">We get the original indices of the most uncertain samples and print <span>the results:</span></p>
			<pre class="source-code">
most_uncertain_indices = indices[-num_queries:]
print(f"sorted_uncertainties: {sorted_uncertainties} \
    nmost_uncertain_indices selected: {most_uncertain_indices}")</pre>			<p class="calibre6">This returns <span>the following:</span></p>
			<pre class="source-code">
sorted_uncertainties: tensor([0.0000, 0.0000, 0.0000,  ..., 0.7419, 0.7460, 0.7928], dtype=torch.float64)
most_uncertain_indices selected: tensor([45820, 36802, 15912,  8635, 32207, 11987, 39232,  6099, 18543, 29082, 42403, 21331,  5633, 29284, 29566, 23878, 47522, 17097, 15229, 11468, 18130, 45120, 25245, 19864, 45457, 20434, 34309, 10034, 45285, 25496, 40169, 31792, 22868, 35525, 31238, 24694, 48734, 18419, 45289, 16126, 31668, 45971, 26393, ... 44338, 19687, 18283, 23128, 20556, 26325])</pre>			<p class="calibre6">Now we have the indices of the images selected using our active ML least-confident strategy. These are the images that would be sent to our oracles to be labeled and then used to train the <span>model again.</span></p>
			<p class="calibre6">Let’s take a look <a id="_idIndexMarker208" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>at five of these <span>selected images:</span></p>
			<pre class="source-code">
fig, axs = plt.subplots(1, 5)
for i in range(5):
    image, label = full_dataset[most_uncertain_indices[i]]
    image = image.squeeze().permute(1, 2, 0) / 2 + 0.5
    axs[i].imshow(image)
    axs[i].axis('off')
plt.show()</pre>			<div class="calibre18">
				<div id="_idContainer058" class="img---figure">
					<img src="image/B21789_04_2.jpg" alt="Figure 4.2 – Five of the chosen images to be labeled next" class="calibre67"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 4.2 – Five of the chosen images to be labeled next</p>
			<p class="calibre6">We have the images that now need to be labeled. Since this is a demo, we already have the labels, so let’s retrain our model with these newly labeled images. First, we need to add those to our <span>labeled set:</span></p>
			<pre class="source-code">
init_indices.extend(most_uncertain_indices)
labeled_set_2 = Subset(full_dataset, init_indices)
labeled_loader_2 = DataLoader(labeled_set, batch_size=64)
print(len(labeled_set_2))</pre>			<p class="calibre6">This returns 2,200, which is correct because we first selected 2,000 images from our dataset and then queried 200 with our active <span>ML sampling.</span></p>
			<p class="calibre6">Let’s start our training from scratch again for <span>100 epochs:</span></p>
			<pre class="source-code">
model_2 = Net(n_classes=len(classes))
model_2 = train(model_2, labeled_loader_2)</pre>			<p class="calibre6">Then, run the evaluation on the <span>test set:</span></p>
			<pre class="source-code">
evaluate(model_2, test_set)</pre>			<p class="calibre6">This returns <span>the following:</span></p>
			<pre class="source-code">
Accuracy of the model on the test images: 41.54 %</pre>			<p class="calibre6">We have improved<a id="_idIndexMarker209" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> the accuracy on the test set from 40.08% to 41.54% by adding images selected with our active ML strategy to the training dataset. We could also fine-tune the model that was originally trained <span>as follows:</span></p>
			<pre class="source-code">
model = train(model, labeled_loader_2)
evaluate(model, test_set)</pre>			<p class="calibre6">This gives us <span>the following:</span></p>
			<pre class="source-code">
Accuracy of the model on the test images: 40.84 %</pre>			<p class="calibre6">We have an interesting result here: the fine-tuned model is performing worse than the model trained from scratch using the larger dataset. Overall, the model’s performance improves when the selected images chosen by the active ML <span>are added.</span></p>
			<p class="calibre6">This approach can be applied to real-world problems. It is important to note, however, that this is a basic demonstration of how to use the least confident sampling method for classification. In a real project, you would need to have oracles label the selected images. Additionally, you would likely need to query more than 200 images and use a larger pretrained model, as <span>mentioned earlier.</span></p>
			<p class="calibre6">While the previous example demonstrated active ML for image classification, the same principles can be <a id="_idIndexMarker210" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>applied to other computer vision tasks such as object detection, as we’ll <span>see next.</span></p>
			<h1 id="_idParaDest-61" class="calibre8"><a id="_idTextAnchor063" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Applying active ML to an object detection project</h1>
			<p class="calibre6">In this section, we will<a id="_idIndexMarker211" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> guide you through the implementation of active ML techniques for an object detection project. An object detection project refers to developing a computer vision model to detect and localize objects within images or videos. The dataset is a collection of images (video frames) containing examples of the objects you want to detect, among other things. The dataset needs to have labels in the form of bounding boxes around the objects. Popular datasets for this purpose include <strong class="bold">COCO</strong> (<a href="https://cocodataset.org/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://cocodataset.org/</a>), <strong class="bold">PASCAL VOC</strong> (<a href="http://host.robots.ox.ac.uk/pascal/VOC/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">http://host.robots.ox.ac.uk/pascal/VOC/</a>), and <strong class="bold">OpenImages</strong> (<a href="https://storage.googleapis.com/openimages/web/index.html" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://storage.googleapis.com/openimages/web/index.html</a>). The model architecture uses a neural network designed for object detection such as Faster R-CNN, YOLO, and so on. This type of architecture can automatically identify and localize real-world objects within visual data. The end result is a model that can detect and draw boxes around objects such as cars, people, furniture, and <span>so on.</span></p>
			<p class="calibre6">The object detection project faces the same problem as classification projects: creating datasets is difficult and time-consuming. In fact, it is even more challenging for object detection tasks because it involves manually drawing bounding boxes around the objects. Once again, active ML provides a solution to this labeling bottleneck by sending the most informative images to the oracles for labeling. We will build an object detection model that will be capable of localizing brain tumors. This dataset we will use is from Roboflow Universe (<a href="https://universe.roboflow.com/" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://universe.roboflow.com/</a>) and is called <em class="italic">Brain Tumor Computer Vision Project</em>. To download this dataset, we use the <span>Roboflow API:</span></p>
			<pre class="source-code">
rf = Roboflow(api_key="your_key")
project = rf.workspace("roboflow-100").project("brain-tumor-m2pbp")
dataset = project.version(2).download("yolov8")</pre>			<p class="calibre6">This downloads the dataset locally. The dataset is downloaded as a folder with the structure shown in <span><em class="italic">Figure 4</em></span><span><em class="italic">.3</em></span><span>.</span></p>
			<div class="calibre18">
				<div id="_idContainer059" class="img---figure">
					<img src="image/B21789_04_3.jpg" alt="Figure 4.3 – Folder structure of the Roboflow Universe dataset, brain-tumor-m2pbp" class="calibre68"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 4.3 – Folder structure of the Roboflow Universe dataset, brain-tumor-m2pbp</p>
			<h2 id="_idParaDest-62" class="calibre9"><a id="_idTextAnchor064" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Preparing and training our model</h2>
			<p class="calibre6">Next, we need to fix the <strong class="source-inline">data.yaml</strong> file to work properly in Google Colab and organize the data for our <a id="_idIndexMarker212" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>active ML demo. The <strong class="source-inline">data.yaml</strong> file is used in the <strong class="source-inline">ultralytics</strong> training to specify where the different sets (<strong class="source-inline">train</strong>, <strong class="source-inline">valid</strong>, and <strong class="source-inline">test</strong>) are placed. We assume here that the original training set is our unlabeled set of images, the validation set is our testing data, and the test set is our labeled data because it has the fewest examples. So, first, we define a function to rename the <span>folders accordingly:</span></p>
			<pre class="source-code">
def rename_folders(current_folder_name, new_folder_name):
    # Check if the folder exists
    if os.path.exists(current_folder_name):
        # Rename the folder
        os.rename(current_folder_name, new_folder_name)
    else:
        print(f'The folder {current_folder_name} does not exist.')
# Now let's run it on our three folders train, valid, and test:
rename_folders(current_folder_name='/content/brain-tumor-2/train',
    new_folder_name='/content/brain-tumor-2/unlabeled')
rename_folders(current_folder_name='/content/brain-tumor-2/valid',
    new_folder_name='/content/brain-tumor-2/testing')
rename_folders(current_folder_name='/content/brain-tumor-2/test',
    new_folder_name='/content/brain-tumor-2/labeled')</pre>			<div class="calibre18">
				<div id="_idContainer060" class="img---figure">
					<img src="image/B21789_04_4.jpg" alt="Figure 4.4 – Structure of the dataset after renaming the subfolders for our demo" class="calibre69"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 4.4 – Structure of the dataset after renaming the subfolders for our demo</p>
			<p class="calibre6"><span><em class="italic">Figure 4</em></span><em class="italic">.4</em> shows the structure that we now have in our brain tumor dataset after renaming the folders<a id="_idIndexMarker213" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> for our demo. We then modify the <span><strong class="source-inline">data.yaml</strong></span><span> file:</span></p>
			<pre class="source-code">
path_data_yaml = '/content/brain-tumor-2/data.yaml'
with open(path_data_yaml, 'r') as file:
    data = yaml.safe_load(file)
data['train'] = 'labeled/images'
data['val'] = ''
data['test'] = 'testing/images'
with open(path_data_yaml, 'w') as file:
    yaml.dump(data, file)</pre>			<p class="calibre6">Here, we renamed the subfolder paths in the <strong class="source-inline">data.yaml</strong> file, which is the file that we will use for our training. We do not want to use the <strong class="source-inline">val</strong> folder for now in <span>our training.</span></p>
			<p class="calibre6">Now let’s take a look at our subfolders to determine the number of images in one <span>of them:</span></p>
			<pre class="source-code">
unlabeled_files = glob.glob('/content/brain-tumor-2/unlabeled/images/*.jpg')
labeled_files = glob.glob('/content/brain-tumor-2/labeled/images/*.jpg')
testing_files = glob.glob('/content/brain-tumor-2/testing/images/*.jpg')
print(f"For our demo, we have {len(unlabeled_files)} unlabeled files,
    {len(labeled_files)} labeled files, and {len(testing_files)} 
    testing files")</pre>			<p class="calibre6">The preceding code returns <span>the following:</span></p>
			<pre class="source-code">
For our demo, we have 6930 unlabeled files, 990 labeled files, and 1980 testing files</pre>			<p class="calibre6">We can now<a id="_idIndexMarker214" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> begin our initial training on our <strong class="source-inline">labeled</strong> dataset. For this training, we will utilize a widely used Python computer vision package called <strong class="source-inline">ultralytics</strong> (<a href="https://github.com/ultralytics/ultralytics" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://github.com/ultralytics/ultralytics</a>) and employ the <strong class="source-inline">yolov8</strong> model. The <strong class="source-inline">yolov8</strong> model is capable of performing tasks such as detection and tracking, instance segmentation, image classification, and pose estimation. We will train our model for 10 epochs only for our demo purposes. We use the <strong class="source-inline">detect</strong> task type because we want to train the model for <span>object detection:</span></p>
			<pre class="source-code">
from ultralytics import YOLO
model = YOLO('yolov8s.pt')
print('Start training ')
results = model.train(data=path_data_yaml,
    batch=32,
    task='detect',
    mode='train',
    epochs=10
    )</pre>			<h2 id="_idParaDest-63" class="calibre9"><a id="_idTextAnchor065" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Analyzing the evaluation metrics</h2>
			<p class="calibre6">Once the training is<a id="_idIndexMarker215" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> done, we evaluate it on the test set. Here is how we can evaluate <span>the model:</span></p>
			<pre class="source-code">
metrics = model.val(data=path_data_yaml, split='test')
print(metrics.results_dict)</pre>			<p class="calibre6">The preceding returns the <span>following output:</span></p>
			<pre class="source-code">
{'metrics/precision(B)': 0.6022637781613859,
'metrics/recall(B)': 0.4763619681952341,
'metrics/mAP50(B)': 0.4953616848732552,
'metrics/mAP50-95(B)': 0.2252478418006819,
'fitness': 0.25225922610793927}</pre>			<p class="calibre6">Let’s analyze <span>these metrics:</span></p>
			<ul class="calibre16">
				<li class="calibre20"><strong class="source-inline">precision(B)</strong> measures how many of the predicted bounding boxes are correct. A value of 0.60 means 60% of the predicted boxes match the ground <span>truth boxes.</span></li>
				<li class="calibre20"><strong class="source-inline">recall(B)</strong> measures how many of the ground truth boxes were correctly detected. A value of 0.48 means the model detected 48% of the <span>true boxes.</span></li>
				<li class="calibre20"><strong class="source-inline">mAP50(B)</strong> is the mean average precision at the <strong class="bold">intersection over union</strong> (<strong class="bold">IoU</strong>) threshold<a id="_idIndexMarker216" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> of 50%, which measures the model’s precision across different confidence thresholds. A prediction is considered correct if the IoU with ground truth is at least 50%. A value of 0.50 means the model has 50% mAP at this <span>IoU threshold.</span></li>
				<li class="calibre20"><strong class="source-inline">mAP50-95(B)</strong> is the mean average precision at IoU thresholds between 50% and 95% and is a more strict metric that expects higher overlap with the ground truth to be considered correct. The lower value of 0.23 indicates performance drops at higher <span>IoU thresholds.</span></li>
				<li class="calibre20"><strong class="source-inline">fitness</strong> combines precision and recall. A model that scores high on precision but low on recall would have poor fitness. Similarly, high recall but low precision also results in low fitness. A high fitness score requires strong performance on both precision and recall metrics. This encourages the model to improve both the accuracy and completeness of detections during training. The specific fitness value of 0.25 here indicates there is significant room for improvement in precision, recall, <span>or both.</span></li>
			</ul>
			<p class="calibre6">The metrics indicate reasonably good precision but lower recall, meaning the model struggles to detect all ground truth boxes. The high precision but lower mAP shows many detections are offset from the ground truth. Overall, the metrics show room for improvement in <a id="_idIndexMarker217" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>the alignment and completeness <span>of detections.</span></p>
			<p class="calibre6">The next step is thus to select the most informative images to label using our active <span>ML approach.</span></p>
			<h2 id="_idParaDest-64" class="calibre9"><a id="_idTextAnchor066" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Implementing an active ML strategy</h2>
			<p class="calibre6">We will <a id="_idIndexMarker218" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>use the <strong class="source-inline">ultralytics</strong> package, which is highly useful for enabling the selection of informative images. This can help us improve the metrics we just discussed. This package provides the confidence score for each bounding box prediction, which we will <span>utilize here.</span></p>
			<p class="calibre6">We apply the model to each image in the unlabeled set using a confidence threshold of <strong class="source-inline">0.15</strong>. This means that any predictions with a confidence score below 0.15 will be discarded. You can choose this value based on your specific needs and use case. It is important to keep in mind that choosing a low confidence score threshold allows for the selection of images where the model <span>lacks confidence:</span></p>
			<pre class="source-code">
results = model(os.path.join('/content/brain-tumor-2/', 
    'unlabeled/images'), verbose=False, conf=0.15)</pre>			<p class="calibre6">Let’s take a look at some of these images and the predicted <span>bounding boxes:</span></p>
			<pre class="source-code">
plt.figure(figsize=(12, 8))
for i in range(1, 33):
    plt.subplot(4,8,i)
    image = results[i].orig_img
    for b in results[i].boxes.xywhn:
        x, y, w, h = b.tolist()
# Convert YOLO format coordinates to OpenCV format coordinates
        dh, dw, _ = image.shape
        l = int((x - w / 2) * dw)
        r = int((x + w / 2) * dw)
        t = int((y - h / 2) * dh)
        b = int((y + h / 2) * dh)
        cv2.rectangle(image, (l, t), (r, b), (0, 255, 0), 1)
    plt.imshow(image)
plt.show()</pre>			<div class="calibre18">
				<div id="_idContainer061" class="img---figure">
					<img src="image/B21789_04_5.jpg" alt="Figure 4.5 – Samples of model predictions on images from the unlabeled set" class="calibre70"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 4.5 – Samples of model predictions on images from the unlabeled set</p>
			<p class="calibre6">We can see in <span><em class="italic">Figure 4</em></span><em class="italic">.5</em> that the model is detecting tumors in the unlabeled <span>brain images.</span></p>
			<p class="calibre6">Let’s collect all the confidence scores of the predicted <span>bounding boxes:</span></p>
			<pre class="source-code">
confidences = []
for result in results:
    confidences.append(result.boxes.conf)</pre>			<p class="calibre6">We'll only keep the minimum confidence value for each image. If there is no predicted bounding box, we add a confidence score of <strong class="source-inline">10</strong> (a high value to put these images at the end of the list of potential images). Confidence scores are values ranging from 0 to 1, with 1 <span>being</span><span><a id="_idIndexMarker219" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/></span><span> high:</span></p>
			<pre class="source-code">
confidence_scores = []
for confidence in confidences:
    if len(confidence) &gt; 0:
        confidence_scores.append(np.min(np.array(confidence.cpu())))
    else:
        confidence_scores.append(10)
print(confidence_scores)</pre>			<p class="calibre6">We have 6,930 confidence scores, which is correct because we have 6,930 <span>unlabeled files.</span></p>
			<p class="calibre6">Next, we select 500 images where the confidence scores are <span>the lowest:</span></p>
			<pre class="source-code">
num_queries = 500
# Sort by uncertainty
sorted_uncertainties, indices = torch.sort(torch.tensor(confidence_scores))
# Get original indices of most uncertain samples
most_uncertain_indices = indices[-num_queries:]
 print(f"sorted_uncertainties: {sorted_uncertainties[0:num_queries]} \
    nmost_uncertain_indices selected: {most_uncertain_indices}")</pre>			<p class="calibre6">This returns <span>the following:</span></p>
			<pre class="source-code">
sorted_uncertainties: tensor([0.1500, 0.1500, 0.1501, 0.1501, 0.1501, 0.1501, ..., 0.1598, 0.1598, 0.1598, 0.1599, 0.1599, 0.1599, 0.1599, 0.1599, 0.1600])
most_uncertain_indices selected: tensor([4714, 4713, 4712, 4304, 4305, 4306,  ...., 5554, 5553, 5552, 5551, 5550, 5549, 5548, 5547, 3135, 5544, 5543])</pre>			<p class="calibre6">We now get the selected images with <span>the following:</span></p>
			<pre class="source-code">
images_selected = np.array(
    glob.glob(os.path.join('/content/brain-tumor-2/', 
        'unlabeled/images', '*.jpg'))
)[np.array(most_uncertain_indices)]</pre>			<p class="calibre6">We move these<a id="_idIndexMarker220" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> selected images (and their corresponding label files) to our labeled set – this mimics the step where we would have an oracle label <span>these images:</span></p>
			<pre class="source-code">
import shutil
for image_path in images_selected:
    shutil.move(image_path, image_path.replace('unlabeled', 
        'labeled'))
    label_file = image_path.replace('images', 'labels').replace('.jpg', '.txt')
    shutil.move(label_file, label_file.replace('unlabeled', 
        'labeled'))</pre>			<p class="calibre6">Let’s check that we have correctly moved the images and <span>label files:</span></p>
			<pre class="source-code">
images_labeled = glob.glob('/content/brain-tumor-2/labeled/images/*.jpg')
labels_labeled = glob.glob('/content/brain-tumor-2/labeled/labels/*.txt')
print(len(images_labeled))
print(len(labels_labeled))</pre>			<p class="calibre6">These two <strong class="source-inline">print</strong> commands both return 1,490, which is what we expected because we had 990 labeled images and then added 500 new <span>image/label pairs.</span></p>
			<p class="calibre6">We can train our model again with this <span>updated dataset:</span></p>
			<pre class="source-code">
model = YOLO('yolov8s.pt')
print('Start training ')
results = model.train(data=path_data_yaml,
    batch=32,
    task='detect',
    mode='train',
    epochs=10
    )</pre>			<p class="calibre6">Then, we evaluate<a id="_idIndexMarker221" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> this model on the <span>test set:</span></p>
			<pre class="source-code">
metrics = model.val(data=path_data_yaml, split='test')
metrics.results_dict</pre>			<p class="calibre6">Now we get the <span>following metrics:</span></p>
			<pre class="source-code">
{'metrics/precision(B)': 0.6469528069030884,
'metrics/recall(B)': 0.5106541285546612,
'metrics/mAP50(B)': 0.543579045283473,
'metrics/mAP50-95(B)': 0.26662268193511757,
'fitness': 0.29431831826995314}</pre>			<p class="calibre6">Comparing these metrics with the previous metrics we got, we can see that the precision improved from 0.60 to 0.65, the recall from 0.48 to 0.51, the mAP50 from 0.50 to 0.54, the mAP50-95 from 0.22 to 0.27, and the fitness from 0.25 to 0.29. So, adding the 500 most informative images to our labeled set improved our metrics across <span>the board.</span></p>
			<p class="calibre6">We can use a <a id="_idIndexMarker222" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>similar method for instance segmentation, which we will cover in the <span>next section.</span></p>
			<h1 id="_idParaDest-65" class="calibre8"><a id="_idTextAnchor067" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Using active ML for a segmentation project</h1>
			<p class="calibre6">In this section, we will reuse <a id="_idIndexMarker223" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>what we did for the object <a id="_idIndexMarker224" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>detection task, but instead of using an object detection dataset, we will use an instance segmentation dataset with the <strong class="source-inline">segment</strong> task <span>of </span><span><strong class="source-inline">yolov8</strong></span><span>.</span></p>
			<p class="calibre6"><strong class="bold">Instance segmentation</strong> is a<a id="_idIndexMarker225" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> computer vision task that involves detecting and segmenting individual objects in an image at the pixel level. It combines elements of object detection, which localizes objects by drawing bounding boxes around them, and semantic segmentation, which classifies each pixel in the image according to the class it belongs to. Instance segmentation goes a step further – it assigns an instance label to each segmented object. The output is a set of masks, one per detected object instance, that indicate the exact pixels that belong to each object. Instance segmentation provides a more detailed delineation of objects compared to the bounding boxes produced in object detection. It segments objects at the pixel level rather than just enclosing them in boxes. It also goes beyond semantic segmentation, which only distinguishes between classes such as <strong class="source-inline">person</strong>, <strong class="source-inline">car</strong>, and so on. Instance segmentation separates individual instances of those classes – person 1 versus person 2, or car 1 versus <span>car 2.</span></p>
			<p class="calibre6">The combination of localization, classification, and separation of instances enables precise analysis of images down to the pixel level. This makes instance segmentation useful for applications such as autonomous driving, medical imaging, and robotics, where understanding scenes at a fine-grained level is required. Some popular instance segmentation<a id="_idIndexMarker226" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> algorithms <a id="_idIndexMarker227" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>and <a id="_idIndexMarker228" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>models are <strong class="bold">Mask R-CNN</strong> (<a href="https://arxiv.org/abs/1703.06870" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://arxiv.org/abs/1703.06870</a>), <strong class="bold">Panoptic FPN</strong> (<a href="https://arxiv.org/abs/1901.02446" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3">https://arxiv.org/abs/1901.02446</a>), and <span><strong class="bold">YOLACT</strong></span><span> (</span><a href="https://arxiv.org/abs/1904.02689" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"><span>https://arxiv.org/abs/1904.02689</span></a><span>).</span></p>
			<p class="calibre6">Let’s download the <strong class="source-inline">strawberry</strong> dataset from <span>Roboflow Universe</span></p>
			<pre class="source-code">
rf = Roboflow(api_key="your_key")
project = rf.workspace("5060tanapoowapat-yumsarn").project("strawberry-2vs5u")
dataset = project.version(2).download("yolov8")</pre>			<p class="calibre6">Then, we work through the same steps as we followed for the object detection dataset in the preceding section. We rename the subfolders for our demo use case and update the YAML file. We end up with the structure shown in <span><em class="italic">Figure 4</em></span><span><em class="italic">.6</em></span><span>.</span></p>
			<div class="calibre18">
				<div id="_idContainer062" class="img---figure">
					<img src="image/B21789_04_6.jpg" alt="Figure 4.6 – Structure of the strawberry dataset after renaming the subfolders for our demo" class="calibre71"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 4.6 – Structure of the strawberry dataset after renaming the subfolders for our demo</p>
			<p class="calibre6">For this dataset, after renaming the folders and updating the YAML file, the code returns <span>the following:</span></p>
			<pre class="source-code">
For our demo, we have 3006 unlabeled files, 184 labeled files, and 659 testing files</pre>			<p class="calibre6">As we are now <a id="_idIndexMarker229" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>training<a id="_idIndexMarker230" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> for instance segmentation, we update the training code <span>as follows:</span></p>
			<pre class="source-code">
model = YOLO('yolov8n-seg.pt')
print('Start training ')
results = model.train(data=path_data_yaml,
    batch=16,
    task='segment',
    mode='train',
    epochs=10
    )</pre>			<p class="calibre6">Once the training is completed, we evaluate the model using the same code as in the previous project and obtain the <span>following metrics:</span></p>
			<pre class="source-code">
{'metrics/precision(B)': 0.673169825129636,
'metrics/recall(B)': 0.7297833796885302,
'metrics/mAP50(B)': 0.7664149988792639,
'metrics/mAP50-95(B)': 0.533442993245899,
'metrics/precision(M)': 0.7415224838967787,
'metrics/recall(M)': 0.7482014388489209,
'metrics/mAP50(M)': 0.8165979711704425,
'metrics/mAP50-95(M)': 0.5967313838152124,
'fitness': 1.175458236359971}</pre>			<p class="calibre6">The metrics with <strong class="source-inline">(B)</strong> represent the metrics for object detection, while the metrics with <strong class="source-inline">(M)</strong> refer to instance segmentation, where <strong class="source-inline">M</strong> stands for masks. The metrics are the same between the<a id="_idIndexMarker231" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> two <a id="_idIndexMarker232" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>types; the only difference is that the <strong class="source-inline">M</strong> metrics are computed on the pixels from the masks rather than those from the <span>bounding boxes.</span></p>
			<p class="calibre6">Following the same logic, we then select the images to label in order to improve <span>our metrics.</span></p>
			<p class="calibre6">The code is slightly different this time when we run the model on each image in the <span>unlabeled set:</span></p>
			<pre class="source-code">
results = model(os.path.join(f'/content/{dataset_name}/',
    'unlabeled/images'), verbose=False, conf=0.25, task='segment')</pre>			<p class="calibre6">We have to specify that we are performing a segmentation task and choose a higher confidence threshold to avoid memory issues <span>in Colab.</span></p>
			<p class="calibre6">Let’s take a look at the model’s predictions on some of the <span>unlabeled images:</span></p>
			<pre class="source-code">
plt.figure(figsize=(12, 8))
# Generate a list of 32 random integers between 0 and 100
random_integers = [random.randint(0, 100) for _ in range(32)]
for i, index in enumerate(random_integers):
    plt.subplot(4,8,i+1)
    image = results[index].orig_img
    for b in results[index].boxes.xywhn:
        x, y, w, h = b.tolist()
        # Convert YOLO format coordinates to OpenCV format coordinates
        dh, dw, _ = image.shape
        l = int((x - w / 2) * dw)
        r = int((x + w / 2) * dw)
        t = int((y - h / 2) * dh)
        b = int((y + h / 2) * dh)
    cv2.rectangle(image, (l, t), (r, b), (0, 255, 0), 2)
    if results[index].masks:
        overlayed_image = image.copy()
        for m in results[index].masks:
            # Make sure both images are of data type uint8
            mask = np.array(m.data.cpu()[0])
            mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
            image = image.astype(np.uint8)
            mask = mask*255
            mask = mask.astype(np.uint8)
            # Overlay the mask on the RGB image
            overlayed_image = cv2.addWeighted(overlayed_image, 1, 
                mask, 0.8, 0)
    plt.imshow(cv2.cvtColor(overlayed_image, cv2.COLOR_BGR2RGB))
plt.show()</pre>			<p class="calibre6">This returns the image depicted in <span><em class="italic">Figure 4</em></span><span><em class="italic">.7</em></span><span>.</span></p>
			<div class="calibre18">
				<div id="_idContainer063" class="img---figure">
					<img src="image/B21789_04_7.jpg" alt="Figure 4.7 – Samples of model predictions on images from the unlabeled set" class="calibre72"/>
				</div>
			</div>
			<p class="calibre6" lang="en-US" xml:lang="en-US">Figure 4.7 – Samples of model predictions on images from the unlabeled set</p>
			<p class="calibre6">In <span><em class="italic">Figure 4</em></span><em class="italic">.7</em>, we can see that the model is correctly detecting most of the strawberries. The object detection <a id="_idIndexMarker233" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>is <a id="_idIndexMarker234" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>represented by the green bounding boxes in the images, while the segmentation is indicated by the white <span>overlaying masks.</span></p>
			<p class="calibre6">We then follow the steps discussed in the preceding section on object detection, where we selected the 500 images to label next, and we get the <span>following result:</span></p>
			<pre class="source-code">
sorted_uncertainties: tensor([0.2500, 0.2501, 0.2501, 0.2501, 0.2501, 0.2502, 0.2503, 0.2503, 0.2503, 0.2503, 0.2503,..., 0.2703, 0.2703, 0.2703, 0.2703, 0.2703, 0.2704, 0.2704, 0.2704, 0.2704, 0.2704])
most_uncertain_indices selected: tensor([2744,  806, 1822, 1025, 1486,  345,  743, 1374, 2329, 1381,  301, 2322, 2272, 1196, ..., 2127, 2004, 2119, 2118, 1401, 1402, 2666, 2105,  100,   47, 2093,   46, 2092, 2085,  970, 1422])</pre>			<p class="calibre6">We move these images to our labeled set, and thus go from 184 images in the labeled set to 684. We run the training on the updated labeled set, followed by the evaluation, and obtain <span>these metrics:</span></p>
			<pre class="source-code">
{'metrics/precision(B)': 0.7522007556106134,
'metrics/recall(B)': 0.7570614064930203,
'metrics/mAP50(B)': 0.800552933790843,
'metrics/mAP50-95(B)': 0.6079730626509038,
'metrics/precision(M)': 0.8061734224988162,
'metrics/recall(M)': 0.8069544364508393,
'metrics/mAP50(M)': 0.8511208111235853,
'metrics/mAP50-95(M)': 0.6554160034789296,
'fitness': 1.3022175340082929}</pre>			<p class="calibre6">Let’s compare those<a id="_idIndexMarker235" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> to<a id="_idIndexMarker236" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/> the metrics we had before the addition of the 500 most <span>informative images:</span></p>
			<pre class="source-code">
{'metrics/precision(B)': 0.673169825129636,
'metrics/recall(B)': 0.7297833796885302,
'metrics/mAP50(B)': 0.7664149988792639,
'metrics/mAP50-95(B)': 0.533442993245899,
'metrics/precision(M)': 0.7415224838967787,
'metrics/recall(M)': 0.7482014388489209,
'metrics/mAP50(M)': 0.8165979711704425,
'metrics/mAP50-95(M)': 0.5967313838152124,
'fitness': 1.175458236359971}</pre>			<p class="calibre6">We can observe that all metrics <span>have improved.</span></p>
			<h1 id="_idParaDest-66" class="calibre8"><a id="_idTextAnchor068" class="pcalibre pcalibre1 calibre5 pcalibre4 pcalibre2 pcalibre3"/>Summary</h1>
			<p class="calibre6">In conclusion, this chapter has demonstrated how active ML can be applied to optimize the training of computer vision models. As we have seen, computer vision tasks such as image classification, object detection, and instance segmentation require large labeled datasets to train <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>). Manually collecting and labeling this much data is expensive <span>and time-consuming.</span></p>
			<p class="calibre6">Active ML provides a solution to this challenge by intelligently selecting the most informative examples to be labeled by a human oracle. Strategies such as uncertainty sampling query the model to find the data points it is least certain about. By labeling only these useful data points, we can train our models with significantly less data-labeling <span>effort required.</span></p>
			<p class="calibre6">In this chapter, we covered implementing active ML approach for diverse computer vision applications. By interactively querying the model and refining the training data, we can rapidly improve model performance at a fraction of the labeling cost. These techniques make it feasible to develop computer vision systems even with <span>limited data.</span></p>
			<p class="calibre6">The active ML implementations presented offer new possibilities for building performant and robust computer vision models without needing massive datasets. With these strategies, you can optimize and target the data collection and training efforts for efficient results. Going forward, active ML will become an essential tool for developing real-world computer <span>vision systems.</span></p>
			<p class="calibre6">In the next chapter, we will explore how to leverage active ML for big data projects that use large amounts of data, such <span>as videos.</span></p>
		</div>
	</div>
</div>
</body></html>