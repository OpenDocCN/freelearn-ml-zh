<html><head></head><body>
		<div id="_idContainer096">
			<h1 id="_idParaDest-111"><em class="italic"><a id="_idTextAnchor131"/>Chapter 8</em>: Training Models with MLflow</h1>
			<p>In this chapter, you will learn about creating production-ready training jobs with MLflow. In the bigger scope of things, we will focus on how to move from the training jobs in the notebook environment that we looked at in the early chapters to a standardized format and blueprint to create training jobs.</p>
			<p>Specifically, we will look at the following sections in this chapter:</p>
			<ul>
				<li>Creating your training project with MLflow</li>
				<li>Implementing the training job</li>
				<li>Evaluating the model</li>
				<li>Deploying the model in the Model Registry</li>
				<li>Creating a Docker image for your training job</li>
			</ul>
			<p>It's time to add to the pyStock <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) platform training infrastructure to take <strong class="bold">proof-of-concept</strong> models created in the workbench developed in <a href="B16783_03_Final_SB_epub.xhtml#_idTextAnchor066"><em class="italic">Chapter 3</em></a>, <em class="italic">Your Data Science Workbench to a Production Environment</em>.</p>
			<p>In this chapter, you will be developing a training project that runs periodically or when triggered by a dataset arrival. The main output of the training project is a new model that is generated as output and registered in the Model Registry with different details. </p>
			<p>Here is an overview of the training workflow:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/image0014.jpg" alt="Figure 8.1 – Training workflow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Training workflow</p>
			<p><em class="italic">Figure 8.1</em> describes at a high level the general process, whereby a training dataset arrives and a training job kicks in. The training job produces a model that is finally evaluated and deployed in the Model Registry. Systems upstream are now able to deploy inference <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>) or batch jobs with the newly deployed model.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor132"/>Technical requirements</h1>
			<p>For this chapter, you will need the following prerequisites:</p>
			<ul>
				<li>The latest version of Docker installed on your machine. If you don't already have it installed, please follow the instructions at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</li>
				<li>The latest version of Docker Compose installed—please follow the instructions at <a href="https://docs.docker.com/compose/install/">https://docs.docker.com/compose/install/</a>.</li>
				<li>Access to Git in the command line, and installed as described at <a href="https://git-scm.com/book/en/v2/Getting-Started-Installing-Git">https://git-scm.com/book/en/v2/Getting-Started-Installing-Git</a>.</li>
				<li>Access to a Bash terminal (Linux or Windows).</li>
				<li>Access to a browser.</li>
				<li>Python 3.5+ installed.</li>
				<li>The latest version of your ML library installed locally as described in <a href="B16783_04_Final_SB_epub.xhtml#_idTextAnchor081"><em class="italic">Chapter 4</em></a>, <em class="italic">Experiment Management in MLflow</em></li>
			</ul>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor133"/>Creating your training project with MLflow</h1>
			<p>You receive a specification<a id="_idIndexMarker273"/> from a data scientist based<a id="_idIndexMarker274"/> on the <strong class="bold">XGBoost</strong> model<a id="_idIndexMarker275"/> being ready to move from a <strong class="bold">proof-of-concept</strong> to<a id="_idIndexMarker276"/> a production phase.</p>
			<p>We can review the original Jupyter notebook from which the model was registered initially by the data scientist, which is a starting point to start creating an ML engineering pipeline. After initial prototyping and training in the notebook, they are ready to move to production.</p>
			<p>Some companies go directly to productionize the notebooks themselves and this is definitely a possibility, but it becomes impossible for the following reasons:</p>
			<ul>
				<li>It's hard to version notebooks.</li>
				<li>It's hard to unit-test the code.</li>
				<li>It's unreliable for long-running tests.</li>
			</ul>
			<p>With these three distinct phases, we ensure reproducibility of the training data-generation process<a id="_idIndexMarker277"/> and visibility and clear separation of the different<a id="_idIndexMarker278"/> steps of the process.</p>
			<p>We will start by organizing our MLflow project into steps and creating placeholders for each of the components of the pipeline, as follows:</p>
			<ol>
				<li>Start a new folder in your local machine and name this <strong class="source-inline">pystock-training</strong>. Add the <strong class="source-inline">MLProject</strong> file, as follows:<p class="source-code">name: pystock_training</p><p class="source-code">conda_env: conda.yaml</p><p class="source-code">entry_points:</p><p class="source-code">  main:</p><p class="source-code">    data_file: path</p><p class="source-code">    command: "python main.py"</p><p class="source-code">  train_model:</p><p class="source-code">    command: "python train_model.py"</p><p class="source-code">  evaluate_model:</p><p class="source-code">    command: "python evaluate_model.py "</p><p class="source-code">  register_model:</p><p class="source-code">    command: "python register_model.py"</p></li>
				<li>Add<a id="_idIndexMarker279"/> the<a id="_idIndexMarker280"/> following <strong class="source-inline">conda.yaml</strong> file:<p class="source-code">name: pystock-training</p><p class="source-code">channels:</p><p class="source-code">  - defaults</p><p class="source-code">dependencies:</p><p class="source-code">  - python=3.8</p><p class="source-code">  - numpy</p><p class="source-code">  - scipy</p><p class="source-code">  - pandas</p><p class="source-code">  - cloudpickle</p><p class="source-code">  - pip:</p><p class="source-code">    - git+git://github.com/mlflow/mlflow</p><p class="source-code">    - sklearn</p><p class="source-code">    - pandas_datareader</p><p class="source-code">    - great-expectations==0.13.15</p><p class="source-code">    - pandas-profiling</p><p class="source-code">    - xgboost</p></li>
				<li>You can add<a id="_idIndexMarker281"/> now a sample <strong class="source-inline">main.py</strong> file to the folder to ensure<a id="_idIndexMarker282"/> that the basic structure of the project is working, as follows:<p class="source-code">import mlflow</p><p class="source-code">import click</p><p class="source-code">import os</p><p class="source-code">def _run(entrypoint, parameters={}, source_version=None, use_cache=True):</p><p class="source-code">    print("Launching new run for entrypoint=%s and parameters=%s" % (entrypoint, parameters))</p><p class="source-code">    submitted_run = mlflow.run(".", entrypoint, parameters=parameters)</p><p class="source-code">    return mlflow.tracking.MlflowClient().get_run(submitted_run.run_id)</p><p class="source-code">@click.command()</p><p class="source-code">def workflow():</p><p class="source-code">    with mlflow.start_run(run_name ="pystock-training") as active_run:</p><p class="source-code">        mlflow.set_tag("mlflow.runName", "pystock-training")</p><p class="source-code">        _run("train_model")</p><p class="source-code">        _run("evaluate_model")        </p><p class="source-code">        _run("register_model")</p><p class="source-code">        </p><p class="source-code">if __name__=="__main__":</p><p class="source-code">    workflow()</p></li>
				<li>Test the basic structure by running the following command:<p class="source-code">mlflow run.</p><p>This command will build<a id="_idIndexMarker283"/> your project based on the environment<a id="_idIndexMarker284"/> created by your <strong class="source-inline">conda.yaml</strong> file and run the basic project you just created. It should error out, as we need to add the missing files.</p></li>
			</ol>
			<p>At this stage, we have the basic blocks of the MLflow project of the data pipeline that we will be building in this chapter. You will next fill in the Python file to train the data.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor134"/>Implementing the training job</h1>
			<p>We will use the training data<a id="_idIndexMarker285"/> produced in the previous chapter. The assumption here is that an independent job populates the data pipeline in a specific folder. In the book's GitHub repository, you can look at the data in https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter08/psystock-training/data/training/data.csv.</p>
			<p>We will now create a <strong class="source-inline">train_model.py</strong> file that will be responsible for loading the training data to fit and produce a model. Test predictions will be produced and persisted in the environment so that other steps of the workflow can use the data to evaluate the model.</p>
			<p>The file produced in this section is available at the following link: </p>
			<p>https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter08/psystock-training/train_model.py<a href="https://github.com/PacktPublishing/Machine-Learning-Engineering-with-Mlflow/blob/master/chapter_8/psytock-training/train_model.py  ">: </a></p>
			<ol>
				<li value="1">We will start by importing the relevant packages. In this case, we will need <strong class="source-inline">pandas</strong> to handle the data, <strong class="source-inline">xgboost</strong> to run the training algorithm, and—obviously—m<strong class="source-inline">lflow</strong> to track and log the data run. Here is the code you'll need to do this:<p class="source-code">import pandas as pd</p><p class="source-code">import mlflow</p><p class="source-code">import xgboost as xgb</p><p class="source-code">import mlflow.xgboost</p><p class="source-code">from sklearn.model_selection import train_test_split</p></li>
				<li>Next, you should add a function to execute the split of the data relying on <strong class="source-inline">train_test_split</strong> from <strong class="source-inline">sklearn</strong>. Our chosen split is 33/67% for testing and training data respectively. We specify the <strong class="source-inline">random_state</strong> parameter in order to<a id="_idIndexMarker286"/> make the process reproducible, as follows:<p class="source-code">def train_test_split_pandas(pandas_df,t_size=0.33,r_state=42):</p><p class="source-code">    X=pandas_df.iloc[:,:-1]</p><p class="source-code">    Y=pandas_df.iloc[:,-1]</p><p class="source-code">    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=t_size, random_state=r_state)</p><p class="source-code">    return X_train, X_test, y_train, y_test</p></li>
				<li>This function returns the train and test dataset and the targets for each dataset. We rely on the <strong class="source-inline">xgboost</strong> matrix <strong class="source-inline">xgb.Dmatrix</strong> data format to efficiently load the training<a id="_idIndexMarker287"/> and testing data and feed the <strong class="source-inline">xgboost.train</strong> method. The code is illustrated in the following snippet:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    THRESHOLD = 0.5</p><p class="source-code">    mlflow.xgboost.autolog()</p><p class="source-code">    with mlflow.start_run(run_name="train_model") as run:</p><p class="source-code">        mlflow.set_tag("mlflow.runName", "train_model")</p><p class="source-code">        pandas_df=pd.read_csv("data/training/data.csv")</p><p class="source-code">        pandas_df.reset_index(inplace=True)</p><p class="source-code">        X_train, X_test, y_train, y_test = train_test_split_pandas(pandas_df)</p><p class="source-code">        train_data = xgb.DMatrix(X_train, label=y_train)</p><p class="source-code">        test_data =  xgb.DMatrix(X_test)</p><p class="source-code">        model = xgb.train(dtrain=train_data,params={})        </p></li>
				<li>We also use this moment to produce test predictions using the <strong class="source-inline">model.predict</strong> method. Some data transformation is executed to discretize the probability of the stock going up or down and transform it into <strong class="source-inline">0</strong> (not going up) or <strong class="source-inline">1</strong> (going up), as follows:<p class="source-code">        y_probas=model.predict(test_data) </p><p class="source-code">        y_preds = [1 if  y_proba &gt; THRESHOLD else 0. for y_proba in y_probas]</p></li>
				<li>As a last<a id="_idIndexMarker288"/> step, we will persist the test predictions on the <strong class="source-inline">result</strong> variable. We drop the index so that the saved <strong class="source-inline">pandas</strong> DataFrame doesn't include the index when running the <strong class="source-inline">result.to_csv</strong> command, as follows:<p class="source-code">        test_prediction_results = pd.DataFrame(data={'y_pred':y_preds,'y_test':y_test})</p><p class="source-code">        result = test_prediction_results.reset_index(drop=True)</p><p class="source-code">        </p><p class="source-code">        result.to_csv("data/predictions/test_predictions.csv")    </p></li>
				<li>You can look at your MLflow <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) by running<a id="_idIndexMarker289"/> the following command to see the metrics logged:<p class="source-code"> mlflow ui</p></li>
			</ol>
			<p>You should be able to look at your MLflow UI, available to view in the following screenshot, where you can see the persisted model and the different model information of the just-trained model:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/image0025.jpg" alt="Figure 8.2 – Training model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2 – Training model</p>
			<p>At this stage, we have our model<a id="_idIndexMarker290"/> saved and persisted on the artifacts of our MLflow installation. We will next add a new step to our workflow to produce the metrics of the model just produced.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor135"/>Evaluating the model</h1>
			<p>We will now move on to collect evaluation metrics<a id="_idIndexMarker291"/> for our model, to add to the metadata of the model.</p>
			<p>We will work on the <strong class="source-inline">evaluate_model.py</strong> file. You can follow along by working in an empty file or by going to https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/blob/master/Chapter08/psystock-training/evaluate_model.py. Proceed as follows:</p>
			<ol>
				<li value="1">Import the relevant<a id="_idIndexMarker292"/> packages—<strong class="source-inline">pandas</strong> and <strong class="source-inline">mlflow—f</strong>or reading and running the steps, respectively. We will rely on importing a selection of model-evaluation metrics available in <strong class="source-inline">sklearn</strong> for classification algorithms, as follows:<p class="source-code">import pandas as pd</p><p class="source-code">import mlflow</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.metrics import  \</p><p class="source-code">    classification_report, \</p><p class="source-code">    confusion_matrix, \</p><p class="source-code">    accuracy_score, \</p><p class="source-code">    auc, \</p><p class="source-code">    average_precision_score, \</p><p class="source-code">    balanced_accuracy_score, \</p><p class="source-code">    f1_score, \</p><p class="source-code">    fbeta_score, \</p><p class="source-code">    hamming_loss, \</p><p class="source-code">    jaccard_score, \</p><p class="source-code">    log_loss, \</p><p class="source-code">    matthews_corrcoef, \</p><p class="source-code">    precision_score, \</p><p class="source-code">    recall_score, \</p><p class="source-code">    zero_one_loss</p><p>At this stage, we have imported all the functions we need for the metrics we need to extract in the next section.</p></li>
				<li>Next, you should add a <strong class="source-inline">classification_metrics</strong> function to generate metrics based on a <strong class="source-inline">df</strong> parameter. The assumption is that the DataFrame has two columns: <strong class="source-inline">y_pred</strong>, which<a id="_idIndexMarker293"/> is the target<a id="_idIndexMarker294"/> predicted by the training model, and <strong class="source-inline">y_test</strong>, which is the target present on the training data file. Here is the code you will need:<p class="source-code">def classification_metrics(df:None):</p><p class="source-code">    metrics={}</p><p class="source-code">    metrics["accuracy_score"]=accuracy_score(df["y_pred"], df["y_test"]  )</p><p class="source-code">    metrics["average_precision_score"]=average_precision_score( df["y_pred"], df["y_test"]  )</p><p class="source-code">    metrics["f1_score"]=f1_score( df["y_pred"], df["y_test"]  )</p><p class="source-code">    metrics["jaccard_score"]=jaccard_score( df["y_pred"], df["y_test"]  )</p><p class="source-code">    metrics["log_loss"]=log_loss( df["y_pred"], df["y_test"]  )</p><p class="source-code">    metrics["matthews_corrcoef"]=matthews_corrcoef( df["y_pred"], df["y_test"]  )</p><p class="source-code">    metrics["precision_score"]=precision_score( df["y_pred"], df["y_test"]  )</p><p class="source-code">    metrics["recall_score"]=recall_score( df["y_pred"], df["y_test"] )</p><p class="source-code">    metrics["zero_one_loss"]=zero_one_loss( df["y_pred"], df["y_test"]  )</p><p class="source-code">    return metrics</p><p>The preceding function produces a <strong class="source-inline">metrics</strong> dictionary based on the predicted values and the test predictions.</p></li>
				<li>After creating this function that generates the metrics, we need to use <strong class="source-inline">start_run</strong>, whereby we basically read the prediction test file and run the metrics. We post<a id="_idIndexMarker295"/> all the metrics in <strong class="bold">MLflow</strong> by<a id="_idIndexMarker296"/> using the <strong class="source-inline">mlflow.log_metrics</strong> method to log a dictionary of multiple metrics at the same time. The code is illustrated in the following snippet:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    with mlflow.start_run(run_name="evaluate_model") as run:</p><p class="source-code">        mlflow.set_tag("mlflow.runName", "evaluate_model")</p><p class="source-code">        df=pd.read_csv("data/predictions/test_predictions.csv")</p><p class="source-code">        metrics = classification_metrics(df)</p><p class="source-code">        mlflow.log_metrics(metrics)    </p></li>
				<li>We can look again<a id="_idIndexMarker297"/> at the MLflow<a id="_idIndexMarker298"/> UI, where we can see the different metrics just persisted. You can view the output here:</li>
			</ol>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/image0035.jpg" alt="Figure 8.3 – Training model metrics persisted&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.3 – Training model metrics persisted</p>
			<p>At this stage, we have a model evaluation<a id="_idIndexMarker299"/> for our training job, providing metrics and information to model implementers/deployers. We will now<a id="_idIndexMarker300"/> move on to the last step of the training process, which is to register the model in the MLflow Model Registry so that it can be deployed in production.</p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor136"/>Deploying the model in the Model Registry</h1>
			<p>Next, you should<a id="_idIndexMarker301"/> add the <strong class="source-inline">register_model.py</strong> function<a id="_idIndexMarker302"/> to register the model in the Model Registry.</p>
			<p>This is as simple as executing the <strong class="source-inline">mlflow.register_model</strong> method with the <strong class="bold">Uniform Resource Identifier</strong> (<strong class="bold">URI</strong>) of the model and the name of the model. Basically, a model<a id="_idIndexMarker303"/> will be created if it doesn't already exist. If it's already in the registry, a new version will be added, allowing the deployment tools to look at the models and trace the training jobs<a id="_idIndexMarker304"/> and metrics. It also allows a decision to be made as to whether to promote the model<a id="_idIndexMarker305"/> to production or not. The code you'll need is illustrate<a id="_idTextAnchor137"/>d in the following snippet: </p>
			<p class="source-code">import mlflow</p>
			<p class="source-code">if __name__ == "__main__":</p>
			<p class="source-code">    </p>
			<p class="source-code">    with mlflow.start_run(run_name="register_model") as run:</p>
			<p class="source-code">        mlflow.set_tag("mlflow.runName", "register_model")</p>
			<p class="source-code">        model_uri = "runs:/{}/sklearn-model".format(run.info.run_id)</p>
			<p class="source-code">        result = mlflow.register_model(model_uri, "training-model-psystock")</p>
			<p class="source-code">        </p>
			<p>In the following screenshot, the registered model is presented, and we can change state and move into staging or production, depending on our workflow:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/image0045.jpg" alt="Figure 8.4 – Registered model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.4 – Registered model</p>
			<p>After having registered<a id="_idIndexMarker306"/> our model, we will now move on to prepare a Docker image of our training job that<a id="_idIndexMarker307"/> can be used in a public cloud environment or in a Kubernetes cluster.</p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor138"/>Creating a Docker image for your training job</h1>
			<p>A Docker image is, in many<a id="_idIndexMarker308"/> contexts, the most critical deliverable of a model developer to a more specialized systems infrastructure team in production for a training job. The project is contained in the following folder of the repository: https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter08/psystock-training-docker. In the following<a id="_idIndexMarker309"/> steps, we will produce a ready-to-deploy Docker image of the code produced:  </p>
			<ol>
				<li value="1">You need to set up a Docker file in the root folder of the project, as shown in the following code snippet:<p class="source-code">FROM continuumio/miniconda3:4.9.2</p><p class="source-code">RUN apt-get update &amp;&amp; apt-get install build-essential -y</p><p class="source-code">RUN pip install \</p><p class="source-code">    mlflow==1.18.0 \</p><p class="source-code">    pymysql==1.0.2 \</p><p class="source-code">    boto3</p><p class="source-code">COPY ./training_project /src</p><p class="source-code">WORKDIR /src</p></li>
				<li>We will start by building and training the image by running the following command:<p class="source-code">docker build -t psystock_docker_training_image .</p></li>
				<li>You can run your image, specifying<a id="_idIndexMarker310"/> your tracking server <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>). If you are using<a id="_idIndexMarker311"/> a local address for your MLflow Tracking Server to test the newly created image, you can use the <strong class="source-inline">$TRACKING_SERVER_URI</strong> value to reach <a href="http://host.docker.internal:5000">http://host.docker.internal:5000</a>, as illustrated<a id="_idIndexMarker312"/> in the following code snippet: <p class="source-code">docker run -e MLflow_TRACKING_SERVER=$TRACKING_SERVER_URI psystock_docker_training_image</p></li>
			</ol>
			<p>At this stage, we have concluded all the steps of our complete training workflow. In the next chapter, we will proceed to deploy the different components of the platform in production environments, leveraging all the MLflow projects created so far.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor139"/>Summary</h1>
			<p>In this chapter, we introduced the concepts and different features in terms of using MLflow to create production training processes.</p>
			<p>We started by setting up the basic blocks of the MLflow training project and followed along throughout the chapter to, in sequence, train a model, evaluate a trained model, and register a trained model. We also delved into the creation of a ready-to-use image for your training job.</p>
			<p>This was an important component of the architecture, and it will allow us to build an end-to-end production system for our ML system in production. In the next chapter, we will deploy different components and illustrate the deployment process of models.</p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor140"/>Further reading</h1>
			<p>In order to further your knowledge, you can consult the official documentation at the following link:</p>
			<p><a href="https://www.mlflow.org/docs/latest/projects.html">https://www.mlflow.org/docs/latest/projects.html</a></p>
		</div>
	</body></html>