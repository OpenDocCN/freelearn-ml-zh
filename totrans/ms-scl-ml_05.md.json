["```py\nakozlov@Alexanders-MacBook-Pro$ scala\n\nWelcome to Scala version 2.11.6 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import scala.util.Random\nimport scala.util.Random\n\nscala> val x = -5 to 5\nx: scala.collection.immutable.Range.Inclusive = Range(-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5)\n\nscala> val y = x.map(_ * 2 + 4 + Random.nextGaussian)\ny: scala.collection.immutable.IndexedSeq[Double] = Vector(-4.317116812989753, -4.4056031270948015, -2.0376543660274713, 0.0184679796245639, 1.8356532746253016, 3.2322795591658644, 6.821999810895798, 7.7977904139852035, 10.288549406814154, 12.424126535332453, 13.611442206874917)\n\nscala> val a = (x, y).zipped.map(_ * _).sum / x.map(x => x * x).sum\na: Double = 1.9498665133868092\n\nscala> val b = y.sum / y.size\nb: Double = 4.115448625564203\n\n```", "```py\n[akozlov@Alexanders-MacBook-Pro]$ scala\nWelcome to Scala version 2.11.7 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40).\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> import scala.util.Random.nextGaussian\nimport scala.util.Random.nextGaussian\n\nscala> val x0 = Vector.fill(201)(100 * nextGaussian)\nx0: scala.collection.immutable.IndexedSeq[Double] = Vector(168.28831870102465, -40.56031270948016, -3.7654366027471324, 1.84679796245639, -16.43467253746984, -76.77204408341358, 82.19998108957988, -20.22095860147962, 28.854940681415442, 42.41265353324536, -38.85577931250823, -17.320873680820082, 64.19368427702135, -8.173507833084892, -198.6064655461397, 40.73700995880357, 32.36849515282444, 0.07758364225363915, -101.74032407199553, 34.789280276495646, 46.29624756866302, 35.54024768650289, 24.7867839701828, -11.931948933554782, 72.12437623460166, 30.51440227306552, -80.20756177356768, 134.2380548346385, 96.14401034937691, -205.48142161773896, -73.48186022765427, 2.7861465340245215, 39.49041527572774, 12.262899592863906, -118.30408039749234, -62.727048950163855, -40.58557796128219, -23.42...\nscala> val y0 = Vector.fill(201)(30 * nextGaussian)\ny0: scala.collection.immutable.IndexedSeq[Double] = Vector(-51.675658534203876, 20.230770706186128, 32.47396891906855, -29.35028743620815, 26.7392929946199, 49.85681312583139, 24.226102932450917, 31.19021547086266, 26.169544117916704, -4.51435617676279, 5.6334117227063985, -59.641661744341775, -48.83082934374863, 29.655750956280304, 26.000847703123497, -17.43319605936741, 0.8354318740518344, 11.44787080976254, -26.26312164695179, 88.63863939038357, 45.795968719043785, 88.12442528090506, -29.829048945601635, -1.0417034396751037, -27.119245702417494, -14.055969115249258, 6.120344305721601, 6.102779172838027, -6.342516875566529, 0.06774080659895702, 46.364626315486014, -38.473161588561, -43.25262339890197, 19.77322736359687, -33.78364440355726, -29.085765762613683, 22.87698648100551, 30.53...\nscala> val x1 = (x0, y0).zipped.map((a,b) => 0.5 * (a + b) )\nx1: scala.collection.immutable.IndexedSeq[Double] = Vector(58.30633008341039, -10.164771001647015, 14.354266158160707, -13.75174473687588, 5.152310228575029, -13.457615478791094, 53.213042011015396, 5.484628434691521, 27.51224239966607, 18.949148678241286, -16.611183794900917, -38.48126771258093, 7.681427466636357, 10.741121561597705, -86.3028089215081, 11.651906949718079, 16.601963513438136, 5.7627272260080895, -64.00172285947366, 61.71395983343961, 46.0461081438534, 61.83233648370397, -2.5211324877094174, -6.486826186614943, 22.50256526609208, 8.229216578908131, -37.04360873392304, 70.17041700373827, 44.90074673690519, -102.70684040557, -13.558616956084126, -17.843507527268237, -1.8811040615871129, 16.01806347823039, -76.0438624005248, -45.90640735638877, -8.85429574013834, 3.55536787...\nscala> val y1 = (x0, y0).zipped.map((a,b) => 0.5 * (a - b) )\ny1: scala.collection.immutable.IndexedSeq[Double] = Vector(109.98198861761426, -30.395541707833143, -18.11970276090784, 15.598542699332269, -21.58698276604487, -63.31442860462248, 28.986939078564482, -25.70558703617114, 1.3426982817493691, 23.463504855004075, -22.244595517607316, 21.160394031760845, 56.51225681038499, -18.9146293946826, -112.3036566246316, 29.08510300908549, 15.7665316393863, -5.68514358375445, -37.73860121252187, -26.924679556943964, 0.2501394248096176, -26.292088797201085, 27.30791645789222, -5.445122746939839, 49.62181096850958, 22.28518569415739, -43.16395303964464, 64.06763783090022, 51.24326361247172, -102.77458121216895, -59.92324327157014, 20.62965406129276, 41.37151933731485, -3.755163885366482, -42.26021799696754, -16.820641593775086, -31.73128222114385, -26.9...\nscala> val a = (x1, y1).zipped.map(_ * _).sum / x1.map(x => x * x).sum\na: Double = 0.8119662470457414\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.mllib.linalg.Vector\n\nscala> import org.apache.spark.util._\nimport org.apache.spark.util._\n\nscala> import org.apache.spark.mllib.util._\nimport org.apache.spark.mllib.util._\n\nscala> val data = MLUtils.loadLibSVMFile(sc, \"data/iris/iris-libsvm.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[291] at map at MLUtils.scala:112\n\nscala> var w = Vector.random(4)\nw: org.apache.spark.util.Vector = (0.9515155226069267, 0.4901713461728122, 0.4308861351586426, 0.8030814804136821)\n\nscala> for (i <- 1.to(10)) println { val gradient = data.map(p => ( - p.label / (1+scala.math.exp(p.label*(Vector(p.features.toDense.values) dot w))) * Vector(p.features.toDense.values) )).reduce(_+_); w -= 0.1 * gradient; w }\n(-24.056553839570114, -16.585585503253142, -6.881629923278653, -0.4154730884796032)\n(38.56344616042987, 12.134414496746864, 42.178370076721365, 16.344526911520397)\n(13.533446160429868, -4.95558550325314, 34.858370076721364, 15.124526911520398)\n(-11.496553839570133, -22.045585503253143, 27.538370076721364, 13.9045269115204)\n(-4.002010810020908, -18.501520148476196, 32.506256310962314, 15.455945245916512)\n(-4.002011353029471, -18.501520429824225, 32.50625615219947, 15.455945209971787)\n(-4.002011896036225, -18.501520711171313, 32.50625599343715, 15.455945174027184)\n(-4.002012439041171, -18.501520992517463, 32.506255834675365, 15.455945138082699)\n(-4.002012982044308, -18.50152127386267, 32.50625567591411, 15.455945102138333)\n(-4.002013525045636, -18.501521555206942, 32.506255517153384, 15.455945066194088)\n\nscala> w *= 0.24 / 4\nw: org.apache.spark.util.Vector = (-0.24012081150273815, -1.1100912933124165, 1.950375331029203, 0.9273567039716453)\n\n```", "```py\nakozlov@Alexanders-MacBook-Pro$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.DecisionTree\n\nscala> import org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\n\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\n\nscala> // Load and parse the data file.\n\nscala> val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:112\n\nscala> // Split the data into training and test sets (30% held out for testing)\n\nscala> val Array(trainingData, testData) = data.randomSplit(Array(0.7, 0.3))\ntrainingData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:26\ntestData: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:26\n\nscala> val categoricalFeaturesInfo = Map[Int, Int]()\ncategoricalFeaturesInfo: scala.collection.immutable.Map[Int,Int] = Map()\n\nscala> val impurity = \"variance\"\nimpurity: String = variance\n\nscala> val maxDepth = 5\nmaxDepth: Int = 5\n\nscala> val maxBins = 32\nmaxBins: Int = 32\n\nscala> val model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity, maxDepth, maxBins)\nmodel: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel regressor of depth 2 with 5 nodes\n\nscala> val labelsAndPredictions = testData.map { point =>\n |   val prediction = model.predict(point.features)\n |   (point.label, prediction)\n | }\nlabelsAndPredictions: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[20] at map at <console>:36\n\nscala> val testMSE = labelsAndPredictions.map{ case(v, p) => math.pow((v - p), 2)}.mean()\ntestMSE: Double = 0.07407407407407407\n\nscala> println(s\"Test Mean Squared Error = $testMSE\")\nTest Mean Squared Error = 0.07407407407407407\n\nscala> println(\"Learned regression tree model:\\n\" + model.toDebugString)\nLearned regression tree model:\nDecisionTreeModel regressor of depth 2 with 5 nodes\n If (feature 378 <= 71.0)\n If (feature 100 <= 165.0)\n Predict: 0.0\n Else (feature 100 > 165.0)\n Predict: 1.0\n Else (feature 378 > 71.0)\n Predict: 1.0\n\n```", "```py\n$ bin/spark-shell \nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _\\ \\/ _ \\/ _ `/ __/  '_/\n /___/ .__/\\_,_/_/ /_/\\_\\   version 1.6.1-SNAPSHOT\n /_/\n\nUsing Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)\nType in expressions to have them evaluated.\nType :help for more information.\nSpark context available as sc.\nSQL context available as sqlContext.\n\nscala> import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n\nscala> import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\n\nscala> \n\nscala> val data = MLUtils.loadLibSVMFile(sc, \"iris-libsvm-3.txt\").toDF()\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector] \n\nscala> \n\nscala> val Array(train, test) = data.randomSplit(Array(0.6, 0.4), seed = 13L)\ntrain: org.apache.spark.sql.DataFrame = [label: double, features: vector]\ntest: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\nscala> // specify layers for the neural network: \n\nscala> // input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n\nscala> val layers = Array(4, 5, 4, 3)\nlayers: Array[Int] = Array(4, 5, 4, 3)\n\nscala> // create the trainer and set its parameters\n\nscala> val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(13L).setMaxIter(100)\ntrainer: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_b5f2c25196f9\n\nscala> // train the model\n\nscala> val model = trainer.fit(train)\nmodel: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel = mlpc_b5f2c25196f9\n\nscala> // compute precision on the test set\n\nscala> val result = model.transform(test)\nresult: org.apache.spark.sql.DataFrame = [label: double, features: vector, prediction: double]\n\nscala> val predictionAndLabels = result.select(\"prediction\", \"label\")\npredictionAndLabels: org.apache.spark.sql.DataFrame = [prediction: double, label: double]\n\nscala> val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"precision\")\nevaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_55757d35e3b0\n\nscala> println(\"Precision = \" + evaluator.evaluate(predictionAndLabels))\nPrecision = 0.9375\n\n```"]