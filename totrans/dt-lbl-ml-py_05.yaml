- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labeling Image Data Using Rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore data labeling techniques tailored specifically
    for image classification, using Python. Our primary objective is to clarify the
    path you need to take to generate precise labels for these images in the dataset,
    relying on meticulously crafted rules founded upon various image properties. You
    will be empowered with the ability to dissect and decode images through manual
    inspection, harnessing the formidable Python ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How to create labeling rules based on manual inspection of image visualizations
    in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to create labeling rules based on the size and aspect ratio of images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to apply transfer learning to label image data, using pre-trained models
    such as **YOLO V3**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overarching goal is to empower you with the ability to generate precise
    and reliable labels for your data. We aim to equip you with a versatile set of
    labeling strategies that can be applied across various machine learning projects.
  prefs: []
  type: TYPE_NORMAL
- en: We will also introduce transformations such as **shearing** and **flipping**
    for image labeling. We will provide you with the knowledge and techniques required
    to harness these transformations effectively, giving your labeling process a dynamic
    edge. we’ll delve into the intricacies of **size**, **aspect ratio**, **bounding
    box**, **polygon annotation**,and **polyline annotation**. You’ll learn how to
    derive labeling rules based on these quantitative image characteristics, providing
    a systematic and reliable approach to labeling data.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Complete code notebooks for the examples used in this chapter are available
    on GitHub at [https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: The sample image dataset used in this chapter is available on GitHub at [https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/images](https://github.com/PacktPublishing/Data-Labeling-in-Machine-Learning-with-Python/tree/main/images).
  prefs: []
  type: TYPE_NORMAL
- en: Labeling rules based on image visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image classification is the process of categorizing an image into one or more
    classes based on its content. It is a challenging task due to the high variability
    and complexity of images. In recent years, machine learning techniques have been
    applied to image classification with great success. However, machine learning
    models require a large amount of labeled data to train effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Image labeling using rules with Snorkel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snorkel is an open source data platform that provides a way to generate large
    amounts of labeled data using weak supervision techniques. Weak supervision allows
    you to label data with noisy or incomplete sources of supervision, such as heuristics,
    rules, or patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Snorkel primarily operates within the paradigm of weak supervision rather than
    traditional semi-supervised learning. Snorkel is a framework designed for weak
    supervision, where the labeling process may involve noisy, limited, or imprecise
    rules rather than a large amount of labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: In Snorkel, users create **labeling functions** (**LFs**) that express heuristic
    or rule-based labeling strategies. These LFs might not be perfect, and there can
    be conflicts or noise in the generated labels. Snorkel’s labeling model then learns
    to denoise and combine these weak labels to create more accurate and reliable
    labeling for the training data.
  prefs: []
  type: TYPE_NORMAL
- en: While semi-supervised learning typically involves having a small amount of labeled
    data and a large amount of unlabeled data, Snorkel focuses on the weak supervision
    scenario, allowing users to leverage various sources of noisy or incomplete supervision
    to train machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, Snorkel is more aligned with the principles of weak supervision,
    where the emphasis is on handling noisy or imprecise labels generated by heuristic
    rules, rather than being strictly categorized as a semi-supervised learning framework.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explore the concept of weak supervision and how to
    generate labels using Snorkel.
  prefs: []
  type: TYPE_NORMAL
- en: Weak supervision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Weak supervision is a technique for generating large amounts of labeled data
    using noisy or incomplete sources of supervision. The idea is to use a set of
    LFs that generate noisy labels for each data point. These labels are then combined
    to generate a final label for each data point. The key advantage of weak supervision
    is that it allows you to generate labeled data quickly and at a low cost.
  prefs: []
  type: TYPE_NORMAL
- en: Snorkel is a framework that provides a way to generate labels using weak supervision.
    It provides a set of tools to create LFs, combine them, and train a model to learn
    from the generated labels. Snorkel uses a technique called data programming to
    combine the LFs and generate a final label for each data point.
  prefs: []
  type: TYPE_NORMAL
- en: An LF is a function that generates a noisy label for a data point. The label
    can be any value, including continuous or discrete values. In the context of image
    classification, an LF is a function that outputs a label of 1 if the image contains
    the object of interest, and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: LFs are created using heuristics, rules, or patterns. The key idea is to define
    a set of rules that capture the relevant information for each data point.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us see how to define the rules and an LF based on the manual visualization
    of an image’s object color for plant disease labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Rules based on the manual visualization of an image’s object color
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, let us see how we can use LFs that look for specific visual
    features that are characteristic of images of a plant’s leaves, which we are interested
    in classifying as “healthy” or “deceased”. For instance, we could use an LF that
    checks whether the image has a certain color distribution, or whether it contains
    specific shapes that are common in those images.
  prefs: []
  type: TYPE_NORMAL
- en: Snorkel’s LFs can be used to label images based on various properties, such
    as the presence of certain objects, colors, textures, and shapes. Here’s an example
    of Python code that uses Snorkel LFs to detect images based on their color distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Creating labeling rules based on manual inspection of image visualizations is
    a manual process that often involves the expertise of a human annotator. This
    process is commonly used in scenarios where there is no existing labeled dataset,
    and you need to create labels for machine learning or analysis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a general outline of how you can create labeling rules based on the
    manual inspection of image visualizations in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Collect a representative sample**: Begin by selecting a representative sample
    of images from your dataset. This sample should cover the range of variations
    and categories you want to classify.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define the labeling criteria**: Clearly define the criteria or rules to label
    images based on their visual properties. For example, if you’re classifying images
    to identify plant diseases from images of leaves, agricultural experts visually
    inspect leaf images for discoloration, spots, or unusual patterns. Rules can be
    defined based on the appearance and location of symptoms. We will use this example
    for our demonstration shortly.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Create a labeling interface**: You can use existing tools or libraries to
    create a labeling interface where human annotators can view images and apply labels
    based on the defined criteria. Libraries such as Labelbox and Supervisely or custom
    interfaces, using Python web frameworks such as Flask or Django, can be used for
    this purpose.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Annotate the images**: Have human annotators manually inspect each image
    in your sample and apply labels according to the defined criteria. This step involves
    the human annotators visually inspecting the images and making classification
    decisions, based on their expertise and the provided guidelines.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Collect annotations**: Collect the annotations generated by the human annotators.
    Each image should have a corresponding label or class assigned based on the visual
    inspection.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Analyze and formalize rules**: After collecting a sufficient number of annotations,
    analyze the patterns and decisions made by the annotators. Try to formalize the
    decision criteria based on the annotations. For example, you might observe that
    images with certain visual features were consistently labeled as a specific class.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Convert rules to code**: Translate the formalized decision criteria into
    code that can automatically classify images based on those rules. This code can
    be written in Python and integrated into your machine learning pipeline or analysis
    workflow.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test and validate rules**: Apply the automated labeling rules to a larger
    portion of your dataset to ensure that they generalize well. Validate the rules
    by comparing the automated labels with ground truth labels if available, or by
    reviewing a subset of the automatically labeled images manually.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Iterate and refine**: Iteratively refine the labeling rules based on feedback,
    error analysis, and additional manual inspection if necessary. This process may
    involve improving the rules, adding more criteria, or adjusting thresholds.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating labeling rules based on manual inspection is a labor-intensive process
    but can be essential to generate labeled data when no other options are available.
    The quality of your labeled dataset and the effectiveness of your rules depend
    on the accuracy and consistency of the human annotators, as well as the clarity
    of the defined criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Manual inspection of images for classification, along with the definition of
    rules or patterns, is common in various real-world applications. Here are some
    practical examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Medical** **image classification**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: Classifying X-ray or MRI images as “normal” or “abnormal.”'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rules/patterns**: Radiologists visually inspect images for abnormalities,
    such as tumors, fractures, or anomalies in anatomy. Rules can be based on the
    presence, size, or location of these features.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plant** **disease detection**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: Identifying plant diseases from images of leaves.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rules/patterns**: Agricultural experts visually inspect leaf images for discoloration,
    spots, or unusual patterns. Rules can be defined based on the appearance and location
    of symptoms.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Food** **quality inspection**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: Classifying food products as “fresh” or “spoiled” from images.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rules/patterns**: Food inspectors visually inspect images of fruits, vegetables,
    or packaged goods for signs of spoilage, mold, or other quality issues. Rules
    can be based on color, texture, or shape.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Defect detection** **in manufacturing**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: Detecting defects in manufactured products from images.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rules/patterns**: Quality control inspectors visually inspect images of products
    for defects such as cracks, scratches, or missing components. Rules can be defined
    based on the location and characteristics of defects.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Traffic** **sign recognition**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: Recognizing traffic signs from images captured by autonomous vehicles.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rules/patterns**: Engineers visually inspect images for the presence of signs
    and their shapes, colors, and symbols. Rules can be defined based on these visual
    cues.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wildlife monitoring**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: Identifying and tracking animals in camera trap images.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rules/patterns**: Wildlife experts visually inspect images for the presence
    of specific animal species, their behavior, or the time of day. Rules can be based
    on the appearance and context of animals.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Historical** **document classification**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: Classifying historical documents based on content or era.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rules/patterns**: Archivists visually inspect scanned documents for handwriting
    style, language, content, or visual elements such as illustrations. Rules can
    be defined based on these characteristics.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security** **and surveillance**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Example**: Identifying security threats or intruders in surveillance camera
    footage.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rules/patterns**: Security personnel visually inspect video feeds for unusual
    behavior, suspicious objects, or unauthorized access. Rules can be defined based
    on these observations.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In all these examples, experts or human annotators visually examine images,
    identify relevant patterns or features, and define rules or criteria for classification.
    These rules are often based on domain knowledge and experience. Once established,
    the rules can be used to create LFs and classify images automatically, assist
    in decision-making, or prioritize further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: A practical example of plant disease detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us see the example LF for plant disease detection. In this code, we have
    created a rule to classify healthy and diseased plants, based on the color distribution
    of leaves. One rule is if `black_pixel_percentage` in the plant leaves is greater
    than the threshold value, then we classify that plant as a diseased plant.
  prefs: []
  type: TYPE_NORMAL
- en: The following are the two different types of plant leaves.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Healthy and diseased plant leaves](img/B18944_05_2_Merged_(2).jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Healthy and diseased plant leaves
  prefs: []
  type: TYPE_NORMAL
- en: 'We calculate the number of black color pixels in a leaf image and then calculate
    the percent of black pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Percent of black pixels = count of black pixels in a leaf image/total number
    of pixels in a* *leaf image*'
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use the rule that if the black pixel percent in a plant leave
    image is greater than the threshold value (in this example, 10%), then we classify
    that plant as a diseased plant and label it as a “diseased plant.”
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if the black pixel percentage is less than 10%, then we classify
    that plant as a healthy plant and label it as a “healthy plant.”
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows how to calculate the black pixel percentage
    in an image using Python libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This line converts the original color image to grayscale using OpenCV’s `cvtColor`
    function. Grayscale images have only one channel (compared to the three channels
    in a color image), representing the intensity or brightness of each pixel. Converting
    to grayscale simplifies subsequent processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'In this line, a thresholding operation is applied to the grayscale image, `gray_image`.
    Thresholding is a technique that separates pixels into two categories, based on
    their intensity values – those above a certain threshold and those below it. Here’s
    what each parameter means:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gray_image`: The grayscale image to be thresholded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`150`: The threshold value. Pixels with intensities greater than or equal to
    150 will be set to the maximum value (`255`), while pixels with intensities lower
    than 150 will be set to `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`255`: The maximum value to which pixels above the threshold are set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cv2.THRESH_BINARY_INV`: The thresholding type. In this case, it’s set to “binary
    inverted,” which means that pixels above the threshold will become 0, and pixels
    below the threshold will become 255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The result of this thresholding operation is stored in `binary_image`, which
    is a binary image where regions with discoloration are highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `cv2.countNonZero(binary_image)` function counts the number of non-zero
    (white) pixels in the binary image. Since we are interested in black pixels (discoloration),
    we subtract this count from the total number of pixels in the image.
  prefs: []
  type: TYPE_NORMAL
- en: '`binary_image.size`: This is the total number of pixels in the binary image,
    which is equal to the width multiplied by the height.'
  prefs: []
  type: TYPE_NORMAL
- en: By dividing the count of non-zero (white) pixels by the total number of pixels
    and multiplying by 100, we obtain the percentage of white pixels in the image.
    This percentage represents the extent of discoloration in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the percentage of black pixels (discoloration), you can use the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Overall, this code snippet is a simple method to quantitatively measure the
    extent of discoloration in a grayscale image, by converting it into a binary image
    and calculating the percentage of black pixels. It can be useful for tasks such
    as detecting defects or anomalies in images. Adjusting the threshold value (in
    this case, 150) can change the sensitivity of the detection.
  prefs: []
  type: TYPE_NORMAL
- en: Let us create the labeling function to classify the plant as `Healthy` or `Diseased`,
    based on the threshold value of `black_pixel_percentage` in the leaf images, as
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This LF returns labels `0` (a diseased plant) or `1` (a healthy plant) based
    on the *black* color pixels percentage in the image. The complete working code
    for this plant disease labeling is available in the GitHub repo.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let us see how we can apply labels using image properties
    such as size and aspect ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling images using rules based on properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us see an example of Python code that demonstrates how to classify images
    using rules, based on image properties such as size and aspect ratio.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we will define rules such as if the black color distribution is greater
    than 50% in leaves, then that is a diseased plant. Similarly, in case of detecting
    a bicycle with a person, if the aspect ratio of an image is greater than some
    threshold value, then that image has a bicycle with a person.
  prefs: []
  type: TYPE_NORMAL
- en: 'In computer vision and image classification, the **aspect ratio** refers to
    the ratio of the width to the height of an image or object. It is a measure of
    how elongated or stretched an object or image appears along its horizontal and
    vertical dimensions. Aspect ratio is often used as a feature or criterion in image
    analysis and classification. It’s worth noting that aspect ratio alone is often
    not sufficient for classification, and it is typically used in conjunction with
    other features, such as **contour height**, **texture**, and **edge**, to achieve
    accurate classification results. Image properties such as bounding boxes, polygon
    annotations, and polyline annotations are commonly used in computer vision tasks
    for object detection and image segmentation. These properties help you to label
    and annotate objects within an image. Here’s an explanation of each feature along
    with Python code examples to demonstrate how to work with them:'
  prefs: []
  type: TYPE_NORMAL
- en: Bounding boxes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A bounding box is a rectangular region that encloses an object of interest
    within an image. It is defined by four values – (`x_min`, `y_min`) for the top-left
    corner and (`x_max`, `y_max`) for the bottom-right corner. Bounding boxes are
    often used for object detection and localization. Here is an example of Python
    code to create and manipulate bounding boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Polygon annotation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A polygon annotation is a set of connected vertices that outline the shape
    of an object in an image. It is defined by a list of (x, y) coordinates representing
    the vertices. Polygon annotations are used for detailed object segmentation. Here
    is some example Python code to work with polygon annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Polyline annotations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A polyline annotation is a series of connected line segments defined by a list
    of (x, y) coordinates for each vertex. Polylines are often used to represent shapes
    with multiple line segments, such as roads or paths. Here is some Python code
    to work with polyline annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: These code examples demonstrate how to work with bounding boxes, polygon annotations,
    and polyline annotations in Python. You can use these concepts to create rules
    to label images in computer vision applications.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us see the following example of how we can use contour height to classify
    whether an image contains a person riding a bicycle or just shows a bicycle on
    its own.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – image classification – a bicycle with and without a person
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Contour height**, in the context of image processing and computer vision,
    refers to the measurement of the vertical extent or size of an object’s outline
    or contour within an image. It is typically calculated by finding the minimum
    and maximum vertical positions (i.e., the topmost and bottommost points) of the
    object’s boundary or contour.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how contour height is generally determined:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Contour detection**: The first step is to detect the contour of an object
    within an image. Contours are essentially the boundaries that separate an object
    from its background.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A bounding rectangle**: Once the contour is detected, a bounding rectangle
    (often referred to as the “**bounding box**”) is drawn around the contour. This
    rectangle encompasses the entire object.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Measurement**: To calculate the contour height, the vertical extent of the
    bounding rectangle is measured. This is done by finding the difference between
    the y coordinates (the vertical positions) of the top and bottom sides of the
    bounding rectangle.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In summary, contour height provides information about the vertical size of an
    object within an image. It can be a useful feature for various computer vision
    tasks, such as object recognition, tracking, and dimension estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Let us see how we will use Python functions to detect the following images,
    based on contour height.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18944_05_2_Merged_(1).jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'a: A bicycle with a person b: A bicycle without a person'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – A comparison of two images with regards to the contour height
  prefs: []
  type: TYPE_NORMAL
- en: Here, the contour height of a person riding a bicycle in an image (*Figure 5**.2a*)
    is greater than the contour height of the image of a bicycle without a person
    (*Figure 5**.2b*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us use the Python library CV2 Canny edge detector to detect the maximum
    contour height for the given image as, follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes an image as input and returns the maximum contour height,
    found using the Canny edge detector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, Python functions are used to find the contour height of the images. As
    seen in the images, the results show that the contour height of the person riding
    a bicycle image is greater than the contour height of the bicycle image. So, we
    can classify these two images by using a certain threshold value for the contour
    height, and if that is greater than that threshold value, then we classify the
    images as a bicycle with a person; otherwise, if the contour height is less than
    that threshold value, we classify those images as just showing a bicycle.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding LF, (we learned about labeling functions in [*Chapter
    2*](B18944_02.xhtml#_idTextAnchor043)) we can automate such image classification
    and object detection tasks using Python, and label the images as either a man
    riding a bicycle or just a bicycle.
  prefs: []
  type: TYPE_NORMAL
- en: The complete code to find the contour height of the preceding two images is
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: By using a diverse set of LFs that capture different aspects of the image content,
    we can increase the likelihood that at least some of the functions will provide
    a useful way to distinguish between images that depict a bicycle, a bicycle with
    a person, or neither. The probabilistic label generated by the majority label
    voter model will then reflect the combined evidence provided by all of the LFs,
    and it can be used to make a more accurate classification decision.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – image classification – dog and cat images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us see another example of labeling images to classify dog or cat images,
    based on rules associated with properties.
  prefs: []
  type: TYPE_NORMAL
- en: The following are some rules to implement as LFs to detect images of dogs, based
    on pointy ears and snouts, the shape of the eyes, fur texture, and the shape of
    the body, as well as additional LFs to detect other features. The complete code
    for these functions is available on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '**Labeling function 1**: The rule is, if the image has pointy ears and a snout,
    label it as a dog:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Labeling function 2**: The rule is, if the image has oval-shaped eyes, label
    it as a cat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Labeling function 3**: The rule is, if the image has a texture with high
    variance, label it as a dog:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Labeling function 4**: The rule is, if the aspect ratio is close to 1 (indicating
    a more circular shape), label it as a cat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `dog_features` LF looks for the presence of pointy ears and snouts in the
    image by examining specific regions of the blue channel. The `cat_features` LF
    looks for the presence of oval-shaped eyes in the green channel. The `dog_fur_texture`
    LF looks for high variance in the grayscale version of the image, which is often
    associated with dog fur texture. The `cat_body_shape` LF looks for a circular
    body shape in the image, which is often associated with cats.
  prefs: []
  type: TYPE_NORMAL
- en: These LFs could be combined with Snorkel to create a model and label the images.
    In the next section, let us see how we can apply labels using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling images using transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Transfer learning is a machine learning technique where a model trained on
    one task is adapted for a second related task. Instead of starting the learning
    process from scratch, transfer learning leverages knowledge gained from solving
    one problem and applies it to a different but related problem. This approach has
    become increasingly popular in deep learning and has several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Faster training**: Transfer learning can significantly reduce the time and
    computational resources required to train a model. Instead of training a deep
    neural network from random initialization, you start with a pre-trained model,
    which already has learned features and representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Better generalization**: Models pre-trained on large datasets, such as ImageNet
    for image recognition, have learned general features that are useful for various
    related tasks. These features tend to generalize well to new tasks, leading to
    better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lower data requirements**: Transfer learning can be especially beneficial
    when you have a limited amount of data for your target task. Pre-trained models
    can provide a head start, enabling effective learning with smaller datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Domain adaptation**: Transfer learning helps adapt models from one domain
    (e.g., natural images) to another (e.g., medical images). This is valuable when
    collecting data in the target domain is challenging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let us see an example of Python code to detect digits in handwritten MNIST images,
    using Snorkel LFs.
  prefs: []
  type: TYPE_NORMAL
- en: Example – digit classification using a pre-trained classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we will first load the MNIST dataset, using Keras, and then
    define an LF that uses a digit classification model to classify the digits in
    each image. We then load the MNIST images into a Snorkel dataset and apply the
    LF to generate labels for the specified digit. Finally, we visualize the labels
    using Snorkel’s viewer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that, in this example, we assume that you have already trained a digit
    classification model and saved it as a file named `digit_classifier.h5`. You can
    replace this with any other model of your choice. Also, make sure to provide the
    correct path to the model file. Finally, the labels generated by the LF will be
    `1` if the image has the specified digit, and `-1` if it doesn’t have it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this block, TensorFlow is imported, along with specific modules needed to
    work with the MNIST dataset and pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MNIST dataset is loaded into two sets – `x_test` contains the images, and
    `y_test` contains the corresponding labels. The training set is not used in this
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'A pre-trained model is loaded using the `load_model` function. Ensure to replace
    `mnist_model.h5` with the correct path to your pre-trained model file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The pixel values of the images are normalized to be in the range [0, 1] by
    converting the data type to `float32` and dividing by 255:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The images are reshaped to match the input shape expected by the model, which
    is (`batch_size`, `height`, `width`, and `channels`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Predictions are made on the test dataset using the pre-trained model, and the
    predictions for the first image are printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Class labels for the MNIST digits (0–9) are created as strings and printed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The script iterates through the test dataset, printing the index of the maximum
    prediction value, the predicted digit, and the actual digit label for each image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – The output of digital classification](img/B18944_05_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – The output of digital classification
  prefs: []
  type: TYPE_NORMAL
- en: Let us see another example of defining rules using a pre-trained classifier
    for image labeling. In the following example, we will use a pre-trained model,
    YOLO V3, to detect a person in the image, and then we will apply an LF to label
    the large set of image data.
  prefs: []
  type: TYPE_NORMAL
- en: Example – person image detection using the YOLO V3 pre-trained classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get started with the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The predefined YOLO model and weights are open source and can be downloaded
    at [https://pjreddie.com/darknet/yolo](https://pjreddie.com/darknet/yolo):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we use OpenCV to load the YOLO V3 model, its weights, and its
    configuration files. Then, we provide an input image, run a forward pass through
    the network, and process the detection results.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to replace `"path/to/yolov3.cfg"`, `"path/to/coco.names"`, and `"path/to/image.jpg"`
    with the actual paths to your YOLOv3 configuration file, the class names file,
    and the image that you want to perform object detection on.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that YOLO V3 is a complex deep learning model designed for real-time
    object detection, and using it effectively often requires some knowledge of computer
    vision and deep learning concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Example – bicycle image detection using the YOLO V3 pre-trained classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following is the code for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In summary, the code snippet utilizes a pre-trained Faster R-CNN model to perform
    object detection on an input image. It resizes the image, converts it to a tensor,
    and then extracts and processes the detection results. To specifically detect
    bicycles, you would need to filter the results based on the class labels provided
    by the model and check for the presence of bicycles in the detected objects.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let us explore how we can apply transformations on a given image dataset
    to generate additional synthetic data. Additional synthetic data helps in training
    and achieving more accurate results, as a model will learn about different positions
    of images.
  prefs: []
  type: TYPE_NORMAL
- en: Labeling images using transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let us see the different types of transformations that can
    be applied to images to generate synthetic data when there is a limited amount
    of data. In machine learning, shearing and flipping are often used as image augmentation
    techniques to increase the diversity of training data. It helps improve a model’s
    ability to recognize objects from different angles or orientations.
  prefs: []
  type: TYPE_NORMAL
- en: Shearing can be used in computer vision tasks to correct for perspective distortion
    in images. For example, it can be applied to rectify skewed text in scanned documents.
  prefs: []
  type: TYPE_NORMAL
- en: '**Image shearing** is a transformation that distorts an image by moving its
    pixels in a specific direction. It involves shifting the pixels of an image along
    one of its axes while keeping the other axis unchanged. There are two primary
    types of shearing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Horizontal shearing**: In this case, pixels are shifted horizontally, usually
    in a diagonal manner, causing an image to slant left or right'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vertical shearing**: Here, pixels are shifted vertically, causing an image
    to slant up or down'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To perform image shearing, you typically specify the amount of shear (the extent
    of distortion) and the direction (horizontal or vertical). The amount of shear
    is usually defined as a shear angle or shear factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image shearing is typically accomplished using a shear matrix. For example,
    in 2D computer graphics, a horizontal shear matrix might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Here, `shear_x` represents the amount of horizontal shearing applied.
  prefs: []
  type: TYPE_NORMAL
- en: By applying a random shearing transformation to an image, we can generate multiple
    versions of the image with slightly different pixel values. These variations can
    provide a useful way to identify visual patterns or features that are characteristic
    of an object.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, **image flipping** is another transformation that can be useful to
    identify flowers. By flipping an image horizontally or vertically, we can generate
    new versions of an image that may contain different visual patterns or features.
    For example, we could use an LF that checks whether an image is flipped along
    a certain axis, labeling images that are flipped as positively depicting flowers.
    This LF would be able to capture the fact that many flowers have bilateral symmetry,
    meaning that they look similar when mirrored along a particular axis.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, by applying image transformations such as shearing or flipping, we
    can generate a larger number of labeled examples that capture different aspects
    of the image content. This can help to increase the accuracy of the classification
    model by providing more varied and robust training data.
  prefs: []
  type: TYPE_NORMAL
- en: We will further explore image transformation along with other data augmentation
    techniques and examples in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on an enlightening journey into the world of image
    labeling and classification. We began by mastering the art of creating labeling
    rules through manual inspection, tapping into the extensive capabilities of Python.
    This newfound skill empowers us to translate visual intuition into valuable data,
    a crucial asset in the realm of machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: As we delved deeper, we explored the intricacies of size, aspect ratio, bounding
    boxes, and polygon and polyline annotations. We learned how to craft labeling
    rules based on these quantitative image characteristics, ushering in a systematic
    and dependable approach to data labeling.
  prefs: []
  type: TYPE_NORMAL
- en: Our exploration extended to the transformative realm of image manipulation.
    We harnessed the potential of image transformations such as shearing and flipping,
    enhancing our labeling process with dynamic versatility.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we applied our knowledge to real-world scenarios, classifying plant
    disease images using rule-based LFs. We honed our skills in predicting objects
    by leveraging aspect ratio and contour height, a valuable asset in scenarios such
    as identifying a person riding a bicycle. Additionally, we delved into the powerful
    domain of pre-trained models and transfer learning for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: But our journey is far from over. In the upcoming chapter, we will dive even
    deeper into the realm of image data augmentation. We’ll explore advanced techniques
    and learn how to perform image classification using augmented data with **support
    vector machines** (**SVMs**) and **convolutional neural networks** (**CNNs**).
    Get ready for the next exciting chapter!
  prefs: []
  type: TYPE_NORMAL
