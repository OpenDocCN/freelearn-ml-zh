- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Transformers** are a new type of machine learning model that has completely
    revolutionized the way human language is processed and understood. These models
    can analyze huge amounts of data, find and understand complex patterns with hitherto
    unmatched accuracy, and produce insights that would otherwise be impossible for
    humans to obtain on tasks such as translation, text summarization, and text generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers are powerful because they can handle large amounts of data, and
    learn from previous examples to make better predictions. They have totally “transformed”
    (pun intended) **NLP** and have outperformed traditional methods in many NLP tasks,
    quickly becoming state of the art.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce transformers, discuss how they work, and
    look at some of their key components. We will then present Hugging Face and see
    how it helps in our task before introducing some useful existing transformer models.
    We’ll also show how we can use Hugging Face to implement two models using transformers.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will demonstrate how to build transformer models while taking you
    through important steps such as linking Google Colab to Google Drive so that files
    can be persisted, preparing the data, using auto classes, and ultimately building
    models that can be used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How data flows through the transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin by having a closer look at transformers, who invented them, and
    how they work.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will briefly introduce transformers. In language and NLP
    tasks, context plays a crucial role – that is, to know what a word means, knowledge
    about the situation (that is, the context) must also be taken into account. Before
    transformers came along, sequence-to-sequence models were used for many NLP tasks.
    These are models that generate an output sequence by predicting a single word
    at a time and encode the source text to gain knowledge about the context. However,
    the problem with languages is that they are complex, fluid, and difficult to turn
    into a rigid rule-based structure. The context itself is also hard to track as
    it is often found far away (that is, many words, sentences, or even paragraphs)
    from where it is required. To address this problem, sequence-to-sequence models
    work by using neural networks, which have some limited form of memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Sequence-to-sequence model versus transformer](img/B18714_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Sequence-to-sequence model versus transformer
  prefs: []
  type: TYPE_NORMAL
- en: 'The ubiquitous paper on transformers, *Attention Is All You Need*, was published
    by Vaswani et al., in 2017\. They presented a new type of neural network architecture,
    known as **transformers**, which could be used for NLP tasks. Transformers have
    several components, as can be seen in *Figure 9**.2*, with an “encoder” (on the
    left), a “decoder” (on the right), and a block of attention and feed-forward components
    that repeat *N* times:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – From the "Attention Is All You Need" paper by Vaswani et al.](img/B18714_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – From the "Attention Is All You Need" paper by Vaswani et al.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer consists of encoder and decoder layers; each usually has more
    than one identical instance (for example, six as in the original research paper),
    and each has its own set of weights. On the left-hand side, the encoder’s job
    is to convert a sequence of inputs into a set of continuous representations. On
    the right-hand side, the decoder uses the output from the encoder as well as the
    output from the previous time step to produce a sequence of outputs. The first
    encoder and decoder in each stack of the architecture has an embedding layer and
    positional encoding as inputs. Each encoder contains a self-attention layer that
    calculates the relationships between different words and a feed-forward layer.
    Each decoder also contains a feed-forward layer, but it has two self-attention
    layers. The output from the last encoder is used as the input to the first decoder.
    These components combine to make the transformer architecture faster and more
    efficient and allow it to handle much longer sequences, making the separation
    between words irrelevant. Consequently, the architecture can outperform other,
    more traditional, methods.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer architecture described by Vaswani et al. was created with the
    goal of translation. The encoder is fed inputs (that is, sentences) in one language
    (for example, English) during training, while the decoder is fed the same inputs
    (that is, sentences) in the intended target language (for example, French). The
    attention layers in the encoder make use of each word in an input sentence, but
    the encoder operates sequentially and can only focus on the words in the translated
    text (that is, only the words before the word that is currently being generated).
    For example, if the first *N* words of the translated target have been predicted,
    these are input to the decoder, which uses all the inputs of the encoder to predict
    the word at *N+1*.
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is supplied with the entire target sentence but is constrained from
    using forthcoming words. Consequently, when predicting a word, the decoder cannot
    refer to any words beyond it in the target sentence. For instance, while predicting
    the *Nth* word, only the words in positions *1* to *N-1* can be considered by
    the attention layer. This constraint is crucial to guarantee that the task remains
    suitably demanding for the model to acquire knowledge competently.
  prefs: []
  type: TYPE_NORMAL
- en: How data flows through the transformer model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will take a closer look at how data flows through the transformer
    model. Understanding how data flows through a transformer, and the steps that
    transform raw input into meaningful output, is crucial to understanding its power
    and potential. The transformer enables efficient and effective modeling of long-range
    dependencies in data, making it highly capable of capturing context and semantics.
    By exploring these inner mechanisms of data flow within the transformer, we will
    gain a deeper understanding of its ability to process and understand language.
    We will look at input embeddings first.
  prefs: []
  type: TYPE_NORMAL
- en: Input embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Starting on the left-hand side, the inputs into the encoder are word tokens
    from the source text. This textual data has to be converted into a numeric representation
    (of size 512 according to the authors) using methods such as GloVe or Word2Vec,
    among others.
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A positional element is then added to these embeddings. This is important as
    it allows the transformer to discover information about the distances between
    words and the order of the words. This information is then passed to the self-attention
    layer of the first encoder block.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Positional encodings do not alter the vector dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Encoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Each encoder has several sub-layers within it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Multi-headed attention**: This allows the transformer to attend to different
    parts of the input sequence at the same time, thus improving its input processing
    abilities, and allowing it to obtain more context and make much better-informed
    decisions. It is the most important part of the architecture and is also the most
    computationally expensive. When working on words in the input, self-attention
    relates every word in the input to every other word. It is interesting to consider
    how the transformer decides which set of weights will yield the best results.
    The aim is that the attention value should be large for words that are somehow
    related in a sentence, and vice versa. For example, let’s look at the sentence
    *The weather was* *very sunny.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The words *weather* and *sunny* are related and hence should generate a high
    attention value; conversely, the attention value for *The* and *was* should be
    small. As described earlier, transformers are trained on embeddings. Consequently,
    the transformer will learn from them and be able to produce the vectors required
    so that words produce attention values that correspond to the relatedness of the
    words. Furthermore, instead of just considering individual meanings, the self-attention
    mechanism weighs the input words differently according to their importance and
    their relationships with other words. This allows it to be able to handle the
    aforementioned long-distance context problems and hence achieve much better performance
    on NLP tasks. Briefly, the attention value is computed using three matrices, with
    each row in each matrix representing an input word. It is important to note that
    the values in these rows are learned by the model so that the desired outputs
    are generated. Let’s take a look at each of these important matrices in turn:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query**: Each row corresponds to the embeddings of an input word. In other
    words, the query word is the specific word for which an attention value is being
    calculated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key**: Each word in the input text that the model is comparing the query
    to – that is, the word that is being paid attention to – to calculate its importance
    to the query word.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value**: The information that the model is trying to generate based on the
    comparison between the query and key matrices. The key and value matrices can
    be the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given these matrices, the attention value is obtained by calculating the dot
    product of the query and key matrices. These are then used to weigh the value
    matrix, hence effectively allowing the model to “learn” which words in the input
    text should be focused upon.
  prefs: []
  type: TYPE_NORMAL
- en: '**Add and norm**: These layers comprise a residual connection layer followed
    by a normalization layer. For our purposes, it is enough to know that they help
    address the vanishing gradient problem and improve the model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feed-forward neural network**: This is a neural network that processes the
    attention vector inputs and transforms them into a form, of the same dimensions
    as the input, that can be input into the next layer. These attention vectors are
    independent of each other; consequently, parallelization can be used in this stage,
    rather than processing them sequentially as in the sequence-to-sequence architecture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now move on to decoders.
  prefs: []
  type: TYPE_NORMAL
- en: Decoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The output from the encoder is used as the input to the second layer of each
    decoder in the decoder stack.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the encoder, masked multi-headed attention takes an output embedding
    and a positional embedding. The target for the transformer is to learn how to
    generate the output, given both the input and the required output.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: During training, the required output (for example, a translation) is provided
    to the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the words are masked so that the model can learn how to predict them.
    These are changed during each iteration. The decoders process this along with
    the encoded representation from the encoders to produce a target sequence.
  prefs: []
  type: TYPE_NORMAL
- en: However, during prediction, an empty sequence (with a special **start of sentence**
    (**<SOS>**) token) is used. This is converted into an embedding; positional encoding
    is added and is used as input to the decoder. The decoder and other layers work
    as before but the last word from the output sequence is used to fill in the first
    blank of the input sequence, hence the input is now the <SOS> and the first predicted
    word. This is, again, fed into the decoder and the process is repeated until the
    end of the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Linear layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The output from the decoder is used as input to this linear layer, a simple
    fully connected neural network that generates the vectors for the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The softmax layer transforms the input into a probability distribution – that
    is, it takes a set of numbers and turns them into positive numbers that sum up
    to 1, applying higher importance to higher values and less importance to smaller
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Output probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally, the output probabilities are the word tokens for the target. The transformer
    compares this output with the target sequence that came from the training data
    and uses it to improve the results via back-propagation.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve learned how transformers work, in the next section, we will have
    a very brief look at how one organization has made the process of implementing
    and experimenting with transformer-based models easy, making it accessible to
    all.
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers need a *lot* of data to be effective and produce good results.
    Furthermore, a huge amount of computing power and time is also needed; the best
    models are usually trained using multiple GPUs and can take days (or even longer)
    to complete training. Consequently, not everyone can afford to train such models
    and usually, this is done by the big players such as Google, Facebook, and OpenAI.
    Luckily, there are pretrained models available.
  prefs: []
  type: TYPE_NORMAL
- en: The name Hugging Face (named after the emoji with a smiling face and open hands)
    is synonymous with transformers, models, and NLP. Hugging Face ([https://huggingface.co](https://huggingface.co))
    provides a repository for pretrained transformer (and other) models to be published.
    These can then be easily downloaded, free of charge, and used for a wide range
    of NLP tasks. Furthermore, if the task involves a domain that has unique nomenclature,
    terminology, and domain-specific language, then models can be “fine-tuned” to
    improve the model’s performance. Fine-tuning is a process that uses the weights
    from a pretrained model as a starting point and uses new domain-specific data
    to update them so that the model becomes better at the domain-specific task. As
    well as publishing models, Hugging Face also provides services that allow models
    to be fine-tuned, trained, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hugging Face also provides a Python library that provides a high-level interface
    for NLP tasks. The library offers a wide range of state-of-the-art pretrained
    models, including BERT, GPT, RoBERTa, T5, and many others (see the next section).
    It can be installed using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Apart from downloading pretrained models, the library can also be used to download
    tokenizers. Both of these can then be used with your datasets to fine-tune tasks
    such as classification to create state-of-the-art NLP systems.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the Hugging Face `transformers` library is a powerful tool for working
    with NLP models, making it easy to work with transformer models. It has an intuitive
    design and extensive model selection and is well worth a look.
  prefs: []
  type: TYPE_NORMAL
- en: Existing models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Driven by boosts in computing, storage, and data capacity, transformers have
    taken the world by storm. Some of the more famous pretrained models include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bidirectional Encoder Representations from Transformers** (**BERT**): Created
    by the Google AI team, and trained on a huge corpus of text data, BERT takes the
    context from both the left and right sides of each word into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficiently Learning an Encoder that Classifies Token Accurately** (**ELECTRA**):
    ELECTRA uses a generator-discriminator model to distinguish between generated
    and real text. The generator is trained to generate text that is similar to real
    text, while the discriminator is trained to distinguish between real and generated
    text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative Pre-trained Transformer 3** (**GPT-3**): Developed by OpenAI and
    pretrained on a huge range of internet text, GPT-3 has 175 billion parameters
    and is one of the largest models available to date.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Megatron (a large transformer model trained by NVIDIA)**: Developed by NVIDIA,
    Megatron is scalable and can be trained on hundreds of GPUs, so it can use much
    larger models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robustly Optimized BERT** (**RoBERTa**): Based on BERT, RoBERTa is designed
    to improve upon BERT by using a larger training corpus and more training steps
    to learn more robust representations of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text-to-Text Transfer Transformer** (**T5**): Developed by Google, T5 treats
    NLP problems as “text-to-text” problems. It is trained on unlabeled and labeled
    data and then fine-tunes it individually for a variety of tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer with extra-long context** (**Transformer-XL**): This model introduces
    a memory module that allows the model to handle and understand long-term dependencies
    much better.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XLNet (generalized autoregressive pretraining)**: Developed by Google, XLNet
    takes the best bits from Transformer-XL and BERT and models dependencies between
    all input words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next section, we look will look closer at how transformers are trained
    for the task we are interested in: classification. Inspiration for this section
    came from the Hugging Face pages.'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformer models are trained as language models. These are a type of algorithm
    that has been trained by analyzing patterns of human language to understand and
    produce human language.
  prefs: []
  type: TYPE_NORMAL
- en: They have knowledge of grammar, syntax, and semantics, and can discern patterns
    and connections among words and phrases. Moreover, they can detect named entities,
    such as individuals, locations, and establishments, and interpret the context
    in which they are referenced. Essentially, a transformer model is a computer program
    that uses statistical models to analyze and generate language.
  prefs: []
  type: TYPE_NORMAL
- en: Language models are trained in a self-supervised manner on large amounts of
    text data, such as books, articles, and online content, to learn patterns and
    relationships between words and phrases. Some of the popular datasets used for
    pretraining transformers include Common Crawl, Wikipedia, and BooksCorpus. For
    example, BERT was trained using around 3.5 billion words in total with around
    2.5 billion from Wikipedia and around 1 billion from BooksCorpus. This allows
    the model to predict the likelihood of a certain word or phrase occurring after
    a given sequence of words. The outputs of a pretrained large language model typically
    involve predictions based on the input text. The model may output probabilities
    of certain words or phrases being used next in a sentence, make predictions of
    the most likely word to follow a given input word, or generate an entire sentence
    or paragraph based on the input text. The output can be used for various purposes,
    such as text generation, translation, sentiment analysis, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Self-supervised learning is a type of machine learning where the model learns
    to extract useful information from unlabeled data without requiring any explicit
    labels or supervision. Instead, the model is trained on a task such as predicting
    the missing part of an image or reconstructing a corrupted sentence. Consequently,
    this type of model builds an understanding of the language it has been trained
    on – but only from a statistical point of view. However, this approach lacks practicality
    for everyday tasks, and therefore, a generalized **pretrained** model must be
    customized through supervised **fine-tuning** using human-annotated labels specific
    to the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining (that is, training a model from scratch) requires huge amounts of
    data, and hence the process can take weeks or months. Fine-tuning is then performed
    *on* the pretrained model, so a pretrained language model is required to do the
    fine-tuning. Essentially, fine-tuning is a further training step with a dataset
    that suits the task.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a model typically adjusts the weights of the model’s pretrained
    layers to better fit a new dataset or task. The process of fine-tuning involves
    initializing the weights of the pretrained layers, and then training the entire
    model on the new dataset or task. During training, the weights of the pretrained
    layers are updated along with the weights of the newly added layers, allowing
    the model to learn more nuanced features from the new dataset while preserving
    the knowledge learned from the pretrained model. The degree to which the weights
    of the pretrained layers are updated during fine-tuning depends on the specifics
    of the new task and the amount of available data. In some cases, only the weights
    of the newly added layers are updated, while in others, the weights of the pretrained
    layers may be updated significantly. Another option is to keep all layers fixed
    apart from the final layer, whose weights are then modified during training. Consequently,
    fixing the layers with these techniques, and using a smaller learning rate, during
    the fine-tuning process, often yields a performance improvement. Sometimes, this
    goes hand in hand with adding new layers on top of the old architecture, thus
    persisting the old fixed weights and only allowing the weights of new layers to
    be changed.
  prefs: []
  type: TYPE_NORMAL
- en: But what exactly happens when fine-tuning? There are various techniques, but
    the general thought is that early layers learn generic patterns that are irrelevant
    to the actual task (for example, classification), while later layers learn the
    patterns that are relevant to the task. This intuition has been verified by various
    research teams.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: During the gradient descent calculation, the size of the step that’s taken in
    each iteration is determined by the learning rate, with the overall goal being
    to locate the minimum of a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: In practical terms, for classification, we would download a model such as **BertForSequenceClassification**
    – a BERT model with a linear layer for sentence classification. So, the final
    layer produces a probability vector that indicates the probability of each potential
    class label for the input sequence.
  prefs: []
  type: TYPE_NORMAL
- en: In short, fine-tuning a model enables it to adapt features it has learned to
    a new task or dataset, which can result in improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at the individual bits of the model earlier in this chapter and saw
    how the model has encoder and decoder blocks. Depending on the task, each of these
    parts can be utilized separately. For the classification task, an encoder-only
    model is recommended. For more details, there are some great Packt books available,
    such as *Transformers for Natural Language Processing*, by Denis Rothman.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will fine-tune a model using a training dataset, make
    some predictions on a test dataset, and evaluate the results.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will work through the code for implementing transformers
    for both single-emotion and multi-emotion datasets. We will be using **Google
    Colaboratory** (**Colab**) as it simplifies the implementation of transformers
    by providing a powerful cloud-based environment with pre-installed libraries and
    resources. So, let’s begin by looking at that.
  prefs: []
  type: TYPE_NORMAL
- en: Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Google Colab is a free notebook environment service that runs in the cloud ([https://colab.research.google.com](https://colab.research.google.com)).
    There are many benefits of using Colab; for example, it allows developers to start
    programming rapidly without having to worry about setup, it allows code to be
    shared with people who do not have the correct software installed locally, and
    it also integrates well with GitHub. However, one of the biggest advantages is
    that Google provides free access to GPUs. Machine learning, at its core, involves
    lots and lots of mathematical operations – something that GPUs are good at. In
    practical terms, even for simple models with small training datasets, the time-saving
    between GPU and non-GPU-powered systems can be many hours (for example, 10 minutes
    compared to 10 hours).
  prefs: []
  type: TYPE_NORMAL
- en: A few words of warning, though. Colab is ephemeral – in other words, files (for
    example, data files) uploaded to a session or generated by a session (for example,
    results) will eventually disappear. The workaround for this is to upload files
    to Google Drive and give permission for Colab to access them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Debugging on Colab is also a little more cumbersome than, say, via VS Code.
    It involves installing and importing the `ipdb` (IPython-enabled Python Debugger)
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Breakpoints are useful for developers and these can be set via code to cause
    the debugger to stop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use command-line arguments to control the debugger, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`c`: Continue execution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n`: Move to the next line'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`r`: Continue execution until the current function returns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Debugging can be globally turned off using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Debugging can also be globally turned on using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that we know what a transformer is, how it works, and how to implement it,
    let’s implement a transformer in Python using Colab to classify the datasets introduced
    in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Single-emotion datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will implement two transformers to cater to the two different types of datasets.
    Let’s start with the single-emotion task. Broadly, we will be following these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide access to Google Drive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create dataset and model variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load and prepare the datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize the datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load a model for classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up trainer arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the trained model to predict.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classifying the single-emotion tweets is a somewhat easier task, so let’s being
    with this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by installing some libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'These libraries are used to easily access datasets, evaluate the results of
    a model, and access the pretrained models available from Hugging Face, respectively.
    We can now import these into our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As mentioned previously, we want to upload our training and test files once
    and access them on-demand whenever we need them, so let’s give Colab access to
    our Google Drive, which is where the files are uploaded. In reality, some of these
    files are already available via the `datasets` library, but for now, let’s assume
    we want to access them from our repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You should replace `BASE_PATH` with your own path.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should now set up a few things to make our task easier for us. Different
    datasets need different parameters, so `enum` can be used to control the execution
    flow of the code. We must name our files so that the names contain the language
    code of the tweets within the file (that is, `AR`, `ES`, and `EN`), and then use
    `enum` and the filename to set variables that are useful in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For now, we will also set a variable such as `NUM_LABELS` to tell the model
    how many labels there are. Later, we will see that we don’t need to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use `enum` to set some dataset-specific variables. This way, when
    we want to try other datasets, we only need to modify the `ds` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We must also set `model_name` to tell the program which language-specific model
    to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can set various file path variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we must set a variable called `stub`, which we will use to save our
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The Hugging Face `transformers` library works well with the `datasets` library.
    So, next, we will load the data files, remove any unwanted columns, and create
    a `DatasetDict` object that will be used in subsequent parts of the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we must create a function that tokenizes the tweets from the training
    and test datasets that are held in the `dataset` variable. Simply put, the job
    of the tokenizer is to prepare the data, making it ready for input to a model.
    It does this by splitting sentences into words (tokens) and then splitting words
    into pieces (for example, *flyfishing* would be split into *fly*, *fish*, and
    *ing*). These tokens are then split into IDs (numbers) via a lookup table. Typically,
    you would use the tokenizer associated with the model that you are using. For
    example, for the `bert-base-cased` model, you would use `BertTokenizer`. However,
    in the following code, we have used something called `AutoTokenizer`. `AutoTokenizer`
    is a generic tokenizer auto class that automatically fetches the correct tokenizer
    class from the Hugging Face tokenizers library, as well as the data associated
    with the model’s tokenizer. An auto class is a generic class that simplifies the
    coding process by automatically finding the architecture of a pretrained model
    based on its name. All we need to do is choose the appropriate `AutoModel` for
    our task. Essentially, they are more flexible and make the programming somewhat
    simpler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the interesting part! We need to load a model for classification.
    As before, we could have used a specific BERT model that has been trained for
    sentence classification, such as `BertForSequenceClassification`. However, we
    have chosen to use an auto class to obtain the text classification model. In this
    case, since we are classifying text, we used `AutoModelForSequenceClassification`
    as `AutoModel`. We just supplied the name of the model and the number of labels
    that we are dealing with – the library takes care of the rest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to train the model, but first, we need to set up some arguments
    specifying what we want the trainer to do. We can do this by simply creating a
    `TrainingArguments` instance telling it where to save our model and that we want
    it to evaluate at the end of each epoch. The arguments are passed to `Trainer`,
    along with the model and the training and test datasets. Now, it is a simple matter
    of invoking the training and waiting for the results. Note how we save the resultant
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If all went well, you should see something like this (truncated):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms and programs used in this chapter all randomize aspects of the
    data, particularly the initial assignments of weights to internal nodes of the
    network, and hence the results that you obtain by running the same scripts on
    the same data may vary slightly from the results in the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a model fine-tuned on our dataset, we can see whether it does
    a good job on our test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can set up a dictionary of measures and iterate through them, computing
    and printing as we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The results from the model are summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Dataset** | **Precision** | **Recall** | **micro F1** | **macro F1** |
    **Jaccard** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.962 | 0.964 | 0.962 | 0.962 | 0.927 |'
  prefs: []
  type: TYPE_TB
- en: '| WASSA-EN | 0.855 | 0.861 | 0.855 | 0.856 | 0.753 |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.881 | 0.921 | 0.927 | 0.896 | 0.816 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-AR | 0.817 | 0.837 | 0.843 | 0.825 | 0.710 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-ES | 0.791 | 0.786 | 0.807 | 0.787 | 0.663 |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.905 | 0.905 | 0.905 | 0.905 | 0.826 |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 – Scores for transformer-based models for the single-label datasets
  prefs: []
  type: TYPE_NORMAL
- en: Most of the scores here are better than the scores that we obtained using the
    classifiers earlier in this book, though the best classifier for WASSA-EN and
    CARER-EN remains the single-class SVM. The scores for SEM4-AR and SEM4-ES are
    both significantly better than the previous scores, possibly because the pretrained
    models do a better job of finding roots, and maybe even doing disambiguation,
    than the simple stemmers we used in the earlier chapters. It is very hard to extract
    intermediate results from a complex DNN such as a transformer, so it is even more
    difficult than was the case in previous chapters to analyze why one classifier
    of this kind does better than another, but it seems likely that this is a key
    factor in these cases.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-emotion datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s build a transformer model to classify multi-label tweets. Much of
    the code is similar, so we won’t reproduce that, concentrating instead on the
    interesting bits of the multi-classification problem. We will be following these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the necessary libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provide access to Google Drive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create dataset variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert datasets into `DatasetDict`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load and prepare the datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenize the datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load a model for classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define metric functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up trainer arguments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: 'We must install and import the libraries as before, and also allow access to
    Google Drive as before. Now, let’s get the data files from the online repository.
    However, the KWT files are in Google Drive, so we need some code to load and convert
    these into a `DatasetDict` object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Our `Dataset` enum now also reflects the fact that we are working with different
    files, so let’s use `enum` to get the right data files and set the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three types of datasets: train, test, and validation. We will use
    the train and validation datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how in the first example, we had to set `NUM_LABELS` and we had no idea
    of what the actual labels were. Here, we are going to dynamically work out the
    labels and also create some lookup tables that allow us to easily go from an emotion
    to a label and vice versa:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output clarifies what each of these looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to tokenize the datasets, as we did previously. The task is slightly
    more complicated here as we have multiple labels for each tweet, and the labels
    are loaded as `True` and `False`, whereas we need `0` and `1` for our model. The
    `tokenize_function` takes 1,000 tweets at a time, tokenizes the tweet text as
    before, and converts the labels into an array of 1s and 0s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using `pytorch` in this example, so we need to set the format of the
    dataset so that it’s compatible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can instantiate an auto class, as we did previously. Note how we have
    to set `problem_type` and also pass in the `id-label` and `label-id` mapping objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to define some functions to calculate some metrics for us. Because
    we have multi-labels, we are dealing with probabilities. Consequently, we need
    a threshold to distinguish between a 0 and a 1 for the emotion – we have arbitrarily
    set this to `0.5` for now. In practice, this would need to be carefully determined.
    These probabilities are turned into 0s and 1s using the threshold and, as before,
    we piggyback on `scikit-learn` functions to do the heavy lifting for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now set up some `TrainingArguments` and train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to evaluate the results using our metric functions. Notice
    how we pass the name of the `compute_metrics` function as a parameter. This function,
    in turn, calls `compute_multi_label_metrics` to calculate the various metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The final results should look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The results from the model are summarized in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Dataset** | **Precision** | **Recall** | **micro F1** | **macro F1** |
    **Jaccard** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.694 | 0.496 | 0.710 | 0.539 | 0.418 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.552 | 0.441 | 0.658 | 0.462 | 0.359 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.132 | 0.074 | 0.224 | 0.092 | 0.053 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.594 | 0.399 | 0.597 | 0.463 | 0.340 |'
  prefs: []
  type: TYPE_TB
- en: Table 9.2 – Scores for transformer-based models for the multi-class datasets
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, the transformer-based models work better for some, but not all, datasets.
    It is worth noting that the previous best classifier for SEM11-AR was the stemmed
    version of the simple lexical model from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector-Space Models*, with a Jaccard score of 0.386\.
    For SEM11-ES, it was the stemmed version of the conditional probability model,
    also from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons
    and Vector Space Models*, with a Jaccard score of 0.278\. As with the single-class
    datasets, it seems likely that using the pretrained models may have helped us
    with identifying and disambiguating tokens, but this time, the underlying model
    is less good at handling multi-class cases. The score for the KWT.M-AR dataset
    is particularly poor: using transformers of the kind described here does not seem
    to be a good way to handle datasets with large numbers of tweets with no emotion
    ascribed to them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In several cases, using transformers produced better results than the classifiers
    from previous chapters. The following table shows the scores for a range of classifiers
    on our datasets (given the number of classifiers we have looked at now, this table
    only includes ones that were the best on at least one dataset):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **LEX (unstemmed)** | **LEX (stemmed)** | **SVM (single)** | **SNN (single)**
    | **Transformers** |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-EN | 0.503 | 0.497 | 0.845 | 0.829 | 0.927 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-EN | 0.347 | 0.348 | 0.224 | 0.242 | 0.418 |'
  prefs: []
  type: TYPE_TB
- en: '| WASSA-EN | 0.445 | 0.437 | 0.770 | 0.737 | 0.753 |'
  prefs: []
  type: TYPE_TB
- en: '| CARER-EN | 0.350 | 0.350 | 0.770 | 0.820 | 0.816 |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB-EN | 0.722 | 0.667 | 0.736 | 0.793 | 0.826 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-AR | 0.506 | 0.509 | 0.514 | 0.504 | 0.710 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-AR | 0.378 | 0.386 | 0.216 | 0.221 | 0.359 |'
  prefs: []
  type: TYPE_TB
- en: '| KWT.M-AR | 0.687 | 0.663 | 0.631 | 0.028 | 0.053 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM4-ES | 0.425 | 0.420 | 0.412 | 0.337 | 0.663 |'
  prefs: []
  type: TYPE_TB
- en: '| SEM11-ES | 0.269 | 0.271 | 0.226 | 0.221 | 0.340 |'
  prefs: []
  type: TYPE_TB
- en: Table 9.3 – Best scores to date for the standard datasets – Jaccard scores
  prefs: []
  type: TYPE_NORMAL
- en: The results of using transformers are better than any of the previous classifiers
    for 6 of our 10 datasets, though surprisingly, the very simple lexicon-based classifiers
    from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and
    Vector Space Models* still produce the best results for the multi-class Arabic
    datasets!
  prefs: []
  type: TYPE_NORMAL
- en: It remains the case that there is a drop-off in performance between the single-emotion
    datasets and the multi-emotion datasets. As before, this is likely to be due to
    a combination of factors. The fact that the multi-class datasets have more labels
    than the others also makes the task harder, simply because there is more scope
    for mistakes. However, we know that multi-emotion classification is much more
    difficult than single-emotion classification because it involves working out how
    many emotions a text expresses, from zero upward, rather than just choosing the
    one with the highest scores. We will look at ways of dealing with this kind of
    data in more detail in [*Chapter* *10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting, natural question here is, why do transformers perform better
    than other methods? We have already seen how the self-attention mechanism allows
    transformers to focus on different parts of the input sequence when making predictions,
    hence allowing them to capture important long-range dependencies and contextual
    information. This is important for robust classification. Furthermore, we have
    also seen how transformers use multi-head attention, which allows them to attend
    to different parts of the input sequence simultaneously, thus making them more
    effective at capturing the different types of information that may be important
    for robust classification. Transformers also handle long input sequences without
    losing important information, and this may also be more useful in classification
    than other tasks. Finally, as we have seen, transformers are pretrained on huge
    datasets. Hence, even before fine-tuning, they already know general representations
    of language. These concepts can be combined in a highly effective way to create
    a mechanism that can generate good results.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s summarize what we’ve learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers have proved to be very successful in a range of natural language
    tasks, with numerous recently released chatbots outperforming existing models
    in their ability to understand and manipulate human language. In this chapter,
    we looked at how transformers can be used for the task of assigning emotions to
    informal texts and investigated how well they perform on this task with a range
    of datasets. We started by taking a brief look at transformers, focusing on the
    individual components of a transformer, and how data flows through them. Transformers
    need a lot of data to be effective and produce good results, and a huge amount
    of computing power and time is also needed. Then, we introduced Hugging Face,
    discussed why it was useful, and introduced some of the more common pretrained
    models that are available on the Hugging Face platform, before moving on to discussing
    how transformers are used for classification. Finally, we showed how to code classifiers
    using transformers for single-emotion datasets and multi-emotion datasets before
    rounding off this chapter by discussing the results. In the next chapter, we will
    look at multiclassifiers.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,
    Kaiser, Ł. and Polosukhin, I., 2017\. *Attention Is All You Need*. Advances in
    neural information processing systems, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rothman, D., 2021\. *Transformers for Natural Language Processing: Build innovative
    deep neural network architectures for NLP with Python, PyTorch, TensorFlow, BERT,
    RoBERTa, and more*. Packt Publishing Ltd.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin, J., Chang, M. W., Lee, K., and Toutanova, K., 2018\. *Bert: Pre-training
    of deep bidirectional transformers for language understanding*. arXiv preprint
    arXiv:1810.04805.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clark, K., Luong, M. T., Le, Q. V. and Manning, C. D., 2020\. *Electra: Pre-training
    text encoders as discriminators rather than generators*. arXiv preprint arXiv:2003.10555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan,
    A., Shyam, P., Sastry, G., Askell, A., and Agarwal, S., 2020\. *Language models
    are few-shot learners*. Advances in neural information processing systems, 33,
    pp.1877-1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
    B., 2019\. *Megatron-lm: Training multi-billion parameter language models using
    model parallelism*. arXiv preprint arXiv:1909.08053.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,
    M., Zettlemoyer, L., and Stoyanov, V., 2019\. *RoBERTa: A Robustly Optimized BERT
    Pretraining Approach*. arXiv preprint arXiv:1907.11692.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
    Y., Li, W., and Liu, P. J., 2020\. *Exploring the limits of transfer learning
    with a unified text-to-text transformer*. J. Mach. Learn. Res., 21(140), pp.1-67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and Salakhutdinov, R.,
    2019\. *Transformer-xl: Attentive language models beyond a fixed-length context*.
    arXiv preprint arXiv:1901.02860.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., and Le, Q.
    V., 2019\. *XLNet: Generalized autoregressive pretraining for language understanding*.
    Advances in neural information processing systems, 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
