<html><head></head><body>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Achieving Generalization</h1></div></div></div><p class="calibre8">We have to confess that, until this point, we've delayed the crucial moment of truth when our linear model has to be put to the test and verified as effectively predicting its target. Up to now, we have just considered whether we were doing a good modeling job by naively looking at a series of good-fit measures, all just telling us if the linear model could be apt at predicting based solely on the information in our training data.</p><p class="calibre8">Unless you love sink-or-swim situations, in much the same procedure you'd employ with new software before going into production, you need to apply the correct tests to your model and to be able to anticipate its live performance.</p><p class="calibre8">Moreover, no matter your level of skill and experience with such types of models, you can easily be misled into thinking you're building a good model just on the basis of the same data you used to define it. We will therefore introduce you to the fundamental distinction between in-sample and out-of-sample statistics and demonstrate how they risk diverging when you use too many predictors, too few predictors, or simply just the wrong ones.</p><p class="calibre8">Here we are then, ready at last to check whether we have done a good job or have to rethink everything from scratch. In this pivotal chapter of the book, before proceeding to more complex techniques, we will introduce you to key data science recipes to thoroughly test your model, fine-tune it optimally, make it economical, and pit it against real, fresh data without any concerns.</p><p class="calibre8">In this chapter, you'll get to know how to:</p><div><ul class="itemizedlist"><li class="listitem">Test your models, using the most appropriate cost measure, on a validation/test set or using cross-validation</li><li class="listitem">Select the best features on the basis of statistical tests and experiments</li><li class="listitem">Make your model more economical by tweaking the cost function</li><li class="listitem">Use stability selection, an almost automated method for variable selection</li></ul></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec35" class="calibre1"/>Checking on out-of-sample data</h1></div></div></div><p class="calibre8">Until this point in the book, we have striven to make the regression model fit data, even by modifying the data itself (inputting missing data, removing outliers, transforming for non-linearity, or creating new features). By keeping an eye on measures such as R-squared, we have tried our best to reduce prediction errors, though we have no idea to what extent this was successful.</p><p class="calibre8">The problem <a id="id465" class="calibre1"/>we face now is that we shouldn't expect a well fit model to automatically perform well on any new data during production.</p><p class="calibre8">While defining and explaining the problem, we recall what we said about underfitting. Since we are working with a linear model, we are actually expecting to apply our work to data that has a linear relationship with the response variable. Having a linear relationship means that, with respect to the level of the response variable, our predictors always tend to constantly increase (or decrease) at the same rate. Graphically, on a scatterplot, this is refigured by a straight and very elongated cloud of points that could be crossed by a straight regression line with little or minimal prediction error.</p><p class="calibre8">When the relationship is instead non-linear, the rate of change and direction are mutable (alternatively increasing or decreasing). In such a situation, in order to have the linear model work better, we will have to try to make the relationship straight by opportune transformations. Otherwise we will have to try guessing the response by a not-always-successful approximation of a non-linear shape to a linear one.</p><p class="calibre8">If for instance the relationship is quadratic (so the functional shape is that of a parabola), using a line will pose the problem of a systematic underestimation or overestimation of the predicted values at certain ranges in the predictor's values. This systematic error is called bias and it is typical of simple models such as linear regression. A prediction model with a high bias will systematically tend to generate erroneous predictions in certain situations. Since inaccuracy of predictions is an undesirable characteristic for a tool that should be able to provide effective forecasts, we have to strive to achieve a better fit to the response by adding new variables and transforming the present ones by polynomial <a id="id466" class="calibre1"/>expansion or other transformations. Such efforts constitute the so-called <strong class="calibre2">feature creation phase</strong>.</p><p class="calibre8">By doing so, we may find ourselves in a different but no less problematic situation. In fact, when we render our model more and more complex, it will not just better fit the response by catching more and more parts of the unknown function that ties it to the predictors, but also, by adding more and more terms, we are enabling our model to receive that part of the information that is exclusively specific to the data at hand (we call this noise), making it more and more unable to work properly with different data.</p><p class="calibre8">You could think about it as a <em class="calibre9">power of memorization</em> so that, the more complex the learning algorithm, the more space there will be to fit not-so-useful information from the data we are using for learning. This memorization brings very inconvenient consequences. Though our model appears to have a good fit on our data, as soon as it is applied to a different set, it reveals its inability to predict correctly. In such a situation, contrary to before when the errors were systematic (systematic under- or over-estimation), errors will appear to be erratic, depending on the dataset. This is called variance of the estimates and it could prove more of a problem to you because it can leave you unaware<a id="id467" class="calibre1"/> of its existence until you test it against real data. It tends to strike in more complex algorithms and, in its simplest form, linear regression tends to present a higher bias on the estimates than variance. Anyway, adding too many terms and interactions or resorting to polynomial expansion does expose linear models to overfitting.</p></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec66" class="calibre1"/>Testing by sample split</h2></div></div></div><p class="calibre8">Since we expect the ability to generalize to new data from a model and since we are seldom interested in just fitting or simply memorizing the present data, we need to take some cautionary<a id="id468" class="calibre1"/> steps as we build our model. To fight against this problem, the practice of learning from data has defined over the years a series of procedures, based on the scientific method of validating and testing, that we are going to illustrate and practice ourselves.</p><p class="calibre8">First, if we want our model to generalize well on new data, we have to test it in such a situation. This means that, if getting new data is not an easy task or a feasible one, we should reserve some data for our tests from the beginning. We can achieve that by randomly splitting our data into two parts, a training set and a test set, using 70–80 percent of the data for the training part and the residual 20–30 percent for testing purposes.</p><p class="calibre8">Scikit-learn's <code class="email">cross_validation</code> module offers a series of methods that can help us in dealing with all these operations. Let's try it by operating on our usual Boston Housing dataset:</p><div><pre class="programlisting">In: import pandas as pd
from sklearn.datasets import load_boston
boston = load_boston() 
dataset = pd.DataFrame(boston.data, columns=boston.feature_names)
dataset['target'] = boston.target
observations = len(dataset)
variables = dataset.columns[:-1]
X = dataset.ix[:,:-1]
y = dataset['target'].values</pre></div><p class="calibre8">After having loaded it, let's first split it into train and test parts:</p><div><pre class="programlisting">In: from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.30, random_state=101)
print ("Train dataset sample size: %i" % len(X_train))
print ("Test dataset sample size: %i" % len(X_test))

Out:   Train dataset sample size: 354
Test dataset sample size: 152</pre></div><p class="calibre8"><code class="email">train_test_split</code> will separate the data according to the specified quota for testing indicated in the <code class="email">test_size</code> parameter. The split will be a random one, and you can deterministically control the results (for replication purposes) using a specific numeric seed in the <code class="email">random_state</code> parameter (our choice for the seed is <code class="email">101</code>).</p><p class="calibre8">Sometimes, reserving an out-of-sample (comprising what is not in-sample—that is, used as a sample for <a id="id469" class="calibre1"/>learning from training activity data) is not enough, because we may have to tune some parameters or make specific choices and we want to test the alternatives without having to use the test data. The solution is to reserve another part of our data for validation purposes, which implies checking what parameters could be optimal for our model. We can achieve that using <code class="email">train_test_split</code> in two steps:</p><div><pre class="programlisting">In: X_train, X_out_sample, y_train, y_out_sample = \
train_test_split(X, y, test_size=0.40, random_state=101)
X_validation, X_test, y_validation, y_test = \
train_test_split(X_out_sample, y_out_sample, test_size=0.50, random_state=101)
print ("Train dataset sample size: %i" % len(X_train))
print ("Validation dataset sample size: %i" % len(X_validation))
print ("Test dataset sample size: %i" % len(X_test))
Out:   Train dataset sample size: 303
Validation dataset sample size: 101
Test dataset sample size: 102</pre></div></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec67" class="calibre1"/>Cross-validation</h2></div></div></div><p class="calibre8">Though helpful in <a id="id470" class="calibre1"/>measuring the true error of an hypothesis, dividing <a id="id471" class="calibre1"/>your data into train and test (and sometimes also into validation) sets presents some risks that you have to take into account:</p><div><ul class="itemizedlist"><li class="listitem">Since it involves sub-sampling (you casually draw out a part of your initial sample), you may incur the risk of drawing sets that are too favorable or unfavorable for training and testing</li><li class="listitem">By leaving aside a portion of your sample, you reduce the number of examples to learn from, whereas linear models need as many as possible in order to reduce the variance of the estimates, disambiguate collinear variables, and properly model non-linearity</li></ul></div><p class="calibre8">Though we always suggest drawing a small test sample (say 10% of the data) as a final check of the validity of your work, the best way to avoid the aforementioned problems, and easily manage different comparisons of models and parameters, is to apply cross-validation, which requires you to split your data for both training and testing but it does so repetitively <a id="id472" class="calibre1"/>until every observation has played the role of training <a id="id473" class="calibre1"/>and testing.</p><p class="calibre8">In other words, you decide how many mutually exclusive parts to split your data into, then you repeatedly keep on training your model using all the folds but a different one every time; this plays the role of a test set.</p><div><h3 class="title2"><a id="note03" class="calibre1"/>Note</h3><p class="calibre8">The number of parts you split your data into is usually set to 3, 5, 10, or 20 and you decide on a large number of splits (each one called a <strong class="calibre2">fold</strong>) when you have little training data.</p></div><p class="calibre8">When you have completed the validation, using every single split available as the test set, you first take the average of the results, which tells you with a good degree of accuracy the overall performance of your model when faced with new data (new but not too dissimilar from the one you have at hand). Then you also notice the standard deviation of the cross-validated performances. This is important because, if there's a high deviation (over half of the average performance value), it can indicate that the model has a high variance of the estimates and that it needs more data to work well.</p><p class="calibre8">In the following example, you can look at how <code class="email">KFold</code> and <code class="email">StratifiedKFold</code> (from the Scikit-learn's <code class="email">cross_validation</code> module) work.</p><p class="calibre8">They are both iterators: you draw the indices for training and testing for each round of cross validation, with the sole <a id="id474" class="calibre1"/>difference that <code class="email">KFold</code> just applies a random draw. Instead, <code class="email">StratifiedKFold</code> takes account of the distribution of a target variable that you want distributed in your training and test samples as if it were on the original set.</p><p class="calibre8">As parameters to both classes, you should provide:</p><div><ul class="itemizedlist"><li class="listitem">The count of observations to <code class="email">KFold</code> and the target vector to <code class="email">StratifiedKFold</code></li><li class="listitem">The number of folds (10 is usually the standard choice, but you can decrease the number of folds if you have many observations, or you can increase it if your dataset is small)</li></ul></div><p class="calibre8">You should also decide:</p><div><ul class="itemizedlist"><li class="listitem">Whether to shuffle the data or take it as it is (shuffling is always recommended)</li><li class="listitem">Whether to apply a random seed and make the results replicable</li></ul></div><div><pre class="programlisting">In: from sklearn.cross_validation import cross_val_score, \
KFold, StratifiedKFold
from sklearn.metrics import make_scorer
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np
def RMSE(y_true, y_pred):
    return np.sum((y_true -y_pred)**2)
lm = LinearRegression()
cv_iterator = KFold(n=len(X), n_folds=10, shuffle=True,\ random_state=101)
edges = np.histogram(y, bins=5)[1]
binning = np.digitize(y, edges)
stratified_cv_iterator = StratifiedKFold(binning, n_folds=10,\ shuffle=True, random_state=101)

second_order=PolynomialFeatures(degree=2, interaction_only=False)
third_order=PolynomialFeatures(degree=3, interaction_only=True)

over_param_X = second_order.fit_transform(X)
extra_over_param_X = third_order.fit_transform(X)
cv_score = cross_val_score(lm, over_param_X, y, cv=cv_iterator,\ scoring='mean_squared_error', n_jobs=1)</pre></div><div><h3 class="title2"><a id="note04" class="calibre1"/>Note</h3><p class="calibre8">The <code class="email">n_jobs</code> parameter will set the number of threads involved in the computation of the results by leveraging parallel computations. When it is set to <code class="email">−1</code> it will automatically use all the available threads, speeding up the calculations to the maximum on your computer. Anyway, depending on the system you are working on, sometimes setting the parameter to something different than <code class="email">1</code> will cause problems, slowing down the results. In our examples, as a precautionary measure, it is always set to <code class="email">1</code>, but you can change its value if you need to cut short the computational time.</p></div><p class="calibre8">At first, we try<a id="id475" class="calibre1"/> to get the cross-validation score of an over-parameterized <a id="id476" class="calibre1"/>model (a second-degree polynomial expansion of the original features of the Boston dataset). Please notice that the results are negative (though they are squared errors) because of the internals of the automatic function for computing the cross-validation of a model, <code class="email">cross_val_score</code>, from Scikit-learn. This function requires the model, the features, and the target variable as input. It also accepts a cross validation iterator of your choice for the parameter <code class="email">cv</code>, a string for <code class="email">scoring</code> indicating the name of the scoring function to be used (more<a id="id477" class="calibre1"/> on this can be found at: <a class="calibre1" href="http://scikit-learn.org/stable/modules/model_evaluation.html">http://scikit-learn.org/stable/modules/model_evaluation.html</a>); and finally the number of threads working in parallel on your PC by specifying <code class="email">n_jobs</code> (<code class="email">1</code> indicates that only one thread is working whereas <code class="email">−1</code> indicates all the available threads in the system are used):</p><div><pre class="programlisting">In: print (cv_score)

Out: [-10.79792467 -19.21944292  -8.39077691 -14.79808458 -10.90565129  -7.08445784 -12.8788423  -16.80309722 -32.40034131 -13.66625192]</pre></div><div><h3 class="title2"><a id="tip31" class="calibre1"/>Tip</h3><p class="calibre8">The mean squared error is negative because of the internals of the function, which can only maximize, whereas our cost metric has to be minimized; this is why it has become negative</p></div><p class="calibre8">After removing <a id="id478" class="calibre1"/>the sign, we can take both the average and the standard <a id="id479" class="calibre1"/>deviation. Here, we can also notice that the standard deviation is high, and maybe we should then try to control the distribution of the target variable, since in the real estate business there are outlying observations due to very rich residential areas:</p><div><pre class="programlisting">In: print ('Cv score: mean %0.3f std %0.3f' % (np.mean(np.abs(cv_score)), np.std(cv_score)))
Out: Cv score: mean 14.694 std 6.855</pre></div><p class="calibre8">To apply such a control, we stratify the target variable; that is, we divide it into bins and we expect the bin distribution to be kept during the cross-validation process:</p><div><pre class="programlisting">In:cv_score = cross_val_score(lm, over_param_X, y,\
cv=stratified_cv_iterator, scoring='mean_squared_error', \
n_jobs=1)
print ('Cv score: mean %0.3f std %0.3f' % \
       (np.mean(np.abs(cv_score)), np.std(cv_score)))
Out: Cv score: mean 13.584 std 5.226</pre></div><p class="calibre8">In the end, controlling for the response distribution really lowers the standard deviation of the estimated error (and our expected average). A successful stratification attempt in cross-validation suggests that we should train on a correctly distributed training sample, otherwise we may achieve an outcome model not always working properly due to bad sampling.</p><div><h3 class="title2"><a id="tip32" class="calibre1"/>Tip</h3><p class="calibre8">As a final remark on the topic of cross-validation, we suggest using it mostly for evaluating parameters, and always relying on a small drawn out test set for performance validation. In fact, it is a bit tricky, but if you cross-validate too many times (for example changing the seed) looking for the best performance, you will end up with the best result, which is another form of overfitting called <a id="id480" class="calibre1"/>snooping (this also happens if you do the same with the test set). Instead, when you use cross-validation to choose between parameters, you just decide on the best among the options, not on the absolute cross-validation value.</p></div></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec68" class="calibre1"/>Bootstrapping</h2></div></div></div><p class="calibre8">Sometimes, if the training data is really small, even dividing into folds can penalize how the model is trained. The statistical technique of bootstrapping allows repeating the training and testing validation sequence (allowing precise estimations of both the mean and standard deviation of expected results) for a large number of times by trying to replicate the<a id="id481" class="calibre1"/> underlying distribution of the data.</p><p class="calibre8">Bootstrapping is<a id="id482" class="calibre1"/> based on sampling with repetition, which implies allowing an observation to be drawn multiple times. Usually bootstraps draw the number of observations equivalent to the original size of the dataset. Also, there's always a part of the observations that it is left untouched, equivalent to a third of the available observations, which can be used for validating:</p><div><pre class="programlisting">In: import random
def Bootstrap(n, n_iter=3, random_state=None):
      """
      Random sampling with replacement cross-validation generator.
      For each iter a sample bootstrap of the indexes [0, n) is
      generated and the function returns the obtained sample
      and a list of all the excluded indexes.
      """
      if random_state:
          random.seed(random_state)
      for j in range(n_iter):
          bs = [random.randint(0, n-1) for i in range(n)]
          out_bs = list({i for i in range(n)} - set(bs))
          yield bs, out_bs
        
boot = Bootstrap(n=10, n_iter=5, random_state=101)
for train_idx, validation_idx in boot:
print (train_idx, validation_idx)</pre></div><p class="calibre8">The output will be shown as the following:</p><div><img src="img/00108.jpeg" alt="Bootstrapping" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">As illustrated by the preceding example (unfortunately, this method is not part of Scikit-learn, having being recently deprecated), in a set of 10 observations, on average four observations are left available for testing purposes. However, in a bootstrapping process, it is not just the left out cases that provide insight. A model is in fact fitted to the training dataset, and we can also inspect how the coefficients are determined in the bootstrap replications, thus<a id="id483" class="calibre1"/> allowing us to figure out how stable each coefficient <a id="id484" class="calibre1"/>is:</p><div><pre class="programlisting">In: import numpy as np
boot = Bootstrap(n=len(X), n_iter=10, random_state=101)
lm = LinearRegression()
bootstrapped_coef = np.zeros((10,13))
for k, (train_idx, validation_idx) in enumerate(boot):
       lm.fit(X.ix[train_idx,:],y[train_idx])
       bootstrapped_coef[k,:] = lm.coef_</pre></div><p class="calibre8">For instance, the tenth coefficient index (PTRATIO) is quite stable in both sign and value:</p><div><pre class="programlisting">In: print(bootstrapped_coef[:,10])

Output: [-1.04150741 -0.93651754 -1.09205904 -1.10422447 -0.9982515
-0.79789273 -0.89421685 -0.92320895 -1.0276369  -0.79189224]</pre></div><p class="calibre8">Whereas the sixth coefficient (AGE) has great variability, often even changing sign:</p><div><pre class="programlisting">In: print(bootstrapped_coef[:,6])

Out: [-0.01930727  0.00053026 -0.00026774  0.00607945  0.02225979 -0.00089469  0.01922754  0.02164681  0.01243348 -0.02693115]</pre></div><p class="calibre8">In conclusion, bootstrap is a form of replication that can be run as many times as you decide, and this allows you to create multiple models and evaluate their results in a similar way to a cross-validation procedure.</p></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec36" class="calibre1"/>Greedy selection of features</h1></div></div></div><p class="calibre8">By following our experiments throughout the book, you may have noticed that adding new variables is always a great success in a linear regression model. That's especially true for training <a id="id485" class="calibre1"/>errors and it happens not just when we insert the right variables but also when <a id="id486" class="calibre1"/>we place the wrong ones. Puzzlingly, when we add redundant or non-useful variables, there is always a more or less positive impact on the fit of the model.</p><p class="calibre8">The reason is easily explained; since regression models are high-bias models, they find it beneficial to augment their complexity by increasing the number of coefficients they use. Thus, some of the new coefficients can be used to fit the noise and other details present in data. It is precisely the memorization/overfitting effect we discussed before. When you have as many coefficients as observations, your model can become saturated (that's the technical term used in statistics) and you could have a perfect prediction because basically you have a coefficient to learn every single response in the training set.</p><p class="calibre8">Let's make this <a id="id487" class="calibre1"/>concept more concrete with a quick example using a training set (in-sample observations) and a test set (out-sample observations). Let's start by finding out how many cases and features we <a id="id488" class="calibre1"/>have and what the baseline performance is (for both in-sample and out-sample):</p><div><pre class="programlisting">In: from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=3)
lm = LinearRegression()
lm.fit(X_train,y_train)
print ('Train (cases, features) = %s' % str(X_train.shape))
print ('Test  (cases, features) = %s' % str(X_test.shape))
print ('In-sample  mean squared error %0.3f' % mean_squared_error(
        y_train,lm.predict(X_train)))
print ('Out-sample mean squared error %0.3f' % mean_squared_error(
        y_test,lm.predict(X_test)))

Out:   Train (cases, features) = (354, 13)
Test  (cases, features) = (152, 13)
In-sample  mean squared error 22.420
Out-sample mean squared error 22.440</pre></div><div><h3 class="title2"><a id="tip33" class="calibre1"/>Tip</h3><p class="calibre8">The best approach would be to use a cross validation or bootstrap for such an experiment, not just a plain train/test split, but we want to make it fast, and that's the reason why we decided on such a solution. We assure you that using more sophisticated estimation techniques doesn't change the results of the experiment.</p></div><p class="calibre8">Therefore, we have similar in-sample and out-sample errors. We can start working on improving our model using polynomial expansions:</p><div><pre class="programlisting">In: from sklearn.preprocessing import PolynomialFeatures
second_order=PolynomialFeatures(degree=2, interaction_only=False)
third_order=PolynomialFeatures(degree=3, interaction_only=True)</pre></div><p class="calibre8">First, we apply the second-order polynomial expansion:</p><div><pre class="programlisting">In: lm.fit(second_order.fit_transform(X_train),y_train)
print ('(cases, features) = %s' % str(second_order.fit_transform(
            X_train).shape))
print ('In-sample  mean squared error %0.3f' %
mean_squared_error(y_train,lm.predict(second_order.fit_transform(
            X_train))))
print ('Out-sample mean squared error %0.3f' %
mean_squared_error(y_test,lm.predict(second_order.fit_transform(
            X_test))))
Out:   (cases, features) = (354, 105)
In-sample  mean squared error 5.522
Out-sample mean squared error 12.034</pre></div><p class="calibre8">It seems that the<a id="id489" class="calibre1"/> good in-sample results have little correspondence with the out-sample test. Though the out-sample performance has improved, the lack of comparability in results is a clear sign of overfitting; there are some more useful coefficients in the model but most of them are just there to catch noise in data.</p><p class="calibre8">We now go to extremes and we test the third-degree polynomial expansion (using only interactions though):</p><div><pre class="programlisting">In: lm.fit(third_order.fit_transform(X_train), y_train)
print ('(cases, features) = %s' % str(third_order.fit_transform(
            X_train).shape))
print ('In-sample  mean squared error %0.3f' %
mean_squared_error(y_train,lm.predict(third_order.fit_transform(
            X_train))))
print ('Out-sample mean squared error %0.3f' %
mean_squared_error(y_test,lm.predict(third_order.fit_transform(
            X_test))))

Out:   (cases, features) = (354, 378)
In-sample  mean squared error 0.438
Out-sample mean squared error 85777.890</pre></div><p class="calibre8">Now, clearly something very bad has happened to our model. Having more coefficients than observations (<code class="email">p&gt;n</code>), we achieved a perfect fit on our training set. However, on the out-sample validation, it seems that our model achieved the same performance as a random number generator. In the next few paragraphs, we will show you how to take advantage of an increased number of features without incurring any of the problems demonstrated by the previous code snippets.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec69" class="calibre1"/>The Madelon dataset</h2></div></div></div><p class="calibre8">For the task <a id="id490" class="calibre1"/>of selecting the best subset of variables among many noisy and collinear ones, we decided to accompany our usual Boston house dataset with a tricky one, the Madelon dataset (<a class="calibre1" href="https://archive.ics.uci.edu/ml/datasets/Madelon">https://archive.ics.uci.edu/ml/datasets/Madelon</a>). It is an artificial dataset (that is <a id="id491" class="calibre1"/>generated using an algorithm) presented at the NIPS 2003 (the seventh Annual Conference on Neural Information Processing Systems) during a contest on feature selection.</p><p class="calibre8">The dataset is particularly challenging because it has been generated by placing 32 distinct clusters of points (16 from the positive group, 16 from the negative one) on the vertices of a five-dimension hypercube. The resulting 500 features and 2,000 cases have been extracted from various transformations of the five metric dimensions. To make things harder, some random numbers have been added to the features to act as noise and a few responses have been flipped (the flipped ones amount to 1%). All these intricate transformations make dealing with the modeling quite difficult, especially for linear models, since<a id="id492" class="calibre1"/> the relationship of most of the features with the response is definitely non-linear. This is really helpful for our exemplification because it clearly demonstrates how a direct inclusion of all the features is detrimental to the accuracy of out-of-sample predictions.</p><p class="calibre8">To download and make available on your computer such an interesting and challenging dataset, please carry out the following instructions and allow some time for your computer to download the data from the external website where it is stored:</p><div><pre class="programlisting">In: try:
import urllib.request as urllib2
  except:
    import urllib2
  import numpy as np
  train_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON/madelon_train.data'
  validation_data = 'https://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON/madelon_valid.data'
train_response = 'https://archive.ics.uci.edu/ml/machine-learning-databases/madelon/MADELON/madelon_train.labels'
  validation_response = 'https://archive.ics.uci.edu/ml/machine-learning-databases/madelon/madelon_valid.labels'
try:
      Xt = np.loadtxt(urllib2.urlopen(train_data))
      yt = np.loadtxt(urllib2.urlopen(train_response))
      Xv = np.loadtxt(urllib2.urlopen(validation_data))
      yv = np.loadtxt(urllib2.urlopen(validation_response))
except:
    # In case downloading the data doesn't works, 
# just manually download the files into the working directory
      Xt = np.loadtxt('madelon_train.data')
      yt = np.loadtxt('madelon_train.labels')
      Xv = np.loadtxt('madelon_valid.data')
      yv = np.loadtxt('madelon_valid.labels')</pre></div><p class="calibre8">After finishing loading both the training and validation sets, we can start exploring some of the information available:</p><div><pre class="programlisting">In: print ('Training set: %i observations %i feature' % (Xt.shape))
  print ('Validation set: %i observations %i feature' % (Xv.shape))
Out:  Training set: 2000 observations 500 feature
    Validation set: 600 observations 500 feature </pre></div><p class="calibre8">Naturally, we <a id="id493" class="calibre1"/>won't touch the validation set (we won't even glance at it or it would be snooping), but we can try to <a id="id494" class="calibre1"/>figure out the situation with the training set:</p><div><pre class="programlisting">In:from scipy.stats import describe
  print (describe(Xt))</pre></div><p class="calibre8">The output is quite lengthy and it is put in matrix form (therefore it is not reported here), but it really tells us everything about the mean, min, max, variance, skewness, and kurtosis for each feature in the dataset. A fast glance through it doesn't reveal anything special; however, it explicits that all the variables have an approximately normal distribution and that they have a limited range of values. We can proceed with our exploration using a graphical representation of correlations among the variables:</p><div><pre class="programlisting">import matplotlib.pyplot as plt
import matplotlib as mpl
%matplotlib inline

def visualize_correlation_matrix(data, hurdle = 0.0):
    R = np.corrcoef(data, rowvar=0)
    R[np.where(np.abs(R)&lt;hurdle)] = 0.0
    heatmap = plt.pcolor(R, cmap=mpl.cm.coolwarm, alpha=0.8)
    heatmap.axes.set_frame_on(False)
    plt.xticks(rotation=90)
    plt.tick_params(axis='both', which='both', bottom='off',\
                  top='off', left = 'off',right = 'off')
    plt.colorbar()
    plt.show()

visualize_correlation_matrix(Xt[:,100:150], hurdle=0.0)</pre></div><p class="calibre8">Check the following screenshot:</p><div><img src="img/00109.jpeg" alt="The Madelon dataset" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">After a glance at a portion of the features and their respective correlation, we can notice that just a couple of them have a significant correlation, whereas the others are mildly related. This<a id="id495" class="calibre1"/> gives the impression of noisy relationships between them, thus rendering an effective selection quite complicated.</p><p class="calibre8">As a last step, we check how a simple logistic regression model would score in terms of the error measured using the area under the curve metric.</p><div><h3 class="title2"><a id="tip34" class="calibre1"/>Tip</h3><p class="calibre8">Area <a id="id496" class="calibre1"/>under the curve (AUC) is a measure derived from comparing the rate of correct positive results against the rate of incorrect ones at different classification thresholds. It is a bit tricky to calculate, so we suggest always relying on the <code class="email">roc_auc_score</code> function from the <code class="email">sklearn.metrics</code> module.</p></div><p class="calibre8">Logistic regression classifies an observation as positive if the threshold is over 0.5 since such a split is always proved to be optimal, but we can freely change that threshold. To increase the precision at a top selection of results we just raise the threshold from 0.5 to 1.0 (raising the threshold increases the accuracy in the selected range). Instead, if we intend to increase the total number of guessed positive cases we just choose a threshold inferior to 0.5 down to almost 0.0 (lowering the threshold increases the coverage of positive cases in the selected range).</p><p class="calibre8">The AUC error measure helps us determine whether our predictions are ordered properly, no matter their effective precision in terms of value. Thus, AUC is the ideal error measure to evaluate an algorithm for selection. If you order results properly based on probability, no matter if the guessed probability is correct or not, you can simply pick the correct selection to be used by your project by changing the 0.5 threshold—that is, by taking a certain number of the top results.</p><p class="calibre8">In our case, the baseline AUC measure is <code class="email">0.602</code>, a quite disappointing value since a random<a id="id497" class="calibre1"/> selection should bring us a <code class="email">0.5</code> value (<code class="email">1.0</code> is the maximum possible):</p><div><pre class="programlisting">In: from sklearn.cross_validation import cross_val_score
  from sklearn.linear_model import LogisticRegression
  logit = LogisticRegression()
  logit.fit(Xt,yt)

  from sklearn.metrics import roc_auc_score
  print ('Training area under the curve: %0.3f' % \roc_auc_score(yt,logit.predict_proba(Xt)[:,1]))
  print ('Validation area under the curve: %0.3f' % \roc_auc_score(yv,logit.predict_proba(Xv)[:,1]))

Out:   Training area under the curve: 0.824
    Validation area under the curve: 0.602</pre></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec70" class="calibre1"/>Univariate selection of features</h2></div></div></div><pre>In addition, if interpreting your model is a valuable addition, you really should remove non-useful variables, striving for the simplest possible form of your linear model as dictated by Occam's razor, a commonplace practice in science, favoring simpler solutions against more complex ones when their difference in performance is not marked.</pre><p class="calibre8">Feature selection can help in both increasing the model out-sample performance and its human readability by retaining only the most predictive set of variables in the model, in some cases just the best ones and in others the set that works the best in unison.</p><p class="calibre8">There are quite a few feature selection methods. The simplest approach is the univariate method, which evaluates how good a variable is by estimating its predictive value when taken alone in respect of the response.</p><p class="calibre8">This usually involves using statistical tests, and Scikit-learn offers three possible tests:</p><div><ul class="itemizedlist"><li class="listitem">The <code class="email">f_regression</code> class, which works out an F-test (a statistical test for comparing different regression solutions) and a p-value (interpretable as the probability value in which we observed a difference by chance) and reveals the best features for a regression</li><li class="listitem">The <code class="email">f_class</code>, which is an Anova F-test (a statistical test for comparing differences among classes), another statistical and related method that will prove useful for classification problems</li><li class="listitem">The <code class="email">Chi2</code> class, which is a chi-squared test (a statistical test on count data), a good choice when your problem is classification and your answer variable is a count or a binary (in every case, a positive number such as units sold or money earned)</li></ul></div><p class="calibre8">All such tests output a score and a statistical test expressed by a p-value. High scores, confirmed<a id="id500" class="calibre1"/> by small p-values (under 0.05, indicating <a id="id501" class="calibre1"/>a low probability that the score has been obtained by luck), will provide you with confirmation that a certain variable is useful for predicting your target.</p><p class="calibre8">In our example, we will use <code class="email">f_class</code> (since we are working on a classification problem now) and we will have the <code class="email">SelectPercentile</code> function help us by selecting a certain percentage of high-scoring features:</p><div><pre class="programlisting">In: from sklearn.feature_selection import SelectPercentile, f_classif
selector = SelectPercentile(f_classif, percentile=50)
selector.fit(Xt,yt)
variable_filter = selector.get_support()</pre></div><p class="calibre8">After selecting the upper half, hoping to have cut off the most irrelevant features and to have kept the important ones, we plot our results on an histogram to reveal the distribution of the scores:</p><div><pre class="programlisting">In: plt.hist(selector.scores_, bins=50, histtype='bar')
plt.grid()
plt.show()</pre></div><p class="calibre8">Look at the following screenshot:</p><div><img src="img/00110.jpeg" alt="Univariate selection of features" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Noticeably, most scores are near zero, with a few high-ranking ones. Now we are going to pick the features we assume to be important by directly selecting a threshold empirically chosen for its convenience:</p><div><pre class="programlisting">  In: variable_filter = selector.scores_ &gt; 10
  print ("Number of filtered variables: %i" % \ np.sum(variable_filter))
  from sklearn.preprocessing import PolynomialFeatures
  interactions = PolynomialFeatures(degree=2, interaction_only=True)
  Xs = interactions.fit_transform(Xt[:,variable_filter])
  print ("Number of variables and interactions: %i" % Xs.shape[1])

Out:   Number of filtered variables: 13
Number of variables and interactions: 92</pre></div><p class="calibre8">Now, we <a id="id502" class="calibre1"/>have reduced our dataset to just the <a id="id503" class="calibre1"/>core features. At this point, it does make sense to test a polynomial expansion and try to automatically catch any relevant non-linear relationship in our model:</p><div><pre class="programlisting">In: logit.fit(Xs,yt)
Xvs = interactions.fit_transform(Xv[:,variable_filter])
 print ('Validation area Under the Curve ' + \
        'before recursive \ selection:   %0.3f' % \
        roc_auc_score(yv,logit.predict_proba(Xvs)[:,1]))

Out:   Validation area Under the Curve before 
recursive selection: 0.808</pre></div><p class="calibre8">The resulting validation score (out-sample) is about 0.81, a very promising value given our initial overfitted score of 0.82 on the training set. Of course, we can decide to stop here or try to go on filtering even the polynomial expansion; feature selection is really a never-ending job, though after a certain point you have to realize that only slightly incremental results are possible from further tuning.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec71" class="calibre1"/>Recursive feature selection</h2></div></div></div><p class="calibre8">The only <a id="id504" class="calibre1"/>problem with univariate selection is that it will <a id="id505" class="calibre1"/>decide the best features by considering each feature separately from the others, not verifying how they work together in unison. Consequently, redundant variables are not infrequently picked (due to collinearity).</p><p class="calibre8">A multivariate approach, such as recursive elimination, can avoid this problem; however, it is more computationally expensive.</p><p class="calibre8">Recursive elimination works by starting with the full model and by trying to exclude each variable in turn, evaluating the removal effect by cross-validation estimation. If certain variables have a negligible effect on the model's performance, then the elimination algorithm just prunes them. The process stops when any further removal is proven to hurt the ability of the model to predict correctly.</p><p class="calibre8">Here is a<a id="id506" class="calibre1"/> demonstration of how <code class="email">RFECV</code>, Scikit-learn's implementation<a id="id507" class="calibre1"/> of recursive elimination, works. We will use the Boston dataset enhanced by second-degree polynomial expansion, thus working on a regression problem this time:</p><div><pre class="programlisting">In: from sklearn.feature_selection import RFECV
from sklearn.cross_validation import KFold
from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, test_size=0.30, random_state=1)
    
lm = LinearRegression()
cv_iterator = KFold(
    n=len(X_train), n_folds=10, shuffle=True, random_state=101)
recursive_selector = RFECV(estimator=lm, step=1, cv=cv_iterator, scoring='mean_squared_error')
recursive_selector.fit(second_order.fit_transform(X_train),
y_train)
print ('Initial number of features : %i' % 
       second_order.fit_transform(X_train).shape[1])
print ('Optimal number of features : %i' % 
       recursive_selector.n_features_)

Out:   Initial number of features : 105
Optimal number of features : 52</pre></div><p class="calibre8">Given an estimator (our model), a cross validation iterator, and an error measure, <code class="email">RFECV</code> will find out after a while that half of the features can be dropped from the model without fear of worsening its performance:</p><div><pre class="programlisting">In: essential_X_train = recursive_selector.transform(
    second_order.fit_transform(X_train))
essential_X_test  = recursive_selector.transform(
    second_order.fit_transform(X_test))
lm.fit(essential_X_train, y_train)
print ('cases = %i features = %i' % essential_X_test.shape)
print ('In-sample  mean squared error %0.3f' % \ 
mean_squared_error(y_train,lm.predict(essential_X_train)))
print ('Out-sample mean squared error %0.3f' % \
mean_squared_error(y_test,lm.predict(essential_X_test)))

Out:   cases = 152 features = 52
In-sample  mean squared error 7.834
Out-sample mean squared error 11.523</pre></div><p class="calibre8">A test-based check<a id="id508" class="calibre1"/> will reveal that now the out-sample<a id="id509" class="calibre1"/> performance is 11.5. For further confirmation, we can also run a cross-validation and obtain a similar result:</p><div><pre class="programlisting">In: edges = np.histogram(y, bins=5)[1]
binning = np.digitize(y, edges)
stratified_cv_iterator = StratifiedKFold(binning, n_folds=10, shuffle=True, random_state=101)
essential_X = recursive_selector.transform(
    second_order.fit_transform(X))
cv_score = cross_val_score(
    lm, essential_X, y, cv=stratified_cv_iterator, 
    scoring='mean_squared_error', n_jobs=1)
print ('Cv score: mean %0.3f std %0.3f' % (np.mean(np.abs(cv_score)), np.std(cv_score)))

Out: Cv score: mean 11.400 std 3.779</pre></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec37" class="calibre1"/>Regularization optimized by grid-search</h1></div></div></div><p class="calibre8">Regularization<a id="id510" class="calibre1"/> is another way to modify the role of variables in a regression model to prevent overfitting and to achieve simpler functional forms. The interesting aspect of this alternative approach is that it actually doesn't require manipulating your original dataset, making it suitable for systems that learn and predict online from large amounts of features and observations, without human intervention. Regularization works by enriching the learning process using a penalization for too complex models to shrink (or reduce to zero) coefficients relative to variables that are irrelevant for your prediction term or are redundant, as they are highly correlated with others present in the model (the collinearity problem seen before).</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec72" class="calibre1"/>Ridge (L2 regularization)</h2></div></div></div><p class="calibre8">The idea behind ridge regression is simple and straightforward: if the problem is the presence of many <a id="id511" class="calibre1"/>variables, which affect the regression <a id="id512" class="calibre1"/>model because of their coefficient, all we have to do is reduce their coefficient so their contribution is minimized and they do not influence the result so much.</p><p class="calibre8">Such a result is easily achieved by working out a different cost function. Working on the error in respect of the answer, the cost function can be balanced by imposing a penalization value depending on how large the coefficients are.</p><p class="calibre8">In the following formula, a reprisal of the formula in the <a class="calibre1" title="Chapter 2. Approaching Simple Linear Regression" href="part0018_split_000.html#H5A42-a2faae6898414df7b4ff4c9a487a20c6">Chapter 2</a>, <em class="calibre9">Approaching Simple Linear Regression</em> paragraph <em class="calibre9">Gradient descent at work</em>, the weight update is modified by the presence of a negative term, which is the square of the weight reduced by a factor expressed by lambda. Consequently, the larger the coefficient, the more it will be reduced during the <a id="id513" class="calibre1"/>update phase of the gradient descent<a id="id514" class="calibre1"/> optimization:</p><div><img src="img/00111.jpeg" alt="Ridge (L2 regularization)" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">In the preceding formula, each single coefficient <code class="email">j</code>, whose value is represented by <code class="email">w<sub class="calibre20">j</sub></code>, is updated by the gradient descent learning rate α <code class="email">/ n</code>, where <code class="email">n</code> is the number of observations. The learning rate is multiplied by the summed deviance of the prediction (the gradient). The novelty is the presence in the gradient of a penalization, calculated as the squared coefficient multiplied by a <code class="email">λ</code> lambda coefficient.</p><p class="calibre8">In this way, the error will be propagated to the coefficients only if there is an advantage (a large deviance in predictions), otherwise the coefficients will be reduced in value. The advantage is controlled by the <code class="email">λ</code> lambda value, which has to be found empirically according to the specific model that we are building.</p><p class="calibre8">An example will clarify how this new approach works. First, we have to use the <code class="email">Ridge</code> class from Scikit-learn, if our problem is a regression, or we use the penalty parameter in the <code class="email">LogisticRegression</code> specification (<code class="email">LogisticRegression(C=1.0, penalty='l2', tol=0.01)</code>):</p><div><pre class="programlisting">In: from sklearn.linear_model import Ridge
ridge = Ridge(normalize=True)
ridge.fit(second_order.fit_transform(X), y)
lm.fit(second_order.fit_transform(X), y)</pre></div><p class="calibre8">The impact of regularization on the model is controlled by the <code class="email">alpha</code> parameter in the <code class="email">Ridge</code>, and by the <code class="email">C</code> parameter in <code class="email">LogisticRegression</code>.</p><p class="calibre8">The smaller the value of <code class="email">alpha</code>, the less the coefficient values are controlled by the regularization, the higher its value with increased regularization, the more the coefficients are shrunk. Its functioning can be easily memorized as a shrinkage parameter: the higher the value, the higher the shrinkage of the complexity of the model. However, the C parameter in <code class="email">LogisticRegression</code> is exactly the inverse, with smaller values corresponding to high regularization (<em class="calibre9">alpha = 1 / 2C</em>).</p><p class="calibre8">After having completely fitted the model, we can have a look at how the values of coefficients are defined now:</p><div><pre class="programlisting">In: print ('Average coefficient: Non regularized = %0.3f Ridge = \
%0.3f' % (np.mean(lm.coef_), np.mean(ridge.coef_)))
print ('Min coefficient: Non regularized = %0.3f Ridge = %0.3f' \
% (np.min(lm.coef_), np.min(ridge.coef_)))
print ('Max coefficient: Non regularized = %0.3f Ridge = %0.3f' \
% (np.max(lm.coef_), np.max(ridge.coef_)))

Out:   Average coefficient: Non regularized = 1.376 Ridge = -0.027
Min coefficient: Non regularized = -40.040 Ridge = -2.013
Max coefficient: Non regularized = 142.329 Ridge = 1.181</pre></div><p class="calibre8">Now, the<a id="id515" class="calibre1"/> average coefficient value is almost near <a id="id516" class="calibre1"/>zero and the values are placed in a much shorter range than before. In the regularized form, no single coefficient has the weight to influence or, worse, disrupt a prediction.</p></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec73" class="calibre1"/>Grid search for optimal parameters</h2></div></div></div><p class="calibre8">Until now, we haven't had much to decide about the model itself, no matter whether we decided on a<a id="id517" class="calibre1"/> logistic or a linear<a id="id518" class="calibre1"/> regression. All that mattered was to properly transform our variables (and actually, we have learned that this is not an easy task either); however, the introduction of the L2 parameter brings forth much more complexity since we also have to heuristically set a value to maximize the performance of the model.</p><p class="calibre8">Keeping on working with cross-validation, which ensures we evaluate the performance of our model in a realistic way, a good solution to this problem is to check systematically the result of our model given a range of possible values of our parameter.</p><p class="calibre8">The <code class="email">GridSearchCV</code> class in the Scikit-learn package can be set using our preferred <code class="email">cv</code> iterator and scoring after setting a dictionary explicating what parameters have to be changed in the model (the key) and a range of values to be evaluated (a list of values related to the key), finally assigning it to the <code class="email">param_grid</code> parameter of the class:</p><div><pre class="programlisting">In: from sklearn.grid_search import GridSearchCV
edges = np.histogram(y, bins=5)[1]
binning = np.digitize(y, edges)
stratified_cv_iterator = StratifiedKFold(
    binning, n_folds=10,shuffle=True, random_state=101)
search = GridSearchCV(
    param_grid={'alpha':np.logspace(-4,2,7)},
    estimator=ridge, scoring ='mean_squared_error', 
    n_jobs=1, refit=True, cv=stratified_cv_iterator)
search.fit(second_order.fit_transform(X), y)
print ('Best alpha: %0.5f' % search.best_params_['alpha'])
print ('Best CV mean squared error: %0.3f' % np.abs(
        search.best_score_))

Out:  Best alpha: 0.00100
Best CV mean squared error: 11.883</pre></div><p class="calibre8">The result of the search, which can take some time when there are many possible model variations to test, can be explored using the attribute <code class="email">grid_scores_</code>:</p><div><pre class="programlisting">In: search.grid_scores_
Out: 
[mean: -12.45899, std: 5.32834, params: {'alpha': 0.0001},
 mean: -11.88307, std: 4.92960, params: {'alpha': 0.001},
 mean: -12.64747, std: 4.66278, params: {'alpha': 0.01},
 mean: -16.83243, std: 5.28501, params: {'alpha': 0.1},
 mean: -22.91860, std: 5.95064, params: {'alpha': 1.0},
 mean: -37.81253, std: 8.63064, params: {'alpha': 10.0},
 mean: -66.65745, std: 10.35740, params: {'alpha': 100.0}]</pre></div><p class="calibre8">The maximum<a id="id519" class="calibre1"/> scoring value (actually using <a id="id520" class="calibre1"/>RMSE we should minimize the result, so the grid search works with the negative value of RMSE) is achieved when alpha is <code class="email">0.001</code>. In addition, the standard deviation of the cross-validation score is minimal in respect of our possible solutions, confirming to us that it is the best solution available at hand.</p><div><h3 class="title2"><a id="tip35" class="calibre1"/>Tip</h3><p class="calibre8">If you would like to further optimize the results, just explore, using a second grid search, the range of values around the winning solution—that is, in our specific case from <code class="email">0.0001</code> to <code class="email">0.01</code>, you may find a slightly better value in terms of expected results or stability of the solution (expressed by the standard deviation).</p></div><p class="calibre8">Naturally, <code class="email">GridSearchCV</code> can be used effectively when more parameters to be optimized are involved. Please be aware that the more parameters, the more trials have to be made, and the resulting number is a combination—that is, a multiplication—of all the possible values to be tested. Consequently, if you are testing four values of a hypermeter and four of another one, in the end you will need <em class="calibre9">4 × 4</em> trials and, depending on the cross-validation folds, let's say in our case 10, you'll have your CPU compute <em class="calibre9">4 × 4 × 10 = 160</em> models. Searches that are more complex may even involve testing thousands of models, and although <code class="email">GridSearchCV</code> can parallelize all its computations, in certain cases it can still be a problem. We are going to address a possible solution in the next paragraph.</p><div><h3 class="title2"><a id="tip36" class="calibre1"/>Tip</h3><p class="calibre8">We have illustrated how to grid-search using the more general <code class="email">GridSearchCV</code>. There is anyway a specialized function for automatically creating out-of-the-box a cross-validated optimized ridge regression using Scikit-learn: <code class="email">RidgeCV</code>. There are automated classes also for the other regularization variants we are going to illustrate, <code class="email">LassoCV</code> and <code class="email">ElasticNetCV</code>. Actually, these classes, apart from being more synthetic than the approach we described, are much faster in finding<a id="id521" class="calibre1"/> the best parameter<a id="id522" class="calibre1"/> because they follow an optimization path (so they actually do not exhaustively search along the grid).</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec74" class="calibre1"/>Random grid search</h2></div></div></div><p class="calibre8">Searching for good combinations of hyper-parameters in a grid is a really time-consuming task, especially if there are many parameters; the number of combinations can really explode and<a id="id523" class="calibre1"/> thus your CPU can take a long time to<a id="id524" class="calibre1"/> compute the results.</p><p class="calibre8">Moreover, it is often the case that not all hyper-parameters are important; in such a case, when grid-searching, you are really wasting time checking on a large number of solutions that aren't really distinguishable from one another, while instead omitting to check important values on critical parameters.</p><p class="calibre8">The solution is a random grid search, which is not only much speedier than the grid search, but it is also much more efficient, as pointed out in a paper by the scholars James Bergstra and <a id="id525" class="calibre1"/>Yoshua Bengio (<a class="calibre1" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf</a>).</p><p class="calibre8">Random search works by sampling possible parameters from ranges or distribution that you point out (the <code class="email">NumPy</code> package has quite a lot of distributions that can be used, but for this test we found that <code class="email">logspace</code> function is ideal for systematically exploring the L1/L2 range). Given a certain number of trials, there is a high chance that you can get the right hyper-parameters.</p><p class="calibre8">Here, we try using just <code class="email">10</code> values sampled from <code class="email">100</code> possible ones (so reducing our running time to <code class="email">1/10</code> in respect of a grid search):</p><div><pre class="programlisting">In: from sklearn.grid_search import RandomizedSearchCV
from scipy.stats import expon
np.random.seed(101)
search_func=RandomizedSearchCV(
    estimator=ridge, n_jobs=1, iid=False, refit=True, n_iter=10,
    param_distributions={'alpha':np.logspace(-4,2,100)}, 
    scoring='mean_squared_error', cv=stratified_cv_iterator)

search_func.fit(second_order.fit_transform(X), y)
print ('Best alpha: %0.5f' % search_func.best_params_['alpha'])
print ('Best CV mean squared error: %0.3f' % np.abs(
        search_func.best_score_))

Out:  Best alpha: 0.00046
Best CV mean squared error: 11.790</pre></div><div><h3 class="title2"><a id="tip37" class="calibre1"/>Tip</h3><p class="calibre8">As a heuristic, the number of trials of a random search depends on the number of possible combinations that may be tried under a grid search. As a matter of statistical<a id="id526" class="calibre1"/> probability, it has been empirically <a id="id527" class="calibre1"/>observed that the most efficient number of random trials should be between 30 and 60. More than 60 random trials is unlikely to bring many more performance improvements from tuning hyper parameters than previously assessed.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_4"><a id="ch06lvl2sec75" class="calibre1"/>Lasso (L1 regularization)</h2></div></div></div><p class="calibre8">Ridge regression is not really a selection method. Penalizing the useless coefficients through keeping them all in the model won't provide much clarity about what variables work the best in<a id="id528" class="calibre1"/> your linear regression and won't<a id="id529" class="calibre1"/> improve its comprehensibility.</p><p class="calibre8">The lasso regularization, a recent addition by Rob Tibshirani, using the absolute value instead of the quadratic one in the regularization penalization, does help to shrink many coefficient values to zero, thus making your vector of resulting coefficients sparse:</p><div><img src="img/00112.jpeg" alt="Lasso (L1 regularization)" class="calibre10"/></div><p class="calibre11"> </p><p class="calibre8">Again, we have a formula similar to the previous one for L2 regularization but now the penalization term is made up of <code class="email">λ</code> lambda multiplied by the absolute value of the coefficient.</p><p class="calibre8">The procedure is the same as in the ridge regression; you just have to use a different class called <code class="email">Lasso</code>. If instead your problem is a classification one, in your logistic regression you just have to specify that the parameter <code class="email">penalty</code> is <code class="email">'l1'</code>:</p><div><pre class="programlisting">In: from sklearn.linear_model import Lasso
lasso = Lasso(alpha=1.0, normalize=True, max_iter=10**5)
#The following comment shows an example of L1 logistic regression
#lr_l1 = LogisticRegression(C=1.0, penalty='l1', tol=0.01)</pre></div><p class="calibre8">Let's check what happens to the previously seen regularization of the linear regression on the Boston dataset when using <code class="email">Lasso</code>:</p><div><pre class="programlisting">In: from sklearn.grid_search import RandomizedSearchCV
from scipy.stats import expon
np.random.seed(101)
search_func=RandomizedSearchCV(
    estimator=lasso, n_jobs=1, iid=False, refit=True, n_iter=15,
    param_distributions={'alpha':np.logspace(-5,2,100)}, 
    scoring='mean_squared_error', cv=stratified_cv_iterator)

search_func.fit(second_order.fit_transform(X), y)
print ('Best alpha: %0.5f' % search_func.best_params_['alpha'])
print ('Best CV mean squared error: %0.3f' % np.abs(
        search_func.best_score_))

Out:  Best alpha: 0.00006
Best CV mean squared error: 12.235</pre></div><p class="calibre8">From the <a id="id530" class="calibre1"/>viewpoint of performance, we obtained a slightly<a id="id531" class="calibre1"/> worse but comparable mean squared error value.</p><div><h3 class="title2"><a id="tip38" class="calibre1"/>Tip</h3><p class="calibre8">You will have noticed that using the <code class="email">Lasso</code> regularization takes more time (there are usually more iterations) than applying the ridge one. A good strategy for speeding up things is to apply the lasso only on a subset of the data (which should take less time), find out the best alpha, and then apply it directly to your complete sample to verify whether the performance results are consistent.</p></div><p class="calibre8">However, what is most interesting is evaluating what coefficients have been reduced to zero:</p><div><pre class="programlisting">In: print ('Zero value coefficients: %i out of %i' % \
(np.sum(~(search_func.best_estimator_.coef_==0.0)),
len(search_func.best_estimator_.coef_)))

Out:  Zero value coefficients: 85 out of 105</pre></div><p class="calibre8">Now, our second-degree polynomial expansion has been reduced to just <code class="email">20</code> working variables, as if the model has been reduced by a recursive selection, with the advantage that you don't have to change the dataset structure; you just apply your data to the model and only the right variables will work out the prediction for you.</p><div><h3 class="title2"><a id="tip39" class="calibre1"/>Tip</h3><p class="calibre8">If you are wondering what kind of regularization to use first, <code class="email">ridge</code> or <code class="email">lasso</code>, a good rule of thumb is to first run a linear regression without any regularization and check the distribution of the standardized coefficients. If there are many with similar values, then <code class="email">ridge</code> is the best choice; if instead you notice that there are a few important coefficients and many lesser ones, using <code class="email">lasso</code> is advisable to remove the unimportant ones. In any case, when you have more variables than observations, you should always use <code class="email">lasso</code>.</p></div></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_5"><a id="ch06lvl2sec76" class="calibre1"/>Elastic net</h2></div></div></div><p class="calibre8">Lasso can rapidly and without much hassle reduce the number of working variables in a prediction model, rendering it simpler and much more generalizable. Its strategy is simple: it aims to retain only the variables that contribute to the solution. Consequently, if, by chance, among your features you have a couple of strongly collinear variables, an L1 regularization <a id="id532" class="calibre1"/>will keep just one of them, on the basis of the<a id="id533" class="calibre1"/> characteristics of the data itself (noise and correlation with other variables contribute to the choice).</p><p class="calibre8">Such a characteristic anyway may prove undesirable because of the instability of the L1 solution (the noise and strength of correlations may change with the data) since having all the correlated variables in the model guarantees a more reliable model (especially if they all depend on a factor that is not included into the model). Thus, the alternative elastic net approach has been devised by combining the effects of L1 and L2 regularization.</p><p class="calibre8">In elastic net (Scikit-learn's <code class="email">ElasticNet</code> class), you always have an <code class="email">alpha</code> parameter that controls the impact of regularization on the determination of the model's coefficients, plus a <code class="email">l1_ratio</code> parameter that helps weight the combination between the L1 and L2 parts of the regularization part of the cost function. When the parameter is <code class="email">0.0</code>, there is no role for L1 so it is equivalent to a ridge. When it is set to <code class="email">1.0</code>, you have a lasso regression. Intermediate values act by mixing the effects of both types of regularizations; thus, while some variables will still be reduced to zero value coefficients, collinear variables will be reduced to the same coefficient, allowing them all to be still present in the model formulation.</p><p class="calibre8">In the following example, we try solving our model with elastic net regularization:</p><div><pre class="programlisting">In: from sklearn.linear_model import ElasticNet
elasticnet = ElasticNet(alpha=1.0, l1_ratio=0.15, normalize=True, max_iter=10**6, random_state=101)
  from sklearn.grid_search import RandomizedSearchCV
  from scipy.stats import expon
  np.random.seed(101)
  search_func=RandomizedSearchCV(estimator=elasticnet, param_distributions={'alpha':np.logspace(-5,2,100), 'l1_ratio':np.arange(0.0, 1.01, 0.05)}, n_iter=30, scoring='mean_squared_error', n_jobs=1, iid=False, 
refit=True, cv=stratified_cv_iterator)
search_func.fit(second_order.fit_transform(X), y)
print ('Best alpha: %0.5f' % search_func.best_params_['alpha'])
print ('Best l1_ratio: %0.5f' % \ search_func.best_params_['l1_ratio'])
print ('Best CV mean squared error: %0.3f' % \ np.abs(search_func.best_score_))

Out:  Best alpha: 0.00002
Best l1_ratio: 0.60000
Best CV mean squared error: 11.900</pre></div><p class="calibre8">By introspecting the solution, we realize that this is achieved by excluding a larger number <a id="id534" class="calibre1"/>of variables than a pure L1 solution; however, the resulting<a id="id535" class="calibre1"/> performance is similar to a L2 solution:</p><div><pre class="programlisting">In: print ('Zero value coefficients: %i out of %i' % (np.sum(~(search_func.best_estimator_.coef_==0.0)), len(search_func.best_estimator_.coef_))) 

Out:   Zero value coefficients: 102 out of 105</pre></div></div></div>

<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec38" class="calibre1"/>Stability selection</h1></div></div></div><p class="calibre8">As presented, L1-penalty offers the advantage of rendering your coefficients' estimates sparse, and<a id="id536" class="calibre1"/> effectively it acts as a variable selector since it tends to leave only essential variables in the model. On the other hand, the selection itself tends to be unstable when data changes and it requires a certain effort to correctly tune the C parameter to make the selection most effective. As we have seen while discussing elastic net, the peculiarity resides in the behavior of Lasso when there are two highly correlated variables; depending on the structure of the data (noise and correlation with other variables), L1 regularization will choose just one of the two.</p><p class="calibre8">In the field of studies related to bioinformatics (DNA, molecular studies), it is common to work with a large number of variables based on a few observations. Typically, such problems are denominated p &gt;&gt; n (features are much more numerous than cases) and they present the necessity to select what features to use for modeling. Because the variables are numerous and also are quite correlated among themselves, resorting to variable selection, whether by greedy selection or L1-penalty, can lead to more than one outcome taken from quite a large range of possible solutions. Two scholars, Nicolai Meinshausen and Peter Buhlmann, respectively from the University of Oxford and ETH Zurich, have come up with the idea of trying to leverage this instability and turn it into a surer selection.</p><p class="calibre8">Their idea is straightforward: since L1-penalty is influenced by the cases and variables present in the dataset to choose a certain variable over others in the case of multicollinearity, we can subsample the cases and the variables to involve and fit with them a L1-penalized model repetitively. Then, for each run, we can record the features that got a zero coefficient and the one that didn't. By pooling these multiple results, we can calculate a frequency statistic of how many times each feature got a non-zero value. In such a fashion, even if the results are unstable and uncertain, the most informative features will score a non-zero coefficient more often than less informative ones. In the end, a threshold can help to exactly retain the important variables and discard the unimportant ones and the collinear, but not so relevant, ones.</p><div><h3 class="title2"><a id="tip40" class="calibre1"/>Tip</h3><p class="calibre8">The scoring can also be interpreted as a ranking of each variable's role in the model.</p></div><p class="calibre8">Scikit-learn offers two implementations of stability selection: <code class="email">RandomizedLogisticRegression</code> for classification tasks and <code class="email">RandomizedLasso</code> as a regressor. They are both in the <code class="email">linear_model</code> module.</p><p class="calibre8">They also both <a id="id537" class="calibre1"/>share the same key hyper-parameters:</p><div><ul class="itemizedlist"><li class="listitem"><code class="email">C</code> : is the regularization parameter, by default set to <code class="email">1.0</code>. If you can manage to find a good C on all the data by cross-validation, put that figure in the parameter. Otherwise, start confidently using the default value; it is a good compromise.</li><li class="listitem"><code class="email">scaling</code> : is the percentage of feature to be kept at every iteration, the default value of <code class="email">0.5</code> is a good figure; lower the number if there are many redundant variables in your data.</li><li class="listitem"><code class="email">sample_fraction</code> : is the percentage of observations to be kept; the default value of <code class="email">0.75</code> should be decreased if you suspect outliers in your data (so they will less likely be drawn).</li><li class="listitem"><code class="email">n_resampling</code> : the number of iterations; the more the better, but 200-300 resamples should bear good results.</li></ul></div></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec77" class="calibre1"/>Experimenting with the Madelon</h2></div></div></div><p class="calibre8">From our <a id="id538" class="calibre1"/>past experimentations, stability selection does help to quickly fix any problem inherent to variable selection, even when dealing with sparse variables such as textual data rendered into indicator variables.</p><p class="calibre8">To demonstrate its effectiveness, we are going to apply it to the Madelon dataset, trying to get a better AUC score after stability selection:</p><div><pre class="programlisting">In: from sklearn.cross_validation import cross_val_score
  from sklearn.linear_model import RandomizedLogisticRegression
  from sklearn.preprocessing import PolynomialFeatures
  from sklearn.pipeline import make_pipeline
  threshold = 0.03 # empirically found value
  stability_selection = RandomizedLogisticRegression(n_resampling=300, n_jobs=1,
    random_state=101, scaling=0.15, 
    sample_fraction=0.50, selection_threshold=threshold)
  interactions = PolynomialFeatures(degree=4, interaction_only=True)
  model = make_pipeline(stability_selection, interactions, logit)
  model.fit(Xt,yt)</pre></div><p class="calibre8">Since it is a classification problem, we are going to use the <code class="email">RandomizedLogisticRegression</code> class, setting <code class="email">300</code> resamples and subsampling 15% of variables and 50% of observations. As a threshold, we are going to retain all those features that appear significant in the model at least 3% of the time. Such settings are quite strict, but they are due to the presence of high redundancy in the dataset and extreme instability of L1 solutions.</p><p class="calibre8">Fitting the<a id="id539" class="calibre1"/> solution using a <code class="email">make_pipeline</code> command allows us to create a sequence of actions to be first fitted and used on training data and then reapplied, using the same configuration, to the validation data. The idea is to first select the important and relevant features based on stability selection and then to create interactions (just multiplicative terms) using polynomial expansion to catch the non-linear components in the data with new derived features. If we were to create polynomial expansion without first selecting which variables we should use, then our dataset would exponentially grow in the number of variables and it could prove impossible even to store it in-memory.</p><p class="calibre8"><code class="email">RandomizedLogisticRegression</code> acts more as a pre-processing filter than a predictive model: after fitting, though allowing us to have a glance at the produced scores, it won't allow any prediction on the basis of the host of created models, but it will allow us to transform any dataset similar to ours (the same number of columns), keeping only the columns whose score is above the threshold that we initially defined when we instantiated the class.</p><p class="calibre8">In our case, after having the resamples run, and it may take some time, we can try to figure out how many variables have been retained by the model:</p><div><pre class="programlisting">In: print ('Number of features picked by stability selection: %i' % \ np.sum(model.steps[0][1].all_scores_ &gt;= threshold))

Out: Number of features picked by stability selection: 19</pre></div><p class="calibre8">Here, 19 variables constitute a small set, which can be expanded into four-way interactions of the type <code class="email">var1 × var2 × var3 × var4</code>, allowing us to better map the unknown transformations at the origin of the Madelon dataset.</p><div><pre class="programlisting">In: from sklearn.metrics import roc_auc_score
print ('Area Under the Curve: %0.3f' % roc_auc_score(
            yv,model.predict_proba(Xv)[:,1]))

Out: Area Under the Curve: 0.885</pre></div><p class="calibre8">A final test <a id="id540" class="calibre1"/>on the obtained probability estimates reveals to us that we reached an AUC value of <code class="email">0.885</code>, a fairly good improvement from the initial <code class="email">0.602</code> baseline.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec39" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">During this chapter, we have covered quite a lot of ground, finally exploring the most experimental and scientific part of the task of modeling linear regression or classification models.</p><p class="calibre8">Starting with the topic of generalization, we explained what can go wrong in a model and why it is always important to check the true performances of your work by train/test splits and by bootstraps and cross-validation (though we recommend using the latter more for validation work than general evaluation itself).</p><p class="calibre8">Model complexity as a source of variance in the estimate gave us the occasion to introduce variable selection, first by greedy selection of features, univariate or multivariate, then using regularization techniques, such as Ridge, Lasso and Elastic Net.</p><p class="calibre8">Finally, we demonstrated a powerful application of Lasso, called stability selection, which, in the light of our experience, we recommend you try for many feature selection problems.</p><p class="calibre8">In the next chapter, we will deal with the problem of incrementally growing datasets, proposing solutions that may work well even if your problem is that of datasets too large to easily and timely fit into the memory of your working computer.</p></div></body></html>