<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer084">
<h1 class="chapter-number" id="_idParaDest-144"><a id="_idTextAnchor146"/>9</h1>
<h1 id="_idParaDest-145"><a id="_idTextAnchor147"/>LightGBM MLOps with AWS SageMaker</h1>
<p>In <a href="B16690_08.xhtml#_idTextAnchor134"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Machine Learning Pipelines and MLOps with LightGBM</em>, we built an end-to-end ML pipeline using scikit-learn. We also looked at encapsulating the pipeline within a REST API and deployed our API to <span class="No-Break">the cloud.</span></p>
<p>This chapter<a id="_idIndexMarker572"/> will look at developing and deploying a pipeline using <strong class="bold">Amazon SageMaker</strong>. SageMaker is a complete set of production services for developing, hosting, monitoring, and<a id="_idIndexMarker573"/> maintaining ML solutions provided by <strong class="bold">Amazon Web </strong><span class="No-Break"><strong class="bold">Services</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AWS</strong></span><span class="No-Break">).</span></p>
<p>We’ll expand our capabilities with ML pipelines by looking at advanced topics such as detecting bias in a trained model and automating deployment to fully scalable, serverless <span class="No-Break">web endpoints.</span></p>
<p>The following main topics will be covered in <span class="No-Break">this chapter:</span></p>
<ul>
<li>An introduction to AWS <span class="No-Break">and SageMaker</span></li>
<li>Model explainability <span class="No-Break">and bias</span></li>
<li>Building an end-to-end pipeline <span class="No-Break">with SageMaker</span></li>
</ul>
<h1 id="_idParaDest-146"><a id="_idTextAnchor148"/>Technical requirements</h1>
<p>This chapter dives deep into building ML models and pipelines using Amazon SageMaker. You need access to an Amazon account, and you must also configure a payment method. Note that running the example code for this chapter will incur costs on AWS. The complete notebooks and scripts for this chapter are available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-9"><span class="No-Break">https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-9</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor149"/>An introduction to AWS and SageMaker</h1>
<p>This section provides a high-level overview of AWS and delves into SageMaker, AWS’ <span class="No-Break">ML offering.</span></p>
<h2 id="_idParaDest-148"><a id="_idTextAnchor150"/>AWS</h2>
<p>AWS is one of the leading<a id="_idIndexMarker574"/> players in the global cloud computing marketplace. AWS<a id="_idIndexMarker575"/> offers many cloud-based products and services, including databases, <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), analytics, networking, storage, developer tools, and enterprise applications. The idea behind AWS is to offer businesses an affordable and scalable solution to their computing needs, regardless of their size <span class="No-Break">or industry.</span></p>
<p>A key advantage of AWS is elasticity, meaning servers and services can be stopped and started quickly and at will, scaling from zero machines to thousands. The elasticity of the services goes hand in hand with its primary pricing model of pay-as-you-go, meaning customers only pay for the services and resources they use without any upfront costs or long-term contracts. This elasticity and pricing allow businesses to scale computing needs as needed, on an ad hoc and granular level, and then only pay for what they use. This approach has transformed how businesses scale IT resources and applications, enabling them to react quickly to changing business needs without incurring the heavy costs traditionally associated with hardware and software procurement <span class="No-Break">and maintenance.</span></p>
<p>Another advantage is the global reach of AWS. AWS services are available in many regions across the globe. Regions are geographically separated, and each region is further divided into availability zones. The region-zone setup allows users to create globally distributed and redundant infrastructure to maximize resilience and architect for disaster recovery. The regional data centers also allow users to create servers and services close to end users, <span class="No-Break">minimizing latency.</span></p>
<h3>Core services</h3>
<p>The core AWS services<a id="_idIndexMarker576"/> provide computing, networking, and storage capability. AWS’s compute services include <strong class="bold">Amazon Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>), which offers configurable virtual machines to customers, and <strong class="bold">AWS Lambda</strong>, a serverless compute platform that allows<a id="_idIndexMarker577"/> you to run code without the need to provision and manage<a id="_idIndexMarker578"/> servers. In ML, both EC2 instances and Lambda functions are often used to train and validate or serve models via API endpoints. The elastic nature of EC2 servers allows ML engineers to scale up training servers to many thousands, which can significantly speed up training or <span class="No-Break">parameter-tuning tasks.</span></p>
<p>AWS’s storage and database services, such as <strong class="bold">Amazon Simple Storage Service</strong> (<strong class="bold">S3</strong>) and <strong class="bold">Amazon RDS</strong> (<strong class="bold">Relational Database Service</strong>), offer reliable, scalable, and secure data storage solutions. These services<a id="_idIndexMarker579"/> manage storage infrastructure <a id="_idIndexMarker580"/>and offer high-level features such as backups, patch management, and vertical and horizontal scaling. S3 is a widely used service for data engineering and ML. S3 offers low-cost, highly redundant secure storage that scales <span class="No-Break">beyond exabytes.</span></p>
<p>AWS also offers data warehousing solutions with <strong class="bold">Amazon Redshift</strong>. Large enterprises frequently use Redshift<a id="_idIndexMarker581"/> as a warehouse or the basis of a data lake, meaning it’s often a data source for <span class="No-Break">ML solutions.</span></p>
<p>AWS also offers networking<a id="_idIndexMarker582"/> services to help businesses meet complex networking and isolation needs. <strong class="bold">AWS Direct Connect</strong> allows customers to set up a dedicated network connection from a customer’s site to the AWS cloud. Routing and name servers<a id="_idIndexMarker583"/> can be managed with Amazon Route 53, a flexible and scalable <strong class="bold">Domain Name System</strong> (<span class="No-Break"><strong class="bold">DNS</strong></span><span class="No-Break">) service.</span></p>
<p>However, chief among the network services is <strong class="bold">Amazon Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>). VPCs offer customers<a id="_idIndexMarker584"/> the ability to configure completely isolated virtual networks. Customers can granularly configure subnetworks, routing tables, address ranges, gateways, and security groups. VPCs allow users to isolate their environment and cloud resources and control<a id="_idIndexMarker585"/> inbound and outbound traffic for increased security <span class="No-Break">and privacy.</span></p>
<h3>Security</h3>
<p>A critical piece<a id="_idIndexMarker586"/> of any infrastructure equation is security. In terms of security, AWS provides a highly secure, scalable, and flexible cloud computing environment. AWS’s security services, including <strong class="bold">AWS Identity and Access Management</strong> (<strong class="bold">IAM</strong>) and <strong class="bold">Amazon Security Hub</strong>, help customers protect their data<a id="_idIndexMarker587"/> and applications by implementing robust <span class="No-Break">security measures.</span></p>
<p>AWS also complies with multiple international and industry-specific compliance standards, such as GDPR, HIPAA, and ISO 27001. Further, in terms of data governance, AWS makes it easy to comply with data residency and privacy requirements. Due to the regional structure of AWS, data can remain resident in specific countries, while engineers have access to the full <a id="_idIndexMarker588"/>suite of <span class="No-Break">AWS services.</span></p>
<h3>Machine learning</h3>
<p>AWS also offers<a id="_idIndexMarker589"/> services<a id="_idIndexMarker590"/> focused on ML and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>). Among these are many<a id="_idIndexMarker591"/> fully managed services for specific ML tasks. <strong class="bold">AWS Comprehend</strong> offers many <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) services, such as document<a id="_idIndexMarker592"/> processing, named entity recognition, and sentiment analysis. <strong class="bold">Amazon Lookout</strong> is a service for anomaly detection<a id="_idIndexMarker593"/> in equipment, metrics, or images. Further, <strong class="bold">Amazon Rekognition</strong> offers services for machine vision use cases<a id="_idIndexMarker594"/> such as image classification and <span class="No-Break">facial recognition.</span></p>
<p>Of particular interest to us is <strong class="bold">Amazon SageMaker</strong>, a complete ML platform that allows us to create, train, and deploy ML models in the Amazon cloud. The following section discusses SageMaker <span class="No-Break">in detail.</span></p>
<h2 id="_idParaDest-149"><a id="_idTextAnchor151"/>SageMaker</h2>
<p><strong class="bold">Amazon SageMaker</strong> is an end-to-end ML platform<a id="_idIndexMarker595"/> that allows data scientists to work with data and develop, train, deploy, and monitor ML models. SageMaker is fully managed, so there is no need to provision or <span class="No-Break">manage servers.</span></p>
<p>The primary appeal of Amazon SageMaker lies in its comprehensive nature as a platform. It encompasses all aspects of the ML process, including data labeling, model building, training, tuning, deployment, management, and monitoring. By taking care of these aspects, SageMaker allows developers and data scientists to focus on the core ML tasks instead of managing <span class="No-Break">the infrastructure.</span></p>
<p>As we have discussed, the ML life cycle<a id="_idIndexMarker596"/> starts with data gathering, which often requires manual data labeling. For this, SageMaker provides a service called <strong class="bold">SageMaker Ground Truth</strong>. This service makes it easy to annotate ML datasets efficiently. It can significantly reduce the time and costs typically associated with data labeling by using automated labeling<a id="_idIndexMarker597"/> workflows, and it also offers a workforce for manual data labeling tasks. Further, SageMaker also provides the <strong class="bold">Data Wrangler</strong> service, which helps with data preparation and <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>). Data Wrangler provides functionality<a id="_idIndexMarker598"/> to query data from S3, Redshift, and other platforms and then cleanse, visualize, and understand the data from a single <span class="No-Break">visual interface.</span></p>
<p>SageMaker provides a fully managed service<a id="_idIndexMarker599"/> for the model training phase that can handle large-scale, distributed model training via <strong class="bold">Training Jobs</strong>. The service is designed to be flexible and adaptable, allowing users to optimize their ML models as needed. Users only need to specify the location of their data, typically S3 and the ML algorithm, and SageMaker takes care of the rest of the training process. The model training service fully leverages the elastic nature of the underlying AWS infrastructure: many servers can be created quickly to perform training jobs and discarded after training is complete to <span class="No-Break">save costs.</span></p>
<p>This paradigm also extends to hyperparameter tuning. To simplify hyperparameter optimization, SageMaker provides an automatic model-tuning feature. Many tuning algorithms are provided, such as Optuna or FLAML, and tuning can be run across <span class="No-Break">multiple servers.</span></p>
<p>SageMaker also has support for a more fully AutoML experience via <strong class="bold">SageMaker Autopilot</strong>. Autopilot is a service that enables <a id="_idIndexMarker600"/>automatic model creation. A user only needs to provide the raw data and set the target; then, Autopilot automatically explores different solutions to find the best model. Autopilot provides complete visibility into the process so that data scientists can understand how the model is created and make any <span class="No-Break">necessary adjustments.</span></p>
<p>Once a model<a id="_idIndexMarker601"/> has been trained and optimized, it must be deployed. SageMaker simplifies this process by providing a one-click deployment process. Users can quickly deploy their models to production with auto-scaling capabilities without worrying about the underlying infrastructure. This deployment autoscaling capability allows users to set metrics-based policies that increase or decrease backing servers. For instance, the deployment can be scaled up if the number of invocations within a period exceeds a specific threshold. SageMaker ensures the high availability of models and allows for A/B testing of models to compare different variants and decide on the best one. SageMaker also supports multi-model endpoints, allowing users to deploy multiple models on a <span class="No-Break">single endpoint.</span></p>
<p>Amazon SageMaker also provides capabilities<a id="_idIndexMarker602"/> to monitor the model’s performance and conduct analysis once deployed. <strong class="bold">SageMaker Model Monitor</strong> monitors the quality of deployed models continuously (for real-time endpoints) or in batches (for asynchronous jobs). Alerts can be defined to notify the user if metric thresholds are exceeded. Model Monitor can monitor data drift and model drift based on metrics such <span class="No-Break">as accuracy.</span></p>
<p>Finally, SageMaker is both a platform within AWS and a software SDK. The SDK is available in both Python and R. The SageMaker SDK provides a range of built-in algorithms and frameworks, including support for the most popular algorithms in the ML community, such as XGBoost, TensorFlow, PyTorch, and MXNet. It also supports a marketplace where users can choose from a vast collection of algorithm and model packages shared by AWS and other <span class="No-Break">SageMaker users.</span></p>
<p>A noteworthy part of SageMaker that simplifies<a id="_idIndexMarker603"/> one of the most important aspects of model development (bias and fairness) is <span class="No-Break"><strong class="bold">SageMaker Clarify</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor152"/>SageMaker Clarify</h2>
<p>Amazon SageMaker Clarify is a tool that provides greater transparency<a id="_idIndexMarker604"/> into ML models. SageMaker Clarify aims to assist in understanding how ML models make predictions, thereby enabling model explainability <span class="No-Break">and fairness.</span></p>
<p>One of the primary features of SageMaker Clarify is its capacity to provide model interpretability. It helps developers understand the relationships between the input data and the model’s predictions. The service generates feature attributions that show how each feature in the dataset influences predictions, which can be critical in many domains, especially those where it’s vital to understand the reasoning behind a model’s prediction. In addition to providing insight into individual predictions, SageMaker Clarify offers global explanatory capabilities. It measures the importance of input features on a model’s predictions in aggregate across the whole dataset. Feature impact analysis allows developers and data scientists to understand the overall behavior of a model, helping them interpret how different features drive model predictions on a <span class="No-Break">global level.</span></p>
<p>Further, Clarify can help identify potential bias in trained models. The service includes pre-training and post-training bias metrics that help us understand if a model favors certain groups unfairly. It’s best practice to check all new models for bias, but it is also imperative in regulated industries such as finance or healthcare, where biased predictions can have <span class="No-Break">severe consequences.</span></p>
<p>Clarify provides model interpretability by using an advanced technique known as <strong class="bold">SHapley Additive </strong><span class="No-Break"><strong class="bold">exPlanations</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SHAP</strong></span><span class="No-Break">).</span></p>
<h3>SHAP</h3>
<p>SHAP is a game theoretic approach<a id="_idIndexMarker605"/> to interpreting the output of any ML model [1]. SHAP aims to provide an understanding of the impact of individual features on a model’s <span class="No-Break">overall prediction.</span></p>
<p>Essentially, SHAP values assess the effect of a particular feature value by contrasting it with a baseline value for that feature, highlighting its contribution to the prediction. A SHAP value is a fair contribution allocation from each feature to the prediction for each instance. SHAP values are rooted in cooperative game theory, representing a solution to the <span class="No-Break">following question:</span></p>
<p>“<em class="italic">Given the difference a feature makes in predicting an outcome, what portion of that difference is attributable to </em><span class="No-Break"><em class="italic">each feature?</em></span><span class="No-Break">”</span></p>
<p>These values are calculated using the concept of Shapley values from game theory. A Shapley value determines the significance of a feature by contrasting a model’s predictions with the presence and absence of that feature. Yet, as the sequence in which a model encounters features can affect its prediction, Shapley values consider all possible orderings. Then, it assigns an importance value to a feature so that it equals the average marginal contribution of that feature across all <span class="No-Break">possible coalitions.</span></p>
<p>There are several advantages to using SHAP for model interpretation. First, it offers consistency in interpretation. If the contribution of a feature changes, the attributed importance of that feature <span class="No-Break">changes proportionally.</span></p>
<p>Secondly, SHAP guarantees local accuracy, which means the sum of the SHAP values for all features would equal the difference between the prediction and the average prediction for <span class="No-Break">the dataset.</span></p>
<p>A great way to visualize SHAP values is by using SHAP summary plots. These plots provide a bird’s-eye view of feature importance and what is driving it. They plot all the SHAP values for a feature on a graph for easy visualization. Each point on the graph represents a SHAP value for a feature and an instance. The position on the Y-axis is determined by the feature and on the X-axis by the <span class="No-Break">SHAP value:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt="Figure 9.1 – Local explanation example for the Census Income dataset. Bars indicate SHAP values or the relative importance of each feature in predicting this specific instance" height="1121" src="image/B16690_09_01.jpg" width="1239"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – Local explanation example for the Census Income dataset. Bars indicate SHAP values or the relative importance of each feature in predicting this specific instance</p>
<p>In the context of SageMaker Clarify, the service<a id="_idIndexMarker606"/> generates a set of SHAP values for each instance in your dataset when you run a clarification job. SageMaker Clarify can also provide global feature importance measures by aggregating SHAP values across the <span class="No-Break">entire dataset.</span></p>
<p>SHAP values can help you understand complex model behavior, highlight potential issues, and improve your model over time. For example, by examining SHAP values, you might discover that a specific feature has a more significant effect on your model’s predictions than expected, prompting you to explore why this <span class="No-Break">might happen.</span></p>
<p>In this section, we looked at AWS and, more specifically, what the AWS ML family of services, SageMaker, offers. The functionality available in SageMaker, such as model explainability, bias detection, and monitoring, are components we have yet to implement in our ML pipelines. In the next section, we’ll look at building a complete end-to-end LightGBM-based ML <a id="_idIndexMarker607"/>pipeline, including these crucial steps, <span class="No-Break">using SageMaker.</span></p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor153"/>Building a LightGBM ML pipeline with Amazon SageMaker</h1>
<p>The dataset we’ll use<a id="_idIndexMarker608"/> for our case study of building a SageMaker pipeline is the Census Income dataset from <span class="No-Break"><em class="italic">Chapter 4</em></span>, <em class="italic">Comparing LightGBM, XGBoost, and Deep Learning</em>. This dataset is also available as a SageMaker sample dataset, so it’s easy to work with on SageMaker if you are <span class="No-Break">getting started.</span></p>
<p>The pipeline we’ll build consists of the <span class="No-Break">following steps:</span></p>
<ol>
<li><span class="No-Break">Data preprocessing.</span></li>
<li>Model training <span class="No-Break">and tuning.</span></li>
<li><span class="No-Break">Model evaluation.</span></li>
<li>Bias and explainability checks <span class="No-Break">using Clarify.</span></li>
<li>Model registration <span class="No-Break">within SageMaker.</span></li>
<li>Model deployment using an <span class="No-Break">AWS Lambda.</span></li>
</ol>
<p>Here’s a graph showing the <span class="No-Break">complete pipeline:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="Figure 9.2 – SageMaker ML pipeline for Census Income classification" height="974" src="image/B16690_09_02.jpg" width="1192"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – SageMaker ML pipeline for Census Income classification</p>
<p>Our approach is to create<a id="_idIndexMarker609"/> the entire pipeline using a Jupyter Notebook running in SageMaker Studio. The sections that follow explain and go through the code for each pipeline step, starting with setting up the <span class="No-Break">SageMaker session.</span></p>
<h2 id="_idParaDest-152"><a id="_idTextAnchor154"/>Setting up a SageMaker session</h2>
<p>The following steps assume<a id="_idIndexMarker610"/> you have already created an AWS account and set up a SageMaker domain to get started. If not, the following documentation can be referenced<a id="_idIndexMarker611"/> to <span class="No-Break">do so:</span></p>
<ul>
<li><span class="No-Break">Prerequisites: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.xhtml"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.xhtml</span></a></li>
<li>Onboarding<a id="_idIndexMarker612"/> to a SageMaker <span class="No-Break">domain: </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.xhtml"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/gs-studio-onboard.xhtml</span></a></li>
</ul>
<p>We must initialize the SageMaker session and create S3, SageMaker, and SageMaker Runtime clients via <strong class="source-inline">boto3</strong> to <span class="No-Break">get started:</span></p>
<pre class="source-code">
sess = sagemaker.Session()
region = sess.boto_region_name
s3_client = boto3.client("s3", region_name=region)
sm_client = boto3.client("sagemaker", region_name=region)
sm_runtime_client = boto3.client("sagemaker-runtime")</pre>
<p>We will use Amazon S3 to store<a id="_idIndexMarker613"/> our training data, source code, and all data and artifacts created by the pipeline, such as the serialized model. Our data and artifacts are split into a read bucket and a separate write bucket. This is a standard best practice as it separates the concerns for <span class="No-Break">data storage.</span></p>
<p>SageMaker sessions have the concept of a default S3 bucket for the session. If no default bucket name is supplied, one is generated, and the bucket is created for you. Here, we’re grabbing a reference to the bucket. This is our output or write bucket. The read bucket is a bucket we’ve created previously that stores our <span class="No-Break">training data:</span></p>
<pre class="source-code">
write_bucket = sess.default_bucket()
write_prefix = "census-income-pipeline"
read_bucket = "sagemaker-data"
read_prefix = "census-income"</pre>
<p>The source code, configuration, and output of each of the steps in the pipeline are captured in folders within our S3 write bucket. It’s useful to create variables for each S3 URI to avoid errors when repeatedly referring to data, <span class="No-Break">like so:</span></p>
<pre class="source-code">
input_data_key = f"s3://{read_bucket}/{read_prefix}"
census_income_data_uri = f"{input_data_key}/census-income.csv"
output_data_uri = f"s3://{write_bucket}/{write_prefix}/"
scripts_uri = f"s3://{write_bucket}/{write_prefix}/scripts"</pre>
<p>SageMaker needs us to specify<a id="_idIndexMarker614"/> the compute instance types we want to use when running the jobs for training, processing, Clarify, and prediction. In our example, we’re using <strong class="source-inline">m5.large</strong> instances. Most EC2 instance types can be used with SageMaker. However, a few special instance types that support GPUs and deep learning frameworks are <span class="No-Break">also available:</span></p>
<pre class="source-code">
train_model_id, train_model_version, train_scope = "lightgbm-classification-model", "*", "training"
process_instance_type = "ml.m5.large"
train_instance_count = 1
train_instance_type = "ml.m5.large"
predictor_instance_count = 1
predictor_instance_type = "ml.m5.large"
clarify_instance_count = 1
clarify_instance_type = "ml.m5.large"</pre>
<p>SageMaker uses standard EC2 instances for training but runs specific Docker images on the instances to provide ML functionality. Amazon SageMaker provides many prebuilt Docker images for various ML frameworks <span class="No-Break">and stacks.</span></p>
<p>The SageMaker SDK also provides a function to search for images that are compatible with the instance type we need within the AWS region we are using. We can search for an image <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">retrieve</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
train_image_uri = retrieve(
    region="us-east-1",
    framework=None,
    model_id=train_model_id,
    model_version=train_model_version,
    image_scope=train_scope,
    instance_type=train_instance_type
)</pre>
<p>We specify <strong class="source-inline">None</strong> for the framework parameter as we manage the LightGBM <span class="No-Break">installation ourselves.</span></p>
<p>To parameterize our pipeline, we must define<a id="_idIndexMarker615"/> SageMaker workflow parameters from the <strong class="source-inline">sagemaker.workflow.parameters</strong> package. Wrappers are available for various <span class="No-Break">parameter types:</span></p>
<pre class="source-code">
train_instance_type_param = ParameterString(
    name="TrainingInstanceType",
    default_value=train_instance_type)
train_instance_count_param = ParameterInteger(
    name="TrainingInstanceCount",
    default_value=train_instance_count)
deploy_instance_type_param = ParameterString(
    name="DeployInstanceType",
    default_value=predictor_instance_type)
deploy_instance_count_param = ParameterInteger(
    name="DeployInstanceCount",
    default_value=predictor_instance_count)</pre>
<p>With our pipeline parameters, S3 data paths, and other configuration variables set, we can move on to creating <a id="_idIndexMarker616"/>our pipeline’s <span class="No-Break">preprocessing step.</span></p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor155"/>Preprocessing step</h2>
<p>Setting up our preprocessing step<a id="_idIndexMarker617"/> has two parts: creating a Python script that performs the preprocessing and creating a processor that is added to <span class="No-Break">the pipeline.</span></p>
<p>The script we’ll be using is a regular Python script with a main function. We’ll use scikit-learn to do our preprocessing. The preprocessing script hasn’t been entirely reproduced here but is available in our source code repository. Notably, when the pipeline executes the step, the data is retrieved from S3 and added to a local staging directory on the preprocessing instance. From here, we can read the data using standard <span class="No-Break">pandas tooling:</span></p>
<pre class="source-code">
local_dir = "/opt/ml/processing"
input_data_path = os.path.join("/opt/ml/processing/census-income", "census-income.csv")
logger.info("Reading claims data from {}".format(input_data_path))
df = pd.read_csv(input_data_path)</pre>
<p>Similarly, after processing is complete, we can write the results to a local directory, from which SageMaker retrieves it and uploads it <span class="No-Break">to S3:</span></p>
<pre class="source-code">
train_output_path = os.path.join(f"{local_dir}/train", "train.csv")
X_train.to_csv(train_output_path, index=False)</pre>
<p>With a preprocessing script defined, we need to upload it to S3 for the pipeline to be able to <span class="No-Break">use it:</span></p>
<pre class="source-code">
s3_client.upload_file(
    Filename="src/preprocessing.py", Bucket=write_bucket, Key=f"{write_prefix}/scripts/preprocessing.py"
)</pre>
<p>We can define the preprocessing step as follows. First, we must create an <span class="No-Break"><strong class="source-inline">SKLearnProcessor</strong></span><span class="No-Break"> instance:</span></p>
<pre class="source-code">
sklearn_processor = SKLearnProcessor(
    framework_version="0.23-1",
    role=sagemaker_role,
    instance_count=1,
    instance_type=process_instance_type,
    base_job_name=f"{base_job_name_prefix}-processing",
)</pre>
<p><strong class="source-inline">SKLearnProcessor</strong> handles the processing<a id="_idIndexMarker618"/> task for jobs that require scikit-learn. We specify the scikit-learn framework version and the instance type and count we <span class="No-Break">defined earlier.</span></p>
<p>The processor is then added to <strong class="source-inline">ProcessingStep</strong> for use in <span class="No-Break">the pipeline:</span></p>
<pre class="source-code">
process_step = ProcessingStep(
    name="DataProcessing",
    processor=sklearn_processor,
    inputs=[...],
    outputs=[...],
    job_arguments=[
        "--train-ratio", "0.8",
        "--validation-ratio", "0.1",
        "--test-ratio", "0.1"
    ],
    code=f"s3://{write_bucket}/{write_prefix}/scripts/preprocessing.py"
)</pre>
<p><strong class="source-inline">inputs</strong> and <strong class="source-inline">outputs</strong> are defined using the <strong class="source-inline">ProcessingInput</strong> and <strong class="source-inline">ProcessingOutput</strong> wrappers, as <span class="No-Break">shown here:</span></p>
<pre class="source-code">
inputs = [ ProcessingInput(source=bank_marketing_data_uri, destination="/opt/ml/processing/bank-marketing") ]
outputs = [ ProcessingOutput(destination=f"{processing_output_uri}/train_data", output_name="train_data",
                         source="/opt/ml/processing/train"), ... ]</pre>
<p><strong class="source-inline">ProcessingStep</strong> takes our scikit-learn processor and the inputs and outputs for the data. The <strong class="source-inline">ProcessingInput</strong> instances define the S3 source and local directory destination to facilitate copying the data (these are the same local directories our preprocessing script uses). Similarly, the <strong class="source-inline">ProcessingOutput</strong> instances take the local directory source and S3 destinations. We also set job arguments, which are passed to the preprocessing script as <span class="No-Break">CLI arguments.</span></p>
<p>Having set up the preprocessing<a id="_idIndexMarker619"/> step, we can move on <span class="No-Break">to training.</span></p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor156"/>Model training and tuning</h2>
<p>We define a training<a id="_idIndexMarker620"/> script in the same<a id="_idIndexMarker621"/> way as a preprocessing script: a Python script with a main function that uses our standard Python tools, such as scikit-learn, to train a LightGBM model. However, we also need to install the LightGBM <span class="No-Break">library itself.</span></p>
<p>An alternative to installing the library is building it into a Docker image and using it as our training image in SageMaker. This is the canonical way of managing environments in SageMaker. However, it entails significant work and includes the long-term need to maintain the image over time. Alternatively, if we only need to install a handful of dependencies, we can do that directly from our training script, as <span class="No-Break">shown here.</span></p>
<p>We must define a helper function to install packages and then use it to <span class="No-Break">install LightGBM:</span></p>
<pre class="source-code">
def install(package):
    subprocess.check_call([sys.executable, "-q", "-m", "pip", "install", package])
install("lightgbm")
import lightgbm as lgb</pre>
<p>This also has the advantage that we install the latest version (or a specific version) every time we <span class="No-Break">run training.</span></p>
<p>With the package installed, the rest<a id="_idIndexMarker622"/> of the training <a id="_idIndexMarker623"/>script trains a standard <strong class="source-inline">LGBMClassifier</strong> on the data prepared by the preprocessing step. We can set up or train data and parameters from the arguments to <span class="No-Break">the script:</span></p>
<pre class="source-code">
train_df = pd.read_csv(f"{args.train_data_dir}/train.csv")
val_df = pd.read_csv(f"{args.validation_data_dir}/validation.csv")
params = {
    "n_estimators": args.n_estimators,
    "learning_rate": args.learning_rate,
    "num_leaves": args.num_leaves,
    "max_bin": args.max_bin,
}</pre>
<p>Then, we must do standard scikit-learn cross-validation scoring, fit the model to the data, and output the training and <span class="No-Break">validation scores:</span></p>
<pre class="source-code">
X, y = prepare_data(train_df)
model = lgb.LGBMClassifier(**params)
scores = cross_val_score(model, X, y, scoring="f1_macro")
train_f1 = scores.mean()
model = model.fit(X, y)
X_test, y_test = prepare_data(val_df)
test_f1 = f1_score(y_test, model.predict(X_test))
print(f"[0]#011train-f1:{train_f1:.2f}")
print(f"[0]#011validation-f1:{test_f1:.2f}")</pre>
<p>As shown here, the script<a id="_idIndexMarker624"/> accepts CLI arguments<a id="_idIndexMarker625"/> to set hyperparameters. This is used by the hyperparameter tuning step to set parameters during the optimization phase. We can use Python’s <strong class="source-inline">ArgumentParser</strong> for <span class="No-Break">this purpose:</span></p>
<pre class="source-code">
parser = argparse.ArgumentParser()
parser.add_argument("--boosting_type", type=str, default="gbdt")
parser.add_argument("--objective", type=str, default="binary")
parser.add_argument("--n_estimators", type=int, default=200)
parser.add_argument("--learning_rate", type=float, default=0.001)
parser.add_argument("--num_leaves", type=int, default=30)
parser.add_argument("--max_bin", type=int, default=300)</pre>
<p>We can also see that we log training and validation F1 scores, allowing SageMaker and CloudWatch to pull the data from logs for reporting and <span class="No-Break">evaluation purposes.</span></p>
<p>Finally, we need to write out the results of the training in a JSON document. The results can then be used in subsequent pipeline processes and are shown as output from the job in the SageMaker interface. The JSON document is stored on disk, along with the serialized <span class="No-Break">model file:</span></p>
<pre class="source-code">
metrics_data = {"hyperparameters": params,
                "binary_classification_metrics":
{"validation:f1": {"value": test_f1},"train:f1": {"value":
train_f1}}
}
metrics_location = args.output_data_dir + "/metrics.json"
model_location = args.model_dir + "/lightgbm-model"
with open(metrics_location, "w") as f:
    json.dump(metrics_data, f)
with open(model_location, "wb") as f:
    joblib.dump(model, f)</pre>
<p>As with the preprocessing step, the results are written to a local directory, where SageMaker picks them up and copies them <span class="No-Break">to S3.</span></p>
<p>With the script defined, we can create<a id="_idIndexMarker626"/> the tuning <a id="_idIndexMarker627"/>step in the pipeline, which trains the model and <span class="No-Break">tunes hyperparameters.</span></p>
<p>We must define a SageMaker <strong class="source-inline">Estimator</strong> that, similar to <strong class="source-inline">SKLearnProcessor</strong>, encapsulates the configuration for training, including a reference to the script (<span class="No-Break">on S3):</span></p>
<pre class="source-code">
static_hyperparams = {
    "boosting_type": "gbdt",
    "objective": "binary",
}
lgb_estimator = Estimator(
    source_dir="src",
    entry_point="lightgbm_train.py",
    output_path=estimator_output_uri,
    code_location=estimator_output_uri,
    hyperparameters=static_hyperparams,
    role=sagemaker_role,
    image_uri=train_image_uri,
    instance_count=train_instance_count,
    instance_type=train_instance_type,
    framework_version="1.3-1",
)</pre>
<p>We can then define<a id="_idIndexMarker628"/> our SageMaker <strong class="source-inline">HyperparameterTuner</strong>, which performs the actual hyperparameter<a id="_idIndexMarker629"/> tuning. Similar to Optuna or FLAML, we must specify valid ranges for the hyperparameters using <span class="No-Break">SageMaker wrappers:</span></p>
<pre class="source-code">
hyperparameter_ranges = {
    "n_estimators": IntegerParameter(10, 400),
    "learning_rate": ContinuousParameter(0.0001, 0.5, scaling_type="Logarithmic"),
    "num_leaves": IntegerParameter(2, 200),
    "max_bin": IntegerParameter(50, 500)
}</pre>
<p><strong class="source-inline">HyperparameterTuner</strong> can be set up <span class="No-Break">as follows:</span></p>
<pre class="source-code">
tuner_config_dict = {
    "estimator": lgb_estimator,
    "max_jobs": 20,
    "max_parallel_jobs": 2,
    "objective_metric_name": "validation-f1",
    "metric_definitions": [{"Name": "validation-f1", "Regex": "validation-f1:([0-9\\.]+)"}],
    "hyperparameter_ranges": hyperparameter_ranges,
    "base_tuning_job_name": tuning_job_name_prefix,
    "strategy": "Random"
}
tuner = HyperparameterTuner(**tuner_config_dict)</pre>
<p>SageMaker supports many strategies for hyperparameter tuning, including Hyperband tuning. More information can be found<a id="_idIndexMarker630"/> in the documentation for hyperparameter tuning: <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.xhtml</a>. Here, we used random search, with a maximum job size of 20. It’s here that AWS’ elastic infrastructure can be used to significant benefit. If we increase the training instance count, SageMaker automatically distributes the training job across all machines. Provisioning additional machines has some overhead and increases cost, but it can also majorly reduce tuning time if we run thousands <span class="No-Break">of trials.</span></p>
<p>The tuner’s metric definitions<a id="_idIndexMarker631"/> define regular expressions<a id="_idIndexMarker632"/> that are used to pull the results metrics from the logs, as we showed in the training script earlier. The parameter optimization framework optimizes relative to the metrics defined here, minimizing or maximizing <span class="No-Break">the metric.</span></p>
<p>With the hyperparameter tuner defined, we can create a <strong class="source-inline">TuningStep</strong> for inclusion into <span class="No-Break">the pipeline:</span></p>
<pre class="source-code">
tuning_step = TuningStep(
    name="LGBModelTuning",
    tuner=tuner,
    inputs={
        "train": TrainingInput(...),
        "validation": TrainingInput(...),
    }
)</pre>
<p>The pipeline steps we’ve defined thus far prepare data and produce a trained model that’s serialized to S3. The pipeline’s next step is to create a SageMaker <strong class="source-inline">Model</strong> that wraps the model<a id="_idIndexMarker633"/> and is used for the evaluation, bias, and inference<a id="_idIndexMarker634"/> steps. This can be done <span class="No-Break">as follows:</span></p>
<pre class="source-code">
model = sagemaker.model.Model(
    image_uri=train_image_uri,
    model_data=tuning_step.get_top_model_s3_uri(
        top_k=0, s3_bucket=write_bucket, prefix=model_prefix
    ),
    sagemaker_session=sess,
    role=sagemaker_role
)
inputs = sagemaker.inputs.CreateModelInput(instance_type=deploy_instance_type_param)
create_model_step = CreateModelStep(name="CensusIncomeModel", model=model, inputs=inputs)</pre>
<p>The <strong class="source-inline">Model</strong> instance encapsulates all the necessary configurations to deploy and run the model. We can see that <strong class="source-inline">model_data</strong> is taken from the top-performing model resulting from the <span class="No-Break">tuning step.</span></p>
<p>The pipeline steps we’ve defined so far will produce processed data and train a tuned model. The layout for the processed data in S3 is shown in <span class="No-Break"><em class="italic">Figure 9</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="Figure 9.3 – S3 directory layout for the results of the processing jobs" height="498" src="image/B16690_09_03.jpg" width="725"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – S3 directory layout for the results of the processing jobs</p>
<p>We could proceed to the deployment step if we needed to. However, we will follow best practice and add quality gates<a id="_idIndexMarker635"/> to our pipeline<a id="_idIndexMarker636"/> that check the model’s performance and bias and produce insights into <span class="No-Break">its function.</span></p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor157"/>Evaluation, bias, and explainability</h2>
<p>So far, we’ve seen the general pattern of adding steps to a SageMaker pipeline: set up the configuration using SageMaker’s configuration classes and then create the relevant <span class="No-Break">pipeline step.</span></p>
<h3>Bias configuration</h3>
<p>To add bias checks<a id="_idIndexMarker637"/> to our pipeline, we must create the <span class="No-Break">following configuration:</span></p>
<pre class="source-code">
bias_config = clarify.BiasConfig(
    label_values_or_threshold=[1], facet_name="Sex", facet_values_or_threshold=[0], group_name="Age"
)
model_predictions_config = sagemaker.clarify.ModelPredictedLabelConfig(probability_threshold=0.5)
model_bias_check_config = ModelBiasCheckConfig(
    data_config=model_bias_data_config,
    data_bias_config=bias_config,
    model_config=model_config,
    model_predicted_label_config=model_predictions_config,
    methods=["DPPL"]
)</pre>
<p><strong class="source-inline">BiasConfig</strong> describes which<a id="_idIndexMarker638"/> facets (features) we want to check for bias. We’ve selected <strong class="source-inline">Sex</strong> and <strong class="source-inline">Age</strong>, which are always essential facets to check when working with <span class="No-Break">demographic data.</span></p>
<p><strong class="source-inline">ModeLBiasCheckConfig</strong> wraps the data configuration, model configuration, and bias confirmation for the bias check<a id="_idIndexMarker639"/> step. It also sets the method to use for the bias check. Here, we use the <strong class="bold">difference in positive proportions in predicted </strong><span class="No-Break"><strong class="bold">labels</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DPPL</strong></span><span class="No-Break">).</span></p>
<p>The DPPL is a metric that’s used to gauge if a model predicts outcomes differently for varying facets of data. The DPPL is calculated as the difference between the proportion of positive predictions for facet “a” and facet “d.” It helps assess whether there’s bias in the model predictions after training by comparing them with the initial bias present in the dataset. For instance, if a model predicting eligibility for a home loan predicts positive outcomes for 70% of male applicants (facet “a”) and 60% for female applicants (facet “d”), the 10% difference could indicate bias against <span class="No-Break">facet “d.”</span></p>
<p>The DPPL formula is represented <span class="No-Break">as follows:</span></p>
<p><span class="_-----MathTools-_Math_Variable">DPPL</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">=</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">q</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">′</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">q</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">′</span><span class="_-----MathTools-_Math_Operator_Extended"> </span></p>
<p>Here, <span class="_-----MathTools-_Math_Variable">q</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">a</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">′</span> is the predicted proportion of facet “a” receiving a positive outcome, and <span class="_-----MathTools-_Math_Variable">q</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">d</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator_Extended">′</span><span class="_-----MathTools-_Math_Operator_Extended"> </span> is the analogous proportion for facet “d.” For binary and multicategory facet labels, normalized DPPL values fall between [-1, 1], while continuous labels vary over the interval (-∞, +∞). A positive DPPL value suggests a higher proportion of positive predictions for facet “a” versus “d,” indicating a positive bias. Conversely, a negative DPPL indicates a higher proportion of positive predictions for facet “d,” signifying a negative bias. A DPPL near zero<a id="_idIndexMarker640"/> points to a relatively equal proportion of positive predictions for both facets, with a value of zero implying perfect <span class="No-Break">demographic parity.</span></p>
<p>You can add the bias check to the pipeline <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">ClarifyCheckStep</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
model_bias_check_step = ClarifyCheckStep(
    name="ModelBiasCheck",
    clarify_check_config=model_bias_check_config,
    check_job_config=check_job_config,
    skip_check=skip_check_model_bias_param,
    register_new_baseline=register_new_baseline_model_bias_param,
    supplied_baseline_constraints=supplied_baseline_constraints_model_bias_param
)</pre>
<h3>Explainability configuration</h3>
<p>The configuration for explainability<a id="_idIndexMarker641"/> is very similar. Instead of creating <strong class="source-inline">BiasConfig</strong>, we must <span class="No-Break">create </span><span class="No-Break"><strong class="source-inline">SHAPConfig</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
shap_config = sagemaker.clarify.SHAPConfig(
    seed=829,
    num_samples=100,
    agg_method="mean_abs",
    save_local_shap_values=True
)</pre>
<p>Alongside <strong class="source-inline">SHAPConfig</strong>, we must create <strong class="source-inline">ModelExplainabilityCheckConfig</strong> to calculate the SHAP values and create an <span class="No-Break">explainability report:</span></p>
<pre class="source-code">
model_explainability_config = ModelExplainabilityCheckConfig(
    data_config=model_explainability_data_config,
    model_config=model_config,
    explainability_config=shap_config
)</pre>
<p>Everything is then combined <a id="_idIndexMarker642"/><span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">ClarifyCheckStep</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
model_explainability_step = ClarifyCheckStep(
    name="ModelExplainabilityCheck",
    clarify_check_config=model_explainability_config,
    check_job_config=check_job_config,
    skip_check=skip_check_model_explainability_param,
    register_new_baseline=register_new_baseline_model_explainability_param,
    supplied_baseline_constraints=supplied_baseline_constraints_model_explainability_param
)</pre>
<h3>Evaluation</h3>
<p>Finally, we also need to evaluate<a id="_idIndexMarker643"/> our model against test data. The evaluation script is very similar to the training script, except it pulls the tuned model from S3 for scoring. The script consists of a main function with two steps. First, we must bootstrap the trained model and perform the scoring (in our case, calculating the <span class="No-Break">F1 score):</span></p>
<pre class="source-code">
...
    test_f1 = f1_score(y_test, model.predict(X_test))
    # Calculate model evaluation score
    logger.debug("Calculating F1 score.")
    metric_dict = {
        "classification_metrics": {"f1": {"value": test_f1}}
    }</pre>
<p>Then, we must output the results to a <span class="No-Break">JSON file:</span></p>
<pre class="source-code">
    # Save model evaluation metrics
    output_dir = "/opt/ml/processing/evaluation"
    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)
    logger.info("Writing evaluation report with F1: %f", test_f1)
    evaluation_path = f"{output_dir}/evaluation.json"
    with open(evaluation_path, "w") as f:
        f.write(json.dumps(metric_dict))</pre>
<p>The evaluation JSON<a id="_idIndexMarker644"/> is used for reporting and subsequent steps that rely on the <span class="No-Break">evaluation metrics.</span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor158"/>Deploying and monitoring the LightGBM model</h2>
<p>We are now ready to add our pipeline’s final steps for supporting deployment. The deployment<a id="_idIndexMarker645"/> part of the pipeline consists of <span class="No-Break">three steps:</span></p>
<ol>
<li>Registering the model <span class="No-Break">in SageMaker.</span></li>
<li>A conditional check to validate that the model evaluation surpasses a <span class="No-Break">minimum threshold.</span></li>
<li>Deploying a model endpoint using an AWS <span class="No-Break">Lambda function.</span></li>
</ol>
<h3>Model registration</h3>
<p>To deploy our model, we first need<a id="_idIndexMarker646"/> to register our model in SageMaker’s <span class="No-Break"><strong class="bold">Model Registry</strong></span><span class="No-Break">.</span></p>
<p>SageMaker’s Model Registry is a central repository where you can manage and deploy <span class="No-Break">your models.</span></p>
<p>The Model Registry provides<a id="_idIndexMarker647"/> the following <span class="No-Break">core functionality:</span></p>
<ul>
<li><strong class="bold">Model versioning</strong>: Every time a model is trained and registered, it’s assigned a version in the Model Registry. This helps you keep track of different iterations of your models, which is useful when you need to compare model performance, roll back to previous versions, or maintain reproducibility in your <span class="No-Break">ML projects.</span></li>
<li><strong class="bold">Approval workflow</strong>: The Model Registry supports an approval workflow, where models can be marked as “Pending Manual Approval,” “Approved,” or “Rejected.” This allows teams to effectively manage the life cycle of their models and ensure that only approved models <span class="No-Break">are deployed.</span></li>
<li><strong class="bold">Model catalog</strong>: The Model Registry acts as a catalog where all your models are centrally stored and accessible. Each model in the registry has metadata associated with it, such as the training data used, hyperparameters, and <span class="No-Break">performance metrics.</span></li>
</ul>
<p>While registering our model, we attach the metrics that were calculated from our evaluation step. These metrics are also used for model <span class="No-Break">drift detection.</span></p>
<p>Two types of drift are possible: <strong class="bold">data drift</strong> and <span class="No-Break"><strong class="bold">model drift</strong></span><span class="No-Break">.</span></p>
<p>Data drift refers to a change<a id="_idIndexMarker648"/> in the statistical distribution of the incoming data compared to our model’s training data. For example, if the training data had a male/female split of 60% to 40%, but the data used for prediction is skewed to 80% male<a id="_idIndexMarker649"/> and 20% female, it’s possible that <span class="No-Break">drift occurred.</span></p>
<p>Model drift is a phenomenon<a id="_idIndexMarker650"/> where the statistical properties of the target variable, which the model tries to predict, change over time in unforeseen ways, causing model performance <span class="No-Break">to degrade.</span></p>
<p>Both data and model drift can occur due to environmental changes, societal behaviors, product usage, or other factors not accounted for during <span class="No-Break">model training.</span></p>
<p>SageMaker supports continuous<a id="_idIndexMarker651"/> monitoring of drift. SageMaker calculates the statistical distribution of both incoming data and the predictions we are making. Both are compared against the distributions present in the training data. Should drift be detected, SageMaker can produce alerts to <span class="No-Break">AWS CloudWatch.</span></p>
<p>We can configure our metrics <span class="No-Break">as follows:</span></p>
<pre class="source-code">
model_metrics = ModelMetrics(
    bias_post_training=MetricsSource(
        s3_uri=model_bias_check_step.properties.CalculatedBaselineConstraints,
        content_type="application/json"
    ),
    explainability=MetricsSource(
        s3_uri=model_explainability_step.properties.CalculatedBaselineConstraints,
        content_type="application/json"
    ),
)</pre>
<p>Then, for the drift metrics, we must set <span class="No-Break">up </span><span class="No-Break"><strong class="source-inline">DriftCheckBaselines</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
drift_check_baselines = DriftCheckBaselines(
    bias_post_training_constraints=MetricsSource(   s3_uri=model_bias_check_step.properties.BaselineUsedForDriftCheckConstraints, content_type="application/json",
    ),
    explainability_constraints=MetricsSource(        s3_uri=model_explainability_step.properties.BaselineUsedForDriftCheckConstraints, content_type="application/json",
    ),
    explainability_config_file=FileSource(        s3_uri=model_explainability_config.monitoring_analysis_config_uri, content_type="application/json",
    ))</pre>
<p>Then, we must create a model registration<a id="_idIndexMarker652"/> step with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
register_step = RegisterModel(
    name="LGBRegisterModel",
    estimator=lgb_estimator,
    model_data=tuning_step.get_top_model_s3_uri(
        top_k=0, s3_bucket=write_bucket, prefix=model_prefix
    ),
    content_types=["text/csv"],
    response_types=["text/csv"],
    inference_instances=[predictor_instance_type],
    transform_instances=[predictor_instance_type],
    model_package_group_name=model_package_group_name,
    approval_status=model_approval_status_param,
    model_metrics=model_metrics,
    drift_check_baselines=drift_check_baselines
)</pre>
<h3>Model validation</h3>
<p>The conditional check<a id="_idIndexMarker653"/> uses the evaluation data from the evaluation step to determine whether the model is suitable <span class="No-Break">for deployment:</span></p>
<pre class="source-code">
cond_gte = ConditionGreaterThanOrEqualTo(
    left=JsonGet(
        step_name=evaluation_step.name,
        property_file=evaluation_report,
        json_path="classification_metrics.f1.value",
    ),
    right=0.9,
)
condition_step = ConditionStep(
    name="CheckCensusIncomeLGBEvaluation",
    conditions=[cond_gte],
    if_steps=[create_model_step, register_step, lambda_deploy_step],
    else_steps=[]
)</pre>
<p>Here, we created <strong class="source-inline">ConditionStep</strong> and compared the F1 score against a threshold of <strong class="source-inline">0.9</strong>. Deployment can proceed if the model has an F1 score higher than <span class="No-Break">the threshold.</span></p>
<h3>Deployment with AWS Lambda</h3>
<p>The deployment script<a id="_idIndexMarker654"/> is a standard AWS Lambda<a id="_idIndexMarker655"/> script in Python that defines a <strong class="source-inline">lambda_handler</strong> function that obtains a client connection to SageMaker and proceeds to create the <span class="No-Break">model endpoint:</span></p>
<pre class="source-code">
def lambda_handler(event, context):
    sm_client = boto3.client("sagemaker")
...
    create_endpoint_config_response = sm_client.create_endpoint_config(
        EndpointConfigName=endpoint_config_name,
        ProductionVariants=[{
            "VariantName": "Alltraffic",
            "ModelName": model_name,
            "InitialInstanceCount": instance_count,
            "InstanceType": instance_type,
            "InitialVariantWeight": 1}])
    create_endpoint_response = sm_client.create_endpoint(
        EndpointName=endpoint_name,
        EndpointConfigName=endpoint_config_name)</pre>
<p>Notably, the Lambda function<a id="_idIndexMarker656"/> does not serve requests<a id="_idIndexMarker657"/> for the model. It only creates the model endpoint <span class="No-Break">within SageMaker.</span></p>
<p>In SageMaker, an <strong class="bold">endpoint</strong> is a web service to get predictions<a id="_idIndexMarker658"/> from your models. Once a model is trained and the training job is complete, you need to deploy the model to make real-time or batch predictions. Deployment in SageMaker parlance means setting up an endpoint – a hosted, <span class="No-Break">production-ready model.</span></p>
<p>An endpoint in SageMaker is a scalable and secure RESTful API that you can use to send real-time inference requests to your models. Your applications can access an endpoint to make predictions directly via the REST API or AWS SDKs. It can scale instances up and down as needed, providing flexibility <span class="No-Break">and cost-effectiveness.</span></p>
<p>SageMaker also supports multi-model endpoints, which can deploy multiple models on a single endpoint. This feature can significantly save on costs if many models are used infrequently or are <span class="No-Break">not resource-intensive.</span></p>
<p>With the Lambda script defined, it can be<a id="_idIndexMarker659"/> incorporated<a id="_idIndexMarker660"/> into the pipeline <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">LambdaStep</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
lambda_deploy_step = LambdaStep(
    name="LambdaStepRealTimeDeploy",
    lambda_func=func,
    inputs={
        "model_name": pipeline_model_name,
        "endpoint_config_name": endpoint_config_name,
        "endpoint_name": endpoint_name,
        "model_package_arn": register_step.steps[0].properties.ModelPackageArn,
        "role": sagemaker_role,
        "instance_type": deploy_instance_type_param,
        "instance_count": deploy_instance_count_param
    }
)</pre>
<p class="callout-heading">Note</p>
<p class="callout">A model endpoint incurs cost as soon as it’s deployed for the duration of its deployment. Once you run your pipeline, an endpoint is created as a result. If you are only experimenting with or testing your pipeline, you should delete the endpoint once <span class="No-Break">you’re done.</span></p>
<h3>Creating and running the pipeline</h3>
<p>All of our pipeline steps<a id="_idIndexMarker661"/> are now in place, which means<a id="_idIndexMarker662"/> we can create the pipeline itself. The <strong class="source-inline">Pipeline</strong> construct takes the name and parameters we’ve <span class="No-Break">already defined:</span></p>
<pre class="source-code">
pipeline = Pipeline(
    name=pipeline_name,
    parameters=[process_instance_type_param,
                train_instance_type_param,
                train_instance_count_param,
                deploy_instance_type_param,
                deploy_instance_count_param,
                clarify_instance_type_param,
                skip_check_model_bias_param,
                register_new_baseline_model_bias_param,                supplied_baseline_constraints_model_bias_param,
                skip_check_model_explainability_param,                register_new_baseline_model_explainability_param,                supplied_baseline_constraints_model_explainability_param,
                model_approval_status_param],</pre>
<p>We must also pass<a id="_idIndexMarker663"/> all the steps we’ve defined <a id="_idIndexMarker664"/>as a list parameter and finally upsert <span class="No-Break">the pipeline:</span></p>
<pre class="source-code">
    steps=[
        process_step,
        train_step,
        evaluation_step,
        condition_step
    ],
    sagemaker_session=sess)
pipeline.upsert(role_arn=sagemaker_role)</pre>
<p>Executing the pipeline is done by calling the <span class="No-Break"><strong class="source-inline">start</strong></span><span class="No-Break"> method:</span></p>
<pre class="source-code">
start_response = pipeline.start(parameters=dict(
        SkipModelBiasCheck=True,
        RegisterNewModelBiasBaseline=True,
        SkipModelExplainabilityCheck=True,
        RegisterNewModelExplainabilityBaseline=True))</pre>
<p>Note the conditions we defined here. When running the pipeline for the first time, we must skip the model bias and explainability checks while registering new bias and <span class="No-Break">explainability baselines.</span></p>
<p>Both checks require an existing baseline to run (otherwise, there is no data to check against). Once baselines<a id="_idIndexMarker665"/> have been established, we can disable<a id="_idIndexMarker666"/> skipping the checks in <span class="No-Break">subsequent runs.</span></p>
<p>More information on the model life cycle<a id="_idIndexMarker667"/> and creating baselines can be found <span class="No-Break">at </span><a href="https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-quality-clarify-baseline-lifecycle.xhtml"><span class="No-Break">https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-quality-clarify-baseline-lifecycle.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-157"><a id="_idTextAnchor159"/>Results</h2>
<p>When the pipeline<a id="_idIndexMarker668"/> is executed, you can view the execution graph to see the status of <span class="No-Break">each step:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="Figure 9.4 – Successful execution of the LightGBM Census Income pipeline" height="1353" src="image/B16690_09_04.jpg" width="1166"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – Successful execution of the LightGBM Census Income pipeline</p>
<p>We can also see the model<a id="_idIndexMarker669"/> itself registered in the Model Registry once the <span class="No-Break">pipeline completes:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="Figure 9.5 – SageMaker Model Registry showing the approved Census Income model and the related endpoint" height="855" src="image/B16690_09_05.jpg" width="1321"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – SageMaker Model Registry showing the approved Census Income model and the related endpoint</p>
<p>The bias and explainability reports<a id="_idIndexMarker670"/> can be viewed when a model is selected. <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.6</em> shows the bias report for the model that was created by the pipeline. We can see a slight imbalance in the DPPL for sex, but less than the class imbalance in the training data. The report indicates there isn’t strong evidence <span class="No-Break">for bias:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 9.6 – Bias report for the Census Income model. We can see a slight imbalance in the DPPL but less than the class imbalance in the training data" height="774" src="image/B16690_09_06.jpg" width="1206"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Bias report for the Census Income model. We can see a slight imbalance in the DPPL but less than the class imbalance in the training data</p>
<p>The explainability report, as shown in <span class="No-Break"><em class="italic">Figure 9</em></span><em class="italic">.7</em>, shows the importance of each feature in terms of SHAP<a id="_idIndexMarker671"/> values. Here, we can see that the <strong class="bold">Capital Gain</strong> and <strong class="bold">Country</strong> features are dominant regarding importance <span class="No-Break">to predictions:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 9.7 – Explainability report for the Census Income model showing the dominant importance of the Capital Gain and Country features" height="1061" src="image/B16690_09_07.jpg" width="1210"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Explainability report for the Census Income model showing the dominant importance of the Capital Gain and Country features</p>
<p>The bias and explainability reports can also be downloaded in PDF format, which can easily be shared with business<a id="_idIndexMarker672"/> or <span class="No-Break">non-technical stakeholders.</span></p>
<h3>Making predictions using the endpoint</h3>
<p>Of course, our deployed <a id="_idIndexMarker673"/>model is not very useful if we can’t make any predictions<a id="_idIndexMarker674"/> using it. We can make predictions with the deployed model via REST calls or the Python SDK. Here is an example of using the <span class="No-Break">Python SDK:</span></p>
<pre class="source-code">
predictor = sagemaker.predictor.Predictor(endpoint_name,                                           sagemaker_session=sess, serializer=CSVSerializer(),                  deserializer=CSVDeserializer())
payload = test_df.drop(["Target"], axis=1).iloc[:5]
result = predictor.predict(payload.values)</pre>
<p>We obtain a SageMaker <strong class="source-inline">Predictor</strong> using the endpoint name and the session. Then, we can call <strong class="source-inline">predict</strong>, passing a NumPy array (obtained from a test DataFrame in <span class="No-Break">this case).</span></p>
<p>With that, we have created<a id="_idIndexMarker675"/> a complete, end-to-end, production-ready pipeline using SageMaker. Our pipeline includes data preprocessing, automatic model tuning, bias validation, drift detection, and<a id="_idIndexMarker676"/> a fully <span class="No-Break">scalable deployment.</span></p>
<h1 id="_idParaDest-158"><a id="_idTextAnchor160"/>Summary</h1>
<p>This chapter introduced AWS and Amazon SageMaker as a platform for building and deploying ML solutions. An overview of the SageMaker service was given, including the Clarify service, which provides advanced features such as model bias checks <span class="No-Break">and explainability.</span></p>
<p>We then proceeded to build a complete ML pipeline with the SageMaker service. The pipeline includes all steps of the ML life cycle, including data preparation, model training, tuning, model evaluation, bias checks, explainability reports, validation against test data, and deployment to cloud-native, <span class="No-Break">scalable infrastructure.</span></p>
<p>Specific examples were given to build each step within the pipeline, emphasizing full automation, looking to enable straightforward retraining and constant monitoring of data and <span class="No-Break">model processes.</span></p>
<p>The next chapter looks at another MLOps platform called <strong class="bold">PostgresML</strong>. PostgresML offers ML capabilities on top of a staple of the server landscape: the <span class="No-Break">Postgres database.</span></p>
<h1 id="_idParaDest-159"><a id="_idTextAnchor161"/>References</h1>
<table class="No-Table-Style" id="table001-8">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">1]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">S. M. Lundberg and S.-I. Lee, A Unified Approach to Interpreting Model Predictions, in Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan and R. Garnett, Eds., Curran Associates, Inc., 2017, </em><span class="No-Break"><em class="italic">p. 4765–4774.</em></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><em class="italic">[</em><span class="No-Break"><em class="italic">2]</em></span></p>
</td>
<td class="No-Table-Style">
<p><em class="italic">R. P. Moro and P. Cortez, Bank </em><span class="No-Break"><em class="italic">Marketing, 2012.</em></span></p>
</td>
</tr>
</tbody>
</table>
</div>
</div></body></html>