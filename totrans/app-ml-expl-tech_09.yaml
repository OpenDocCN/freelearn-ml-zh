- en: '*Chapter 7*: Practical Exposure to Using SHAP in ML'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第7章*：在机器学习中使用SHAP的实际经验'
- en: In the previous chapter, we discussed **SHapley Additive exPlanation** (**SHAP**),
    which is one of the most popular model explainability frameworks. We also covered
    a practical example of using SHAP for explaining regression models. However, SHAP
    can explain other types of models trained on different types of datasets. In the
    previous chapter, you did receive a brief conceptual understanding of the different
    types of **explainers** available in SHAP for explaining models trained on different
    types of datasets. But in this chapter, you will get the practical exposure needed
    to apply the various types of explainers available in SHAP.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了**SHapley Additive exPlanation**（**SHAP**），这是最受欢迎的模型可解释性框架之一。我们还介绍了使用SHAP解释回归模型的一个实际例子。然而，SHAP可以解释在多种类型数据集上训练的其他类型的模型。在上一章中，你确实对SHAP中可用于解释不同类型数据集上训练的模型的**不同类型的解释器**有一个简要的概念理解。但在本章中，你将获得应用SHAP中各种类型解释器所需的实际经验。
- en: More specifically, you learn how to apply **TreeExplainers** for explaining
    tree ensemble models trained on structured tabular data. You will also learn how
    to apply **DeepExplainer** and **GradientExplainer** SHAP with deep learning models
    trained on image data. As you learned in the previous chapter, the **KernelExplainer**
    in SHAP is model-agnostic, and you will get practical exposure to KernelExplainers
    in this chapter. We will also cover the practical aspect of using **LinearExplainers**
    on linear models. Finally, you will get to explore how SHAP can be used to explain
    the complicated state-of-the-art **Transformer** models trained on text data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，你将学习如何应用**TreeExplainers**来解释在结构化表格数据上训练的树集成模型。你还将学习如何应用**DeepExplainer**和**GradientExplainer**
    SHAP，这些SHAP与在图像数据上训练的深度学习模型一起使用。正如你在上一章中学到的，SHAP中的**KernelExplainer**是模型无关的，你将在本章中获得KernelExplainers的实际经验。我们还将涵盖在线性模型上使用**LinearExplainers**的实际方面。最后，你将探索如何使用SHAP来解释在文本数据上训练的复杂的**Transformer**模型。
- en: 'In this chapter, we will cover the following important topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下重要主题：
- en: Applying TreeExplainers to tree ensemble models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将TreeExplainers应用于树集成模型
- en: Explaining deep learning models using DeepExplainer and GradientExplainer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DeepExplainer和GradientExplainer解释深度学习模型
- en: Model-agnostic explainability using KernelExplainer
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用KernelExplainer进行模型无关的可解释性
- en: Exploring LinearExplainer in SHAP
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索SHAP中的LinearExplainer
- en: Explaining transformers using SHAP
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SHAP解释Transformer
- en: Let's get started!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This code tutorial along with the necessary resources can be downloaded or
    cloned from the GitHub repository for this chapter: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07).
    Python and Jupyter notebooks are used to implement the practical application of
    the theoretical concepts covered in this chapter. However, I will recommend that
    you run the notebooks only after you go through this chapter for a better understanding.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本代码教程以及必要的资源可以从本章的GitHub仓库下载或克隆：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07)。Python和Jupyter笔记本用于实现本章中涵盖的理论概念的实际应用。然而，我建议你在阅读本章后运行笔记本，以便更好地理解。
- en: Applying TreeExplainers to tree ensemble models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将TreeExplainers应用于树集成模型
- en: As discussed in the previous chapter, the Tree SHAP implementation can work
    with tree ensemble models such as **Random Forests**, **XGBoost**, and **LightGBM**
    algorithms. Now, decision trees are inherently interpretable. But tree-based ensemble
    learning models, either implementing boosting or bagging, are not inherently interpretable
    and can be quite complex to interpret. So, SHAP is one of the popular choices
    of algorithms used to explain such complex models. The Kernel SHAP implementation
    of SHAP is model-agnostic and can explain any model. However, the algorithm can
    be really slow with larger datasets with many features. That is why the **Tree
    SHAP** ([https://arxiv.org/abs/1802.03888](https://arxiv.org/abs/1802.03888))
    implementation of the algorithm is a high-speed exact algorithm for tree ensemble
    models. TreeExplainer is the fast C++ implementation of the Tree SHAP algorithm,
    which supports algorithms such as XGBoost, CatBoost, LightGBM, and other tree
    ensemble models from scikit-learn. In this section, I will cover how to apply
    TreeExplainer in practice.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，Tree SHAP 实现可以与树集成模型如 **随机森林**、**XGBoost** 和 **LightGBM** 算法一起工作。决策树本身是可解释的。但是，基于树的集成学习模型，无论是实现提升还是袋装，本身并不是可解释的，并且可能相当复杂。因此，SHAP
    是用于解释此类复杂模型的最受欢迎的算法之一。SHAP 的核 SHAP 实现是模型无关的，可以解释任何模型。然而，对于具有许多特征的较大数据集，该算法可能非常慢。这就是为什么
    **Tree SHAP** ([https://arxiv.org/abs/1802.03888](https://arxiv.org/abs/1802.03888))
    算法的实现是一个针对树集成模型的高速精确算法。TreeExplainer 是 Tree SHAP 算法的快速 C++ 实现，支持 XGBoost、CatBoost、LightGBM
    以及来自 scikit-learn 的其他树集成模型。在本节中，我将介绍如何在实践中应用 TreeExplainer。
- en: Installing the required Python modules
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装所需的 Python 模块
- en: 'The complete tutorial is provided in the GitHub repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/TreeExplainers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/TreeExplainers.ipynb)
    and I strongly recommend that you read this section and execute the code side
    by side. If you have followed the previous tutorials provided in the other chapters,
    most of the required Python packages should be installed by now. Otherwise, you
    can install the necessary packages using the `pip` installer:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的教程可以在 GitHub 仓库中找到，地址为 [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/TreeExplainers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/TreeExplainers.ipynb)，我强烈建议您阅读这一部分并边读边执行代码。如果您已经跟随着其他章节中提供的先前教程，那么现在应该已经安装了大多数所需的
    Python 包。否则，您可以使用 `pip` 安装程序安装必要的包：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You can import these packages to verify their successful installations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以导入这些包以验证它们的成功安装：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[PRE7]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For certain JavaScript-based SHAP visualizations in Jupyter notebooks, make
    sure to initialize the SHAP JavaScript module:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Jupyter 笔记本中的某些基于 JavaScript 的 SHAP 可视化，请确保初始化 SHAP JavaScript 模块：
- en: '[PRE8]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, let's discuss the dataset that we are going to use for this example.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论我们将用于本例的数据集。
- en: Discussion about the dataset
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关于数据集的讨论
- en: For this example, we will use the German Credit Risk dataset from Kaggle ([https://www.kaggle.com/uciml/german-credit](https://www.kaggle.com/uciml/german-credit)).
    This dataset is used to build a classification model for classifying good and
    bad credit risk. The Kaggle dataset is actually a simplified version of the original
    data available in UCI ([https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)))
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将使用 Kaggle 上的德国信用风险数据集 ([https://www.kaggle.com/uciml/german-credit](https://www.kaggle.com/uciml/german-credit))。该数据集用于构建一个分类模型，用于区分良好的和不良的信用风险。Kaggle
    数据集实际上是 UCI ([https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)))
    中可用的原始数据的简化版本）
- en: Statlog (German Credit Data) Data Set
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Statlog (德国信用数据) 数据集
- en: The credit for the dataset goes to *Professor Dr. Hans Hofmann, Institut für
    Statistik und Ökonometrie Universität Hamburg*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集的归功于 *教授博士汉斯·霍夫曼，汉堡统计学与计量经济学研究所*。
- en: 'Please refer to the notebook for more information on the dataset. The dataset
    is already provided in the GitHub repository for this chapter: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07/datasets](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07/datasets).
    We can use the pandas Python module to load and display the dataset as a DataFrame:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅笔记本以获取有关数据集的更多信息。该数据集已提供在本章的 GitHub 仓库中：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07/datasets](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07/datasets)。我们可以使用
    pandas Python 模块来加载数据集并将其显示为 DataFrame：
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following diagram illustrates the pandas DataFrame for this data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表展示了该数据的 pandas DataFrame：
- en: '![Figure 7.1 – pandas DataFrame snapshot of the German Credit Risk dataset'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.1 – pandas DataFrame snapshot of the German Credit Risk dataset'
- en: '](img/B18216_07_001.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_07_001.jpg]'
- en: Figure 7.1 – pandas DataFrame snapshot of the German Credit Risk dataset
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 德国信用风险数据集的 pandas DataFrame 快照
- en: I do recommend that you perform a thorough **Exploratory Data Analysis** (**EDA**).
    You can also use pandas profiling ([https://github.com/ydataai/pandas-profiling](https://github.com/ydataai/pandas-profiling))
    as shown in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*, for automated EDA. Since we have covered this already, I will skip the
    EDA part for this example.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议您进行彻底的**数据探索分析**（**EDA**）。您还可以使用如[*第 2 章*](B18216_02_ePub.xhtml#_idTextAnchor033)中所示，*模型可解释性方法*中的
    pandas profiling ([https://github.com/ydataai/pandas-profiling](https://github.com/ydataai/pandas-profiling))
    进行自动 EDA。由于我们已经覆盖了这一点，我将跳过本例的 EDA 部分。
- en: 'However, the dataset does have some missing values, which needs to be handled
    before building a model. We can check that using the following lines of code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，数据集确实存在一些缺失值，在构建模型之前需要处理。我们可以使用以下代码行进行检查：
- en: '[PRE11]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following visualization is generated as output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的可视化是输出结果：
- en: '![Figure 7.2 – Missing value visualization for the German Credit Risk dataset'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.2 – Missing value visualization for the German Credit Risk dataset'
- en: '](img/B18216_07_002.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_07_002.jpg]'
- en: Figure 7.2 – Missing value visualization for the German Credit Risk dataset
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 德国信用风险数据集的缺失值可视化
- en: 'The dataset has around 18% missing values for the `Saving accounts` feature
    and 40% missing values for the `Checking account` feature. Since the percentage
    of missing data is high, and the features can be important, we cannot completely
    ignore or drop these features. Please remember that the focus of this tutorial
    is on model explainability using TreeExplainers. So, we will not spend too much
    time doing data imputation as we are not concerned with building an efficient
    model for this example. As both the features are categorical features, we will
    simply create an `Unknown` category for the missing values. This can be done by
    means of the following line of code:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集在`储蓄账户`特征上大约有 18% 的缺失值，在`支票账户`特征上有 40% 的缺失值。由于缺失数据的百分比较高，且这些特征可能很重要，我们不能完全忽略或删除这些特征。请记住，本教程的重点是使用
    TreeExplainers 进行模型可解释性。因此，我们不会花太多时间进行数据插补，因为我们不关心构建一个高效的模型。由于这两个特征都是分类特征，我们将简单地为缺失值创建一个`未知`类别。这可以通过以下代码行完成：
- en: '[PRE20]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We will need to perform **Label Encoding** for the categorical features as
    we need to convert the string-like feature values to an integer format:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对分类特征进行**标签编码**，因为我们需要将字符串类型的特征值转换为整数格式：
- en: '[PRE21]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now, for this example, we will use the **LightGBM algorithm** ([https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)),
    which can work directly on categorical variables, and hence we do not need to
    perform **one-hot encoding**. But for other algorithms, we might need to perform
    one-hot encoding. Moreover, we will not perform other complex data pre-processing
    or feature engineering steps. I do recommend that you perform robust *feature
    engineering*, *outlier detection*, and *data normalization* for building efficient
    ML models. However, for this example, even if the model is not very accurate,
    we can use SHAP to generate explanations. Let's proceed to the model training
    part.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before training the model, we will need to create the training and test sets:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Since we will be using the LightGBM algorithm, we will need to create LightGBM
    dataset objects, which are used during the training process:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We also need to define the model parameters as a dictionary:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, we can train the model using the parameters and dataset object created:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: We are again skipping the hyperparameter tuning process to obtain a more efficient
    model, but I would definitely recommend spending some time on hyperparameter tuning
    to get a model with higher accuracy. Now, let's proceed to the model explainability
    part using SHAP.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Application of TreeExplainer in SHAP
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Applying TreeExplainer in SHAP is very easy as the framework is well modularized:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Once we have approximated the SHAP values, we can then apply the visualization
    methods provided in SHAP to obtain the model's explainability. I would recommend
    that you refer to the *Visualizations in SHAP* section in [*Chapter 6*](B18216_06_ePub.xhtml#_idTextAnchor107),
    *Model Interpretability Using SHAP*, to refresh your memory regarding the various
    visualization methods that we can use with SHAP for model explainability. We will
    start with global explainability with summary plots.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.3* illustrates the SHAP summary plot using the SHAP values generated
    by TreeExplainer on this dataset:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – SHAP summary plot on SHAP values generated by TreeExplainer'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_003.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – SHAP summary plot on SHAP values generated by TreeExplainer
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the preceding figure, the summary plot highlights the important
    features based on SHAP values, ordered from most important to least important.
    The model considered `Checking account` and `Duration` as one of the most influential
    features, compared to the `Sex` or `Job` features.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'For local explainability, we can apply the **force plot** and **decision plot**
    visualization methods:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: I often find decision plots to be more interpretable than force plots as decision
    plots show you the deviation from the mean expected value for each feature. The
    direction of deviation also indicates whether the feature is positively impacting
    the model's decision or whether it has a negative impact. But some of you might
    prefer force plots as well, as this indicates the positively or negatively affecting
    features based on their feature values and how they can impact in terms of achieving
    a higher prediction value or a lower prediction value.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常发现决策图比力图更易于理解，因为决策图显示了每个特征的预期平均值偏差。偏差的方向也表明了特征是正向影响模型决策还是具有负面影响。但有些人可能也更喜欢力图，因为这表明了基于特征值及其如何影响以实现更高的预测值或更低的预测值，哪些特征是正向或负向影响的。
- en: '*Figure 7.4* illustrates the force plot and decision plot that we have obtained:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.4* 展示了我们获得的力图和决策图：'
- en: '![Figure 7.4 – Force and decision plots for local interpretability'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.4 – 用于局部可解释性的力和决策图'
- en: '](img/B18216_07_004.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_07_004.jpg)'
- en: Figure 7.4 – Force and decision plots for local interpretability
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 用于局部可解释性的力和决策图
- en: 'In certain cases, understanding the inter-feature dependence becomes important
    as SHAP doesn''t consider features in isolation to obtain the most influential
    features. Rather SHAP-based feature importance is estimated based on the collective
    impact of multiple features together. So, for analyzing the feature importance,
    we can try out the SHAP feature dependence plots:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，理解特征间的相互依赖性变得很重要，因为SHAP不会孤立地考虑特征来获得最有影响力的特征。相反，基于SHAP的特征重要性是基于多个特征共同影响的集体估计。因此，为了分析特征重要性，我们可以尝试SHAP特征依赖性图：
- en: '[PRE62]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The following diagram shows the feature dependence plot for the `Purpose` and
    `Age` features:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了`Purpose`和`Age`特征的依赖性图：
- en: '![Figure 7.5 – SHAP feature dependence plot for the Purpose and Age features'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.5 – 目的和年龄特征的SHAP特征依赖图'
- en: '](img/B18216_07_005.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_07_005.jpg)'
- en: Figure 7.5 – SHAP feature dependence plot for the Purpose and Age features
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 目的和年龄特征的SHAP特征依赖图
- en: From the SHAP in *Figure 7.3*, it was slightly surprising for me to find out
    that the features `Purpose` and `Age` are not as important as `Duration` or `Credit
    amount`. In such cases, the feature dependence plots automatically calculate the
    most dependent feature for a selected feature. So, from *Figure 7.5*, we can see
    that for both `Purpose` and `Age`, `Credit Amount` is the dependent feature, and
    we can also see how these features vary with the dependent feature. This justifies
    that collectively, `Credit amount` is more influential than `Purpose` and `Age`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图7.3*中的SHAP来看，我发现特征`Purpose`和`Age`并不像`Duration`或`Credit amount`那样重要，这让我有些惊讶。在这种情况下，特征依赖性图会自动计算所选特征的依赖性最强的特征。因此，从*图7.5*中，我们可以看到对于`Purpose`和`Age`，`Credit
    Amount`是依赖性最强的特征，我们还可以看到这些特征如何与依赖性特征变化。这证明了`Credit amount`作为一个整体，比`Purpose`和`Age`更有影响力。
- en: You can also try out other visualization methods covered in [*Chapter 6*](B18216_06_ePub.xhtml#_idTextAnchor107),
    *Model Interpretability Using SHAP*, and definitely recommend you to play around
    with the SHAP values generated using the TreeExplainer so that you can come up
    with your own custom visualization method! In the next section, we are going to
    apply SHAP explainers to deep learning models trained on image data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以尝试其他在[*第6章*](B18216_06_ePub.xhtml#_idTextAnchor107)中介绍的可视化方法，即使用SHAP进行模型可解释性，并且强烈建议您尝试使用TreeExplainer生成的SHAP值来玩转自定义可视化方法！在下一节中，我们将应用SHAP解释器到在图像数据上训练的深度学习模型。
- en: Explaining deep learning models using DeepExplainer and GradientExplainer
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DeepExplainer和GradientExplainer解释深度学习模型
- en: In the previous section, we covered the use of TreeExplainer in SHAP, which
    is a model-specific explainability method only applicable to tree ensemble models.
    We will now discuss GradientExplainer and DeepExplainer, two other model-specific
    explainers in SHAP that are mostly used with deep learning models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了SHAP中的TreeExplainer的使用，这是一个仅适用于树集成模型的特定模型解释方法。现在我们将讨论GradientExplainer和DeepExplainer，SHAP中的另外两种特定模型解释器，它们主要用于深度学习模型。
- en: GradientExplainer
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GradientExplainer
- en: 'As discussed in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model
    Explainability Methods*, one of the most widely adopted ways to explain deep learning
    models trained on unstructured data such as images is **layer-wise relevance propagation**
    (**LRP**). LRP is about analyzing the gradient flow for the intermediate layers
    of the deep neural network. SHAP GradientExplainers also function in a similar
    way. As discussed in [*Chapter 6*](B18216_06_ePub.xhtml#_idTextAnchor107), *Model
    Interpretability Using SHAP*, GradientExplainer combines the idea of *SHAP*, *integrated
    gradients*, and *SmoothGrad* into a single expected value equation. GradientExplainer
    finally uses a sensitivity map-based gradient visualization method. The red pixels
    in the visualization map represent pixels having positive SHAP values, which increases
    the probability of the output class. The blue pixels represent pixels having negative
    SHAP values that decrease the likelihood of the output class. Now, let me walk
    you through the tutorial provided in the code repository: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb).
    Please load the necessary modules and follow the detailed steps provided in the
    notebook tutorial as I will be covering only the important coding steps in this
    section for helping you to understand the flow of the code tutorial.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如在 [*第 2 章*](B18216_02_ePub.xhtml#_idTextAnchor033) 中讨论的，*模型可解释性方法*，解释在图像等非结构化数据上训练的深度学习模型的最广泛采用的方法之一是**层相关传播**（**LRP**）。LRP
    是关于分析深度神经网络中间层的梯度流。SHAP GradientExplainers 也以类似的方式工作。如在第 [*第 6 章*](B18216_06_ePub.xhtml#_idTextAnchor107)
    中讨论的，*使用 SHAP 的模型可解释性*，GradientExplainer 将 *SHAP*、*集成梯度* 和 *SmoothGrad* 的思想结合成一个单一的期望值方程。GradientExplainer
    最终使用基于敏感度图的梯度可视化方法。可视化图中的红色像素表示具有正 SHAP 值的像素，这增加了输出类的概率。蓝色像素表示具有负 SHAP 值的像素，这减少了输出类的可能性。现在，让我带您参观代码仓库中提供的教程：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb)。请加载必要的模块，并按照笔记本教程中提供的详细步骤进行操作，因为我将只在本节中涵盖重要的编码步骤，以帮助您理解代码教程的流程。
- en: Discussion on the dataset used for training the model
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论用于训练模型的训练集
- en: 'For this example, we will be using the SHAP ImageNet dataset, which will be
    used to generate the background reference required by the GradientExplainer algorithm.
    We will also pick up the inference image from the same dataset. However, you are
    always free to pick up any other image dataset or inference image of your choice:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用 SHAP ImageNet 数据集，该数据集将被用于生成 GradientExplainer 算法所需的背景参考。我们还将从同一数据集中选取推理图像。然而，你始终可以自由选择任何其他图像数据集或你选择的推理图像：
- en: '[PRE66]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'For this example, we have selected the following image as our inference image:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们选择了以下图像作为我们的推理图像：
- en: '![Figure 7.6 – Inference image from SHAP ImageNet50'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.6 – 来自 SHAP ImageNet50 的推理图像'
- en: '](img/B18216_07_006.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_07_006.jpg)'
- en: Figure 7.6 – Inference image from SHAP ImageNet50
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 来自 SHAP ImageNet50 的推理图像
- en: As we can see from the inference image, it contains many possible objects, including
    a man, chair, and computer. All these can be potential model outcomes and the
    actual outcome depends on the exact region where the model is looking to make
    the prediction. So, explainability is very important in such cases. Next, let's
    discuss the model used for this example.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从推理图像中可以看到，它包含许多可能的物体，包括一个人、一把椅子和一台电脑。所有这些都可以是潜在模型的结果，实际结果取决于模型预测时关注的精确区域。因此，在这种情况下，可解释性非常重要。接下来，让我们讨论这个示例中使用的模型。
- en: Using a pre-trained CNN model for this example
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用预训练的 CNN 模型
- en: 'I have used a pre-trained CNN model, `tensorflow` Python module:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经使用了一个预训练的 CNN 模型，`tensorflow` Python 模块：
- en: '[PRE68]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Next, let's apply GradientExplainer to SHAP.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将 GradientExplainer 应用到 SHAP 上。
- en: Application of GradientExplainer in SHAP
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GradientExplainer 在 SHAP 中的应用
- en: 'GradientExplainer helps to map the gradient flow of intermediate layers of
    a deep learning model such as **Convolution Neural Network** (**CNN**) to explain
    the workings of the model. So, we will try to explore the 10th layer of the model
    and visualize the gradients based on SHAP values. The choice of the 10th layer
    is completely arbitrary; you can choose other layers as well:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: GradientExplainer 有助于将深度学习模型（如 **卷积神经网络**（**CNN**））的中间层的梯度流映射出来，以解释模型的运作原理。因此，我们将尝试探索模型的第
    10 层，并基于 SHAP 值可视化梯度。选择第 10 层完全是随机的；你也可以选择其他层：
- en: '[PRE70]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '[PRE87]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '[PRE88]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'In this example, we are trying to estimate the GradientExplainer-based SHAP
    values for the 10th layer of the pre-trained model. Using the SHAP image plot
    method, we can visualize the sensitivity map for the top 4 probable outcomes of
    the model, which is illustrated in *Figure 7.7*:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们试图估计预训练模型第 10 层基于 GradientExplainer 的 SHAP 值。使用 SHAP 图像绘制方法，我们可以可视化模型前
    4 个可能结果的对敏感度图，如图 *图 7.7* 所示：
- en: '![Figure 7.7 – SHAP image plot visualization based on applying GradientExplainer
    to the inference image'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.7 – 基于应用 GradientExplainer 到推理图像的 SHAP 图像绘制可视化](img/B18216_07_007.jpg)'
- en: '](img/B18216_07_007.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_07_007.jpg)'
- en: Figure 7.7 – SHAP image plot visualization based on applying GradientExplainer
    to the inference image
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 基于应用 GradientExplainer 到推理图像的 SHAP 图像绘制可视化
- en: The top four predictions from the model are **desktop_computer**, **desk**,
    **monitor**, and **screen**. All of these are valid outcomes depending on which
    region the model is focusing on. Using the SHAP image plots shown in *Figure 7.7*,
    we can identify the exact regions contributing to the specific model prediction.
    The pixel regions marked in red are making a positive contribution to the specific
    model prediction, whereas the blue pixel regions are negatively contributing to
    the model predictions. You can try visualizing other layers of the model as well
    and analyze how the model prediction varies throughout the layers!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的前四个预测结果是 **desktop_computer**、**desk**、**monitor** 和 **screen**。所有这些结果都是有效的，取决于模型关注的区域。使用
    *图 7.7* 中所示的 SHAP 图像绘制，我们可以识别出对特定模型预测有贡献的确切区域。标记为红色的像素区域对特定模型预测有正面贡献，而蓝色像素区域对模型预测有负面贡献。你也可以尝试可视化模型的其它层，并分析模型预测在各个层中的变化情况！
- en: Exploring DeepExplainers
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 DeepExplainers
- en: In the previous section, we covered GradientExplainers in SHAP. However, deep
    learning models can also be explained using DeepExplainers in SHAP based on the
    Deep SHAP algorithm. Deep SHAP is a high-speed implementation for estimating SHAP
    values for deep learning models. It uses a distribution of background samples
    and Shapley equations to linearize predominant non-linear operations used in deep
    learning models such as max, products, and softmax.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了 SHAP 中的 GradientExplainers。然而，深度学习模型也可以使用基于 Deep SHAP 算法的 SHAP 中的
    DeepExplainers 来解释。Deep SHAP 是一种用于估计深度学习模型 SHAP 值的高速实现。它使用背景样本的分布和 Shapley 方程来线性化深度学习模型中使用的最大值、乘积和
    softmax 等主要非线性操作。
- en: The tutorial notebook provided in [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb)
    covers an example of a deep learning model trained from scratch on the CIFAR-10
    dataset ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    for multi-class classification. The dataset contains highly compressed images
    of size 32x32 belonging to 10 different classes. In this section, I will skip
    the model training process as it is already covered in sufficient detail in the
    notebook tutorial. Instead, I will discuss the model explainability part using
    DeepExplainers, which is our primary focus. You can also try out the same tutorial
    with a pre-trained CNN model instead of training a model from scratch. Now, let's
    discuss the model explainability part.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的教程笔记本 [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb)
    涵盖了一个从 CIFAR-10 数据集 ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    中从头开始训练的多类分类深度学习模型的示例。该数据集包含 32x32 大小的高度压缩图像，属于 10 个不同的类别。在本节中，我将跳过模型训练过程，因为它已经在笔记本教程中得到了充分的详细说明。相反，我将讨论使用
    DeepExplainers 的模型可解释性部分，这是我们主要关注的焦点。您也可以尝试使用预训练的 CNN 模型而不是从头开始训练模型进行相同的教程。现在，让我们讨论模型可解释性部分。
- en: Application of DeepExplainer in SHAP
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP 中 DeepExplainer 的应用
- en: 'In order to apply DeepExplainer, we need to first form the background samples.
    The robustness of the explainability actually depends a lot on the selection of
    the background samples. For this example, we will randomly select 1,000 samples
    from the training data. You can increase your sample size as well, but please
    ensure that the background samples have no data drift between the training or
    the inference data by ensuring that the data collection process is consistent:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用 DeepExplainer，我们首先需要形成背景样本。可解释性的鲁棒性实际上在很大程度上取决于背景样本的选择。在这个例子中，我们将从训练数据中随机选择
    1,000 个样本。您也可以增加样本大小，但请确保背景样本在训练或推理数据之间没有数据漂移，通过确保数据收集过程是一致的：
- en: '[PRE89]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Once the background samples have been selected, we can create a SHAP explainer
    object using the DeepExplainer method on the trained model and the background
    samples and estimate the SHAP values for the inference data:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选定了背景样本，我们就可以使用在训练模型和背景样本上应用 DeepExplainer 方法来创建一个 SHAP 解释器对象，并估计推理数据的 SHAP
    值：
- en: '[PRE90]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '[PRE91]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'After the SHAP values are computed, we can use the SHAP image plot to visualize
    the pixels influencing the model in both a positive and negative manner using
    a similar sensitivity plot as used for GradientExplainer:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算 SHAP 值之后，我们可以使用 SHAP 图像图来可视化以类似梯度解释器使用的敏感性图来影响模型的像素，无论是正面还是负面的影响：
- en: '[PRE92]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The following figure shows the SHAP image plot for some sample inference data:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了某些样本推理数据的 SHAP 图像：
- en: '![Figure 7.8 – SHAP image plot visualization after applying DeepExplainers
    in SHAP'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.8 – 在 SHAP 中应用 DeepExplainer 后的 SHAP 图像可视化'
- en: '](img/B18216_07_008.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_07_008.jpg)'
- en: Figure 7.8 – SHAP image plot visualization after applying DeepExplainers in
    SHAP
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 在 SHAP 中应用 DeepExplainer 后的 SHAP 图像可视化
- en: As we can see from *Figure 7.8*, even if the model is trained on a very compressed
    dataset, DeepExplainer was able to calculate SHAP values that can help us identify
    the regions of the image (highlighted in pinkish-red pixels) that have positively
    contributed to the model's prediction. The model did correctly predict the outcome
    as a *horse*, which is the correct classification from the compressed image. However,
    applying DeepExplainer was quite simple and the method was very fast compared
    to conventional methods to approximate SHAP values for deep learning models trained
    on unstructured data such as images. Next, we will learn about KernelExplainer
    for model agnostic explainability.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从 *图 7.8* 中所见，即使模型是在一个非常压缩的数据集上训练的，DeepExplainer 仍然能够计算出 SHAP 值，帮助我们识别图像中（以粉红色像素突出显示）对模型预测有正面贡献的区域。模型确实正确地预测了结果为
    *马*，这是从压缩图像中得到的正确分类。然而，应用 DeepExplainer 非常简单，与传统的用于近似在非结构化数据（如图像）上训练的深度学习模型的 SHAP
    值的方法相比，这种方法非常快速。接下来，我们将学习关于 KernelExplainer 的模型无关可解释性。
- en: Model-agnostic explainability using KernelExplainer
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 KernelExplainer 的模型无关可解释性
- en: In the previous sections, we have discussed three model-specific explainers
    available in SHAP – TreeExplainer, GradientExplainer, and DeepExplainer. The KernelExplainer
    in SHAP actually makes SHAP a model-agnostic explainability approach. However,
    unlike the previous methods, KernelExplainer based on the Kernel SHAP algorithm
    is much slower, especially for large and high dimensional datasets. Kernel SHAP
    tries to combine ideas from Shapley values and **Local Interpretable Model-agnostic
    Explanations (LIME)** for both global and local interpretability of black-box
    models. Similar to the approach followed in LIME, the Kernel SHAP algorithm also
    creates locally linear perturbed samples and computes Shapley values of the same
    to identify features contributing to or against the model prediction.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们已经讨论了 SHAP 中可用的三种特定于模型的解释器 – TreeExplainer、GradientExplainer 和 DeepExplainer。SHAP
    中的 KernelExplainer 实际上使 SHAP 成为一个模型无关的解释方法。然而，与之前的方法不同，基于 Kernel SHAP 算法的 KernelExplainer
    要慢得多，尤其是在大型和高维数据集上。Kernel SHAP 尝试结合 Shapley 值和 **局部可解释模型无关解释 (LIME**) 的思想，以实现黑盒模型的全球和局部可解释性。类似于
    LIME 采取的方法，Kernel SHAP 算法也创建了局部线性扰动样本，并计算相同样本的 Shapley 值，以识别对模型预测有贡献或相反的特征。
- en: 'KernelExplainer is the practical implementation of the Kernel SHAP algorithm.
    The complete tutorial demonstrating the application of SHAP KernelExplainer is
    provided in the following notebook: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/KernelExplainers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/KernelExplainers.ipynb).
    I have used the same *German Credit Risk dataset* as used for the *TreeExplainer
    tutorial*. Please refer to the *Applying TreeExplainers to tree ensemble models*
    section for a detailed discussion of the dataset and the model if you are starting
    from this section directly. In this section, we will discuss the application of
    KernelExplainers for the same problem discussed in the TreeExplainer tutorial.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: KernelExplainer 是 Kernel SHAP 算法的实际实现。以下笔记本提供了完整的教程，展示了 SHAP KernelExplainer
    的应用：[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/KernelExplainers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/KernelExplainers.ipynb)。我使用了与
    *TreeExplainer 教程* 中相同的 *德国信用风险数据集*。如果你直接从本节开始，请参阅 *将 TreeExplainers 应用于树集成模型*
    部分，以详细了解数据集和模型。在本节中，我们将讨论 KernelExplainer 在 TreeExplainer 教程中讨论的相同问题中的应用。
- en: Application of KernelExplainer in SHAP
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP 中 KernelExplainer 的应用
- en: 'The KernelExplainer method in SHAP takes the model and the background data
    as the input to compute the SHAP values. For larger datasets or high dimensional
    datasets having many features, it is recommended to use only a subset of the training
    data as the background samples. Otherwise, Kernel SHAP can be a very slow algorithm
    and would take a lot of time to generate the SHAP values. Like the previous explainer
    methods covered, applying KernelExplainer is very simple and can be done using
    the following lines of code:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP 中的 KernelExplainer 方法将模型和背景数据作为输入来计算 SHAP 值。对于较大的数据集或具有许多特征的维数据集，建议仅使用训练数据的一个子集作为背景样本。否则，Kernel
    SHAP 可能是一个非常慢的算法，并且生成 SHAP 值需要很长时间。与之前介绍的解释方法一样，应用 KernelExplainer 非常简单，可以使用以下代码行完成：
- en: '[PRE93]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: If we log the wall time (using `%%time` in Jupyter notebooks) for computing
    SHAP values and compare KernelExplainer with TreeExplainer on the same dataset,
    we will observe that KernelExplainer takes a significantly longer time to execute
    (almost 1,000 times longer in our case!). This shows that even though KernelExplainer
    is model-agnostic, the slowness of the algorithm is a major drawback.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们记录计算 SHAP 值的墙时（在 Jupyter 笔记本中使用 `%%time`），并将 KernelExplainer 与 TreeExplainer
    在同一数据集上比较，我们将观察到 KernelExplainer 执行时间明显更长（在我们的案例中几乎长 1,000 倍！）这表明尽管 KernelExplainer
    是模型无关的，但算法的缓慢是一个主要的缺点。
- en: 'For explaining black-box models, the same visualization methods covered for
    TreeExplainer are applicable, which can be generated by the following code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于解释黑盒模型，适用于 TreeExplainer 的相同可视化方法也适用于 KernelExplainer，可以通过以下代码生成：
- en: '[PRE95]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: '[PRE97]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '*Figure 7.9* shows the summary plot, decision plot, and force plots used to
    explain the black-box model:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 7.9* 展示了用于解释黑盒模型的总结图、决策图和力图：'
- en: '![Figure 7.9 – Summary plot, decision plot, and force plots obtained after
    using SHAP KernelExplainer'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.9 – 使用 SHAP KernelExplainer 获得的总结图、决策图和力图'
- en: '](img/B18216_07_009.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_07_009.jpg)'
- en: Figure 7.9 – Summary plot, decision plot, and force plots obtained after using
    SHAP KernelExplainer
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 使用SHAP KernelExplainer获得的总结图、决策图和力图
- en: The plots shown in *Figure 7.9* can be obtained using the same approach as covered
    for TreeExplainer. In the next section, we will cover LinearExplainer in SHAP,
    which is another model-specific explanation method.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与TreeExplainer中介绍的方法相同的途径，可以获得*图7.9*中显示的图表。在下一节中，我们将介绍SHAP中的LinearExplainer，这是另一种特定于模型的解释方法。
- en: Exploring LinearExplainer in SHAP
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索SHAP中的LinearExplainer
- en: LinearExplainer in SHAP is particularly developed for linear machine learning
    models. In the previous section, we have seen that although KernelExplainer is
    model-agnostic, it can be very slow. So, I think that is one of the main motivations
    behind using LinearExplainer to explain a linear model with independent features
    and even consider feature correlation. In this section, we will discuss applying
    the LinearExplainer method in practice. The detailed notebook tutorial is available
    at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/LinearExplainers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/LinearExplainers.ipynb).
    We have used the same *Red Wine Quality dataset* as used for the tutorial discussed
    in [*Chapter 6*](B18216_06_ePub.xhtml#_idTextAnchor107), *Model Interpretability
    Using SHAP*. You can refer to the same tutorial to learn more about the dataset
    as we will only focus on the LinearExplainer application part in this section.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP中的LinearExplainer特别为线性机器学习模型开发。在前一节中，我们了解到虽然KernelExplainer是模型无关的，但它可能非常慢。因此，我认为这是使用LinearExplainer解释具有独立特征的线性模型，甚至考虑特征相关性的主要动机之一。在本节中，我们将讨论在实践中应用LinearExplainer方法。详细的笔记本教程可在[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/LinearExplainers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/LinearExplainers.ipynb)找到。我们使用了与[*第6章*](B18216_06_ePub.xhtml#_idTextAnchor107)中讨论的教程相同的*红酒质量数据集*，*使用SHAP进行模型可解释性*。您可以参考相同的教程了解更多关于数据集的信息，因为我们将在本节中仅关注LinearExplainer的应用部分。
- en: Application of LinearExplainer in SHAP
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SHAP中LinearExplainer的应用
- en: 'For this example, we have actually trained a linear regression model on the
    dataset. Similar to the other explainers, we can apply LinearExplainer using a
    few lines of code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们实际上在数据集上训练了一个线性回归模型。与其他解释器类似，我们可以使用几行代码应用LinearExplainer：
- en: '[PRE98]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: '[PRE99]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'To explain the trained linear model, the same visualization methods covered
    for TreeExplainer and KernelExplainer are applicable. *Figure 7.10* shows the
    summary plot, feature dependence plot, and force plots used to explain the trained
    linear model:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释训练好的线性模型，适用于TreeExplainer和KernelExplainer的相同可视化方法同样适用。*图7.10*展示了用于解释训练好的线性模型的总结图、特征依赖图和力图：
- en: '![Figure 7.10 – Summary plot, feature dependence plot, and force plots obtained
    after using SHAP LinearExplainer'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.10 – 使用SHAP LinearExplainer获得的总结图、特征依赖图和力图'
- en: '](img/B18216_07_010.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18216_07_010.jpg)'
- en: Figure 7.10 – Summary plot, feature dependence plot, and force plots obtained
    after using SHAP LinearExplainer
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 使用SHAP LinearExplainer获得的总结图、特征依赖图和力图
- en: 'We can obtain the visualization plots shown in *Figure 7.10* using the following
    lines of code:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码行获得*图7.10*中显示的视觉化图表：
- en: '[PRE100]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '[PRE101]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: '[PRE102]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: I will recommend that you explore other visualization methods or even create
    your custom visualizations using the SHAP values generated by the LinearExplainer.
    Next, we will discuss applying SHAP to transformer models trained on text data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我会建议您探索其他可视化方法，甚至使用LinearExplainer生成的SHAP值创建自定义可视化。接下来，我们将讨论将SHAP应用于在文本数据上训练的转换器模型。
- en: Explaining transformers using SHAP
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SHAP解释转换器
- en: In this chapter, so far we have seen examples of various SHAP explainers used
    to explain different types of models trained on structured and image datasets.
    Now, we will cover approaches to explain complicated models trained on text data.
    For text data, getting high accuracy with models trained on conventional **Natural
    Language Processing (NLP)** methods is always challenging. This is because extracting
    contextual information in sequential text data is always difficult using the classical
    approaches.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，到目前为止，我们已经看到了使用SHAP解释器解释在结构化和图像数据集上训练的不同类型模型的例子。现在，我们将介绍解释在文本数据上训练的复杂模型的方法。对于文本数据，使用传统的**自然语言处理（NLP）**方法训练模型以获得高精度始终具有挑战性。这是因为使用经典方法在顺序文本数据中提取上下文信息总是很困难。
- en: However, with the invention of the **Transformer** deep learning architecture
    ([https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/)),
    which is based on an **attention mechanism**, obtaining higher accuracy models
    trained on text data became much easier. However, transformer models are extremely
    complicated and it can be really difficult to interpret the workings of such models.
    Fortunately, being model-agnostic, SHAP can be applied with transformer models
    as well.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着基于**注意力机制**的**Transformer**深度学习架构的发明（[https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/)），在文本数据上训练更高精度的模型变得容易得多。然而，transformer模型极其复杂，解释这些模型的工作原理可能非常困难。幸运的是，由于SHAP对模型无偏见，它也可以应用于transformer模型。
- en: So in this section, we will cover how SHAP can be applied with *text-based,
    pre-trained transformer models from Hugging Face* ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers))
    used for different applications. The complete tutorial can be accessed from [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining_Transformers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining_Transformers.ipynb).
    Now, let's see the first example of explaining transformer-based sentiment analysis
    models using SHAP.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本节中，我们将介绍如何使用来自Hugging Face的基于文本的、预训练的transformer模型（[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)）应用于不同的应用场景。完整的教程可以从[https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining_Transformers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining_Transformers.ipynb)获取。现在，让我们看看使用SHAP解释基于transformer的情感分析模型的第一个例子。
- en: Explaining transformer-based sentiment analysis models
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释基于transformer的情感分析模型
- en: '`transformers` Python module using the `pip` installer:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pip`安装程序安装`transformers` Python模块：
- en: '[PRE103]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'You can confirm the successful installation of the transformers framework by
    importing the module:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过导入模块来确认transformers框架的成功安装：
- en: '[PRE104]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Now, let''s load a sentiment analysis pre-trained model:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载一个情感分析预训练模型：
- en: '[PRE105]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'We can take any sentence as an input and we will apply the model to check whether
    it has positive or negative sentiment. So, we will use the sentence `"Hugging
    Face transformers are absolutely brilliant!"` as our inference data:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将任何句子作为输入，并将模型应用于检查其是否具有积极或消极的情感。因此，我们将使用句子“Hugging Face transformers are
    absolutely brilliant!”作为我们的推理数据：
- en: '[PRE106]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: '[PRE107]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'The model will predict the probability of the inference data being positive
    and negative:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将预测推理数据为积极和消极的概率：
- en: '[PRE108]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: '[PRE109]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'With a very high probability (99.99%), the model has predicted the sentence
    to be positive, which is a correct prediction. Now, let''s apply SHAP to explain
    the model prediction:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎可以肯定（99.99%的概率），模型预测该句子为积极，这是一个正确的预测。现在，让我们应用SHAP来解释模型预测：
- en: '[PRE110]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: '[PRE111]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Once the SHAP values are successfully computed, we can apply SHAP text plot
    visualization and bar plot visualization to highlight words that are positively
    and negatively contributing to the model''s prediction:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功计算出SHAP值，我们可以应用SHAP文本图可视化和条形图可视化来突出对模型预测有积极和消极贡献的单词：
- en: '[PRE112]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '[PRE113]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: '*Figure 7.11* shows us the SHAP text plots, which look similar to force plots:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.11*向我们展示了SHAP文本图，它们看起来类似于力图：'
- en: '![Figure 7.11 – Explaining transformer models trained on text data using SHAP
    text plots'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.11 – 使用SHAP文本图解释在文本数据上训练的transformer模型'
- en: '](img/B18216_07_011.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18216_07_011.jpg)'
- en: Figure 7.11 – Explaining transformer models trained on text data using SHAP
    text plots
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 使用SHAP文本图解释在文本数据上训练的transformer模型
- en: As we can see from *Figure 7.11*, the words highlighted in red, such as *brilliant*,
    *absolutely*, and *Hugging*, make a positive contribution and increase the model
    prediction score, whereas the other words are lowering the model prediction and
    thus make a negative contribution to the model's prediction.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们从*图7.11*中可以看到的，突出显示为红色的单词，如*brilliant*、*absolutely*和*Hugging*，对模型预测分数有正面的贡献，而其他单词则降低了模型预测，因此对模型的预测有负面的贡献。
- en: 'The same inference can also be drawn from the SHAP bar plot shown in the following
    figure:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下图中所示的SHAP条形图中也可以得出相同的推断：
- en: '![Figure 7.12 – SHAP bar plot used to explain a transformer model trained on
    text data'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.12 – 用于解释在文本数据上训练的变压器模型的SHAP条形图'
- en: '](img/B18216_07_012.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_07_012.jpg]'
- en: Figure 7.12 – SHAP bar plot used to explain a transformer model trained on text
    data
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – 用于解释在文本数据上训练的变压器模型的SHAP条形图
- en: I find it easier to interpret bar plots, which clearly show the positive or
    negative impact of each word present in the sentence, as shown in *Figure 7.12*.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现解释条形图更容易，它清楚地显示了句子中每个单词的正负影响，如图*图7.12*所示。
- en: Next, let's explore another example, in which a transformer-based multi-class
    classification model is trained on text data.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索另一个示例，其中基于变压器的多类分类模型是在文本数据上训练的。
- en: Explaining a multi-class prediction transformer model using SHAP
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SHAP解释多类预测变压器模型
- en: 'In the previous example, we applied SHAP to explain a text-based binary classification
    model. Now, let''s apply SHAP to explain a pre-trained transformer model used
    for detecting one of the following six emotions: *sadness*, *joy*, *love*, *anger*,
    *fear*, and *surprise*.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个示例中，我们应用SHAP来解释一个基于文本的二分类模型。现在，让我们应用SHAP来解释用于检测以下六种情感之一的预训练变压器模型：*悲伤*、*喜悦*、*爱情*、*愤怒*、*恐惧*和*惊讶*。
- en: 'Let''s load the pre-trained transformer model for emotion detection:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载用于情感检测的预训练变压器模型：
- en: '[PRE114]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: '[PRE115]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: '[PRE116]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '[PRE117]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Now, let''s use the same inference data as in the previous example and compute
    the SHAP values using SHAP:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用与上一个示例中相同的推断数据，并使用SHAP计算SHAP值：
- en: '[PRE118]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: '[PRE119]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: 'We can then use the SHAP text plot to highlight words that make a positive
    or negative contribution to each of the six possible outcomes:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用SHAP文本图来突出显示对六个可能结果中的每一个都有正负贡献的单词：
- en: '[PRE120]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: '*Figure 7.13* illustrates the output of the SHAP text plot obtained from the
    previous line of code:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.13*展示了从上一行代码中获得的SHAP文本图的输出：'
- en: '![Figure 7.13 – Interactive SHAP text plot highlighting the words that make
    a positive and negative contribution'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.13 – 交互式SHAP文本图突出显示对正负贡献的单词'
- en: '](img/B18216_07_013.jpg)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18216_07_013.jpg]'
- en: Figure 7.13 – Interactive SHAP text plot highlighting the words that make a
    positive and negative contribution
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 交互式SHAP文本图突出显示对正负贡献的单词
- en: SHAP text plots are interactive. As we can see from *Figure 7.13*, this highlights
    the model's predicted outcome in red along with the words that make a positive
    and negative contribution to the model's decision. We can also click on other
    possible outcomes and visualize the influence of each word on the model's prediction.
    For example, if we click on *surprise* instead of *joy*, we will see that all
    the words other than the word *face* are highlighted in blue, as these words are
    contributing negatively to that specific outcome. Personally, I found this approach
    of explaining transformer models trained on text data using SHAP to be really
    interesting and efficient! Next, let's cover another interesting use case of applying
    SHAP to explain NLP zero-shot learning models.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: SHAP文本图是交互式的。正如我们从*图7.13*中可以看到的，这突出了模型预测结果的红色，以及对模型决策有正负贡献的单词。我们还可以点击其他可能的结果，并可视化每个单词对模型预测的影响。例如，如果我们点击*惊讶*而不是*喜悦*，我们会看到除了单词*face*之外的所有单词都被突出显示为蓝色，因为这些单词对该特定结果有负面的贡献。我个人认为，使用SHAP解释在文本数据上训练的变压器模型的方法非常有趣且高效！接下来，让我们探讨应用SHAP解释NLP零样本学习模型的另一个有趣用例。
- en: Explaining zero-shot learning models using SHAP
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SHAP解释零样本学习模型
- en: '**Zero-shot learning** is one of the most fascinating concepts in NLP, which
    involves applying models on inference data for predicting any custom category
    that is not used during the training process without the need for fine-tuning.
    You can find more information about zero-shot learning in this reference link:
    [https://aixplain.com/2021/09/23/zero-shot-learning-in-natural-language-processing/](https://aixplain.com/2021/09/23/zero-shot-learning-in-natural-language-processing/).
    Applying SHAP to zero-shot learning models is also very straightforward.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will need to load the pre-trained transformer models:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: '[PRE122]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'We will need to create a custom zero-shot learning pipeline in order for SHAP
    to work:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: '[PRE124]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: '[PRE125]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: '[PRE126]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: '[PRE127]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: '[PRE128]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: '[PRE129]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'We will then need to define the custom labels and the inference text data and
    configure the new labels for the zero-shot learning model. For this example, we
    have selected the text `"I love playing cricket!"` as our inference data and we
    want our zero-shot learning model to predict whether the inference text data belongs
    to the `insect`, `sports`, or `animal` class:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: '[PRE131]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: '[PRE132]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: '[PRE133]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: '[PRE134]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: '[PRE135]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: 'Once the process of setting up the zero-shot learning model is ready, we can
    easily apply SHAP for the model explainability:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: '[PRE137]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'After the SHAP values have been computed successfully, we can use text plots
    or bar plots for the model explainability:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: '[PRE139]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: '*Figure 7.14* shows the bar plot visualization obtained as an output:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Bar plot visualization for the outcome of the predicted sport
    for the zero-shot learning model'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_014.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.14 – Bar plot visualization for the outcome of the predicted sport
    for the zero-shot learning model
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: The inference text sentence – `"I love playing cricket!"` used for this example
    was indeed related to the `Sports` class, which was correctly predicted by the
    model. However, cricket is not only a sport, but an insect as well. When the phrase
    `playing cricket` is used, collectively it indicates that we are talking about
    a sport. So, these words should make a positive contribution to the model's prediction.
    Unfortunately, from *Figure 7.14*, we can see that both the words `playing` and
    `cricket` are negatively contributing with negative SHAP values. This gives us
    an indication that even though the model prediction is correct, this is not a
    very good model as the model is relying heavily on the word `love` instead of
    the words `cricket` or `playing`. This is a classic example that highlights the
    need to make **Explainable AI** (**XAI**) a mandatory part of the AI process cycle
    and we should not blindly trust models even if the model prediction is correct.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: We have now arrived at the end of this chapter, and I will summarize the important
    topics that we have discussed in this chapter.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this chapter, you have received some practical exposure to using
    SHAP with tabular structured data as well as unstructured data such as images
    and texts. We have discussed the different explainers available in SHAP for both
    model-specific and model-agnostic explainability. We have applied SHAP to explain
    linear models, tree ensemble models, convolution neural network models, and even
    transformer models in this chapter. Using SHAP, we can explain different types
    of models trained on different types of data. I highly recommend trying out the
    end-to-end tutorials provided in the GitHub code repository and exploring things
    in more depth to acquire deeper practical knowledge.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss another interesting topic of concept activation
    vectors and explore the practical part of applying the **Testing with Concept
    Activation Vectors** (**TCAV**) framework from Google AI for explaining models
    with human-friendly concepts.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following resources to gain additional information:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '*Shapley, Lloyd S. "A value for n-person games." Contributions to the Theory
    of Games 2.28 (1953)*: [https://doi.org/10.1515/9781400881970-018](https://doi.org/10.1515/9781400881970-018)'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Red Wine Quality Dataset – Kaggle*: [https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SHAP GitHub Project*: [https://github.com/slundberg/shap](https://github.com/slundberg/shap)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SHAP Documentation:* [https://shap.readthedocs.io/en/latest/index.html](https://shap.readthedocs.io/en/latest/index.html)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hugging Face Models*: [https://huggingface.co/](https://huggingface.co/)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zero-Shot Learning*: [https://en.wikipedia.org/wiki/Zero-shot_learning](https://en.wikipedia.org/wiki/Zero-shot_learning)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
