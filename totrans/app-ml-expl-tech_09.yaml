- en: '*Chapter 7*: Practical Exposure to Using SHAP in ML'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed **SHapley Additive exPlanation** (**SHAP**),
    which is one of the most popular model explainability frameworks. We also covered
    a practical example of using SHAP for explaining regression models. However, SHAP
    can explain other types of models trained on different types of datasets. In the
    previous chapter, you did receive a brief conceptual understanding of the different
    types of **explainers** available in SHAP for explaining models trained on different
    types of datasets. But in this chapter, you will get the practical exposure needed
    to apply the various types of explainers available in SHAP.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, you learn how to apply **TreeExplainers** for explaining
    tree ensemble models trained on structured tabular data. You will also learn how
    to apply **DeepExplainer** and **GradientExplainer** SHAP with deep learning models
    trained on image data. As you learned in the previous chapter, the **KernelExplainer**
    in SHAP is model-agnostic, and you will get practical exposure to KernelExplainers
    in this chapter. We will also cover the practical aspect of using **LinearExplainers**
    on linear models. Finally, you will get to explore how SHAP can be used to explain
    the complicated state-of-the-art **Transformer** models trained on text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following important topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying TreeExplainers to tree ensemble models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining deep learning models using DeepExplainer and GradientExplainer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model-agnostic explainability using KernelExplainer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring LinearExplainer in SHAP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining transformers using SHAP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This code tutorial along with the necessary resources can be downloaded or
    cloned from the GitHub repository for this chapter: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07).
    Python and Jupyter notebooks are used to implement the practical application of
    the theoretical concepts covered in this chapter. However, I will recommend that
    you run the notebooks only after you go through this chapter for a better understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: Applying TreeExplainers to tree ensemble models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, the Tree SHAP implementation can work
    with tree ensemble models such as **Random Forests**, **XGBoost**, and **LightGBM**
    algorithms. Now, decision trees are inherently interpretable. But tree-based ensemble
    learning models, either implementing boosting or bagging, are not inherently interpretable
    and can be quite complex to interpret. So, SHAP is one of the popular choices
    of algorithms used to explain such complex models. The Kernel SHAP implementation
    of SHAP is model-agnostic and can explain any model. However, the algorithm can
    be really slow with larger datasets with many features. That is why the **Tree
    SHAP** ([https://arxiv.org/abs/1802.03888](https://arxiv.org/abs/1802.03888))
    implementation of the algorithm is a high-speed exact algorithm for tree ensemble
    models. TreeExplainer is the fast C++ implementation of the Tree SHAP algorithm,
    which supports algorithms such as XGBoost, CatBoost, LightGBM, and other tree
    ensemble models from scikit-learn. In this section, I will cover how to apply
    TreeExplainer in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the required Python modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The complete tutorial is provided in the GitHub repository at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/TreeExplainers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/TreeExplainers.ipynb)
    and I strongly recommend that you read this section and execute the code side
    by side. If you have followed the previous tutorials provided in the other chapters,
    most of the required Python packages should be installed by now. Otherwise, you
    can install the necessary packages using the `pip` installer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can import these packages to verify their successful installations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For certain JavaScript-based SHAP visualizations in Jupyter notebooks, make
    sure to initialize the SHAP JavaScript module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's discuss the dataset that we are going to use for this example.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion about the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this example, we will use the German Credit Risk dataset from Kaggle ([https://www.kaggle.com/uciml/german-credit](https://www.kaggle.com/uciml/german-credit)).
    This dataset is used to build a classification model for classifying good and
    bad credit risk. The Kaggle dataset is actually a simplified version of the original
    data available in UCI ([https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)))
  prefs: []
  type: TYPE_NORMAL
- en: Statlog (German Credit Data) Data Set
  prefs: []
  type: TYPE_NORMAL
- en: The credit for the dataset goes to *Professor Dr. Hans Hofmann, Institut für
    Statistik und Ökonometrie Universität Hamburg*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please refer to the notebook for more information on the dataset. The dataset
    is already provided in the GitHub repository for this chapter: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07/datasets](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/tree/main/Chapter07/datasets).
    We can use the pandas Python module to load and display the dataset as a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram illustrates the pandas DataFrame for this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – pandas DataFrame snapshot of the German Credit Risk dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.1 – pandas DataFrame snapshot of the German Credit Risk dataset
  prefs: []
  type: TYPE_NORMAL
- en: I do recommend that you perform a thorough **Exploratory Data Analysis** (**EDA**).
    You can also use pandas profiling ([https://github.com/ydataai/pandas-profiling](https://github.com/ydataai/pandas-profiling))
    as shown in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model Explainability
    Methods*, for automated EDA. Since we have covered this already, I will skip the
    EDA part for this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the dataset does have some missing values, which needs to be handled
    before building a model. We can check that using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following visualization is generated as output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Missing value visualization for the German Credit Risk dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.2 – Missing value visualization for the German Credit Risk dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset has around 18% missing values for the `Saving accounts` feature
    and 40% missing values for the `Checking account` feature. Since the percentage
    of missing data is high, and the features can be important, we cannot completely
    ignore or drop these features. Please remember that the focus of this tutorial
    is on model explainability using TreeExplainers. So, we will not spend too much
    time doing data imputation as we are not concerned with building an efficient
    model for this example. As both the features are categorical features, we will
    simply create an `Unknown` category for the missing values. This can be done by
    means of the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to perform **Label Encoding** for the categorical features as
    we need to convert the string-like feature values to an integer format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now, for this example, we will use the **LightGBM algorithm** ([https://lightgbm.readthedocs.io/en/latest/](https://lightgbm.readthedocs.io/en/latest/)),
    which can work directly on categorical variables, and hence we do not need to
    perform **one-hot encoding**. But for other algorithms, we might need to perform
    one-hot encoding. Moreover, we will not perform other complex data pre-processing
    or feature engineering steps. I do recommend that you perform robust *feature
    engineering*, *outlier detection*, and *data normalization* for building efficient
    ML models. However, for this example, even if the model is not very accurate,
    we can use SHAP to generate explanations. Let's proceed to the model training
    part.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before training the model, we will need to create the training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we will be using the LightGBM algorithm, we will need to create LightGBM
    dataset objects, which are used during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to define the model parameters as a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can train the model using the parameters and dataset object created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: We are again skipping the hyperparameter tuning process to obtain a more efficient
    model, but I would definitely recommend spending some time on hyperparameter tuning
    to get a model with higher accuracy. Now, let's proceed to the model explainability
    part using SHAP.
  prefs: []
  type: TYPE_NORMAL
- en: Application of TreeExplainer in SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Applying TreeExplainer in SHAP is very easy as the framework is well modularized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Once we have approximated the SHAP values, we can then apply the visualization
    methods provided in SHAP to obtain the model's explainability. I would recommend
    that you refer to the *Visualizations in SHAP* section in [*Chapter 6*](B18216_06_ePub.xhtml#_idTextAnchor107),
    *Model Interpretability Using SHAP*, to refresh your memory regarding the various
    visualization methods that we can use with SHAP for model explainability. We will
    start with global explainability with summary plots.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.3* illustrates the SHAP summary plot using the SHAP values generated
    by TreeExplainer on this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – SHAP summary plot on SHAP values generated by TreeExplainer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.3 – SHAP summary plot on SHAP values generated by TreeExplainer
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the preceding figure, the summary plot highlights the important
    features based on SHAP values, ordered from most important to least important.
    The model considered `Checking account` and `Duration` as one of the most influential
    features, compared to the `Sex` or `Job` features.
  prefs: []
  type: TYPE_NORMAL
- en: 'For local explainability, we can apply the **force plot** and **decision plot**
    visualization methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: I often find decision plots to be more interpretable than force plots as decision
    plots show you the deviation from the mean expected value for each feature. The
    direction of deviation also indicates whether the feature is positively impacting
    the model's decision or whether it has a negative impact. But some of you might
    prefer force plots as well, as this indicates the positively or negatively affecting
    features based on their feature values and how they can impact in terms of achieving
    a higher prediction value or a lower prediction value.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.4* illustrates the force plot and decision plot that we have obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – Force and decision plots for local interpretability'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.4 – Force and decision plots for local interpretability
  prefs: []
  type: TYPE_NORMAL
- en: 'In certain cases, understanding the inter-feature dependence becomes important
    as SHAP doesn''t consider features in isolation to obtain the most influential
    features. Rather SHAP-based feature importance is estimated based on the collective
    impact of multiple features together. So, for analyzing the feature importance,
    we can try out the SHAP feature dependence plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram shows the feature dependence plot for the `Purpose` and
    `Age` features:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – SHAP feature dependence plot for the Purpose and Age features'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.5 – SHAP feature dependence plot for the Purpose and Age features
  prefs: []
  type: TYPE_NORMAL
- en: From the SHAP in *Figure 7.3*, it was slightly surprising for me to find out
    that the features `Purpose` and `Age` are not as important as `Duration` or `Credit
    amount`. In such cases, the feature dependence plots automatically calculate the
    most dependent feature for a selected feature. So, from *Figure 7.5*, we can see
    that for both `Purpose` and `Age`, `Credit Amount` is the dependent feature, and
    we can also see how these features vary with the dependent feature. This justifies
    that collectively, `Credit amount` is more influential than `Purpose` and `Age`.
  prefs: []
  type: TYPE_NORMAL
- en: You can also try out other visualization methods covered in [*Chapter 6*](B18216_06_ePub.xhtml#_idTextAnchor107),
    *Model Interpretability Using SHAP*, and definitely recommend you to play around
    with the SHAP values generated using the TreeExplainer so that you can come up
    with your own custom visualization method! In the next section, we are going to
    apply SHAP explainers to deep learning models trained on image data.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining deep learning models using DeepExplainer and GradientExplainer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we covered the use of TreeExplainer in SHAP, which
    is a model-specific explainability method only applicable to tree ensemble models.
    We will now discuss GradientExplainer and DeepExplainer, two other model-specific
    explainers in SHAP that are mostly used with deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: GradientExplainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As discussed in [*Chapter 2*](B18216_02_ePub.xhtml#_idTextAnchor033), *Model
    Explainability Methods*, one of the most widely adopted ways to explain deep learning
    models trained on unstructured data such as images is **layer-wise relevance propagation**
    (**LRP**). LRP is about analyzing the gradient flow for the intermediate layers
    of the deep neural network. SHAP GradientExplainers also function in a similar
    way. As discussed in [*Chapter 6*](B18216_06_ePub.xhtml#_idTextAnchor107), *Model
    Interpretability Using SHAP*, GradientExplainer combines the idea of *SHAP*, *integrated
    gradients*, and *SmoothGrad* into a single expected value equation. GradientExplainer
    finally uses a sensitivity map-based gradient visualization method. The red pixels
    in the visualization map represent pixels having positive SHAP values, which increases
    the probability of the output class. The blue pixels represent pixels having negative
    SHAP values that decrease the likelihood of the output class. Now, let me walk
    you through the tutorial provided in the code repository: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb).
    Please load the necessary modules and follow the detailed steps provided in the
    notebook tutorial as I will be covering only the important coding steps in this
    section for helping you to understand the flow of the code tutorial.'
  prefs: []
  type: TYPE_NORMAL
- en: Discussion on the dataset used for training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this example, we will be using the SHAP ImageNet dataset, which will be
    used to generate the background reference required by the GradientExplainer algorithm.
    We will also pick up the inference image from the same dataset. However, you are
    always free to pick up any other image dataset or inference image of your choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'For this example, we have selected the following image as our inference image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Inference image from SHAP ImageNet50'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.6 – Inference image from SHAP ImageNet50
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the inference image, it contains many possible objects, including
    a man, chair, and computer. All these can be potential model outcomes and the
    actual outcome depends on the exact region where the model is looking to make
    the prediction. So, explainability is very important in such cases. Next, let's
    discuss the model used for this example.
  prefs: []
  type: TYPE_NORMAL
- en: Using a pre-trained CNN model for this example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I have used a pre-trained CNN model, `tensorflow` Python module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's apply GradientExplainer to SHAP.
  prefs: []
  type: TYPE_NORMAL
- en: Application of GradientExplainer in SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GradientExplainer helps to map the gradient flow of intermediate layers of
    a deep learning model such as **Convolution Neural Network** (**CNN**) to explain
    the workings of the model. So, we will try to explore the 10th layer of the model
    and visualize the gradients based on SHAP values. The choice of the 10th layer
    is completely arbitrary; you can choose other layers as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we are trying to estimate the GradientExplainer-based SHAP
    values for the 10th layer of the pre-trained model. Using the SHAP image plot
    method, we can visualize the sensitivity map for the top 4 probable outcomes of
    the model, which is illustrated in *Figure 7.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – SHAP image plot visualization based on applying GradientExplainer
    to the inference image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.7 – SHAP image plot visualization based on applying GradientExplainer
    to the inference image
  prefs: []
  type: TYPE_NORMAL
- en: The top four predictions from the model are **desktop_computer**, **desk**,
    **monitor**, and **screen**. All of these are valid outcomes depending on which
    region the model is focusing on. Using the SHAP image plots shown in *Figure 7.7*,
    we can identify the exact regions contributing to the specific model prediction.
    The pixel regions marked in red are making a positive contribution to the specific
    model prediction, whereas the blue pixel regions are negatively contributing to
    the model predictions. You can try visualizing other layers of the model as well
    and analyze how the model prediction varies throughout the layers!
  prefs: []
  type: TYPE_NORMAL
- en: Exploring DeepExplainers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we covered GradientExplainers in SHAP. However, deep
    learning models can also be explained using DeepExplainers in SHAP based on the
    Deep SHAP algorithm. Deep SHAP is a high-speed implementation for estimating SHAP
    values for deep learning models. It uses a distribution of background samples
    and Shapley equations to linearize predominant non-linear operations used in deep
    learning models such as max, products, and softmax.
  prefs: []
  type: TYPE_NORMAL
- en: The tutorial notebook provided in [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining%20Deep%20Learning%20models.ipynb)
    covers an example of a deep learning model trained from scratch on the CIFAR-10
    dataset ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    for multi-class classification. The dataset contains highly compressed images
    of size 32x32 belonging to 10 different classes. In this section, I will skip
    the model training process as it is already covered in sufficient detail in the
    notebook tutorial. Instead, I will discuss the model explainability part using
    DeepExplainers, which is our primary focus. You can also try out the same tutorial
    with a pre-trained CNN model instead of training a model from scratch. Now, let's
    discuss the model explainability part.
  prefs: []
  type: TYPE_NORMAL
- en: Application of DeepExplainer in SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to apply DeepExplainer, we need to first form the background samples.
    The robustness of the explainability actually depends a lot on the selection of
    the background samples. For this example, we will randomly select 1,000 samples
    from the training data. You can increase your sample size as well, but please
    ensure that the background samples have no data drift between the training or
    the inference data by ensuring that the data collection process is consistent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the background samples have been selected, we can create a SHAP explainer
    object using the DeepExplainer method on the trained model and the background
    samples and estimate the SHAP values for the inference data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'After the SHAP values are computed, we can use the SHAP image plot to visualize
    the pixels influencing the model in both a positive and negative manner using
    a similar sensitivity plot as used for GradientExplainer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the SHAP image plot for some sample inference data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – SHAP image plot visualization after applying DeepExplainers
    in SHAP'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.8 – SHAP image plot visualization after applying DeepExplainers in
    SHAP
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from *Figure 7.8*, even if the model is trained on a very compressed
    dataset, DeepExplainer was able to calculate SHAP values that can help us identify
    the regions of the image (highlighted in pinkish-red pixels) that have positively
    contributed to the model's prediction. The model did correctly predict the outcome
    as a *horse*, which is the correct classification from the compressed image. However,
    applying DeepExplainer was quite simple and the method was very fast compared
    to conventional methods to approximate SHAP values for deep learning models trained
    on unstructured data such as images. Next, we will learn about KernelExplainer
    for model agnostic explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Model-agnostic explainability using KernelExplainer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we have discussed three model-specific explainers
    available in SHAP – TreeExplainer, GradientExplainer, and DeepExplainer. The KernelExplainer
    in SHAP actually makes SHAP a model-agnostic explainability approach. However,
    unlike the previous methods, KernelExplainer based on the Kernel SHAP algorithm
    is much slower, especially for large and high dimensional datasets. Kernel SHAP
    tries to combine ideas from Shapley values and **Local Interpretable Model-agnostic
    Explanations (LIME)** for both global and local interpretability of black-box
    models. Similar to the approach followed in LIME, the Kernel SHAP algorithm also
    creates locally linear perturbed samples and computes Shapley values of the same
    to identify features contributing to or against the model prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'KernelExplainer is the practical implementation of the Kernel SHAP algorithm.
    The complete tutorial demonstrating the application of SHAP KernelExplainer is
    provided in the following notebook: [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/KernelExplainers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/KernelExplainers.ipynb).
    I have used the same *German Credit Risk dataset* as used for the *TreeExplainer
    tutorial*. Please refer to the *Applying TreeExplainers to tree ensemble models*
    section for a detailed discussion of the dataset and the model if you are starting
    from this section directly. In this section, we will discuss the application of
    KernelExplainers for the same problem discussed in the TreeExplainer tutorial.'
  prefs: []
  type: TYPE_NORMAL
- en: Application of KernelExplainer in SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The KernelExplainer method in SHAP takes the model and the background data
    as the input to compute the SHAP values. For larger datasets or high dimensional
    datasets having many features, it is recommended to use only a subset of the training
    data as the background samples. Otherwise, Kernel SHAP can be a very slow algorithm
    and would take a lot of time to generate the SHAP values. Like the previous explainer
    methods covered, applying KernelExplainer is very simple and can be done using
    the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: If we log the wall time (using `%%time` in Jupyter notebooks) for computing
    SHAP values and compare KernelExplainer with TreeExplainer on the same dataset,
    we will observe that KernelExplainer takes a significantly longer time to execute
    (almost 1,000 times longer in our case!). This shows that even though KernelExplainer
    is model-agnostic, the slowness of the algorithm is a major drawback.
  prefs: []
  type: TYPE_NORMAL
- en: 'For explaining black-box models, the same visualization methods covered for
    TreeExplainer are applicable, which can be generated by the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7.9* shows the summary plot, decision plot, and force plots used to
    explain the black-box model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Summary plot, decision plot, and force plots obtained after
    using SHAP KernelExplainer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.9 – Summary plot, decision plot, and force plots obtained after using
    SHAP KernelExplainer
  prefs: []
  type: TYPE_NORMAL
- en: The plots shown in *Figure 7.9* can be obtained using the same approach as covered
    for TreeExplainer. In the next section, we will cover LinearExplainer in SHAP,
    which is another model-specific explanation method.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring LinearExplainer in SHAP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LinearExplainer in SHAP is particularly developed for linear machine learning
    models. In the previous section, we have seen that although KernelExplainer is
    model-agnostic, it can be very slow. So, I think that is one of the main motivations
    behind using LinearExplainer to explain a linear model with independent features
    and even consider feature correlation. In this section, we will discuss applying
    the LinearExplainer method in practice. The detailed notebook tutorial is available
    at [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/LinearExplainers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/LinearExplainers.ipynb).
    We have used the same *Red Wine Quality dataset* as used for the tutorial discussed
    in [*Chapter 6*](B18216_06_ePub.xhtml#_idTextAnchor107), *Model Interpretability
    Using SHAP*. You can refer to the same tutorial to learn more about the dataset
    as we will only focus on the LinearExplainer application part in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Application of LinearExplainer in SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this example, we have actually trained a linear regression model on the
    dataset. Similar to the other explainers, we can apply LinearExplainer using a
    few lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'To explain the trained linear model, the same visualization methods covered
    for TreeExplainer and KernelExplainer are applicable. *Figure 7.10* shows the
    summary plot, feature dependence plot, and force plots used to explain the trained
    linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Summary plot, feature dependence plot, and force plots obtained
    after using SHAP LinearExplainer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.10 – Summary plot, feature dependence plot, and force plots obtained
    after using SHAP LinearExplainer
  prefs: []
  type: TYPE_NORMAL
- en: 'We can obtain the visualization plots shown in *Figure 7.10* using the following
    lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: I will recommend that you explore other visualization methods or even create
    your custom visualizations using the SHAP values generated by the LinearExplainer.
    Next, we will discuss applying SHAP to transformer models trained on text data.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining transformers using SHAP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, so far we have seen examples of various SHAP explainers used
    to explain different types of models trained on structured and image datasets.
    Now, we will cover approaches to explain complicated models trained on text data.
    For text data, getting high accuracy with models trained on conventional **Natural
    Language Processing (NLP)** methods is always challenging. This is because extracting
    contextual information in sequential text data is always difficult using the classical
    approaches.
  prefs: []
  type: TYPE_NORMAL
- en: However, with the invention of the **Transformer** deep learning architecture
    ([https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/](https://blogs.nvidia.com/blog/2022/03/25/what-is-a-transformer-model/)),
    which is based on an **attention mechanism**, obtaining higher accuracy models
    trained on text data became much easier. However, transformer models are extremely
    complicated and it can be really difficult to interpret the workings of such models.
    Fortunately, being model-agnostic, SHAP can be applied with transformer models
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: So in this section, we will cover how SHAP can be applied with *text-based,
    pre-trained transformer models from Hugging Face* ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers))
    used for different applications. The complete tutorial can be accessed from [https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining_Transformers.ipynb](https://github.com/PacktPublishing/Applied-Machine-Learning-Explainability-Techniques/blob/main/Chapter07/Explaining_Transformers.ipynb).
    Now, let's see the first example of explaining transformer-based sentiment analysis
    models using SHAP.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining transformer-based sentiment analysis models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`transformers` Python module using the `pip` installer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'You can confirm the successful installation of the transformers framework by
    importing the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s load a sentiment analysis pre-trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'We can take any sentence as an input and we will apply the model to check whether
    it has positive or negative sentiment. So, we will use the sentence `"Hugging
    Face transformers are absolutely brilliant!"` as our inference data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'The model will predict the probability of the inference data being positive
    and negative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'With a very high probability (99.99%), the model has predicted the sentence
    to be positive, which is a correct prediction. Now, let''s apply SHAP to explain
    the model prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the SHAP values are successfully computed, we can apply SHAP text plot
    visualization and bar plot visualization to highlight words that are positively
    and negatively contributing to the model''s prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7.11* shows us the SHAP text plots, which look similar to force plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11 – Explaining transformer models trained on text data using SHAP
    text plots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.11 – Explaining transformer models trained on text data using SHAP
    text plots
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from *Figure 7.11*, the words highlighted in red, such as *brilliant*,
    *absolutely*, and *Hugging*, make a positive contribution and increase the model
    prediction score, whereas the other words are lowering the model prediction and
    thus make a negative contribution to the model's prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The same inference can also be drawn from the SHAP bar plot shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12 – SHAP bar plot used to explain a transformer model trained on
    text data'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.12 – SHAP bar plot used to explain a transformer model trained on text
    data
  prefs: []
  type: TYPE_NORMAL
- en: I find it easier to interpret bar plots, which clearly show the positive or
    negative impact of each word present in the sentence, as shown in *Figure 7.12*.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's explore another example, in which a transformer-based multi-class
    classification model is trained on text data.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining a multi-class prediction transformer model using SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous example, we applied SHAP to explain a text-based binary classification
    model. Now, let''s apply SHAP to explain a pre-trained transformer model used
    for detecting one of the following six emotions: *sadness*, *joy*, *love*, *anger*,
    *fear*, and *surprise*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the pre-trained transformer model for emotion detection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use the same inference data as in the previous example and compute
    the SHAP values using SHAP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use the SHAP text plot to highlight words that make a positive
    or negative contribution to each of the six possible outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7.13* illustrates the output of the SHAP text plot obtained from the
    previous line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.13 – Interactive SHAP text plot highlighting the words that make
    a positive and negative contribution'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.13 – Interactive SHAP text plot highlighting the words that make a
    positive and negative contribution
  prefs: []
  type: TYPE_NORMAL
- en: SHAP text plots are interactive. As we can see from *Figure 7.13*, this highlights
    the model's predicted outcome in red along with the words that make a positive
    and negative contribution to the model's decision. We can also click on other
    possible outcomes and visualize the influence of each word on the model's prediction.
    For example, if we click on *surprise* instead of *joy*, we will see that all
    the words other than the word *face* are highlighted in blue, as these words are
    contributing negatively to that specific outcome. Personally, I found this approach
    of explaining transformer models trained on text data using SHAP to be really
    interesting and efficient! Next, let's cover another interesting use case of applying
    SHAP to explain NLP zero-shot learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining zero-shot learning models using SHAP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Zero-shot learning** is one of the most fascinating concepts in NLP, which
    involves applying models on inference data for predicting any custom category
    that is not used during the training process without the need for fine-tuning.
    You can find more information about zero-shot learning in this reference link:
    [https://aixplain.com/2021/09/23/zero-shot-learning-in-natural-language-processing/](https://aixplain.com/2021/09/23/zero-shot-learning-in-natural-language-processing/).
    Applying SHAP to zero-shot learning models is also very straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will need to load the pre-trained transformer models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need to create a custom zero-shot learning pipeline in order for SHAP
    to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then need to define the custom labels and the inference text data and
    configure the new labels for the zero-shot learning model. For this example, we
    have selected the text `"I love playing cricket!"` as our inference data and we
    want our zero-shot learning model to predict whether the inference text data belongs
    to the `insect`, `sports`, or `animal` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the process of setting up the zero-shot learning model is ready, we can
    easily apply SHAP for the model explainability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'After the SHAP values have been computed successfully, we can use text plots
    or bar plots for the model explainability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7.14* shows the bar plot visualization obtained as an output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14 – Bar plot visualization for the outcome of the predicted sport
    for the zero-shot learning model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18216_07_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7.14 – Bar plot visualization for the outcome of the predicted sport
    for the zero-shot learning model
  prefs: []
  type: TYPE_NORMAL
- en: The inference text sentence – `"I love playing cricket!"` used for this example
    was indeed related to the `Sports` class, which was correctly predicted by the
    model. However, cricket is not only a sport, but an insect as well. When the phrase
    `playing cricket` is used, collectively it indicates that we are talking about
    a sport. So, these words should make a positive contribution to the model's prediction.
    Unfortunately, from *Figure 7.14*, we can see that both the words `playing` and
    `cricket` are negatively contributing with negative SHAP values. This gives us
    an indication that even though the model prediction is correct, this is not a
    very good model as the model is relying heavily on the word `love` instead of
    the words `cricket` or `playing`. This is a classic example that highlights the
    need to make **Explainable AI** (**XAI**) a mandatory part of the AI process cycle
    and we should not blindly trust models even if the model prediction is correct.
  prefs: []
  type: TYPE_NORMAL
- en: We have now arrived at the end of this chapter, and I will summarize the important
    topics that we have discussed in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading this chapter, you have received some practical exposure to using
    SHAP with tabular structured data as well as unstructured data such as images
    and texts. We have discussed the different explainers available in SHAP for both
    model-specific and model-agnostic explainability. We have applied SHAP to explain
    linear models, tree ensemble models, convolution neural network models, and even
    transformer models in this chapter. Using SHAP, we can explain different types
    of models trained on different types of data. I highly recommend trying out the
    end-to-end tutorials provided in the GitHub code repository and exploring things
    in more depth to acquire deeper practical knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss another interesting topic of concept activation
    vectors and explore the practical part of applying the **Testing with Concept
    Activation Vectors** (**TCAV**) framework from Google AI for explaining models
    with human-friendly concepts.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please refer to the following resources to gain additional information:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Shapley, Lloyd S. "A value for n-person games." Contributions to the Theory
    of Games 2.28 (1953)*: [https://doi.org/10.1515/9781400881970-018](https://doi.org/10.1515/9781400881970-018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Red Wine Quality Dataset – Kaggle*: [https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009](https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SHAP GitHub Project*: [https://github.com/slundberg/shap](https://github.com/slundberg/shap)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*SHAP Documentation:* [https://shap.readthedocs.io/en/latest/index.html](https://shap.readthedocs.io/en/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hugging Face Models*: [https://huggingface.co/](https://huggingface.co/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Zero-Shot Learning*: [https://en.wikipedia.org/wiki/Zero-shot_learning](https://en.wikipedia.org/wiki/Zero-shot_learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
