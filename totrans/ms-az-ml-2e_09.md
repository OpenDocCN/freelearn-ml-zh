# *第7章*：使用NLP的高级特征提取

在前面的章节中，我们学习了在Azure机器学习服务中许多标准的转换和预处理方法，以及使用Azure机器学习数据标注服务进行典型标注技术的应用。在本章中，我们希望更进一步，从文本和分类数据中提取语义特征——这是用户在训练机器学习模型时经常遇到的问题。本章将描述使用**自然语言处理**（**NLP**）进行特征提取的基础。这将帮助您在实际的机器学习管道中实现使用NLP的语义嵌入。

首先，我们将探讨**文本**、**分类**、**名义**和**有序**数据之间的差异。这种分类将帮助您根据特征类型决定最佳的特征提取和转换技术。稍后，我们将查看分类值最常见的转换方法，即**标签编码**和**独热编码**。这两种技术将被比较和测试，以了解这两种技术的不同用例和应用。

接下来，我们将处理文本数据的数值嵌入。为了实现这一点，我们将构建一个简单的**词袋**模型，使用**计数向量器**。为了净化输入，我们将构建一个包含**分词器**、停用词去除、**词干提取**和**词形还原**的NLP管道。我们将逐步学习这些不同的技术如何影响样本数据集。

此后，我们将用一种更好的词频加权方法——**词频-逆文档频率**（**TF-IDF**）算法来替换词计数方法。这将帮助您在给定整个文档集合的情况下，通过加权一个文档中术语的出现频率相对于文档集合中的频率来计算单词的重要性。此外，我们将探讨**奇异值分解**（**SVD**）以减少术语字典的大小。作为下一步，我们将通过利用词义来提高术语嵌入的质量，并深入了解语义嵌入，如**全局向量**（**GloVe**）和**Word2Vec**。

在最后一节，我们将探讨基于序列到序列深度神经网络且超过一亿参数的当前最先进的语言模型。我们将使用**长短期记忆**（**LSTM**）训练一个小的端到端模型，使用**双向编码器表示从Transformer**（**BERT**）进行词嵌入和情感分析，并将这两种自定义解决方案与Azure认知服务中的文本分析能力进行比较。

本章将涵盖以下主题：

+   理解分类数据

+   构建简单的词袋模型

+   利用术语重要性和语义

+   实现端到端语言模型

# 技术要求

在本章中，我们将使用以下Python库和版本来创建分类编码、创建语义嵌入、训练端到端模型以及执行经典的NLP预处理步骤：

+   `azureml-sdk 1.34.0`

+   `azureml-widgets 1.34.0`

+   `tensorflow 2.6.0`

+   `numpy 1.19.5`

+   `pandas 1.3.2`

+   `scikit-learn 0.24.2`

+   `nltk 3.6.2`

+   `gensim 3.8.3`

与前几章类似，您可以使用本地Python解释器或Azure Machine Learning中托管的笔记本环境来执行此代码。

本章中所有的代码示例都可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter07)。

# 理解分类数据

**分类数据**以多种形式、形状和意义存在。了解你正在处理的数据类型至关重要——它是一个字符串、文本还是伪装成分类值的数值？这些信息对于数据预处理、特征提取和模型选择至关重要。

在本节中，首先，我们将查看不同类型的分类数据——即*顺序*、*名义*和*文本*。根据类型，你可以使用不同的方法从中提取信息或其他有价值的数据。请记住，分类数据无处不在，无论是ID列、名义类别、顺序类别还是自由文本字段。值得一提的是，你对数据的了解越多，预处理就越容易。

接下来，我们将通过将其转换为数值来实际预处理顺序和名义分类数据。当你想要使用不能解释分类数据的机器学习算法时，这是一个必要的步骤，这对于大多数算法都是真实的，例如基于决策树的算法。大多数其他算法只能对数值值进行操作（例如，计算损失函数），因此需要进行转换。

## 比较文本、分类和顺序数据

许多机器学习算法，如支持向量机、神经网络、线性回归等，只能应用于数值数据。然而，在现实世界的数据集中，我们经常发现非数值列，例如包含文本数据的列。本章的目标是将文本数据转换为数值数据，作为高级特征提取步骤，这样我们就可以将处理后的数据插入到任何机器学习算法中。

当处理现实世界数据时，你将面临许多不同类型的文本和/或分类数据。为了优化机器学习算法，你需要了解这些差异，以便对不同的类型应用不同的预处理技术。但首先，让我们定义三种不同的文本数据类型：

+   *文本数据*：自由文本

+   *分类名义数据*：不可排序的类别

+   *有序类别数据*：可排序的类别

文本数据和类别数据之间的区别在于，在文本数据中，我们想要捕捉语义相似性（即词语的意义相似性），而在类别数据中，我们想要区分少数几个变量。

有序类别数据和有序类别数据之间的区别在于，名义数据不能排序（所有类别具有相同的权重），而有序类别可以在有序尺度上逻辑排序。

*图7.1* 展示了一个新闻文章评论的示例数据集，其中第一列，命名为 `statement`，是一个文本字段，名为 `topic` 的列是一个名义类别，而 `rating` 是一个有序类别：

![图7.1 – 比较不同的文本数据类型](img/B17928_07_001.jpg)

图7.1 – 比较不同的文本数据类型

理解这些数据表示之间的差异对于之后找到适当的嵌入技术至关重要。用有序数值尺度替换有序类别似乎很自然，将名义类别嵌入到正交空间中。相反，将文本数据嵌入到保留语义的数值空间中并不明显——这将在本章后面的部分中介绍，这部分内容涉及NLP。

请注意，除了类别值之外，你还会看到表示类别信息的连续数值变量，例如来自维度或查找表的ID。尽管这些是数值，但如果可能的话，你应该考虑将它们作为类别名义值处理。以下是一个示例数据集：

![图7.2 – 比较数值类别值](img/B17928_07_002.jpg)

图7.2 – 比较数值类别值

在这个例子中，我们可以看到 `sensorId` 值是一个数值，应该将其解释为类别名义值，而不是默认的数值，因为它没有数值意义。当你从 `sensorId` `1` 减去 `sensorId` `2` 时，你得到什么？`sensorId` `10` 是 `sensorId` `1` 的10倍大吗？这些问题是发现和编码这些类别值的典型问题。我们将在 [*第9章*](B17928_09_ePub.xhtml#_idTextAnchor152)，*使用Azure机器学习构建ML模型* 中发现，通过指定这些值是类别数据，梯度提升树模型可以优化这些特征，而不是将它们作为连续变量处理。

## 将类别转换为数值

让我们先从将分类变量（序数和名义）转换为数值开始。在本节中，我们将探讨两种常见的分类编码技术：**标签编码**和**独热编码**（也称为*虚拟编码*）。虽然**标签编码**用一个数值特征列替换分类特征列，**独热编码**则使用多个列（列的数量等于唯一值的数量）来编码一个单一特征。

这两种技术以相同的方式进行应用。在训练迭代过程中，这些技术会找到特征列中的所有唯一值，并给它们分配一个特定的数值（对于独热编码，是一个多维数值）。结果，一个定义这种替换的查找字典存储在编码器中。当应用编码器时，应用列中的值会使用查找字典进行转换（替换）。如果事先知道可能的值列表，大多数实现允许编码器直接从已知值的列表初始化查找字典，而不是在训练集中找到唯一值。这有利于指定字典中值的顺序，从而对编码值进行排序。

重要提示

请注意，通常可能存在某些分类特征值在测试集中没有出现在训练集中，因此没有存储在查找字典中。因此，你应该在你的编码器中添加一个默认类别，该类别也可以将未见过的值转换为数值。 

现在，我们将使用两个不同的分类数据列，一个是序数类别，另一个是名义类别，来展示不同的编码。*图7.3* 显示了一个名义特征`topic`，它可能代表一个新闻机构的文章列表：

![图7.3 – 名义分类数据](img/B17928_07_003.jpg)

图7.3 – 名义分类数据

*图7.4* 包含了`rating`的序数类别；它可能代表一个网站购买文章的反馈表单：

![图7.4 – 序列分类数据](img/B17928_07_004.jpg)

图7.4 – 序列分类数据

为了保留类别的含义，我们需要为不同的分类数据类型采用不同的预处理技术。首先，我们来看一下*标签编码器*。标签编码器为特征列中的每个唯一分类值分配一个递增的值。因此，它将类别转换为介于`0`和`N-1`之间的数值，其中`N`代表唯一值的数量。

让我们在第一个表中的`topic`列中测试标签编码器。我们在数据上训练编码器，并用数值主题ID替换`topic`列。以下是一个训练标签编码器并转换数据集的示例片段：

[PRE0]

*图7.5* 显示了先前转换的结果。每个主题都被编码为一个数值增量，`topicId`：

![图7.5 – 标签编码的主题](img/B17928_07_005.jpg)

图7.5 – 标签编码的主题

`topicId`生成的查找表如图*7.6*所示。这个查找字典是在`fit()`方法期间由编码器学习到的，可以使用`transform()`方法应用于分类数据：

![图7.6 – 主题查找字典](img/B17928_07_006.jpg)

图7.6 – 主题的查找字典

如前几个截图所示，使用标签对名义数据进行编码既简单又直接。然而，生成的数值数据具有与不同的名义类别不同的数学属性。因此，让我们找出这种方法对有序数据是如何工作的。

在下一个例子中，我们天真地将标签编码器应用于评分数据集。编码器通过迭代训练数据来训练，以创建查找字典：

[PRE1]

*图7.7*显示了编码后的评分结果作为`ratingId`，这与前面的例子非常相似。然而，在评分的情况下，评分数据的数值属性与分类评分的有序属性相似：

![图7.7 – 标签编码的评分](img/B17928_07_007.jpg)

图7.7 – 标签编码的评分

此外，让我们看看编码器从输入数据中学习到的查找字典，如图*7.8*所示：

![图7.8 – 评分的查找字典](img/B17928_07_008.jpg)

图7.8 – 评分的查找字典

你在自动生成的查找字典中看到什么奇怪的地方了吗？由于训练数据中分类值的顺序，我们按照以下顺序创建了一个数字列表：

[PRE2]

这可能不是我们在将标签编码器应用于有序分类值时所预期的结果。我们希望寻找的顺序类似于以下内容：

[PRE3]

为了创建具有正确顺序的标签编码器，我们可以将分类值的有序列表传递给编码器。这将创建一个更有意义的编码，如图*7.9*所示：

![图7.9 – 带有自定义顺序的标签编码的评分](img/B17928_07_009.jpg)

图7.9 – 带有自定义顺序的标签编码的评分

要在Python中实现这一点，我们必须使用pandas的分类顺序变量，这是一种特殊的标签编码器，它需要一个有序分类列表作为输入：

[PRE4]

在幕后，我们通过直接将类别传递给编码器来隐式地创建了以下查找字典：

![图7.10 – 带有自定义顺序的评分查找字典](img/B17928_07_010.jpg)

图7.10 – 带有自定义顺序的评分的查找字典

如前例所示，标签编码器可以迅速应用于任何分类数据，无需过多思考。标签编码器的结果是单个数值特征和分类查找表。此外，我们还可以看到，在主题和评分的示例中，标签编码更适合有序数据。

重要提示

主要的收获是标签编码器非常适合编码有序分类数据。你也了解到元素的顺序很重要，因此将类别按正确顺序手动传递给编码器是一个好的实践。

## 使用独热编码的正交嵌入

在本节的第二部分，我们将探讨`N`的含义，其中`N`代表唯一值的数量。这个向量除了包含一个列值为`1`的列，代表这个特定值所在的列外，其余列都包含`0`。以下是一个代码片段，展示了如何将独热编码器应用于`articles`数据集：

[PRE5]

前面代码的输出显示在*图7.11*中：

![图7.11 – 独热编码的文章](img/B17928_07_011.jpg)

图7.11 – 独热编码的文章

独热编码的查找字典有`N+1`列，其中`N`是编码列中唯一值的数量。正如我们在*图7.12*中的查找字典中可以看到的那样，字典中的所有N维向量都是正交的，长度相等，为`1`：

![图7.12 – 文章的查找字典](img/B17928_07_012.jpg)

图7.12 – 文章的查找字典

现在，让我们将这种技术与有序数据进行比较，并将独热编码应用于评分表。结果显示在*图7.13*中：

![图7.13 – 独热编码的评分](img/B17928_07_013.jpg)

图7.13 – 独热编码的评分

在前面的图中，我们可以看到，即使原始的类别值是有序的，编码后的值也无法排序，因此，在数值编码后，这个属性就丢失了。因此，我们可以得出结论，独热编码非常适合唯一值数量较少的名称分类值。

到目前为止，我们已经学习了如何通过使用查找字典和一维或N维数值嵌入将名称和有序分类值嵌入到数值中。然而，我们发现它在许多方面都有一定的局限性，例如唯一类别的数量和嵌入自由文本的能力。在接下来的几节中，我们将学习如何使用简单的NLP管道提取单词。

## 语义和文本值

值得花时间去理解的是，分类值和文本值并不相同。尽管它们可能都存储为字符串，并且在你的数据集中可能有相同的数据类型，但通常，分类值代表一组有限的类别，而文本值可以包含任何文本信息。

那么，这种区分为什么很重要呢？一旦你预处理了分类数据并将其嵌入到数值空间中，名称类别通常会被实现为正交向量。你将无法自动计算类别A到类别B的距离或创建类别之间的语义意义。

然而，对于文本数据，通常您会采用不同的方法来开始特征提取，该方法假设您将在数据集样本的相同文本特征中找到相似术语。您可以使用这些信息来计算两个文本列之间的有意义相似度得分；例如，测量共同单词的数量。

因此，我们建议您彻底检查您有哪些类型的分类值以及您打算如何预处理它们。此外，一个很好的练习是计算两行之间的相似度，看看它是否与您的预测相符。让我们看看使用基于字典的词袋嵌入的简单文本预处理方法。

# 构建简单的词袋模型

在本节中，我们将探讨一个惊人的简单概念，即使用称为词袋的技术来解决标签编码在文本数据中的不足，这将为一个简单的NLP管道打下基础。当您阅读这些技术时，如果它们看起来太简单，请不要担心；我们将通过调整、优化和改进逐步构建现代NLP管道。

## 使用计数构建的简单词袋模型

在本节中，我们将构建的主要概念是词袋模型。这是一个非常简单的概念；也就是说，它涉及将任何文档建模为包含在给定文档中的单词集合，每个单词的频率。因此，我们丢弃句子结构、单词顺序、标点符号等，并将文档简化为单词的原始计数。在此基础上，我们可以将这个单词计数向量化为一个数值向量表示，然后可以用于机器学习、分析、文档比较等等。虽然这个单词计数模型听起来非常简单，但在路上我们将会遇到很多语言特定的障碍，我们需要解决。

让我们开始并定义一个示例文档，我们将在这个部分对其进行转换：

[PRE6]

将简单的单词计数应用于文档为我们提供了我们的第一个（过于简单）词袋模型：

![图7.14 – 一个简单的词袋模型](img/B17928_07_014.jpg)

图7.14 - 一个简单的词袋模型

然而，像前面那样简单的方法有很多问题。我们混合了不同的标点符号、符号、名词、动词、副词和形容词的不同变形、屈折、时态和格。因此，我们必须构建一个管道来使用NLP清理和标准化数据。在本节中，在将数据输入到**计数向量器**之前，我们将构建以下清理步骤的管道，该向量器最终会计算单词出现次数并将它们收集到特征向量中。

## 分词 - 将字符串转换为单词列表

构建管道的第一步是将语料库分为文档，将文档分为单词。这个过程被称为`nltk`：

[PRE7]

上一段代码将输出一个包含单词和标点符号的标记列表：

[PRE8]

当你执行前面的代码片段时，`nltk`将下载预训练的标点模型以运行分词器。分词器的输出是单词和标点符号。

在下一步中，我们将移除标点符号，因为它们对于随后的*词形还原*过程不相关。然而，我们将在本节稍后将其恢复：

[PRE9]

结果将只包含原始文档中的单词，没有任何标点符号：

[PRE10]

在前面的代码中，我们使用了`word.isalnum()`函数来仅提取字母数字标记并将它们全部转换为小写。前面的单词列表已经比最初的原始模型好得多。然而，它仍然包含许多不必要的词，如*the*、*we*、*had*等，这些词不传达任何信息。

为了过滤掉特定语言的噪声，有道理移除那些经常出现在文本中且不增加任何语义意义的词。在Python中，移除这些词是常见的做法，使用`nltk`库：

[PRE11]

现在得到的列表只包含不是停用词的单词：

[PRE12]

上述代码为我们提供了一个很好的管道，我们最终只得到具有语义意义的词。我们可以将这个词表带到下一步，并对每个词应用更复杂的转换/归一化。如果我们在这个阶段应用计数向量器，我们最终会得到如图7.15所示的简单词袋模型：

![图7.15 – 一个简单的词袋模型](img/B17928_07_015.jpg)

图7.15 – 一个简单的词袋模型

如前图所示，词袋模型中包含的术语列表已经比原始示例干净得多。这是因为它不包含任何标点符号或停用词。

你可能会问，除了在文本中相对频繁出现之外，什么使一个词成为停用词？嗯，这是一个非常好的问题！我们可以使用**TF-IDF**方法来衡量每个词在当前上下文中的重要性，与它在整个文本中的出现频率进行比较，这将在*使用TF-IDF衡量词的重要性*部分进行讨论。

## 词干提取 – 基于规则的词缀移除

在下一步中，我们想要归一化词缀——单词的结尾以创建复数和动词变位。你可以看到，随着每一步的进行，我们都在更深入地探讨单一语言的概念——在这个案例中，是英语。然而，当将这些步骤应用于不同的语言时，可能需要使用完全不同的转换。这就是为什么NLP是一个如此困难的领域。

移除单词的词缀以获得词根也称为**词干提取**。词干提取是指将每个单词的出现转换为它的词根的基于规则（启发式）方法。以下是一些预期的转换示例：

[PRE13]

如前例所示，这种针对词根的启发式方法必须为每种语言专门构建。这对于所有其他NLP算法也是普遍适用的。为了简洁起见，在这本书中，我们只将讨论英语示例。

英语中一个流行的词根化算法是Porter算法，它定义了五个连续的缩减规则，例如从单词末尾移除*ed*、*ing*、*ate*、*tion*、*ence*、*ance*等。`nltk`库包含Porter词根化算法的实现：

[PRE14]

词根化后的单词列表看起来像这样：

[PRE15]

在前面的代码中，我们只是简单地将`stemmer`应用于分词文档中的每个单词。经过这一步骤后的词袋模型如图*7.16*所示：

![图7.16 – 词根化后的词袋模型](img/B17928_07_016.jpg)

图7.16 – 词根化后的词袋模型

虽然这个算法与词缀配合得很好，但它无法避免对动词的变形和时态进行规范化。这是我们接下来要解决的问题，我们将使用词形还原来解决。

## 词形还原 – 基于词典的词规范化

当查看词根化示例时，我们已能看出该方法的局限性。例如，对于像*are*、*am*或*is*这样的不规则动词变形，它们都应该被规范化为同一个词*be*，会发生什么？这正是词形还原试图通过使用预训练的词汇集和转换规则（称为词元）来解决的问题。**词元**存储在查找字典中，类似于以下转换：

[PRE16]

讨论词形还原时，有一个非常重要的观点需要提出。每个词元都需要应用于正确的词型，因此名词、动词、形容词等都有词元。这样做的原因是，一个词可以是名词或动词的过去时。在我们的例子中，`ground`可能来自名词*ground*或动词*grind*；`left`可能是形容词或*leave*的过去时。因此，我们还需要从句子中的单词中提取词型——这个过程称为`nltk`库再次为我们提供了支持。为了估计正确的POS标签，我们还需要提供标点符号：

[PRE17]

这里是生成的POS标签：

[PRE18]

POS标签描述了文档中每个标记的词型。您可以使用`nltk.help.upenn_tagset()`命令找到完整的标签列表。以下是从命令行执行此操作的示例：

[PRE19]

前面的命令将打印出POS标签列表：

[PRE20]

POS标签还包括动词和其他非常有用的时态信息。然而，在本节的词形还原中，我们只需要知道单词类型——*名词*、*动词*、*形容词*或*副词*。一个可能的词形还原器选择是`nltk`中的WordNet词形还原器。WordNet是一个英语词汇数据库，它将单词分组到概念和词型组中。

要将词形还原器应用于词干分析的结果，我们需要通过标点符号和停用词过滤POS标签，类似于之前的预处理步骤。然后，我们可以使用结果单词的词标签。让我们使用`nltk`应用词形还原器：

[PRE21]

代码输出了词形还原后的单词：

[PRE22]

上述单词列表看起来比我们在之前的模型中找到的干净得多。这是因为我们对动词的时态进行了归一化，并将它们转换成不定式形式。得到的词袋模型在*图7.17*中显示：

![图7.17 – 词袋模型在词形还原后的样子](img/B17928_07_017.jpg)

图7.17 – 词形还原后的词袋模型

这种技术对于清理数据集中单词的不规则形式非常有帮助。然而，它基于规则——称为词元——因此，它只能用于有此类词元的语言和单词。

## scikit-learn中的词袋模型

最后，我们可以将我们之前的所有步骤结合起来，创建一个最先进的自然语言处理预处理流程，以归一化输入文档，并通过计数向量器运行它们，以便我们可以将它们转换成数值特征向量。对多个文档这样做，我们可以轻松地在数值空间中比较文档的语义。我们可以计算文档特征向量之间的余弦相似度来计算它们的相似度，将它们插入到监督分类方法中，或者对生成的文档概念进行聚类。

回顾一下，让我们看看简单词袋模型的最终流程。我想强调的是，这个模型只是我们使用自然语言处理进行特征提取旅程的开始。我们进行了以下步骤进行归一化：

1.  分词

1.  删除标点符号

1.  删除停用词

1.  词干提取

1.  基于POS标签的词形还原

在最后一步中，我们在scikit-learn中应用了`CountVectorizer`。这将计算每个词的出现次数，创建一个全局单词语料库，并输出一个包含单词频率的稀疏特征向量。以下是将预处理数据从`nltk`传递到`CountVectorizer`的示例代码：

[PRE23]

转换后的词袋模型包含坐标和计数：

[PRE24]

坐标指的是`(文档ID，术语ID)`对，而计数指的是术语频率。为了更好地理解这个输出，我们还可以查看模型的内部词汇表。`vocabulary_`参数包含术语ID的查找字典：

[PRE25]

代码输出了模型的单词字典：

[PRE26]

在前面的例子中，我们在将其传递到`CountVectorizer`之前将预处理文档转换回字符串。这样做的原因是`CountVectorizer`自带一些可配置的预处理技术，例如分词、停用词删除等。对于这个演示，我们想将其应用于预处理数据。转换的输出是一个包含术语频率的稀疏特征向量。

让我们来找出如何将多个术语与语义概念相结合。

# 利用术语重要性和语义

我们到目前为止所做的一切都相对简单，并且基于词干或所谓的标记。词袋模型只不过是一个标记字典，它按字段统计标记的出现次数。在本节中，我们将探讨一种常见的技巧，通过术语的n-gram和skip-gram组合来进一步改进文档之间的匹配。

以多种方式组合术语将使你的词典爆炸。如果你有一个大语料库，比如一千万个单词，这就会变成一个问题。因此，我们将探讨一种常见的预处理技术，通过SVD来降低大型词典的维度。

虽然，现在，这种方法要复杂得多，但它仍然基于一个在大语料库上已经工作得很好的词袋模型。然而，当然，我们可以做得更好，并尝试理解词语的重要性。因此，我们将探讨NLP中另一种流行的技术来计算术语的重要性。

## 使用n-gram和skip-gram进行词语泛化

在之前的管道中，我们考虑了每个单词本身，没有任何上下文。然而，众所周知，上下文在语言中非常重要。有时，词语在一起才有意义，而不是单独存在。为了将这种上下文引入同类型的算法，我们将引入**n-gram**和**skip-gram**。这两种技术都在NLP中广泛用于预处理数据集和从文本数据中提取相关特征。

让我们从n-gram开始。一个输入数据集的`N`个连续实体（即字符、单词或标记）。以下是一些在字符列表中计算n-gram的示例：

[PRE27]

这里是一个示例，使用scikit-learn的`CountVectorizer`中的内置`ngram_range`参数来为输入数据生成多个n-gram：

[PRE28]

如您所见，词汇现在包含每个术语的1-gram和2-gram表示：

[PRE29]

在前面的代码中，我们可以看到，我们现在在训练词汇中拥有两个连续词语的组合，而不是原始词语。

我们可以将n-gram的概念扩展到允许模型跳过词语。如果我们想要执行一个2-gram，但其中一个样本中两个词语之间有一个形容词，而在另一个样本中这些词语是直接相邻的，这是一个很好的选项。为了实现这一点，我们需要一种方法来定义我们允许跳过多少个词语来找到匹配的词语。以下是一个使用之前相同字符的示例：

[PRE30]

幸运的是，我们在`nltk`中找到了n-gram的通用版本，即`nltk.skipgrams`方法。将跳过距离设置为`0`会导致传统的n-gram算法。我们可以将其应用于我们的原始数据集：

[PRE31]

与2-gram示例类似，该方法产生了一组成对术语的组合列表。然而，在这种情况下，我们在这些对之间允许存在一个跳过的单词：

[PRE32]

在前面的代码中，我们可以观察到skip-grams可以为NLP模型生成大量的额外有用特征维度。在现实场景中，这两种技术通常都会使用，因为单个单词的顺序在语义中起着重要作用。

然而，如果输入文档是来自网络的所有网站或大型文档，新特征维度的爆炸可能会造成灾难。因此，我们还需要一种方法来避免维度爆炸，同时捕获输入数据中的所有语义。我们将在下一节中解决这个挑战。

## 使用SVD减小词字典大小

NLP的一个常见问题是语料库中的单词数量庞大，因此字典大小会爆炸。在先前的例子中，我们看到字典的大小定义了正交项向量的大小。因此，20,000个术语的字典大小将导致20,000维的特征向量。即使没有任何n-gram丰富，这个特征向量维度也太大，无法在标准PC上处理。

因此，我们需要一个算法来减小生成的`CountVectorizer`的维度，同时保留现有信息。理想情况下，我们只会从输入数据中移除冗余信息，并将其投影到低维空间，同时保留所有原始信息。

PCA变换非常适合我们的解决方案，并帮助我们将输入数据转换成更低维度的线性无关维度。然而，计算特征值需要一个对称矩阵（行数和列数相同），在我们的情况下，我们没有这样的矩阵。因此，我们可以使用SVD算法，它将特征向量计算推广到非对称矩阵。由于其数值稳定性，它通常用于NLP和信息检索系统中。

SVD在NLP应用中的使用也被称为**潜在语义分析**（**LSA**），因为主成分可以解释为潜在特征空间中的概念。SVD嵌入将高维特征向量转换成低维概念空间。概念空间中的每个维度都是由术语向量的线性组合构成的。通过丢弃方差最小的概念，我们也减小了结果概念空间的维度，使其变得小得多，更容易处理。典型的概念空间有10到100个维度，而单词字典通常有超过100,000个。

让我们通过 `sklearn` 的 `TruncatedSVD` 实现来查看一个示例。SVD 被实现为一个转换器类，因此我们需要调用 `fit_transform()` 来拟合一个字典并使用相同的步骤进行转换。SVD 使用 `n_components` 参数配置为仅保留方差最高的成分：

[PRE33]

在前面的代码中，我们使用 SVD 对 `X_train_counts` 数据和 `CountVectorizer` 的输出进行 LSA。我们配置 SVD 只保留方差最高的前五个成分。

通过降低数据集的维度，你会丢失信息。幸运的是，我们可以使用训练好的 SVD 对象来计算剩余数据集中方差的数量，如下例所示：

[PRE34]

前面的命令将方差输出为一个介于 0 和 1 之间的数字，其中 1 表示 SVD 变换是原始数据到潜在空间的精确无损映射：

[PRE35]

在这种情况下，仅使用五个成分，SVD 保留了原始数据集 20% 的方差。

重要提示

根据任务的不同，我们通常的目标是在潜在变换后保留超过 80-90% 的原始方差。

在前面的代码示例中，我们计算了转换后保留的数据的方差。因此，我们现在可以增加或减少成分的数量，以保持转换数据中特定百分比的信息。这是一个非常有用的操作，并在许多实际的 NLP 应用中得到了使用。

注意，我们仍在使用词袋模型的原始单词字典。这个模型的一个特定缺点是，一个术语出现的频率越高，它的计数（以及因此的权重）就越高。这是一个问题，因为现在，任何不是停用词且在文本中频繁出现的术语都将获得高权重——无论该术语在特定文档中的重要性如何。因此，我们引入了另一个极其流行的预处理技术——**TF-IDF**。

## 使用 TF-IDF 测量单词的重要性

词袋方法的一个特定缺点是，我们仅仅计算一个上下文中单词的绝对数量，而不检查该单词是否在所有文档中普遍出现。一个在所有文档中都出现的术语可能对我们模型来说并不相关，因为它包含的信息较少，并且更频繁地出现在其他文档中。因此，在文本挖掘中，计算给定上下文中某个单词的重要性是一项重要的技术。

因此，我们希望计算一个上下文中术语的相对数量，而不是上下文中术语的绝对计数。通过这样做，我们将给只出现在特定上下文中的术语赋予更高的权重，并减少给出现在许多不同文档中的术语的权重。这正是 TF-IDF 算法所做的事情。根据以下方程式，很容易计算文档中术语 (*t*) 的权重 (*w*)：

![公式 07_001](img/Formula_07_001.png)

虽然词频 (*f*t) 计算了文档中的所有术语，但逆文档频率是通过将总文档数 (*N*) 除以所有文档中术语的计数 (*f*d*) 来计算的。*IDF* 术语通常进行对数变换，因为所有文档中术语的总数可能相当大。

在下面的示例中，我们不会直接使用 TF-IDF 函数。相反，我们将使用 `TfidfVectorizer`，它在一步中完成计数并将 TF-IDF 函数应用于结果。再次强调，该函数作为 `sklearn` 转换器实现，因此我们调用 `fit_transform()` 来训练和转换数据集：

[PRE36]

结果的格式与前面的示例类似，包含 `(document id, term id)` 对及其 TF-IDF 值：

[PRE37]

在前面的代码中，我们直接应用 `TfidfVectorizer`，它返回与使用 `CountVectorizer` 和 `TfidfTransformer` 结合相同的结果。我们转换包含词袋模型中单词的数据集，并返回 TF-IDF 值。我们还可以为每个 TF-IDF 值返回术语：

[PRE38]

上述代码返回模型的词汇表：

[PRE39]

在这个示例中，我们可以看到 `ground` 获得了 TF-IDF 值为 `0.667`，而所有其他术语的值均为 `0.333`。当向语料库中添加更多文档时，这个计数将相对缩放——因此，如果单词 `hold` 再次出现，TF-IDF 值将降低。

在任何实际的管道中，我们都会始终使用本章中介绍的所有技术——分词、停用词去除、词干提取、词形还原、n-gram/skip-gram、TF-IDF 和 SVD——结合在一个单一的管道中。结果将是一个由重要性加权的 n-gram/skip-gram 的标记的数值表示，并转换到潜在语义空间。使用这些技术进行你的第一个 NLP 管道将让你走得很远，因为你现在可以从你的文本数据中捕获大量信息。

到目前为止，我们已经学习了如何使用一维或 N 维标签、计数和加权词干和字符组合来数值化许多种类的分类和文本值。虽然许多这些方法在需要简单数值嵌入的许多情况下都表现良好，但它们都有一个严重的限制——它们不编码语义。让我们看看我们如何在同一个管道中提取文本的语义意义。

## 使用词嵌入提取语义

当计算新闻的相似性时，你会想象到像网球、*一级方程式*或*足球*这样的主题在语义上比像政治、经济或科学这样的主题更相似。然而，在之前讨论的技术中，所有编码的分类都被视为在语义上是相同的。在本节中，我们将讨论一种简单的语义嵌入方法，这也可以称为**词嵌入**。

之前讨论的管道使用LSA将多个文档转换为术语，然后将这些术语转换为可以与其他文档比较的语义概念。然而，语义意义基于术语出现和重要性——没有对单个术语之间的语义进行测量。

因此，我们寻找的是将术语嵌入到数值多维空间中的嵌入，这样每个单词就代表这个空间中的一个点。这使我们能够计算这个空间中多个单词之间的数值距离，以比较两个单词的语义意义。词嵌入最有趣的好处是，在词嵌入上的代数运算不仅数值上是可能的，而且是有意义的。考虑以下示例：

[PRE40]

我们可以通过将单词语料库映射到N维数值空间，并根据单词语义优化数值距离（例如，基于语料库中单词之间的距离）来创建这样的嵌入。结果优化输出语料库中单词及其N维数值表示的字典。在这个数值空间中，单词具有与语义空间中相同或至少相似的属性。一个巨大的好处是，这些嵌入可以无监督地训练，因此不需要标记的训练数据。

最早的嵌入之一被称为**Word2Vec**，它基于连续的词袋模型或连续的跳字模型来计数和测量窗口中的单词。让我们尝试这个功能，并使用Word2Vec进行语义词嵌入：

1.  最好的Python词嵌入实现是**Gensim**，我们也将在这里使用它。我们需要将我们的标记输入到模型中以便训练它：

    [PRE41]

在前面的代码中，我们加载了`Word2Vec`模型，并用之前章节中存储在`words`变量中的标记列表初始化它。`size`属性定义了结果向量的维度，`window`参数决定了我们应该考虑多少个单词作为每个窗口。一旦模型被训练，我们就可以简单地在该模型的字典中查找词嵌入。

代码将自动在我们提供的标记集上训练嵌入。结果模型将单词到向量的映射存储在`wv`属性中。理想情况下，我们还使用一个大型语料库或预训练模型，该模型由`gensim`或另一个NLP库（如`NLTK`）提供，以训练嵌入并使用较小的数据集进行微调。

1.  接下来，我们可以使用训练好的模型通过Word2Vec嵌入将我们文档中的所有术语嵌入。然而，这将导致多个向量，因为每个单词都返回其自己的嵌入。因此，你需要使用所有嵌入的数学平均值将所有向量组合成一个单一的向量。这个过程与用于生成LSA中概念的类似过程非常相似。此外，还有其他可能的缩减技术；例如，使用TF-IDF值对单个嵌入向量进行加权：

    [PRE42]

在前面的函数中，我们计算所有术语的词嵌入向量的平均值——这被称为**平均嵌入**，它代表了文档在嵌入空间中的概念。如果一个单词在嵌入中未找到，我们需要在计算中将它替换为零。

您可以通过下载预训练嵌入，例如在维基百科语料库上，来使用此类语义嵌入为您的应用程序。然后，您可以遍历您的清洗过的输入标记，并在数字嵌入的字典中查找单词。

GloVe是另一种流行的将单词编码为数值向量的技术，由斯坦福大学开发。与基于连续窗口的方法相比，它使用全局单词到单词共现统计来确定单词之间的线性关系：

1.  让我们看看在维基百科和Gigaword新闻档案上训练的预训练6B标记嵌入：

    [PRE43]

在前面的代码中，我们只打开并解析预训练的词嵌入，以便将单词和向量存储在查找字典中。

1.  然后，我们使用这个字典在我们的训练数据中查找标记，并通过计算所有GloVe向量的平均值来合并它们：

    [PRE44]

前面的代码与之前非常相似，每个单词返回一个向量，最后通过取平均值进行聚合。再次强调，这与使用训练数据中所有标记的语义概念相对应。

Gensim提供了其他流行的语义嵌入模型，如*doc2word*、*fastText*和*GloVe*。`gensim` Python库是利用这些预训练嵌入或训练您自己的模型的绝佳场所。现在您可以用单词向量的平均嵌入替换您的词袋模型，以捕获词义。然而，您的管道仍然由许多可调组件构建。

在下一节中，我们将探讨构建端到端最先进的语言模型以及重用Azure认知服务中的一些语言特征。

# 实现端到端语言模型

在前面的章节中，我们训练和连接了多个部分以实现一个最终算法，其中大多数单个步骤也需要进行训练。词形还原包含一个转换规则字典。停用词存储在字典中。词干提取需要为每种语言和每个需要嵌入训练的单词制定规则——TF-IDF 和 SVD 仅在训练数据上计算，但彼此独立。

这是一个类似于传统计算机视觉方法的问题，我们将在[*第 10 章*](B17928_10_ePub.xhtml#_idTextAnchor165)“在 Azure 上训练深度神经网络”中更深入地讨论，其中许多经典算法被组合成一个特征提取器和分类器的管道。类似于计算机视觉中通过梯度下降和反向传播训练的端到端模型的突破，深度神经网络——尤其是序列到序列模型——已经取代了手动执行每个转换和训练步骤的经典方法。

在本节中，首先，我们将查看如何通过自定义嵌入和 LSTM 实现来改进我们之前的模型，以对标记序列进行建模。这将帮助你更好地理解我们是如何从基于单个预处理器管道的个体方法过渡到使用深度学习的完整端到端方法的。

序列到序列模型是基于在可变输入集上训练的编码器和解码器模型。这种编码器/解码器架构用于各种任务，如机器翻译、图像标题和摘要。这些模型的优点之一是，你可以重用这个网络中的编码器部分，将一组输入转换为编码器的固定集数值表示。

接下来，我们将探讨最先进的语言表示模型，并讨论它们如何用于特征工程和文本数据的预处理。我们将使用 BERT 进行情感分析和数值嵌入。

最后，我们还将探讨如何重用 Azure 认知服务 API 进行文本分析，以执行高级建模和特征提取，例如文本或句子情感、关键词或实体识别。这是一个很好的方法，因为你可以利用微软的知识和大量训练数据，通过简单的 HTTP 请求来执行复杂的文本分析。

## 标记序列的端到端学习

我们不想将不同的算法片段连接成一个单一的管道，而是想要构建和训练一个端到端模型，该模型可以训练词嵌入、预形式潜在语义转换，并在单个模型中捕获文本中的顺序信息。

这种模型的优点是，每个处理步骤都可以在单个联合优化过程中针对用户的预测任务进行微调：

1.  管道的第一部分将看起来与前几节非常相似。我们将构建一个标记化器，将文档转换为标记序列，然后将其转换为基于标记序列的数值模型。然后，我们将使用`pad_sequences`将所有文档对齐到相同的长度：

    [PRE45]

1.  在下一步中，我们将使用Keras构建一个简单的模型，包括一个嵌入层和一个LSTM层来捕获标记序列。嵌入层将执行类似于GloVe的操作，将单词嵌入到语义空间中。LSTM单元将确保我们比较的是单词序列而不是单个单词。然后，我们将使用一个带有*softmax*激活函数的密集层来实现分类头：

    [PRE46]

如前所述的函数所示，我们使用三个层（即`Embedding`、`LSTM`和`Dense`）和一个`softmax`激活函数构建了一个简单的神经网络，用于分类。这意味着为了训练此模型，我们还需要同时解决一个分类问题。因此，我们需要标记的训练数据来使用这种方法进行分析。在下一节中，我们将探讨序列到序列模型是如何在输入输出文本序列中用于学习隐式文本表示的。

## 最前沿的序列到序列模型

在近年来，另一种类型的模型已经取代了传统的NLP管道——基于transformer的模型。这些类型的模型是完全端到端的，并使用序列到序列映射、位置编码和多头注意力层。这使得模型能够在文本中向前和向后查看，关注特定模式，并完全端到端地学习任务。正如你可能已经猜到的，这些模型具有复杂的架构，通常有超过一亿或超过十亿的参数。

序列到序列模型现在在许多复杂的端到端自然语言处理（NLP）问题中处于最前沿，例如分类（例如，情感或文本分析）、语言理解（例如，实体识别）、翻译、文本生成、摘要等等。

一种流行的序列到序列模型是BERT，今天，它存在许多不同的变体和配置。基于BERT架构的模型似乎表现特别出色，但已经被更新的架构、调整的参数或具有更多训练数据的模型所超越。

使用这些新的NLP模型的最简单方法是通过*Hugging Face*的`transformers`库，该库提供了端到端模型（或管道）以及预训练的标记器和模型。`transformers`库实现了*TensorFlow*和*PyTorch*的所有模型架构。这些模型可以轻松地在应用程序中使用，从头开始训练，或者使用特定领域的自定义训练数据进行微调。

以下示例展示了如何使用默认的 `sentiment-analysis` 流程实现情感分析，该流程在撰写本文时使用 `TFDistilBertForSequenceClassification` 模型：

[PRE47]

如前例所示，使用预训练模型进行端到端预测任务非常简单。这三行代码可以轻松集成到你的特征提取流程中，以丰富你的训练数据中的情感。

除了端到端模型之外，NLP 的另一个流行应用是在预处理文本数据时提供语义嵌入。这也可以使用 `transformers` 库和许多支持的模型之一来实现。

要做到这一点，首先，我们初始化一个预训练的 BERT 分词器。这将帮助我们将输入数据分割成适合 BERT 模型的正确格式：

[PRE48]

一旦我们将输入转换为标记序列，我们就可以评估 BERT 模型。要检索数值嵌入，我们需要理解编码器的潜在状态，我们可以使用 `last_hidden_state` 属性来检索：

[PRE49]

最后的隐藏层包含模型的潜在表示，我们现在可以用作模型中的语义数值表示：

[PRE50]

这些模型的关键启示是它们使用基于编码器/解码器的架构，这使我们能够简单地借用编码器将文本嵌入到语义数值特征空间中。因此，一个常见的方法是下载预训练模型，并通过网络的编码器部分进行正向传递。现在，固定大小的数值输出可以用作任何其他模型的特征向量。这是一个常见的预处理步骤，也是使用最先进的语言模型进行数值嵌入的良好权衡。

## 使用 Azure 认知服务的文本分析

在许多工程学科中，一个好的方法是不重复造轮子，因为许多其他公司已经比你更好地解决了相同的问题。对于微软开发、实施和训练的基本文本分析和文本理解任务，现在作为服务提供的情况可能也是如此。

如果我告诉你，当使用 Azure 时，文本理解功能，如情感分析、关键词提取、语言检测、命名实体识别以及**个人身份信息**（**PII**）的提取，只需一个请求即可？Azure 提供的 Text Analytics API 作为认知服务的一部分，将为您解决所有这些问题。

这并不能解决将文本转换为数值的需求，但它会使从文本中提取语义变得更加容易。一个例子就是使用认知服务作为额外的特征工程步骤，执行关键词提取或情感分析，而不是实现自己的 NLP 流程。

让我们实现一个函数，使用认知服务的文本分析API返回给定文档的情感。当你想在文本中添加额外的属性，如整体情感时，这非常棒。让我们首先设置所有需要调用认知服务API的参数：

[PRE51]

接下来，我们定义请求的内容和元数据。我们创建一个包含单个文档和要分析的文本的`payload`对象：

[PRE52]

最后，我们需要将payload、头部和参数发送到认知服务API：

[PRE53]

之前的代码看起来与我们在[*第2章*](B17928_02_ePub.xhtml#_idTextAnchor034)中看到的计算机视觉示例非常相似，即*在Azure中选择合适的机器学习服务*。事实上，它使用的是相同的API，但只是为文本分析和在此情况下情感分析功能使用了不同的端点。让我们运行这段代码并查看输出，输出看起来与以下片段非常相似：

[PRE54]

我们可以观察到，JSON响应包含每个文档的情感分类（`积极`、`中性`和`消极`）以及每个类别的数字置信度分数。此外，你可以看到，生成的文档存储在一个数组中，并标记了一个`id`值。因此，你可以使用ID来标识每个文档，向此API发送多个文档。

使用自定义预训练的语言模型很棒，但对于标准化的文本分析，我们可以简单地重用认知服务。微软在研究和生产这些语言模型上投入了大量的资源，你可以用相对较少的费用为自己的数据管道使用这些模型。因此，如果你更喜欢使用托管服务而不是运行自己的客户transformer模型，你应该尝试这个文本分析API。

# 摘要

在本章中，你学习了如何使用最先进的NLP技术对文本和分类的定名和有序数据进行预处理。

现在，你可以使用停用词去除、*词形还原*和*词干提取*、*n-gram*和计数词项出现来构建一个经典的NLP管道，使用*词袋模型*。我们使用*SVD*来降低结果特征向量的维度，并生成低维度的主题编码。对基于计数的词袋模型的一个重要调整是比较文档中术语的相对频率。你学习了*TF-IDF*函数，并可以使用它来计算一个词在文档中的重要性，与语料库相比。

在下一节中，我们探讨了*Word2Vec*和*GloVe*，它们是预训练的数字词嵌入字典。现在，你可以轻松地重用预训练的词嵌入，在商业NLP应用中实现显著的改进和准确性，这得益于词的语义嵌入。

最后，我们通过研究一种最先进的方法来结束这一章节，该方法使用端到端语言表示，例如 *BERT* 和基于 BERT 的架构，这些架构被训练为序列到序列模型。这些模型的好处是你可以重用编码器将一系列文本转换为数值表示，这在特征提取过程中是一个非常常见的任务。

在下一章中，我们将探讨如何使用 Azure Machine Learning 训练一个机器学习模型，应用我们迄今为止所学的一切。
