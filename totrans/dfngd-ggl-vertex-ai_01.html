<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><div id="_idContainer013">
			<h1 id="_idParaDest-15" class="chapter-number"><a id="_idTextAnchor014"/>1</h1>
			<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Machine Learning Project Life Cycle and Challenges</h1>
			<p>Today, <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) and <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) are integral parts of business<a id="_idIndexMarker000"/> strategy <a id="_idIndexMarker001"/>for many organizations, and more organizations are using them every year. The major reason for this adoption is the power of ML and AI solutions to garner more revenue, brand value, and cost savings. This increase in the adoption of AI and ML demands more skilled data and ML specialists and technical leaders. If you are an ML practitioner or beginner, this book will help you become a confident ML engineer or data scientist with knowledge of Google’s best practices. In this chapter, we will discuss the basics of the life cycle and the challenges and limitations of ML when developing <span class="No-Break">real-world applications.</span></p>
			<p>ML projects often involve a defined set of steps from problem statements to deployments. It is essential to understand the importance and common challenges involved with these steps to complete a successful and impactful project. In this chapter, we will discuss the importance of understanding the business problem, the common steps involved in a typical ML project life cycle, and the challenges and limitations of ML in detail. This will help new ML practitioners understand the basic project flow; plus, it will help create a foundation for forthcoming chapters in <span class="No-Break">this book.</span></p>
			<p>This chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>ML project <span class="No-Break">life cycle</span></li>
				<li>Common challenges in developing real-world <span class="No-Break">ML solutions</span></li>
				<li>Limitations <span class="No-Break">of ML</span></li>
			</ul>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>ML project life cycle</h1>
			<p>In this <a id="_idIndexMarker002"/>section, we will learn about the typical life cycle of an ML project, from defining the problem to model development, and finally, to the operationalization of the model. <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.1</em> shows the high-level steps almost every ML project goes through. Let’s go through all these steps <span class="No-Break">in detail.</span></p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B17792_01_1.jpg" alt="Figure 1.1 – Life cycle of a typical ML project" width="798" height="672"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Life cycle of a typical ML project</p>
			<p>Just like the <strong class="bold">Software Development Life Cycle</strong> (<strong class="bold">SDLC</strong>), the <strong class="bold">Machine Learning Project</strong>/<strong class="bold">Development Lifecycle </strong>(<strong class="bold">MDLC</strong>) guides<a id="_idIndexMarker003"/> the end-to-end process of ML model <a id="_idIndexMarker004"/>development and <a id="_idIndexMarker005"/>operationalization. At a high level, the life cycle of a typical ML project in an enterprise setting remains somewhat consistent and includes eight <span class="No-Break">key steps:</span></p>
			<ol>
				<li>Define the ML use case: The first step of any ML project is where the ML team works <a id="_idIndexMarker006"/>with business stakeholders to assess the business needs around predictive analytics and identifies a use case where ML can be used, along with some success criteria, performance metrics, and possible datasets that can be used to build <span class="No-Break">the models.</span><p class="list-inset">For example, if the sales/marketing department of an insurance company called ABC Insurance Inc. wants to better utilize its resources to target customers who are more likely to buy a certain product, they might approach the ML team to build a solution that can sift through all possible leads/customers and, based on the data points for each lead (age, prior purchase, length of policy history, income level, etc.), identify<a id="_idIndexMarker007"/> the customers who are most likely to buy a policy. Then the sales team can ask their customer representatives to prioritize reaching out to these customers instead of calling all possible customers blindly. This can significantly improve the outcome of outbound calls by the reps and improve the <span class="No-Break">sales-related KPIs.</span></p><p class="list-inset">Once the use case is defined, the next step is to define a set of KPIs to measure the success of the solution. For this sales use case, this could be the customer sign-up rate—what percentage of the customers whom sales reps talk to sign up for a new <span class="No-Break">insurance policy?</span></p><p class="list-inset">To measure the effectiveness of the ML solution, the sales team and the ML team might agree to measure the increase or decrease in customer sign-up rate once the ML model is live and iteratively improve on the model to optimize the <span class="No-Break">sign-up rate.</span></p><p class="list-inset">At this stage, there will also be a discussion about the possible datasets that can be utilized for the model training. These could include <span class="No-Break">the following:</span></p><ul><li>Internal customer/product datasets being generated by marketing and sales teams, for example, customer metadata, such as their age, education profile, income level, prior purchase behavior, number and type of vehicles they <span class="No-Break">own, etc.</span></li><li>External datasets that can be acquired through third parties; for example, an external marketing consultancy might have collected data about the insurance purchase behavior of customers based on the car brand they own. This additional data can be used to predict how likely they are to purchase the insurance policy being sold by ABC <span class="No-Break">Insurance Inc.</span></li></ul></li>
				<li>Explore/analyze data: The <a id="_idIndexMarker008"/>next step is to do a detailed analysis of the datasets. This is usually an iterative process in which the ML team works closely with the data and business SMEs to better understand the nuances of the available datasets, including <span class="No-Break">the following:</span><ul><li><span class="No-Break">Data sources</span></li><li><span class="No-Break">Data granularity</span></li><li><span class="No-Break">Update frequency</span></li><li>Description of individual data points and their <span class="No-Break">business meaning</span></li></ul><p class="list-inset">This is<a id="_idIndexMarker009"/> a key step where data scientists/ML engineers analyze the available data and decide what datasets might be relevant to the ML solution being considered, analyze the robustness of the data, and identify any gaps. Issues that the team might identify at this stage could relate to the cleanliness and completeness of data or problems with the timely availability of the data in production. For example, the age of the customer could be a great indicator of their purchase behavior, but if it’s an optional field in the customer profile, only a handful of customers might have provided their date of birth <span class="No-Break">or age.</span></p><p class="list-inset">So, the team would need to figure out if they want to use the field and, if so, how to handle the samples where age is missing. They could also work with sales and marketing teams to make the field a <em class="italic">required field</em> whenever a new customer requests an insurance quote online and generates a lead in <span class="No-Break">the system.</span></p></li>
				<li>Select ML model type: Once <a id="_idIndexMarker010"/>the use case has been identified along with the datasets that can possibly be used to train the model, the next step is to consider the types of models that can be used to achieve the requirements. We won’t go too deep into the topic of general model selection here since entire books could be written on the topic, but in the next few chapters, you will see what different model types can be built for specific use cases in Vertex AI. At a very high level, the key considerations at this stage are <span class="No-Break">as follows:</span><ul><li>Type of model: For example, for the insurance customer/lead ranking example, we could build a classification model that will predict whether a new<a id="_idIndexMarker011"/> customer is <em class="italic">high/medium/low</em> in terms of their likelihood to purchase a policy. Or a regression model could be built to output a sales probability number for each <span class="No-Break">likely customer.</span></li><li>Does the conventional ML model satisfy our requirements or do we need a deep <span class="No-Break">learning model?</span></li><li>Explainability requirements: Does the use case require an explanation for each prediction as to why the sample was classified a <span class="No-Break">certain way?</span></li><li>Single versus ensemble model: Do we need a single model to give us the final prediction, or do we need to employ a set of interconnected models that feed into each other? For example, a first model might assign a customer to a particular customer group, and the next model might use that grouping to identify the final likelihood <span class="No-Break">of purchase.</span></li><li>Separation of models: For example, sometimes we might build a single global model for the entire customer base, or we might need separate models for each region due to significant differences in products and user behavior in <span class="No-Break">different regions.</span></li></ul></li>
				<li>Feature engineering: This <a id="_idIndexMarker012"/>process is usually the most time-consuming and involves <span class="No-Break">several steps:</span><ol><li class="upper-roman">Data cleanup–Imputing missing values where possible, dropping fields with too many <span class="No-Break">missing values</span></li><li class="upper-roman">Data and feature augmentation–Joining datasets to bring in additional fields, and cross-joining existing features to generate <span class="No-Break">new features</span></li><li class="upper-roman">Feature analysis–Calculating feature correlation and analyzing collinearity, checking for data leakage <span class="No-Break">in features</span></li></ol><p class="list-inset">Again, since this is an extremely broad topic, we are not diving too deep into it and suggest you refer to other books on <span class="No-Break">this topic.</span></p></li>
				<li>Iterate over the model design/build: The actual design and build of the ML model is an <a id="_idIndexMarker013"/>iterative process involving the following <span class="No-Break">key steps:</span><ol><li class="upper-roman">Select <span class="No-Break">model architecture</span></li><li class="upper-roman">Split acquired data into <span class="No-Break">train/validation/test subsets</span></li><li class="upper-roman">Run model training experiments, <span class="No-Break">tune hyperparameters</span></li><li class="upper-roman">Evaluate trained models with the <span class="No-Break">test dataset</span></li><li class="upper-roman">Rank and select the <span class="No-Break">best models</span></li></ol><p class="list-inset"><span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.2</em> shows the typical ML model development <span class="No-Break">life cycle:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B17792_01_2.jpg" alt="Figure 1.2 – ML model development life cycle" width="1650" height="567"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – ML model development life cycle</p>
			<ol>
				<li value="6">Consensus on results: Once<a id="_idIndexMarker014"/> a satisfactory model has been obtained, the ML team shares the results with the business stakeholders to ensure the results fully align with the business needs and performs additional optimizations and post-processing steps to make the model predictions usable by the business. To assure business stakeholders that ML solution is aligned with the business goals and is accurate enough to drive value, ML teams<a id="_idIndexMarker015"/> could use one of a number <span class="No-Break">of approaches:</span><ul><li>Evaluate using historical test datasets: ML teams can run historical data through the new ML models and evaluate the predictions against the ground truth values. For example, in the insurance use case discussed previously, the ML team can take last month’s data on customer leads and use the ML model to predict which customers are most likely to purchase a new insurance policy. Then they can compare the model’s predictions against the actual purchase history from the previous month and see how accurate the model’s predictions were. If the model’s output is close to the real purchase behavior of customers, then the model is working as desired, and this information can be presented to business stakeholders to convince them of the ML solution’s efficacy in driving additional revenue. On the contrary, if the model’s output significantly deviates from the customer’s behavior, the ML team needs to go back and work on improving the model. This usually is an iterative process and can take a number of iterations, depending on the complexity of <span class="No-Break">the model.</span></li><li>Evaluate with live data: In some scenarios, an organization might decide to conduct a small pilot in a production environment with real-time data to assess the performance of the new ML model. This is usually done in the <span class="No-Break">following scenarios:</span><ul><li>When there is no historical data available to conduct the evaluation or where testing with historical data is not expected to be an accurate; for example, during the onset of COVID, customer behavior patterns abruptly changed to the extent that testing with any historical data became <span class="No-Break">nearly useless</span></li><li>When there is an existing model in production being used for critical real-time predictions, the sanity check for the new model needs to be performed not just in terms of its accuracy but also its subtle impact on downstream KPIs such as revenue per <span class="No-Break">user session</span></li></ul></li></ul><p class="list-inset">In such cases, teams<a id="_idIndexMarker016"/> might deploy the model in production, divert a small number of prediction requests to the newer model, and periodically compare the overall impact on the KPIs. For example, in the case of a recommendation model deployed on an e-commerce website, a recommendation model might start recommending products that are comparatively cheaper than the predictions from the older model already live in production. In this scenario, the likelihood of a customer completing a purchase would go up, but at the same time, the revenue generated per user session would decrease, impacting overall revenue for the organization. So, although it might seem like the ML model is working as designed, it might not be considered a success by the business/sales stakeholders, and more discussions would be required to <span class="No-Break">optimize it.</span></p></li>
				<li>Operationalize model: Once the model has been approved for deployment in production, the<a id="_idIndexMarker017"/> ML team will work with their organization’s IT and data engineering teams to deploy the model so that other applications can start utilizing it to generate insights. Depending on the size of the organization, there can be significant overlap in the roles these <span class="No-Break">teams play.</span><p class="list-inset">The actual deployment architecture would depend on <span class="No-Break">the following:</span></p><ul><li>Prediction SLAs – Ranging from periodic batch jobs to solutions that require sub-second <span class="No-Break">prediction performance.</span></li><li>Compliance requirements – Can the user data be sent to third-party cloud providers, or does it need to always reside within an organization’s <span class="No-Break">data centers?</span></li><li>Infrastructure requirements – This depends on the size of the model and its compute requirements. Small models can be served from a shared compute node. Some large models might need a large <span class="No-Break">GPU-connected node.</span></li></ul><p class="list-inset">We will discuss this topic in detail in later chapters, but the following figure shows some key <a id="_idIndexMarker018"/>components you might consider as part of your <span class="No-Break">deployment architecture.</span></p></li>
			</ol>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B17792_01_3.jpg" alt="Figure 1.3 – Key components of ML model training and deployment" width="1650" height="628"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – Key components of ML model training and deployment</p>
			<ol>
				<li value="8">Monitor and retrain: It might seem as if the ML team’s job is done once the model has <a id="_idIndexMarker019"/>been operationalized, but in real-world<a id="_idIndexMarker020"/> deployments, most models require periodic or sometimes constant monitoring to ensure the model is operating within the required performance thresholds. Model performance could become sub-optimal for <span class="No-Break">several reasons:</span><ul><li>Data drift: Changes in data being used to generate predictions could change significantly and impact the model’s performance. As we discussed before, during COVID, customer behavior changed significantly. Models that were trained on pre-COVID customer behavior data were not equipped to handle this sudden change in usage patterns. Change due to the pandemic was relatively rare but high-impact, but there are plenty of other smaller changes in prediction input data that might impact your model’s performance adversely. The impact could range from a subtle drop in accuracy to a model generating erroneous responses. So, it is important to keep an eye on the key performance metrics of your <span class="No-Break">ML solution.</span></li><li>Change prediction request volume: If your solution was designed to handle 100 requests per second but now is seeing periodic bursts in traffic of around 1,000 requests per second, your solution might not be able to keep up with the demand, or latency might go above acceptable levels. So, your solution also needs to have monitoring and certain levels of auto-scaling built in to handle such scenarios. For larger changes in traffic volumes, you might even need to completely rethink the <span class="No-Break">serving architecture.</span></li></ul><p class="list-inset">There<a id="_idIndexMarker021"/> would be scenarios where through <a id="_idIndexMarker022"/>monitoring, you will discover that your ML model no longer meets the prediction accuracy and requires retraining. If the change in data patterns is expected, the ML team should design the solution to support automatic periodic retraining. For example, in the retail industry, product catalogs, pricing, and promotions constantly evolve, requiring regular retraining of the models. In other scenarios, the change might be gradual or unexpected, and when the monitoring system alerts the ML team of the model performance degradation, they need to take a call on retraining the model with more recent data, or maybe even completely rebuilding the model with <span class="No-Break">new features.</span></p></li>
			</ol>
			<p>Now that we have a good idea of the life cycle of an ML project, let’s learn about some of the common challenges faced by ML developers when creating and deploying <span class="No-Break">ML solutions.</span></p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Common challenges in developing real-world 
ML solutions</h1>
			<p>A real-world ML project<a id="_idIndexMarker023"/> is always filled with some unexpected challenges that we get to experience at different stages. The main reason for this is that the data present in the real world, and the ML algorithms, are not perfect. Though these challenges hamper the performance of the overall ML setup, they don’t prevent us from creating a valuable ML application. In a new ML project, it is difficult to know the challenges up front. They are often found during different stages of the project. Some of these challenges are not obvious and require skilled or experienced ML practitioners (or data scientists) to identify them and apply countermeasures to reduce <span class="No-Break">their effect.</span></p>
			<p>In this <a id="_idIndexMarker024"/>section, we will understand some of the common challenges encountered during the development of a typical ML solution. The following list shows some common challenges we will discuss in <span class="No-Break">more detail:</span></p>
			<ul>
				<li>Data collection <span class="No-Break">and security</span></li>
				<li>Non-representative <span class="No-Break">training set</span></li>
				<li>Poor quality <span class="No-Break">of data</span></li>
				<li>Underfitting of the <span class="No-Break">training dataset</span></li>
				<li>Overfitting of the <span class="No-Break">training dataset</span></li>
				<li><span class="No-Break">Infrastructure requirements</span></li>
			</ul>
			<p>Now, let’s learn about each of these common challenges <span class="No-Break">in detail.</span></p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Data collection and security</h2>
			<p>One of the <a id="_idIndexMarker025"/>most common challenges that organizations face is data availability. ML algorithms require a large amount of good-quality data in order to provide quality results. Thus, the availability of raw data is critical for a business if it wants to implement ML. Sometimes, even if the raw data is available, gathering data is not the only concern; we often need to transform or process the data in a way that our ML <span class="No-Break">algorithm supports.</span></p>
			<p>Data security is another important challenge that is very frequently faced by ML developers. When we get data from a company, it is essential to differentiate between sensitive and non-sensitive information to implement ML correctly and efficiently. The sensitive part of data needs to be stored in fully secured servers (storage systems) and should always be kept encrypted. Sensitive data should be avoided for security purposes, and only the less-sensitive data access should be given to trusted team members working on the project. If the data contains <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>), it can<a id="_idIndexMarker026"/> still be used by anonymizing <span class="No-Break">it properly.</span></p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>Non-representative training data</h2>
			<p>A good <a id="_idIndexMarker027"/>ML model is one that performs equally well on unseen data and training data. It is only possible when your training data is a good representative of most possible business scenarios. Sometimes, when the dataset is small, it may not be a true representative of the inherent distribution, and the resulting model may provide inaccurate predictions on unseen datasets despite having high-quality results on the training dataset. This kind of non-representative data is either the result of sampling bias or the unavailability of data. Thus, an ML model trained on such a non-representative dataset may have less value when it is deployed <span class="No-Break">in production.</span></p>
			<p>If it is impossible to get a true representative training dataset for a business problem, then it’s better to limit the scope of the problem to only the scenarios for which we have a sufficient amount of training samples. In this way, we will only get known scenarios in the unseen dataset, and the model should provide quality predictions. Sometimes, the data related to a business problem keeps changing with time, and it may not be possible to develop a single static model that works well; in such cases, continuous retraining of the model on the latest data <span class="No-Break">becomes essential.</span></p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>Poor quality of data</h2>
			<p>The<a id="_idIndexMarker028"/> performance of ML algorithms is very sensitive to the quality of training samples. A small number of outliers, missing data cases, or some abnormal scenarios can affect the quality of the model significantly. So, it is important to treat such scenarios carefully while analyzing the data before training any ML algorithm. There are multiple methods for identifying and treating outliers; the best method depends upon the nature of the problem and the data itself. Similarly, there are multiple ways of treating the missing values as well. For example, mean, median, mode, and so on are some frequently used methods to fill in missing data. If the training data size is sufficiently large, dropping a small number of rows with missing values is also a <span class="No-Break">good option.</span></p>
			<p>As discussed, the quality of the training dataset is important if we want our ML system to learn accurately and provide quality results on the unseen dataset. It means that the data pre-processing part of the ML life cycle should be taken <span class="No-Break">very seriously.</span></p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor021"/>Underfitting the training dataset</h2>
			<p>Underfitting<a id="_idIndexMarker029"/> an ML model means that the model is too simple to learn the inherent information or structure of the training dataset. It may occur when we try to fit a non-linear distribution using a linear ML algorithm such as linear regression. Underfitting may also occur when we utilize only a minimal set of features (that may not have much information about the target distribution) while training the model. This type of model can be too simple to learn the target distribution. An underfitted model learns too little from the training data and, thus, makes mistakes on unseen or <span class="No-Break">test datasets.</span></p>
			<p>There are multiple ways to tackle the problem of underfitting. Here is a list of some <span class="No-Break">common methods:</span></p>
			<ul>
				<li>Feature engineering – add more features that represent <span class="No-Break">target distribution</span></li>
				<li>Non-linear algorithms – switch to a non-linear algorithm if the target distribution is <span class="No-Break">not linear</span></li>
				<li>Removing noise from <span class="No-Break">the data</span></li>
				<li>Add more power to the model – increase trainable parameters, increase depth or number of trees in <span class="No-Break">tree-based ensembles</span></li>
			</ul>
			<p>Just like underfitting the model on training data, overfitting is also a big issue. Let’s deep dive <span class="No-Break">into it.</span></p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Overfitting the training dataset</h2>
			<p>The <a id="_idIndexMarker030"/>overfitting problem is the opposite of the underfitting problem. Overfitting is the scenario when the ML model learns too much unnecessary information from the training data and fails to generalize on a test or unseen dataset. In this case, the model performs extremely well on the training dataset, but the metric value (such as accuracy) is very low on the test set. Overfitting usually occurs when we implement a very complex algorithm on <span class="No-Break">simple datasets.</span></p>
			<p>Some common methods to address the problem of overfitting are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Increase training data size – ML models often overfit on <span class="No-Break">small datasets</span></li>
				<li>Use simpler models – When problems are simple or linear in nature, choose simple <span class="No-Break">ML algorithms</span></li>
				<li>Regularization – There are multiple regularization methods that prevent complex models from overfitting on the <span class="No-Break">training dataset</span></li>
				<li>Reduce<a id="_idIndexMarker031"/> model complexity – Use a smaller number of trainable parameters, train for a smaller number of epochs, and reduce the depth of <span class="No-Break">tree-based models</span></li>
			</ul>
			<p>Overfitting and underfitting are common challenges and should be addressed carefully, as discussed earlier. Now, let’s discuss some <span class="No-Break">infrastructure-related challenges.</span></p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>Infrastructure requirements</h2>
			<p>ML is<a id="_idIndexMarker032"/> expensive. A typical ML project often involves crunching large datasets with millions or billions of samples. Slicing and dicing such datasets requires a lot of memory and high-end multi-core processors. Additionally, once the development of the project is complete, dedicated servers are required to deploy the models and match the scale of consumers. Thus, business organizations willing to practice ML need some dedicated infrastructure to implement and consume ML efficiently. This requirement increases further when working with large, deep learning models such<a id="_idIndexMarker033"/> as transformers, <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>), and so on. Such models usually require a set of <a id="_idIndexMarker034"/>accelerators, <strong class="bold">graphical processing units</strong> (<strong class="bold">GPUs</strong>), or <strong class="bold">tensor processing units (TPUs</strong>) for training, finetuning, <span class="No-Break">and</span><span class="No-Break"><a id="_idIndexMarker035"/></span><span class="No-Break"> deployment.</span></p>
			<p>As we have discussed, infrastructure is critical for practicing ML. Companies that lack such infrastructure can consult with other firms or adopt cloud-based offerings to start developing <span class="No-Break">ML-based applications.</span></p>
			<p>Now that we understand the common challenges faced during the development of an ML project, we should be able to make more informed decisions about them. Next, let’s learn about some of the limitations <span class="No-Break">of ML.</span></p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor024"/>Limitations of ML</h1>
			<p>ML is <a id="_idIndexMarker036"/>very powerful, but it’s not the answer to every single problem. There are problems that ML is just not suitable for, and there are some cases where ML can’t be applied due to technical or business constraints. As an ML practitioner, it is important to develop the ability to find relevant business problems where ML can provide significant value instead of applying it blindly everywhere. Additionally, there are algorithm-specific limitations that can render an ML solution not applicable in some business applications. In this section, we will learn about some common limitations of ML that should be kept in mind while finding relevant <span class="No-Break">use cases.</span></p>
			<p>Keep in mind that<a id="_idIndexMarker037"/> the limitations we are discussing in this section are very general. In real-world applications, there are more limitations possible due to the nature of the problem we are solving. Some common limitations that we will discuss in detail are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break">Data-related concerns</span></li>
				<li>Deterministic nature <span class="No-Break">of problems</span></li>
				<li>Lack of interpretability <span class="No-Break">and reproducibility</span></li>
				<li>Concerns related to cost <span class="No-Break">and customizations</span></li>
				<li>Ethical concerns <span class="No-Break">and bias</span></li>
			</ul>
			<p>Let’s now deep dive into each of these <span class="No-Break">common limitations.</span></p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>Data-related concerns</h2>
			<p>The<a id="_idIndexMarker038"/> quality of an ML model highly depends upon the quality of the training data it is provided with. Data present in the real world is often noisy, incomplete, unlabeled, and sometimes unusable. Moreover, most supervised learning algorithms require large amounts of properly labeled training data to produce good results. The training data requirements of some algorithms (e.g., deep learning) are so high that even manually labeling data is not an option. And even if we manage to label the data manually, it is often error-prone due to <span class="No-Break">human bias.</span></p>
			<p>Another major issue is incompleteness or missing data. For example, consider the problem of automatic speech recognition. In this case, model results are highly biased toward the accent present in the training dataset. A model that is trained on the American accent doesn’t produce good results on other accented speech. Since accents change significantly as we travel to different parts of the world, it is hard to gather and label relevant<a id="_idIndexMarker039"/> amounts of training data for every possible accent. For this reason, developing a single speech recognition model that works for everyone is not yet feasible, and thus, the tech giants providing speech recognition solutions often develop accent-specific models. Developing a new model for each new accent is not <span class="No-Break">very scalable.</span></p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Deterministic nature of problems</h2>
			<p>ML has achieved <a id="_idIndexMarker040"/>great success in solving some highly complex problems, such as numerical weather prediction. One problem with most of the current ML algorithms is that they are stochastic in nature and thus cannot be trusted blindly when the problem is deterministic. Considering the case of numerical weather prediction, today we have ML models that can predict rain, wind speed, air pressure, and so on, with acceptable accuracy, but they completely fail to understand the physics behind real weather systems. For example, an ML model might provide negative value estimations of parameters such <span class="No-Break">as density.</span></p>
			<p>However, it is very likely that these kinds of limitations can be overcome in the near future. Future research in the field of ML might discover new algorithms that are smart enough to understand the physics of our world. Such models will open infinite possibilities in <span class="No-Break">the future.</span></p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Lack of interpretability and reproducibility</h2>
			<p>One major issue with <a id="_idIndexMarker041"/>many ML algorithms (and often with neural networks) is the lack of interpretability of results. Many business applications, such <a id="_idIndexMarker042"/>as fraud detection and disease prediction, require a justification for model results. If an ML model classifies a financial transaction as fraud, it should also provide solid evidence for the decision; otherwise, this output may not be useful for the business. Deep learning or neural network models often lack interpretability, and the explainability of such models is an active area of research. Multiple methods have been developed for model interpretability or explainability purposes. Though these methods can provide some insights into the results, they are still far from the <span class="No-Break">actual requirements.</span></p>
			<p>Reproducibility, on the other hand, is another complex and growing issue with ML solutions. Some of the<a id="_idIndexMarker043"/> latest research papers might show us great improvements in results using <a id="_idIndexMarker044"/>some technological advancements on a fixed set of datasets, but the same method may not work in real-world scenarios. Secondly, ML models are often unstable, which means that they produce different results when trained on different partitions of the dataset. This is a challenging situation because models developed for one business segment may be completely useless for another business segment, even though the underlying problem statement is similar. This makes them <span class="No-Break">less reusable.</span></p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>Concerns related to cost and customizations</h2>
			<p>Developing <a id="_idIndexMarker045"/>and maintaining ML solutions is often expensive, more so in the case of deep learning algorithms. Development costs may come from employing highly skilled developers as well as the infrastructure needed for data analytics and ML experimentation. Deep learning models usually require high-compute <a id="_idIndexMarker046"/>resources such as GPUs and TPUs for training and experimentation. Running a hyperparameter tuning job with such models is even more costly and time-consuming. Once the model is ready for production, it requires dedicated resources for deployment, monitoring, and maintenance. This cost further increases as you scale your deployments to serve a large number of customers, and even more if there are very low latency concerns. Thus, it is very important to understand the value that our solution is going to bring before jumping into the development phase and check whether it is worth <span class="No-Break">the investment.</span></p>
			<p>Another concern with the ML solutions is their lack of customizations. ML models are often very difficult to customize, meaning it can be hard to change their parameters or make them adapt to new datasets. Pre-built general-purpose ML solutions often do not work well on specific business use cases, and this leaves them with two choices – either to develop the solution from scratch or customize the prebuilt general-purpose solutions. Though the customization of prebuilt models seems like a better choice here, even the customization is not easy in the case of ML models. ML model customization requires a skilled set of data engineers and ML specialists with a deep understanding of technical concepts such as deep learning, predictive modeling, and <span class="No-Break">transfer learning.</span></p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Ethical concerns and bias</h2>
			<p>ML is quite<a id="_idIndexMarker047"/> powerful and is adopted today by many organizations to guide their business strategy and decisions. As we know, some of these ML algorithms are <em class="italic">black boxes</em>; they may not provide reasons behind their decisions. ML systems are trained on a finite set of datasets, and they may not apply to some real-world scenarios; if those scenarios are encountered in the future, we can’t tell what decision the ML system will take. There might be ethical concerns related to such black-box decisions. For example, if a self-driving car is involved in a road accident, whom should you blame – the driver, the team that developed the AI system, or the car manufacturer? Thus, it is clear that the current advancements in ML and AI are not suitable for ethical or moral decision-making. Also, we need a framework to solve ethical concerns involving ML and <span class="No-Break">AI systems.</span></p>
			<p>The accuracy and speed of ML solutions are often commendable, but these solutions cannot always be trusted to be fair and unbiased. Consider AI software that recognizes faces or objects in a given image; this system could go wrong on photos where the camera is not able to capture racial sensitivity properly, or it may classify a certain type of dog (that is somewhat similar to a cat) as a cat. This kind of bias may come from a biased set of training or testing datasets used for developing AI systems. Data present in the real world is often collected and labeled by humans; thus, the bias that exists in humans is transferred into AI systems. Avoiding bias completely is impossible as we all are humans and are thus biased, but there are measures that can be taken to reduce it. Establishing a culture of ethics and building teams from diverse backgrounds can be a good step to reduce bias to a <span class="No-Break">certain extent.</span></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Summary</h1>
			<p>ML is an integral part of any business strategy and decisions for many organizations today, thus it is very important to do it right. In this chapter, we learned about the general steps involved in a typical ML project development life cycle and their significance. We also highlighted some common challenges that ML practitioners face while undergoing project development. Finally, we listed some of the common limitations of ML in real-world scenarios to help us choose the right business problem and a fitting ML algorithm to <span class="No-Break">solve it.</span></p>
			<p>In this chapter, we learned about the importance of choosing the right business problem in order to deliver the maximum impact using ML. We also learned about the general flow of a typical ML project. We should now be confident about identifying the underlying ML-related challenges in a business process and making informed decisions about them. Finally, we have learned about the common limitations of ML algorithms, and it will help us apply ML in a better way to get the best out <span class="No-Break">of it.</span></p>
			<p>Just developing a high-performing ML model is not enough. The real value comes when it is deployed and used in real-world applications. Taking an ML model to production is not trivial and should be done in the right way. The next chapter is all about the guidelines and best practices to follow while operationalizing an ML model and it is going to be extremely important to understand it thoroughly before jumping into the later chapters of <span class="No-Break">this book.</span></p>
		</div>
	</div>
</div>
</body></html>