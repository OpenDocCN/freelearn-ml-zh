- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Working with H2O AutoML and Apache Spark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与H2O AutoML和Apache Spark协同工作
- en: In [*Chapter 10*](B17298_10.xhtml#_idTextAnchor196), *Working with Plain Old
    Java Objects (POJOs)*, and [*Chapter 11*](B17298_11.xhtml#_idTextAnchor210), *Working
    with Model Object, Optimized (MOJO)*, we explored how to build and deploy our
    **Machine Learning** (**ML**) models as POJOs and MOJOs in production systems
    and use them to make predictions. In the majority of real-world problems, you
    will often need to deploy your entire ML pipeline in production so that you can
    deploy as well as train models on the fly. Your system will also be gathering
    and storing new data that you can later use to retrain your models. In such a
    scenario, you will eventually need to integrate your H2O server into your business
    product and coordinate the ML effort.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第10章*](B17298_10.xhtml#_idTextAnchor196) *使用普通的Java对象（POJOs）*和[*第11章*](B17298_11.xhtml#_idTextAnchor210)
    *使用模型对象，优化（MOJO）*中，我们探讨了如何在生产系统中构建和部署我们的**机器学习**（**ML**）模型作为POJOs和MOJOs，并使用它们进行预测。在大多数现实世界的问题中，你通常会需要在生产中部署你的整个ML管道，这样你就可以实时部署和训练模型。你的系统也将收集和存储新的数据，你可以稍后使用这些数据重新训练你的模型。在这种情况下，你最终需要将你的H2O服务器集成到你的商业产品中，并协调ML工作。
- en: Apache Spark is one of the more commonly used technologies in the domain of
    ML. It is an analytics engine used for large-scale data processing using cluster
    computing. It is completely open source and widely supported by the Apache Software
    Foundation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是机器学习领域更常用的技术之一。它是一个用于大规模数据处理的集群计算分析引擎。它是完全开源的，并且得到了Apache软件基金会的广泛支持。
- en: Considering the popularity of Spark in the field of data processing, H2O.ai
    developed an elegant software solution that combines the benefits of both Spark
    and AutoML into a single one-stop solution for ML pipelines. This software product
    is called H2O Sparkling Water.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到Spark在数据处理领域的普及，H2O.ai开发了一个优雅的软件解决方案，将Spark和AutoML的优点结合成一个一站式解决方案，用于机器学习（ML）管道。这个软件产品被称为H2O
    Sparkling Water。
- en: In this chapter, we will learn more about H2O Sparkling Water. First, we will
    understand what Spark is and how it works and then move on to understanding how
    H2O Sparkling Water operates H2O AutoML in conjunction with Spark to solve fast
    data processing needs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地了解H2O Sparkling Water。首先，我们将了解Spark是什么以及它是如何工作的，然后继续了解H2O Sparkling
    Water如何与Spark结合操作H2O AutoML，以满足快速数据处理需求。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Exploring Apache Spark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Apache Spark
- en: Exploring H2O Sparkling Water
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索H2O Sparkling Water
- en: By the end of this chapter, you should have a general idea of how we can incorporate
    H2O AI along with Apache Spark using H2O Sparkling Water and how you can benefit
    from the best of both these worlds.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该对如何使用H2O Sparkling Water将H2O AI与Apache Spark相结合有一个大致的了解，以及你如何从这两个世界的最佳之处受益。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will require the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，你需要以下内容：
- en: The latest version of your preferred web browser.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你首选的网页浏览器的最新版本。
- en: An **Integrated Development Environment** (**IDE**) of your choice or a Terminal.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你选择的**集成开发环境**（**IDE**）或终端。
- en: All experiments conducted in this chapter have been performed on a Terminal.
    You are free to follow along using the same setup or perform the same experiments
    using any IDE of your choice.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章中进行的所有实验都是在终端上进行的。你可以自由地使用相同的设置来跟随，或者使用你选择的任何IDE进行相同的实验。
- en: So, let’s start by understanding what exactly Apache Spark is.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们首先了解Apache Spark究竟是什么。
- en: Exploring Apache Spark
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Apache Spark
- en: Apache Spark started as a project in UC Berkeley AMPLab in 2009\. It was then
    open sourced under a BSD license in 2010\. Three years later, in 2013, it was
    donated to the Apache Software Foundation and became a top-level project. A year
    later, it was used by Databricks in a data sorting competition where it set a
    new world record. Ever since then, it has been picked up and used widely for in-memory
    distributed data analysis in the big data industry.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark始于2009年在加州大学伯克利分校的AMPLab的一个项目。它在2010年以BSD许可证开源。三年后，即2013年，它被捐赠给Apache软件基金会，成为顶级项目。一年后，它在Databricks举办的数据排序比赛中被使用，并创下了新的世界纪录。从那时起，它被广泛用于大数据行业中的内存分布式数据分析。
- en: Let’s see what the various components of Apache Spark are and their respective
    functionalities.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Apache Spark的各个组件及其相应的功能。
- en: Understanding the components of Apache Spark
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Apache Spark的组件
- en: '**Apache Spark** is an open source data processing engine. It is used to process
    data in real time, as well as in batches using cluster computing. All data processing
    tasks are performed in memory, making task executions very fast. Apache Spark’s
    data processing capabilities coupled with H2O’s AutoML functionality can make
    your ML system perform more efficiently and powerfully. But before we dive deep
    into H2O Sparkling Water, let’s understand what Apache Spark is and what it consists
    of.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Spark**是一个开源的数据处理引擎。它用于实时处理数据，以及通过集群计算进行批量处理。所有数据处理任务都在内存中执行，使得任务执行非常快速。Apache
    Spark的数据处理能力与H2O的AutoML功能相结合，可以使您的机器学习系统运行得更高效、更强大。但在我们深入探讨H2O Sparkling Water之前，让我们先了解Apache
    Spark是什么以及它由什么组成。'
- en: 'Let’s start by understanding what the various components of the Spark ecosystem
    are:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解Spark生态系统中的各种组件：
- en: '![Figure 12.1 – Apache Spark components ](img/B17298_12_001.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图12.1 – Apache Spark组件](img/B17298_12_001.jpg)'
- en: Figure 12.1 – Apache Spark components
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – Apache Spark组件
- en: 'The various components of the Spark ecosystem are as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Spark生态系统的各种组件如下：
- en: '**Spark Core**: The Spark Core component is the most vital component of the
    Spark ecosystem. It is responsible for basic functions such as input-output operations
    and scheduling and monitoring jobs. All the other components are built on top
    of this component. This component supports the Scala, Java, Python, and R programming
    languages using specific interfaces. The Spark Core component itself is written
    in the Scala programming language.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Core**：Spark Core组件是Spark生态系统中最关键的部分。它负责基本功能，如输入输出操作、作业调度和监控。所有其他组件都是基于这个组件构建的。该组件通过特定的接口支持Scala、Java、Python和R编程语言。Spark
    Core组件本身是用Scala编程语言编写的。'
- en: '**Spark SQL**: The Spark SQL component is used to leverage the power of SQL
    queries to run data queries on data stored in Spark nodes.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**：Spark SQL组件用于利用SQL查询的强大功能，在Spark节点存储的数据上运行数据查询。'
- en: '**Spark Streaming**: The Spark Streaming component is used to batch as well
    as stream data in the same application.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**：Spark Streaming组件用于在同一个应用程序中批量处理以及流式处理数据。'
- en: '**Spark MLlib**: Spark MLlib is the ML library used by Spark to develop and
    deploy scalable ML pipelines. It is also used to perform ML analytics tasks such
    as feature extraction, feature engineering, dimensionality reduction, and so on.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark MLlib**：Spark MLlib是Spark用于开发和部署可扩展机器学习管道的ML库。它还用于执行机器学习分析任务，如特征提取、特征工程、降维等。'
- en: '**GraphX**: The GraphX component is a library that is used to perform data
    analytics on graph-based data. It is used to perform graph data construction and
    traversals.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GraphX**：GraphX组件是一个用于在基于图的数据上执行数据分析的库。它用于执行图数据构建和遍历。'
- en: '**Spark R**: The Spark R component is an R package that provides a front-end
    shell for users to communicate with Spark via the R programming language. All
    data processing done by R is carried out on a single node. This makes R not ideal
    for processing large amounts of data. The Spark R component helps users perform
    these data operations on huge datasets in a distributed manner by using the underlying
    Spark cluster.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark R**：Spark R组件是一个R包，为用户提供了一个通过R编程语言与Spark通信的前端shell。所有由R执行的数据处理都在单个节点上完成。这使得R不适用于处理大量数据。Spark
    R组件通过使用底层的Spark集群，帮助用户以分布式方式在大型数据集上执行这些数据操作。'
- en: Understanding the Apache Spark architecture
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Apache Spark架构
- en: Apache Spark has a well-defined architecture. As mentioned previously, Spark
    is run on a cluster system. Within this cluster, you will have one node that is
    assigned as the master while the others act as workers. All this work is performed
    by independent processes in the worker nodes and the combined effort is coordinated
    by the Spark context.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark具有一个明确的架构。如前所述，Spark在集群系统上运行。在这个集群中，您将有一个节点被指定为主节点，而其他节点则作为工作节点。所有这些工作都是由工作节点上的独立进程完成的，而整个工作的协调则由Spark上下文完成。
- en: 'Refer to the following diagram to get a better understanding of the Apache
    Spark architecture:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图表以更好地理解Apache Spark架构：
- en: '![Figure 12.2 – Apache Spark architecture ](img/B17298_12_002.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – Apache Spark架构](img/B17298_12_002.jpg)'
- en: Figure 12.2 – Apache Spark architecture
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – Apache Spark架构
- en: 'The Spark architecture comprises the following components:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Spark架构包括以下组件：
- en: '**Spark Cluster Manager**: The Spark cluster manager is responsible for managing
    the allocation of resources to nodes and monitoring their health. It is responsible
    for maintaining the cluster of machines on which the Spark application is running.
    When you start a Spark application, the cluster manager will start up different
    nodes in the cluster, depending on the specified configuration, and restart any
    services that fail in the middle of execution.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark 集群管理器**：Spark 集群管理器负责管理资源分配给节点并监控其健康状态。它负责维护 Spark 应用程序运行的机器集群。当你启动一个
    Spark 应用程序时，集群管理器将根据指定的配置启动集群中的不同节点，并在执行过程中重启任何失败的服务。'
- en: 'The Spark cluster manager comes in three types:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 集群管理器有三种类型：
- en: '**Standalone**: This is a simple cluster manager that comes bundled with Spark
    and is very easy to set up and use.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Standalone**：这是一个简单的集群管理器，与 Spark 一起捆绑提供，并且非常容易设置和使用。'
- en: '**Hadoop YARN**: **Yet Another Resource Negotiator** (**YARN**) is a resource
    manager that comes with the Hadoop ecosystem. Spark, being a data processing system,
    can integrate with many data storage systems. **Hadoop Distributed File System**
    (**HDFS**) is one of the most commonly used distributed filesystems in the big
    data industry and using Spark with HDFS has been a common setup in companies.
    Since YARN comes with the Hadoop ecosystem, you can use the same resource manager
    to manage your Spark resources.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop YARN**：**YARN（Yet Another Resource Negotiator**）是 Hadoop 生态系统附带的一个资源管理器。作为一个数据处理系统，Spark
    可以与许多数据存储系统集成。**Hadoop 分布式文件系统（HDFS**）是大数据行业中应用最广泛的分布式文件系统之一，Spark 与 HDFS 的集成在许多公司中已成为常见的配置。由于
    YARN 是 Hadoop 生态系统的一部分，你可以使用相同的资源管理器来管理你的 Spark 资源。'
- en: '**Kubernetes**: Kubernetes is an open source container orchestration system
    for automating deployment operations, scaling services, and other forms of server
    management. Kubernetes is also capable of managing Spark cluster resources.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes**：Kubernetes 是一个开源的容器编排系统，用于自动化部署操作、扩展服务以及其他形式的服务器管理。Kubernetes
    还能够管理 Spark 集群资源。'
- en: '**Spark Driver**: The Spark driver is the main program of the Spark application.
    It is responsible for controlling the execution of the application and keeps track
    of the different states of the nodes, as well as the tasks that have been assigned
    to each node. The program can be any script that you run or even the Spark interface.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark 驱动程序**：Spark 驱动程序是 Spark 应用程序的主要程序。它负责控制应用程序的执行并跟踪节点的不同状态，以及分配给每个节点的任务。该程序可以是任何你运行的脚本，甚至是
    Spark 接口。'
- en: '**Spark Executors**: The Spark executors are the actual processes that perform
    the computation task on the worker nodes. They are pretty simple processes whose
    aim is to take the assigned task, compute it, and then send back the results to
    the Spark Context.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark 执行器**：Spark 执行器是实际在工作节点上执行计算任务的进程。它们是相当简单的进程，目的是接收分配的任务，计算它，然后将结果发送回
    Spark Context。'
- en: '**SparkContext**: The Spark Context, as its name suggests, keeps track of the
    context of the execution. Any command that the Spark driver executes goes through
    this context. The Spark Context communicates with the Spark cluster manager to
    coordinate the execution activities with the correct executor.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SparkContext**：正如其名所示，Spark Context 跟踪执行上下文。Spark 驱动程序执行的任何命令都通过这个上下文。Spark
    Context 与 Spark 集群管理器通信，以协调正确的执行器执行活动。'
- en: The Spark driver program is the primary function that manages the parallel execution
    of operations on a cluster. The driver program does so using a data structure
    called a **Resilient Distributed Dataset** (**RDD**).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 驱动程序是管理集群上操作并行执行的主要功能。驱动程序通过一个称为 **弹性分布式数据集**（**RDD**）的数据结构来实现这一点。
- en: Understanding what a Resilient Distributed Dataset is
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解什么是弹性分布式数据集
- en: Apache Spark is built on the foundation of the **RDD**. It is a fault-tolerant
    record of data that resides on multiple nodes and is immutable. Everything that
    you do in Spark is done using an RDD. Since it is immutable, any transformation
    that you do eventually creates a new RDD. RDDs are partitioned into logical sets
    that are then distributed among the Spark nodes for execution. Spark handles all
    this distribution internally.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 建立在 **RDD** 的基础上。它是一个容错的数据记录，位于多个节点上且不可变。你在 Spark 中所做的所有操作都是使用
    RDD 完成的。由于它是不可变的，因此你进行的任何转换最终都会创建一个新的 RDD。RDD 被划分为逻辑集合，然后这些集合被分配到 Spark 节点上进行执行。Spark
    内部处理所有这些分配。
- en: 'Let’s understand how Spark uses RDDs to perform data processing at scale. Refer
    to the following diagram:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解Spark如何使用RDD在规模上进行数据处理。参考以下图表：
- en: '![Figure 12.3 – Linear RDD transformations ](img/B17298_12_003.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图12.3 – 线性RDD转换](img/B17298_12_003.jpg)'
- en: Figure 12.3 – Linear RDD transformations
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 线性RDD转换
- en: So, RDDs are immutable, which means that once the dataset has been created,
    it cannot be modified. So, if you want to make changes in the dataset, then Spark
    will create a new RDD from the existing RDD and keeps track of the changes. Here,
    you have your initial data stored in **RDD 1**, so you must assume you need to
    drop a column and convert the type of another column from a string into a number.
    Spark will create **RDD 2**, which will contain these changes, as well as make
    note of the changes it has made. Eventually, as you further transform the data,
    Spark will contain many RDDs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，RDD是不可变的，这意味着一旦数据集被创建，就不能对其进行修改。所以，如果你想对数据集进行更改，那么Spark将从现有的RDD创建一个新的RDD，并跟踪所做的更改。在这里，你的初始数据存储在**RDD
    1**中，所以你必须假设你需要删除一列并将另一列的类型从字符串转换为数字。Spark将创建**RDD 2**，它将包含这些更改，并记录所做的更改。最终，随着你进一步转换数据，Spark将包含许多RDD。
- en: You may be wondering what happens if you need to perform many transformations
    on the data; will Spark create that many RRDs and eventually run out of memory?
    Remember, RDDs are resilient and immutable, so if you have created **RDD 3** from
    **RDD 2,** then you will only need to keep **RDD2** and the data transformation
    process from **RDD 2** to **RDD 3.** You will no longer need **RDD 1** so that
    can be removed to free up space. Spark does all the memory management for you.
    It will remove any RDDs that are not needed.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，如果你需要对数据进行许多转换，Spark是否会创建那么多的RRD，最终耗尽内存？记住，RDD是容错的且不可变的，所以如果你从**RDD
    2**创建了**RDD 3**，那么你只需要保留**RDD 2**以及从**RDD 2**到**RDD 3**的数据转换过程。你将不再需要**RDD 1**，因此可以将其删除以释放空间。Spark为你处理所有的内存管理。它将删除任何不再需要的RDD。
- en: 'That was a very simplified explanation for a simple problem. What if you are
    creating multiple RDDs that contain different transformations from the same RDD?
    This can be seen in the following diagram:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个对简单问题的非常简化的解释。如果你从同一个RDD创建多个包含不同转换的RDD，会怎样？这可以在以下图表中看到：
- en: '![Figure 12.4 – Branched RDD transformations ](img/B17298_12_004.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图12.4 – 分支RDD转换](img/B17298_12_004.jpg)'
- en: Figure 12.4 – Branched RDD transformations
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 – 分支RDD转换
- en: 'In this case, you will need to keep all the RDDs. This is where Spark’s **lazy
    evaluation** comes into play. Lazy evaluation is an evaluation technique where
    evaluation expressions are delayed until the resultant value is needed. Let’s
    understand this better by looking into RDD operations. There are two types of
    operations:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你需要保留所有RDD。这就是Spark的**延迟评估**发挥作用的地方。延迟评估是一种评估技术，其中评估表达式被延迟到结果值需要时才进行。让我们通过查看RDD操作来更好地理解这一点。有两种类型的操作：
- en: '**Transformations**: Transformations are operations that produce a new RDD
    from an existing RDD that contains changes in the dataset. These operations mostly
    consist of converting raw datasets into a refined final dataset that can be used
    to extract evaluation metrics or other processes. This mostly involves data manipulation
    operations such as union operations or groupby operations.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：转换是产生新RDD的操作，该RDD基于包含数据集更改的现有RDD。这些操作主要是由将原始数据集转换为可以用于提取评估指标或其他过程的精炼最终数据集的数据操作组成。这主要涉及如并集操作或分组操作之类的数据操作。'
- en: '**Actions**: Actions are operations that take an RDD as input but don’t generate
    a new RDD as output. The output value derived from the action operation is sent
    back to the driver. This mostly involves operations such as count, which returns
    the number of elements in the RDD, or aggregate, which performs aggregate operations
    on the contents of the RDD and sends the result back.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：动作是接受RDD作为输入但不生成新RDD作为输出的操作。从动作操作中导出的输出值被发送回驱动程序。这通常涉及如count（返回RDD中元素的数量）或aggregate（对RDD的内容执行聚合操作并将结果发送回）之类的操作。'
- en: Transformation operations are lazy. When performing transformation operations
    on an RDD, Spark will keep a note of what needs to be done but won’t do it immediately.
    It will only start the transformation process when it gets an action operation,
    hence the name lazy evaluation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 转换操作是惰性的。当在 RDD 上执行转换操作时，Spark 会记录需要执行的操作，但不会立即执行。它只有在接收到动作操作时才会开始转换过程，因此得名惰性评估。
- en: Let’s understand the whole process with a simple example. Let’s assume you have
    an RDD that contains a raw dataset of all the employees of a company and you want
    to calculate the average salary of all the senior ML engineers. Your transformation
    operations are to filter all the ML engineers into **RDD 2** and then further
    filter by seniority into **RDD 3** When you pass this transformation operation
    to Spark, it won’t create **RDD 3** It will just keep a note of it. When it gets
    the action operation – that is, to calculate the average salary – that is when
    the lazy evaluation kicks in and Spark starts performing the transformation and,
    eventually, the action.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个简单的例子来理解整个过程。假设你有一个包含一家公司所有员工原始数据集的 RDD，你想计算所有高级机器学习工程师的平均工资。你的转换操作是将所有机器学习工程师过滤到
    **RDD 2**，然后进一步按资历过滤到 **RDD 3**。当你将这个转换操作传递给 Spark 时，它不会创建 **RDD 3**。它只会记录下来。当它接收到动作操作——即计算平均工资——这时惰性评估开始发挥作用，Spark
    开始执行转换，并最终执行动作。
- en: Lazy evaluation helps Spark understand what required transformation operations
    are needed to perform the action operation and find the most efficient way of
    doing the transformation while keeping the space complexity in mind.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 惰性评估帮助 Spark 理解执行动作操作所需的转换操作，并找到在考虑空间复杂性的同时进行转换的最有效方式。
- en: Tip
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Spark is a very sophisticated and powerful technology. It provides plenty of
    flexibility and can be configured to best suit different kinds of data processing
    needs. In this chapter, we have just explored the tip of the iceberg of Apache
    Spark. If you are interested in understanding the capabilities of Spark to their
    fullest extent, I highly encourage you to explore the Apache Spark documentation,
    which can be found at [https://spark.apache.org/](https://spark.apache.org/).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一种非常复杂且强大的技术。它提供了大量的灵活性，可以根据不同的数据处理需求进行配置。在本章中，我们只是探索了 Apache Spark 的冰山一角。如果你对了解
    Spark 的全部功能感兴趣，我强烈建议你探索 Apache Spark 文档，可以在 [https://spark.apache.org/](https://spark.apache.org/)
    找到。
- en: Now that we have a basic idea of how Spark works, let’s understand how H2O Sparkling
    Water combines both H2O and Spark.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对 Spark 的工作原理有了基本的了解，让我们来了解 H2O Sparkling Water 如何结合 H2O 和 Spark。
- en: Exploring H2O Sparkling Water
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 H2O Sparkling Water
- en: '**Sparkling Water** is an H2O product that combines the fast and scalable ML
    of H2O with the analytics capabilities of Apache Spark. The combination of both
    these technologies allows users to make SQL queries for data munging, feed the
    results to H2O for model training, build and deploy models to production, and
    then use them for predictions.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sparkling Water** 是一个结合了 H2O 快速且可扩展的机器学习功能与 Apache Spark 分析能力的 H2O 产品。这两种技术的结合使用户能够进行数据清洗的
    SQL 查询，将结果输入 H2O 进行模型训练，构建和部署模型到生产环境，然后用于预测。'
- en: H2O Sparkling Water is designed in a way that you can run H2O in regular Spark
    applications. It has provisions to run the H2O server inside of Spark executors
    so that the H2O server has access to all the data stored in executors for performing
    any ML-based computations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: H2O Sparkling Water 是设计成可以在常规 Spark 应用程序中运行 H2O 的。它提供了在 Spark 执行器内部运行 H2O 服务器的能力，这样
    H2O 服务器就可以访问存储在执行器中的所有数据，以执行任何基于机器学习的计算。
- en: 'The transparent integration between H2O and Spark provides the following benefits:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: H2O 和 Spark 之间的透明集成提供了以下好处：
- en: H2O algorithms, including AutoML, can be used in Spark workflows
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: H2O 算法，包括 AutoML，可以在 Spark 工作流程中使用
- en: Application-specific data structures can be transformed and supported between
    H2O and Spark
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用特定的数据结构可以在 H2O 和 Spark 之间进行转换和支持
- en: You can use Spark RDDs as datasets in H2O ML algorithms
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用 Spark RDDs 作为 H2O ML 算法的数据集
- en: 'Sparkling Water supports two types of backends:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkling Water 支持两种类型的后端：
- en: '**Internal Backend**: In this type of setup, the H2O application is launched
    inside the Spark executor once the H2O context is initialized. H2O then starts
    its service by initializing its key-value store and memory manager inside each
    of the executors. It is easy to deploy H2O Sparkling Water as an internal backend,
    but if Spark’s cluster manager decides to shut down any of the executors, then
    the H2O server running in the executor is also shut down. The internal backend
    is a default setup used by H2O Sparkling Water. The architecture of the internally
    running H2O Sparkling Water looks as follows:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部后端**：在这种配置中，一旦初始化H2O上下文，H2O应用就在Spark executor内部启动。然后，H2O通过在每个executor内部初始化其键值存储和内存管理器来启动其服务。部署H2O
    Sparkling Water作为内部后端很容易，但如果Spark的集群管理器决定关闭任何executor，那么在该executor中运行的H2O服务器也会关闭。内部后端是H2O
    Sparkling Water使用的默认设置。内部运行的H2O Sparkling Water架构如下所示：'
- en: '![Figure 12.5 – Sparkling Water internal backend architecture ](img/B17298_12_005.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5 – Sparkling Water内部后端架构](img/B17298_12_005.jpg)'
- en: Figure 12.5 – Sparkling Water internal backend architecture
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 – Sparkling Water内部后端架构
- en: As you can see, the H2O service resides inside each of the Spark executors.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，H2O服务位于每个Spark executor内部。
- en: '**External Backend**: In this type of setup, the H2O service is deployed separately
    from the Spark executors and the communication between the H2O servers and the
    Spark executors is handled by the Spark driver. The architecture of H2O Sparkling
    Water as an external backend works as follows:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**外部后端**：在这种配置中，H2O服务与Spark executor分开部署，H2O服务器和Spark executor之间的通信由Spark driver处理。作为外部后端的H2O
    Sparkling Water架构如下所示：'
- en: '![Figure 12.6 – Sparkling Water external backend architecture ](img/B17298_12_006.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图12.6 – Sparkling Water外部后端架构](img/B17298_12_006.jpg)'
- en: Figure 12.6 – Sparkling Water external backend architecture
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 – Sparkling Water外部后端架构
- en: As you can see, the H2O cluster is run separately from the Spark executor. The
    separation has benefits since the H2O clusters are no longer affected by the shutting
    down of Spark Executors. However, this adds the overhead of the H2O driver needing
    to coordinate the communication between the H2O cluster and the Spark Executors.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，H2O集群是独立于Spark executor运行的。这种分离有好处，因为H2O集群不再受Spark Executor关闭的影响。然而，这也增加了H2O
    driver需要协调H2O集群和Spark Executor之间通信的开销。
- en: Sparkling Water, despite being built on top of Spark, uses an H2OFrame when
    performing computations using the H2O server in the Sparkling Water cluster. Thus,
    there is a lot of data exchange and interchange between the Spark RDD and the
    H2OFrame.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Sparkling Water虽然建立在Spark之上，但在使用Sparkling Water集群中的H2O服务器进行计算时，使用H2OFrame。因此，Spark
    RDD和H2OFrame之间存在大量的数据交换和交互。
- en: 'DataFrames are converted between different types as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame之间的转换如下所示：
- en: '**H2OFrame into RDD**: When converting an H2OFrame into an RDD, instead of
    recreating the data into a different type, Sparkling Water creates a wrapper around
    the H2OFrame that acts like an RDD API. This wrapper interprets all RDD-based
    operations into identical H2OFrame operations.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H2OFrame转换为RDD**：当将H2OFrame转换为RDD时，Sparkling Water不是重新创建数据为不同类型，而是在H2OFrame周围创建一个包装器，该包装器类似于RDD
    API。这个包装器将所有基于RDD的操作解释为相同的H2OFrame操作。'
- en: '**RDD into H2OFrame**: Converting an RDD into an H2OFrame involves evaluating
    the data in the RDD and then converting it into an H2OFrame. The data in the H2OFrame,
    however, is heavily compressed. Data being shared between H2O and Spark depends
    on the type of backend used for deployment.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RDD转换为H2OFrame**：将RDD转换为H2OFrame涉及评估RDD中的数据，然后将其转换为H2OFrame。然而，H2OFrame中的数据被高度压缩。H2O和Spark之间共享的数据取决于部署时使用的后端类型。'
- en: '**Data Sharing in Internal Sparkling Water Backend**: In the internal Sparkling
    Water backend, since the H2O service is launched inside the Spark Executor, both
    the Spark service and the H2O service inside the executor use the same **Java
    Virtual Machine** (**JVM**) and as such, the data is accessible to both the services.
    The following diagram shows the process of data sharing in the internal Sparkling
    Water backend:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部Sparkling Water后端的数据共享**：在内部Sparkling Water后端，由于H2O服务是在Spark Executor内部启动的，因此Spark服务和Executor内部的H2O服务都使用相同的**Java虚拟机**（**JVM**），因此数据对两个服务都是可访问的。以下图表显示了内部Sparkling
    Water后端的数据共享过程：'
- en: '![Figure 12.7 – Data sharing in the internal Sparkling Water backend ](img/B17298_12_007.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图12.7 – 内部Sparkling Water后端的数据共享](img/B17298_12_007.jpg)'
- en: Figure 12.7 – Data sharing in the internal Sparkling Water backend
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 – 内部Sparkling Water后端的数据共享
- en: Since both services are on the same executor, you need to consider memory when
    converting DataFrames between the two types. You will need to allocate enough
    memory for both Spark and H2O to perform their respective operations. Spark will
    need the minimum memory of your dataset, plus additional memory for any transformations
    that you wish to perform. Also, converting RDDs into H2OFrames will lead to duplication
    of data, so it’s recommended that a 4x bigger dataset should be used for H2O.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个服务都在同一个executor上，您在转换两种类型之间的DataFrames时需要考虑内存。您需要为Spark和H2O执行各自的操作分配足够的内存。Spark需要您数据集的最小内存，以及您希望执行的任何转换的额外内存。此外，将RDD转换为H2OFrames会导致数据重复，因此建议使用4倍更大的数据集用于H2O。
- en: '**Data Sharing in External Sparkling Water Backend**: In the external Sparkling
    Water backend, the H2O service is launched in a cluster that is separate from
    the Spark Executor. So, there is an added overhead of transferring the data from
    one cluster to another over the network. The following diagram should help you
    understand this:'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部Sparkling Water后端的数据共享**：在外部Sparkling Water后端，H2O服务是在一个与Spark Executor分开的集群中启动的。因此，在网络上从一个集群传输数据到另一个集群会有额外的开销。以下图表应该能帮助您理解这一点：'
- en: '![Figure 12.8 – Data sharing in the external Sparkling Water backend ](img/B17298_12_008.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图12.8 – 外部Sparkling Water后端的数据共享](img/B17298_12_008.jpg)'
- en: Figure 12.8 – Data sharing in the external Sparkling Water backend
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 – 外部Sparkling Water后端的数据共享
- en: Since both services reside in their own cluster (if you have allocated enough
    memory to the respective clusters), you don’t need to worry about memory constraints.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个服务都位于它们自己的集群中（如果您为各自的集群分配了足够的内存），您不需要担心内存限制。
- en: Tip
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: H2O Sparkling Water can be run on different types of platforms in various kinds
    of ways. If you are interested in learning more about the various ways in which
    you can deploy H2O Sparkling Water, as well as getting more information about
    its backends, then feel free to check out [https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/design/supported_platforms.xhtml](https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/design/supported_platforms.xhtml).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: H2O Sparkling Water可以在各种平台上以各种方式运行。如果您想了解更多关于您可以部署H2O Sparkling Water的各种方式，以及获取更多有关其后端的信息，请随时查看[https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/design/supported_platforms.xhtml](https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/design/supported_platforms.xhtml)。
- en: Now that we know how H2O Sparkling Water works, let’s see how we can download
    and install it.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了H2O Sparkling Water是如何工作的，让我们看看我们如何下载和安装它。
- en: Downloading and installing H2O Sparkling Water
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载和安装H2O Sparkling Water
- en: 'H2O Sparkling Water has some specific requirements that need to be satisfied
    before you can install it on your system. The requirements for installing H2O
    Sparkling Water version 3.36 are as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在您可以在系统上安装H2O Sparkling Water之前，H2O Sparkling Water有一些特定的要求需要满足。H2O Sparkling
    Water版本3.36的安装要求如下：
- en: '**Operating System**: H2O Sparkling Water is only supported for Linux, macOS,
    and Windows.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作系统**：H2O Sparkling Water仅支持Linux、macOS和Windows。'
- en: '**Java Version**: H2O Sparkling Water supports all Java versions above Java
    1.8.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java版本**：H2O Sparkling Water支持所有高于Java 1.8的Java版本。'
- en: '**Python Version**: If you plan to use the Python version of Sparkling Water,
    known as PySparkling, then you will need a Python version above 3.6 installed
    on your system.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python版本**：如果您计划使用Sparkling Water的Python版本，即PySparkling，那么您需要在系统上安装一个高于3.6的Python版本。'
- en: '**H2O Version**: H2O Sparkling Water version 3.36.1 requires the same version
    of H2O installed on your system. However, H2O Sparkling Water comes prepackaged
    with a compatible H2O version, so you don’t need to separately install H2O to
    use H2O Sparkling Water.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**H2O版本**：H2O Sparkling Water版本3.36.1需要与您系统上安装的H2O相同版本。然而，H2O Sparkling Water已经预包装了一个兼容的H2O版本，因此您不需要单独安装H2O来使用H2O
    Sparkling Water。'
- en: '**Spark Version**: H2O Sparkling Water version 3.36.1 strictly supports Spark
    3.2\. Any Spark version above or below version 3.2 may cause issues with installation
    or how H2O Sparkling Water works. Spark 3.2 has its own dependencies, which are
    as follows:'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark版本**：H2O Sparkling Water版本3.36.1严格支持Spark 3.2。任何高于或低于3.2版本的Spark版本可能会导致安装问题或H2O
    Sparkling Water的工作问题。Spark 3.2有其自己的依赖项，如下所示：'
- en: '**Java Version**: Spark 3.2 strictly supports Java 8 and Java 11'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Java版本**：Spark 3.2严格支持Java 8和Java 11'
- en: '**Scala Version**: Spark 3.2 strictly runs on Scala 2.12/2.13'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scala版本**：Spark 3.2严格运行在Scala 2.12/2.13上'
- en: '**R Version**: Spark 3.2 supports any R version above 3.5'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R版本**：Spark 3.2支持任何高于3.5的R版本'
- en: '**Python Version**: Spark 3.2 supports any Python version above 3.6'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Python版本**：Spark 3.2支持任何高于3.6的Python版本'
- en: '`SPARK_HOME` environment variable to point to your local Spark 3.2 installation.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`SPARK_HOME`环境变量设置为指向您的本地Spark 3.2安装。
- en: 'Now, let’s set up our system so that we can download and install H2O Sparkling
    Water. Follow these steps to set up H2O Sparkling Water:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们设置我们的系统，以便我们可以下载和安装H2O Sparkling Water。按照以下步骤设置H2O Sparkling Water：
- en: 'We will start by installing Java 11, which is needed for both Spark and H2O
    Sparkling Water. Even though Spark supports Java 8 as well, it is preferable to
    use Java 11 since it is the newer version with improvements and security patches.
    You can download and install Java 11 by executing the following command:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先安装Java 11，这是Spark和H2O Sparkling Water所需的。尽管Spark也支持Java 8，但建议使用Java 11，因为它是一个更新的版本，具有改进和安全性补丁。您可以通过执行以下命令来下载和安装Java
    11：
- en: '[PRE0]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Optionally, if you wish to use the PySparkling Python interpreter, then install
    Python version 3.10\. You can do so by executing the following command:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您希望使用PySparkling Python解释器，则可以安装Python版本3.10。您可以通过执行以下命令来实现：
- en: '[PRE1]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now that we have the basic languages installed, let’s go ahead and download
    and install Spark version 3.2\. You can download the specific version for Spark
    from the Apache Software Foundation official download page (https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz)
    or by directly running the following command in your Terminal:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经安装了基本语言，接下来我们继续下载并安装Spark版本3.2。您可以从Apache软件基金会官方下载页面（https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz）下载Spark的特定版本，或者直接在您的终端中运行以下命令：
- en: '[PRE2]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'If you are using the **Maven project**, then you can directly specify the Spark
    core Maven dependency, as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用**Maven项目**，则可以直接指定Spark核心Maven依赖项，如下所示：
- en: '[PRE3]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can find the Maven repository for Spark at [https://mvnrepository.com/artifact/org.apache.spark/spark-core](https://mvnrepository.com/artifact/org.apache.spark/spark-core).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://mvnrepository.com/artifact/org.apache.spark/spark-core](https://mvnrepository.com/artifact/org.apache.spark/spark-core)找到Spark的Maven仓库。
- en: 'Then, you can extract the `.tar` file by executing the following command in
    your Terminal:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以在终端中执行以下命令来解压缩`.tar`文件：
- en: '[PRE4]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have extracted the Spark binaries, let’s set our environment variables,
    as follows:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经提取了Spark的二进制文件，接下来我们设置环境变量，如下所示：
- en: '[PRE5]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We must also set the `MASTER` environment variable to `local[*]` to launch
    a local Spark cluster:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还必须将`MASTER`环境变量设置为`local[*]`以启动一个本地Spark集群：
- en: '[PRE6]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have all the dependencies of H2O Sparkling Water installed and ready,
    let’s go ahead and download H2O Sparkling Water. You can download the latest version
    from [https://h2o.ai/products/h2o-sparkling-water/](https://h2o.ai/products/h2o-sparkling-water/).
    Upon clicking the **Download Latest** button, you should be redirected to the
    H2O Sparkling Water repository website, where you can download the H2O Sparkling
    Water version *3.36* ZIP file.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经安装并准备好了H2O Sparkling Water的所有依赖项，接下来我们继续下载H2O Sparkling Water。您可以从[https://h2o.ai/products/h2o-sparkling-water/](https://h2o.ai/products/h2o-sparkling-water/)下载最新版本。点击**下载最新版**按钮后，您应该会被重定向到H2O
    Sparkling Water仓库网站，在那里您可以下载H2O Sparkling Water版本*3.36*的ZIP文件。
- en: 'Once the download has finished, you can unzip the ZIP file by executing the
    following command in your Terminal:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载完成后，您可以在终端中执行以下命令来解压ZIP文件：
- en: '[PRE7]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can see if everything is working fine by starting the H2O Sparkling Water
    shell by executing the following command inside your Sparkling Water installation
    folder:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过在Sparkling Water安装文件夹内执行以下命令来启动H2O Sparkling Water shell，以检查一切是否正常工作：
- en: '[PRE8]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'By doing this, you can see if Sparkling Water has integrated with Spark by
    starting an H2O cloud inside the Spark cluster. You can do so by executing the
    following commands inside `sparkling-shell`:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过这样做，您可以通过在Spark集群内启动一个H2O云来查看Sparkling Water是否已与Spark集成。您可以在`sparkling-shell`内部执行以下命令来实现：
- en: '[PRE9]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You should get an output similar to the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该会得到以下类似的输出：
- en: '![Figure 12.9 – Successfully starting up H2O Sparkling Water ](img/B17298_12_009.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图12.9 – 成功启动H2O Sparkling Water](img/B17298_12_009.jpg)'
- en: Figure 12.9 – Successfully starting up H2O Sparkling Water
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.9 – 成功启动H2O Sparkling Water
- en: 'Now that we have successfully downloaded and installed both Spark and H2O Sparkling
    Water and ensured that both are working correctly, there is some general recommended
    tuning that you must do, as per H2O.ai’s documentation. Let’s take a look:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已成功下载并安装了Spark和H2O Sparkling Water，并确保它们都能正常工作，根据H2O.ai的文档，还有一些通用的推荐调整您必须进行。让我们看一下：
- en: 'Increase the available memory for the Spark driver as well as the Spark Executors
    from the default value of `config` parameter when starting the Sparkling shell:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在启动Sparkling shell时，增加Spark驱动器和Spark执行器的可用内存，从`config`参数的默认值：
- en: '[PRE10]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you are using YARN or your cluster manager, then use `config spark.yarn.am.memory`
    instead of `spark.driver.memory`. You can also set these values as default configuration
    properties by setting the values in the `spark-defaults.conf` file. This can be
    found among your Spark installation files.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用YARN或您的集群管理器，则使用`config spark.yarn.am.memory`而不是`spark.driver.memory`。您还可以通过在`spark-defaults.conf`文件中设置这些值来将这些值设置为默认配置属性。这些可以在您的Spark安装文件中找到。
- en: 'Along with cluster memory, it is also recommended to increase the PermGen size
    of your Spark nodes. The default PermGen size often proves to be very small and
    can lead to `OutOfMemoryError`. `spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions`
    configuration options, as follows:'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了集群内存外，还建议增加Spark节点的PermGen大小。默认的PermGen大小通常非常小，可能导致`OutOfMemoryError`。以下为`spark.driver.extraJavaOptions`和`spark.executor.extraJavaOptions`配置选项：
- en: '[PRE11]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It is also recommended to keep your cluster homogeneous – that is, both the
    Spark driver and Executor have the same amount of resources allocated to them.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 还建议保持您的集群同质化——也就是说，Spark驱动器和执行器分配的资源数量相同。
- en: 'The following configurations are also recommended to speed up and stabilize
    the creation of H2O services on top of the Spark cluster:'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下配置也被推荐以加快和稳定在Spark集群上创建H2O服务：
- en: 'Increase the number of seconds to wait for a task launched in data-local mode
    so that H2O tasks are processed locally with data. You can set this as follows:'
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加在数据本地模式下启动的任务等待秒数，以便H2O任务在本地使用数据进行处理。您可以按以下方式设置：
- en: '[PRE12]'
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Enforcing Spark starts scheduling jobs only when it is allocated 100% of its
    resources:'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强制Spark仅在分配了100%的资源后才开始调度作业：
- en: '[PRE13]'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Don’t retry failed tasks:'
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要重试失败的任务：
- en: '[PRE14]'
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Set the interval between each executor’s heartbeats to the driver to less than
    Spark’s network timeout – that is `spark.network.timeout` – whose default value
    is *120 seconds*. So, set the heartbeat value to around *10 seconds*:'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个执行器心跳之间的间隔设置为小于Spark的网络超时时间（即`spark.network.timeout`），其默认值为*120秒*。因此，将心跳值设置为约*10秒*：
- en: '[PRE15]'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now that we have appropriately configured Spark and H2O Sparkling Water, let’s
    see how we can use these technologies to solve an ML problem using both Spark
    and H2O AutoML.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已适当地配置了Spark和H2O Sparkling Water，让我们看看如何使用这些技术通过Spark和H2O AutoML解决ML问题。
- en: Implementing Spark and H2O AutoML using H2O Sparkling Water
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用H2O Sparkling Water实现Spark和H2O AutoML
- en: For this experiment, we will be using the Concrete Compressive Strength dataset.
    You can find this dataset at https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们将使用混凝土抗压强度数据集。您可以在https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength找到这个数据集。
- en: 'Here are more details on the dataset: I-Cheng Yeh, *Modeling of strength of
    high performance concrete using artificial neural networks*, Cement and Concrete
    Research, Vol. 28, No. 12, pp. 1797-1808 (1998).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据集的更多详细信息：I-Cheng Yeh，*使用人工神经网络模拟高性能混凝土强度*，水泥与混凝土研究，第28卷，第12期，第1797-1808页（1998年）。
- en: Let’s start by understanding the problem statement we will be working with.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先了解我们将要解决的问题陈述。
- en: Understanding the problem statement
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解问题陈述
- en: 'The Concrete Compressive Strength dataset is a dataset that consists of *1,030*
    data points consisting of the following features:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 混凝土抗压强度数据集是一个包含以下特征的*1,030*个数据点的数据集：
- en: '**Cement**: This feature denotes the amount of cement added in kg in m3 of
    the mixture'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**水泥**: 此功能表示混合物每立方米的混凝土量（以千克计）'
- en: '**Blast Furnace Slag**: This feature denotes the amount of slag added in kgs
    in m3 of the mixture'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高炉渣**: 此功能表示混合物每立方米的渣量（以千克计）'
- en: '**Fly Ash**: This feature denotes the amount of fly ash added in kgs in m3
    of the mixture'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**粉煤灰**: 此功能表示混合物每立方米的粉煤灰量（以千克计）'
- en: '**Water**: This feature denotes the amount of water added in kgs in m3 of the
    mixture'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Superplasticizer**: This feature denotes the amount of superplasticizer added
    in kgs in m3 of the mixture'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coarse Aggregate**: This feature denotes the amount of coarse aggregate –
    in other words, stone – added in kgs in m3 of the mixture'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine Aggregate**: This feature denotes the amount of fine aggregate – in
    other words, sand – added in kgs in m3 of the mixture'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Age**: This feature denotes the age of the cement'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concrete compressive strength**: This feature denotes the compressive strength
    of the concrete in **Megapascals** (**MPa**)'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ML problem is to use all the features to predict the compressive strength
    of the concrete.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'The content of the dataset is as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Concrete Compressive Strength dataset sample ](img/B17298_12_010.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 – Concrete Compressive Strength dataset sample
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s see how we can solve this problem using H2O Sparkling Water. First,
    we shall learn how to train models using H2O AutoML and Spark.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Running AutoML training in Sparkling Water
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have successfully installed both Spark 3.2 and H2O Sparkling Water,
    as well as set the correct environment variables (`SPARK_HOME` and `MASTER`),
    you can start the model training process.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Sparkling shell by executing the command inside the H2O Sparkling
    Water extracted folder:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This should start a Scala shell in your Terminal. The output should look as
    follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Scala shell for H2O Sparkling Water ](img/B17298_12_011.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 – Scala shell for H2O Sparkling Water
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also perform the same experiment in Python using the `PySparkling`
    shell. You can start the `PySparkling` shell by executing the following command:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You should get an output similar to the following:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Python shell for H2O Sparkling Water ](img/B17298_12_012.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 – Python shell for H2O Sparkling Water
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to start an H2O cluster inside the Spark environment. We can do
    this by creating an H2OContext and then executing its `getOrCreate()`function.
    So, execute the following code in your Sparkling shell to import the necessary
    dependencies and execute the H2O context code:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the PySparkling shell, the code will be as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You should get an output similar to the following that states that your H2O
    context has been created:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.13 – H2O context created successfully ](img/B17298_12_013.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
- en: Figure 12.13 – H2O context created successfully
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must ensure that our Concrete Compressive Strength dataset can be downloaded
    on every node using Spark’s built-in file I/O system. So, execute the following
    commands to import your dataset:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the PySparkling shell, we must import the dataset using H2O’s `import` function.
    The Python code will be as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once added, we must parse the dataset into a Spark Dataframe by executing the
    following commands in the Scala shell:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the PySparkling shell, the equivalent code will be as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, `sparkDataFrame` contains the dataset as a Spark DataFrame. So, let’s
    perform a train-test split on it to split the DataFrame into testing and training
    DataFrames. You can do so by executing the following commands in the Sparkling
    shell:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the PySparkling shell, execute the following command:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We now have `trainingDataFrame` and `testingDataFrame` ready for training and
    testing, respectively. Let’s create an H2OAutoML instance to auto-train models
    on `trainingDataFrame`. Execute the following commands to instantiate an H2O AutoML
    object:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In PySparkling, when initializing the H2O AutoML object, we also set the label
    column. The code for this is as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s see how we can set the label of the dataset so that the AutoML object
    is aware of which columns from the DataFrame are to be predicted in the Scala
    shell. Execute the following command:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: H2O will treat all the columns of the DataFrame as features unless explicitly
    specified. It will, however, ignore columns that are set as **labels**, **fold
    columns**, **weights**, or any other explicitly set ignored columns.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: H2O AutoML distinguishes between regression and classification problems depending
    on the type of the response column. If the response column is a string, then H2O
    AutoML assumes it is a `ai.h2o.sparkling.ml.algos.classification.H2OAutoMLClassifier`
    object or the `ai.h2o.sparkling.ml.algos.regression.H2OAutoMLRegressor` object
    instead of `ai.h2o.sparkling.ml.algos.H2OautoML`, as we did in this example.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s limit AutoML model training to only 10 models. Execute the following
    command:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The equivalent Python syntax for this code is the same, so execute this same
    command in your PySparkling shell.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our AutoML object all set up, the only thing remaining is to trigger
    the training. To do so, execute the following command:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The equivalent code for Python is as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once training is finished, you should get an output similar to the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.14 – H2O AutoML result in H2O Sparkling Water ](img/B17298_12_014.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 12.14 – H2O AutoML result in H2O Sparkling Water
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we got a stacked ensemble model as the leader model with the
    model key below it. Below **Model Key** is **Model summary**, which contains the
    training and cross-validation metrics.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: As we did in [*Chapter 2*](B17298_02.xhtml#_idTextAnchor038), *Working with
    H2O Flow (H2O’s Web UI)*, we have not set the sort metric for the `aml` object,
    so by default, H2O AutoML will use the default metrics. This will be `deviance`
    since it is a `automl.setSortMetric()`and pass in the sort metric of your choice.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also get a detailed view of the model by using the `getModelDetails()`function.
    Execute the following command:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This command will work on both the PySparkling and Scala shells and will output
    very detailed JSON about the model’s metadata.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also view the AutoML leaderboard by executing the following command:'
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The equivalent Python code for the PySparkling shell is as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You should get an output similar to the following:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.15 – H2O AutoML leaderboard in H2O Sparkling Water ](img/B17298_12_015.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
- en: Figure 12.15 – H2O AutoML leaderboard in H2O Sparkling Water
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: This will display the leaderboard containing all the models that have been trained
    and ranked based on the sort metric.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 'Making predictions using H2O Sparkling Water is also very easy. The prediction
    functionality is wrapped behind a simple, easy-to-use wrapper function called
    `transform`. Execute the following code to make predictions on the testing DataFrame:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'In the PySparkling shell, it is slightly different. Here, you must execute
    the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You should get an output similar to the following:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.16 – Prediction results combined with the testing DataFrame ](img/B17298_12_016.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: Figure 12.16 – Prediction results combined with the testing DataFrame
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: The output of the `transform` function shows the entire **testDataFrame** with
    two additional columns on the right-hand side called **detailed_prediction** and
    **prediction**.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s download this model as a MOJO so that we can use it for the next
    experiment, where we shall see how H2O Sparkling Water loads and uses MOJO models.
    Execute the following command:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This command is the same for both the Scala and Python shells and should download
    the model MOJO in your specified path. If you are using the Hadoop filesystem
    as your Spark data storage engine, then the command uses HDFS by default.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to import a dataset, train models, and make predictions
    using H2O Sparkling Water, let’s take it one step further and see how we can reuse
    existing model binaries, also called MOJOs, by loading them into H2O Sparkling
    Water and making predictions on them.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Making predictions using model MOJOs in H2O Sparkling Water
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When you train models using H2O Sparkling Water, the models that are generated
    are always of the MOJO type. H2O Sparkling Water can load model MOJOs generated
    by H2O-3 and is also backward compatible with the different versions of H2O-3\.
    You do not need to create an H2O context to use model MOJOs for predictions, but
    you do need a scoring environment. Let’s understand this by completing an experiment.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'To make predictions using imported model MOJOs, you need a scoring environment.
    We can create a scoring environment in two ways; let’s take a look:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use Sparkling Water prepared scripts, which set all the dependencies that are
    needed to load MOJOs and make predictions on the Spark classpath. Refer to the
    following commands:'
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following command is for a Scala shell:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following command is for a Python shell:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Use Spark directly and set the dependencies manually.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once we have our scoring environment set up, we can load the model MOJOs. Model
    MOJOs loaded into Sparkling Water are immutable. So, making any configuration
    changes is not possible once you have loaded the model. However, you can set the
    configurations before you load the model. You can do so by using the `H2OMOJOSettings()`function.
    Refer to the following example:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'For PySparkling, refer to the following code:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Once you have set the configuration settings, you can load the model MOJO using
    the `createFromMojo()` function from the `H2OMOJOModel` library. So, execute the
    following code to load the model MOJO that you created in the *Running AutoML
    training in Sparkling Water* section and pass the configuration settings:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The Python equivalent is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: If you specify the model MOJO path as a relative path and if HDFS is enabled,
    Sparkling Water will check the HDFS home directory; otherwise, it will search
    for it from the current directory. You can also pass an absolute path to your
    model MOJO file.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also manually specify where you want to load your model MOJO. For the
    HDFS filesystem, you can use the following command:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'For a local filesystem, you can use the following command:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Once successfully loaded, you can simply use the model to make predictions,
    as we did in the *Running AutoML training in Sparkling Water* section. So, execute
    the following command to make predictions using your recently loaded model MOJO:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The prediction results are stored as another Spark DataFrame. So, to view the
    prediction values, we can just display the prediction results by executing the
    following command:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'You should get an output similar to the following:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.17 – Prediction results from the model MOJO ](img/B17298_12_017.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
- en: Figure 12.17 – Prediction results from the model MOJO
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we had specifically set `withDetailedPredictionCol` to `False`
    when loading the MOJO. That is why we can’t see the detailed `_prediction_column`
    in the prediction results.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of configurations that you can set up when loading H2O model
    MOJOs into Sparkling Water. There are also additional methods available for MOJO
    models that can help gather more information about your model MOJO. All these
    details can be found on H2O’s official documentation page at [https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/deployment/load_mojo.xhtml#loading-and-usage-of-h2o-3-mojo-model](https://docs.h2o.ai/sparkling-water/3.2/latest-stable/doc/deployment/load_mojo.xhtml#loading-and-usage-of-h2o-3-mojo-model).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations – you have just learned how to use Spark and H2O AutoML together
    using H2O Sparkling Water.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to use H2O AutoML with Apache Spark using an
    H2O system called H2O Sparkling Water. We started by understanding what Apache
    Spark is. We investigated the various components that make up the Spark software.
    Then, we dived deeper into its architecture and understood how it uses a cluster
    of computers to perform data analysis. We investigated the Spark cluster manager,
    the Spark driver, Executor, and also the Spark Context. Then, we dived deeper
    into RDDs and understood how Spark uses them to perform lazy evaluations on transformation
    operations on the dataset. We also understood that Spark is smart enough to manage
    its resources efficiently and remove any unused RDDs during operations.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用H2O Sparkling Water系统中的H2O AutoML与Apache Spark结合使用。我们首先了解了Apache
    Spark是什么。我们调查了构成Spark软件的各种组件。然后，我们深入研究了其架构，并了解了它是如何使用计算机集群来执行数据分析的。我们还调查了Spark集群管理器、Spark驱动程序、Executor以及Spark
    Context。然后，我们进一步深入研究了RDDs，并了解了Spark是如何使用它们在数据集上的转换操作上执行懒加载评估的。我们还了解到，Spark足够智能，能够高效地管理其资源，并在操作期间删除任何未使用的RDD。
- en: Building on top of this knowledge of Spark, we started exploring what H2O Sparkling
    Water is and how it uses Spark and H2O together in a seamlessly integrated system.
    We then dove deeper into its architecture and understood its two types of backends
    that can be used to deploy the system. We also understood how it handles data
    interchange between Spark and H2O.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握了Spark的相关知识的基础上，我们开始探索H2O Sparkling Water是什么，以及它是如何在一个无缝集成的系统中结合使用Spark和H2O的。然后，我们深入研究了其架构，并了解了可以用来部署系统的两种后端类型。我们还了解了它是如何处理Spark和H2O之间的数据交换的。
- en: Once we had a clear idea of what H2O Sparkling Water was, we proceeded with
    the practical implementation of using the system. We learned how to download and
    install the system and the strict dependencies it needs to run smoothly. We also
    explored the various configuration tunings that are recommended by H2O.ai when
    starting H2O Sparkling Water. Once the system was up and running, we performed
    an experiment where we used the Concrete Compressive Strength dataset to make
    predictions on the compressive strength of concrete using H2O Sparkling Water.
    We imported the dataset into a Spark cluster, performed AutoML using H2O AutoML,
    and used the leading model to make predictions. Finally, we learned how to export
    and import model MOJOs into H2O Sparkling Water and use them to make predictions.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对H2O Sparkling Water有了清晰的认识，我们就开始实际使用该系统的实施。我们学习了如何下载和安装系统以及它运行顺畅所需的严格依赖项。我们还探索了H2O.ai在启动H2O
    Sparkling Water时推荐的各种配置调整。一旦系统启动并运行，我们进行了一个实验，使用混凝土抗压强度数据集，通过H2O Sparkling Water对混凝土的抗压强度进行预测。我们将数据集导入Spark集群，使用H2O
    AutoML进行自动机器学习，并使用领先模型进行预测。最后，我们学习了如何将模型MOJOs导出和导入到H2O Sparkling Water中，并使用它们进行预测。
- en: In the next chapter, we shall explore a few case studies conducted by H2O.ai
    and understand the real-world implementation of H2O by businesses and how H2O
    helped them solve their ML problems.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨H2O.ai进行的一些案例研究，了解企业如何在实际应用中实施H2O，以及H2O如何帮助他们解决机器学习问题。
