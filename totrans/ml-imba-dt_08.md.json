["```py\nclass Net(torch.nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = torch.nn.Dropout2d()\n        self.fc1 = torch.nn.Linear(320, 50)\n        self.fc2 = torch.nn.Linear(50, 10)\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)),2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n```", "```py\ntorch.nn.CrossEntropyLoss(weight=None, …)\n```", "```py\nfrom sklearn.utils import class_weight\ny = imbalanced_train_loader.dataset.targets\nclass_weights=class_weight.compute_class_weight( \\\n    'balanced', np.unique(y), y.numpy())\nprint(class_weights)\n```", "```py\narray([0.25002533, 0.41181869, 0.68687384, 1.14620743, 1.91330749, 3.19159483, 5.32697842, 8.92108434, 14.809 , 24.68166667])\n```", "```py\ndef train(train_loader):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = torch.nn.functional.nll_loss(output, target weight)\n        loss.backward()\n        optimizer.step()\n```", "```py\nfrom datasets import load_dataset\ndataset = load_dataset(\"trec\")\n```", "```py\nfrom transformers import AutoTokenizer\nmodel_name = 'distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.model_max_length = 512\n```", "```py\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=False, truncation=True)\ntokenized_train_dataset = dataset[\"train\"].shuffle(seed=42).\\\n    map(tokenize_function, batched=True)\ntokenized_test_dataset = dataset[\"test\"].shuffle(seed=42).\\\n    map(tokenize_function, batched=True)\n```", "```py\nfrom transformers import \\\n    AutoModelForSequenceClassification, TrainingArguments\nmodel = AutoModelForSequenceClassification.from_pretrained(\\\n    model_name, num_labels=6)\n```", "```py\ndef get_training_args(runname):\n    training_args = TrainingArguments(\n        run_name=runname, output_dir=\"./results\", \\\n        num_train_epochs=5, evaluation_strategy=\"epoch\",\\\n        save_strategy=\"epoch\", warmup_ratio=0.1, \\\n        lr_scheduler_type='cosine', \\\n        auto_find_batch_size=True, \\\n        gradient_accumulation_steps=4, fp16=True, \\\n        log_level=\"error\"\n    )\n    return training_args\ntraining_args = get_training_args(model_name)\n```", "```py\nfrom transformers import EvalPrediction\nfrom typing import Dict\nfrom sklearn.metrics import precision_score, recall_score, f1_score\ndef custom_compute_metrics(res: EvalPrediction) -> Dict:\n    pred = res.predictions.argmax(axis=1)\n    target = res.label_ids\n    precision = precision_score(target, pred, average='macro')\n    recall = recall_score(target, pred, average='macro')\n    f1 = f1_score(target, pred, average='macro')\n    return {'precision': precision, 'recall': recall, 'f1': f1}\n```", "```py\nfrom transformers import Trainer\nclass CustomTrainerWeighted(Trainer):\n    def compute_loss(self, model, inputs, return_outputs):\n        labels = inputs.get(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.get('logits')\n        loss_fct = nn.CrossEntropyLoss( \\\n            weight=torch.from_numpy(class_weights).cuda(0).float()\n        )\n        loss = loss_fct(logits.view(-1,self.model.config.num_labels),\\\n            labels.view(-1))\n        return (loss, outputs) if return_outputs else loss\n```", "```py\nmodelWeighted = AutoModelForSequenceClassification \\\n    .from_pretrained(model_name, num_labels=4)\ntrainerWeighted = CustomTrainerWeighted(\\\n    model=modelWeighted, \\\n    args=training_args, \\\n    train_dataset=tokenized_train_dataset, \\\n    eval_dataset=tokenized_test_dataset, \\\n    tokenizer=tokenizer, \\\n    data_collator=data_collator, \\\n    compute_metrics=custom_compute_metrics)\ntrainerWeighted.train()\n```", "```py\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, gamma: float = 2, alpha =.98) -> None:\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n    def forward(self, pred: torch.Tensor, target: torch.Tensor):\n        # pred is tensor with log probabilities\n        nll_loss = torch.nn.NLLLoss(pred, target)\n        p_t = torch.exp(-nll_loss)\n        loss = (1 – p_t)**self.gamma * self.alpha * nll_loss\n        return loss.mean()\n```", "```py\ntorchvision.ops.sigmoid_focal_loss(\\\n    inputs: Tensor, targets: Tensor, alpha: float = 0.25,\\\n    gamma: float = 2, reduction: str = 'none')\n```", "```py\nclass ClassBalancedCrossEntropyLoss(torch.nn.Module):\n    def __init__(self, samples_per_cls, no_of_classes, beta=0.9999):\n        super().__init__()\n        self.beta = beta\n        self.samples_per_cls = samples_per_cls\n        self.no_of_classes = no_of_classes\n    def forward(self, model_log_probs: torch.Tensor, \\\n                labels: torch.Tensor):\n        effective_num = 1.0-np.power(self.beta,self.samples_per_cls)\n        weights = (1.0 - beta)/np.array(effective_num)\n        weights = weights/np.sum(weights) * self.no_of_classes\n        weights = torch.tensor(weights).float()\n        loss = torch.nn.NLLLoss(weights)\n        cb_loss = loss(model_log_probs, labels)\n        return cb_loss\n```", "```py\nclass CDT(torch.nn.Module):\n    def __init__(self, num_class_list, gamma=0.36):\n        super(CDT, self).__init__()\n        self.gamma = gamma\n        self.num_class_list = num_class_list\n        self.cdt_weight = torch.FloatTensor(\n            [\n            (max(self.num_class_list)/i) ** self.gamma\\\n            for i in self.num_class_list\n            ]\n        ).to(device)\n    def forward(self, inputs, targets):\n        inputs = inputs/self.cdt_weight\n        loss=torch.nn.functional.nll_loss(inputs,targets)\n        return loss\n```", "```py\nclass ClassDifficultyBalancedLoss(torch.nn.Module):\n    def __init__(self, class_difficulty, tau=1.5):\n        super().__init__()\n        self.class_difficulty = class_difficulty\n        self.weights = self.class_difficulty ** float(tau)\n        self.weights = self.weights / (\n            self.weights.sum() * len(self.weights))\n        self.loss = torch.nn.NLLLoss(\n            weight= torch.FloatTensor(self.weights))\n    def forward(self, input: torch.Tensor, target: torch.Tensor):\n        return self.loss(input, target)\n```"]