- en: '*Chapter 5*: Exploring Heuristic Search'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Heuristic search** is the third out of four groups of hyperparameter tuning
    methods. The key difference between this group and the other groups is that all
    the methods that belong to this group work by performing *trial and error* to
    achieve the optimal solution. Similar to the acquisition function in Bayesian
    optimization (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Exploring
    Bayesian Optimization*), all methods in this group also employ the concept of
    *exploration versus exploitation*. **Exploration** means performing a search in
    the unexplored space to lower the probability of being stuck in the local optima,
    while **exploitation** means performing a search in the local space that is known
    to have a good chance of containing the optimal solution.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss several methods that belong to the heuristic
    search group, including **simulated annealing** (**SA**), **genetic algorithms**
    (**GAs**), **particle swarm optimization** (**PSO**), and **Population-Based Training**
    (**PBT**). Similar to [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036), we
    will discuss the definition of each method, what the differences are between them,
    how they work, and the pros and cons of each method.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will understand the concept of the aforementioned
    hyperparameter tuning methods that belong to the heuristic search group. You will
    be able to explain these methods with confidence when someone asks you, at both
    a high-level and detailed fashion, along with the pros and cons. Once you are
    confident enough to explain them to other people, this means you have understood
    the ins and outs of each method. Thus, in practice, you can understand what’s
    happening if there are errors or you don’t get the expected results; you will
    also know how to configure the method so that it matches your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding simulated annealing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding genetic algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding particle swarm optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding population based training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding simulated annealing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**SA** is the heuristic search method that is inspired by the process of **metal
    annealing** in metallurgy. This method is similar to the random search hyperparameter
    tuning method (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)*, Exploring
    Exhaustive Search*), except for the existence of a criterion that guides how the
    hyperparameter tuning process works. In other words, SA is like a *smoothed version
    of random search*. Just like random search, it is suggested to use SA when each
    trial doesn’t take too much time and you have enough computational resources.'
  prefs: []
  type: TYPE_NORMAL
- en: In the metal annealing process, the metal is heated to a very high temperature
    for a certain time and slowly cooled to increase its strength, reducing its hardness
    and making it easier to work with. The goal of giving a very high heat is to excite
    the metal’s atoms so that they can move around freely and randomly. During this
    random movement, atoms usually tend to form a better configuration. Then, the
    slow cooling process is performed so that we can have a crystalline form of the
    material.
  prefs: []
  type: TYPE_NORMAL
- en: Just like in the metal annealing process, SA works by randomly choosing the
    set of hyperparameters to be tested. At each trial, the method will consider some
    of the “neighbors” of the current set, randomly. If the acceptance criterion is
    met, then the method will change its focus to that “neighbor” set. The acceptance
    criterion is not a deterministic function, it is a stochastic function, which
    means probability comes into play during the process. This probabilistic way of
    deciding is similar to the cooling phase in the metal annealing process, where
    we accept a smaller number of bad hyperparameter sets as more parts of the search
    space are explored.
  prefs: []
  type: TYPE_NORMAL
- en: SA is a modified version of one of the most popular heuristic optimization methods,
    known as **stochastic hill climbing** (**SHC**). SHC is very simple to understand
    and implement, which means that SA is as well. *In general*, SHC works by initializing
    the random point within a pre-defined bound (the *hyperparameter space, in our
    case*) and treating it as the current best solution. Then, it randomly searches
    for the next candidate within the surrounding of the selected point. Then, we
    need to compare the selected candidate with the current best solution. If the
    candidate is better than or equal to the current best solution, SHC will treat
    the candidate as the new best solution. This process is repeated until the stopping
    criterion is met.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps show how SHC optimization works in general:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the bound of the space, *B*, and the step size, *S*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the stopping criterion. Usually, it is defined as the number of iterations,
    but other stopping criteria definitions also work.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the random point within the bound, *B*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the selected point from *Step 3* as the current point, *current_point*,
    as well as the best point, *best_point*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample the next candidate within the *S* distance from *best_point*
    and within the bound, *B*, then store it as *candidate_point*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *candidate_point* is better than or equal to *best_point*, then replace *best_point*
    with *candidate_point*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace *current_point* with *candidate_point*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 5* to *7* until the stopping criterion is met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The main difference between SA and SHC is located in *Steps 5* and *6*. In
    SHC, we always sample the next candidate from the surrounding of the *best_point*,
    while in SA, we sample from the surrounding of *current_point*. In SHC, we only
    accept a candidate that is better than or equal to the current best solution,
    while in SA, we *may also accept a worse candidate* with a certain probability
    that is guided by the acceptance criterion, *AC*, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B18753_05_002.png) , ![](img/Formula_B18753_05_003.png)is
    the objective function and ![](img/Formula_B18753_05_004.png) is *temperature*
    with a positive value. See [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)
    if you are not familiar with the objective function term.
  prefs: []
  type: TYPE_NORMAL
- en: The ![](img/Formula_B18753_05_005.png)formula results in a value between 0 and
    1, where it always results in a value of 1 when the *candidate_point* is better
    than or equal to the *current_point*. In other words, we always accept the *candidate_point*
    when it is better than or equal to the *current_point*. It is worth noting that
    *better* does not necessarily mean has a greater value. If you are working with
    a maximization problem, then better means greater. However, if you are working
    with a minimization problem, then it is the other way around. For example, if
    the cross-validation score you are measuring is the **mean squared error** (**MSE**),
    where a lower score corresponds to better performance, then the *candidate_point*
    is considered better than the *current_point* if the ![](img/Formula_B18753_05_006.png)
    value is less than ![](img/Formula_B18753_05_007.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Although ![](img/Formula_B18753_05_008.png) is impacted by both ![](img/Formula_B18753_05_009.png)
    and ![](img/Formula_B18753_05_010.png), we can only control the value of ![](img/Formula_B18753_05_011.png).
    In practice, the *initial value* of ![](img/Formula_B18753_05_012.png) *is treated
    as a hyperparameter* and is usually set to a high value. Over the number of trials,
    the value of ![](img/Formula_B18753_05_013.png) is decreased following the so-called
    **annealing schedule** or cooling schedule scheme. There are several annealing
    schedule schemes that we can follow. The three most popular schemes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Geometric cooling**: This annealing schedule works by decreasing the temperature
    via a cooling factor of ![](img/Formula_B18753_05_014.png). In geometric cooling,
    the initial temperature, ![](img/Formula_B18753_05_015.png), is multiplied by
    the cooling factor ![](img/Formula_B18753_05_016.png) number of times, where ![](img/Formula_B18753_05_017.png)
    is the current number of iterations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be seen in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Effect of the initial temperature in geometric cooling on the
    acceptable criterion'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Effect of the initial temperature in geometric cooling on the acceptable
    criterion
  prefs: []
  type: TYPE_NORMAL
- en: '**Linear cooling**: This annealing schedule works by decreasing the temperature
    linearly via a cooling factor, ![](img/Formula_B18753_05_019.png). The value of
    ![](img/Formula_B18753_05_020.png)is chosen in such a way that ![](img/Formula_B18753_05_021.png)will
    still have a positive value after ![](img/Formula_B18753_05_022.png)iterations.
    For example, ![](img/Formula_B18753_05_023.png), where ![](img/Formula_B18753_05_024.png)
    is the expected final temperature after ![](img/Formula_B18753_05_025.png) iterations:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following graph shows this annealing schedule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – Effect of the initial temperature in linear cooling on the acceptable
    criterion'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 – Effect of the initial temperature in linear cooling on the acceptable
    criterion
  prefs: []
  type: TYPE_NORMAL
- en: '**Fast SA**: This annealing schedule works by decreasing the temperature proportional
    to the current number of iterations, ![](img/Formula_B18753_05_027.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This annealing schedule can be seen in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – Effect of the initial temperature in fast SA on the acceptable
    criterion'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – Effect of the initial temperature in fast SA on the acceptable
    criterion
  prefs: []
  type: TYPE_NORMAL
- en: Based on *Figures 5.1* to *5.3*, we can see that no matter what annealing schedule
    scheme we use and what the initial temperature is, we will always have a lower
    ![](img/Formula_B18753_05_029.png) value as the number of iterations increases,
    which means we will *accept fewer bad candidates as the number of iterations increases*.
    However, why do we need to accept bad candidates in the first place? The main
    purpose of SA not directly rejecting worse candidates, as in the SHC method, is
    to *balance the exploration and exploitation trade-off*. The high initial value
    of temperature allows SA to explore most of the parts of the hyperparameter space,
    and slowly focus on specific parts of the space as the number of iterations increases,
    just like how the metal annealing process works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that ![](img/Formula_B18753_05_030.png)only takes ![](img/Formula_B18753_05_031.png)into
    account when the *candidate_point* is worse than the *current_point.* This means
    that, based on *Figure 5.4*, we can say that the worse the suggested candidate
    is (the higher ![](img/Formula_B18753_05_032.png) is), the lower the value of
    ![](img/Formula_B18753_05_033.png)will be, and thus, the lower the probability
    of accepting the suggested bad candidate. This is the other way around for ![](img/Formula_B18753_05_034.png)
    in that the higher the value of ![](img/Formula_B18753_05_035.png)is, the higher
    the value of ![](img/Formula_B18753_05_036.png)will be, and thus, the higher the
    probability of accepting the suggested bad candidate (see *Figures 5.1* to *5.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Effect of Δf on the acceptable criterion'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – Effect of Δf on the acceptable criterion
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, the following steps show how *SA works as a hyperparameter tuning
    method*:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the original full data into train and test sets (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space, *H*, with the accompanied distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the initial temperature, *T0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the objective function, *f*, based on the train set (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the stopping criterion. Usually, the number of trials is used. However,
    it is also possible to use the time taken or convergence as the stopping criterion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the current temperature, *T*, using the value from *T0*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a random set of hyperparameters that have been sampled from the hyperparameter
    space, *H.*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the selected set from *Step 7* as the current set, *current_set*, as well
    as the best set, *best_set*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample the next candidate set, *candidate_set*, from the “neighbor”
    of the *current_set* within the hyperparameter space, *H*. The definition of the
    “neighbor” may differ across different types of hyperparameter distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a random number between 0 and 1 from the uniform distribution and store
    it as *rnd*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Decide whether to accept the *candidate_set* or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the value of ![](img/Formula_B18753_05_038.png) using the value of
    *T*, *f(candidate_set)*, and *f(current_set)*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the value of *rnd* is smaller than ![](img/Formula_B18753_05_039.png), then
    replace *current_set* with *candidate_set*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If *candidate_set* is better than or equal to *current_set*, then replace *best_set*
    with *candidate_set*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the annealing schedule to the temperature, *T*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 9* to *12* until the stopping criterion is met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on the full training set using the *best_set* hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following table lists the list of pros and cons of SA as a hyperparameter
    tuning method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Pros and cons of SA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Pros and cons of SA
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned about SA, starting from what it is, how it works,
    what makes it different from SHC and random search, and its pros and cons. We
    will discuss another interesting heuristic search method that is inspired by the
    natural selection theory in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding genetic algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GAs** are popular heuristic search methods that are inspired by Charles Darwin’s
    *theory of natural selection*. Unlike SA, which is classified as a **single-point-based**
    heuristic search method, GAs are categorized as **population-based** methods since
    they maintain a group of possible candidate solutions instead of just a single
    candidate solution at each trial. As a hyperparameter tuning method, you are recommended
    to utilize a GA when each trial doesn’t take too much time and you have enough
    computational resources, such as parallel computing resources.'
  prefs: []
  type: TYPE_NORMAL
- en: To have a better understanding of GAs, let’s start with a simple example. Let’s
    say we have a task to generate a pre-defined target word based on *only* a collection
    of words that are built from 26 alphabet letters in lowercase. For instance, the
    target word is “big,” and we have a collection that consists of the words “sea,”
    “pig,” “dog,” “bus,” and “tie.”
  prefs: []
  type: TYPE_NORMAL
- en: Based on the given collection of words, what should we do to generate the word
    “big?” It is no doubt a very easy and straightforward task. We just have to pick
    the letter “b” from the word “bus,” “i” from the word “pig” or “tie,” and “g”
    from the word “dog.” Voila! We get the word “big.” You may be wondering how this
    example is related to the GA method or even the natural selection theory. This
    example is a very simple task and there is no need to utilize a GA to solve the
    problem. However, we need this kind of example so that you have a better understanding
    of how GAs work since you already know the correct answer in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this task using GA, you must know the three key items in GA related
    to the evolution theory. The first key item is **variation**. Imagine if the given
    collection of words consists of only the word “sea.” There’s no way we can generate
    the word “big” based on only the word “sea.” This is why variation is needed in
    the **initial population** (*the collection of words, in our example*). Without
    enough variation, we may not be able to achieve the optimal solution (*to generate
    the word “big,” in our example*) since there is no **individual** (*each word
    in the collection of words, in our example*) within the population that can evolve
    to the target word.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Population is *not the hyperparameter space*. In GAs or other population-based
    heuristic search methods, population refers to the candidates of the optimal hyperparameter
    set.
  prefs: []
  type: TYPE_NORMAL
- en: The second key item is **selection**. You can think of this item as being similar
    to the idea of natural selection that happens in the real world. It’s about selecting
    individuals that are more suitable for the surrounding environment (*words that
    are similar to the word “big,” in our example*) and thus can survive in the world.
    In GAs, we need quantitative guidance for us to perform the selection, which is
    usually called the **fitness function**. This function helps us judge how good
    an individual is concerning the objective we want to achieve. In our example,
    we can create a fitness function that measures the proportion of indexes of the
    word that has the same letters as the target word in the corresponding indexes.
    For example, the word “tie” has a fitness score of ![](img/Formula_B18753_05_041.png)
    since only one out of three indexes contains the same letters as the target word,
    which is the index one that has the letter “i.”
  prefs: []
  type: TYPE_NORMAL
- en: Using this fitness function, we can evaluate the fitness score for each individual
    in the population, and then select which individuals should be added to the **mating
    pool** as **parents**. The mating pool is a collection of individuals that are
    considered high-quality individuals and thus called parents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The third key item is **heredity**. This item refers to the concept of **reproduction**
    or passing parents’ **genes** (*each letter in the word, in our example*) to their
    children or **offspring**. How is reproduction done in GAs? Taking the same spirit
    of natural selection, in Gas, we only perform the reproduction step from parents
    in the mating pool, meaning we only want to mate high-quality individuals with
    the hope to get only high-quality offspring in the next **generation** (a *new
    population is created in the next iteration*). There are two steps in the reproduction
    phase, namely the **crossover** and **mutation** steps. The crossover step is
    when we randomly mix or permute parents’ genes to generate offspring’s genes,
    while the mutation step is when we randomly change the value of offspring’s genes
    to add variation to the genes (see *Figure 5.6*). An individual that is mutated
    is called a **mutant**. The random value that is used in the mutation step should
    be drawn from the same gene’s distribution, meaning we can only use lower-case
    letters as the random values in our example, not floating points or integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – The crossover and mutation steps in a GA'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – The crossover and mutation steps in a GA
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are aware of the three key items in a GA, we can start solving
    the task from the previous example using a GA. Let’s assume we haven’t been given
    the collection of words so that we can learn the complete procedures of the GA.
    The target word is still “big.”
  prefs: []
  type: TYPE_NORMAL
- en: First, we must initialize a population with the *NPOP* number of individuals.
    The initialization process is usually done randomly to ensure we have enough variation
    in the population. By random, this means that the genes of each individual in
    the population are generated randomly. Let’s say we want to generate the initial
    population, which consists of seven individuals, where the generated results are
    “bee,” “tea,” “pie,” “bit,” “dog,” “cat,” and “dig.”
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can evaluate the fitness score of each individual in the population.
    Let’s say we use the fitness function that was defined previously. So, we got
    the following scores for each individual; “bee:” ![](img/Formula_B18753_05_042.png),
    “tea:” ![](img/Formula_B18753_05_043.png), “pie:” ![](img/Formula_B18753_05_044.png),
    “bit:” ![](img/Formula_B18753_05_045.png), ”dog:” ![](img/Formula_B18753_05_046.png),
    “cat:” ![](img/Formula_B18753_05_047.png), and “dig:” ![](img/Formula_B18753_05_048.png).
  prefs: []
  type: TYPE_NORMAL
- en: Based on the fitness score of each individual, we can select which individual
    should be added to the mating pool as a parent. There are many strategies that
    we can adopt to select the best individuals from the population, but in this case,
    let’s just get the top three individuals based on the fitness score and randomly
    select individuals that have the same fitness score. Let’s say that, after running
    the selection strategy, we get a mating pool that consists of “bit,” “dig,” and
    “bee” as parents.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to perform the crossover and mutation steps. Before that, however,
    we need to specify the crossover probability, *CXPB*, and the mutation probability,
    *MUTPB*, which defines the probability of crossing two parents in the mating pool
    and mutating an offspring, respectively. This means we are neither performing
    crossover on all parent pairs nor mutating all offspring – we will only perform
    those steps based on the predefined probability. Let’s say that only “dig” and
    “bee” have chosen to be crossed, and the resulting offspring of the crossover
    is “deg” and “bie.” So, the mating pool currently consists of “bit,” “deg,” and
    “bie.” Now, we need to perform mutation on “deg” and “bie.” Let’s say that after
    mutating them, we got “den” and “tie.” This means that the mating pool is currently
    consisting of “bit,” “den,” and “tie.”
  prefs: []
  type: TYPE_NORMAL
- en: After performing the crossover and mutation steps, we need to generate a new
    population for the next generation. The new population will consist of all crossed
    parents, mutated offspring, as well as other individuals from the current population.
    So, the next population consists of “bit,” “den,” “tie,” “tea,” “pie,” “dog,”
    and “cat.”
  prefs: []
  type: TYPE_NORMAL
- en: Based on the new population, we have to repeat the selection, crossover, and
    mutation process. This procedure needs to be done *NGEN* times, where NGEN refers
    to the number of generations, and it is predefined by the developer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps define *how GA works in general*, as an optimization method:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the population size, *NPOP*, the crossover probability, *CXPB*, the mutation
    probability, *MUTPB*, and the number of generations or number of trials, *NGEN*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the fitness function, *f*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a population with *NPOP* individuals, where each individual’s genes
    are initialized randomly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate all individuals in the population based on the fitness function, *f*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the best individuals based on *Step 4* and store them in a mating pool.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the crossover process on the parents in the mating pool with a probability
    of *CXPB*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the mutation process on the offspring results from *Step 8* with a probability
    of *MUTPB*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a new population consisting of all the individuals from *Step 6*, *Step
    7*, and the rest of the individuals from the current population.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the current population with the new population.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 6* to *9* *NGEN* times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let’s look at a more concrete example of how a GA works in general. We
    will use the same objective function that we used in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Evaluating Machine Learning Models* and treat this as a minimization problem.
    The objective function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_049.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B18753_05_050.png) is the noise that follows the standard
    normal distribution. We are only going to perform a search within the ![](img/Formula_B18753_05_051.png)
    range. It is worth noting that in this example, we assume that we know what the
    true objective function is. However, in practice, this function is unknown. In
    this case, each individual will only have one gene, which is the value of ![](img/Formula_B18753_05_052.png)
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say we define the hyperparameters for the GA method as *NPOP = 25*, *CXPB
    = 0.5*, *MUTPB = 0.15*, and *NGEN = 6*. As for the strategy of each **genetic
    operator**, we are using the **Tournament**, **Blend**, and **PolynomialBounded**
    strategies for selection, crossover, and mutation operators, respectively. The
    *Tournament* selection strategy works by selecting the best individuals among
    *tournsize* and the randomly chosen individual’s *NPOP* times, where *tournsize*
    is the number of individuals participating in the tournament. The *Blend* crossover
    strategy works by performing a linear combination between two continuous individual
    genes, where the weight of the linear combination is governed by the *alpha* hyperparameter.
    The *PolynomialBounded* mutation strategy works by passing continuous individual
    genes to a predefined polynomial mapping.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many strategies available that you can follow based on your hyperparameter
    space specification. We will talk more about different strategies and how to implement
    the GA method using the **DEAP** package in [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*,
    Advanced Hyperparameter Tuning with DEAP and Microsoft NNI*. For now, let’s see
    the results of applying a GA on the dummy objective function, *f*. Note that the
    points in each plot correspond to each individual in the population:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – GA process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – GA process
  prefs: []
  type: TYPE_NORMAL
- en: Based on the preceding figure, we can see that in the first generation, individuals
    are scattered all around the place since it is initialized randomly. In the second
    generation, several individuals that are initialized around point –1.0 moved to
    other places that have lower fitness scores. However, in the third generation,
    there are new individuals around point –1.0 again. This may be due to the random
    mutation operator that’s been applied to them. There are also several individuals
    stuck in the local optima, which is around point –0.5\. In the fourth generation,
    most of the individuals have moved to places with lower fitness scores, although
    some of them are still stuck in the local optima. In the fifth generation, individuals
    are starting to converge in several places.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, in the sixth generation, all of them converged to the near-global
    optima, which is around point 1.5\. Note that we still have *NPOP=25* individuals
    in the sixth generation, but all of them are located in the same place, which
    is why you can only see one dot in the plot. This also applies to other generations
    if you see that there are fewer than 25 individuals in the plot. The convergence
    trend across generations can be seen in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – Convergence plot'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – Convergence plot
  prefs: []
  type: TYPE_NORMAL
- en: The trend that’s shown in the preceding graph matches our previous analysis.
    However, we can get additional information from this plot. At first, many of the
    individuals are located in places with high fitness scores, but some individuals
    already get the best fitness score. Across generations, most of the individuals
    started to converge, and finally, in the last generation, all individuals had
    the best fitness score. It is worth noting that, in practice, it is not guaranteed
    that a GA will achieve the global optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you may be wondering, how can a GA be adopted as a hyperparameter
    tuning method? What is the corresponding definition of all terms in the GA within
    the context of hyperparameter tuning? What does an individual mean when performing
    hyperparameter tuning with a GA?
  prefs: []
  type: TYPE_NORMAL
- en: '*As a hyperparameter tuning method*, the GA method treats a set of hyperparameters
    as an individual where the hyperparameter values are the genes. To have better
    clarity on what each important term in the GA method means, in the context of
    hyperparameter tuning, please refer to the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Definition of GA method terms in the hyperparameter tuning context'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 – Definition of GA method terms in the hyperparameter tuning context
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you are aware of the corresponding definition of each important term
    in the GA method, we can define the formal procedure to utilize *the GA method
    as a hyperparameter tuning method*:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the original full data into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space, *H*, with the accompanied distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the population size, *NPOP*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the crossover probability, *CXPB*, and mutation probability, *MUTPB*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the number of trials, *NGEN*, as the stopping criterion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the objective function, *f*, based on the train set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Initialize* a population with *NPOP* sets of hyperparameters, where each set
    is drawn randomly from the hyperparameter space, *H*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate all hyperparameter sets in the population based on the objective function,
    *f*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Select* several best candidate sets based on *Step 8*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform *crossover* on candidate sets from *Step 9* with a probability of *CXPB*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform *mutation* on the crossed candidate sets from *Step 10* with a probability
    of *MUTPB*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a new population consisting of all sets of hyperparameters from *Step
    10*, *Step 11*, and the rest of the sets from the current population. The new
    population will also consist of *NPOP* sets of hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 8* to *12* *NGEN* times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on the full training set using the final hyperparameter values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is worth noting that when utilizing a GA as a hyperparameter tuning method,
    the GA itself has four hyperparameters, namely *NPOP*, *CXPB*, *MUTPB*, and *NGEN*,
    that control the performance of the hyperparameter tuning results, as well as
    the *exploration versus exploitation trade-off*. To be more precise, *CXPB* and
    *MUTPB*, or the crossover and mutation probability, respectively, are responsible
    for controlling the *exploration* rate, while the *selection* step, along with
    its strategy, controls the *exploitation* rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists the pros and cons of using a GA as a hyperparameter
    tuning method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Pros and Cons of the GA method'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – Pros and Cons of the GA method
  prefs: []
  type: TYPE_NORMAL
- en: The need to evaluate all individuals in each generation means we multiplied
    the original time complexity that our objective has by *NPOP * NGEN*. It’s very
    costly! That’s why the GA method is not suitable for you if you have an expensive
    objective function and/or low computational resources. However, if you do have
    time to wait for the experiment to be done, and you have massively parallel computing
    resources, then the GA method is suitable for you. From a theoretical perspective,
    the GA method can also work with various types of hyperparameters – we just need
    to choose the appropriate crossover and mutation strategies for the corresponding
    hyperparameters. The GA method is better than SA in terms of having a population
    to guide which part of the subspace needs to be exploited more. However, it is
    worth noting that the GA method can still be stuck in local optima.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discussed the GA method, starting with what it is, how it
    works both in terms of its general setup and the hyperparameter tuning context,
    and its pros and cons. We will discuss another interesting population-based heuristic
    search method in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding particle swarm optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PSO** is also a population-based heuristic search method, similar to the
    GA method. PSO is inspired by the schools of fish and flocks of birds’ social
    interaction in nature. As a hyperparameter tuning method, PSO is suggested to
    be utilized if your search space contains many non-categorical hyperparameters,
    each trial doesn’t take much time, and you have enough computational resources
    – especially parallel computing resources.'
  prefs: []
  type: TYPE_NORMAL
- en: PSO is one of the most popular methods within the bigger **swarm intelligence**
    (**SI**) group of methods. There are various methods in SI that are inspired by
    the social interaction of animals in nature, such as herds of land animals, colonies
    of ants, flocks of birds, schools of fish, and many more. The common characteristics
    of SI methods are *population-based*, individuals within the population are relatively
    *similar to each other*, and the ability of the population to move in a specific
    direction systemically *without a single coordinator* inside or outside the population.
    In other words, the population can organize themselves based on the *local interactions*
    of individuals interacting with each other and/or the surrounding environment.
  prefs: []
  type: TYPE_NORMAL
- en: When a flock of birds is looking for food, it is believed that each bird can
    contribute to the group by sharing information about their sights, so that the
    group can move in the right direction. PSO is a method that simulates the movement
    of a flock of birds to optimize the objective function. In PSO, the flock of birds
    is called a **swarm** and each bird is called a **particle**.
  prefs: []
  type: TYPE_NORMAL
- en: Each particle is defined by its **position** vector and **velocity** vector.
    The movement of each particle consists of both stochastic and deterministic components.
    In other words, the movement of each particle is not only based on a predefined
    rule but is also influenced by random components. Each particle also remembers
    its own **best position**, which gives the best objective function value along
    the trajectory it has passed. Then, along with the **global best position**, it
    is used to update the velocity and position of each particle at a particular time.
    The global best position is just the position of the best particle from the previous
    step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that ![](img/Formula_B18753_05_053.png) is the position vector in
    a *d*-dimensional space of the ![](img/Formula_B18753_05_054.png) particle out
    of *m* particles in the swarm, and that ![](img/Formula_B18753_05_055.png) is
    the velocity vector of the same size for the ![](img/Formula_B18753_05_056.png)
    particle, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_057.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_B18753_05_058.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s also define the best position for each particle and the global best position
    vectors, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_059.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_B18753_05_060.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following formulas define how each particle’s position and velocity vectors
    are updated in each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_061.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_B18753_05_062.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B18753_05_063.png), ![](img/Formula_B18753_05_064.png),
    and ![](img/Formula_B18753_05_065.png) are the hyperparameters that control the
    *exploration versus exploitation trade-off*. ![](img/Formula_B18753_05_066.png)
    has a value between zero and one is usually called the **inertia weight coefficient**,
    while ![](img/Formula_B18753_05_067.png) and ![](img/Formula_B18753_05_068.png)
    are called the **cognitive** and **social coefficients**, respectively. ![](img/Formula_B18753_05_069.png)
    and ![](img/Formula_B18753_05_070.png) are the random values between zero and
    one and act as the stochastic components of the particle movement. Note that the
    *d*-dimensions of the position and velocity vectors refer to the number of hyperparameters
    we have in the search space, while the *m* particles refer to the number of candidate
    hyperparameters that are sampled from the hyperparameter space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Updating the velocity vector may seem intimidating the first time, but actually,
    you can understand it more easily by treating the formula as three separate parts.
    The first part, or the left-most side of the formula, aims to update the next
    velocity proportional to the current velocity. The second part, or the middle
    part of the formula, aims to update the velocity toward the direction of the best
    position that the ![](img/Formula_B18753_05_071.png) particle has, while also
    adding a stochastic component to it. The third part, or the right-most side of
    the formula, aims to bring the ![](img/Formula_B18753_05_072.png) particle closer
    to the global best position, with additional random behavior applied to it. The
    following diagram helps illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.11 – Updating the particle’s position and velocity'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.11 – Updating the particle’s position and velocity
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram isn’t the same as the stated formula since the random
    components and the hyperparameters are missing from the picture. However, this
    diagram can help us understand the high-level concept of how each particle’s position
    and velocity vectors are updated in each iteration. We can see that the final
    updated velocity (*see the orange line*) is calculated based on three vectors,
    namely the current velocity (*see the brown line*), the particle best position
    (*see the green line*), and the global best position (*see the purple line*).
    Based on the final updated velocity, we can get the updated position of the ![](img/Formula_B18753_05_073.png)
    particle – that is, ![](img/Formula_B18753_05_074.png).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss how the hyperparameters affect the formula. The inertia weight
    coefficient, ![](img/Formula_B18753_05_075.png),controls how much we want to put
    our focus on the current velocity when updating the velocity vector. On the other
    hand, the cognitive coefficient, ![](img/Formula_B18753_05_076.png), and the social
    coefficient, ![](img/Formula_B18753_05_077.png), control how much we should focus
    on the particle’s past trajectory history and swarm’s search result, respectively.
    When we set ![](img/Formula_B18753_05_078.png), we don’t take into account the
    influence of the best position of the ![](img/Formula_B18753_05_079.png) particle,
    which may lead us to be *trapped in the local optima*. When we set ![](img/Formula_B18753_05_080.png),
    we ignore the influence of the global best position, which may lead us to a *slower
    convergence* speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you are aware of the position and velocity components of each particle
    in the swarm, take a look at the following steps, which define *how PSO works
    in general* as an optimization method:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the swarm size, *N*, the inertia weight coefficient, *w*, the cognitive
    coefficient, *c1*, the social coefficient, *c2*, and the maximum number of trials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the fitness function, *f*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a swarm with *N* particles, where each particle’s position and velocity
    vectors are initialized randomly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set each particle’s current position vector as their best position vector, *pbi*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the current global best position, *gb*, by selecting a position vector from
    all *N* particles that have the most optimal fitness score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update each particle’s position and velocity vector based on the updating formula.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate all the particles in the swarm based on the fitness function, *f*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update each particle’s best position vectors, *pbi*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare each particle’s current fitness score from *Step 7* with its *pbi* fitness
    score.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the current fitness score is better than the *pbi* fitness score, update
    *pbi* with the current position vector.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the global best position vector, *gb*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare each particle’s current fitness score from *Step 7* with the previous
    *gb* fitness score.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the current fitness score is better than the *gb* fitness score, update *gb*
    with the current position vector.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update each particle’s position and velocity vector based on the updating formula.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 7* to *10* until the maximum number of trials is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the final global best position, *gb*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is worth noting that the definition of the optimal fitness score (or a better
    fitness score in the previously stated procedure) will depend on what type of
    optimization problem you are trying to solve. If it is a minimization problem,
    then a smaller fitness score is better. If it is a maximization problem, then
    it is the other way around.
  prefs: []
  type: TYPE_NORMAL
- en: 'To have even a better understanding of how PSO works, let’s go through an example.
    Let’s define the fitness function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_081.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/Formula_B18753_05_082.png) and ![](img/Formula_B18753_05_083.png)
    are only defined within the ![](img/Formula_B18753_05_084.png) range. The following
    *contour plot* shows what our objective function looks like. We will learn more
    about how to implement PSO using the **DEAP** package in [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*,
    Advanced Hyperparameter Tuning with DEAP and Microsoft NNI*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.12 – A contour plot showing the objective function and its global
    minimum'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.12 – A contour plot showing the objective function and its global minimum
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can see that the global minimum (*see the red cross marker*) is located
    at (0.497, 0.295) with an objective function value of –0.649\. Let’s try to utilize
    PSO to see how well it estimates the minimum value of the objective function compared
    to the true global minimum. Let’s say we define the hyperparameter for PSO as
    *N=20*, *w=0.5*, *c1=0.3*, and *c2=0.5*, and set the maximum number of trials
    to 16\.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the initial swarm illustration in the following contour plot. The
    blue dots refer to each of the particles, the blue arrow on each particle refers
    to the particle’s velocity vector, the black dots refer to each particle’s best
    position vectors, and the red star marker refers to the current global best position
    vector at a particular iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.13 – A PSO initial swarm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.13 – A PSO initial swarm
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the initial particles at the swarm are initialized randomly, the direction
    of the velocity vectors is all over the place (see *Figure 5.13*). You can see
    how each particle’s position and velocity vectors are updated in each iteration,
    along with the global best position vector, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.14 – PSO process'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.14 – PSO process
  prefs: []
  type: TYPE_NORMAL
- en: Even at the first iteration, each particle’s velocity vector is pointing toward
    the global minimum, which is located in the bottom left of the plot. In each iteration,
    the position and velocity vectors are updated and move closer to the global minimum.
    At the end of the iteration loop, most of the particles are located around the
    global minimum position, where the final global best position vector is located
    at (0.496, 0.290) with a fitness score of around –0.648\. This estimation is very
    close to the true global minimum of the objective function!
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth noting that the velocity vector of each particle contains two components:
    magnitude and direction. The magnitude will impact the length of the velocity
    vector in *Figure 5.14*. While you may not see the difference in length between
    each particle’s velocity vector, they are different from each other!'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: '*As a hyperparameter tuning method*, in the PSO method, *particle* and *swarm*
    refer to the candidate set of hyperparameters that are sampled from the hyperparameter
    space and the collection of hyperparameter set candidates, respectively. The position
    vector of each particle refers to the values of each hyperparameter in a particle.
    Finally, the velocity vector refers to the *delta of hyperparameter values* that
    will be utilized to update the values of each hyperparameter in a particle.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps define *how PSO works as a hyperparameter tuning method*:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the original full data into train and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space, *H*, with the accompanied distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the collection size, *N*, the inertia weight coefficient, *w*, the cognitive
    coefficient, *c1*, the social coefficient, *c2*, and the maximum number of trials.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the objective function, *f*, based on the train set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a collection of *N* sets of hyperparameters, where each set is drawn
    randomly from the hyperparameter space, *H*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly initialize the velocity vector for each set of hyperparameters in the
    collection.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set each set’s current hyperparameter values as their best values, *pbi*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set the current global best set of hyperparameters, *gb*, by selecting a set
    from all *N* sets of hyperparameters that have the most optimal objective function
    score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update each set’s hyperparameter values and velocity vector based on the updating
    formula.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate all sets of hyperparameters in the collection based on the objective
    function, *f*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update each set’s best hyperparameter values, *pbi*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare each set’s current score from *Step 10* with its *pbi* score.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the current score is better than the *pbi* score, update *pbi* with the current
    hyperparameter values.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the global best set of hyperparameters, *gb*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compare each set’s current score from *Step 10* with the previous *gb* score.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If the current score is better than the *gb* score, update *gb* with the current
    set of hyperparameters.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update each set’s hyperparameter values and velocity vector based on the updating
    formula.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *Steps 10* to *13* until the maximum number of trials is reached.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train on the full training set using the global best set of hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final trained model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'One issue with the updating formula in the PSO method is that it only works
    on numerical variables, especially continuous variables, meaning we can’t directly
    utilize the original PSO as a hyperparameter tuning method if our hyperparameter
    space contains discrete hyperparameters. Motivated by this issue, there are several
    variants of PSO that are designed to be able to work in discrete spaces as well.
    The first variant is designed to work specifically for binary variables and is
    called **binary PSO**. In this variant, the updating formula for the velocity
    vector is the same, meaning we still treat the velocity vector in a continuous
    space, but the updating formula for the position vector is modified, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B18753_05_085.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B18753_05_086.png) is a random number drawn from a uniform
    distribution within the ![](img/Formula_B18753_05_087.png), ![](img/Formula_B18753_05_088.png)
    interval, and the *j* subscript refers to each component in the *i*thparticle.
    As you can see, in the binary PSO variant, we can work within the discrete space,
    but we are restricted to only having binary variables.
  prefs: []
  type: TYPE_NORMAL
- en: What about when we have a combination of discrete and continuous numerical hyperparameters?
    For example, our hyperparameter space for a neural network model contains the
    learning rate, dropout rate, and the number of layers. We can’t utilize the original
    PSO method directly since the number of layers hyperparameter expects an integer
    input, not a continuous or floating-point input. We also can’t utilize the binary
    PSO variant since the learning rate and dropout rate are continuous, and the number
    of layers hyperparameter is also not binary.
  prefs: []
  type: TYPE_NORMAL
- en: One simple thing we can do is *round the updated velocity* vector component
    values, but only for components that correspond to the discrete position component,
    before passing it to the position vector updating formula. This way, we can ensure
    that our discrete hyperparameters will still always be within the discrete space.
    However, this workaround still has an issue. The rounding operation may make the
    updating procedure of the velocity vector suboptimal. Why? Because of the possibility
    that no matter the updated values of the velocity vector, so long as they are
    still within a similar range of one integer point, then the position vector will
    not be updated anymore. This will contribute to a lot of redundant computational
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: There is another workaround to make PSO operate well both in continuous and
    discrete spaces. On top of rounding the updated velocity vector component values,
    we can also *update the inertia weight coefficient dynamically*. The motivation
    is to help a particle focus on its past velocity values so that it is not stuck
    in the local or global optimum, which is influenced by ![](img/Formula_B18753_05_089.png)
    or ![](img/Formula_B18753_05_090.png). The dynamic inertia weight updating procedure
    can be done based on several factors, such as the relative distance between its
    current position vector and its best position vector, the difference between the
    current number of trials and the maximum number of trials, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: There are many variants of how we can dynamically update the inertia weight
    coefficient during trials; we will leave it to you to choose what works well for
    your specific case.
  prefs: []
  type: TYPE_NORMAL
- en: Although we can modify the updating formula in PSO to make it work not only
    for continuous but also discrete variables, we are still faced with several issues,
    as stated previously. Thus, to utilize the maximum power of PSO within the continuous
    space, there’s another variant of PSO that tries to synergize PSO with the Bayesian
    optimization method, called **PSO-BO**. The goal of PSO-BO is to utilize PSO as
    a replacement for Bayesian optimization’s acquisition function optimizer (see
    [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)). So, rather than using a
    second-order optimization method to optimize the acquisition function, we can
    utilize PSO as the optimizer to help decide which set of hyperparameters to be
    tested in the next trial of the Bayesian optimization hyperparameter tuning procedure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the pros and cons of utilizing PSO as a hyperparameter
    tuning method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.15 – Pros and cons of PSO'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.15 – Pros and cons of PSO
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are aware of what PSO is, how it works, its several variants, and
    its pros and cons, let’s discuss another interesting population-based heuristic
    search method.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Population-Based Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**PBT** is a population-based heuristic search method, just like the GA method
    and PSO. However, PBT is not a nature-inspired algorithm like GA or PSO. Instead,
    inspired by the GA method itself. PBT is suggested for when you are working with
    a neural-network-based type of model and *just need the final trained model* without
    knowing the specifically chosen hyperparameter configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: PBT is specifically designed to *work only with a neural network-based* type
    of models, such as a multilayer perceptron, deep reinforcement learning, transformers,
    GAN, and any other neural network-based models. It can be said that PBT does both
    hyperparameter tuning and *model training* since the weights of the neural network
    model are inherited during the process. So, PBT is not only for choosing the most
    optimal hyperparameter configurations but also for transferring the weights or
    parameters of the model to other individuals within the population. That’s why
    the output of PBT is not a hyperparameter configuration but a model.
  prefs: []
  type: TYPE_NORMAL
- en: PBT is a *hybrid* method of the *random search* and *sequential search* methods,
    such as manual search and Bayesian search (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)*,
    Exploring Exhaustive Search* and [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization* for more details). Random search is a very good
    method for finding a good subspace for sensitive hyperparameters. Sequential search
    methods tend to give better performance than random search if we have enough computational
    resources and time to execute the optimization process. However, the fact that
    those methods need to be executed sequentially makes the experiment take a very
    long time to run. PBT comes with a solution to combine the best of both worlds
    into a *single training optimization process*, meaning the model training and
    hyperparameter tuning process are merged into a single process.
  prefs: []
  type: TYPE_NORMAL
- en: The term *Population-Based* in PBT comes from the fact that it is inspired by
    the GA method in terms of utilizing knowledge of the whole population to produce
    a better-performing individual. Note that the **individual** part of PBT refers
    to each of the *N* models with different parameters and hyperparameters in the
    **population** or a collection of all those *N* models.
  prefs: []
  type: TYPE_NORMAL
- en: The search process in PBT starts by *initializing a population*, *P*, that contains
    *N* models, ![](img/Formula_B18753_05_091.png), with their own randomly sampled
    parameters, ![](img/Formula_B18753_05_092.png), and randomly sampled hyperparameters,
    ![](img/Formula_B18753_05_093.png). Within each iteration of the search process,
    the *training step* is triggered for each of the *N* models. The training step
    consists of both forward and backward propagation procedures that utilize gradient-based
    optimization methods, just like the usual training procedure for a neural network-based
    model. Once the training step is done, the next step is to perform an *evaluation
    step*. The purpose of the evaluation step is to evaluate the current model’s *Mi*
    performance on the unseen validation data.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model, *Mi*, is considered *ready*, PBT will trigger the *exploit*
    and *explore* steps. The definition of a model being ready may vary, but we can
    define “ready” as passing a predefined number of steps or passing a predefined
    performance threshold. Both the exploit and explore steps have the same goal,
    which is to update the model’s parameters and hyperparameters. The difference
    is determined by how they do the update process.
  prefs: []
  type: TYPE_NORMAL
- en: The **exploit** step will decide, based on the evaluation results from the whole
    population, whether to keep utilizing the current set of parameters and hyperparameters
    or to focus on a more promising set. For example, the exploit step can be done
    by replacing a model that is considered as part of the bottom X% models in the
    whole population with a randomly sampled model from the top X% models in the population.
    Note that a model consists of all the parameters and hyperparameters. On the other
    hand, the **explore** step updates the model’s set of hyperparameters, *not parameters*,
    by proposing a new set. You can propose a new set by randomly perturbing the current
    set of hyperparameters with a predefined probability or by resampling the set
    of hyperparameters from the top X% models in the population. Note that this exploration
    step is only done on the chosen model from the exploitation step.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The exploration step in PBT is inspired by random search. This step can identify
    which subspace of hyperparameters needs to be explored more using *partially trained
    models* chosen from the exploitation step. The evaluation step that is done within
    the search process also enables us to remove the drawback of the sequential optimization
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The exploitation and exploration procedure in the PBT method allows us to update
    a model’s set of hyperparameters in an *online fashion*, while also putting more
    focus on the promising hyperparameter and weight space. The iterative process
    of train-eval-exploit-explore is performed *asynchronously in parallel* for each
    of the *N* individuals in the population until the stopping criterion is met.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps summarize *how PBT works as a single training optimization
    process*:'
  prefs: []
  type: TYPE_NORMAL
- en: Split the original full data into train, validation, and test sets (see [*Chapter
    1*](B18753_01_ePub.xhtml#_idTextAnchor014)*, Evaluating Machine Learning Models*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the hyperparameter space, *H*, with the accompanied distributions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the population size, *N*, the exploration perturbation factor, *perturb_fact*,
    the exploration resampling probability, *resample_prob*, and the exploitation
    fraction, *frac*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the model’s *readiness criterion*. Usually, the number of SGD optimization
    steps is used. However, it is also possible to use the model’s performance threshold
    as the criterion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the *checkpoint directory* that is used to store the model’s weights
    and hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the evaluation function, *f*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize a population, *P*, that contains *N* models, ![](img/Formula_B18753_05_094.png),
    with their own randomly sampled parameters, ![](img/Formula_B18753_05_095.png),
    and randomly sampled hyperparameters, ![](img/Formula_B18753_05_096.png), from
    the hyperparameter space, *H*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each model in the population, *P*, run the following steps *in parallel*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run one step of the training process for the model, *M*i, with the ![](img/Formula_B18753_05_097.png)
    parameter and a set of hyperparameters, ![](img/Formula_B18753_05_098.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If the *readiness criterion* has been met, do the following. If not, go back
    to *Step I*:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the *evaluation* step based on *f* on the validation set.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform the *exploitation* step on the model, *M*i, based on the predefined
    exploitation fraction, *frac*. This step will result in a new set of parameters
    and hyperparameters.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform the *exploration* step on the set of hyperparameters from the exploitation
    step based on the predefined *perturb_fact* and *resample_prob*.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform the *evaluation* step on the new set of parameters and hyperparameters
    based on *f* on the validation set.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Update* the model, *M*i, with the new set of parameters and hyperparameters.'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat Steps I and II until the end of the training loop. Usually, it is defined
    by the number of epochs.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the model with the best evaluation score in the population, *P*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the final model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is worth noting that, in practice, such as in the implementation of the **NNI**
    package (see [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*, Advanced
    Hyperparameter Tuning with DEAP and Microsoft NNI*), the readiness criterion defined
    in *Step 4* is an epoch. In other words, the second step within *Step 8* will
    only be run after each training epoch, not in the middle of an epoch. It is also
    worth noting that the checkpoint directory defined in *Step 5* is needed because,
    in PBT, we need to copy weights from another model in the population, while that’s
    not the case for the other hyperparameter tuning methods we’ve learned about so
    far.
  prefs: []
  type: TYPE_NORMAL
- en: While the original PBT algorithm states that we can run *Step 8* asynchronously
    in parallel, this is not the case in the implementation of the **NNI** package,
    which will be used in this book to implement PBT. In the NNI package implementation,
    the process is run *synchronously*, meaning that we can continue to the next epoch
    once all of the individuals or models in the population have finished the previous
    epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table lists the pros and cons of the PBT method:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.16 – Pros and cons of PBT'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18753_05_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.16 – Pros and cons of PBT
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned all you need to know about PBT, including what
    it is, how it works, what makes it different from other heuristic search methods,
    and its pros and cons.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the third out of four groups of hyperparameter
    tuning methods, called the heuristic search group. We discussed what the heuristic
    search method is in general and several variants of heuristic search methods,
    including SA, the GA method, PSO, and PBT. We saw what makes each of the variants
    differ from each other, along with the pros and cons of each. At this point, you
    should be able to explain heuristic search in confidence when someone asks you.
    You should also be able to debug and set up the most suitable configuration of
    the chosen method that suits your specific problem definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will start discussing multi-fidelity optimization,
    the last group of hyperparameter tuning methods. The goal of the next chapter
    is similar to this one’s: to provide a better understanding of the methods that
    belong to the multi-fidelity optimization group so that you can explain those
    methods in confidence when someone asks you. By doing this, you will be able to
    configure each of the methods for your specific problem!'
  prefs: []
  type: TYPE_NORMAL
