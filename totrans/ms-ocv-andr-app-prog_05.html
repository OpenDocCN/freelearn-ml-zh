<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Tracking Objects in Videos</h1></div></div></div><p>Object tracking<a id="id249" class="indexterm"/> is one of the most important applications of computer<a id="id250" class="indexterm"/> vision. It can be used for many applications, some of which are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Human–computer interaction: We might want to track the position of a person's finger and use its motion to control the cursor on our machines</li><li class="listitem" style="list-style-type: disc">Surveillance: Street cameras can capture pedestrians' motions that can be tracked to detect suspicious activities</li><li class="listitem" style="list-style-type: disc">Video stabilization and compression</li><li class="listitem" style="list-style-type: disc">Statistics in sports: By tracking a player's movement in a game of football, we can provide statistics such as distance travelled, heat maps, and so on</li></ul></div><p>In this chapter, you will learn the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Optical flow</li><li class="listitem" style="list-style-type: disc">Image Pyramids</li><li class="listitem" style="list-style-type: disc">Global Motion Estimation</li><li class="listitem" style="list-style-type: disc">The KLT tracker</li></ul></div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Optical flow</h1></div></div></div><p>Optical flow is an<a id="id251" class="indexterm"/> algorithm that detects the pattern of the motion of objects, or edges, between consecutive frames in a video. This motion may be caused by the motion of the object or the motion of the camera. Optical flow is a vector that depicts the motion of a point from the first frame to the second.</p><p>The optical flow algorithm works under two basic assumptions:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The pixel intensities are almost constant between consecutive frames</li><li class="listitem" style="list-style-type: disc">The neighboring pixels have the same motion as the anchor pixel</li></ul></div><p>We can represent the intensity of a pixel in any frame by <em>f(x,y,t)</em>. Here, the parameter <em>t</em> represents the frame in a video. Let's assume that, in the next <em>dt</em> time, the pixel moves by <em>(dx,dy)</em>. Since we <a id="id252" class="indexterm"/>have assumed that the intensity doesn't change in consecutive frames, we can say:</p><p>
<em>f(x,y,t) = f(x + dx,y + dy,t + dt)</em>
</p><p>Now we take the Taylor series expansion of the RHS in the preceding equation:</p><div><img src="img/B02052_05_10.jpg" alt="Optical flow"/></div><p>Cancelling the common term, we get:</p><div><img src="img/B02052_05_11.jpg" alt="Optical flow"/></div><p>Where <img src="img/B02052_05_12.jpg" alt="Optical flow"/>.</p><p>Dividing both sides of the equation by <em>dt</em> we get:</p><div><img src="img/B02052_05_13.jpg" alt="Optical flow"/></div><p>This equation is called the optical flow equation. Rearranging the equation we get:</p><div><img src="img/B02052_05_14.jpg" alt="Optical flow"/></div><p>We can see that this represents the equation of a line in the <em>(u,v)</em> plane. However, with only one equation available and two unknowns, this problem is under constraint at the moment. Two of the most widely used methods to calculate the optical flow are explained in the upcoming section.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec43"/>The Horn and Schunck method</h2></div></div></div><p>By taking <a id="id253" class="indexterm"/>into account our assumptions, we get:</p><div><img src="img/B02052_05_15.jpg" alt="The Horn and Schunck method"/></div><p>We can say that the first term will be small due to our assumption that the brightness is constant between consecutive frames. So, the square of this term will be even smaller. The second term corresponds to the assumption that the neighboring pixels have similar motion to the anchor pixel. We need to minimize the preceding equation. For this, we differentiate the preceding equation with respect to <em>u</em> and <em>v</em>. We get the following equations:</p><div><img src="img/B02052_05_16.jpg" alt="The Horn and Schunck method"/></div><div><img src="img/B02052_05_17.jpg" alt="The Horn and Schunck method"/></div><p>Here, <img src="img/B02052_05_18.jpg" alt="The Horn and Schunck method"/> and <img src="img/B02052_05_19.jpg" alt="The Horn and Schunck method"/> are the Laplacians of <em>u</em> and <em>v</em> respectively.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec44"/>The Lucas and Kanade method</h2></div></div></div><p>We start<a id="id254" class="indexterm"/> off with the optical flow equation that we derived earlier and noticed that it is under constrained as it has one equation and two variables:</p><div><img src="img/B02052_05_20.jpg" alt="The Lucas and Kanade method"/></div><p>To overcome this problem, we make use of the assumption that pixels in a 3x3 neighborhood <a id="id255" class="indexterm"/>have the same optical flow:</p><div><img src="img/B02052_05_21.jpg" alt="The Lucas and Kanade method"/></div><p>We can rewrite these equations in the form of matrices, as shown here:</p><div><img src="img/B02052_05_64.jpg" alt="The Lucas and Kanade method"/></div><p>This can be rewritten in the form:</p><div><img src="img/B02052_05_22.jpg" alt="The Lucas and Kanade method"/></div><p>Where:</p><div><img src="img/B02052_05_23.jpg" alt="The Lucas and Kanade method"/></div><p>As we can see, <em>A</em> is a 9x2 matrix, <em>U</em> is a 2x1 matrix, and <em>b</em> is a 9x1 matrix. Ideally, to solve for <em>U</em>, we just need to multiply by <img src="img/B02052_05_24.jpg" alt="The Lucas and Kanade method"/> on both sides of the equation. However, this is not possible, as we can only take the inverse of square matrices. Thus, we try to transform <em>A</em> into a square matrix by first multiplying the equation by <img src="img/B02052_05_25.jpg" alt="The Lucas and Kanade method"/> on both sides of the equation:</p><div><img src="img/B02052_05_26.jpg" alt="The Lucas and Kanade method"/></div><p>Now <img src="img/B02052_05_27.jpg" alt="The Lucas and Kanade method"/> is a <a id="id256" class="indexterm"/>square matrix of dimension 2x2. Hence, we can take its inverse:</p><div><img src="img/B02052_05_28.jpg" alt="The Lucas and Kanade method"/></div><p>On solving this equation, we get:</p><div><img src="img/B02052_05_29.jpg" alt="The Lucas and Kanade method"/></div><p>This method of multiplying the transpose and then taking an inverse is called <a id="id257" class="indexterm"/>
<strong>pseudo-inverse</strong>.</p><p>This equation can also be obtained by finding the minimum of the following equation:</p><div><img src="img/B02052_05_30.jpg" alt="The Lucas and Kanade method"/></div><p>According to the optical flow equation and our assumptions, this value should be equal to zero. Since the neighborhood pixels do not have exactly the same values as the anchor pixel, this value is very small. This method is called <a id="id258" class="indexterm"/>
<strong>Least Square Error</strong>. To solve for the minimum, we differentiate this equation with respect to <em>u</em> and <em>v</em>, and equate it to zero. We get the following equations:</p><div><img src="img/B02052_05_31.jpg" alt="The Lucas and Kanade method"/></div><div><img src="img/B02052_05_32.jpg" alt="The Lucas and Kanade method"/></div><p>Now we have two<a id="id259" class="indexterm"/> equations and two variables, so this system of equations can be solved. We rewrite the preceding equations as follows:</p><div><img src="img/B02052_05_33.jpg" alt="The Lucas and Kanade method"/></div><div><img src="img/B02052_05_34.jpg" alt="The Lucas and Kanade method"/></div><p>So, by arranging these equations in the form of a matrix, we get the same equation as obtained earlier:</p><div><img src="img/B02052_05_35.jpg" alt="The Lucas and Kanade method"/></div><p>Since, the matrix <em>A</em> is now a 2x2 matrix, it is possible to take an inverse. On taking the inverse, the equation obtained is as follows:</p><div><img src="img/B02052_05_36.jpg" alt="The Lucas and Kanade method"/></div><p>This can be simplified as:</p><div><img src="img/B02052_05_37.jpg" alt="The Lucas and Kanade method"/></div><p>Solving for <em>u</em> and <em>v</em>, we get:</p><div><img src="img/B02052_05_38.jpg" alt="The Lucas and Kanade method"/></div><div><img src="img/B02052_05_39.jpg" alt="The Lucas and Kanade method"/></div><p>Now we <a id="id260" class="indexterm"/>have the values for all the <img src="img/B02052_05_40.jpg" alt="The Lucas and Kanade method"/>, <img src="img/B02052_05_41.jpg" alt="The Lucas and Kanade method"/>, and <img src="img/B02052_05_42.jpg" alt="The Lucas and Kanade method"/>. Thus, we can find the values of <em>u</em> and <em>v</em> for each pixel.</p><p>When we implement this algorithm, it is observed that the optical flow is not very smooth near the edges of the objects. This is due to the brightness constraint not being satisfied. To overcome this situation, we use <a id="id261" class="indexterm"/>
<strong>image pyramids</strong> (explained in detail in the following sections).</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec45"/>Checking out the optical flow on Android</h2></div></div></div><p>To see the optical flow in action on <a id="id262" class="indexterm"/>Android, we will create a grid of points over a video feed from the camera, and then the lines will be drawn for each point that will depict the motion of the point on the video, which is superimposed by the point on the grid.</p><p>Before we begin, we will set up our project to use OpenCV and obtain the feed from the camera. We will process the frames to calculate the optical flow.</p><p>First, create a new<a id="id263" class="indexterm"/> project in Android Studio, in the same way as we did in the previous chapters. We will set the activity name to <code class="literal">MainActivity.java</code> and the XML resource file as <code class="literal">activity_main.xml</code>. Second, we will give the app the permissions to access the camera. In the <code class="literal">AndroidManifest.xml</code> file, add the following lines to the manifest tag:</p><div><pre class="programlisting">&lt;uses-permission android:name="android.permission.CAMERA" /&gt;</pre></div><p>Make sure that your activity tag for <code class="literal">MainActivity</code> contains the following line as an attribute:</p><div><pre class="programlisting">android:screenOrientation="landscape"</pre></div><p>Our <code class="literal">activity_main.xml</code> file will contain a simple <code class="literal">JavaCameraView</code>. This is a custom OpenCV defined layout that enables us to access the camera frames and processes them as normal <code class="literal">Mat</code> objects. The XML code has been shown here:</p><div><pre class="programlisting">&lt;LinearLayout 
    
    android:layout_width="match_parent"
    android:layout_height="match_parent"
    android:orientation="horizontal"&gt;

    &lt;org.opencv.android.JavaCameraView
        android:layout_width="fill_parent"
        android:layout_height="fill_parent"
        android:id="@+id/main_activity_surface_view" /&gt;

&lt;/LinearLayout&gt;</pre></div><p>Now, let's work on some Java code. First, we'll define some global variables that we will use later in the code or for other sections in this chapter:</p><div><pre class="programlisting">private static final String    TAG = "com.packtpub.masteringopencvandroid.chapter5.MainActivity";

    private static final int       VIEW_MODE_KLT_TRACKER = 0;
    private static final int       VIEW_MODE_OPTICAL_FLOW = 1;

    private int                    mViewMode;
    private Mat                    mRgba;
    private Mat                    mIntermediateMat;
    private Mat                    mGray;
    private Mat                    mPrevGray;

    MatOfPoint2f prevFeatures, nextFeatures;
    MatOfPoint features;

    MatOfByte status;
    MatOfFloat err;

    private MenuItem               mItemPreviewOpticalFlow, mItemPreviewKLT;

    private CameraBridgeViewBase   mOpenCvCameraView;</pre></div><p>We will need <a id="id264" class="indexterm"/>to create a callback function for OpenCV, like we did earlier. In addition to the code we used earlier, we will also enable <code class="literal">CameraView</code> to capture frames for processing:</p><div><pre class="programlisting">private BaseLoaderCallback  mLoaderCallback = new BaseLoaderCallback(this) {
        @Override
        public void onManagerConnected(int status) {
            switch (status) {
                case LoaderCallbackInterface.SUCCESS:
                {
                    Log.i(TAG, "OpenCV loaded successfully");

                    <strong>mOpenCvCameraView.enableView();</strong>
                } break;
                default:
                {
                    super.onManagerConnected(status);
                } break;
            }
        }
    };</pre></div><p>We will now check whether the OpenCV manager is installed on the phone, which contains the required libraries. In the <code class="literal">onResume</code> function, add the following line of code:</p><div><pre class="programlisting">OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION_2_4_10, this, mLoaderCallback);</pre></div><p>In the <code class="literal">onCreate()</code> function, add <a id="id265" class="indexterm"/>the following line before calling <code class="literal">setContentView</code> to prevent the screen from turning off, while using the app:</p><div><pre class="programlisting">getWindow().addFlags(WindowManager.LayoutParams.FLAG_KEEP_SCREEN_ON);</pre></div><p>We will now initialize our <code class="literal">JavaCameraView</code> object. Add the following lines after <code class="literal">setContentView</code> has been called:</p><div><pre class="programlisting">mOpenCvCameraView = (CameraBridgeViewBase) findViewById(R.id.main_activity_surface_view);
mOpenCvCameraView.setCvCameraViewListener(this);</pre></div><p>Notice that we called <code class="literal">setCvCameraViewListener</code> with the <code class="literal">this</code> parameter. For this, we need to make our activity implement the <code class="literal">CvCameraViewListener2</code> interface. So, your class definition for the <code class="literal">MainActivity</code> class should look like the following code:</p><div><pre class="programlisting">public class MainActivity extends Activity implements CvCameraViewListener2</pre></div><p>We will add a menu to this activity to toggle between different examples in this chapter. Add the following lines to the <code class="literal">onCreateOptionsMenu</code> function:</p><div><pre class="programlisting">mItemPreviewKLT = menu.add("KLT Tracker");
mItemPreviewOpticalFlow = menu.add("Optical Flow");</pre></div><p>We will now add some actions to the menu items. In the <code class="literal">onOptionsItemSelected</code> function, add the following lines:</p><div><pre class="programlisting">if (item == mItemPreviewOpticalFlow) {
            mViewMode = VIEW_MODE_OPTICAL_FLOW;
            resetVars();
        } else if (item == mItemPreviewKLT){
            mViewMode = VIEW_MODE_KLT_TRACKER;
            resetVars();
        }

        return true;</pre></div><p>We used a <code class="literal">resetVars</code> function to reset all the <code class="literal">Mat</code> objects. It has been defined as follows:</p><div><pre class="programlisting">private void resetVars(){
        mPrevGray = new Mat(mGray.rows(), mGray.cols(), CvType.CV_8UC1);
        features = new MatOfPoint();
        prevFeatures = new MatOfPoint2f();
        nextFeatures = new MatOfPoint2f();
        status = new MatOfByte();
        err = new MatOfFloat();
    }</pre></div><p>We will also add the <a id="id266" class="indexterm"/>code to make sure that the camera is released for use by other applications, whenever our application is suspended or killed. So, add the following snippet of code to the <code class="literal">onPause</code> and <code class="literal">onDestroy</code> functions:</p><div><pre class="programlisting">if (mOpenCvCameraView != null)
            mOpenCvCameraView.disableView();</pre></div><p>After the OpenCV camera has been started, the <code class="literal">onCameraViewStarted</code> function is called, which is where we will add all our object initializations:</p><div><pre class="programlisting">public void onCameraViewStarted(int width, int height) {
        mRgba = new Mat(height, width, CvType.CV_8UC4);
        mIntermediateMat = new Mat(height, width, CvType.CV_8UC4);
        mGray = new Mat(height, width, CvType.CV_8UC1);
        resetVars();
    }</pre></div><p>Similarly, the <code class="literal">onCameraViewStopped</code> function is called when we stop capturing frames. Here we will release all the objects we created when the view was started:</p><div><pre class="programlisting">public void onCameraViewStopped() {
        mRgba.release();
        mGray.release();
        mIntermediateMat.release();
    }</pre></div><p>Now we will add the implementation to process each frame of the feed that we captured from the camera. OpenCV calls the <code class="literal">onCameraFrame</code> method for each frame, with the frame as a parameter. We will use this to process each frame. We will use the <code class="literal">viewMode</code> variable to distinguish between the optical flow and the KLT tracker, and have different case constructs for the two:</p><div><pre class="programlisting">public Mat onCameraFrame(CvCameraViewFrame inputFrame) {
        final int viewMode = mViewMode;
        switch (viewMode) {
            case VIEW_MODE_OPTICAL_FLOW:</pre></div><p>We will use the <code class="literal">gray()</code>function to obtain the Mat object that contains the captured frame in a grayscale format. OpenCV also provides a similar function called <code class="literal">rgba()</code> to obtain a colored frame. Then we <a id="id267" class="indexterm"/>will check whether this is the first run. If this is the first run, we will create and fill up a <code class="literal">features</code> array that stores the position of all the points in a grid, where we will compute the optical flow:</p><div><pre class="programlisting">                mGray = inputFrame.gray();
                if(features.toArray().length==0){
                    int rowStep = 50, colStep = 100;
                    int nRows = mGray.rows()/rowStep, nCols = mGray.cols()/colStep;

                    Point points[] = new Point[nRows*nCols];
                    for(int i=0; i&lt;nRows; i++){
                        for(int j=0; j&lt;nCols; j++){
                            points[i*nCols+j]=new Point(j*colStep, i*rowStep);
                        }
                    }

                    features.fromArray(points);

                    prevFeatures.fromList(features.toList());
                    mPrevGray = mGray.clone();
                    break;
                }</pre></div><p>The <code class="literal">mPrevGray</code> object refers to the previous frame in a grayscale format. We copied the points to a <code class="literal">prevFeatures</code> object that we will use to calculate the optical flow and store the corresponding points in the next frame in <code class="literal">nextFeatures</code>. All of the computation is carried out in the <code class="literal">calcOpticalFlowPyrLK</code> OpenCV defined function. This function takes in the grayscale version of the previous frame, the current grayscale frame, an object that contains the feature points whose optical flow needs to be calculated, and an object that will store the position of the corresponding points in the current frame:</p><div><pre class="programlisting">                nextFeatures.fromArray(prevFeatures.toArray());
                Video.calcOpticalFlowPyrLK(mPrevGray, mGray, prevFeatures, nextFeatures, status, err);</pre></div><p>Now, we have the position of the grid of points and their position in the next frame as well. So, we will now draw a line that depicts the motion of each point on the grid:</p><div><pre class="programlisting">                List&lt;Point&gt; prevList=features.toList(), nextList=nextFeatures.toList();
                Scalar color = new Scalar(255);

                for(int i = 0; i&lt;prevList.size(); i++){
                    Core.line(mGray, prevList.get(i), nextList.get(i), color);
                }</pre></div><p>Before the loop ends, we have to copy the current frame to <code class="literal">mPrevGray</code> so that we can calculate the optical flow in the subsequent frames:</p><div><pre class="programlisting">                mPrevGray = mGray.clone();
                break;
default: mViewMode = VIEW_MODE_OPTICAL_FLOW;</pre></div><p>After we end the switch <a id="id268" class="indexterm"/>case construct, we will return a Mat object. This is the image that will be displayed as an output to the user of the application. Here, since all our operations and processing were performed on the grayscale image, we will return this image:</p><div><pre class="programlisting">return mGray;</pre></div><p>So, this is all about optical flow. The result can be seen in the following image:</p><div><img src="img/B02052_05_01.jpg" alt="Checking out the optical flow on Android"/><div><p>Optical flow at various points in the camera feed</p></div></div></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Image pyramids</h1></div></div></div><p>Pyramids are <a id="id269" class="indexterm"/>multiple copies of the same images that differ in their sizes. They are represented as layers, as shown in the following figure. Each level in the pyramid is obtained by reducing the rows and columns by half. Thus, effectively, we make the image's size one quarter of its original size:</p><div><img src="img/B02052_05_02.jpg" alt="Image pyramids"/><div><p>Relative sizes of pyramids</p></div></div><p>Pyramids intrinsically define<a id="id270" class="indexterm"/> <strong>reduce</strong> and <strong>expand</strong> as their two operations. Reduce refers <a id="id271" class="indexterm"/>to a reduction in the image's size, whereas expand refers to an increase in its size.</p><div><div><h3 class="title"><a id="note15"/>Note</h3><p>We will use a convention that lower levels in a pyramid mean downsized images and higher levels mean upsized images.</p></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec46"/>Gaussian pyramids</h2></div></div></div><p>In the reduce<a id="id272" class="indexterm"/> operation, the<a id="id273" class="indexterm"/> equation that we use to successively find levels in pyramids, while using a 5x5 sliding window, has been written as follows. Notice that the size of the image reduces to a quarter of its original size:</p><div><img src="img/B02052_05_43.jpg" alt="Gaussian pyramids"/></div><p>The elements of the <a id="id274" class="indexterm"/>weight kernel, <em>w</em>, should add up to 1. We use a 5x5 Gaussian kernel for this task. This operation is similar to convolution with <a id="id275" class="indexterm"/>the exception that the resulting image doesn't have the same size as the original image. The following image shows you the reduce operation:</p><div><img src="img/B02052_05_03.jpg" alt="Gaussian pyramids"/><div><p>The reduce operation</p></div></div><p>The expand operation is the reverse process of reduce. We try to generate images of a higher size from images that belong to lower layers. Thus, the resulting image is blurred and is of a lower resolution. The equation we use to perform expansion is as follows:</p><div><img src="img/B02052_05_44.jpg" alt="Gaussian pyramids"/></div><p>The weight kernel in this case, <em>w</em>, is the same as the one used to perform the reduce operation. The<a id="id276" class="indexterm"/> following image shows you the expand operation:</p><div><img src="img/B02052_05_04.jpg" alt="Gaussian pyramids"/><div><p>The expand operation</p></div></div><p>The weights are <a id="id277" class="indexterm"/>calculated using the Gaussian function that we used in <a class="link" href="ch01.html" title="Chapter 1. Applying Effects to Images">Chapter 1</a>, <em>Applying Effects to Images</em>, to perform Gaussian blur.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec47"/>Laplacian pyramids</h2></div></div></div><p>Laplacian<a id="id278" class="indexterm"/> pyramids <a id="id279" class="indexterm"/>are images that generally represent the edges. They are obtained from Gaussian pyramids. They are calculated using the following formula:</p><div><img src="img/B02052_05_45.jpg" alt="Laplacian pyramids"/></div><p><em>g<sub>i</sub> and Expand</em> (<em>g</em><sub><em>i</em>+1</sub>) are not the same once we downsize an image; we lose information, which cannot be recovered.</p><div><img src="img/B02052_05_05.jpg" alt="Laplacian pyramids"/></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec14"/>Gaussian and Laplacian pyramids in OpenCV</h3></div></div></div><p>To see how<a id="id280" class="indexterm"/> pyramids are <a id="id281" class="indexterm"/>created in OpenCV, we will create two new activities called <code class="literal">PyramidActivity</code> and <code class="literal">HomeActivity</code>. The <code class="literal">PyramidActivity</code> class will load an image from the gallery, and then, based on the user's options, perform the required actions. <code class="literal">HomeActivity</code> is used to call either <code class="literal">PyramidActivity</code> or <code class="literal">MainActivity</code> based on options provided by the user. So first, we make the resource for the <code class="literal">HomeActivity</code> class and call it <code class="literal">activity_home.xml</code>:</p><div><pre class="programlisting">&lt;?xml version="1.0" encoding="utf-8"?&gt;

&lt;ScrollView
    
    android:layout_width="match_parent"
    android:layout_height="match_parent" &gt;
    &lt;LinearLayout
        android:layout_height="match_parent"
        android:layout_width="match_parent"
        android:orientation="vertical" &gt;

        &lt;Button
            android:id="@+id/bPyramids"
            android:layout_height="wrap_content"
            android:layout_width="wrap_content"
            android:text="Image Pyramids" /&gt;

        &lt;Button
            android:id="@+id/bOptFlowKLT"
            android:layout_height="wrap_content"
            android:layout_width="wrap_content"
            android:text="Optical Flow and KLT Tracker" /&gt;

    &lt;/LinearLayout&gt;
&lt;/ScrollView&gt;</pre></div><p>In our Java code, we <a id="id282" class="indexterm"/>will <a id="id283" class="indexterm"/>add listeners to these buttons to call the respective activities, as follows:</p><div><pre class="programlisting">Button bPyramids, bOptFlowKLT;
        bPyramids = (Button) findViewById(R.id.bPyramids);
        bOptFlowKLT = (Button) findViewById(R.id.bOptFlowKLT);
        bOptFlowKLT.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Intent i = new Intent(getApplicationContext(), MainActivity.class);
                startActivity(i);
            }
        });
        bPyramids.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Intent i = new Intent(getApplicationContext(), PyramidActivity.class);
                startActivity(i);
            }
        });</pre></div><p>Now we move on to the implementation of <code class="literal">PyramidActivity</code>. First, we will take a look at <code class="literal">activity_pyramid.xml</code>. We will add buttons to perform various actions as per the user's options. The possible <a id="id284" class="indexterm"/>options are Gaussian pyramid up, Gaussian pyramid down, and Laplacian pyramid calculation. The <a id="id285" class="indexterm"/>following code is inserted into <code class="literal">LinearLayout</code> inside <code class="literal">ScrollView</code>:</p><div><pre class="programlisting">&lt;ImageView
            android:layout_width="match_parent"
            android:layout_height="0dp"
            android:layout_weight="0.5"
            android:id="@+id/ivImage" /&gt;
        &lt;LinearLayout
            android:layout_width="match_parent"
            android:layout_height="wrap_content"
            android:orientation="horizontal"&gt;
            &lt;Button
                android:layout_width="match_parent"
                android:layout_height="wrap_content"
                android:layout_weight="0.5"
                android:id="@+id/bGaussianPyrUp"
                android:text="Gaussian Pyramid Up"/&gt;
            &lt;Button
                android:layout_width="match_parent"
                android:layout_height="wrap_content"
                android:layout_weight="0.5"
                android:id="@+id/bGaussianPyrDown"
                android:text="Gaussian Pyramid Down"/&gt;
        &lt;/LinearLayout&gt;

        &lt;LinearLayout
            android:layout_width="match_parent"
            android:layout_height="wrap_content"
            android:orientation="horizontal"&gt;
            &lt;Button
                android:layout_width="match_parent"
                android:layout_height="wrap_content"
                android:id="@+id/bLaplacianPyr"
                android:text="Laplacian Pyramid"/&gt;
        &lt;/LinearLayout&gt;</pre></div><p>We will also have a menu file for this activity that will be used to load images from the gallery. We will have a <a id="id286" class="indexterm"/>similar method to load images from the gallery that we did in the earlier chapters. We will have the following lines in <code class="literal">PyramidActivity.java</code>:</p><div><pre class="programlisting">@Override
    public boolean onCreateOptionsMenu(Menu menu) {
        getMenuInflater().inflate(R.menu.menu_pyramid, menu);
        return true;
    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {
        int id = item.getItemId();

        if (id == R.id.action_load_first_image) {
            Intent photoPickerIntent = new Intent(Intent.ACTION_PICK);
            photoPickerIntent.setType("image/*");
            startActivityForResult(photoPickerIntent, SELECT_PHOTO);
            return true;
        }

        return super.onOptionsItemSelected(item);
    }</pre></div><p>Now we will define some global variables that we will need:</p><div><pre class="programlisting">private final int SELECT_PHOTO = 1;
private ImageView ivImage;
Mat src;
static int ACTION_MODE = 0;
static final int MODE_NONE = 0, MODE_GAUSSIAN_PYR_UP = 1, MODE_GAUSSIAN_PYR_DOWN = 2, MODE_LAPLACIAN_PYR = 3;
private boolean srcSelected = false;
Button bGaussianPyrUp, bGaussianPyrDown, bLaplacianPyr;</pre></div><p>We also need to specify the OpenCV callback function and initialize it in <code class="literal">onResume</code>, as we did earlier.</p><p>In our <code class="literal">onCreate</code> function, after <a id="id287" class="indexterm"/>we initialize all our buttons, we will first disable them until an image has been loaded from the gallery. So, add the following lines after initializing all the buttons in this activity:</p><div><pre class="programlisting">bGaussianPyrDown.setEnabled(false);
bGaussianPyrUp.setEnabled(false);
bLaplacianPyr.setEnabled(false);</pre></div><p>In our <code class="literal">onActivityResult</code>, we will check whether the image has been loaded successfully, and if it <a id="id288" class="indexterm"/>has<a id="id289" class="indexterm"/> been we activate the buttons. We also load the image to a Mat and store it for later use:</p><div><pre class="programlisting">switch(requestCode) {
            case SELECT_PHOTO:
                if(resultCode == RESULT_OK){
                    try {
                        final Uri imageUri = imageReturnedIntent.getData();
                        final InputStream imageStream = getContentResolver().openInputStream(imageUri);
                        final Bitmap selectedImage = BitmapFactory.decodeStream(imageStream);
                        src = new Mat(selectedImage.getHeight(), selectedImage.getWidth(), CvType.CV_8UC4);
                        Utils.bitmapToMat(selectedImage, src);
                        srcSelected = true;
                        bGaussianPyrUp.setEnabled(true);
                        bGaussianPyrDown.setEnabled(true);
                        bLaplacianPyr.setEnabled(true);
                    } catch (FileNotFoundException e) {
                        e.printStackTrace();
                    }
                }
                break;
        }</pre></div><p>Now we will add the listeners for each of the buttons. In your <code class="literal">onCreate</code>, add the following lines:</p><div><pre class="programlisting">bGaussianPyrUp.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                ACTION_MODE = MODE_GAUSSIAN_PYR_UP;
                executeTask();
            }
        });

bGaussianPyrDown.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                ACTION_MODE = MODE_GAUSSIAN_PYR_DOWN;
                executeTask();
            }
        });

bLaplacianPyr.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                ACTION_MODE = MODE_LAPLACIAN_PYR;
                executeTask();
            }
        });</pre></div><p>Now we <a id="id290" class="indexterm"/>will <a id="id291" class="indexterm"/>implement the <code class="literal">executeTask</code> function that will perform the required computations in <code class="literal">AsyncTask</code>, and after they are completed, they will be loaded into <code class="literal">ImageView</code> that we have in our layout:</p><div><pre class="programlisting">private void executeTask(){
        if(srcSelected){

            new AsyncTask&lt;Void, Void, Bitmap&gt;() {
                @Override
                protected void onPreExecute() {
                    super.onPreExecute();
                }

                @Override
                protected Bitmap doInBackground(Void... params) {
                    Mat srcRes = new Mat();
                    switch (ACTION_MODE){
                        case MODE_GAUSSIAN_PYR_UP:
                            Imgproc.pyrUp(src, srcRes);
                            break;
                        case MODE_GAUSSIAN_PYR_DOWN:
                            Imgproc.pyrDown(src, srcRes);
                            break;
                        case MODE_LAPLACIAN_PYR:
                            Imgproc.pyrDown(src, srcRes);
                            Imgproc.pyrUp(srcRes, srcRes);
                            Core.absdiff(src, srcRes, srcRes);
                            break;
                    }

                    if(ACTION_MODE != 0) {
                        Bitmap image = Bitmap.createBitmap(srcRes.cols(), srcRes.rows(), Bitmap.Config.ARGB_8888);

                        Utils.matToBitmap(srcRes, image);
                        return image;
                    }
                    return null;
                }

                @Override
                protected void onPostExecute(Bitmap bitmap) {
                      super.onPostExecute(bitmap);
                      if(bitmap!=null) {
                        ivImage.setImageBitmap(bitmap);
                    }
                }
            }.execute();
        }
    }</pre></div><p>Here, we have called <code class="literal">pyrUp</code> and <code class="literal">pyrDown</code> with<a id="id292" class="indexterm"/> just two arguments; however, you can specify a custom size for the results by calling the function as <code class="literal">Imgproc.pyrUp(srcMat, dstMat, resultSize)</code>.</p><p>OpenCV doesn't provide a <a id="id293" class="indexterm"/>separate function to calculate the Laplacian pyramid, but we can use the Gaussian pyramids to generate our Laplacian pyramids.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec34"/>Basic 2D transformations</h1></div></div></div><p>An <a id="id294" class="indexterm"/>object in 3D space can cast a projection in 2D space that is different from the original projection. Such transformations are called 2D transformations. They are shown in the following image. We will use some of these transformations to explain concepts discussed later in the chapter and also in other chapters:</p><div><img src="img/B02052_05_06.jpg" alt="Basic 2D transformations"/></div><p>We write these <a id="id295" class="indexterm"/>transformations in the mathematical form, along with their matrix representations, as shown here:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Translation</strong>: The <a id="id296" class="indexterm"/>mathematical representation of <a id="id297" class="indexterm"/>a translation transformation is given by:<div><img src="img/B02052_05_47.jpg" alt="Basic 2D transformations"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Affine</strong>: The mathematical representation of an <a id="id298" class="indexterm"/>affine transformation<a id="id299" class="indexterm"/> is given by:<div><img src="img/B02052_05_65.jpg" alt="Basic 2D transformations"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Rigid</strong>: The <a id="id300" class="indexterm"/>mathematical representation of a <a id="id301" class="indexterm"/>rigid transformation is given by:<div><img src="img/B02052_05_48.jpg" alt="Basic 2D transformations"/></div></li><li class="listitem" style="list-style-type: disc"><strong>Projective</strong>: The mathematical representation of a <a id="id302" class="indexterm"/>projective <a id="id303" class="indexterm"/>transformation is given by:<div><img src="img/B02052_05_49.jpg" alt="Basic 2D transformations"/></div></li></ul></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec35"/>Global motion estimation</h1></div></div></div><p>Global motion estimation, as the<a id="id304" class="indexterm"/> name suggests, is the detection of motion using all pixels in a frame in its calculation. Some of the applications of global motion estimation include:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Video stabilization</li><li class="listitem" style="list-style-type: disc">Video encoding/decoding</li><li class="listitem" style="list-style-type: disc">Object segmentation</li></ul></div><p>This method was proposed by Bergen et.al. (1992). In this method, when the distance between the camera and the background scenes is large, we can approximate the motion of objects as affine transformations. The equations we saw earlier were as follows:</p><div><img src="img/B02052_05_50.jpg" alt="Global motion estimation"/></div><p>We can rewrite these equations in the matrix form as follows:</p><div><img src="img/B02052_05_51.jpg" alt="Global motion estimation"/></div><p>This can be written as <img src="img/B02052_05_52.jpg" alt="Global motion estimation"/>.</p><p>According to the optical flow equation:</p><div><img src="img/B02052_05_53.jpg" alt="Global motion estimation"/></div><p>We try to estimate the motion in the image such that all the pixels satisfy it. Thus, we sum up the optical flow equation for all the pixels and try to generate an estimate:</p><div><img src="img/B02052_05_54.jpg" alt="Global motion estimation"/></div><p><img src="img/B02052_05_55.jpg" alt="Global motion estimation"/> should ideally<a id="id305" class="indexterm"/> be zero but practically, it is a small value. Thus, the squared error will be small. Hence, we need to minimize it for the best results:</p><div><img src="img/B02052_05_56.jpg" alt="Global motion estimation"/></div><div><img src="img/B02052_05_57.jpg" alt="Global motion estimation"/></div><p>This equation can be minimized with respect to <img src="img/B02052_05_61.jpg" alt="Global motion estimation"/> to the following linear equation:</p><div><img src="img/B02052_05_58.jpg" alt="Global motion estimation"/></div><p>This linear equation can be written as:</p><div><img src="img/B02052_05_59.jpg" alt="Global motion estimation"/></div><p>This algorithm is <a id="id306" class="indexterm"/>now divided into four subparts: pyramid construction, motion estimation, image warping, and coarse-to-fine refinement.</p><p>For the pyramid construction, we first take a Gaussian pyramid of the images at time <em>t</em> and <em>t-1</em>, and compute the global flows iteratively, starting from the smallest layers going toward the bigger layers.</p><p>Then, for each layer, to find the motion estimation, we use the linear equation derived earlier to compute A and B for the frames at time <em>t</em> and <em>t-1</em>, and use this information to compute an estimate for <em>a</em> <img src="img/B02052_05_60.jpg" alt="Global motion estimation"/>. We then warp the image at time <em>t-1</em> to an image, which tries to generate the object motion from the original image. This new image is compared to the image captured at time <em>t</em>. We then iteratively warp the image frame obtained at <em>t-1</em> to compute the value of <img src="img/B02052_05_61.jpg" alt="Global motion estimation"/>. With this value of <img src="img/B02052_05_61.jpg" alt="Global motion estimation"/>, we generate another warped image, which is then compared to the image at time <em>t</em>. We use this value of <img src="img/B02052_05_61.jpg" alt="Global motion estimation"/> to update the value of <em>a</em>. This process is performed multiple times until we have a good enough estimate of the motion of the image.</p><p>Image warping is the process of performing any transformation on an image to produce another image. For this method, we perform affine transformations because of our earlier assumptions:</p><div><img src="img/B02052_05_07.jpg" alt="Global motion estimation"/></div><p>For the final step, coarse-to-fine refinement, we make use of image pyramids to extend our model to include dense <a id="id307" class="indexterm"/>images (for example, representing a depth map).</p></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec36"/>The Kanade-Lucas-Tomasi tracker</h1></div></div></div><p>Having seen<a id="id308" class="indexterm"/> local and global motion estimation, we will now take a look at object tracking. Tracking objects is one of the most important applications of computer vision. The <strong>Kanade-Lucas-Tomasi</strong> (<strong>KLT</strong>) tracker implements an optical flow to<a id="id309" class="indexterm"/> track objects in videos. The steps to implement the algorithm are explained as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Detect Harris corners in the first frame of the video.</li><li class="listitem">For each detected Harris corner, compute the motion between consecutive frames using the optical flow (translator) and local affine transformation (affine).</li><li class="listitem">Now link these motion vectors from frame-to-frame to track the corners.</li><li class="listitem">Generate new Harris corners after a specific number of frames (say, 10 to 20) to compensate for new points entering the scene or to discard the ones going out of the scene.</li><li class="listitem">Track the new and old Harris points.</li></ol></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec48"/>Checking out the KLT tracker on OpenCV</h2></div></div></div><p>As we have seen earlier, the KLT<a id="id310" class="indexterm"/> tracker is one of the best algorithms available to track objects in videos. For this example, we will take a feed from the camera, detect some good trackable features, and update these to the new locations, as obtained by the <code class="literal">calcOpticalFlowPyrLK</code> function. We will just add a new case construct to the code that we wrote for the optical flow:</p><div><pre class="programlisting">case VIEW_MODE_KLT_TRACKER:
                mGray = inputFrame.gray();

                if(features.toArray().length==0){
                    Imgproc.goodFeaturesToTrack(mGray, features, 10, 0.01, 10);
                    prevFeatures.fromList(features.toList());
                    mPrevGray = mGray.clone();
                    break;
                }</pre></div><p>The <code class="literal">goodFeaturesToTrack</code> function uses the Shi-Tomasi method to calculate good trackable features in an image. This could be replaced by any reliable feature calculation technique. It takes the frame in a grayscale format as the input and returns the list of features. It also takes the parameter of the maximum number of features, to track, the quality of features, and the minimum distance between features respectively. For the purpose of this sample, we will only calculate features in the first frame and track these features in the subsequent frames.</p><p>Now we will obtain the optical flow for the feature points obtained previously. Note that <code class="literal">nextFeatures</code> contains the locations of the corresponding points in the previous frame in <code class="literal">prevFeatures</code>. We will mark the location of the feature points with circles. Note that we are drawing the circles at the new locations of the features:</p><div><pre class="programlisting">                Video.calcOpticalFlowPyrLK(mPrevGray, mGray, prevFeatures, nextFeatures, status, err);
                List&lt;Point&gt; drawFeature = nextFeatures.toList();
                for(int i = 0; i&lt;drawFeature.size(); i++){
                    Point p = drawFeature.get(i);
                    Core.circle(mGray, p, 5, new Scalar(255));
                }</pre></div><p>Now we need to set the current frame as the previous frame, and the current feature point locations as the locations of the features in the previous frame so as to enable tracking:</p><div><pre class="programlisting">                mPrevGray = mGray.clone();
                prevFeatures.fromList(nextFeatures.toList());
                break;</pre></div><p>The results of Shi-Tomasi<a id="id311" class="indexterm"/> tracker and the KLT tracker can be seen in the following image:</p><div><img src="img/B02052_05_08.jpg" alt="Checking out the KLT tracker on OpenCV"/></div><p>The white circles in the following image represent the features that we are tracking:</p><div><img src="img/B02052_05_09.jpg" alt="Checking out the KLT tracker on OpenCV"/></div><p>As it is visible, a small number of points are not tracked properly. For example, consider the feature point at the <em>L</em> key. As you can see, in one frame, it is at the <em>L</em> key, while in the other frame, it shifts to the<a id="id312" class="indexterm"/> key with the semicolon. If you consider the feature points at the <em>Y</em> and <em>J</em> keys, they remain in their positions. This is because at the keys <em>Y</em> and <em>J</em>, there are well-defined corners; hence, the feature points are better there.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Summary</h1></div></div></div><p>In this chapter, we have seen how to detect a local and global motion in a video, and how we can track objects. We have also learned about Gaussian and Laplacian pyramids, and how they can be used to improve the performance of some computer vision tasks.</p><p>In the next chapter, we will learn how to align multiple images and how to stitch them together to form a panoramic image.</p></div></body></html>