["```py\n#30 random numbers between -1 and 1 which are uniformly distributed\nx1 <- runif(30,-1,1) \nx2 <- runif(30,-1,1)\n#form the input vector x\nx <- cbind(x1,x2)\n\n```", "```py\n#helper function to calculate distance from hyperplane\ncalculate_distance = function(x,w,b) {\n sum(x*w) + b\n}\n\n#linear classifier\nlinear_classifier = function(x,w,b) {\ndistances =apply(x, 1, calculate_distance, w, b)\nreturn(ifelse(distances < 0, -1, +1))\n}\n\n```", "```py\n#function to calculate 2nd norm\nsecond_norm = function(x) {sqrt(sum(x * x))}\n\n#perceptron training algorithm\nperceptron = function(x, y, learning_rate=1) {\n\nw = vector(length = ncol(x)) # initialize w\nb = 0 # Initialize b\nk = 0 # count iterations\n\n#constant with value greater than distance of furthest point\nR = max(apply(x, 1, second_norm)) \n\nincorrect = TRUE # flag to identify classifier\n\n#initialize plot\nplot(x,cex=0.2)\n\n#loop till correct classifier is not found\nwhile (incorrect ) {\n\n incorrect =FALSE \n\n #classify with current weights\n yc <- linear_classifier(x,w,b)\n #Loop over each point in the input x\n for (i in 1:nrow(x)) {\n #update weights if point not classified correctly\n if (y[i] != yc[i]) {\n w <- w + learning_rate * y[i]*x[i,]\n b <- b + learning_rate * y[i]*R^2\n k <- k+1\n\n #currect classifier's components\n # update plot after ever 5 iterations\n if(k%%5 == 0){\n intercept <- - b / w[[2]]\n slope <- - w[[1]] / w[[2]]\n #plot the classifier hyper plane\n abline(intercept,slope,col=\"red\")\n #wait for user input\n cat (\"Iteration # \",k,\"\\n\")\n cat (\"Press [enter] to continue\")\n line <- readline()\n }\n incorrect =TRUE\n }\n }\n}\n\ns = second_norm(w)\n#scale the classifier with unit vector\nreturn(list(w=w/s,b=b/s,updates=k))\n}\n\n```", "```py\n#train the perceptron\np <- perceptron(x,Y)\n\n```", "```py\n#classify based on calculated \ny <- linear_classifier(x,p$w,p$b)\n\nplot(x,cex=0.2)\n\n#zoom into points near the separator and color code them\n#marking data points as + which have y=1 and â€“ for others\npoints(subset(x,Y==1),col=\"black\",pch=\"+\",cex=2)\npoints(subset(x,Y==-1),col=\"red\",pch=\"-\",cex=2)\n\n# compute intercept on y axis of separator\n# from w and b\nintercept <- - p$b / p$w[[2]]\n\n# compute slope of separator from w\nslope <- - p$w[[1]] /p$ w[[2]]\n\n# draw separating boundary\nabline(intercept,slope,col=\"green\")\n\n```", "```py\n#Height and weight vectors for 19 children\n\nheight <- c(69.1,56.4,65.3,62.8,63,57.3,59.8,62.5,62.5,59.0,51.3,64,56.4,66.5,72.2,65.0,67.0,57.6,66.6)\n\nweight <- c(113,84,99,103,102,83,85,113,84,99,51,90,77,112,150,128,133,85,112)\n\nplot(height,weight)\ncor(height,weight)\n\n```", "```py\n[1] 0.8848454\n\n```", "```py\n#Fitting the linear model\n\nmodel <- lm(weight ~ height) # weight = slope*weight + intercept\n\n#get the intercept(b0) and the slope(b1) values\nmodel\n\n```", "```py\n#check all attributes calculated by lm\nattributes(model)\n\n#getting only the intercept\nmodel$coefficients[1] #or model$coefficients[[1]]\n\n#getting only the slope\nmodel$coefficients[2] #or model$coefficients[[2]]\n\n#checking the residuals\nresiduals(model)\n\n#predicting the weight for a given height, say 60 inches\nmodel$coefficients[[2]]*50 + model$coefficients[[1]]\n\n#detailed information about the model\nsummary(model)\n\n```", "```py\n#plot data points\n plot(height,weight)\n\n#draw the regression line\nabline(model)\n\n```", "```py\niris\n#this should print the contents of data set onto the console.\n\n```", "```py\n#skip these steps if you already have iris on your system\niris <- read.csv(url(\"http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"), header = FALSE)\n\n#assign proper headers\nnames(iris) <- c(\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\", \"Species\")\n\n```", "```py\n#to view top few rows of data\nhead(iris)\n\n```", "```py\n#to view data types, sample values, categorical values, etc\nstr(iris)\n\n```", "```py\n#detailed view of the data set\nsummary(iris)\n\n```", "```py\ninstall.packages(\"ggvis\")\n\n```", "```py\n#load the package\nlibrary(ggvis)\n\n#plot the species\niris %>% ggvis(~Petal.Length, ~Petal.Width, fill = ~factor(Species)) %>% layer_points()\n\n```", "```py\n%>%, to pass input data to ggvis and again uses the pipe operator to pass on the output to layer_points for final plotting. The ~ operator signifies to ggvis that Petal.Length is a variable in the input dataset (iris). Read more about ggvis at http://ggvis.rstudio.com/ggvis-basics.html.\n```", "```py\n#normalization function\n\nmin_max_normalizer <- function(x)\n{\nnum <- x - min(x) \ndenom <- max(x) - min(x)\nreturn (num/denom)\n}\n\n```", "```py\n#normalizing iris data set\nnormalized_iris <- as.data.frame(lapply(iris[1:4], min_max_normalizer))\n\n#viewing normalized data\nsummary(normalized_iris)\n\n```", "```py\n#checking the data constituency\ntable(iris$Species)\n\n```", "```py\n#set seed for randomization\nset.seed(1234)\n\n# setting the training-test split to 67% and 33% respectively\nrandom_samples <- sample(2, nrow(iris), replace=TRUE, prob=c(0.67, 0.33))\n\n# training data set\niris.training <- iris[\nrandom_samples ==1, 1:4] \n\n#training labels\niris.trainLabels <- iris[\nrandom_samples ==1, 5]\n\n# test data set\niris.test <- iris[\nrandom_samples ==2, 1:4]\n\n#testing labels\niris.testLabels <- iris[\nrandom_samples ==2, 5]\n\n```", "```py\n#setting library\nlibrary(class)\n\n#executing knn for k=3\niris_model <- knn(train = iris.training, test = iris.test, cl = iris.trainLabels, k=3)\n\n#summary of the model learnt\niris_model\n\n```", "```py\n#setting library\nlibrary(gmodels)\n\n#Preparing cross table\nCrossTable(x = iris.testLabels, y = iris_model, prop.chisq=FALSE)\n\n```", "```py\n# setting the apriori library\nlibrary(arules)\n\n# loading data\ndata(\"Adult\");\n\n```", "```py\n# summary of data set\nsummary(Adult);\n\n# Sample 5 records\ninspect(Adult[0:5]);\n\n```", "```py\n# executing apriori with support=50% confidence =80%\nrules <- apriori(Adult, parameter=list(support=0.5, confidence=0.8,target=\"rules\"));\n\n# view a summary\nsummary(rules);\n\n#view top 3 rules\nas(head(sort(rules, by = c(\"confidence\", \"support\")), n=3), \"data.frame\")\n\n```", "```py\n# prepare a copy of iris data set\nkmean_iris <- iris\n\n#Erase/ Nullify species labels\nkmean_iris$Species <- NULL\n\n#apply k-means with k=3\n(clusters <- kmeans(kmean_iris, 3))\n\n```", "```py\n# comparing cluster labels with actual iris  species labels.\ntable(iris$Species, clusters$cluster)\n\n```", "```py\n# plot the clustered points along sepal length and width\nplot(kmean_iris[c(\"Sepal.Length\", \"Sepal.Width\")], col=clusters$cluster,pch = c(15,16,17)[as.numeric(clusters$cluster)])\n\npoints(clusters$centers[,c(\"Sepal.Length\", \"Sepal.Width\")], col=1:3, pch=8, cex=2)\n\n```"]