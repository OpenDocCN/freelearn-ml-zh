["```py\ndef cvResizeCapture(capture, preferredSize):\n\n    # Try to set the requested dimensions.\n    w, h = preferredSize\n    capture.set(cv2.CAP_PROP_FRAME_WIDTH, w)\n    capture.set(cv2.CAP_PROP_FRAME_HEIGHT, h)\n\n    # Sometimes the dimensions fluctuate at the start of capture.\n    # Discard two frames to allow for this.\n    capture.read()\n    capture.read()\n\n    # Try to return the actual dimensions of the third frame.\n    success, image = capture.read()\n    if success and image is not None:\n        h, w = image.shape[:2]\n    return (w, h) \n```", "```py\nimport binascii\n\ndef fourCharsToInt(s):\n    return int(binascii.hexlify(bytearray(s, 'ascii')), 16)\n\ndef intToFourChars(i):\n    return binascii.unhexlify(format(i, 'x')).decode('ascii')\n```", "```py\nimport numpy\nimport cv2\nimport os\nimport sys\nimport threading\nimport wx\n\nimport BinasciiUtils\nimport ResizeUtils\nimport WxUtils\n```", "```py\nclass InteractiveRecognizer(wx.Frame):\n\n    def __init__(self, recognizerPath, cascadePath,\n                 scaleFactor=1.3, minNeighbors=4,\n                 minSizeProportional=(0.25, 0.25),\n                 rectColor=(0, 255, 0),\n                 cameraDeviceID=0, imageSize=(1280, 720),\n                 title='Interactive Recognizer'):\n```", "```py\n        self.mirrored = True\n```", "```py\n        self._running = True\n```", "```py\n        self._capture = cv2.VideoCapture(cameraDeviceID)\n        size = ResizeUtils.cvResizeCapture(\n                self._capture, imageSize)\n        self._imageWidth, self._imageHeight = size\n```", "```py\n        self._image = None\n        self._grayImage = None\n        self._equalizedGrayImage = None\n\n        self._imageFrontBuffer = None\n        self._imageFrontBufferLock = threading.Lock()\n```", "```py\n        self._currDetectedObject = None\n\n        self._recognizerPath = recognizerPath\n        self._recognizer = cv2.face.LBPHFaceRecognizer_create()\n        if os.path.isfile(recognizerPath):\n            self._recognizer.read(recognizerPath)\n            self._recognizerTrained = True\n        else:\n            self._recognizerTrained = False\n\n        self._detector = cv2.CascadeClassifier(cascadePath)\n        self._scaleFactor = scaleFactor\n        self._minNeighbors = minNeighbors\n        minImageSize = min(self._imageWidth, self._imageHeight)\n        self._minSize = (int(minImageSize * minSizeProportional[0]),\n                         int(minImageSize * minSizeProportional[1]))\n        self._rectColor = rectColor\n```", "```py\n        style = wx.CLOSE_BOX | wx.MINIMIZE_BOX | wx.CAPTION | \\\n            wx.SYSTEM_MENU | wx.CLIP_CHILDREN\n        wx.Frame.__init__(self, None, title=title,\n                          style=style, size=size)\n        self.SetBackgroundColour(wx.Colour(232, 232, 232))\n\n        self.Bind(wx.EVT_CLOSE, self._onCloseWindow)\n```", "```py\n        quitCommandID = wx.NewId()\n        self.Bind(wx.EVT_MENU, self._onQuitCommand,\n                  id=quitCommandID)\n        acceleratorTable = wx.AcceleratorTable([\n            (wx.ACCEL_NORMAL, wx.WXK_ESCAPE, quitCommandID)\n        ])\n        self.SetAcceleratorTable(acceleratorTable)\n```", "```py\n        self._videoPanel = wx.Panel(self, size=size)\n        self._videoPanel.Bind(\n                wx.EVT_ERASE_BACKGROUND,\n                self._onVideoPanelEraseBackground)\n        self._videoPanel.Bind(\n                wx.EVT_PAINT, self._onVideoPanelPaint)\n\n        self._videoBitmap = None\n\n        self._referenceTextCtrl = wx.TextCtrl(\n                self, style=wx.TE_PROCESS_ENTER)\n        self._referenceTextCtrl.SetMaxLength(4)\n        self._referenceTextCtrl.Bind(\n                wx.EVT_KEY_UP, self._onReferenceTextCtrlKeyUp)\n\n        self._predictionStaticText = wx.StaticText(self)\n        # Insert an endline for consistent spacing.\n        self._predictionStaticText.SetLabel('\\n')\n\n        self._updateModelButton = wx.Button(\n                self, label='Add to Model')\n        self._updateModelButton.Bind(\n                wx.EVT_BUTTON, self._updateModel)\n        self._updateModelButton.Disable()\n\n        self._clearModelButton = wx.Button(\n                self, label='Clear Model')\n        self._clearModelButton.Bind(\n                wx.EVT_BUTTON, self._clearModel)\n        if not self._recognizerTrained:\n            self._clearModelButton.Disable()\n```", "```py\n        border = 12\n\n        controlsSizer = wx.BoxSizer(wx.HORIZONTAL)\n        controlsSizer.Add(self._referenceTextCtrl, 0,\n                          wx.ALIGN_CENTER_VERTICAL | wx.RIGHT,\n                          border)\n        controlsSizer.Add(\n                self._updateModelButton, 0,\n                wx.ALIGN_CENTER_VERTICAL | wx.RIGHT, border)\n        controlsSizer.Add(self._predictionStaticText, 0,\n                          wx.ALIGN_CENTER_VERTICAL)\n        controlsSizer.Add((0, 0), 1) # Spacer\n        controlsSizer.Add(self._clearModelButton, 0,\n                          wx.ALIGN_CENTER_VERTICAL)\n\n        rootSizer = wx.BoxSizer(wx.VERTICAL)\n        rootSizer.Add(self._videoPanel)\n        rootSizer.Add(controlsSizer, 0, wx.EXPAND | wx.ALL, border)\n        self.SetSizerAndFit(rootSizer)\n```", "```py\n        self._captureThread = threading.Thread(\n                target=self._runCaptureLoop)\n        self._captureThread.start()\n```", "```py\n    def _onCloseWindow(self, event):\n        self._running = False\n        self._captureThread.join()\n        if self._recognizerTrained:\n            modelDir = os.path.dirname(self._recognizerPath)\n            if not os.path.isdir(modelDir):\n                os.makedirs(modelDir)\n            self._recognizer.write(self._recognizerPath)\n        self.Destroy()\n```", "```py\n    def _onQuitCommand(self, event):\n        self.Close()\n```", "```py\n    def _onVideoPanelEraseBackground(self, event):\n        pass\n\n    def _onVideoPanelPaint(self, event):\n\n        self._imageFrontBufferLock.acquire()\n\n        if self._imageFrontBuffer is None:\n            self._imageFrontBufferLock.release()\n            return\n\n        # Convert the image to bitmap format.\n        self._videoBitmap = \\\n                WxUtils.wxBitmapFromCvImage(self._imageFrontBuffer)\n\n        self._imageFrontBufferLock.release()\n\n        # Show the bitmap.\n        dc = wx.BufferedPaintDC(self._videoPanel)\n        dc.DrawBitmap(self._videoBitmap, 0, 0)\n```", "```py\n    def _onReferenceTextCtrlKeyUp(self, event):\n        self._enableOrDisableUpdateModelButton()\n```", "```py\n    def _updateModel(self, event):\n        labelAsStr = self._referenceTextCtrl.GetValue()\n        labelAsInt = BinasciiUtils.fourCharsToInt(labelAsStr)\n        src = [self._currDetectedObject]\n        labels = numpy.array([labelAsInt])\n        if self._recognizerTrained:\n            self._recognizer.update(src, labels)\n        else:\n            self._recognizer.train(src, labels)\n            self._recognizerTrained = True\n            self._clearModelButton.Enable()\n```", "```py\n    def _clearModel(self, event=None):\n        self._recognizerTrained = False\n        self._clearModelButton.Disable()\n        if os.path.isfile(self._recognizerPath):\n            os.remove(self._recognizerPath)\n        self._recognizer = cv2.face.LBPHFaceRecognizer_create()\n```", "```py\n    def _runCaptureLoop(self):\n        while self._running:\n            success, self._image = self._capture.read(\n                    self._image)\n            if self._image is not None:\n                self._detectAndRecognize()\n                if (self.mirrored):\n                    self._image[:] = numpy.fliplr(self._image)\n\n                # Perform a thread-safe swap of the front and\n                # back image buffers.\n                self._imageFrontBufferLock.acquire()\n                self._imageFrontBuffer, self._image = \\\n                        self._image, self._imageFrontBuffer\n                self._imageFrontBufferLock.release()\n\n                # Send a refresh event to the video panel so\n                # that it will draw the image from the front\n                # buffer.\n                self._videoPanel.Refresh()\n```", "```py\n    def _detectAndRecognize(self):\n        self._grayImage = cv2.cvtColor(\n                self._image, cv2.COLOR_BGR2GRAY,\n                self._grayImage)\n        self._equalizedGrayImage = cv2.equalizeHist(\n                self._grayImage, self._equalizedGrayImage)\n        rects = self._detector.detectMultiScale(\n                self._equalizedGrayImage,\n                scaleFactor=self._scaleFactor,\n                minNeighbors=self._minNeighbors,\n                minSize=self._minSize)\n        for x, y, w, h in rects:\n            cv2.rectangle(self._image, (x, y), (x+w, y+h),\n                          self._rectColor, 1)\n        if len(rects) > 0:\n            x, y, w, h = rects[0]\n            self._currDetectedObject = cv2.equalizeHist(\n                    self._grayImage[y:y+h, x:x+w])\n```", "```py\n            if self._recognizerTrained:\n                try:\n                    labelAsInt, distance = self._recognizer.predict(\n                            self._currDetectedObject)\n                    labelAsStr = BinasciiUtils.intToFourChars(labelAsInt)\n                    self._showMessage(\n                            'This looks most like %s.\\n'\n                            'The distance is %.0f.' % \\\n                            (labelAsStr, distance))\n                except cv2.error:\n                    print >> sys.stderr, \\\n                            'Recreating model due to error.'\n                    self._clearModel()\n            else:\n                self._showInstructions()\n```", "```py\n        else:\n            self._currDetectedObject = None\n            if self._recognizerTrained:\n                self._clearMessage()\n            else:\n                self._showInstructions()\n\n        self._enableOrDisableUpdateModelButton()\n```", "```py\n    def _enableOrDisableUpdateModelButton(self):\n        labelAsStr = self._referenceTextCtrl.GetValue()\n        if len(labelAsStr) < 1 or \\\n                    self._currDetectedObject is None:\n            self._updateModelButton.Disable()\n        else:\n            self._updateModelButton.Enable()\n```", "```py\n    def _showInstructions(self):\n        self._showMessage(\n                'When an object is highlighted, type its name\\n'\n                '(max 4 chars) and click \"Add to Model\".')\n\n    def _clearMessage(self):\n        # Insert an endline for consistent spacing.\n        self._showMessage('\\n')\n\n    def _showMessage(self, message):\n        wx.CallAfter(self._predictionStaticText.SetLabel, message)\n```", "```py\n#!/usr/bin/env python\n\nimport wx\n\nfrom InteractiveRecognizer import InteractiveRecognizer\nimport PyInstallerUtils\n\ndef main():\n    app = wx.App()\n    recognizerPath = PyInstallerUtils.resourcePath(\n            'recognizers/lbph_human_faces.xml')\n    cascadePath = PyInstallerUtils.resourcePath(\n            # Uncomment the next argument for LBP.\n            #'cascades/lbpcascade_frontalface.xml')\n            # Uncomment the next argument for Haar.\n            'cascades/haarcascade_frontalface_alt.xml')\n    interactiveRecognizer = InteractiveRecognizer(\n            recognizerPath, cascadePath,\n            title='Interactive Human Face Recognizer')\n    interactiveRecognizer.Show()\n    app.MainLoop()\n\nif __name__ == '__main__':\n    main()\n```", "```py\n#!/usr/bin/env python\n\nimport wx\n\nfrom InteractiveRecognizer import InteractiveRecognizer\nimport PyInstallerUtils\n\ndef main():\n    app = wx.App()\n    recognizerPath = PyInstallerUtils.resourcePath(\n            'recognizers/lbph_cat_faces.xml')\n    cascadePath = PyInstallerUtils.resourcePath(\n            # Uncomment the next argument for LBP.\n            #'cascades/lbpcascade_frontalcatface.xml')\n            # Uncomment the next argument for Haar with basic\n            # features.\n            #'cascades/haarcascade_frontalcatface.xml')\n            # Uncomment the next argument for Haar with extended\n            # features.\n            'cascades/haarcascade_frontalcatface_extended.xml')\n    interactiveRecognizer = InteractiveRecognizer(\n            recognizerPath, cascadePath,\n            scaleFactor=1.2, minNeighbors=1,\n            minSizeProportional=(0.125, 0.125),\n            title='Interactive Cat Face Recognizer')\n    interactiveRecognizer.Show()\n    app.MainLoop()\n\nif __name__ == '__main__':\n    main()\n```", "```py\n#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport cv2\nimport glob\nimport math\nimport sys\n```", "```py\noutputImageExtension = '.out.jpg'\n```", "```py\ndef equalizedGray(image):\n    return cv2.equalizeHist(cv2.cvtColor(\n            image, cv2.COLOR_BGR2GRAY))\n```", "```py\ndef describeNegativeHelper(imagePath, output):\n    outputImagePath = '%s%s' % (imagePath, outputImageExtension)\n    image = cv2.imread(imagePath)\n    # Save an equalized version of the image.\n    cv2.imwrite(outputImagePath, equalizedGray(image))\n    # Append the equalized image to the negative description.\n    print(outputImagePath, file=output)\n```", "```py\ndef describeNegative():\n    output = open('negative_description.txt', 'w')\n    # Append all images from Caltech Faces 1999, since all are\n    # non-cats.\n    for imagePath in glob.glob('faces/*.jpg'):\n        if imagePath.endswith(outputImageExtension):\n            # This file is equalized, saved on a previous run.\n            # Skip it.\n            continue\n        describeNegativeHelper(imagePath, output)\n```", "```py\n    # Append all images from the Urtho negative training set,\n    # since all are non-cats.\n    for imagePath in glob.glob('urtho_negatives/*.jpg'):\n        if imagePath.endswith(outputImageExtension):\n            # This file is equalized, saved on a previous run.\n            # Skip it.\n            continue\n        describeNegativeHelper(imagePath, output)\n```", "```py\n    # Append non-cat images from VOC2007.\n    input = open('VOC2007/ImageSets/Main/cat_test.txt', 'r')\n    while True:\n        line = input.readline().rstrip()\n        if not line:\n            break\n        imageNumber, flag = line.split()\n        if int(flag) < 0:\n            # There is no cat in this image.\n            imagePath = 'VOC2007/JPEGImages/%s.jpg' % imageNumber\n            describeNegativeHelper(imagePath, output)\n```", "```py\ndef rotateCoords(coords, center, angleRadians):\n    # Positive y is down so reverse the angle, too.\n    angleRadians = -angleRadians\n    xs, ys = coords[::2], coords[1::2]\n    newCoords = []\n    n = min(len(xs), len(ys))\n    i = 0\n    centerX = center[0]\n    centerY = center[1]\n    cosAngle = math.cos(angleRadians)\n    sinAngle = math.sin(angleRadians)\n    while i < n:\n        xOffset = xs[i] - centerX\n        yOffset = ys[i] - centerY\n        newX = xOffset * cosAngle - yOffset * sinAngle + centerX\n        newY = xOffset * sinAngle + yOffset * cosAngle + centerY\n        newCoords += [newX, newY]\n        i += 1\n    return newCoords\n```", "```py\ndef preprocessCatFace(coords, image):\n\n    leftEyeX, leftEyeY = coords[0], coords[1]\n    rightEyeX, rightEyeY = coords[2], coords[3]\n    mouthX = coords[4]\n    if leftEyeX > rightEyeX and leftEyeY < rightEyeY and \\\n            mouthX > rightEyeX:\n        # The \"right eye\" is in the second quadrant of the face,\n        # while the \"left eye\" is in the fourth quadrant (from the\n        # viewer's perspective.) Swap the eyes' labels in order to\n        # simplify the rotation logic.\n        leftEyeX, rightEyeX = rightEyeX, leftEyeX\n        leftEyeY, rightEyeY = rightEyeY, leftEyeY\n\n    eyesCenter = (0.5 * (leftEyeX + rightEyeX),\n                  0.5 * (leftEyeY + rightEyeY))\n\n    eyesDeltaX = rightEyeX - leftEyeX\n    eyesDeltaY = rightEyeY - leftEyeY\n    eyesAngleRadians = math.atan2(eyesDeltaY, eyesDeltaX)\n    eyesAngleDegrees = eyesAngleRadians * 180.0 / math.pi\n\n    # Straighten the image and fill in gray for blank borders.\n    rotation = cv2.getRotationMatrix2D(\n            eyesCenter, eyesAngleDegrees, 1.0)\n    imageSize = image.shape[1::-1]\n    straight = cv2.warpAffine(image, rotation, imageSize,\n                              borderValue=(128, 128, 128))\n```", "```py\n    # Straighten the coordinates of the features.\n    newCoords = rotateCoords(\n            coords, eyesCenter, eyesAngleRadians)\n```", "```py\n    # Make the face as wide as the space between the ear bases.\n    # (The ear base positions are specified in the reference\n    # coordinates.)\n    w = abs(newCoords[16] - newCoords[6])\n    # Make the face square.\n    h = w\n    # Put the center point between the eyes at (0.5, 0.4) in\n    # proportion to the entire face.\n    minX = eyesCenter[0] - w/2\n    if minX < 0:\n        w += minX\n        minX = 0\n    minY = eyesCenter[1] - h*2/5\n    if minY < 0:\n        h += minY\n        minY = 0\n\n    # Crop the face.\n    crop = straight[int(minY):int(minY+h), int(minX):int(minX+w)]\n    # Convert the crop to equalized grayscale.\n    crop = equalizedGray(crop)\n    # Return the crop.\n    return crop\n```", "```py\ndef describePositive():\n    output = open('positive_description.txt', 'w')\n    dirs = ['CAT_DATASET_01/CAT_00',\n            'CAT_DATASET_01/CAT_01',\n            'CAT_DATASET_01/CAT_02',\n            'CAT_DATASET_02/CAT_03',\n            'CAT_DATASET_02/CAT_04',\n            'CAT_DATASET_02/CAT_05',\n            'CAT_DATASET_02/CAT_06']\n    for dir in dirs:\n        for imagePath in glob.glob('%s/*.jpg' % dir):\n            if imagePath.endswith(outputImageExtension):\n                # This file is a crop, saved on a previous run.\n                # Skip it.\n                continue\n            # Open the '.cat' annotation file associated with this\n            # image.\n            input = open('%s.cat' % imagePath, 'r')\n            # Read the coordinates of the cat features from the\n            # file. Discard the first number, which is the number\n            # of features.\n            coords = [int(i) for i in input.readline().split()[1:]]\n            # Read the image.\n            image = cv2.imread(imagePath)\n            # Straighten and crop the cat face.\n            crop = preprocessCatFace(coords, image)\n            if crop is None:\n                sys.stderr.write(\n                        'Failed to preprocess image at %s.\\n' % \\\n                        imagePath)\n                continue\n            # Save the crop.\n            cropPath = '%s%s' % (imagePath, outputImageExtension)\n            cv2.imwrite(cropPath, crop)\n            # Append the cropped face and its bounds to the\n            # positive description.\n            h, w = crop.shape[:2]\n            print('%s 1 0 0 %d %d' % (cropPath, w, h), file=output)\n```", "```py\nCAT_DATASET_02/CAT_06/00001493_005.jpg.out.jpg 1 0 0 64 64\n```", "```py\nCAT_DATASET_02/CAT_06/00001493_005.jpg.out.jpg 2 0 0 8 8 56 56 8 8\n```", "```py\ndef main(): \n    describeNegative()\n    describePositive()\n\nif __name__ == '__main__':\n    main()\n```", "```py\nREM On Windows, opencv_createsamples and opencv_traincascades expect\nREM absolute paths.\nREM Set baseDir to be the absolute path to this script's directory.\nset baseDir=%~dp0\n\nREM Use baseDir to construct other absolute paths.\n\nset vec=%baseDir%\\binary_description\nset info=%baseDir%\\positive_description.txt\nset bg=%baseDir%\\negative_description.txt\n\nREM Uncomment the next 4 variables for LBP training.\nREM set featureType=LBP\nREM set data=%baseDir%\\lbpcascade_frontalcatface\\\\\nREM set dst=%baseDir%\\..\\\\cascades\\\\lbpcascade_frontalcatface.xml\nREM set mode=BASIC\n\nREM Uncomment the next 4 variables for Haar training with basic\nREM features.\nset featureType=HAAR\nset data=%baseDir%\\haarcascade_frontalcatface\\\\\nset dst=%baseDir%\\..\\\\cascades\\\\haarcascade_frontalcatface.xml\nset mode=BASIC\n\nREM Uncomment the next 4 variables for Haar training with\nREM extended features.\nREM set featureType=HAAR\nREM set data=%baseDir%\\haarcascade_frontalcatface_extended\\\\\nREM set dst=%baseDir%\\..\\\\cascades\\\\haarcascade_frontalcatface_extended.xml\nREM set mode=ALL\n\nREM Set numPosTotal to be the line count of info.\nfor /f %%c in ('find /c /v \"\" ^< \"%info%\"') do set numPosTotal=%%c\n\nREM Set numNegTotal to be the line count of bg.\nfor /f %%c in ('find /c /v \"\" ^< \"%bg%\"') do set numNegTotal=%%c\n\nset /a numPosPerStage=%numPosTotal%*9/10\nset /a numNegPerStage=%numNegTotal%*9/10\nset numStages=20\nset minHitRate=0.995\nset maxFalseAlarmRate=0.5\n\nREM Ensure that the data directory exists and is empty.\nif not exist \"%data%\" (mkdir \"%data%\") else del /f /q \"%data%\\*.xml\"\n\nopencv_createsamples -vec \"%vec%\" -info \"%info%\" -bg \"%bg%\" ^\n        -num \"%numPosTotal%\"\nopencv_traincascade -data \"%data%\" -vec \"%vec%\" -bg \"%bg%\" ^\n        -numPos \"%numPosPerStage%\" -numNeg \"%numNegPerStage%\" ^\n        -numStages \"%numStages%\" -minHitRate \"%minHitRate%\" ^\n        -maxFalseAlarmRate \"%maxFalseAlarmRate%\" ^\n        -featureType \"%featureType%\" -mode \"%mode%\"\n\ncopy /Y \"%data%\\cascade.xml\" \"%dst%\"\n```", "```py\n#!/bin/sh\n\nvec=binary_description\ninfo=positive_description.txt\nbg=negative_description.txt\n\n# Uncomment the next 4 variables for LBP training.\n#featureType=LBP\n#data=lbpcascade_frontalcatface/\n#dst=../cascades/lbpcascade_frontalcatface.xml\n#mode=BASIC\n\n# Uncomment the next 4 variables for Haar training with basic\n# features.\nfeatureType=HAAR\ndata=haarcascade_frontalcatface/\ndst=../cascades/haarcascade_frontalcatface.xml\nmode=BASIC\n\n# Uncomment the next 4 variables for Haar training with\n# extended features.\n#featureType=HAAR\n#data=haarcascade_frontalcatface_extended/\n#dst=../cascades/haarcascade_frontalcatface_extended.xml\n#mode=ALL\n\n# Set numPosTotal to be the line count of info.\nnumPosTotal=`wc -l < $info`\n\n# Set numNegTotal to be the line count of bg.\nnumNegTotal=`wc -l < $bg`\n\nnumPosPerStage=$(($numPosTotal*9/10))\nnumNegPerStage=$(($numNegTotal*9/10))\nnumStages=20\nminHitRate=0.995\nmaxFalseAlarmRate=0.5\n\n# Ensure that the data directory exists and is empty.\nif [ ! -d \"$data\" ]; then\n    mkdir \"$data\"\nelse\n    rm \"$data/*.xml\"\nfi\n\nopencv_createsamples -vec \"$vec\" -info \"$info\" -bg \"$bg\" \\\n        -num \"$numPosTotal\"\nopencv_traincascade -data \"$data\" -vec \"$vec\" -bg \"$bg\" \\\n        -numPos \"$numPosPerStage\" -numNeg \"$numNegPerStage\" \\\n        -numStages \"$numStages\" -minHitRate \"$minHitRate\" \\\n        -maxFalseAlarmRate \"$maxFalseAlarmRate\" \\\n        -featureType \"$featureType\" -mode \"$mode\"\n\ncp \"$data/cascade.xml\" \"$dst\"\n```", "```py\ndef intersects(rect0, rect1):\n    x0, y0, w0, h0 = rect0\n    x1, y1, w1, h1 = rect1\n    if x0 > x1 + w1: # rect0 is wholly to right of rect1\n        return False\n    if x1 > x0 + w0: # rect1 is wholly to right of rect0\n        return False\n    if y0 > y1 + h1: # rect0 is wholly below rect1\n        return False\n    if y1 > y0 + h0: # rect1 is wholly below rect0\n        return False\n    return True\n```", "```py\ndef difference(rects0, rects1):\n    result = []\n    for rect0 in rects0:\n        anyIntersects = False\n        for rect1 in rects1:\n            if intersects(rect0, rect1):\n                anyIntersects = True\n                break\n        if not anyIntersects:\n            result += [rect0]\n    return result\n```", "```py\nimport smtplib\n```", "```py\ndef sendEmail(fromAddr, toAddrList, ccAddrList, subject, message,\n              login, password, smtpServer='smtp.gmail.com:587'):\n\n    # Taken from http://rosettacode.org/wiki/Send_an_email#Python\n\n    header = 'From: %s\\n' % fromAddr\n    header += 'To: %s\\n' % ','.join(toAddrList)\n    header += 'Cc: %s\\n' % ','.join(ccAddrList)\n    header += 'Subject: %s\\n\\n' % subject\n    message = header + message\n\n    server = smtplib.SMTP(smtpServer)\n    server.starttls()\n    server.login(login,password)\n    problems = server.sendmail(fromAddr, toAddrList, message)\n    server.quit()\n    return problems\n```", "```py\n#!/usr/bin/env python\n\nimport numpy # Hint to PyInstaller\nimport cv2\nimport getpass\nimport os\nimport socket\nimport sys\n\nimport BinasciiUtils\nimport GeomUtils\nimport MailUtils\nimport PyInstallerUtils\nimport ResizeUtils\n```", "```py\ndef recognizeAndReport(recognizer, grayImage, rects, maxDistance,\n                       noun, smtpServer, login, password, fromAddr,\n                       toAddrList, ccAddrList):\n    for x, y, w, h in rects:\n        crop = cv2.equalizeHist(grayImage[y:y+h, x:x+w])\n        labelAsInt, distance = recognizer.predict(crop)\n        labelAsStr = BinasciiUtils.intToFourChars(labelAsInt)\n```", "```py\n        #print('%s %s %d' % (noun, labelAsStr, distance))\n```", "```py\n        if distance <= maxDistance:\n            subject = 'Angora Blue'\n            message = 'We have sighted the %s known as %s.' % \\\n                    (noun, labelAsStr)\n            try:\n                problems = MailUtils.sendEmail(\n                        fromAddr, toAddrList, ccAddrList, subject,\n                        message, login, password, smtpServer)\n                if problems:\n                    sys.stderr.write(\n                            'Email problems: {0}\\n'.format(problems))\n                else:\n                    return True\n            except socket.gaierror:\n                sys.stderr.write('Unable to reach email server\\n')\n    return False\n```", "```py\ndef main():\n\n    humanCascadePath = PyInstallerUtils.resourcePath(\n            # Uncomment the next argument for LBP.\n            #'cascades/lbpcascade_frontalface.xml')\n            # Uncomment the next argument for Haar.\n            'cascades/haarcascade_frontalface_alt.xml')\n    humanRecognizerPath = PyInstallerUtils.resourcePath(\n            'recognizers/lbph_human_faces.xml')\n    if not os.path.isfile(humanRecognizerPath):\n        sys.stderr.write(\n                'Human face recognizer not trained. Exiting.\\n')\n        return\n\n    catCascadePath = PyInstallerUtils.resourcePath(\n            # Uncomment the next argument for LBP.\n            #'cascades/lbpcascade_frontalcatface.xml')\n            # Uncomment the next argument for Haar with basic\n            # features.\n            #'cascades/haarcascade_frontalcatface.xml')\n            # Uncomment the next argument for Haar with extended\n            # features.\n            'cascades/haarcascade_frontalcatface_extended.xml')\n    catRecognizerPath = PyInstallerUtils.resourcePath(\n            'recognizers/lbph_cat_faces.xml')\n    if not os.path.isfile(catRecognizerPath):\n        sys.stderr.write(\n                'Cat face recognizer not trained. Exiting.\\n')\n        return\n```", "```py\n    print('What email settings shall we use to send alerts?')\n\n    defaultSMTPServer = 'smtp.gmail.com:587'\n    print('Enter SMTP server (default: %s):' % defaultSMTPServer)\n    smtpServer = sys.stdin.readline().rstrip()\n    if not smtpServer:\n        smtpServer = defaultSMTPServer\n\n    print('Enter username:')\n    login = sys.stdin.readline().rstrip()\n\n    print('Enter password:')\n    password = getpass.getpass('')\n\n    defaultAddr = '%s@gmail.com' % login\n    print('Enter \"from\" email address (default: %s):' % defaultAddr)\n    fromAddr = sys.stdin.readline().rstrip()\n    if not fromAddr:\n        fromAddr = defaultAddr\n\n    print('Enter comma-separated \"to\" email addresses (default: '\n          '%s):' % defaultAddr)\n    toAddrList = sys.stdin.readline().rstrip().split(',')\n    if toAddrList == ['']:\n        toAddrList = [defaultAddr]\n\n    print('Enter comma-separated \"c.c.\" email addresses:')\n    ccAddrList = sys.stdin.readline().rstrip().split(',')\n```", "```py\n    capture = cv2.VideoCapture(0)\n    imageWidth, imageHeight = \\\n            ResizeUtils.cvResizeCapture(capture, (1280, 720))\n    minImageSize = min(imageWidth, imageHeight)\n```", "```py\n    humanDetector = cv2.CascadeClassifier(humanCascadePath)\n    humanRecognizer = cv2.face.LBPHFaceRecognizer_create()\n    humanRecognizer.read(humanRecognizerPath)\n    humanMinSize = (int(minImageSize * 0.25),\n                    int(minImageSize * 0.25))\n    humanMaxDistance = 25\n\n    catDetector = cv2.CascadeClassifier(catCascadePath)\n    catRecognizer = cv2.face.LBPHFaceRecognizer_create()\n    catRecognizer.read(catRecognizerPath)\n    catMinSize = (int(minImageSize * 0.125),\n                  int(minImageSize * 0.125))\n    catMaxDistance = 25\n```", "```py\n    while True:\n        success, image = capture.read()\n        if image is not None:\n            grayImage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            equalizedGrayImage = cv2.equalizeHist(grayImage)\n\n            humanRects = humanDetector.detectMultiScale(\n                    equalizedGrayImage, scaleFactor=1.3,\n                    minNeighbors=4, minSize=humanMinSize)\n            if recognizeAndReport(\n                    humanRecognizer, grayImage, humanRects,\n                    humanMaxDistance, 'human', smtpServer, login,\n                    password, fromAddr, toAddrList, ccAddrList):\n                break\n```", "```py\n            catRects = catDetector.detectMultiScale(\n                    equalizedGrayImage, scaleFactor=1.2,\n                    minNeighbors=1, minSize=catMinSize)\n            # Reject any cat faces that overlap with human faces.\n            catRects = GeomUtils.difference(catRects, humanRects)\n            if recognizeAndReport(\n                    catRecognizer, grayImage, catRects,\n                    catMaxDistance, 'cat', smtpServer, login,\n                    password, fromAddr, toAddrList, ccAddrList):\n                break\n\nif __name__ == '__main__':\n    main()\n```"]