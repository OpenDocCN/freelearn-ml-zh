- en: Chapter 6. Boosting Refinements
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：提升改进
- en: In the previous chapter, we learned about the boosting algorithm. We looked
    at the algorithm in its structural form, illustrated with a numerical example,
    and then applied the algorithm to regression and classification problems. In this
    brief chapter, we will cover some theoretical aspects of the boosting algorithm
    and its underpinnings. The boosting theory is also important here.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了提升算法。我们研究了算法的结构形式，用一个数值示例进行了说明，然后将算法应用于回归和分类问题。在本章简要介绍中，我们将涵盖提升算法及其基础的一些理论方面。提升理论在这里也很重要。
- en: 'In this chapter, we will also look at why the boosting algorithm works from
    a few different perspectives. Different classes of problems require different
    types of loss functions in order for the boosting techniques to be effective.
    In the next section, we will explore the different kinds of loss functions that
    we can choose from. The extreme gradient boosting method is outlined in the section
    dedicated to working with the `xgboost` package. Furthermore, the `h2o` package
    will ultimately be discussed in the final section, and this might be useful for
    other ensemble methods too. The chapter will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章，我们还将从几个不同的角度探讨提升算法为什么有效。不同类型的问题需要不同类型的损失函数，以便提升技术能够有效。在下一节中，我们将探讨我们可以选择的不同类型的损失函数。极端梯度提升方法在专门用于处理`xgboost`包的章节中概述。此外，`h2o`包将在最后一节中最终讨论，这可能对其他集成方法也很有用。本章将涵盖以下主题：
- en: Why does boosting work?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升为什么有效？
- en: The `gbm` package
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gbm`包'
- en: The `xgboost` package
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xgboost`包'
- en: The `h2o` package
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o`包'
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the following R libraries in this chapter:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将使用以下R库：
- en: '`adabag`'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adabag`'
- en: '`gbm`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gbm`'
- en: '`h2o`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`h2o`'
- en: '`kernlab`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernlab`'
- en: '`rpart`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpart`'
- en: '`xgboost`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xgboost`'
- en: Why does boosting work?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升为什么有效？
- en: The *Adaptive boosting algorithm* section in the previous chapter contained
    *m* models, classifiers ![Why does boosting work?](img/00282.jpeg), *n* observations
    and weights, and a voting power that is determined sequentially. The adaptation
    of the adaptive boosting method was illustrated using a toy example, and then
    applied using specialized functions. When compared with the bagging and random
    forest methods, we found that boosting provides the highest accuracy, which you
    may remember from the results in the aforementioned section in the previous chapter.
    However, the implementation of the algorithm does not tell us why it was expected
    to perform better.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章中关于**自适应提升算法**的部分包含了*m*个模型、分类器![提升为什么有效？](img/00282.jpeg)、*n*个观察值和权重，以及一个按顺序确定的投票权。通过一个玩具示例说明了自适应提升方法的适应，然后使用专用函数应用了该方法。与袋装法和随机森林方法相比，我们发现提升提供了最高的准确率，这你可能还记得上一章前面提到的那个部分的结果。然而，算法的实现并没有告诉我们为什么它预期会表现得更好。
- en: 'We don''t have a universally accepted answer on why boosting works, but according
    to subsection 6.2.2 of Berk (2016), there are three possible explanations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有一个普遍接受的答案来解释提升为什么有效，但根据Berk（2016）的第6.2.2小节，有三种可能的解释：
- en: Boosting is a margin maximizer
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升是边缘最大化器
- en: Boosting is a statistical optimizer
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升是统计优化器
- en: Boosting is an interpolator
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升是一个插值器
- en: 'But what do these actually mean? We will now cover each of these points one
    by one. The margin for an observation in a boosting algorithm is calculated as
    follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些实际上意味着什么呢？现在我们将逐一介绍这些观点。在提升算法中，一个观察值的边缘计算如下：
- en: '![Why does boosting work?](img/00283.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![提升为什么有效？](img/00283.jpeg)'
- en: We can see that the margin is the difference between the sum of the passes over
    the correct classifications and incorrect classifications. In the preceding formula,
    the quantity ![Why does boosting work?](img/00284.jpeg) denotes the voting power.
    The reason why the boosting algorithm works so well, especially for the classification
    problem, is because it is a *margin maximizer*. In their ground-breaking paper,
    Schapire et al., the inventors of the boosting algorithm, claim that boosting
    is particularly good at finding classifiers with large margins in that it concentrates
    on those examples whose margins are small (or negative) and forces the base learning
    algorithm to generate good classifications for those examples. The bold section
    of this quote is what will be illustrated in the next R code block.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，边界值是正确分类和错误分类总和之间的差异。在先前的公式中，![为什么提升工作？](img/00284.jpeg)表示投票权重。提升算法之所以工作得如此之好，尤其是在分类问题上，是因为它是一个*边界最大化器*。在他们的开创性论文中，提升算法的发明者Schapire等人声称，提升特别擅长找到具有大边界的分类器，因为它专注于那些边界值小（或负）的例子，并迫使基本学习算法为这些例子生成良好的分类。下面引文中的粗体部分将在下一个R代码块中展示。
- en: 'The spam dataset from the `kernlab` package will be used for illustrating this
    key idea. The `boosting` function from the `gbm` package will fit on the data
    to distinguish between the spam emails and the good ones. We will begin an initial
    model with a single iteration only, accessing the accuracy, obtaining the margins,
    and then producing the summary in the following code block:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用来自`kernlab`包的垃圾邮件数据集来说明这一关键思想。`gbm`包中的`boosting`函数将拟合数据，以区分垃圾邮件和正常邮件。我们将从一个仅包含一次迭代的初始模型开始，获取准确率，获取边界值，然后在下面的代码块中生成摘要：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will increase the number of iterations to 5, 10, 20, 50, and 200\.
    The reader should track the accuracy and the summary of the margins as they hover
    over the following results:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将迭代次数增加到5、10、20、50和200。读者应跟踪以下结果的准确率和边界值摘要：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The first important difference is that the margins have moved completely away
    from negative numbers, and each of them is non-negative after the number of iterations
    reaches 50 or more. To get a clearer picture of this, we will column-bind the
    margins and then look at all of the observations that had a negative margin upon
    initialization:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个重要的区别是，边界值完全摆脱了负数，并且在迭代次数达到50或更多之后，每个边界值都是非负的。为了更清楚地了解这一点，我们将边界值进行列绑定，然后查看初始化时具有负边界的所有观察值：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following snapshot shows the result of the preceding code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以下快照显示了前面代码的结果：
- en: '![Why does boosting work?](img/00285.jpeg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![为什么提升工作？](img/00285.jpeg)'
- en: 'Figure 1: Margins of misclassified observations over the iterations'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：迭代过程中误分类观察值的边界
- en: Thus, we can clearly see that the margins increase as the number of iterations
    increases.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以清楚地看到，随着迭代次数的增加，边界值也在增加。
- en: The second point to bear in mind about boosting, especially adaptive boosting,
    is that it is a statistical optimizer. Pages 264–5 of Berk (2016) and 25–6 of
    Zhou (2012) show that boosting ensembles achieves the Bayes error rate. This means
    that since the exponential loss is minimized, the classification error rate is
    also minimized.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 关于提升，特别是自适应提升，需要记住的第二点是它是一个统计优化器。Berk（2016）第264-5页和Zhou（2012）第25-6页显示，提升集成达到了贝叶斯误差率。这意味着由于指数损失被最小化，分类错误率也被最小化。
- en: The third point about boosting being an interpolator is straightforward. It
    is evident that the iterations of boosting can be seen as the weighted averaging
    of a random forest.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 关于提升作为插值器的第三点非常直接。很明显，提升的迭代可以看作是随机森林的加权平均。
- en: Up to this point, the boosting methods have only addressed classification and
    regression problems. The loss function is central for a machine learning algorithm,
    and the next section will discuss a variety of loss functions that will help in
    setting the boosting algorithm for different formats of data.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，提升方法仅解决了分类和回归问题。损失函数对于机器学习算法至关重要，下一节将讨论各种损失函数，这些函数将有助于为不同格式的数据设置提升算法。
- en: The gbm package
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`gbm`包'
- en: The R `gbm` package, created by Greg Ridgeway, is a very versatile package.
    The details of this package can be found at [http://www.saedsayad.com/docs/gbm2.pdf](http://www.saedsayad.com/docs/gbm2.pdf).
    The document details the theoretical aspects of the gradient boosting and illustrates
    various other parameters of the `gbm` function. First, we will consider the shrinkage
    factor available in the `gbm` function.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Greg Ridgeway 创建的 R `gbm` 包是一个非常通用的包。该包的详细信息可以在[http://www.saedsayad.com/docs/gbm2.pdf](http://www.saedsayad.com/docs/gbm2.pdf)找到。该文档详细介绍了梯度提升的理论方面，并说明了
    `gbm` 函数的各种其他参数。首先，我们将考虑 `gbm` 函数中可用的收缩因子。
- en: 'Shrinkage parameters are very important, and also help with the problem of
    overfitting. Penalization is achieved through this option. For the spam dataset,
    we will set the shrinkage option to 0.1 (very large) and 0.0001 (very small) and
    also look at how the performance is affected:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 收缩参数非常重要，也有助于解决过拟合问题。通过此选项实现惩罚。对于垃圾邮件数据集，我们将收缩选项设置为0.1（非常大）和0.0001（非常小），并观察性能如何受到影响：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The summary function also plots the relative variable importance plot. This
    is shown in the following screenshot:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要函数还绘制了相对变量重要性图。如下截图所示：
- en: '![The gbm package](img/00286.jpeg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![gbm 包](img/00286.jpeg)'
- en: 'Figure 2: Relative variable influence plot'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：相对变量影响图
- en: 'The details of the fitted object are obtained next:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来获取拟合对象的详细信息：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The shrinkage parameter is too low, and so almost none of the variables are
    influential. Next, we will generate a plot, as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 收缩参数太低，因此几乎没有变量具有影响力。接下来，我们将生成以下图表：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![The gbm package](img/00287.jpeg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![gbm 包](img/00287.jpeg)'
- en: 'Figure 3: Convergence as a factor of the shrinkage factor'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：收缩因子作为收敛的一个因素
- en: We don't have a clear convergence for the extremely small shrinkage factor.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对于极小的收缩因子没有清晰的收敛。
- en: 'Further details of the `gbm` function can be obtained from the package documentation
    or the source that was provided earlier. Boosting is a very versatile technique,
    and Ridgeway has implemented this for the varieties of data structures. The next
    table lists four of the most commonly occurring data structures, showing the statistical
    model, the deviance (related to the loss function), the initial values, the gradient,
    and the estimate of the terminal node output for each one:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`gbm` 函数的更多详细信息可以从包文档或之前提供的源中获取。提升是一种非常通用的技术，Ridgeway 为各种数据结构实现了这一技术。下表列出了四种最常见的几种数据结构，展示了统计模型、偏差（与损失函数相关）、初始值、梯度和每个终端节点的输出估计：'
- en: '| Output type | Stat model | Deviance | Initial value | Gradient | Terminal
    node estimate |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 输出类型 | 统计模型 | 偏差 | 初始值 | 梯度 | 终端节点估计 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Numeric | Gaussian | ![The gbm package](img/00288.jpeg) | ![The gbm package](img/00289.jpeg)
    | ![The gbm package](img/00290.jpeg) | ![The gbm package](img/00291.jpeg) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 数值 | 高斯 | ![gbm 包](img/00288.jpeg) | ![gbm 包](img/00289.jpeg) | ![gbm 包](img/00290.jpeg)
    | ![gbm 包](img/00291.jpeg) |'
- en: '| Binary | Bernoulli | ![The gbm package](img/00292.jpeg) | ![The gbm package](img/00293.jpeg)
    | ![The gbm package](img/00294.jpeg) | ![The gbm package](img/00295.jpeg) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 二元 | 伯努利 | ![gbm 包](img/00292.jpeg) | ![gbm 包](img/00293.jpeg) | ![gbm 包](img/00294.jpeg)
    | ![gbm 包](img/00295.jpeg) |'
- en: '| Count | Poisson | ![The gbm package](img/00296.jpeg) | ![The gbm package](img/00297.jpeg)
    | ![The gbm package](img/00298.jpeg) | ![The gbm package](img/00299.jpeg) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 计数 | 泊松 | ![gbm 包](img/00296.jpeg) | ![gbm 包](img/00297.jpeg) | ![gbm 包](img/00298.jpeg)
    | ![gbm 包](img/00299.jpeg) |'
- en: '| Survival data | Cox proportional hazards model | ![The gbm package](img/00300.jpeg)
    | ![The gbm package](img/00301.jpeg) | 0 | Newton–Raphson algorithm |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 生存数据 | Cox 比例风险模型 | ![gbm 包](img/00300.jpeg) | ![gbm 包](img/00301.jpeg) |
    0 | 牛顿-拉夫森算法 |'
- en: 'Table 1: GBM boosting options'
  id: totrans-58
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表1：GBM 提升选项
- en: We will apply the `gbm` function for the count data and the survival data.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用 `gbm` 函数对计数数据和生存数据进行处理。
- en: Boosting for count data
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计数数据的提升
- en: The number of accidents, mistakes/typographical errors, births, and so on are
    popular examples of count data. Here, we count the number of incidents over a
    particular time, place, and/or space. Poisson distribution is very popular for
    modeling count data. When we have additional information in the form of covariates
    and independent variables, the related regression problem is often of interest.
    The generalized linear model is a popular technique for modeling the count data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 事故、错误/打字错误、出生等数量数据是流行的例子。在这里，我们计算特定时间、地点和/或空间内事件的数量。泊松分布非常适合用于建模数量数据。当我们有以协变量和独立变量形式提供的额外信息时，相关的回归问题通常很有兴趣。广义线性模型是建模数量数据的一种流行技术。
- en: 'Let''s look at a simulated dataset available at [https://stats.idre.ucla.edu/r/dae/poisson-regression/](https://stats.idre.ucla.edu/r/dae/poisson-regression/).
    The necessary changes are made as indicated in this source. First, we fit the
    Poisson regression model using the `glm` function. Next, we fit the boosting model,
    as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看可从 [https://stats.idre.ucla.edu/r/dae/poisson-regression/](https://stats.idre.ucla.edu/r/dae/poisson-regression/)
    获取的模拟数据集。必要的更改如本源所示进行。首先，我们使用 `glm` 函数拟合泊松回归模型。接下来，我们拟合提升模型，如下所示：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This has been a very simple example with only two covariates. However, in practice,
    we have seen that the count data is treated as the regression problem many times.
    This is unfortunate, and the general regression technique is not any sort of alchemy.
    The data structure must be respected and the count data analysis needs to be carried
    out. Note that the model fitted here is nonlinear. Though the benefit is not apparent
    here, it is natural that as the number of variables increases, the count data
    framework becomes more appropriate. We will close this discussion of count data
    analysis with a plot of the variable importance of two trees, as well as a tabular
    display:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的例子，只有两个协变量。然而，在实践中，我们多次看到数量数据被当作回归问题处理。这是不幸的，一般回归技术并不是任何形式的炼金术。我们必须尊重数据结构，并且需要对数量数据进行分析。请注意，这里拟合的模型是非线性的。尽管这里的好处并不明显，但随着变量数量的增加，数量数据框架变得更加合适。我们将通过两个树的变量重要性图以及表格显示来结束对数量数据分析的讨论：
- en: '![Boosting for count data](img/00302.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![用于数量数据的提升](img/00302.jpeg)'
- en: 'Figure 4: Boosting count data – variable importance'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：提升数量数据 – 变量重要性
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `pretty.gbm.tree` function helps in extracting the hidden trees of the `gbm`
    objects. In the next section, we will deal with a gradient boosting technique
    for survival data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`pretty.gbm.tree` 函数有助于提取 `gbm` 对象的隐藏树。在下一节中，我们将处理生存数据的梯度提升技术。'
- en: Boosting for survival data
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生存数据的提升
- en: 'The `pbc` dataset has already been introduced in [Chapters 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    and [Chapter 2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "Chapter 2. Bootstrapping"), *Bootstrapping*. As seen earlier, the survival data
    has incomplete observations, and we need specialized techniques for this. In Table
    1, we saw that the deviance function is quite complex. Thanks to Ridgeway, we
    don''t have to worry much about such computations. Instead, we simply use the
    `gbm` function with the option of `dist="coxph"` and carry out the analyses as
    follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`pbc` 数据集已在 [第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第1章。集成技术简介") 和 *集成技术简介* 以及 [第2章](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "第2章。自助法") 和 *自助法* 中介绍。如前所述，生存数据有缺失观测值，我们需要专门的技术来处理这种情况。在表1中，我们看到偏差函数相当复杂。多亏了Ridgeway，我们不必过多担心这类计算。相反，我们只需使用带有
    `dist="coxph"` 选项的 `gbm` 函数，并按以下方式进行数据分析：'
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Hence, using the versatile `gbm` function, we can easily carry out the gradient
    boosting technique for a variety of data structures.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用多功能的 `gbm` 函数，我们可以轻松地对各种数据结构执行梯度提升技术。
- en: The xgboost package
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: xgboost 软件包
- en: The `xgboost` R package is an optimized, distributed implementation of the gradient
    boosting method. This is an engineering optimization that is known to be efficient,
    flexible, and portable—see [https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)
    for more details and regular updates. This provides parallel tree boosting, and
    therefore has been found to be immensely useful in the data science community.
    This is especially the case given that a great fraction of the competition winners
    at [www.kaggle.org](http://www.kaggle.org) use the `xgboost` technique. A partial
    list of Kaggle winners is available at [https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgboost` R包是梯度提升方法的优化、分布式实现。这是一个已知效率高、灵活且可移植的工程优化——有关更多详细信息及常规更新，请参阅[https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)。这提供了并行树提升，因此在数据科学社区中被发现非常有用。特别是在[www.kaggle.org](http://www.kaggle.org)的比赛中，许多获胜者使用了`xgboost`技术。Kaggle获胜者的部分列表可在[https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions)找到。'
- en: 'The main advantages of the extreme gradient boosting implementation are shown
    in the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 极端梯度提升实现的优点如下所示：
- en: '**Parallel computing**: This package is enabled with parallel processing using
    OpenMP, which then uses all the cores of the computing machine'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行计算**: 此包通过OpenMP启用并行处理，然后使用计算机的所有核心'
- en: '**Regularization**: This helps in circumventing the problem of overfitting
    by incorporating the regularization ideas'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**: 这通过结合正则化思想来避免过拟合问题'
- en: '**Cross-validation**: No extra coding is required for carrying out cross-validation'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉验证**: 执行交叉验证不需要额外的编码'
- en: '**Pruning**: This grows the tree up to the maximum depth and then prunes backward'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝**: 这将树增长到最大深度，然后反向剪枝'
- en: '**Missing values**: Missing values are internally handled'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失值**: 缺失值在内部被处理'
- en: '**Saving and reloading**: This has features that not only help in saving an
    existing model, but can also continue the iterations from the step where it was
    last stopped'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**保存和重新加载**: 这不仅有助于保存现有模型，还可以从上次停止的地方继续迭代'
- en: '**Cross platform**: This is available for Python, Scala, and so on'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨平台**: 这适用于Python、Scala等'
- en: 'We will illustrate these ideas with the spam dataset that we saw earlier in
    the book. The functions of the `xgboost` package require all the variables to
    be numeric, and the output should also be labelled as `0` and `1`. Furthermore,
    the covariate matrix and the output need to be given to the `xgboost` R package
    separately. As a result, we will first load the spam dataset and then create the
    partitions and the formula, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用书中早些时候看到的垃圾邮件数据集来说明这些想法。`xgboost`包的函数要求所有变量都是数值型，输出也应标记为`0`和`1`。此外，协变量矩阵和输出需要分别提供给`xgboost`
    R包。因此，我们首先加载垃圾邮件数据集，然后创建分区和公式，如下所示：
- en: '[PRE9]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `xgboost` package also requires the training regression data to be specified
    in a special `dgCMatrix` matrix. Thus, we can convert it using the `as` function:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgboost`包还要求指定训练回归数据为特殊的`dgCMatrix`矩阵。因此，我们可以使用`as`函数将其转换：'
- en: '[PRE10]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We now have the infrastructure ready for applying the `xgboost` function. The
    option of `nrounds=100` and the logistic function are chosen, and the results
    are obtained as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已准备好应用`xgboost`函数的基础设施。选择`nrounds=100`和逻辑函数，结果如下所示：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Using the fitted boosting model, we now apply the `predict` function and assess
    the accuracy:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 使用拟合的增强模型，我们现在应用`predict`函数并评估准确性：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If the probability (and the response, in this case) of the observation being
    marked 1 is more than 0.5, then we can label the observation as 1, and 0 otherwise.
    The predicted label and actual label contingency table is obtained by using the
    table R function. Clearly, we have very good accuracy, and there are only three
    misclassifications.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果观察到的概率（在这种情况下是响应）被标记为1的概率超过0.5，那么我们可以将观察到的标签标记为1，否则为0。通过使用table R函数，我们可以获得预测标签和实际标签的列联表。显然，我们具有非常好的准确性，并且只有三个错误分类。
- en: 'We claimed that the `xgboost` package does not require extra coding for the
    cross-validation analysis. The `xgb.cv` function is useful here, and it works
    with the same arguments as the `xgboost` function with the cross-validation folds
    specified by the `nfold` option. Here, we choose `nfold=10`. Now, using the `xgb.cv`
    function, we carry out the analysis and assess the prediction accuracy:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声称`xgboost`包不需要额外的编码来进行交叉验证分析。`xgb.cv`函数在这里很有用，它使用与`xgboost`函数相同的参数，并通过`nfold`选项指定的交叉验证折数来工作。在这里，我们选择`nfold=10`。现在，使用`xgb.cv`函数，我们进行该分析并评估预测准确率：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The cross-validation analysis shows that the accuracy has decreased. This is
    an indication that we had an overfitting problem with the `xgboost` function.
    We will now look at the other features of the `xgboost` package. At the beginning
    of this section, we claimed that the technique allowed flexibility through early
    stoppings and also the resumption of earlier fitted model objects.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证分析显示准确率有所下降。这表明我们在`xgboost`函数中遇到了过拟合问题。现在，我们将查看`xgboost`包的其他特性。在本节的开头，我们声称该技术通过提前停止和恢复早期拟合的模型对象提供了灵活性。
- en: 'However, an important question is *when do you need to stop the iterations
    early?* We don''t have any underlying theory with regards to the number of iterations
    that are required as a function of the number of variables and the number of observations.
    Consequently, we will kick off the proceedings with a specified number of iterations.
    If the convergence of error reduction does not fall below the threshold level,
    then we will continue with more iterations, and this task will be taken up next.
    However, if the specified number of iterations is way too high and the performance
    of the boosting method is getting worse, then we will have to stop the iterations.
    This is achieved by specifying the `early_stopping_rounds` option, which we will
    put into action in the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个重要的问题是*何时需要提前停止迭代？*我们没有关于迭代次数的任何基础理论，这个次数是作为变量数量和观测数量的函数。因此，我们将以指定的迭代次数开始这个过程。如果误差减少的收敛性没有低于阈值水平，那么我们将继续进行更多的迭代，这项任务将在下一部分进行。然而，如果指定的迭代次数过高，并且提升方法的性能正在变差，那么我们不得不停止迭代。这是通过指定`early_stopping_rounds`选项来实现的，我们将在下面的代码中将其付诸实践：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Here, the best iteration has already occurred at number `37`, and the confirmation
    of this is obtained five iterations down the line, thanks to the `early_stopping_rounds
    = 5` option. Now that we've found the best iteration, we stop the process.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，最佳迭代已经发生在编号`37`，这是通过`early_stopping_rounds = 5`选项获得的确认。现在我们已经找到了最佳迭代，我们停止这个过程。
- en: 'We will now look at how to add more iterations. This coding is for illustration
    purposes only. Using the option of `nrounds = 10`, and the earlier fitted `spam_xgb`,
    along with the options of data and label, we will ask the `xgboost` function to
    perform ten more iterations:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看如何添加更多的迭代。以下代码仅用于说明。使用`nrounds = 10`选项，以及早期拟合的`spam_xgb`，以及数据和标签的选项，我们将要求`xgboost`函数进行十次额外的迭代：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The bold and large font of the iteration number is not the format thrown out
    by the R software. This change has been made to emphasize the fact that the number
    of iterations from the earlier fitted `spam_xgb` object will now continue from
    `101` and will go up to `110`. Adding additional iterations is easily achieved
    with the `xgboost` function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代次数的粗体和粗大字体并不是R软件输出的格式。这种变化是为了强调从早期拟合的`spam_xgb`对象开始的迭代次数现在将从`101`继续，并增加到`110`。使用`xgboost`函数很容易增加额外的迭代次数。
- en: 'The `xgb.plot.importance` function, working with the `xgb.importance` function,
    can be used to extract and display the most important variables as identified
    by the boosting method:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`xgb.plot.importance`函数与`xgb.importance`函数一起使用，可以用来提取并显示由提升方法确定的最重要的变量：'
- en: '[PRE16]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The result is the following plot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是以下图表：
- en: '![The xgboost package](img/00303.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![xgboost包](img/00303.jpeg)'
- en: 'Figure 5: Variable importance plot of the xgboost function'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：xgboost函数的变量重要性图
- en: We have now seen the power of the `xgboost` package. Next, we will outline the
    capabilities of the `h2o` package.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到了`xgboost`包的力量。接下来，我们将概述`h2o`包的功能。
- en: The h2o package
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: h2o包
- en: 'The R software `.exe` file size is 75 MB (version 3.4.1). The size of the `h2o`
    R package is 125 MB. This will probably indicate to you the importance of the
    `h2o` package. All the datasets used in this book are very limited in size, with
    the number of observations not exceeding 10,000\. In most cases, the file size
    has been of a maximum of a few MB. However, the data science world works hard,
    and throws around files in GB, and in even higher formats. Thus, we need more
    capabilities, and the `h2o` package provides just that. We simply load the `h2o`
    package and have a peek:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: R软件的`.exe`文件大小为75 MB（版本3.4.1）。`h2o` R包的大小为125 MB。这可能会让你意识到`h2o`包的重要性。本书中使用的所有数据集大小都非常有限，观测数不超过10,000。在大多数情况下，文件大小最大仅为几MB。然而，数据科学界的工作非常努力，经常处理GB级别甚至更高格式的文件。因此，我们需要更多的能力，而`h2o`包正好提供了这些能力。我们只需简单地加载`h2o`包，就可以一窥究竟：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Clusters and threads help in scaling up computations. For the more enthusiastic
    reader, the following sources will help in using the `h2o` package:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 集群和线程有助于扩展计算。对于更热情的读者，以下资源将帮助使用`h2o`包：
- en: '[http://blog.revolutionanalytics.com/2014/04/a-dive-into-h2o.html](http://blog.revolutionanalytics.com/2014/04/a-dive-into-h2o.html)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://blog.revolutionanalytics.com/2014/04/a-dive-into-h2o.html](http://blog.revolutionanalytics.com/2014/04/a-dive-into-h2o.html)'
- en: '[http://docs.h2o.ai/](http://docs.h2o.ai/)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://docs.h2o.ai/](http://docs.h2o.ai/)'
- en: '[https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/](https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/](https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/)'
- en: Using the `gbm`, `xgboost`, and `h2o` packages, the reader can analyze complex
    and large datasets.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`gbm`、`xgboost`和`h2o`包，读者可以分析复杂和大型数据集。
- en: Summary
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We began this chapter by briefly thinking about why boosting works. There are
    three perspectives that possibly explain the success of boosting, and these were
    covered before we looked deeper into this topic. The `gbm` package is very powerful,
    and it offers different options for tuning the gradient boosting algorithm, which
    deals with numerous data structures. We illustrated its capabilities with the
    shrinkage option and applied it to the count and survival data structures. The
    `xgboost` package is an even more efficient implementation of the gradient boosting
    method. It is faster and offers other flexibilities, too. We illustrated using
    the `xgboost` function with cross-validation, early stopping, and continuing further
    iterations as required. The `h2o` package/platform helps to implement the ensemble
    machine learning techniques on a bigger scale.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时简要思考了为什么提升（boosting）有效。有三个视角可能解释了提升的成功，这些内容在我们深入探讨这个主题之前已经讨论过了。`gbm`包非常强大，它提供了调整梯度提升算法的不同选项，该算法可以处理多种数据结构。我们通过收缩选项展示了其能力，并将其应用于计数和生存数据结构。`xgboost`包是梯度提升方法的更高效实现。它更快，还提供了其他灵活性。我们通过使用`xgboost`函数进行交叉验证、早期停止以及根据需要继续迭代来展示其使用方法。`h2o`包/平台有助于在大规模上实现集成机器学习技术。
- en: In the next chapter, we will look into the details of why ensembling works.
    In particular, we will see why putting multiple models together is often a useful
    practice, and we will also explore the scenarios that we can do this in.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨为什么集成（ensembling）有效。特别是，我们将看到为什么将多个模型组合在一起通常是一种有用的实践，我们还将探讨我们可以这样做的情况。
