- en: Chapter 6. Boosting Refinements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned about the boosting algorithm. We looked
    at the algorithm in its structural form, illustrated with a numerical example,
    and then applied the algorithm to regression and classification problems. In this
    brief chapter, we will cover some theoretical aspects of the boosting algorithm
    and its underpinnings. The boosting theory is also important here.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will also look at why the boosting algorithm works from
    a few different perspectives. Different classes of problems require different
    types of loss functions in order for the boosting techniques to be effective.
    In the next section, we will explore the different kinds of loss functions that
    we can choose from. The extreme gradient boosting method is outlined in the section
    dedicated to working with the `xgboost` package. Furthermore, the `h2o` package
    will ultimately be discussed in the final section, and this might be useful for
    other ensemble methods too. The chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why does boosting work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `gbm` package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `xgboost` package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `h2o` package
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the following R libraries in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`adabag`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gbm`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`h2o`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernlab`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rpart`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xgboost`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why does boosting work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Adaptive boosting algorithm* section in the previous chapter contained
    *m* models, classifiers ![Why does boosting work?](img/00282.jpeg), *n* observations
    and weights, and a voting power that is determined sequentially. The adaptation
    of the adaptive boosting method was illustrated using a toy example, and then
    applied using specialized functions. When compared with the bagging and random
    forest methods, we found that boosting provides the highest accuracy, which you
    may remember from the results in the aforementioned section in the previous chapter.
    However, the implementation of the algorithm does not tell us why it was expected
    to perform better.
  prefs: []
  type: TYPE_NORMAL
- en: 'We don''t have a universally accepted answer on why boosting works, but according
    to subsection 6.2.2 of Berk (2016), there are three possible explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is a margin maximizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting is a statistical optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting is an interpolator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But what do these actually mean? We will now cover each of these points one
    by one. The margin for an observation in a boosting algorithm is calculated as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why does boosting work?](img/00283.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: We can see that the margin is the difference between the sum of the passes over
    the correct classifications and incorrect classifications. In the preceding formula,
    the quantity ![Why does boosting work?](img/00284.jpeg) denotes the voting power.
    The reason why the boosting algorithm works so well, especially for the classification
    problem, is because it is a *margin maximizer*. In their ground-breaking paper,
    Schapire et al., the inventors of the boosting algorithm, claim that boosting
    is particularly good at finding classifiers with large margins in that it concentrates
    on those examples whose margins are small (or negative) and forces the base learning
    algorithm to generate good classifications for those examples. The bold section
    of this quote is what will be illustrated in the next R code block.
  prefs: []
  type: TYPE_NORMAL
- en: 'The spam dataset from the `kernlab` package will be used for illustrating this
    key idea. The `boosting` function from the `gbm` package will fit on the data
    to distinguish between the spam emails and the good ones. We will begin an initial
    model with a single iteration only, accessing the accuracy, obtaining the margins,
    and then producing the summary in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will increase the number of iterations to 5, 10, 20, 50, and 200\.
    The reader should track the accuracy and the summary of the margins as they hover
    over the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The first important difference is that the margins have moved completely away
    from negative numbers, and each of them is non-negative after the number of iterations
    reaches 50 or more. To get a clearer picture of this, we will column-bind the
    margins and then look at all of the observations that had a negative margin upon
    initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following snapshot shows the result of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why does boosting work?](img/00285.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Margins of misclassified observations over the iterations'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can clearly see that the margins increase as the number of iterations
    increases.
  prefs: []
  type: TYPE_NORMAL
- en: The second point to bear in mind about boosting, especially adaptive boosting,
    is that it is a statistical optimizer. Pages 264–5 of Berk (2016) and 25–6 of
    Zhou (2012) show that boosting ensembles achieves the Bayes error rate. This means
    that since the exponential loss is minimized, the classification error rate is
    also minimized.
  prefs: []
  type: TYPE_NORMAL
- en: The third point about boosting being an interpolator is straightforward. It
    is evident that the iterations of boosting can be seen as the weighted averaging
    of a random forest.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, the boosting methods have only addressed classification and
    regression problems. The loss function is central for a machine learning algorithm,
    and the next section will discuss a variety of loss functions that will help in
    setting the boosting algorithm for different formats of data.
  prefs: []
  type: TYPE_NORMAL
- en: The gbm package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The R `gbm` package, created by Greg Ridgeway, is a very versatile package.
    The details of this package can be found at [http://www.saedsayad.com/docs/gbm2.pdf](http://www.saedsayad.com/docs/gbm2.pdf).
    The document details the theoretical aspects of the gradient boosting and illustrates
    various other parameters of the `gbm` function. First, we will consider the shrinkage
    factor available in the `gbm` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Shrinkage parameters are very important, and also help with the problem of
    overfitting. Penalization is achieved through this option. For the spam dataset,
    we will set the shrinkage option to 0.1 (very large) and 0.0001 (very small) and
    also look at how the performance is affected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary function also plots the relative variable importance plot. This
    is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The gbm package](img/00286.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Relative variable influence plot'
  prefs: []
  type: TYPE_NORMAL
- en: 'The details of the fitted object are obtained next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The shrinkage parameter is too low, and so almost none of the variables are
    influential. Next, we will generate a plot, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![The gbm package](img/00287.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Convergence as a factor of the shrinkage factor'
  prefs: []
  type: TYPE_NORMAL
- en: We don't have a clear convergence for the extremely small shrinkage factor.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further details of the `gbm` function can be obtained from the package documentation
    or the source that was provided earlier. Boosting is a very versatile technique,
    and Ridgeway has implemented this for the varieties of data structures. The next
    table lists four of the most commonly occurring data structures, showing the statistical
    model, the deviance (related to the loss function), the initial values, the gradient,
    and the estimate of the terminal node output for each one:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Output type | Stat model | Deviance | Initial value | Gradient | Terminal
    node estimate |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Numeric | Gaussian | ![The gbm package](img/00288.jpeg) | ![The gbm package](img/00289.jpeg)
    | ![The gbm package](img/00290.jpeg) | ![The gbm package](img/00291.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: '| Binary | Bernoulli | ![The gbm package](img/00292.jpeg) | ![The gbm package](img/00293.jpeg)
    | ![The gbm package](img/00294.jpeg) | ![The gbm package](img/00295.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: '| Count | Poisson | ![The gbm package](img/00296.jpeg) | ![The gbm package](img/00297.jpeg)
    | ![The gbm package](img/00298.jpeg) | ![The gbm package](img/00299.jpeg) |'
  prefs: []
  type: TYPE_TB
- en: '| Survival data | Cox proportional hazards model | ![The gbm package](img/00300.jpeg)
    | ![The gbm package](img/00301.jpeg) | 0 | Newton–Raphson algorithm |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: GBM boosting options'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: We will apply the `gbm` function for the count data and the survival data.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting for count data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The number of accidents, mistakes/typographical errors, births, and so on are
    popular examples of count data. Here, we count the number of incidents over a
    particular time, place, and/or space. Poisson distribution is very popular for
    modeling count data. When we have additional information in the form of covariates
    and independent variables, the related regression problem is often of interest.
    The generalized linear model is a popular technique for modeling the count data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at a simulated dataset available at [https://stats.idre.ucla.edu/r/dae/poisson-regression/](https://stats.idre.ucla.edu/r/dae/poisson-regression/).
    The necessary changes are made as indicated in this source. First, we fit the
    Poisson regression model using the `glm` function. Next, we fit the boosting model,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This has been a very simple example with only two covariates. However, in practice,
    we have seen that the count data is treated as the regression problem many times.
    This is unfortunate, and the general regression technique is not any sort of alchemy.
    The data structure must be respected and the count data analysis needs to be carried
    out. Note that the model fitted here is nonlinear. Though the benefit is not apparent
    here, it is natural that as the number of variables increases, the count data
    framework becomes more appropriate. We will close this discussion of count data
    analysis with a plot of the variable importance of two trees, as well as a tabular
    display:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Boosting for count data](img/00302.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Boosting count data – variable importance'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `pretty.gbm.tree` function helps in extracting the hidden trees of the `gbm`
    objects. In the next section, we will deal with a gradient boosting technique
    for survival data.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting for survival data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `pbc` dataset has already been introduced in [Chapters 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*,
    and [Chapter 2](part0018_split_000.html#H5A41-2006c10fab20488594398dc4871637ee
    "Chapter 2. Bootstrapping"), *Bootstrapping*. As seen earlier, the survival data
    has incomplete observations, and we need specialized techniques for this. In Table
    1, we saw that the deviance function is quite complex. Thanks to Ridgeway, we
    don''t have to worry much about such computations. Instead, we simply use the
    `gbm` function with the option of `dist="coxph"` and carry out the analyses as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Hence, using the versatile `gbm` function, we can easily carry out the gradient
    boosting technique for a variety of data structures.
  prefs: []
  type: TYPE_NORMAL
- en: The xgboost package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `xgboost` R package is an optimized, distributed implementation of the gradient
    boosting method. This is an engineering optimization that is known to be efficient,
    flexible, and portable—see [https://github.com/dmlc/xgboost](https://github.com/dmlc/xgboost)
    for more details and regular updates. This provides parallel tree boosting, and
    therefore has been found to be immensely useful in the data science community.
    This is especially the case given that a great fraction of the competition winners
    at [www.kaggle.org](http://www.kaggle.org) use the `xgboost` technique. A partial
    list of Kaggle winners is available at [https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions).
  prefs: []
  type: TYPE_NORMAL
- en: 'The main advantages of the extreme gradient boosting implementation are shown
    in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallel computing**: This package is enabled with parallel processing using
    OpenMP, which then uses all the cores of the computing machine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: This helps in circumventing the problem of overfitting
    by incorporating the regularization ideas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross-validation**: No extra coding is required for carrying out cross-validation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pruning**: This grows the tree up to the maximum depth and then prunes backward'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing values**: Missing values are internally handled'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Saving and reloading**: This has features that not only help in saving an
    existing model, but can also continue the iterations from the step where it was
    last stopped'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cross platform**: This is available for Python, Scala, and so on'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will illustrate these ideas with the spam dataset that we saw earlier in
    the book. The functions of the `xgboost` package require all the variables to
    be numeric, and the output should also be labelled as `0` and `1`. Furthermore,
    the covariate matrix and the output need to be given to the `xgboost` R package
    separately. As a result, we will first load the spam dataset and then create the
    partitions and the formula, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The `xgboost` package also requires the training regression data to be specified
    in a special `dgCMatrix` matrix. Thus, we can convert it using the `as` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have the infrastructure ready for applying the `xgboost` function. The
    option of `nrounds=100` and the logistic function are chosen, and the results
    are obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the fitted boosting model, we now apply the `predict` function and assess
    the accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If the probability (and the response, in this case) of the observation being
    marked 1 is more than 0.5, then we can label the observation as 1, and 0 otherwise.
    The predicted label and actual label contingency table is obtained by using the
    table R function. Clearly, we have very good accuracy, and there are only three
    misclassifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'We claimed that the `xgboost` package does not require extra coding for the
    cross-validation analysis. The `xgb.cv` function is useful here, and it works
    with the same arguments as the `xgboost` function with the cross-validation folds
    specified by the `nfold` option. Here, we choose `nfold=10`. Now, using the `xgb.cv`
    function, we carry out the analysis and assess the prediction accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The cross-validation analysis shows that the accuracy has decreased. This is
    an indication that we had an overfitting problem with the `xgboost` function.
    We will now look at the other features of the `xgboost` package. At the beginning
    of this section, we claimed that the technique allowed flexibility through early
    stoppings and also the resumption of earlier fitted model objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, an important question is *when do you need to stop the iterations
    early?* We don''t have any underlying theory with regards to the number of iterations
    that are required as a function of the number of variables and the number of observations.
    Consequently, we will kick off the proceedings with a specified number of iterations.
    If the convergence of error reduction does not fall below the threshold level,
    then we will continue with more iterations, and this task will be taken up next.
    However, if the specified number of iterations is way too high and the performance
    of the boosting method is getting worse, then we will have to stop the iterations.
    This is achieved by specifying the `early_stopping_rounds` option, which we will
    put into action in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Here, the best iteration has already occurred at number `37`, and the confirmation
    of this is obtained five iterations down the line, thanks to the `early_stopping_rounds
    = 5` option. Now that we've found the best iteration, we stop the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now look at how to add more iterations. This coding is for illustration
    purposes only. Using the option of `nrounds = 10`, and the earlier fitted `spam_xgb`,
    along with the options of data and label, we will ask the `xgboost` function to
    perform ten more iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The bold and large font of the iteration number is not the format thrown out
    by the R software. This change has been made to emphasize the fact that the number
    of iterations from the earlier fitted `spam_xgb` object will now continue from
    `101` and will go up to `110`. Adding additional iterations is easily achieved
    with the `xgboost` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `xgb.plot.importance` function, working with the `xgb.importance` function,
    can be used to extract and display the most important variables as identified
    by the boosting method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The xgboost package](img/00303.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Variable importance plot of the xgboost function'
  prefs: []
  type: TYPE_NORMAL
- en: We have now seen the power of the `xgboost` package. Next, we will outline the
    capabilities of the `h2o` package.
  prefs: []
  type: TYPE_NORMAL
- en: The h2o package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The R software `.exe` file size is 75 MB (version 3.4.1). The size of the `h2o`
    R package is 125 MB. This will probably indicate to you the importance of the
    `h2o` package. All the datasets used in this book are very limited in size, with
    the number of observations not exceeding 10,000\. In most cases, the file size
    has been of a maximum of a few MB. However, the data science world works hard,
    and throws around files in GB, and in even higher formats. Thus, we need more
    capabilities, and the `h2o` package provides just that. We simply load the `h2o`
    package and have a peek:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Clusters and threads help in scaling up computations. For the more enthusiastic
    reader, the following sources will help in using the `h2o` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://blog.revolutionanalytics.com/2014/04/a-dive-into-h2o.html](http://blog.revolutionanalytics.com/2014/04/a-dive-into-h2o.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://docs.h2o.ai/](http://docs.h2o.ai/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/](https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `gbm`, `xgboost`, and `h2o` packages, the reader can analyze complex
    and large datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began this chapter by briefly thinking about why boosting works. There are
    three perspectives that possibly explain the success of boosting, and these were
    covered before we looked deeper into this topic. The `gbm` package is very powerful,
    and it offers different options for tuning the gradient boosting algorithm, which
    deals with numerous data structures. We illustrated its capabilities with the
    shrinkage option and applied it to the count and survival data structures. The
    `xgboost` package is an even more efficient implementation of the gradient boosting
    method. It is faster and offers other flexibilities, too. We illustrated using
    the `xgboost` function with cross-validation, early stopping, and continuing further
    iterations as required. The `h2o` package/platform helps to implement the ensemble
    machine learning techniques on a bigger scale.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look into the details of why ensembling works.
    In particular, we will see why putting multiple models together is often a useful
    practice, and we will also explore the scenarios that we can do this in.
  prefs: []
  type: TYPE_NORMAL
