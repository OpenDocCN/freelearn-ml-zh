<html><head></head><body>
<div id="_idContainer101" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-133"><a id="_idTextAnchor141" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.1.1">7</span></h1>
<h1 id="_idParaDest-134" class="calibre5"><a id="_idTextAnchor142" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.2.1">Federated Learning Benchmarks, Start-Ups, and the Next Opportunity</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">This chapter focuses on the importance of </span><strong class="bold"><span class="kobospan" id="kobo.4.1">Federated Learning</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">FL</span></strong><span class="kobospan" id="kobo.7.1">) benchmarks and highlights products offered by start-up companies in </span><span><span class="kobospan" id="kobo.8.1">the field.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.9.1">We will cover the following </span><span><span class="kobospan" id="kobo.10.1">main topics:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><span class="kobospan" id="kobo.11.1">FL benchmarks:</span></span><ul class="calibre16"><li class="calibre11"><span class="kobospan" id="kobo.12.1">An introduction to FL benchmarks, including </span><span><span class="kobospan" id="kobo.13.1">their significance</span></span></li><li class="calibre11"><span class="kobospan" id="kobo.14.1">Considerations when designing </span><span><span class="kobospan" id="kobo.15.1">FL benchmarks</span></span></li><li class="calibre11"><span class="kobospan" id="kobo.16.1">An overview of </span><span><span class="kobospan" id="kobo.17.1">FL datasets</span></span></li><li class="calibre11"><span class="kobospan" id="kobo.18.1">A high-level overview of various FL </span><span><span class="kobospan" id="kobo.19.1">benchmark suites</span></span></li><li class="calibre11"><span class="kobospan" id="kobo.20.1">Selecting the appropriate FL framework for </span><span><span class="kobospan" id="kobo.21.1">a project</span></span></li></ul></li>
<li class="calibre11"><span class="kobospan" id="kobo.22.1">State-of-the-art research </span><span><span class="kobospan" id="kobo.23.1">in FL</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.24.1">Netxt Opportunity and Key start-up company products </span><span><span class="kobospan" id="kobo.25.1">in FL</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.26.1">By exploring these topics, you will gain a comprehensive understanding of the need for FL benchmarks and the latest advancements in the field. </span><span class="kobospan" id="kobo.26.2">Additionally, we will showcase notable products developed by start-up companies that are closely related </span><span><span class="kobospan" id="kobo.27.1">to FL.</span></span></p>
<h1 id="_idParaDest-135" class="calibre5"><a id="_idTextAnchor143" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.28.1">FL benchmarks</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.29.1">FL is a machine learning technique</span><a id="_idIndexMarker622" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.30.1"> that allows multiple devices/clients or servers to collaboratively train a model and keep their data private. </span><span class="kobospan" id="kobo.30.2">There has been an increasing need for standardized benchmarks to evaluate the performance of different FL algorithms </span><span><span class="kobospan" id="kobo.31.1">and frameworks/platforms.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.32.1">The IEEE 3652.1-2020 standard, officially titled </span><em class="italic"><span class="kobospan" id="kobo.33.1">IEEE Guide for Architectural Framework and Application of Federated Machine Learning</span></em><span class="kobospan" id="kobo.34.1">, is a comprehensive guide that provides an architectural</span><a id="_idIndexMarker623" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.35.1"> framework for </span><strong class="bold"><span class="kobospan" id="kobo.36.1">Federated Machine Learning</span></strong><span class="kobospan" id="kobo.37.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.38.1">FML</span></strong><span class="kobospan" id="kobo.39.1">). </span><span class="kobospan" id="kobo.39.2">More details about the IEEE 3652.1-2020 standard can be found </span><span><span class="kobospan" id="kobo.40.1">at </span></span><a href="https://ieeexplore.ieee.org/document/9382202" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.41.1">https://ieeexplore.ieee.org/document/9382202</span></span></a><span><span class="kobospan" id="kobo.42.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.43.1">FL benchmarks are datasets and evaluation metrics that are used to compare and evaluate the performance of different FL algorithms and frameworks/platforms. </span><span class="kobospan" id="kobo.43.2">These benchmarks can help engineers and researchers to identify the strengths and weaknesses of different algorithms and the pros and cons of different </span><span><span class="kobospan" id="kobo.44.1">FL frameworks.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.45.1">Next, we will discuss the</span><a id="_idIndexMarker624" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.46.1"> importance of FL benchmarks and the key considerations that should be taken into account when </span><span><span class="kobospan" id="kobo.47.1">designing them.</span></span></p>
<h2 id="_idParaDest-136" class="calibre7"><a id="_idTextAnchor144" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.48.1">The importance of FL benchmarks</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.49.1">FL benchmarks are essential for </span><span><span class="kobospan" id="kobo.50.1">various reasons.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.51.1">Firstly, they help researchers</span><a id="_idIndexMarker625" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.52.1"> evaluate the performance of different FL algorithms on standard public datasets, which can help to identify which algorithms are most effective for particular use cases. </span><span class="kobospan" id="kobo.52.2">Comparing the performance of different algorithms can help drive progress in the field by identifying areas where new algorithms are needed, or where existing algorithms need to </span><span><span class="kobospan" id="kobo.53.1">be improved.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.54.1">Secondly, these benchmarks also help to compare the performance of different FL frameworks and identify which framework is best suited to solving business use cases </span><span><span class="kobospan" id="kobo.55.1">using FL.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.56.1">Finally, benchmarks can help to improve the reproducibility of research results by providing a standardized set of metrics and </span><span><span class="kobospan" id="kobo.57.1">evaluation procedures.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.58.1">Key considerations when designing FL benchmarks: When designing FL benchmarks, several key considerations should be taken into</span><a id="_idIndexMarker626" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.59.1"> account. </span><span class="kobospan" id="kobo.59.2">Some of them are </span><span><span class="kobospan" id="kobo.60.1">as follows:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.61.1">Data distribution</span></strong><span class="kobospan" id="kobo.62.1">: FL algorithms are designed to work with data that is distributed across multiple devices or servers. </span><span class="kobospan" id="kobo.62.2">Therefore, benchmarks should include datasets that are representative of the types of data that might be encountered in real-world </span><span><span class="kobospan" id="kobo.63.1">use cases.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.64.1">Heterogeneity</span></strong><span class="kobospan" id="kobo.65.1">: FL involves training models on data that is distributed across different devices or servers, each with potentially different hardware capabilities and network conditions. </span><span class="kobospan" id="kobo.65.2">Therefore, benchmarks should include datasets that are diverse and reflect different types of devices and </span><span><span class="kobospan" id="kobo.66.1">network conditions.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.67.1">Privacy</span></strong><span class="kobospan" id="kobo.68.1">: One of the main </span><a id="_idIndexMarker627" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.69.1">benefits of FL is its ability to protect the privacy of data by keeping it on users’ devices. </span><span class="kobospan" id="kobo.69.2">Therefore, benchmarks should ensure that the</span><a id="_idIndexMarker628" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.70.1"> data used for training is representative of real-world use cases while still protecting </span><span><span class="kobospan" id="kobo.71.1">users’ privacy.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.72.1">Evaluation metrics</span></strong><span class="kobospan" id="kobo.73.1">: Benchmarks should include standard metrics to evaluate the performance of FL algorithms. </span><span class="kobospan" id="kobo.73.2">These metrics should be carefully selected to ensure that they accurately reflect the performance of the algorithm and apply to the specific </span><span><span class="kobospan" id="kobo.74.1">use case.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.75.1">Reproducibility</span></strong><span class="kobospan" id="kobo.76.1">: FL benchmarks should be designed to be reproducible, meaning that the datasets and evaluation procedures should be clearly documented and made available </span><span><span class="kobospan" id="kobo.77.1">to others.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.78.1">We have discussed the key considerations in designing the FL benchmarks. </span><span class="kobospan" id="kobo.78.2">Let’s explore further the datasets that need to be considered in </span><span><span class="kobospan" id="kobo.79.1">FL benchmarks.</span></span></p>
<h2 id="_idParaDest-137" class="calibre7"><a id="_idTextAnchor145" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.80.1">FL datasets</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.81.1">One of the key aspects of FL is the selection of datasets that will be used to train the model. </span><span class="kobospan" id="kobo.81.2">Next, we will explore the various </span><a id="_idIndexMarker629" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.82.1">considerations that need to be taken into account </span><a id="_idIndexMarker630" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.83.1">when selecting datasets </span><span><span class="kobospan" id="kobo.84.1">for FL:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.85.1">Data privacy</span></strong><span class="kobospan" id="kobo.86.1">: The most critical consideration </span><a id="_idIndexMarker631" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.87.1">when selecting datasets for FL is data privacy. </span><span class="kobospan" id="kobo.87.2">Since the data is stored locally on each device, it is essential to ensure that the data is not exposed to any unauthorized parties. </span><span class="kobospan" id="kobo.87.3">Data privacy can be ensured by using encryption techniques or by anonymizing the data. </span><span class="kobospan" id="kobo.87.4">Additionally, it is important to have proper consent and permission from the device owners to use their data for FL. </span><span class="kobospan" id="kobo.87.5">Support for differential privacy also needs to be considered because data anonymization alone may not </span><span><span class="kobospan" id="kobo.88.1">be sufficient.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.89.1">Data diversity</span></strong><span class="kobospan" id="kobo.90.1">: Another important consideration when selecting datasets for FL is data diversity. </span><span class="kobospan" id="kobo.90.2">The datasets should be diverse enough to capture a wide range of data patterns and features. </span><span class="kobospan" id="kobo.90.3">This is essential to ensure that the model is not biased toward a particular set of data. </span><span class="kobospan" id="kobo.90.4">For example, if the datasets only include data from a </span><a id="_idIndexMarker632" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.91.1">particular region or demographic, the model may not be able to generalize well to other regions </span><span><span class="kobospan" id="kobo.92.1">or demographics.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.93.1">Data quality</span></strong><span class="kobospan" id="kobo.94.1">: Data quality is also a critical consideration when selecting data sets for FL. </span><span class="kobospan" id="kobo.94.2">The datasets should be clean and free from errors or inconsistencies. </span><span class="kobospan" id="kobo.94.3">Data cleaning and preprocessing techniques may be required to ensure that the data is suitable for training the model. </span><span class="kobospan" id="kobo.94.4">Additionally, the data should be representative of the real-world scenario to ensure that the model can perform well in </span><span><span class="kobospan" id="kobo.95.1">practical situations.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.96.1">Data size</span></strong><span class="kobospan" id="kobo.97.1">: The size of the datasets is also an important consideration for FL. </span><span class="kobospan" id="kobo.97.2">The datasets should be large enough to train the model effectively but not too large that they become unwieldy to work with. </span><span class="kobospan" id="kobo.97.3">A good balance needs to be struck between the size of the datasets and the computational resources available to train </span><span><span class="kobospan" id="kobo.98.1">the model.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.99.1">Data distribution</span></strong><span class="kobospan" id="kobo.100.1">: Finally, the distribution of data across devices is also an important consideration for FL. </span><span class="kobospan" id="kobo.100.2">The data should be distributed in such a way that each device has access to a representative sample of the data. </span><span class="kobospan" id="kobo.100.3">This ensures that each device contributes to the training of the model in a meaningful way. </span><span class="kobospan" id="kobo.100.4">Additionally, the data distribution should be balanced to avoid any devices being overloaded </span><span><span class="kobospan" id="kobo.101.1">with data.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.102.1">Datasets are critical in benchmarking FL systems. </span><span class="kobospan" id="kobo.102.2">Next, we will explore the following popular standard datasets that are </span><a id="_idIndexMarker633" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.103.1">used to benchmark </span><span><span class="kobospan" id="kobo.104.1">FL systems:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><span class="kobospan" id="kobo.105.1">FLAIR</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.106.1">Federated EMNIST</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.107.1">Shakespeare</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.108.1">CIFAR-10</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.109.1">OpenStreetMap</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.110.1">Medical image </span><span><span class="kobospan" id="kobo.111.1">analysis datasets</span></span></li>
</ul>
<h3 class="calibre9"><span class="kobospan" id="kobo.112.1">FLAIR</span></h3>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.113.1">Federated Learning Annotated Image Repository</span></strong><span class="kobospan" id="kobo.114.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.115.1">FLAIR</span></strong><span class="kobospan" id="kobo.116.1">) is an open source project that aims to provide a centralized </span><a id="_idIndexMarker634" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.117.1">repository of annotated images for FL. </span><span class="kobospan" id="kobo.117.2">The repository contains a diverse</span><a id="_idIndexMarker635" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.118.1"> set of images, including medical images, satellite images, and natural images, with corresponding annotations. </span><span class="kobospan" id="kobo.118.2">FLAIR provides a platform for researchers and developers to evaluate FL algorithms on image-based tasks, such as image classification and object detection. </span><span class="kobospan" id="kobo.118.3">The repository allows you to share and collaborate on annotated images, reducing the need for individual organizations to collect and label their own data. </span><span class="kobospan" id="kobo.118.4">The annotations in FLAIR are designed to be privacy-preserving, with sensitive information removed or obfuscated. </span><span class="kobospan" id="kobo.118.5">The privacy-preserving annotations and standardized format make FLAIR a valuable resource for evaluating FL algorithms on image-based tasks, enabling easy integration with popular machine learning frameworks </span><span><span class="kobospan" id="kobo.119.1">and tools.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.120.1">FLAIR has become a popular resource in the FL community, with several FL frameworks integrating it into their evaluation pipeline. </span><span class="kobospan" id="kobo.120.2">The repository is continually updated, with new datasets and annotations added regularly. </span><span class="kobospan" id="kobo.120.3">It consists of 430,000 images from 51,000 Flickr users, which are mapped to 17 coarse-grained labels such as art, food, plant, </span><span><span class="kobospan" id="kobo.121.1">and outdoor.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.122.1">Visit the following GitHub</span><a id="_idIndexMarker636" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.123.1"> URL for more </span><span><span class="kobospan" id="kobo.124.1">information: </span></span><a href="https://github.com/apple/ml-flair" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.125.1">https://github.com/apple/ml-flair</span></span></a><span><span class="kobospan" id="kobo.126.1">.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.127.1">Federated EMNIST</span></h3>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.128.1">Federated Extended MNIST</span></strong><span class="kobospan" id="kobo.129.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.130.1">Federated EMNIST</span></strong><span class="kobospan" id="kobo.131.1">) is a benchmark dataset for FL developed by Google. </span><span class="kobospan" id="kobo.131.2">It consists of </span><a id="_idIndexMarker637" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.132.1">a set of images of handwritten digits and letters, similar to the MNIST dataset. </span><span class="kobospan" id="kobo.132.2">The dataset is distributed</span><a id="_idIndexMarker638" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.133.1"> across multiple devices, and the goal is to train a model that can classify images correctly while keeping </span><span><span class="kobospan" id="kobo.134.1">data private.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.135.1">Shakespeare</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.136.1">This dataset consists of a collection of </span><a id="_idIndexMarker639" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.137.1">Shakespearean texts, which can be used to</span><a id="_idIndexMarker640" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.138.1"> train a language model for </span><span><span class="kobospan" id="kobo.139.1">text generation.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.140.1">CIFAR-10</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.141.1">This is a widely used </span><a id="_idIndexMarker641" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.142.1">benchmark dataset for</span><a id="_idIndexMarker642" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.143.1"> computer vision tasks, consisting of a set of 60,000 color images of 10 different classes. </span><span class="kobospan" id="kobo.143.2">It is used to train a model to classify images correctly while keeping </span><span><span class="kobospan" id="kobo.144.1">data private.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.145.1">OpenStreetMap</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.146.1">This is a dataset of geographic data, including maps, satellite imagery, and GPS coordinates. </span><span class="kobospan" id="kobo.146.2">The dataset can be distributed</span><a id="_idIndexMarker643" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.147.1"> across multiple devices, and the goal is to</span><a id="_idIndexMarker644" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.148.1"> train a model that can predict traffic patterns or other features of the environment while preserving the privacy </span><span><span class="kobospan" id="kobo.149.1">of data.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.150.1">Medical image analysis datasets</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.151.1">There are several medical imaging datasets that can </span><a id="_idIndexMarker645" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.152.1">be used for FL, such as the </span><strong class="bold"><span class="kobospan" id="kobo.153.1">Brain Tumor Segmentation</span></strong><span class="kobospan" id="kobo.154.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.155.1">BraTS</span></strong><span class="kobospan" id="kobo.156.1">) challenge dataset and the </span><strong class="bold"><span class="kobospan" id="kobo.157.1">Prostate MR Image Segmentation</span></strong><span class="kobospan" id="kobo.158.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.159.1">PROMISE12</span></strong><span class="kobospan" id="kobo.160.1">) challenge dataset. </span><span class="kobospan" id="kobo.160.2">These </span><a id="_idIndexMarker646" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.161.1">datasets consist of medical images that can be used to</span><a id="_idIndexMarker647" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.162.1"> train models for diagnosis and treatment planning while preserving the privacy of </span><span><span class="kobospan" id="kobo.163.1">patient data.</span></span></p>
<h2 id="_idParaDest-138" class="calibre7"><a id="_idTextAnchor146" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.164.1">Frameworks for FL benchmarks</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.165.1">There are several FL benchmark </span><a id="_idIndexMarker648" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.166.1">frameworks available in the open source community, as well as commercially. </span><span class="kobospan" id="kobo.166.2">FL frameworks provide libraries/platforms to </span><a id="_idIndexMarker649" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.167.1">benchmark FL systems </span><span><span class="kobospan" id="kobo.168.1">and applications.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.169.1">Some of the most popular FL systems and benchmarks are </span><span><span class="kobospan" id="kobo.170.1">the following:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><span class="kobospan" id="kobo.171.1">LEAF</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.172.1">FedML</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.173.1">FATE</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.174.1">FedScale</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.175.1">PySyft</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.176.1">MLCommons – </span><span><span class="kobospan" id="kobo.177.1">MLPerf</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.178.1">MedPerf</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.179.1">TensorFlow </span><span><span class="kobospan" id="kobo.180.1">Federated (TFF)</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.181.1">Flower (we covered this in the previous chapter so won’t go into detail again in </span><span><span class="kobospan" id="kobo.182.1">this chapter)</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.183.1">We’ll look at these in more detail in the </span><span><span class="kobospan" id="kobo.184.1">following subsections.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.185.1">The LEAF FL benchmarks suite</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.186.1">The LEAF FL benchmarks suite is a project</span><a id="_idIndexMarker650" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.187.1"> initiated by Carnegie Mellon University, which aims to provide a standardized benchmarking suite for FL </span><span><span class="kobospan" id="kobo.188.1">algorithms (</span></span><a href="https://leaf.cmu.edu/" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.189.1">https://leaf.cmu.edu/</span></span></a><span><span class="kobospan" id="kobo.190.1">).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.191.1">The LEAF benchmarks enable researchers and developers to evaluate the performance of FL algorithms across different domains and configurations. </span><span class="kobospan" id="kobo.191.2">LEAF provides a standardized set of tasks and datasets to evaluate the performance of FL algorithms on various metrics, including convergence speed, communication efficiency, and accuracy. </span><span class="kobospan" id="kobo.191.3">The broad range of tasks includes image classification, text classification, and </span><span><span class="kobospan" id="kobo.192.1">regression tasks.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.193.1">The datasets used in the benchmarks are designed to be representative of real-world scenarios, with varying levels of data distribution, data size, and data complexity. </span><span class="kobospan" id="kobo.193.2">The benchmarks are conducted under various settings, such as varying the number of clients, the communication </span><a id="_idIndexMarker651" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.194.1">rounds, and the amount of data available to each client. </span><span class="kobospan" id="kobo.194.2">The benchmarks also evaluate the algorithms’ robustness to adversarial attacks and </span><span><span class="kobospan" id="kobo.195.1">noisy data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.196.1">LEAF’s core components are </span><span><span class="kobospan" id="kobo.197.1">as follows:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span><span class="kobospan" id="kobo.198.1">Datasets</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.199.1">Reference implementation</span></span></li>
<li class="calibre11"><span><span class="kobospan" id="kobo.200.1">Metrics</span></span></li>
</ul>
<div class="calibre2">
<div class="img---figure" id="_idContainer096">
<span class="kobospan" id="kobo.201.1"><img alt="Figure 7.1 – LEAF’s core components and flow" src="image/B16573_07_01.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.202.1">Figure 7.1 – LEAF’s core components and flow</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.203.1">More details about the benchmarks of LEAF can be obtained </span><span><span class="kobospan" id="kobo.204.1">at </span></span><a href="https://arxiv.org/abs/1812.01097" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.205.1">https://arxiv.org/abs/1812.01097</span></span></a><span><span class="kobospan" id="kobo.206.1">.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.207.1">FedML</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.208.1">FedML (</span><a href="https://github.com/FedML-AI/FedML" class="pcalibre1 calibre6 pcalibre"><span class="kobospan" id="kobo.209.1">https://github.com/FedML-AI/FedML</span></a><span class="kobospan" id="kobo.210.1">) is a benchmark suite for FL that includes several datasets and tasks, such</span><a id="_idIndexMarker652" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.211.1"> as image classification, language modeling, and speech recognition. </span><span class="kobospan" id="kobo.211.2">FedML evaluates FL algorithms on their robustness in </span><strong class="bold"><span class="kobospan" id="kobo.212.1">non-independent and identically distributed</span></strong><span class="kobospan" id="kobo.213.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.214.1">non-iid</span></strong><span class="kobospan" id="kobo.215.1">) data distributions, scalability to</span><a id="_idIndexMarker653" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.216.1"> large datasets, and </span><span><span class="kobospan" id="kobo.217.1">communication efficiency.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.218.1">The framework supports various types of FL setups, such as horizontal, vertical, and federated transfer learning, as well as different types of optimization algorithms, including federated averaging, FedProx, and FedAdapt. </span><span class="kobospan" id="kobo.218.2">The FedML framework is designed to be modular, flexible, and scalable, allowing users to customize and extend the existing algorithms or develop new ones easily. </span><span class="kobospan" id="kobo.218.3">It also provides a set of benchmark datasets and evaluation metrics to facilitate the comparison of </span><span><span class="kobospan" id="kobo.219.1">different algorithms.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.220.1">The FedML community</span><a id="_idIndexMarker654" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.221.1"> actively develops and maintains the framework and organizes workshops and challenges to promote research and innovation in FL. </span><span class="kobospan" id="kobo.221.2">The community also collaborates with industry partners to apply FL to real-world use cases, such as healthcare, finance, and telecommunications. </span><span class="kobospan" id="kobo.221.3">FedML is an important initiative that addresses the challenges and opportunities of FL and enables the development of secure, privacy-preserving, and efficient machine learning models in a distributed setting. </span><span class="kobospan" id="kobo.221.4">FedML supports three types of computing platforms – IoT/mobile, distributed computing, and </span><span><span class="kobospan" id="kobo.222.1">standalone simulation.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.223.1">More details can be found at the following reference research </span><span><span class="kobospan" id="kobo.224.1">URL: </span></span><a href="https://arxiv.org/abs/2007.13518" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.225.1">https://arxiv.org/abs/2007.13518</span></span></a><span><span class="kobospan" id="kobo.226.1">.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer097">
<span class="kobospan" id="kobo.227.1"><img alt="Figure 7.2 – FedML architecture" src="image/B16573_07_02.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.228.1">Figure 7.2 – FedML architecture</span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.229.1">FATE</span></h3>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.230.1">Federated AI Technology Enabler</span></strong><span class="kobospan" id="kobo.231.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.232.1">FATE</span></strong><span class="kobospan" id="kobo.233.1">) is an open source project that provides a secure and privacy-preserving </span><a id="_idIndexMarker655" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.234.1">way to collaborate on </span><strong class="bold"><span class="kobospan" id="kobo.235.1">artificial intelligence</span></strong><span class="kobospan" id="kobo.236.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.237.1">AI</span></strong><span class="kobospan" id="kobo.238.1">) models. </span><span class="kobospan" id="kobo.238.2">FATE is designed to enable FL, which is a distributed machine learning approach that allows multiple parties to train a </span><a id="_idIndexMarker656" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.239.1">shared model without sharing their data. </span><span class="kobospan" id="kobo.239.2">FATE is developed by the WeBank AI department, a subsidiary of </span><span><span class="kobospan" id="kobo.240.1">Tencent. </span></span><a href="https://github.com/FederatedAI/FATE" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.241.1">https://github.com/FederatedAI/FATE</span></span></a></p>
<p class="calibre3"><span class="kobospan" id="kobo.242.1">FATE provides a platform for developers to implement FL techniques in their AI applications. </span><span class="kobospan" id="kobo.242.2">It supports different types of FL, such as horizontal and vertical, and provides tools to manage the communication, synchronization, and aggregation of models across different devices. </span><span class="kobospan" id="kobo.242.3">FATE uses a variety of cryptographic techniques to ensure the privacy and security of the FL process. </span><span class="kobospan" id="kobo.242.4">For example, FATE employs differential privacy to add noise to training data, preventing the leakage of sensitive information about individual users. </span><span class="kobospan" id="kobo.242.5">FATE also uses homomorphic encryption to enable computation on </span><a id="_idIndexMarker657" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.243.1">encrypted data, which allows parties to collaborate without revealing their data to each other. </span><span class="kobospan" id="kobo.243.2">We will learn about homomorphic encryption in the </span><span><span class="kobospan" id="kobo.244.1">next chapter.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.245.1">FATE is built on top of Kubernetes, a popular open source container orchestration system. </span><span class="kobospan" id="kobo.245.2">Kubernetes provides FATE with the ability to manage containers and automate the deployment, scaling, and monitoring of the FL infrastructure. </span><span class="kobospan" id="kobo.245.3">FATE leverages cryptographic techniques and Kubernetes to ensure the privacy and security of the FL process. </span><span class="kobospan" id="kobo.245.4">FATE provides a </span><a id="_idIndexMarker658" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.246.1">web-based </span><strong class="bold"><span class="kobospan" id="kobo.247.1">graphical user interface</span></strong><span class="kobospan" id="kobo.248.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.249.1">GUI</span></strong><span class="kobospan" id="kobo.250.1">) for developers to interact with the platform, monitor the training process, and visualize the results. </span><span class="kobospan" id="kobo.250.2">It has been adopted by various companies and organizations to build privacy-preserving AI applications. </span><span class="kobospan" id="kobo.250.3">For example, Tencent has used FATE to develop a privacy-preserving recommendation system for its e-commerce platform. </span><span class="kobospan" id="kobo.250.4">Huawei has also used FATE to build an FL platform for the healthcare industry. </span><span class="kobospan" id="kobo.250.5">FATE’s popularity is expected to grow as FL becomes increasingly important in the </span><span><span class="kobospan" id="kobo.251.1">AI industry.</span></span></p>
<h4 class="calibre17"><span class="kobospan" id="kobo.252.1">The architecture of FATE</span></h4>
<p class="calibre3"><span class="kobospan" id="kobo.253.1">The following figure shows</span><a id="_idIndexMarker659" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.254.1"> the </span><span><span class="kobospan" id="kobo.255.1">FATE architecture:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer098">
<span class="kobospan" id="kobo.256.1"><img alt="Figure 7.3 – FATE architecture" src="image/B16573_07_03.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.257.1">Figure 7.3 – FATE architecture</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.258.1">The preceding figure was</span><a id="_idIndexMarker660" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.259.1"> sourced </span><span><span class="kobospan" id="kobo.260.1">from </span></span><a href="https://fate.readthedocs.io/en/latest/architecture/" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.261.1">https://fate.readthedocs.io/en/latest/architecture/</span></span></a><span><span class="kobospan" id="kobo.262.1">.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.263.1">The FedScale benchmarking platform</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.264.1">The FedScale benchmark suite includes several datasets and workloads that are designed to evaluate the performance of FL algorithms and systems. </span><span class="kobospan" id="kobo.264.2">The datasets are selected to reflect the heterogeneity </span><a id="_idIndexMarker661" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.265.1">and non-IIDness of real-world data. </span><span class="kobospan" id="kobo.265.2">The workloads are designed to simulate the communication overhead and computation complexity of real-world FL scenarios, as well as evaluate the performance of FL algorithms and systems in </span><span><span class="kobospan" id="kobo.266.1">these scenarios.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.267.1">One of the datasets included in the FedScale benchmark suite is the Synthetic IID dataset, which consists of synthetic data that is randomly generated and distributed evenly among the clients. </span><span class="kobospan" id="kobo.267.2">This dataset is designed to evaluate the scalability and efficiency of FL algorithms and systems. </span><span class="kobospan" id="kobo.267.3">Another dataset included in the FedScale benchmark suite is the Heterogeneous Image dataset, which consists of images from different sources and domains. </span><span class="kobospan" id="kobo.267.4">This dataset is designed to evaluate the robustness of FL algorithms and systems to </span><span><span class="kobospan" id="kobo.268.1">heterogeneous data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.269.1">The FedScale benchmark</span><a id="_idIndexMarker662" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.270.1"> suite also includes more complex workloads, such as the Federated Meta-Learning workload, which involves clients learning from each </span><span><span class="kobospan" id="kobo.271.1">other’s data.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer099">
<span class="kobospan" id="kobo.272.1"><img alt="Figure 7.4 – The FedScale runtime to run FL benchmarks" src="image/B16573_07_04.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.273.1">Figure 7.4 – The FedScale runtime to run FL benchmarks</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.274.1">The preceding figure was sourced </span><span><span class="kobospan" id="kobo.275.1">from </span></span><a href="https://github.com/SymbioticLab/FedScale/blob/master/docs/fedscale-sim-mode.png" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.276.1">https://github.com/SymbioticLab/FedScale/blob/master/docs/fedscale-sim-mode.png</span></span></a><span><span class="kobospan" id="kobo.277.1">.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.278.1">MLCommons</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.279.1">MLCommons is a non-profit organization that </span><a id="_idIndexMarker663" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.280.1">aims to accelerate </span><a id="_idIndexMarker664" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.281.1">machine learning innovation and development by providing a platform for collaboration and the sharing of resources among its members. </span><span class="kobospan" id="kobo.281.2">It was founded in June 2020 by a group of leading researchers, engineers, and entrepreneurs in the field of </span><span><span class="kobospan" id="kobo.282.1">machine learning.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.283.1">The organization’s primary focus is on</span><a id="_idIndexMarker665" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.284.1"> developing and promoting best practices for designing and deploying machine learning systems, including hardware and software frameworks, datasets, benchmarks, and evaluation metrics. </span><span class="kobospan" id="kobo.284.2">One of the key initiatives of MLCommons is the MLPerf benchmark suite, which is designed to measure the performance of machine learning systems across a range of applications, including computer vision, natural language processing, and recommendation systems. </span><span class="kobospan" id="kobo.284.3">The benchmarks are developed and maintained by a global community of researchers and engineers, and they are used</span><a id="_idIndexMarker666" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.285.1"> to evaluate the performance of hardware and software platforms for machine learning. </span><span class="kobospan" id="kobo.285.2">MLPerf has quickly become the industry standard for benchmarking machine learning performance and is used by leading technology companies, researchers, and government agencies around </span><span><span class="kobospan" id="kobo.286.1">the world.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.287.1">MedPerf</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.288.1">MedPerf is an open benchmarking platform</span><a id="_idIndexMarker667" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.289.1"> for medical AI using FL. </span><span class="kobospan" id="kobo.289.2">The MLCommons team</span><a id="_idIndexMarker668" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.290.1"> piloted multiple use cases in collaboration with multiple institutions and universities to run FL models using MDPerf benchmarks. </span><span class="kobospan" id="kobo.290.2">The use cases are brain tumor segmentation, pancreas segmentation, surgical workflow phase recognition, and </span><span><span class="kobospan" id="kobo.291.1">cloud experiments.</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer100">
<span class="kobospan" id="kobo.292.1"><img alt="" role="presentation" src="image/B16573_07_05.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.293.1">Figure 7.5 – MedPerf architecture for cloud experiments</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.294.1">The preceding figure was sourced </span><span><span class="kobospan" id="kobo.295.1">from </span></span><a href="https://github.com/mlcommons/medperf" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.296.1">https://github.com/mlcommons/medperf</span></span></a><span><span class="kobospan" id="kobo.297.1">.</span></span></p>
<h2 id="_idParaDest-139" class="calibre7"><a id="_idTextAnchor147" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.298.1">Selecting an FL framework for a project</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.299.1">Selecting the best FL framework can be a challenging task, since there are several factors to consider, such as the features, scalability, security, ease of use, documentation, the machine learning algorithms to support (out of the box), neural network support, and the </span><span><span class="kobospan" id="kobo.300.1">learning curve.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.301.1">The following are some of the key </span><a id="_idIndexMarker669" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.302.1">considerations when selecting an </span><span><span class="kobospan" id="kobo.303.1">FL framework:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.304.1">Features</span></strong><span class="kobospan" id="kobo.305.1">: It is important to consider the features that the framework offers. </span><span class="kobospan" id="kobo.305.2">Some of the critical features to look for include support for different machine learning algorithms, distributed training, data privacy, and data validation. </span><span class="kobospan" id="kobo.305.3">You should choose a framework that has features that match your use </span><span><span class="kobospan" id="kobo.306.1">case needs.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.307.1">Scalability</span></strong><span class="kobospan" id="kobo.308.1">: Another critical consideration is scalability. </span><span class="kobospan" id="kobo.308.2">The framework should be able to handle large datasets and a high number of participants. </span><span class="kobospan" id="kobo.308.3">You should also consider whether the framework can scale horizontally or vertically. </span><span class="kobospan" id="kobo.308.4">In the case of cross-device (mobile, web, etc.), the framework should be able to handle a large number </span><span><span class="kobospan" id="kobo.309.1">of clients.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.310.1">Security and privacy</span></strong><span class="kobospan" id="kobo.311.1">: FL involves multiple parties sharing data, so it’s essential to select a framework that offers robust security features. </span><span class="kobospan" id="kobo.311.2">The framework should have encryption and authentication capabilities to protect the data from unauthorized access. </span><span class="kobospan" id="kobo.311.3">You should also consider whether the framework has a transparent security model, allowing you to audit the security measures in place. </span><span class="kobospan" id="kobo.311.4">You need to consider whether or not the framework supports privacy protection by default, such as </span><span><span class="kobospan" id="kobo.312.1">differential privacy.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.313.1">Ease of use</span></strong><span class="kobospan" id="kobo.314.1">: The ease of use of the framework is also a critical consideration. </span><span class="kobospan" id="kobo.314.2">The framework should have an intuitive user interface that makes it easy to set up, configure, and use. </span><span class="kobospan" id="kobo.314.3">You should also consider whether the framework has good documentation and community support, as this can help you troubleshoot any issues </span><span><span class="kobospan" id="kobo.315.1">that arise.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.316.1">Learning curve</span></strong><span class="kobospan" id="kobo.317.1">: You should consider the learning curve of the framework as well in terms of user documentation, deployment setup, programming language support, the examples and </span><a id="_idIndexMarker670" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.318.1">samples provided, and </span><span><span class="kobospan" id="kobo.319.1">so on.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.320.1">Compatibility</span></strong><span class="kobospan" id="kobo.321.1">: Finally, you should consider whether the framework is compatible with your existing technology stack and programming language. </span><span class="kobospan" id="kobo.321.2">For example, if you’re using TensorFlow for machine learning, you may want to select an FL framework that is compatible </span><span><span class="kobospan" id="kobo.322.1">with TensorFlow.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.323.1">In conclusion, selecting the best framework requires careful consideration of several factors, including features, scalability, security, privacy, ease of use, the learning curve, and compatibility with your existing technology stack. </span><span class="kobospan" id="kobo.323.2">By considering these factors, you can select a framework that meets your requirements to solve the business use case </span><span><span class="kobospan" id="kobo.324.1">using FL.</span></span></p>
<h2 id="_idParaDest-140" class="calibre7"><a id="_idTextAnchor148" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.325.1">A comparison of FedScale, FATE, Flower, and TensorFlow Federated</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.326.1">FedScale, FATE, Flower, FedML, and </span><strong class="bold"><span class="kobospan" id="kobo.327.1">TensorFlow Federated</span></strong><span class="kobospan" id="kobo.328.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.329.1">TFF</span></strong><span class="kobospan" id="kobo.330.1">) are some of the popular FL frameworks that offer unique features and capabilities. </span><span class="kobospan" id="kobo.330.2">In the following subsections, we will compare these features </span><span><span class="kobospan" id="kobo.331.1">and capabilities.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.332.1">Features</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.333.1">FedScale is a framework that </span><a id="_idIndexMarker671" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.334.1">supports FL for a wide range of machine learning </span><a id="_idIndexMarker672" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.335.1">models, including deep neural </span><a id="_idIndexMarker673" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.336.1">networks, decision trees, and </span><strong class="bold"><span class="kobospan" id="kobo.337.1">Support Vector Machines</span></strong><span class="kobospan" id="kobo.338.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.339.1">SVMs</span></strong><span class="kobospan" id="kobo.340.1">). </span><span class="kobospan" id="kobo.340.2">It also supports various optimization algorithms such as federated </span><strong class="bold"><span class="kobospan" id="kobo.341.1">stochastic gradient descent</span></strong><span class="kobospan" id="kobo.342.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.343.1">SGD</span></strong><span class="kobospan" id="kobo.344.1">) and federated </span><a id="_idIndexMarker674" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.345.1">proximal algorithms. </span><span class="kobospan" id="kobo.345.2">FATE, conversely, offers a comprehensive suite of FL tools and supports various machine learning</span><a id="_idIndexMarker675" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.346.1"> algorithms, including logistic regression, decision</span><a id="_idIndexMarker676" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.347.1"> trees, and deep learning models. </span><span class="kobospan" id="kobo.347.2">Flower is a lightweight framework that supports FL for various machine learning models, including neural networks, decision trees, and logistic regression. </span><span class="kobospan" id="kobo.347.3">TFF is a widely used FL framework that supports TensorFlow-based machine </span><span><span class="kobospan" id="kobo.348.1">learning models.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.349.1">Scalability</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.350.1">FedScale and FATE are </span><a id="_idIndexMarker677" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.351.1">designed for large-scale deployments and can handle a high number </span><a id="_idIndexMarker678" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.352.1">of participants. </span><span class="kobospan" id="kobo.352.2">Flower is a lightweight framework that is ideal for</span><a id="_idIndexMarker679" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.353.1"> small to large-scale FL projects. </span><span class="kobospan" id="kobo.353.2">TFF is a scalable</span><a id="_idIndexMarker680" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.354.1"> framework that can handle large-scale FL projects and can be used to deploy FL algorithms in </span><span><span class="kobospan" id="kobo.355.1">production environments.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.356.1">Security</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.357.1">All four frameworks offer robust security</span><a id="_idIndexMarker681" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.358.1"> features such as encryption and authentication </span><a id="_idIndexMarker682" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.359.1">to protect data during the FL process. </span><span class="kobospan" id="kobo.359.2">FATE has additional</span><a id="_idIndexMarker683" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.360.1"> features such as differential privacy and homomorphic encryption to enhance data privacy. </span><span class="kobospan" id="kobo.360.2">TFF also </span><a id="_idIndexMarker684" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.361.1">offers secure </span><strong class="bold"><span class="kobospan" id="kobo.362.1">multi-party computation</span></strong><span class="kobospan" id="kobo.363.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.364.1">MPC</span></strong><span class="kobospan" id="kobo.365.1">) protocols that enable multiple parties to</span><a id="_idIndexMarker685" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.366.1"> securely collaborate on </span><span><span class="kobospan" id="kobo.367.1">FL projects.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.368.1">Ease of use</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.369.1">FedScale and FATE have a </span><a id="_idIndexMarker686" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.370.1">steeper learning curve than Flower and TFF, as they offer more complex</span><a id="_idIndexMarker687" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.371.1"> features and tools. </span><span class="kobospan" id="kobo.371.2">Flower</span><a id="_idIndexMarker688" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.372.1"> and TFF are </span><a id="_idIndexMarker689" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.373.1">both easy to use, with intuitive APIs and documentation that make it easy to get started. </span><span class="kobospan" id="kobo.373.2">TFF also offers a high-level API that enables users to build FL algorithms without requiring deep knowledge of the </span><span><span class="kobospan" id="kobo.374.1">underlying details.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.375.1">Community support</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.376.1">All four </span><a id="_idIndexMarker690" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.377.1">frameworks have active </span><a id="_idIndexMarker691" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.378.1">open source contributors that provide </span><a id="_idIndexMarker692" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.379.1">support and documentation. </span><span class="kobospan" id="kobo.379.2">TFF, being</span><a id="_idIndexMarker693" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.380.1"> developed by Google, has the largest community and support, followed by Flower, FedScale, </span><span><span class="kobospan" id="kobo.381.1">and FATE.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.382.1">Each of these frameworks has its unique features and capabilities, making them suitable for different use cases. </span><span class="kobospan" id="kobo.382.2">FedScale and FATE are ideal for large-scale FL deployments, while Flower and TFF are more suited for small to medium-scale projects. </span><span class="kobospan" id="kobo.382.3">TFF has the largest community and support, making it an excellent choice for developers looking for an FL framework with extensive support and resources. </span><span class="kobospan" id="kobo.382.4">Ultimately, the</span><a id="_idIndexMarker694" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.383.1"> choice of framework will depend</span><a id="_idIndexMarker695" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.384.1"> on the specific use </span><a id="_idIndexMarker696" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.385.1">case, requirements, and familiarity with </span><span><span class="kobospan" id="kobo.386.1">the framework.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.387.1">Research papers</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.388.1">Each framework</span><a id="_idIndexMarker697" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.389.1"> published its own benchmark and results in the research journals, with a list of supported features </span><a id="_idIndexMarker698" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.390.1">and accuracy metrics. </span><span class="kobospan" id="kobo.390.2">Currently, there are </span><a id="_idIndexMarker699" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.391.1">no industry standard benchmarks similar to </span><a href="http://TPC.Org" class="pcalibre1 calibre6 pcalibre"><span class="kobospan" id="kobo.392.1">TPC.Org</span></a><span class="kobospan" id="kobo.393.1"> or </span><a href="http://SPEC.org" class="pcalibre1 calibre6 pcalibre"><span class="kobospan" id="kobo.394.1">SPEC.org</span></a><span class="kobospan" id="kobo.395.1"> that are </span><span><span class="kobospan" id="kobo.396.1">widely </span></span><span><a id="_idIndexMarker700" class="pcalibre1 calibre6 pcalibre"/></span><span><span class="kobospan" id="kobo.397.1">accepted.</span></span></p>
<h1 id="_idParaDest-141" class="calibre5"><a id="_idTextAnchor149" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.398.1">State-of-the-art research in FL</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.399.1">FL is a rapidly growing technology, and there are several state-of-the-art research directions in this space. </span><span class="kobospan" id="kobo.399.2">The following are</span><a id="_idIndexMarker701" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.400.1"> some of the recent advances in</span><a id="_idIndexMarker702" class="pcalibre1 calibre6 pcalibre"/> <span><span class="kobospan" id="kobo.401.1">FL research.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.402.1">Here is a high-level comparison</span><a id="_idIndexMarker703" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.403.1"> of the different </span><span><span class="kobospan" id="kobo.404.1">FL frameworks:</span></span></p>
<table class="no-table-style" id="table001-7">
<colgroup class="calibre12">
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
<col class="calibre13"/>
</colgroup>
<tbody class="calibre14">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.405.1">High-level capabilities</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.406.1">FATE</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.407.1">FedML</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.408.1">Flower</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.409.1">FedScale</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.410.1">TFF</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.411.1">Regression models</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.412.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.413.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.414.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.415.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.416.1">Y</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.417.1">Neural networks</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.418.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.419.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.420.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.421.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.422.1">Y</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.423.1">Tree-based models</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.424.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.425.1">N</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.426.1">N</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.427.1">N</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.428.1">N</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.429.1">Communication protocol</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.430.1">Customized</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.431.1">MPI</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.432.1">gRPC</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.433.1">gRPC</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.434.1">gRPC</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.435.1">Support for </span><span><span class="kobospan" id="kobo.436.1">differential privacy</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.437.1">N</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.438.1">N</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.439.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.440.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.441.1">Y</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.442.1">Single </span><span><span class="kobospan" id="kobo.443.1">host deployment</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.444.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.445.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.446.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.447.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.448.1">Y</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.449.1">Cross-device deployment</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.450.1">N</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.451.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.452.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.453.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.454.1">N</span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.455.1">Research papers</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.456.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.457.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.458.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.459.1">Y</span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.460.1">Y</span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.461.1">Table 7.1 – Comparison of the different FL frameworks</span></p>
<h2 id="_idParaDest-142" class="calibre7"><a id="_idTextAnchor150" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.462.1">Communication-efficient FL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.463.1">One of the main challenges in FL is the high communication cost of transmitting the model updates between the devices</span><a id="_idIndexMarker704" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.464.1"> and the central server. </span><span class="kobospan" id="kobo.464.2">Recent research has focused on reducing the communication cost in FL by developing new compression techniques, such as quantization, sparsification, and differential compression. </span><span class="kobospan" id="kobo.464.3">These techniques can significantly reduce the communication cost without sacrificing the model’s accuracy. </span><span class="kobospan" id="kobo.464.4">Several researchers and research groups are actively </span><a id="_idIndexMarker705" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.465.1">working on </span><strong class="bold"><span class="kobospan" id="kobo.466.1">communication-efficient FL</span></strong><span class="kobospan" id="kobo.467.1"> (</span><span><strong class="bold"><span class="kobospan" id="kobo.468.1">CE-FL</span></strong></span><span><span class="kobospan" id="kobo.469.1">) research:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.470.1">Google Research</span></strong><span class="kobospan" id="kobo.471.1">: Google Research </span><a id="_idIndexMarker706" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.472.1">has been actively working on CE-FL research and has developed several techniques to reduce communication costs in FL. </span><span class="kobospan" id="kobo.472.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.473.1">Communication-Efficient Learning of Deep Networks from Decentralized Data</span></em><span class="kobospan" id="kobo.474.1">, which proposes a new</span><a id="_idIndexMarker707" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.475.1"> compression technique called </span><strong class="bold"><span class="kobospan" id="kobo.476.1">Quantized SGD</span></strong><span class="kobospan" id="kobo.477.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.478.1">QSGD</span></strong><span class="kobospan" id="kobo.479.1">) that can significantly reduce the communication cost in FL. </span><span class="kobospan" id="kobo.479.2">This is a family of compression schemes that allow the compression of gradient updates at each node, while guaranteeing convergence under standard assumptions. </span><span class="kobospan" id="kobo.479.3">See the Arxiv paper </span><span><span class="kobospan" id="kobo.480.1">at </span></span><a href="https://arxiv.org/abs/1602.05629" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.481.1">https://arxiv.org/abs/1602.05629</span></span></a><span><span class="kobospan" id="kobo.482.1">.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.483.1">The Federated Learning Community</span></strong><span class="kobospan" id="kobo.484.1">: The Federated Learning Community is an open community of</span><a id="_idIndexMarker708" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.485.1"> researchers and practitioners who are working on FL research. </span><span class="kobospan" id="kobo.485.2">The community has several working groups, including a CE-FL working group, which is focused on developing new techniques to reduce communication costs </span><span><span class="kobospan" id="kobo.486.1">in FL.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.487.1">Carnegie Mellon University</span></strong><span class="kobospan" id="kobo.488.1">: The Machine Learning department at Carnegie Mellon University has several researchers who are working on CE-FL research. </span><span class="kobospan" id="kobo.488.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.489.1">Communication-Efficient Distributed Learning with Feature-Selective Sampling</span></em><span class="kobospan" id="kobo.490.1">, which proposes a new sampling technique that can reduce the communication </span><a id="_idIndexMarker709" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.491.1">cost </span><span><span class="kobospan" id="kobo.492.1">in FL.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.493.1">IBM Research</span></strong><span class="kobospan" id="kobo.494.1">: IBM researchers and the University of Michigan are working on CE-FL research, and their most recent work is </span><em class="italic"><span class="kobospan" id="kobo.495.1">Federated Learning with Matched Averaging</span></em><span class="kobospan" id="kobo.496.1">, which proposes a new aggregation technique that can reduce the communication cost in FL. </span><span class="kobospan" id="kobo.496.2">See the Arxiv paper </span><span><span class="kobospan" id="kobo.497.1">at </span></span><a href="https://arxiv.org/abs/2002.06440" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.498.1">https://arxiv.org/abs/2002.06440</span></span></a><span><span class="kobospan" id="kobo.499.1">.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.500.1">These are just a few examples of the many researchers and research groups who are actively working on CE-FL research. </span><span class="kobospan" id="kobo.500.2">As the field of FL continues to evolve, it is expected that more researchers and research groups will focus on developing new techniques to reduce the communication cost </span><span><span class="kobospan" id="kobo.501.1">in FL.</span></span></p>
<h2 id="_idParaDest-143" class="calibre7"><a id="_idTextAnchor151" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.502.1">Privacy-preserving FL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.503.1">Privacy is a critical concern in FL, since data is </span><a id="_idIndexMarker710" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.504.1">distributed across </span><a id="_idIndexMarker711" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.505.1">multiple devices, and each device’s owner wants to protect their data privacy. </span><span class="kobospan" id="kobo.505.2">Recent research has focused on developing new privacy-preserving techniques, such as differential privacy and secure multi-party computation, to protect the data privacy of the devices’ owners while enabling collaborative model training. </span><span class="kobospan" id="kobo.505.3">These techniques can provide strong privacy guarantees while preserving the model’s </span><a id="_idIndexMarker712" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.506.1">accuracy. </span><span class="kobospan" id="kobo.506.2">Several researchers and research groups are actively </span><a id="_idIndexMarker713" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.507.1">working on </span><strong class="bold"><span class="kobospan" id="kobo.508.1">privacy-preserving FL</span></strong><span class="kobospan" id="kobo.509.1"> (</span><span><strong class="bold"><span class="kobospan" id="kobo.510.1">PP-FL</span></strong></span><span><span class="kobospan" id="kobo.511.1">) research.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.512.1">Here are some </span><span><span class="kobospan" id="kobo.513.1">notable examples:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.514.1">OpenMined</span></strong><span class="kobospan" id="kobo.515.1">: OpenMined is an open source community dedicated to advancing PP-FL research. </span><span class="kobospan" id="kobo.515.2">It has developed several PP-FL frameworks, including PySyft, which is a Python library </span><span><span class="kobospan" id="kobo.516.1">for PP-FL.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.517.1">Google Research</span></strong><span class="kobospan" id="kobo.518.1">: Google</span><a id="_idIndexMarker714" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.519.1"> Research has been actively working on PP-FL research and has developed several techniques to protect the privacy of user data in FL. </span><span class="kobospan" id="kobo.519.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.520.1">Privacy-Preserving Federated Learning with Byzantine Robust Aggregation</span></em><span class="kobospan" id="kobo.521.1">, which proposes a new aggregation technique that can protect user privacy in the presence of </span><span><span class="kobospan" id="kobo.522.1">malicious actors.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.523.1">Carnegie Mellon University</span></strong><span class="kobospan" id="kobo.524.1">: The CyLab Security and Privacy Institute at Carnegie Mellon University has several researchers who are working on PP-FL research. </span><span class="kobospan" id="kobo.524.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.525.1">Privacy-Preserving Federated Learning via Randomized Smoothing</span></em><span class="kobospan" id="kobo.526.1">, which proposes a new technique that can protect user privacy by adding noise to the </span><span><span class="kobospan" id="kobo.527.1">user data.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.528.1">The University of Oxford</span></strong><span class="kobospan" id="kobo.529.1">: The University of Oxford has several researchers who are working on PP-FL research. </span><span class="kobospan" id="kobo.529.2">One of their recent works is </span><em class="italic"><span class="kobospan" id="kobo.530.1">Secure Federated Learning on Curves</span></em><span class="kobospan" id="kobo.531.1">, which proposes a new technique that can protect user privacy by using elliptic curves to encrypt </span><span><span class="kobospan" id="kobo.532.1">user data.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.533.1">Microsoft Research</span></strong><span class="kobospan" id="kobo.534.1">: Microsoft Research has been actively working on PP-FL research and has developed several techniques to protect user privacy in FL. </span><span class="kobospan" id="kobo.534.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.535.1">Private Federated Learning with Secure Aggregation</span></em><span class="kobospan" id="kobo.536.1">, which proposes a new aggregation technique that can protect user privacy by using </span><span><span class="kobospan" id="kobo.537.1">homomorphic encryption.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.538.1">These are some examples</span><a id="_idIndexMarker715" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.539.1"> of the many researchers and research groups who are actively working on PP-FL research. </span><span class="kobospan" id="kobo.539.2">As the field of FL continues to evolve, it is expected that more researchers and research</span><a id="_idIndexMarker716" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.540.1"> groups will focus on developing new techniques to protect the privacy of user data </span><span><span class="kobospan" id="kobo.541.1">in FL.</span></span></p>
<h2 id="_idParaDest-144" class="calibre7"><a id="_idTextAnchor152" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.542.1">Federated Meta-Learning</span></h2>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.543.1">Federated Meta-Learning</span></strong><span class="kobospan" id="kobo.544.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.545.1">FML</span></strong><span class="kobospan" id="kobo.546.1">) is a new research direction that combines FL and meta-learning to learn from multiple data </span><a id="_idIndexMarker717" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.547.1">sources and tasks while preserving privacy. </span><span class="kobospan" id="kobo.547.2">FML enables collaborative model training across multiple devices and organizations while preserving the data privacy of the devices’ owners. </span><span class="kobospan" id="kobo.547.3">Recent</span><a id="_idIndexMarker718" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.548.1"> research has explored new FML techniques and applications, such as personalized medicine and personalized recommendations. </span><span class="kobospan" id="kobo.548.2">FML is a relatively new research direction in FL, and only a few research groups are working actively </span><span><span class="kobospan" id="kobo.549.1">on it.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.550.1">Here are </span><span><span class="kobospan" id="kobo.551.1">some examples:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.552.1">Google Research</span></strong><span class="kobospan" id="kobo.553.1">: Google Research has been actively working on FML research and has developed several techniques to learn from multiple clients’ meta-knowledge while preserving their privacy. </span><span class="kobospan" id="kobo.553.2">One of its research works is </span><em class="italic"><span class="kobospan" id="kobo.554.1">Federated Meta-Learning for Recommendation</span></em><span class="kobospan" id="kobo.555.1">, which proposes a new framework for FML for </span><span><span class="kobospan" id="kobo.556.1">recommendation tasks.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.557.1">Carnegie Mellon University</span></strong><span class="kobospan" id="kobo.558.1">: The Machine Learning department at Carnegie Mellon University has several researchers who are working on FML research. </span><span class="kobospan" id="kobo.558.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.559.1">Federated Meta-Learning for Fast Model Adaptation in Healthcare</span></em><span class="kobospan" id="kobo.560.1">, which proposes a new approach for learning from multiple healthcare institutions while preserving </span><span><span class="kobospan" id="kobo.561.1">their privacy.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.562.1">The University of Oxford</span></strong><span class="kobospan" id="kobo.563.1">: The University of Oxford has several researchers who are working on FML research. </span><span class="kobospan" id="kobo.563.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.564.1">Federated Meta-Learning for Recommendation with Private and Communication-Efficient Model Aggregation</span></em><span class="kobospan" id="kobo.565.1">, which proposes a new approach for FML for recommendation tasks that preserves privacy and reduces </span><span><span class="kobospan" id="kobo.566.1">communication costs.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.567.1">The University of California, Berkeley</span></strong><span class="kobospan" id="kobo.568.1">: The RISELab at the University of California, Berkeley, has </span><a id="_idIndexMarker719" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.569.1">several researchers who are working on FML research. </span><span class="kobospan" id="kobo.569.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.570.1">Federated Meta-Learning for Few-Shot Learning Across Heterogeneous Devices</span></em><span class="kobospan" id="kobo.571.1">, which proposes a new approach for FML that can learn from data across </span><span><span class="kobospan" id="kobo.572.1">heterogeneous devices.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.573.1">These are a few examples </span><a id="_idIndexMarker720" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.574.1">of the research groups that are actively working on FML research. </span><span class="kobospan" id="kobo.574.2">As the field of FML continues to grow, it is expected that more researchers and research groups will focus on developing new techniques for FML in </span><span><span class="kobospan" id="kobo.575.1">various applications.</span></span></p>
<h2 id="_idParaDest-145" class="calibre7"><a id="_idTextAnchor153" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.576.1">Adaptive FL</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.577.1">In FL, devices’ data distribution can</span><a id="_idIndexMarker721" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.578.1"> change over time, leading to a concept </span><a id="_idIndexMarker722" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.579.1">drift problem that can degrade a model’s performance. </span><span class="kobospan" id="kobo.579.2">Recent research has focused on developing new adaptive learning techniques that can efficiently update the model while preserving privacy. </span><span class="kobospan" id="kobo.579.3">These techniques can adapt the model to the changing data distribution and improve the model’s performance </span><a id="_idIndexMarker723" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.580.1">over time. </span><strong class="bold"><span class="kobospan" id="kobo.581.1">Adaptive federated learning</span></strong><span class="kobospan" id="kobo.582.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.583.1">AFL</span></strong><span class="kobospan" id="kobo.584.1">) is an emerging research direction in FL that focuses on dynamically adapting the FL system to </span><span><span class="kobospan" id="kobo.585.1">changing conditions.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.586.1">Here are some notable examples of researchers and research groups who are working actively </span><span><span class="kobospan" id="kobo.587.1">on AFL:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.588.1">Carnegie Mellon University</span></strong><span class="kobospan" id="kobo.589.1">: The Machine Learning department at Carnegie Mellon University has several researchers who are working on AFL research. </span><span class="kobospan" id="kobo.589.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.590.1">Adaptive Federated Optimization</span></em><span class="kobospan" id="kobo.591.1">, which proposes a new framework for AFL that adapts the learning rate and aggregation strategy to </span><span><span class="kobospan" id="kobo.592.1">network conditions.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.593.1">Google Research</span></strong><span class="kobospan" id="kobo.594.1">: Google Research has been actively working on AFL research and has developed </span><a id="_idIndexMarker724" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.595.1">several techniques to adapt the FL system to changing conditions. </span><span class="kobospan" id="kobo.595.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.596.1">Adaptive Federated Optimization with Local Descent</span></em><span class="kobospan" id="kobo.597.1">, which proposes a new approach for AFL that adapts the learning rate and aggregation strategy to the client’s local </span><span><span class="kobospan" id="kobo.598.1">data distribution.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.599.1">The University of Southern California</span></strong><span class="kobospan" id="kobo.600.1">: The Department of Computer Science at the University of Southern California has several researchers who are working on AFL research. </span><span class="kobospan" id="kobo.600.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.601.1">Decentralized Adaptive Federated Learning with Gradient Compression</span></em><span class="kobospan" id="kobo.602.1">, which proposes a new approach for AFL that </span><a id="_idIndexMarker725" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.603.1">adapts the learning rate and aggregation strategy to network conditions while compressing the gradients to reduce </span><span><span class="kobospan" id="kobo.604.1">communication costs.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.605.1">Tsinghua University</span></strong><span class="kobospan" id="kobo.606.1">: The Department of Computer Science and Technology at Tsinghua University has several researchers who are working on AFL research. </span><span class="kobospan" id="kobo.606.2">One of its recent works is </span><em class="italic"><span class="kobospan" id="kobo.607.1">Adaptive Federated Learning via Second-Order Information Exchange</span></em><span class="kobospan" id="kobo.608.1">, which proposes a new approach for AFL that adapts the learning rate and aggregation strategy based on the second-order information exchange </span><span><span class="kobospan" id="kobo.609.1">among clients.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.610.1">These are a few examples of the research groups that are actively working on AFL research. </span><span class="kobospan" id="kobo.610.2">As the field of AFL continues to grow, it is expected that more researchers and research groups will focus on developing new techniques to adapt the FL system to </span><span><span class="kobospan" id="kobo.611.1">changing conditions.</span></span></p>
<h2 id="_idParaDest-146" class="calibre7"><a id="_idTextAnchor154" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.612.1">Federated reinforcement learning</span></h2>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.613.1">Reinforcement learning</span></strong><span class="kobospan" id="kobo.614.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.615.1">RL</span></strong><span class="kobospan" id="kobo.616.1">) is a machine</span><a id="_idIndexMarker726" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.617.1"> learning paradigm that enables agents to learn through trial and error. </span><strong class="bold"><span class="kobospan" id="kobo.618.1">Federated reinforcement learning</span></strong><span class="kobospan" id="kobo.619.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.620.1">FRL</span></strong><span class="kobospan" id="kobo.621.1">) is a new research direction that combines FL and RL to enable </span><a id="_idIndexMarker727" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.622.1">privacy-preserving collaborative learning for decision-making tasks. </span><span class="kobospan" id="kobo.622.2">Recent research has explored new FRL algorithms and applications, such as autonomous driving </span><span><span class="kobospan" id="kobo.623.1">and robotics.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.624.1">There are many researchers and organizations actively working on FRL to improve machine learning techniques that protect user privacy and </span><span><span class="kobospan" id="kobo.625.1">data security.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.626.1">Here are a </span><span><span class="kobospan" id="kobo.627.1">few examples:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.628.1">Google’s </span><strong class="bold"><span class="kobospan" id="kobo.629.1">Federated Learning of Cohorts</span></strong><span class="kobospan" id="kobo.630.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.631.1">FLoC</span></strong><span class="kobospan" id="kobo.632.1">) team is working on developing FRL to create privacy-preserving</span><a id="_idIndexMarker728" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.633.1"> user models for </span><span><span class="kobospan" id="kobo.634.1">ad targeting.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.635.1">Researchers at Stanford University have been exploring the use of FRL to train autonomous robots. </span><span class="kobospan" id="kobo.635.2">A team of researchers from Carnegie Mellon University is investigating FRL as a</span><a id="_idIndexMarker729" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.636.1"> way to improve the efficiency and privacy of healthcare machine </span><span><span class="kobospan" id="kobo.637.1">learning models.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.638.1">The OpenMined project is a community-driven initiative that aims to create an ecosystem for privacy-preserving machine learning, </span><span><span class="kobospan" id="kobo.639.1">including FRL.</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.640.1">Microsoft Research is working on developing FRL methods for distributed </span><span><span class="kobospan" id="kobo.641.1">robotics applications.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.642.1">These are just a few examples of the many researchers and organizations working on FRL, which is a rapidly growing field with many exciting developments </span><span><span class="kobospan" id="kobo.643.1">and applications.</span></span></p>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.644.1">Federated Tumour Segmentation (FeTs</span></strong><span class="kobospan" id="kobo.645.1">) is a real-world medical FL platform developed by Intel and the </span><strong class="bold"><span class="kobospan" id="kobo.646.1">University of Pennsylvania</span></strong><span class="kobospan" id="kobo.647.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.648.1">UPenn</span></strong><span class="kobospan" id="kobo.649.1">). </span><span class="kobospan" id="kobo.649.2">They make use of OpenFL as a </span><a id="_idIndexMarker730" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.650.1">backend for </span><span><span class="kobospan" id="kobo.651.1">the </span></span><span><a id="_idIndexMarker731" class="pcalibre1 calibre6 pcalibre"/></span><span><span class="kobospan" id="kobo.652.1">platform.</span></span></p>
<h1 id="_idParaDest-147" class="calibre5"><a id="_idTextAnchor155" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.653.1">Key company products related to FL</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.654.1">As we have seen FL is a rapidly growing field that has gained significant attention from start-up companies. </span><span class="kobospan" id="kobo.654.2">Here is a summary of some of the companies that are working or providing </span><span><span class="kobospan" id="kobo.655.1">FL products:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.656.1">DynamoFL</span></strong><span class="kobospan" id="kobo.657.1">: DynamoFL is built by privacy and machine learning experts from </span><strong class="bold"><span class="kobospan" id="kobo.658.1">MIT</span></strong><span class="kobospan" id="kobo.659.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.660.1">Massachusetts Institute of Technology</span></strong><span class="kobospan" id="kobo.661.1">) and Harvard, who built leading FL solutions at Google AI and</span><a id="_idIndexMarker732" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.662.1"> privacy-enhanced technologies at Microsoft. </span><span class="kobospan" id="kobo.662.2">As per its website, the</span><a id="_idIndexMarker733" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.663.1"> current </span><strong class="bold"><span class="kobospan" id="kobo.664.1">large language models</span></strong><span class="kobospan" id="kobo.665.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.666.1">LLMs</span></strong><span class="kobospan" id="kobo.667.1">) are not private, but the LLMs from DynamoFL are. </span><span class="kobospan" id="kobo.667.2">They provide personalized FL, which is another key area of research. </span><span class="kobospan" id="kobo.667.3">LLMs are covered in </span><a href="B16573_10.xhtml#_idTextAnchor219" class="pcalibre1 calibre6 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.668.1">Chapter 10</span></em></span></a><span class="kobospan" id="kobo.669.1"> in the </span><em class="italic"><span class="kobospan" id="kobo.670.1">Privacy-preserved generative </span></em><span><em class="italic"><span class="kobospan" id="kobo.671.1">AI</span></em></span><span><span class="kobospan" id="kobo.672.1"> section.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.673.1">NVIDIA FLARE</span></strong><span class="kobospan" id="kobo.674.1">: NVIDIA </span><strong class="bold"><span class="kobospan" id="kobo.675.1">Federated Learning Application Runtime Environment</span></strong><span class="kobospan" id="kobo.676.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.677.1">FLARE</span></strong><span class="kobospan" id="kobo.678.1">) is an SDK for FL that is</span><a id="_idIndexMarker734" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.679.1"> open sourced by NVIDIA. </span><span class="kobospan" id="kobo.679.2">FLARE</span><a id="_idIndexMarker735" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.680.1"> supports various FL algorithms (FedAvg, FedProx, FedOpt, etc.), neural networks, and ML algorithms. </span><span class="kobospan" id="kobo.680.2">It supports differential privacy and homomorphic encryption features as part of the security and privacy preservation stack. </span><span class="kobospan" id="kobo.680.3">The SDK also provides a simulator that can be used to start servers and clients and execute FL notebooks and </span><span><span class="kobospan" id="kobo.681.1">FL applications.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.682.1">OpenMined</span></strong><span class="kobospan" id="kobo.683.1">: OpenMined is an open source community that provides a platform for privacy-preserving</span><a id="_idIndexMarker736" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.684.1"> ML, including FL tools and libraries for secure aggregation and </span><span><span class="kobospan" id="kobo.685.1">differential privacy.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.686.1">DataFleets</span></strong><span class="kobospan" id="kobo.687.1">: DataFleets provides a </span><a id="_idIndexMarker737" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.688.1">platform for privacy-preserving data access and analytics, including FL tools for secure data processing </span><span><span class="kobospan" id="kobo.689.1">and analysis.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.690.1">Scaleout</span></strong><span class="kobospan" id="kobo.691.1">: Scaleout provides a platform for distributed ML, including FL tools for privacy-preserving and collaborative </span><a id="_idIndexMarker738" class="pcalibre1 calibre6 pcalibre"/><span><span class="kobospan" id="kobo.692.1">model training.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.693.1">Edge Impulse</span></strong><span class="kobospan" id="kobo.694.1">: Edge Impulse provides a platform for developing and deploying ML models on edge devices, including</span><a id="_idIndexMarker739" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.695.1"> FL tools for privacy-preserving model training </span><span><span class="kobospan" id="kobo.696.1">and inference.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.697.1">Decentralized Machine Learning</span></strong><span class="kobospan" id="kobo.698.1">: Decentralized Machine Learning provides a platform for privacy-preserving and</span><a id="_idIndexMarker740" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.699.1"> decentralized ML, including FL tools for secure aggregation and </span><span><span class="kobospan" id="kobo.700.1">differential privacy.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.701.1">PySyft</span></strong><span class="kobospan" id="kobo.702.1">: PySyft is an open source</span><a id="_idIndexMarker741" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.703.1"> library for FL</span><a id="_idIndexMarker742" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.704.1"> and secure multi-party computation, enabling privacy-preserving and collaborative ML on </span><span><span class="kobospan" id="kobo.705.1">distributed data.</span></span></li>
<li class="calibre11"><strong class="bold"><span class="kobospan" id="kobo.706.1">Intellegens</span></strong><span class="kobospan" id="kobo.707.1">: Intellegens’ product originated from the University of Cambridge and Ichnite and is an FL platform product that can</span><a id="_idIndexMarker743" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.708.1"> be deployed in the cloud or </span><span><span class="kobospan" id="kobo.709.1">on-premises (</span></span><a href="https://intellegens.com/" class="pcalibre1 calibre6 pcalibre"><span><span class="kobospan" id="kobo.710.1">https://intellegens.com/</span></span></a><span><span class="kobospan" id="kobo.711.1">).</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.712.1">These companies are actively contributing to the development and advancement of FL, providing tools and platforms for privacy-preserving and collaborative ML on distributed data. </span><span class="kobospan" id="kobo.712.2">As FL continues to evolve, it is expected that more start-up companies will emerge, contributing to the growth and innovation of </span><span><span class="kobospan" id="kobo.713.1">this field.</span></span></p>
<h1 id="_idParaDest-148" class="calibre5"><a id="_idTextAnchor156" class="pcalibre1 calibre6 pcalibre"/><span class="kobospan" id="kobo.714.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.715.1">In this chapter, we explored FL benchmarks, their importance, and how to design benchmarks, among other things. </span><span class="kobospan" id="kobo.715.2">We also discussed various FL frameworks and looked at what to consider when choosing a framework to implement FL applications. </span><span class="kobospan" id="kobo.715.3">Finally, we covered the state-of-the-art research undertaken by various enterprises in collaboration with key universities, highlighting some of the key companies that are actively working and offering products/platforms to </span><span><span class="kobospan" id="kobo.716.1">support FL.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.717.1">In the next chapter, we will learn about homomorphic encryption and secure multi-party computation and how they help in achieving privacy in </span><span><span class="kobospan" id="kobo.718.1">ML models.</span></span></p>
</div>
</body></html>