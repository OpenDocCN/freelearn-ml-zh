<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Bayesian Networks and Hidden Markov Models</h1>
                </header>
            
            <article>
                
<p>In this chapter, we're going to introduce the basic concepts of Bayesian models, which allow working with several scenarios where it's necessary to consider uncertainty as a structural part of the system. The discussion will focus on static (time-invariant) and dynamic methods that can be employed where necessary to model time sequences.</p>
<p>In particular, the chapter covers the following topics:</p>
<ul>
<li>Bayes' theorem and its applications</li>
<li>Bayesian networks</li>
<li>Sampling from a Bayesian network using direct methods and <strong>Markov chain Monte Carlo</strong> (<strong>MCMC</strong>) ones (Gibbs and Metropolis-Hastings samplers)</li>
<li>Modeling a Bayesian network with PyMC3</li>
<li><strong>Hidden Markov Models</strong> (<strong>HMMs</strong>)</li>
<li>Examples with hmmlearn</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conditional probabilities and Bayes' theorem</h1>
                </header>
            
            <article>
                
<p>If we have a probability space <em>S</em> and two events <em>A</em> and <em>B</em>, the probability of <em>A</em> given <em>B</em> is called <strong>conditional probability</strong>, and it's defined as:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/352da6fc-79b9-478c-acef-eac9462996be.png" style="width:9.33em;height:2.83em;"/></div>
<p>As <em>P(A, B)</em> = <em>P(B, A)</em>, it's possible to derive <strong>Bayes' theorem</strong>:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c34fa3ad-5cf8-4d3d-80f1-768034c0d1a3.png" style="width:23.50em;height:2.67em;"/></p>
<p>This theorem allows expressing a conditional probability as a function of the opposite one and the two marginal probabilities <em>P(A)</em> and <em>P(B)</em>. This result is fundamental to many machine learning problems, because, as we're going to see in this and in the next chapters, normally it's easier to work with a conditional probability in order to get the opposite, but it's hard to work directly from the latter. A common form of this theorem can be expressed as:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d7d71973-5b65-4bbc-8eef-433b4cd5950d.png" style="width:10.17em;height:1.17em;"/></p>
<p>Let's suppose that we need to estimate the probability of an event <em>A</em> given some observations <em>B</em>, or using the standard notation, <strong>the posterior probability of A</strong>; the previous formula expresses this value as proportional to the term <em>P(A)</em>, which is the marginal probability of <em>A</em>, called <strong>prior probability</strong>, and the conditional probability of the observations <em>B</em> given the event <em>A</em>. <em>P(B|A)</em> is called <strong>likelihood</strong>, and defines how event <em>A</em> is likely to determine <em>B</em>. Therefore, we can summarize the relation as <em>posterior probability ∝ likelihood · prior probability</em>. The proportion is not a limitation, because the term <em>P(B)</em> is always a normalizing constant that can be omitted. Of course, the reader must remember to normalize <em>P(A|B)</em> so that its terms always sum up to one.</p>
<p>This is a key concept of Bayesian statistics, where we don't directly trust the prior probability, but we reweight it using the likelihood of some observations. As an example, we can think to toss a coin 10 times (event <em>A</em>). We know that <em>P(A) = 0.5</em> if the coin is fair. If we'd like to know what the probability is to get 10 heads, we could employ the Binomial distribution obtaining <em>P(10 heads) = 0.5<sup>k</sup></em>; however, let's suppose that we don't know whether the coin is fair or not, but we suspect it's loaded with a prior probability <em>P(Loaded) = 0.7</em> in favor of tails. We can define a complete prior probability <em>P(Coin status)</em> using the indicator functions:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d017c7ab-85b8-4687-8cea-ce1f6222e7c9.png" style="width:35.67em;height:1.58em;"/></div>
<p>Where <em>P(Fair) = 0.5</em> and <em>P(Loaded) = 0.7</em>, the indicator <em>I<sub>Coin=Fair</sub></em> is equal to 1 only if the coin is fair, and 0 otherwise. The same happens with <em><span>I</span><sub>Coin=Loaded</sub></em> when the coin is loaded. Our goal now is to determine the posterior probability <em>P(Coin status|<span>B</span><sub>1</sub><span>, B</span><sub>2</sub><span>, ..., </span></em><span><em>B<sub>n</sub>)</em> to be able to confirm or to reject our hypothesis.</span></p>
<p><span>Let's imagine to observe <em>n = 10</em> events with <em>B<sub>1</sub> = Head</em> and <em>B<sub>2</sub>, ..., B<sub>n</sub> = Tail</em>. We can express the probability using the binomial distribution:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/845c235e-2666-4685-88fe-65cdb967a8ba.png" style="width:49.92em;height:3.08em;"/></p>
<p>After simplifying the expression, we get:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/48d546d0-7d6d-4bb8-8824-202451d938da.png" style="width:32.42em;height:1.50em;"/></div>
<p>We still need to normalize by dividing both terms by 0.083 (the sum of the two terms), so we get the final posterior probability <em><span>P(Coin status|</span><span>B</span><sub>1</sub><span>, B</span><sub>2</sub><span>, ..., </span></em><span><em>Bn) = 0.04I<sub>Fair</sub> + 0.96I<sub>Loaded</sub></em>. This result confirms and strengthens our hypothesis. The probability of a loaded coin is now about 96%, thanks to the sequence of nine tail observations after one head.</span></p>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">This example was presented to show how the data (observations) is plugged into the Bayesian framework. If the reader is interested in studying these concepts in more detail, in <em>Introduction to Statistical Decision Theory</em>, <em>Pratt J.</em>, <em>Raiffa H.</em>, <em>Schlaifer R.</em>, <em>The MIT Press</em>,<em> </em>it's possible to find many interesting examples and explanations; however, before introducing Bayesian networks, it's useful to define two other essential concepts.</p>
<p>The first concept is called <strong>conditional independence</strong>, and it can be formalized considering two variables <em>A</em> and <em>B</em>, which are conditioned to a third one, <em>C</em>. We say that <em>A</em> and <em>B</em> are conditionally independent given <em>C</em> if:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/36bb9203-b429-4355-a8dc-85614c716870.png" style="width:13.17em;height:1.25em;"/></div>
<p>Now, let's suppose we have an event <em>A</em> that is conditioned to a series of causes <em>C<sub>1</sub>, C<sub>2</sub>, ..., C<sub>n</sub></em>; the conditional probability is, therefore, <em>P(A|<span>C</span><sub>1</sub><span>, C</span><sub>2</sub><span>, ..., C</span><sub>n</sub>)</em>. Applying Bayes' theorem, we get:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/fe21f59c-5b35-43b8-8e36-a87618f446de.png" style="width:28.50em;height:1.58em;"/></div>
<p> If there is conditional independence, the previous expression can be simplified and rewritten as:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/981fd6f3-7f95-411f-8c54-81cb045249d8.png" style="width:41.75em;height:3.58em;"/></div>
<p><span>This property is fundamental in Naive Bayes classifiers, where we assume that the effect produced by a cause does not influence the other causes. For example, in a spam detector, we could say that the length of the mail and the presence of some particular keywords are independent events, and we only need to compute <em>P(Length|Spam)</em> and <em>P(Keywords|Spam)</em> without considering the joint probability <em>P(Length, Keywords|Spam)</em>.</span></p>
<p>Another important element is the <strong>chain rule</strong> of probabilities. Let's suppose we have the joint probability <em>P(X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>n</sub>)</em>. It can be expressed as:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/107285da-ab3d-4be3-a4b0-d680ff35d2cd.png" style="width:30.17em;height:1.50em;"/></div>
<p>Repeating the procedure with the joint probability on the right side, we get:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2fe5a7d2-8909-4da8-9322-2582f45b29a0.png" style="width:56.42em;height:4.00em;"/></div>
<p>In this way, it's possible to express a full joint probability as the product of hierarchical conditional probabilities, until the last term, which is a marginal distribution. We are going to use this concept extensively in the next paragraph when exploring Bayesian networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayesian networks</h1>
                </header>
            
            <article>
                
<p>A <strong>Bayesian network</strong> is a probabilistic model represented by a direct acyclic graph <em>G = {V, E}</em>, where the vertices are random variables <em>X<sub>i</sub></em>, and the edges determine a conditional dependence among them. In the following diagram, there's an example of simple Bayesian networks with four variables:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cae2f7b1-4b9f-4cc2-b225-0e12772b4866.png" style="width:9.83em;height:10.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Example of Bayesian network</div>
<p>The variable <em>x<sub>4</sub></em> is dependent on <em>x<sub>3</sub></em>, which is dependent on <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em>. To describe the network, we need the marginal probabilities <em>P(x<sub>1</sub>)</em> and <em>P(x<sub>2</sub>)</em> and the conditional probabilities <em>P(x<sub>3</sub>|x<sub>1</sub>,x<sub>2</sub>)</em> and <em>P(x<sub>4</sub>|x<sub>3</sub>)</em>. In fact, using the chain rule, we can derive the full joint probability as:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c97c045a-48c4-48c0-88f0-bd904ce463c6.png" style="width:30.25em;height:1.58em;"/></div>
<p>The previous expression shows an important concept: as the graph is direct and acyclic, each variable is conditionally independent of all other variables that are not successors given its predecessors. To formalize this concept, we can define the function <em>Predecessors(x<sub>i</sub>)</em>, which returns the set of nodes that influence <em>x<sub>i</sub></em> directly, for example, <em>Predecessors(x<sub>3</sub>) = {x<sub>1</sub>,x<sub>2</sub>}</em> (we are using lowercase letters, but we are considering the random variable, not a sample). Using this function, it's possible to write a general expression for the full joint probability of a Bayesian network with <em>N</em> nodes:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/303e7bb6-83c4-48be-afea-573888168eef.png" style="width:27.25em;height:4.08em;"/></div>
<p>The general procedure to build a Bayesian network should always start with the first causes, adding their effects one by one, until the last nodes are inserted into the graph. If this rule is not respected, the resulting graph can contain useless relations that can increase the complexity of the model. For example, if <em>x<sub>4</sub></em> is caused indirectly by both <em>x<sub>1</sub></em> and <em>x<sub>2</sub></em>, therefore adding the edges <em>x<sub>1</sub> → x<sub>4</sub></em> and <em><span>x</span><sub>2</sub> → x<sub>4</sub></em> could seem a good modeling choice; however, we know that the final influence on <em>x<sub>4</sub></em> is determined only by the values of <em>x</em><sub><em>3</em>, </sub>whose probability must be conditioned on <em><span class="underline">x<sub>1</sub></span></em> and <em>x<sub>2</sub></em>, hence we can remove the spurious edges. I suggest reading <em><span>Introduction to Statistical Decision Theory, Pratt J., Raiffa H., Schlaifer R., The MIT Press</span></em><em> </em>to learn many best practices that should be employed in this procedure.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling from a Bayesian network</h1>
                </header>
            
            <article>
                
<p>Performing a direct inference on a Bayesian network can be a very complex operation when the number of variables and edges is high. For this reason, several sampling methods have been proposed. In this paragraph, we are going to show how to determine the full joint probability sampling from a network using a direct approach, and two MCMC algorithms.</p>
<p>Let's start considering the previous network and, for simplicity, let's assume to have only <em>Bernoulli</em> distributions. <em>X<sub>1</sub></em> and <em>X<sub>2</sub></em> are modeled as:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/eb8c8451-da95-42e4-923a-d096f4041bc0.png" style="width:10.42em;height:2.67em;"/></div>
<p>The conditional distribution <em>X<sub>3</sub></em> is defined as:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/789d0fe3-5573-429b-a2e3-37a4043b7fed.png" style="width:30.75em;height:2.75em;"/></div>
<p>While the conditional distribution <em>X<sub>4</sub></em> is defined as:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c964debc-3b61-4b6d-9b5a-7d68f7f4a514.png" style="width:24.17em;height:2.83em;"/></div>
<p>We can now use a direct sampling to estimate the full joint probability <em>P(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, x<sub>4</sub>)</em> using the chain rule previously introduced.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Direct sampling</h1>
                </header>
            
            <article>
                
<p>With <strong>direct sampling</strong>, our goal is to approximate the full joint probability through a sequence of samples drawn from each conditional distribution. If we assume that the graph is well-structured (without unnecessary edges) and we have <em>N</em> variables, the algorithm is made up of the following steps:</p>
<ol>
<li>Initialize the variable <em>N<sub>Samples</sub></em>.</li>
<li>Initialize a vector <em>S</em> with shape <em>(N, N<sub>Samples</sub>)</em>.</li>
<li>Initialize a frequency vector <em>F<sub>Samples</sub></em> with shape <em>(N, N<sub>Samples</sub>)</em>. In Python, it's better to employ a dictionary where the key is a combination <em>(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>N</sub>)</em>.</li>
<li>For <em>t=1</em> to <em><span>N</span></em><sub><em>Samples</em>:</sub>
<ol>
<li>For <em>i=1</em> to <em>N</em>:
<ol>
<li>Sample from <em>P(X<sub>i</sub>|Predecessors(X<sub>i</sub>))</em></li>
<li>Store the sample in <em>S[i, t]</em></li>
</ol>
</li>
<li>If <em><span>F</span><sub>Samples</sub></em> contains the sampled tuple <em>S[:, t]</em>:
<ol>
<li><em><em><span>F</span><sub>Samples</sub>[<span>S[:, t]] += 1</span></em></em></li>
</ol>
</li>
<li>Else:
<ol>
<li><em><span>F</span><sub>Samples</sub>[</em><span><em>S[:, t]] = 1</em> (both these operations are immediate with Python dictionaries)</span></li>
</ol>
</li>
</ol>
</li>
<li>Create a vector <em>P<sub>Sampled</sub></em> with shape <em>(</em><span><em>N, 1)</em>.</span></li>
<li>Set <em><span>P</span><sub>Sampled</sub>[i, 0] = <span>F</span><sub>Samples</sub><span>[</span></em><span><em>i]/N</em>.</span></li>
</ol>
<p class="mce-root">From a mathematical viewpoint, we are first creating a frequency vector <em><span>F</span><sub>Samples</sub><span>(x</span><sub>1</sub><span>, x</span><sub>2</sub><span>, x</span><sub>3</sub><span>, ..., x</span><sub>N</sub>; <span>N</span><sub>Samples</sub></em><span><em>)</em> and then we approximate the full joint probability considering <em>N<sub>Samples</sub> → ∞</em>:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/66ba24b0-7188-465c-925c-17d0b4d179e2.png" style="width:41.67em;height:2.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of direct sampling</h1>
                </header>
            
            <article>
                
<p>We can now implement this algorithm in Python. Let's start by defining the sample methods using the NumPy function <kbd>np.random.binomial(1, p)</kbd>, which draws a sample from a <em>Bernoulli</em> distribution with probability <kbd>p</kbd>:</p>
<pre>import numpy as np<br/><br/>def X1_sample(p=0.35):<br/>    return np.random.binomial(1, p)<br/><br/>def X2_sample(p=0.65):<br/>    return np.random.binomial(1, p)<br/><br/>def X3_sample(x1, x2, p1=0.75, p2=0.4):<br/>    if x1 == 1 and x2 == 1:<br/>        return np.random.binomial(1, p1)<br/>    else:<br/>        return np.random.binomial(1, p2)<br/>    <br/>def X4_sample(x3, p1=0.65, p2=0.5):<br/>    if x3 == 1:<br/>        return np.random.binomial(1, p1)<br/>    else:<br/>        return np.random.binomial(1, p2)</pre>
<p>At this point, we can implement the main cycle. As the variables are Boolean, the total number of probabilities is 16, so we set <kbd>Nsamples</kbd> to <kbd>5000</kbd> (smaller values are also acceptable):</p>
<pre>N = 4<br/>Nsamples = 5000<br/><br/>S = np.zeros((N, Nsamples))<br/>Fsamples = {}<br/><br/>for t in range(Nsamples):<br/>    x1 = X1_sample()<br/>    x2 = X2_sample()<br/>    x3 = X3_sample(x1, x2)<br/>    x4 = X4_sample(x3)<br/>    <br/>    sample = (x1, x2, x3, x4)<br/>    <br/>    if sample in Fsamples:<br/>        Fsamples[sample] += 1<br/>    else:<br/>        Fsamples[sample] = 1</pre>
<p>When the sampling is complete, it's possible to extract the full joint probability:</p>
<pre>samples = np.array(list(Fsamples.keys()), dtype=np.bool_)<br/>probabilities = np.array(list(Fsamples.values()), dtype=np.float64) / Nsamples<br/><br/>for i in range(len(samples)):<br/>    print('P{} = {}'.format(samples[i], probabilities[i]))<br/><br/>P[ True False  True  True] = 0.0286
P[ True  True False  True] = 0.024
P[ True  True  True False] = 0.06
P[False False False False] = 0.0708
P[ True False  True False] = 0.0166
P[False  True  True  True] = 0.1006
P[False False  True  True] = 0.054<br/>...</pre>
<p>We can also query the model. For example, we could be interested in <em>P(X<sub>4</sub>=True)</em>. We can do this by looking for all the elements where <em>X<sub>4</sub>=True</em>, and summing up the relative probabilities:</p>
<pre>p4t = np.argwhere(samples[:, 3]==True)<br/>print(np.sum(probabilities[p4t]))<br/><br/>0.5622</pre>
<p>This value is coherent with the definition of <em>X<sub>4</sub></em>, which is always <em>p &gt;= 0.5</em>. The reader can try to change the values and repeat the simulation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A gentle introduction to Markov chains</h1>
                </header>
            
            <article>
                
<p>In order to discuss the MCMC algorithms, it's necessary to introduce the concept of Markov chains. In fact, while the direct sample method draws samples without any particular order, the MCMC strategies draw a sequence of samples according to a precise transition probability from a sample to the following one.</p>
<p>Let's consider a time-dependent random variable <em>X(t)</em>, and let's assume a discrete time sequence <strong>X<sub>1</sub></strong>, <strong>X<sub>2</sub></strong>, ..., <strong>X<sub>t</sub></strong>, <strong>X<sub>t+1</sub></strong>, ... where <strong>X<sub>t</sub></strong> represents the value assumed at time <em>t</em>. In the following diagram, there's a schematic representation of this sequence:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/69b55183-7574-40b2-9b1e-96e2d4cf5a3e.png" style="width:46.67em;height:6.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Structure of a generic Markov chain</div>
<p>We can suppose to have <em>N</em> different states <em>s<sub>i</sub></em> for <em>i=1..N</em>, therefore it's possible to consider the probability <em>P(X<sub>t</sub>=s<sub>i</sub>|X<sub>t-1</sub>=s<sub>j</sub>, ..., X<sub>1</sub>=s<sub>p</sub>)</em>. <em>X(t)</em> is defined as a <strong>first-order Markov process</strong> if:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4414956f-285c-4f5d-bf7a-7de1a5f60c52.png" style="width:36.50em;height:1.67em;"/></div>
<p>In other words, in a Markov process (from now on, we omit <em>first-order</em>, even if there are cases when it's useful to consider more previous states), the probability that <em>X(t)</em> is in a certain state depends only on the state assumed in the previous time instant. Therefore, we can define a <strong>transition probability </strong>for every couple <em>i</em>, <em>j</em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a16ad93b-a206-445b-9ffb-42f3407edb16.png" style="width:18.50em;height:1.50em;"/></div>
<p>Considering all the couples <em>(i, j)</em>, it's also possible to build a transition probability matrix <em>T(i, j) = P(i → j)</em>. The marginal probability that <em>X<sub>t</sub>=s<sub>i</sub></em> using a standard notation is defined as:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/17cc4835-7ef7-4686-a2e4-124c42e4a982.png" style="width:9.25em;height:1.33em;"/></div>
<p class="mce-root">At this point, it's easy to prove (<strong>Chapman-Kolmogorov</strong> equation) that:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ee16d959-98d0-4b92-9ceb-7a345491283c.png" style="width:30.00em;height:3.08em;"/></div>
<p>In the previous expression, in order to compute <em>π<sub>i</sub>(t+1)</em>, we need to sum over all possible previous states, considering the relative transition probability. This operation can be rewritten in matrix form, using a vector <em>π(t)</em> containing all states and the transition probability matrix <em>T</em> (the uppercase superscript <em>T</em> means that the matrix is transposed). The evolution of the chain can be computed recursively:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d9526323-5850-4106-aa9d-b2bddc9343fa.png" style="width:30.42em;height:1.83em;"/></div>
<p>For our purposes, it's important to consider Markov chains that are able to reach a <em>stationary distribution</em> <em>π<sub>s</sub></em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/66b29160-1d3f-4c7c-ae04-157fcd2ef634.png" style="width:7.25em;height:1.92em;"/></div>
<p>In other words, the state does not depend on the initial condition <em>π(1)</em>, and it's no longer able to change. The stationary distribution is unique if the underlying Markov process is <em>ergodic</em>. This concept means that the process has the same properties if averaged over time (which is often impossible), or averaged vertically (freezing the time) over the states (which is simpler in the majority of cases).</p>
<p>The process of ergodicity for Markov chains is assured by two conditions. The first is aperiodicity<strong> </strong><span>for all states</span>, which means that it is impossible to find a positive number <em>p</em> so that the chain returns in the same state sequence after a number of instants equal to a <span>multiple of <em>p</em></span>. The second condition is that all states must be positive recurrent: this means that, given a random variable <em><span>N</span><sub>instants</sub></em><span><em>(i)</em>, describing the number of time instants needed to return to the state <em>s<sub>i</sub></em>,</span> <em>E[N<sub>instants</sub>(i)] &lt; ∞</em>; therefore, potentially, all the states can be revisited in a finite time.</p>
<p>The reason why we need the ergodicity condition, and hence the existence of a unique stationary distribution, is that we are considering the sampling processes modeled as Markov chains, where the next value is sampled according to the current state. The transition from one state to another is done in order to find better samples, as we're going to see in the Metropolis-Hastings sampler, where we can also decide to reject a sample and keep the chain in the same state. For this reason, we need to be sure that the algorithms converge to the unique stable distribution (that approximates the real full joint distribution of our Bayesian network). It's possible to prove that a chain always reaches a stationary distribution if:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ec1e6164-4511-457d-9598-93c25d184f80.png" style="width:23.42em;height:2.00em;"/></div>
<p>The previous equation is called detailed balance, and implies the reversibility of the chain. Intuitively, it means that the probability of finding the chain in the state <em>A</em> times the probability of a transition to the state <em>B</em> is equal to the probability of finding the chain in the state <em>B</em> times the probability of a transition to <em>A</em>.</p>
<p>For both methods that we are going to discuss, it's possible to prove that they satisfy the previous condition, and therefore their convergence is assured.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gibbs sampling</h1>
                </header>
            
            <article>
                
<p>Let's suppose that we want to obtain the full joint probability for a Bayesian network <em>P<span>(x</span><sub>1</sub><span>, x</span><sub>2</sub><span>, x</span><sub>3</sub><span>, ..., x</span><sub>N</sub></em><span><em>)</em></span>;<span> however, the number of variables is large and there's no way to solve this problem easily in a closed form. Moreover, imagine that we would like to get some marginal distribution, such as <em>P(x<sub>2</sub>)</em>, but to do so we should integrate the full joint probability, and this task is even harder.</span> Gibbs sampling <span>allows approximating of all marginal distributions with an iterative process. If we have <em>N</em> variables, the algorithm proceeds with the following steps:</span></p>
<ol>
<li>Initialize the variable <em>N<sub>Iterations</sub></em></li>
<li>Initialize a vector <em>S</em> with shape <em>(N, <span>N</span><sub>Iterations</sub>)</em></li>
<li>Randomly initialize <em><span>x</span><sub>1</sub><sup>(0)</sup>, <span>x</span><sub>2</sub><sup>(0)</sup>, ..., <span>x</span><sub>N</sub><sup>(0)</sup></em> (the superscript index is referred to the iteration)</li>
<li>For <em>t=1</em> to <em>N<sub>Iterations</sub></em>:
<ol>
<li>Sample <em>x<sub>1</sub><sup>(t)</sup></em> from <em>p(x<sub>1</sub>|<span>x</span><sub>2</sub><sup>(t-1)</sup>, <span>x</span><sub>3</sub><sup>(t-1)</sup>, ..., <span>x</span><sub>N</sub><sup>(t-1)</sup>)</em> and store it in <em>S[0, t]</em></li>
<li>Sample <em><span>x</span><sub>2</sub><sup>(t)</sup></em><span> from <em>p(</em></span><em><span>x</span><sub>2</sub><span>|</span><span>x</span><sub>1</sub><sup>(t)</sup><span>, </span><span>x</span><sub>3</sub><sup>(t-1)</sup><span>, ..., </span><span>x</span><sub>N</sub><sup>(t-1)</sup></em><span><em>)</em> and store it in <em>S[1, t]</em></span></li>
<li>Sample <em><span>x</span><sub>3</sub><sup>(t)</sup></em><span> from <em>p(</em></span><em><span>x</span><sub>3</sub><span>|</span><span>x</span><sub>1</sub><sup>(t)</sup><span>, </span><span>x</span><sub>2</sub><sup>(t)</sup><span>, ..., </span><span>x</span><sub>N</sub><sup>(t-1)</sup></em><span><em>)</em> and store it in <em>S[2, t]</em></span></li>
<li>...</li>
<li>Sample <em><span>x</span><sub>N</sub><sup>(t)</sup></em><span> from <em>p(</em></span><em><span>x</span><sub>N</sub><span>|</span><span>x</span><sub>1</sub><sup>(t)</sup><span>, </span><span>x</span><sub>2</sub><sup>(t)</sup><span>, ..., </span><span>x</span><sub>N-1</sub><sup>(t)</sup></em><span><em>)</em> and store it in <em>S[N-1, t]</em></span></li>
</ol>
</li>
</ol>
<p>At the end of the iterations, vector <em>S</em> will contain <em><span>N</span></em><sub><em>Iterations</em> </sub>samples for each distribution. As we need to determine the probabilities, it's necessary to proceed like in the direct sampling algorithm, counting the number of single occurrences and normalizing dividing by <em><span>N</span><sub>Iterations</sub></em>. If the variables are continuous, it's possible to consider intervals, counting how many samples are contained in each of them. </p>
<p>For small networks, this procedure is very similar to direct sampling, except that when working with very large networks, the sampling process could become slow; however, the algorithm can be simplified after introducing the concept of the Markov blanket of <em>X<sub>i</sub></em>, which is the set of random variables that are predecessors, successors, and successors' predecessors of <em><span>X</span><sub>i</sub></em> (in some books, they use the terms <em>parents</em> and <em>children</em>). In a Bayesian network, a variable <em>X<sub>i</sub></em> is a conditional independent of all other variables given its Markov blanket. Therefore, if we define the function <em>MB(<span>X</span><sub>i</sub>)</em>, which returns the set of variables in the blanket, the generic sampling step can be rewritten as <em>p(x<sub>i</sub>|MB(X<sub>i</sub>))</em>, and there's no more need to consider all the other variables.</p>
<p>To understand this concept, let's consider the network shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1f40b1fa-6e12-401a-a769-3bd4b0438600.png" style="width:15.58em;height:12.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Bayesian network for the Gibbs sampling example</div>
<p>The Markov blankets are:</p>
<ul>
<li><em>MB(X<sub>1</sub>)</em> = <em>{ X<sub>2</sub>, X<sub>3</sub> }</em></li>
<li><em>MB(X<sub>2</sub>)</em> = <em>{ <span>X</span><sub>1, </sub>X<sub>3</sub>, X<sub>4</sub><span> </span>}</em></li>
<li><em>MB(X<sub>3</sub>)</em> = <em>{<span> </span><span>X</span><sub>1, </sub>X<sub>2</sub>, <span>X<sub>4</sub>, </span>X<sub>5</sub><span> </span>}</em></li>
<li><em>MB(X<sub>4</sub>)</em> = <em>{<span> </span><span>X</span><sub>3</sub><span> </span>}</em></li>
<li><em>MB(X<sub>5</sub>)</em> = <em>{<span> </span><span>X</span><sub>3</sub><span> </span>}</em></li>
<li><em>MB(X<sub>6</sub>)</em> = <em>{<span> </span><span>X</span><sub>2</sub><span> </span>}</em></li>
</ul>
<p>In general, if <em>N</em> is very large, the cardinality of <em>|MB(<span>X</span><sub>i</sub>)| &lt;&lt; N</em>, thus simplifying the process (the <em>vanilla</em> Gibbs sampling needs <em>N-1</em> conditions for each variable). We can prove that the Gibbs sampling generates samples from a Markov chain that is in detailed balance:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/30105cca-76a4-4cad-bf32-b2c541feed6f.png" style="width:58.00em;height:6.00em;"/></div>
<p>Therefore, the procedure converges to the unique stationary distribution. This algorithm is quite simple; however, its performance is not excellent, because the random walks are not tuned up in order to explore the right regions of the state-space, where the probability to find good samples is high. Moreover, the trajectory can also return to bad states, slowing down the whole process. An alternative (also implemented by PyMC3 for continuous random variables) is the <strong>No-U-Turn</strong> algorithm, which we don't discuss in this book. The reader interested in this topic can find a full description in <em>The</em> <em>No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo</em>, <em>Hoffmann M. D.</em>, <em>Gelman A.</em>, <em>arXiv:1111.4246</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Metropolis-Hastings sampling</h1>
                </header>
            
            <article>
                
<p class="mce-root">We have seen that the full joint probability distribution of a Bayesian network <em><span>P</span><span>(x</span><sub>1</sub><span>, x</span><sub>2</sub><span>, x</span><sub>3</sub><span>, ..., x</span><sub>N</sub></em><span><em>)</em> </span>can become intractable when the number of variables is large. The problem can become even harder when it's needed to marginalize it in order to obtain, for example, <em>P(x<sub>i</sub>)</em>, because it's necessary to integrate a very complex function. The same problem happens when applying the Bayes' theorem in simple cases. Let's suppose we have the expression <em>p(A|B) = K · P(B|A)P(A)</em>. I've expressly inserted the normalizing constant <em>K</em>, because if we know it, we can immediately obtain the posterior probability; however, finding it normally requires integrating <em>P(B|A)P(A)</em>, and this operation can be impossible in closed form.</p>
<p class="mce-root">The Metropolis-Hastings algorithm can help us in solving this problem. Let's imagine that we need to sample from <em><span>P</span><span>(x</span><sub>1</sub><span>, x</span><sub>2</sub><span>, x</span><sub>3</sub><span>, ..., x</span><sub>N</sub></em><span><em>)</em>, but we know this distribution up to a normalizing constant, so <em>P(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>N</sub>) ∝ g(x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ..., x<sub>N</sub>)</em>. For simplicity, from now on we collapse all variables into a single vector, so <em>P(x) ∝ g(x)</em>.</span></p>
<p class="mce-root"><span>Let's take another distribution <em>q(x'|x<sup>(i-1)</sup>)</em>, which is called <strong>candidate-generating distribution</strong>. There are no particular restrictions on this choice, only that <em>q</em> is easy to sample. In some situations, <em>q</em> can be chosen as a function very similar to the distribution <em>p(x)</em>, which is our target, while in other cases, it's possible to use a</span> normal <span>distribution with mean equal to <em>x<sup>(i-1)</sup></em>. As we're going to see, this function acts as a proposal-generator, but we're not obliged to accept all the samples drawn from it therefore, potentially any distribution with the same domain of <em>P(X)</em> can be employed. When a sample is accepted, the Markov chain transitions to the next state, otherwise it remains in the current one. This decisional process is based on the idea that the sampler must explore the most important state-space regions and discard the ones where the probability to find good samples is low. </span></p>
<p>The algorithm <span>proceeds with the following steps:</span></p>
<ol>
<li>Initialize the variable <em>N<sub>Iterations</sub></em></li>
<li>Initialize <em>x<sup>(0)</sup></em> randomly</li>
<li><span>For <em>t=1</em> to <em>N</em></span><em><sub>Iterations</sub></em><span>:</span>
<ol>
<li>Draw a candidate sample <em>x'</em> from <em><span>q(x'|x</span><sup>(i-1)</sup></em><span><em>)</em></span></li>
<li>Compute the following value:
<div class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e2dfa594-aaac-4af2-a7b0-133f55e1fd30.png" style="width:14.67em;height:3.75em;"/></div>
</li>
<li>If <em>α ≥ 1</em>:
<ol>
<li>Accept the sample <em>x<sup>(t)</sup> = x'</em></li>
</ol>
</li>
<li>Else if <em>0 &lt; α &lt; 1</em>:
<ol>
<li>Accept the sample <em><span>x</span><sup>(t)</sup></em><span><em> = x'</em> with probability <em>α</em>; </span>or</li>
<li>Reject the sample <em>x'</em> setting <em><span>x</span><sup>(t)</sup> = x<sup>(t-1)</sup></em> with probability <em>1 - </em><span><em>α</em></span></li>
</ol>
</li>
</ol>
</li>
</ol>
<p>It's possible to prove (the proof will be omitted, but it's available in <em>Markov Chain Monte Carlo and Gibbs Sampling, </em><em>Walsh B., Lecture Notes for EEB 596z</em>) that the transition probability of the Metropolis-Hastings algorithm satisfies the detailed balance equation, and therefore the algorithm converges to the true posterior distribution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of Metropolis-Hastings sampling</h1>
                </header>
            
            <article>
                
<p>We can implement this algorithm to find the posterior distribution <em>P(A|B)</em> given the product of <em>P(B|A)</em> and <em>P(A)</em>, without considering the normalizing constant that requires a complex integration.</p>
<p>Let's suppose that:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/54a4353b-48d1-4357-90cd-01824204939f.png" style="width:16.83em;height:2.92em;"/></div>
<p>Therefore, the resulting <em>g(x)</em> is:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c55f8193-4471-4ed9-907c-e98992b1a0f5.png" style="width:21.75em;height:4.08em;"/></div>
<p>To solve this problem, we adopt the random walk Metropolis-Hastings, which consists of choosing <em>q ∼ Normal(μ=<span>x</span><sup>(t-1)</sup>)</em>. This choice allows simplifying the value <em>α</em>, because the two terms <em>q(<span>x</span><sup>(t-1)</sup>|x')</em> and <em>q(x'|<span>x</span><sup>(t-1)</sup>)</em> are equal (thanks to the symmetry around the vertical axis passing through <em>x<sub>mean</sub></em>) and can be canceled out, so <em>α</em> becomes the ratio between <em>g(x')</em> and <em>g(<span>x</span><sup>(t-1)</sup></em>).</p>
<p>The first thing is defining the functions:</p>
<pre>import numpy as np<br/><br/>def prior(x):<br/>    return 0.1 * np.exp(-0.1 * x)<br/><br/>def likelihood(x):<br/>    a = np.sqrt(0.2 / (2.0 * np.pi * np.power(x, 3)))<br/>    b = - (0.2 * np.power(x - 1.0, 2)) / (2.0 * x)<br/>    return a * np.exp(b)<br/><br/>def g(x):<br/>    return likelihood(x) * prior(x)<br/><br/>def q(xp):<br/>    return np.random.normal(xp)</pre>
<p>Now, we can start our sampling process with 100,000 iterations and <em>x<sup>(0)</sup> = 1.0</em>:</p>
<pre>nb_iterations = 100000<br/>x = 1.0<br/>samples = []<br/><br/>for i in range(nb_iterations):<br/>    xc = q(x)<br/>    <br/>    alpha = g(xc) / g(x)<br/>    if np.isnan(alpha):<br/>        continue<br/>    <br/>    if alpha &gt;= 1:<br/>        samples.append(xc)<br/>        x = xc<br/>    else:<br/>        if np.random.uniform(0.0, 1.0) &lt; alpha:<br/>            samples.append(xc)<br/>            x = xc</pre>
<p>To get a representation of the posterior distribution, we need to create a histogram through the NumPy function <kbd>np.histogram()</kbd>, which accepts an array of values and the number of desired intervals (<kbd>bins</kbd>); in our case, we set <kbd>100</kbd> intervals:</p>
<pre>hist, _ = np.histogram(samples, bins=100)<br/>hist_p = hist / len(samples)</pre>
<p>The resulting plot of <em>p(x)</em> is shown in the following graph:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7d6fb2c3-0a41-45e3-93e8-2ca302dbbf90.png" style="width:61.67em;height:34.00em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Sampled probability density function</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sampling example using PyMC3</h1>
                </header>
            
            <article>
                
<p><strong>PyMC3</strong> is a powerful Python Bayesian framework that relies on Theano to perform high-speed computations (see the information box at the end of this paragraph for the installation instructions). It implements all the most important continuous and discrete distributions, and performs the sampling process mainly using the No-U-Turn and Metropolis-Hastings algorithms. For all the details about the API (distributions, functions, and plotting utilities), I suggest visiting the documentation home page <a href="http://docs.pymc.io/index.html" target="_blank">http://docs.pymc.io/index.html</a>, where it's also possible to find some very intuitive tutorials.</p>
<p>The example we want to model and simulate is based on this scenario: a daily flight from London to Rome has a scheduled departure time at 12:00 am, and a standard flight time of two hours. We need to organize the operations at the destination airport, but we don't want to allocate resources when the plane hasn't landed yet. Therefore, we want to model the process using a Bayesian network and considering some common factors that can influence the arrival time. In particular, we know that the onboarding process can be longer than expected, as well as the refueling one, even if they are carried out in parallel. London air traffic control can also impose a delay, and the same can happen when the plane is approaching Rome. We also know that the presence of rough weather can cause another delay due to a change of route. We can summarize this analysis with the following plot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/db234ff4-d869-4c51-80bb-7d28a56e0783.png" style="width:31.83em;height:14.75em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Bayesian network representing the air traffic control problem</div>
<p>Considering our experience, we decide to model the random variables using the following distributions:</p>
<ul>
<li><em>Passenger onboarding ∼ Wald(μ=0.5, λ=0.2)</em></li>
<li><em>Refueling <span>∼ Wald(μ=0.25, λ=0.5)</span></em></li>
<li><em>Departure traffic delay <span>∼ Wald(μ=0.1, λ=0.2)</span></em></li>
<li><em>Arrival traffic delay <span>∼ Wald(μ=0.1, λ=0.2)</span></em></li>
<li><em>Departure time = 12 + Departure traffic delay + max(Passenger onboarding, Refueling)</em></li>
<li><em>Rough weather <span>∼ Bernoulli(p=0.35)</span></em></li>
<li><em>Flight time <span>∼ </span>Exponential<span>(λ=0.5 - (0.1 · Rough weather))</span></em> (The output of a Bernoulli distribution is <em>0</em> or <em>1</em> corresponding to False and True)</li>
<li><em>Arrival time = Departure time + Flight time + Arrival traffic delay</em></li>
</ul>
<p>The probability density functions are:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6b14d7f2-e3a1-4226-b874-59a38946bbd5.png" style="width:27.08em;height:12.50em;"/></div>
<p><kbd>Departure Time</kbd> and <kbd>Arrival Time</kbd> are functions of random variables, and the parameter λ of <kbd>Flight Time</kbd> is also a function of <kbd>Rough Weather</kbd>. </p>
<p>Even if the model is not very complex, the direct inference is rather inefficient, and therefore we want to simulate the process using PyMC3.</p>
<p>The first step is to create a <kbd>model</kbd> instance:</p>
<pre class="mce-root">import pymc3 as pm<br/> <br/> model = pm.Model()</pre>
<p>From now on, all operations must be performed using the context manager provided by the <kbd>model</kbd> variable. We can now set up all the random variables of our Bayesian network:</p>
<pre>import pymc3.distributions.continuous as pmc<br/>import pymc3.distributions.discrete as pmd<br/>import pymc3.math as pmm<br/><br/>with model:<br/>    passenger_onboarding = pmc.Wald('Passenger Onboarding', mu=0.5, lam=0.2)<br/>    refueling = pmc.Wald('Refueling', mu=0.25, lam=0.5)<br/>    departure_traffic_delay = pmc.Wald('Departure Traffic Delay', mu=0.1, lam=0.2)<br/>    <br/>    departure_time = pm.Deterministic('Departure Time', <br/>                                      12.0 + departure_traffic_delay + <br/>                                      pmm.switch(passenger_onboarding &gt;= refueling, <br/>                                                 passenger_onboarding, <br/>                                                 refueling))<br/>    <br/>    rough_weather = pmd.Bernoulli('Rough Weather', p=0.35)<br/>    <br/>    flight_time = pmc.Exponential('Flight Time', lam=0.5 - (0.1 * rough_weather))<br/>    arrival_traffic_delay = pmc.Wald('Arrival Traffic Delay', mu=0.1, lam=0.2)<br/>    <br/>    arrival_time = pm.Deterministic('Arrival time', <br/>                                    departure_time + <br/>                                    flight_time + <br/>                                    arrival_traffic_delay)</pre>
<p>We have imported two namespaces, <kbd>pymc3.distributions.continuous</kbd> and <span><kbd>pymc3.distributions.discrete</kbd>, because we are using both kinds of variable.</span> Wald <span>and e</span>xponential <span>are continuous distributions, while <kbd>Bernoulli</kbd> is discrete. In the first three rows, we declare the variables <kbd>passenger_onboarding</kbd>, <kbd>refueling</kbd>, and <kbd>departure_traffic_delay</kbd>. The structure is always the same: we need to specify the class corresponding to the desired distribution, passing the name of the variable and all the required parameters.</span></p>
<p>The <kbd>departure_time</kbd> variable is declared as <kbd>pm.Deterministic</kbd>. In PyMC3, this means that, once all the random elements have been set, its value becomes completely determined. Indeed, if we sample from <kbd>departure_traffic_delay</kbd>, <kbd>passenger_onboarding</kbd>, and <kbd>refueling</kbd>, we get a determined value for <kbd>departure_time</kbd>. In this declaration, we've also used the utility function <kbd>pmm.switch</kbd>, which operates a binary choice based on its first parameter (for example, if <em>A &gt; B</em>, return <em>A</em>, else return <em>B</em>).</p>
<p>The other variables are very similar, except for <kbd>flight_time</kbd>, which is an exponential variable with a parameter <em>λ</em>, which is a function of another variable (<kbd>rough_weather</kbd>). As a Bernoulli variable outputs <em>1</em> with probability <em>p</em> and <em>0</em> with probability <em>1 - p</em>, <em>λ = 0.4</em> if there's rough weather, and <em>0.5</em> otherwise.</p>
<p>Once the model has been set up, it's possible to simulate it through a sampling process. PyMC3 picks the best sampler automatically, according to the type of variables. As the model is not very complex, we can limit the process to <kbd>500</kbd> samples:</p>
<pre>nb_samples = 500<br/><br/>with model:<br/>    samples = pm.sample(draws=nb_samples, random_seed=1000)</pre>
<p>The output can be analyzed using the built-in <kbd>pm.traceplot()</kbd> function, which generates the plots for each of the sample's variables. The following graph shows the detail of one of them:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2a2fae86-4a34-4d33-8fcb-d95901ceb486.png"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref"> Distribution and samples for the arrival time random variable</div>
<p>The right column shown the samples generated for the random variable (in this case, the arrival time), while the left column shows the relative frequencies. This plot can be useful to have a visual confirmation of our initial ideas; in fact, the arrival time has the majority of its mass concentrated in the interval 14:00 to 16:00 (the numbers are always decimal, so it's necessary to convert the times); however, we should integrate to get the probabilities. Instead, through the <kbd>pm.summary()</kbd> <span>function</span>, <span>PyMC3 provides a statistical summary</span> that can help us in making the right decisions. In the following snippet, the output containing the summary of a single variable is shown:</p>
<pre>pm.summary(samples)<br/><br/>...

Arrival time:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------
  
  15.174           2.670            0.102            [12.174, 20.484]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|
  
  12.492         13.459         14.419         16.073         22.557</pre>
<p>For each variable, it contains mean, standard deviation, Monte Carlo error, 95% highest posterior density interval, and the posterior quantiles. In our case, we know that the plane will land at about 15:10 (<kbd>15.174</kbd>).</p>
<p>This is only a very simple example to show the power of Bayesian networks. For deep insight, I suggest the book <em><span>Introduction to Statistical Decision Theory</span></em>, <em><span>Pratt J.</span></em>, <em><span>Raiffa H.</span></em>, <em><span>Schlaifer R.</span></em>, <em><span>The MIT Press</span></em>, where it's possible to study different Bayesian applications that are out of the scope of this book.</p>
<div class="packt_infobox">PyMC3 (<a href="http://docs.pymc.io/index.html" target="_blank">http://docs.pymc.io/index.html</a>) can be installed using the <kbd>pip install -U pymc3</kbd> command. As it requires Theano (which is installed automatically), it's also necessary to provide it with a C/C++ compiler. I suggest using distributions such as Anaconda (<a href="https://www.anaconda.com/download/" target="_blank">https://www.anaconda.com/download/</a>), which allows installing MinGW through the <kbd>conda install -c anaconda mingw</kbd><span> command</span>. For any problems, on the website you can find detailed installation instructions. For further information on how to configure Theano to work with GPU support (the default installation is based on CPU NumPy algorithms), please visit this page: <a href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hidden Markov Models (HMMs)</h1>
                </header>
            
            <article>
                
<p>Let's consider a stochastic process <em>X(t)</em> that can assume <em>N</em> different states: <em>s<sub>1</sub>, s<sub>2</sub>, ..., s<sub>N</sub></em> with first-order Markov chain dynamics. Let's also suppose that we cannot observe the state of <em>X(t)</em>, but we have access to another process <em>O(t)</em>, connected to <em>X(t)</em>, which produces observable outputs (often known as <strong>emissions</strong>). The resulting process is called a <strong>Hidden Markov Model</strong> (<strong>HMM</strong>), and a generic schema is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3f054b58-c9c5-4832-b900-76d03dd95007.png" style="width:51.33em;height:13.33em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Structure of a generic Hidden Markov Model</div>
<p>For each hidden state <em>s<sub>i</sub></em>, we need to define a transition probability <em>P(i → j)</em>, normally represented as a matrix if the variable is discrete. For the Markov assumption, we have:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ae2d037e-d1ac-4fbf-a67a-6fd10b65dcf1.png" style="width:20.50em;height:1.67em;"/></div>
<p>Moreover, given a sequence of observations <em>o<sub>1</sub>, o<sub>2</sub>, ..., o<sub>M</sub></em>, we also assume the following assumption about the independence of the <strong>emission probability</strong>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/592ba42f-5436-4409-8576-8f415fd3cb99.png" style="width:27.08em;height:1.67em;"/></div>
<p>In other words, the probability of the observation <em>o<sub>i</sub></em> (in this case, we mean the value at time <em>i</em>) is conditioned only by the state of the hidden variable at time <em>i (x<sub>i</sub>)</em>. Conventionally, the first state <em>x<sub>0</sub></em> and the last one <em>x<sub>Ending</sub></em> are never emittied, and therefore all the sequences start with the index <em>1</em> and end with an extra timestep corresponding to the final state.</p>
<p>HMMs can be employed in all those contexts where it's impossible to measure the state of a system (we can only model it as a stochastic variable with a known transition probability), but it's possible to access some data connected to it. An example can be a complex engine that is made up of a large number of parts. We can define some internal states and learn a transition probability matrix (we're going to learn how to do that), but we can only receive measures provided by specific sensors.</p>
<p>Sometimes, even if not extremely realistic, but it's useful to include the Markov assumption and the emission probability independence into our model. The latter can be justified considering that we can sample all the <em>peak</em> emissions corresponding to precise states and, as the random process <em>O(t)</em> is implicitly dependent on <em>X(t)</em>, it's not unreasonable to think of it like a <em>pursuer</em> of <em>X(t)</em>.</p>
<p>The Markov assumption holds for many real-life processes if either they are naturally first-order Markov ones, or if the states contain all the history needed to justify a transition. In other words, in many cases, if the state is <em>A</em>, then there's a transit to <em>B</em> and finally to <em>C</em>. We assume that when in <em>C</em>, the system moved from a state <em>(B)</em> that carries a part of the information provided by <em>A</em>.</p>
<p>For example, if we are filling a tank, we can measure the level (the state of our system) at time <em>t</em>, <em>t+1</em>, ... If the water flow is modeled by a random variable because we don't have a stabilizer, we can find the probability that the water has reached a certain level at time <em>t</em>, <em>p(L<sub>t</sub>=x|<span>L</span><sub>t-1</sub>)</em>. Of course, it doesn't make sense to condition over all the previous states, because if the level is, for example, 80 m at time t-1, all the information needed to determine the probability of a new level (state) at time <em>t</em> is already contained in this state (80 m).</p>
<p>At this point, we can start analyzing how to train a hidden Markov model, and how to determine the most likely hidden states given a sequence of observations. For simplicity, we call <em>A</em> the transition probability matrix, and <em>B</em> the matrix containing all <em>P(o<sub>i</sub>|x<sub>t</sub>)</em>. The resulting model can be determined by the knowledge of those elements: <em>HMM = { A, B }</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Forward-backward algorithm</h1>
                </header>
            
            <article>
                
<p>The <strong>forward-backward algorithm</strong> is a simple but effective method to find the transition probability matrix <em>T</em> given a sequence of observations <em>o<sub>1</sub>, o<sub>2</sub>, ..., o<sub>t</sub></em>. The first step is called the <em>forward phase</em>, and consists of determining the probability of a sequence of observations <em>P(o<sub>1</sub>, o<sub>2</sub>, ..., o<sub>Sequence Length</sub>|A, B)</em>. This piece of information can be directly useful if we need to know the likelihood of a sequence and it's necessary, together with the <em>backward phase</em>, to estimate the structure (<em>A</em> and <em>B</em>) of the underlying HMM.</p>
<p>Both algorithms are based on the concept of dynamic programming, which consists of splitting a complex problem into sub-problems that can be easily solved, and reusing the solutions to solve more complex steps in a recursive/iterative fashion. For further information on this, please refer to <em>Dynamic Programming and Markov Process, Ronald A. Howard, The MIT Press</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Forward phase</h1>
                </header>
            
            <article>
                
<p>If we call <em>p<sub>ij</sub></em> the transition probability <span><em>P(i → j)</em>, we define a recursive procedure considering the following probability:</span></p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/aa938322-d572-4c1e-98fc-f027a2787715.png" style="width:18.50em;height:1.58em;"/></div>
<p>The variable <em>f<sub>t</sub><sup>i</sup></em> represents the probability that the HMM is in the state <em>i</em><span> (at time <em>t</em>)</span> after <em>t</em> observations (from <em>1</em> to <em>t</em>). Considering the HMM assumptions, we can state that <em><span>f</span><sub>t</sub></em><sup><em>i</em> </sup>depends on all possible <em><span>f</span><sub>t-1</sub><sup>j</sup></em>. More precisely, we have:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e46b6f9b-3b8a-4129-b8fb-c0ceb4d53ece.png" style="width:16.08em;height:3.75em;"/></div>
<p>With this process, we are considering that the HMM can reach any of the states at time <em>t-1</em> (with the first <em>t-1</em> observations), and transition to the state <em>i</em> at time <em>t</em> with probability <em>p<sub>ji</sub></em>. We need also to consider the emission probability for the final state <em>o<sub>t</sub></em> conditioned to each of the possible previous states.</p>
<p>For definition, the initial and ending states are not emitting. It means that we can write any sequence of observations as <em>0, <span>o</span><sub>1</sub><span>, o</span><sub>2</sub><span>, ..., o<sub>Sequence Length</sub></span>, 0</em>, where the first and the final values are null. The procedure starts with computing the forward message at time <em>1</em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f825279c-c495-40c8-866c-4a7279c157a2.png" style="width:9.42em;height:1.67em;"/></div>
<p>The non-emitting ending state must be also considered:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c62826ee-9eab-404a-b9b6-e3656e23da6f.png" style="width:28.67em;height:3.58em;"/></div>
<p>The expression for the last state <em><span>x</span></em><sub><em>Ending</em> </sub>is interpreted here as the index of the ending state in both <em>A</em> and <em>B</em> matrices. For example, we indicate <em>p<sub>ij</sub></em> as <em>A[i, j]</em>, meaning the transition probability at a generic time instant from the state <em>x<sub>t</sub> = i</em> to the state <em>x<sub>t+1</sub> = j</em>. In the same way, <em>p<sub>i</sub><sub>Ending</sub></em> is represented as <em>A[i, <span>x</span><sub>Ending</sub>]</em>, meaning the transition probability from the penultimate state <em>x<sub>Sequence Length-1</sub> = i</em> to the ending one<sub> </sub><em>x<sub>Sequence Length</sub> = Ending State</em>.</p>
<p>The Forward algorithm can, therefore, be summarized in the following steps (we assume to have <em>N</em> states, hence we need to allocate <em>N+2</em> positions, considering the initial and the ending states):</p>
<ol>
<li>Initialization of a <em>Forward</em> vector with shape (<em>N + 2</em>, <em>Sequence Length</em>).</li>
<li>Initialization of <em>A</em> (transition probability matrix) with shape (<em>N, N</em>). Each element is <em><span>P(x</span><sub>i</sub><span>|x</span><sub>j</sub></em><span><em>)</em>.</span></li>
<li>Initialization of <em>B</em> with shape (<em>Sequence Length</em>, <em>N</em>). Each element is <em><span>P(o</span><sub>i</sub><span>|x</span><sub>j</sub></em><span><em>)</em>.</span></li>
<li>For <em>i=1</em> to <em>N</em>:
<ol>
<li>Set <em>Forward[i, 1]</em> = <em>A[0, i] · B[1, i]</em></li>
</ol>
</li>
<li>For <em>t=2</em> to <em>Sequence Length-1</em>:
<ol>
<li>For <em>i=1</em> to <em>N</em>:
<ol>
<li>Set <em>S = 0</em></li>
</ol>
</li>
<li>For <em>j=1</em> to <em>N</em>:
<ol>
<li>Set <em>S = S + Forward[j, t-1] · A[j, i] · B[t, i]</em></li>
</ol>
</li>
<li>Set <em>Forward[i, t] = S</em></li>
</ol>
</li>
</ol>
<ol start="6">
<li>Set <em>S = 0</em>.</li>
<li><span>For <em>i=1</em> to <em>N</em>:</span>
<ol>
<li>Set <em>S = S + Forward[i, Sequence Length] · A[i, x<sub>Ending</sub>]</em></li>
</ol>
</li>
<li>Set <em>Forward[<span>x</span><sub>Ending</sub>, Sequence Length] = S</em>.</li>
</ol>
<p>Now it should be clear that the name <strong>forward</strong> derives from the procedure to propagate the information from the previous step to the next one, until the ending state, which is not emittied.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backward phase</h1>
                </header>
            
            <article>
                
<p>During the <strong>backward phase</strong>, we need to compute the probability of a sequence starting at time <em>t+1: o<sub>t+1</sub>, o<sub>t+2</sub>, ..., <span>o</span><sub>Sequence Length</sub></em>, given that the state at time <em>t</em> is <em>i</em>. Just like we have done before, we define the following probability:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7c024525-9838-4223-a54f-c158fb18e0fd.png" style="width:29.58em;height:1.83em;"/></div>
<p>The backward algorithm is very similar to the forward one, but in this case, we need to move in the opposite direction, <span>assuming we know that the state at time <em>t</em> is </span><em>i</em>. The first state to consider is the last one <em><span>x</span><sub>Ending</sub></em>, which is not emitting, like the initial state; therefore we have:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9cae7840-24b7-4382-aad0-91eb97efa387.png" style="width:15.83em;height:2.25em;"/></div>
<p>We terminate the recursion with the initial state:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d62d9350-f901-4766-a303-43d0ec77f12c.png" style="width:13.33em;height:3.33em;"/></div>
<p>The steps are the following ones: </p>
<ol>
<li>Initialization of a vector <em>Backward</em> with shape <em>(N + 2, Sequence Length)</em>.</li>
<li>Initialization of <em>A</em> (transition probability matrix) with shape <em>(N, N)</em>. Each element is <em><span>P(x</span><sub>i</sub><span>|x</span><sub>j</sub></em><span><em>)</em>.</span></li>
<li>Initialization of <em>B</em> with shape <em>(Sequence Length, N)</em>. Each element is<em> <span>P(o</span><sub>i</sub><span>|x</span><sub>j</sub></em><span><em>)</em>.</span></li>
</ol>
<ol start="4">
<li>For <em>i=1</em> to <em>N</em>:
<ol>
<li>Set <em>Backward[<span>x</span><sub>Endind</sub><span>, Sequence Length</span>] = A[i, <span>x</span><sub>Endind</sub>]</em></li>
</ol>
</li>
<li>For <em>t=Sequence Length-1</em> to <em>1</em>:
<ol>
<li>For <em>i=1</em> to <em>N</em>:
<ol>
<li>Set <em>S = 0</em></li>
<li>For <em>j=1</em> to <em>N</em>
<ol>
<li>Set <em>S = S + Backward[j, t+1] · A[j, i] · B[t+1, i]</em></li>
</ol>
</li>
<li>Set <em>Backward[i, t] = S</em></li>
</ol>
</li>
</ol>
</li>
<li>Set <em>S = 0</em>.</li>
<li><span>For <em>i=1</em> to <em>N</em>:</span>
<ol>
<li>Set <em>S = S + Backward[i, 1] · A[0, i] · B[1, i]</em></li>
</ol>
</li>
<li>Set <em>Backward[0, 1] = S</em>.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">HMM parameter estimation</h1>
                </header>
            
            <article>
                
<p>Now that we have defined both the forward and the backward algorithms, we can use them to estimate the structure of the underlying HMM. The procedure is an application of the Expectation-Maximization algorithm, which will be discussed in the next chapter, <a href="8d541a43-8790-4a91-a79b-e48496f75d90.xhtml" target="_blank">Chapter 5</a>, <em>EM Algorithm and Applications</em>, and its goal can be summarized as defining how we want to estimate the values of <em>A</em> and <em>B</em>. If we define <em>N(i, j)</em> as the number of transitions from the state <em>i</em> to the state <em>j</em>, and <em>N(i)</em> the total number of transitions from the state <em>i</em>, we can approximate the transition probability <span><em>P(i → j)</em> </span>with:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/599b50d2-75d5-4c85-bc1f-e36280247077.png" style="width:19.00em;height:4.00em;"/></div>
<p>In the same way, if we define <em>M(i, p)</em> the number of times we have observed the emission <em>o<sub>p</sub></em> in the state <em>i</em>, we can approximate the emission probability <em><span>P(o</span><sub>p</sub><span>|x</span><sub>i</sub><span>)</span></em> with:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0efa14ed-4579-43c4-8458-49df67a3689a.png" style="width:19.17em;height:4.00em;"/></div>
<p>Let's start with the estimation of the transition probability matrix <em>A</em>. If we consider the probability that the HMM is in the state <em>i</em> at time <em>t</em>, and in the state <em>j</em> at time <em>t+1</em> given the observations, we have:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9c0c449d-33da-43c9-b894-c5c211bf45c5.png" style="width:32.42em;height:1.83em;"/></div>
<p>We can compute this probability using the forward and backward algorithms, given a sequence of observations <em><span>o</span><sub>1</sub><span>, o</span><sub>2</sub><span>, ..., o<sub>Sequence Length</sub></span></em>. In fact, we can use both the forward message <em><span>f</span><sub>t</sub><sup>i</sup></em>, which is the probability that the HMM is in the state <em>i</em> after <em>t</em> observations, and the backward message <em>b<sub>t+1</sub><sup>j</sup></em>, which is the probability of a sequence <em><span>o</span><sub>t+1</sub><span>, o</span><sub>t+1</sub><span>, ..., o<sub>Sequence Length</sub></span></em> starting at time <em>t+1</em>, given that the HMM is in state <em>j</em> at time <em>t+1</em>. Of course, we need also to include the emission probability and the transition probability <em>p<sub>ij</sub></em>, which is what we are estimating. The algorithm, in fact, starts with a random hypothesis and iterates until the values of <em>A</em> become stable. The estimation <em>α<sub>ij</sub></em> at time <em>t</em> is equal to:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/24edfb52-1fa3-405e-a9a3-768e2ef640c7.png" style="width:15.17em;height:4.50em;"/></div>
<p>In this context, we are omitting the full proof due to its complexity; however, the reader can find it in <em>A tutorial on hidden Markov models and selected applications in speech recognition,</em> <em>Rabiner L. R., Proceedings of the IEEE 77.2</em><em>.</em></p>
<p>To compute the emission probabilities, it's easier to start with the probability of being in the state <em>i</em> at time <em>t</em> given the sequence of observations:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8d28be2d-6fc3-499d-86af-6ee4523a4625.png" style="width:29.00em;height:2.25em;"/></div>
<p>In this case, the computation is immediate, because we can multiply the forward and backward messages computed at the same time <em>t</em> and state <em>i</em> (remember that considering the observations, the backward message is conditioned to <em>x<sub>t</sub> = i</em>, while the forward message computes the probability of the observations joined with <em><span>x</span><sub>t</sub><span> = </span>i</em>. Hence, the multiplication is the unnormalized probability of <span>being in the state </span><em>i</em><span> at time <em>t</em></span>). Therefore, we have:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/75ac8487-2059-47de-9904-5f7f0e9ed4fc.png" style="width:12.08em;height:4.50em;"/></div>
<p>The proof of how the normalizing constant is obtained can be found in the aforementioned paper. We can now plug these expressions to the estimation of <em>a<sub>ij</sub></em> and <em>b<sub>ip</sub></em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4f1a3d0a-b4f0-4249-b65a-07c68b0a4ccf.png" style="width:20.50em;height:8.58em;"/></div>
<p>In the numerator of the second formula, we adopted the indicator function (it's <em>1</em> only if the condition is true, <em>0</em> otherwise) to limit the sum only where those elements are <em>o<sub>t</sub> = p</em>. During an iteration <em>k</em>, <em>p<sub>ij</sub></em> is the estimated value <em>a<sub>ij</sub></em> found in the previous iteration <em>k-1</em>.</p>
<p>The algorithm is based on the following steps:</p>
<ol>
<li><span>Randomly </span>initialize the matrices <em>A</em> and <em>B</em></li>
<li>Initialize a tolerance variable <em>Tol</em> (for example, <em>Tol = 0.001</em>)</li>
<li>While <em>Norm(A<sup>k</sup> - A<sup>k-1</sup>) &gt; Tol</em> and <em><span>Norm(B</span><sup>k</sup><span> - B</span><sup>k-1</sup></em><span><em>) &gt; Tol</em> (<em>k</em> is the iteration index):</span>
<ol>
<li>For <em>t=1</em> to <em>Sequence Length-1</em>:
<ol>
<li>For <em>i=1</em> to <em>N</em>: 
<ol>
<li>For <em>j=1</em> to <em>N</em>: 
<ol>
<li>Compute <em>α<sup>t</sup></em><sub><em>ij</em></sub></li>
</ol>
</li>
<li>Compute <em>β<sup>t</sup><sub>i</sub></em></li>
</ol>
</li>
</ol>
</li>
<li>Compute the estimations of <em>a<sub>ij</sub></em> and <em>b<sub>ip</sub></em> and store them in <em><span>A</span><sup>k</sup></em></li>
</ol>
</li>
</ol>
<p>Alternatively, it's possible to fix the number of iterations, even if the best solution is using both a tolerance and a maximum number of iterations, to terminate the process when the first condition is met.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example of HMM training with hmmlearn</h1>
                </header>
            
            <article>
                
<p>For this example, we are going to use hmmlearn, which is a package for HMM computations (see the information box at the end of this section for further details). For simplicity, let's consider the airport example discussed in the paragraph about the Bayesian networks, and let's suppose we have a single hidden variable that represents the weather (of course, this is not a real hidden variable!), modeled as a multinomial distribution with two components (good and rough).</p>
<p>We observe the arrival time of our flight London-Rome (which partially depends on the weather conditions), and we want to train an HMM to infer future states and compute the posterior probability of hidden states corresponding to a given sequence.</p>
<p>The schema for our example is shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f88ad311-0701-48b9-9f86-024a2edf0510.png" style="width:35.17em;height:10.67em;"/></div>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">HMM for the weather-arrival delay problem</div>
<p>Let's start by defining our observation vector. As we have two states, its values will be <kbd>0</kbd> and <kbd>1</kbd>. Let's assume that <kbd>0</kbd> means <strong>On-time</strong> and <kbd>1</kbd> means <strong>Delay</strong>:</p>
<pre>import numpy as np<br/><br/>observations = np.array([[0], [1], [1], [0], [1], [1], [1], [0], [1], <br/>                         [0], [0], [0], [1], [0], [1], [1], [0], [1], <br/>                         [0], [0], [1], [0], [1], [0], [0], [0], [1], <br/>                         [0], [1], [0], [1], [0], [0], [0], [0], [0]], dtype=np.int32)</pre>
<p>We have 35 consecutive observations whose values are either <kbd>0</kbd> or <kbd>1</kbd>.</p>
<p>To build the HMM, we are going to use the <kbd>MultinomialHMM</kbd> class, with <kbd>n_components=2</kbd>, <kbd>n_iter=100</kbd>, and <kbd>random_state=1000</kbd> (it's important to always use the same seed to avoid differences in the results). The number of iterations is sometimes hard to determine; for this reason, hmmlearn provides a utility <kbd>ConvergenceMonitor</kbd> class which can be checked to be sure that the algorithm has successfully converged.</p>
<p>Now we can train our model using the <kbd>fit()</kbd> method, passing as argument the list of observations (the array must be always bidimensional with shape <em>Sequence Length × N<sub>Components</sub></em>):</p>
<pre>from hmmlearn import hmm<br/><br/>hmm_model = hmm.MultinomialHMM(n_components=2, n_iter=100, random_state=1000)<br/>hmm_model.fit(observations)<br/><br/>print(hmm_model.monitor_.converged)<br/>True</pre>
<p>The process is very fast, and the monitor (available as instance variable <kbd>monitor</kbd>) has confirmed the convergence. If the model is very big and needs to be retrained, it's also possible to check smaller values of <kbd>n_iter</kbd>). Once the model is trained, we can immediately visualize the transition probability matrix, which is available as an instance variable <kbd>transmat_</kbd>:</p>
<pre>print(hmm_model.transmat_)<br/><br/>[[ 0.0025384   0.9974616 ]
 [ 0.69191905  0.30808095]]</pre>
<p>We can interpret these values as saying that the probability to transition from <kbd>0</kbd> (good weather) to <kbd>1</kbd> (rough weather) is higher (<em>p<sub>01</sub></em> is close to <em>1</em>) than the opposite, and it's more likely to remain in state <kbd>1</kbd> than in state <kbd>0</kbd> (<em>p<sub>00</sub></em> is almost null). We could deduce that the observations have been collected during the winter period! After explaining the Viterbi algorithm<strong> </strong>in the next paragraph, we can also check, given some observations, what the most likely hidden state sequence is.</p>
<div class="packt_infobox">hmmlearn (<a href="http://hmmlearn.readthedocs.io/en/latest/index.html">http://hmmlearn.readthedocs.io/en/latest/index.html</a>) is a framework originally built to be a part of Scikit-Learn. It supports multinomial and Gaussian HMM, and allows training and inferring using the most common algorithms. It can be installed using the <kbd>pip install hmmlearn</kbd> <span>command</span>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Viterbi algorithm</h1>
                </header>
            
            <article>
                
<p>The <strong>Viterbi algorithm</strong> is one of most common decoding algorithms for HMM. Its goal is to find the most likely hidden state sequence corresponding to a series of observations. The structure is very similar to the forward algorithm, but instead of computing the probability of a sequence of observations joined with the state at the last time instant, this algorithm looks for:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1ba7094a-03d5-4453-9755-557392e2bffd.png" style="width:29.58em;height:1.67em;"/></div>
<p>The variable <em>v<sub>t</sub><sup>i</sup></em> represents that maximum probability of the given observation sequence joint with <em>x<sub>t</sub> = i</em>, considering all possible hidden state paths (from time instant <em>1</em> to <em>t-1</em>). We can compute <em><span>v</span><sub>t</sub><sup>i</sup></em> recursively by evaluating all the <em><span>v</span><sub>t-1</sub><sup>j</sup></em> multiplied by the corresponding transition probabilities <em>p<sub>ji</sub></em> and emission probability <em>P(o<sub>t</sub>|x<sub>i</sub>)</em>, and always picking the maximum overall possible values of <em>j</em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/09735a93-2d90-4f2f-b9fe-125a6626c989.png" style="width:13.33em;height:1.83em;"/></div>
<p>The algorithm is based on a backtracking approach, using a backpointer <em>bp<sub>t</sub><sup>i</sup></em> whose recursive expression is the same as <em><span>v</span><sub>t</sub><sup>i</sup></em>, but with the <em>argmax</em> function instead of <em>max</em>:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/613be74e-74c1-4af3-b1ce-a250c35c1a21.png" style="width:17.17em;height:2.00em;"/></div>
<p>Therefore, <em><span>bp</span><sub>t</sub></em><sup><em>i</em> </sup>represents <span>the partial sequence of hidden states <em>x</em></span><em><sub>1</sub><span>, x</span><sub>2</sub><span>, ..., </span><span>x</span></em><sub><em>t-1</em> </sub>that maximizes<em> <span>v</span><sub>t</sub><sup>i</sup></em>. During the recursion, we add the timesteps one by one, so the previous path could be invalidated by the last observation. That's why we need to backtrack the partial result and replace the sequence built at time <em>t</em> that doesn't maximize <em><span>v</span><sub>t+1</sub><sup>i</sup></em> anymore.</p>
<p>The algorithm is based on the following steps (like in the other cases, the initial and ending states are not emitting):</p>
<ol>
<li>Initialization of a vector <em>V</em> with shape <em>(N + 2, Sequence Length)</em>.</li>
<li>Initialization of a vector <em>BP</em> with shape <em>(N + 2, Sequence Length)</em>.</li>
<li>Initialization of <em>A</em> (transition probability matrix) with shape <em>(N, N)</em>. Each element is <em><span>P(x</span><sub>i</sub><span>|x</span><sub>j</sub></em><span><em>)</em>.</span></li>
<li>Initialization of <em>B</em> with shape <em>(Sequence Length, N)</em>. Each element is <em><span>P(o</span><sub>i</sub><span>|x</span><sub>j</sub></em><span><em>)</em>.</span></li>
<li>For <em>i=1</em> to <em>N</em>:
<ol>
<li>Set <em>V[i<span>, 1</span>]</em> = <em>A[i,<span> 0</span>] </em><span><em>· B[1, i]</em></span></li>
<li><em>BP[i, 1]</em> = Null (or any other value that cannot be interpreted as a state)</li>
</ol>
</li>
<li>For <em>t=1</em> to <em>Sequence Length</em>:
<ol>
<li>For <em>i=1</em> to <em>N</em>:
<ol>
<li>Set <em>V[i, t] = max<sub>j</sub> V[j, t-1] <span>· A[j, i] · B[t, i]</span></em></li>
<li>Set <em>BP[i, t] = arg<span>max</span><sub>j</sub><span> V[j, t-1] </span><span>· A[j, i] · B[t, i]</span></em></li>
</ol>
</li>
</ol>
</li>
<li>Set <em><span>V[x<sub>Endind</sub>, Sequence Length] = max</span><sub>j</sub><span> V[j, Sequence Length] </span></em><span><em>· A[j, x<sub>Endind</sub>]</em>.</span></li>
</ol>
<ol start="8">
<li><span>Set <em>BP</em></span><em><span>[x<sub>Endind</sub>, Sequence Length] = argmax</span><sub>j</sub><span> V[j, Sequence Length] </span></em><span><em>· A[j, x<sub>Endind</sub>]</em>.</span></li>
<li>Reverse <em>BP</em>.</li>
</ol>
<p>The output of the Viterbi algorithm is a tuple with the most likely sequence <em>BP</em>, and the corresponding probabilities <em>V</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding the most likely hidden state sequence with hmmlearn</h1>
                </header>
            
            <article>
                
<p>At this point, we can continue with the previous example, using our model to find the most likely hidden state sequence given a set of possible observations. We can use either the <kbd>decode()</kbd> <span>method</span> or the <kbd>predict()</kbd> <span>method</span>. The first one returns the log probability of the whole sequence and the sequence itself; however, they all use the Viterbi algorithm as a default decoder:</p>
<pre>sequence = np.array([[1], [1], [1], [0], [1], [1], [1], [0], [1], <br/>                     [0], [1], [0], [1], [0], [1], [1], [0], [1], <br/>                     [1], [0], [1], [0], [1], [0], [1], [0], [1], <br/>                     [1], [1], [0], [0], [1], [1], [0], [1], [1]], dtype=np.int32)<br/><br/>lp, hs = hmm_model.decode(sequence)<br/><br/>print(hs)<br/>[0 1 1 0 1 1 1 0 1 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1]<br/><br/>print(lp)<br/>-30.489992468878615</pre>
<p>The sequence is coherent with the transition probability matrix; in fact, it's more likely the persistence of rough weather (<kbd>1</kbd>) than the opposite. As a consequence, the transition from <kbd>1</kbd> to X is less likely than the one from <kbd>0</kbd> to <kbd>1</kbd>. The choice of state is made by selecting the highest probability; however, in some cases, the differences are minimal (in our example, it can happen to have <em>p = [0.49, 0.51]</em>, meaning that there's a high error chance), so it's useful to check the posterior probabilities for all the states in the sequence:</p>
<pre>pp = hmm_model.predict_proba(sequence)<br/>print(pp)<br/><br/>[[  1.00000000e+00   5.05351938e-19]
 [  3.76687160e-05   9.99962331e-01]
 [  1.31242036e-03   9.98687580e-01]
 [  9.60384736e-01   3.96152641e-02]
 [  1.27156616e-03   9.98728434e-01]
 [  3.21353749e-02   9.67864625e-01]
 [  1.23481962e-03   9.98765180e-01]
<br/>...<strong><br/></strong></pre>
<p>In our case, there are a couple of states that have <em>p ∼ [0.495, 0.505]</em>, so even if the output state is <em>1</em> (rough weather), it's also useful to consider a moderate probability to observe good weather. In general, if a sequence is coherent with the transition probability previously learned (or manually input), those cases are not very common. I suggest trying different configurations and observations sequences, and to also assess the probabilities for the <em>strangest</em> situations (like a sequence of zero second). At that point, it's possible to retrain the model and recheck the new evidence has been correctly processed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have introduced Bayesian networks, describing their structure and relations. We have seen how it's possible to build a network to model a probabilistic scenario where some elements can influence the probability of others. We have also described how to obtain the full joint probability using the most common sampling methods, which allow reducing the computational complexity through an approximation.</p>
<p>The most common sampling methods belong to the family of MCMC algorithms, which model the transition probability from a sample to another one as a first-order Markov chain. In particular, the Gibbs sampler is based on the assumption that it's easier to sample from conditional distribution than work directly with the full joint probability. The method is very easy to implement, but it has some performance drawbacks that can be avoided by adopting more complex strategies. The Metropolis-Hastings sampler, instead, works with a candidate-generating distribution and a criterion to accept or reject the samples. Both methods satisfy the detailed balance equation, which guarantees the convergence (the underlying Markov chain will reach the unique stationary distribution).</p>
<p>In the last part of the chapter, we introduced HMMs, which allow modeling time sequences based on observations corresponding to a series of hidden states. The main concept of such models, in fact, is the presence of unobservable states that condition the emission of a particular observation (which is observable). We have discussed the main assumptions and how to build, train, and infer from a model. In particular, the Forward-Backward algorithm can be employed when it's necessary to learn the transition probability matrix and the emission probabilities, while the Viterbi algorithm is adopted to find the most likely hidden state sequence given a set of consecutive observations.</p>
<p>In the next chapter, <a href="8d541a43-8790-4a91-a79b-e48496f75d90.xhtml" target="_blank">Chapter 5</a>, <em>EM Algorithm and Applications</em>, we're going to briefly discuss the Expectation-Maximization algorithm, focusing on some important applications based on the <strong>Maximum Likelihood Estimation</strong> (<strong>MLE</strong>) approach.</p>


            </article>

            
        </section>
    </body></html>