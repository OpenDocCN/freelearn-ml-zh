<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer141">
			<h1 id="_idParaDest-138"><a id="_idTextAnchor142"/><em class="italic">Chapter 6</em>: Improving Your Neural Network</h1>
			<p>In <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, we designed a network that is able to achieve almost 93% accuracy in the training dataset, but that translated to less than 66% accuracy in the validation dataset.</p>
			<p>In this chapter, we will continue working on that neural network, with the aim to improve the validation accuracy significantly. Our goal is to reach at least 80% validation accuracy. We will apply some of the knowledge acquired in <a href="B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118"><em class="italic">Chapter 5</em></a>, <em class="italic">Deep Learning Workflow</em>, and we will also learn new techniques that will help us very much, such as batch normalization.</p>
			<p>We will cover the following topics:</p>
			<ul>
				<li>Reducing the number of parameters</li>
				<li>Increasing the size of the network and the number of layers</li>
				<li>Understanding batch normalization</li>
				<li>Improving validation with early stopping</li>
				<li>Virtually increasing the dataset size with data augmentation</li>
				<li>Improving validation accuracy with dropout</li>
				<li>Improving validation accuracy with spatial dropout</li>
			</ul>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor143"/>Technical requirements</h1>
			<p>The full source code for the chapter can be found here: <a href="https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter6">https://github.com/PacktPublishing/Hands-On-Vision-and-Behavior-for-Self-Driving-Cars/tree/master/Chapter6</a></p>
			<p>This chapter requires the following software prerequisites, and a basic knowledge of the following would help to understand the chapter better:</p>
			<ul>
				<li>Python 3.7</li>
				<li>The NumPy module</li>
				<li>The Matplotlib module</li>
				<li>The TensorFlow module</li>
				<li>The Keras module</li>
				<li>The OpenCV-Python module</li>
				<li>A recommended GPU</li>
			</ul>
			<p>The Code in Action videos for this chapter can be found here:</p>
			<p><a href="https://bit.ly/3dGIdJA">https://bit.ly/3dGIdJA</a></p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor144"/>A bigger model</h1>
			<p>Training your <a id="_idIndexMarker398"/>own neural network is an art; you need intuition, some luck, a lot of patience, and all the knowledge and help that you can find. You will also need money and time to either buy a faster GPU, use clusters to test more configurations, or pay to get a better dataset.</p>
			<p>But there are no real recipes. That said, we will divide our journey into two phases, as explained in <a href="B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118"><em class="italic">Chapter 5</em></a>, <em class="italic">Deep Learning Workflow</em>:</p>
			<ul>
				<li>Overfitting the training dataset</li>
				<li>Improving generalization</li>
			</ul>
			<p>We will <a id="_idIndexMarker399"/>start from where we left off in <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, with our basic model reaching 66% validation accuracy on CIFAR-10, and then we will improve it significantly, first to make it faster, and then to make it more precise.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor145"/>The starting point</h2>
			<p>The following is the model that we developed in <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, a model that <a id="_idIndexMarker400"/>overfits the dataset because it achieves a high training accuracy value at relatively low validation accuracy:</p>
			<p class="source-code">model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',    input_shape=x_train.shape[1:]))</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p class="source-code">model.add(Conv2D(filters=256, kernel_size=(3, 3),    activation='relu'))</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p class="source-code"> </p>
			<p class="source-code">model.add(Flatten())</p>
			<p class="source-code">model.add(Dense(units=512, activation='relu'))</p>
			<p class="source-code">model.add(Dense(units=256, activation='relu'))</p>
			<p class="source-code">model.add(Dense(units=num_classes, activation = 'softmax'))</p>
			<p>It is a shallow but relatively big model as it has the following number of parameters:</p>
			<p class="source-code">Total params: 5,002,506</p>
			<p>We previously trained it for 12 epochs, with these results:</p>
			<p class="source-code">Training time: 645.9990749359131</p>
			<p class="source-code">Min Loss: 0.12497963292273692</p>
			<p class="source-code">Min Validation Loss: 0.9336215916395187</p>
			<p class="source-code">Max Accuracy: 0.95826</p>
			<p class="source-code">Max Validation Accuracy: 0.6966000199317932</p>
			<p>The training accuracy is actually good enough for us (here, in this run, it is higher than in <a href="B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118"><em class="italic">Chapter 5</em></a>, <em class="italic">Deep Learning Workflow</em>, mostly because of randomness), but the validation accuracy is low too. It is overfitting. So, we could even keep it as a starting point, but it would be nice to tune it a bit and see whether we can do better or make it faster.</p>
			<p>We should also keep an eye on five epochs, as we might do some tests on fewer epochs, to speed up the whole process:</p>
			<p class="source-code">52s 1ms/step - loss: 0.5393 - accuracy: 0.8093 - val_loss: 0.9496 - val_accuracy: 0.6949 </p>
			<p>When you use fewer epochs, you are betting on being able to understand the evolution of the curves, so you are trading speed of development for accuracy of your choices. Sometimes, this is fine, but sometimes it is not.</p>
			<p>Our model is too big, so we will start reducing its size and speeding up the training a bit.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor146"/>Improving the speed</h2>
			<p>Our model <a id="_idIndexMarker401"/>is not just very big – in fact, it is too big. The second convolutional layer has 256 filters and, combined with the 512 neurons of the dense layer, they use a high number of parameters. We can do better. We know that we could split them into layers of 128 filters, and this would save almost half of the parameters, as the dense layer now needs half of the connections.</p>
			<p>We can try that. We learned in <a href="B16322_04_Final_NM_ePUB.xhtml#_idTextAnchor091"><em class="italic">Chapter 4</em></a>, <em class="italic">Deep Learning with Neural Networks</em>, that to not lose resolution after a convolution, we can use padding in the <em class="italic">same</em> way on both the layers (dense layers omitted), as follows:</p>
			<p class="source-code">model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',    input_shape=x_train.shape[1:]))</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p class="source-code"> </p>
			<p class="source-code">model.add(Conv2D(filters=128, kernel_size=(3, 3),     activation='relu', padding="same"))</p>
			<p class="source-code">model.add(Conv2D(filters=128, kernel_size=(3, 3),     activation='relu', padding="same"))</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p>Here, we can see that the number of parameters now is lower:</p>
			<p class="source-code">Total params: 3,568,906</p>
			<p>Let's check the full results:</p>
			<p class="source-code">Training time: 567.7167596817017</p>
			<p class="source-code">Min Loss: 0.1018450417491654</p>
			<p class="source-code">Min Validation Loss: 0.8735350118398666</p>
			<p class="source-code">Max Accuracy: 0.96568</p>
			<p class="source-code">Max Validation Accuracy: 0.7249000072479248</p>
			<p>Nice! It is faster, the accuracy went up slightly, and also, the validation improved!</p>
			<p>Let's do <a id="_idIndexMarker402"/>the same on the first layer, but this time without increasing the resolution so as not to increase the parameters, since, between two convolutional layers, the gain is lower:</p>
			<p class="source-code">model.add(Conv2D(filters=32, kernel_size=(3, 3),     activation='relu', input_shape=x_train.shape[1:]))model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu',    input_shape=x_train.shape[1:], padding="same"))model.add(AveragePooling2D())model.add(Conv2D(filters=128, kernel_size=(3, 3),     activation='relu', padding="same"))model.add(Conv2D(filters=128, kernel_size=(3, 3),     activation='relu', padding="same"))model.add(AveragePooling2D())</p>
			<p>When we try this, we get these results:</p>
			<p class="source-code">Training time: 584.955037355423</p>
			<p class="source-code">Min Loss: 0.10728564778155182</p>
			<p class="source-code">Min Validation Loss: 0.7890052844524383</p>
			<p class="source-code">Max Accuracy: 0.965</p>
			<p class="source-code">Max Validation Accuracy: 0.739300012588501</p>
			<p>This is similar to before, though the validation accuracy improved slightly.</p>
			<p>Next, we will add more layers.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor147"/>Increasing the depth</h2>
			<p>The previous model is actually an excellent starting point. </p>
			<p>But we <a id="_idIndexMarker403"/>will add more layers, to increase the number of non-linear activations and to be able to learn more complex functions. This is the model (dense layers omitted):</p>
			<p class="source-code">model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:], padding="same"))model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:], padding="same"))model.add(AveragePooling2D())model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding="same"))model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding="same"))model.add(AveragePooling2D())model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding="same"))model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu', padding="same"))model.add(AveragePooling2D())</p>
			<p>This is the result:</p>
			<p class="source-code">Training time: 741.1498856544495</p>
			<p class="source-code">Min Loss: 0.22022022939510644</p>
			<p class="source-code">Min Validation Loss: 0.7586277635633946</p>
			<p class="source-code">Max Accuracy: 0.92434</p>
			<p class="source-code">Max Validation Accuracy: 0.7630000114440918</p>
			<p>The network <a id="_idIndexMarker404"/>is now significantly slower and the accuracy went down (maybe because more epochs are required), but the validation accuracy improved.</p>
			<p>Let's try now to reduce the dense layers, as follows (convolutional layers omitted):</p>
			<p class="source-code">model.add(Flatten())model.add(Dense(units=256, activation='relu'))model.add(Dense(units=128, activation='relu'))model.add(Dense(units=num_classes, activation = 'softmax'))</p>
			<p>Now, we have fewer parameters:</p>
			<p class="source-code">Total params: 2,162,986</p>
			<p>But something very bad happened:</p>
			<p class="source-code">Training time: 670.0584089756012</p>
			<p class="source-code">Min Loss: 2.3028031995391847</p>
			<p class="source-code">Min Validation Loss: 2.302628245162964</p>
			<p class="source-code">Max Accuracy: 0.09902</p>
			<p class="source-code">Max Validation Accuracy: 0.10000000149011612</p>
			<p>Both the validations dropped! In fact, they are now 10%, or if you prefer, the network is now producing a random result—it did not learn!</p>
			<p>You might conclude that we broke it. In fact, that is not the case. It's enough to run it again, using the randomness to our advantage, and our network learns as expected:</p>
			<p class="source-code">Training time: 686.5172057151794</p>
			<p class="source-code">Min Loss: 0.24410496438018978</p>
			<p class="source-code">Min Validation Loss: 0.7960220139861107</p>
			<p class="source-code">Max Accuracy: 0.91434</p>
			<p class="source-code">Max Validation Accuracy: 0.7454000115394592</p>
			<p>However, this is not a very good sign. This might be due to the increase in layers, as networks with more layers are more difficult to train, due to the fact that the original input can have problems in terms of being propagated to the upper layers.</p>
			<p>Let's check the graph:</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="Images/Figure_6.1_B16322.jpg" alt="Figure 6.1 – Losses and accuracies graph" width="575" height="452"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Losses and accuracies graph</p>
			<p>You can <a id="_idIndexMarker405"/>see that while the training loss (blue line) keeps decreasing, the validation loss (orange line) after some epochs starts to increase. As explained in <a href="B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118"><em class="italic">Chapter 5</em></a><em class="italic">,</em> <em class="italic">Deep Learning Workflow</em>, this means that the model is overfitting. This is not necessarily the best model, but we will continue developing it.</p>
			<p>In the next section, we will simplify this model.</p>
			<h1 id="_idParaDest-144"><a id="_idTextAnchor148"/>A more efficient network</h1>
			<p>Training the <a id="_idIndexMarker406"/>previous model requires 686 seconds on my laptop, and achieves a validation accuracy of 74.5%, and a training accuracy of 91.4%. Ideally, to improve the efficiency, we want to keep accuracy at the same level while reducing the training time.</p>
			<p>Let's check some of the convolutional layers:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="Images/Figure_6.2_B16322.jpg" alt="Figure 6.2 – First convolutional layer, 32 channels" width="1259" height="427"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – First convolutional layer, 32 channels</p>
			<p>We have already seen these activation graphs in <a href="B16322_05_Final_NM_ePUB.xhtml#_idTextAnchor118"><em class="italic">Chapter 5</em></a>, <em class="italic">Deep Learning Workflow</em>, and we know that channels that are black do not achieve a big activation, so they don't contribute much to the result. In practice, it looks like half of the channels are not in use. Let's try to halve the number of channels in every convolutional layer:</p>
			<p class="source-code">model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:], padding="same"))</p>
			<p class="source-code">model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu',    input_shape=x_train.shape[1:], padding="same"))</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p class="source-code"> </p>
			<p class="source-code">model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu',    padding="same"))</p>
			<p class="source-code">model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu',    padding="same"))</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p class="source-code"> </p>
			<p class="source-code">model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',    padding="same"))</p>
			<p class="source-code">model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',    padding="same"))</p>
			<p>This is <a id="_idIndexMarker407"/>what we get as the result:</p>
			<p class="source-code">Total params: 829,146</p>
			<p>As expected, the number of parameters is now much less, and training is much faster:</p>
			<p class="source-code">Training time: 422.8525400161743</p>
			<p class="source-code">Min Loss: 0.27083665314182637</p>
			<p class="source-code">Min Validation Loss: 0.8076118688702584</p>
			<p class="source-code">Max Accuracy: 0.90398</p>
			<p class="source-code">Max Validation Accuracy: 0.7415000200271606</p>
			<p>Here we see that we lost some accuracy also, but not too much:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="Images/Figure_6.3_B16322.jpg" alt="Figure 6.3 – First convolutional layer, 16 channels" width="658" height="428"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – First convolutional layer, 16 channels</p>
			<p>Now it's a bit better. Let's check the second layer:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="Images/Figure_6.4_B16322.jpg" alt="Figure 6.4 – Second convolutional layer, 16 channels" width="643" height="401"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Second convolutional layer, 16 channels</p>
			<p>This is <a id="_idIndexMarker408"/>also better. Let's check the fourth convolutional layer:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="Images/Figure_6.5_B16322.jpg" alt="Figure 6.5 – Fourth convolutional layer, 64 channels" width="1300" height="237"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Fourth convolutional layer, 64 channels</p>
			<p>It seems a bit empty. Let's halve the third and fourth layers:</p>
			<p class="source-code">Total params: 759,962</p>
			<p>We then get these results:</p>
			<p class="source-code">Training time: 376.09818053245544</p>
			<p class="source-code">Min Loss: 0.30105597005218265</p>
			<p class="source-code">Min Validation Loss: 0.8148738072395325</p>
			<p class="source-code">Max Accuracy: 0.89274</p>
			<p class="source-code">Max Validation Accuracy: 0.7391999959945679</p>
			<p>The training accuracy went down, but the validation accuracy is still fine:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="Images/Figure_6.6_B16322.jpg" alt="Figure 6.6 – Fourth convolutional layer, 32 channels" width="1248" height="432"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Fourth convolutional layer, 32 channels</p>
			<p>Let's check the sixth convolutional layer:</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="Images/Figure_6.7_B16322.jpg" alt="Figure 6.7 – Sixth convolutional layer, 128 channels" width="1296" height="151"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Sixth convolutional layer, 128 channels</p>
			<p>It's a bit empty. Let's also halve the last two convolutional layers:</p>
			<p class="source-code">Total params: 368,666</p>
			<p>It is much smaller, with these results:</p>
			<p class="source-code">Training time: 326.9148383140564</p>
			<p class="source-code">Min Loss: 0.296858479853943</p>
			<p class="source-code">Min Validation Loss: 0.7925313812971115</p>
			<p class="source-code">Max Accuracy: 0.89276</p>
			<p class="source-code">Max Validation Accuracy: 0.7425000071525574</p>
			<p>It still <a id="_idIndexMarker409"/>looks good. Let's check the activations:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="Images/Figure_6.8_B16322.jpg" alt="Figure 6.8 – Sixth convolutional layer, 64 channels" width="1292" height="230"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Sixth convolutional layer, 64 channels</p>
			<p>You can see that now, many channels are activated, which hopefully is an indication that the neural network is better at taking advantage of its resources.</p>
			<p>Comparing this model with the one built in the previous section, you can see that this model can be trained in a bit less than half the time, the validation accuracy is almost unchanged, and the training accuracy decreased a bit, but not much. So, it is indeed more efficient.</p>
			<p>In the next section, we will discuss batch normalization, a layer very common on modern neural networks.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor149"/>Building a smarter network with batch normalization</h1>
			<p>We normalize <a id="_idIndexMarker410"/>the input that we provide to the network, constraining the range from 0 to 1, so it could be beneficial to do that also in the middle of the network. This is called <strong class="bold">batch normalization</strong>, and it does wonders!</p>
			<p>In general, you <a id="_idIndexMarker411"/>should add the batch normalization after the output that you want to <a id="_idIndexMarker412"/>normalize, and before the activation, but adding it after the activation might provide faster performance, and this is what we will do.</p>
			<p>This is the new code (dense layers omitted):</p>
			<p class="source-code">model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu',    input_shape=x_train.shape[1:], padding="same"))model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu',    input_shape=x_train.shape[1:], padding="same"))model.add(BatchNormalization())model.add(AveragePooling2D())model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu',    padding="same"))model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu',    padding="same"))model.add(BatchNormalization())model.add(AveragePooling2D())model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',    padding="same"))model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu',    padding="same"))model.add(BatchNormalization())model.add(AveragePooling2D())</p>
			<p>The number of parameters increased just a bit:</p>
			<p class="source-code">Total params: 369,114</p>
			<p>This is the result:</p>
			<p class="source-code">Training time: 518.0608556270599</p>
			<p class="source-code">Min Loss: 0.1616916553277429</p>
			<p class="source-code">Min Validation Loss: 0.7272815862298012</p>
			<p class="source-code">Max Accuracy: 0.94308</p>
			<p class="source-code">Max Validation Accuracy: 0.7675999999046326</p>
			<p>Not bad, even <a id="_idIndexMarker413"/>if unfortunately, now it is much slower. But we <a id="_idIndexMarker414"/>can add even more batch normalization, to see whether this improves the situation:</p>
			<p class="source-code">Training time: 698.9837136268616</p>
			<p class="source-code">Min Loss: 0.13732857785719446</p>
			<p class="source-code">Min Validation Loss: 0.6836542286396027</p>
			<p class="source-code">Max Accuracy: 0.95206</p>
			<p class="source-code">Max Validation Accuracy: 0.7918999791145325</p>
			<p>Yes, both the accuracies improved. We are actually very close to our initial goal of having 80% accuracy. But let's go further and see what we can do.</p>
			<p>Until now, we only used ReLU activation, but even if it is used very much, it's not the only one. Keras supports a variety of activations, and sometimes it is worth experimenting with. We will stick to ReLU.</p>
			<p>Let's check some activations:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="Images/Figure_6.9_B16322.jpg" alt="Figure 6.9 – Second convolutional layer, 16 channels, batch normalization" width="651" height="429"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – Second convolutional layer, 16 channels, batch normalization</p>
			<p>Now, all the channels of the second layer are learning. Very good!</p>
			<p>Here is the result of the fourth layer:</p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="Images/Figure_6.10_B16322.jpg" alt="Figure 6.10 – Fourth convolutional layer, 32 channels, batch normalization" width="1246" height="432"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10 – Fourth convolutional layer, 32 channels, batch normalization</p>
			<p>Here is <a id="_idIndexMarker415"/>the result of the sixth layer:</p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="Images/Figure_6.11_B16322.jpg" alt="Figure 6.11 – Sixth convolutional layer, 64 channels, batch normalization" width="1290" height="239"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.11 – Sixth convolutional layer, 64 channels, batch normalization</p>
			<p>It is <a id="_idIndexMarker416"/>starting to look good!</p>
			<p>Let's try to visualize the effect of batch normalization on the activations of the first layer before and after batch normalization:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="Images/Figure_6.12_B16322.jpg" alt="Figure 6.12 – First convolutional layer, 16 channels, before and after batch normalization" width="723" height="237"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.12 – First convolutional layer, 16 channels, before and after batch normalization</p>
			<p>You can see that the intensity of the channels is now more uniform; there are no longer channels without activity and channels with very strong activations. However, the channels that are inactive are still without real information.</p>
			<p>Let's also check the second layer:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="Images/Figure_6.13_B16322.jpg" alt="Figure 6.13 – Second convolutional layer, 16 channels, before and after batch normalization" width="782" height="258"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.13 – Second convolutional layer, 16 channels, before and after batch normalization</p>
			<p>Here, it is <a id="_idIndexMarker417"/>maybe less evident, but you can still see that the <a id="_idIndexMarker418"/>difference between the channels is reduced, as clearly, they have been normalized. Intuitively, this helps propagate weaker signals through layers, and it has some regularization effect, which results in better validation accuracy.</p>
			<p>Now that we have talked about batch normalization, it is time to discuss further what a batch is and what implications the size of a batch has.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor150"/>Choosing the right batch size</h2>
			<p>During training, we have a high number of samples, typically from thousands to millions. If you remember, the optimizer will <a id="_idIndexMarker419"/>compute the loss function and update the hyperparameters to try to reduce the loss. It could do this after every sample, but the result could be noisy, with continuous changes that can slow down the training. At the opposite end, the optimizer could update the hyperparameters only once per epoch – for example, using the average of the gradients – but this typically leads to bad generalization. Usually, there is a range on the batch size that performs better than these two extremes, but unfortunately, it depends on the specific neural network.</p>
			<p>A larger batch size can marginally improve the training time on a GPU, but if your model is big, you might find that the memory of your GPU is a limit to how big your batch size can be.</p>
			<p>Batch normalization is also affected by the size of the batch, as small batches reduce its effectiveness (as there is not enough data do to a proper normalization).</p>
			<p>Given these considerations, the best thing to do is to just try. Normally, you can try using 16, 32, 64, and 128, and eventually extend the range if you see that the best value is at the limit of the range.</p>
			<p>As we have seen, the best batch size can improve accuracy and possibly increase the speed, but there is another technique that can help us to improve the validation accuracy while speeding up, or at least simplifying, the training: early stopping.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor151"/>Early stopping</h1>
			<p>When should <a id="_idIndexMarker420"/>we stop training? That's a good question! Ideally, you want to stop at the minimum validation error. While you cannot know this in advance, you can check the losses and get an idea of how many epochs you need. However, when you train your network, sometimes you need more epochs depending on how you tune your model, and it is not simple to know in advance when to stop.</p>
			<p>We already know that we can use <strong class="source-inline">ModelCheckpoint</strong>, a callback of Keras, to save the model with the best validation error seen during training.</p>
			<p>But there is also another very useful callback, <strong class="source-inline">EarlyStopping</strong>, which stops the training when a predefined set of conditions happen:</p>
			<p class="source-code">stop = EarlyStopping(min_delta=0.0005, patience=7, verbose=1)</p>
			<p>The most important parameters to configure early stopping are the following:</p>
			<ul>
				<li><strong class="source-inline">monitor</strong>: This decides which parameter to monitor, by default: validation loss.</li>
				<li><strong class="source-inline">min_delta</strong>: If the difference in validation loss between epochs is below this value, the loss is considered to not have changed.</li>
				<li><strong class="source-inline">patience</strong>: This is the number of epochs with no validation improvement to allow before stopping the training.</li>
				<li><strong class="source-inline">verbose</strong>: This is instructing Keras to provide more information.</li>
			</ul>
			<p>The reason <a id="_idIndexMarker421"/>why we need early stopping is that with data augmentation and dropout, we will need many more epochs, and instead of guessing when it is time to stop, we will use early stopping to do that for us. </p>
			<p>Let's talk now about data augmentation.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor152"/>Improving the dataset with data augmentation</h1>
			<p>It's time <a id="_idIndexMarker422"/>to use data augmentation, and basically, increase the size of our dataset. </p>
			<p>From this <a id="_idIndexMarker423"/>moment, we will no longer care about the accuracy of the training dataset, as this technique will reduce it, but we will focus on the validation accuracy, which is expected to improve.</p>
			<p>We also expect to need more epochs, as our dataset is now more difficult, so we will now set the epochs to <strong class="source-inline">500</strong> (though we don't plan to reach it) and use <strong class="source-inline">EarlyStopping</strong> with a patience of <strong class="source-inline">7</strong>.</p>
			<p>Let's try with this augmentation:</p>
			<p class="source-code">ImageDataGenerator(rotation_range=15, width_shift_range=[-5, 0, 5],    horizontal_flip=True)</p>
			<p>You should take care not to overdo things because the network might learn a dataset too different from validation, and in this case, you will see the validation accuracy stuck at 10%.</p>
			<p>This is the result:</p>
			<p class="source-code">Epoch 00031: val_loss did not improve from 0.48613</p>
			<p class="source-code">Epoch 00031: early stopping</p>
			<p class="source-code">Training time: 1951.4751739501953</p>
			<p class="source-code">Min Loss: 0.3638068118467927</p>
			<p class="source-code">Min Validation Loss: 0.48612626193910835</p>
			<p class="source-code">Max Accuracy: 0.87454</p>
			<p class="source-code">Max Validation Accuracy: 0.8460999727249146</p>
			<p>Early stopping interrupted the training after <strong class="source-inline">31</strong> epochs, and we reached a validation accuracy of above 84% –not bad. As expected, we need many more epochs now. This is a graph of the losses:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="Images/Figure_6.14_B16322.jpg" alt="Figure 6.14 – Losses with data augmentation and early stopping" width="562" height="455"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.14 – Losses with data augmentation and early stopping</p>
			<p>You can <a id="_idIndexMarker424"/>see how the training accuracy kept increasing, while <a id="_idIndexMarker425"/>the validation accuracy at some point decreased. The network is still overfitting a bit.</p>
			<p>Let's check the activations of the first convolutional layer:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="Images/Figure_6.15_B16322.jpg" alt="Figure 6.15 – First convolutional layer, 16 channels, with data augmentation and early stopping" width="650" height="417"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.15 – First convolutional layer, 16 channels, with data augmentation and early stopping</p>
			<p>It's marginally better, but it can probably improve again.</p>
			<p>We can try to augment the data a little more:</p>
			<p class="source-code">ImageDataGenerator(rotation_range=15, width_shift_range=[-8, -4, 0,    4, 8], horizontal_flip=True, height_shift_range=[-5, 0, 5],    zoom_range=[0.9, 1.1])</p>
			<p>This is the result:</p>
			<p class="source-code">Epoch 00040: early stopping</p>
			<p class="source-code">Training time: 2923.3936190605164</p>
			<p class="source-code">Min Loss: 0.5091392234659194</p>
			<p class="source-code">Min Validation Loss: 0.5033097203373909</p>
			<p class="source-code">Max Accuracy: 0.8243</p>
			<p class="source-code">Max Validation Accuracy: 0.8331999778747559</p>
			<p>This <a id="_idIndexMarker426"/>model is slower and less accurate. Let's see the graph:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="Images/Figure_6.16_B16322.jpg" alt="Figure 6.16 – Losses with more data augmentation and early stopping" width="578" height="444"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.16 – Losses with more data augmentation and early stopping</p>
			<p>Maybe it <a id="_idIndexMarker427"/>needs more patience. We will stick to the previous data augmentation then.</p>
			<p>In the next section, we will analyze a simple but effective way to increase the validation accuracy using the dropout layer.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor153"/>Improving the validation accuracy with dropout</h1>
			<p>A source of <a id="_idIndexMarker428"/>overfitting is the fact that the neural network relies more on some neurons to draw its conclusions, and if those neurons are wrong, the network is wrong. One way to <a id="_idIndexMarker429"/>reduce this problem is simply to randomly shut down some neurons during training while keeping them working normally during inference. In this way, the neural network <a id="_idIndexMarker430"/>learns to be more resistant to errors and to generalize better. This mechanism is called <strong class="bold">dropout</strong>, and obviously, Keras supports it. Dropout increases the training time, as the network needs more epochs to converge. It might also require a bigger network, as some neurons are randomly deactivated during training. It is also more useful when the dataset is not very big for the network, as it is more likely to overfit. In practice, as dropout is meant to reduce overfitting, it brings little benefit if your network is not overfitting.</p>
			<p>A typical value of dropout for dense layers is 0.5, though we might use a bit less, as our model is not overfitting much. We will also increase the <em class="italic">patience</em> to <strong class="source-inline">20</strong>, as the model needs more epochs to be trained now and the validation loss can fluctuate for more time.</p>
			<p>Let's try to add some dropout to the dense layers:</p>
			<p class="source-code">model.add(Flatten())</p>
			<p class="source-code">model.add(Dense(units=256, activation='relu'))</p>
			<p class="source-code">model.add(Dropout(0.4))</p>
			<p class="source-code">model.add(Dense(units=128, activation='relu'))</p>
			<p class="source-code">model.add(Dropout(0.2))</p>
			<p class="source-code">model.add(Dense(units=num_classes, activation = 'softmax'))</p>
			<p>This is the result:</p>
			<p class="source-code">Epoch 00097: early stopping</p>
			<p class="source-code">Training time: 6541.777503728867</p>
			<p class="source-code">Min Loss: 0.38114651718586684</p>
			<p class="source-code">Min Validation Loss: 0.44884318161308767</p>
			<p class="source-code">Max Accuracy: 0.87218</p>
			<p class="source-code">Max Validation Accuracy: 0.8585000038146973</p>
			<p>A bit disappointing. It took a lot of time to train, with small gains. We assume that our dense layers are a bit small.</p>
			<p>Let's increase <a id="_idIndexMarker431"/>the size of the layers by 50%, and we will also <a id="_idIndexMarker432"/>increase the dropout of the first dense layer while reducing the dropout of the second one:</p>
			<p class="source-code">model.add(Flatten())model.add(Dense(units=384, activation='relu'))model.add(Dropout(0.5))model.add(Dense(units=192, activation='relu'))model.add(Dropout(0.1))model.add(Dense(units=num_classes, activation='softmax'))</p>
			<p>It is, of course, bigger, as we can see here:</p>
			<p class="source-code">Total params: 542,426</p>
			<p>It has a slightly better result:</p>
			<p class="source-code">Epoch 00122: early stopping</p>
			<p class="source-code">Training time: 8456.040931940079</p>
			<p class="source-code">Min Loss: 0.3601766444931924</p>
			<p class="source-code">Min Validation Loss: 0.4270844452492893</p>
			<p class="source-code">Max Accuracy: 0.87942</p>
			<p class="source-code">Max Validation Accuracy: 0.864799976348877</p>
			<p>As we improve the validation accuracy, even small gains are difficult to achieve.</p>
			<p>Let's check the graph:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="Images/Figure_6.17_B16322.jpg" alt="Figure 6.17 – Losses with more data augmentation and dropout on dense layers" width="572" height="444"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.17 – Losses with more data augmentation and dropout on dense layers</p>
			<p>There is <a id="_idIndexMarker433"/>a bit of overfitting, so let's try to fix it. We can also <a id="_idIndexMarker434"/>use <strong class="source-inline">Dropout</strong> in the convolutional layers.</p>
			<p>Let's try this:</p>
			<p class="source-code">model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:],    padding="same"))</p>
			<p class="source-code">model.add(BatchNormalization())</p>
			<p class="source-code">model.add(Dropout(0.5))</p>
			<p class="source-code">model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:],    padding="same"))</p>
			<p class="source-code">model.add(BatchNormalization())</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p class="source-code">model.add(Dropout(0.5))</p>
			<p>This is the disappointing result:</p>
			<p class="source-code">Epoch 00133: early stopping</p>
			<p class="source-code">Training time: 9261.82032418251</p>
			<p class="source-code">Min Loss: 0.6104169194960594</p>
			<p class="source-code">Min Validation Loss: 0.4887285701841116</p>
			<p class="source-code">Max Accuracy: 0.79362</p>
			<p class="source-code">Max Validation Accuracy: 0.8417999744415283</p>
			<p>The network did not improve!</p>
			<p>Here, we see an interesting case—the validation accuracy is significantly higher than the training accuracy. How is that possible?</p>
			<p>Assuming that your split is correct (for example, you don't have a validation set that is too easy or containing images too similar to the training dataset), two factors can create this situation:</p>
			<ul>
				<li>Data augmentation can potentially make the training dataset harder than the validation dataset.</li>
				<li>Dropout is active during the training phase and deactivated during the prediction phase, so this means that the training dataset can be significantly harder than the validation dataset.</li>
			</ul>
			<p>In our case, the culprit is the dropout. You don't necessarily need to avoid this situation, if it is justified, but in our case, the validation accuracy went down, so we need to fix our dropout, or maybe increase the size of the network.</p>
			<p>I find <strong class="source-inline">Dropout</strong> more difficult to use with the convolutional layers, and I would personally not use a big dropout in that case. Here, there are some guidelines:</p>
			<ul>
				<li>No batch normalization right after <strong class="source-inline">Dropout</strong>, as normalization would suffer.</li>
				<li><strong class="source-inline">Dropout</strong> is more effective after <strong class="source-inline">MaxPooling</strong> than before.</li>
				<li><strong class="source-inline">Dropout</strong> after a convolutional layer drops single pixels, but <strong class="source-inline">SpatialDropout2D</strong> drops channels, and it is recommended on the first few layers at the beginning of the neural network.</li>
			</ul>
			<p>I ran <a id="_idIndexMarker435"/>another few (long!) experiments and I decided to increase the size of the convolutional layers, reduce the dropout, and use <strong class="source-inline">Spatial Dropout</strong> in a couple of layers. I ended <a id="_idIndexMarker436"/>up with this neural network, which is what I consider my final version.</p>
			<p>This is the code of the convolutional layers:</p>
			<p class="source-code">model = Sequential()</p>
			<p class="source-code">model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:], padding="same"))</p>
			<p class="source-code">model.add(BatchNormalization())</p>
			<p class="source-code">model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=x_train.shape[1:], padding="same"))</p>
			<p class="source-code">model.add(BatchNormalization())</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p class="source-code">model.add(SpatialDropout2D(0.2))</p>
			<p class="source-code"> </p>
			<p class="source-code">model.add(Conv2D(filters=48, kernel_size=(3, 3), activation='relu',    padding="same"))</p>
			<p class="source-code">model.add(BatchNormalization())</p>
			<p class="source-code">model.add(Conv2D(filters=48, kernel_size=(3, 3), activation='relu',    padding="same"))</p>
			<p class="source-code">model.add(BatchNormalization())</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p class="source-code">model.add(SpatialDropout2D(0.2))</p>
			<p class="source-code"> </p>
			<p class="source-code">model.add(Conv2D(filters=72, kernel_size=(3, 3), activation='relu',    padding="same"))</p>
			<p class="source-code">model.add(BatchNormalization())</p>
			<p class="source-code">model.add(Conv2D(filters=72, kernel_size=(3, 3), activation='relu',    padding="same"))</p>
			<p class="source-code">model.add(BatchNormalization())</p>
			<p class="source-code">model.add(AveragePooling2D())</p>
			<p class="source-code">model.add(Dropout(0.1))</p>
			<p class="source-code"> </p>
			<p class="source-code">And this is the part with the dense layers:model.add(Flatten())</p>
			<p class="source-code">model.add(Dense(units=384, activation='relu'))</p>
			<p class="source-code">model.add(Dropout(0.5))</p>
			<p class="source-code">model.add(Dense(units=192, activation='relu'))</p>
			<p class="source-code">model.add(Dropout(0.1))</p>
			<p class="source-code">model.add(Dense(units=num_classes, activation='softmax'))</p>
			<p>These <a id="_idIndexMarker437"/>are the results:</p>
			<p class="source-code">Epoch 00168: early stopping</p>
			<p class="source-code">Training time: 13122.931826591492</p>
			<p class="source-code">Min Loss: 0.4703261657243967</p>
			<p class="source-code">Min Validation Loss: 0.3803714614287019</p>
			<p class="source-code">Max Accuracy: 0.84324</p>
			<p class="source-code">Max Validation Accuracy: 0.8779000043869019</p>
			<p>There<a id="_idIndexMarker438"/> is an improvement in the validation accuracy:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="Images/Figure_6.18_B16322.jpg" alt="Figure 6.18 – Losses with more data augmentation and dropout on dense and convolutional layers" width="579" height="449"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.18 – Losses with more data augmentation and dropout on dense and convolutional layers</p>
			<p>Congratulations! Now you <a id="_idIndexMarker439"/>have an idea of how to train a neural <a id="_idIndexMarker440"/>network, and feel free to experiment and go crazy! Every task is different, and the possibilities are really endless.</p>
			<p>For fun, let's train it again and to see how the same logical model performs on MNIST. </p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor154"/>Applying the model to MNIST</h2>
			<p>Our previous <a id="_idIndexMarker441"/>MNIST model achieved 98.3% validation accuracy and, as you might have noticed, the closer you get to 100%, the more difficult it is to improve the model.</p>
			<p>Our CIFAR-10 is trained on a different task than MNIST, but let's see how it performs:</p>
			<p class="source-code">Epoch 00077: early stopping</p>
			<p class="source-code">Training time: 7110.028198957443</p>
			<p class="source-code">Min Loss: 0.04797766085289389</p>
			<p class="source-code">Min Validation Loss: 0.02718053938352254</p>
			<p class="source-code">Max Accuracy: 0.98681664</p>
			<p class="source-code">Max Validation Accuracy: 0.9919000267982483</p>
			<p>Here is the graph for it:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="Images/Figure_6.19_B16322.jpg" alt="Figure 6.19 – MNIST, losses" width="570" height="445"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.19 – MNIST, losses</p>
			<p>I wish <a id="_idIndexMarker442"/>every task was as easy as MNIST!</p>
			<p>Out of curiosity, these are the activations of the first layer:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="Images/Figure_6.20_B16322.jpg" alt="Figure 6.20 – MNIST, activations of the first convolutional level " width="1249" height="432"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.20 – MNIST, activations of the first convolutional level </p>
			<p>As you can see, many channels are activated, and they easily detect the most important features of the numbers.</p>
			<p>This could be an excellent time for you to try the code in GitHub and experiment with it.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor155"/>Now it's your turn!</h2>
			<p>If you <a id="_idIndexMarker443"/>have some time, you should really experiment with a public dataset, or even create your own dataset and train a neural network from scratch.</p>
			<p>If you are out of ideas, you could use CIFAR-100.</p>
			<p>Remember that training a neural network usually is not linear—you might have to guess what can help you, or you might try many different things. And remember to repeat, going back and forth, because while your model evolves, the importance of different techniques and different layers can change.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor156"/>Summary</h1>
			<p>This has been a very practical chapter, showing one way to proceed when training a neural network. We started from a big model, achieving 69.7% validation accuracy, and then we reduced its size and added some layers to increase the number of non-linear activations. We used batch normalization to equalize the contribution of all the channels and then we learned about early stopping, which helped us to decide when to stop the training.</p>
			<p>After learning how to automatically stop the training, we applied it immediately with data augmentation, which increases not only the size of the dataset but also the number of epochs required to properly train the network. We then introduced <strong class="source-inline">Dropout</strong> and <strong class="source-inline">SpatialDropout2D</strong>, a powerful way to reduce overfitting, though not always easy to use.</p>
			<p>We ended up with a network achieving 87.8% accuracy.</p>
			<p>In the next chapter, we will train a neural network that will be able to drive a car on an empty track!</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor157"/>Questions</h1>
			<p>After this chapter, you will be able to answer the following questions:</p>
			<ol>
				<li>Why do we want to use more layers?</li>
				<li>Is a network with more layers automatically slower than a shallower one?</li>
				<li>How do we know how to stop training the model?</li>
				<li>Which Keras function can we use to stop the training before the model starts to overfit?</li>
				<li>How can you normalize the channels?</li>
				<li>How can you effectively make your dataset bigger and more difficult?</li>
				<li>Does dropout make your model more robust?</li>
				<li>If you use data augmentation, would you expect the training to become slower or faster?</li>
				<li>If you use dropout, would you expect the training to become slower or faster?</li>
			</ol>
		</div>
	</div>



  </body></html>