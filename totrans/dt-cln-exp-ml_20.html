<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer254">
<h1 id="_idParaDest-168"><em class="italic"><a id="_idTextAnchor170"/>Chapter 15</em>: Principal Component Analysis</h1>
<p>Dimension reduction is one of the more important concepts/strategies in machine learning. It is sometimes equated with feature selection, but that is too narrow a view of dimension reduction. Our models often have to deal with an excess of features, some of which are capturing the same information. Not addressing the issue substantially increases the risk of overfitting or of unstable results. But dropping some of our features is not the only tool in our toolbox here. Feature <a id="_idIndexMarker1102"/>extraction strategies, such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>), can often yield good results.</p>
<p>We can use PCA to reduce the dimensions (the number of features) of our dataset without losing significant predictive power. The number of principal components necessary to capture most of the variance in the data is typically less than the number of features, often much less.</p>
<p>These components can be used in our regression or classification models rather than the initial features. Not only can this speed up how quickly our model learns, but it may decrease the variance of our estimates. The key disadvantage of this feature extraction strategy is that the new features will usually be more difficult to interpret. PCA is also not a good choice when we have categorical features.</p>
<p>We will develop our understanding of how PCA works by first examining how each component is constructed. We will construct a PCA, interpret the results, and then use those results in a classification model. Finally, we will use kernels to improve PCA when our components might not be linearly separable.</p>
<p>Specifically, we will explore the following topics in this chapter:</p>
<ul>
<li>Key concepts of PCA</li>
<li>Feature extraction with PCA</li>
<li>Using kernels with PCA<a id="_idTextAnchor171"/></li>
</ul>
<h1 id="_idParaDest-169"><a id="_idTextAnchor172"/>Technical requirements</h1>
<p>We will mainly stick to the pandas, NumPy, and scikit-learn libraries in this chapter. All code was tested with scikit-learn versions 0.24.2 and 1.0.2.</p>
<p>The code for this chapter can be downloaded from the GitHub repository: <a href="https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning">https://github.com/PacktPublishing/Data-Cleaning-and-Exploration-with-Machine-Learning</a>.</p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor173"/>Key concepts of PCA</h1>
<p>PCA produces<a id="_idIndexMarker1103"/> multiple linear combinations of features and each linear combination is a component. It identifies a component that captures the largest amount of variance, and a second component that captures the largest amount of remaining variance, and then a third component, and so on until a stopping point we specify is reached. The stopping point can be based on the number of components, the percent of the variation explained, or domain knowledge.</p>
<p>One very useful characteristic of principal components is that they are mutually orthogonal. This means that they are uncorrelated, which is really good news for modeling. <em class="italic">Figure 15.1</em> shows two components constructed from the features x<span class="subscript">1</span> and x<span class="subscript">2</span>. The maximum variance is captured with <em class="italic">PC1</em>, the maximum remaining variance with <em class="italic">PC2</em>. (The data points in the figure are made up.)  Notice that the two vectors are orthogonal (perpendicular).</p>
<div>
<div class="IMG---Figure" id="_idContainer249">
<img alt="Figure 15.1 – An illustration of PCA with two features " height="503" src="image/B17978_15_001.jpg" width="696"/>
</div>
</div>
<p class="figure-caption">Figure 15.1 – An illustration of PCA with two features</p>
<p>Those of you <a id="_idIndexMarker1104"/>who have done a lot of factor analysis probably get the general idea, even if this is your first time exploring PCA. Principal components are not very different from factors, though there are some conceptual and mathematical differences. With PCA, all of the variance is analyzed. Only the shared variance between variables is analyzed with factor analysis. In factor analysis, the unobserved factors are understood as having <em class="italic">caused</em> the observed variables. No assumptions about underlying, unobserved forces need to be made with PCA.</p>
<p>So, how is this bit <a id="_idIndexMarker1105"/>of computational magic done? Principal components can be calculated by following these steps:</p>
<ol>
<li>Standardize your data.</li>
<li>Calculate the covariance matrix of your variables.</li>
<li>Calculate the eigenvectors and eigenvalues of the covariance matrix.</li>
<li>Sort the eigenvectors by eigenvalues in descending order. The first eigenvector is principal component 1, the second is principal component 2, and so on.</li>
</ol>
<p>It is not necessary to completely understand these steps to understand the discussion in the rest of this chapter. We will get scikit-learn to do this work for us. Still, it might improve your intuition if you computed the covariance matrix of a very small subset of data (two or three columns and just a few rows) and then did the eigendecomposition of that matrix. A somewhat easier way to experiment with constructing components, while still being illustrative, is to use the NumPy linear algebra functions (<strong class="source-inline">numpy.linalg</strong>). The key point here is how computationally straightforward it is to derive the principal components.</p>
<p>PCA is used for <a id="_idIndexMarker1106"/>many machine learning tasks. It can be used to resize images, to analyze financial data, or for recommender systems. Essentially, it may be a good choice for any application where there are a large number of features and many of them are correlated.</p>
<p>Some of you no doubt noticed that I slipped in, without remarking on it, that PCA constructs <em class="italic">linear combinations</em> of features. What do we do when linear separability is not feasible, as we encountered with support vector machines? Well, it turns out that the kernel trick that we relied on with support vector machines also works with PCA. We will explore how to implement kernel PCA in this chapter. However, we will start with a relatively straightforward PCA example.</p>
<h1 id="_idParaDest-171"><a id="_idTextAnchor174"/>Feature extraction with PCA</h1>
<p>PCA can be <a id="_idIndexMarker1107"/>used for dimension reduction in preparation for a model we will run subsequently. Although PCA is not, strictly speaking, a feature selection tool, we can run it in pretty much the same way we ran the wrapper feature selection methods in <a href="B17978_05_ePub.xhtml#_idTextAnchor058"><em class="italic">Chapter 5</em></a>, <em class="italic">Feature Selection</em>. After some preprocessing (such as handling outliers), we generate the components, which we can then use as our new features. Sometimes we do not actually use these components in a model. Rather, we generate them mainly to help us visualize our data better.</p>
<p>To illustrate the use of PCA, we will work with data on <strong class="bold">National Basketball Association</strong> (<strong class="bold">NBA</strong>) games. The dataset has statistics from each NBA game from the 2017/2018 season through the 2020/2021 season. This includes the home team; whether the home team won; the visiting team; shooting percentages for visiting and home teams; turnovers, rebounds, and assists by both teams; and a number of other measures.</p>
<p class="callout-heading">Note</p>
<p class="callout">NBA game data is available for download for the public at <a href="https://www.kaggle.com/datasets/wyattowalsh/basketball">https://www.kaggle.com/datasets/wyattowalsh/basketball</a>. This dataset has game data starting with the 1946/1947 NBA season. It uses the <strong class="source-inline">nba_api</strong> to pull stats from nba.com. That API is available at <a href="https://github.com/swar/nba_api">https://github.com/swar/nba_api</a>.</p>
<p>Let’s use<a id="_idIndexMarker1108"/> PCA in a model:</p>
<ol>
<li value="1">We start by loading <a id="_idIndexMarker1109"/>the required libraries. You have seen all of these in the previous chapters except for scikit-learn’s <strong class="source-inline">PCA</strong> module:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import StandardScaler</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">from sklearn.decomposition import PCA</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">from scipy.stats import uniform</p><p class="source-code">from scipy.stats import randint</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + “/helperfunctions”)</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>Next, we load the NBA data and do a little cleaning. A few instances do not have values for whether the home team won or lost, <strong class="source-inline">WL_HOME</strong>, so we remove them. <strong class="source-inline">WL_HOME</strong> will be our target. We will try to model it later, after we have constructed our components. Notice that the home team wins a majority of the time, but the class imbalance is not bad:<p class="source-code">nbagames = pd.read_csv(“data/nbagames2017plus.csv”, parse_dates=[‘GAME_DATE’])</p><p class="source-code">nbagames = nbagames.loc[nbagames.WL_HOME.isin([‘W’,’L’])]</p><p class="source-code">nbagames.shape</p><p class="source-code"><strong class="bold">(4568, 149)</strong></p><p class="source-code">nbagames[‘WL_HOME’] = \</p><p class="source-code">  np.where(nbagames.WL_HOME==’L’,0,1).astype(‘int’)</p><p class="source-code">nbagames.WL_HOME.value_counts(dropna=False)</p><p class="source-code"><strong class="bold">1    2586</strong></p><p class="source-code"><strong class="bold">0    1982</strong></p><p class="source-code"><strong class="bold">Name: WL_HOME, dtype: int64</strong></p></li>
<li>We should <a id="_idIndexMarker1110"/>look at some descriptive statistics:<p class="source-code">num_cols = [‘FG_PCT_HOME’,’FTA_HOME’,’FG3_PCT_HOME’,</p><p class="source-code">  ‘FTM_HOME’,’FT_PCT_HOME’,’OREB_HOME’,’DREB_HOME’,</p><p class="source-code">  ‘REB_HOME’,’AST_HOME’,’STL_HOME’,’BLK_HOME’,</p><p class="source-code">  ‘TOV_HOME’, ‘FG_PCT_AWAY’,’FTA_AWAY’,’FG3_PCT_AWAY’,</p><p class="source-code">  ‘FT_PCT_AWAY’,’OREB_AWAY’,’DREB_AWAY’,’REB_AWAY’,</p><p class="source-code">  ‘AST_AWAY’,’STL_AWAY’,’BLK_AWAY’,’TOV_AWAY’]</p><p class="source-code">nbagames[[‘WL_HOME’] + num_cols].agg([‘count’,’min’,’median’,’max’]).T</p></li>
</ol>
<p>This produces <a id="_idIndexMarker1111"/>the following output. There are no missing values, but our features have very different ranges. We will need to do some scaling:</p>
<p class="source-code"><strong class="bold">                count       min     median    max</strong></p>
<p class="source-code"><strong class="bold">WL_HOME         4,568.00    0.00    1.00      1.00</strong></p>
<p class="source-code"><strong class="bold">FG_PCT_HOME     4,568.00    0.27    0.47      0.65</strong></p>
<p class="source-code"><strong class="bold">FTA_HOME        4,568.00    1.00    22.00     64.00</strong></p>
<p class="source-code"><strong class="bold">FG3_PCT_HOME    4,568.00    0.06    0.36      0.84</strong></p>
<p class="source-code"><strong class="bold">FTM_HOME        4,568.00    1.00    17.00     44.00</strong></p>
<p class="source-code"><strong class="bold">FT_PCT_HOME     4,568.00    0.14    0.78      1.00</strong></p>
<p class="source-code"><strong class="bold">OREB_HOME       4,568.00    1.00    10.00     25.00</strong></p>
<p class="source-code"><strong class="bold">DREB_HOME       </strong><strong class="bold">4,568.00    18.00   35.00     55.00</strong></p>
<p class="source-code"><strong class="bold">REB_HOME        4,568.00    22.00   45.00     70.00</strong></p>
<p class="source-code"><strong class="bold">AST_HOME        4,568.00    10.00   </strong><strong class="bold">24.00     50.00</strong></p>
<p class="source-code"><strong class="bold">STL_HOME        4,568.00    0.00    7.00      22.00</strong></p>
<p class="source-code"><strong class="bold">BLK_HOME        4,568.00    0.00    5.00      20.00</strong></p>
<p class="source-code"><strong class="bold">TOV_HOME        </strong><strong class="bold">4,568.00    1.00    14.00     29.00</strong></p>
<p class="source-code"><strong class="bold">FG_PCT_AWAY     4,568.00    0.28    0.46      0.67</strong></p>
<p class="source-code"><strong class="bold">FTA_AWAY        4,568.00    3.00    </strong><strong class="bold">22.00     54.00</strong></p>
<p class="source-code"><strong class="bold">FG3_PCT_AWAY    4,568.00    0.08    0.36      0.78</strong></p>
<p class="source-code"><strong class="bold">FT_PCT_AWAY     4,568.00    0.26    0.78      1.00</strong></p>
<p class="source-code"><strong class="bold">OREB_AWAY       </strong><strong class="bold">4,568.00    0.00    10.00     26.00</strong></p>
<p class="source-code"><strong class="bold">DREB_AWAY       4,568.00    18.00   34.00     56.00</strong></p>
<p class="source-code"><strong class="bold">REB_AWAY        4,568.00    22.00   </strong><strong class="bold">44.00     71.00</strong></p>
<p class="source-code"><strong class="bold">AST_AWAY        4,568.00    9.00    24.00     46.00</strong></p>
<p class="source-code"><strong class="bold">STL_AWAY        4,568.00    0.00    8.00      19.00</strong></p>
<p class="source-code"><strong class="bold">BLK_AWAY        </strong><strong class="bold">4,568.00    0.00    5.00      15.00</strong></p>
<p class="source-code"><strong class="bold">TOV_AWAY        4,568.00    3.00    14.00     30.00</strong></p>
<ol>
<li value="4">Let’s also examine <a id="_idIndexMarker1112"/>how our features are correlated:<p class="source-code">corrmatrix = nbagames[[‘WL_HOME’] + num_cols].\</p><p class="source-code">  corr(method=”pearson”)</p><p class="source-code">sns.heatmap(corrmatrix, xticklabels=corrmatrix.columns,</p><p class="source-code">  yticklabels=corrmatrix.columns, cmap=”coolwarm”)</p><p class="source-code">plt.title(‘Heat Map of Correlation Matrix’)</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer250">
<img alt="Figure 15.2 – Heat map of NBA features" height="460" src="image/B17978_15_002.jpg" width="608"/>
</div>
</div>
<p class="figure-caption">Figure 15.2 – Heat map of NBA features</p>
<p>A number of <a id="_idIndexMarker1113"/>features are significantly positively or negatively correlated. For example, the field goal (shooting) percentage of the home team (<strong class="source-inline">FG_PCT_HOME</strong>) and three-point field goal percentage of the home team (<strong class="source-inline">FG3_PCT_HOME</strong>) are positively correlated, not surprisingly. Also, rebounds of the home team (<strong class="source-inline">REB_HOME</strong>) and defensive rebounds of the home team (<strong class="source-inline">DREB_HOME</strong>) are likely too closely correlated for any model to disentangle their impact.</p>
<p>This dataset might be a good candidate for PCA. Although some features are highly correlated, we will still lose information by dropping some. PCA at least offers the possibility of dealing with the correlation without losing that information.</p>
<ol>
<li value="5">Now we create training and testing DataFrames:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(nbagames[num_cols],\</p><p class="source-code">  nbagames[[‘WL_HOME’]],test_size=0.2, random_state=0)</p></li>
<li>We are <a id="_idIndexMarker1114"/>ready now to create the components. We, somewhat arbitrarily, indicate that we want seven components. (Later, we will use hyperparameter tuning to choose the number of components.) We set up our pipeline to do some preprocessing before running the PCA:<p class="source-code">pca = PCA(n_components=7)</p><p class="source-code">pipe1 = make_pipeline(OutlierTrans(2),</p><p class="source-code">      SimpleImputer(strategy=”median”),</p><p class="source-code">      StandardScaler(), pca)</p><p class="source-code">pipe1.fit(X_train)</p></li>
<li>We can now use the <strong class="source-inline">components_</strong> attribute of the <strong class="source-inline">pca</strong> object. This returns the scores of all 23 features on each of the seven components:<p class="source-code">components = pd.DataFrame(pipe1[‘pca’].components_,</p><p class="source-code">  columns=num_cols)</p><p class="source-code">components.T.to_excel(‘views/components.xlsx’)</p></li>
</ol>
<p>This produces the following spreadsheet:</p>
<div>
<div class="IMG---Figure" id="_idContainer251">
<img alt="Figure 15.3 – Principal components of NBA features " height="1020" src="image/B17978_15_003.jpg" width="1145"/>
</div>
</div>
<p class="figure-caption">Figure 15.3 – Principal components of NBA features</p>
<p>Each<a id="_idIndexMarker1115"/> feature accounts for some portion of the variance with each component. (If for each component, you square each of the 23 scores and then sum the squares, you get a total of 1.) If you want to understand which features really drive a component, look for the ones with the largest absolute value. For component 1, the field goal percentage of the home team (<strong class="source-inline">FG_PCT_HOME</strong>) is most important, followed by the number of rebounds of the away team (<strong class="source-inline">REB_AWAY</strong>).</p>
<p>Recall from our discussion at the beginning of this chapter that each component attempts to capture the variance that remains after the previous component or components.</p>
<ol>
<li value="8">Let’s show the five most important features for the first three components. The first component seems to be largely about the field goal percentage of the home team <a id="_idIndexMarker1116"/>and the rebounding of each team. The second component does not seem very different, but the third one is driven by free throws made and attempted (<strong class="source-inline">FTM_HOME</strong> and <strong class="source-inline">FTA_HOME</strong>) and turnovers (<strong class="source-inline">TOV_HOME</strong> and <strong class="source-inline">TOV_AWAY</strong>):<p class="source-code">components.pc1.abs().nlargest(5)</p><p class="source-code"><strong class="bold">FG_PCT_HOME    0.38</strong></p><p class="source-code"><strong class="bold">REB_AWAY       0.37</strong></p><p class="source-code"><strong class="bold">DREB_AWAY      0.34</strong></p><p class="source-code"><strong class="bold">REB_HOME       0.33</strong></p><p class="source-code"><strong class="bold">FG_PCT_AWAY    0.30</strong></p><p class="source-code"><strong class="bold">Name: pc1, dtype: float64</strong></p><p class="source-code">components.pc2.abs().nlargest(5)</p><p class="source-code"><strong class="bold">DREB_HOME      0.38</strong></p><p class="source-code"><strong class="bold">FG_PCT_AWAY    0.37</strong></p><p class="source-code"><strong class="bold">DREB_AWAY      0.32</strong></p><p class="source-code"><strong class="bold">REB_HOME       0.32</strong></p><p class="source-code"><strong class="bold">FG_PCT_HOME    0.29</strong></p><p class="source-code"><strong class="bold">Name: pc2, dtype: float64</strong></p><p class="source-code">components.pc3.abs().nlargest(5)</p><p class="source-code"><strong class="bold">FTM_HOME    0.55</strong></p><p class="source-code"><strong class="bold">FTA_HOME    0.53</strong></p><p class="source-code"><strong class="bold">TOV_HOME    0.30</strong></p><p class="source-code"><strong class="bold">STL_AWAY    0.27</strong></p><p class="source-code"><strong class="bold">TOV_AWAY    0.26</strong></p><p class="source-code"><strong class="bold">Name: pc3, dtype: float64</strong></p></li>
<li>We can use the <strong class="source-inline">explained_variance_ratio_</strong> attribute of the <strong class="source-inline">pca</strong> object to examine how much of the variance is captured by each component. The first component explains 14.5% of the variance of the features. The second component explains another 13.4%. If we use NumPy’s <strong class="source-inline">cumsum</strong> method, we can see that the seven components explain about 65% of the variance altogether.</li>
</ol>
<p>So, there<a id="_idIndexMarker1117"/> is still a fair bit of variance out there. We might want to use more components for any model we build:</p>
<p class="source-code">np.set_printoptions(precision=3)</p>
<p class="source-code">pipe1[‘pca’].explained_variance_ratio_</p>
<p class="source-code"><strong class="bold">array([0.145, 0.134, 0.095, 0.086, 0.079, 0.059, 0.054])</strong></p>
<p class="source-code">np.cumsum(pipe1[‘pca’].explained_variance_ratio_)</p>
<p class="source-code"><strong class="bold">array([0.145, 0.279, 0.374, 0.46 , 0.539, 0.598, 0.652])</strong></p>
<ol>
<li value="10">We can plot the first two principal components to see how well they can separate home team wins and losses. We can use the <strong class="source-inline">transform</strong> method of our pipeline to create a DataFrame with the principal components and join that with the DataFrame for the target.</li>
</ol>
<p>We use the handy <strong class="source-inline">hue</strong> attribute of Seaborn’s <strong class="source-inline">scatterplot</strong> to display wins and losses. The first two principal components do an okay job of separating wins and losses, despite together only accounting for about 28% of the variance in our features:</p>
<p class="source-code">X_train_pca = pd.DataFrame(pipe1.transform(X_train),</p>
<p class="source-code">  columns=components.columns, index=X_train.index).join(y_train)</p>
<p class="source-code">sns.scatterplot(x=X_train_pca.pc1, y=X_train_pca.pc2, hue=X_train_pca.WL_HOME)</p>
<p class="source-code">plt.title(“Scatterplot of First and Second Components”)</p>
<p class="source-code">plt.xlabel(“Principal Component 1”)</p>
<p class="source-code">plt.ylabel(“Principal Component 2”)</p>
<p class="source-code">plt.show()</p>
<p>This <a id="_idIndexMarker1118"/>produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer252">
<img alt="Figure 15.4 – Scatterplot of wins and losses by first and second principal components " height="450" src="image/B17978_15_004.jpg" width="564"/>
</div>
</div>
<p class="figure-caption">Figure 15.4 – Scatterplot of wins and losses by first and second principal components</p>
<ol>
<li value="11">Let’s use principal components to predict whether the home team wins. We just add a logistic regression to our pipeline to do that. We also do a grid search to find the <a id="_idIndexMarker1119"/>best hyperparameter values:<p class="source-code">lr = LogisticRegression()</p><p class="source-code">pipe2 = make_pipeline(OutlierTrans(2),</p><p class="source-code">  SimpleImputer(strategy=”median”), StandardScaler(),</p><p class="source-code">  pca, lr)</p><p class="source-code">lr_params = {</p><p class="source-code">  “pca__n_components”: randint(3, 20),</p><p class="source-code">  “logisticregression__C”: uniform(loc=0, scale=10)</p><p class="source-code">}</p><p class="source-code">rs = RandomizedSearchCV(pipe2, lr_params, cv=4, </p><p class="source-code">  n_iter=40, scoring=’accuracy’, random_state=1)</p><p class="source-code">rs.fit(X_train, y_train.values.ravel())</p></li>
<li>We can now look at the best parameters and score. As we suspected from an earlier step, the grid search suggests that our logistic regression model does better with more components. We get a very high score.</li>
</ol>
<p>We discuss the hyperparameter <em class="italic">C</em> in detail in <a href="B17978_10_ePub.xhtml#_idTextAnchor126"><em class="italic">Chapter 10</em></a><em class="italic">, Logistic Regression</em>:</p>
<p class="source-code">rs.best_params_</p>
<p class="source-code"><strong class="bold">{‘logisticregression__C’: 6.865009276815837, ‘pca__n_components’: 19}</strong></p>
<p class="source-code">rs.best_score_</p>
<p class="source-code"><strong class="bold">0.9258345296842831</strong></p>
<p>This<a id="_idIndexMarker1120"/> section demonstrated how we can generate principal components from our dataset and how to interpret those components. We also looked at how to use principal components in a model, rather than the initial features. But we assumed that the principal components can be well described as linear combinations of features. This is often not the case. In the next section, we will use kernel PCA to handle nonlinear relationships.</p>
<h1 id="_idParaDest-172"><a id="_idTextAnchor175"/>Using kernels with PCA</h1>
<p>With <a id="_idIndexMarker1121"/>some data, it is not possible to construct principal components that are linearly separable. This may not actually be easy to visualize in advance of our modeling. Fortunately, there are tools we can use to determine the kernel that will yield the best results, including a linear kernel. Kernel PCA with a linear kernel should perform similarly to standard PCA.</p>
<p>In this section, we will use <a id="_idIndexMarker1122"/>kernel PCA for feature extraction with data on labor force participation rates, educational attainment, teenage birth frequency, and participation in politics by gender at the country level. </p>
<p class="callout-heading">Note</p>
<p class="callout">This dataset on gender-based differences in educational and labor force outcomes is made available for public use by the United Nations Development Program at <a href="https://www.kaggle.com/datasets/undp/human-development">https://www.kaggle.com/datasets/undp/human-development</a>. There is one record per country with aggregate employment, income, and education data by gender for 2015.</p>
<p>Let’s start building the model:</p>
<ol>
<li value="1">We will import the same libraries we have been using plus scikit-learn’s <strong class="source-inline">KernelPCA</strong> module. We <a id="_idIndexMarker1123"/>will also import the <strong class="source-inline">RandomForestRegressor</strong> module:<p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import MinMaxScaler</p><p class="source-code">from sklearn.pipeline import make_pipeline</p><p class="source-code">from sklearn.impute import SimpleImputer</p><p class="source-code">from sklearn.decomposition import KernelPCA</p><p class="source-code">from sklearn.ensemble import RandomForestRegressor</p><p class="source-code">from sklearn.model_selection import RandomizedSearchCV</p><p class="source-code">import seaborn as sns</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">sys.path.append(os.getcwd() + “/helperfunctions”)</p><p class="source-code">from preprocfunc import OutlierTrans</p></li>
<li>We load<a id="_idIndexMarker1124"/> the data on educational and labor force outcomes by gender. We construct series for the ratio of female to male incomes, the years of education ratio, the labor force participation ratio, and the human development index ratio:<p class="source-code">un_income_gap = pd.read_csv(“data/un_income_gap.csv”)</p><p class="source-code">un_income_gap.set_index(‘country’, inplace=True)</p><p class="source-code">un_income_gap[‘incomeratio’] = \</p><p class="source-code">  un_income_gap.femaleincomepercapita / \</p><p class="source-code">    un_income_gap.maleincomepercapita</p><p class="source-code">un_income_gap[‘educratio’] = \</p><p class="source-code">  un_income_gap.femaleyearseducation / \</p><p class="source-code">     un_income_gap.maleyearseducation</p><p class="source-code">un_income_gap[‘laborforcepartratio’] = \</p><p class="source-code">  un_income_gap.femalelaborforceparticipation / \</p><p class="source-code">     un_income_gap.malelaborforceparticipation</p><p class="source-code">un_income_gap[‘humandevratio’] = \</p><p class="source-code">  un_income_gap.femalehumandevelopment / \</p><p class="source-code">     un_income_gap.malehumandevelopment</p><p class="source-code">un_income_gap.dropna(subset=[‘incomeratio’], inplace=True)</p></li>
<li>Let’s look <a id="_idIndexMarker1125"/>at some descriptive <a id="_idIndexMarker1126"/>statistics. There are a few missing values, particularly for <strong class="source-inline">genderinequality</strong> and <strong class="source-inline">humandevratio</strong>. Some features have much larger ranges than others:<p class="source-code">num_cols = [‘educratio’,’laborforcepartratio’,</p><p class="source-code">  ‘humandevratio’,’genderinequality’,</p><p class="source-code">  ‘maternalmortality’,’adolescentbirthrate’,</p><p class="source-code">  ‘femaleperparliament’,’incomepercapita’]</p><p class="source-code">gap_sub = un_income_gap[[‘incomeratio’] + num_cols]</p><p class="source-code">gap_sub.\</p><p class="source-code">  agg([‘count’,’min’,’median’,’max’]).T</p><p class="source-code"><strong class="bold">                       count    min    median  max</strong></p><p class="source-code"><strong class="bold">incomeratio            177.00   0.16   0.60    0.93</strong></p><p class="source-code"><strong class="bold">educratio              169.00   0.24   0.93    1.35</strong></p><p class="source-code"><strong class="bold">laborforcepartratio    177.00   0.19   0.75    1.04</strong></p><p class="source-code"><strong class="bold">humandevratio          161.00   0.60   0.95    1.03</strong></p><p class="source-code"><strong class="bold">genderinequality       155.00   0.02   0.39    0.74</strong></p><p class="source-code"><strong class="bold">maternalmortality     174.00   1.00   60.00  1,100.00</strong></p><p class="source-code"><strong class="bold">adolescentbirthrate    177.00   0.60   40.90  204.80</strong></p><p class="source-code"><strong class="bold">femaleperparliament    174.00   0.00   19.35    57.50</strong></p><p class="source-code"><strong class="bold">incomepercapita        177.00   581.00 10,512.00  123,124.00</strong></p></li>
<li>We <a id="_idIndexMarker1127"/>should also look at some <a id="_idIndexMarker1128"/>correlations:<p class="source-code">corrmatrix = gap_sub.corr(method=”pearson”)</p><p class="source-code">sns.heatmap(corrmatrix, </p><p class="source-code">  xticklabels=corrmatrix.columns,</p><p class="source-code">  yticklabels=corrmatrix.columns, cmap=”coolwarm”)</p><p class="source-code">plt.title(‘Heat Map of Correlation Matrix’)</p><p class="source-code">plt.tight_layout()</p><p class="source-code">plt.show()</p></li>
</ol>
<p>This produces the following plot:</p>
<div>
<div class="IMG---Figure" id="_idContainer253">
<img alt="Figure 15.5 – Correlation matrix of NBA games data " height="463" src="image/B17978_15_005.jpg" width="615"/>
</div>
</div>
<p class="figure-caption">Figure 15.5 – Correlation matrix of NBA games data</p>
<p><strong class="source-inline">humandevratio</strong> and <strong class="source-inline">educratio</strong> are highly correlated, as are <strong class="source-inline">genderinequality</strong> and <strong class="source-inline">adolescentbirthrate</strong>. We can see that <strong class="source-inline">educratio</strong> and <strong class="source-inline">maternalmortality</strong> are highly negatively correlated. It would be hard<a id="_idIndexMarker1129"/> to build a well-performing<a id="_idIndexMarker1130"/> model with all of these features given their high correlation. However, we might be able to reduce dimensions with kernel PCA.</p>
<ol>
<li value="5">We create training and testing DataFrames:<p class="source-code">X_train, X_test, y_train, y_test =  \</p><p class="source-code">  train_test_split(gap_sub[num_cols],\</p><p class="source-code">  gap_sub[[‘incomeratio’]], test_size=0.2,</p><p class="source-code">  random_state=0)</p></li>
<li>We are now ready to instantiate the <strong class="source-inline">KernelPCA</strong> and <strong class="source-inline">RandomForestRegressor</strong> objects. We add both to a pipeline. We also create a dictionary with our hyperparameters for the kernel PCA and the random forest regressor.</li>
</ol>
<p>The dictionary<a id="_idIndexMarker1131"/> has a range of<a id="_idIndexMarker1132"/> hyperparameter values for the number of components, gamma, and the kernel to use with the kernel PCA. For those kernels that do not use gamma, those values are ignored. Notice that one option for kernel is the linear kernel.</p>
<p>We discuss gamma in more detail in <a href="B17978_08_ePub.xhtml#_idTextAnchor106"><em class="italic">Chapter 8</em></a><em class="italic">, Support Vector Regression</em>, and <a href="B17978_13_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 13</em></a><em class="italic">, Support Vector Machine Classification</em>:</p>
<p class="source-code">rfreg = RandomForestRegressor()</p>
<p class="source-code">kpca = KernelPCA()</p>
<p class="source-code">pipe1 = make_pipeline(OutlierTrans(2),</p>
<p class="source-code">  SimpleImputer(strategy=”median”), MinMaxScaler(),</p>
<p class="source-code">  kpca, rfreg)</p>
<p class="source-code">rfreg_params = {</p>
<p class="source-code"> ‘kernelpca__n_components’:</p>
<p class="source-code">    randint(2, 9),</p>
<p class="source-code"> ‘kernelpca__gamma’:</p>
<p class="source-code">     np.linspace(0.03, 0.3, 10),</p>
<p class="source-code"> ‘kernelpca__kernel’:</p>
<p class="source-code">     [‘linear’, ‘poly’, ‘rbf’, </p>
<p class="source-code">      ‘sigmoid’, ‘cosine’],</p>
<p class="source-code"> ‘randomforestregressor__max_depth’:</p>
<p class="source-code">     randint(2, 20),</p>
<p class="source-code"> ‘randomforestregressor__min_samples_leaf’:</p>
<p class="source-code">     randint(5, 11)</p>
<p class="source-code">}</p>
<ol>
<li value="7">Now let’s do <a id="_idIndexMarker1133"/>a randomized grid search with these hyperparameter values. The kernel for the PCA that gives<a id="_idIndexMarker1134"/> us the best performance with the random forest regressor is polynomial. We get a good square for mean squared error, about 10% of the size of the mean:<p class="source-code">rs = RandomizedSearchCV(pipe1, rfreg_params,</p><p class="source-code">  cv=4, n_iter=40,</p><p class="source-code">  scoring=’neg_mean_absolute_error’,</p><p class="source-code">  random_state=1)</p><p class="source-code">rs.fit(X_train, y_train.values.ravel())</p><p class="source-code">rs.best_params_</p><p class="source-code"><strong class="bold">{‘kernelpca__gamma’: 0.12000000000000001,</strong></p><p class="source-code"><strong class="bold"> ‘kernelpca__kernel’: ‘poly’,</strong></p><p class="source-code"><strong class="bold"> ‘kernelpca__n_components’: 4,</strong></p><p class="source-code"><strong class="bold"> ‘randomforestregressor__max_depth’: 18,</strong></p><p class="source-code"><strong class="bold"> ‘randomforestregressor__min_samples_leaf’: 5}</strong></p><p class="source-code">rs.best_score_</p><p class="source-code"><strong class="bold">-0.06630618838886537</strong></p></li>
<li>Let’s look at other top-performing models. A model with an <strong class="source-inline">rbf</strong> kernel and one with a sigmoid<a id="_idIndexMarker1135"/> kernel do nearly as well. The second and third best-performing models have<a id="_idIndexMarker1136"/> more principal components than the best-performing model:<p class="source-code">results = \</p><p class="source-code">  pd.DataFrame(rs.cv_results_[‘mean_test_score’], \</p><p class="source-code">    columns=[‘meanscore’]).\</p><p class="source-code">  join(pd.DataFrame(rs.cv_results_[‘params’])).\</p><p class="source-code">  sort_values([‘meanscore’], ascending=False)</p><p class="source-code">results.iloc[1:3].T</p><p class="source-code"><strong class="bold">                                        39     0 </strong></p><p class="source-code"><strong class="bold">meanscore                              -0.067 -0.070</strong></p><p class="source-code"><strong class="bold">kernelpca__gamma                        0.240  0.180</strong></p><p class="source-code"><strong class="bold">kernelpca__kernel                       rbf    sigmoid</strong></p><p class="source-code"><strong class="bold">kernelpca__n_components                 6      6</strong></p><p class="source-code"><strong class="bold">randomforestregressor__max_depth        12     10</strong></p><p class="source-code"><strong class="bold">randomforestregressor__min_samples_leaf 5      6</strong></p></li>
</ol>
<p>Kernel PCA is a relatively easy-to-implement dimension reduction option. It is most useful when we have a number of highly correlated features that might not be linearly separable, and interpretation of predictions is not important. </p>
<h1 id="_idParaDest-173"><a id="_idTextAnchor176"/>Summary</h1>
<p>This chapter explored principal component analysis, including how it works and when we might want to use it. We learned how to examine the components created from PCA, including how each feature contributes to each component, and how much of the variance is explained. We went over how to visualize components and how to use components in subsequent analysis. We also examined how to use kernels for PCA and when that might give us better results.</p>
<p>We explore another unsupervised learning technique in the next chapter, k-means clustering.</p>
</div>
</div>
</body></html>