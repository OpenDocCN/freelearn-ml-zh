<html><head></head><body>
        <section>

            <header>
                <h1 class="header-title">Loading and Preparing the Dataset</h1>
            </header>

            <article>
                
<p>Data preparation involves data cleaning and feature engineering. It is the most time-consuming part of a machine learning project. Amazon ML offers powerful features to transform and slice the data. In this chapter, we will create the datasources that Amazon ML requires to train and select models. Creating a datasource involves three steps:</p>
<ol>
<li>Making the dataset available on AWS S3.</li>
<li>Informing Amazon ML about the nature of the data using the <em>schema.</em></li>
<li>Transforming the initial dataset with recipes for feature engineering.</li>
</ol>
<p>In a second part, we will extend Amazon ML data modification capabilities in order to carry out powerful feature engineering and data cleansing by using Amazon SQL service Athena. Athena is a serverless SQL-based <span>query service perfectly suited for data munging in a predictive analytics context.</span></p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Working with datasets</h1>
            </header>

            <article>
                
<p>You cannot do predictive analytics without a dataset. Although we are surrounded by data, finding datasets that are adapted to predictive analytics is not always straightforward. In this section, we present some resources that are freely available. We then focus on the dataset we are going to work with for several chapters. The <kbd>Titanic</kbd> dataset is a classic introductory datasets for predictive analytics.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Finding open datasets</h1>
            </header>

            <article>
                
<p>There is a multitude of dataset repositories available online, from local to global public institutions, from non-profit organizations to data-focused start-ups. Here's a small list of open dataset resources that are well suited for predictive analytics. This, by far, is not an exhaustive list:</p>
<div class="packt_tip">This thread on Quora points to many other interesting data sources: <a href="https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public">https://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public</a> . You can also ask for specific datasets on Reddit at <a href="https://www.reddit.com/r/datasets/">https://www.reddit.com/r/datasets/.</a> </div>
<ul>
<li><strong>UCI Machine Learning Repository</strong> is a collection of datasets maintained by <em>UC Irvine</em> since 1987, hosting over 300 datasets related to classification, clustering, regression, and other ML tasks, <a href="https://archive.ics.uci.edu/ml/">https://archive.ics.uci.edu/ml/</a></li>
<li><strong>The Stanford Large Network Dataset Collection</strong> at <a href="https://snap.stanford.edu/data/index.html">https://snap.stanford.edu/data/index.html</a> and other major universities also offer great collections of open datasets</li>
<li><strong>Kdnuggets</strong> has an extensive list of open datasets at <a href="http://www.kdnuggets.com/datasets">http://www.kdnuggets.com/datasets</a></li>
<li><a href="http://Data.gov">Data.gov</a> and other US government agencies; <a href="http://data.UN.org">data.UN.org</a> and other UN agencies</li>
<li><strong>Open data websites</strong> from governments across the world: CA: <a href="http://open.canada.ca/">http://open.canada.ca/</a>, FR: <a href="http://www.data.gouv.fr/fr/">http://www.data.gouv.fr/fr/</a><a href="http://open.canada.ca/">,</a> JA: <a href="http://www.data.go.jp/" target="_blank">http://www.data.go.jp/</a>, IN: <a href="https://data.gov.in/">https://data.gov.in/</a></li>
<li>AWS offers open datasets via partners at <a href="https://aws.amazon.com/government-education/open-data/">https://aws.amazon.com/government-education/open-data/</a></li>
</ul>
<p>The following startups are data centered and give open access to rich data repositories:</p>
<ul>
<li><strong>Quandl</strong> and <strong>Quantopian</strong> for financial datasets</li>
<li><a href="https://datahub.io/" target="_blank">Datahub.io</a>, <a href="https://www.enigma.com/" target="_blank">Enigma.com</a>, and <a href="https://data.world/" target="_blank">Data.world</a> are dataset-sharing sites</li>
<li><a href="https://datamarket.com/" target="_blank">Datamarket.com</a> is great for time series datasets</li>
<li><a href="https://www.kaggle.com/" target="_blank">Kaggle.com</a>, the data science competition website, hosts over a 100 very interesting datasets</li>
</ul>
<div class="packt_infobox"><span><span><strong>AWS public datasets:</strong> </span>AWS hosts a variety of public datasets,</span> such as the Million Song Dataset, the mapping of the <strong>Human Genome</strong>, the US Census data as well as many others in Astrology, Biology, Math, Economics, and so on. These datasets are mostly available using EBS snapshots although some are directly accessible on S3. The datasets are large, from a few gigabytes to several terabytes, and they are not meant to be downloaded on your local machine, they are only to be accessible via an EC2 instance (take a look at <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-public-data-sets.html</a> for further details). AWS public datasets are accessible at <a href="https://aws.amazon.com/public-datasets/">https://aws.amazon.com/public-datasets/</a>. </div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Introducing the Titanic dataset</h1>
            </header>

            <article>
                
<p>Throughout this present chapter and in <a href="b71d5007-58a0-4b79-9446-4a36e7476037.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Model Creation </em>and <a href="436c43e7-4294-4e09-a90e-10c13af2b32a.xhtml" target="_blank">Chapter 6</a>, <em><span class="item-title">Predictions and Performances</span></em>, we will use the classic <kbd>Titanic</kbd> dataset. The data consists of demographic and traveling information for 1309 of the Titanic passengers, and the goal is to predict the survival of these passengers. The full Titanic dataset is available from the <em>Department of Biostatistic</em>s at the <em>Vanderbilt University School of Medicine</em> (<a href="http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv">http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic3.csv</a>) in several formats. The <em>Encyclopedia Titanica</em> website (<a href="https://www.encyclopedia-titanica.org/">https://www.encyclopedia-titanica.org/</a>) is the website of reference regarding the Titanic. It contains all the facts, history, and data surrounding the Titanic, including a full list of passengers and crew members. The Titanic dataset is also the subject of the introductory competition on Kaggle (<a href="https://www.kaggle.com/c/titanic">https://www.kaggle.com/c/titanic</a>, requires opening an account with Kaggle). You can also find a CSV version in this book's GitHub repository at <a href="https://github.com/alexperrier/packt-aml/blob/master/ch4">https://github.com/alexperrier/packt-aml/blob/master/ch4</a>.</p>
<p>The Titanic data contains a mix of textual, Boolean, continuous, and categorical variables. It exhibits interesting characteristics such as missing values, outliers, and text variables ripe for text mining, a rich dataset that will allow us to demonstrate data transformations. Here's a brief summary of the 14 attributes:</p>
<ul>
<li><kbd>pclass</kbd>: Passenger class (1 = 1st; 2 = 2nd; 3 = 3rd)</li>
<li><kbd>survival</kbd>: A Boolean indicating whether the passenger survived or not (0 = No; 1 = Yes); this is our target</li>
<li><kbd>name</kbd>: A field rich in information as it contains title and family names</li>
<li><kbd>sex</kbd>: Male/female</li>
<li><kbd>age</kbd>: Age, a significant portion of values are missing</li>
<li><kbd>sibsp</kbd>: Number of siblings/spouses aboard</li>
<li><kbd>parch</kbd>: Number of parents/children aboard</li>
<li><kbd>ticket</kbd>: Ticket number.</li>
<li><kbd>fare</kbd>: Passenger fare (British Pound). </li>
<li><kbd>cabin</kbd>: Cabin. Does the location of the cabin influence chances of survival?</li>
<li><kbd>embarked</kbd>: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)</li>
<li><kbd>boat</kbd>: Lifeboat, many missing values</li>
<li><kbd>body</kbd>: Body Identification Number</li>
<li><kbd>home.dest</kbd>: Home/destination</li>
</ul>
<p>Take a look at <a href="http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf">http://campus.lakeforest.edu/frank/FILES/MLFfiles/Bio150/Titanic/TitanicMETA.pdf</a> for more details on these variables.</p>
<p>We have 1309 records and 14 attributes, three of which we will discard. The <kbd>home.dest</kbd> <span>attribute</span> has too few existing values, the <kbd>boat</kbd> attribute is only present for passengers who have survived, and the <kbd>body</kbd> <span>attribute</span> is only for passengers who have not survived. We will discard these three columns later on while using the data schema. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Preparing the data</h1>
            </header>

            <article>
                
<p>Now that we have the initial raw dataset, we are going to shuffle it, split it into a training and a held-out subset, and load it to an S3 bucket.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Splitting the data</h1>
            </header>

            <article>
                
<p>As we saw in <a href="8ec21c70-b7f9-4bb2-bbfd-df8337db86a2.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Machine Learning Definitions and Concepts</em>, in order to build and select the best model, we need to split the dataset into three parts: training, validation, and test, with the usual ratios being 60%, 20%, and 20%. The training and validation sets are used to build several models and select the best one while the held-out set is used for the final performance evaluation on previously unseen data. We will use the held-out subset in <a href="436c43e7-4294-4e09-a90e-10c13af2b32a.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 6</span></a>, <em><span class="item-title">Predictions and Performances</span></em> to simulate batch predictions with the model we build in <a href="b71d5007-58a0-4b79-9446-4a36e7476037.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Model Creation</em>.</p>
<p>Since Amazon ML does the job of splitting the dataset used for model training and model evaluation into training and validation subsets, we only need to split our initial dataset into two parts: the global training/evaluation subset (80%) for model building and selection, and the held-out subset (20%) for predictions and final model performance evaluation.</p>
<div class="packt_tip"><strong>Shuffle before you split</strong>: If you download the original data from the <span>Vanderbilt University website,</span> you will notice that it is ordered by <kbd>pclass</kbd>, the class of the passenger, and by alphabetical order of the <kbd>name</kbd> column. The first 323 rows correspond to the first class followed by second (277) and third (709) class passengers. It is important to shuffle the data before you split it so that all the different variables have have similar distributions in each training and held-out subsets. You can shuffle the data directly in the spreadsheet by creating a new column, generating a random number for each row, and then ordering by that column.  </div>
<p>You will find an already shuffled <kbd>titanic.csv</kbd> file for this book at <a href="https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic.csv">https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic.csv</a>. In addition to shuffling the data, we have removed punctuation in the name column: commas, quotes, and parenthesis, which can add confusion when parsing a CSV file.</p>
<p>We end up with two files: <kbd>titanic_train.csv</kbd> with 1047 rows and <kbd>titanic_heldout.csv</kbd> with 263 rows. The next step is to upload these files on S3 so that Amazon ML can access them.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Loading data on S3</h1>
            </header>

            <article>
                
<p>AWS S3 is one of the main AWS services dedicated to hosting files and managing their access. Files in S3 can be public and open to the Internet or have access restricted to specific users, roles, or services. S3 is also used extensively by AWS for operations such as storing log files or results (predictions, scripts, queries, and so on).</p>
<p>Files in S3 are organized around the notion of buckets. Buckets are placeholders with unique names similar to domain names for websites. A file in S3 will have a unique locator URI: <kbd>s3://bucket_name/{path_of_folders}/filename</kbd>. The bucket name is unique across S3.  In this section, we will create a bucket for our data, upload the titanic training file, and open its access to Amazon ML.</p>
<p>We will show in <em><span class="item-title"><a href="https://cdp.packtpub.com/effective_amazon_machine_learning/wp-admin/post.php?post=609">Chapter 7</a>, Command Line and SDK</span></em> <span class="item-title"><span>and the</span></span> files in S3 can be entirely managed via the command line. For now, we will use the S3 online interface. Go to <a href="https://console.aws.amazon.com/s3/home">https://console.aws.amazon.com/s3/home</a>, and open an S3 account if you don't have one yet.</p>
<div class="packt_infobox"><strong>S3 pricing:</strong> S3 charges for the total volume of files you host and the volume of file transfers depends on the region where the files are hosted. At the time of writing, for less than 1TB, AWS S3 charges $0.03/GB per month in the US east region. All S3 prices are available at <a href="https://aws.amazon.com/s3/pricing/">https://aws.amazon.com/s3/pricing/</a> also <a href="http://calculator.s3.amazonaws.com/index.html">http://calculator.s3.amazonaws.com/index.html</a> for the AWS cost calculator.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating a bucket</h1>
            </header>

            <article>
                
<p>Once you have created your S3 account, the next step is to create a bucket for your files. Click on the <span class="packt_screen">C</span><span class="packt_screen">reate bucket</span> button:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/B05028_04_01.png"/></div>
<ol>
<li><strong>Name and a region</strong>: Since bucket names are unique across S3, you must choose a name for your bucket that has not been already taken. We chose the name <kbd>aml.packt</kbd> for our bucket, and we will use this bucket throughout book. Regarding the region, you should always select a region that is the closest to the person or application accessing the files in order to reduce latency and prices.</li>
<li><strong>Set Versioning, Logging, and Tags</strong>: Versioning will keep a copy of every version of your files, which prevents accidental deletions. Since versioning and logging induce extra costs, we chose to disable them. </li>
<li><strong>Set permissions</strong>. </li>
<li><strong>Review and save.</strong></li>
</ol>
<p>These steps are illustrated by the following screenshots:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="238" src="assets/B05028_04_02.png" width="628"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Loading the data</h1>
            </header>

            <article>
                
<p>To upload the data, simply click on the upload button and select the <kbd>titanic_train.csv</kbd> file that we created earlier on. You should, at this point, have the training dataset uploaded to your AWS S3 bucket. We added a <kbd>/data</kbd> folder in our <kbd>aml.packt</kbd> bucket to compartmentalize our objects. It will be useful later on when the bucket will also contain folders created by Amazon ML.</p>
<p>At this point, only the owner of the bucket (that is you) is able to access and modify its contents. We need to grant the Amazon ML service permissions to read the data and add other files to the bucket. When creating the Amazon ML datasource, we will be prompted to grant these permissions via the Amazon ML console. We can also modify the bucket's policy upfront. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Granting permissions </h1>
            </header>

            <article>
                
<p>We need to edit the policy of the <kbd>aml.packt</kbd> bucket. To do so, we have to perform the following steps:</p>
<ol>
<li>Click on your bucket.</li>
<li>Select the <span class="packt_screen">Permissions</span> tab.</li>
</ol>
<ol start="3">
<li>In the dropdown, select <kbd>Bucket Policy</kbd> as shown in the following screenshot. This will open an editor:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><span><img class="image-border" height="205" src="assets/B05028_04_03.png" width="409"/><br/></span></div>
<ol start="4">
<li>Paste in the following JSON file. Make sure to replace <kbd>{YOUR_BUCKET_NAME}</kbd> with the name of your bucket and save:</li>
</ol>
<pre>
        {<br/>          "Version": "2012-10-17",<br/>          "Statement": [<br/>            {<br/>              "Sid": "AmazonML_s3:ListBucket",<br/>              "Effect": "Allow",<br/>              "Principal": {<br/>                "Service": "machinelearning.amazonaws.com"<br/>              },<br/>              "Action": "s3:ListBucket",<br/>              "Resource": "arn:aws:s3:::{YOUR_BUCKET_NAME}",<br/>              "Condition": {<br/>                "StringLike": {<br/>                  "s3:prefix": "*"<br/>                }<br/>              }<br/>            },<br/>            {<br/>              "Sid": "AmazonML_s3:GetObject",<br/>              "Effect": "Allow",<br/>              "Principal": {<br/>                "Service": "machinelearning.amazonaws.com"<br/>              },<br/>              "Action": "s3:GetObject",<br/>              "Resource": "arn:aws:s3:::{YOUR_BUCKET_NAME}/*"<br/>            },<br/>            {<br/>              "Sid": "AmazonML_s3:PutObject",<br/>              "Effect": "Allow",<br/>              "Principal": {<br/>                "Service": "machinelearning.amazonaws.com"<br/>              },<br/>              "Action": "s3:PutObject",<br/>              "Resource": "arn:aws:s3:::{YOUR_BUCKET_NAME}/*"<br/>            }<br/>          ]<br/>        }
</pre>
<p>Further details on this policy are available at <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/granting-amazon-ml-permissions-to-read-your-data-from-amazon-s3.html">http://docs.aws.amazon.com/machine-learning/latest/dg/granting-amazon-ml-permissions-to-read-your-data-from-amazon-s3.html</a>. Once again, this step is optional since Amazon ML will prompt you for access to the bucket when you create the datasource. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Formatting the data</h1>
            </header>

            <article>
                
<p>Amazon ML works on comma separated values files (<kbd>.csv</kbd>), a very simple format where each row is an observation and each column is a variable or attribute. There are, however, a few conditions that should be met:</p>
<ul>
<li>The data must be encoded in plain text using a character set, such as <span>ASCII, Unicode, or EBCDIC</span></li>
<li>All values must be separated by commas; if a value contains a comma, it should be enclosed by double quotes</li>
<li>Each observation (row) must be smaller than 100k</li>
</ul>
<p>There are also conditions regarding end of line characters that separate rows. Special care must be taken when using Excel on <em>OS X (Mac)</em>, as explained on this page: <a href="http://docs.aws.amazon.com/machine-learning/latest/dg/understanding-the-data-format-for-amazon-ml.html">http://docs.aws.amazon.com/machine-learning/latest/dg/understanding-the-data-format-for-amazon-ml.html.</a></p>
<div class="packt_infobox">
<p><strong>What about other data file formats?</strong></p>
<p>Unfortunately, Amazon ML datasources are only compatible with CSV files and Redshift or RDS databases and they do not accept formats such as JSON, TSV, or XML. However, other services such as Athena, a serverless database service, do accept a wider range of formats. We will see later in this chapter how to circumvent the Amazon ML file format restrictions using Athena.</p>
</div>
<p>Now that the data is on S3, the next step is to tell Amazon ML its location by creating a datasource.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating the datasource</h1>
            </header>

            <article>
                
<p>When working with Amazon ML, the data always resides in S3 and it is not duplicated in Amazon ML. A datasource is the metadata that indicates the location of the input data allowing Amazon ML to access it. Creating a datasource also generates descriptive statistics related to the data and a schema with information on the nature of the variables. Basically, the datasource gives Amazon ML all the information it requires to be able to train a model. The following are the steps you need to follow to create a datasource:</p>
<ol>
<li>Go to Amazon Machine Learning: <a href="https://console.aws.amazon.com/machinelearning/home">https://console.aws.amazon.com/machinelearning/home</a>.</li>
<li>Click on getting started, you will be given a choice between accessing the <span class="packt_screen">Dashboard</span> and <span class="packt_screen">Standard setup</span>. This time choose the standard setup:</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="257" src="assets/B05028_04_04.png" width="572"/></div>
<p>Perform the following steps, as shown in the following screenshot:</p>
<ol>
<li>Choose an <span class="packt_screen">S3</span> location.</li>
<li>Start typing the name of the bucket in the <span class="packt_screen">s3 location</span> field, and the list folders and files should show up.</li>
<li>Select the <kbd>titanic_train.csv</kbd> file.</li>
</ol>
<ol start="4">
<li>Give the datasource the name <span class="packt_screen">Titanic training set</span>, and click on <span class="packt_screen">Verify</span>.</li>
</ol>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="327" src="assets/B05028_04_05.png" width="494"/></div>
<p>If you haven't previously set up the bucket policy, you will be asked to grant permissions to Amazon ML to read the file in S3; click on <span class="packt_screen">Yes</span> to confirm:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="189" src="assets/B05028_04_06.png" width="465"/></div>
<p>You will see a confirmation that your datasource was successfully created and is accessible by Amazon ML:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" height="218" src="assets/B05028_04_07.png" width="439"/></div>
<p>Click on <kbd>Continue</kbd> in order to finalize the datasource creation. At this point, Amazon ML has scanned the <kbd>titanic_train.csv</kbd> file and inferred the data type for each column. This meta information is regrouped in the schema. </p>
<div class="packt_infobox"><strong>Other datasources</strong>: Amazon ML allows you to define a Redshift or RDS database as the source of your data instead of S3. Amazon Redshift is a "<em>fast, fully managed, petabyte-scale data warehouse solution</em>". It is far less trivial to set up than S3, but it will handle much larger and more complex volumes of data. Redshift allows you to a<span>nalyze your data with any SQL client using industry-standard ODBC/JDBC connections.</span> We will come back to Redshift in <em><span class="item-title"><span><a href="e4d85418-6fa5-43e9-940f-b6b908e47836.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 8</span></a>, Creating Datasources from Redshift</span></span></em>.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Verifying the data schema</h1>
            </header>

            <article>
                
<p>The data schema is the meta information, the dictionary of a data file. It informs Amazon ML of the type of each variable in the dataset. Amazon ML will use that information to correctly read, interpret, analyze, and transform the data. Once created, the data schema can be saved and reused for other subsets of the same data. Although Amazon ML does a good job of guessing the nature of your dataset, it is always a good idea to double-check and sometimes make some necessary adjustments.</p>
<p>By default, Amazon ML assumes that the first row of your file contains an observation. In our case, the first row of the <kbd>titanic_train.csv</kbd> file contains the names of the variables. Be sure to confirm that this is the case by selecting <span class="packt_screen">Yes</span> in that form:</p>
<div class="packt_figref CDPAlignCenter"><img class="image-border" height="26" src="assets/B05028_04_08.png" width="639"/></div>
<p>Amazon ML classifies your data according to four data types: </p>
<ul>
<li><strong>Binary</strong>: 0 or 1, Yes or No, Male or Female, False or True (<kbd>survived</kbd> in the Titanic dataset)</li>
<li><strong>Categorical</strong>: Variables that can take a finite number of values with numbers or characters (<kbd>pclass</kbd> or <kbd>embarked</kbd>, <kbd>sibsp</kbd>, <kbd>parch</kbd>)</li>
<li><strong>Numeric</strong>: A quantity with continuous values (<kbd>age</kbd> or <kbd>fare</kbd>)</li>
<li><strong>Text</strong>: Free text (<kbd>name</kbd>, <kbd>ticket</kbd>, <kbd>home.dest</kbd>)</li>
</ul>
<p>Note that some variables in the <kbd>Titanic</kbd> dataset could be associated with different data types. <kbd>ticket</kbd>, <kbd>cabin</kbd>, or <kbd>home.dest</kbd>, for instance, could be interpreted as text or categorical values. The <kbd>age</kbd> attribute could be transformed from a numerical value into a categorical one by binning. The <kbd>sex</kbd> attribute could also be interpreted as being binary, categorical, or even a text type. Having the <kbd>sex</kbd> attribute as binary would require transforming its values from male/female to 0/1, so we'll keep it as categorical even though it only takes two values.</p>
<p>The model-building algorithm will process the variables and their values differently depending on their data type. It may be worthwhile to explore different data types for certain variables, when that makes sense, in order to improve predictions. </p>
<p>The following screenshot shows the schema that Amazon ML has deduced from our <kbd>titanic_train.csv</kbd> data:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" height="352" src="assets/B05028_04_09.png" width="558"/></div>
<p class="packt_figref CDPAlignCenter CDPAlign CDPAlignLeft">Note that the sampled values shown here will be different on your own schema since the data has been randomized.</p>
<p>You can choose to keep the default choices made by Amazon ML or make some modifications. Consider the following for instance:</p>
<ul>
<li><kbd>sibsp</kbd> and <kbd>parch</kbd>, respectively the number of siblings and parents, could be categorical instead of numeric if we wanted to regroup passengers with similar numbers of family members.</li>
<li><kbd>pclass</kbd> should be corrected as Categorical and not Numeric since there are only 3 possible values for <kbd>pcalss</kbd>: 1, 2, and 3.</li>
<li><kbd>cabin</kbd> could be interpreted as categorical since there are a finite number of cabins. However, the data indicates that the field could be parsed further. It has values such as C22 C26, which seems to indicate not one cabin but two. A text data type would be more appropriate.</li>
</ul>
<p>Amazon ML needs to know what attribute we aim to predict. On the next screen, you will be asked to select the target. Confirm that <span class="packt_screen">Do you plan to use this dataset to create or evaluate an ML model</span>, and select the <kbd>survived</kbd> attribute, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="484" src="assets/B05028_04_10_v3.png" width="600"/></div>
<p><strong>Deciding between numeric and categorical</strong>: Numeric values are ordered, categorical values are not. When setting a variable to numeric, you are actually telling the model that the values are ordered. This information is useful to the algorithm when building the model. Categorical variables do not imply any order between values.</p>
<p>Going back to the example of the <kbd>sibsp</kbd> and <kbd>parch</kbd> attributes, these variables have a finite number of values (0 to 8 for <kbd>sibps</kbd> and 0 to 9 for <kbd>parch</kbd>) and could be categorical. However, the order of the values holds some important information. Eight siblings is more than one and indicates a big family. Therefore, it also makes sense to keep the values as numeric.</p>
<p><strong>Categorical values and one-hot encoding</strong>: In the case of linear regression, categorical values are one-hot encoded. They are broken down into <em>N-1</em> binary variables when there are <em>N</em> categories. For instance, a variable with just three values <em>A</em>, <em>B</em>, and <em>C</em> is broken into two binary variables <em>is_it_A?</em> and <em>is_it_B?</em> that only take true and false values. Note that there is no need to define a third <em>is_it_C?</em> binary variable as it is directly deduced from the values of <span><em>is_it_A?</em> and <em>is_it_B?</em>. In the Titanic case, we have three values for the embarked variable; Amazon ML will create two binary variables equivalent to <em>passenger embarked at Queenstown</em> and <em>passenger </em><em>embarked at Cherbourg</em>, with the third variable <span><em>passenger </em></span><em>embarked at Southhampton</em> inferred from the values of the two first ones. </span></p>
<p>The type of the target dictates the model Amazon ML will use. Since we chose a binary target, Amazon ML will use logistic regression to train the models. A numeric target would have implied linear regression and a categorical target also logistic regression but this time for multiclass classification. Amazon ML confirms the nature of the model it will use, as shown by the following screenshot:</p>
<div class="packt_figref CDPAlignCenter"><img class="image-border" height="43" src="assets/B05028_04_11.png" width="584"/></div>
<p>The final step consists of telling Amazon ML that our data does not contain a row identifier and finally reviewing our datasource. The datasource creation is now pending; depending on its size, it will take a few minutes to finish.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Reusing the schema</h1>
            </header>

            <article>
                
<p>The schema is a JSON file, which implies that we can create one from scratch for our data or modify the one generated by Amazon ML. We will now modify the one created by Amazon ML, save it to S3, and use it to create a new datasource that does not include the <kbd>body</kbd>, <kbd>boat</kbd>, and <kbd>home.dest</kbd> variables.</p>
<p>Click on <span class="packt_screen">View Input Schema</span> as shown in the next screenshot:</p>
<div class="packt_figref CDPAlignCenter"><img class="image-border" height="273" src="assets/B05028_04_12.png" width="547"/></div>
<p>This gives us the raw schema in JSON format. Save it on your local machine with the filename <kbd>titanic_train.csv.schema</kbd>. We will load this file on S3 in the same bucket/folder where the <span><kbd>titanic_train.csv</kbd> file resides. By adding <kbd>.schema</kbd> to the data CSV filename, we allow Amazon ML to automatically associate the schema file to the data file and bypass the creation of its own schema. </span></p>
<p>Open the schema file with your favorite editor and edit as such:</p>
<ul>
<li>Add <kbd>home.dest</kbd><em>,</em> <kbd>body</kbd>, <kbd>boat</kbd> in the <kbd>excludedAttributeNames</kbd> field</li>
<li>Change the datatype from <kbd>CATEGORICAL</kbd> to <kbd>NUMERIC</kbd> for <kbd>sibps</kbd> and <kbd>parch</kbd></li>
</ul>
<p>Note that although we want to remove the fields <kbd>boat</kbd>, <kbd>body</kbd>, and <kbd>home.dest</kbd>, we still need to declare them and their data types in the schema. Your JSON file should now look as follows:</p>
<pre>
{<br/>  "version" : "1.0",<br/>  "rowId" : null,<br/>  "rowWeight" : null,<br/>  "targetAttributeName" : "survived",<br/>  "dataFormat" : "CSV",<br/>  "dataFileContainsHeader" : true,<br/>  "attributes" : [ {<br/>    "attributeName" : "pclass",<br/>    "attributeType" : "CATEGORICAL"<br/>    }, {<br/>     "attributeName" : "survived",<br/>     "attributeType" : "BINARY"<br/>    }, {<br/>     "attributeName" : "name",<br/>     "attributeType" : "TEXT"<br/>    }, {<br/>     "attributeName" : "sex",<br/>     "attributeType" : "BINARY"<br/>    }, {<br/>     "attributeName" : "age",<br/>     "attributeType" : "NUMERIC"<br/>    }, {<br/>     "attributeName" : "sibsp",<br/>     "attributeType" : "NUMERIC"<br/>    }, {<br/>     "attributeName" : "parch",<br/>     "attributeType" : "NUMERIC"<br/>    }, {<br/>     "attributeName" : "ticket",<br/>     "attributeType" : "TEXT"<br/>    }, {<br/>     "attributeName" : "fare",<br/>     "attributeType" : "NUMERIC"<br/>    }, {<br/>     "attributeName" : "cabin",<br/>     "attributeType" : "TEXT"<br/>    }, {<br/>     "attributeName" : "embarked",<br/>     "attributeType" : "CATEGORICAL"<br/>    }, {<br/>     "attributeName" : "boat",<br/>     "attributeType" : "CATEGORICAL"<br/>    }, {<br/>     "attributeName" : "body",<br/>     "attributeType" : "NUMERIC"<br/>    }, {<br/>     "attributeName" : "home.dest",<br/>     "attributeType" : "TEXT"<br/>    } ],<br/>     "excludedAttributeNames" : ["home.dest", "body", "boat"]<br/>    }
</pre>
<p>Upload that modified <kbd>titanic_train.csv.schema</kbd> file to the same S3 location as your <span><kbd>titanic_train.csv</kbd> data file. Your bucket/folder should now look like this:</span></p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="304" src="assets/B05028_04_13.png" width="578"/></div>
<p>Let us now create a new datasource. Since the schema file has the same name as the data file and is contained at the same location, Amazon ML will use the schema we provided:</p>
<ol>
<li>Go back to the Amazon ML dashboard and click on Create a new datasource in the main menu.</li>
<li>Indicate location and name the datasource; we named this new datasource <span class="packt_screen">Titanic train set 11 variables.</span></li>
</ol>
<p>Amazon ML confirms that the schema you provided has been taken into account:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="156" src="assets/B05028_04_14.png" width="398"/></div>
<p>Go through the remaining datasource creation steps and notice that the data types correspond to the ones you specified in the schema file and that the three fields are no longer present.  </p>
<div class="packt_tip packt_infobox"><strong>Schema recap</strong>: To associate a schema to a file, it suffices to name the schema with the same name as the data file and add the .schema extension. For instance, for a data file named <kbd>my_data.csv</kbd>, the schema file should be named <kbd>my_data.csv.schema</kbd> and be uploaded to the same S3 location as the data file.</div>
<p>Our datasource has now been created, and we can explore what type of insights into the data Amazon ML gives us.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Examining data statistics </h1>
            </header>

            <article>
                
<p>When Amazon ML created the data source, it carried out a basic statistical analysis of the different variables. For each variable, it estimated the following information:</p>
<ul>
<li>Correlation of each attribute to the target</li>
<li>Number of missing values</li>
<li>Number of invalid values</li>
<li>Distribution of numeric variables with histogram and box plot </li>
<li>Range, mean, and median for numeric <span>variables</span></li>
<li>Most and least frequent categories for categorical <span>variables</span></li>
<li>Word counts for text <span>variables</span></li>
<li>Percentage of true values for binary variables</li>
</ul>
<p>Go to the Datasource dashboard, and click on the new datasource you just created in order to access the data summary page. The left side menu lets you access data statistics for the target and different attributes, grouped by data types. The following screenshot shows data insights for the <span class="packt_screen">Numeric</span> attributes. The <kbd>age</kbd> and <kbd>fare</kbd> variables are worth looking at more closely:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="249" src="assets/B05028_04_15.png" width="636"/></div>
<p>Two things stand out:</p>
<ul>
<li><kbd>age</kbd> has <kbd>20%</kbd> missing values. We should replace these missing values by the mean or the median values of the existing values of <kbd>age</kbd>.</li>
<li>The mean for <kbd>fare</kbd> is 33.28, but the range is <kbd>0-512.32</kbd>, indicating a highly skewed distribution. Looking at the <kbd>fare</kbd> distribution confirms that. Click on the <span class="packt_screen">Preview</span>. </li>
</ul>
<p>The following screenshot shows the histogram and the associated box plot for the <kbd>fare</kbd> attribute. We can see that most of the values are below 155, with very few values above 195. This shows that the 521.32 value may well be an outlier, an invalid value caused by human error. Looking back at the original dataset, we see that four passengers from the same family with the same ticket number (<em>PC 17755</em>) paid this price for four first class cabins <em>(B51, B53, B55,</em> and <em>B101).</em> Although that fare of 512.32 value is well above any other fare, it does look legit and not like an error of some sort. We should probably not discard or replace it. The following histogram shows the <kbd>fare</kbd> distribution:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="514" src="assets/B05028_04_16.png" width="499"/></div>
<p>The <span class="packt_screen">Text attributes</span> have automatically been tokenized by Amazon ML and each word has been extracted from the original attribute. Punctuation has also been removed. Amazon ML then calculates word frequency as shown in the following screenshot for the <kbd>name</kbd> attribute:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="352" src="assets/B05028_04_17.png" width="521"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Feature engineering with Athena</h1>
            </header>

            <article>
                
<p>At this point, we have a decent set of variables that can help predict whether a passenger survived the Titanic disaster. However, that data could use a bit of cleaning up in order to handle outliers and missing values. We could also try to extract other meaningful features from existing attributes to boost our predictions. In other terms, we want to do some feature engineering. Feature engineering is the key to boosting the accuracy of your predictions. </p>
<div class="packt_quote">Feature engineering <span>is the process of using domain knowledge of the data to create</span> features <span>that make machine learning algorithms work.<br/>
                                                                                                                                          - Wikipedia</span></div>
<p>ML offers what it calls data recipes to transform the data and adapt it to its linear regression and logistic regression algorithm. In Amazon ML, data recipes are part of building the predictive model, not creating the datasource. We study Amazon ML's data recipes extensively in <a href="b71d5007-58a0-4b79-9446-4a36e7476037.xhtml" target="_blank">Chapter 5</a>, <em>Model Creation</em>. Amazon ML's data recipes are mostly suited to adapt the data to the algorithm and is a bit limited to correcting problems in the original data or creating new attributes from existing ones. For instance, removing outliers, extracting keywords from text, or replacing missing values are not possible with Amazon ML's recipes. We will, therefore, use SQL queries to perform some data cleaning and feature creation.</p>
<p>Several AWS services are based on SQL databases: Redshift, RDS, and more recently, <strong>Athena</strong>. SQL is not only widely used to query data but, as we will see, it is particularly well suited for data transformation and data cleaning.  Many creative ideas on how to squeeze out information for the original Titanic dataset can be found online. These online sources, for instance, offer many ideas on the subject of feature engineering on the Titanic dataset:</p>
<ul>
<li>Analysis of the Titanic dataset for Kaggle competition on the Ultraviolet blog with code example in Python at <a href="http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/">http://www.ultravioletanalytics.com/2014/10/30/kaggle-titanic-competition-part-i-intro/</a></li>
<li>Similar analysis with example in R on Trevor Stephens blog: <a href="http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/">http://trevorstephens.com/kaggle-titanic-tutorial/r-part-4-feature-engineering/</a></li>
<li>The Titanic Kaggle competition forums are ripe with ideas: <a href="https://www.kaggle.com/c/titanic">https://www.kaggle.com/c/titanic</a></li>
</ul>
<p>In our case, we will focus on the following transformations:</p>
<ul>
<li>Replacing the missing <kbd>age</kbd> values with the average of existing <kbd>age</kbd> values</li>
<li>Instead of replacing or discarding the fare outlier values, we will create a new log (<kbd>fare</kbd>) variable with a distribution less skewed</li>
<li>Extracting titles from the <kbd>name</kbd> field</li>
<li>Each cabin number is referred to by three characters, where the first character is a letter relative to the deck level (A, B, C, ..., F) and the number of cabin number, on that deck; we will extract the first letter of each cabin as a new <kbd>deck</kbd> variable</li>
<li>We will also combine <kbd>sibps</kbd> and <kbd>parch</kbd> to create a <kbd>family_size</kbd> variable</li>
</ul>
<p>In the following section, we will use Athena to do these data transformations. We will first create a database and table in Athena and fill it with the Titanic data. We will then create new features using standard SQL queries on the newly created table. Finally, we will export the results to CSV in S3 and create a new datasource from it.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Introducing Athena</h1>
            </header>

            <article>
                
<p>Athena was launched during the <em>AWS re:Invent</em> conference in <em>December 2016</em>. It complements other AWS SQL based services by offering a simple serverless service that directly interacts with S3. According to AWS, Amazon Athena is an interactive query service that makes it easy to analyze data directly from Amazon S3 using standard SQL<em>. </em>Several attributes of Amazon Athena make it particularly adapted for data preparation with Amazon ML:</p>
<ul>
<li>Athena can generate tables directly from data available in S3 in different formats (CSV, JSON, TSV, amazon logs, and others). Since datasources in Amazon ML are also S3-based, it is easy to manipulate files, perform various data transformation, and create various datasets on the fly to test your models.</li>
<li>Athena is fast, simple, and can handle massive datasets.</li>
<li>Athena uses <kbd>prestodb</kbd>, a distributed SQL query engine developed by Facebook that offers a very rich library of SQL functions, which are well suited to do data transformations and feature engineering.</li>
</ul>
<div class="packt_tip">Amazon ML only accepts CSV files to create datasources from S3. However, since Athena can gather data from other file formats besides CSV, (JSON, TSV, ORC, and others) and export query results to a CSV file, it's possible to use Athena as a conversion tool and expand Amazon ML sourcing capabilities that way. </div>
<p>Presto is an open-source distributed SQL query engine optimized for low-latency, ad-hoc analysis of data. It supports the ANSI SQL standard, including complex queries, aggregations, joins, and window functions. More information can be found at <a href="https://prestodb.io/">https://prestodb.io/</a>.</p>
<p>In this section, we will perform the following:</p>
<ol>
<li>Create an Athena account and a database.</li>
<li>Create and populate a table directly from the S3 Titanic CSV file.</li>
<li>Perform queries on the dataset and create new features.</li>
<li>Download the results to S3.</li>
<li>Create new training and testing datasources with the extra features in Amazon ML.</li>
</ol>
<p>Athena is simple. Let's start with a simple overview.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">A brief tour of AWS Athena</h1>
            </header>

            <article>
                
<p>In Athena, the data is not stored in a database; it remains in S3. When you create a table in Athena, you are creating an information layer that tells Athena where to find the data, how it is structured, and what format it is in. The schema in Athena is a logical namespace of objects. Once the data structure and location are known to Athena, you can query the data via standard SQL statements. </p>
<p>Athena uses <strong>Hive Data Definition Language (DDL)</strong> to create or drop databases and tables (more information on Hive DDL can be found at <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL</a>). Athena can understand multiple formats (CSV, TSV, JSON, and so on) through the use of <span><kbd>serializer-deserializer (SerDes)</kbd> libraries.</span> Athena is available either via the console (<a href="https://console.aws.amazon.com/athena/">https://console.aws.amazon.com/athena/</a>) or via <span>JDBC connection (<a href="http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html">http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html</a>). We will use the console.</span></p>
<p>The <strong>Athena</strong> service offers four sections:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="40" src="assets/B05028_04_18.png" width="440"/></div>
<p>The <span class="packt_screen">Query Editor:</span></p>
<ol>
<li> Lets you write your queries, save them, and see the results as well as navigate among your databases and tables.</li>
<li>A <span class="packt_screen">Saved Queries</span> page listing all the queries you saved.</li>
<li>A <span class="packt_screen">History</span> page listing all the queries you ran.</li>
<li>A <span class="packt_screen">Catalog Manager</span> that lets you explore your stored data.</li>
</ol>
<p>These sections are self-explanatory to use, and we will let you explore them at your leisure.</p>
<div class="packt_infobox">At the time of writing, Amazon Athena does not have a <strong>Command-Line interface (cli)</strong>. However, Athena can be accessed via a <span><strong>Java Database Connectivity (JDBC)</strong> driver available on S3. You will find more information at <a href="http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html">http://docs.aws.amazon.com/athena/latest/ug/connect-with-jdbc.html</a></span>.</div>
<p>A few things to know:</p>
<ul>
<li>You can create a table by specifying the location of the data in S3 or explicitly via an SQL query.</li>
<li>All tables must be created as <kbd>EXTERNAL</kbd> and it is not possible to<strong> CREATE TABLE AS SELECT</strong>. <span>Dropping a table created with the External keyword does not delete the underlying data:</span></li>
</ul>
<pre>
<span class="k">        CREATE</span> <span class="k">EXTERNAL</span> <span class="k">TABLE</span> temp ();
</pre>
<ul>
<li>The presto SQL functions are available in Athena. Take a look at <a href="https://prestodb.io/docs/current/functions.html">https://prestodb.io/docs/current/functions.html</a> for a full list.</li>
<li>You can only submit one query and run five concurrent queries at the same time. </li>
</ul>
<div class="packt_infobox"><strong>Athena Pricing:</strong> Athena <span>charges based on the amount of data scanned by the query with (at the time of writing) a $5 fee per TB of data scanned with a minimum of 10 MB per query. You can reduce your costs by c<span>onverting your data to columnar formats or p<span>artitioning your data</span></span>. Refer to <a href="https://aws.amazon.com/athena/pricing/">https://aws.amazon.com/athena/pricing/</a> for more information.</span></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating a titanic database</h1>
            </header>

            <article>
                
<p>We are going to start from scratch and go back to the original Titanic dataset available at <a href="https://github.com/alexperrier/packt-aml/blob/master/ch4/original_titanic.csv">https://github.com/alexperrier/packt-aml/blob/master/ch4/original_titanic.csv</a>. Follow these steps to prepare the CSV file:</p>
<ol>
<li>Open the <kbd>original_titanic.csv</kbd> file.</li>
<li>Remove the header row.</li>
<li>Remove the following punctuation characters: <kbd>,"()</kbd>.</li>
</ol>
<p>The file should only contain data, not column names. This is the original file with 1309 rows. These rows are ordered by <kbd>pclass</kbd> and alphabetical names. The resulting file is available at <a href="https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic_for_athena.csv">https://github.com/alexperrier/packt-aml/blob/master/ch4/titanic_for_athena.csv</a>. Let us create a new <kbd><span>athena_data</span></kbd> folder in our S3 bucket and upload the <kbd>titanic_for_athena.csv</kbd> file. Now go to the Athena console. We will create a <kbd>titanic_db</kbd> database and a <kbd>titanic</kbd> table with the data.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Using the wizard</h1>
            </header>

            <article>
                
<p>There are two ways to create databases and tables in Athena, via the wizard or by running queries. The wizard is accessible by clicking on the <em><span class="packt_screen">Add table</span></em> link in the <span class="packt_screen">Query Editor</span> page:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="241" src="assets/B05028_04_19.png" width="486"/></div>
<p>In four steps, the wizard allows us to create the database, the table, and load the data. Creating the columns in <em>step 3</em> involves manually typing the name of each column and specifying each column type. With 14 columns in the dataset, this manual approach is time-consuming. We will, therefore, not use the wizard and switch to creating the database and table directly in SQL:</p>
<div class="CDPAlignCenter"><img class="image-border" height="73" src="assets/B05028_04_20.png" width="510"/></div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating the database and table directly in SQL</h1>
            </header>

            <article>
                
<p>To create the database, run the following query in the query editor:</p>
<pre>
CREATE DATABASE titanic_db;
</pre>
<p>Then select <kbd>titanic_db</kbd> in the database dropdown menu on the left side as follows:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" height="133" src="assets/B05028_04_21.png" width="320"/></div>
<p>To create the table and load the data, run the following SQL query:</p>
<pre>
CREATE EXTERNAL TABLE IF NOT EXISTS titanic_db.titanic (<br/>  pclass tinyint,<br/>  survived tinyint,<br/>  name string,<br/>  sex string,<br/>  age double,<br/>  sibsp tinyint,<br/>  parch tinyint,<br/>  ticket string,<br/>  fare double,<br/>  cabin string,<br/>  embarked string,<br/>  boat string,<br/>  body string,<br/>  home_dest string<br/> )<br/> ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' WITH SERDEPROPERTIES (<br/>  'serialization.format' = ',',<br/>  'field.delim' = ','<br/>  ) LOCATION 's3://aml.packt/athena_data/'
</pre>
<p>A few things to note about that query:</p>
<ul>
<li>The location <kbd>s3://aml.packt/athena_data/</kbd> points to the folder we have specially created, not to the file itself . All the files in that folder will be considered as data for that table.</li>
<li>The <kbd>SERDE</kbd> corresponds to the CSV format with a comma as the field delimiter. The list of supported formats and respective SERDE is available at <a href="https://docs.aws.amazon.com/athena/latest/ug/supported-formats.html">https://docs.aws.amazon.com/athena/latest/ug/supported-formats.html</a>.</li>
<li>The field types are standard SQL types (string, double, tinyint, and so on).</li>
</ul>
<p>Once the query has finished running, the <kbd>titanic</kbd> table name appears in the left section of the page. You can click on the eye icon to select the first 10 rows of the table as follows:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="381" src="assets/B05028_04_22.png" width="591"/></div>
<div class="packt_infobox"><strong>Specifying the results location</strong>: Athena stores the results of your queries in a new S3 folder. You can specify what folder you want these results to be stored in by going to the settings page and specifying the desired S3 path. We have created a new folder titled  <kbd><em>athena_query_results</em></kbd> in our <kbd>aml.packt</kbd> bucket and set the result location to  <kbd><em>s3://aml.packt/athena_query_results/</em></kbd>.</div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Data munging in SQL</h1>
            </header>

            <article>
                
<p>We will now define the SQL query that will correct the missing value and outlier problems we saw earlier and create some new variables. </p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Missing values</h1>
            </header>

            <article>
                
<p>We have missing values for the <kbd>age</kbd> and <kbd>fare</kbd> variables with respective median values <kbd>28</kbd> and <kbd>14.5</kbd>. We can replace all the missing values with the median values with this statement:</p>
<pre>
select coalesce(age, 28) as age, coalesce(fare, 14.5) as fare from titanic;
</pre>
<p>We also want to keep the information that there was a missing value at least for the <kbd>age</kbd> variable. We can do that with the query that creates a new binary variable that we name: <kbd>is_age_missing</kbd>:</p>
<pre>
SELECT age, CASE <br/> WHEN age is null THEN 0<br/> ELSE 1<br/> END as is_age_missing<br/>from titanic;
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Handling outliers in the fare</h1>
            </header>

            <article>
                
<p>We have seen that the <kbd>fare</kbd> had four passengers paying a much higher price than the others. There are several ways to deal with these values, one of which can be to bin the variable by defining a series of specific ranges. For instance, below 20; from 20 to 50; 50 to 100, ..., and over 200. Binning can be done with Amazon ML recipes.  We could also cap the fare value at a specific threshold, such as the 95% percentile. However, we decided that these large fare values were legit and that we ought to keep them. We can still create a new variable, <kbd>log_fare</kbd><em>,</em> with a more compact range and a less skewed distribution by taking the log of the <kbd>fare:</kbd></p>
<pre>
select fare, log(fare +1, 2) as log_fare from titanic;
</pre>
<p>The <kbd>log_fare</kbd> variable has a range of <span><em>0 - 9.0</em>, a mean of <em>4.3</em>, and a median of <em>3.95</em>, whereas the original <kbd>fare</kbd> variable had a range of <em>[0, 512.3]</em>, mean <span><em>32.94</em>, and median <em>14.5</em>. The distribution of the <kbd>log_fare</kbd> is closer to a Gaussian distribution than the distribution of the original <kbd>fare</kbd> variable.</span></span></p>
<div class="packt_tip"><strong>Box-Cox transformation</strong>: Because of the linear regression assumptions, it is better to have variables with Gaussian distributions. The Box-Cox transformation, also known as the power transform (see <a href="https://en.wikipedia.org/wiki/Power_transform">https://en.wikipedia.org/wiki/Power_transform</a>), is a common method to reshape a variable into a normally-distributed one. The Box-Cox transformation is a generalization of the log transformation we just applied to the <kbd>fare</kbd> variable. </div>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Extracting the title from the name</h1>
            </header>

            <article>
                
<p>If you look closely at the passenger names, you will notice that they are all in the <kbd>{family name}{title}{first names}</kbd><em> </em>format. It would be interesting to extract the <kbd>title</kbd> as a new variable. The following query uses the split function, which returns an array. We need the second element of that array:</p>
<pre>
SELECT name, split(name, ' ')[2] as title from titanic;
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Inferring the deck from the cabin</h1>
            </header>

            <article>
                
<p>The cabin variable has three character values, where the first character corresponds to the Deck number (A, B, C, D, E). This is surely important information that we would like to extract.  We can do so with the following query:</p>
<pre>
SELECT cabin, substr(cabin, 1, 1) as deck  FROM titanic;
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Calculating family size</h1>
            </header>

            <article>
                
<p>Finally, it makes sense to assume that the overall family size a passenger belongs to might have been a decisive factor in survival. We can aggregate the number of siblings and the number of parents and add 1 for the passenger. The following simple query will create the family size variable:</p>
<pre>
select sibps + parch + 1 as family_size from titanic; 
</pre>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Wrapping up</h1>
            </header>

            <article>
                
<p>We can combine all these queries while also selecting the original attributes. Since the data is still ordered by <kbd>pclass</kbd> and passenger <kbd>name</kbd> in alphabetical order, we should also randomize the results. We end up with the following query:</p>
<pre>
SELECT pclass, <br/>survived, <br/>name, <br/>sex, <br/>COALESCE(age, 28) as age, <br/>sibsp, <br/>parch, <br/>ticket, <br/>COALESCE(fare, 14.5) as fare, <br/>cabin, <br/>embarked, <br/>boat, <br/>body, <br/>home_dest, <br/>CASE <br/> WHEN age is null THEN 0<br/> ELSE 1<br/> END as is_age_missing, <br/>log(fare + 1, 2) as log_fare,<br/>split(name, ' ')[2] as title,<br/>substr(cabin, 1, 1) as deck,<br/>sibsp + parch + 1 as family_size <br/>FROM titanic<br/>ORDER BY RAND();
</pre>
<p>Let us run that query. The results will be displayed in the results panel and also written in a CSV file in the query result location on S3. You can also save it on your local machine by clicking at the icon in the upper right corner of the results panel:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" height="114" src="assets/B05028_04_23.png" width="178"/></div>
<p>At this point, we want to split the data into a training and a testing set like we did previously and create a new Amazon ML datasource with an extended schema.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Creating an improved datasource</h1>
            </header>

            <article>
                
<p>We need to do some manipulation on the new <kbd>Titanic</kbd> dataset before we upload it to S3 and create a new datasource in Amazon ML:</p>
<ol>
<li>Open this new Titanic dataset in your favorite editor.</li>
<li>Select the first <em>1047</em> rows, and save them to a new CSV: <kbd>ext_titanic_training.csv</kbd>.</li>
<li>Select the next <em>263</em> rows and the header row, and save them to a file <kbd>ext_titanic_heldout.csv</kbd><span>.</span></li>
</ol>
<p>We need to update our schema. Open the schema file <kbd>titanic_training.csv.schema</kbd>, and add the following lines to the JSON:</p>
<pre>
{<br/>  "attributeName" : "is_age_missing",<br/>  "attributeType" : "BINARY"<br/>  }, {<br/>  "attributeName" : "log_fare",<br/>  "attributeType" : "NUMERIC"<br/>  }, {<br/>  "attributeName" : "title",<br/>  "attributeType" : "CATEGORICAL"<br/>  }, {<br/>  "attributeName" : "deck",<br/>  "attributeType" : "CATEGORICAL"<br/>  }, {<br/>  "attributeName" : "family_size",<br/>  "attributeType" : "NUMERIC"<br/> }
</pre>
<p>The new schema file as well as the training and held-out sets can be found at <a href="https://github.com/alexperrier/packt-aml/tree/master/ch4">https://github.com/alexperrier/packt-aml/tree/master/ch4</a>.</p>
<p>We then need to upload the training and the schema file to S3. These files should be in the same S3 location, <kbd>{bucket}/{folders}</kbd>.</p>
<p>We are now ready to create a new datasource based on this extended Titanic training set following the exact same steps as before:</p>
<ol>
<li>Specify the location of the input data.</li>
<li>Review the schema.</li>
<li>Set the target as <kbd>survived</kbd>.</li>
<li>Bypass the <kbd>rowID</kbd>.</li>
<li>Review and create.</li>
</ol>
<p>We now have 19 attributes in the schema and a brand new datasource.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    

        <section>

            <header>
                <h1 class="header-title">Summary</h1>
            </header>

            <article>
                
<p>In this chapter, we focused on what is commonly known as the <strong>Extract Load Transform (ETL)</strong> part of the data science flow with regard to the Amazon ML service. We saw that the Amazon ML datasource is a set of information comprised of location, data structure, and data analytics given to the service so that it can use that data to start training models. You should now feel comfortable creating an Amazon ML datasource from an original CSV data file made accessible via S3. </p>
<p>We have also explored ways to transform the data and create new features via the AWS Athena service using simple SQL queries. The ability to complement the features of Amazon ML by leveraging the AWS ecosystem is one of the main benefits of using Amazon ML.</p>
<p>We now have a couple of Titanic datasets, the original one and the extended one, which are split into training and held-out subsets, and we have created the associated datasources.</p>
<p>In <a href="b71d5007-58a0-4b79-9446-4a36e7476037.xhtml" target="_blank"><span class="ChapterrefPACKT">Chapter 5</span></a>, <em>Model Creation</em>, we will use these datasets to train models, and we will see if our new features and data cleaning result in better models.</p>


            </article>

            <footer style="margin-top: 5em;">
                
            </footer>

        </section>
    </body></html>