<html><head></head><body>
		<div id="_idContainer094">
			<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>Chapter 5: Model Training and Inference</h1>
			<p>In the last chapter, we discussed <strong class="bold">Feast deployment</strong> in the AWS cloud and set up S3 as an offline store and DynamoDB as an online store for the model. We also revisited the few stages of the ML life cycle using the <strong class="bold">Customer Lifetime Value</strong> (<strong class="bold">LTV</strong>/<strong class="bold">CLTV</strong>) model built in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>. During the processing of model development, we performed data cleaning and feature engineering and produced the feature set for which the feature definitions were created and applied to Feast. In the end, we ingested the features into Feast successfully and we were also able to query the ingested data.</p>
			<p>In this chapter, we will continue with the rest of the ML life cycle, which will involve model training, packaging, batch, and online model inference using the feature store. The goal of this chapter is to continue using the feature store infrastructure that was created in the previous chapter and go through the rest of the ML life cycle. As we go through this process, it will provide an opportunity to learn how using the feature store in ML development can improve the time to production of the model, decouples different stages of the ML life cycle, and helps in collaboration. We will also look back at <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>, and compare the different stages as we go through these steps. This chapter will help you understand how to use the feature store for model training, followed by model inference. We will also learn what use case the online store serves and use cases served by the offline store.  </p>
			<p> We will discuss the following topics in order:</p>
			<ul>
				<li>Model training with the feature store</li>
				<li>Model packaging </li>
				<li>Batch model inference with Feast</li>
				<li>Online model inference with Feast</li>
				<li>Handling changes to the feature set during development</li>
			</ul>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Prerequisites</h1>
			<p>To run through the examples and to get a better understanding of this chapter, the resources created in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>, are required. In this chapter, we will use the resources created in the previous chapter and also use the feature repository created in the chapter. The following GitHub link points to the feature repository I created: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/customer_segmentation</a>.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Technical requirements</h1>
			<p>To follow the code examples in the chapter, all you need is familiarity with Python and any notebook environment, which could be a local setup such as Jupyter or an online notebook environment such as Google Colab, Kaggle, or SageMaker. You will also need an AWS account with full access to resources such as Redshift, S3, Glue, DynamoDB, the IAM console, and more. You can create a new account and use all the services for free during the trial period. In the last part, you will need an IDE environment to develop the REST endpoints for the online model. You can find the code examples of the book at the following GitHub link: <a href="https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter05">https://github.com/PacktPublishing/Feature-Store-for-Machine-Learning/tree/main/Chapter05</a>.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>Model training with the feature store</h1>
			<p>In <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>, after feature engineering, we jumped right<a id="_idIndexMarker295"/> into model training in the same notebook. Whereas <a id="_idIndexMarker296"/>in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>, the generated features were ingested into the feature store. This is one of the standardizations that the feature store helps with in the ML life cycle. By ingesting features into the feature store, a discoverable, sharable, reusable, and versioned dataset/feature set was created. </p>
			<p>Now let's assume that two data scientists, Ram and Dee, are working on the same model. Both can use this feature set without having to do anything extra. Not only that, if the background data gets refreshed every day, then all that needs to be done is to run the feature engineering notebook once a day when data scientists comes in, and the latest features will be available for consumption. An<a id="_idIndexMarker297"/> even better thing to do is schedule the feature engineering <a id="_idIndexMarker298"/>notebook using an orchestration framework such as <strong class="bold">Airflow</strong>, <strong class="bold">AWS Step Functions</strong>, or even <strong class="bold">GitHub</strong> workflows. Once that is done, the latest features <a id="_idIndexMarker299"/>are available for experimentation for both Dee and Ram when they come to work.</p>
			<p>As we have been discussing, one of the biggest advantages that data engineers and scientists get out of feature stores is collaboration. Let's try and see how our two data scientists, Dee and Ram, can collaborate/compete in model building. Every day when Dee and Ram come into work, assuming that the scheduled feature engineering has run successfully, they start with the model training. The other important thing to note here is, for model training, the source is the feature store. Data scientists don't need to go into raw data sources to generate features unless they are not happy with the model produced by the existing features. In which case, data scientists would go into data exploration again, generate additional feature sets, and ingest them into the feature store. The ingested features are again available for everybody to use. This will go on until the team/data scientist is happy with the model's performance. </p>
			<p>Before we split the workflow of two data scientists, Dee and Ram, let's run through the common steps of their model training notebook. Let's open a new Python notebook, call it <strong class="source-inline">model-training.ipynb</strong>, and generate training data. The offline store will be used for generating training datasets as it stores the historical data and versions the data with a timestamp. In Feast, the interface to data stores is through an API, as we looked at in <a href="B18024_03_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 3</em></a><em class="italic">, Feature Store Fundamentals, Terminology, and Usage. </em>and in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>. Hence, to generate the training dataset, we will be using <strong class="source-inline">get_historical_features</strong>. One of the inputs of the <strong class="source-inline">get_historical_features</strong> API is entity IDs. Usually, in enterprises, entity IDs can be fetched <a id="_idIndexMarker300"/>from the raw data source. The typical raw sources <a id="_idIndexMarker301"/>include databases, data warehouses, object stores, and more. The queries to fetch entities could be as simple as <strong class="source-inline">select unique {entity_id} from {table};</strong>. Let's do something similar here. Our raw data source is the CSV file. Let's use that to fetch the entity IDs. Before we go further, let's install the required packages:</p>
			<ol>
				<li>The following code block installs the required packages for the model training:<p class="source-code">!pip install feast[aws]==0.19.3 pandas xgboost</p></li>
				<li>After installing the required packages, if you haven't already cloned the feature repository, please do so, since we need to connect to the feature store to generate the training dataset. The following code clones the repository: <p class="source-code">!git clone &lt;repo_url&gt;</p></li>
				<li>Now that we have the feature repository, let's connect to Feast/the feature store and make sure that everything works as expected before we move on:<p class="source-code"># change directory</p><p class="source-code">%cd customer_segmentation</p><p class="source-code">"""import feast and load feature store object with the path to the directory which contains <strong class="bold">feature_story.yaml</strong>."""</p><p class="source-code">from feast import FeatureStore</p><p class="source-code">store = FeatureStore(repo_path=".")</p><p class="source-code">for entity in store.list_entities():</p><p class="source-code">  print(f"entity: {entity}")</p></li>
			</ol>
			<p>The <a id="_idIndexMarker302"/>preceding code block connects to the Feast feature<a id="_idIndexMarker303"/> repository The <strong class="source-inline">repo_path="."</strong> parameter indicates that <strong class="source-inline">feature_store.yaml</strong> is in the current working directory. It also lists the available entities in the <strong class="source-inline">customer_segmentation</strong> feature repository</p>
			<p>Now that we are able to connect to the feature repository let's create the list of entity IDs that are required for training the model. To get the list of entity IDs, in this case, <strong class="source-inline">CustomerId</strong>, let's use the raw dataset and filter out the entity IDs from it. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">We are using the same raw dataset that was used in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>. Here is the URL of the dataset: <a href="https://www.kaggle.com/datasets/vijayuv/onlineretail">https://www.kaggle.com/datasets/vijayuv/onlineretail</a>.</p>
			<ol>
				<li value="4">The following code block loads the raw data:<p class="source-code">import pandas as pd</p><p class="source-code">##Read the OnlineRetail.csv</p><p class="source-code">retail_data = pd.read_csv('/content/OnlineRetail.csv',</p><p class="source-code">                          encoding= 'unicode_escape')</p><p class="source-code">retail_data['InvoiceDate'] = pd.to_datetime(</p><p class="source-code">  retail_data['InvoiceDate'], errors = 'coerce')</p><p class="callout-heading">Important Note</p><p class="callout">You might question why we need raw data here. Feast allows queries on entities. Hence, we need the entity IDs for which the features are needed. </p></li>
				<li>Let's<a id="_idIndexMarker304"/> filter out the customer IDs that are of<a id="_idIndexMarker305"/> interest, similar to the filtering done during feature creation. The following code block selects the dataset that doesn't belong to the United Kingdom and also the customer IDs that exists in the three months dataset (the reason for picking the customers in the three months dataset is, after generating the RFM feature, we performed a left join on the dataset in the feature engineering notebook). </li>
			</ol>
			<p>The following code block performs the described filtering:</p>
			<p class="source-code">## filter data for <strong class="bold">United Kingdom</strong></p>
			<p class="source-code">uk_data = retail_data.query("Country=='United Kingdom'").reset_index(drop=True)</p>
			<p class="source-code">t1 = pd.Timestamp("2011-06-01 00:00:00.054000")</p>
			<p class="source-code">t2 = pd.Timestamp("2011-03-01 00:00:00.054000")</p>
			<p class="source-code">uk_data_3m = uk_data[(uk_data.InvoiceDate &lt; t1) &amp; (uk_data.InvoiceDate &gt;= t2)].reset_index(drop=True)</p>
			<p>From <strong class="source-inline">uk_data_3m</strong>, we need to fetch the unique <strong class="source-inline">CustomerId</strong>. The additional column required in the entity data is the timestamp to perform point-in-time joins. For now, I'm going to use the latest timestamp for all the entity IDs. </p>
			<ol>
				<li value="6">The following <a id="_idIndexMarker306"/>code block creates the entity DataFrame<a id="_idIndexMarker307"/> required for querying the historical store:<p class="source-code">from datetime import datetime</p><p class="source-code">entity_df = pd.DataFrame(data = {</p><p class="source-code">    "customerid": [str(item) for item in uk_data_3m.CustomerID.unique().tolist()],</p><p class="source-code">    "event_timestamp": datetime.now()</p><p class="source-code">})</p><p class="source-code">entity_df.head()</p></li>
			</ol>
			<p>The preceding code block produces the following output:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B18024_05_001.jpg" alt="Figure 5.1 – Entity DataFrame for generating the training dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Entity DataFrame for generating the training dataset</p>
			<p>As you can see in <em class="italic">Figure 5.1</em>, the entity DataFrame contains two columns: </p>
			<ul>
				<li><strong class="bold">CustomerID</strong>: A list <a id="_idIndexMarker308"/>of customers for whom the features<a id="_idIndexMarker309"/> need to be fetched.</li>
				<li><strong class="bold">event_timestamp</strong>: The timestamp for a point-in-time join. The latest features for a given customer are at the given <strong class="source-inline">event_timestamp</strong>.</li>
			</ul>
			<p>Now that the common steps in Dee and Ram's model training notebook are complete, let's split their workflow and look at how they can collaborate.</p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>Dee's model training experiments</h2>
			<p>Continuing from<a id="_idIndexMarker310"/> the last step (feel free to copy the code blocks and run them in a different notebook and name it as <strong class="source-inline">dee-model-training.ipynb</strong>), it is time to pick the feature set required for training the model: </p>
			<ol>
				<li value="1">To pick the features, Dee would run the following command to look at the available features in the existing feature view:<p class="source-code">feature_view = store.get_feature_view("customer_rfm_features")</p><p class="source-code">print(feature_view.to_proto())</p></li>
			</ol>
			<p>The preceding command outputs the feature view. The following block displays a part of the output that includes features and entities that are part of the feature view:</p>
			<p class="source-code">  name: "customer_rfm_features"</p>
			<p class="source-code">  entities: "customer"</p>
			<p class="source-code">  features {</p>
			<p class="source-code">    name: "recency"</p>
			<p class="source-code">    value_type: INT32</p>
			<p class="source-code">  }</p>
			<p class="source-code">  features {</p>
			<p class="source-code">    name: "frequency"</p>
			<p class="source-code">    value_type: INT32</p>
			<p class="source-code">  }</p>
			<p class="source-code">  features {</p>
			<p class="source-code">    name: "monetaryvalue"</p>
			<p class="source-code">    value_type: DOUBLE</p>
			<p class="source-code">  }</p>
			<p class="source-code">  …</p>
			<p class="source-code">  </p>
			<p class="source-code">meta {</p>
			<p class="source-code">  created_timestamp {</p>
			<p class="source-code">    seconds: 1647301293</p>
			<p class="source-code">    nanos: 70471000</p>
			<p class="source-code">  }</p>
			<p class="source-code">  last_updated_timestamp {</p>
			<p class="source-code">    seconds: 1647301293</p>
			<p class="source-code">    nanos: 70471000</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>From<a id="_idIndexMarker311"/> the feature set, let's assume that Dee wants to leave out frequency-related features and see how the performance of the model is affected. Hence, she picks all other features for the query and leaves out <em class="italic">frequency</em> and <em class="italic">F</em>, which indicates frequency group.</p>
			<ol>
				<li value="2">The<a id="_idIndexMarker312"/> following code block queries the historical/offline store to fetch the required features using the entity DataFrame shown in <em class="italic">Figure 5.1</em>: <p class="source-code">import os</p><p class="source-code">from datetime import datetime</p><p class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key_id&gt;"</p><p class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] = "&lt;aws_secret&gt;"</p><p class="source-code">os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</p><p class="source-code">job = store.get_historical_features(</p><p class="source-code">    entity_df=entity_df,</p><p class="source-code">    features=[</p><p class="source-code">              "customer_rfm_features:recency", </p><p class="source-code">              "customer_rfm_features:monetaryvalue", </p><p class="source-code">              "customer_rfm_features:r", </p><p class="source-code">              "customer_rfm_features:m",</p><p class="source-code">              "customer_rfm_features:rfmscore",</p><p class="source-code">              "customer_rfm_features:segmenthighvalue",</p><p class="source-code">              "customer_rfm_features:segmentlowvalue"</p><p class="source-code">              "customer_rfm_features:segmentmidvalue",</p><p class="source-code">              "customer_rfm_features:ltvcluster"</p><p class="source-code">              ]</p><p class="source-code">    )</p><p class="source-code">feature_data = job.to_df()</p><p class="source-code">feature_data = feature_data.dropna()</p><p class="source-code">feature_data.head()</p></li>
			</ol>
			<p>The <a id="_idIndexMarker313"/>previous code block outputs the DataFrame shown as follows:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B18024_05_002.jpg" alt="Figure 5.2 – Training dataset for Dee's model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Training dataset for Dee's model</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Replace <strong class="source-inline">&lt;aws_key_id&gt;</strong> and <strong class="source-inline">&lt;aws_secret&gt;</strong> in the preceding code block with the user credentials created in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>.</p>
			<ol>
				<li value="3">Now that Dee has the training dataset generated, the next step is model training. Let's build the XGBoost model with the same parameters that were used in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>. The following code block splits the dataset into training and testing: <p class="source-code">from sklearn.metrics import classification_report,confusion_matrix</p><p class="source-code">import xgboost as xgb</p><p class="source-code">from sklearn.model_selection import KFold, cross_val_score, train_test_split</p><p class="source-code">#Drop prediction column along with event time and customerId columns from X</p><p class="source-code">X = feature_data.drop(['ltvcluster', 'customerid', </p><p class="source-code">                       'event_timestamp'], axis=1)</p><p class="source-code">y = feature_data['ltvcluster']</p><p class="source-code">X_train, X_test, y_train, y_test = \ </p><p class="source-code">train_test_split(X, y, test_size=0.1)</p></li>
				<li>The <a id="_idIndexMarker314"/>following code block uses the training and test dataset created in the previous example and trains an <strong class="source-inline">XGBClassifier</strong> model:<p class="source-code">xgb_classifier = xgb.XGBClassifier(max_depth=5, objective='multi:softprob')</p><p class="source-code">#model training</p><p class="source-code">xgb_model = xgb_classifier.fit(X_train, y_train)</p><p class="source-code">#Model scoring</p><p class="source-code">acc = xgb_model.score(X_test,y_test)</p><p class="source-code">print(f"Model accuracy: {acc}")</p></li>
			</ol>
			<p>The preceding code block prints the accuracy of the model: </p>
			<p class="source-code">Model accuracy: 0.8840579710144928</p>
			<ol>
				<li value="5">The following code block runs the <strong class="source-inline">predict</strong> function on the test dataset and prints the classification report:<p class="source-code">#Run prediction on the test dataset</p><p class="source-code">y_pred = xgb_model.predict(X_test)</p><p class="source-code">print(classification_report(y_test, y_pred))</p></li>
			</ol>
			<p>The preceding code block produces the following output:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B18024_05_003.jpg" alt="Figure 5.3 – Classification report of Dee's model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Classification report of Dee's model</p>
			<p>Not only this but<a id="_idIndexMarker315"/> Dee can also try out different feature sets and algorithms. For now, let's assume Dee is happy with her model. Let's move on and look at what Ram does. </p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>Ram's model training experiments</h2>
			<p>Again, we'll <a id="_idIndexMarker316"/>continue in the notebook from the step after <em class="italic">Figure 5.1</em> (feel free to copy the code blocks, run them in a different notebook, and name it as <strong class="source-inline">ram-model-training.ipynb</strong>). It's time to pick the feature set required for training the model. To pick the features, Ram would follow similar steps as Dee did. Let's assume that Ram thinks differently – instead of dropping out one specific category, he drops the features with actual values and just uses the R, F, and M categorical features and segments categorical features. According to Ram, these categorical variables are some transformations of the actual values: </p>
			<ol>
				<li value="1">The following code block produces the features set required by Ram to train the model:<p class="source-code">import os</p><p class="source-code">from datetime import datetime</p><p class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key_id&gt;"</p><p class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] = "&lt;aws_secret&gt;"</p><p class="source-code">os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</p><p class="source-code">job = store.get_historical_features(</p><p class="source-code">    entity_df=entity_df,</p><p class="source-code">    features=[</p><p class="source-code">             "customer_rfm_features:r", </p><p class="source-code">             "customer_rfm_features:m",</p><p class="source-code">             "customer_rfm_features:f",</p><p class="source-code">             "customer_rfm_features:segmenthighvalue",</p><p class="source-code">             "customer_rfm_features:segmentlowvalue",</p><p class="source-code">             "customer_rfm_features:segmentmidvalue",</p><p class="source-code">             "customer_rfm_features:ltvcluster"</p><p class="source-code">             ]</p><p class="source-code">    )</p><p class="source-code">feature_data = job.to_df()</p><p class="source-code">feature_data = feature_data.dropna()</p><p class="source-code">feature_data.head()</p><p class="callout-heading">Important Note</p><p class="callout">Replace <strong class="source-inline">&lt;aws_key_id&gt;</strong> and <strong class="source-inline">&lt;aws_secret&gt;</strong> in the preceding code block with the user credentials created in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>.</p></li>
			</ol>
			<p>The preceding <a id="_idIndexMarker317"/>code block produces the following output:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B18024_05_004.jpg" alt="Figure 5.4 – Training dataset for Ram's model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Training dataset for Ram's model</p>
			<ol>
				<li value="2">The next step is<a id="_idIndexMarker318"/> similar to what Dee performed, which is to train the model and look at its classification report. Let's do that.</li>
			</ol>
			<p>The following code block trains the model on the feature set in <em class="italic">Figure 5.4</em>:</p>
			<p class="source-code">from sklearn.metrics import classification_report,confusion_matrix</p>
			<p class="source-code">from sklearn.linear_model import LogisticRegression</p>
			<p class="source-code">from sklearn.model_selection import KFold, cross_val_score, train_test_split</p>
			<p class="source-code">X = feature_data.drop(['ltvcluster', 'customerid',</p>
			<p class="source-code">                       'event_timestamp'], axis=1)</p>
			<p class="source-code">y = feature_data['ltvcluster']</p>
			<p class="source-code">X_train, X_test, y_train, y_test = \ </p>
			<p class="source-code">train_test_split(X, y, test_size=0.1)</p>
			<p class="source-code">model =  (random_state=0).fit(X_train, y_train)</p>
			<p class="source-code">acc = model.score(X_test,y_test)</p>
			<p class="source-code">print(f"Model accuracy: {acc}")</p>
			<p>The preceding code block prints the model accuracy after training and scoring the model on the test set. The code is similar to what Dee was using, but instead of <strong class="source-inline">XGBClassifier</strong> uses <strong class="source-inline">LogisticRegression</strong>. The code block produces the following output:</p>
			<p class="source-code">Model accuracy: 0.8623188405797102</p>
			<ol>
				<li value="3">Let's print the<a id="_idIndexMarker319"/> classification report on the test dataset so that we can compare Ram and Dee's models. The following code block produces the classification report for the model:<p class="source-code">y_pred = model.predict(X_test)</p><p class="source-code">print(classification_report(y_test, y_pred))</p></li>
			</ol>
			<p>The preceding code block produces the following output:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B18024_05_005.jpg" alt="Figure 5.5 – Classification report of Ram's model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Classification report of Ram's model</p>
			<p>Ram and Dee can now compare each other's work by looking at the experiments each of them has run. Not only these two experiments but they can run multiple experiments and come up with the best model after all the comparisons. Not only that, but they can also automate the experimentation by writing code to try out all combinations of feature sets and look at and explore more data or work on some other aspect while these experiments are run. </p>
			<p>One other thing I suggest here is to use one of the experiment tracking tools/software. There are many out there on the market. Some of them come with the notebook infrastructure that you use. For<a id="_idIndexMarker320"/> example, <strong class="bold">Databricks</strong> offers <strong class="bold">MLflow</strong>, <strong class="bold">SageMaker</strong> has<a id="_idIndexMarker321"/> its own, and there are also <a id="_idIndexMarker322"/>third-party experiment tracking<a id="_idIndexMarker323"/> tools such<a id="_idIndexMarker324"/> as <strong class="bold">Neptune</strong>, <strong class="bold">ClearML</strong>, and others. More tools for experiment tracking and comparison can be found in the following blog: <a href="https://neptune.ai/blog/best-ml-experiment-tracking-tools">https://neptune.ai/blog/best-ml-experiment-tracking-tools</a>.</p>
			<p>Let's assume that Dee and <a id="_idIndexMarker325"/>Ram, after all the experimentation, conclude that <strong class="source-inline">XGBClassifier</strong> performed better and decide to use that model. Let's look at model packaging in the next section. </p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Model packaging</h1>
			<p>In the previous section, we built<a id="_idIndexMarker326"/> two versions of the model. In this section, let's package one of the models and save it for model scoring and deployment. As mentioned in the previous section, let's package the <strong class="source-inline">XGBClassifier</strong> model. Again, for packaging, there are different solutions and tools available. To avoid setting up another tool, I will be using the <strong class="source-inline">joblib</strong> library to package the model: </p>
			<ol>
				<li value="1">Continuing in the same notebook that produced the <strong class="source-inline">XGBClassifier</strong> model, the following code block installs the <strong class="source-inline">joblib</strong> library:<p class="source-code">#install job lib library for model packaging</p><p class="source-code">!pip install joblib</p></li>
				<li>After installing the <strong class="source-inline">joblib</strong> library, the next step is to package the model object using it. The following code block packages the model and writes the model to a specific location on the filesystem:<p class="source-code">import joblib</p><p class="source-code">joblib.dump(xgb_model, '/content/customer_segment-v0.0')</p></li>
			</ol>
			<p>The preceding code block creates a file in the <strong class="source-inline">/content</strong> folder. To verify that, run an <strong class="source-inline">ls</strong> command and check whether the file exists. Let's also verify whether the model can be loaded and if we can run the <strong class="source-inline">predict</strong> function on it.</p>
			<ol>
				<li value="3">The following code block loads the model from the location <strong class="source-inline">/content/customer_segment-v0.0</strong> and runs predictions on a sample dataset:<p class="source-code">loaded_model = joblib.load('/content/customer_segment-v0.0')</p><p class="source-code">prediction = loaded_model.predict(X_test.head())</p><p class="source-code">prediction.tolist()</p></li>
			</ol>
			<p>The preceding code block should run without any errors and print the following prediction output:</p>
			<p class="source-code">[0.0, 0.0, 0.0, 2.0, 0.0]</p>
			<ol>
				<li value="4">Now that we have<a id="_idIndexMarker327"/> the packaged model, the next step is to register it in the model repository. Again, there are a bunch of tools available to use to manage the model, such as MLflow, SageMaker, and others. I would highly recommend using one of them as they handle a lot of use cases for sharing, deployment, standard versioning, and more. For simplicity, I will use an S3 bucket as the model registry here and upload the trained model to it. </li>
			</ol>
			<p>The following code uploads the packaged model into the S3 bucket:</p>
			<p class="source-code">import boto3</p>
			<p class="source-code">s3_client = boto3.client('s3')</p>
			<p class="source-code">s3_client.upload_file(</p>
			<p class="source-code">  '/content/customer_segment-v0.0', </p>
			<p class="source-code">  "feast-demo-mar-2022", </p>
			<p class="source-code">  "model-repo/customer_segment-v0.0")</p>
			<p>The preceding code block uploads the file S3 bucket, <strong class="source-inline">feast-demo-mar-2022</strong>, into the following prefix: <strong class="source-inline">model-repo/customer_segment-v0.0</strong>. Please verify this by visiting the AWS console to make sure the model is uploaded to the specified location.</p>
			<p>So far, we are done with model training and experimentation and have registered a candidate model in the model registry (S3 bucket). Let's create the model prediction notebook for a batch model use case in the next section.  </p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/>Batch model inference with Feast</h1>
			<p>In <a id="_idIndexMarker328"/>this section, let's look at how to run prediction for batch<a id="_idIndexMarker329"/> models. To perform prediction for a batch model, we need two things: one is a model and the other is a list of customers and their feature set for prediction. In the previous section, we created and registered a model in the model registry (which is S3). Also, the required features are available in the feature store. All we need is the list of customers for whom we need to run the predictions. The list of customers can be generated from the raw dataset as we did before, during model training. However, for the purpose of this exercise, we will take a small subset of customers and run predictions on them. </p>
			<p>Let's create a model prediction notebook and load the model that is registered in the model registry:</p>
			<ol>
				<li value="1">The following code block installs the required dependencies<a id="_idTextAnchor086"/> for the prediction notebook:<p class="source-code">!pip install feast[aws]==0.19.3 pandas xgboost joblib</p></li>
				<li>After installing the dependencies, the other required step is to fetch the feature repository if you haven't already. This is one of the common requirements in all the notebooks that use Feast. However, the process may not be the same in other feature stores. One of the reasons for this being Feast is SDK/CLI oriented. Other feature stores, such as SageMaker and Databricks, might just need the credentials to access it. We will look at an example in a later chapter.</li>
				<li>Assuming that you have cloned the Feast repository that was created in the previous chapter (which was also used during the model creation), the next step is to fetch the model from the model registry S3. </li>
			</ol>
			<p>The following code block downloads the model from the S3 location (the same location to which the model was uploaded):</p>
			<p class="source-code">import boto3</p>
			<p class="source-code">import os</p>
			<p class="source-code">#aws Credentials</p>
			<p class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key_id&gt;"</p>
			<p class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] = "&lt;aws_secret&gt;"</p>
			<p class="source-code">os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</p>
			<p class="source-code">#Download model from s3</p>
			<p class="source-code">model_name = "customer_segment-v0.0"</p>
			<p class="source-code">s3 = boto3.client('s3')</p>
			<p class="source-code">s3.download_file("feast-demo-mar-2022", </p>
			<p class="source-code">                 f"model-repo/{model_name}", </p>
			<p class="source-code">                 model_name)</p>
			<p>After executing the preceding code block, you should see a file named <strong class="source-inline">customer_segment-v0.0</strong> in the current work directory. You can verify it using the <strong class="source-inline">ls</strong> command or through the folder explorer.</p>
			<p class="callout-heading">Important Note </p>
			<p class="callout">Replace <strong class="source-inline">&lt;aws_key_id&gt;</strong> and <strong class="source-inline">&lt;aws_secret&gt;</strong> in the preceding code block with the user credentials created in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>.</p>
			<ol>
				<li value="4">The <a id="_idIndexMarker330"/>next step is to get the list of customers <a id="_idIndexMarker331"/>who need to be scored. As mentioned before, this can be fetched from the raw data source, but for the purpose of the exercise, I will be hardcoding a sample list of customers. To mimic fetching the customers from the raw data source, I will be invoking a function that returns the list of customers. </li>
			</ol>
			<p>The following code block displays the mock function to fetch customers from the raw data source:</p>
			<p class="source-code">def fetch_customers_from_raw_data():</p>
			<p class="source-code">  ## todo: code to fetch customers from raw data</p>
			<p class="source-code">  return ["12747.0", "12841.0", "12849.0", </p>
			<p class="source-code">          "12854.0", "12863.0"]</p>
			<p class="source-code">customer_to_be_scored=fetch_customers_from_raw_data()</p>
			<ol>
				<li value="5">Now that<a id="_idIndexMarker332"/> we have the list of customers to be scored, the <a id="_idIndexMarker333"/>next step is to fetch the features for these customers. There are different ways to do it. One way is to use the online store and the other is to use the offline store. For batch models, since latency is not a requirement, the most cost-efficient way is to use the offline store; it's just that offline stores need to be queried for the latest features. This can be done by using the <strong class="source-inline">event_timestamp</strong> column. Let's use the offline store and query the required features for the given customer list. To do that, we need the entity DataFrame. Let's create that next.</li>
				<li>The following code block creates the required entity DataFrame to fetch the latest features:<p class="source-code">import pandas as pd</p><p class="source-code">from datetime import datetime</p><p class="source-code">entity_df = pd.DataFrame(data={</p><p class="source-code">    "customerid": customer_to_be_scored,</p><p class="source-code">    "event_timestamp": datetime.now()</p><p class="source-code">})</p><p class="source-code">entity_df.head()</p></li>
			</ol>
			<p>The preceding code block outputs the following entity DataFrame:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B18024_05_006.jpg" alt="Figure 5.6 – Entity DataFrame for prediction&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Entity DataFrame for prediction</p>
			<p>To fetch the<a id="_idIndexMarker334"/> latest features for any customer, you <a id="_idIndexMarker335"/>need to set <strong class="source-inline">event_timestamp</strong> to <strong class="source-inline">datetime.now()</strong>. Let's use the entity DataFrame in <em class="italic">Figure 5.4</em> to query the offline store.</p>
			<ol>
				<li value="7">The following code block fetches the features for the given entity DataFrame:<p class="source-code">%cd customer_segmentation</p><p class="source-code">from feast import FeatureStore</p><p class="source-code">store = FeatureStore(repo_path=".")</p><p class="source-code">job = store.get_historical_features(</p><p class="source-code">    entity_df=entity_df,</p><p class="source-code">    features=[</p><p class="source-code">              "customer_rfm_features:recency", </p><p class="source-code">              "customer_rfm_features:monetaryvalue", </p><p class="source-code">              "customer_rfm_features:r", </p><p class="source-code">              "customer_rfm_features:m",</p><p class="source-code">              "customer_rfm_features:rfmscore",</p><p class="source-code">              "customer_rfm_features:segmenthighvalue",</p><p class="source-code">              "customer_rfm_features:segmentlowvalue",</p><p class="source-code">              "customer_rfm_features:segmentmidvalue"</p><p class="source-code">          ]</p><p class="source-code">    )</p><p class="source-code">pred_feature_data = job.to_df()</p><p class="source-code">pred_feature_data = pred_feature_data.dropna()</p><p class="source-code">pred_feature_data.head()</p></li>
			</ol>
			<p>The preceding code block produces the following output:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B18024_05_007.jpg" alt="Figure 5.7 – Features for prediction&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Features for prediction</p>
			<ol>
				<li value="8">Now that we <a id="_idIndexMarker336"/>have the features for prediction, the <a id="_idIndexMarker337"/>next step is to load the downloaded model and run the prediction for the customers using the features in <em class="italic">Figure 5.5</em>. The following code block does just that:<p class="source-code">import joblib</p><p class="source-code">## Drop unwanted columns</p><p class="source-code">features = pred_feature_data.drop(</p><p class="source-code">    ['customerid', 'event_timestamp'], axis=1)</p><p class="source-code">loaded_model = joblib.load('/content/customer_segment-v0.0')</p><p class="source-code">prediction = loaded_model.predict(features)</p></li>
				<li>The last step after running the prediction is to store the prediction results in a database or object store for later consumption. In this exercise, I will be writing the prediction results to an S3 bucket. Feel free to sink the results into other data stores. </li>
				<li>The following code block saves the prediction results along with the features in an S3 location:<p class="source-code">file_name = f"customer_ltv_pred_results_{datetime.now()}.parquet"</p><p class="source-code">pred_feature_data["predicted_ltvcluster"] = prediction.tolist()</p><p class="source-code">s3_url = f's3://feast-demo-mar-2022/prediction_results/{file_name}'</p><p class="source-code">pred_feature_data.to_parquet(s3_url)</p></li>
			</ol>
			<p>With that last<a id="_idIndexMarker338"/> code block, we are done with the implementation <a id="_idIndexMarker339"/>of a batch model. The question in your mind will be <em class="italic">how has the introduction of the feature store changed the ML life cycle so far?</em>. The early adoption of it decoupled the steps of feature engineering, model training, and model scoring. Any of them can be run independently without having to disturb the other parts of the pipeline. That's a huge benefit. The other part is deployment. The notebook we created in step one is concrete and does a specific job such as feature engineering, model training, and model scoring. </p>
			<p>Now, to productionize the model, all we need to do is schedule the feature engineering notebook and model scoring notebook using an orchestration framework and the model will be running at full scale. We will look at the productionization of the model in the next chapter. </p>
			<p>In the next section, let's look at what needs to be done for the online model use case.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor087"/>Online model inference with Feast</h1>
			<p>In the last section, we <a id="_idIndexMarker340"/>discussed how to use Feast in batch model <a id="_idIndexMarker341"/>inference. Now, it's time to look at the online model use case. One of the requirements of online model inference is that it should return results in low latency and also be invoked from anywhere. One of the common paradigms is to expose the model as a REST API endpoint. In the <em class="italic">Model packaging</em> section, we logged the model using the <strong class="source-inline">joblib</strong> library. That model needs to be wrapped with the RESTful framework to be deployable as a REST endpoint. Not only that but the features also need to be fetched in real time when the inference endpoint is invoked. Unlike in <a href="B18024_01_ePub.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">An Overview of the Machine Learning Life Cycle</em>, where we didn't have the <a id="_idIndexMarker342"/>infrastructure for serving features in real time, here, we <a id="_idIndexMarker343"/>already have that in place thanks to Feast. However, we need to run the command to sync offline features to the online store using the Feast library. Let's do that first. Later, we will look into packaging.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor088"/>Syncing the latest features from the offline to the online store</h2>
			<p>To load features <a id="_idIndexMarker344"/>from the offline to the online store, we need the Feast library:</p>
			<ol>
				<li value="1">Let's open a notebook and install the required dependencies: <p class="source-code">!pip install feast[aws]==0.19.3</p></li>
				<li>After installing the required dependencies, clone the feature store repository As mentioned before, this is a requirement for all notebooks. Assuming you have cloned the repository in the current working directory, the following command will load the latest features from the offline to the online store:<p class="source-code">%cd customer_segmentation/</p><p class="source-code">from datetime import datetime</p><p class="source-code">import os</p><p class="source-code">#aws Credentials</p><p class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "&lt;aws_key_id&gt;"</p><p class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] = "&lt;aws_secret&gt;"</p><p class="source-code">os.environ["AWS_DEFAULT_REGION"] = "us-east-1"</p><p class="source-code"># Command to sync offline features into online.</p><p class="source-code">!feast materialize-incremental {datetime.now().isoformat()}</p></li>
			</ol>
			<p>The preceding command outputs the progress as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B18024_05_008.jpg" alt="Figure 5.8 – Sync the offline to the online store&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – Sync the offline to the online store</p>
			<ol>
				<li value="3">After loading<a id="_idIndexMarker345"/> the offline data into the online store, let's run a query on the online store and make sure that it works as expected. To query the online store, initialize the feature store object and invoke the <strong class="source-inline">get_online_features</strong> API, as shown in the following code block:<p class="source-code">import pandas as pd</p><p class="source-code">from feast import FeatureStore</p><p class="source-code">store = FeatureStore(repo_path=".")</p><p class="source-code">feature_vector = store.get_online_features(</p><p class="source-code">    features=[</p><p class="source-code">        "customer_rfm_features:recency", </p><p class="source-code">        "customer_rfm_features:monetaryvalue", </p><p class="source-code">        "customer_rfm_features:r", </p><p class="source-code">        "customer_rfm_features:m",</p><p class="source-code">    ],</p><p class="source-code">    entity_rows=[</p><p class="source-code">        {"customer": "12747.0"},</p><p class="source-code">        {"customer": "12841.0"},</p><p class="source-code">{"customer": "abcdef"},</p><p class="source-code">    ],</p><p class="source-code">).to_dict()</p><p class="source-code">df = pd.DataFrame(feature_vector)</p><p class="source-code">df.head()</p></li>
			</ol>
			<p>The preceding <a id="_idIndexMarker346"/>code block fetches the data from the online store (<strong class="bold">DynamoDB</strong>) at low latency. When you run the preceding block, you will notice how quickly it responds compared to the historical store queries. The output of the code block is as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B18024_05_009.jpg" alt="Figure 5.9 – Query the online store&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – Query the online store</p>
			<p>In <em class="italic">Figure 5.7</em>, the last row contains <strong class="source-inline">NaN</strong> values. That is an example of how Feast would respond if any of the given entity IDs don't exist in the online store. In this example, the customer with the ID <strong class="source-inline">abcdef</strong> doesn't exist in the feature store, hence it returns <strong class="source-inline">NaN</strong> values for the corresponding row.</p>
			<p>Now that the online store is ready with the latest features, let's look into packaging the model as a RESTful API next.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor089"/>Packaging the online model as a REST endpoint with Feast code</h2>
			<p>This part is<a id="_idIndexMarker347"/> more about software engineering than data engineering or data science skills. There<a id="_idIndexMarker348"/> are many REST API frameworks for Python that are available out there, namely <strong class="bold">Flask</strong>, <strong class="bold">Django</strong>, <strong class="bold">FastAPI</strong>, and <a id="_idIndexMarker349"/>others, which can be used for the implementation. The <a id="_idIndexMarker350"/>implementation is straightforward. We will expose the <strong class="source-inline">POST</strong> method endpoint, which will take a list of customer IDs as input and return the prediction list:</p>
			<ol>
				<li value="1">The following code block shows the API contract that will be implemented:<p class="source-code">POST /invocations</p><p class="source-code">{</p><p class="source-code">   "customer_list": ["id1", "id2", …]</p><p class="source-code">}</p><p class="source-code">Response: status 200</p><p class="source-code">{</p><p class="source-code">"predictions": [0, 1, …]</p><p class="source-code">}</p></li>
				<li>Now that we have the API contract, the next step is to choose the REST framework that we are going to use. There are different trade-offs in choosing one over the other among the existing REST frameworks. Since that is out of scope for this book, I will use <strong class="source-inline">fastapi</strong> (<a href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com/</a>) as it is an async framework. If you are familiar with other frameworks such as <strong class="source-inline">flask</strong> or <strong class="source-inline">django</strong>, feel free to use them. The prediction result will be the same irrespective of the framework you use. Whatever framework you choose, just remember that we will be dockerizing the REST API before deployment. </li>
			</ol>
			<p>To build the API, I will be using the PyCharm IDE. If you have another favorite IDE, feel free to use that. Also, for the development of the API and for running the API, we need the following libraries:  <strong class="source-inline">feast[aws]</strong>, <strong class="source-inline">uvicorn[standard]</strong>, <strong class="source-inline">fastapi</strong>, <strong class="source-inline">joblib</strong>, and <strong class="source-inline">xgboost</strong>. You can install the libraries using the <strong class="source-inline">pip install</strong> command. I will leave it up to you since the steps to install differ based on the IDE and the platform you are using and also personal preferences. However, I will be using <strong class="source-inline">virtualenv</strong> to manage my Python environment.</p>
			<p>The folder <a id="_idIndexMarker351"/>structure of my project looks as shown in the following figure. If you haven't noticed already, the feature repository is also copied into the same folder as we need to initialize the feature store object and also the online store for features:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B18024_05_010.jpg" alt="Figure 5.10 – Online model folder structure in the IDE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – Online model folder structure in the IDE</p>
			<ol>
				<li value="3">In the <strong class="source-inline">main.py</strong> file, let's define the APIs that we will be implementing. Copy the following code and paste it into the <strong class="source-inline">main.py</strong> file:<p class="source-code">from fastapi import FastAPI</p><p class="source-code">app = FastAPI()</p><p class="source-code">@app.get("/ping")</p><p class="source-code">def ping():</p><p class="source-code">    return {"ping": "ok"}</p><p class="source-code">@app.post("/invocations")</p><p class="source-code">def inference(customers: dict):</p><p class="source-code">    return customers</p></li>
			</ol>
			<p>As you can see in the preceding code block, there are two APIs: <strong class="source-inline">ping</strong> and <strong class="source-inline">inference</strong>:</p>
			<ul>
				<li> <strong class="source-inline">ping</strong>: The <strong class="source-inline">ping</strong> API is a health check endpoint that will be required when deploying the application. The ping URL will be used by the infrastructure, such as ECS or Kubernetes, to check whether the application is healthy.</li>
				<li><strong class="source-inline">inference</strong>: The <strong class="source-inline">inference</strong> API, on the other hand, will contain the logic for fetching the features for the given customers from the feature store, scoring against the model, and returning the results.</li>
			</ul>
			<ol>
				<li value="4">Once you've<a id="_idIndexMarker352"/> copied the preceding code and pasted it into <strong class="source-inline">main.py</strong> and saved, go to the terminal and run the following command: <p class="source-code"><strong class="bold">cd &lt;project_folder&gt;</strong></p><p class="source-code"><strong class="bold">uvicorn main:app --reload</strong></p></li>
				<li>The preceding commands will run the FastAPI server in a local server and print output similar to the following code block:<p class="source-code"><strong class="bold">$ uvicorn main:app --reload</strong></p><p class="source-code"><strong class="bold">INFO:     Will watch for changes in these directories: ['&lt;folder path&gt;']</strong></p><p class="source-code"><strong class="bold">INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)</strong></p><p class="source-code"><strong class="bold">INFO:     Started reloader process [24664] using watchgod</strong></p><p class="source-code"><strong class="bold">WARNING:  The --reload flag should not be used in production on Windows.</strong></p><p class="source-code"><strong class="bold">INFO:     Started server process [908]</strong></p><p class="source-code"><strong class="bold">INFO:     Waiting for application startup.</strong></p><p class="source-code"><strong class="bold">INFO:     Application startup complete.</strong></p><p class="callout-heading">Important Note</p><p class="callout">Make sure that you have activated the virtual environment in the terminal before running the command.</p></li>
				<li>Once the<a id="_idIndexMarker353"/> application is run, visit the URL <a href="http://127.0.0.1:8000/docs">http://127.0.0.1:8000/docs</a>. You should see a Swagger UI, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B18024_05_011.jpg" alt="Figure 5.11 – Swagger UI for the API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.11 – Swagger UI for the API</p>
			<p>We will be using the Swagger UI in <em class="italic">Figure 5.9</em> to invoke the APIs later. For now, feel free to play around, explore what is available, and invoke the APIs. </p>
			<ol>
				<li value="7">Now that we have the API structure set up, let's implement the <strong class="source-inline">inference</strong> API next. As mentioned, the <strong class="source-inline">inference</strong> API will read the features from the feature store and run predictions. </li>
				<li>We also<a id="_idIndexMarker354"/> need to load the model from the model repository. In our case, the repository is S3. Hence, we need code to download the model from the S3 location and load the model into the memory. The following code block downloads the model from S3 and loads it into the memory. Please note that this is a one-time activity during the initial application load. Hence, let's add the following code outside the functions in the <strong class="source-inline">main.py</strong> file:<p class="source-code">import boto3</p><p class="source-code">Import joblib</p><p class="source-code">model_name = "customer_segment-v0.0"</p><p class="source-code">s3 = boto3.client('s3')</p><p class="source-code">## download file from s3</p><p class="source-code">s3.download_file(</p><p class="source-code">    "feast-demo-mar-2022",</p><p class="source-code">    f"model-repo/{model_name}",</p><p class="source-code">    model_name)</p><p class="source-code">## Load the model into memory.</p><p class="source-code">loaded_model = joblib.load('customer_segment-v0.0')</p></li>
				<li>Now that the model is loaded into the memory, the next step is to initialize the feature store object. The initialization can also be outside the method since it is a one-time activity:<p class="source-code">#initialize the feature store object.</p><p class="source-code">store = FeatureStore(repo_path=os.path.join(os.getcwd(), "customer_segmentation"))</p></li>
				<li>As the <strong class="source-inline">customer_segmentation</strong> feature repository is at the same level as that of <strong class="source-inline">main.py</strong> file, as shown in <em class="italic">Figure 5.8</em>, I have set <strong class="source-inline">repo_path</strong> appropriately. The remaining logic to fetch features from the online store, run prediction, and return results goes into the <strong class="source-inline">inference</strong> method definition. The following<a id="_idIndexMarker355"/> code block contains the same. Copy the method and replace it in the <strong class="source-inline">main.py</strong> file:<p class="source-code">@app.post("/invocations")</p><p class="source-code">def inference(customers: dict):</p><p class="source-code">    ##Step1: list of features required for scoring the model</p><p class="source-code">    required_features = [</p><p class="source-code">        "customer_rfm_features:recency",</p><p class="source-code">        "customer_rfm_features:monetaryvalue",</p><p class="source-code">        "customer_rfm_features:r",</p><p class="source-code">        "customer_rfm_features:m",</p><p class="source-code">        "customer_rfm_features:rfmscore",</p><p class="source-code">        "customer_rfm_features:segmenthighvalue",</p><p class="source-code">        "customer_rfm_features:segmentlowvalue",</p><p class="source-code">        "customer_rfm_features:segmentmidvalue"</p><p class="source-code">    ]</p><p class="source-code">    ##step 2: get entity rows from the input</p><p class="source-code">    entity_rows = [{"customer": cust_id} for cust_id in customers["customer_list"]]</p><p class="source-code">    ##Step 3: query online store</p><p class="source-code">    feature_vector = store.get_online_features(</p><p class="source-code">        features=required_features,</p><p class="source-code">        entity_rows=entity_rows,</p><p class="source-code">    ).to_dict()</p><p class="source-code">    ##Step 4: convert features to dataframe and reorder the feature columns in the same order that model expects.</p><p class="source-code">    features_in_order = ['recency', 'monetaryvalue', </p><p class="source-code">                         'r', 'm', 'rfmscore', </p><p class="source-code">                         'segmenthighvalue', </p><p class="source-code">                         'segmentlowvalue', </p><p class="source-code">                         'segmentmidvalue']</p><p class="source-code">    df = pd.DataFrame(feature_vector)</p><p class="source-code">    features = df.drop(['customerid'], axis=1)</p><p class="source-code">    features = features.dropna()</p><p class="source-code">    features = features[features_in_order]</p><p class="source-code">    ##Step 5: run prediction and return the list</p><p class="source-code">    prediction = loaded_model.predict(features)</p><p class="source-code">    return {"predictions": prediction.tolist()}</p></li>
				<li>Now that <a id="_idIndexMarker356"/>the prediction logic is complete, let's run the application and try running the prediction. To run the application, the command is the same as the one used before:<p class="source-code"><strong class="bold">export AWS_ACCESS_KEY_ID=&lt;aws_key_id&gt;</strong></p><p class="source-code"><strong class="bold">export AWS_SECRET_ACCESS_KEY=&lt;aws_secret&gt;</strong></p><p class="source-code"><strong class="bold">export AWS_DEFAULT_REGION=us-east-1</strong></p><p class="source-code"><strong class="bold">cd &lt;project_folder&gt;</strong></p><p class="source-code"><strong class="bold">uvicorn main:app --reload</strong></p><p class="callout-heading">Important Note</p><p class="callout">Replace <strong class="source-inline">&lt;aws_key_id&gt;</strong> and <strong class="source-inline">&lt;aws_secret&gt;</strong> in the preceding code block with the user credentials created in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>.</p></li>
				<li>Once the<a id="_idIndexMarker357"/> application loads successfully, visit the Swagger UI URL (<a href="http://localhost:8000/docs">http://localhost:8000/docs</a>). In the Swagger UI, expand the <strong class="source-inline">invocations</strong> API and click on <strong class="bold">Try out</strong>. You should see a screen similar to the one in <em class="italic">Figure 5.12</em>. </li>
			</ol>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B18024_05_012.jpg" alt="Figure 5.12 – Swagger UI invocation API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.12 – Swagger UI invocation API</p>
			<ol>
				<li value="13">In the request body, provide the input as shown in <em class="italic">Figure 5.12</em> (the one in the following code block):<p class="source-code">{"customer_list":["12747.0", "12841.0"]}</p></li>
				<li>With this input, submit a request by clicking on <strong class="bold">Execute</strong>. The API should respond within milliseconds and the output will be visible when you scroll down on the screen. The following figure shows an example output:</li>
			</ol>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B18024_05_013.jpg" alt="Figure: 5.13 – Online model response&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure: 5.13 – Online model response</p>
			<p>That completes<a id="_idIndexMarker358"/> the steps of building a REST API for an online model with code to fetch features from Feast. Now that we have both the online and the batch model, in the next chapter, we will look at how to productionize these and how the transition from development to production is simple as we adopted the feature store and MLOps early. </p>
			<p>One thing that we are yet to look into is how to change/update or add additional features. Let's look at this briefly before we move on.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor090"/>Handling changes to the feature set during development</h1>
			<p>Model <a id="_idIndexMarker359"/>development is an evolving process. So are models – they evolve over time. Today, we may be using a few features for a specific model, but as and when we discover and try out new features, if the performance is better than the current model, we might end up including the new features in the model training and scoring. Hence, the feature set may change over time. What that means with the feature store is some of the steps we performed in <a href="B18024_04_ePub.xhtml#_idTextAnchor065"><em class="italic">Chapter 4</em></a>, <em class="italic">Adding Feature Store to ML Models</em>, might need to be revisited. Let's look at what<a id="_idIndexMarker360"/> those steps are. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The assumption here is feature definitions change during model development, not after production. We will look at how to handle changes to the feature set after the model goes into production in later chapters.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor091"/>Step 1 – Change feature definitions</h2>
			<p>If the features or<a id="_idIndexMarker361"/> entities change during the model development, the first step is to update the feature definitions in the feature repository If you recall correctly, when the features were finalized, the first thing that we did was to create feature definitions. In the feature repository, the file <strong class="source-inline">rfm_features.py</strong> contains the definitions. After making the changes, run the <strong class="source-inline">feast apply</strong> command to update the feature definition in the resource. If you create or delete new entities or views, the corresponding online store resources (DynamoDB tables) will be created or deleted. You can verify that in the console. If there are minor changes such as changing the data type or feature name, the changes will be saved in the feature repository registry. </p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor092"/>Step 2 – Add/update schema in the Glue/Lake Formation console</h2>
			<p>The second step<a id="_idIndexMarker362"/> is to define new tables in the Glue/Lake Formation database that we created. If the old tables are not required, you can delete them to avoid any confusion later. In case of the schema changes (if the feature name or data type changes), you need to update the existing schema to reflect the changes. If the schema is not updated with the changes, there will be errors when you query the historical store or try to load the latest feature from an offline to an online store. One other thing to note here is, when defining the schema, we set an S3 location for the feature views. Now that this location contains the old data, which works only with the old schema, you need to define a new path to which the data that adheres to the new schema will be written. </p>
			<p>An alternate approach would be to define a brand new table with the new schema definitions and new S3 path for data and also update the Redshift source definitions in the feature repository with the new table name. If you do that, you can query the data in both old and new definitions. However, keep in mind that you may be managing two versions of the feature set, one with the older schema and one with the new schema. Also, there will be<a id="_idIndexMarker363"/> two DynamoDB tables.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor093"/>Step 3 – Update notebooks with the changes</h2>
			<p>The last step is <a id="_idIndexMarker364"/>simple, which is to go update all the affected notebooks. In the feature engineering notebook, the update would be to write data into the new location, whereas in the model training and scoring notebook, it would be to update the feature name or fetch additional features during training and scoring respectively. </p>
			<p>These are the three steps you need to perform every time there are updates to the feature definitions. With that, let's summarize what we learned in this chapter, and in the next chapter, we will look at how to productionize the online and batch models that we built in the chapter and what the challenges are beyond production.  </p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor094"/>Summary </h1>
			<p>In this chapter, our aim was to look at how model training and scoring change with the feature store. To go through the training and scoring stages of the ML life cycle, we used the resources that were created in the last chapter. In the model training phase, we looked at how data engineers and data scientists can collaborate and work towards building a better model. In model prediction, we discussed batch model scoring and how using an offline store is a cost-effective way of running a batch model. We also built a REST wrapper for the online model and added Feast code to fetch the features for prediction during runtime. At the end of the chapter, we looked at the required changes if there are updates to features during development. </p>
			<p>In the next chapter, we will continue using the batch model and the online model that we built in this chapter, productionize them and look at what the challenges are once the models are in production. </p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor095"/>Further reading </h1>
			<p>You can find more information on Feast in the following references:</p>
			<ul>
				<li>Feast: <a href="https://docs.feast.dev/">https://docs.feast.dev/</a></li>
				<li>Feast AWS credit scoring tutorial: <a href="https://github.com/feast-dev/feast-aws-credit-scoring-tutorial">https://github.com/feast-dev/feast-aws-credit-scoring-tutorial</a></li>
			</ul>
		</div>
	</body></html>