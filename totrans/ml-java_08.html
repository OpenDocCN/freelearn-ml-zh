<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image Recognition with Deeplearning4j</h1>
                </header>
            
            <article>
                
<p> Images have become ubiquitous in web services, social networks, and web stores. In contrast to humans, computers have great difficulty in understanding what is in the image and what it represents. In this chapter, we'll first look at the challenges behind teaching computers how to understand images, and then focus on an approach based on deep learning. We'll look at a high-level theory thats required to configure a deep learning model and discuss how to implement a model that is able to classify images using a Java library, Deeplearning4j.</p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>Introducing image recognition</li>
<li>Discussing deep learning fundamentals</li>
<li>Building an image recognition model</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introducing image recognition</h1>
                </header>
            
            <article>
                
<p>A typical goal of image recognition is to detect and identify an object in a digital image. Image recognition is applied in factory automation to monitor product quality; surveillance systems to identify potentially risky activities, such as moving persons or vehicles; security applications to provide biometric identification through fingerprints, iris, or facial features; autonomous vehicles to reconstruct conditions on the road and environment; and so on.</p>
<p>Digital images are not presented in a structured way with attribute-based descriptions; instead, they are encoded as the amount of color in different channels, for instance, black-white and red-green-blue channels. The learning goal is to identify patterns that are associated with a particular object. The traditional approach for image recognition consists of transforming an image into different forms, for instance, to identify object corners, edges, same-color blobs, and basic shapes. Such patterns are then used to train a learner to distinguish between objects. Some notable examples of traditional algorithms are listed here:</p>
<ul>
<li>Edge detection finds boundaries of objects within an image</li>
<li>Corner detection identifies intersections of two edges or other interesting points, such as line endings, curvature maxima or minima, and so on</li>
<li>Blob detection identifies regions that differ in a property, such as brightness or color, compared to its surrounding regions</li>
<li>Ridge detection identifies additional interesting points in the image using smooth functions</li>
<li><strong>Scale invariant feature transform</strong> (<strong>SIFT</strong>) is a robust algorithm that can match objects, even if their scale or orientation differs from the representative samples in the database</li>
<li>Hough transform identifies particular patterns in the image</li>
</ul>
<p>A more recent approach is based on deep learning. Deep learning is a form of neural network, which mimics how the brain processes information. The main advantage of deep learning is that it's possible to design neural networks that can automatically extract relevant patterns, which in turn can be used to train a learner. With recent advances in neural networks, image recognition accuracy has significantly boosted. For instance, the <strong>ImageNet</strong> challenge, where competitors are provided more than 1.2 million images from 1,000 different object categories, reports that the error rate of the best algorithm was reduced from 28% in 2010, using <strong>support vector machines</strong> (<strong>SVM</strong>), to only 7% in 2014, using a deep neural network.</p>
<p>In this chapter, we'll take a quick look at neural networks, starting from the basic building block, the perceptron,<strong> </strong>and gradually introducing more complex structures.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Neural networks</h1>
                </header>
            
            <article>
                
<p>The first neural networks, which were introduced in the sixties, were inspired by biological neural networks. The idea of a neural network is to map the biological nervous system, that is, how the brain processes information. It consists of layers with interconnected neurons working together. In computer terms, they are also known as an <strong>artificial neural network</strong> (<strong>ANN</strong>). With computers, it requires training to make this model learn, the same as a human brain. A neuron in the brain gets activated on receiving a signal from nearby interconnected neurons, and the same applies to an ANN. Recent advances in neural networks has proved that deep neural networks fit very well in pattern recognition tasks, as they are able to automatically extract interesting features and learn the underlying presentation. In this section, we'll refresh ourselves on the fundamental structures and components, from a single perceptron to deep networks.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Perceptron</h1>
                </header>
            
            <article>
                
<p>A perceptron is a basic neural network building block and one of the earliest supervised algorithms. It is defined as a sum of features, which is multiplied by the corresponding weights and a bias. When the input signals is received, it multiplies with the assigned weights. These weights are defined for each incoming signal or input, and the weight gets adjusted continuously during the learning phase. The adjustment of weight depends on the error of the last result. After multiplying with the respective weights, all of the inputs are summed up with some offset value called <strong>bias</strong>. The value of the bias is also adjusted by the weights. So, it starts with random weights and bias, and with each iteration, the weights and bias are adjusted so that the next result moves toward the desired output. At the end, the final result is turned into an output signal. The function that sums all of this together is called the <strong>sum transfer function,</strong> and it is fed into an activation function. If the binary step activation function reaches a threshold, the output is 1, otherwise it is 0, which gives us a binary classifier. A schematic illustration is shown in the following diagram:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-683 image-border" src="Images/9d234876-bb61-48f0-98b4-33d0da465223.png" style="width:40.92em;height:15.00em;" width="791" height="290"/></div>
<p>Training perceptrons involves a fairly simple learning algorithm that calculates the errors between the calculated output values and correct training output values, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent. This algorithm is usually called the <strong>delta rule</strong>.</p>
<p>A single-layer perceptron is not very advanced, and nonlinearly separable functions, such as XOR, cannot be modeled using it. To address this issue, a structure with multiple perceptrons was introduced, called the <strong>multilayer perceptron</strong>, also known as the <strong>feedforward neural network</strong>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Feedforward neural networks</h1>
                </header>
            
            <article>
                
<p>A feedforward neural network is an ANN that consists of several perceptrons, which are organized into layers, as shown in the following diagram: input layer, output layer, and one or more hidden layers. The hidden layers have nothing to do with the outside world, <span>hence the name</span>. Each layer perceptron, also known as a neuron, has direct connections to the perceptrons in the next layer, whereas connections between two neurons carry a weight thats similar to the perceptron weights. So, all the perceptrons in one layer are connected with the perceptrons in the next layer, and the information is fed forward to the next layer. This diagram shows a network with a four-unit <strong>Input layer</strong>, corresponding to the size of the  feature vector of length <kbd>4</kbd>, a four-unit <strong>Hidden layer</strong>, and a two-unit <strong>Output layer</strong>, where each unit corresponds to one class value:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-684 image-border" src="Images/6ab8b4ab-f8aa-4184-8620-bd78071abb2c.png" style="width:38.92em;height:21.92em;" width="577" height="325"/></p>
<p>A feedforward neural network learns by finding the relationship between input and output values, which are fed into the network multiple times. The most popular approach to training multilayer networks is backpropagation. In backpropagation, the calculated output values are compared with the correct values in the same way as in the delta rule. The error is then fed back through the network by various techniques, adjusting the weights of each connection in order to reduce the value of the error. The error is calculated using the squared difference between the output value of the network and the original output value. The error indicates how far we are from the original output values. This process is repeated for a sufficiently large number of training cycles, until the error is under a certain threshold.</p>
<p>A feedforward neural network can have more than one hidden layer, where each additional hidden layer builds a new abstraction atop the preceding layers. This often leads to more accurate models; however, increasing the number of hidden layers leads to two known issues:</p>
<ul>
<li><strong>Vanishing gradients problem</strong>: With more hidden layers, the training with backpropagation becomes less and less useful for passing information to the front layers, causing these layers to train very slowly</li>
<li><strong>Overfitting</strong>: The model fits the training data too well and performs poorly on real examples</li>
</ul>
<p>Let's look at some other networks structures that address these issues.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Autoencoder</h1>
                </header>
            
            <article>
                
<p><strong>An autoencoder</strong> is a feedforward neural network that aims to learn how to compress the original dataset. Its aim is to copy input to its output. Therefore, instead of mapping features to the input layer and labels to the output layer, we will map the features to both the input and output layers. The number of units in the hidden layers is usually different from the number of units in the input layers, which forces the network to either expand or reduce the number of original features. This way, the network will learn the important features, while effectively applying dimensionality reduction.</p>
<p>An example network is shown in the following diagram. The three-unit input layer is first expanded into a four-unit layer and then compressed into a single-unit layer. The other side of the network restores the single layer unit back in to the four-unit layer, and then to the original three-input layer:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-685 image-border" src="Images/f229c5d1-e298-4e0f-b8e0-eaf36699f2be.png" style="width:38.67em;height:21.75em;" width="1200" height="675"/></p>
<p>Once the network is trained, we can take the left-hand side to extract the image features like we would with traditional image processing. It consists of encoders and decoders, where the encoder's work is to create or make hidden a layer or layers that captures the essence of the input, and the decoders reconstruct <span>the </span>input from the layers.</p>
<p>The autoencoders can be also combined into <strong>stacked autoencoders</strong>, as shown in the following diagram. First, we will discuss the hidden layer in a basic autoencoder, as described previously. Then, we will take the learned hidden layer (green circles) and repeat the procedure, which in effect learns a more abstract presentation. We can repeat this procedure multiple times, transforming the original features into increasingly reduced dimensions. At the end, we will take all of the hidden layers and stack them into a regular feedforward network, as shown at the top-right part of the diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-686 image-border" src="Images/e053137e-b22e-4ff8-ad2c-65eea70d5c4a.png" style="width:59.92em;height:20.17em;" width="982" height="331"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Restricted Boltzmann machine</h1>
                </header>
            
            <article>
                
<p><strong>A restricted Boltzman machine</strong> (<strong>RBM</strong>) is an undirected neural network, also denoted as <strong>generative stochastic networks</strong> (<strong>GSNs</strong>), and can learn probability distribution over its set of inputs. As the name suggests, they originate from the Boltzman machine, a recurrent neural network that was introduced in the eighties. In a Boltzmann machine, every node or neuron is connected with all other nodes, which makes it difficult to process when the node count increases. Restricted means that the neurons must form two fully connected layers, an input layer and a hidden layer, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-687 image-border" src="Images/4595eeb9-cbda-4f68-b845-a9f3b9bf91d6.png" style="width:17.42em;height:12.58em;" width="296" height="214"/></p>
<p>Unlike feedforward networks, the connections between the visible and hidden layers are undirected, hence the values can be propagated in both visible-to-hidden and hidden-to-visible directions.</p>
<p>Training RBMs is based on the contrastive divergence algorithm, which uses a gradient descent procedure, similar to backpropagation, to update weights, and Gibbs sampling is applied on the Markov chain to estimate the gradient, that is the direction on how to change the weights.</p>
<p>RBMs can also be stacked to create a class known as <strong>deep belief networks</strong> (<strong>DBNs</strong>). In this case, the hidden layer of an RBM acts as a visible layer for the RBM layer, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-688 image-border" src="Images/3710f522-4873-439c-a0b0-89625751661a.png" style="width:38.67em;height:19.67em;" width="1846" height="939"/></p>
<p>The training, in this case, is incremental: training layer by layer.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deep convolutional networks</h1>
                </header>
            
            <article>
                
<p>A network structure that recently achieved very good results at image recognition benchmarks is the <strong>convolutional neural network</strong> (<strong>CNN</strong>) or ConvNet. CNNs are a type of feedforward neural network that are structured in such a way that it emulates the behavior of the visual cortex, exploiting 2D structures of an input image, that is, patterns that exhibit spatially local correlation. It works on the basic principles of how the brain recalls or remembers images. We, as humans, remember images on the basis of features only. Given the features, our brain will start forming the image itself. In computers, consider the following diagram, which shows how a feature is detected:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-689 image-border" src="Images/28592e50-146d-45af-9de0-426d399d967f.png" style="width:91.33em;height:50.92em;" width="1096" height="611"/></p>
<p>In the same way, many features are detected from the image, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-690 image-border" src="Images/26759343-9494-4e76-a340-39649c60028c.png" style="width:102.17em;height:48.50em;" width="1226" height="582"/></p>
<p>A CNN consists of a number of convolutional and subsampling layers, optionally followed by fully connected layers. An example of this shown in the following diagram. The input layer reads all of the pixels in an image and then we apply multiple filters. In the following diagram, four different filters are applied. Each filter is applied to the original image; for example, one pixel of a 6 x 6 filter is calculated as the weighted sum of a 6 x 6 square of input pixels and corresponding 6 x 6 weights. This effectively introduces filters that are similar to standard image processing, such as smoothing, correlation, edge detection, and so on. The resulting image is called a <strong>feature map</strong>. In the example in the following diagram, we have four feature maps, one for each filter.</p>
<p>The next layer is the subsampling layer, which reduces the size of the input. Each feature map is subsampled typically with mean or max pooling over a contiguous region of 2 x 2 (up to 5 x 5 for large images). For example, if the feature map size is 16 x 16 and the subsampling region is 2 x 2, the reduced feature map size is 8 x 8, where 4 pixels (a 2 x 2 square) are combined into a single pixel by calculating the max, min, mean, or some other functions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-691 image-border" src="Images/eb58d84f-21ad-423b-b211-fd1e634e2a48.png" style="width:162.50em;height:40.58em;" width="1950" height="487"/></p>
<p>The network may contain several consecutive convolution and subsampling layers, as shown in the preceding diagram. A particular feature map is connected to the next reduced/convoluted feature map, while feature maps at the same layer are not connected to each other.</p>
<p>After the last subsampling or convolutional layer, there is usually a fully connected layer, identical to the layers in a standard multilayer neural network, which represents the target data.</p>
<p>A CNN is trained using a modified backpropagation algorithm that takes the subsampling layers into account and updates the convolutional filter weights based on all the values where this filter is applied.</p>
<div class="packt_infobox">Some good CNN designs can be found at the ImageNet competition results page: <a href="http://www.image-net.org/">http://www.image-net.org/</a>. An example is <em>AlexNet</em>, which is described in the <em>ImageNet Classification with Deep Covolutional Neural Networks</em> paper by <em>A. Krizhevsky et al</em>.</div>
<p>This concludes our review of the main neural network structures. In the following section, we'll move on to the actual implementation.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Image classification</h1>
                </header>
            
            <article>
                
<p>In this section, we will discuss how to implement some of the neural network structures with the Deeplearning4j library. Let's get started.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deeplearning4j</h1>
                </header>
            
            <article>
                
<p>As we discussed in <a href="6fd557d7-2807-4a6d-8f93-d7c4ca094b7e.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Java Libraries and Platforms for Machine Learning</em>, Deeplearning4j is an open source, distributed deep learning project in Java and Scala. Deeplearning4j relies on Spark and Hadoop for MapReduce, trains models in parallel, and iteratively averages the parameters they produce in a central model. A detailed library summary is presented in <a href="6fd557d7-2807-4a6d-8f93-d7c4ca094b7e.xhtml"><span class="ChapterrefPACKT">Chapter 2</span></a>, <em>Java Libraries and Platforms for Machine Learning</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting DL4J</h1>
                </header>
            
            <article>
                
<p>The most convenient way to get Deeplearning4j is through the Maven repository:</p>
<ol>
<li>Start a new Eclipse project and pick <span class="packt_screen">Maven Project</span>, as shown in the following screenshot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-692 image-border" src="Images/8aed3b3c-3950-4926-b37c-4bbfc82bc7df.png" style="width:36.67em;height:34.75em;" width="521" height="493"/></p>
<ol start="2">
<li>Open the <kbd>pom.xml</kbd> file and add the following dependencies under the <kbd>&lt;dependencies&gt;</kbd> section:</li>
</ol>
<pre>&lt;dependency&gt; 
    &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt; 
    &lt;artifactId&gt;deeplearning4j-nlp&lt;/artifactId&gt; 
   &lt;version&gt;${dl4j.version}&lt;/version&gt; 
&lt;/dependency&gt; 
 
&lt;dependency&gt; 
    &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt; 
    &lt;artifactId&gt;deeplearning4j-core&lt;/artifactId&gt; 
    &lt;version&gt;${dl4j.version}&lt;/version&gt; 
&lt;/dependency&gt; </pre>
<ol start="3">
<li>Finally, right-click on <span class="packt_screen">Project</span>, select <span class="packt_screen">Maven</span>, and pick <span class="packt_screen">Update project</span>.</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">MNIST dataset</h1>
                </header>
            
            <article>
                
<p>One of the most famous datasets is <span>the </span>MNIST dataset, which consists of handwritten digits, as shown in the following image. The dataset consists of 60,000 training and 10,000 test images:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-693 image-border" src="Images/d3340f60-5ca7-425a-bb00-97edef834272.png" style="width:41.92em;height:8.33em;" width="560" height="111"/></p>
<p>The dataset is commonly used in image recognition problems to benchmark algorithms. The worst recorded error rate is 12%, with no preprocessing and using an SVM in a one-layer neural network. Currently, as of 2016, the lowest error rate is only 0.21%, using the <strong>DropConnect</strong> neural network, followed by a deep convolutional network at 0.23%, and a deep feedforward network at 0.35%.</p>
<p>Now, let's look at how to load the dataset.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Loading the data</h1>
                </header>
            
            <article>
                
<p>Deeplearning4j provides the MNIST dataset loader out of the box. The loader is initialized as <kbd>DataSetIterator</kbd>. First let's import the <kbd>DataSetIterator</kbd> class and all of the supported datasets that are part of the <kbd>impl</kbd> package, for example, iris, MNIST, and others:</p>
<pre>import org.deeplearning4j.datasets.iterator.DataSetIterator; 
import org.deeplearning4j.datasets.iterator.impl.*; </pre>
<p>Next, we'll define some constants, for instance, the images consist of 28 x 28 pixels and there are 10 target classes and 60,000 samples. We'll initialize a new <kbd>MnistDataSetIterator</kbd> class that will download the dataset and its labels. The parameters are the iteration batch size, total number of examples, and whether the datasets should be binarized or not:</p>
<pre>int numRows = 28; 
int numColumns = 28; 
int outputNum = 10;<br/>int numSamples = 60000;<br/>int batchSize = 100;<br/>int iterations = 10;<br/>int seed = 123;
DataSetIterator iter = new MnistDataSetIterator(batchSize, 
numSamples,true);  </pre>
<p>Having an already-implemented data importer is really convenient, but it won't work on your data. Let's take a quick look at how it is implemented and what needs to be modified to support your dataset. If you're eager to start implementing neural networks, you can safely skip the rest of this section and return to it when you need to import your own data.</p>
<div class="mce-root packt_tip"><span>To load the custom data, you'll need to implement two classes: </span><kbd>DataSetIterator</kbd><span>, which holds all of the information about the dataset, and </span><kbd>BaseDataFetcher</kbd><span>, which actually pulls the data either from a file, database, or the web. Sample implementations are available on GitHub at </span><span class="URLPACKT"><a href="https://github.com/deeplearning4j/deeplearning4j/tree/master/deeplearning4j-core/src/main/java/org/deeplearning4j/datasets/iterator/impl">https://github.com/deeplearning4j/deeplearning4j/tree/master/deeplearning4j-core/src/main/java/org/deeplearning4j/datasets/iterator/impl</a></span><span>. <br/></span><span>Another option is to use the </span><strong>Canova</strong><span> library, which was developed by the same authors, at </span><span class="URLPACKT"><a href="http://deeplearning4j.org/canovadoc/">http://deeplearning4j.org/canovadoc/</a></span><span>.</span></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building models</h1>
                </header>
            
            <article>
                
<p>In this section, we'll discuss how to build an actual neural network model. We'll start with a basic single-layer neural network to establish a benchmark and discuss the basic operations. Later, we'll improve this initial result with DBN and a multilayer convolutional network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a single-layer regression model</h1>
                </header>
            
            <article>
                
<p>Let's start by building a single-layer regression model based on the softmax activation function, as shown in the following diagram. As we have a single layer, <strong>Input</strong> to the neural network will be all the figure pixels, that is, 28 x 28 = <strong>748</strong> neurons. The number of <strong>Output</strong> neurons is <strong>10</strong>, one for each digit. The network layers are fully connected, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-694 image-border" src="Images/7a5ce7ad-1fe2-46db-95f4-8e456cefd72d.png" style="width:24.92em;height:30.83em;" width="400" height="494"/></p>
<p>A neural network is defined through a <kbd>NeuralNetConfiguration.Builder()</kbd> object as follows:</p>
<pre>MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder() </pre>
<p>We will define the parameters for gradient search in order to perform iterations with the conjugate gradient optimization algorithm. The <kbd>momentum</kbd> parameter determines how fast the optimization algorithm converges to an local optimum. The higher the <kbd>momentum</kbd>, the faster the training; but higher speed can lower the model's accuracy:</p>
<pre>.seed(seed) 
.gradientNormalization(GradientNormalization.ClipElementWiseAbsolu<br/>   teValue) 
   .gradientNormalizationThreshold(1.0) 
   .iterations(iterations) 
   .momentum(0.5) 
   .momentumAfter(Collections.singletonMap(3, 0.9)) 
   .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT) </pre>
<p>Next, we will specify that the network has one layer and also define the error function, <kbd>NEGATIVELOGLIKELIHOOD</kbd>, internal perceptron activation function, <kbd>softmax</kbd>, and the number of input and output layers that correspond to the total<br/>
image pixels and the number of target variables, as shown in the following code block:</p>
<pre>.list(1) 
.layer(0, new  
OutputLayer.Builder(LossFunction.NEGATIVELOGLIKELIHOOD) 
.activation("softmax") 
.nIn(numRows*numColumns).nOut(outputNum).build()) </pre>
<p>Finally, we will set the network to <kbd>pretrain</kbd>, disable backpropagation, and actually build the untrained network structure:</p>
<pre>   .pretrain(true).backprop(false) 
   .build(); </pre>
<p>Once the network structure is defined, we can use it to initialize a new <kbd>MultiLayerNetwork</kbd>, as follows:</p>
<pre>MultiLayerNetwork model = new MultiLayerNetwork(conf); 
model.init(); </pre>
<p>Next, we will point the model to the training data by calling the <kbd>setListeners</kbd> method, as follows:</p>
<pre>model.setListeners(Collections.singletonList((IterationListener) <br/>   new ScoreIterationListener(listenerFreq))); </pre>
<p>We will also call the <kbd>fit(int)</kbd> method to trigger end-to-end network training:</p>
<pre>model.fit(iter);  </pre>
<p>To evaluate the model, we will initialize a new <kbd>Evaluation</kbd> object that will store batch results:</p>
<pre>Evaluation eval = new Evaluation(outputNum); </pre>
<p>We can then iterate over the dataset in batches in order to keep the memory consumption at a reasonable rate and store the results in an <kbd>eval</kbd> object:</p>
<pre>DataSetIterator testIter = new MnistDataSetIterator(100,10000); 
while(testIter.hasNext()) { 
    DataSet testMnist = testIter.next(); 
    INDArray predict2 =  
    model.output(testMnist.getFeatureMatrix()); 
    eval.eval(testMnist.getLabels(), predict2); 
} </pre>
<p>Finally, we can get the results by calling the <kbd>stats()</kbd> function:</p>
<pre>log.info(eval.stats()); </pre>
<p>A basic one-layer model achieves the following accuracy:</p>
<pre>    <strong>Accuracy:  0.8945 </strong>
    <strong>Precision: 0.8985</strong>
    <strong>Recall:    0.8922</strong>
    <strong>F1 Score:  0.8953</strong>
  </pre>
<p>Getting 89.22% accuracy, that is, a 10.88% error rate, on <span>the </span>MNIST dataset is quite bad. We'll improve this by going from a simple one-layer network to the moderately sophisticated deep belief network using Restricted Boltzmann machines and a Multilayer Convolutional Network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a deep belief network</h1>
                </header>
            
            <article>
                
<p>In this section, we'll build a deep belief network (DBN) based on the RBM, as shown in the following diagram. The network consists of four layers. The first layer recedes the <strong>748</strong> inputs to <strong>500</strong> neurons, then to <strong>250</strong>, followed by <strong>200</strong>, and finally to the last <strong>10</strong> target values:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-695 image-border" src="Images/7859d3a0-9129-4f63-9274-6e95a9819d6a.png" style="width:45.83em;height:31.83em;" width="794" height="551"/></p>
<p>As the code is the same as in the previous example, let's take a look at how to configure such a network:</p>
<pre>MultiLayerConfiguration conf = new <br/>   NeuralNetConfiguration.Builder() </pre>
<p>We will define the gradient optimization algorithm, as shown in the following code:</p>
<pre>    .seed(seed) 
    .gradientNormalization( 
    GradientNormalization.ClipElementWiseAbsoluteValue) 
    .gradientNormalizationThreshold(1.0) 
    .iterations(iterations) 
    .momentum(0.5) 
    .momentumAfter(Collections.singletonMap(3, 0.9)) 
    .optimizationAlgo(OptimizationAlgorithm.CONJUGATE_GRADIENT) </pre>
<p>We will also specify that our network will have four layers:</p>
<pre>   .list(4) </pre>
<p>The input to the first layer will be <kbd>748</kbd> neurons and the output will be <kbd>500</kbd> neurons. We'll use the root mean squared error cross entropy, and the Xavier algorithm to initialize weights by automatically determining the scale of initialization based on the number of input and output neurons, as follows:</p>
<pre>.layer(0, new RBM.Builder() 
.nIn(numRows*numColumns) 
.nOut(500)          
.weightInit(WeightInit.XAVIER) 
.lossFunction(LossFunction.RMSE_XENT) 
.visibleUnit(RBM.VisibleUnit.BINARY) 
.hiddenUnit(RBM.HiddenUnit.BINARY) 
.build()) </pre>
<p>The next two layers will have the same parameters, except for the number of input and output neurons:</p>
<pre>.layer(1, new RBM.Builder() 
.nIn(500) 
.nOut(250) 
.weightInit(WeightInit.XAVIER) 
.lossFunction(LossFunction.RMSE_XENT) 
.visibleUnit(RBM.VisibleUnit.BINARY) 
.hiddenUnit(RBM.HiddenUnit.BINARY) 
.build()) 
.layer(2, new RBM.Builder() 
.nIn(250) 
.nOut(200) 
.weightInit(WeightInit.XAVIER) 
.lossFunction(LossFunction.RMSE_XENT) 
.visibleUnit(RBM.VisibleUnit.BINARY) 
.hiddenUnit(RBM.HiddenUnit.BINARY) 
.build()) </pre>
<p>Now, the last layer will map the neurons to outputs, where we'll use the <kbd>softmax</kbd> activation function, as follows:</p>
<pre>.layer(3, new OutputLayer.Builder() 
.nIn(200) 
.nOut(outputNum) 
.lossFunction(LossFunction.NEGATIVELOGLIKELIHOOD) 
.activation("softmax") 
.build()) 
.pretrain(true).backprop(false) 
.build(); </pre>
<p>The rest of the training and evaluation is the same as in the single-layer network example. Note that training a deep network might take significantly more time compared to a single-layer network. The accuracy should be around 93%.</p>
<p>Now, let's take a look at another deep network.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a multilayer convolutional network</h1>
                </header>
            
            <article>
                
<p>In this final example, we'll discuss how to build a convolutional network, as shown in the following diagram. The network will consist of seven layers. First, we'll repeat two pairs of convolutional and subsampling layers with max pooling. The last subsampling layer is then connected to a densely connected feedforward neuronal network, consisting of 120 neurons, 84 neurons, and 10 neurons in the last three layers, respectively. Such a network effectively forms the complete image recognition pipeline, where the first four layers correspond to feature extraction and the last three layers correspond to the learning model:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-696 image-border" src="Images/56a26eae-f5f1-4d22-b174-2c2c43484a8a.png" style="width:129.17em;height:49.17em;" width="1550" height="590"/></p>
<p>Network configuration is initialized as we did earlier:</p>
<pre>MultiLayerConfiguration.Builder conf = new <br/>   NeuralNetConfiguration.Builder() </pre>
<p>We will specify the gradient descent algorithm and its parameters, as follows:</p>
<pre>.seed(seed) 
.iterations(iterations) 
.activation("sigmoid") 
.weightInit(WeightInit.DISTRIBUTION) 
.dist(new NormalDistribution(0.0, 0.01)) 
.learningRate(1e-3) 
.learningRateScoreBasedDecayRate(1e-1) 
.optimizationAlgo( 
OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT) </pre>
<p>We will also specify the seven network layers, as follows:</p>
<pre>.list(7) </pre>
<p>The input to the first convolutional layer is the complete image, while the output is six feature maps. The convolutional layer will apply a 5 x 5 filter, and the result will be stored in a 1 x 1 cell:</p>
<pre>.layer(0, new ConvolutionLayer.Builder( 
    new int[]{5, 5}, new int[]{1, 1}) 
    .name("cnn1") 
    .nIn(numRows*numColumns) 
    .nOut(6) 
    .build()) </pre>
<p>The second layer is a subsampling layer that will take a 2 x 2 region and store the max result in a 2 x 2 element:</p>
<pre>.layer(1, new SubsamplingLayer.Builder( 
SubsamplingLayer.PoolingType.MAX,  
new int[]{2, 2}, new int[]{2, 2}) 
.name("maxpool1") 
.build()) </pre>
<p>The next two layers will repeat the previous two layers:</p>
<pre>.layer(2, new ConvolutionLayer.Builder(new int[]{5, 5}, new <br/>   int[]{1, 1}) 
    .name("cnn2") 
    .nOut(16) 
    .biasInit(1) 
    .build()) 
.layer(3, new SubsamplingLayer.Builder<br/>   (SubsamplingLayer.PoolingType.MAX, new <br/>   int[]{2, 2}, new int[]{2, 2}) 
    .name("maxpool2") 
    .build()) </pre>
<p>Now, we will wire the output of the subsampling layer into a dense feedforward network, consisting of <kbd>120</kbd> neurons, and then through another layer, into <kbd>84</kbd> neurons, as follows:</p>
<pre>.layer(4, new DenseLayer.Builder() 
    .name("ffn1") 
    .nOut(120) 
    .build()) 
.layer(5, new DenseLayer.Builder() 
    .name("ffn2") 
    .nOut(84) 
    .build()) </pre>
<p>The final layer connects <kbd>84</kbd> neurons with <kbd>10</kbd> output neurons:</p>
<pre>.layer(6, new OutputLayer.Builder<br/>   (LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD) 
    .name("output") 
    .nOut(outputNum) 
    .activation("softmax") // radial basis function required 
    .build()) 
.backprop(true) 
.pretrain(false) 
.cnnInputSize(numRows,numColumns,1); </pre>
<p>To train this structure, we can reuse the code that we developed in the previous two examples. Again, the training might take some time. The network accuracy should be around 98%.</p>
<div class="packt_infobox"><span>Since model training significantly relies on linear algebra, training can be significantly sped up by using a </span><strong>graphics processing unit</strong><span> (</span><strong>GPU</strong><span>) for an order of magnitude. As the GPU backend, at the time of writing this book, is undergoing a rewrite, please check the latest documentation at </span><span class="URLPACKT"><a href="http://deeplearning4j.org/documentation">http://deeplearning4j.org/documentation</a>.</span></div>
<p>As we saw in different examples, increasingly complex neural networks allow us to extract relevant features automatically, thus completely avoiding traditional image processing. However, the price we pay for this is an increased processing time and a lot of learning examples to make this approach efficient.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we discussed how to recognize patterns in images in order to distinguish between different classes by covering fundamental principles of deep learning and discussing how to implement them with the Deeplearning4j library. We started by refreshing the basic neural network structure and discussed how to implement them to solve handwritten digit recognition problems.</p>
<p>In the next chapter, we'll look into patterns<span> </span><span>further</span><span>; however, instead of patterns in images, we'll tackle patterns with temporal dependencies, which can be found in sensor data.</span></p>


            </article>

            
        </section>
    </div>



  </body></html>