- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decreasing Bias and Achieving Fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fairness is an important topic when it comes to using machine learning across
    different industries, as we discussed in [*Chapter 3*](B16369_03.xhtml#_idTextAnchor119),
    *Debugging toward Responsible AI*. In this chapter, we will provide you with some
    widely used notions and definitions of fairness in machine learning settings,
    as well as how to use fairness and explainability Python libraries that are designed
    to not only help you in assessing fairness in your models but also improve them
    in this regard.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter includes many figures and code examples to help you better understand
    these concepts and start benefiting from them in your projects. Note that one
    chapter is far from enough to make you an expert on the topic of fairness, but
    this chapter will provide you with the necessary knowledge and tools to start
    practicing this subject in your projects. You can learn more about this topic
    using more advanced resources dedicated to machine learning fairness.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Fairness in machine learning modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sources of bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using explainability techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fairness assessment and improvement in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have learned about some technical details
    and Python tools that you can use to assess fairness and reduce biases in your
    models. You will also learn how to benefit from the machine learning explainability
    techniques you learned about in [*Chapter 6*](B16369_06.xhtml#_idTextAnchor201),
    *Interpretability and Explainability in Machine* *Learning Modeling*.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Python library requirements:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sklearn` >= 1.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy` >= 1.22.4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytest` >= 7.2.2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shap` >= 0.41.0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aif360` >= 0.5.0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fairlearn` >= 0.8.0'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic knowledge of the machine learning explainability concepts discussed in
    the previous chapter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter07](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter07).
  prefs: []
  type: TYPE_NORMAL
- en: Fairness in machine learning modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To assess fairness, we need to have specific considerations in mind and then
    use proper metrics to quantify fairness in our models. *Table 7.1* provides you
    with some of the considerations, definitions, and approaches to either evaluate
    or achieve fairness in machine learning modeling. We will go through the mathematical
    definitions of **demographic parity**, **equality of odds** or **equalized odds**,
    and **equality of opportunity** here as different group fairness definitions.
    Group fairness definitions ensure the fairness of groups of people with common
    attributes and characteristics instead of individuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topics in Machine** **Learning Fairness** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Demographic parity | Ensures predictions are not dependent on a given sensitive
    attribute, such as ethnicity, sex, or race |'
  prefs: []
  type: TYPE_TB
- en: '| Equality of odds | Ensures the independence of predictions to a given sensitive
    attribute, such as ethnicity, sex, or race given a true output |'
  prefs: []
  type: TYPE_TB
- en: '| Equality of opportunity | Ensures the equality of opportunities provided
    for individuals or groups of people |'
  prefs: []
  type: TYPE_TB
- en: '| Individual fairness | Ensures fairness for individuals rather than groups
    of people with common attributes |'
  prefs: []
  type: TYPE_TB
- en: '| Consistency | Provides consistency in decision-making not only between similar
    data points or users but also across time |'
  prefs: []
  type: TYPE_TB
- en: '| Fairness through unawareness | Achieves fairness if you’re unaware of sensitive
    attributes in decision making |'
  prefs: []
  type: TYPE_TB
- en: '| Fairness through transparency | Improves fairness through transparency and
    trust building through explainability |'
  prefs: []
  type: TYPE_TB
- en: Table 7.1 – Some important topics and considerations in fairness in machine
    learning and artificial intelligence
  prefs: []
  type: TYPE_NORMAL
- en: 'Demographic parity is a group fairness definition that ensures that a model’s
    predictions are not dependent on a given sensitive attribute, such as ethnicity
    or sex. Mathematically, we can define it as the equality of probability of predicting
    a class, such as C i, for different groups of a given attribute, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: P(C = C i|G = g 1) = P(C = C i|G = g 2)
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the meaning of demographic parity, we can consider the
    following examples, which meet fairness according to demographic parity:'
  prefs: []
  type: TYPE_NORMAL
- en: The same percentage of bail denial in each race group in COMPAS. We covered
    COMPAS in [*Chapter 3*](B16369_03.xhtml#_idTextAnchor119), *Debugging toward*
    *Responsible AI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same acceptance rate for loan applications between men and women.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same likelihood of hospitalization between poor and rich neighborhoods.
    We covered more about this problem in [*Chapter 3*](B16369_03.xhtml#_idTextAnchor119),
    *Debugging toward* *Responsible AI*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disparate impact ratio** (**DIR**) is a metric that quantifies the deviation
    from equality based on demographic parity:'
  prefs: []
  type: TYPE_NORMAL
- en: DIR =  P(C = 1|G = g 1)  _____________  P(C = 1|G = g 2)
  prefs: []
  type: TYPE_NORMAL
- en: The DIR value’s range is [0, ∞), where a value of 1 satisfies demographic parity
    while deviation toward higher or lower values translates to deviation from fairness
    based on this definition. DIR values of greater and less than 1 are referred to
    as negative and positive bias, respectively, considering the group we use in the
    numerator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the importance of demographic parity in fairness, it has its limitations.
    For example, in the case of DIR in the data itself (that is, the difference in
    class prevalence between different groups), a perfect model will not meet demographic
    parity criteria. Also, it doesn’t reflect the quality of predictions for each
    group. Other definitions help us improve our fairness assessment. Equality of
    odds or equalized odds is one such definition. Equalized odds is satisfied when
    a given prediction is independent of the group of a given sensitive attribute
    and the real output:'
  prefs: []
  type: TYPE_NORMAL
- en: P( ˆ y |y, G = g 1) = P( ˆ y |y, G = g 2) = P( ˆ y |y)
  prefs: []
  type: TYPE_NORMAL
- en: 'The definition of equality of opportunity is very similar to equalized odds,
    which assesses the independence of a prediction concerning groups for a given
    real output. But equality of opportunity focuses on a particular label of true
    values. Usually, the positive class is considered the target class and is representative
    of providing an opportunity for individuals, such as admission to school or having
    a high salary. Here is a formula for equality of opportunity:'
  prefs: []
  type: TYPE_NORMAL
- en: P( ˆ y |y = 1, G = g 1) = P( ˆ y |y = 1, G = g 2) = P( ˆ y |y = 1)
  prefs: []
  type: TYPE_NORMAL
- en: According to these notions of fairness, each could give you a different result.
    You need to consider the differences between different notions so that you don’t
    generalize fairness based on one definition or another.
  prefs: []
  type: TYPE_NORMAL
- en: Proxies for sensitive variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the challenges in assessing fairness in machine learning models is the
    existence of proxies for sensitive attributes such as sex and race. These proxies
    could be among the major contributors in generating model outputs and could result
    in bias in our models to specific groups. However, we cannot simply remove them
    as this could have a significant effect on performance. *Table 7.2* provides some
    examples of these proxies for different sensitive attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Sensitive Variable** | **Example Proxies** |'
  prefs: []
  type: TYPE_TB
- en: '| Sex | Level of education, salary and income (in some countries), occupation,
    history of a felony charge, keywords in user-generated content (for example, in
    a resume or social media), being a university faculty |'
  prefs: []
  type: TYPE_TB
- en: '| Race | History of a felony charge, keywords in user-generated content (for
    example, in a resume or social media), ZIP or postal code |'
  prefs: []
  type: TYPE_TB
- en: '| Disabilities | Speed of walking, eye movement, body posture |'
  prefs: []
  type: TYPE_TB
- en: '| Marital status | Level of education, salary and income (in some countries),
    and house size and number of bedrooms |'
  prefs: []
  type: TYPE_TB
- en: '| Age | Posture and keywords in user-generated content (for example, in a resume
    or social media) |'
  prefs: []
  type: TYPE_TB
- en: Table 7.2 – Examples of proxies for some of the important sensitive variables,
    in the context of fairness (Caton and Haas, 2020)
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned about the importance of fairness and some important
    definitions under this topic, let’s review some of the possible sources of bias
    that play against your goal of achieving fairness in your models.
  prefs: []
  type: TYPE_NORMAL
- en: Sources of bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different sources of bias in a machine learning life cycle. Bias could
    exist in the collected data, introduced in the data subsampling, cleaning and
    filtering, or model training and selection. Here, we will review examples of such
    sources to help you better understand how to avoid or detect such biases throughout
    the life cycle of a machine learning project.
  prefs: []
  type: TYPE_NORMAL
- en: Biases introduced in data generation and collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The data that we feed into our models could be biased by default, even before
    the modeling starts. The first source of such biases we want to review here is
    the issue of dataset size. Consider a dataset as a sample of a bigger population
    – for example, a survey of 100 students or the loan application information of
    200 customers of a bank. The small size of these datasets could increase the chance
    of bias. Let’s simulate this with a simple random data generation. We will write
    a function that generates two vectors of random binary values using `np.random.randint()`
    and then calculates *DIR* between the two groups of 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s use this function to calculate DIR for 1,000 different groups of
    different sizes, including `50`, `100`, `1000`, `10000`, and `1000000` data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following boxplots show the distributions of *DIR* across different sample
    sizes. You can see that lower sample sizes have wider distributions covering very
    low or high *DIR* values, distant from the ideal case of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Distributions of DIR across different sampling sizes](img/B16369_07_01..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Distributions of DIR across different sampling sizes
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also calculate the percentage of sampled groups of different sizes that
    don’t pass a specific threshold, such as >=0.8 and <=1.2\. *Figure 7**.2* shows
    that higher dataset sizes result in a lower chance of having datasets that have
    positive or negative bias given a sensitive attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Percentage of sets of samples that don’t pass DIR thresholds](img/B16369_07_02..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Percentage of sets of samples that don’t pass DIR thresholds
  prefs: []
  type: TYPE_NORMAL
- en: 'The source of existing bias in datasets might not just be an artifact of a
    small sample size. For example, if you were to train a model to predict if an
    individual will end up in STEM, which is an acronym for fields of science, technology,
    engineering, and math, then you must consider the reality of the existence of
    it being imbalanced toward men over women in the corresponding data in fields,
    such as engineering, even up until recently (*Figure 7**.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3 – Percentage of women in STEM jobs between 1970 and 2019](img/B16369_07_03..jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Percentage of women in STEM jobs between 1970 and 2019
  prefs: []
  type: TYPE_NORMAL
- en: Having less than 20% of engineers being women over the years, because of their
    lower interest, bias in hiring processes, or stereotypes in society, has resulted
    in bias in the data on workers in this field. If this is not rectified with fairness
    in your data processing and modeling tasks, it could result in predicting a higher
    chance for men getting into STEM compared to women, despite their talents, knowledge,
    and experience.
  prefs: []
  type: TYPE_NORMAL
- en: There is another category of intrinsic bias in the data, although it needs to
    be considered when developing machine learning models. For example, less than
    1% of breast cancer cases occur in men ([www.breastcancer.org](https://www.breastcancer.org)).
    This prevalence difference between men and women is not caused by any sort of
    bias in data generation or collection or biases that have existed in societies.
    It is the natural difference between the prevalence of breast cancer occurrence
    between men and women. But if you were responsible for developing a machine learning
    model to diagnose breast cancer, there could be a high chance of false negatives
    (that is, not diagnosing breast cancer) in men. If your model doesn’t consider
    the high prevalence of women over men, it will not be a fair model in breast cancer
    diagnosis for men. This was a high-level example to clarify this kind of bias.
    There are many other considerations in building a machine learning tool for cancer
    diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: Bias in model training and testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If a dataset has a high imbalance toward men or women, different ethnicities,
    or any sort of bias considering different sensitive attributes, our models could
    have biases due to the way the corresponding machine learning algorithms use the
    features in predicting the outcome of data points. For example, our models could
    be highly reliant on sensitive attributes or their proxies (*Table 7.2*). This
    is an important consideration in model selection. In the model selection process,
    we need to select a model among the trained models, with different methods or
    hyperparameters of the same method, to be pushed for further testing or production.
    If we base our decision-making solely on performance, then we might select a model
    that is not fair. We need to consider both fairness and performance in our model
    selection process if we have sensitive attributes and those models will directly
    or indirectly affect individuals of different groups.
  prefs: []
  type: TYPE_NORMAL
- en: Bias in production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bias and unfairness in production could happen because of differences in the
    distribution of data between training, testing, and production. For example, women
    and men could have some differences in the production stage that don’t exist in
    your training and test data. This situation could result in biases in production
    that might not have been detectable in previous stages of the life cycle. We will
    talk about such kinds of differences in more detail in [*Chapter 11*](B16369_11.xhtml#_idTextAnchor300),
    *Avoiding and Detecting Data and* *Concept Drifts*.
  prefs: []
  type: TYPE_NORMAL
- en: The next step in this chapter is to start practicing with techniques and Python
    libraries that help you in detecting and eliminating model biases. First, will
    practice using the explainability techniques that were introduced in [*Chapter
    6*](B16369_06.xhtml#_idTextAnchor201), *Interpretability and Explainability in
    Machine* *Learning Modeling*.
  prefs: []
  type: TYPE_NORMAL
- en: Using explainability techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use explainability techniques to identify potential biases in our models
    and then plan to improve them toward fairness. Here, we want to practice this
    concept with SHAP and identify fairness issues between male and female groups
    in the adult income dataset we practiced with in the previous chapter. Using the
    same SHAP explainer object we built for the XGBoost model we trained on adult
    income data in the previous chapter, in the following bar plots, we can see that
    there is a low, but non-negligible, dependency on *sex* regarding the whole dataset
    or only the incorrectly predicted data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4 – SHAP summary plot for the whole adult income dataset and incorrectly
    predicted data points](img/B16369_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – SHAP summary plot for the whole adult income dataset and incorrectly
    predicted data points
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can extract the fraction of misclassified data points in each sex group,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5 – Number of males and females among correct and incorrect predictions](img/B16369_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Number of males and females among correct and incorrect predictions
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have 6.83% and 20.08% misclassification percentages for female and
    male groups, respectively. The ROC-AUC of the predictions of the model for only
    male and female groups in the test set are 0.90 and 0.94, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might consider identifying the correlation between features as an approach
    to identifying proxies and potential ways of removing biases in your models. The
    following code and heatmap (*Figure 7**.6*) show a correlation between the features
    of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6 – Correlation DataFrame between the features of the adult income
    dataset](img/B16369_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Correlation DataFrame between the features of the adult income
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are disadvantages to using such correlation analysis as the
    way of approaching the problem of proxy identification or even for filtering features
    toward improving performance. Here are two of these disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to consider proper correlation measures for each pair of features.
    For example, *Pearson* correlation cannot be used for all feature pairs as the
    distribution of data for each pair has to satisfy the assumptions for this method.
    Both variables need to follow normal distributions and data should not have any
    outliers as two of the assumptions for proper use of *Pearson* correlation. This
    means that to have a proper use of the feature correlation analysis approach,
    you need to use proper correlation measures to compare the features. Non-parametric
    statistical measures such as *Spearman* rank correlation could be more suitable
    as there are fewer assumptions behind its use across different variable pairs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not all numerical values have the same meaning. Some of the features are categorical
    and, through different methods, are transformed into numerical features. Sex is
    one of those features. Values of 0 and 1 can be used to show female and male groups
    but they don’t have any numerical meaning that you can find in numerical features
    such as age or salary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Explainability techniques such as SHAP tell you about dependencies to sensitive
    attributes and their contributions to the outcome of data points. However, by
    default, they don’t offer a way to improve the models in terms of fairness. In
    this example, we can try to split the data into male and female groups for training
    and testing. The following code shows this approach for the female group. Similarly,
    you can repeat this for the male group by separating the train and test input
    and output data with the `Sex` feature of `1`. The models that were built separately
    for male and female groups resulted in 0.90 and 0.93 ROC-AUCs, respectively, which
    is almost the same as the performance without the separation of the groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We didn’t remove the `Sex` feature from the models. This feature cannot contribute
    to the model’s performance as there is no difference between the values of this
    feature across the data points of each model. This is also shown by zero Shapely
    values in the bar plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7 – SHAP summary plot for models trained and tested on female and
    male groups separately](img/B16369_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – SHAP summary plot for models trained and tested on female and male
    groups separately
  prefs: []
  type: TYPE_NORMAL
- en: This approach of separating groups according to a sensitive attribute, although
    sometimes seen as taken, is not an ideal way of dealing with the issue of fairness.
    It might not be an effective approach as the model could be highly reliant on
    other sensitive features. Also, we cannot split the data into small chunks according
    to all combinations of all sensitive attributes in our dataset. There are fairness
    tools that could help you not only assess fairness and detect biases but select
    a model that better satisfies fairness notions.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to libraries for explainability, there are Python libraries that
    are designed specifically for fairness detection and improvement in machine learning
    modeling, which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness assessment and improvement in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are few widely used Python libraries to assess fairness in your models
    (*Table 7.3*). You can use these libraries to identify if the model satisfies
    fairness definitions according to the different sensitive attributes in a dataset
    you want to or have used for modeling:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Library** | **Library Name for Importing** **and Installation** | **URL**
    |'
  prefs: []
  type: TYPE_TB
- en: '| IBM AI Fairness 360 | `aif360` | [https://pypi.org/project/aif360/](https://pypi.org/project/aif360/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fairlearn | `fairlearn` | [https://pypi.org/project/fairlearn/](https://pypi.org/project/fairlearn/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Black Box Auditing | `BlackBoxAuditing` | [https://pypi.org/project/BlackBoxAuditing/](https://pypi.org/project/BlackBoxAuditing/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Aequitas | `aequitas` | [https://pypi.org/project/aequitas/](https://pypi.org/project/aequitas/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Responsible AI Toolbox | `responsibleai` | [https://pypi.org/project/responsibleai/](https://pypi.org/project/responsibleai/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Responsibly | `responsibly` | [https://pypi.org/project/responsibly/](https://pypi.org/project/responsibly/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon Sagemaker Clarify | `smclarify` | [https://pypi.org/project/smclarify/](https://pypi.org/project/smclarify/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fairness-aware machine learning | `fairness` | [https://pypi.org/project/fairness/](https://pypi.org/project/fairness/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Bias correction | `biascorrection` | [https://pypi.org/project/biascorrection/](https://pypi.org/project/biascorrection/)
    |'
  prefs: []
  type: TYPE_TB
- en: Table 7.3 – Python libraries or repositories with available functionalities
    for machine learning fairness
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s load the adult income dataset, after importing the required libraries,
    and prepare the training and test sets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can train and test an XGBoost model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we want to use `aif360` to calculate the *DIR* of real and predicted
    outcomes in the training and test data according to the `Sex` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following group bar plot shows that the predictions make the *DIR* even
    worse in both the training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Comparison of DIR in the original data and predicted outputs](img/B16369_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Comparison of DIR in the original data and predicted outputs
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `aif360` to improve our models toward fairness. Reject option classification
    is a postprocessing technique that gives favorable outcomes to unprivileged groups
    and unfavorable outcomes to privileged groups in a confidence band around the
    decision boundary with the highest uncertainty ([https://aif360.readthedocs.io/](https://aif360.readthedocs.io/),
    Kamira et al., 2012). First, let’s import all the necessary libraries and functionalities
    we need for doing so in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can use `RejectOptionClassifierCV()`to train and validate a random
    forest classifier on the adult dataset available in `aif360`. We switched from
    XGBoost to random forest solely for the sake of practicing with different models.
    We need to fit a `PostProcessingMeta()` object with an initial random forest model
    and `RejectOptionClassifierCV()`. `''sex''` is considered the sensitive feature
    in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then plot the balanced accuracy and *DIR* across different attempts
    in the grid search to show the best-chosen parameters, which is the starred point
    in the scatter plot in *Figure 7**.9*. The points in cyan show you the Pareto
    front for the tradeoff between balanced accuracy and *DIR*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Balanced accuracy versus DIR in a grid search](img/B16369_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Balanced accuracy versus DIR in a grid search
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, there is a compromise between performance and fairness in this
    case. But in this case, a less than 4% decrease in performance results in improving
    *DIR* from lower than 0.4 to 0.8.
  prefs: []
  type: TYPE_NORMAL
- en: As you saw in this example, we can use `aif360` to assess fairness and improve
    our model’s fairness with little loss in performance. You can use other libraries
    in Python similarly. And each one has its functionality for the two objectives
    of fairness assessment and improvement in machine learning modeling.
  prefs: []
  type: TYPE_NORMAL
- en: What we provided in this chapter was only the tip of the iceberg of fairness
    in machine learning. But at this point, you are ready to try different libraries
    and techniques and learn about them with the help of the practices we went through.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned more about the concept of fairness in the machine
    learning era, as well as the metrics, definitions, and challenges for assessing
    fairness. We talked about example proxies for sensitive attributes such as *sex*
    and *race*. We also talked about possible sources of bias, such as in data collection
    or model training. You also learned how you can use Python libraries for model
    explainability and fairness to assess fairness or improve it in your models, as
    well as avoid biases that not only would be unethical but could have legal and
    financial consequences for your organization.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about test-driven development and concepts
    such as unit and differential testing. We will also talk about machine learning
    experiment tracking and how it helps us avoid issues in our models in the model
    training, testing, and selection processes.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Does fairness depend only on observable features?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are examples of proxy features for `'sex'`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If one model is fair according to demographic parity, would it be fair according
    to other notions of fairness such as equalized odds?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between demographic parity and equalized odds as two
    fairness metrics?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you have a `'sex'` feature in your model and your model would have a low
    dependency on that, does it mean that your model is fair across different sex
    groups?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How could you use explainability techniques to assess fairness in your models?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Barocas, Solon, Moritz Hardt, and Arvind Narayanan. *Fairness in machine learning*.
    Nips tutorial 1 (2017): 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehrabi, Ninareh, et al. *A survey on bias and fairness in machine learning*.
    ACM Computing Surveys (CSUR) 54.6 (2021): 1-35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caton, Simon, and Christian Haas. *Fairness in machine learning: A survey*.
    arXiv preprint arXiv:2010.04053 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pessach, Dana, and Erez Shmueli. *A review on fairness in machine learning*.
    ACM Computing Surveys (CSUR) 55.3 (2022): 1-44.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lechner, Tosca, et al. *Impossibility results for fair representations*. arXiv
    preprint arXiv:2107.03483 (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCalman, Lachlan, et al. *Assessing AI fairness in finance*. Computer 55.1
    (2022): 94-97.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F. Kamiran, A. Karim, and X. Zhang, *Decision Theory for Discrimination-Aware
    Classification*. IEEE International Conference on Data Mining, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 3:Low-Bug Machine Learning Development and Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this part of the book, we will provide the essential practices to ensure
    the robustness and reliability of machine learning models, especially in production.
    We will start with the adoption of Test-Driven Development, illustrating its crucial
    role in mitigating risks during model development. Subsequently, we will delve
    into the testing techniques and the significance of model monitoring, ensuring
    that our models remain dependable when deployed. We will then explain techniques
    and challenges in achieving reproducibility in machine learning through code,
    data, and model versioning. We will conclude this part by addressing the challenges
    of data and concept drifts to have reliable models in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B16369_08.xhtml#_idTextAnchor243), *Controlling Risks Using Test-Driven
    Development*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B16369_09.xhtml#_idTextAnchor261), *Testing and Debugging for
    Production*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B16369_10.xhtml#_idTextAnchor286), *Versioning and Reproducible
    Machine Learning Modeling*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B16369_11.xhtml#_idTextAnchor300), *Avoiding and Detecting Data
    and Concept Drifts*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
