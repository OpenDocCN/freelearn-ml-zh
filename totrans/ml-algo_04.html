<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Linear Regression</h1>
                </header>
            
            <article>
                
<p>Linear models are the simplest parametric methods and always deserve the right attention, because many problems, even intrinsically non-linear ones, can be easily solved with these models. As discussed previously, a regression is a prediction where the target is continuous and its applications are several, so it's important to understand how a linear model can fit the data, what its strengths and weaknesses are, and when it's preferable to pick an alternative. In the last part of the chapter, we're going to discuss an interesting method to work efficiently with non-linear data using the same models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear models</h1>
                </header>
            
            <article>
                
<p>Consider a dataset of real-values vectors:</p>
<div class="CDPAlignCenter CDPAlign"><img height="41" width="229" src="assets/c11132d3-2d0a-4829-bf9a-1f0683bc1b54.png"/></div>
<p class="CDPAlignLeft CDPAlign">Each input vector is associated with a real value y<sub>i</sub>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="35" width="204" src="assets/8d55285c-b14f-4500-90ec-b71cc653fc91.png"/></div>
<p class="CDPAlignLeft CDPAlign">A linear model is based on the assumption that it's possible to approximate the output values through a regression process based on the rule:</p>
<div class="CDPAlignCenter CDPAlign"><img height="58" width="274" src="assets/7b1cdea2-df22-4389-92b9-4555648cc184.png"/></div>
<p class="CDPAlignLeft CDPAlign">In other words, the strong assumption is that our dataset and all other unknown points lie on a hyperplane and the maximum error is proportional to both the training quality and the adaptability of the original dataset. One of the most common problems arises when the dataset is clearly non-linear and other models have to be considered (such as neural networks or kernel support vector machines).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A bidimensional example</h1>
                </header>
            
            <article>
                
<p>Let's consider a small dataset built by adding some uniform noise to the points belonging to a segment bounded between -6 and 6. The original equation is: <em>y = x + 2 + n</em>, where n is a noise term.</p>
<p>In the following figure, there's a plot with a candidate regression function:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="assets/0b861a0a-9b14-4014-b4d7-d96af0867025.png"/></div>
<p class="mce-root">As we're working on a plane, the regressor we're looking for is a function of only two parameters:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="35" width="99" src="assets/0b0783e1-6cda-43b0-819e-e068cf5de528.png"/></div>
<p>In order to fit our model, we must find the best parameters and to do that we choose an ordinary least squares approach. The loss function to minimize is:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="53" width="364" src="assets/2978dc42-6e7b-443e-baf2-834395af5fa1.png"/></div>
<p>With an analytic approach, in order to find the global minimum, we must impose:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="100" width="211" src="assets/0805d7ec-87ec-4aaa-9351-c83d29be9651.png"/></div>
<p>So (for simplicity, it accepts a vector containing both variables):</p>
<pre><strong>import numpy as np</strong><br/><br/><strong>def loss(v):</strong><br/><strong>   e = 0.0</strong><br/><strong>   for i in range(nb_samples):</strong><br/><strong>      e += np.square(v[0] + v[1]*X[i] - Y[i])</strong><br/><strong>   return 0.5 * e</strong></pre>
<p>And the gradient can be defined as:</p>
<pre><strong>def gradient(v):</strong><br/><strong>   g = np.zeros(shape=2)</strong><br/><strong>   for i in range(nb_samples):</strong><br/><strong>     g[0] += (v[0] + v[1]*X[i] - Y[i])</strong><br/><strong>     g[1] += ((v[0] + v[1]*X[i] - Y[i]) * X[i])</strong><br/><strong>   return g</strong></pre>
<p>The optimization problem can now be solved using SciPy:</p>
<pre><strong>from scipy.optimize import minimize</strong><br/><br/><strong>&gt;&gt;&gt; minimize(fun=loss, x0=[0.0, 0.0], jac=gradient, method='L-BFGS-B')</strong><br/><strong>fun: 9.7283268345966025</strong><br/><strong> hess_inv: &lt;2x2 LbfgsInvHessProduct with dtype=float64&gt;</strong><br/><strong>      jac: array([  7.28577538e-06,  -2.35647522e-05])</strong><br/><strong>  message: 'CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH'</strong><br/><strong>     nfev: 8</strong><br/><strong>      nit: 7</strong><br/><strong>   status: 0</strong><br/><strong>  success: True</strong><br/><strong>        x: array([ 2.00497209,  1.00822552])</strong></pre>
<p>As expected, the regression denoised our dataset, rebuilding the original equation: <em>y = x + 2</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Linear regression with scikit-learn and higher dimensionality</h1>
                </header>
            
            <article>
                
<p>scikit-learn offers the class <kbd>LinearRegression</kbd>, which works with n-dimensional spaces. For this purpose, we're going to use the Boston dataset:</p>
<pre><strong>from sklearn.datasets import load_boston</strong><br/><br/><strong>&gt;&gt;&gt; boston = load_boston()</strong><br/><br/><strong>&gt;&gt;&gt; boston.data.shape</strong><br/><strong>(506L, 13L)</strong><br/><strong>&gt;&gt;&gt; boston.target.shape</strong><br/><strong>(506L,)</strong></pre>
<p>It has 506 samples with 13 input features and one output. In the following figure, there' a collection of the plots of the first 12 features:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" src="assets/34b5f0a3-0aba-4aff-a89a-d92a089bb68b.png"/></div>
<div class="packt_tip">When working with datasets, it's useful to have a tabular view to manipulate data. pandas is a perfect framework for this task, and even though it's beyond the scope of this book, I suggest you create a data frame with the command <kbd>pandas.DataFrame(boston.data, columns=boston.feature_names)</kbd> and use Jupyter to visualize it. For further information, refer to Heydt M., <em>Learning pandas - Python Data Discovery and Analysis Made Easy</em>, Packt.</div>
<p>There are different scales and outliers (which can be removed using the methods studied in the previous chapters), so it's better to ask the model to normalize the data before processing it. Moreover, for testing purposes, we split the original dataset into training (90%) and test (10%) sets:</p>
<pre><strong>from sklearn.linear_model import LinearRegression</strong><br/><strong>from sklearn.model_selection import train_test_split</strong><br/><br/><strong>&gt;&gt;&gt; X_train, X_test, Y_train, Y_test = train_test_split(boston.data, boston.target, test_size=0.1)</strong><br/><br/><strong>&gt;&gt;&gt; lr = LinearRegression(normalize=True)</strong><br/><strong>&gt;&gt;&gt; lr.fit(X_train, Y_train)</strong><br/><strong>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=True)</strong></pre>
<p>When the original data set isn't large enough, splitting it into training and test sets may reduce the number of samples that can be used for fitting the model. k-fold cross-validation can help in solving this problem with a different strategy. The whole dataset is split into k folds using always k-1 folds for training and the remaining one to validate the model. K iterations will be performed, using always a different validation fold. In the following figure, there's an example with 3 folds/iterations:</p>
<div class="CDPAlignCenter CDPAlign"><img height="294" width="430" class="image-border" src="assets/818fee35-73a6-482c-9733-cf5b1b2570bf.png"/></div>
<p>In this way, the final score can be determined as average of all values and all samples are selected for training k-1 times.</p>
<p>To check the accuracy of a regression, scikit-learn provides the internal method <kbd>score(X, y)</kbd> which evaluates the model on test data:</p>
<pre><strong>&gt;&gt;&gt; lr.score(X_test, Y_test)</strong><br/><strong>0.77371996006718879</strong></pre>
<p>So the overall accuracy is about 77%, which is an acceptable result considering the non-linearity of the original dataset, but it can be also influenced by the subdivision made by <kbd>train_test_split</kbd><em><strong> </strong></em>(like in our case). Instead, for k-fold cross-validation, we can use the function <kbd>cross_val_score()</kbd>, which works with all the classifiers. The scoring parameter is very important because it determines which metric will be adopted for tests. As <kbd>LinearRegression</kbd> works with ordinary least squares, we preferred the negative mean squared error, which is a cumulative measure that must be evaluated according to the actual values (it's not relative). </p>
<pre><strong>from sklearn.model_selection import cross_val_score</strong><br/><br/><strong>&gt;&gt;&gt; scores = cross_val_score(lr, boston.data, boston.target, cv=7, scoring='neg_mean_squared_error')</strong><br/><strong>array([ -11.32601065,  -10.96365388,  -32.12770594,  -33.62294354,</strong><br/><strong>        -10.55957139, -146.42926647,  -12.98538412])</strong><br/><br/><strong>&gt;&gt;&gt; scores.mean()</strong><br/><strong>-36.859219426420601</strong><br/><strong>&gt;&gt;&gt; scores.std()</strong><br/><strong>45.704973900600457</strong></pre>
<p>Another very important metric used in regressions is called the <strong>coefficient of determination</strong> or <em>R<sup>2</sup></em>. It measures the amount of variance on the prediction which is explained by the dataset. We define <strong>residuals</strong>, the following quantity:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="39" width="175" src="assets/b1e61431-300d-4745-b697-7fde495ae484.png"/></div>
<p>In other words, it is the difference between the sample and the prediction. So the <em>R<sup>2</sup></em> is defined as follows:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img height="48" width="171" src="assets/79b4dfc9-5c5c-4f67-bdf8-e4aa967ad90f.png"/></div>
<p>For our purposes, <em>R<sup>2</sup></em> values close to 1 mean an almost perfect regression, while values close to 0 (or negative) imply a bad model. Using this metric is quite easy with cross-validation:</p>
<pre><strong>&gt;&gt;&gt; cross_val_score(lr, X, Y, cv=10, scoring='r2')</strong><br/><strong>0.75</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Regressor analytic expression</h1>
                </header>
            
            <article>
                
<p>If we want to have an analytical expression of our model (a hyperplane), <kbd>LinearRegression</kbd> offers two instance variables, <kbd>intercept_</kbd> and <kbd>coef_</kbd>:</p>
<pre><strong>&gt;&gt;&gt; print('y = ' + str(lr.intercept_) + ' ')</strong><br/><strong>&gt;&gt;&gt; for i, c in enumerate(lr.coef_):</strong><br/><strong>       print(str(c) + ' * x' + str(i))</strong><br/><br/><strong>y = 38.0974166342 </strong><br/><strong>-0.105375005552 * x0</strong><br/><strong>0.0494815380304 * x1</strong><br/><strong>0.0371643549528 * x2</strong><br/><strong>3.37092201039 * x3</strong><br/><strong>-18.9885299511 * x4</strong><br/><strong>3.73331692311 * x5</strong><br/><strong>0.00111437695492 * x6</strong><br/><strong>-1.55681538908 * x7</strong><br/><strong>0.325992743837 * x8</strong><br/><strong>-0.01252057277 * x9</strong><br/><strong>-0.978221746439 * x10</strong><br/><strong>0.0101679515792 * x11</strong><br/><strong>-0.550117114635 * x12</strong></pre>
<p>As for any other model, a prediction can be obtained through the method <kbd>predict(X)</kbd>. As an experiment, we can try to add some Gaussian noise to our training data and predict the value:</p>
<pre><strong>&gt;&gt;&gt; X = boston.data[0:10] + np.random.normal(0.0, 0.1)</strong><br/><br/><strong>&gt;&gt;&gt; lr.predict(X)</strong><br/><strong>array([ 29.5588731 ,  24.49601998,  30.0981552 ,  28.01864586,</strong><br/><strong>        27.28870704,  24.65881135,  22.46335968,  18.79690943,</strong><br/><strong>        10.53493932,  18.18093544])</strong><br/><br/><strong>&gt;&gt;&gt; boston.target[0:10]</strong><br/><strong>array([ 24. ,  21.6,  34.7,  33.4,  36.2,  28.7,  22.9,  27.1,  16.5,  18.9])</strong></pre>
<p>It's obvious that the model is not performing in an ideal way and there are many possible reasons, the foremost being nonlinearities and the presence of outliers. However, in general, a linear regression model is not a perfectly robust solution. In Hastie T., Tibshirani R., Friedman J., <em>The Elements of Statistical Learning: Data Mining, Inference, and, Prediction</em>, Springer, <span>you can find a very detailed discussion about its strengths and weaknesses. However, in this context, a common threat is represented by collinearities that lead to low-rank</span> <em>X</em> <span>matrix. This determines an ill-conditioned matrix that is particularly sensitive to noise, causing the explosion of some parameters as well. The following methods have been studied in order to mitigate this risk and provide more robust solutions.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ridge, Lasso, and ElasticNet</h1>
                </header>
            
            <article>
                
<p><strong>Ridge</strong> regression imposes an additional shrinkage penalty to the ordinary least squares loss function to limit its squared <em>L2</em> norm:</p>
<div class="CDPAlignCenter CDPAlign"><img height="36" width="186" src="assets/97b9c114-646c-42e9-bbfc-de728b2df441.png"/></div>
<p class="CDPAlignLeft CDPAlign">In this case, <em>X</em> is a matrix containing all samples as columns and the term <em>w</em> represents the weight vector. The additional term (through the coefficient alpha—if large it implies a stronger regularization and smaller values) forces the loss function to disallow an infinite growth of <em>w</em>, which can be caused by multicollinearity or ill-conditioning. In the following figure, there's a representation of what happens when a Ridge penalty is applied:</p>
<div class="CDPAlignCenter CDPAlign"><img height="431" width="618" class="image-border" src="assets/8b9f8364-acdf-4dce-b9ec-22231b2ef2f1.png"/></div>
<p>The gray surface represents the loss function (here, for simplicity, we're working with only two weights), while the circle center <strong>O</strong> is the boundary imposed by the Ridge condition. The minimum will have smaller <em>w</em> values and potential explosions are avoided.</p>
<p>In the following snippet, we're going to compare <kbd>LinearRegression</kbd> and <kbd>Ridge</kbd><em><strong> </strong></em>with a cross-validation:</p>
<pre><strong>from sklearn.datasets import load_diabetes</strong><br/><strong>from sklearn.linear_model import LinearRegression, Ridge</strong><br/><br/><strong>&gt;&gt;&gt; diabetes = load_diabetes()</strong><br/><br/><strong>&gt;&gt;&gt; lr = LinearRegression(normalize=True)</strong><br/><strong>&gt;&gt;&gt; rg = Ridge(0.001, normalize=True)</strong><br/><br/><strong>&gt;&gt;&gt; lr_scores = cross_val_score(lr, diabetes.data, diabetes.target, cv=10)</strong><br/><strong>&gt;&gt;&gt; lr_scores.mean()</strong><br/><strong>0.46196236195833718</strong><br/><br/><strong>&gt;&gt;&gt; rg_scores = cross_val_score(rg, diabetes.data, diabetes.target, cv=10)</strong><br/><strong>&gt;&gt;&gt; rg_scores.mean()</strong><br/><strong>0.46227174692391299</strong></pre>
<p>Sometimes, finding the right value for alpha (Ridge coefficient) is not so immediate. scikit-learn provides the class <kbd>RidgeCV</kbd>, which allows performing an automatic grid search (among a set and returning the best estimation):</p>
<pre><strong>from sklearn.linear_model import RidgeCV</strong><br/><br/><strong>&gt;&gt;&gt; rg = RidgeCV(alphas=(1.0, 0.1, 0.01, 0.005, 0.0025, 0.001, 0.00025), normalize=True)</strong><br/><strong>&gt;&gt;&gt; rg.fit(diabetes.data, diabetes.target)</strong><br/><br/><strong>&gt;&gt;&gt; rg.alpha_</strong><br/><strong>0.0050000000000000001</strong></pre>
<p>A Lasso regressor imposes a penalty on the <em>L1</em> norm of <em>w</em> to determine a potentially higher number of null coefficients:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="48" width="199" src="assets/52b950db-dba6-47b3-97f5-f7fa04526062.png"/></p>
<p>The sparsity is a consequence of the penalty term (the mathematical proof is non-trivial and will be omitted).</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img class="image-border" src="assets/35ad9c79-5d86-4ef4-83b2-a86d468b4677.png"/></div>
<p>In this case, there are vertices where a component is non-null while all the other weights are zero. The probability of an intersection with a vertex is proportional to the dimensionality of <em>w</em> and, therefore, it's normal to discover a rather sparse model after training a Lasso regressor.</p>
<p>In the following snippet, the diabetes dataset is used to fit a Lasso model:</p>
<pre><strong>from sklearn.linear_model import Lasso</strong><br/><br/><strong>&gt;&gt;&gt; ls = Lasso(alpha=0.001, normalize=True)</strong><br/><strong>&gt;&gt;&gt; ls_scores = cross_val_score(ls, diabetes.data, diabetes.target, cv=10)</strong><br/><strong>&gt;&gt;&gt; ls_scores.mean()</strong><br/><strong>0.46215747851504058</strong></pre>
<p>Also for Lasso, there's the possibility of running a grid search for the best alpha parameter. The class, in this case, is <kbd>LassoCV</kbd> and its internal dynamics are similar to what was already seen for Ridge. Lasso can also perform efficiently on the sparse data generated through the <kbd>scipy.sparse</kbd> class, allowing for training bigger models without the need for partial fitting:</p>
<pre><strong>from scipy import sparse</strong><br/><br/><strong>&gt;&gt;&gt; ls = Lasso(alpha=0.001, normalize=True)</strong><br/><strong>&gt;&gt;&gt; ls.fit(sparse.coo_matrix(diabetes.data), diabetes.target)</strong><br/><strong>Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,</strong><br/><strong>   normalize=True, positive=False, precompute=False, random_state=None,</strong><br/><strong>   selection='cyclic', tol=0.0001, warm_start=False)</strong></pre>
<div class="packt_infobox">When working with a huge amount of data, some models cannot fit completely in memory, so it's impossible to train them. scikit-learn offers some models, such as <strong>stochastic gradient descent</strong> (<strong>SGD</strong>), which work in a way quite similar to <kbd>LinearRegression</kbd> with <kbd>Ridge</kbd>/<kbd>Lasso</kbd>; however, they also implement the method <kbd>partial_fit()</kbd>, which also allows continuous training through Python generators. See <a href="http://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd">http://scikit-learn.org/stable/modules/linear_model.html#stochastic-gradient-descent-sgd</a>, for further details.</div>
<p>The last alternative is <strong>ElasticNet</strong>, which combines both Lasso and Ridge into a single model with two penalty factors: one proportional to <em>L1</em> norm and the other to <em>L2</em> norm. In this way, the resulting model will be sparse like a pure Lasso, but with the same regularization ability as provided by Ridge. The resulting loss function is:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="50" width="337" src="assets/08b661fa-86da-4f1c-a9df-7f6648a4da0e.png"/></p>
<p>The <kbd>ElasticNet</kbd> class provides an implementation where the alpha parameter works in conjunction with <kbd>l1_ratio</kbd> (beta in the formula<strong>)</strong>. The main peculiarity of <kbd>ElasticNet</kbd> is avoiding a selective exclusion of correlated features, thanks to the balanced action of the <em>L1</em> and <em>L2</em> norms.</p>
<p>In the following snippet, there's an example using both the <kbd>ElasticNet</kbd> and <kbd>ElasticNetCV</kbd> classes:</p>
<pre><strong>from sklearn.linear_model import ElasticNet, ElasticNetCV</strong><br/><br/><strong>&gt;&gt;&gt; en = ElasticNet(alpha=0.001, l1_ratio=0.8, normalize=True)</strong><br/><strong>&gt;&gt;&gt; en_scores = cross_val_score(en, diabetes.data, diabetes.target, cv=10)</strong><br/><strong>&gt;&gt;&gt; en_scores.mean()</strong><br/><strong>0.46358858847836454</strong><br/><br/><strong>&gt;&gt;&gt; encv = ElasticNetCV(alphas=(0.1, 0.01, 0.005, 0.0025, 0.001), l1_ratio=(0.1, 0.25, 0.5, 0.75, 0.8), normalize=True)</strong><br/><strong>&gt;&gt;&gt; encv.fit(dia.data, dia.target)</strong><br/><strong>ElasticNetCV(alphas=(0.1, 0.01, 0.005, 0.0025, 0.001), copy_X=True, cv=None,</strong><br/><strong>       eps=0.001, fit_intercept=True, l1_ratio=(0.1, 0.25, 0.5, 0.75, 0.8),</strong><br/><strong>       max_iter=1000, n_alphas=100, n_jobs=1, normalize=True,</strong><br/><strong>       positive=False, precompute='auto', random_state=None,</strong><br/><strong>       selection='cyclic', tol=0.0001, verbose=0)</strong><br/><br/><strong>&gt;&gt;&gt; encv.alpha_</strong><br/><strong>0.001</strong><br/><strong>&gt;&gt;&gt; encv.l1_ratio_</strong><br/><strong>0.75</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Robust regression with random sample consensus</h1>
                </header>
            
            <article>
                
<p>A common problem with linear regressions is caused by the presence of outliers. An ordinary least square approach will take them into account and the result (in terms of coefficients) will be therefore biased. In the following figure, there's an example of such a behavior:</p>
<p class="packt_figref CDPAlignCenter CDPAlign"><img height="357" width="514" class="image-border" src="assets/758d76d5-e5cb-4d23-a38b-f0de93d9bce3.png"/></p>
<p>The less sloped line represents an acceptable regression which discards the outliers, while the other one is influenced by them. An interesting approach to avoid this problem is offered by <strong>random sample consensus</strong> (<strong>RANSAC</strong>), which works with every regressor by subsequent iterations, after splitting the dataset into inliers and outliers. The model is trained only with valid samples (evaluated internally or through the callable <kbd>is_data_valid()</kbd>) and all samples are re-evaluated to verify if they're still inliers or they have become outliers. The process ends after a fixed number of iterations or when the desired score is achieved.</p>
<p>In the following snippet, there's an example of simple linear regression applied to the dataset shown in the previous figure.</p>
<pre><strong>from sklearn.linear_model import LinearRegression</strong><br/><br/><strong>&gt;&gt;&gt; lr = LinearRegression(normalize=True)</strong><br/><strong>&gt;&gt;&gt; lr.fit(X.reshape((-1, 1)), Y.reshape((-1, 1)))</strong><br/><strong>&gt;&gt;&gt; lr.intercept_</strong><br/><strong>array([ 5.500572])</strong><br/><strong>&gt;&gt;&gt; lr.coef_</strong><br/><strong>array([[ 2.53688672]])</strong></pre>
<p>As imagined, the slope is high due to the presence of outliers. The resulting regressor is <em>y = 5.5 + 2.5x</em> (slightly less sloped than what was shown in the figure). Now we're going to use RANSAC with the same linear regressor:</p>
<pre><strong>from sklearn.linear_model import RANSACRegressor</strong><br/><br/><strong>&gt;&gt;&gt; rs = RANSACRegressor(lr)</strong><br/><strong>&gt;&gt;&gt; rs.fit(X.reshape((-1, 1)), Y.reshape((-1, 1)))</strong><br/><strong>&gt;&gt;&gt; rs.estimator_.intercept_</strong><br/><strong>array([ 2.03602026])</strong><br/><strong>&gt;&gt;&gt; es.estimator_.coef_</strong><br/><strong>array([[ 0.99545348]])</strong></pre>
<p>In this case, the regressor is about <em>y = 2 + x</em> (which is the original clean dataset without outliers).</p>
<div class="packt_infobox">If you want to have further information, I suggest visiting the page <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html</a>. For other robust regression techniques, visit: <a href="http://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors">http://scikit-learn.org/stable/modules/linear_model.html#robustness-regression-outliers-and-modeling-errors</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Polynomial regression</h1>
                </header>
            
            <article>
                
<p>Polynomial regression is a technique based on a trick that allows using linear models even when the dataset has strong non-linearities. The idea is to add some extra variables computed from the existing ones and using (in this case) only polynomial combinations:</p>
<div class="CDPAlignCenter CDPAlign"><img height="57" width="493" src="assets/47b8dff1-28fb-46b7-abbc-b976d7678923.png"/></div>
<p class="CDPAlignLeft CDPAlign">For example, with two variables, it's possible to extend to a second-degree problem by transforming the initial vector (whose dimension is equal to <em>m</em>) into another one with higher dimensionality <span>(whose</span> dimension <span>is <em>k</em> &gt; <em>m</em>)</span>:</p>
<div class="CDPAlignCenter CDPAlign"><img height="39" width="288" src="assets/64a917dd-dd97-4221-b9a3-57c62b96b9a6.png"/></div>
<p class="CDPAlignLeft CDPAlign">In this case, the model remains externally linear, but it can capture internal non-linearities. To show how scikit-learn implements this technique, let's consider the dataset shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="432" width="520" class="image-border" src="assets/287dbe08-4340-4462-90d5-0067ccc84196.png"/></div>
<p>This is clearly a non-linear dataset, and any linear regression based only on the original two-dimensional points cannot capture the dynamics. Just to try, we can train a simple model (testing it on the same dataset):</p>
<pre><strong>from sklearn.linear_model import LinearRegression</strong><br/><br/><strong>&gt;&gt;&gt; lr = LinearRegression(normalize=True)</strong><br/><strong>&gt;&gt;&gt; lr.fit(X.reshape((-1, 1)), Y.reshape((-1, 1)))</strong><br/><strong>&gt;&gt;&gt; lr.score(X.reshape((-1, 1)), Y.reshape((-1, 1)))</strong><br/><strong>0.10888218817034558</strong></pre>
<p>Performances are poor, as expected. However, looking at the figure, we might suppose that a quadratic regression could easily solve this problem. scikit-learn provides the class <kbd>PolynomialFeatures</kbd>, which transforms an original set into an expanded one according to the parameter <kbd>degree</kbd>:</p>
<pre><strong>from sklearn.preprocessing import PolynomialFeatures</strong><br/><br/><strong>&gt;&gt;&gt; pf = PolynomialFeatures(degree=2)</strong><br/><strong>&gt;&gt;&gt; Xp = pf.fit_transform(X.reshape(-1, 1))</strong><br/><br/><strong>&gt;&gt;&gt; Xp.shape</strong><br/><strong>(100L, 3L)</strong></pre>
<p>As expected, the old <em>x<sub>1</sub></em> coordinate has been replaced by a triplet, which also contains the quadratic and mixed terms. At this point, a linear regression model can be trained:</p>
<pre><strong>&gt;&gt;&gt; lr.fit(Xp, Y.reshape((-1, 1)))</strong><br/><strong>&gt;&gt;&gt; lr.score(Xp, Y.reshape((-1, 1)))</strong><br/><strong>0.99692778265941961</strong></pre>
<p>The score is quite higher and the only price we have paid is an increase in terms of features. In general, this is feasible; however, if the number grows over an accepted threshold, it's useful to try a dimensionality reduction or, as an extreme solution, to move to a non-linear model (such as SVM-Kernel). Usually, a good approach is using the class <kbd>SelectFromModel</kbd> to let scikit-learn select the best features based on their importance. In fact, when the number of features increases, the probability that all of them have the same importance gets lower. This is the result of mutual correlation or of the co-presence of major and minor trends, which act like noise and don't have the strength to alter perceptibility the hyperplane slope. Moreover, when using a polynomial expansion, some weak features (that cannot be used for a linear separation) are substituted by their functions and so the actual number of strong features decreases.</p>
<p>In the following snippet, there's an example with the previous Boston dataset. The <kbd>threshold</kbd> parameter is used to set a minimum importance level. If missing, the class will try to maximize the efficiency by removing the highest possible number of features.</p>
<pre><strong>from sklearn.feature_selection import SelectFromModel</strong><br/><br/><strong>&gt;&gt;&gt; boston = load_boston()</strong><br/><br/><strong>&gt;&gt;&gt; pf = PolynomialFeatures(degree=2)</strong><br/><strong>&gt;&gt;&gt; Xp = pf.fit_transform(boston.data)</strong><br/><strong>&gt;&gt;&gt; Xp.shape</strong><br/><strong>(506L, 105L)</strong><br/><br/><strong>&gt;&gt;&gt; lr = LinearRegression(normalize=True)</strong><br/><strong>&gt;&gt;&gt; lr.fit(Xp, boston.target)</strong><br/><strong>&gt;&gt;&gt; lr.score(Xp, boston.target)</strong><br/><strong>0.91795268869997404</strong><br/><br/><strong>&gt;&gt;&gt; sm = SelectFromModel(lr, threshold=10)</strong><br/><strong>&gt;&gt;&gt; Xt = sm.fit_transform(Xp, boston.target)</strong><br/><strong>&gt;&gt;&gt; sm.estimator_.score(Xp, boston.target)</strong><br/><strong>0.91795268869997404</strong><br/><br/><strong>&gt;&gt;&gt; Xt.shape</strong><br/><strong>(506L, 8L)</strong></pre>
<p>After selecting only the best features (with the threshold set to 10), the score remains the same, with a consistent dimensionality reduction (only 8 features are considered important for the prediction). If, after any other processing step, it's necessary to return to the original dataset, it's possible to use the inverse transformation:</p>
<pre><strong>&gt;&gt;&gt; Xo = sm.inverse_transform(Xt)</strong><br/><strong>&gt;&gt;&gt; Xo.shape</strong><br/><strong>(506L, 105L)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Isotonic regression</h1>
                </header>
            
            <article>
                
<p>There are situations when we need to find a regressor for a dataset of non-decreasing points which can present low-level oscillations (such as noise). A linear regression can easily achieve a very high score (considering that the slope is about constant), but it works like a denoiser, producing a line that can't capture the internal dynamics we'd like to model. For these situations, scikit-learn offers the class <kbd>IsotonicRegression</kbd>, which produces a piecewise interpolating function minimizing the functional:</p>
<div class="CDPAlignCenter CDPAlign"><img height="53" width="294" src="assets/9f8b669d-6385-4b37-b4dd-c1dca9d761a0.png"/></div>
<p class="CDPAlignLeft CDPAlign">An example (with a toy dataset) is provided next:</p>
<pre><strong>&gt;&gt;&gt; X = np.arange(-5, 5, 0.1)</strong><br/><strong>&gt;&gt;&gt; Y = X + np.random.uniform(-0.5, 1, size=X.shape)</strong></pre>
<p>Following is a plot of the dataset. As everyone can see, it can be easily modeled by a linear regressor, but without a high non-linear function, it is very difficult to capture the slight (and local) modifications in the slope:</p>
<div class="CDPAlignCenter CDPAlign"><sub><img height="328" width="400" class="image-border" src="assets/f992fdee-7f5b-4403-acd3-8ce8a3de7b28.png"/></sub></div>
<p>The class <kbd>IsotonicRegression</kbd> needs to know <em>y<sub>min</sub></em> and <em>y<sub>max</sub></em> (which correspond to the variables <em>y</em><sub><em>0</em> </sub>and <em>y</em><sub><em>n</em> </sub>in the loss function). In this case, we impose -6 and 10:</p>
<pre><strong>from sklearn.isotonic import IsotonicRegression</strong><br/><br/><strong>&gt;&gt;&gt; ir = IsotonicRegression(-6, 10)</strong><br/><strong>&gt;&gt;&gt; Yi = ir.fit_transform(X, Y)</strong></pre>
<p>The result is provided through three instance variables:</p>
<pre><strong>&gt;&gt;&gt; ir.X_min_</strong><br/><strong>-5.0</strong><br/><strong>&gt;&gt;&gt; ir.X_max_</strong><br/><strong>4.8999999999999648</strong><br/><strong>&gt;&gt;&gt; ir.f_</strong><br/><strong>&lt;scipy.interpolate.interpolate.interp1d at 0x126edef8&gt;</strong></pre>
<p>The last one, <kbd>(ir.f_)</kbd>, is an interpolating function which can be evaluated in the domain [<em>x<sub>min</sub></em>, <em>x<sub>max</sub></em>]. For example:</p>
<pre><strong>&gt;&gt;&gt; ir.f_(2)</strong><br/><strong>array(1.7294334618146134)</strong></pre>
<p>A plot of this function (the green line), together with the original data set, is shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img height="284" width="344" class="image-border" src="assets/0b30ebc1-8ec9-489a-961b-877ba107af1d.png"/></div>
<div class="packt_infobox">For further information about interpolation with SciPy, visit <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/interpolate.html">https://docs.scipy.org/doc/scipy-0.18.1/reference/interpolate.html</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<p>Hastie T., Tibshirani R., Friedman J., <em>The Elements of Statistical Learning: Data Mining, Inference, and, Prediction</em>, Springer</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have introduced the important concepts of linear models and have described how linear regression works. In particular, we focused on the basic model and its main variants: Lasso, Ridge, and ElasticNet. They don't modify the internal dynamics but work as normalizers for the weights, in order to avoid common problems when the dataset contains unscaled samples. These penalties have specific peculiarities. While Lasso promotes sparsity, Ridge tries to find a minimum with the constraints that the weights must lay on a circle centered at the origin (whose radius is parametrized to increase/decrease the normalization strength). ElasticNet is a mix of both these techniques and it tries to find a minimum where the weights are small enough and a certain degree of sparsity is achieved.</p>
<p>We also discussed advanced techniques such as RANSAC, which allows coping with outliers in a very robust way, and polynomial regression, which is a very smart way to include virtual non-linear features into our model and continue working with them with the same linear approach. In this way, it's possible to create another dataset, containing the original columns together with polynomial combinations of them. This new dataset can be used to train a linear regression model, and then it's possible to select only those features that contributed towards achieving good performances. The last method we saw was isotonic regression, which is particularly useful when the function to interpolate is always not decreasing. Moreover it can capture the small oscillations that would be flattened by a generic linear regression.</p>
<p>In the next chapter, we're going to discuss some linear models for classifications. In particular, we'll focus our attention on the logistic regression and stochastic gradient descent algorithms. Moreover, we're going to introduce some useful metrics to evaluate the accuracy of a classification system, and a powerful technique to automatically find the best hyperparameters. </p>


            </article>

            
        </section>
    </body></html>