<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Drilling Deeper into Object Detection – Using Cascade Classifiers</h1></div></div></div><p>In the previous chapter, we looked at some very sophisticated algorithms used for object detection. In this chapter, we plan to look further into a different set of algorithms, known as cascade classifiers and HOG descriptors. These algorithms are widely used to detect human expressions and find application in surveillance systems, face recognition systems, and other simple biometric systems. Face detection was one of the first applications of <a id="id207" class="indexterm"/>
<strong>cascade classifiers</strong> (Haar-cascade classifier) and from then on, there have been many different applications that have been developed.</p><p>Have you ever wondered how cameras detect smiling faces in an image and click a picture automatically? It is no rocket science. This chapter will talk about the different ways of detecting human expressions, using which you can build your own version of the aforementioned applications on an Android platform.</p><p>We will take a look at the following algorithms in this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Cascade classifiers</li><li class="listitem" style="list-style-type: disc">HOG descriptors</li></ul></div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec27"/>An introduction to cascade classifiers</h1></div></div></div><p>What are <a id="id208" class="indexterm"/>cascade classifiers? Let's take a look at both the words individually and then combine them to see what the phrase actually means. Classifiers are like black boxes that classify objects into various classes on the basis of a training set. Initially, we take a large set of training data, feed it to any learning algorithm, and compute a trained model (classifier), which is capable of classifying new unknown data.</p><p>Let's understand the word cascade. In the literal sense of the word, cascading means to form a chain. In the current context, cascading implies forming a multistage classifier, where the output of one stage is passed on to the next stage, and so on. Cascade classifiers are used in situations where you have low computational power and you do not want to compromise on the speed of your algorithm.</p><p>Cascade classifiers <a id="id209" class="indexterm"/>that will be covered in this chapter are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Haar cascades (Viola and Jones – face detection)</li><li class="listitem" style="list-style-type: disc">LBP cascades</li></ul></div><p>Let's briefly understand Haar and LBP Cascades and then build an Android application that uses these cascades to detect faces in images.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec41"/>Haar cascades</h2></div></div></div><p>One of the<a id="id210" class="indexterm"/> first real-time face detection algorithms, developed <a id="id211" class="indexterm"/>by Viola and Jones, was inspired by the concept of Haar wavelets. The algorithm exploits the inherent structure and similarities in human faces. For example, in every human face, the eye region is darker than the cheeks, and the nose bridge region is darker than the eyes. Using such characteristics of a human face, we learn the generic models of the face and then use these trained models to detect faces in images.</p><p>Initially, we feed a learning algorithm with positive images (images with faces) and negative images (images with out faces) and learn the classifier. Then we extract Haar features from the images using convolutional kernels (as shown in the following image). Feature values are obtained by subtracting the sum of white pixels under the white rectangle from the sum of pixels under the black rectangle. We slide these kernels (nothing but Haar features) over the entire image and calculate the feature values. If the value is above a certain user-defined threshold, we say that there is a match, otherwise we reject that region. To reduce calculations, we make use of integral images.</p><div><div><h3 class="title"><a id="tip09"/>Tip</h3><p>An explanation <a id="id212" class="indexterm"/>of integral images can be found at <a class="ulink" href="http://en.wikipedia.org/wiki/Summed_area_table">http://en.wikipedia.org/wiki/Summed_area_table</a>.</p></div></div><div><img src="img/B02052_04_01.jpg" alt="Haar cascades"/><div><p>Haar features</p></div></div><p>Training the <a id="id213" class="indexterm"/>classifier every time before using it is unacceptable in terms of the performance<a id="id214" class="indexterm"/> because it takes a lot of time; sometimes up to 6-7 hours or more. Hence, we use the pretrained classifiers provided by OpenCV (or any other source).</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec42"/>LBP cascades</h2></div></div></div><p>The <a id="id215" class="indexterm"/>
<strong>Local Binary Patterns</strong> (<strong>LBP</strong>) cascade is another type of a<a id="id216" class="indexterm"/> cascade classifier that is used widely in computer vision. Compared to Haar cascades, LBP cascades deal with integers rather than double values. So, both training and testing is faster with LBP cascades and hence is preferred while developing embedded applications. Another important property of LBP is their tolerance against illumination variations.</p><p>In LBP, an 8-bit binary feature vector is created for each pixel in the image by considering the eight neighboring pixels (top-left, top-right, left, right, bottom-left, and bottom-right). For every neighboring pixel, there is a corresponding bit which is assigned a value 1 if the pixel value is greater than the center pixel's value, otherwise 0. The 8-bit feature vector is treated as a binary number (later convert it to a decimal value), and using the decimal values for each pixel, a 256-bin histogram is computed. This histogram is used as a representative of the image.</p><p>LBP features have some primitives coded in them, as shown in the following image:</p><div><img src="img/B02052_04_02.jpg" alt="LBP cascades"/><div><p>Examples of texture primitives</p></div></div><p>For <a id="id217" class="indexterm"/>Haar cascade, we<a id="id218" class="indexterm"/> also make a set of positive images (with faces) and negative images (without faces). We compute histograms for each image and feed it to any learning algorithm.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Face detection using the cascade classifier</h1></div></div></div><p>One of the most <a id="id219" class="indexterm"/>common applications of the cascade classifier is face detection. Implementation for both Haar and LBP classifiers on Android using OpenCV is very similar; the only difference is in the model that we use to detect faces. Let's work on a generic application for face detection and make<a id="id220" class="indexterm"/> relevant changes to the application to accommodate both Haar and LBP cascades. The application will display the camera preview on the entire screen (landscape orientation) and make rectangles around faces in each frame. It will also provide an option to switch between the front and back camera. Following are the steps to create this application:</p><div><ol class="orderedlist arabic"><li class="listitem">Create a new Eclipse (or Android Studio) project with a blank activity and call the application <em>Face Detection</em>. It will be a landscape application with a fullscreen camera preview.</li><li class="listitem">In the application tag, add the following line to make a fullscreen application:<div><pre class="programlisting">android:theme="@android:style/Theme.NoTitleBar.Fullscreen"</pre></div></li><li class="listitem">Give the following permissions in <code class="literal">AndroidManifest.xml</code>:<div><pre class="programlisting">&lt;uses-permission android:name="android.permission.CAMERA"/&gt;
    &lt;uses-feature android:name="android.hardware.camera" android:required="false"/&gt;
    &lt;uses-feature android:name="android.hardware.camera.autofocus"      android:required="false"/&gt;
    &lt;uses-feature android:name="android.hardware.camera.front" android:required="false"/&gt;
    &lt;uses-feature android:name="android.hardware.camera.front.autofocus" android:required="false"/&gt;</pre></div></li><li class="listitem">In the main activity, add a camera preview view. This will display the camera's output on the screen. Add the view using the following lines:<div><pre class="programlisting">&lt;org.opencv.android.JavaCameraView
        android:layout_width="fill_parent"
        android:layout_height="fill_parent"
        android:id="@+id/java_surface_view" /&gt;</pre></div><div><div><h3 class="title"><a id="note13"/>Note</h3><p>OpenCV provides two camera preview views: <code class="literal">JavaCameraView</code> and <code class="literal">NativeCameraView</code>. Both the views work in a similar way except for a few differences. Refer to <a class="ulink" href="http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/dev_with_OCV_on_Android.html?highlight=nativecameraview">http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/dev_with_OCV_on_Android.html?highlight=nativecameraview</a> for a detailed explanation of the differences.</p></div></div></li></ol></div><p>In this <a id="id221" class="indexterm"/>application, we will implement the <code class="literal">CvCameraViewListener2</code> interface that has function definitions that provide some control over <a id="id222" class="indexterm"/>the camera (refer to the camera preview tutorial of OpenCV). We will take a look at these functions later in this section.</p><p>Unlike other applications seen so far in this book, this application has a different implementation of the <code class="literal">BaseLoaderCallback</code> class (for those who are not able to recollect, the <code class="literal">BaseLoaderCallback</code> class initializes and loads OpenCV modules in the application).</p><p>For this application, we will load the cascade classifiers after we have loaded OpenCV in our application. Here is the <code class="literal">BaseLoaderCallback</code> class for this application:</p><div><pre class="programlisting">private BaseLoaderCallback mLoaderCallback = new BaseLoaderCallback(this) {
        @Override
        public void onManagerConnected(int status) {
            switch (status) {
                case LoaderCallbackInterface.SUCCESS:
                {
                    Log.i(TAG, "OpenCV loaded successfully");
                    try{
                        InputStream is = getResources().openRawResource(&lt;INSERT_RESOURCE_IDENTIFIER&gt;);
                        File cascadeDir = getDir("cascade", Context.MODE_PRIVATE);
                        mCascadeFile = new File(cascadeDir, "cascade.xml");
                        FileOutputStream os = new FileOutputStream(mCascadeFile);

                        byte[] buffer = new byte[4096];
                        int bytesRead;
                        while((bytesRead = is.read(buffer)) != -1)
                        {
                            os.write(buffer, 0, bytesRead);
                        }
                        is.close();
                        os.close();

                        haarCascade = new CascadeClassifier(mCascadeFile.getAbsolutePath());
                        if (haarCascade.empty())
                        {
                            Log.i("Cascade Error","Failed to load cascade classifier");
                            haarCascade = null;
                        }
                    }
                    catch(Exception e)
                    {
                        Log.i("Cascade Error: ","Cascase not found");
                    }
                    mOpenCvCameraView.enableView();
                } break;
                default:
                {
                    super.onManagerConnected(status);
                } break;
            }
        }
    };</pre></div><p>In the preceding code snippet, we first check whether OpenCV was successfully loaded. After doing this, we <a id="id223" class="indexterm"/>copy the cascade file from the project resources to our application using <code class="literal">InputStream</code> and <code class="literal">FileOutputStream</code>, as shown next. Create a new folder <code class="literal">cascade</code> and <a id="id224" class="indexterm"/>copy the contents of the cascade file to a new file in that folder. Now comes the difference between using Haar cascades and LBP cascades. Replace <code class="literal">&lt;INSERT_RESOURCE_IDENTIFIER&gt;</code> with your favorite cascade file.</p><p>Note: The rest of the code works independently of your choice of the type of cascade.</p><div><div><h3 class="title"><a id="note14"/>Note</h3><p>OpenCV provides pre-learnt cascades for both Haar and LBP. Copy the cascade file to the <code class="literal">res/raw</code> folder in your Android project. Let's assume that your cascade files for Haar and LBP are named <code class="literal">haar_cascade.xml</code> and <code class="literal">lbp_cascade.xml</code> respectively. Replace <code class="literal">&lt;INSERT_RESOURCE_IDENTIFIER&gt;</code> with <code class="literal">R.raw.id.haar_casacde</code> or <code class="literal">R.raw.id.lbp_cascade</code>, depending on which classifier you want to use.</p></div></div><p>The reason why we copy and save at the same time is to bring the file from your project directory into your phone's filesystem:</p><div><pre class="programlisting">InputStream is = getResources().openRawResource(&lt;INSERT_RESOURCE_IDENTIFIER&gt;);
File cascadeDir = getDir("cascade", Context.MODE_PRIVATE);
mCascadeFile = new File(cascadeDir, "cascade.xml");
FileOutputStream os = new FileOutputStream(mCascadeFile);

byte[] buffer = new byte[4096];
int bytesRead;
while((bytesRead = is.read(buffer)) != -1)
{
os.write(buffer, 0, bytesRead);
}
is.close();
os.close();</pre></div><p>After this is done, create a new <code class="literal">CascadeClassifier</code> object that will be used later to detect faces in the camera feed, as shown in the following code snippet:</p><div><pre class="programlisting">haarCascade = new CascadeClassifier(mCascadeFile.getAbsolutePath());
if (cascade.empty())
{
    Log.i("Cascade Error","Failed to load cascade classifier");
    cascade = null;
}</pre></div><p>So far, we have been able to initialize OpenCV in our project, and we have loaded our favorite cascade classifier<a id="id225" class="indexterm"/> in to the application. The next step is to get our camera preview ready. As <a id="id226" class="indexterm"/>mentioned earlier, we are implementing the <code class="literal">CvCameraViewListener2</code> interface and hence, we need to implement its member functions:</p><div><pre class="programlisting">    @Override
    public void onCameraViewStarted(int width, int height) {
        mRgba = new Mat(height, width, CvType.CV_8UC4);
    }
    
    @Override
    public void onPause()
    {
        super.onPause();
        if (mOpenCvCameraView != null)
            mOpenCvCameraView.disableView();
    }

    @Override
    public void onResume()
    {
        super.onResume();
        OpenCVLoader.initAsync(OpenCVLoader.OPENCV_VERSION_2_4_9, this, mLoaderCallback);
    }

    public void onDestroy() {
        super.onDestroy();
        if (mOpenCvCameraView != null)
            mOpenCvCameraView.disableView();
    }</pre></div><p>Another function that needs to be implemented is <code class="literal">onCameraFrame()</code>. This is where all the magic happens. In this function, we will process each frame and find faces in it:</p><div><pre class="programlisting">@Override
    public Mat onCameraFrame(CameraBridgeViewBase.CvCameraViewFrame inputFrame) {

        //Rotating the input frame
        Mat mGray = inputFrame.gray();
        mRgba = inputFrame.rgba();
        if (mIsFrontCamera)
        {
            Core.flip(mRgba, mRgba, 1);
        }


        //Detecting face in the frame
        MatOfRect faces = new MatOfRect();
        if(cascade != null)
        {
            cascade.detectMultiScale(mGray, faces, 1.1, 2, 2, new Size(200,200), new Size());
        }

        Rect[] facesArray = faces.toArray();
        for (int i = 0; i &lt; facesArray.length; i++)
            Core.rectangle(mRgba, facesArray[i].tl(), facesArray[i].br(), new Scalar(100), 3);
        return mRgba;
    }</pre></div><p>Here, we <a id="id227" class="indexterm"/>first <a id="id228" class="indexterm"/>store the output of the camera in <code class="literal">mRgba</code>, and <code class="literal">mGray</code> stores the grayscale image of the camera output. Then we check whether we are using the front camera or the back camera of our phone (how to handle the front camera is explained later in this chapter) through a Boolean value <code class="literal">mIsFrontCamera</code> (data member of the class). If the front camera is being used, just flip the image. Now create a <code class="literal">MatOfRect</code> object that will store the rectangles that bound the faces in the frame. Then, call the magical function:</p><div><pre class="programlisting">cascade.detectMultiScale(mGray, faces, 1.1, 2, 2, new Size(200,200), new Size());</pre></div><p>The <code class="literal">detectMultiScale()</code> function takes in a grayscale image and returns rectangles that bound the faces (if any). The third parameter of the function is the scaling factor that specifies how much the image size is reduced at each image scale. For more accurate results, face detection happens at different scales. The last two parameters are the minimum and maximum size of the face that can be detected. These parameters sort of decide the speed at which your application runs. Having a minimum size can make your application perform <a id="id229" class="indexterm"/>poorly, that is, have very few frames per second. Be careful while setting these parameters.</p><p>Done! The <a id="id230" class="indexterm"/>application is almost complete with just one bit of functionality remaining: handling the front camera. In order to do this, follow these steps:</p><div><ol class="orderedlist arabic"><li class="listitem">We first add a menu option in the application's menu that allows the user to switch between the front and back camera, as follows:<div><pre class="programlisting">@Override
    public boolean onCreateOptionsMenu(Menu menu) {
        // Inflate the menu; this adds items to the action bar if it is present.
        getMenuInflater().inflate(R.menu.menu_main, menu);
        Log.i(TAG, "called onCreateOptionsMenu");
        mItemSwitchCamera = menu.add("Toggle Front/Back camera");
        return true;
    }</pre></div></li><li class="listitem">In the <code class="literal">onOptionsItemSelected()</code> function, add the functionality to switch between cameras:<div><pre class="programlisting">@Override
    public boolean onOptionsItemSelected(MenuItem item) {
        String toastMesage = "";
        
        if (item == mItemSwitchCamera) {
            mOpenCvCameraView.setVisibility(SurfaceView.GONE);
            mIsFrontCamera = !mIsFrontCamera;
            mOpenCvCameraView = (CameraBridgeViewBase) findViewById(R.id.java_surface_view);
            if (mIsFrontCamera) {
                
                mOpenCvCameraView.setCameraIndex(1);
                toastMesage = "Front Camera";
            } else {
                mOpenCvCameraView.setCameraIndex(-1);
                toastMesage = "Back Camera";
            }

            mOpenCvCameraView.setVisibility(SurfaceView.VISIBLE);
            mOpenCvCameraView.setCvCameraViewListener(this);
            mOpenCvCameraView.enableView();
            Toast toast = Toast.makeText(this, toastMesage, Toast.LENGTH_LONG);
            toast.show();
        }

        return true;
    }</pre></div></li><li class="listitem">Whenever the user selects this option, we first toggle the <code class="literal">isFrontCamera</code> value. After this, we change the camera index of the <code class="literal">mOpenCvCameraView</code> object by running the following code:<div><pre class="programlisting">mOpenCvCameraView.setCameraIndex(-1);</pre></div></li></ol></div><p>The default <a id="id231" class="indexterm"/>camera index in Android is <code class="literal">-1</code>, which represents <a id="id232" class="indexterm"/>the back camera. The front camera's index is 1 (this is not a fixed number; it can vary from one phone to another). Set the camera index according to the <code class="literal">isFrontCamera</code> value, as shown in the preceding code, and set the toast message to notify the user.</p><p>With this, we have successfully built our own version of a face detection application!</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec29"/>HOG descriptors</h1></div></div></div><p>
<strong>Histogram of Oriented Gradients</strong> (<strong>HOG</strong>) descriptors are <a id="id233" class="indexterm"/>feature descriptors that use the direction of intensity of the gradients and edge directions. For HOG descriptors, we divide the image into small cells, compute a histogram for each cell, and further combine these histograms to compute one single descriptor. They are similar to SIFT descriptors in the sense that both use image gradients and both divide the image into spatial bins and form a histogram, but SIFT descriptors help you to match local regions (using keypoint locations), while HOG descriptors use sliding windows to detect objects. The HOG descriptor works well with geometric and illumination transformations, but does not work well with object orientations (unlike SIFT, which <a id="id234" class="indexterm"/>works well with change in orientations).</p><p>The HOG descriptor is divided into multiple steps:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Computing gradient</strong>: We <a id="id235" class="indexterm"/>first calculate the gradient values for all the pixels in the image using any derivative mask over the image in horizontal and vertical directions (you can choose from either one direction or both directions). Some common derivative masks are the Sobel operator, Prewitt operator, and the likes, but the original algorithm recommends that you use a 1D derivative mask, that is, [-1, 0, +1].</li><li class="listitem" style="list-style-type: disc"><strong>Orientation binning</strong>: Create a <a id="id236" class="indexterm"/>histogram of the weighted gradients that were computed in the previous step. The gradient values are divided into bin values, ranging from 0 to 180, or 0 to 360 (depending on whether we are using signed or unsigned gradient values).</li><li class="listitem" style="list-style-type: disc"><strong>Combining cells to form blocks</strong>: After <a id="id237" class="indexterm"/>computing histograms for each cell, we combine these cells into blocks and form a combined histogram of the block using its constituent cell's normalized histograms. The final HOG descriptor is a vector of the normalized histograms.</li><li class="listitem" style="list-style-type: disc"><strong>Building the classifier</strong>: In<a id="id238" class="indexterm"/> the final step of the algorithm, feed the HOG feature vectors that were computed in the previous step in to your favorite learning algorithm, and build a model that will later be used to detect objects in images:<div><img src="img/B02052_04_03.jpg" alt="HOG descriptors"/><div><p>Flowchart of a HOG Descriptor</p></div></div></li></ul></div><p>Let's now take a <a id="id239" class="indexterm"/>look at an Android application that detects objects using HOG descriptors.</p><p>Since OpenCV provides a pretrained HOG descriptor to detect people in images, we will write an Android application that can detect people in images (we won't have to train our descriptor). Since the calculations involved in computing HOG descriptors are expensive, making a real-time application for a mobile platform with limited computational resources turns out to be a difficult task. So instead, we will build an application that will only detect people in single images.</p><p>For this, let's refer to <a class="link" href="ch02.html" title="Chapter 2. Detecting Basic Features in Images">Chapter 2</a>, <em>Detecting Basic Features in Images</em>, where we built an application that could read images from your phone's gallery and perform any operation based on the user's choice (hopefully, you still have that project saved somewhere in your computer). We won't need the entire application. We will only take the base of that application and make a new function to detect people in any image from the gallery.</p><p>If you have<a id="id240" class="indexterm"/> the project from <a class="link" href="ch02.html" title="Chapter 2. Detecting Basic Features in Images">Chapter 2</a>,<em> Detecting Basic Features in Images</em>, saved, make the following changes to it. Add a new menu option <em>Detect Face</em> to the application menu (refer to <a class="link" href="ch02.html" title="Chapter 2. Detecting Basic Features in Images">Chapter 2</a>,<em> Detecting Basic Features in Images</em>), and in the <code class="literal">onSelectedOptionItem()</code> function, add the following lines:</p><div><pre class="programlisting">else if (id == R.id.FaceDetect) {
            //Detec Faces
            HOGDescriptor();
        }</pre></div><p>Make a new function <code class="literal">HOGDescriptor()</code>, where we'll implement people detection as follows:</p><div><pre class="programlisting">void HOGDescriptor() {
        Mat grayMat = new Mat();
        Mat people = new Mat();

        //Converting the image to grayscale
        Imgproc.cvtColor(originalMat, grayMat, Imgproc.COLOR_BGR2GRAY);

        HOGDescriptor hog = new HOGDescriptor();
        hog.setSVMDetector(HOGDescriptor.getDefaultPeopleDetector());

        MatOfRect faces = new MatOfRect();
        MatOfDouble weights = new MatOfDouble();

        hog.detectMultiScale(grayMat, faces, weights);
        originalMat.copyTo(people);
        //Draw faces on the image
        Rect[] facesArray = faces.toArray();
        for (int i = 0; i &lt; facesArray.length; i++)
            Core.rectangle(people, facesArray[i].tl(), facesArray[i].br(), new Scalar(100), 3);

        //Converting Mat back to Bitmap
        Utils.matToBitmap(people, currentBitmap);
        loadImageToImageView();
    }</pre></div><p>In the preceding code snippet, we first convert the image to a grayscale image. Then, we initialize <code class="literal">HOGDescriptor</code> with a pretrained model (using SVM) using the following lines:</p><div><pre class="programlisting">HOGDescriptor hog = new HOGDescriptor();
hog.setSVMDetector(HOGDescriptor.getDefaultPeopleDetector());</pre></div><p>The next step <a id="id241" class="indexterm"/>is simple; we will call the <code class="literal">detectMultiScale()</code> function, which will return all the faces in the image. The second parameter in the function stores the regions where people were detected. We will then iterate through all such regions and draw rectangles around them on the image.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Project – Happy Camera</h1></div></div></div><p>Practice is better than theory. It's<a id="id242" class="indexterm"/> time to apply your learning from this chapter and build a cool camera application, which automatically clicks a picture when it detects smiling faces.</p><p>The trick is that we will use two different types of cascade classifiers. First, we will use Haar cascades to find faces on the image and store the positions of all the faces. Then we will use the Haar cascades to detect smiles in an image and store them. Now we try to match the face with a smile. For each smile, we find the corresponding face in the image. This is simple: if the smiling region is within any detected face region, we say that it's a match.</p><p>After locating all the smiling faces in the image, find the ratio of the smiling faces to all faces to nonsmiling faces, and if that ratio is greater than a certain threshold we say that it's a happy picture and click the image. Though one thing to note here is the ratio that we are using. We can use a different metric to tag an image as a happy image. If we calculate the ratio of smiling faces to total faces, there is a problem that if you have just two people in the image and one of them is not smiling (or has a standard expression), then our application will not click an image. Hence, to avoid such situations, we choose to have a relaxed ratio of smiling faces to nonsmiling faces in order to classify images as happy images.</p><p>How do we go about building this application? Most parts of the application have already been discussed in this chapter. The remaining parts of the application are as follows:</p><div><ol class="orderedlist arabic"><li class="listitem"><strong>Adding a smile detector</strong>: This<a id="id243" class="indexterm"/> is very simple. It is exactly the same as what we did to detect faces; instead here, we will use <a id="id244" class="indexterm"/>Haar cascades for smiles. You can find a pretrained model at <a class="ulink" href="https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_smile.xml">https://github.com/Itseez/opencv/blob/master/data/haarcascades/haarcascade_smile.xml</a>.</li><li class="listitem"><strong>Correlating faces and smiles</strong>: Once<a id="id245" class="indexterm"/> we have faces and smiles, we need to find matching pairs of faces and smiles in the image. Why do we want to correlate them? Why not use the number of smiles directly? Yes, we can do that. It is not necessary to correlate faces and smiles. The only advantage of doing this extra step is to reduce the false positives. If there is no corresponding face for a smile, we can choose to ignore that smile in our calculations.</li><li class="listitem"><strong>Tagging happy images</strong>: Once<a id="id246" class="indexterm"/> you have the face and smile pairs ready, calculate the ratio (explained earlier) and make a decision on whether you want to save that image or not.</li><li class="listitem"><strong>Actually saving the image</strong>: After <a id="id247" class="indexterm"/>tagging the image as a happy image, make a function that will actually save the image to your phone.</li></ol></div><p>You just made a cool camera application!</p><p>Only after you <a id="id248" class="indexterm"/>have tried to build this application yourself, you can take a look at a sample implementation from the code bundle that accompanies this book.</p></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Summary</h1></div></div></div><p>This chapter was a continuation of the last chapter, where we saw some basic feature detection algorithms. Here we have learnt a few more algorithms that can be used in face, eye, and person detection. Cascade classifiers are a type of supervised learning models, where we first train a classifier with some labelled data, and then use the trained model to detect new unencountered data.</p><p>In the coming chapters, we will take a look at topics such as image stitching and how to use machine learning in computer vision algorithms.</p></div></body></html>