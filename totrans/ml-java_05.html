<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Affinity Analysis</h1>
                </header>
            
            <article>
                
<p>Affinity analysis is the heart of <strong>Market Basket Analysis</strong> (<strong>MBA</strong>). It can discover co-occurring relationships among activities performed by specific users or groups. In retail, affinity analysis can help you understand the purchasing behavior of customers. These insights can drive revenue through smart cross-selling and upselling strategies and can assist you in developing loyalty programs, sales promotions, and discount plans.</p>
<p>In this chapter, we will look into the following topics:</p>
<ul>
<li>MBA</li>
<li>Association rule learning</li>
<li>Other applications in various domains</li>
</ul>
<p>First, we will revise the core association rule-learning concepts and algorithms, such as support and lift Apriori algorithms and the FP-Growth algorithm. Next, we will use Weka to perform our first affinity analysis on a supermarket dataset and study how to interpret the resulting rules. We will conclude this chapter by analyzing how association rule learning can be applied in other domains, such as IT operations analytics, and medicine.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Market basket analysis</h1>
                </header>
            
            <article>
                
<p>Since the introduction of electronic points of sale, retailers have been collecting an incredible amount of data. To leverage this data in to produce business value, they first developed a way to consolidate and aggregate the data to understand the basics of the business.</p>
<p>Recently, the focus shifted to the lowest level of granularity—the market basket transaction. At this level of detail, the retailers have direct visibility into the market basket of each customer who shopped at their store, understanding not only the quantity of the purchased items in that particular basket, but also how these items were bought in conjunction with one another. This can be used to drive decisions about how to differentiate store assortment and merchandise, as well as to effectively combine offers of multiple products, within and across categories, to drive higher sales and profits. These decisions can be implemented across an entire retail chain, by channel, at the local store level, and even for a specific customer, with so-called personalized marketing, where a unique product offering is made for each customer:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-662 image-border" src="Images/edb61ab2-da9d-439b-ae53-4148bb02191a.png" style="width:55.67em;height:34.42em;" width="1500" height="929"/></p>
<p>MBA covers a wide variety of analysis:</p>
<ul>
<li><strong>Item affinity</strong>: This defines the likelihood of two (or more) items being purchased together.</li>
<li><strong>Identification of driver items</strong>: This enables the identification of the items that drive people to the store and always need to be in stock.</li>
<li><strong>Trip classification</strong>: This analyzes the content of the basket and classifies the shopping trip into a category: weekly grocery trip, special occasion, and so on.</li>
<li><strong>Store-to-store comparison</strong>: Understanding the number of baskets allows any metric to be divided by the total number of baskets, effectively creating a convenient and easy way to compare the stores to different characteristics (units sold per customer, revenue per transaction, number of items per basket, and so on).</li>
<li><strong>Revenue optimization</strong>: This helps in determining the magic price points for this store, increasing the size and the value of the market basket.</li>
<li><strong>Marketing</strong>: This helps in identifying more profitable advertising and promotions, targeting offers more precisely to improve ROI, generating better loyalty card promotions with longitudinal analysis and attracting more traffic to the store.</li>
<li><strong>Operations optimization</strong>: This helps in matching the inventory to the requirement by customizing the store and assortment to trade area demographics, and optimizing store layout.</li>
</ul>
<p>Predictive models help retailers to direct the right offer to the right customer segments or profiles, as well as to gain an understanding of what is valid for which customer, predict the probability score of customers responding to this offer, and understand the customer value gain from the offer acceptance.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Affinity analysis</h1>
                </header>
            
            <article>
                
<p><strong>Affinity analysis</strong> is used to determine the likelihood that a set of items will be bought together. In retail, there are natural product affinities; for example, it is very typical for people who buy hamburger patties to buy hamburger rolls, along with ketchup, mustard, tomatoes, and other items that make up the burger experience.</p>
<p>While there are some product affinities that might seem trivial, there are some affinities that are not very obvious. A classic example is toothpaste and tuna. It seems that people who eat tuna are more prone to brushing their teeth right after finishing their meal. So, why it is important for retailers to get a good grasp of product affinities? This information is critical to appropriately plan promotions, as reducing the price for some items may cause a spike on related high-affinity items without the need to further promote these related items.</p>
<p>In the following section, we'll look into the algorithms for association rule learning: Apriori and FP-Growth.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Association rule learning</h1>
                </header>
            
            <article>
                
<p><strong>Association rule learning</strong> has been a popular approach to discover interesting relationships among items in large databases. It is most commonly applied in retail to reveal regularities between products.</p>
<p>Association rule learning approaches find patterns as interesting strong rules in the database using different measures of interestingness. For example, the following rule would indicate, that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat: {onions, potatoes} -&gt; {burger}.</p>
<p>Another classic story probably told in every machine-learning class is the beer and diaper story. An analysis of supermarket shoppers' behavior showed that customers, presumably young men, who buy diapers also tend to buy beer. It immediately became a popular example of how an unexpected association rule might be found from everyday data; however, there are varying opinions as to how much of the story is true. In <em>DSS News 2002,</em> Daniel Power<em>s</em> says this:</p>
<div class="packt_quote">"In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis did discover that between 5:00 and 7:00 pm, consumers bought beer and diapers. Osco managers did not exploit the beer and diapers relationship by moving the products closer together on the shelves."</div>
<p>In addition to the preceding example from MBA, association rules are today employed in many application areas, including web usage mining, intrusion detection, continuous production, and bioinformatics. We'll take a closer look at these areas later in this chapter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Basic concepts</h1>
                </header>
            
            <article>
                
<p>Before we dive into algorithms, let's first review the basic concepts.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Database of transactions</h1>
                </header>
            
            <article>
                
<p>In association rule mining, the dataset is structured a bit differently than the approach presented in the first chapter. First, there is no class value, as this is not required for learning association rules. Next, the dataset is presented as a transactional table, where each supermarket item corresponds to a binary attribute. Hence, the feature vector could be extremely large.</p>
<p>Consider the following example. Suppose we have four receipts, as shown next. Each receipt corresponds to a purchasing transaction:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-663 image-border" src="Images/ac76a6f0-2347-4519-9838-bc68759379dc.png" style="width:71.92em;height:23.92em;" width="863" height="287"/></p>
<p>To write these receipts in the form of a transactional database, we first identify all of the possible items that appear in the receipts. These items are <strong>onions</strong>, <strong>potatoes</strong>, <strong>burger</strong>, <strong>beer</strong>, and <strong>dippers</strong>. Each purchase, that is, transaction, is presented in a row, and there is <em>1</em> if an item was purchased within the transaction and <em>0</em> otherwise, as shown in the following table:</p>
<div class="packt_figure">
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong>Transaction ID</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Onions</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Potatoes</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Burger</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Beer</strong></td>
<td class="CDPAlignCenter CDPAlign"><strong>Dippers</strong></td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>4</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<p>This example is really small. In practical applications, the dataset often contains thousands or millions of transactions, which allow the learning algorithm the discovery of statistically significant patterns.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Itemset and rule</h1>
                </header>
            
            <article>
                
<p><strong>Itemset</strong> is simply a set of items, for example, {onions, potatoes, burger}. A rule consists of two itemsets, X and Y, in the following format:</p>
<p class="CDPAlignCenter CDPAlign">X -&gt; Y</p>
<p>This indicates a pattern that when the X itemset is observed, Y is also observed. To select interesting rules, various measures of significance can be used.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Support</h1>
                </header>
            
            <article>
                
<p>Support, for an itemset, is defined as the proportion of transactions that contain the itemset. The <kbd>{potatoes, burger}</kbd> itemset in the previous table has the following support, as it occurs in 50% of transactions (two out of four transactions): supp({potatoes, burger}) = 2/4 = 0.5.</p>
<p>Intuitively, it indicates the share of transactions that support the pattern.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Lift</h1>
                </header>
            
            <article>
                
<p class="mce-root CDPAlignLeft CDPAlign">Lift is a measure of the performance of a targeting model (association rule) at predicting or classifying cases as having an enhanced response (with respect to the population as a whole), measured against a random choice targeting model. It is defined using the following formula:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="Images/6fcd6f36-1c1a-47f6-b4f8-a0e8de3f4b7b.png" style="width:10.83em;height:3.08em;" width="1680" height="480"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Confidence</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">The confidence of a rule indicates its accuracy. It is defined using the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><sub><img class="fm-editor-equation" src="Images/5eaba711-2c9e-4443-bb45-987eb9c7aa41.png" style="width:18.17em;height:1.83em;" width="2880" height="290"/></sub></p>
<p>For example, the {onions, burger} -&gt; {beer} rule has the confidence <em>0.5/0.5 = 1.0</em> in the previous table, which means that 100% of the time when onions and burger are bought together, beer is bought as well.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Apriori algorithm</h1>
                </header>
            
            <article>
                
<p>The <strong>Apriori algorithm</strong> is a classic algorithm used for frequent pattern mining and association rule learning over transactional. By identifying the frequent individual items in a database and extending them to larger itemsets, Apriori can determine the association rules, which highlight general trends about a database.</p>
<p>The Apriori algorithm constructs a set of itemsets, for example, itemset1= {Item A, Item B}, and calculates support, which counts the number of occurrences in the database. Apriori then uses a bottom up approach, where frequent itemsets are extended, one item at a time, and it works by eliminating the largest sets as candidates by first looking at the smaller sets and recognizing that a large set cannot be frequent unless all of its subsets are. The algorithm terminates when no further successful extensions are found.</p>
<p>Although the Apriori algorithm is an important milestone in machine learning, it suffers from a number of inefficiencies and tradeoffs. In the following section, we'll look into a more recent FP-Growth technique.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">FP-Growth algorithm</h1>
                </header>
            
            <article>
                
<p><strong>FP-Growth</strong> (where FP is frequent patterns) represents the transaction database as a suffix tree. First, the algorithm counts the occurrence of items in the dataset. In the second pass, it builds a suffix tree, an ordered tree data structure commonly used to store a string. An example of a suffix tree based on the previous example is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-664 image-border" src="Images/6dc05f73-8ffc-4870-81a2-bb5ee883ec77.png" style="width:25.50em;height:24.42em;" width="659" height="632"/></p>
<p>If many transactions share the most frequent items, the suffix tree provides high compression close to the tree root. Large itemsets are grown directly, instead of generating candidate items and testing them against the entire database. Growth starts at the bottom of the tree, by finding all of the itemsets matching minimal support and confidence. Once the recursive process has completed, all large itemsets with minimum coverage have been found and association rule creation begins.</p>
<p>An FP-Growth algorithm has several advantages. First, it constructs an FP-tree, which encodes the original dataset in a substantially compact presentation. Second, it efficiently builds frequent itemsets, leveraging the FP-tree structure and the divide-and-conquer strategy.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The supermarket dataset</h1>
                </header>
            
            <article>
                
<p>The supermarket dataset, located in <kbd>data/supermarket.arff</kbd>, describes the shopping habits of supermarket customers. Most of the attributes stand for a particular item group, for example, diary foods, beef, and potatoes; or they stand for a department, for example, department 79, department 81, and so on. The following table shows an excerpt of the database, where the value is <em>t</em> if the customer had bought an item and missing otherwise. There is one instance per customer. The dataset contains no class attribute, as this is not required to learn association rules. A sample of data is shown in the following table:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-665 image-border" src="Images/795041e5-78aa-46a5-88c4-7ee1e2b0d6c7.png" style="width:68.42em;height:30.92em;" width="962" height="435"/></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Discover patterns</h1>
                </header>
            
            <article>
                
<p>To discover shopping patterns, we will use the two algorithms that we have looked into before: Apriori and FP-Growth.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Apriori</h1>
                </header>
            
            <article>
                
<p>We will use the <kbd>Apriori</kbd> algorithm as implemented in Weka. It iteratively reduces the minimum support until it finds the required number of rules with the given minimum confidence. We'll implement the algorithm using the following steps:</p>
<ol>
<li>We'll import the required libraries using the following lines of code:</li>
</ol>
<pre style="padding-left: 60px">import java.io.BufferedReader; 
import java.io.FileReader; 
import weka.core.Instances; 
import weka.associations.Apriori; </pre>
<ol start="2">
<li>First, we'll load the <kbd>supermarket.arff</kbd> dataset:</li>
</ol>
<pre style="padding-left: 60px">Instances data = new Instances(new BufferedReader(new FileReader("data/supermarket.arff"))); </pre>
<ol start="3">
<li>We'll initialize an <kbd>Apriori</kbd> instance and call the <kbd>buildAssociations(Instances)</kbd> function to start frequent pattern mining, as follows:</li>
</ol>
<pre style="padding-left: 60px">Apriori model = new Apriori(); 
model.buildAssociations(data); </pre>
<ol start="4">
<li>We can output the discovered itemsets and rules, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">System.out.println(model); </pre>
<p style="padding-left: 60px">The output is as follows:</p>
<pre style="padding-left: 30px">   <strong>Apriori</strong>
    <strong>=======</strong>
    
    <strong>Minimum support: 0.15 (694 instances)</strong>
    <strong>Minimum metric &lt;confidence&gt;: 0.9</strong>
    <strong>Number of cycles performed: 17</strong>
    
    <strong>Generated sets of large itemsets:</strong>
    <strong>Size of set of large itemsets L(1): 44</strong>
    <strong>Size of set of large itemsets L(2): 380</strong>
    <strong>Size of set of large itemsets L(3): 910</strong>
    <strong>Size of set of large itemsets L(4): 633</strong>
    <strong>Size of set of large itemsets L(5): 105</strong>
    <strong>Size of set of large itemsets L(6): 1</strong>
    
    <strong>Best rules found:</strong>
    
    <strong> 1. biscuits=t frozen foods=t fruit=t total=high 788 ==&gt; bread and cake=t 723    &lt;conf:(0.92)&gt; lift:(1.27) lev:(0.03) [155] conv:(3.35)</strong>
    <strong> 2. baking needs=t biscuits=t fruit=t total=high 760 ==&gt; bread and cake=t 696    &lt;conf:(0.92)&gt; lift:(1.27) lev:(0.03) [149] conv:(3.28)</strong>
    <strong> 3. baking needs=t frozen foods=t fruit=t total=high 770 ==&gt; bread and cake=t 705    &lt;conf:(0.92)&gt; lift:(1.27) lev:(0.03) [150] conv:(3.27)</strong>
    <strong>...</strong>
  </pre>
<p>The algorithm outputs the ten best rules according to confidence. Let's look at the first rule and interpret the output, as follows:</p>
<pre><strong>biscuits=t frozen foods=t fruit=t total=high 788 ==&gt; bread and cake=t 723    &lt;conf:(0.92)&gt; lift:(1.27) lev:(0.03) [155] conv:(3.35)</strong>
  </pre>
<p>It says that when <kbd>biscuits</kbd>, <kbd>frozen foods</kbd>, and <kbd>fruits</kbd> are bought together and the total purchase price is high, it is also very likely that <kbd>bread</kbd> and <kbd>cake</kbd> are purchased as well. The <kbd>{biscuits, frozen foods, fruit, total high}</kbd> itemset appears in <kbd>788</kbd> transactions, while the <kbd>{bread, cake}</kbd> itemset appears in <kbd>723</kbd> transactions. The confidence of this rule is <kbd>0.92</kbd>, meaning that the rule holds true in 92% of transactions where the <kbd>{biscuits, frozen foods, fruit, total high}</kbd> itemset is present.</p>
<p>The output also reports additional measures such as lift, leverage, and conviction, which estimate the accuracy against our initial assumptions; for example, the <kbd>3.35</kbd> conviction value indicates that the rule would be incorrect <kbd>3.35</kbd> times as often if the association was purely a random chance. Lift measures the number of times X and Y occur together than expected if they were statistically independent <kbd>(lift=1)</kbd>. The <kbd>2.16</kbd> lift in the X -&gt; Y rule means that the probability of X is <kbd>2.16</kbd> times greater than the probability of Y.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">FP-Growth</h1>
                </header>
            
            <article>
                
<p>Now, let's try to get the same results with the more efficient FP-Growth algorithm.<br/>
FP-Growth is also implemented in the <kbd>weka.associations</kbd> package:</p>
<pre>import weka.associations.FPGrowth; </pre>
<p>The FP-Growth algorithm is initialized similarly, as we did earlier:</p>
<pre>FPGrowth fpgModel = new FPGrowth(); 
fpgModel.buildAssociations(data); 
System.out.println(fpgModel); </pre>
<p>The output reveals that FP-Growth discovered <kbd>16 rules</kbd>:</p>
<pre>    <strong>FPGrowth found 16 rules (displaying top 10)</strong>
    
    <strong> 1. [fruit=t, frozen foods=t, biscuits=t, total=high]: 788 ==&gt; [bread and cake=t]: 723   &lt;conf:(0.92)&gt; lift:(1.27) lev:(0.03) conv:(3.35) </strong>
    <strong> 2. [fruit=t, baking needs=t, biscuits=t, total=high]: 760 ==&gt; [bread and cake=t]: 696   &lt;conf:(0.92)&gt; lift:(1.27) lev:(0.03) conv:(3.28) </strong>
    <strong>...</strong>
  </pre>
<p>We can observe that FP-Growth found the same set of rules as Apriori; however, the time required to process larger datasets can be significantly shorter.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Other applications in various areas</h1>
                </header>
            
            <article>
                
<p>We looked into affinity analysis to demystify shopping behavior patterns in supermarkets. Although the roots of association rule learning are in analyzing point-of-sale transactions, they can be applied outside the retail industry to find relationships among other types of baskets. The notion of a basket can easily be extended to services and products, for example, to analyze items purchased using a credit card, such as rental cars and hotel rooms, and to analyze information on value-added services purchased by telecom customers (call waiting, call forwarding, DSL, speed call, and so on), which can help the operators determine the ways to improve their bundling of service packages.</p>
<p>Additionally, we will look into the following examples of potential cross industry applications:</p>
<ul>
<li>Medical diagnosis</li>
<li>Protein sequences</li>
<li>Census data</li>
<li>Customer relationship management</li>
<li>IT operations analytics</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Medical diagnosis</h1>
                </header>
            
            <article>
                
<p>Applying association rules in medical diagnosis can be used to assist physicians while curing patients. The general problem of the induction of reliable diagnostic rules is hard as, theoretically, no induction process can guarantee the correctness of induced hypotheses by itself. Practically, diagnosis is not an easy process, as it involves unreliable diagnosis tests and the presence of noise in training examples.</p>
<p>Nevertheless, association rules can be used to identify likely symptoms appearing together. A transaction, in this case, corresponds to a medical case, while symptoms correspond to items. When a patient is treated, a list of symptoms is recorded as one transaction.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Protein sequences</h1>
                </header>
            
            <article>
                
<p>A lot of research has gone into understanding the composition and nature of proteins; yet, many things remain to be understood satisfactorily. It is now generally believed that amino acid sequences of proteins are not random.</p>
<p>With association rules, it is possible to identify associations between different amino acids that are present in a protein. A protein is a sequence made up of 20 types of amino acids. Each protein has a unique three-dimensional structure, which depends on the amino-acid sequence; a slight change in the sequence may change the functioning of protein. To apply association rules, a protein corresponds to a transaction, while amino acids and their structure correspond to the items.</p>
<p>Such association rules are desirable for enhancing our understanding of protein composition and hold the potential to give clues regarding the global interactions among some particular sets of amino acids occurring in the proteins. Knowledge of these association rules or constraints is highly desirable for the synthesis of artificial proteins.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Census data</h1>
                </header>
            
            <article>
                
<p>Censuses make a huge variety of general statistical information about a society available to both researchers and general public. The information related to population and economic censuses can be forecasted in planning public services (education, health, transport, and funds) as well as in business (for setting up new factories, shopping malls, or banks and even marketing particular products).</p>
<p>To discover frequent patterns, each statistical area (for example, municipality, city, and neighborhood) corresponds to a transaction, and the collected indicators correspond to the items.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Customer relationship management</h1>
                </header>
            
            <article>
                
<p>The <strong>Customer Relationship Management</strong> (<strong>CRM</strong>), as we briefly discussed in the previous chapters, is a rich source of data through which companies hope to identify the preference of different customer groups, products, and services in order to enhance the cohesion among their products and services and their customers.</p>
<p>Association rules can reinforce the knowledge management process and allow the marketing personnel to know their customers well to provide better quality services. For example, association rules can be applied to detect a change of customer behavior at different time snapshots from customer profiles and sales data. The basic idea is to discover changes from two datasets and generate rules from each dataset to carry out rule matching.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">IT operations analytics</h1>
                </header>
            
            <article>
                
<p>Based on records of a large number of transactions, association rule learning is well-suited to be applied to the data that is routinely collected in day-to-day IT operations, enabling IT operations analytics tools to detect frequent patterns and identify critical changes. IT specialists need to see the big picture and understand, for example, how a problem on a database could impact an application server.</p>
<p>For a specific day, IT operations may take in a variety of alerts, presenting them in a transactional database. Using an association rule-learning algorithm, IT operations analytics tools can correlate and detect the frequent patterns of alerts appearing together. This can lead to a better understanding about how one component impacts another.</p>
<p>With identified alert patterns, it is possible to apply predictive analytics. For example, a particular database server hosts a web application and suddenly an alert about a database is triggered. By looking into frequent patterns identified by an association rule-learning algorithm, this means that the IT staff need to take action before the web application is impacted.</p>
<p>Association rule learning can also discover alert events originating from the same IT event. For example, every time a new user is added, six changes in the Windows operating system are detected. Next, in <strong>Application Portfolio Management</strong> (<strong>APM</strong>), IT may face multiple alerts, showing that the transactional time in a database as high. If all of these issues originate from the same source (such as getting hundreds of alerts about changes that are all due to a Windows update), this frequent pattern mining can help to quickly cut through a number of alerts, allowing the IT operators to focus on truly critical changes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, you learned how to leverage association rule learning on transactional datasets to gain insight about frequent patterns. We performed an affinity analysis in Weka and learned that the hard work lies in the analysis of results—careful attention is required when interpreting rules, as association (that is, correlation) is not the same as causation.</p>
<p>In the next chapter, we'll look at how to take the problem of item recommendation to the next level using a scalable machine-learning library, Apache Mahout, which is able to handle big data.</p>


            </article>

            
        </section>
    </div>



  </body></html>