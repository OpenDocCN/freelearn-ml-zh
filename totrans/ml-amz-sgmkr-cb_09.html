<html><head></head><body>
		<div id="_idContainer156">
			<h1 id="_idParaDest-109"><em class="italic"><a id="_idTextAnchor110"/>Chapter 7</em>: Model Understanding and Explainability</h1>
			<p>In the last chapter, we learned how to build models, and we will now learn how to use output generated by DataRobot to understand the models and also use this information to explain why a model provides a particular prediction. As we have discussed before, this aspect is critically important to ensure that we are using the results correctly. DataRobot automates much of the task of creating charts and plots to help someone understand a model, but you still need to know how to interpret what it is showing in the context of the problem you are trying to solve. This is another reason why we will need people involved in the process, even if much of a task has been automated. As you can imagine, the task of interpreting the results will therefore become more and more valuable as the degree of automation increases.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Reviewing and understanding model details</li>
				<li>Assessing model performance and metrics</li>
				<li>Generating model explanations</li>
				<li>Understanding model learning curves and trade-offs</li>
			</ul>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor111"/>Reviewing and understanding model details</h1>
			<p>In the last chapter, we created several models for different projects. DataRobot creates 10 to 20 models in a <a id="_idIndexMarker333"/>project, and it would be very onerous to look at and analyze the details of all of these models. You do not have to review each of these models, and it is common to review only the top few models before making a final selection. We will now look at the leaderboard for models in the <strong class="source-inline">Automobile Example 2</strong> project and select the top model, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/Figure_7.1_B17159.jpg" alt="Figure 7.1 – Model information&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Model information</p>
			<p>In the preceding screenshot, we selected the <strong class="bold">Model Info</strong> tab within the <strong class="bold">Describe</strong> tab to get a view of how large the <a id="_idIndexMarker334"/>model is and the expected time it takes to create predictions. This information is useful in real-time applications that are time-sensitive and need to score thousands of transactions quickly. Let's now go to the <strong class="bold">Feature Impact</strong> tab within the <strong class="bold">Understand</strong> tab, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/Figure_7.2_B17159.jpg" alt="Figure 7.2 – Feature impacts &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Feature impacts </p>
			<p>This is one of the most important charts for the model as it shows how much a feature contributes to this XGBoost model. We can see that the top contributors are <strong class="source-inline">curb_weight</strong>, <strong class="source-inline">engine_size</strong>, <strong class="source-inline">horsepower</strong>, <strong class="source-inline">highway_mpg</strong>, and <strong class="source-inline">cylinder_count</strong>. On the other hand, <strong class="source-inline">cylinder_size</strong> and <strong class="source-inline">engine_type</strong> contribute very little. While it is true that <strong class="source-inline">cylinder_size</strong> is not very predictive, we must not forget that prediction is not always the end objective. We know that <strong class="source-inline">cylinder_size</strong> has an effect on <strong class="source-inline">engine_size</strong>, an important feature. The objective might be to use this information to figure out ways to reduce costs. For that, we might want to reduce <strong class="source-inline">engine_size</strong>, but you cannot reduce <strong class="source-inline">engine_size</strong> directly. For that, you need to reduce the size or count of cylinders, which will lead to a reduction in <strong class="source-inline">engine_size</strong>. Having a causal diagram of this problem to guide you becomes very helpful in determining the best actions to take to achieve our objectives.</p>
			<p>Before we take action, let's <a id="_idIndexMarker335"/>inspect what the results look like for a <strong class="bold">Generalized Additive Model</strong> (<strong class="bold">GAM</strong>), as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/Figure_7.3_B17159.jpg" alt="Figure 7.3 – Feature impacts for a GAM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Feature impacts for a GAM</p>
			<p><em class="italic">Figure 7.3</em> shows the important features of the GAM. While many of the features look similar, we notice that <strong class="source-inline">engine_type</strong> is fairly high in importance for this model, whereas <strong class="source-inline">engine_type</strong> was very low in importance for the previous model. This is not an error—it points to the fact that many of the features are interrelated and different models can pick up signals from different features, and that predictive power is not necessarily the <a id="_idIndexMarker336"/>same as the root cause. To take action, we need to understand the root feature that leads to a change in the target feature. To put this another way, the feature that best predicts something is not always the feature that can be changed to create the desired change in the target.</p>
			<p>To further understand how a <a id="_idIndexMarker337"/>feature affects the target, let's select the <strong class="bold">Feature Effects</strong> tab within the <strong class="bold">Understand</strong> tab of the model, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer135" class="IMG---Figure">
					<img src="image/Figure_7.4_B17159.jpg" alt="Figure 7.4 – Feature effects &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Feature effects </p>
			<p>The preceding <a id="_idIndexMarker338"/>screenshot shows partial dependence plots for various features. The selected plot is for <strong class="source-inline">curb_weight</strong>. The plot shows a fairly linear relationship between <strong class="source-inline">curb_weight</strong> and price. We do see some unusual dips in price in a few spots—for example, around a <strong class="source-inline">curb_weight</strong> value of <strong class="source-inline">2700</strong>. Before we take that too seriously, we notice that the amount of data around that is very limited. This tells us that this particular observation is likely due to a lack of data. This does raise the issue that our model is likely to predict a lower price in that small region, which in turn could result in lower revenue.</p>
			<p>Let's look at another feature in the following screenshot:</p>
			<div>
				<div id="_idContainer136" class="IMG---Figure">
					<img src="image/Figure_7.5_B17159.jpg" alt="Figure 7.5 – Partial dependence for engine_size&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Partial dependence for engine_size</p>
			<p>The preceding screenshot shows a highly non-linear relationship between <strong class="source-inline">engine_size</strong> and price. We see a very dramatic rise in price around the <strong class="source-inline">engine_size</strong> value of <strong class="source-inline">180</strong>. It is hard to know how real this effect is without discussing it with domain experts. We can notice that the amount of data available for sizes <a id="_idIndexMarker339"/>greater than <strong class="source-inline">130</strong> is very small, hence the effects we see could be simply due to a lack of data. Taken as is, it indicates that prices stagnate beyond a size of <strong class="source-inline">200</strong>, and this could be an important insight for the business.</p>
			<p>Let's take a look at another partial dependence plot for <strong class="source-inline">highway_mpg</strong> in the following screenshot:</p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/Figure_7.6_B17159.jpg" alt="Figure 7.6 – Partial dependence plot for highway_mpg&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Partial dependence plot for highway_mpg</p>
			<p><em class="italic">Figure 7.6</em> shows another <a id="_idIndexMarker340"/>highly non-linear relationship, with a key transition point happening around a <strong class="source-inline">highway_mpg</strong> value of <strong class="source-inline">28</strong>. This clearly shows a big price drop around <strong class="source-inline">28</strong>, hence this is a critical point. This could be due to regulations, where going below <strong class="source-inline">28</strong> places you in a different type of vehicle or engine. We also notice that once you get above that threshold, any further change is not very meaningful from a price impact (however, it could still be very impactful from other perspectives). If you do not know why this is, it is <a id="_idIndexMarker341"/>important for you to discuss this with your <strong class="bold">subject-matter experts</strong> (<strong class="bold">SMEs</strong>).</p>
			<p>My main objective for showing and discussing these plots is to show you how important it is to spend your time analyzing and reviewing these plots rather than spending all of your time coding up these plots. Since DataRobot automatically creates these for you, you can now spend your time doing the more value-added work of analyzing these results to help improve your business.</p>
			<p>Let's revisit the <strong class="source-inline">engine_size</strong> plot, but <a id="_idIndexMarker342"/>this time for the GAM, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/Figure_7.7_B17159.jpg" alt="Figure 7.7 – Partial dependence plot for the GAM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Partial dependence plot for the GAM</p>
			<p><em class="italic">Figure 7.7</em> shows the partial dependence for the GAM. Comparing this with <em class="italic">Figure 7.5</em>, we see that <em class="italic">Figure 7.7</em> shows clearer thresholds around values <strong class="source-inline">95</strong> and <strong class="source-inline">180</strong>. Discussing this with domain experts could help you determine which model is a better representation <a id="_idIndexMarker343"/>of reality and which model helps you to better set pricing. One of the benefits of GAMs is that you can easily smooth out these curves and shape them for deployment. Remember—accurate prediction is not always the same as better intervention or action.</p>
			<p>GAMs are a lot easier to understand and explain. Let's look at another chart here that helps in that understanding:</p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/Figure_7.8_B17159.jpg" alt=" Figure 7.8 – Feature coefficients&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 7.8 – Feature coefficients</p>
			<p><em class="italic">Figure 7.8</em> shows the coefficients for different features in the GAM. You will notice that DataRobot has <a id="_idIndexMarker344"/>created some derived features. You can click on them to see more details. This provides a high-level view of the coefficients, but there is another view that provides a better view for understanding the model. For that, let's click on the <strong class="bold">Rating Table</strong> tab within the <strong class="bold">Describe</strong> tab for the GAM, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/Figure_7.9_B17159.jpg" alt="Figure 7.9 – Rating table for a GAM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – Rating table for a GAM</p>
			<p>This view lets you download the rating table built by DataRobot; you can also modify this table and upload it <a id="_idIndexMarker345"/>back to use the modified table. This mechanism thus allows you to manually fine-tune your model based on your understanding of the problem. This feature is therefore very powerful as it allows you a lot of flexibility, but at the same time, you must <a id="_idIndexMarker346"/>use this carefully. Let's click on the <strong class="bold">Download table</strong> button and download the <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) file. Once downloaded, we can open the file using <strong class="bold">Excel</strong>, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/Figure_7.10_B17159.jpg" alt="Figure 7.10 – Rating table for a GAM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.10 – Rating table for a GAM</p>
			<p>You can now see what the rating table looks like. Here, you see that DataRobot has created bins for <a id="_idIndexMarker347"/>various features. For each bin, it has assigned the coefficient and relativity as to how changes in a feature impact the target variable. To understand this a bit better, we can create plots for individual features in Excel, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/Figure_7.11_B17159.jpg" alt="Figure 7.11 – Feature relativities&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.11 – Feature relativities</p>
			<p>In <em class="italic">Figure 7.11</em>, you can see how a given feature such as <strong class="source-inline">body_style</strong> contributes to the price. The GAM model is essentially a sum of all the contributions from the selected features. Given the <a id="_idIndexMarker348"/>rating table, anyone can easily calculate the price, and this can also be implemented in a very simple manner. Given that the individual feature effects are non-linear (and still very understandable), this allows these models to perform very well while still being very easy to understand. It is no wonder that GAMs are becoming very popular.</p>
			<p>There is one more chart that we want to look at that is frequently helpful in understanding the contributions of features. For this, we will click on the <strong class="bold">Insights</strong> menu item at the top of the page, which brings up the chart shown here:</p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/Figure_7.12_B17159.jpg" alt="Figure 7.12 – Model insights&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.12 – Model insights</p>
			<p><em class="italic">Figure 7.12</em> shows the variable effects using a DataRobot selected model that is built using constant splines (in this case, the <strong class="bold">Ridge Regressor</strong> model). This shows the effects of the key feature values in one view, and you can get a sense of relative impact as well as the positive versus the negative contribution of features. A <strong class="bold">constant spline</strong> is a feature transformation <a id="_idIndexMarker349"/>where a numeric feature is converted into pieces made up of constant splines. The value of the feature is <strong class="source-inline">one</strong> if the value falls within a specific interval; otherwise, it is <strong class="source-inline">zero</strong>. You can review this chart with reference to the feature effects for the models you have selected to see if there are any inconsistencies between these charts.</p>
			<p>Now that we understand the model from the perspective of which features are important and how they contribute toward the target value, we can focus on how well the model is doing.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor112"/>Assessing model performance and metrics</h1>
			<p>In this section, we will focus on how well a model is doing in trying to predict the target values. Let's start by <a id="_idIndexMarker350"/>looking at the overall performance comparison across different models, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/Figure_7.13_B17159.jpg" alt="Figure 7.13 – Performance across models&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.13 – Performance across models</p>
			<p>The preceding screenshot shows the overall leaderboard, which we have seen before. Here, we can see the overall performance of different models based on the <strong class="bold">Gamma Deviance</strong> metric. We can also review the performance based on other metrics by clicking on the drop-down arrow near the metric, which shows us a list of metrics we can choose from, as illustrated in the following screenshot:</p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/Figure_7.14_B17159.jpg" alt="Figure 7.14 – Performance metrics&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14 – Performance metrics</p>
			<p><em class="italic">Figure 7.14</em> shows the various metrics we can select from. You will typically see a similar trend across <a id="_idIndexMarker351"/>different metrics in terms of which models surface to the top spots. In general, the metric that DataRobot selects is a very good choice, if not the best choice. Let's now inspect the performance details of specific models by clicking on the model and selecting the <strong class="bold">Lift Chart</strong> tab within the <strong class="bold">Evaluate</strong> tab, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/Figure_7.15_B17159.jpg" alt="Figure 7.15 – Lift chart&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.15 – Lift chart</p>
			<p>The lift chart illustrated in the preceding screenshot shows how the predictions stack up against the actual values. You can <a id="_idIndexMarker352"/>select the number of bins to aggregate the results. The maximum value is <strong class="source-inline">60</strong>, and that is normally a good starting point. This means that the predictions are first sorted in ascending order and then grouped into <strong class="source-inline">60</strong> bins. The results you see are the average values within that bin. The reason for binning is that if you look at the entire dataset, there will be so much data that you will not be able to make any sense out of it. You can see that the model does very well over the entire range of values, with some small pockets where the differences seem higher than the rest. We typically want to see lift charts for multiple models, to see if there are areas where one model does better than another model. Let's now look at the lift chart for the GAM, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/Figure_7.16_B17159.jpg" alt="Figure 7.16 – Lift chart for the GAM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.16 – Lift chart for the GAM</p>
			<p>The results in <em class="italic">Figure 7.16</em> look very similar to the results from <em class="italic">Figure 7.15</em>, but we can see that the GAM did not do as well for higher values. We now know where specifically the GAM <a id="_idIndexMarker353"/>is weaker as compared to the XGBoost model. Let's look further by clicking on the <strong class="bold">Residuals</strong> tab within the <strong class="bold">Evaluate</strong> tab, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/Figure_7.17_B17159.jpg" alt="Figure 7.17 – Model residuals&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.17 – Model residuals</p>
			<p>The residuals seem to be well distributed around the mean but with a small skew toward -ve values. Let's <a id="_idIndexMarker354"/>also check how the residuals are distributed for the GAM. We can see the output in the following screenshot:</p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/Figure_7.18_B17159.jpg" alt="Figure 7.18 – Residuals for the GAM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.18 – Residuals for the GAM</p>
			<p>The residuals for the GAM are also well distributed but with a slightly larger skew compared to the <a id="_idIndexMarker355"/>XGBoost model. Overall, the performance of the models looks very good. We can now look into understanding individual predictions and their explanations.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor113"/>Generating model explanations</h1>
			<p>Another key capability of DataRobot is that it automatically generates instance-level explanations for <a id="_idIndexMarker356"/>each prediction. This is important in understanding why a particular prediction turned out the way it did. This is not only important for understanding the model; many times, this is needed for compliance purposes as well. I am sure you have seen explanations generated or offered if you are denied credit. The ability to generate these explanations is not straightforward <a id="_idIndexMarker357"/>and can be very time-consuming. Let's first look at the explanations generated for the XGBoost model, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/Figure_7.19_B17159.jpg" alt="Figure 7.19 – Model explanations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.19 – Model explanations</p>
			<p>Since we selected the <strong class="bold">SHAP</strong> option for this project, the model explanations are based on <strong class="bold">SHapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>) algorithms. Here, you can see the overall <a id="_idIndexMarker358"/>distribution of predictions on the left, and you can see that most of the dataset lies in the range of <strong class="source-inline">0</strong> to <strong class="source-inline">10000</strong>. You can select some specific points and see the components that make up that prediction. In <em class="italic">Figure 7.19</em>, we have selected the prediction point of <strong class="source-inline">27788.86</strong>. We can see the top contributing elements on the right, where <strong class="source-inline">engine_size</strong> is contributing the most, and in this case, the value of <strong class="source-inline">engine_size</strong> is <strong class="source-inline">183</strong>. Notice that the relative contribution of features can vary on a case-by-case basis, and the ordering of features here will not exactly match the feature-impacts order we saw in the preceding section. Let's compare this with explanations generated by the GAM, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/Figure_7.20_B17159.jpg" alt="Figure 7.20 – Model explanations for GAM&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.20 – Model explanations for GAM</p>
			<p>In the preceding<a id="_idIndexMarker359"/> screenshot, the point selected is for a prediction of <strong class="source-inline">31465.18</strong>. For this point, we can see the features that are the main contributors toward that price, and we also note that there was a reduction or -ve contribution due to the <strong class="bold">make</strong> of the vehicle being <strong class="bold">Mercedes-Benz</strong>. We can also see that in this case, the contribution of <strong class="source-inline">engine_size</strong> of <strong class="source-inline">183</strong> is much larger for the GAM.</p>
			<p>The explanations for the entire dataset can be downloaded and analyzed for additional insights. You can also upload an entirely new dataset to score it and generate these explanations very easily, by clicking on the <strong class="bold">Upload new dataset</strong> button.</p>
			<p>As you have seen in this chapter, different models have different performance, use the features a little bit differently, and have different levels of understandability. There are a few other dimensions that should be looked at before making a final selection of the model you want to use. Let's now look at model learning curves and some of the model trade-offs.</p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor114"/>Understanding model learning curves and trade-offs</h1>
			<p>In <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) problems, we are always trying to find more data to improve our models, but as you can imagine, there comes a time when we reach a point of diminishing returns. It is <a id="_idIndexMarker360"/>very hard to know when you have reached that point, but you can get indications by looking at the learning curves. Fortunately, DataRobot makes that task easy by automatically building these learning curves. When DataRobot s<a id="_idIndexMarker361"/>tarts building models, it first tries a broad range of algorithms on small samples of data. Promising models are then built with bigger sample sizes, and so on.</p>
			<p>In this process, we discover how much performance improvement happens as more data is added. To look at the learning curves, you can click on the <strong class="bold">Learning Curves</strong> menu item at the top of the screen, as seen in the following screenshot:</p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/Figure_7.21_B17159.jpg" alt="Figure 7.21 – Model learning curves&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.21 – Model learning curves</p>
			<p>You can see the different <a id="_idIndexMarker362"/>model types on the right-hand side of the page. Here, you can click on the models you want to inspect and compare. After selecting the models, you click on the <strong class="bold">+ Compute Learning Curves</strong> button. This brings up a dialog box showing the selected models and corresponding sample sizes, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/Figure_7.22_B17159.jpg" alt="Figure 7.22 – Models selected for comparison&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.22 – Models selected for comparison</p>
			<p>If the selections in <em class="italic">Figure 7.22</em> look <a id="_idIndexMarker363"/>correct, you can click the <strong class="bold">Compute</strong> button. You will now see the learning curves for the selected models, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/Figure_7.23_B17159.jpg" alt="Figure 7.23 – Comparison of learning curves&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.23 – Comparison of learning curves</p>
			<p>You can now see the improvement in performance as the sample size increases. We can see that the GAM <a id="_idIndexMarker364"/>learns very rapidly, but as the sample size increases, the XGBoost model takes over. We can see that both models will benefit from additional data. We can also see that if we only had half of the data we currently have, then the GAM would have been the clear winner.</p>
			<p>We can now look at <a id="_idIndexMarker365"/>another trade-off for models—namely, the <a id="_idIndexMarker366"/>trade-off between speed and accuracy. If you click on the <strong class="bold">Speed vs Accuracy</strong> menu item at the top of the page, you will see a chart, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/Figure_7.24_B17159.jpg" alt="Figure 7.24 – Speed versus accuracy trade-off&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.24 – Speed versus accuracy trade-off</p>
			<p>You will notice the <a id="_idIndexMarker367"/>DataRobot has built an AVG Blender model that seems to be the top model, but not by much. Blended <a id="_idIndexMarker368"/>models can sometimes produce substantial lift over individual models, so it is worthwhile exploring this option. We can select this model and click on the <strong class="bold">Blueprint</strong> tab within the <strong class="bold">Describe</strong> menu item.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor115"/>Summary</h1>
			<p>In this chapter, we covered how to build and compare models by leveraging DataRobot's capabilities. As you saw, DataRobot makes it very easy to build many models quickly and helps us compare them. As you experienced, we tried many things and built dozens of models. This is DataRobot's key capability, and its importance to a data science team cannot be overstated. If you were to build these models on your own in Python, it would have taken a lot more time and effort. Instead, we used that time and thinking to experiment with different ideas and put more energy toward understanding the problem. We also learned about blueprints that encode best practices. These blueprints can be useful learning tools for new and experienced data scientists alike. We also learned how DataRobot can build ensemble or blended models for us.</p>
			<p>It might be tempting to jump ahead and start deploying one of these models, but it is important to not directly jump to that without doing some analysis. In the next chapter, we will dig deeper into the models to understand them and see if we can gain more insights from them.</p>
		</div>
	</body></html>