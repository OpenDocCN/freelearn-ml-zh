- en: Deep Learning Using Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will go on a hands-on exploration of the exciting and cutting-edge
    world of deep learning! We will use third-party deep learning libraries in conjunction
    with Apache Spark''s `MLlib` to perform accurate **optical character recognition**
    (**OCR**) and automatically recognize and classify images via the following types
    of artificial neural networks and machine learning algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer perceptrons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we studied in [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*, an **artificial neural network** (**ANN**)
    is a connected group of artificial neurons that is aggregated into three types
    of linked neural layers—the input layer, zero or more hidden layers, and the output
    layer. A **monolayer** ANN consists of just *one* layer of links between the input
    nodes and output nodes, while **multilayer** ANNs are characterized by the segmentation
    of artificial neurons across multiple linked layers.
  prefs: []
  type: TYPE_NORMAL
- en: An ANN where signals are propagated in one direction only—that is, the signals
    are received by the input layer and forwarded to the next layer for processing—are
    called **feedforward** networks. ANNs where a signal may be propagated back to
    artificial neurons or neural layers that have already processed that signal are
    called **feedback** networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Backwards propagation** is a supervised learning process by which multilayer
    ANNs can learn—that is, derive an optimal set of weight coefficients. First, all
    weights are initially set as random and the output from the network is calculated.
    If the predicted output does not match the desired output, the total error at
    the output nodes is propagated back through the entire network in an effort to
    readjust all weights in the network so that the error is reduced in the output
    layer. In other words, backwards propagation seeks to minimize the difference
    between the actual output and the desired output via an iterative weight adjustment
    process.'
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **single-layer perceptron** (**SLP**) is a basic type of ANN that consists
    of just two layers of nodes—an input layer containing input nodes and an output
    layer containing output nodes. A **multilayer perceptron** (**MLP**), however,
    introduces one or more hidden layers between the input and output layers, giving
    them the ability to learn nonlinear functions, as illustrated in *Figure 7.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ca10ecb-d7ec-4d8c-9814-6a71948f7819.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Multilayer perceptron neural architecture'
  prefs: []
  type: TYPE_NORMAL
- en: MLP classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark's machine learning library, `MLlib`, provides an out-of-the-box
    **multilayer perceptron classifier** (**MLPC**) that can be applied to classification
    problems where we are required to predict from *k* possible classes.
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In `MLlib`''s MLPC, the nodes in the input layer represent the input data.
    Let''s denote this input data as a vector, *X*, with *m* features, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e5fdf3c-d101-4692-8748-4c03a602c366.png)'
  prefs: []
  type: TYPE_IMG
- en: Hidden layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The input data is then passed to the hidden layers. For the sake of simplicity,
    let''s say that we have only one hidden layer, *h¹*, and that within this one
    hidden layer, we have *n* neurons, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3905ed0-3ff0-4448-886c-950002a5b445.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The net input, *z*, into the activation function for each of these hidden neurons
    is then the input data set vector, *X*, multiplied by a weight set vector, *W*^n
    (corresponding to the weight sets assigned to the *n* neurons in the hidden layer),
    where each weight set vector, *W*^n, contains *m* weights (corresponding to the
    *m* features in our input data set vector *X*), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9f36ad6-5367-4754-aa2d-e09a06d96ae3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In linear algebra, the product of multiplying one vector by another is called
    the **dot product**, and it outputs a scalar (that is, a number) represented by *z*,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a12eea96-9a2e-49c2-a62b-31d49b582111.png)'
  prefs: []
  type: TYPE_IMG
- en: The **bias**, as illustrated in [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*, and shown in *Figure 3.5*, is a *stand-alone*
    constant analogous to the intercept term in a regression model, and may be added
    to non-output layers in feedforward neural networks. It is called standalone because
    bias nodes are not connected to preceding layers. By introducing a constant, we
    allow for the output of an activation function to be shifted left or right by
    that constant, thereby increasing the flexibility of an ANN to learn patterns
    more effectively by providing the ability to shift decision boundaries based on
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that in a single hidden layer containing *n* hidden neurons, *n* dot product
    calculations will be computed, as illustrated in *Figure 7.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5ea4e211-6519-4181-951a-55aab034be62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Hidden layer net input and output'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `MLlib`''s MLPC, the hidden neurons use the **sigmoid** activation function,
    as shown in the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b83c9aed-746c-4a14-8e42-c1e1c0b0d976.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we saw in [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*, the sigmoid (or logistic) function is bounded
    between 0 and 1, and is smoothly defined for all real input values. By using the
    sigmoid activation function, the nodes in the hidden layers actually correspond
    to a logistic regression model. If we study the sigmoid curve, as shown in *Figure
    7.3*, we can state that if the net input, *z*, is a large positive number, then
    the output of the sigmoid function, and hence the activation function for our
    hidden neurons, will be close to 1\. Conversely, if the net input, z, is a negative
    number with a large absolute value, then the output of the sigmoid function will
    be close to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1c9a1b6-f7d5-4fa2-b3f3-e56f9ec4e9d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: 'In all cases, each hidden neuron will take the net input, *z*, which is the
    dot product of the input data, *X*, and the weight set, *W^n*, plus a bias, and
    apply that to the sigmoid function, finally outputting a number between 0 and
    1\. After all hidden neurons have computed the result of their activation function,
    we will then have *n* hidden outputs from our hidden layer *h¹*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75bb784d-640c-4f8c-84b8-a7d203e8ff67.png)'
  prefs: []
  type: TYPE_IMG
- en: Output layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The hidden layer outputs are then used as inputs to calculate the final outputs
    in the output layer. In our case, we only have a single hidden layer, *h¹*, with
    outputs ![](img/bcbb9457-7f00-4b1b-baa1-f65957755e17.png). These then become *n*
    inputs into the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The net input into the activation function for the output layer neurons is
    then these *n* inputs computed by the hidden layer and multiplied by a weight
    set vector, *W^h*, where each weight set vector, *W^h*, contains *n* weights (corresponding
    to the *n* hidden layer inputs). For the sake of simplicity, let''s assume that
    we only have one output neuron in our output layer. The weight set vector for
    this neuron is therefore the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea76a3ab-cd53-432f-8ff0-7529a5ffecf0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, since we are multiplying vectors together, we use the dot product calculation, which
    will compute the following scalar representing our net input, *z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/30abd217-cdd9-48e5-9c85-9918a5d4770c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In `MLlib`''s MLPC, the output neurons use the softmax function as the activation
    function, which extends logistic regression by predicting *k* classes instead
    of a standard binary classification. This function takes the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87f99f9f-51bc-4666-82a6-b603514c8cb6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, the number of nodes in the output layer corresponds to the number
    of possible classes that you wish to predict from. For example, if your use case
    has five possible classes, then you would train an MLP with five nodes in the
    output layer. The final output from the activation function is therefore the prediction
    that the output neuron in question makes, as illustrated in *Figure 7.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d50b70a5-c32c-4e22-b6e1-884a0b11d36b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Output layer net input and output'
  prefs: []
  type: TYPE_NORMAL
- en: Note that *Figure 7.4* illustrates the initial **forward propagation** of the
    MLP, whereby input data is propagated to the hidden layer and the output from
    the hidden layer is propagated to the output layer where the final output is computed.
    `MLlib`'s MLPC thereafter uses **backwards propagation** to train the neural network
    and learn the model where the difference between the actual output and the desired
    output is minimized via an iterative weight adjustment process. MLPC achieves
    this by seeking to minimize a **loss function**. A loss function calculates a
    measure of the price paid for inaccurate predictions regarding classification
    problems. The specific loss function that MLPC employs is the **logistic loss
    function**, where predictions made with a high value of confidence are penalized
    less. To learn more about loss functions, please visit [https://en.wikipedia.org/wiki/Loss_functions_for_classification](https://en.wikipedia.org/wiki/Loss_functions_for_classification).
  prefs: []
  type: TYPE_NORMAL
- en: Case study 1 – OCR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A great real-world use case to demonstrate the power of MLPs is that of OCR.
    In OCR, the challenge is to recognize human writing, classifying each handwritten
    symbol as a letter. In the case of the English alphabet, there are 26 letters.
    Therefore, when applied to the English language, OCR is actually a classification
    problem that has *k *= 26 possible classes!
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset that we will be using has been derived from the **University of
    California''s** (**UCI**) Machine Learning Repository, which is found at [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php).
    The specific letter recognition dataset that we will use, available from both
    the GitHub repository accompanying this book and from [https://archive.ics.uci.edu/ml/datasets/letter+recognition](https://archive.ics.uci.edu/ml/datasets/letter+recognition),
    was created by David J. Slate at Odesta Corporation; 1890 Maple Ave; Suite 115;
    Evanston, IL 60201, and was used in the paper *Letter Recognition Using Holland-style
    Adaptive Classifiers* by P. W. Frey and D. J. Slate (from Machine Learning Vol
    6 #2 March 91).'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.5* provides an example illustration of this dataset rendered visually.
    We will train an MLP classifier to recognize and classify each of the symbols,
    such as those shown in *Figure 7.5*, as a letter of the English alphabet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c356df0-4c5d-4c5e-a656-bd2a295c0b1e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Letter recognition dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Input data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we delve further into the schema of our specific dataset, let's first
    understand how a MLP will actually help us with this problem. Firstly, as we saw
    in [Chapter 5](eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml), *Unsupervised Learning
    Using Apache Spark*, when studying image segmentation, images can be broken down
    into a matrix of either pixel-intensity values (for grayscale images) or pixel
    RGB values (for images with color). A single vector containing (*m* x *n*) numerical
    elements can then be generated, corresponding to the pixel height (*m*) and width
    (*n*) of the image.
  prefs: []
  type: TYPE_NORMAL
- en: Training architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, imagine that we want to train an MLP using our entire letter recognition
    dataset, as illustrated in *Figure 7.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18a703a1-9640-48e9-934e-228916131d45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Multilayer perceptron for letter recognition'
  prefs: []
  type: TYPE_NORMAL
- en: In our MLP, we have *p* (= *m* x *n*) neurons in our input layer that represent
    the *p* pixel-intensity values from our image. A single hidden layer has *n* neurons,
    and the output layer has 26 neurons that represent the 26 possible classes or
    letters in the English alphabet. When training this neural network, since we do
    not know initially what weights should be assigned to each layer, we initialize
    the weights randomly and perform a first iteration of forward propagation. We
    then iteratively employ backwards propagation to train the neural network, resulting
    in a set of weights that have been optimized so that the predictions/classifications
    made by the output layer are as accurate as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting patterns in the hidden layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The job of the neurons in the hidden layer is to learn to detect patterns within
    the input data. In our case, the neurons in the hidden layer(s) will detect the
    presence of certain substructures that constitute a wider symbol. This is illustrated
    in *Figure 7.7*, where we assume that the first three neurons in the hidden layer
    learn to recognize forward slash, back slash and horizontal line type patterns
    respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94bcd340-a59d-41b5-a205-4ef7066b99eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Neurons in the hidden layer detect patterns and substructures'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying in the output layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our neural network, the first neuron in the output layer is trained to decide
    whether a given symbol is the uppercase English letter *A*. Assuming that the
    first three neurons in the hidden layer fire, we would expect the first neuron
    in the output layer to fire and the remaining 25 neurons not to fire. Our MLP
    would then classify this symbol as the letter *A*!
  prefs: []
  type: TYPE_NORMAL
- en: Note that our training architecture employed only a single hidden layer, which
    would only be able to learn very simple patterns. By adding more hidden layers,
    an ANN can learn more complicated patterns at the cost of computational complexity,
    resources, and training runtime. However, with the advent of distributed storage
    and processing technologies, as discussed in [Chapter 1](337b904f-87f1-4741-bd75-d7fd983185f6.xhtml), *The
    Big Data Ecosystem*, where huge volumes of data may be stored in memory and a
    large number of calculations may be processed on that data in a distributed manner,
    today we are able to train extremely complex neural networks with architecture
    that may contain large numbers of hidden layers and hidden neurons. Such complex
    neural networks are currently being applied to a broad range of applications,
    including facial recognition, speech recognition, real-time threat detection,
    image-based searching, fraud detection, and advances in healthcare.
  prefs: []
  type: TYPE_NORMAL
- en: MLPs in Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s return to our dataset and train an MLP in Apache Spark to recognize
    and classify letters from the English alphabet. If you open `ocr-data/letter-recognition.data`
    in any text editor, from either the GitHub repository accompanying this book or
    from UCI''s machine learning repository, you will find 20,000 rows of data, described
    by the following schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column name** | **Data type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `lettr` | `String` | English letter (one of 26 values, from A to Z) |'
  prefs: []
  type: TYPE_TB
- en: '| `x-box` | `Integer` | Horizontal position of box |'
  prefs: []
  type: TYPE_TB
- en: '| `y-box` | `Integer` | Vertical position of box |'
  prefs: []
  type: TYPE_TB
- en: '| `width` | `Integer` | Width of box |'
  prefs: []
  type: TYPE_TB
- en: '| `high` | `Integer` | Height of box |'
  prefs: []
  type: TYPE_TB
- en: '| `onpix` | `Integer` | Total number of on pixels |'
  prefs: []
  type: TYPE_TB
- en: '| `x-bar` | `Integer` | Mean *x* of on pixels in the box |'
  prefs: []
  type: TYPE_TB
- en: '| `y-bar` | `Integer` | Mean *y* of on pixels in the box |'
  prefs: []
  type: TYPE_TB
- en: '| `x2bar` | `Integer` | Mean *x* variance |'
  prefs: []
  type: TYPE_TB
- en: '| `y2bar` | `Integer` | Mean *y* variance |'
  prefs: []
  type: TYPE_TB
- en: '| `xybar` | `Integer` | Mean *x y* correlation |'
  prefs: []
  type: TYPE_TB
- en: '| `x2ybr` | `Integer` | Mean of *x* * *x* * *y* |'
  prefs: []
  type: TYPE_TB
- en: '| `xy2br` | `Integer` | Mean of *x* * *y* * *y* |'
  prefs: []
  type: TYPE_TB
- en: '| `x-ege` | `Integer` | Mean edge count left to right |'
  prefs: []
  type: TYPE_TB
- en: '| `xegvy` | `Integer` | Correlation of `x-ege` with *y* |'
  prefs: []
  type: TYPE_TB
- en: '| `y-ege` | `Integer` | Mean edge count, bottom to top |'
  prefs: []
  type: TYPE_TB
- en: '| `yegvx` | `Integer` | Correlation of `y-ege` with *x* |'
  prefs: []
  type: TYPE_TB
- en: This dataset describes 16 numerical attributes representing statistical features
    of the pixel distribution based on scanned character images, such as those illustrated
    in *Figure 7.5*. These attributes have been standardized and scaled linearly to
    a range of integer values from 0 to 15\. For each row, a label column called `lettr`
    denotes the letter of the English alphabet that it represents, where no feature
    vector maps to more than one class—that is, each feature vector maps to only one
    letter in the English alphabet.
  prefs: []
  type: TYPE_NORMAL
- en: You will have noticed that we are not using the pixel data from the *raw* images
    themselves, but rather statistical features derived from the distribution of the
    pixels. However, using what we have learned from [Chapter 5](eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml), *Unsupervised
    Learning Using Apache Spark*, when we looked at specifically converting images
    into numerical feature vectors, the exact same steps that we will look at in a
    moment may be followed to train an MLP classifier using the raw images themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now use this dataset to train an MLP classifier to recognize symbols
    and classify them as letters from the English alphabet:'
  prefs: []
  type: TYPE_NORMAL
- en: The following subsections describe each of the pertinent cells in the corresponding
    Jupyter notebook for this use case, called `chp07-01-multilayer-perceptron-classifier.ipynb`.
    This notebook can be found in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the prerequisite PySpark libraries as normal, including `MLlib`''s
    `MultilayerPerceptronClassifier` classifier and `MulticlassClassificationEvaluator`
    evaluator respectively, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After instantiating a Spark context, we are now ready to ingest our dataset
    into a Spark dataframe. Note that in our case, we have preprocessed the dataset
    into CSV format, where we have converted the `lettr` column from a `string` datatype
    to a `numeric` datatype representing one of the 26 characters in the English alphabet.
    This preprocessed CSV file is available in the GitHub repository accompanying
    this book. Once we have ingested this CSV file into a Spark dataframe, we then
    generate feature vectors using `VectorAssembler`, comprising the 16 feature columns,
    as usual. The resulting Spark dataframe, called `vectorised_df`, therefore contains
    two columns—the numeric `label` column, representing one of the 26 characters
    in the English alphabet, and the `features` column, containing our feature vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we split our dataset into training and test datasets with a ratio of
    75% to 25% respectively, using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to train our MLP classifier. First, we must define the size
    of the respective layers of our neural network. We do this by defining a Python
    list with the following elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first element defines the size of the input layer. In our case, we have
    16 features in our dataset, and so we set this element to `16`.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The next elements define the sizes of the intermediate hidden layers. We shall
    define two hidden layers of sizes `8` and `4` respectively.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final element defines the size of the output layer. In our case, we have
    26 possible classes representing the 26 letters of the English alphabet, and so
    we set this element to `26`:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have defined the architecture of our neural network, we can train
    an MLP using `MLlib`''s `MultilayerPerceptronClassifier` classifier and fit it
    to the training dataset, as shown in the following code. Remember that `MLlib`''s
    `MultilayerPerceptronClassifier` classifier uses the sigmoid activation function
    for hidden neurons and the softmax activation function for output neurons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now apply our trained MLP classifier to the test dataset in order to
    predict which of the 26 letters of the English alphabet the 16 numerical pixel-related
    features represent, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we compute the accuracy of our trained MLP classifier on the test dataset
    using the following code. In our case, it performs very poorly, with an accuracy
    rate of only 34%. We can conclude from this that an MLP with two hidden layers
    of sizes 8 and 4 respectively performs very poorly in recognizing and classifying
    letters from scanned images in the case of our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'How can we increase the accuracy of our neural classifier? To answer this question,
    we must revisit our definition of what the hidden layers do. Remember that the
    job of the neurons in the hidden layers is to learn to detect patterns within
    the input data. Therefore, defining more hidden neurons in our neural architecture
    should increase the ability of our neural network to detect more patterns at greater
    resolutions. To test this hypothesis, we shall increase the number of neurons
    in our two hidden layers to 16 and 12 respectively, as shown in the following
    code. Then, we retrain our MLP classifier and reapply it to the test dataset.
    This results in a far better performing model, with an accuracy rate of 72%:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have seen how MLPs, which receive a single input vector that is then transformed
    through one or more intermediate hidden layers, can be used to recognize and classify
    small images such as letters and digits in OCR. However, one limitation of MLPs
    is their ability to scale with larger images, taking into account not just individual
    pixel intensity or RGB values, but the height, width, and depth of the image itself.
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) assume that the input data is
    of a grid-like topology, and so they are predominantly used to recognize and classify
    objects in images since an image can be represented as a grid of pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end neural architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The end-to-end architecture of a convolutional neural network is illustrated
    in *Figure* *7.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87bcf4e7-4543-4898-a2cf-0aec91618213.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Convolutional neural network architecture'
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will describe each of the layers and transformations
    that constitute a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Given that CNNs are predominantly used to classify images, the input data into
    CNNs consists of image matrices of the dimensions *h* (height in pixels), *w*
    (width in pixels) and *d* (depth). In the case of RGB images, the depth would
    be three corresponding, to the three color channels, **red**, **green**, and **blue**
    (**RGB**). This is illustrated in *Figure 7.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c5b71f9a-c7c7-443c-b12f-67c8e6a5a0a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Image matrix dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: Convolution layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next transformations that occur in the CNN are processed in *convolution*
    layers. The purpose of the convolution layers is to detect features in the image,
    which is achieved through the use of **filters** (also called kernels). Imagine
    taking a magnifying glass and looking at an image, starting at the top-left of
    the image. As we move the magnifying glass from left to right and top to bottom,
    we detect the different features in each of the locations that our magnifying
    glass moves over. At a high level, this is the job of the convolution layers,
    where the magnifying glass represents the filter or kernel and the size of each
    step that the filter takes, normally pixel by pixel, is referred to as the **stride**
    size. The output of a convolution layer is called a **feature map**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an example to understand the processes undertaken within a convolution
    layer better. Imagine that we have an image that is 3 pixels (height) by 3 pixels
    (width). For the sake of simplicity, we will overlook the third dimension representing
    the image depth in our example, but note that real-world convolutions are computed
    in three dimensions for RGB images. Next, imagine that our filter is a matrix
    of 2 pixels (height) by 2 pixels (width) and that our stride size is 1 pixel.
  prefs: []
  type: TYPE_NORMAL
- en: 'These respective matrices are illustrated in *Figure 7.10*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8e86093-7fa5-40c9-b751-c1f09993f6b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Image matrix and filter matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we place our filter matrix at the top-left corner of our image matrix
    and perform a **matrix multiplication** of the two at that location. We then move
    the filter matrix to the right by our stride size—1 pixel—and perform a matrix
    multiplication at that location. We continue this process until the filter matrix
    has traversed the entire image matrix. The resulting feature map matrix is illustrated
    in *Figure 7.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d4c664c9-11eb-4df2-81e2-aae3ff9594f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Feature map'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the feature map is smaller in its dimensions than the input matrix
    of the convolution layer. To ensure that the dimensions of the output match the
    dimensions of the input, a layer of zero-value pixels is added in a process called
    **padding**. Also note that the filter must have the same number of channels as
    the input image—so in the case of RGB images, the filter must also have three
    channels.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do convolutions help a neural network to learn? To answer this question,
    we must revisit the concept of filters. Filters themselves are matrices of *weights*
    that are trained to detect specific patterns within an image, and different filters
    can be used to detect different patterns, such as edges and other features. For
    example, if we use a filter that has been pretrained to detect simple edges, as
    we pass this filter over an image, the convolution computation will output a high-valued
    real number (as a result of matrix multiplication and summation) if an edge is
    present and a low-valued real number if an edge is not present.
  prefs: []
  type: TYPE_NORMAL
- en: As the filter finishes traversing the entire image, the output is a feature
    map matrix that represents the convolutions of this filter over all parts of the
    image. By using different filters during different convolutions per layer, we
    get different feature maps, which form the output of the convolution layer.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified linear units
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As with other neural networks, an activation function defines the output of
    a node and is used so that our neural network can learn nonlinear functions. Note
    that our input data (the RGB pixels making up the images) is itself nonlinear,
    so we need a nonlinear activation function. **Rectified linear units** (**ReLUs**)
    are commonly used in CNNs, and are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81dc3852-c5cb-41a3-88cb-a7d51b7bb209.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, the ReLU function returns 0 for every negative value in its
    input data, and returns the value itself for every positive value in its input
    data. This is shown in *Figure 7.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c50602e9-747c-4a87-968f-2c7aee8fe3eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ReLU function can be plotted as shown in *Figure 7.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48fcb0e4-f861-4487-bfbe-44e896a65b00.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: ReLU function graph'
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next transformations that occur in the CNN are processed in *pooling* layers.
    The goal of the pooling layers is to reduce the dimensionality of the feature
    maps output by the convolution layers (but not their depth) while preserving the
    spatial variance of the original input data. In other words, the size of the data
    is reduced in order to reduce computational complexity, memory requirements, and
    training times while overcoming over fitting so that patterns detected during
    training can be detected in test data even if their appearance varies. There are
    various pooling algorithms available, given a specified window size, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Max pooling**: Takes the maximum value in each window'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average pooling**: Takes the average value across each window'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sum pooling**: Takes the sum of values in each window'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 7.14* shows the effect of performing max pooling on a 4 x 4 feature
    map using a 2 x 2 window size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df33f184-e0c5-471f-8dca-97add6d76529.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: Max pooling on a 4 x 4 feature map using a 2 x 2 window'
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the 3-D input data has been transformed through a series of convolution
    and pooling layers, a fully connected layer flattens the feature maps output by
    the last convolution and pooling layer into a long 1-D feature vector, which is
    then used as the input data for a regular ANN in which all of the neurons in each
    layer are connected to all of the neurons in the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The output neurons in this ANN then use an activation function such as the softmax
    function (as seen in the MLP classifier) to classify the outputs and thereby recognize
    and classify the objects contained in the input image data!
  prefs: []
  type: TYPE_NORMAL
- en: Case study 2 – image recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this case study, we will use a pretrained CNN to recognize and classify objects
    in images that it has never encountered before.
  prefs: []
  type: TYPE_NORMAL
- en: InceptionV3 via TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The pretrained CNN that we will use is called **Inception-v3**. This deep CNN
    has been trained on the **ImageNet** image database (an academic benchmark for
    computer vision algorithms containing a vast library of labelled images covering
    a wide range of nouns) and can classify entire images into 1,000 classes found
    in everyday life, such as "pizza", "plastic bag", "red wine", "desk", "orange",
    and "basketball", to name just a few.
  prefs: []
  type: TYPE_NORMAL
- en: The Inception-v3 deep CNN was developed and trained by **TensorFlow** ^(TM),
    an open source machine learning framework and software library for high-performance
    numerical computation originally developed within Google's AI organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about TensorFlow, Inception-v3, and ImageNet, please visit the
    following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ImageNet:** [http://www.image-net.org/](http://www.image-net.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow:** [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inception-v3:** [https://www.tensorflow.org/tutorials/images/image_recognition](https://www.tensorflow.org/tutorials/images/image_recognition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning pipelines for Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this case study, we will access the Inception-v3 TensorFlow deep CNN via
    a third-party Spark package called `sparkdl`. This Spark package has been developed
    by Databricks, a company formed by the original creators of Apache Spark, and
    provides high-level APIs for scalable deep learning within Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about Databricks and `sparkdl`, please visit the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Databricks**: [https://databricks.com/](https://databricks.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**sparkdl**: [https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The images that we will use to test the pretrained Inception-v3 deep CNN have
    been selected from the **Open Images v4** dataset, a collection of over 9 million
    images that have been released under the Creative Common Attribution license,
    and which may be found at [https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the GitHub repository accompanying this book, you can find 30 images of
    birds (`image-recognition-data/birds`) and 30 images of planes (`image-recognition-data/planes`)
    respectively. *Figure 7.15* shows a couple of examples of the images that you
    might find in these test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b455f360-046b-4640-bc0f-cc260e7eba3c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: Example images from the Open Images v4 dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Our goal in this case study will be to apply the pretrained Inception-v3 deep
    CNN to these test images and quantify the accuracy of a trained classifier model
    when it comes to distinguishing between images of birds and planes within a single
    test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark image recognition application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that for the purposes of this case study, we will not be using Jupyter
    notebooks for development but rather standard Python code files with the `.py`
    file extension. This case study provides a first glimpse into how a production-grade
    pipeline should be developed and executed; rather than instantiating a `SparkContext`
    explicitly within our code, we will instead submit our code and all its dependencies
    to `spark-submit` (including any third-party Spark packages, such as `sparkdl`)
    via the Linux command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now take a look at how we can use the Inception-v3 deep CNN via PySpark
    to classify test images. In our Python-based image-recognition application, we
    perform the following steps (numbered to correspond to the numbered comments in
    our Python code file):'
  prefs: []
  type: TYPE_NORMAL
- en: The following Python code file, called `chp07-02-convolutional-neural-network-transfer-learning.py`,
    can be found in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, using the following code, we import the required Python dependencies,
    including the relevant modules from the third-party `sparkdl` package and the
    `LogisticRegression` classifier native to `MLlib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike our Jupyter notebook case studies, there is no need to instantiate a
    `SparkContext`, as this will be done for us when we execute our PySpark application
    via `spark-submit` on the command line. In this case study, we will create a `SparkSession`,
    as shown in the following code, that acts as an entry point into the Spark execution
    environment (even if it is already running) that subsumes SQLContext. We can therefore
    use `SparkSession` to undertake the same SQL-like operations over data that we
    have seen previously while still using the Spark Dataset/DataFrame API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As of Version 2.3, Spark provides native support for image data sources via
    its `MLlib` API. In this step, we invoke the `readImages` method on `MLlib`''s
    `ImageSchema` class to load our bird and plane test images from the local filesystem
    into Spark dataframes called `birds_df` and `planes_df` respectively. We then
    label all images of birds with the `0` literal and label all images of planes
    with the `1` literal, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have loaded our test images into separate Spark dataframes differentiated
    by their label, we consolidate them into single training and test dataframes accordingly.
    We achieve this by using the `unionAll` method via the Spark dataframe API, which
    simply appends one dataframe onto another, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As with previous case studies, we need to generate feature vectors from our
    input data. However, rather than training a deep CNN from scratch—which could
    take many days, even with distributed technologies—we will take advantage of the
    pretrained Inception-v3 deep CNN. To do this, we will use a process called **transfer
    learning**. In this process, knowledge gained while solving one machine learning
    problem is applied to a different but related problem. To use transfer learning
    in our case study, we employ the `DeepImageFeaturizer` module of the third-party
    `sparkdl` Spark package. The `DeepImageFeaturizer` not only transforms our images
    into numeric features, it also performs fast transfer learning by peeling off
    the last layer of a pretrained neural network and then uses the output from all
    the previous layers as features for a standard classification algorithm. In our
    case, the `DeepImageFeaturizer` will be peeling off the last layer of the pretrained
    Inception-v3 deep CNN, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the features from all previous layers of the pretrained Inception-v3
    deep CNN extracted via transfer learning, we input them into a classification
    algorithm. In our case, we will use `MLlib`''s `LogisticRegression` classifier,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute the transfer learning and logistic regression model training, we
    build a standard `pipeline` and `fit` that pipeline to our training dataframe,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a trained classification model, using the features derived
    by the Inception-v3 deep CNN, we apply our trained logistic regression model to
    our test dataframe to make predictions as normal, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we quantify the accuracy of our model on the test dataframe using
    `MLlib`''s `MulticlassClassificationEvaluator`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Spark submit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are now ready to run our image recognition application! Since it is a Spark
    application, we can execute it via `spark-submit` on the Linux command line. To
    do this, navigate to the directory where we installed Apache Spark (see [Chapter
    2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting Up a Local Development
    Environment*). Then, we can execute the `spark-submit` program by passing it the
    following command-line arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`--master`: The Spark Master URL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--packages`: The third-party libraries and dependencies required for the Spark
    application to work. In our case, our image-recognition application is dependent
    on the availability of the `sparkdl` third-party library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--py-files`: Since our image-recognition application is a PySpark application,
    we pass the filesystem paths to any Python code files that our application is
    dependent on. In our case, since our image-recognition application is self-contained
    within a single code file, there are no further dependencies to pass to `spark-submit`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final argument is the path to the Python code file containing our Spark
    driver program, namely `chp07-02-convolutional-neural-network-transfer-learning.py`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The final commands to execute, therefore, look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Image-recognition results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assuming that the image-recognition application ran successfully, you should
    see the following results output to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Origin** | **Prediction** |'
  prefs: []
  type: TYPE_TB
- en: '| `planes/plane-005.jpg` | `1.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `planes/plane-008.jpg` | `1.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `planes/plane-009.jpg` | `1.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `planes/plane-016.jpg` | `1.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `planes/plane-017.jpg` | `0.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `planes/plane-018.jpg` | `1.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `birds/bird-005.jpg` | `0.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `birds/bird-008.jpg` | `0.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `birds/bird-009.jpg` | `0.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `birds/bird-016.jpg` | `0.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `birds/bird-017.jpg` | `0.0` |'
  prefs: []
  type: TYPE_TB
- en: '| `birds/bird-018.jpg` | `0.0` |'
  prefs: []
  type: TYPE_TB
- en: 'The `Origin` column refers to the absolute filesystem path of the image, and
    the value in the `Prediction` column is `1.0` if our model predicts that the object
    in the image is a plane and `0.0` if our model predicts that the object in the
    image is a bird. Our model has an astonishingly high accuracy of 92% when run
    on the test dataset. The only mistake that our model made was on `plane-017.jpg`,
    illustrated in *Figure* *7.16*, which was incorrectly classified as a bird when
    it was in fact a plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/677136f7-68b4-4b1c-8202-81f483f1fda0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: Incorrect classification of plane-017.jpg'
  prefs: []
  type: TYPE_NORMAL
- en: If we look at `plane-017.jpg` in *Figure 7.16*, we can quickly understand why
    the model made this mistake. Though it is a man-made plane, it has been physically
    modeled to look like a bird for increased efficiency and aerodynamic purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we used a pretrained CNN to featurize images. We then passed
    the resulting features to a standard logistic regression algorithm to predict
    whether a given image is a bird or a plane.
  prefs: []
  type: TYPE_NORMAL
- en: Case study 3 – image prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In case study 2 (image recognition), we still explicitly labelled our test images
    before training our final logistic regression classifier. In this case study,
    we will simply send random images to the pretrained Inception-v3 deep CNN without
    labeling them and let the CNN itself classify the objects contained within the
    images. Again, we will take advantage of the third-party `sparkdl` Spark package
    to access the pretrained Inception-v3 CNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The assortment of random images that we will use have again been downloaded
    from the **Open Images v4 dataset**, and may be found in the GitHub repository
    accompanying this book under `image-recognition-data/assorted`. *Figure 7.17*
    shows a couple of typical images that you may find in this test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b763134a-ece7-47ff-b8d4-888928a49752.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.17: Assortment of random images'
  prefs: []
  type: TYPE_NORMAL
- en: PySpark image-prediction application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our Python-based image-prediction application, we go through the following
    steps (numbered to correspond to the numbered comments in our Python code file):'
  prefs: []
  type: TYPE_NORMAL
- en: The following Python code file, called `chp07-03-convolutional-neural-network-image-predictor.py`,
    can be found in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we import the required Python dependencies as usual, including the `DeepImagePredictor`
    class from the third-party `sparkdl` Spark package, as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a `SparkSession` that acts as an entry point into the Spark
    execution environment, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We then load our assortment of random images into a Spark dataframe using the
    `readImages` method of the `ImageSchema` class that we first encountered in the
    previous case study, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we pass our Spark dataframe containing our assortment of random images
    to `sparkdl`''s `DeepImagePredictor`, which will apply a specified pretrained
    neural network to the images in an effort to classify the objects found within
    them. In our case, we will be using the pretrained Inception-v3 deep CNN. We also
    tell the `DeepImagePredictor` to return the top 10 (`topK=10`) predicted classifications
    for each image in descending order of confidence, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'To run this PySpark image-prediction application, we again invoke `spark-submit`
    via the command line, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Image-prediction results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Assuming that the image-prediction application ran successfully, you should
    see the following results output to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Origin** | **First Predicted Label** |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/snowman.jpg` | `Teddy` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/bicycle.jpg` | `Mountain Bike` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/house.jpg` | `Library` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/bus.jpg` | `Trolley Bus` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/banana.jpg` | `Banana` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/pizza.jpg` | `Pizza` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/toilet.jpg` | `Toilet Seat` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/knife.jpg` | `Cleaver` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/apple.jpg` | `Granny Smith (Apple)` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/pen.jpg` | `Ballpoint` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/lion.jpg` | `Lion` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/saxophone.jpg` | `Saxophone` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/zebra.jpg` | `Zebra` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/fork.jpg` | `Spatula` |'
  prefs: []
  type: TYPE_TB
- en: '| `assorted/car.jpg` | `Convertible` |'
  prefs: []
  type: TYPE_TB
- en: As you can see, the pretrained Inception-v3 deep CNN has an astonishing ability
    to recognize and classify the objects found in images. Though the images provided
    in this case study were relatively simple, the Inception-v3 CNN has a top-five
    error rate— how often the model fails to predict the correct answer as one of
    its top five guesses—of just 3.46% on the ImageNet image database. Remember that
    the Inception-v3 CNN attempts to classify entire images into 1,000 classes, hence
    a top-5 error rate of just 3.46% is truly impressive, and clearly demonstrates
    the learning ability and power of not only convolution neural networks but ANNs
    in general when detecting and learning patterns!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went on a hands-on exploration through the exciting and
    cutting-edge world of deep learning. We developed applications to recognize and
    classify objects in images with astonishingly high rates of accuracy, and demonstrated
    the truly impressive learning ability of ANNs to detect and learn patterns in
    input data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will extend our deployment of machine learning models
    beyond batch processing in order to learn from data and make predictions in real
    time!
  prefs: []
  type: TYPE_NORMAL
