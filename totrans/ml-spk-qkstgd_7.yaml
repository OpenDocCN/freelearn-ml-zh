- en: Deep Learning Using Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行深度学习
- en: 'In this chapter, we will go on a hands-on exploration of the exciting and cutting-edge
    world of deep learning! We will use third-party deep learning libraries in conjunction
    with Apache Spark''s `MLlib` to perform accurate **optical character recognition**
    (**OCR**) and automatically recognize and classify images via the following types
    of artificial neural networks and machine learning algorithms:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将亲身体验深度学习这个激动人心且前沿的世界！我们将结合使用第三方深度学习库和Apache Spark的`MLlib`来执行精确的光学字符识别（**OCR**），并通过以下类型的人工神经网络和机器学习算法自动识别和分类图像：
- en: Multilayer perceptrons
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层感知器
- en: Convolutional neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Transfer learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Artificial neural networks
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: As we studied in [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*, an **artificial neural network** (**ANN**)
    is a connected group of artificial neurons that is aggregated into three types
    of linked neural layers—the input layer, zero or more hidden layers, and the output
    layer. A **monolayer** ANN consists of just *one* layer of links between the input
    nodes and output nodes, while **multilayer** ANNs are characterized by the segmentation
    of artificial neurons across multiple linked layers.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第3章](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml)《人工智能与机器学习》中所研究的，**人工神经网络**（**ANN**）是一组连接的人工神经元，它们被聚合为三种类型的链接神经网络层——输入层、零个或多个隐藏层和输出层。**单层**ANN仅由输入节点和输出节点之间的*一个*层链接组成，而**多层**ANN的特点是人工神经元分布在多个链接层中。
- en: An ANN where signals are propagated in one direction only—that is, the signals
    are received by the input layer and forwarded to the next layer for processing—are
    called **feedforward** networks. ANNs where a signal may be propagated back to
    artificial neurons or neural layers that have already processed that signal are
    called **feedback** networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 信号仅沿一个方向传播的人工神经网络——也就是说，信号被输入层接收并转发到下一层进行处理——被称为**前馈**网络。信号可能被传播回已经处理过该信号的输入神经元或神经层的ANN被称为**反馈**网络。
- en: '**Backwards propagation** is a supervised learning process by which multilayer
    ANNs can learn—that is, derive an optimal set of weight coefficients. First, all
    weights are initially set as random and the output from the network is calculated.
    If the predicted output does not match the desired output, the total error at
    the output nodes is propagated back through the entire network in an effort to
    readjust all weights in the network so that the error is reduced in the output
    layer. In other words, backwards propagation seeks to minimize the difference
    between the actual output and the desired output via an iterative weight adjustment
    process.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播**是一种监督学习过程，通过这个过程多层ANN可以学习——也就是说，推导出一个最优的权重系数集。首先，所有权重都最初设置为随机，然后计算网络的输出。如果预测输出与期望输出不匹配，则输出节点的总误差会通过整个网络反向传播，以尝试重新调整网络中的所有权重，以便在输出层减少误差。换句话说，反向传播通过迭代权重调整过程来最小化实际输出和期望输出之间的差异。'
- en: Multilayer perceptrons
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器
- en: 'A **single-layer perceptron** (**SLP**) is a basic type of ANN that consists
    of just two layers of nodes—an input layer containing input nodes and an output
    layer containing output nodes. A **multilayer perceptron** (**MLP**), however,
    introduces one or more hidden layers between the input and output layers, giving
    them the ability to learn nonlinear functions, as illustrated in *Figure 7.1*:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**单层感知器**（**SLP**）是一种基本的人工神经网络（ANN），它仅由两层节点组成——一个包含输入节点的输入层和一个包含输出节点的输出层。然而，**多层感知器**（**MLP**）在输入层和输出层之间引入了一个或多个隐藏层，这使得它们能够学习非线性函数，如图7.1所示：'
- en: '![](img/2ca10ecb-d7ec-4d8c-9814-6a71948f7819.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2ca10ecb-d7ec-4d8c-9814-6a71948f7819.png)'
- en: 'Figure 7.1: Multilayer perceptron neural architecture'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：多层感知器神经网络架构
- en: MLP classifier
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MLP分类器
- en: Apache Spark's machine learning library, `MLlib`, provides an out-of-the-box
    **multilayer perceptron classifier** (**MLPC**) that can be applied to classification
    problems where we are required to predict from *k* possible classes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark的机器学习库`MLlib`提供了一个现成的**多层感知器分类器**（**MLPC**），可以应用于需要从*k*个可能的类别中进行预测的分类问题。
- en: Input layer
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入层
- en: 'In `MLlib`''s MLPC, the nodes in the input layer represent the input data.
    Let''s denote this input data as a vector, *X*, with *m* features, as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在`MLlib`的MLPC中，输入层的节点代表输入数据。让我们将这个输入数据表示为一个具有*m*个特征的向量，*X*，如下所示：
- en: '![](img/5e5fdf3c-d101-4692-8748-4c03a602c366.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5e5fdf3c-d101-4692-8748-4c03a602c366.png)'
- en: Hidden layers
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏层
- en: 'The input data is then passed to the hidden layers. For the sake of simplicity,
    let''s say that we have only one hidden layer, *h¹*, and that within this one
    hidden layer, we have *n* neurons, as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将输入数据传递到隐藏层。为了简化，让我们假设我们只有一个隐藏层*h¹*，并且在这个隐藏层中，我们有*n*个神经元，如下所示：
- en: '![](img/f3905ed0-3ff0-4448-886c-950002a5b445.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f3905ed0-3ff0-4448-886c-950002a5b445.png)'
- en: 'The net input, *z*, into the activation function for each of these hidden neurons
    is then the input data set vector, *X*, multiplied by a weight set vector, *W*^n
    (corresponding to the weight sets assigned to the *n* neurons in the hidden layer),
    where each weight set vector, *W*^n, contains *m* weights (corresponding to the
    *m* features in our input data set vector *X*), as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些隐藏神经元中的每一个，激活函数的净输入*z*是输入数据集向量*X*乘以一个权重集向量*W*^n（对应于分配给隐藏层中*n*个神经元的权重集），其中每个权重集向量*W*^n包含*m*个权重（对应于我们输入数据集向量*X*中的*m*个特征），如下所示：
- en: '![](img/b9f36ad6-5367-4754-aa2d-e09a06d96ae3.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b9f36ad6-5367-4754-aa2d-e09a06d96ae3.png)'
- en: 'In linear algebra, the product of multiplying one vector by another is called
    the **dot product**, and it outputs a scalar (that is, a number) represented by *z*,
    as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数中，将一个向量乘以另一个向量的乘积称为**点积**，它输出一个由*z*表示的标量（即一个数字），如下所示：
- en: '![](img/a12eea96-9a2e-49c2-a62b-31d49b582111.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a12eea96-9a2e-49c2-a62b-31d49b582111.png)'
- en: The **bias**, as illustrated in [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*, and shown in *Figure 3.5*, is a *stand-alone*
    constant analogous to the intercept term in a regression model, and may be added
    to non-output layers in feedforward neural networks. It is called standalone because
    bias nodes are not connected to preceding layers. By introducing a constant, we
    allow for the output of an activation function to be shifted left or right by
    that constant, thereby increasing the flexibility of an ANN to learn patterns
    more effectively by providing the ability to shift decision boundaries based on
    the data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**偏差**，如[第3章](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml)《人工智能与机器学习》中所示，并在*图3.5*中展示，是一个独立的常数，类似于回归模型中的截距项，并且可以添加到前馈神经网络的非输出层。它被称为独立，因为偏差节点没有连接到前面的层。通过引入一个常数，我们可以允许激活函数的输出向左或向右移动该常数，从而增加人工神经网络学习模式的有效性，提供基于数据移动决策边界的功能。'
- en: 'Note that in a single hidden layer containing *n* hidden neurons, *n* dot product
    calculations will be computed, as illustrated in *Figure 7.2*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在一个包含*n*个隐藏神经元的单隐藏层中，将计算*n*个点积运算，如图*图7.2*所示：
- en: '![](img/5ea4e211-6519-4181-951a-55aab034be62.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5ea4e211-6519-4181-951a-55aab034be62.png)'
- en: 'Figure 7.2: Hidden layer net input and output'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：隐藏层净输入和输出
- en: 'In `MLlib`''s MLPC, the hidden neurons use the **sigmoid** activation function,
    as shown in the following formula:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在`MLlib`的MLPC中，隐藏神经元使用**sigmoid**激活函数，如下公式所示：
- en: '![](img/b83c9aed-746c-4a14-8e42-c1e1c0b0d976.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b83c9aed-746c-4a14-8e42-c1e1c0b0d976.png)'
- en: 'As we saw in [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*, the sigmoid (or logistic) function is bounded
    between 0 and 1, and is smoothly defined for all real input values. By using the
    sigmoid activation function, the nodes in the hidden layers actually correspond
    to a logistic regression model. If we study the sigmoid curve, as shown in *Figure
    7.3*, we can state that if the net input, *z*, is a large positive number, then
    the output of the sigmoid function, and hence the activation function for our
    hidden neurons, will be close to 1\. Conversely, if the net input, z, is a negative
    number with a large absolute value, then the output of the sigmoid function will
    be close to 0:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第3章](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml)《人工智能与机器学习》中看到的，Sigmoid（或逻辑）函数在0和1之间有界，并且对所有实数输入值都有平滑的定义。通过使用sigmoid激活函数，隐藏层中的节点实际上对应于一个逻辑回归模型。如果我们研究sigmoid曲线，如图*图7.3*所示，我们可以声明，如果净输入*z*是一个大的正数，那么sigmoid函数的输出，以及我们隐藏神经元的激活函数，将接近1。相反，如果净输入z是一个具有大绝对值的负数，那么sigmoid函数的输出将接近0：
- en: '![](img/d1c9a1b6-f7d5-4fa2-b3f3-e56f9ec4e9d8.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1c9a1b6-f7d5-4fa2-b3f3-e56f9ec4e9d8.png)'
- en: 'Figure 7.3: Sigmoid function'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：sigmoid函数
- en: 'In all cases, each hidden neuron will take the net input, *z*, which is the
    dot product of the input data, *X*, and the weight set, *W^n*, plus a bias, and
    apply that to the sigmoid function, finally outputting a number between 0 and
    1\. After all hidden neurons have computed the result of their activation function,
    we will then have *n* hidden outputs from our hidden layer *h¹*, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有情况下，每个隐藏神经元都会接收净输入，即*z*，它是输入数据*X*和权重集*W^n*的点积，再加上一个偏置，并将其应用于sigmoid函数，最终输出一个介于0和1之间的数字。在所有隐藏神经元计算了它们的激活函数的结果之后，我们就会从隐藏层*h¹*中得到*n*个隐藏输出，如下所示：
- en: '![](img/75bb784d-640c-4f8c-84b8-a7d203e8ff67.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/75bb784d-640c-4f8c-84b8-a7d203e8ff67.png)'
- en: Output layer
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出层
- en: The hidden layer outputs are then used as inputs to calculate the final outputs
    in the output layer. In our case, we only have a single hidden layer, *h¹*, with
    outputs ![](img/bcbb9457-7f00-4b1b-baa1-f65957755e17.png). These then become *n*
    inputs into the output layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的输出随后被用作输入，以计算输出层的最终输出。在我们的例子中，我们只有一个隐藏层，即*h¹*，其输出![图片](img/bcbb9457-7f00-4b1b-baa1-f65957755e17.png)。这些输出随后成为输出层的*n*个输入。
- en: 'The net input into the activation function for the output layer neurons is
    then these *n* inputs computed by the hidden layer and multiplied by a weight
    set vector, *W^h*, where each weight set vector, *W^h*, contains *n* weights (corresponding
    to the *n* hidden layer inputs). For the sake of simplicity, let''s assume that
    we only have one output neuron in our output layer. The weight set vector for
    this neuron is therefore the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层神经元的激活函数的净输入是隐藏层计算出的这些*n*个输入，乘以一个权重集向量，即*W^h*，其中每个权重集向量*W^h*包含*n*个权重（对应于*n*个隐藏层输入）。为了简化，让我们假设我们输出层只有一个输出神经元。因此，这个神经元的权重集向量如下所示：
- en: '![](img/ea76a3ab-cd53-432f-8ff0-7529a5ffecf0.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ea76a3ab-cd53-432f-8ff0-7529a5ffecf0.png)'
- en: 'Again, since we are multiplying vectors together, we use the dot product calculation, which
    will compute the following scalar representing our net input, *z*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，由于我们是在乘以向量，我们使用点积计算，这将计算以下表示我们的净输入*z*的标量：
- en: '![](img/30abd217-cdd9-48e5-9c85-9918a5d4770c.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/30abd217-cdd9-48e5-9c85-9918a5d4770c.png)'
- en: 'In `MLlib`''s MLPC, the output neurons use the softmax function as the activation
    function, which extends logistic regression by predicting *k* classes instead
    of a standard binary classification. This function takes the following form:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在`MLlib`的MLPC中，输出神经元使用softmax函数作为激活函数，它通过预测*k*个类别而不是标准的二分类来扩展逻辑回归。此函数具有以下形式：
- en: '![](img/87f99f9f-51bc-4666-82a6-b603514c8cb6.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/87f99f9f-51bc-4666-82a6-b603514c8cb6.png)'
- en: 'Therefore, the number of nodes in the output layer corresponds to the number
    of possible classes that you wish to predict from. For example, if your use case
    has five possible classes, then you would train an MLP with five nodes in the
    output layer. The final output from the activation function is therefore the prediction
    that the output neuron in question makes, as illustrated in *Figure 7.4*:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，输出层的节点数对应于你希望预测的可能类别数。例如，如果你的用例有五个可能的类别，那么你将训练一个输出层有五个节点的MLP。因此，激活函数的最终输出是相关输出神经元所做的预测，如图*7.4*所示：
- en: '![](img/d50b70a5-c32c-4e22-b6e1-884a0b11d36b.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d50b70a5-c32c-4e22-b6e1-884a0b11d36b.png)'
- en: 'Figure 7.4: Output layer net input and output'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：输出层净输入和输出
- en: Note that *Figure 7.4* illustrates the initial **forward propagation** of the
    MLP, whereby input data is propagated to the hidden layer and the output from
    the hidden layer is propagated to the output layer where the final output is computed.
    `MLlib`'s MLPC thereafter uses **backwards propagation** to train the neural network
    and learn the model where the difference between the actual output and the desired
    output is minimized via an iterative weight adjustment process. MLPC achieves
    this by seeking to minimize a **loss function**. A loss function calculates a
    measure of the price paid for inaccurate predictions regarding classification
    problems. The specific loss function that MLPC employs is the **logistic loss
    function**, where predictions made with a high value of confidence are penalized
    less. To learn more about loss functions, please visit [https://en.wikipedia.org/wiki/Loss_functions_for_classification](https://en.wikipedia.org/wiki/Loss_functions_for_classification).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*图7.4* 说明了MLP的初始**正向传播**，其中输入数据传播到隐藏层，隐藏层的输出传播到输出层，在那里计算最终输出。MLlib的MLPC随后使用**反向传播**来训练神经网络并学习模型，通过迭代权重调整过程最小化实际输出和期望输出之间的差异。MLPC通过寻求最小化**损失函数**来实现这一点。损失函数计算了关于分类问题的不准确预测所付出的代价的度量。MLPC使用的特定损失函数是**逻辑损失函数**，其中具有高置信度预测的惩罚较小。要了解更多关于损失函数的信息，请访问[https://en.wikipedia.org/wiki/Loss_functions_for_classification](https://en.wikipedia.org/wiki/Loss_functions_for_classification)。
- en: Case study 1 – OCR
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究1 – OCR
- en: A great real-world use case to demonstrate the power of MLPs is that of OCR.
    In OCR, the challenge is to recognize human writing, classifying each handwritten
    symbol as a letter. In the case of the English alphabet, there are 26 letters.
    Therefore, when applied to the English language, OCR is actually a classification
    problem that has *k *= 26 possible classes!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 证明MLP强大功能的一个很好的实际案例是OCR。在OCR中，挑战是识别人类书写，将每个手写符号分类为字母。在英文字母的情况下，有26个字母。因此，当应用于英语时，OCR实际上是一个具有*k*=26个可能类别的分类问题！
- en: 'The dataset that we will be using has been derived from the **University of
    California''s** (**UCI**) Machine Learning Repository, which is found at [https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php).
    The specific letter recognition dataset that we will use, available from both
    the GitHub repository accompanying this book and from [https://archive.ics.uci.edu/ml/datasets/letter+recognition](https://archive.ics.uci.edu/ml/datasets/letter+recognition),
    was created by David J. Slate at Odesta Corporation; 1890 Maple Ave; Suite 115;
    Evanston, IL 60201, and was used in the paper *Letter Recognition Using Holland-style
    Adaptive Classifiers* by P. W. Frey and D. J. Slate (from Machine Learning Vol
    6 #2 March 91).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要使用的这个数据集是从加州大学（**加州大学**）的机器学习仓库中提取的，该仓库位于[https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)。我们将使用的特定字母识别数据集，可以从本书附带的GitHub仓库以及[https://archive.ics.uci.edu/ml/datasets/letter+recognition](https://archive.ics.uci.edu/ml/datasets/letter+recognition)获取，由Odesta公司的David
    J. Slate创建；地址为1890 Maple Ave；Suite 115；Evanston, IL 60201，并在P. W. Frey和D. J. Slate合著的论文《使用荷兰风格自适应分类器的字母识别》（来自《机器学习》第6卷第2期，1991年3月）中使用。
- en: '*Figure 7.5* provides an example illustration of this dataset rendered visually.
    We will train an MLP classifier to recognize and classify each of the symbols,
    such as those shown in *Figure 7.5*, as a letter of the English alphabet:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.5* 展示了该数据集的视觉示例。我们将训练一个MLP分类器来识别和分类每个符号，例如*图7.5*中所示，将其识别为英文字母：'
- en: '![](img/3c356df0-4c5d-4c5e-a656-bd2a295c0b1e.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3c356df0-4c5d-4c5e-a656-bd2a295c0b1e.png)'
- en: 'Figure 7.5: Letter recognition dataset'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：字母识别数据集
- en: Input data
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入数据
- en: Before we delve further into the schema of our specific dataset, let's first
    understand how a MLP will actually help us with this problem. Firstly, as we saw
    in [Chapter 5](eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml), *Unsupervised Learning
    Using Apache Spark*, when studying image segmentation, images can be broken down
    into a matrix of either pixel-intensity values (for grayscale images) or pixel
    RGB values (for images with color). A single vector containing (*m* x *n*) numerical
    elements can then be generated, corresponding to the pixel height (*m*) and width
    (*n*) of the image.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步探讨我们特定数据集的架构之前，让我们首先了解MLP如何真正帮助我们解决这个问题。首先，正如我们在[第5章](eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml)中看到的，“使用Apache
    Spark进行无监督学习”，在研究图像分割时，图像可以被分解为像素强度值（用于灰度图像）或像素RGB值（用于彩色图像）的矩阵。然后可以生成一个包含(*m*
    x *n*)数值元素的单一向量，对应于图像的像素高度(*m*)和宽度(*n*)。
- en: Training architecture
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练架构
- en: 'Now, imagine that we want to train an MLP using our entire letter recognition
    dataset, as illustrated in *Figure 7.6*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，我们想要使用我们的整个字母识别数据集来训练一个MLP，如图*图7.6*所示：
- en: '![](img/18a703a1-9640-48e9-934e-228916131d45.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/18a703a1-9640-48e9-934e-228916131d45.png)'
- en: 'Figure 7.6: Multilayer perceptron for letter recognition'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：用于字母识别的多层感知器
- en: In our MLP, we have *p* (= *m* x *n*) neurons in our input layer that represent
    the *p* pixel-intensity values from our image. A single hidden layer has *n* neurons,
    and the output layer has 26 neurons that represent the 26 possible classes or
    letters in the English alphabet. When training this neural network, since we do
    not know initially what weights should be assigned to each layer, we initialize
    the weights randomly and perform a first iteration of forward propagation. We
    then iteratively employ backwards propagation to train the neural network, resulting
    in a set of weights that have been optimized so that the predictions/classifications
    made by the output layer are as accurate as possible.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的MLP中，输入层有*p*（= *m* x *n*）个神经元，它们代表图像中的*p*个像素强度值。一个单一的隐藏层有*n*个神经元，输出层有26个神经元，代表英语字母表中的26个可能的类别或字母。在训练这个神经网络时，由于我们最初不知道应该分配给每一层的权重，我们随机初始化权重并执行第一次前向传播。然后我们迭代地使用反向传播来训练神经网络，从而得到一组经过优化的权重，使得输出层做出的预测/分类尽可能准确。
- en: Detecting patterns in the hidden layer
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 隐藏层中的模式检测
- en: 'The job of the neurons in the hidden layer is to learn to detect patterns within
    the input data. In our case, the neurons in the hidden layer(s) will detect the
    presence of certain substructures that constitute a wider symbol. This is illustrated
    in *Figure 7.7*, where we assume that the first three neurons in the hidden layer
    learn to recognize forward slash, back slash and horizontal line type patterns
    respectively:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中神经元的任务是学习在输入数据中检测模式。在我们的例子中，隐藏层中的神经元将检测构成更广泛符号的某些子结构。这如图*图7.7*所示，我们假设隐藏层中的前三个神经元分别学会了识别正斜杠、反斜杠和水平线类型的模式：
- en: '![](img/94bcd340-a59d-41b5-a205-4ef7066b99eb.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/94bcd340-a59d-41b5-a205-4ef7066b99eb.png)'
- en: 'Figure 7.7: Neurons in the hidden layer detect patterns and substructures'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：隐藏层中的神经元检测模式和子结构
- en: Classifying in the output layer
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出层的分类
- en: In our neural network, the first neuron in the output layer is trained to decide
    whether a given symbol is the uppercase English letter *A*. Assuming that the
    first three neurons in the hidden layer fire, we would expect the first neuron
    in the output layer to fire and the remaining 25 neurons not to fire. Our MLP
    would then classify this symbol as the letter *A*!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的神经网络中，输出层中的第一个神经元被训练来决定给定的符号是否是大写英文字母**A**。假设隐藏层中的前三个神经元被激活，我们预计输出层中的第一个神经元将被激活，而剩下的25个神经元不会被激活。这样，我们的MLP就会将这个符号分类为字母**A**！
- en: Note that our training architecture employed only a single hidden layer, which
    would only be able to learn very simple patterns. By adding more hidden layers,
    an ANN can learn more complicated patterns at the cost of computational complexity,
    resources, and training runtime. However, with the advent of distributed storage
    and processing technologies, as discussed in [Chapter 1](337b904f-87f1-4741-bd75-d7fd983185f6.xhtml), *The
    Big Data Ecosystem*, where huge volumes of data may be stored in memory and a
    large number of calculations may be processed on that data in a distributed manner,
    today we are able to train extremely complex neural networks with architecture
    that may contain large numbers of hidden layers and hidden neurons. Such complex
    neural networks are currently being applied to a broad range of applications,
    including facial recognition, speech recognition, real-time threat detection,
    image-based searching, fraud detection, and advances in healthcare.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的训练架构仅使用单个隐藏层，这只能学习非常简单的模式。通过添加更多隐藏层，人工神经网络可以学习更复杂的模式，但这会以计算复杂性、资源和训练运行时间的增加为代价。然而，随着分布式存储和处理技术的出现，正如在第
    1 章“大数据生态系统”中讨论的，其中大量数据可以存储在内存中，并且可以在分布式方式下对数据进行大量计算，今天我们能够训练具有大量隐藏层和隐藏神经元的极其复杂的神经网络。这种复杂的神经网络目前正在应用于广泛的领域，包括人脸识别、语音识别、实时威胁检测、基于图像的搜索、欺诈检测和医疗保健的进步。
- en: MLPs in Apache Spark
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 中的 MLP
- en: 'Let''s return to our dataset and train an MLP in Apache Spark to recognize
    and classify letters from the English alphabet. If you open `ocr-data/letter-recognition.data`
    in any text editor, from either the GitHub repository accompanying this book or
    from UCI''s machine learning repository, you will find 20,000 rows of data, described
    by the following schema:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的数据集，并在 Apache Spark 中训练一个 MLP 来识别和分类英文字母。如果你在任何文本编辑器中打开 `ocr-data/letter-recognition.data`，无论是来自本书配套的
    GitHub 仓库还是来自 UCI 的机器学习仓库，你将找到 20,000 行数据，这些数据由以下模式描述：
- en: '| **Column name** | **Data type** | **Description** |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| **列名** | **数据类型** | **描述** |'
- en: '| `lettr` | `String` | English letter (one of 26 values, from A to Z) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `lettr` | `字符串` | 英文字母（26 个值之一，从 A 到 Z） |'
- en: '| `x-box` | `Integer` | Horizontal position of box |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `x-box` | `整数` | 矩形水平位置 |'
- en: '| `y-box` | `Integer` | Vertical position of box |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `y-box` | `整数` | 矩形垂直位置 |'
- en: '| `width` | `Integer` | Width of box |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `width` | `整数` | 矩形宽度 |'
- en: '| `high` | `Integer` | Height of box |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `high` | `整数` | 矩形高度 |'
- en: '| `onpix` | `Integer` | Total number of on pixels |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `onpix` | `整数` | 颜色像素总数 |'
- en: '| `x-bar` | `Integer` | Mean *x* of on pixels in the box |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `x-bar` | `整数` | 矩形内颜色像素的 x 均值 |'
- en: '| `y-bar` | `Integer` | Mean *y* of on pixels in the box |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `y-bar` | `整数` | 矩形内颜色像素的 y 均值 |'
- en: '| `x2bar` | `Integer` | Mean *x* variance |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `x2bar` | `整数` | x 的方差平均值 |'
- en: '| `y2bar` | `Integer` | Mean *y* variance |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| `y2bar` | `整数` | y 的方差平均值 |'
- en: '| `xybar` | `Integer` | Mean *x y* correlation |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `xybar` | `整数` | x 和 y 的相关平均值 |'
- en: '| `x2ybr` | `Integer` | Mean of *x* * *x* * *y* |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `x2ybr` | `整数` | x 和 y 的平均值 |'
- en: '| `xy2br` | `Integer` | Mean of *x* * *y* * *y* |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `xy2br` | `整数` | x 和 y 的平方平均值 |'
- en: '| `x-ege` | `Integer` | Mean edge count left to right |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| `x-ege` | `整数` | 从左到右的平均边缘计数 |'
- en: '| `xegvy` | `Integer` | Correlation of `x-ege` with *y* |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| `xegvy` | `整数` | `x-ege` 与 y 的相关性 |'
- en: '| `y-ege` | `Integer` | Mean edge count, bottom to top |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| `y-ege` | `整数` | 从下到上的平均边缘计数 |'
- en: '| `yegvx` | `Integer` | Correlation of `y-ege` with *x* |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| `yegvx` | `整数` | `y-ege` 与 x 的相关性 |'
- en: This dataset describes 16 numerical attributes representing statistical features
    of the pixel distribution based on scanned character images, such as those illustrated
    in *Figure 7.5*. These attributes have been standardized and scaled linearly to
    a range of integer values from 0 to 15\. For each row, a label column called `lettr`
    denotes the letter of the English alphabet that it represents, where no feature
    vector maps to more than one class—that is, each feature vector maps to only one
    letter in the English alphabet.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集描述了 16 个数值属性，这些属性基于扫描字符图像的像素分布的统计特征，如 *图 7.5* 中所示。这些属性已经标准化并线性缩放到 0 到 15
    的整数范围内。对于每一行，一个名为 `lettr` 的标签列表示它所代表的英文字母，其中没有特征向量映射到多个类别——也就是说，每个特征向量只映射到英文字母表中的一个字母。
- en: You will have noticed that we are not using the pixel data from the *raw* images
    themselves, but rather statistical features derived from the distribution of the
    pixels. However, using what we have learned from [Chapter 5](eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml), *Unsupervised
    Learning Using Apache Spark*, when we looked at specifically converting images
    into numerical feature vectors, the exact same steps that we will look at in a
    moment may be followed to train an MLP classifier using the raw images themselves.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们没有使用原始图像本身的像素数据，而是使用从像素分布中得到的统计特征。然而，当我们从[第5章](eed6ec04-8a1b-459e-b225-b2761b26a460.xhtml)，“使用Apache
    Spark进行无监督学习”中学习到的知识时，我们特别关注将图像转换为数值特征向量时，我们将在下一刻看到的相同步骤可以遵循来使用原始图像本身训练MLP分类器。
- en: 'Let''s now use this dataset to train an MLP classifier to recognize symbols
    and classify them as letters from the English alphabet:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在使用这个数据集来训练一个MLP分类器以识别符号并将它们分类为英文字母表中的字母：
- en: The following subsections describe each of the pertinent cells in the corresponding
    Jupyter notebook for this use case, called `chp07-01-multilayer-perceptron-classifier.ipynb`.
    This notebook can be found in the GitHub repository accompanying this book.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节描述了对应于本用例的Jupyter笔记本中每个相关的单元格，该笔记本称为`chp07-01-multilayer-perceptron-classifier.ipynb`。这个笔记本可以在本书附带的GitHub仓库中找到。
- en: 'First, we import the prerequisite PySpark libraries as normal, including `MLlib`''s
    `MultilayerPerceptronClassifier` classifier and `MulticlassClassificationEvaluator`
    evaluator respectively, as shown in the following code:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们像往常一样导入必要的PySpark库，包括`MLlib`的`MultilayerPerceptronClassifier`分类器和`MulticlassClassificationEvaluator`评估器，如下面的代码所示：
- en: '[PRE0]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After instantiating a Spark context, we are now ready to ingest our dataset
    into a Spark dataframe. Note that in our case, we have preprocessed the dataset
    into CSV format, where we have converted the `lettr` column from a `string` datatype
    to a `numeric` datatype representing one of the 26 characters in the English alphabet.
    This preprocessed CSV file is available in the GitHub repository accompanying
    this book. Once we have ingested this CSV file into a Spark dataframe, we then
    generate feature vectors using `VectorAssembler`, comprising the 16 feature columns,
    as usual. The resulting Spark dataframe, called `vectorised_df`, therefore contains
    two columns—the numeric `label` column, representing one of the 26 characters
    in the English alphabet, and the `features` column, containing our feature vectors:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实例化Spark上下文之后，我们现在准备好将我们的数据集导入Spark数据框中。请注意，在我们的情况下，我们已经将数据集预处理为CSV格式，其中我们将`lettr`列从`string`数据类型转换为表示英文字母表中26个字符之一的`numeric`数据类型。这个预处理好的CSV文件可以在本书附带的GitHub仓库中找到。一旦我们将这个CSV文件导入Spark数据框中，我们就使用`VectorAssembler`生成特征向量，包含16个特征列，就像通常那样。因此，生成的Spark数据框，称为`vectorised_df`，包含两个列——表示英文字母表中26个字符之一的数值`label`列，以及包含我们的特征向量的`features`列：
- en: '[PRE1]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we split our dataset into training and test datasets with a ratio of
    75% to 25% respectively, using the following code:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码以75%到25%的比例将我们的数据集分为训练集和测试集：
- en: '[PRE2]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We are now ready to train our MLP classifier. First, we must define the size
    of the respective layers of our neural network. We do this by defining a Python
    list with the following elements:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好训练我们的MLP分类器。首先，我们必须定义我们神经网络各自层的尺寸。我们通过定义一个包含以下元素的Python列表来完成此操作：
- en: The first element defines the size of the input layer. In our case, we have
    16 features in our dataset, and so we set this element to `16`.
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个元素定义了输入层的尺寸。在我们的情况下，我们的数据集中有16个特征，因此我们将此元素设置为`16`。
- en: The next elements define the sizes of the intermediate hidden layers. We shall
    define two hidden layers of sizes `8` and `4` respectively.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个元素定义了中间隐藏层的尺寸。我们将定义两个隐藏层，分别大小为`8`和`4`。
- en: 'The final element defines the size of the output layer. In our case, we have
    26 possible classes representing the 26 letters of the English alphabet, and so
    we set this element to `26`:'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个元素定义了输出层的尺寸。在我们的情况下，我们有26个可能的类别，代表英文字母表中的26个字母，因此我们将此元素设置为`26`：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that we have defined the architecture of our neural network, we can train
    an MLP using `MLlib`''s `MultilayerPerceptronClassifier` classifier and fit it
    to the training dataset, as shown in the following code. Remember that `MLlib`''s
    `MultilayerPerceptronClassifier` classifier uses the sigmoid activation function
    for hidden neurons and the softmax activation function for output neurons:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的神经网络架构，我们可以使用`MLlib`的`MultilayerPerceptronClassifier`分类器来训练一个MLP，并将其拟合到训练数据集上，如下面的代码所示。记住，`MLlib`的`MultilayerPerceptronClassifier`分类器为隐藏神经元使用sigmoid激活函数，为输出神经元使用softmax激活函数：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now apply our trained MLP classifier to the test dataset in order to
    predict which of the 26 letters of the English alphabet the 16 numerical pixel-related
    features represent, as follows:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以将我们的训练好的MLP分类器应用到测试数据集上，以预测16个与像素相关的数值特征代表英语字母表中的26个字母中的哪一个，如下所示：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, we compute the accuracy of our trained MLP classifier on the test dataset
    using the following code. In our case, it performs very poorly, with an accuracy
    rate of only 34%. We can conclude from this that an MLP with two hidden layers
    of sizes 8 and 4 respectively performs very poorly in recognizing and classifying
    letters from scanned images in the case of our dataset:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下代码计算我们训练好的MLP分类器在测试数据集上的准确性。在我们的案例中，它的表现非常糟糕，准确率仅为34%。我们可以得出结论，在我们的数据集中，具有大小分别为8和4的两个隐藏层的MLP在识别和分类扫描图像中的字母方面表现非常糟糕：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'How can we increase the accuracy of our neural classifier? To answer this question,
    we must revisit our definition of what the hidden layers do. Remember that the
    job of the neurons in the hidden layers is to learn to detect patterns within
    the input data. Therefore, defining more hidden neurons in our neural architecture
    should increase the ability of our neural network to detect more patterns at greater
    resolutions. To test this hypothesis, we shall increase the number of neurons
    in our two hidden layers to 16 and 12 respectively, as shown in the following
    code. Then, we retrain our MLP classifier and reapply it to the test dataset.
    This results in a far better performing model, with an accuracy rate of 72%:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何提高我们神经网络分类器的准确性？为了回答这个问题，我们必须重新审视我们对隐藏层功能的定义。记住，隐藏层中神经元的任务是学习在输入数据中检测模式。因此，在我们的神经网络架构中定义更多的隐藏神经元应该会增加我们的神经网络检测更多模式并具有更高分辨率的能力。为了测试这个假设，我们将我们两个隐藏层中的神经元数量分别增加到16和12，如下面的代码所示。然后，我们重新训练我们的MLP分类器并将其重新应用到测试数据集上。这导致了一个性能远更好的模型，准确率达到72%：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Convolutional neural networks
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: We have seen how MLPs, which receive a single input vector that is then transformed
    through one or more intermediate hidden layers, can be used to recognize and classify
    small images such as letters and digits in OCR. However, one limitation of MLPs
    is their ability to scale with larger images, taking into account not just individual
    pixel intensity or RGB values, but the height, width, and depth of the image itself.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，MLPs可以通过一个或多个中间隐藏层对单个输入向量进行转换来识别和分类小图像，如OCR中的字母和数字。然而，MLP的一个局限性是它们在处理较大图像时的扩展能力，这不仅要考虑单个像素强度或RGB值，还要考虑图像本身的高度、宽度和深度。
- en: '**Convolutional neural networks** (**CNNs**) assume that the input data is
    of a grid-like topology, and so they are predominantly used to recognize and classify
    objects in images since an image can be represented as a grid of pixels.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）假设输入数据具有网格状拓扑结构，因此它们主要用于识别和分类图像中的对象，因为图像可以被表示为像素的网格。'
- en: End-to-end neural architecture
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 端到端神经网络架构
- en: 'The end-to-end architecture of a convolutional neural network is illustrated
    in *Figure* *7.8*:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络的端到端架构如图*7.8*所示：
- en: '![](img/87bcf4e7-4543-4898-a2cf-0aec91618213.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/87bcf4e7-4543-4898-a2cf-0aec91618213.png)'
- en: 'Figure 7.8: Convolutional neural network architecture'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：卷积神经网络架构
- en: In the following subsections, we will describe each of the layers and transformations
    that constitute a CNN.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我们将描述构成卷积神经网络（CNN）的每一层和变换。
- en: Input layer
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入层
- en: 'Given that CNNs are predominantly used to classify images, the input data into
    CNNs consists of image matrices of the dimensions *h* (height in pixels), *w*
    (width in pixels) and *d* (depth). In the case of RGB images, the depth would
    be three corresponding, to the three color channels, **red**, **green**, and **blue**
    (**RGB**). This is illustrated in *Figure 7.9*:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 由于卷积神经网络（CNN）主要用于图像分类，因此输入CNN的数据是具有维度*h*（像素高度）、*w*（像素宽度）和*d*（深度）的图像矩阵。在RGB图像的情况下，深度将是三个相应的颜色通道，即**红色**、**绿色**和**蓝色**（**RGB**）。这如图7.9所示：
- en: '![](img/c5b71f9a-c7c7-443c-b12f-67c8e6a5a0a7.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c5b71f9a-c7c7-443c-b12f-67c8e6a5a0a7.png)'
- en: 'Figure 7.9: Image matrix dimensions'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：图像矩阵维度
- en: Convolution layers
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: The next transformations that occur in the CNN are processed in *convolution*
    layers. The purpose of the convolution layers is to detect features in the image,
    which is achieved through the use of **filters** (also called kernels). Imagine
    taking a magnifying glass and looking at an image, starting at the top-left of
    the image. As we move the magnifying glass from left to right and top to bottom,
    we detect the different features in each of the locations that our magnifying
    glass moves over. At a high level, this is the job of the convolution layers,
    where the magnifying glass represents the filter or kernel and the size of each
    step that the filter takes, normally pixel by pixel, is referred to as the **stride**
    size. The output of a convolution layer is called a **feature map**.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中接下来发生的转换是在*卷积*层中处理的。卷积层的目的是在图像中检测特征，这是通过使用**滤波器**（也称为核）实现的。想象一下拿一个放大镜观察一个图像，从图像的左上角开始。当我们从左到右和从上到下移动放大镜时，我们检测到放大镜移动过的每个位置的不同特征。在高层上，这就是卷积层的工作，其中放大镜代表滤波器或核，滤波器每次移动的步长大小，通常是像素级，被称为**步长**大小。卷积层的输出称为**特征图**。
- en: Let's look at an example to understand the processes undertaken within a convolution
    layer better. Imagine that we have an image that is 3 pixels (height) by 3 pixels
    (width). For the sake of simplicity, we will overlook the third dimension representing
    the image depth in our example, but note that real-world convolutions are computed
    in three dimensions for RGB images. Next, imagine that our filter is a matrix
    of 2 pixels (height) by 2 pixels (width) and that our stride size is 1 pixel.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来更好地理解卷积层中进行的处理过程。想象一下，我们有一个3像素（高度）乘以3像素（宽度）的图像。为了简化，我们将在例子中忽略代表图像深度的第三维度，但请注意，现实世界的卷积对于RGB图像是在三个维度上计算的。接下来，想象一下我们的滤波器是一个2像素（高度）乘以2像素（宽度）的矩阵，并且我们的步长大小是1像素。
- en: 'These respective matrices are illustrated in *Figure 7.10*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些相应的矩阵在*图7.10*中展示：
- en: '![](img/a8e86093-7fa5-40c9-b751-c1f09993f6b0.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a8e86093-7fa5-40c9-b751-c1f09993f6b0.png)'
- en: 'Figure 7.10: Image matrix and filter matrix'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10：图像矩阵和滤波器矩阵
- en: 'First, we place our filter matrix at the top-left corner of our image matrix
    and perform a **matrix multiplication** of the two at that location. We then move
    the filter matrix to the right by our stride size—1 pixel—and perform a matrix
    multiplication at that location. We continue this process until the filter matrix
    has traversed the entire image matrix. The resulting feature map matrix is illustrated
    in *Figure 7.11*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将我们的滤波器矩阵放置在图像矩阵的左上角，并在该位置进行两个矩阵的**矩阵乘法**。然后，我们将滤波器矩阵向右移动我们的步长大小——1个像素，并在该位置进行矩阵乘法。我们继续这个过程，直到滤波器矩阵穿越整个图像矩阵。结果的特征图矩阵在*图7.11*中展示：
- en: '![](img/d4c664c9-11eb-4df2-81e2-aae3ff9594f1.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d4c664c9-11eb-4df2-81e2-aae3ff9594f1.png)'
- en: 'Figure 7.11: Feature map'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11：特征图
- en: Note that the feature map is smaller in its dimensions than the input matrix
    of the convolution layer. To ensure that the dimensions of the output match the
    dimensions of the input, a layer of zero-value pixels is added in a process called
    **padding**. Also note that the filter must have the same number of channels as
    the input image—so in the case of RGB images, the filter must also have three
    channels.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，特征图的维度比卷积层的输入矩阵小。为了确保输出维度与输入维度匹配，通过一个称为**填充**的过程添加了一个零值像素层。此外，滤波器必须具有与输入图像相同的通道数——因此，在RGB图像的情况下，滤波器也必须具有三个通道。
- en: So, how do convolutions help a neural network to learn? To answer this question,
    we must revisit the concept of filters. Filters themselves are matrices of *weights*
    that are trained to detect specific patterns within an image, and different filters
    can be used to detect different patterns, such as edges and other features. For
    example, if we use a filter that has been pretrained to detect simple edges, as
    we pass this filter over an image, the convolution computation will output a high-valued
    real number (as a result of matrix multiplication and summation) if an edge is
    present and a low-valued real number if an edge is not present.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，卷积是如何帮助神经网络学习的呢？为了回答这个问题，我们必须回顾一下过滤器概念。过滤器本身是训练用来检测图像中特定模式的权重矩阵，不同的过滤器可以用来检测不同的模式，如边缘和其他特征。例如，如果我们使用一个预先训练用来检测简单边缘的过滤器，当我们把这个过滤器移动到图像上时，如果存在边缘，卷积计算将输出一个高值实数（作为矩阵乘法和求和的结果），如果不存在边缘，则输出一个低值实数。
- en: As the filter finishes traversing the entire image, the output is a feature
    map matrix that represents the convolutions of this filter over all parts of the
    image. By using different filters during different convolutions per layer, we
    get different feature maps, which form the output of the convolution layer.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当过滤器完成对整个图像的遍历后，输出是一个特征图矩阵，它表示该过滤器在图像所有部分的卷积。通过在每一层的不同卷积中使用不同的过滤器，我们得到不同的特征图，这些特征图构成了卷积层的输出。
- en: Rectified linear units
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩形线性单元
- en: 'As with other neural networks, an activation function defines the output of
    a node and is used so that our neural network can learn nonlinear functions. Note
    that our input data (the RGB pixels making up the images) is itself nonlinear,
    so we need a nonlinear activation function. **Rectified linear units** (**ReLUs**)
    are commonly used in CNNs, and are defined as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他神经网络一样，激活函数定义了节点的输出，并用于使我们的神经网络能够学习非线性函数。请注意，我们的输入数据（构成图像的RGB像素）本身是非线性的，因此我们需要一个非线性激活函数。**矩形线性单元**（**ReLU**）在CNN中常用，其定义如下：
- en: '![](img/81dc3852-c5cb-41a3-88cb-a7d51b7bb209.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/81dc3852-c5cb-41a3-88cb-a7d51b7bb209.png)'
- en: 'In other words, the ReLU function returns 0 for every negative value in its
    input data, and returns the value itself for every positive value in its input
    data. This is shown in *Figure 7.12*:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，ReLU函数对其输入数据中的每个负值返回0，对其输入数据中的每个正值返回其本身值。这如图7.12所示：
- en: '![](img/c50602e9-747c-4a87-968f-2c7aee8fe3eb.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c50602e9-747c-4a87-968f-2c7aee8fe3eb.png)'
- en: 'Figure 7.12: ReLU function'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12：ReLU函数
- en: 'The ReLU function can be plotted as shown in *Figure 7.13*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU函数可以绘制如图7.13所示：
- en: '![](img/48fcb0e4-f861-4487-bfbe-44e896a65b00.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/48fcb0e4-f861-4487-bfbe-44e896a65b00.png)'
- en: 'Figure 7.13: ReLU function graph'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13：ReLU函数图
- en: Pooling layers
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: 'The next transformations that occur in the CNN are processed in *pooling* layers.
    The goal of the pooling layers is to reduce the dimensionality of the feature
    maps output by the convolution layers (but not their depth) while preserving the
    spatial variance of the original input data. In other words, the size of the data
    is reduced in order to reduce computational complexity, memory requirements, and
    training times while overcoming over fitting so that patterns detected during
    training can be detected in test data even if their appearance varies. There are
    various pooling algorithms available, given a specified window size, including
    the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: CNN中接下来发生的变换在*池化*层中处理。池化层的目标是在保持原始输入数据的空间方差的同时，减少卷积层输出的特征图维度（但不是深度）。换句话说，通过减小数据的大小，可以减少计算复杂性、内存需求和训练时间，同时克服过拟合，以便在测试数据中检测到训练期间检测到的模式，即使它们的形状有所变化。给定一个特定的窗口大小，有各种池化算法可用，包括以下几种：
- en: '**Max pooling**: Takes the maximum value in each window'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大池化**：取每个窗口中的最大值'
- en: '**Average pooling**: Takes the average value across each window'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均池化**：取每个窗口的平均值'
- en: '**Sum pooling**: Takes the sum of values in each window'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**求和池化**：取每个窗口中值的总和'
- en: '*Figure 7.14* shows the effect of performing max pooling on a 4 x 4 feature
    map using a 2 x 2 window size:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.14*显示了使用2x2窗口大小对4x4特征图执行最大池化的效果：'
- en: '![](img/df33f184-e0c5-471f-8dca-97add6d76529.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/df33f184-e0c5-471f-8dca-97add6d76529.png)'
- en: 'Figure 7.14: Max pooling on a 4 x 4 feature map using a 2 x 2 window'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：使用2x2窗口对4x4特征图进行最大池化
- en: Fully connected layer
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层
- en: After the 3-D input data has been transformed through a series of convolution
    and pooling layers, a fully connected layer flattens the feature maps output by
    the last convolution and pooling layer into a long 1-D feature vector, which is
    then used as the input data for a regular ANN in which all of the neurons in each
    layer are connected to all of the neurons in the previous layer.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过一系列卷积和池化层将 3-D 输入数据转换后，一个全连接层将最后一个卷积和池化层输出的特征图展平成一个长的 1-D 特征向量，然后将其用作一个常规
    ANN 的输入数据，在这个 ANN 中，每一层的所有神经元都与前一层的所有神经元相连。
- en: Output layer
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输出层
- en: The output neurons in this ANN then use an activation function such as the softmax
    function (as seen in the MLP classifier) to classify the outputs and thereby recognize
    and classify the objects contained in the input image data!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个人工神经网络（ANN）中，输出神经元使用诸如 softmax 函数（如 MLP 分类器中所示）这样的激活函数来分类输出，从而识别和分类输入图像数据中的对象！
- en: Case study 2 – image recognition
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 2 - 图像识别
- en: In this case study, we will use a pretrained CNN to recognize and classify objects
    in images that it has never encountered before.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将使用一个预训练的 CNN 来识别和分类它以前从未遇到过的图像中的对象。
- en: InceptionV3 via TensorFlow
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 TensorFlow 使用 InceptionV3
- en: The pretrained CNN that we will use is called **Inception-v3**. This deep CNN
    has been trained on the **ImageNet** image database (an academic benchmark for
    computer vision algorithms containing a vast library of labelled images covering
    a wide range of nouns) and can classify entire images into 1,000 classes found
    in everyday life, such as "pizza", "plastic bag", "red wine", "desk", "orange",
    and "basketball", to name just a few.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的预训练 CNN 被称为 **Inception-v3**。这个深度 CNN 是在 **ImageNet** 图像数据库（一个包含大量标记图像的计算机视觉算法学术基准，覆盖了广泛的名词）上训练的，可以将整个图像分类为日常生活中发现的
    1,000 个类别，例如“披萨”、“塑料袋”、“红葡萄酒”、“桌子”、“橙子”和“篮球”，仅举几例。
- en: The Inception-v3 deep CNN was developed and trained by **TensorFlow** ^(TM),
    an open source machine learning framework and software library for high-performance
    numerical computation originally developed within Google's AI organization.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Inception-v3 深度 CNN 是由 **TensorFlow** (TM)，一个最初在 Google 的 AI 组织内部开发的开源机器学习框架和软件库，用于高性能数值计算，开发和训练的。
- en: 'To learn more about TensorFlow, Inception-v3, and ImageNet, please visit the
    following links:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 TensorFlow、Inception-v3 和 ImageNet 的信息，请访问以下链接：
- en: '**ImageNet:** [http://www.image-net.org/](http://www.image-net.org/)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ImageNet:** [http://www.image-net.org/](http://www.image-net.org/)'
- en: '**TensorFlow:** [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow:** [https://www.tensorflow.org/](https://www.tensorflow.org/)'
- en: '**Inception-v3:** [https://www.tensorflow.org/tutorials/images/image_recognition](https://www.tensorflow.org/tutorials/images/image_recognition)'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Inception-v3:** [https://www.tensorflow.org/tutorials/images/image_recognition](https://www.tensorflow.org/tutorials/images/image_recognition)'
- en: Deep learning pipelines for Apache Spark
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 的深度学习管道
- en: In this case study, we will access the Inception-v3 TensorFlow deep CNN via
    a third-party Spark package called `sparkdl`. This Spark package has been developed
    by Databricks, a company formed by the original creators of Apache Spark, and
    provides high-level APIs for scalable deep learning within Apache Spark.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们将通过一个名为 `sparkdl` 的第三方 Spark 包来访问 Inception-v3 TensorFlow 深度 CNN。这个
    Spark 包是由 Apache Spark 的原始创建者成立的公司 Databricks 开发的，并为 Apache Spark 中的可扩展深度学习提供了高级
    API。
- en: 'To learn more about Databricks and `sparkdl`, please visit the following links:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Databricks 和 `sparkdl` 的信息，请访问以下链接：
- en: '**Databricks**: [https://databricks.com/](https://databricks.com/)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Databricks**: [https://databricks.com/](https://databricks.com/)'
- en: '**sparkdl**: [https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**sparkdl**: [https://github.com/databricks/spark-deep-learning](https://github.com/databricks/spark-deep-learning)'
- en: Image library
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像库
- en: The images that we will use to test the pretrained Inception-v3 deep CNN have
    been selected from the **Open Images v4** dataset, a collection of over 9 million
    images that have been released under the Creative Common Attribution license,
    and which may be found at [https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于测试预训练的 Inception-v3 深度卷积神经网络（CNN）的图像已从 **Open Images v4** 数据集中选取，这是一个包含超过
    900 万张图片的集合，这些图片是在 Creative Common Attribution 许可下发布的，并且可以在 [https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html)
    找到。
- en: 'In the GitHub repository accompanying this book, you can find 30 images of
    birds (`image-recognition-data/birds`) and 30 images of planes (`image-recognition-data/planes`)
    respectively. *Figure 7.15* shows a couple of examples of the images that you
    might find in these test datasets:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书配套的 GitHub 仓库中，您可以找到 30 张鸟类图像（`image-recognition-data/birds`）和 30 张飞机图像（`image-recognition-data/planes`）。*图
    7.15* 显示了您可能在这些测试数据集中找到的一些图像示例：
- en: '![](img/b455f360-046b-4640-bc0f-cc260e7eba3c.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b455f360-046b-4640-bc0f-cc260e7eba3c.png)'
- en: 'Figure 7.15: Example images from the Open Images v4 dataset'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.15：Open Images v4 数据集的示例图像
- en: Our goal in this case study will be to apply the pretrained Inception-v3 deep
    CNN to these test images and quantify the accuracy of a trained classifier model
    when it comes to distinguishing between images of birds and planes within a single
    test dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们的目标将是将预训练的 Inception-v3 深度 CNN 应用到这些测试图像上，并量化训练好的分类器模型在区分单个测试数据集中鸟类和飞机图像时的准确率。
- en: PySpark image recognition application
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 图像识别应用程序
- en: Note that for the purposes of this case study, we will not be using Jupyter
    notebooks for development but rather standard Python code files with the `.py`
    file extension. This case study provides a first glimpse into how a production-grade
    pipeline should be developed and executed; rather than instantiating a `SparkContext`
    explicitly within our code, we will instead submit our code and all its dependencies
    to `spark-submit` (including any third-party Spark packages, such as `sparkdl`)
    via the Linux command line.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了本案例研究的目的，我们不会使用 Jupyter notebook 进行开发，而是使用具有 `.py` 文件扩展名的标准 Python 代码文件。本案例研究提供了一个关于如何开发和执行生产级管道的初步了解；而不是在我们的代码中显式实例化
    `SparkContext`，我们将通过 Linux 命令行将我们的代码及其所有依赖项提交给 `spark-submit`（包括任何第三方 Spark 包，如
    `sparkdl`）。
- en: 'Let''s now take a look at how we can use the Inception-v3 deep CNN via PySpark
    to classify test images. In our Python-based image-recognition application, we
    perform the following steps (numbered to correspond to the numbered comments in
    our Python code file):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何通过 PySpark 使用 Inception-v3 深度 CNN 来对测试图像进行分类。在我们的基于 Python 的图像识别应用程序中，我们执行以下步骤（编号与
    Python 代码文件中的编号注释相对应）：
- en: The following Python code file, called `chp07-02-convolutional-neural-network-transfer-learning.py`,
    can be found in the GitHub repository accompanying this book.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 以下名为 `chp07-02-convolutional-neural-network-transfer-learning.py` 的 Python 代码文件，可以在本书配套的
    GitHub 仓库中找到。
- en: 'First, using the following code, we import the required Python dependencies,
    including the relevant modules from the third-party `sparkdl` package and the
    `LogisticRegression` classifier native to `MLlib`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用以下代码，我们导入所需的 Python 依赖项，包括来自第三方 `sparkdl` 包的相关模块和 `MLlib` 内置的 `LogisticRegression`
    分类器：
- en: '[PRE8]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Unlike our Jupyter notebook case studies, there is no need to instantiate a
    `SparkContext`, as this will be done for us when we execute our PySpark application
    via `spark-submit` on the command line. In this case study, we will create a `SparkSession`,
    as shown in the following code, that acts as an entry point into the Spark execution
    environment (even if it is already running) that subsumes SQLContext. We can therefore
    use `SparkSession` to undertake the same SQL-like operations over data that we
    have seen previously while still using the Spark Dataset/DataFrame API:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与我们的 Jupyter notebook 案例研究不同，我们没有必要实例化一个 `SparkContext`，因为当我们通过命令行执行 PySpark
    应用程序时，这会为我们完成。在本案例研究中，我们将创建一个 `SparkSession`，如下所示，它作为 Spark 执行环境（即使它已经在运行）的入口点，它包含
    SQLContext。因此，我们可以使用 `SparkSession` 来执行与之前所见相同的类似 SQL 的操作，同时仍然使用 Spark Dataset/DataFrame
    API：
- en: '[PRE9]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As of Version 2.3, Spark provides native support for image data sources via
    its `MLlib` API. In this step, we invoke the `readImages` method on `MLlib`''s
    `ImageSchema` class to load our bird and plane test images from the local filesystem
    into Spark dataframes called `birds_df` and `planes_df` respectively. We then
    label all images of birds with the `0` literal and label all images of planes
    with the `1` literal, as follows:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 截至 2.3 版本，Spark 通过其 `MLlib` API 提供了对图像数据源的原生支持。在此步骤中，我们通过在 `MLlib` 的 `ImageSchema`
    类上调用 `readImages` 方法，将我们的鸟类和飞机测试图像从本地文件系统加载到名为 `birds_df` 和 `planes_df` 的 Spark
    数据帧中。然后，我们用 `0` 文字标签所有鸟类图像，用 `1` 文字标签所有飞机图像，如下所示：
- en: '[PRE10]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now that we have loaded our test images into separate Spark dataframes differentiated
    by their label, we consolidate them into single training and test dataframes accordingly.
    We achieve this by using the `unionAll` method via the Spark dataframe API, which
    simply appends one dataframe onto another, as shown in the following code:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将测试图像加载到分别以它们的标签区分的单独Spark数据框中，我们相应地将它们合并为单个训练和测试数据框。我们通过使用Spark数据框API的`unionAll`方法来实现这一点，该方法简单地将一个数据框附加到另一个数据框上，如下面的代码所示：
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As with previous case studies, we need to generate feature vectors from our
    input data. However, rather than training a deep CNN from scratch—which could
    take many days, even with distributed technologies—we will take advantage of the
    pretrained Inception-v3 deep CNN. To do this, we will use a process called **transfer
    learning**. In this process, knowledge gained while solving one machine learning
    problem is applied to a different but related problem. To use transfer learning
    in our case study, we employ the `DeepImageFeaturizer` module of the third-party
    `sparkdl` Spark package. The `DeepImageFeaturizer` not only transforms our images
    into numeric features, it also performs fast transfer learning by peeling off
    the last layer of a pretrained neural network and then uses the output from all
    the previous layers as features for a standard classification algorithm. In our
    case, the `DeepImageFeaturizer` will be peeling off the last layer of the pretrained
    Inception-v3 deep CNN, as follows:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与之前的案例研究一样，我们需要从我们的输入数据生成特征向量。然而，我们不会从头开始训练一个深度CNN——即使有分布式技术，这也可能需要好几天——我们将利用预训练的Inception-v3深度CNN。为此，我们将使用称为**迁移学习**的过程。在这个过程中，解决一个机器学习问题获得的知识被应用于不同但相关的问题。为了在我们的案例研究中使用迁移学习，我们采用第三方`sparkdl`
    Spark包的`DeepImageFeaturizer`模块。`DeepImageFeaturizer`不仅将我们的图像转换为数值特征，还通过剥离预训练神经网络的最后一层来执行快速迁移学习，然后使用所有先前层的输出作为标准分类算法的特征。在我们的案例中，`DeepImageFeaturizer`将剥离预训练的Inception-v3深度CNN的最后一层，如下所示：
- en: '[PRE13]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we have the features from all previous layers of the pretrained Inception-v3
    deep CNN extracted via transfer learning, we input them into a classification
    algorithm. In our case, we will use `MLlib`''s `LogisticRegression` classifier,
    as follows:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经通过迁移学习从预训练的Inception-v3深度CNN的所有先前层中提取了特征，我们将它们输入到分类算法中。在我们的案例中，我们将使用`MLlib`的`LogisticRegression`分类器，如下所示：
- en: '[PRE14]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To execute the transfer learning and logistic regression model training, we
    build a standard `pipeline` and `fit` that pipeline to our training dataframe,
    as follows:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要执行迁移学习和逻辑回归模型训练，我们构建一个标准的`pipeline`并将该管道拟合到我们的训练数据框中，如下所示：
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now that we have a trained classification model, using the features derived
    by the Inception-v3 deep CNN, we apply our trained logistic regression model to
    our test dataframe to make predictions as normal, as shown in the following code:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一个分类模型，使用由Inception-v3深度CNN推导出的特征，我们将我们的训练逻辑回归模型应用于测试数据框以进行正常预测，如下面的代码所示：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, we quantify the accuracy of our model on the test dataframe using
    `MLlib`''s `MulticlassClassificationEvaluator`, as follows:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用`MLlib`的`MulticlassClassificationEvaluator`在测试数据框上量化我们模型的准确性，如下所示：
- en: '[PRE17]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Spark submit
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark提交
- en: 'We are now ready to run our image recognition application! Since it is a Spark
    application, we can execute it via `spark-submit` on the Linux command line. To
    do this, navigate to the directory where we installed Apache Spark (see [Chapter
    2](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml), *Setting Up a Local Development
    Environment*). Then, we can execute the `spark-submit` program by passing it the
    following command-line arguments:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以运行我们的图像识别应用程序了！由于它是一个Spark应用程序，我们可以在Linux命令行通过`spark-submit`来执行它。为此，导航到我们安装Apache
    Spark的目录（见[第2章](673f253e-c30f-41af-b5e0-e5fd301f829c.xhtml)，*设置本地开发环境*）。然后，我们可以通过传递以下命令行参数来执行`spark-submit`程序：
- en: '`--master`: The Spark Master URL.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--master`: Spark Master的URL。'
- en: '`--packages`: The third-party libraries and dependencies required for the Spark
    application to work. In our case, our image-recognition application is dependent
    on the availability of the `sparkdl` third-party library.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--packages`: Spark应用程序运行所需的第三方库和依赖项。在我们的案例中，我们的图像识别应用程序依赖于`sparkdl`第三方库的可用性。'
- en: '`--py-files`: Since our image-recognition application is a PySpark application,
    we pass the filesystem paths to any Python code files that our application is
    dependent on. In our case, since our image-recognition application is self-contained
    within a single code file, there are no further dependencies to pass to `spark-submit`.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--py-files`：由于我们的图像识别应用程序是一个 PySpark 应用程序，我们将传递应用程序依赖的任何 Python 代码文件的文件系统路径。在我们的情况下，由于我们的图像识别应用程序包含在一个单独的代码文件中，因此没有其他依赖项需要传递给`spark-submit`。'
- en: The final argument is the path to the Python code file containing our Spark
    driver program, namely `chp07-02-convolutional-neural-network-transfer-learning.py`.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个参数是包含我们的 Spark 驱动程序的 Python 代码文件的路径，即`chp07-02-convolutional-neural-network-transfer-learning.py`。
- en: 'The final commands to execute, therefore, look as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，执行的最后命令如下：
- en: '[PRE18]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Image-recognition results
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像识别结果
- en: 'Assuming that the image-recognition application ran successfully, you should
    see the following results output to the console:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 假设图像识别应用程序运行成功，你应该会在控制台看到以下结果输出：
- en: '| **Origin** | **Prediction** |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **Origin** | **Prediction** |'
- en: '| `planes/plane-005.jpg` | `1.0` |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| `planes/plane-005.jpg` | `1.0` |'
- en: '| `planes/plane-008.jpg` | `1.0` |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| `planes/plane-008.jpg` | `1.0` |'
- en: '| `planes/plane-009.jpg` | `1.0` |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| `planes/plane-009.jpg` | `1.0` |'
- en: '| `planes/plane-016.jpg` | `1.0` |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| `planes/plane-016.jpg` | `1.0` |'
- en: '| `planes/plane-017.jpg` | `0.0` |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| `planes/plane-017.jpg` | `0.0` |'
- en: '| `planes/plane-018.jpg` | `1.0` |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| `planes/plane-018.jpg` | `1.0` |'
- en: '| `birds/bird-005.jpg` | `0.0` |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| `birds/bird-005.jpg` | `0.0` |'
- en: '| `birds/bird-008.jpg` | `0.0` |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| `birds/bird-008.jpg` | `0.0` |'
- en: '| `birds/bird-009.jpg` | `0.0` |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| `birds/bird-009.jpg` | `0.0` |'
- en: '| `birds/bird-016.jpg` | `0.0` |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| `birds/bird-016.jpg` | `0.0` |'
- en: '| `birds/bird-017.jpg` | `0.0` |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| `birds/bird-017.jpg` | `0.0` |'
- en: '| `birds/bird-018.jpg` | `0.0` |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| `birds/bird-018.jpg` | `0.0` |'
- en: 'The `Origin` column refers to the absolute filesystem path of the image, and
    the value in the `Prediction` column is `1.0` if our model predicts that the object
    in the image is a plane and `0.0` if our model predicts that the object in the
    image is a bird. Our model has an astonishingly high accuracy of 92% when run
    on the test dataset. The only mistake that our model made was on `plane-017.jpg`,
    illustrated in *Figure* *7.16*, which was incorrectly classified as a bird when
    it was in fact a plane:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '`Origin`列指的是图像的绝对文件系统路径，`Prediction`列中的值如果是我们的模型预测图像中的物体是飞机，则为`1.0`；如果是鸟，则为`0.0`。当在测试数据集上运行时，我们的模型具有惊人的92%的准确率。我们的模型唯一的错误是在`plane-017.jpg`上，如图*7.16*所示，它被错误地分类为鸟，而实际上它是一架飞机：'
- en: '![](img/677136f7-68b4-4b1c-8202-81f483f1fda0.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/677136f7-68b4-4b1c-8202-81f483f1fda0.png)'
- en: 'Figure 7.16: Incorrect classification of plane-017.jpg'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.16：plane-017.jpg 的错误分类
- en: If we look at `plane-017.jpg` in *Figure 7.16*, we can quickly understand why
    the model made this mistake. Though it is a man-made plane, it has been physically
    modeled to look like a bird for increased efficiency and aerodynamic purposes.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看*图 7.16*中的`plane-017.jpg`，我们可以快速理解模型为什么会犯这个错误。尽管它是一架人造飞机，但它被物理建模成鸟的样子，以提高效率和空气动力学性能。
- en: In this case study, we used a pretrained CNN to featurize images. We then passed
    the resulting features to a standard logistic regression algorithm to predict
    whether a given image is a bird or a plane.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们使用预训练的 CNN 对图像进行特征提取。然后，我们将得到的特征传递给标准的逻辑回归算法，以预测给定图像是鸟还是飞机。
- en: Case study 3 – image prediction
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 3 – 图像预测
- en: In case study 2 (image recognition), we still explicitly labelled our test images
    before training our final logistic regression classifier. In this case study,
    we will simply send random images to the pretrained Inception-v3 deep CNN without
    labeling them and let the CNN itself classify the objects contained within the
    images. Again, we will take advantage of the third-party `sparkdl` Spark package
    to access the pretrained Inception-v3 CNN.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在案例研究 2（图像识别）中，我们在训练最终的逻辑回归分类器之前，仍然明确地为我们的测试图像进行了标注。在这个案例研究中，我们将简单地发送随机图像到预训练的
    Inception-v3 深度 CNN，而不对其进行标注，并让 CNN 本身对图像中包含的物体进行分类。同样，我们将利用第三方`sparkdl` Spark
    包来访问预训练的 Inception-v3 CNN。
- en: 'The assortment of random images that we will use have again been downloaded
    from the **Open Images v4 dataset**, and may be found in the GitHub repository
    accompanying this book under `image-recognition-data/assorted`. *Figure 7.17*
    shows a couple of typical images that you may find in this test dataset:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的随机图像再次从**Open Images v4数据集**下载，可以在本书附带的 GitHub 仓库中的`image-recognition-data/assorted`找到。*图
    7.17*显示了测试数据集中可能找到的一些典型图像：
- en: '![](img/b763134a-ece7-47ff-b8d4-888928a49752.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b763134a-ece7-47ff-b8d4-888928a49752.png)'
- en: 'Figure 7.17: Assortment of random images'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17：随机图像组合
- en: PySpark image-prediction application
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark图像预测应用程序
- en: 'In our Python-based image-prediction application, we go through the following
    steps (numbered to correspond to the numbered comments in our Python code file):'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基于Python的图像预测应用程序中，我们按照以下步骤进行（编号与Python代码文件中的注释编号相对应）：
- en: The following Python code file, called `chp07-03-convolutional-neural-network-image-predictor.py`,
    can be found in the GitHub repository accompanying this book.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下名为`chp07-03-convolutional-neural-network-image-predictor.py`的Python代码文件，可以在本书附带的GitHub存储库中找到。
- en: 'First, we import the required Python dependencies as usual, including the `DeepImagePredictor`
    class from the third-party `sparkdl` Spark package, as shown in the following
    code:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们像往常一样导入所需的Python依赖项，包括来自第三方`sparkdl` Spark包的`DeepImagePredictor`类，如下所示：
- en: '[PRE19]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we create a `SparkSession` that acts as an entry point into the Spark
    execution environment, as follows:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`SparkSession`，它作为Spark执行环境的入口点，如下所示：
- en: '[PRE20]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We then load our assortment of random images into a Spark dataframe using the
    `readImages` method of the `ImageSchema` class that we first encountered in the
    previous case study, as shown in the following code:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用我们在上一个案例研究中首次遇到的`ImageSchema`类的`readImages`方法将我们的随机图像组合加载到Spark数据框中，如下所示：
- en: '[PRE21]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we pass our Spark dataframe containing our assortment of random images
    to `sparkdl`''s `DeepImagePredictor`, which will apply a specified pretrained
    neural network to the images in an effort to classify the objects found within
    them. In our case, we will be using the pretrained Inception-v3 deep CNN. We also
    tell the `DeepImagePredictor` to return the top 10 (`topK=10`) predicted classifications
    for each image in descending order of confidence, as follows:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将包含我们的随机图像组合的Spark数据框传递给`sparkdl`的`DeepImagePredictor`，它将应用指定的预训练神经网络来对图像中的对象进行分类。在我们的案例中，我们将使用预训练的Inception-v3深度CNN。我们还告诉`DeepImagePredictor`按置信度降序返回每个图像的前10个（`topK=10`）预测分类，如下所示：
- en: '[PRE22]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To run this PySpark image-prediction application, we again invoke `spark-submit`
    via the command line, as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此PySpark图像预测应用程序，我们再次通过命令行调用`spark-submit`，如下所示：
- en: '[PRE23]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Image-prediction results
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像预测结果
- en: 'Assuming that the image-prediction application ran successfully, you should
    see the following results output to the console:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 假设图像预测应用程序运行成功，您应该在控制台看到以下结果输出：
- en: '| **Origin** | **First Predicted Label** |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| **原始** | **首次预测标签** |'
- en: '| `assorted/snowman.jpg` | `Teddy` |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/snowman.jpg` | `泰迪熊` |'
- en: '| `assorted/bicycle.jpg` | `Mountain Bike` |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/bicycle.jpg` | `山地自行车` |'
- en: '| `assorted/house.jpg` | `Library` |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/house.jpg` | `图书馆` |'
- en: '| `assorted/bus.jpg` | `Trolley Bus` |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/bus.jpg` | `有轨电车` |'
- en: '| `assorted/banana.jpg` | `Banana` |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/banana.jpg` | `香蕉` |'
- en: '| `assorted/pizza.jpg` | `Pizza` |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/pizza.jpg` | `披萨` |'
- en: '| `assorted/toilet.jpg` | `Toilet Seat` |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/toilet.jpg` | `马桶座圈` |'
- en: '| `assorted/knife.jpg` | `Cleaver` |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/knife.jpg` | `大刀` |'
- en: '| `assorted/apple.jpg` | `Granny Smith (Apple)` |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/apple.jpg` | `红富士（苹果）` |'
- en: '| `assorted/pen.jpg` | `Ballpoint` |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/pen.jpg` | `圆珠笔` |'
- en: '| `assorted/lion.jpg` | `Lion` |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/lion.jpg` | `狮子` |'
- en: '| `assorted/saxophone.jpg` | `Saxophone` |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/saxophone.jpg` | `萨克斯风` |'
- en: '| `assorted/zebra.jpg` | `Zebra` |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/zebra.jpg` | `斑马` |'
- en: '| `assorted/fork.jpg` | `Spatula` |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/fork.jpg` | `勺子` |'
- en: '| `assorted/car.jpg` | `Convertible` |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| `assorted/car.jpg` | `敞篷车` |'
- en: As you can see, the pretrained Inception-v3 deep CNN has an astonishing ability
    to recognize and classify the objects found in images. Though the images provided
    in this case study were relatively simple, the Inception-v3 CNN has a top-five
    error rate— how often the model fails to predict the correct answer as one of
    its top five guesses—of just 3.46% on the ImageNet image database. Remember that
    the Inception-v3 CNN attempts to classify entire images into 1,000 classes, hence
    a top-5 error rate of just 3.46% is truly impressive, and clearly demonstrates
    the learning ability and power of not only convolution neural networks but ANNs
    in general when detecting and learning patterns!
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，预训练的Inception-v3深度CNN具有惊人的识别和分类图像中对象的能力。尽管本案例研究中提供的图像相对简单，但Inception-v3
    CNN在ImageNet图像数据库上的前五错误率——即模型未能将其正确答案预测为其前五个猜测之一的情况——仅为3.46%。请记住，Inception-v3
    CNN试图将整个图像分类到1,000个类别中，因此仅3.46%的前五错误率确实令人印象深刻，并且清楚地展示了卷积神经网络以及一般的人工神经网络在检测和学习模式时的学习能力和力量！
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went on a hands-on exploration through the exciting and
    cutting-edge world of deep learning. We developed applications to recognize and
    classify objects in images with astonishingly high rates of accuracy, and demonstrated
    the truly impressive learning ability of ANNs to detect and learn patterns in
    input data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们亲身体验了激动人心且前沿的深度学习世界。我们开发了能够以惊人的准确率识别和分类图像中的应用程序，并展示了人工神经网络在检测和学习输入数据中的模式方面的真正令人印象深刻的学习能力。
- en: In the next chapter, we will extend our deployment of machine learning models
    beyond batch processing in order to learn from data and make predictions in real
    time!
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将扩展我们的机器学习模型部署，使其超越批量处理，以便从数据中学习并在实时中进行预测！
