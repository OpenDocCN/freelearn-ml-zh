- en: '*Chapter 6*: Getting Started with Deep Learning: Crash Course in Neural Networks'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you'll learn the basics of deep learning and artificial neural
    networks. You'll discover the basic idea and theory behind these topics and how
    to train simple neural network models with Python. The chapter will serve as an
    excellent primer for the upcoming chapters, where the ideas of pipeline optimization
    and neural networks are combined.
  prefs: []
  type: TYPE_NORMAL
- en: We'll cover the essential topics and ideas behind deep learning, why it has
    gained popularity in the last few years, and the cases in which neural networks
    work better than traditional machine learning algorithms. You'll also get hands-on
    experience in coding your own neural networks, both from scratch and through pre-made
    libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: An overview of deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing artificial neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using neural networks to classify handwritten digits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing neural networks in regression and classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No prior experience with deep learning and neural networks is necessary. You
    should be able to understand the basics from this chapter alone. Previous experience
    is helpful, as deep learning isn't something you can learn in one sitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the source code and dataset for this chapter here: [https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter06](https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter06).'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning is a subfield of machine learning that focuses on neural networks.
    Neural networks aren't that new as a concept – they were introduced back in the
    1940s but didn't gain much in popularity until they started winning data science
    competitions (somewhere around 2010).
  prefs: []
  type: TYPE_NORMAL
- en: Potentially the biggest year for deep learning and AI was 2016, all due to a
    single event. *AlphaGo*, a computer program that plays the board game Go, defeated
    the highest-ranking player in the world. Before this event, Go was considered
    to be a game that computers couldn't master, as there are so many potential board
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, deep learning is based on neural networks. You can think
    of neural networks as **directed acyclic graphs** – a graph consisting of vertices
    (nodes) and edges (connections). The input layer (the first layer, on the far
    left side) takes in the raw data from your datasets, passes it through one or
    multiple hidden layers, and constructs an output.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see an example architecture of a neural network in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – An example neural network architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – An example neural network architecture
  prefs: []
  type: TYPE_NORMAL
- en: The small black nodes on the far left side represent the input data – data that
    comes directly from your dataset. These values are then connected with the hidden
    layers, with their respective weights and biases. A common way to refer to these
    weights and biases is by using the term **tunable parameters**. We'll address
    this term and show how to calculate them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every node of a neural network is called a **neuron**. Let''s take a look at
    the architecture of an individual neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Individual neuron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Individual neuron
  prefs: []
  type: TYPE_NORMAL
- en: The X's correspond to the values either from the input layer or from the previous
    hidden layer. These values are multiplied together (*x1 * w1*, *x2 * w2*) and
    then added together (*x1w1 + x2w2*). After the summation, a bias term is added,
    and finally, everything is passed through an **activation function**. This function
    determines if the neuron will "fire" or not. It's something like an on-off switch,
    in the simplest terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'A brief explanation of weights and biases is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weights:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) Multiplied with values from the previous layer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) Can change the magnitude or entirely flip the value from positive to negative
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) In function terms – adjusting the weight changes the slope of the function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Biases:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) Interpreted as an offset of the function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) An increase in bias leads to an upward shift of a function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) A decrease in bias leads to a downward shift of a function
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'There are many types of neural network architectures besides artificial neural
    networks, and they are discussed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**) – a type of neural network most
    commonly applied to analyzing images. They are based on the convolution operation
    – an operation between two matrices in which the second one slides (convolves)
    over the first one and computes element-wise multiplication. The goal of this
    operation is to find a sliding matrix (kernel) that can extract the correct features
    from the input image and hence make image classification tasks easy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNNs**) – a type of neural network most commonly
    used on sequence data. Today these networks are applied in many tasks, such as
    handwriting recognition, speech recognition, machine translation, and time series
    forecasting. The RNN model processes a single element in the sequence at a time.
    After processing, the new updated unit''s state is passed down to the next time
    step. Imagine predicting a single character based on the previous *n* characters;
    that''s the general gist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative adversarial networks** (**GANs**) – a type of neural network most
    commonly used to create new samples after learning from real data. The GAN architecture
    comprises two separate models – generators and discriminators. The job of a generator
    model is to make fake images and send them to the discriminator. The discriminator
    works like a judge and tries to tell whether an image is fake or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Autoencoders** – unsupervised learning techniques, designed to learn a low-dimensional
    representation of a high-dimensional dataset. In a way, they work similarly to
    **Principal Component Analysis** (**PCA**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These four deep learning concepts won't be covered in this book. We'll focus
    only on artificial neural networks, but it's good to know they exist in case you
    want to dive deeper on your own.
  prefs: []
  type: TYPE_NORMAL
- en: The next section looks at artificial neural networks and shows you how to implement
    them in Python, both from scratch and with data science libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing artificial neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The fundamental building block of an artificial neural network is the neuron.
    By itself, a single neuron is useless, but it can have strong predictive power
    when combined into a more complex network.
  prefs: []
  type: TYPE_NORMAL
- en: If you can't reason why, think about your brain and how it works for a minute.
    Just like artificial neural networks, it is also made from millions of neurons,
    which function only when there's communication between them. Since artificial
    neural networks try to imitate the human brain, they need to somehow replicate
    neurons in the brain and connections between them (weights). This association
    will be made less abstract throughout this section.
  prefs: []
  type: TYPE_NORMAL
- en: Today, artificial neural networks can be used to tackle any problem that regular
    machine learning algorithms can. In a nutshell, if you can solve a problem with
    linear or logistic regression, you can solve it with neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Before we can explore the complexity and inner workings of an entire network,
    we have to start simple – with the theory of a single neuron.
  prefs: []
  type: TYPE_NORMAL
- en: Theory of a single neuron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Modeling a single neuron is easy with Python. For example, let''s say a neuron
    receives values from five other neurons (inputs, or X''s). Let''s examine this
    behavior visually before implementing it in code. The following diagram shows
    how a single neuron looks when receiving values from five neurons in the previous
    layers (we''re modeling the neuron on the right):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Modeling a single neuron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Modeling a single neuron
  prefs: []
  type: TYPE_NORMAL
- en: The X's represent input features, either from the raw data or from the previous
    hidden layer. Each input feature has a weight assigned to it, denoted with W's.
    Corresponding input values and weights are multiplied and summed, and then the
    bias term (b) is added on top of the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula for calculating the output value of our neuron is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s work with concrete values to get this concept a bit clearer. The following
    diagram looks identical to Figure 6.3, but has actual numbers instead of variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Neuron value calculation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – Neuron value calculation
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plug the values directly into the preceding formula to calculate the
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_06_002.jpg)![](img/Formula_06_003.jpg)![](img/Formula_06_004.jpg)![](img/Formula_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In reality, single neurons get their value from potentially thousands of neurons
    in the previous layers, so calculating values manually and expressing visually
    isn't practical.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you decide to do so, that's only a single forward pass. Neural networks
    learn during the backward pass, which is much more complicated to calculate by
    hand.
  prefs: []
  type: TYPE_NORMAL
- en: Coding a single neuron
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, let''s see how you can semi-automate neuron value calculation with Python:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, let''s declare input values, their respective weights, and a value
    for the bias term. The first two are lists, and the bias is just a number:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's all you need to calculate the output value. Let's examine what your options
    are next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are three simple methods for calculating neuron output values. The first
    one is the most manual, and that is to explicitly multiply corresponding inputs
    and weights, adding them together with the bias.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s a Python implementation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see a value of `11.6` printed out after executing this code. To be
    more precise, the value should be `11.600000000000001`, but don't worry about
    this calculation error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next method is a bit more scalable, and it boils down to iterating through
    inputs and weights at the same time and incrementing the variable declared earlier
    for the output. After the loop finishes, the bias term is added. Here''s how to
    implement this calculation method:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is still identical, but you can immediately see how much more scalable
    this option is. Just imagine using the first option if the previous network layer
    had 1,000 neurons – it's not even remotely convenient.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The third and preferred method is to use a scientific computing library, such
    as NumPy. With it, you can calculate the vector dot product and add the bias term.
    Here''s how:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This option is the fastest, both to write and to execute, so it's the preferred
    one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You now know how to code a single neuron – but neural networks employ layers
    of neurons. You'll learn more about layers next.
  prefs: []
  type: TYPE_NORMAL
- en: Theory of a single layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To make things simpler, think of layers as vectors or simple groups. Layers
    aren't some complicated or abstract data structure. In code terms, you can think
    of them as lists. They contain a number of neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Coding a single layer of neurons is quite similar to coding a single neuron.
    We still have the same inputs, as they are coming either from a previous hidden
    layer or an input layer. What changes are weights and biases. In code terms, weights
    aren't treated as a list anymore, but as a list of lists instead (or a matrix).
    Similarly, bias is now a list instead of a scalar value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Put simply, your matrix of weights will have as many rows as there are neurons
    in the new layer and as many columns as there are neurons in the previous layer.
    Let''s take a look at a sample diagram to make this concept a bit less abstract:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Layer of neurons'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Layer of neurons
  prefs: []
  type: TYPE_NORMAL
- en: 'Weight values deliberately weren''t placed on the previous diagram, as it would
    look messy. To implement this layer in code, you''ll need to have the following
    structures:'
  prefs: []
  type: TYPE_NORMAL
- en: Vector of inputs (1 row, 5 columns)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix of weights (2 rows, 5 columns)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector of biases (1 row, 2 columns)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A matrix multiplication rule from linear algebra states that two matrices need
    to be of shapes (m, n) and (n, p) in order to produce an (m, p) matrix after multiplication.
    Bearing that in mind, you could easily perform matrix multiplication by transposing
    the matrix of weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, here''s the formula you can use to calculate the values of
    the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_06_007.png) is the vector of inputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_06_008.png) is the matrix of weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_06_009.png) is the vector of biases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s declare values for all of these and see how to calculate the values
    for an output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_06_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The previously mentioned formula can now be used to calculate the values of
    the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_06_011.jpg)![](img/Formula_06_012.jpg)![](img/Formula_06_013.jpg)![](img/Formula_06_014.jpg)![](img/Formula_06_015.jpg)![](img/Formula_06_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And that's essentially how you can calculate outputs for an entire layer. The
    calculations will grow in size for actual neural networks, as there are thousands
    of neurons per layer, but the logic behind the math is identical.
  prefs: []
  type: TYPE_NORMAL
- en: You can see how tedious it is to calculate layer outputs manually. You'll learn
    how to calculate the values in Python next.
  prefs: []
  type: TYPE_NORMAL
- en: Coding a single layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's now examine three ways in which you could calculate the output values
    for a single layer. As with single neurons, we'll start with the manual approach
    and finish with a NumPy one-liner.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll have to declare values for inputs, weights, and biases first, so here''s
    how to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s proceed with calculating the values of the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the manual approach. No, we won't do the same procedure as
    with neurons. You could, of course, but it would look too messy and impractical.
    Instead, we'll immediately use the `zip()` function to iterate over the `weights`
    matrix and `biases` array and calculate the value of a single output neuron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This procedure is repeated for however many neurons there are, and each output
    neuron is appended to a list that represents the output layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here''s the entire code snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The result is a list with the value `[11.6, 6.8]`, which are the same results
    we got from the manual calculation earlier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While this approach works, it's still not optimal. Let's see how to improve
    next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You'll now calculate the values of the output layer by taking the vector dot
    product between input values and every row of the `weights` matrix. The bias term
    will be added after this operation is completed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s see how it works in action:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The layer values are still identical – `[11.6, 6.8]`, and this approach is a
    bit more scalable than the previous one. It can still be improved upon. Let's
    see how next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can perform a matrix multiplication between inputs and transposed weights
    and add the corresponding biases with a single line of Python code. Here''s how:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's the recommended way if, for some reason, you want to calculate outputs
    manually. NumPy handles it completely, so it's the fastest one at the same time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You now know how to calculate output values both for a single neuron and for
    a single layer of a neural network. So far, we haven't covered a crucial idea
    in neural networks that decides whether a neuron will "fire" or "activate" or
    not. These are called activation functions, and we'll cover them next.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Activation functions are essential for the output of neural networks, and hence
    to the output of the deep learning model. They are nothing but mathematical equations,
    and relatively simple ones to be precise. Activation functions are those that
    determine whether the neuron should be "activated" or not.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think about the activation function is as a sort of gate that
    stands between the input coming into the current neuron and its output, which
    goes to the next layer. Activation function can be as simple as a step function
    (turns neurons on or off), or a bit more complicated and non-linear. It's the
    non-linear functions that prove useful in learning complex data and providing
    accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: We'll go over a couple of the most common activation functions next.
  prefs: []
  type: TYPE_NORMAL
- en: Step function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The step function is based on a threshold. If the value coming in is above the
    threshold, the neuron is activated. That's why we can say the step function serves
    as an on-off switch – there are no values in between.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can easily use Python and NumPy to declare and visualize a basic step function.
    The procedure is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: To start, you'll have to define a step function. The typical threshold value
    is 0, so the neuron will activate if and only if the value passed in to the function
    is greater than 0 (the input value being the sum of the previous inputs multiplied
    by the weights and added bias).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This kind of logic is trivial to implement in Python:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now declare a list of values that will serve as an input to this function,
    and then apply `step_function()` to this list. Here''s an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can visualize the function with the help of the Matplotlib library
    in just two lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see how the function works visually in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Step activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – Step activation function
  prefs: []
  type: TYPE_NORMAL
- en: The biggest problem of the step function is that it doesn't allow multiple outputs
    – only two. We'll dive into a set of non-linear functions next, and you'll see
    what makes them different.
  prefs: []
  type: TYPE_NORMAL
- en: Sigmoid function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The sigmoid activation function is frequently referred to as the logistic function.
    It is a very popular function in the realm of neural networks and deep learning.
    It essentially transforms the input into a value between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: You'll see how the function works later, and you'll immediately notice an advantage
    over the step function – the gradient is smooth, so there are no jumps in the
    output values. For example, you wouldn't get a jump from 0 to 1 if the value changed
    slightly (for example, from -0.000001 to 0.0000001).
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function does suffer from a common problem in deep learning – **vanishing
    gradient**. It is a problem that often occurs during backpropagation (a process
    of learning in neural networks, way beyond this chapter's scope). Put simply,
    the gradient "vanishes" during the backward pass, making it impossible for the
    network to learn (tweak weights and biases), as the suggested tweaks are too close
    to zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use Python and NumPy to easily declare and visualize the sigmoid function.
    The procedure is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, you''ll have to define the sigmoid function. Its formula is pretty
    well established: *(1 / (1 + exp(-x)))*, where *x* is the input value.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s how to implement this formula in Python:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now declare a list of values that will serve as an input to this function,
    and then apply `sigmoid_function()` to this list. Here''s an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can visualize the function with the help of the Matplotlib library
    in just two lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see how the function works visually in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Sigmoid activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – Sigmoid activation function
  prefs: []
  type: TYPE_NORMAL
- en: One big disadvantage is that the values returned by the sigmoid function are
    not centered around zero. This is a problem because modeling inputs that are highly
    negative or highly positive gets harder. The hyperbolic tangent function fixes
    this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic tangent function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hyperbolic tangent function (or TanH) is closely related to the sigmoid function.
    It's also a type of activation function that suffers from the vanishing gradient
    issue, but its outputs are centered around zero – as the function ranges from
    -1 to +1\.
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes it much easier to model inputs that are highly negative or highly
    positive. You can use Python and NumPy to easily declare and visualize the hyperbolic
    tangent function. The procedure is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: To start, you'll have to define the hyperbolic tangent function. You can use
    the `tanh()` function from NumPy for the implementation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s how to implement it in Python:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now declare a list of values that will serve as an input to this function,
    and then apply the `tanh_function()` to this list. Here''s an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can visualize the function with the help of the Matplotlib library
    in just two lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see how the function works visually in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Hyperbolic tangent activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – Hyperbolic tangent activation function
  prefs: []
  type: TYPE_NORMAL
- en: To train and optimize neural networks efficiently, you need an activation function
    that acts as a linear function but is non-linear in nature, allowing the network
    to learn the complex relationships in the data. That's where the last activation
    function in this section comes in.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified linear unit function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The rectified linear unit (or ReLU) function is an activation function you can
    see in most modern-day deep learning architectures. Put simply, it returns the
    larger of two values between 0 and x, where x is the input value.
  prefs: []
  type: TYPE_NORMAL
- en: 'ReLU is one of the most computationally efficient functions, and it allows
    the relatively quick finding of the convergence point. You''ll see how to implement
    it in Python next:'
  prefs: []
  type: TYPE_NORMAL
- en: To start, you'll have to define the ReLU function. This can be done entirely
    from scratch or with NumPy, as you only have to find the larger values of the
    two (0 and x).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s how to implement ReLU in Python:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now declare a list of values that will serve as an input to this function,
    and then apply `relu_function()` to this list. Here''s an example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, you can visualize the function with the help of the Matplotlib library
    in just two lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can see how the function works visually in the following diagram:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9 – ReLU activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.9 – ReLU activation function
  prefs: []
  type: TYPE_NORMAL
- en: And that's ReLU in a nutshell. You can use the default version or any of the
    variations (for example, leaky ReLU or parametric ReLU), depending on the use
    case.
  prefs: []
  type: TYPE_NORMAL
- en: You now know enough of the theory to code a basic neural network with Python.
    We haven't covered all of the theoretical topics, so terms such as loss, gradient
    descent, backpropagation, and others may still feel abstract. We'll try to demystify
    them in the hands-on example that's coming up next.
  prefs: []
  type: TYPE_NORMAL
- en: Using neural networks to classify handwritten digits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The "hello world" of deep learning is training a model that can classify handwritten
    digits. That's just what you'll do in this section. It will only require a couple
    of lines of code to implement with the TensorFlow library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can proceed, you''ll have to install TensorFlow. The process is
    a bit different depending on whether you''re on Windows, macOS, or Linux, and
    whether you have a CUDA-compatible GPU or not. You can refer to the official installation
    instructions: [https://www.tensorflow.org/install](https://www.tensorflow.org/install).
    The rest of this section assumes you have TensorFlow 2.x installed. Here are the
    steps to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: To start, you'll have to import the TensorFlow library along with some additional
    modules. The `datasets` module enables you to download data straight from the
    notebook. The `layers` and `models` modules will be used later to design the architecture
    of the neural network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s the code snippet for the imports:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can now proceed with data gathering and preparation. A call to `datasets.mnist.load_data()`
    will download train and test images alongside the train and test labels. The images
    are grayscale and 28x28 pixels in size. This means you'll have a bunch of 28x28
    matrices with values ranging from 0 (black) to 255 (white).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can then further prepare the dataset by rescaling the images – dividing
    the values by 255 to bring everything into a zero-to-one range:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s what you should see in your notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Downloading the MNIST dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_06_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.10 – Downloading the MNIST dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Furthermore, you can inspect the matrix values for one of the images to see
    if you can spot the pattern inside.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following line of code makes it easy to inspect matrices – it prints them
    and rounds all floating-point numbers to a single decimal point:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Inspecting a single image matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_06_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.11 – Inspecting a single image matrix
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Do you notice how easy it is to spot a 5 in the image? You can execute `train_labels[0]`
    to verify.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can continue with laying out the neural network architecture next. As mentioned
    earlier, the input images are 28x28 pixels in size. Artificial neural networks
    can't process a matrix directly, so you'll have to convert this matrix to a vector.
    This process is known as `layers.Dense()` to construct a layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This hidden layer will also need an activation function, so you can use ReLU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, you can add the final (output) layer, which needs to have as many neurons
    as there are distinct classes – 10 in this case.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here''s the entire code snippet for the network architecture:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `models.Sequential` function allows you to stack layers one after the other,
    and, well, to make a network out of the individual layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can view the architecture of your model by calling the `summary()` method
    on it:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Neural network architecture'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_06_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.12 – Neural network architecture
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There's still one thing you need to do before model training, and that is to
    compile the model. During the compilation, you'll have to specify values for the
    optimizer, loss, and the optimization metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These haven''t been covered in this chapter, but a brief explanation of each
    follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Optimizers* – algorithms used to change the attributes of the neural networks
    to reduce the loss. These attributes include weights, learning rates, and so on.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Loss* – a method used to calculate gradients, which are then used to update
    the weights in the neural network.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Metrics* – the metric(s) you''re optimizing for (for example, accuracy).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Going deeper into any of these topics is beyond the scope of this book. There
    are plenty of resources for discovering the theory behind deep learning. This
    chapter only aims to cover the essential basics.
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can compile your neural network by executing the following code:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Now you're ready to train the model. The training subset will be used to train
    the network, and the testing subset will be used for the evaluation. The network
    will be trained for 10 epochs (10 complete passes through the entire training
    data).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can use the following code snippet to train the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Executing the preceding code will start the training process. How long it will
    take depends on the hardware you have and whether you''re using a GPU or CPU.
    You should see something similar to the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.13 – MNIST model training'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16954_06_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.13 – MNIST model training
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After 10 epochs, the accuracy on the validation set was 97.7% – excellent if
    we consider that regular neural networks don't work too well with images.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To test your model on a new instance, you can use the `predict()` method. It
    returns an array that tells you how likely it is that the prediction for a given
    class is correct. There will be 10 items in this array, as there were 10 classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can then call `np.argmax()` to get the item with the highest value:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Testing the MNIST model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16954_06_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.14 – Testing the MNIST model
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the prediction is correct.
  prefs: []
  type: TYPE_NORMAL
- en: And that's how easy it is to train neural networks with libraries such as TensorFlow.
    Keep in mind that this way of handling image classification isn't recommended
    in the real world, as we've flattened a 28x28 image and immediately lost all two-dimensional
    information. CNNs would be a better approach for image classification, as they
    can extract useful features from two-dimensional data. Our artifical neural network
    worked well here because MNIST is a simple and clean dataset – not something you'll
    get a whole lot of in your job.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you'll learn the differences in approaching classification
    and regression tasks with neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks in regression versus classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you've done any machine learning with scikit-learn, you know there are dedicated
    classes and models for regression and classification datasets. For example, if
    you would like to apply a decision tree algorithm to a classification dataset,
    you would use the `DecisionTreeClassifier` class. Likewise, you would use the
    `DecisionTreeRegressor` class for regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: But what do you do with neural networks? There are no dedicated classes or layers
    for classification and regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, you can accommodate by tweaking the number of neurons in the output
    layer. Put simply, if you're dealing with regression tasks, there has to be a
    single neuron in the output layer. If you're dealing with classification tasks,
    there will be as many neurons in the output layer as there are distinct classes
    in your target variable.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you saw how the neural network in the previous section had 10 neurons
    in the output layer. The reason is that there are 10 distinct digits, from zero
    to nine. If you were instead predicting the price of something (regression), there
    would be only a single neuron in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: The task of the neural network is to learn the adequate parameter values (weights
    and biases) to produce the best output value, irrespective of the type of problem
    you're solving.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter might be hard to process if this was your first encounter with
    deep learning and neural networks. Going over the materials a couple of times
    could help, but it won't be enough to understand the topic fully. Entire books
    have been written on deep learning, and even on small subsets of deep learning.
    Hence, covering everything in a single chapter isn't possible.
  prefs: []
  type: TYPE_NORMAL
- en: Still, you should have the basic theory behind the concepts of neurons, layers,
    and activation functions, and you can always learn more on your own. The following
    chapter, [*Chapter 7*](B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086)*, Neural
    Network Classifier with TPOT*, will show you how to connect neural networks and
    pipeline optimization, so you can build state-of-the-art models in a completely
    automated fashion.
  prefs: []
  type: TYPE_NORMAL
- en: As always, please feel free to explore the theory and practice of deep learning
    and neural networks on your own. It is definitely a field of study worth exploring
    further.
  prefs: []
  type: TYPE_NORMAL
- en: Q&A
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you define the term "deep learning"?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between traditional machine learning algorithms and algorithms
    used in deep learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: List and briefly describe five types of neural networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you figure out how to calculate the number of trainable parameters in a
    network given the number of neurons per layer? For example, a neural network with
    the architecture [10, 8, 8, 2] has in total 178 trainable parameters (160 weights
    and 18 biases).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name four different activation functions and briefly explain them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In your own words, describe *loss* in neural networks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain why modeling imagine classification models with regular artificial neural
    networks isn't a good idea.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
